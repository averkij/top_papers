{
    "paper_title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
    "authors": [
        "Yida Xue",
        "Mingjun Mao",
        "Xiangyuan Ru",
        "Yuqi Zhu",
        "Baochang Ren",
        "Shuofei Qiao",
        "Mengru Wang",
        "Shumin Deng",
        "Xinyu An",
        "Ningyu Zhang",
        "Ying Chen",
        "Huajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym."
        },
        {
            "title": "Start",
            "content": "Work in progress. OCEANGYM: BENCHMARK ENVIRONMENT FOR UNDERWATER EMBODIED AGENTS Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen Zhejiang University National University of Singapore State Key Laboratory of Ocean Sensing, Zhejiang University {xueyida,zhangningyu,huajunsir}@zju.edu.cn https://oceangpt.github.io/OceanGym"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce OCEANGYM, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OCEANGYM encompasses eight realistic task domains and unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing high-fidelity, rigorously designed platform, OCEANGYM establishes testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking decisive step toward intelligent agents capable of operating in one of Earths last unexplored frontiers1. 5 2 0 2 0 3 ] . [ 1 6 3 5 6 2 . 9 0 5 2 : r Figure 1: Illustration of OCEANGYM. The OCEANGYM benchmark introduces unified agent framework across 8 real-world underwater scenarios. The agent interprets language instruction, fuses optical and sonar imagery, and controls Autonomous Underwater Vehicles (AUVs). Corresponding Author. 1The code and data are available at https://github.com/OceanGPT/OceanGym. 1 Work in progress."
        },
        {
            "title": "INTRODUCTION",
            "content": "As Richard S. Sutton famously noted, we are entering an era of experience (Silver & Sutton, 2025). Embodied agents equipped with language models (Zhao et al., 2023; OpenAI, 2024) are emerging as central paradigm for intelligent systems, as they accumulate and leverage experience through continuous interaction to close the perception-decision-action loop in physical or simulated environments (Gupta et al., 2021; Ding et al., 2024; Liu et al., 2025). Unlike static decision or generative models, these agents must integrate rich multimodal sensory streams and execute continuous-control policies to achieve long-horizon objectives. This necessitates unified treatment of perceptual representation, planning, online inference, and sequential policy optimization (Fung et al., 2025). Significant progress has been demonstrated across diverse domains, including robotic manipulators (Anderson et al., 2018; Caesar et al., 2020; Vasudevan et al., 2021; Gao et al., 2024), drones (Wang et al., 2024a; Lee et al., 2025; Gao et al., 2025b), and autonomous vehicles (Ma et al., 2025b). In contrast, underwater2 embodied agents remain largely unexplored despite their critical scientific and engineering importance (Visbeck, 2018; Kelly et al., 2022; Zheng et al., 2023; Li et al., 2024a; Gao et al., 2025a). Deploying embodied agents in marine environments offers unique opportunities for ocean exploration, offshore resource development, environmental monitoring, and subsea rescue operations. These tasks impose stringent requirements on the robustness and reliability of autonomous platforms, making the development of agents capable of functioning under real marine conditions key bridge between simulated research and practical deployment (Ma et al., 2025a). Challenges. Underwater embodied agents face distinct challenges that set them apart from overland and aerial systems. Perceptually, poor visibility and low-light conditions, combined with the inherent limitations of optical sensors, compel reliance on sonar, inertial measurements, and other sparse modalities (Li et al., 2024b; Aubard et al., 2025). These heterogeneous and noisy observations complicate sensor fusion and perception. Environmentally, deep-sea and offshore settings are largely unexplored, with unstable localization, absent prior knowledge, and dynamic currents. The lack of prior knowledge prevents effective environmental modeling, requiring agents to reason under circumstances of extreme partial observability and uncertainty (Sariman et al., 2025). Together, these factors constrain the development of underwater agents, leaving their capabilities in early stages. Building OceanGym. To address these challenges, we introduce OCEANGYM, an open environment benchmark for underwater embodied agents. OCEANGYM constructs comprehensive marine environment spanning approximately 800m 800m (length width), with dynamically adjustable depth to simulate varying lighting conditions. The platform incorporates eight distinct task domains designed to reflect real-world operational requirements. Additionally, it provides multimodal LLM-based agent framework that integrates perception, memory, and action decision-making capabilities for controlling Autonomous Underwater Vehicles (AUVs). The benchmark unifies perception and decision-making in simulated underwater scenarios, where agents must infer target states from contextual cues or multi-view sensor data and execute complex behaviors such as search, inspection, and docking. By simulating these environments, OCEANGYM enables systematic evaluation of language models capabilities in underwater embodied settings and provides pathway for transferring learned skills to real-world underwater vehicles. We discuss the limitations of OCEANGYM in 3.3. Benchmark Results and Insights. Extensive experiments on OCEANGYM reveal that Multi-modal Large Language Models (MLLMs) exhibit significant gaps compared to human experts, particularly under low-visibility conditions (decision-making success rate drops to 14.8%). Agents frequently struggle to interpret sonar data accurately, distinguish objects in complex environments, and maintain consistent devision strategies over extended missions. Limitations also arise in memory retention and adaptability when objects are occluded or conditions change dynamically. These findings highlight persistent challenges for embodied AI in underwater environments and underscore the need for continued research in robust perception, reasoning, and control under extreme uncertainty."
        },
        {
            "title": "2 OCEANGYM",
            "content": "OCEANGYM is high-fidelity embodied underwater environment that simulates realistic ocean setting with diverse scenes. As illustrated in Figure 2, OCEANGYM establishes robust bench2Underwater refers to the ocean environment throughout this work and is not further specified. 2 Work in progress. Figure 2: OCEANGYM Tasks. OCEANGYM comprises Perception Tasks (divided into Multi-view Perception and Context-based Perception settings) and Decision Tasks for evaluating embodied agents. mark for evaluating autonomous agents through series of challenging tasks, encompassing various perception analyses and decision-making navigation. OCEANGYM facilitates these evaluations by enabling MLLM-driven agents with multi-modal perception and parameterized action spaces. 2.1 OCEANGYM ENVIRONMENT We develop OCEANGYM atop Unreal Engine (UE) 5.3 (Epic Games, 2025), providing comprehensive set of underwater environments, including both natural terrains and engineered structures. The environment features several semantic regions such as open water, seabed plains, underwater cliffs, pipeline networks, wreckage sites, and energy infrastructure zones. Each region is modeled with realistic physical and geometric properties, incorporating elements like oil pipelines, chemical waste barrels, submerged shipwrecks, electrical equipment, wind turbine foundations, and aircraft debris (more details in Appendix A.4). These elements are built using intricate 3D assets based on real-world references, ensuring accurate representation of structural and material characteristics. We also simulate different lighting conditions by controlling the depth of the underwater environment. In our experiments, we configure two representative depths to emulate shallow (50m) and deep water (500m)3 scenarios. For each task, the starting position is randomly selected to vary task difficulty, because tasks become increasingly challenging when the start point is far from the target, the target is initially out of view, or the initial orientation faces away from the goal. Furthermore, OCEANGYM is completely scalable, allowing users to customize the environment by selecting new depths to simulate more complex lighting conditions, or by adding new props and designing additional tasks based on the existing environment, thereby extending the diversity and difficulty. 2.2 UNDERWATER EMBODIED AGENTS We model the agents controlperception loop as Partially Observable Markov Decision Process (POMDP) enhanced with contextual memory. At each time step t, the agent processes the task specification = (Itarget, c), where Itarget is visual reference image of the target and provides its textual identity and features. It also considers language instruction L, synchronized observations Ot, and its memory state mt. These elements collectively shape the agents perception and objectives. With the above information, the agent must generate either textual perception response yt for perception tasks, or determine control action at for decision tasks. Here, at is 3For deep water scenarios, optical sensing relies on artificial light sources, with visibility range of approximately 010m. 3 Work in progress. discrete action selected from the action space A. decision trajectory is described by σ = (O1, a1, s1, m1, . . . , mt1, Ot, at, st). In this sequence, Oi represents the observations, ai the actions, si the states, and mi the memory states at each time step i. The episode concludes when the target is achieved or when the maximum time tmax is exhausted. The ultimate reward is based on the successful score of the task, as defined in 2.5. State and Observation. The agents state at time is given by st = {(xt, yt, zt), (ϕt, θt, ψt)}, where (xt, yt, zt) represent the agents 3D positional coordinates, and (ϕt, θt, ψt) denote the roll, pitch, and yaw angles, respectively. At each timestep, the agent receives synchronized RGB and sonar images from sensors oriented in six different directions. The directions are defined by the set Dsensor = {f, b, l, r, u, d}, corresponding to front, back, left, right, up, and down. The RGB images from these directions are denoted as OR t,d}dDsensor, and the sonar images are represented similarly as OS t,d}dDsensor. Therefore, the complete observation at time can be expressed as combination of both image sets, Ot = (OR = {oR = {oS , OS ). Action Space. The agents action direction set is defined as Daction = {f, b, l, r, u, d, rl, rr}, which encompasses both directional and rotational movements. Directional actions include translations along the primary axes: forward (f ), backward (b), left (l), right (r), up (u), and down (d). Rotational actions consist of rotate left (rl) and rotate right (rr). At each timestep t, the agent selects an action at from this discrete set and applies control magnitude δ R0 to determine the execution intensity. Memory. Memory systems play crucial role in storing and structuring past information, thereby enhancing the agents resilience in dynamic and partially observable environments (Xi et al., 2025; Liu et al., 2023; Zhong et al., 2024; Wu et al., 2024; Maharana et al., 2024). OCEANGYM agent maintains an explicit memory mt, structured as sliding window that records the last steps: mt = {(dtk, atk) = 1, 2, . . . , K}. (1) Within this memory structure, dtk denotes the textual description at time tk, and atk represents the corresponding action executed. The perception module Pθ, modeled as an MLLM, generates summary based on the current context and the interaction history {(Ok, ak)}t k=tK: dt = Pθ (cid:0){(Ok, ak)}t k=tK (cid:1) . (2) This summary is subsequently used to refresh the memory: mt+1 = update(mt, dt, at). Memory-augmented Markov Process. To maintain the Markov property while incorporating memory, we introduce an augmented hidden state st = (st, mt). The state transition is then modeled as: p(st+1 st, at, δ), (3) where p( ) represents the augmented state transition function of the environment. This function captures both the evolution of memory, ensuring that the system remains Markovian despite the added complexity of memory integration. Agent Policy. The agent policy is multimodal, memory-augmented mapping parameterized by an MLLM with parameter vector θ: πθ(at, yt L, Ot, mt, , δ), (4) Concretely, for perception tasks, we sample an answer yt πθ(y L, Ot, mt, , δ), and for decision tasks, we sample an action at πθ(a L, Ot, mt, , δ). An episode terminates at time when the agent either outputs STOP command (for decision tasks) or provides final answer to the question (for perception tasks) or when the maximum time tmax is reached. The policy, combined with the memory-augmented transition dynamics, induces the trajectory distribution: Pθ(σ L, ) = 1 (cid:89) t= πθ(at, yt L, Ot, mt, , δ) p(st+1 st, at, δ), (5) where σ represents the trajectory of the agent through the state space over time, influenced by the specified policy πθ and the transition model p(st+1 st, at, δ). 4 Work in progress."
        },
        {
            "title": "2.3 OCEANGYM PERCEPTION TASKS",
            "content": "The perception tasks are categorized into two settings: Multi-View Perception and Context-based Perception. These tasks primarily use RGB images as input, with sonar data added in certain experiments to enhance perception. The data for each setting are collected by human operators and designed to evaluate different aspects of MLLMs perceptual abilities. More details in Appendix A.3. Multi-view Perception Setting. This setting evaluates the agents ability to interpret visual information from multiple synchronized viewpoints. At each timestep t, the agent captures set of six simultaneous RGB images, denoted as OR t,d}dDsensor, where refers to the different sensor orientations: front, back, left, right, up, and down. The objective is to consistently identify and localize underwater objects across these varied viewpoints. This setting examines whether objects visible from certain angles can be correctly perceived when the visual inputs from all directions are sequentially processed by the MLLM, thereby evaluating robustness to viewpoint variations. = {oR from fixed orientation, forming chronological sequence OR Context-based Perception Setting. This setting assesses the agents ability to perceive and interpret sequential observations gathered during navigation. At each timestep t, the agent captures an RGB image oR t=1, where is the total number of timesteps. The agent must track and understand changes over time, ensuring consistent and accurate identification and localization of underwater objects. This evaluation emphasizes temporal consistency and the agents capacity to build coherent perception from stable visual perspective in dynamic and complex underwater environments. 1:m = {oR }m Running Example: Shipwreck Area Perception Task: (1) Multi-view perception setting. The agent receives perception images (visual and sonar) from different sensors at the same time to determine the target, such as whether it is shipwreck. (2) Context-based perception setting. The agent analyzes images one by one along trajectory from fixed viewpoint to identify the target. Decision Task: The agent receives task instruction, such as Search for sunken ship, and then explores the area for 30 minutes to complete it. 2.4 OCEANGYM DECISION TASKS Decision Task Definition. Decision tasks evaluate decision-making in continuous 3D environments, where agents must integrate multimodal sensory input with task specifications. Each episode begins from an initial state s0 = {(x0, y0, z0), (ϕ0, θ0, ψ0)} and requires the agent to reach the target defined by . The agent must combine sensory observations Ot, temporal memory, and goal information to execute precise maneuvers in cluttered, low-visibility environments. Key parameters 4. The decision of the task include the decision interval tinterval and the tasks limited duration tmax interval tinterval determines how frequently the agent makes decisions and executes actions. The total task duration tmax sets the temporal constraint, within which the agent must meet its objectives, thereby influencing the planning and movement strategies employed by the agent. Compared with grid-based navigation benchmarks, this task emphasizes continuous control and realistic underwater environment, reflecting the challenges of autonomous exploration and inspection tasks. Decision Task Design. To evaluate the decision-making capabilities of MLLMs in marine environments, we design eight representative task scenarios that are commonly used in actual underwater operations (more details in Appendix A.4). The task construction methods are divided into two categories: detection tasks and tracking tasks. Detection tasks focus on assessing the ability of MLLMs to locate specific underwater objects, including searching for large targets such as sunken ships or aircraft wreckage, and smaller targets like scientific research robots. Tracking tasks focus on evaluating the ability of MLLMs to perform inspection and monitoring tasks underwater, including scenarios like pipeline inspection and platform approaches. To further investigate the performance in challenging environments, four representative tasks are conducted under low light deep-sea conditions. In the experimental design, systematic initial positioning strategy is adopted for each task. The first two starting positions remain consistent across all tasks to ensure experimental re4By default, tinterval takes 30 seconds and tmax takes 0.5 hours in decision tasks. 5 Work in progress. Table 1: Performance of perception tasks across different models and conditions. Values represent accuracy percentages (%). Adding sonar means using both RGB and sonar images. Model Shallow Water Environment (High Illumination) Deep Water Environment (Low Illumination) Multi-View Perception Context-based Perception Avg Multi-View Perception Context-based Perception Vision +Sonar Vision +Sonar Vision +Sonar Vision +Sonar GPT-4o-mini Gemini-2.5 Qwen2.5-VL-7B Minicpm-4.5 34.55 29.09 58.18 52.73 34.55 30.91 43.64 43.64 20.00 50.00 56.67 36.67 33.33 33.33 70.00 23. 30.61 35.83 57.12 39.09 14.55 9.09 27.27 29.09 Human 100.00 100.00 100. 100.00 100.00 94.55 20.00 5.45 20.00 23.64 98.18 3.33 20.00 33.33 43. 86.67 6.67 30.00 33.33 13.33 90.00 Avg 11.14 16.14 28.48 27.35 92. producibility. The third starting position is randomly generated within the operational boundary to evaluate the adaptability of the agent to different initial conditions. 2.5 EVALUATION METRICS Perception Task Evaluation. We evaluate model performance using exact match accuracy. Let yi denote the ground-truth answer and ˆyi represent the models predicted answer for the i-th sample. Acc = 100% (cid:88) i=1 [ˆyi = yi] , (6) For multiple-choice items, yi and ˆyi are treated as sets and equality requires an exact set match. Decision Task Evaluation. We evaluate decision tasks using distance-based scoring method. Each episode ends when the agent issues STOP command or reaches the time limit tmax. For task with evaluation points, let pi be the i-th target location. If the target is detected, we use the closest position from the agents trajectory to pi; otherwise, we use the agents final position. The Euclidean distance is computed as di = ˆpi pi2, and the score for each point is defined as: 100, 100 0, τ2 di τ2 τ τ1 < di τ2, di τ1, Si = (7) , di > τ2, where the distance thresholds are set to τ1 = 30 meters and τ2 = 100 meters by default. The total score is weighted sum as Stotal = (cid:80)n i=1 wiSi, where wi are task-specific weights5."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EXPERIMENTAL SETTINGS To thoroughly evaluate the perception and decision capabilities of MLLMs in underwater environments, we conduct experiments using variety of models6. Among the open-source models, we assess MiniCPM-V-4.5 (Yao et al., 2025) and Qwen2.5-VL-7B (Bai et al., 2025). For proprietary models, we test GPT-4o-mini (OpenAI, 2024) and Gemini-2.5 (Gemini Team, 2024). We run each task three times and report the average results. 3.2 MAIN RESULTS Perception Results. The comprehensive results for perception tasks are summarized in Table 1. In shallow, well-illuminated water environments, Qwen2.5-VL-7B achieves the strongest overall 5For single-point task w1 = 1.0; for two points (w1, w2) = (0.6, 0.4); for three points (w1, w2, w3) = (0.6, 0.2, 0.2). 6Note that our setup is designed to real-world deployment of MLLMs in the future; accordingly, we prioritize smaller-scale models that can run natively on edge devices. 6 Work in progress. Table 2: Performance in decision tasks requiring autonomous completion by MLLM-driven agents. Task Shallow Water Environment (High Illumination) GPT-4o-mini Gemini-2.5 Qwen2.5-VL-7B Model Human Locate the robot Locate the oil drums Locate the electrical box Search for sunken ship Search for the aircraft Inspect oil pipe Inspect the wind turbine Docking Average Deep Water Environment (Low Illumination) Locate oil drums Search for sunken ship Inspect the oil pipe Inspect the wind turbine Average 8.910.1 11.119.2 36.621.9 13.419.3 16.917.8 27.123.6 13.914.33 19.233.28 18.419.9 5.69.69 12.814.48 15.815.5 25.116.0 14.813.9 0.00.00 3.56.0 15.927.4 20.514.3 11.715.6 18.315.8 25.122.1 19.433.6 14.416.1 0.00.0 8.214.1 6.611.4 10.610.0 6.48.8 7.813.5 5.79.8 8.715.0 10.310.3 7.810.0 30.825.2 14.717.0 8.37.2 11.813. 0.00.0 3.45.8 21.725.3 0.40.6 6.48.4 100 100 100 100 100 100 100 100 100 40.8 100 78.2 100 69.6 Figure 3: Performance comparison between human and MLLMs after adding sonar and sonar reference examples for objects in deep water environments. Figure 4: Case analysis in perception tasks. Agents are susceptible to perception errors under challenging conditions such as low-light environments, multi-object scenarios, and occlusions. performance among the evaluated MLLMs, with an average accuracy of 57.12%. Multi-view perception generally yields higher accuracy than the context-based setting across most models, likely because targets of similar size across viewpoints are easier to interpret, whereas distant objects in sequential views can introduce ambiguity. Under deep water conditions with low illumination, all models exhibit significant performance degradation, though Qwen2.5-VL-7B remains the most robust (28.48% average accuracy), followed by Minicpm-4.5 (27.35%). Notably, incorporating sonar data does not consistently improve performance across models or tasks (further analysis in 3.3). Decision Results. Performance on decision tasks is shown in Table 2. Several tasks resulted in zero scores, indicating extreme difficulty due to small object size or time constraints. GPT-4o-mini achieves the best average performance in both shallow (18.4%) and deep water (14.8%) environments, with Gemini-2.5 ranking second under shallow conditions (14.4%). Performance declines markedly in deep water, where Gemini-2.5 and Qwen2.5-VL-7B both average 6.4%. Human performance substantially outperforms all models, reaching 100% in shallow water and 69.6% in deep water, underscoring the gap between current MLLM-driven decision-making and human proficiency. 3.3 ANALYSIS MLLM agents struggle to exploit sonar data for enhanced underwater perception, in stark contrast to humans who leverage it effectively. To investigate the role of sonar data in deep-water environments, we compare the performance of human experts with the two MLLMs, Qwen2.5-VL and Minicpm-4.5, on perception tasks. Specifically, we either let the models directly comprehend 7 Work in progress. Figure 5: Scaling analysis performance over time in decision tasks. Figure 6: Impact of different memory transfer mechanisms on model performance. sonar images or provide them with human-annotated interpretations as prompts. As shown in Figure 3, human experts consistently benefit from incorporating sonar data across tasks. By contrast, MLLMs exhibit only limited gains when using raw sonar images, and this gap becomes even more pronounced when reference sonar images of each object are introduced. This limitation likely stems from the current inability of MLLMs to effectively comprehend sonar imagery, as well as potential constraints in the sonar simulation within OceanGym, an issue we discuss in 3.3. Extended exploration enhances an agents acquisition of environmental knowledge and task performance, following scaling law that eventually plateaus. We analyze the relationship between navigation performance and operational duration using the representative MLLMs, across both shallowand deep-water scenarios. The performance was evaluated over durations of 0.5, 1, 1.5, 2, and 3 hours. As shown in Figure 5, performance initially improves with longer operation time, consistent with prior studies on test-time scaling (Zhang et al., 2025; Zhu et al., 2025), but eventually plateaus. This plateau reflects inherent limitations in perception, memory, and reasoning, as well as lack of intrinsic curiosity to explore new regions. These findings underscore the need to improve both fundamental MLLM capabilities and agent strategies, such as enhanced memory and long-horizon planning, to break through performance ceilings in embodied environments. Memory transfer enables agents to leverage past experience to tackle new challenges. We investigate whether knowledge and experience accumulated from previous tasks (Hou et al., 2024; Hu et al., 2024a; Tan et al., 2025; Tang et al., 2025) can enhance performance in new tasks. Experiments are conducted in both shallow water and deep water environments, evaluating two transfer conditions: within-task transfer (different starting points) and cross-task transfer (different but related tasks). As shown in Figure 6, memory transfer improves decision-making performance in shallow water environments under both transfer conditions. However, in the more challenging deep water environment, only cross-task transfer demonstrates stable performance improvements, while withintask transfer shows limited benefits. This suggests that more appropriate prior experiences provide more robust guidance under perceptually degraded conditions. Transfer learning helps compensate for perceptual limitations by providing informed priors about environmental structure and effective navigation strategies. These findings underscore the importance of developing adaptive memory retrieval mechanisms that can selectively leverage relevant past experiences to enhance decisionmaking in autonomous underwater agents operating under diverse environmental conditions. 8 Work in progress. Figure 7: Case analysis in decision tasks. Case analysis. We present case analyses and illustrate failure cases in Figure 4, mainly due to: (1) Occlusions, where targets are partially blocked; (2) Multi-object Scenes, causing identification and localization ambiguities; and (3) Low Illumination, which severely reduces vision-based perception accuracy. Figure 7 shows common decision task failures, primarily from: (1) Perception Errors, where inaccurate detection leads to wrong actions; and (2) Memory Forgetting, where the agent cannot retain crucial past information, such as visited locations or previous decisions. Discusses and Limitations of OCEANGYM. OCEANGYM offers versatile testbed for underwater embodied agents, though it cannot fully replicate real-world conditions as factors like currents, salinity, marine life, and sonar noise remain imperfectly modeled. Despite these constraints, OCEANGYM supports synthetic data generation and facilitates reinforcement learning with rich feedback, and serves as sim-to-real bridge for deploying models on AUVs (See A.2)."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Embodied Simulations. Embodied intelligence describes artificial intelligence systems whose intelligent behavior emerges through continuous physical and sensory interactions with the environment (Gupta et al., 2021; Ding et al., 2024; Shi et al., 2024). Simulation platforms are essential for advancing such systems across ground, aerial, and marine domains (Liu et al., 2024; Han et al., 2025; Aldhaheri et al., 2025). In ground applications, platforms like Matterport3D (Chang et al., 2017), House3D (Wu et al., 2018), and Habitat (Manolis Savva et al., 2019) provide realistic indoor and outdoor environments for navigation, scene understanding, and human-robot interaction research. Aerial robotics benefits from simulators such as AirSim (Shah et al., 2018) and OpenUAV (Wang et al., 2024a), which offer high-fidelity simulations with accurate physics and sensor models. Similarly, marine robotics utilizes tools like HoloOcean (Potokar et al., 2022) and MarineGym (Chu et al., 2025), which model hydrodynamic effects and underwater dynamics. MLLM-driven Embodied Agents. Building upon the rapid advancement of LLMs (Achiam et al., 2023; Touvron et al., 2023; Chiang et al., 2023; Yang et al., 2025a), the emergence of MLLMs (OpenAI, 2024) has further strengthened agent capabilities by incorporating visual understanding for multimodal perception. Despite impressive results in various agent applications (Hu et al., 2024b; Ning et al., 2025), MLLM-driven agents still face substantial challenges in real-world and simulated embodied environments. Key difficulties persist in spatial cognition (Prasad et al., 2023; Du et al., 2024; Tong et al., 2024; Shiri et al., 2024; Zheng et al., 2024; Dang et al., 2025; Yang et al., 2025c; Li et al., 2025), object navigation (Wang et al., 2024b; Khanna et al., 2024; Guo et al., 2025; Qiao et al., 2025; Cheng et al., 2025), and robotic manipulation (Zheng et al., 2022; Yang et al., 2025b; Wang et al., 2025). To evaluate agent capabilities, embodied benchmarks have been developed across diverse settings, including indoor (Anderson et al., 2018; Wu et al., 2018), urban (Chen et al., 2019; Caesar et al., 2020; Vasudevan et al., 2021; Gao et al., 2024), aerial (Lee et al., 2025; Wang et al., 2024a; Gao et al., 2025b), and real-world scenarios (Zhao et al., 2025). 9 Work in progress."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce OCEANGYM, the first bechmark environment specifically designed for underwater embodied agents. Our experiments reveal significant limitations in current MLLMs. We hope OCEANGYM can bridge the gap between simulated research and real-world deployment, offering foundation for developing robust autonomous systems for marine applications."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research is conducted in strict compliance with established ethical guidelines and best practices in scientific research. All data employed in this study are obtained from publicly accessible datasets, with no utilization of proprietary or confidential information. Proper and accurate citations are provided for all data sources referenced throughout this paper. We emphatically advise all users to maintain the highest ethical standards when utilizing our dataset, ensuring principles of fairness, transparency, and responsibility in their research applications. Any use of the dataset that may potentially cause harm or adversely affect societal welfare is expressly prohibited. REPRODUCIBILITY STATEMENT We provide data from our benchmark under file size limitation, along with the corresponding evaluation code, in the supplementary materials. Detailed descriptions of the environment setup and data construction procedures are available in 2.1, 2.3 and 2.4. Additional data details and comprehensive benchmark statistics can be found in Appendix A.3 and Appendix A.4. Specific configurations of the tested models are documented in Section 3.1."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sara Aldhaheri, Yang Hu, Yongchang Xie, Peng Wu, Dimitrios Kanoulas, and Yuanchang Liu. Underwater robotic simulators review for autonomous system development, 2025. URL https: //arxiv.org/abs/2504.06245. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Martin Aubard, Ana Madureira, Luıs Teixeira, and Jose Pinto. Sonar-based deep learning in underwater robotics: Overview, robustness, and challenges. IEEE Journal of Oceanic Engineering, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. doi: 10.48550/ARXIV.2502.13923. URL https://doi.org/ 10.48550/arXiv.2502.13923. Philip Ball, Bauer, Belletti, et al. Genie 3: new frontier for world models, 2025. Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1162111631, 2020. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV), 2017. 10 Work in progress. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural In 2019 IEEE/CVF language navigation and spatial reasoning in visual street environments. Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1253012539, 2019. doi: 10.1109/CVPR.2019.01282. Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, et al. Embodiedeval: Evaluate multimodal llms as embodied agents. arXiv preprint arXiv:2501.11858, 2025. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Shuguang Chu, Zebin Huang, Yutong Li, Mingwei Lin, Ignacio Carlucho, Yvan R. Petillot, and Canjun Yang. Marinegym: high-performance reinforcement learning platform for underwater robotics, 2025. URL https://arxiv.org/abs/2503.09203. Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, and Deli Zhao. Rynnec: Bringing mllms into embodied world, 2025. URL https: //arxiv.org/abs/2508.14160. Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al. Understanding world or predicting future? comprehensive survey of world models. arXiv preprint arXiv:2411.14499, 2024. Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. CoRR, abs/2406.05756, 2024. doi: 10.48550/ARXIV.2406.05756. URL https://doi.org/10. 48550/arXiv.2406.05756. Epic Games. Unreal engine, 2025. URL https://www.unrealengine.com. Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, Herve Jegou, Alessandro Lazaric, et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025. Chen Gao, Baining Zhao, Weichen Zhang, Jinzhu Mao, Jun Zhang, Zhiheng Zheng, Fanhang Man, Jianjie Fang, Zile Zhou, Jinqiang Cui, Xinlei Chen, and Yong Li. Embodiedcity: benchmark platform for embodied agent in real-world city environment, 2024. URL https://arxiv. org/abs/2410.09604. Yuan Gao, Ruiqi Shu, Hao Wu, Fan Xu, Yanfei Xiang, Ruijian Gou, Qingsong Wen, Xian Wu, and Xiaomeng Huang. Neuralom: Neural ocean model for subseasonal-to-seasonal simulation. arXiv preprint arXiv:2505.21020, 2025a. Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, and Xuelong Li. Openfly: comprehensive platform for aerial vision-language navigation, 2025b. URL https://arxiv.org/abs/2502.18041. Gemini Team. Gemini: family of highly capable multimodal models, 2024. Mingning Guo, Mengwei Wu, Jiarun He, Shaoxian Li, Haifeng Li, and Chao Tao. BEDI: comprehensive benchmark for evaluating embodied agents on uavs. CoRR, abs/2505.18229, 2025. doi: 10.48550/ARXIV.2505.18229. URL https://doi.org/10.48550/arXiv.2505. 18229. Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-Fei. Embodied intelligence via learning and evolution. Nature communications, 12(1):5721, 2021. 11 Work in progress. Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An, Changwei Wang, Li Guo, Weiliang Meng, Xiaopeng Zhang, Rongtao Xu, and Shibiao Xu. Multimodal fusion and vision-language models: survey for robot vision. CoRR, abs/2504.02477, 2025. doi: 10.48550/ ARXIV.2504.02477. URL https://doi.org/10.48550/arXiv.2504.02477. Yuki Hou, Haruki Tamoto, and Homei Miyashita. my agent understands me better: Integrating dynamic human-like memory recall and consolidation in llm-based agents. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pp. 17, 2024. Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model, 2024a. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shawn Wang, Xinchen Xu, Shuofei Qiao, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, and Fei Wu. Os agents: survey on mllm-based agents for general computing devices use. https://github. com/OS-Agent-Survey/OS-Agent-Survey/, 2024b. Rachel Kelly, Laura Elsler, Andrei Polejack, Sander van der Linden, Kajsa Tonnesson, Sarah Schoedinger, Francesca Santoro, Gretta Pecl, Michael Palmgren, Patrizio Mariani, et al. Empowering young people with climate and ocean science: Five strategies for adults to consider. One Earth, 5(8):861874, 2022. Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, and Roozbeh Mottaghi. Goatbench: benchmark for multi-modal lifelong navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1637316383, 2024. Jungdae Lee, Taiki Miyanishi, Shuhei Kurita, Koya Sakamoto, Daichi Azuma, Yutaka Matsuo, and Nakamasa Inoue. Citynav: large-scale dataset for real-world aerial navigation, 2025. URL https://arxiv.org/abs/2406.14240. Yun Li, Yiming Zhang, Tao Lin, Xiangrui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready for precise spatial-temporal world understanding?, 2025. URL https:// arxiv.org/abs/2503.23765. Zhe Li, Ronghui Xu, Jilin Hu, Zhong Peng, Xi Lu, Chenjuan Guo, and Bin Yang. Ocean significant wave height estimation with spatio-temporally aware large language models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pp. 3892 3896, 2024a. Zikang Li, Zhuojun Xie, Puhong Duan, Xudong Kang, and Shutao Li. Dual spatial attention network IEEE Sensors Journal, 24(5):69987008, for underwater object detection with sonar imagery. 2024b. Huaping Liu, Di Guo, and Angelo Cangelosi. Embodied intelligence: synergy of morphology, action, perception and learning. ACM Computing Surveys, 57(7):136, 2025. Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-memory: Recalling and post-thinking enable llms with long-term memory, 2023. Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, and Liang Lin. Aligning cyber space with physical world: comprehensive survey on embodied ai, 2024. URL https://arxiv.org/abs/2407.06886. Dong Ma, Ye Li, Teng Ma, and Antonio Pascoal. The state of the art in key technologies for autonomous underwater vehicles: review. Engineering, 2025a. Yunsheng Ma, Wenqian Ye, Can Cui, Haiming Zhang, Shuo Xing, Fucai Ke, Jinhong Wang, Chenglin Miao, Jintai Chen, Hamid Rezatofighi, et al. Position: Prospective of autonomous driving-multimodal llms world models embodied intelligence ai alignment and mamba. In Proceedings of the Winter Conference on Applications of Computer Vision, pp. 10101026, 2025b. Work in progress. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents, 2024. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao yong Wei, Shanru Lin, Hui Liu, Philip S. Yu, and Qing Li. survey of webagents: Towards nextgeneration ai agents for web automation with large foundation models, 2025. URL https: //arxiv.org/abs/2503.23350. OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Easton Potokar, Spencer Ashford, Michael Kaess, and Joshua Mangelson. Holoocean: An underwater robotics simulator. In 2022 International Conference on Robotics and Automation (ICRA), pp. 30403046. IEEE, 2022. Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Rephrase, augment, reason: Visual grounding of questions for vision-language models. arXiv preprint arXiv:2310.05861, 2023. Yanyuan Qiao, Haodong Hong, Wenqi Lyu, Dong An, Siqi Zhang, Yutong Xie, Xinyu Wang, and Qi Wu. Navbench: Probing multimodal large language models for embodied navigation, 2025. URL https://arxiv.org/abs/2506.01031. Cagatay Sariman, Ahmed Hallawa, and Anke Schmeink. Ur-earl: framework for designing underwater robots using evolutionary algorithm-driven reinforcement learning. Ocean Engineering, 321:120402, 2025. Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics: Results of the 11th International Conference, pp. 621635. Springer, 2018. Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre Cˆote, and Bang Liu. OPEx: component-wise analysis of LLM-centric agents in embodied instruction following. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 622636, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 37. URL https://aclanthology.org/2024.acl-long.37/. Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 2144021455. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.emnlp-main.1195. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 1, 2025. Xiaoyu Tan, Bin Li, Xihe Qiu, Chao Qu, Wei Chu, Yinghui Xu, and Yuan Qi. Meta-agent-workflow: In Streamlining tool usage in llms through workflow construction, retrieval, and refinement. Companion Proceedings of the ACM on Web Conference 2025, WWW 25, pp. 458467, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400713316. doi: 10. 1145/3701716.3715247. URL https://doi.org/10.1145/3701716.3715247. Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, and Wangchunshu Zhou. Agent kb: Leveraging cross-domain experience for agentic problem solving, 2025. URL https://arxiv.org/abs/2507.06229. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 9568 9578. IEEE, 2024. doi: 10.1109/CVPR52733.2024.00914. URL https://doi.org/10. 1109/CVPR52733.2024.00914. 13 Work in progress. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971. Arun Balajee Vasudevan, Dengxin Dai, and Luc Van Gool. Talk2nav: Long-range vision-andlanguage navigation with dual attention and spatial memory. International Journal of Computer Vision, 129(1):246266, 2021. Martin Visbeck. Ocean science research is key for sustainable future. Nature communications, 9 (1):690, 2018. Chen Wang, Fei Xia, Wenhao Yu, Tingnan Zhang, Ruohan Zhang, C. Karen Liu, Li Fei-Fei, Jie Tan, and Jacky Liang. Chain-of-modality: Learning manipulation programs from multimodal human videos with vision-language-models, 2025. URL https://arxiv.org/abs/2504. 13351. Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao, and Si Liu. Towards realistic uav vision-language navigation: Platform, benchmark, and methodology, 2024a. URL https://arxiv.org/abs/2410.07087. Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao, and Si Liu. Towards realistic uav vision-language navigation: Platform, benchmark, and methodology, 2024b. URL https://arxiv.org/abs/2410.07087. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory, 2024. Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with realistic and rich 3d environment, 2018. URL https://openreview.net/forum?id= rkaT3zWCZ. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, Qi Zhang, and Tao Gui. The rise and potential of large language model based agents: survey. Sci. China Inf. Sci., 68(2), 2025. doi: 10.1007/S11432-024-4222-0. URL https://doi.org/10.1007/ s11432-024-4222-0. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025a. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, and Tong Zhang. Embodiedbench: Comprehensive benchmarking multi-modal large language models for visiondriven embodied agents, 2025b. URL https://arxiv.org/abs/2502.09560. Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, and Jiangmiao Pang. Mmsi-bench: benchmark for multi-image spatial intelligence, 2025c. URL https://arxiv.org/abs/ 2505.23764. 14 Work in progress. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. Nat Commun 16, 5509 (2025), 2025. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. CoRR, abs/2503.24235, 2025. doi: 10.48550/ARXIV.2503.24235. URL https://doi.org/10.48550/arXiv.2503.24235. Baining Zhao, Jianjie Fang, Zichao Dai, Ziyou Wang, Jirong Zha, Weichen Zhang, Chen Gao, Yue Wang, Jinqiang Cui, Xinlei Chen, and Yong Li. Urbanvideo-bench: Benchmarking visionlanguage models on embodied intelligence with video data in urban spaces, 2025. URL https: //arxiv.org/abs/2503.06157. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. Kaizhi Zheng, Xiaotong Chen, Odest Chadwicke Jenkins, and Xin Wang. Vlmbench: compositional benchmark for vision-and-language manipulation. Advances in Neural Information Processing Systems, 35:665678, 2022. Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, and Xuming Hu. Reefknot: comprehensive benchmark for relation hallucination evaluation, analysis and mitigation in multimodal large language models. CoRR, abs/2408.09429, 2024. doi: 10.48550/ARXIV.2408.09429. URL https://doi.org/10.48550/arXiv.2408.09429. Ziqiang Zheng, Jipeng Zhang, Tuan-Anh Vu, Shizhe Diao, Yue Him Wong Tim, and Sai-Kit Yeung. Marinegpt: Unlocking secrets of ocean to the public. arXiv preprint arXiv:2310.13596, 2023. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, Changwang Zhang, Chenghua Lin, Jun Wang, Ge Zhang, and Wangchunshu Zhou. Scaling test-time compute for LLM agents. CoRR, abs/2506.12928, 2025. doi: 10.48550/ARXIV.2506.12928. URL https://doi.org/10.48550/arXiv.2506. 12928. 15 Work in progress."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 THE USE OF LARGE LANGUAGE MODELS (LLMS) We confirm that LLMs are used only as an auxiliary tool to assist in refining wording and sentence structure. Their application in experiments is strictly confined to scientific research purposes, and all such uses have been clearly documented in the Experimental Settings. No additional reliance on LLMs has been involved in this work. A.2 MORE DETAILED DISCUSSES AND LIMITATIONS Limitations. While OCEANGYM provides valuable testbed for underwater embodied agents, several limitations should be acknowledged. First, OCEANGYM is built on Unreal Engine (UE) 5.3 (Epic Games, 2025) with HoloOcean (Potokar et al., 2022). It cannot fully replicate the real underwater environment, as factors such as ocean currents, salinity, marine life, and geological changes are not captured. Future work may leverage generative models (Ball et al., 2025) or physics-informed machine learning to incorporate these complexities. The optical and sonar images still differ from those in the real world, particularly since sonar simulation introduces errors. We will continue to refine the system to reduce these discrepancies, noting that real-world sonar itself is also subject to noise and inaccuracies. In addition, the environment is large and requires considerable computational resources, with at least 24GB of GPU memory. We recommend running without graphical interface, as enabling it can cause significant lag. These limitations highlight opportunities for future work to expand task coverage, improve physical realism, and optimize computational efficiency. Applications of OceanGym. (1 competitive arena for evaluating foundational models and embodied agent frameworks, particularly memory mechanisms. Future work can leverage OCEANGYM to optimize prompt design, memory utilization, and base model capabilities. (2) platform for synthesizing underwater simulation data to enhance both perception and decision-making skills of agents. (3) testbed for reinforcement learning, providing rich feedback for training autonomous behaviors. (4) sim-to-real bridge, enabling the transfer of trained models to real-world AUVs. By connecting virtual training with real-world deployment, OCEANGYM substantially reduces dependence on costly and hazardous field trials, accelerates development cycles, and enhances the reliability and robustness of autonomous underwater systems. A.3 PERCEPTION TASK STATISTICS Figure 8: Statistics of perception tasks. Figure 8 presents the statistical distribution of different perception settings analyzed in our dataset. The dataset consists of 85 sets of data, which include 55 sets focusing on Multi-view Perception and 30 sets on Context-based Perception. Within the Multi-view Perception data, 55 sets are categorized as follows: 23 sets involve normal pipelines, 8 sets entail damaged pipelines, 5 sets are related to planes, 4 sets concern ships, 2 sets focus on towers, 5 sets involve container boxes, and 6 sets do not feature any specific dominant object. For the Context-based Perception data, the 30 sets are evenly divided among three distinct sub-tasks, each comprising 10 sets. These sub-tasks involve the agent following pipelines, inspecting pipelines for potential damage, and scanning around shipwrecks. 16 Work in progress. Figure 9: Target object for the Locate the robot task. Figure 10: Target object the oil for the Inspect pipe task. Figure 11: Target object for the Locate oil drums task. Figure 12: Target object for the Search for sunken ship task. Figure 13: Target object for the Locate the electrical box task. Figure 14: Target object for the Inspect the wind turbine task. Figure 15: Target object for the Search for the aircraft task. Figure 16: Target object for the Docking task. A.4 DECISION TASK DETAILS Locate the robot. Locate and approach the mining robot in complex underwater environment within an abandoned subsea research zone characterized by variable terrain, low visibility, and artificial structures. The operational protocol mandates an initial memory check for the targets coordinates; if available, the system engages in direct coordinate-based navigation. Absent prior data, the robot utilizes its six camera feeds for visual comparison against reference image, identifying the target by its distinct shape, structure, and color. systematic exploration pattern, such as grid or linear search, is then executed. Throughout the mission, all encountered special objects and artificial structures are documented. Maintaining strict minimum standoff distance of 10 meters from all rocks and obstacles is the highest priority, superseding all other actions. The vehicle must remain within the predefined operational boundaries at all times, and all reports must exclusively detail artificial structures, explicitly ignoring any marine life. Inspect the oil pipe. Locate and identify the abandoned subsea oil pipeline network situated in central zone where pipelines may be partially buried and serve as potential navigation references. The procedure begins with query of the robots memory for known pipeline coordinates, initiating direct navigation if the data is present. Without prior coordinates, the robot employs its camera feeds to detect linear structures and surface features that match the reference imagery of pipeline. This is followed by systematic exploration of the area to comprehensively document all artificial structures and special objects. critical safety requirement is to maintain safe distance from all obstacles, executing immediate directional changes upon hazard detection. All reporting must focus solely on artificial structures, with biological entities entirely omitted from logs. Locate oil drumss. Locate and identify oil drums or barrels submerged in an environment where they may be partially buried or scattered within sediment under conditions of poor visibility. The first action is memory scan for stored coordinates of oil drums, proceeding with direct waypoint navigation if the search is successful. If no coordinates exist, the robot must use its camera systems to identify cylindrical objects and any visible markings that align with the target description. methodical search pattern is then conducted across the operational area, with all special objects documented. Strict obstacle avoidance protocols are continuously enforced, and the robots trajectory must never exceed the designated operational boundaries. Reports are confined to artificial structures and special objects only. Search for sunken ship. Locate and identify sunken shipwrecks, which are typically structurally complex entities that may be partially buried or obscured by various underwater obstacles. The mission initiates with an access of the robots memory for any known coordinates of shipwrecks, utilizing them for direct navigation if available. In the absence of positional data, the robot relies on its camera feeds to recognize large structural features and surface details that correspond to the reference images of shipwreck. systematic exploration is subsequently performed to document Work in progress. all special objects within the area. safe distance from all obstacles must be maintained throughout the operation, and the vehicle is required to stay within its prescribed operational limits. All marine life is systematically ignored and excluded from reporting. Locate the electrical box. Locate and identify underwater electrical boxes, which are often partially buried in sediment and possess distinctive structural features. The operational sequence starts with retrieval attempt from the robots memory for the coordinates of electrical boxes, followed by direct navigation to any located waypoints. Without prior coordinate data, the robot must analyze its camera feeds to identify the target based on its specific shape, structural characteristics, and any identifiable markings. thorough and systematic exploration of the zone is then carried out, with all special objects recorded. The mission must adhere to strict obstacle avoidance procedures and remain within the defined operational boundaries at all times. All communications and reports are restricted to artificial structures and special objects. Inspect the wind turbine. Locate and identify underwater wind power station structures, which are large installations featuring multiple pillars and mechanical components. The robot first searches its internal memory for stored coordinates of the wind power station, navigating directly to the location if the data is found. If the coordinates are not located, the system uses its camera arrays to identify the major structural and mechanical elements that match the reference documentation. systematic exploration pattern is executed to document every special object in the vicinity. safe buffer distance from all obstacles is perpetually maintained, and the robots path must comply strictly with the operational boundaries. Any biological entities encountered are disregarded and not included in any reports. Search for the aircraft. Locate and identify underwater aircraft wreckage, which can be complex and potentially dispersed across different areas of the seafloor. The initial phase involves memory check for any existing coordinates related to aircraft wreckage, with immediate navigation initiated upon successful find. If no data is available, the robot switches to using its visual feeds to identify key structural features and surface details that are consistent with the target wreckage. comprehensive systematic search is then conducted, ensuring all special objects are documented. Strict obstacle avoidance is paramount, and the vehicle must operate entirely within the set boundaries. Reports are exclusively to contain information on artificial structures and special objects. Docking. Locate and identify an underwater landing platform marked with distinctive symbol, structure with regular form that provides reliable navigation reference. The robots first action is to consult its memory for the platforms coordinates, proceeding with direct navigation if the information is available. Should the coordinates be absent, the platform must be identified visually through the camera feeds by recognizing the marking and the overall platform structure. This is followed by systematic exploration to document all special objects in the area. safe distance from all obstacles must be maintained, and the operation is confined to the approved boundaries. All reporting is limited to artificial structures and special objects, with no mention of biological activity. A.5 PROMPT FOR OCEANGYM Prompt for Perception Tasks [RGB Image] You are an assistant that analyzes an image and checks which of the following options appear in it. Options:[Options] Instructions: - Carefully examine the image, even the corners. - You can choose single or multiple options, if none of the options appear, just return an empty list. - For multiple-choice questions, no points will be awarded for incomplete selections, overselections, or incorrect selections. 18 Work in progress. - The output must be valid list (only list, no explanation, no extra text). Prompt for Perception Tasks (Add Sonar) [Sonar Image] This sonar image can be used as reference to assist in identifying the next color image. [RGB Image] You are an assistant that analyzes an image and checks which of the following options appear in it. Before that, have already provide you sonar image to help you choose the correct one. Options:[Options] Instructions: - Only when you find it difficult to recognize the color image, suggest you refer to the previous sonar image together. - Carefully examine the image, even the corners. - You can choose single or multiple options, if none of the options appear, just return an empty list. - For multiple-choice questions, no points will be awarded for incomplete selections, overselections, or incorrect selections. - The output must be valid list (only list, no explanation, no extra text). Prompt for Perception Tasks (Add Sonar and Examples) [Object Sonar Image] This sonar image example is [Object A]. [Object Sonar Image] This sonar image example is [Object B]. ... [Sonar Image] This sonar image can be used as reference to assist in identifying the next color image. [RGB Image] You are an assistant that analyzes an image and checks which of the following options appear in it. Before that, have already provide you sonar image to help you choose the correct one. Options:[Options] Instructions: - Only when you find it difficult to recognize the color image, suggest you refer to the previous sonar image together. - Carefully examine the image, even the corners. - You can choose single or multiple options, if none of the options appear, just return an empty list. - For multiple-choice questions, no points will be awarded for incomplete selections, overselections, or incorrect selections. - The output must be valid list (only list, no explanation, no extra text). Prompt for Navigation Tasks You are an expert pilot for an Autonomous Underwater Vehicle (AUV), designated as the Control Expert. Your mission is to navigate complex underwater environment to complete specific tasks. You will receive data from six cameras and location sensors. Your decisions must be precise, safe, and strategic. Work in progress. 1. Tactical Briefing for the Area of Operations Before the mission begins, you must internalize the following intelligence about the operational area. This context is vital for interpreting sensor data and forming macro-level strategy. ... 3. Mission Briefing and Sensor Data Task Description: [Task Description] Target Object Name: [Object Name] Target Object Reference Image: [Object Image] Target Object Description: [Object Description] ... 5. Survey Navigation Commands Available Commands: ascend, descend, move left, move right, move forward, move backward, rotate left, rotate right, stop. Command Execution: You must only issue ONE command per turn from the list above. ... Remember: Conduct comprehensive reconnaissance! Systematic coverage = priority! Use efficient exploration patterns! Catalog all special objects! Maintain exploration momentum! Always use format! Ignore all marine life! One continuous line between markers!"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "State Key Laboratory of Ocean Sensing, Zhejiang University",
        "Zhejiang University"
    ]
}