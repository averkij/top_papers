{
    "paper_title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process",
    "authors": [
        "Jiayi Chen",
        "Wenxuan Song",
        "Pengxiang Ding",
        "Ziyang Zhou",
        "Han Zhao",
        "Feilong Tang",
        "Donglin Wang",
        "Haoang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/."
        },
        {
            "title": "Start",
            "content": "Preprint. Under Review UNIFIED DIFFUSION VLA: VISION-LANGUAGEACTION MODEL VIA JOINT DISCRETE DENOISING DIFFUSION PROCESS Jiayi Chen1, Wenxuan Song1,, Pengxiang Ding2,3 Ziyang Zhou1 Han Zhao2,3 Feilong Tang4 Donglin Wang2 Haoang Li 1,(cid:0) 1HKUST(GZ) Equal contribution Project lead: songwenxuan0115@gmail.com (cid:0) Corresponding author 2Westlake University 3Zhejiang University 4Monash University 5 2 0 2 3 ] . [ 1 8 1 7 1 0 . 1 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and actreading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is joint diffusion process that integrates multiple modalities into single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on unified tokenized space of all modalities and hybrid attention mechanism. We further propose two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4 faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-language-action models (VLAs) (Kim et al., 2025b; Black et al., 2024; Zhao et al., 2025a; Song et al., 2024; 2025c; Ding et al., 2024; Wang et al., 2025a; Zhao et al., 2025b; Bai et al., 2025) aim to understand natural language instructions and visual observations, and to execute corresponding actions as an embodied agent. In recent years, some research (Wang et al., 2025b; Lv et al., 2025) has integrated future images into the understanding-acting loop to build robust, foresight-driven policies. This paradigm confers planning capabilities to the model by predicting future images before action inference, thereby converting the abstract task of action prediction into more tractable inverse kinematics problem. In this work, we term these models as unified VLAs, which understand, generate, and act across modalitiesreading visual content and text and outputting visual content and actions. To build unified VLAs, two existing popular paradigms fall short  (Table 1)  : (1) One line of research (Wu et al., 2024; Tian et al., 2025; Zhang et al., 2025a) unifies these modalities using extrinsic experts as encoders (Dosovitskiy et al., 2020; Radford et al., 2021) and decoders. These VLAs encode images and language and output intermediate tokens, which serve as conditions for visual and action generation models (e.g., diffusion models) to produce future images and actions. However, such 1 Preprint. Under Review modular separation often introduces misalignment, higher complexity, and weak coupling between visual generation and action prediction. (2) The other line (Zhao et al., 2025c; Wang et al., 2025b; Cen et al., 2025) unifies the input and output space through visual (Van Den Oord et al., 2017) and action tokenizers (Pertsch et al., 2025), which allows vision-language-action alignment in token-level space and does not require extra encoders or decoders. However, in these designs, image generation and action prediction still remain separate processes, restricting the ability of the model to leverage rich future visual information for action prediction. Moreover, some of these approaches (Wang et al., 2025b; Zhong et al., 2025) only predicted images during training as an auxiliary task, thereby forfeiting the value of using future images as explicit guidance at inference time. We therefore posit that genuine unification of understanding, generation, and acting requires these processes to be intrinsically synergistic, ensuring that actions are formulated as implicit mappings to the desired future observations. One potential approach is to generate image and action tokens in an autoregressive manner. However, by design, each action token only integrates contextual information through single computation, which restricts the extent to which image generation can guide action prediction. In contrast, our core philosophy is to optimize visual generation and action prediction jointly through synchronous denoising process. Here, at every denoising iteration, all action tokens causally attend to all future image tokens, and this computation is repeated multiple times across the denoising trajectory. This iterative scheme ensures that each action token is progressively refined under sufficient guidance from future visual predictions. The coarse-to-fine refinement allows actions to evolve from initialization alongside the denoising of future images, and finally converge into precise actions under confidencebased criterion. This design effectively transforms latent visual representations into temporally structured actions. To this end, we propose our Unified Diffusion VLA (UD-VLA) and Joint Discrete Denoising Diffusion Process (JD3P). JD3P is joint diffusion process that integrates multiple modalities into single synchronous denoising trajectory, which serves as the key mechanism enabling understanding, generation, and acting to mutually reinforce one another. We construct our model and theory on the foundation of two techniques. (1) We unify the multimodal space through discrete tokenization, employing VQ-based visual tokenizer (Van Den Oord et al., 2017; Zheng et al., 2022) for images and the FAST action tokenizer (Pertsch et al., 2025) for actions. (2) We design hybrid attention mechanism that enables rich intra-modal interactions for images and actions while enforcing causal attention across modalities, thereby preserving the nature of inverse kinematics and ensuring that action tokens are sufficiently conditioned on future visual information. For training, we conduct two-stage pipeline to extend pretrained VLM with image prediction capability to capture and model world dynamics in stage (i), and then apply JD3P on robot action datasets to jointly train image generation and action prediction in stage (ii). During inference, efficiency is improved through KV-cache and prefilled special tokens, candidate tokens are narrowed by remapping into smaller range, and precision is further ensured via confidence-based decoding. Our method achieves state-of-the-art (SOTA) performance on multiple popular benchmarks, including CALVIN (Mees et al., 2022), LIBERO (Liu et al., 2023), and SimplerEnv (Li et al., 2024b), while maintaining 4 over autoregressive methods. Furthermore, in-depth analysis verifies the effectiveness of our designs, and real-world evaluations further demonstrate practical utility. In summary, our contributions are threefold: We propose the unified diffusion VLA, which tightly couples understanding, generation, and acting in mutually beneficial manner. We instantiate this design via discrete tokenization, hybrid attention, and the JD3P process as the central mechanism for cross-modal synergy. We design two-stage training pipeline to activate the image generation capabilities and introduce several test-time techniques that ensure both high performance and efficiency."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Unified VLAs. Recent work has increasingly adopted unified visionlanguageaction (VLA) architectures, as summarized in Table 1. Early methods incorporated an auxiliary objective for predicting future images, and employed external encoders and decoders during both training and inference to 2 Preprint. Under Review Table 1: Component comparison across unified VLAs. WorldVLA and UniVLA model visual and action tokens in post-training, but decode only action tokens at inference (shown in gray text). IBA denotes bidirectional attention within the visual modality and the action modality separately, while they use causal attention across modalities. CA-BA denotes causal attention on visual tokens and bidirectional attention on action tokens. CA denotes causal attention. For the decoding process, denotes separate vision and action decoding processes. Diff denotes diffusion-based decoding process. AR denotes autoregressive decoding. See Appendix Section for details of losses. Method Visual Output Action Output Loss Expert Loss Expert Attention Mechanism Decoding Process Unified Modalities with Extrinsic Experts GR-1 (Wu et al., 2024) SEER (Tian et al., 2025) DreamVLA (Zhang et al., 2025b) F1 (Lv et al., 2025) UP-VLA (Zhang et al., 2025a) LMSE LMSE LMSE LDiff-disc LDiff-disc LDiff-cont LDiff-cont LDiff-cont LDiff-cont LMSE CA CA CA IBA IBA Diff-Diff Diff-Diff Diff-Diff Diff-Diff Diff-Diff Unified Input and Output Space in Seperate Decoding Processes COT-VLA (Zhao et al., 2025c) WorldVLA (Cen et al., 2025) UniVLA (Wang et al., 2025b) LDiff-disc LNTP LNTP LDiff-disc LNTP LNTP CA-BA CA AR-Diff AR AR Unified Input, Output in Joint Decoding Processes UD-VLA (Ours) LDiff -disc LDiff -disc IBA Diff unify image generation and action prediction. One line of work (Wu et al., 2024; Tian et al., 2025; Zhang et al., 2025b) supervises visual representation learning via reconstruction losses and learns action prediction through diffusion-based contrastive objectives under causal attention mask in an autoregressive manner. UP-VLA (Zhang et al., 2025a) pioneers unified VLA pre-training paradigm that jointly optimizes multimodal understanding and future visual prediction, retaining semantic and spatial detail for stronger generalization and precise control. F1 (Lv et al., 2025) integrates visual foresight into the perceptionacting loop via mixture-of-transformers with next-scale prediction, generating future visual states as planning targets and guiding actions through foresight-driven inverse dynamics. Building on advances in unified multimodal models (Wang et al., 2024; Yang et al., 2025; Deng et al., 2025), recent VLA approaches treat inputs and outputs within shared token space, enabling native processing of all modalities and obviating external encoders or decoders. CoT-VLA (Zhao et al., 2025c) adopts diffusion-based objectives and asymmetric attention in unified token space, decoding vision in an autoregressive manner and actions via diffusion, thereby making the visual chain of thought explicit through subgoal images. WorldVLA (Cen et al., 2025), UniVLA (Wang et al., 2025b), and FlowVLA (Zhong et al., 2025) treat both perception and control as next token under causal attention. In post-training, both visual and action tokens are modeled, while at inference only action tokens are decoded with vision serving as conditioning. However, current unified VLAs seldom unify decoding, which remains the dominant source of latency. Visual chain of thought pipelines still perform iterative image denoising and autoregressive action decoding, incurring high computational cost on both branches. Discrete Diffusion VLA. PD-VLA (Song et al., 2025b), as an early model employing discrete diffusion inference, adopts BART-style (Lewis et al., 2020) denoising strategy. In this approach, subset of action tokens is randomly substituted with vocabulary tokens and then iteratively refined to recover the ground-truth sequence. In contrast, Discrete Diffusion VLA (Liang et al., 2025) and LLADA-VLA (Wen et al., 2025) follow the BERT-style (Devlin et al., 2019) masked prediction strategy, where selected action tokens are replaced with special [MASK] token, and the model directly learns to predict the original tokens at these masked positions. To further improve efficiency, CEED-VLA (Song et al., 2025a) employs consistency distillation to reduce the number of iterations in the discrete diffusion process, leading to over 4 speedup without compromising performance. However, these discrete diffusion VLAs focus exclusively on action prediction while largely ignoring 3 Preprint. Under Review Figure 1: Overview of our Unified Diffusion VLA. We first construct unified multimodal space by quantizing multimodal information into discrete tokens. We then formalize Joint Discrete Denoising Diffusion Process (JD3P) to allow visual generation and action prediction to be intrinsically synergistic. Moreover, we build our model on pre-trained VLM and conduct two-stage training. Finally, we balance performance and efficiency during inference through several key techniques. the interplay between visual and action tokens, thus failing to fully exploit the potential benefits of cross-modal representation learning."
        },
        {
            "title": "3 METHODS",
            "content": "We propose UD-VLA, unified diffusion VLA that bridges vision-language understanding, future image generation, and action prediction in single transformer. We first construct unified multimodal space by quantizing multimodal information into discrete tokens (Section 3.1). Next, we design hybrid attention mechanism to maximize the utilization of information from each modality. We then formalize our core theory, Joint Discrete Denoising Diffusion Process (JD3P), and reformulate the loss computation to support its training (Section 3.2). Finally, we balance performance and efficiency during inference through several key techniques (Section 3.3). 3.1 UNIFIED DIFFUSION VLA Unified Tokenization. As illustrated in Figure 1, our method unifies language, vision, and action modalities by converting each into discrete tokens and concatenating them into single multimodal sequence. The language tokens follow the same design as Emu3 (Wang et al., 2024), visual observations are discretized with VQ tokenizer (Zheng et al., 2022) into Vv tokens in the vocabulary, and actions are represented using FAST (Pertsch et al., 2025) into Va tokens in the vocabulary. To explicitly structure the diffusion process for different modalities, we employ special tokens, <BOI> and <EOI> for images, and <BOA> and <EOA> for actions, to mark the beginning and ending of their respective token sequences. Then, the complete token sequences are formatted as: [ text tokens ; current image tokens ; future image tokens ; action tokens ], (1) where text tokens and current image tokens serve as input, and future image tokens and action tokens are output. Hybrid Attention Mechanism. To coordinate multimodal tokens, we introduce hybrid attention mechanism in Figure 2. The input text and current image follow causal attention and bidirectional attention separately. We split output tokens into generation (future image) blocks and acting blocks. Within each block, bidirectional attention enables comprehensive token interactions, which further break the time-serial dependence between action tokens to avoid shortcut learning (Torne et al., 2025). Tokens across blocks are connected through causal attention: the generation block attends to the input, and the acting block attends to both, while no information flows backward. This progressive design recasts the otherwise difficult objective of end-to-end action prediction in VLMs into two coupled processes: (i) foresight process that predicts the next visual state, 4 Figure 2: Hybrid attention mechanism in UD-VLA. Preprint. Under Review and (ii) an inverse kinematics process that infers the action conditioned on that visual prediction. This factorization enables principled, targeted training of both stages (see Section 3.2). By explicitly prohibiting action-to-vision pathways, it eliminates coarse action information leakage and the attendant error compounding, improves interpretability, and ensures that downstream control is genuinely grounded in predicted visual consequences rather than spurious correlations. Joint Discrete Denoising Diffusion Process (JD3P). Actions and images are generated in parallel within the same denoising step. Given the quantized future image tokens in fixed-length sequence v0 = (v0,1, . . . , v0,j, . . . , v0,Lv ) is Lv-dimension vector denoted by v0,j {1, ..., Vv} and the variable-length quantized action tokens a0 = (a0,1, . . . , a0,i, . . . , a0,La ) is La-dimension vector denoted by a0,i {1, ..., Va}, the complete sequence in JD3P is: v0, a0 = (v0,1, . . . , v0,Lv , a0,1, . . . , a0,La ). (2) We augment the vocabulary with special mask token (i.e., <MASK>), yielding Vm = Vv + Va+1 symbols and one-hot basis {e1, . . . , eVv+Va , eM}. The noising process of JD3P is Markov chain {vt, at}T t=0 with per-step transition matrices Qt RVmVm that independently map each token to with probability βt and keep it unchanged with probability 1βt. Formally, for any one-hot vector et,r of token vt,j and at,i, where denotes any position in the concatenated sequence vt, at. The transition matrices compose as Qt = Qt Q1, leading to corrupted distribution at time that factorizes position-wise: Qt et,r = (1βt) et,r + βt eM, (3) q(vt, at v0, a0) = Lv +La(cid:89) r=1 (cid:0)vt,j, at,i (cid:12) (cid:12) Qte0,r (cid:1) , (4) where denotes categorical distribution. The denoising process defines conditionals pθ(vt1, at1 vt, at, c) under input context (i.e., text and current image) as pθ(vt1, at1 vt, at, c) = pθ(vt1 vt, c) pθ(at1 vt, at, c). Considering the mask, it can be decomposed as pθ(vt1,j vt, c) = (cid:104) δ(vt1,j = vt,j) (cid:105)11{vt,j =M}(cid:104) C(cid:0)vt1,j π(v) θ (j vt, c)(cid:1)(cid:105)1{vt,j =M} (5) (6) and (cid:105)11{at,i=M}(cid:104) (cid:104) δ(at1,i = at,i) C(cid:0)at1,i π(a) θ (i vt, at, c)(cid:1)(cid:105)1{at,i=M} pθ(at1,i vt, at, c) = . (7) Here, 1 is the indicator function (1 if the condition is true, 0 otherwise), δ() is the Kronecker delta (1 if the arguments are equal, 0 otherwise), π(v) θ (i ) correspond to the models predictive distributions over visual and action tokens. At each denoising step, UD-VLA selectively reconstructs subset of masked positions while leaving clean others hidden, thereby annealing the mask ratio from high to low until the original signals v0 and a0 are recovered. θ (j ) and π(a) Loss Function. We discard the explicit diffusion chain and adopt singlestep mask-predict objective. At each update we sample mask ratio ρt (0, 1] and apply it to the clean sequences v0 and a0 by replacing the selected positions with <MASK>, yielding vt and at. We then optimize the VLA pθ by maximum likelihood restricted to the masked sites, computing cross-entropy only on those positions to recover the original tokens. LCE(θ) = β Lv(cid:88) log p(v) θ (cid:0)v0,j vt, c(cid:1) 1{vt,j = M} La(cid:88) log p(a) θ (cid:0)a0,i vt, at, c(cid:1) 1{at,i = M}, (8) where β down-weights the visual tokens to avoid their dominance. This design promotes stronger visionaction interaction and replaces the multi-step corruption chain with single-step masking objective. 5 Preprint. Under Review"
        },
        {
            "title": "3.2 TRAINING",
            "content": "We initialize our model from pretrained VLM backbone (Wang et al., 2024), which is trained in an autoregressive manner with causal attention. At the beginning, we adopt stage (i) to post-train UD-VLA on large-scale video dataset, where the token sequences are constructed as: [ text tokens ; current image tokens ; future image tokens]. (9) This stage injects capabilities of future image generation into the VLA, enabling the robot to understand and model future states. In stage (ii), we jointly optimize image and action generation under unified framework on the downstream robot action dataset. The token sequences are constructed as Equation (1). Specifically, we reformulate the autoregressive decoding as diffusion process following JD3P. Unlike standard diffusion models that predict tokens at masked positions, we adopt shift operation strategy to predict the next token, which enables the model to inherit capacity learned from next-token-prediction training while gaining the advantages of bidirectional context and parallel decoding. 3."
        },
        {
            "title": "INFERENCE",
            "content": "At inference, the denoising process is instantiated through parallel decoding with adaptive masking: (i) all positions of vT and aT are initialized as <MASK>, (ii) token distributions for all positions are predicted in parallel, and (iii) this procedure is repeated for small number of iterations. Prefix KV Cache and Pre-filling Tokens. Our UD-VLA employs prefix KV-Cache to cache the keys and values of current visual and prompt tokens. Besides, because the length of visual tokens is fixed while action tokens have varied length, we pre-fill the <BOI>, <EOI>, and <BOA> tokens at the corresponding positions, which guides the denosing of visual tokens and action tokens. Our experiments empirically show that the cache and pre-filling reduce latency and contribute to higher speed. Confidence-Guided Decoding. We initialize sequence with noise at = and iterate backward to = 0, applying the cosine mask schedule ρt = cos (cid:0) π (cid:1) for = T, . . . , 1, which yields smoother sampling and refinement. Follow Equation (5), the model evaluates the joint reverse conditionals at each step. We rank only the currently masked positions and let Mt {1, . . . , Lv +La} be the masked set at step with size Mt. For each location r, confidence score is computed: +1t +1 2 qt1,r = max ℓ (cid:40)pθ (cid:0)ℓ vt, u(cid:1), (cid:0)ℓ vt, at, u(cid:1), pθ {1, . . . , Lv}, {Lv + 1, . . . , Lv + La}. We then update the top (1 ρt)Mt entries among the masked indices: Ωt = TopK { qt1,r : Mt }. (1ρt)Mt (10) (11) For visual indices and action indices in Ωt, tokens are updated via tempered Gumbel sampling: vt1,j, at1,i = arg max (cid:104) 1 κt log pθ(y vt, at, u) + ηc (cid:105) , ηc i.i.d. Gumbel(0, 1). (12) where the temperature κt decreases with t. Positions outside Ωt remain masked for future steps. Decoding Space Mapping. The tokenized image and action tokens come from small codebooks that constitute only subset of the models vocabulary. During inference, we restrict the classification space within each modality to its designated range. This prevents the model from producing tokens of the wrong modality and stabilizes the generation process. In addition, once an <EOA> is predicted at index i, we fix the action length and deterministically replace all subsequent tokens at,ii>i with <MASK>, preventing them from corrupting action prediction."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "To comprehensively evaluate the effectiveness of our UD-VLA, we conduct experiments on three simulated benchmarks as well as real-world robotic platform, comparing against widely adopted 6 Preprint. Under Review Table 2: Comprehensive Evaluation of Long-Horizon Robotic Manipulation on the CALVIN Benchmark. UniVLA denotes the variant without historical frames for fair comparison. Method Task Tasks Completed in Row Avg. Len 1 2 3 5 MCIL (Lynch & Sermanet, 2020) ABCDD 0.373 ABCDD 0.844 RT-1 (Brohan et al., 2023) Robo-Flamingo (Li et al., 2024a) ABCDD 0.964 ABCDD 0.982 Deer (Yue et al., 2024) ABCDD 0.949 GR-1 (Wu et al., 2024) ABCDD 0.980 ReconVLA (Song et al., 2025d) UniVLA (Wang et al., 2025b) ABCDD 0.958 ABCDD 0.971 MODE (Reuss et al., 2025) ABCDD 0.962 UP-VLA (Zhang et al., 2025a) ABCDD 0.986 MDT (Reuss et al., 2024) ABCDD 0.992 UD-VLA (ours) 0.027 0.617 0.896 0.902 0.896 0.900 0.918 0.925 0.921 0.958 0.968 0.002 0.438 0.824 0.821 0.844 0.845 0.874 0.879 0.879 0.916 0.936 0.000 0.323 0.740 0.759 0.789 0.785 0.846 0.835 0.842 0.862 0.904 0.000 0.227 0.660 0.670 0.731 0.705 0.824 0.779 0.812 0.801 0. 0.40 2.45 4.09 4.13 4.21 4.23 4.26 4.39 4.42 4.52 4.64 baselines and ablated methods. Section 4.1 first introduces all benchmarks. Then, Section 4.2 presents results on simulated benchmarks, where our method achieves SOTA performance across diverse paradigms. Third, we provide an in-depth analysis to assess the contributions of each component in Section 4.3. At last, real-world experiments demonstrate the strong performance and generalization capability of our UD-VLA in Section 4.4. All used baselines are detailed in Section C. 4.1 BENCHMARKS CALVIN. The CALVIN benchmark (Mees et al., 2022) is simulated suite for evaluating longhorizon, language-conditioned robotic manipulation. It spans four environments (A, B, C, and D) with 34 tasks and 1,000 language instructions. We evaluate 500 rollouts per model, where each rollout involves sequence of 5 consecutive sub-tasks. We report the average length (avg. len.) of successful sub-task completions of all rollouts with maximum value of 5. LIBERO. LIBERO (Liu et al., 2023) is simulated manipulation benchmark with 4 suites (Spatial, Object, Goal, Long). Spatial probes layout reasoning, Object tests object generalization, Goal evaluates goal-conditioned control, and Long targets long-horizon compositional skills. We report success rates per suite and overall average, each suite containing 10 tasks and 50 rollouts per task. SimplerEnv. SimplerEnv (Li et al., 2024b) is real-to-sim suite for assessing transfer and generalization of robot policies trained on real-world video data. We evaluate on WidowX robots under varied lighting, textures, colors, and viewpoints. Tasks include Put Spoon on Towel, Put Carrot on Plate, Stack Green on Yellow Block, and Put Eggplant in Yellow Basket. We report per-task success rates and the overall average. 4.2 MAIN RESULTS IN SIMULATION CALVIN. Table 2 shows that UD-VLA achieves an average success length of 4.64 on CALVIN ABCDD benchmark, outperforming all baselines and demonstrating strong capabilities in longhorizon reasoning and execution. Compared with UniVLA, which autoregressively predicts future frames during post-training while only inferring actions, our superior performance suggests that explicit visual generation serves as form of chain-of-thought (CoT), providing more effective guidance for action prediction. In contrast to UP-VLA, which predicts both future frames and actions in single step on filled tokens, our multi-step diffusion more efficiently facilitates information exchange between generated images and actions. LIBERO. Table 3 shows that UD-VLA achieves an average success rate of 92.7%, which is SOTA performance on the LIBERO benchmark. In particular, it attains 95.7% on the Object suite and 89.6% on the Long suite, demonstrating robust generalization in object recognition and strong temporal reasoning capabilities through future-frame prediction. Our method outperforms approaches that unify modalities through extrinsic experts (e.g., DreamVLA) as well as those that unify input and output spaces (e.g., CoT-VLA, WorldVLA), demonstrating the advantage of our end-to-end unified VLA paradigm with joint denoising. Compared to DreamVLA, which builds progressive generation 7 Preprint. Under Review Table 3: Evaluation and comparison on the LIBERO benchmark. Method Spatial Object Octo (Octo Team et al., 2024) SpatialVLA (Qu et al., 2025) CoT-VLA (Zhao et al., 2025c) WorldVLA (Cen et al., 2025) ThinkAct (Huang et al., 2025) π0-FAST (Pertsch et al., 2025) MolmoAct (Lee et al., 2025) FlowVLA (Zhong et al., 2025) DreamVLA (Zhang et al., 2025b) UD-VLA (ours) 78.9% 88.2% 87.5% 87.6% 88.3% 96.4% 87.0% 93.2% 97.5% 94.1% 85.7% 89.9% 91.6% 96.2% 91.4% 96.8% 95.4% 95.0% 94.0% 95.7% Goal 84.6% 78.6% 87.6% 83.4% 87.1% 88.6% 87.6% 91.6% 89.5% 91.2% Long 51.1% 55.5% 69.0% 60.0% 70.9% 60.2% 77.2% 72.6% 89.5% 89.6% Average 75.1% 78.1% 81.1% 81.8% 84.4% 85.5% 86.6% 88.1% 92.6% 92.7% Table 4: Evaluation results on SimplerEnv-WidowX. Model Put Spoon Put Carrot Stack Block Put Eggplant Overall OpenVLA (Kim et al., 2025b) RT-1-X (Brohan et al., 2023) Octo (Octo Team et al., 2024) RoboVLMs (Liu et al., 2025) OpenVLA-OFT (Kim et al., 2025a) π0 (Black et al., 2024) SpatialVLA (Qu et al., 2025) π0-FAST (Pertsch et al., 2025) F1 (Lv et al., 2025) UD-VLA (ours) 0.0% 0.0% 47.2% 45.8% 12.5% 29.1% 16.7% 29.1% 50.0% 58.3% 0.0% 4.2% 9.7% 20.8% 4.2% 0.0% 25.0% 21.9% 70.8% 62.5% 0.0% 0.0% 4.2% 4.2% 8.3% 16.7% 29.2% 10.8% 50.0% 54.1% 4.1% 0.0% 56.9% 79.2% 37.5% 62.5% 100% 66.6% 66.7% 75.0% 1.0% 1.1% 29.5% 37.5% 39.6% 40.1% 42.7% 48.3% 59.4% 62.5% process for future images and actions, UD-VLA delivers higher performance, highlighting that the joint discrete diffusion denoising process enables more effective fusion of the two modalities. SimplerEnv. On the SimplerEnv-WidowX benchmark (see Table 4), UD-VLA achieves an average success rate of 59.4%, significantly outperforming all baselines. Unlike methods (Qu et al., 2025), that require additional inputs, UD-VLA predicts precise motion and grasp targets by explicitly modeling future frames. Our method outperforms F1, demonstrating its potential as superior paradigm for integrating understanding, generation, and acting. We also surpass UniVLA, which requires additional historical information as input, highlighting the advantages of incorporating visual information into the reasoning process. In the stack block task, which requires precise manipulation, our approach achieves 24.9% higher success rate than SpatialVLA with 3D perception capabilities. 4. IN-DEPTH ANALYSIS Effectiveness of Hybrid Attention Mechanism. As shown in Table 5, the hybrid attention design achieves better performance than purely bidirectional or causal attention mechanism on CALVIN, reaching an average success length of 4.64. This demonstrates that hybrid attention achieves the most effective information transfusion. Tokens in one image are able to attend to each other to ensure better global consistency. Likewise, actions of different dimensions, such as position and rotation, do not follow strict causal order, and bidirectional attention enables the model to capture their correlations more effectively. However, bidirectional attention across modalities leads to information leakage, as mentioned in Section 3.1, demonstrating 0.3 lower average length. Table 5: Effectiveness of different attention schemes. Bidirectional applies bidirectional attention over the visual generation and the action block. Causal uses strict lower-triangular mask, and Hybrid follows Figure 2. Attention Avg. Len. Causal Bidirectional Hybrid (ours) 4.04 4.32 4.64 Effectiveness of Future Image Generation. In Table 6, we compare three settings: not generating visual information, reconstructing current frames, and predicting future frames. While reconstructing the current images improves the models fine-grained perception and thereby strengthens action prediction, it remains limited because the model only learns static scene information. In contrast, jointly predicting future images and 8 Preprint. Under Review Table 6: Target of Visual Generation. Null denotes action prediction without visual generation. Current Image denotes reconstruct current observation. Table 7: Decoding Mechanism. AR denotes autoregressive decoding, Jacobi denotes the parallel decoding scheme used in (Song et al., 2025b), and ID denotes independent diffusion of future images and actions. Target Avg. Len. Null Current Image Future Image (ours) 4.21 4.39 (+0.18) 4.64 (+0.43) Method Avg. Len. Speed (tokens/s) AR Jacobi ID JD3P (ours) 4.18 4.16 (-0.02) 4.35 (+0.19) 4.64 (+0.46) 50.2 (1.0) 101.6 (2.0) 144.4 (2.9) 219.3 (4.3) actions provides richer temporal cues, allowing the model to anticipate visual dynamics and align them with action planning, which leads to the best performance. Effectiveness of JD3P. As shown in Table 7, autoregressive decoding is not well-suited for visual generation, since image tokens are poorly modeled by next-token generation and the process is extremely slow with only 50.2 tokens per second and limited average length of 4.18. The Jacobi method improves decoding speed 2, but its performance is still constrained by the inherent limitations of the autoregressive model. The independent diffusion process allows the optimization of visual generation and action prediction iteratively, which naturally suits them (Deng et al., 2025; Black et al., 2024), thus reaching an average length of 4.35 and much higher speed. In contrast, JD3P attains both the highest success rate of 4.64 and the 4.3 faster decoding speed, demonstrating that it is more effective and efficient approach for unifying visual-action generation. This demonstrates that joint denoising enables the actions to better refine their predictions by leveraging image information from intermediate denoising steps, which can be regarded as computational scaling. In contrast, the independent diffusion process allows for only limited information flow, because it predicts actions with only single computation conditioned on future images. 4.4 REAL-WORLD EXPERIMENT Setup and Tasks. As is shown in Figure 3, our real-world setup consists of 6-DoF UR5e robotic arm equipped with 6-DoF Inspire RH56E2 robotic hand for dexterous manipulation. wrist-mounted Intel RealSense D435i depth camera provides close-range RGB-D observations of the manipulated objects, while static Gemini 336L camera captures the entire operational workspace from fixed viewpoint. We collect three categories of tasks, namely stacking bowls, putting blocks (into box), and flipping towers. Each category includes objects with varied colors and shapes, and data is captured under three distinct backgrounds. For each category, we record 200 trajectories at 15 Hz. During evaluation, we consider both seen and unseen settings, where unseen tasks feature novel scenes and objects. Results and Analysis. We evaluate each methods for 30 times. Figure 3 shows that our UDVLA consistently outperforms the GR00T N1 (Bjorck et al., 2025) and UniVLA (Wang et al., 2025b) baselines across all tasks, achieving success rates above 80%. For seen tasks, our action quantization brings the precision of action representations, while our joint denoising process ensures the quality of the actions. For unseen tasks, GR00T N1 exhibits poor generalization on unseen targets and backgrounds, while our UD-VLA owns visual generalization to generate images with unseen targets, which further leads to correct actions. Although UniVLA also benefits from visual post-training, Figure 3: Real-world Setup and Results. 9 Preprint. Under Review Figure 4: Visualization of Generated Future Frames and Ground-truth Frames. The leftmost frame of each trajectory represents the current observation. its inference lacks explicit visual reasoning, thus falling short on unseen objects. In contrast, our joint modeling of future visual tokens and actions yields markedly stronger generalization to unseen scenes, leading to higher success rates. 4.5 VISUALIZATION To intuitively illustrate the effectiveness and limitations of image generation, we present representative examples of generated future images in Figure 4. Overall, the predicted frames follow instructions closely and capture task-level dynamics, remaining well aligned with the ground-truth trajectories. This consistency indicates that the model has internalized temporal understanding of task logic, which in turn enhances temporal reasoning and enables more coherent action generation. However, the generation lacks visual fidelity, especially in fine-grained details such as robotic arms and backgrounds. This limitation arises from the absence of large-scale generative pretraining and the use of compressed images with few tokens for efficiency. Despite these shortcomings, the generated frames reliably convey task progression and remain informative for downstream control. While pixel-level accuracy is difficult, the model consistently produces foresight images sufficient for action planning."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose unified diffusion VLA that unifies understanding, generation, and acting through joint discrete denoising diffusion process. It jointly refines image and action tokens via single synchronous denoising trajectory. We construct unified multimodal space and hybrid attention mechanism as foundations. Besides, we design two-stage training and several test-time techniques to balance performance and efficiency. Experiments show that our method achieves state-of-the-art results in both simulation and real-world environments. ACKNOWLEDGMENTS We are grateful to Yuqi Wang and Zhide Zhong for insightful discussions and experimental guidance."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Wei Zhao, Zhe Li, et al. Towards unified understanding of robot manipulation: comprehensive survey. arXiv preprint arXiv:2510.10903, 2025. Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. 10 Preprint. Under Review Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. Robotics: Science and Systems Foundation, 2023. Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Pengxiang Ding, Han Zhao, Wenjie Zhang, Wenxuan Song, Min Zhang, Siteng Huang, Ningxi Yang, and Donglin Wang. Quar-vla: Vision-language-action model for quadruped robots. In European Conference on Computer Vision, pp. 352367. Springer, 2024. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025a. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, et al. Openvla: An open-source vision-language-action model. In Conference on Robot Learning, pp. 26792713. PMLR, 2025b. Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 78717880, 2020. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. 2024a. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024b. Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, and Ping Luo. Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies. arXiv preprint arXiv:2508.20072, 2025. 11 Preprint. Under Review Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, and Hanbo Zhang. Towards generalist robot policies: What matters in building vision-language-action models. 2025. Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, and Jiangmiao Pang. F1: vision-language-action model bridging understanding and generation to actions. arXiv preprint arXiv:2509.06951, 2025. Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020. Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters (RA-L), 7(3):73277334, 2022. Octo Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-languageaction model. arXiv preprint arXiv:2501.15830, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Moritz Reuss, Omer Erdinc Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. Robotics: Science and Systems XX. Ed.: D. Kulic, 2024. Moritz Reuss, Jyothish Pari, Pulkit Agrawal, and Rudolf Lioutikov. Efficient diffusion transformer policies with mixture of expert denoisers for multitask learning. In The Thirteenth International Conference on Learning Representations, 2025. Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning Fan, and Donglin Wang. Germ: generalist robotic model with mixture-of-experts for quadruped robot. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1187911886. IEEE, 2024. Wenxuan Song, Jiayi Chen, Pengxiang Ding, Yuxin Huang, Han Zhao, Donglin Wang, and Haoang Li. Ceed-vla: Consistency vision-language-action model with early-exit decoding. arXiv preprint arXiv:2506.13725, 2025a. Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, and Haoang Li. Accelerating vision-language-action model integrated with action chunking via parallel decoding. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025b. Wenxuan Song, Jiayi Chen, Wenxue Li, Xu He, Han Zhao, Can Cui, Pengxiang Ding Shiyan Su, Feilong Tang, Xuelian Cheng, Donglin Wang, et al. Rationalvla: rational vision-language-action model with dual system. arXiv preprint arXiv:2506.10826, 2025c. 12 Preprint. Under Review Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Haodong Yan, Yuxin Huang, Feilong Tang, Donglin Wang, and Haoang Li. Reconvla: Reconstructive vision-language-action model as effective robot perceiver. arXiv preprint arXiv:2508.10333, 2025d. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=meRCKuUpmc. Marcel Torne, Andy Tang, Yuejiang Liu, and Chelsea Finn. Learning long-context diffusion policies via past-token prediction. 2025. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. volume 30, 2017. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, and Donglin Wang. Vla-adapter: An effective paradigm for tiny-scale vision-language-action model. arXiv preprint arXiv:2509.09372, 2025a. Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified vision-language-action model. arXiv preprint arXiv:2506.19850, 2025b. Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, and Xiaoyan Sun. Llada-vla: Vision language diffusion action models. arXiv preprint arXiv:2509.06932, 2025. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In International Conference on Learning Representations, 2024. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Yang Yue, Yulin Wang, Bingyi Kang, Yizeng Han, Shenzhi Wang, Shiji Song, Jiashi Feng, and Gao Huang. Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution. Advances in Neural Information Processing Systems, 37:5661956643, 2024. Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867, 2025a. Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, et al. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. arXiv preprint arXiv:2507.04447, 2025b. Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, Pengxiang Ding, Xuelian Cheng, and Zongyuan Ge. More: Unlocking scalability in reinforcement learning for quadruped visionlanguage-action models. arXiv preprint arXiv:2503.08007, 2025a. Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, and Donglin Wang. Vlaˆ 2: Empowering vision-language-action models with an agentic framework for unseen concept manipulation. arXiv preprint arXiv:2510.14902, 2025b. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. pp. 17021713, 2025c. Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022. 13 Preprint. Under Review Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li. Flowvla: Thinking in motion with visual chain of thought. arXiv preprint arXiv:2508.18269, 2025. 14 Preprint. Under Review"
        },
        {
            "title": "A LOSS FORMULATIONS",
            "content": "In Table 1, LMSE denotes reconstruction objective that meaPixel-level Mean Squared Error. sures the average squared discrepancy between the predicted output and the ground-truth target, applicable to both visual and action modalities. The loss is defined as LMSE = 1 Ω (cid:88) uΩ (cid:13)x(u) ˆxθ(u c)(cid:13) (cid:13) (cid:13) 2 2 , (13) where is the ground-truth target, ˆxθ( c) is the model prediction conditioned on (e.g., instruction, proprioception), Ω is the index set over which the discrepancy is computed, and enumerates elements in Ω. In Table 1, LDiff-cont denotes conditional diffusion objective that learns Continuous Diffusion. to predict the injected noise for noised sample at timestep (optionally conditioned on c), enabling iterative denoising back to the clean data. Formally, LDiff-cont = Et, x0q(x0), ϵN (0,I), (cid:13)ϵ ϵθ(xt, t, c)(cid:13) (cid:13) 2, (cid:13) where x0 is the clean continuous target, xt is its noised version at timestep t, ϵ is Gaussian noise, is an optional conditioning signal (e.g., instruction, proprioception), αt and σt are the noise schedule coefficients at timestep t, is the diffusion timestep sampled from predefined schedule, and {1, 2} denotes the norm used for the loss (typically = 2 for L2 or = 1 for L1/Smooth-L1). The model predicts ϵ and is then used in reverse updates xt1 = Fθ(xt, t, c) to reconstruct x0. xt = αtx0 + σtϵ, {1, 2}, (14) By default LDiff-cont uses an L2 error for noise prediction. Some implementations (e.g., GR-1 (Wu et al., 2024), SEER (Tian et al., 2025)) replace it with Smooth-L1 (Huber) without altering the diffusion formulation. Formally, replace the L2 error with Smooth-L1 ρβ(r) = 1 2 r2 2 β , r2 1 r2 < β, β, otherwise, (15) with = ϵ ϵθ(xt, t, c) and β > 0 (default β = 1 unless specified). The diffusion objective becomes E[ρβ(r)]. Discrete Diffusion. sequences over finite vocabulary (including special [MASK]). Formally, (cid:35) In Table 1, LDiff-disc denotes masked-token prediction objective for discrete (cid:34) LDiff-disc = EM log pθ (cid:0)z z(M ) masked, c(cid:1) , (16) 1 (cid:88) iM where is the set of masked positions, masked is the visible context obtained by replacing positions in with [MASK], and is an optional conditioning signal. The model outputs categorical distribution over the vocabulary and learns to recover the masked tokens from the visible context. is the ground-truth token at position i, z(M ) In Table 1, LNTP denotes the standard autoregressive objective (negative Next-Token Prediction. log-likelihood over the vocabulary) that maximizes the probability of the next token given its history: LNTP = Ei (cid:2) log Pθ(xi x<i)(cid:3), (17) where x<i is the preceding context and xi is the next token. This loss encourages the model to assign high probability to the ground-truth continuation under causal (history-only) factorization. TRAINING DETAILS. The training for the three virtual environment experiments and the real-world experiments was conducted on H100 GPUs. For the Calvin-ABCD dataset, we set the action chunk to 10 and trained for approximately 24 hours across 8 H100 GPUs. For the LIBERO dataset, we jointly trained on four 15 Preprint. Under Review tasks instead of training each task separately, with the action chunk set to 10, and the training lasted around 30 hours on 8 H100 GPUs. In the SimplerEnv environment, we used the Bridge dataset for training, with the action chunk set to 5, and trained for about 30 hours on 8 H100 GPUs. For the real-world experiments, we collected over 600 trajectories across 3 tasks, designed with an action chunk of 8, and trained for around 8 hours on 8 H100 GPUs."
        },
        {
            "title": "C BASELINES",
            "content": "MCIL (Lynch & Sermanet, 2020). MCIL introduces language-conditioned robotic manipulation with single end-to-end visuomotor policy that learns perception from pixels, natural language understanding, and multitask continuous control. key contribution is multicontext imitation learning, which enables training from largely unstructured and unlabeled demonstrations (no task or language labels), reducing language annotation to under 1% of the dataset while improving language-conditioned performance. At test time, the policy follows long-horizon free-form instructions in 3D tabletop environment using only text prompts. MCIL further combines any language-conditioned policy with large pretrained language models to handle many out-of-distribution synonym instructions across multiple languages without collecting new demonstrations. RT-1 (Brohan et al., 2023). RT-1 is single Transformer visuomotor policy that encodes camera images, natural language instructions, and motor commands into token sequences for real-time control. It is trained on over 130k demonstrations collected across 13 robots. RT-1 reports 97% success on over 700 instructions, improved generalization to novel tasks, distractors, and backgrounds. RT-1 can also execute long-horizon procedures within SayCan (Ahn et al., 2022), with as many as 50 stages. RT-1 can also absorb heterogeneous data from simulation and other robot morphologies while retaining performance on the original tasks. Robo-Flamingo (Li et al., 2024a). RoboFlamingo builds on the open-source VLM OpenFlamingo to form visionlanguage manipulation framework. It decouples visuallanguage understanding and decision making: the pretrained VLM is used for per-step comprehension of images and instructions, while an explicit policy head models history. Then the policy is fine-tuned solely on language-conditioned manipulation datasets via imitation learning. This design lets small amount of demonstrations adapt the model to downstream tasks, supports open-loop control. On the CALVIN benchmark, it outperforms prior baselines (Brohan et al., 2023) and demonstrates zero-shot generalization to new settings. GR-1 (Wu et al., 2024). GR-1 is straightforward GPT-style transformer for multi-task, language-conditioned visual robot manipulation. It takes as input language instruction, sequence of observation images, and robot states, and predicts both robot actions and future images end-to-end. The model is first pre-trained for video prediction on large-scale video dataset, then seamlessly fine-tuned on robot data. On the CALVIN benchmark, the paper reports improvements over contemporaneous baselines (e.g., success rate from 88.9% to 94.9%, average length from 3.06 to 4.21), strong zero-shot unseen-scene generalization (53.3% to 85.4%). Real-robot experiments further show performance gains and robustness to unseen scenes and objects. OpenVLA (Kim et al., 2025b). OpenVLA is 7B-parameter open-source VLA trained on 970k real-world robot manipulation trajectories from Open-X Embodiment. It uses visually conditioned Llama-2 backbone and fuses pretrained features from DINOv2 and SigLIP, capturing visual cues at multiple granularities. The paper reports strong generalist manipulation results, outperforming the 55B RT-2-X by 16.5% absolute success across 29 tasks and multiple embodiments with roughly 7 fewer parameters. ReconVLA (Song et al., 2025d). ReconVLA is reconstructive VLA with an implicit grounding paradigm that addresses the observation of dispersed visual attention in prior VLAs. It takes current images, language instructions, and robot proprioception as inputs. By reconstructing gaze regions corresponding to target objects, the policy acquires fine-grained representations and allocates attention to task-relevant regions, enabling precise manipulation. The authors also curate large-scale pretraining corpus (over 100k trajectories and 2M samples) from open-source robotic datasets to improve generalization in visual reconstruction. Experiments in long-horizon simulation and real-world settings report superior performance, directive attention visualizations, and generalization to unseen objects and scenes. Preprint. Under Review UP-VLA (Zhang et al., 2025a). UP-VLA revisits VLA pre-training by coupling multi-modal understanding with future visual prediction to retain both high-level semantics and low-level spatial cues. Concretely, it co-trains an autoregressive policy with flexible attention mask on heterogeneous datasets, using visionlanguage understanding to align semantic features and video-style future prediction to capture fine-grained visual patterns. UP-VLA reports that it understands instructions, predicts future images, and plans actions within one framework, with gains across simulated and real-world manipulation; in particular, it improves CALVIN ABCD generalization by about 33% over prior methods and shows stronger success on tasks requiring precise spatial control, while maintaining competitive in-distribution multitask and real-unseen performance. UniVLA (Wang et al., 2025b). UniVLA learns cross-embodiment VLA policies by planning in unified, task-centric latent action space extracted from videos. The approach derives latent actions in DINO feature space to separate task-relevant dynamics from irrelevant visual changes, enabling the use of heterogeneous, web-scale data (including unlabeled human videos) without action labels. lightweight decoder (about 10.8M parameters) translates latent actions into executable trajectories, supporting efficient adaptation with limited downstream data. The paper reports strong results across manipulation, navigation, and real-robot evaluations, including improvements over OpenVLA (Kim et al., 2025b) with fraction of the compute (1/20 pretraining) and data (1/10 downstream), as well as gains on LIBERO (+18.5%), navigation (+29.6%), and real-world deployments (+36.7%). Performance scales with data diversity and remains robust under embodiment and viewpoint shifts. Octo (Octo Team et al., 2024). OCTO is an open-source, transformer-based generalist robot policy pretrained on 800k trajectories from the Open X-Embodiment dataset. It maps multimodal input tokens (observations and tasks) to action tokens, supporting instruction via language commands or goal images and handling diverse camera configurations and robot platforms. The model can be adapted to new sensory inputs, action spaces, or morphologies by adding lightweight adapters and fine-tuning on small target dataset within hours on consumer GPUs. OCTO employs diffusion action head to model expressive action distributions. Experiments across 9 robots and 4 institutions show strong out-of-the-box multi-robot control for singleand dual-arm manipulation and effective initialization for downstream fine-tuning. SpatialVLA (Qu et al., 2025). SpatialVLA targets 3D spatial understanding for generalist robot policies by aligning spatial representations of observations and actions. It introduces Egocentric 3D (Ego3D) Position Encoding to inject 3D context into visual features in the camera frame (avoiding per-robot calibration), and Adaptive Action Grids to discretize continuous robot actions into data-driven spatial grids, learning spatial action tokens that transfer across embodiments. Pretrained on 1.1M real-world robot episodes, SpatialVLA is applied zero-shot to numerous tasks and shows strong in-domain multi-task generalization and long-horizon trajectory inference in both simulation and real robots. CoT-VLA (Zhao et al., 2025c). CoT-VLA introduces visual chain-of-thought for robotic control by first generating subgoal images as explicit intermediate reasoning steps and then conditioning actions on the current observation and the generated subgoal. The system is built on unified multimodal backbone, trained on Open X-Embodiment and action-less video datasets, and fine-tuned on downstream task demonstrations. hybrid attention design uses causal next-token prediction for text and image generation and full attention to predict action dimensions jointly. The policy further employs action chunking to output short action sequences per step. Experiments in simulation and the real world show that visual chain-of-thought improves policy performance, with reported gains over prior VLAs (e.g., +17% real-world, +6% simulation), and strong results across multiple robot platforms and tasks. π0-FAST (Pertsch et al., 2025). π0-FAST proposes frequency-space action tokenization scheme that compresses robot actions with discrete cosine transform followed by byte-pair encoding, reducing inter-token correlation and enabling next-token prediction training of autoregressive VLAs on high-frequency and dexterous data. It improves markedly over per-dimension binning and learned VQ tokenizers in simulation and real-robot studies, and makes large-scale training on DROID feasible with zero-shot evaluation via language prompting. Building on this, π0-FAST is universal action tokenizer trained on 1M real trajectories across embodiments, action spaces, and control rates, providing an off-the-shelf tokenizer for VLA training. 17 Preprint. Under Review WorldVLA (Cen et al., 2025). WorldVLA is an autoregressive actionworld model that unifies image, text and action understanding and generation within single LLM framework. It encodes images, language, and actions with separate tokenizers that share one vocabulary, so the world module learns environment dynamics by predicting future visuals from actions, while the action module uses visual tokens to generate subsequent actionsyielding mutual enhancement between world modeling and control. The authors observe performance drops when generating multiple actions autoregressively due to error propagation and limited action generalization; they address this with an action-attention masking strategy that selectively masks prior actions when predicting the current one, improving chunked action generation. On LIBERO, WorldVLA improves grasp success by about 4% over an action-only backbone, reduces FVD for video generation by about 10% over vanilla world model. FlowVLA (Zhong et al., 2025). FlowVLA instantiates Visual Chain of Thought for world modeling by decomposing next-frame prediction into motionthenappearance process, i.e., vt ft vt+1 with ft an intermediate optical-flow target. By first committing to motion, the model learns disentangled dynamics that yield more coherent visual forecasts and more efficient policy learning. Practically, optical flows are encoded as RGB-like images and tokenized with the same VQ tokenizer as camera frames, so single autoregressive Transformer can process an interleaved sequence of motion and appearance tokens under shared vocabulary. Experiments on challenging manipulation benchmarks report state-of-the-art performance with substantially improved sample efficiency, supporting the claim that explicit motion reasoning better bridges pretraining and policy fine-tuning. DreamVLA (Zhang et al., 2025b). DreamVLA recasts VLA as perceptionpredictionaction framework by forecasting compact world knowledge to support inverse dynamics. Instead of full frame prediction, it predicts targeted cuesdynamic regions (via optical flow), depth maps, and high-level semantic features to provide concise yet informative guidance for action planning. block-wise structured attention masks cross-type interactions to prevent leakage and keep representations disentangled, and diffusion-based transformer decodes actions from shared latents while separating action factors from non-action features. Experiments show strong results in simulation and the real world, including 4.44 average task length on CALVIN ABCD and 76.7% real-robot success. Ablations indicate dynamic-region forecasting contributes the largest gains, with depth and semantics offering smaller, complementary benefits. F1 (Lv et al., 2025). F1 integrates goal-conditioned visual foresight into the perceptionaction loop and frames control as foresight-guided inverse dynamics. The model adopts Mixture-of-Transformer with three experts for understanding, foresight generation, and action execution. progressive attention scheme regulates information flow, and the generation expert uses next-scale prediction mechanism to produce explicit planning targets. Training follows three-stage recipe: Stage aligns the generation expert with pretrained understanding expert; Stage II pretrains the full model on large-scale public robot datasets; Stage III post-trains on task-specific data for embodiment adaptation and fine-grained skills. Experiments across simulation and real-world platforms report consistent gains over reactive baselines, with improved robustness and generalization in dynamic and long-horizon tasks. RoboVLMs (Liu et al., 2025). RoboVLMs provides systematic study and flexible framework for transferring foundation VLMs into VLAs, aiming to identify effective design choices. The work indicate that continuous-action policy heads perform best and that model choice, architecture, and data strategy all materially affect performance and data efficiency. The study also examines when to leverage cross-embodiment data by contrasting pre-training on Open-X Embodiment, fine-tuning on target domains, and post-training (pre-train then fine-tune). The resulting RoboVLMs achieve strong results in simulation and real-robot settings and generalize to unseen distractors, backgrounds, objects, and novel skill descriptions."
        },
        {
            "title": "D COMPARISON OF DISCRETE AND CONTINUOUS DIFFUSION",
            "content": "In discrete diffusion, the process involves transitioning between discrete states over time. For instance, in text generation, at each diffusion step, token might be randomly masked or replaced by another token. These transitions are controlled by predefined schedule or transition matrix, which specifies the probability of moving between states (e.g., retaining token, replacing it, or masking it). classic 18 Preprint. Under Review Figure 5: Qualitative Comparison of discrete diffusion and continuous diffusion. example is the mask-predict method used in language models, where tokens gradually emerge during sampling. Discrete models often rely on techniques like absorbing states (where tokens are masked) or uniform transitions between vocabulary items. In contrast, continuous diffusion models gradually add Gaussian noise to the data based on predefined noise schedule. For example, at each step, the pixel values of an image may be slightly perturbed by noise until it becomes pure noise. During sampling, the model learns to reverse this process by predicting and subtracting the noise at each step. Figure 5 visualizes the differences in the denoising and noising processes between the two approaches."
        }
    ],
    "affiliations": [
        "HKUST(GZ)",
        "Monash University",
        "Westlake University",
        "Zhejiang University"
    ]
}