{
    "paper_title": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions",
    "authors": [
        "Chao He",
        "Jianqiang Ren",
        "Liefeng Bo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The 2D cartoon style is a prominent art form in digital character creation, particularly popular among younger audiences. While advancements in digital human technology have spurred extensive research into photorealistic digital humans and 3D characters, interactive 2D cartoon characters have received comparatively less attention. Unlike 3D counterparts, which require sophisticated construction and resource-intensive rendering, Live2D, a widely-used format for 2D cartoon characters, offers a more efficient alternative, which allows to animate 2D characters in a manner that simulates 3D movement without the necessity of building a complete 3D model. Furthermore, Live2D employs lightweight HTML5 (H5) rendering, improving both accessibility and efficiency. In this technical report, we introduce Textoon, an innovative method for generating diverse 2D cartoon characters in the Live2D format based on text descriptions. The Textoon leverages cutting-edge language and vision models to comprehend textual intentions and generate 2D appearance, capable of creating a wide variety of stunning and interactive 2D characters within one minute. The project homepage is https://human3daigc.github.io/Textoon_webpage/."
        },
        {
            "title": "Start",
            "content": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions Chao He, Jianqiang Ren, Liefeng Bo Tongyi Lab, Alibaba Group {yichao.hc, jianqiang.rjq, liefeng.bo}@alibaba-inc.com 5 2 0 2 7 1 ] . [ 1 0 2 0 0 1 . 1 0 5 2 : r Figure 1. Examples of animatable 2D cartoon characters generated by Textoon."
        },
        {
            "title": "Abstract",
            "content": "The 2D cartoon style is prominent art form in digital character creation, particularly popular among younger audiences. While advancements in digital human technology have spurred extensive research into photorealistic digital humans and 3D characters, interactive 2D cartoon characters have received comparatively less attention. Unlike 3D counterparts, which require sophisticated construction and resource-intensive rendering, Live2D, widely-used format for 2D cartoon characters, offers more efficient alternative, which allows to animate 2D characters in manner that simulates 3D movement without the necessity of building complete 3D model. Furthermore, Live2D employs lightweight HTML5 (H5) rendering, improving both accessibility and efficiency. In this technical report, we introduce Textoon, an innovative method for generating diverse 2D cartoon characters in the Live2D format based on text descriptions. The Textoon leverages cutting-edge language and vision models to comprehend textual intentions and generate 2D appearance, capable of creating wide variety of stunning and interactive 2D characters within one minute. The project homepage is https://human3daigc.github.io/ Textoon_webpage/. 1. Introduction Cartoon characters are fictional beings often often distinguished by their adorable appearances and vivid colors. They are widely used in films, games, social media, and advertising. Cartoon characters can be crafted in both 2D and 3D styles. While 3D animations provide greater freedom for creation and control, they typically come with higher costs and depend on rendering engines. Conversely, creating 2D cartoon digital characters is more straightforward and efficient, making them especially suitable for devices with limited processing power, such as mobile phones and web applications. In the realm of 2D cartoon characters, Live2D[3] has emerged as leading standard for delivering real-time interactive performance. Live2D is technology used to create 2D character models with 3D-like interactivity. Live2D works by using model that consists of base illustration and set of control points that define how different parts of the character can move. These movements can be controlled, allowing for wide range of expressions and actions. Live2D effectively converts original 2D artwork into dynamic, animated characters. Its user-friendly, 2D interface makes it accessible for new illustrators and designers. Additionally, the lightweight models are highly compatible across various platforms, including HTML5 (H5). 1 While Live2D has made it easier to create animated 2D characters, the detailed processes of layering and mesh binding can still be time-consuming and labor-intensive. Additionally, modifying existing Live2D models to achieve different appearances remains challenge. To tackle these issues, we present Textoona framework designed to generate diverse Live2D models from text descriptions. Utilizing existing Live2D models and advanced generative technologies, Textoon empowers users to create customized Live2D models with straightforward textual prompts. The key features of Textoon include: Accurate Text Parsing. Our text parsing model excels at extracting detailed information from complex user descriptions. It accurately identifies features such as back hair, side hair, bangs, eye color, eyebrows, face shape, clothing type, and shoe type. This advanced text parsing capability allows for more flexible user inputs. Controllable Appearance Generation. After parsing the text, each component is synthesized into comprehensive character template. The contour boundaries offer precise control over the shape of the generated character, while text-to-image model takes charge of generating the inner color and texture. Editable. If users are not satisfied with the initial generated result and wish to modify specific details,our framework provides assistance in selecting specific positions to add, remove, or modify elements. Animation. The control coefficients for the Live2D models mouth primarily include MouthOpenY and MouthForm. MouthOpenY controls the vertical movement of the mouth, while MouthForm adjusts the expressions, such as upturning and grimacing. However, these controls often result in suboptimal driving performance. To enhance the accuracy of speech animations for cartoon characters, we integrate ARKits face blend shape capabilities into the Live2D lip-sync functionality. This integration significantly improves the realism and precision of the animated speech. Our main contributions in this work are as follows: To the best of our knowledge, Textoon is the first method that enables the generation of Live2D characters from text prompts, capable of creating new 2D cartoon character within minute without the need for manual binding. We fine-tune the LLM to accurately extract descriptive terms for each body component from complex texts, ensuring that the generated results closely align with the users input. Additionally, we leverage the capabilities of text-to-image models to create wide variety of stunning animatable 2D cartoon characters. By refactoring the Live2D facial animation mechanism, we significantly improve facial expressiveness. 2. Related Work Text-to-digital human. ChatAvatar[13] utilizes text inputs to generate facial assets with ultra-high-resolution textures, revolutionizing the traditional process of creating 3D assets. Using diffusion model and comprehensive facial asset dataset, it produces computer-generated (CG) assets that are compatible with mainstream rendering engines through text-based interactions, thereby simplifying the user experience. However, the generated outputs are incomplete and lack elements such as hair, eyeballs, bodies, and clothing, making them difficult for users to use directly. Make-A-Character[8] allows users to create high-quality, fully detailed, and animatable 3D digital humans through simple text descriptions. Users can specify features such as facial shape, eye characteristics, iris color, hairstyles and their colors, types of eyebrows, mouths, and noses, as well as the addition of wrinkles and freckles. However, the generated results require powerful rendering engine and incur high computational costs to render. Diffusion models and ControlNet. Stable Diffusion (SD)[9] exemplifies the powerful capabilities of diffusion models by integrating the UNet framework to iteratively generate images conditioned on text descriptions. ControlNet[12] enhances the control and flexibility of generative models, including diffusion models. It introduces additional control signals or constraints into the generation process, thereby improving the accuracy and consistency of the generated results. This approach significantly expands the range of applications and use cases of generative models. Through conditional generation methods, ControlNet can produce high-quality data that meet specific constraints or requirements, showcasing powerful functionality and potential in areas such as artistic creation, virtual reality, and film production. Large Language Models. Large Language Models (LLMs) typically refer to language models with hundreds of billions or even more parameters. Using vast amounts of data, powerful computational hardware, and Transformer architectures[10], these language models have been scaled to unprecedented sizes. Early research on LLM, such as T5[6], employed transfer learning techniques. Subsequently, GPT-3[2] demonstrated that LLMs could achieve zero-shot transfer to downstream tasks without the need for fine-tuning. When provided with task descriptions and example prompts, LLMs can generate accurate responses. For example, the latest Qwen2.5 model[11], trained on an exceptionally large dataset of 18 trillion tokens, excels at following instructions, generating extended texts, understanding structured data and producing structured outputs. 2 Figure 2. Pipeline of the Textoon. The framework includes text parsing, controllable appearance generation, re-editing, and component completion and repair modules. 3. Live2D Generation Figure 3. Meshes of different layers. This section provides detailed introduction to our approach. We begin with brief review of the implementation principles of Live2D and introduce our component splitting method designed to enhance generation diversity. Then, we present our carefully designed modules for text parsing, controllable appearance generation, re-editing, and component completion, which enable us to generate brand-new Live2D character from single sentence within minute, as shown in Fig. 2. 3.1. Preliminary of Live2D typical Live2D character comprises multiple layers, including the body, clothing, hair, and other elements. Each layer is segmented into polygon mesh, which controls the respective parts of the 2D character, as illustrated in Fig. 3. This layered composition facilitates smooth deformations by manipulating the positions of the mesh points. 3.2. Component Splitting Given that Live2D models consist of numerous layers, we decide to merge some intricate, smaller layers to decrease the overall layer count. While this may slightly impact the expressiveness of detailed movements, it simplifies the generation process. For each body part, we choose to use larger elements to generate smaller-scale elements, thereby enhancing the Figure 4. Splitting model components, larger elements can be utilized to create short variations. Figure 5. Using the fine-tuned LLM to parse component categories from complex input text. models diversity. As illustrated in Fig. 4, the long hair element can be utilized to create short hair variations. 3.3. Text Parsing We need to parse the corresponding body parts from the text input and then combine them to control the next step of generation. Large language models are unable to directly extract suitable component words from complex and varied input texts. To address this, we generate descriptive text by randomly combining existing components with commonly used words, resulting in 640,000 text-component pairs. This generated data is used to fine-tune the Qwen2.5-1.5B[11] model. As shown in Fig. 5, we can accurately parse component categories from complex input text at millisecond speeds within 4GB of memory (RTX 4090), achieving an accuracy rate of more than 90%. 3.4. Controllable Appearance Generation The facial proportions of 2D cartoon characters tend to remain consistent, with their unique traits primarily found in their hair, clothing, and accessories. Furthermore, the high resolution of 2D original artwork makes the choice of image generation model especially important. We evaluated the top text-to-image models based on control accuracy, generation quality, text relevance, and their ability to create text or patterns. Ultimately, we selected SDXL [5] as the optimal choice. SDXL excels in controllability, produces images with sharp edges, and supports maximum resolution of 1024 pixels. Additionally, it handles long text descriptions effectively and generates precise text patterns accurately. To maintain the models driving performance, it is essential that the generated output adhere to the specified areas for each component. In our template model, we categorize the components as follows: 5 types of back hair, 3 types of mid hair, 3 types of front hair, 5 types of tops, 6 types of sleeves, 5 types of pants, 5 types of skirts, and 6 types of shoes, as shown in Fig. 6. By combining these components and utilizing the control features of the base model, we have managed to achieve wide variety of outputs while preserving the original driving performance. 3.5. Re-editing We employ an image editing technique[4] that enables users to freely sketch specific areas and annotate these sketches with text to refine the details of the initial character image. Once the users specifications are satisfied, the adjusted character image is finalized as the final character appearance. Figure 6. The divisions of each component within our template model. Figure 7. Restoring the back hair: First, extract the pixels (b) from the generated image using the model pattern (a). Then, fill the area occluded by the head with pixels from the region connected to the front hair (c). Finally, perform image-to-image generation (d). 3.6. Component Completion Once the final character image is established, the next step involves parsing the image and precisely positioning each component onto their respective layers. This process presents two main challenges. The first one is achieving pixel-level segmentation for each component, task that remains difficult even when employing the SAM2 method [7]. The second challenge involves addressing occluded areas. To address the first challenge, we use template generated by the control mechanism as mask to extract pixels directly from the original image. For the second challenge, we initially fill the occluded areas with pixels from the unoccluded regions, followed by the application of image-to-image control generation for refinement. For example, when restoring the back hair, which is largely obscured by the head  (Fig. 7)  , we first erase the pixels of the head and then use image-to-image inpainting for reconstruction. This method not only prevents the generation of unwanted content but also ensures color consistency with the front hair. 5 Figure 8. Live2D model supporting ARKit lip-sync driving. Figure 9. The overall animation effects of the generated Live2D model. 3.7. Animation Most Live2D models typically rely on just two parametersMouthOpenY and MouthFormfor lip-syncing, leading to limited dynamic interactions. In contrast, ARKit [1] offers 52 parameters to capture detailed facial expressions. Inspired by ARKits extensive parameter set, we have developed more comprehensive lip-sync parameters, as illustrated in Fig. 8. By creating Live2D mouth blendshapes that correspond to the ARKit framework, we significantly enhance the liveliness and expressiveness of model animations, as demonstrated in Fig. 9. 6 Figure 10. Examples of Live2D cartoon characters created along with their corresponding text prompts. 4. Results Combining the modules mentioned above, our system is capable of generating controllable, aesthetically pleasing, and drivable Live2D character based on single sentence within one minute. Fig. 10 shows some of our generated outputs, which validate the effectiveness of our method in terms of visual appeal and diversity. 5. Limitation Although we innovatively apply generative models to the creation of Live2D characters, enabling the automated generation of various 2D cartoon avatars, there are still some limitations. Firstly, our process relies on input text to generate Live2D characters, but text has difficulty conveying complex and nuanced information, making the controlled generation of details challenge. Secondly, our generated results are constrained by the layout of the component layers in the original Live2D models, with limited variety of component styles available. 6. Conclusion We present Textoon, the first method for generating diverse Live2D cartoon characters from text descriptions. By harnessing cutting-edge language and vision models, Textoon can quickly create variety of stunning and interactive 2D characters in less than minute. We also integrate ARKit-compatible facial blendshapes, enhancing mouth movements for more expressive interactions, allowing lively conversations with users. The live2D cartoon characters generated can be seamlessly rendered using HTML5, offering wide range of application possibilities."
        },
        {
            "title": "References",
            "content": "[1] Arkit face blendshapes. https://arkit-face-blendshapes.com/. Accessed: 2023-10-15. 6 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. 2 [3] Live2D Inc. Live2d official website. https://www.live2d.com, 2023. Accessed: October 1, 2023. 1 [4] Zichen Liu, Yue Yu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Wen Wang, Zhiheng Liu, Qifeng Chen, and Yujun Shen. Magicquill: An intelligent interactive image editing system. arXiv preprint arXiv:2411.09703, 2024. 4 7 [5] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 2 [7] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 5 [8] Jianqiang Ren, Chao He, Lin Liu, Jiahao Chen, Yutong Wang, Yafei Song, Jianfang Li, Tangli Xue, Siqi Hu, Tao Chen, Kunkun Zheng, Jianjing Xiang, and Liefeng Bo. Make-a-character: High quality text-to-3d character generation within minutes. arXiv preprint arXiv:2312.15430, 2023. 2 [9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [10] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 2 [11] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2, 4 [12] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 2 [13] Qixuan Zhang, Longwen Zhang, Lan Xu, Di Wu, and Jingyi Yu. Chatavatar: Creating hyper-realistic physically-based 3d facial assets through ai-driven conversations. In ACM SIGGRAPH 2023 Real-Time Live!, New York, NY, USA, 2023. Association for Computing Machinery."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}