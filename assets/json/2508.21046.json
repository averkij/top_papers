{
    "paper_title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
    "authors": [
        "Wei Li",
        "Renshan Zhang",
        "Rui Shao",
        "Jie He",
        "Liqiang Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 6 4 0 1 2 . 8 0 5 2 : r CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification Wei Li Renshan Zhang Rui Shao Jie He Liqiang Nie School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen liwei2024@stu.hit.edu.cn shaorui@hit.edu.cn https://jiutian-vl.github.io/CogVLA-page Figure 1: Overview of our proposed CogVLA. Traditional VLA models process initial observations (Fig.(a)) without vision compression, leading to high computational cost. As shown in Fig.(b) and Fig.(d), existing compression methods retain irrelevant inputs and fail to focus on instructionrelevant targets . CogVLA employs EFA-Routing and LFP-Routing to sparsify visual inputs based on instruction relevance. Comparing Fig.(c) and Fig.(e), CAtten further enhances logical consistency . Fig.(f), Fig.(g), and Fig.(h) illustrate the architectural and action coherence for final targeted objects innovations of CogVLA and its superiority in efficiency and performance."
        },
        {
            "title": "Abstract",
            "content": "Recent Vision-Language-Action (VLA) models built on pre-trained VisionLanguage Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment. Existing sparsification strategiessuch as Mixture-of-Depths, layer skipping, and early exitfall short by neglecting the semantic coupling across vision-language-action modalities, Corresponding author Preprint. Under review. and focusing narrowly on intra-LLM computation while overlooking end-to-end coherence from perception to control. To address these challenges, we propose CogVLA, Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5 and decreasing inference latency by 2.8 compared to OpenVLA."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Action (VLA)[81, 35, 29, 6, 78, 79] research has advanced rapidly, fueled by the rich visual and linguistic representations provided by powerful pre-trained Vision-Language Models (VLMs)[34, 1, 42, 21, 2, 43]. Leveraging these foundational models, the VLA paradigm is progressing toward end-to-end robotic control and embodied intelligence, enabling agents to comprehend natural language instructions, perceive complex scenes, and perform manipulation tasks with minimal task-specific engineering. Pioneering works such as RT-2 [8], Octo [65], OpenVLA [29], π0 [6], and π0.5 [26] have demonstrated the potential of this paradigm. However, aligning the high-dimensional multimodal features output by VLMs with continuous action spaces remains computationally expensive [80, 70, 10, 5, 29]. Standard fine-tuning and joint training procedures often entail substantial memory consumption, high FLOPs overhead, and extended training times, severely limiting scalability and practical deployment on resourceconstrained platforms. For instance, fine-tuning 7B VLA model [28] with action chunking on single-task dataset from the LIBERO benchmark [41] consumes over 600 GPU hours (using 80G A100 GPUs), incurring significant computational costs. Although techniques such as Mixture-ofDepths [52, 74, 47], layer skipping [76, 71], and early exit [73, 16] have been proposed to sparsify and accelerate the model training and inference, these methods primarily focus on computation optimization within language models, overlooking the semantic coupling across perception, language alignment, and action decoding. This modular optimization paradigm often leads to cross-modal semantic degradation, manifesting as follows: i) visual compression within encoders discards task-relevant fine-grained features, ii) token skipping within LLMs disrupts the contextual coherence necessary for reference resolution, and iii) action generation lacks causal reasoning over multimodal state transitions. From cognitive science perspective [53, 31], humans exhibit highly optimized and efficient multimodal coordination mechanism during manipulation. For example, when receiving the instruction \"place the red cup at the corner of the table,\" the human Visual and Attention System (VAS) selectively focuses [7] on the color attributes of the cup and the spatial structure of the table. Concurrently, the Supplementary Motor Area (SMA) injects task-relevant action intentions [62] derived from key semantic associations (e.g., redcupcorner) into the visual processing stream, while the Premotor Cortex (PMC) dynamically integrates both visual and linguistic information to plan coherent motion trajectories. This organic unification of perception, reasoning, and control results in remarkable task efficiency. Inspired by this, we propose CogVLA, Cognition-Aligned Vision-Language-Action framework based on Instruction-Driven Routing & Sparsification, as shown in Fig. 1 (f). Unlike existing modular pipelines, CogVLA establishes task-semantic-consistent joint optimization mechanism across vision, language, and action modalities, reinforcing cross-modal coherence while improving computational efficiency. 2 Specifically, CogVLA adopts 3-stage progressive design to jointly enhance computational efficiency and task performance, as shown in Fig. 1 (d) and (e): 1) Encoder-FiLM based Aggregation Routing (EFA-Routing): To alleviate visual information redundancy and achieve VAS-like visual focus, EFA-Routing compresses visual tokens to 25% of the original input scale, guided by task-specific instructions. This process begins by dynamically encoding the instruction into modulation parameters that guide the aggregation of visual tokens within the visual encoder. Subsequently, the outputs from different encoder branches are adaptively fused to produce cross-branch representations that are semantically aligned with the given task. 2) LLM-FiLM based Pruning Routing (LFP-Routing): Building upon the aggregated visual encoding, LFP-Routing learns novel, instruction-aware sparsity pattern to prune visual tokens within the language model. By emulating the functionality of SMA, which injects action intentions into visual features, the mechanism selectively skips attention computations over 50% of task-irrelevant tokens. As result, it significantly reduces the computational burden of the language model and effectively minimizes latency in action generation. 3) V-L-A Coupled Attention (CAtten): To ensure that the compressed visual inputs retain the capacity to support accurate and coherent action sequence, CAtten introduces coupled attention mechanism inspired by PMC: i) Cross-modal causal attention is applied between the V-L-A interaction layer to preserve temporal reasoning capabilities; ii) Unidirectional attention is employed within the V-L layer to ensure semantic consistency, where visual features have been pre-enhanced with task-specific language intent; iii) Bidirectional attention is utilized within the Action layer to enhance the coherence of action sequences and enable efficient parallel decoding. We conduct comprehensive evaluations of CogVLA on the LIBERO benchmark and real-world robotic manipulation tasks. Experimental results show that CogVLA achieves state-of-the-art task success rates while reducing end-to-end computational costs significantly, as shown in Fig. 1 (g) and (h). Ablation studies further validate the complementarity and synergistic effect of the routing modules and the coupled attention mechanism. Our main contributions are summarized as follows: We propose CogVLA, Cognition-Aligned Vision-Language-Action framework inspired by human multimodal coordination, which establishes biomimetic 3-stage architecture: VAS (visual information focusing) SMA (semantic intent filtering) PMC (action sequence planning).\" We develop synergistic EFA-Routing and LFP-Routing, enabling instruction-driven vision sparsification in perception-reasoning pipelines. We formulate CAtten ensuring cross-modal logical consistency and temporal action coherence in doubly compressed multimodal representations. Through extensive experiments on the LIBERO benchmark and real-world robotic tasks, we demonstrate the superior performance and efficiency of CogVLA."
        },
        {
            "title": "2 Methods",
            "content": "2.1 Preliminary: Parallel Decoding in Action Chunk We consider sequence prediction setting where Vision-Language-Action (VLA) model outputs sequence of actions across future timesteps. Traditional autoregressive (AR) decoding predicts actions sequentially, whereas parallel decoding enables simultaneous prediction of all actions within the chunk, improving inference efficiency and supporting scalable deployment. Action Chunk. Given the current input context = {I, t} RM +T , which comprises the visual observation and task instruction, the model predicts chunk of future actions: = [a0, a1, . . . , aK1] RKD (1) Here, denotes the dimensionality of each atomic action (e.g., = 7 for 3-DoF translation , 3-DoF rotation R, and binary gripper control). Autoregressive Decoding. In causal autoregressive decoding, the action sequence is generated incrementally. For each timestep 0, . . . , 1, the atomic action vector ai RD is produced token-by-token, with each token a(k) conditioned on the preceding tokens and previously actions: i = fAR([X, {aτ }τ <i, a(1:k1) a(k) (2) This decoding necessitates forward passes, introducing latency from token-level dependencies. ai = [a(1) , , a(D) , a(2) ], ]) i 3 Figure 2: Overview of CogVLA Framework. CogVLA employs cognition-aligned, instructiondriven routing & sparsification strategy for efficient action chunk prediction. Inspired by human multimodal coordination, it integrates task-guided visual aggregation, semantic pruning, and coherent decoding, ensuring efficient cross-modal representation alignment from perception to control. Parallel Decoding. In contrast, parallel decoding eliminates the sequential dependency. The model receives the input observation embeddings along with empty placeholder embeddings: = [X, 00, 01, . . . , 0K1] RM +T +KD (3) where 0i RD denotes learnable zero-action embedding. Under bidirectional attention scheme (instead of causal masking), the decoder jointly produces all future actions in single pass: = fparallel( X) (4) 2.2 CogVLA: Framework Recent methods [73, 76, 50, 63, 28] primarily focus on lightweight computation within isolated stages of action chunk prediction in VLA models, often leading to cross-modal semantic degradation due to modular disconnection. To address this, we propose CogVLA, cognition-aligned framework that enhances both efficiency and performance via Instruction-Driven Routing & Sparsification. As illustrated in Fig. 2, the framework operates through 3-stage progressive architecture inspired by human multimodal coordination. In Stage 1, CogVLA incorporates vision encoders {Enc1, . . . , EncN } that extract visual tokens from image observations I(i). Each encoder is modulated by the instruction tr (obtained via LLM embedding layer) through an Encoder-FiLM module: agg = Encoder-FiLMi(I(i), v(i) v(i) agg, tr), = 1, ..., (5) where v(i) tokens are dynamically dual-aggregated via an instruction-conditioned routing mechanism: agg denotes the aggregation token for the i-th encoder. These modality-specific aggregated vagg = (cid:88) αi v(i) agg (6) i=1 α = [α1, . . . , αN ] = Softmax (MLProute(tr)) (7) In Stage 2, the dual-aggregation tokens are injected into the LLM, where LFP-Routing selectively filters out instruction-irrelevant visual tokens. This instruction-driven sparsification enables tokenlevel efficiency by reducing redundant attention computation, aligning the retained tokens with task-relevant semantics. The filtered representation is then processed by the proposed CAtten module to produce the task-aligned representation in Stage 3. The module operates as follows: Zl+1 = CAtten(LFP-Routing(Zl, tl)) where Zl, and tl denote the visual and instruction input tokens at the (l + 1)-th transformer layer, respectively, with Z0 = vagg as the initial state. Finally, action chunks are decoded in parallel using the compressed multimodal context combined with placeholder action tokens: At = fparallel( X) = fparallel([Z0, t0, 00, 01, . . . , 0K1]) (9) (8) 4 Figure 3: Illustration of 3-Stage Progressive Design. CogVLA emulates human multimodal coordination via instruction-driven routing and sparsification. EFA-Routing (Stage 1), LFP-Routing (Stage 2), and CAtten (Stage 3) correspond to the VAS, SMA, and PMC, respectively. Fig.(c) highlights the advantages of CAtten over prior attention mechanisms in combining uni-&bi-directional attention, injecting action intent, enabling parallel decoding, and leveraging sparse visual tokens. Through this progressive design across Stage 13, CogVLA realizes instruction-driven sparsification and routing across the vision-language-action pipeline, effectively reducing computational overhead while preserving task-relevant semantics and enhancing cross-modal reasoning fidelity. 2.3 CogVLA: Cognition-Aligned 3-Stage Progressive Design As illustrated in Fig. 3, CogVLA adopts 3-Stage Progressive Design, which emulates the humans optimized coordination during manipulation tasks: EFA-Routing mimics the VAS by selectively aggregating visual tokens conditioned on task-specific instructions, thereby achieving focused perception. LFP-Routing emulates the SMA by introducing action intentions into the visual context within the language model, selectively pruning irrelevant tokens to achieve instruction-driven token sparsity. CAtten simulates the PMC by dynamically integrating compressed multimodal representations, ensuring cross-modal logical consistency and temporal coherence in action decoding. 2.3.1 Encoder-FiLM based Aggregation Routing Step 1: Intra-encoder Aggregation. To aggregate visual information and enable instruction-guided representation learning, we introduce Encoder-FiLM, which dynamically consolidates observation tokens into aggregation tokens based on task-specific instructions. The language instruction tr modulates visual tokens I(i) and aggregation tokens v(i) agg within each visual encoder branch: fFA(I(i), v(i) agg, tr) = (1 + γi(tr)) Self-Att(I(i), v(i) agg = Aggregate(FFN(fFA())) + v(i) v(i) agg where γi and βi denote the FiLM-generated scale and shift vectors conditioned on tr, and represents element-wise multiplication. Through iterative visual encoder blocks, the aggregation token v(i) agg adaptively integrates instruction-relevant information from observation tokens while discarding redundant information. Consequently, only the final v(i) agg is retained while the image tokens I(i) are discarded, effectively reducing the number of visual tokens to 25% of the original size. agg) + βi(tr) (10) Step 2: Cross-encoder Aggregation. To integrate the aggregated visual representations from two heterogeneous vision encoder branches (SigLIP and DINOv2), we design an instruction-conditioned aggregation routing gate that computes fusion weight α (0, 1) based on the input language 5 instruction. Rather than statically assigning equal contributions, the fusion ratio is dynamically predicted for different tasks to reflect instruction-dependent visual preferences: α = Sigmoid(W2(σ(W1tr + b1)) + b2) (11) where W1, W2 are trainable weight matrices, b1, b2 are biases, and σ denotes the GeLU [23] non-linearity. The final dual-aggregated visual token is computed as: vagg = α vSigLIP agg + (1 α) vDINOv2 agg (12) This instruction-conditioned aggregation routing allows the model to adaptively balance visual features from different encoders based on the semantics of the instruction, promoting more effective cross-modal fusion, as shown in Fig. 2 and Fig. 3 (a). At the first transformer layer, we denote Z0 = vagg as the dual-aggregation visual tokens and t0 as the instruction tokens, which serve as the initial inputs for Stage 2. 2.3.2 LLM-FiLM based Pruning Routing Motivated by sparse token routing techniques [74, 47, 71], we recognize that EFA-Routing (Stage 1) aggregates features across all image tokens, potentially retaining redundant or semantically irrelevant visual information. To further reduce computational overhead and steer the visual representation toward the intended action semantics, we propose lightweight LFP-Routing module prior to injecting visual context into the large language model, as illustrated in Fig. 3(b). Given the dual-aggregation tokens and the corresponding task instruction at transformer layer l, denoted as Zl and tl respectively, LLM-FiLM performs semantic-aware modulation as follows: fFP(Zl, tl) = Prune((1 + γLLM(tl)) Zl) + βLLM(tl)) (13) Zl+1 = FFN(Self-Att(fFP())) + Zl where γLLM() and βLLM() denote instruction-conditioned scaling and shifting functions, respectively, both implemented as lightweight MLPs. The Prune() operation selectively discards tokens with low task relevance, producing filtered representation that maintains critical visual semantics. We introduce Task-Guided Pruning Router to implement the Prune() operation. This module filters tokens based on their instruction-aware relevance, preserving only those most critical to the task. At Transformer layer l, routing weights Rj are computed for each visual token Zj using an MLP: Rj = MLP(Zj ) (14) β as the β-th percentile of We define token retention ratio β, and determine relevance threshold the routing weights at layer l. Only tokens whose scores exceed β are preserved. Formally: (cid:40) if Rj > β otherwise , tl]) + Zj , Rj fSF([Zj Zj , l+1 = Zj (15) where fSF() represents the self-attention and feed-forward operations within the current layer. The hyperparameter β [0, 1] governs the sparsity level by controlling the proportion of retained tokens. 2.3.3 V-L-A Coupled Attention To maintain semantic consistency and temporal coherence under compressed multimodal inputs, we introduce V-L-A Coupled Attention (CAtten), biologically inspired mechanism grounded in the functional role of the PMC for planning and coordination. As shown in Fig. 3 (c), CAtten hierarchically combines causal and bidirectional attention across vision, language, and action streams. At the l-th transformer layer of the LLM, the input multimodal token sequence is defined as: = [Zl, tl, Al] RM +T +KD (16) where Al = [al K1] denotes the action chunk. CAtten operates in three consecutive stages: Causal Vision-Language Attention. To preserve instruction-conditioned visual reasoning, causal attention is applied over the concatenated vision-language token segment: (cid:17) 0, . . . , al AttnVL([Zl, tl]) = Softmax (cid:16) [Zl,tl][Zl,tl] + MVL causal [Zl, tl] (17) where MVL causal R(M +T )(M +T ) is lower-triangular mask within vision-language tokens. Table 1: Simulation Experimental Results. Comparison of task success rates (SR) and their ranks (RK) on the LIBERO benchmark across four task types. indicates our reproduced results. Method Spatial Goal SR RK SR RK SR RK SR RK SR RK Average Object Long Diffusion Policy [RSS23] [13] Octo fine-tuned [RSS23] [65] OpenVLA [CoRL24] [29] π0 fine-tuned [RSS25] [6] π0-Fast [RSS25] [50] π0.5-KI [arXiv25] [17] OpenVLA-OFT [RSS25] [28] SpatialVLA [RSS25] [51] PD-VLA [arXiv25] [63] STAR [ICML25] [22] Dita [arXiv25] [24] CoT-VLA [CVPR25] [78] CogVLA 78.3 78.9 84.7 96.8 96.4 98.0 97.6 88.2 95.5 95.5 84.2 87.5 98.6 11 10 8 3 5 2 4 6 6 7 9 7 1 92.5 85.7 88.4 98.8 96.8 97.8 98.4 89.9 96.7 98.3 96.3 91.6 98.8 7 11 10 1 6 5 3 9 7 4 8 8 68.3 84.6 79.2 95.8 88.6 95.6 97.9 78.6 94.9 95.0 85.4 87.6 96.6 11 8 9 3 7 4 1 10 6 5 9 8 2 50.5 51.1 53.7 85.2 60.2 85.8 94.5 55.5 91.7 88.5 63.8 69.0 95.4 11 10 9 5 7 4 2 8 3 6 6 6 1 72.4 75.1 76.5 94.2 85.5 96.0 97.1 78.1 94.7 94.3 82.4 83.9 97.4 11 10 9 5 7 4 2 8 3 6 7 6 Table 2: Real-world Experimental Results. Performance comparison on the Cobot Agilex ALOHA tasks. indicates our reproduced results, while * denotes results reported in the original paper. Average Method SR CubePlate +ToyBowl Open +Place +Close Step 1 +Step 2 +Step 3 Drawer Manipulation Object Placement T-shirt Folding VQ-BeT [33]* QueST [48]* STAR* [22] PD-VLA [63] OpenVLA-OFT [28] CogVLA 5/10 6/10 8/10 8/10 8/10 9/10 3/10 4/10 6/10 7/10 7/10 8/10 4/10 3/10 6/10 6/10 8/10 8/10 3/10 1/10 4/10 6/10 6/10 7/10 1/10 0/10 3/10 4/10 5/10 7/10 - - - 7/10 7/10 9/ - - - 6/10 7/10 8/10 - - - 4/10 5/10 6/10 20.0% 20.0% 45.0% 50.0% 56.7% 70.0% Bidirectional Action Chunk Decoding. To support coherent action generation, bidirectional attention is employed within the action decoding, allowing full context integration among future action tokens: Attnact(Al) = Softmax (18) bi R(KD)(KD) enables full continuous parallel decoding within each action chunk where Mact while maintaining causal dependencies from visual-language inputs. Unified Hybrid Attention Mask. global attention mask Mhybrid R(M +T +KD)(M +T +KD) enforces hierarchical token dependencies across vision, language, and action modalities: Al + Mact bi (cid:16) AlA (cid:17) CAtten( X) = Softmax (cid:17) + Mhybrid (cid:16) X MVL causal 0 0 0 Mact 0 bi Mhybrid = (19) (20) This coupled attention structure enables CogVLA to retain fine-grained cross-modal alignment and planning consistency under significant sparsification of visual inputs, ensuring that action sequences remain both semantically relevant and temporally coherent throughout the decoding process."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experiments Setting All experiments are conducted on 4 A800 GPUs (80GB), benefiting from CogVLAs efficient instruction-driven sparsification. Implementation details are in Appendix A. Simulation Benchmark. We use the LIBERO benchmark [41] to evaluate task performance and efficiency. Its long and diverse instructions (avg. 10.48 words vs. 3.34 in RLBench) reflect the 7 Table 3: Efficiency Optimization Results. CogVLA maintains superior performance while achieving the highest efficiency. Ablation studies on Stage 1 and Stage 2 validate the efficiency contribution of each routing module. indicates our reproduced results. Method Inference Time Throughput FLOPs Taining Cost LIBERO SR OpenVLA [29] OpenVLA-OFT [28] PD-VLA [63] CogVLA w/o Stage 1 w/o Stage 2 0.254 0.132 0.143 0.091 0.162 0.117 3.9 Hz 60.6 Hz 55.9 Hz 87.9 Hz 49.4 Hz 68.4 Hz 8.48 8.45 8.48 2.72 5.38 3.52 11.7 h/10k steps 12.5 h/10k steps 11.7 h/10k steps 4.7 h/10k steps 8.4 h/10k steps 5.3 h/10k steps 76.5% 97.1% 94.7% 97.4% - - Figure 4: Visualization comparison between CogVLA and OpenVLA-OFT. CogVLA outperforms OpenVLA-OFT in success rates on both simulation and real-world tasks, achieving state-of-the-art performance with 31% reduction in inference time. It also demonstrates superior training efficiency, requiring 3.1 fewer FLOPs and 2.7 shorter training time. models language understanding. LIBERO covers four suitesSpatial, Object, Goal, and Longeach with 10 tasks and 50 demonstrations. Real-World Experiments. CogVLA is deployed on the Cobot Agilex ALOHA platform for three long-horizon tasks: Object Placement, Drawer Manipulation, and T-shirt Folding (45, 45, and 30 demonstrations). We introduce spatial and semantic variations during data collection. Baselines. We compare CogVLA with multiple state-of-the-art methods, such as OpenVLA, SpatialVLA, STAR, and CoT-VLA. For efficiency assessment, we further evaluate OpenVLA, along with the top-performing PD-VLA and OpenVLA-OFTan improved variant of OpenVLA that achieves higher performance and efficiencyunder the same fine-tuning and inference settings as CogVLA. 3.2 Performance improvement Simulation Experimental Results. The diverse task suites in the LIBERO benchmark reflect varying levels of instruction-following requirements from different perspectives. We conducted 500 trials for each task suite, and CogVLA achieved the highest success rate of 97.4%, as shown in 8 Table 4: Ablation study on model components. Pruning and TG-Pruning denote LFP-Routing w/o and w/ instruction guidance. All ablations maintain fixed 8 overall sparsification ratio. Table 5: Ablation on sparsification ratio allocation. Spf.Ratio denotes sparsification ratio, which can be tuned based on performanceefficiency trade-off. Stage 1 Stage Stage 3 Spatial SR Stage 1 Stage 2 Spf.Ratio Spatial SR Step 1 Step 2 Pruning TG-Pruning 91.2 (-7.4) 96.0 (-2.6) 95.2 (-3.4) 92.0 (-6.6) 96.2 (-2.4) 92.0 (-6.6) 98.6 1 8 2 4 8 1 4 2 8 8 8 8 91.2 (-7.4) 92.0 (-6.6) 94.6 (-4.0) 98. Table 6: Comparison of Stage 1+2 with other visual compression methods. FastV [12] SliME [77] Stage 1+2 SR 88.2 (-10.4) 77.6 (-21.0) 98.6 Tab. 1. This strong performance stems from CogVLAs 3-stage progressive design, which enhances instruction-driven perception throughout the manipulation process. Notably, CogVLA ranks second only in the LIBERO-Goal suite, primarily due to deliberate trade-off between performance and efficiencyCogVLA reduces visual input by 8 compared to other VLA models in the table. Real-world Experimental Results. We conducted real-world training and evaluation of the top four models from the LIBERO simulation benchmark on complex long-horizon tasks with rich instructions (Object Placement and Drawer Manipulation) and the representative dual-arm task T-shirt Folding. As shown in Tab. 2, CogVLA achieved the highest subtask and overall success rates. To better assess instruction-following ability in real-world settings, we collected ALOHA-based experimental data and applied data augmentation following LIBEROs protocol, including variations in spatial arrangements, manipulated objects, and their attributes. The results demonstrate that CogVLAs performance advantage generalizes effectively to real-world tasks. 3.3 Efficiency Optimization As shown in Tab. 3, CogVLA achieves 2.79 faster inference time, 22.54 higher throughput, 3.12 lower FLOPs, and 2.49 reduction in training cost compared to OpenVLA. Moreover, CogVLA also outperforms state-of-the-art efficient VLA models such as OpenVLA-OFT and PD-VLA in both training and inference efficiency. These gains stem from: 1) instruction-driven vision sparsification achieved via EFA-Routing and LFP-Routing, reducing visual input by up to 8; and 2) parallel action decoding enabled by bidirectional attention in the CAtten module. 3.4 Qualitative Analysis As shown in Fig. 4, we compare the performance and efficiency of CogVLA and OpenVLA-OFT in both the LIBERO simulation and the ALOHA real-world setting. With strong instruction guidance and improved logical consistency, CogVLA executes manipulation tasks more accurately and avoids failures such as drawer collisions (e.g., row 3, column 5). CogVLA also demonstrates shorter action inference time, with its efficiency advantage becoming more pronounced as the task length increases. 3.5 Ablation Studies Tab. 4 validates the effectiveness of each module within CogVLAs 3-stage progressive design, highlighting their synergistic contributions under unified framework. Tab. 5 presents different sparsity ratio allocations across Stage 1 and Stage 2 under fixed 8 vision sparsification. Results show that both stages contribute to performance gains, with larger Stage 1 ratios yielding greater improvements, as Stage 2 further filters instruction-relevant tokens based on Stage 1 outputs. Tab. 6 demonstrates that the combined Stage 1+2 sparsification outperforms existing methods, as it is more instruction-driven and deeply integrated into the overall CogVLA architecture. Additional ablation studies are provided in Appendix C.3."
        },
        {
            "title": "4 Related Work",
            "content": "Vision-Language Action (VLA) Models. Vision-Language Models (VLMs) [34, 1, 42, 21, 58, 2, 43, 75, 36] have propelled robotic control by providing rich multimodal representations, fostering the development of VLA models [10, 38, 59, 44, 67, 69, 57] that bridge perception and action generation. Early works like CLIPort [60] and PerAct [61] aligned visual features with language-conditioned action policies. The RT series [9, 8, 4] introduced action tokenization to enable scalable web-to-robot transfer. More recently, Octo [65] constructed diverse multi-robot dataset to support multitask training, while OpenVLA [29] demonstrated superior generalization to household tasks compared to diffusion-based methods. The π series [6, 26] proposed heterogeneous co-training across robots and semantic prediction tasks to enhance open-world generalization. However, directly fine-tuning billion-parameter VLMs for action prediction remains computationally intensive, limiting scalability. Efficient Design in VLA Models. Improving VLA efficiency has largely followed two paths: LLMcentric and vision-centric. LLM-centric approaches include Mixture-of-Depth (MoD) pruning [52, 74, 47], dynamic reasoning depth adjustment [73, 76], sparse Mixture-of-Experts (MoE) architectures [15, 40, 11], and lightweight models like DeeR-VLA [73], RoboMamb [45], and TinyVLA [68], all aiming to reduce decoding overhead. Vision-centric approaches focus on reducing the number of visual tokens passed to the LLM, employing techniques such as patch token selection based on similarity [56, 37], cropping-based techniques [46, 25], and an additional compression module [3, 39, 72]. However, naively adapting these methods often leads to semantic inconsistency across modalities due to lack of unified sparsification. To address this, we propose cognition-aligned, instruction-driven sparsification framework that jointly improves efficiency and cross-modal consistency."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented CogVLA, cognition-aligned and instruction-driven Vision-Language-Action framework designed to address the computational inefficiencies and semantic fragmentation in existing VLA models. By integrating EFA-Routing, LFP-Routing, and CAtten into unified 3-stage progressive design, CogVLA achieves effective vision sparsification and coherent cross-modal reasoning. Extensive evaluations on both the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA not only achieves state-of-the-art performance but also significantly reduces computational cost and inference latency. This work highlights the importance of instruction-driven multimodal sparsification in building scalable and efficient embodied AI systems. 10 CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification"
        },
        {
            "title": "Appendix",
            "content": "This appendix provides comprehensive supplementary material to support the methodology, analysis, and findings presented in the main paper. Section describes implementation details, including model and training details. Section outlines experimental details for both simulation and real-world settings. Section presents extended quantitative analyses, including multi-seed evaluations, additional ablation studies, and expanded real-world results. Section provides supplementary qualitative analyses, such as diverse task executions and instruction-to-observation attention visualizations. Section discusses additional insights into the motivation behind CogVLA, highlights its current limitations, and reflects on the broader societal implications and potential risks. We provide third-person view videos at https://jiutian-vl.github.io/CogVLA-page , demonstrating CogVLA performing manipulation tasks in fully autonomous mode, played at 1 speed. Due to the requirement of remote communication during each action chunk prediction, slight delays are introduced by network latency. For future deployments, we plan to run CogVLA locally on hardware with more than 20 GB of GPU memory (e.g., RTX 4090 with 24 GB) to eliminate such latency."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Model Details EFA-Routing. In Step 1, each of the two vision encoders uses 64 aggregation tokens, thereby reducing the number of visual tokens to 25% of the original. In addition, the scale and shift vectors for FiLM, γi and βi, are derived from linear transformation of the text embedding. In Step 2, two-layer MLP is applied to the text embedding to produce routing weights for the two vision encoders. LFP-Routing. In this module, we employ shifted cosine schedule [74] to control the proportion of visual tokens retained at each layer. The formulation is as follows: βl = 1 2 cos πl + η, = 1, 2, , (21) where denotes the total number of layers in the LLM, which is = 32 for CogVLA. The constant η is shift factor that vertically adjusts the cosine decay curve, providing flexible mechanism to control the overall computational cost of the model. In our implementation, η is set to 0.5. Specifically, we apply clamp operation to constrain βl within the range [0.05, 0.85]. As result, LFP-Routing achieves approximately 50% token pruning rate. In addition, the instruction-conditioned scaling and shifting functions γLLM() and βLLM() in LFPRouting are both implemented using two-layer MLPs, with hidden layer dimension of 2048, resulting in parameter count almost identical to that of direct linear layer. A.2 Training Details LIBERO Training Setup. We adopt OpenVLA [30] as the backbone model and set the action chunk size to = 8. Fine-tuning is performed using Low-Rank Adaptation (LoRA) with rank of 32 and an α value of 64. The model is trained for 60K steps with batch size of 64 and an initial learning rate of 5e-4. Checkpoints are evaluated every 10K steps, and the best-performing checkpoint is selected for reporting. 11 Real-World Training Setup. For the real-world experiments, we set the chunk size to = 25 and fine-tune OpenVLA using LoRA with rank of 32 and an alpha value of 64. The model is trained with batch size of 32 for total of 80K steps. The initial learning rate was set to 5e-4, which is reduced to 5e-5 after 50K steps. Starting from step 60K, we evaluate checkpoints every 10K steps and report the best-performing checkpoint."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Simulation Benchmark We evaluate CogVLA on the LIBERO simulation benchmark [41], standardized suite of languageconditioned robotic manipulation tasks. Unlike earlier benchmarks such as RLBench [27], LIBERO features more complex and diverse instructions, averaging 10.48 words per command compared to only 3.34 in RLBench. This makes it more suitable testbed for assessing the models capacity in language grounding and multimodal reasoning. LIBERO comprises four task suitesSpatial, Object, Goal, and Longeach containing 10 tasks with 50 human-teleoperated demonstrations. These suites are designed to probe distinct reasoning capabilities: LIBERO-Spatial evaluates spatial reasoning capabilities by presenting identical objects arranged in different spatial configurations. The agent must interpret spatial relations (e.g., left/right, front/behind) described in the instruction to complete the task correctly. LIBERO-Object measures the models ability to generalize across object categories. While spatial layouts remain fixed, the manipulated objects vary in type, shape, or color, requiring the agent to ground object-referential language and adapt its actions accordingly. LIBERO-Goal tests task-oriented comprehension by altering the goal specification while keeping object types and spatial layouts constant. The agent must disambiguate subtle differences in instruction semantics to execute distinct manipulation outcomes. LIBERO-Long challenges the agent with multi-step, long-horizon tasks involving diverse objects and environments. Success requires not only grounded perception and instruction following, but also sequential planning. CogVLA is trained and evaluated under the same setting as OpenVLA [29] to ensure comparability. We report results on all four suites to validate the models generalization, efficiency, and semantic grounding capabilities. B.2 Real-World Setup We deploy CogVLA on Cobot Agilex ALOHA [19] manipulation platform, to validate its real-world applicability. The real-world evaluation consists of five diverse tasks involving both single-arm and coordinated dual-arm manipulation. To assess robustness and generalization, we introduce moderate data augmentation by varying object attributes (e.g., size, color) and rearranging spatial layouts. We collect real-world training data for the Cobot Agilex ALOHA robot via human teleoperation. For Tasks 15, we gather 45, 45, 30, 30, and 45 expert demonstrations, respectively. We report the results of Tasks 13 in the main paper, and provide additional results for Tasks 45 in this appendix.The instructions and descriptions for Tasks 15 are provided below: Task 1: Put the cube into the plate, and then put the toy into the bowl. two-step pick-and-place task involving object category understanding and temporal sequencing. This is dual-arm task consisting of two sequential subtasks: 1) Put the cube into the plate with the left arm, and 1) Put the toy into the bowl with the right arm. Task success is achieved only when both subtasks are completed successfully. We report success rates for each subtask and the overall task. Task 2: Open the drawer, place the toy into the drawer, and then close it. composite task requiring interaction with articulated objects and multi-stage execution. This is dual-arm task consisting of three sequential subtasks: 1) Open the drawer with the left arm, 2) Place the toy into the drawer with the right arm, and 3) Close the drawer with the left arm. Task success requires all three subtasks to be completed. We report success rates for each subtask and the overall task. 12 Table 7: Multi-seed evaluation results in simulation. Task success rates (SR) are compared across four task categories on the LIBERO benchmark. denotes our reproduced results. CogVLA demonstrates strong and consistent performance. Method Spatial SR Object SR Goal SR Long SR Average SR RK OpenVLA [CoRL24] [29] 84.7 0.9 88.4 0.8 79.2 1.0 53.7 1.3 76.5 0.6 SpatialVLA [RSS25] [51] 88.2 0.5 89.9 0.7 78.6 0.6 55.5 1.0 78.1 0.7 STAR [ICML25] [22] 95.5 0.6 98.3 0.2 95.0 0.7 88.5 0.3 94.3 0.1 CoT-VLA [CVPR25] [78] 87.5 1.4 91.6 0.5 87.6 0.6 69.0 0.8 83.9 0.6 97.4 0.4 CogVLA 98.5 0.5 96.5 0.6 98.8 0.4 95.2 1.1 5 4 2 3 Task 3: Fold the T-shirt. soft-body manipulation task that evaluates the systems ability to handle deformable objects. This is dual-arm task consisting of three sequential folding steps. Task success is determined by the successful execution of all three steps. We report intermediate success rates for each step and the overall task performance. Task 4: Pick the red cube into the plate, and then pick the big cube into the bowl. multi-attribute grounding task requiring comprehension of both color and size references. This is dual-arm task consisting of two sequential subtasks: 1) Pick the red cube into the plate with the left arm, and 2) Pick the big cube into the bowl with the right arm. Task success is achieved only when both subtasks are completed. We report success rates for each subtask and the overall task. Task 5: Pick the left cube into the plate. spatial reasoning task focusing on relative positioning and egocentric understanding. This is single-arm task consisting of one pick-and-place action. We report the final task success rate."
        },
        {
            "title": "C Supplementary Quantitative Analysis",
            "content": "C.1 Multi-Seed Evaluation To evaluate the statistical robustness and consistency of CogVLAs performance, we conduct multiseed evaluations on the LIBERO benchmark. For each of the four task suites (Spatial, Object, Goal, and Long), we run experiments using three independent random seeds and report the mean success rate along with the standard deviation. As shown in Tab. 7, CogVLA exhibits consistently high performance across different seeds, with standard deviations ranging from 0.2% to 0.6%. This indicates stable learning behavior and further validates the strong generalization capability of CogVLAs three-stage instruction-driven architecture across diverse task types. C.2 Extended Real-World Task Results In addition to the results reported in the main paper, we present the performance of CogVLA on Tasks 4 and 5, as shown in Tab. 8. Task 4 (Pick the red cube into the plate, and then pick the big cube into the bowl) evaluates the models ability to ground multi-attribute language and execute sequential actions. CogVLA achieves the highest success rates across both subtasks and the overall task, demonstrating strong compositional understanding of attribute references such as color and size. Task 5 (Pick the left cube into the plate) focuses on egocentric spatial reasoning, requiring precise interpretation of relative spatial references from the agents visual perspective. CogVLA maintains high success rate in this setting, indicating robust grounding of spatial concepts. 13 Table 8: Extended real-world results on Tasks 45. Performance comparison on the Cobot Agilex ALOHA tasks. indicates our reproduced results. Method Task 4 Red CubePlate +Big CubeBowl PD-VLA [63] OpenVLA-OFT [28] CogVLA 7/10 7/10 8/10 5/10 6/10 7/10 Task 5 Left CubePlate Average SR 6/10 6/10 8/10 60.0% 63.3% 76.7% These results further validate CogVLAs ability to generalize to real-world tasks that demand finegrained language grounding and spatial understanding. C.3 Extended Ablation Studies We extend the sparsification analysis by evaluating additional Stage 1/Stage 2 configurations: 22 and 44, while keeping the total sparsification ratio fixed at 4 and 16, respectively. These configurations are compared alongside the baseline 8 setting with different asymmetric allocations (e.g., 24 and 42), allowing us to systematically assess how the distribution of sparsity across stages impacts downstream performance. As shown in Tab.3, the 22 setting provides favorable trade-off between performance and computational efficiency. In contrast, the 44 setting leads to slight degradation in performance, suggesting that excessive sparsification across both stages may hinder the preservation of task-relevant information. Table 9: Supplementary ablation on sparsification ratio allocation. Spf.Ratio denotes the sparsification ratio, which can be adjusted based on the performanceefficiency tradeoff. CogVLA achieves better performance when relatively higher sparsification ratio is allocated to Stage 1 compared to Stage 2. Stage 1 Stage 2 Spf.Ratio Spatial SR FLOPs 2 4 2 4 Interestingly, the asymmetric configurations, particularly the 42 setup, outperform their symmetric counterparts, achieving the highest spatial success rate of 98.6. This highlights the advantage of applying more aggressive token reduction in Stage 1 (EFA-Routing), where redundant visual tokens can be effectively compressed via instruction-guided aggregation. Subsequently, Stage 2 (LFP-Routing) performs finer-grained token pruning in context-aware manner within the language model, allowing for better preservation of task-relevant information. 96.4 (-2.2) 93.2 (-5.4) 94.6 (-4.0) 98.6 3.87 2.30 2.72 2.72 4 16 8 8 2 4 4 2 These findings support the core design principle of CogVLA: progressive sparsification with an asymmetric allocation tailored to the representational characteristics of each stage. By balancing early-stage compression and late-stage selectivity, the model achieves both computational efficiency and high task accuracy, reinforcing the importance of stage-aware sparsity scheduling in multimodal architectures."
        },
        {
            "title": "D Supplementary Qualitative Analysis",
            "content": "D.1 Additional Visualizations of Simulation and Real-World Results We present additional qualitative results from both simulation and real-world experiments to illustrate CogVLAs generalization and execution capabilities. As shown in Fig. 8, the model consistently completes multi-step tasks across diverse environments, object configurations, and instruction variants. In real-world tasks with varying instructions, CogVLA accurately interprets long-horizon commands and produces coherent action sequences. These examples further highlight the models ability to maintain cross-modal consistency and temporal reasoning, as well as its robustness in simulationto-reality transfer. Fig. 5 illustrates the real-world manipulation workflows for Tasks 1-5. For Task 1, we provide multi-view observations from the Front Camera, Left Wrist Camera, and Right Wrist 14 Figure 5: Real-world Manipulation Workflows and Visualizations for Tasks 15. Each task panel illustrates the initial setup and CogVLAs execution process based on the natural language instruction. For Task 1, multi-view observations from the Front Camera, Left Wrist Camera, and Right Wrist Camera are provided to capture dual-arm coordination. For Tasks 25, representative frames from the Front Camera highlight key manipulation stages. These visualizations support interpretation of task complexity and grounding behavior. Camera. For Tasks 2-5, only Front Camera observations are shown for clarity. In Fig. 6, we present third-person view demonstration of CogVLA performing manipulation task in the lab. The corresponding MP4 video file is provided in the supplementary materials. D.2 Instruction-to-Observation Attention Maps To gain deeper insights into how CogVLA aligns language instructions with visual observations, we visualize the attention maps generated by the cross-modal attention modules. As shown in Fig. 7, the attention weights highlight task-relevant regions in the input image. These visualizations demonstrate that CogVLAs instruction-aware routing mechanisms effectively guide the perception module to attend to semantically meaningful areas, enabling robust visual grounding even in cluttered or ambiguous scenes."
        },
        {
            "title": "E Discussion",
            "content": "E.1 Supplementary Details on the Motivation CogVLA is motivated by the need to improve both computational efficiency and cross-modal semantic alignment in instruction-conditioned robotic systems. Its architectural design is informed by cognitive science research on how humans process language, perceive their environment, and execute actions in coherent and goal-directed manner. Cognitive studies suggest that humans rely on structured inductive biasesoften termed \"intuitive theories\"to interpret the world, including intuitive physics, causality, and theory of mind [32, 66]. 15 Figure 6: Third-person visualization of CogVLA performing manipulation task. The corresponding video is provided in the supplementary materials. Gripper details are highlighted with red circles. While recent multimodal large language models exhibit partial competence in these areas, they often lack robustness in compositional reasoning and causally grounded behavior [54]. To address these limitations, CogVLA adopts biologically inspired architecture that reflects the division of functional roles observed in the human brain. Specifically, we draw connections between the models three routing modules and key components in human multimodal cognition: the Visual Attention System (VAS), the Supplementary Motor Area (SMA), and the Premotor Cortex (PMC). Visual Attention System (VAS) Encoder-FiLM. The human visual attention system selectively enhances perception of task-relevant features while suppressing distractors [14]. Top-down signals from frontal and parietal cortices bias visual processing toward objects or regions mentioned in language or necessary for action. This selective modulation improves efficiency and semantic grounding in complex scenes. In CogVLA, the Encoder-FiLM module mimics VAS by dynamically modulating visual encoder features conditioned on instructions, focusing perception on semantically relevant regions and reducing redundancy [49]. This allows the models perception to be grounded in context, much as the brains attention system tunes visual processing to relevant aspects of scene during coordinated vision-language tasks. Supplementary Motor Area (SMA) LLM-FiLM. The SMA plays key role in planning and sequencing actions, even in the absence of physical movement [55, 64]. It integrates multimodal information and high-level goals to shape future motor behavior, before engaging primary motor circuits. In CogVLA, the LLM-FiLM module can be seen as the intention planner of the model and serves similar function: it injects task-specific intent into the language model, pruning irrelevant visual-linguistic tokens and steering the model toward generating appropriate action plans. This enables more efficient and intention-aligned reasoning, analogous to how the SMA organizes abstract motor programs before execution. Premotor Cortex (PMC) V-L-A Coupled Attention. The premotor cortex is involved in translating perceptual cues into executable motor plans [20, 18]. It contains visuomotor neurons that represent both the perception of object affordances and the intended grasping actions, enabling visuomotor grounding. CogVLAs V-L-A Coupled Attention module reflects this mechanism by integrating visual, linguistic, and action representations through unified attention mechanism. This ensures that generated actions are causally and temporally coherent with respect to both the observed scene and the given instruction. By aligning its modular design with biologically plausible cognitive functions, CogVLA offers not only performance and efficiency gains, but also cognitively grounded pathway for improving generalization and interpretability in embodied multimodal agents. Figure 7: Attention maps between aggregation tokens and patch tokens in DINOv2 and SigLIP. We visualize the attention maps from 17 out of 64 aggregation tokens to the patch tokens of the observation, covering four sets of visualizations across two visual encoders and two camera views. The input language instruction is: Pick up the black bowl between the plate and the ramekin and place it on the plate. Both DINOv2 and SigLIP exhibit varying degrees of focused attention on patch tokens relevant to the instruction. E.2 Limitation and Future Work While CogVLA demonstrates strong performance across simulation and real-world tasks, several limitations remain. First, the current instruction-to-vision routing relies on predefined sparsity ratios and fixed token pruning schedules, which may not adapt optimally to varying instruction complexity or scene difficulty. Second, although the model generalizes well within the LIBERO and ALOHA settings, its performance under out-of-distribution instructions or unseen manipulation categories is yet to be thoroughly evaluated. In future work, we aim to explore adaptive sparsification mechanisms conditioned on task semantics and environmental uncertainty. Moreover, integrating lifelong learning or online adaptation strategies may further enhance CogVLAs robustness in open-world deployment scenarios. Lastly, extending the framework to support multimodal feedback (e.g., haptic or force sensing) could improve its applicability to fine-grained manipulation tasks. 17 Figure 8: Manipulation Workflows and Visualizations in the LIBERO Simulation Environment. We present the execution processes of CogVLA across LIBERO-Spatial, LIBERO-Object, LIBEROGoal, and LIBERO-Long, demonstrating its strong performance under diverse instructions and wide range of tasks. 18 E.3 Broader Impact and Potential Risk CogVLA advances the efficiency and interpretability of instruction-driven robotic manipulation, offering potential benefits in applications such as assistive robotics, household automation, and industrial assembly. Its biologically inspired sparsification and routing mechanisms reduce computation cost, making it more accessible for resource-constrained platforms. However, as with any vision-language-action system, risks include misinterpretation of ambiguous instructions, failure in unpredictable environments, and bias amplification from training data. If deployed in safety-critical settings without appropriate safeguards, such failures could lead to unintended behaviors or physical harm. We encourage the community to adopt robust evaluation protocols, prioritize transparency in model behavior, and consider human-in-the-loop designs to mitigate such risks. Broader societal considerationsincluding data diversity, accessibility, and responsible deploymentshould guide future development of systems built upon CogVLA."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arxiv 2023. arXiv preprint arXiv:2308.12966. [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [4] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. arXiv preprint arXiv:2403.01823, 2024. [5] Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, and Vikash Kumar. Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 47884795. IEEE, 2024. [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control, 2024. URL https://arxiv. org/abs/2410.24164. [7] Neil Bramley, Tobias Gerstenberg, Joshua Tenenbaum, and Todd Gureckis. Intuitive experimentation in the physical world. Cognitive psychology, 105:938, 2018. [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [9] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [10] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [11] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2654026550, 2024. [12] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. [13] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. 19 [14] Maurizio Corbetta and Gordon Shulman. Control of goal-directed and stimulus-driven attention in the brain. Nature reviews neuroscience, 3(3):201215, 2002. [15] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. [16] Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, and Subhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. arXiv preprint arXiv:2307.02628, 2023. [17] Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. [18] Leonardo Fogassi, Pier Francesco Ferrari, Benno Gesierich, Stefano Rozzi, Fabian Chersi, and Giacomo Rizzolatti. Parietal lobe: from action organization to intention understanding. Science, 308(5722):662667, 2005. [19] Zipeng Fu, Tony Z. Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. In Conference on Robot Learning (CoRL), 2024. [20] Vittorio Gallese, Luciano Fadiga, Leonardo Fogassi, and Giacomo Rizzolatti. Action recognition in the premotor cortex. Brain, 119(2):593609, 1996. [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [22] Li Hao, Lv Qi, Shao Rui, Deng Xiang, Li Yinchuan, HAO Jianye, and Nie Liqiang. Star: Learning diverse robot skill abstractions through rotation-augmented vector quantization. International Conference on Machine Learning (ICML), 2025. [23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [24] Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, et al. Dita: Scaling diffusion transformer for generalist vision-language-action policy. arXiv preprint arXiv:2503.19757, 2025. [25] Runhui Huang, Xinpeng Ding, Chunwei Wang, Jianhua Han, Yulong Liu, Hengshuang Zhao, Hang Xu, Lu Hou, Wei Zhang, and Xiaodan Liang. Hires-llava: Restoring fragmentation input in high-resolution large vision-language models. arXiv preprint arXiv:2407.08706, 2024. [26] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [27] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 2020. [28] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. [29] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [30] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In 8th Annual Conference on Robot Learning, 2024. [31] Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. [32] Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. 20 [33] Seungjae Lee, Yibin Wang, Haritheja Etukuru, Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Behavior generation with latent actions. arXiv preprint arXiv:2403.03181, 2024. [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [35] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. [36] Wei Li, Bing Hu, Rui Shao, Leyang Shen, and Liqiang Nie. Lion-fs: Fast & slow video-language thinker as online video assistant. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 32403251, 2025. [37] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. [38] Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, et al. Hamster: Hierarchical action models for open-world robot manipulation. arXiv preprint arXiv:2502.05485, 2025. [39] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2676326773, 2024. [40] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning, et al. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024. [41] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. [42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [44] Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, et al. Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model. arXiv preprint arXiv:2503.10631, 2025. [45] Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Lily Lee, Kaichen Zhou, Pengju An, Senqiao Yang, Renrui Zhang, Yandong Guo, and Shanghang Zhang. Robomamba: Multimodal state space model for efficient robot reasoning and manipulation. arXiv preprint arXiv:2406.04339, 2024. [46] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024. [47] Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, and Rongrong Ji. γ mod: Exploring mixture-of-depth adaptation for multimodal large language models. arXiv preprint arXiv:2410.13859, 2024. [48] Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, and Animesh Garg. Quest: Self-supervised skill abstractions for learning continuous control, 2024. [49] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [50] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. [51] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. 21 [52] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. [53] Luca Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, pages 111, 2025. [54] Luca Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, pages 111, 2025. [55] Michael Schwartze, Kathrin Rothermich, and Sonja Kotz. Functional dissociation of pre-sma and sma-proper in temporal processing. Neuroimage, 60(1):290298, 2012. [56] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. [57] Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, and Liqiang Nie. Large vlmbased vision-language-action models for robotic manipulation: survey. arXiv preprint arXiv:2508.13073, 2025. [58] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. Mome: Mixture of multimodal experts for generalist multimodal large language models. Advances in neural information processing systems, 37:4204842070, 2024. [59] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, et al. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. arXiv preprint arXiv:2502.19417, 2025. [60] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on robot learning, pages 894906. PMLR, 2022. [61] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785799. PMLR, 2023. [62] Kevin Smith, Peter Battaglia, and Edward Vul. Different physical intuitions exist between tasks, not domains. Computational Brain & Behavior, 1:101118, 2018. [63] Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, and Haoang Li. Accelerating vision-language-action model integrated with action chunking via parallel decoding. arXiv preprint arXiv:2503.02310, 2025. [64] Shoji Tanaka and Eiji Kirino. Dynamic reconfiguration of the supplementary motor area network during imagined music performance. Frontiers in human neuroscience, 11:606, 2017. [65] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. [66] Joshua Tenenbaum, Charles Kemp, Thomas Griffiths, and Noah Goodman. How to grow mind: Statistics, structure, and abstraction. science, 331(6022):12791285, 2011. [67] Sicheng Wang, Sheng Liu, Weiheng Wang, Jianhua Shan, and Bin Fang. Robobert: An end-to-end multimodal robotic manipulation model. arXiv preprint arXiv:2502.07837, 2025. [68] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025. [69] Rosa Wolf, Yitian Shi, Sheng Liu, and Rania Rayyes. Diffusion models for robotic manipulation: survey. arXiv preprint arXiv:2504.08438, 2025. [70] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. arXiv preprint arXiv:2312.13139, 2023. [71] Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, and Mike Zheng Shou. Videollm-mod: Efficient video-language streaming with mixtureof-depths vision computation. Advances in Neural Information Processing Systems, 37:109922109947, 2024. 22 [72] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 1304013051, 2024. [73] Yang Yue, Yulin Wang, Bingyi Kang, Yizeng Han, Shenzhi Wang, Shiji Song, Jiashi Feng, and Gao Huang. Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution. Advances in Neural Information Processing Systems, 37:5661956643, 2024. [74] Jun Zhang, Desen Meng, Ji Qi, Zhenpeng Huang, Tao Wu, and Limin Wang. p-mod: Building mixture-ofdepths mllms via progressive ratio decay. arXiv preprint arXiv:2412.04449, 2024. [75] Renshan Zhang, Rui Shao, Gongwei Chen, Miao Zhang, Kaiwen Zhou, Weili Guan, and Liqiang Nie. Falcon: Resolving visual redundancy and fragmentation in high-resolution multimodal large language models via visual registers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2025. [76] Rongyu Zhang, Menghang Dong, Yuan Zhang, Liang Heng, Xiaowei Chi, Gaole Dai, Li Du, Dan Wang, Yuan Du, and Shanghang Zhang. Mole-vla: Dynamic layer-skipping vision language action model via mixture-of-layers for efficient robot manipulation. arXiv preprint arXiv:2503.20384, 2025. [77] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. [78] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. arXiv preprint arXiv:2503.22020, 2025. [79] Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, and Yuanpei Chen. Dexgraspvla: vision-language-action framework towards general dexterous grasping, 2025. [80] Zhiyuan Zhou, Pranav Atreya, You Liang Tan, Karl Pertsch, and Sergey Levine. Autoeval: Autonomous evaluation of generalist robot manipulation policies in the real world. arXiv preprint arXiv:2503.24278, 2025. [81] Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, and Feifei Feng. Chatvla: Unified multimodal understanding and robot control with vision-language-action model, 2025."
        }
    ],
    "affiliations": [
        "School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen"
    ]
}