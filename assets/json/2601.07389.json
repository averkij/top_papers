{
    "paper_title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
    "authors": [
        "Xueyan Niu",
        "Bo Bai",
        "Wei Han",
        "Weixi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 9 8 3 7 0 . 1 0 6 2 : r On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training Xueyan Niu Theory Laboratory Central Research Institute, 2012 Laboratories Huawei Technologies Co., Ltd. Bo Bai Wei Han Weixi Zhang niuxueyan3@huawei.com baibo8@huawei.com harvey.hanwei@huawei.com zhangweixi1@huawei.com Abstract Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training pipeline."
        },
        {
            "title": "1 Introduction",
            "content": "The capacity for reasoning and general tasks have been greatly improved in contemporary Large Language Models (LLMs) thanks to post-training techniques such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Training LLMs typically comprises two stages, self-supervised pretraining and post-training, as illustrated in Figure 1. Testtime strategies can also be applied to improve the performance of post-trained models, but they do not modify the parameters of the post-trained model. During the pretraining stage, the model acquires general language patterns, structure, grammar, factual knowledge, and reasoning abilities by processing vast amounts of textual data. This stage of LLM training demands substantial computational resources. In addition, it involves extensive data cleaning to ensure that the model learns effectively and safely. Post-training often involves combination of SFT and RL. SFT is process that teaches the model how to respond to user prompts by providing task-specific input-output pairs, while the RL stage encourages certain patterns of text that have been positively reinforced by humans or rulebased verifiers. SFT has been shown to be prone to memorization, while RL is related to generalization (Chu et al., 2025; Huan et al., 2025). In this work, we study the synergy of SFT and RL in the post-training pipeline, as shown in Figure 1, where pretrained models are further adapted with SFT and RL. Modern reasoning models are built with alternating SFT and RL in practice. For example, DeepSeek-R1-Zero (Guo et al., 2025) is developed from DeepSeek-V3-Base using pure 1 Figure 1: Training pipeline for modern LLMs. This work focuses on two post-training methods, Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), that refine pretrained base model after its initial pretraining phase. RL and has demonstrated remarkable reasoning capabilities. To further improve performance, DeepSeek-R1 (Guo et al., 2025) is post-trained from the DeepSeek-V3-Base model by alternating SFT and RL twice. The more recent post-training recipe (Wang et al., 2025) applies cascaded, domain-wise RL on top of broad multi-stage SFT. Empirical evidence such as (Liu et al., 2025) suggests that stronger SFT model consistently leads to better final performance after large-scale RL training. Notably, models often experience catastrophic forgetting during the transition from SFT to RL (Chen et al., 2025b). Liu et al. applied RL to series of different SFT models and find that RL unlocks new abilities that the starting SFT models do not possess. However, Yan et al. showed that on-policy RL amplifies existing behaviors instead of introducing new capacities. In (Chen et al., 2025a), the authors proposed the synergy dilemma for large vision-language models by comparing three post-training strategies: two-stage SFT & RL, interleaved SFT & RL, and progressive SFT & RL. Their results show no synergy between SFT and RL in multi-modal reasoning. He et al. used single RL stage to achieve state-of-the-art performance on two distilled 1.5B reasoning models. The diversity of techniques and their conflicting empirical outcomes raise two central questions: Does RL improve on prior SFT, and does SFT improve on prior RL? Can the two post-training stages therefore be decoupled? In this paper, we approach these questions by analyzing two post-training pipelines: (a) SFT-then-RL, and (b) RL-then-SFT, as illustrated in Figure 2, as any cascade of the two strategies can be decomposed into these two elementary schemes. From the perspective of the SFT loss and the RL reward, we show that these two stages cannot be decoupled in terms of these objectives. In particular, our theoretical contributions are as follows. 1. SFT-then-RL coupling: In Theorem 3.1, we prove that when transitioning to RL training from SFT, the model performance in terms of SFT loss inevitably drops; therefore, SFT and RL cannot be decoupled in the SFT-then-RL pipeline. 2. RL-then-SFT coupling: In Theorem 4.1, we prove that when transitioning to SFT from RL, the SFT step can create persistent performance gap that decreases achieved RL reward. Therefore, SFT and RL interact non-trivially in the RL-thenSFT pipeline. Empirically, we verify our Theorem 3.1 and Theorem 4.1 by conducting experiments on the Qwen3-0.6B model (Team, 2025). We post-train the model with the Corpus of Linguistic Figure 2: Any combination of SFT and RL in post-training reduces to the two canonical pipelines: (a) SFT-then-RL and (b) RL-then-SFT. Acceptability (CoLA) dataset (Warstadt et al., 2019) using both SFT-then-RL and RLthen-SFT pipelines. Results show that RL diminishes SFT memory, as reflected by increased cross-entropy loss, and that RL becomes sensitive to further SFT, as shown by reward degradation. These paired changes indicate that the two training stages remain coupled and cannot be separated without loss of prior performance. Consequently, practitioners should treat SFT and RL as single joint optimization problem rather than as separable blocks."
        },
        {
            "title": "1.1 Notation",
            "content": "Let (X , F, µ) be measurable space with fixed dominating measure µ. We use lower case to denote the density of the corresponding probability measure with respect to µ. model corresponds to the probability distribution pθ(x) parametrized by θ Θ. xl+m := (xl, xl+1, . . . , xl+k) denotes consecutive subsequence of tokens."
        },
        {
            "title": "1.2.1 Next-token-prediction Loss",
            "content": "In general, foundation model is pretrained with an autoregressive objective, followed by an SFT stage using labeled data. An autoregressive language model, parameterized by θ Θ, is (discrete) distribution pθ over such that pθ(x) = ˆp(EOSx) (cid:89) i=1 ˆp(xix<i) (1) for = (x1, x2, . . . , xL) and = x, where the conditional distribution ˆp(xx) is sequence model for and . During decoding, the next token is sampled according to sampler q(pθ) to generate sequences at the decoder that approximate the distribution of the language model. The model recursively generates each token until EOS to form complete string. Training is often conducted over large corpus using the next-token-prediction task by predicting the next token(s) xi given text chunk x<i. The objective is to minimize the 3 negative log-likelihood L(θ) := 1 x (cid:88) (cid:88) xD i=1 [ log pθ(xi x<i)] . (2) 1.2.2 TV and KL Inequalities Let pX and pY be two probability distributions defined on the same σ-algebra (X , F), the TV distance is defined as dTV(pX , pY ) = sup SX pX (S) pY (S), which is equivalent to the L1 norm when the alphabets are finite, i.e., when pX and pY are probability mass functions, dTV(pX , pY ) = 1 2 pX pY 1 = 1 2 (cid:88) xX pX (x) pY (x). As metric on the space of probability measures, dTV satisfies the triangle inequality. dTV(pX , pZ) dTV(pX , pY ) + dTV(pY , pZ). (3) (4) notable bound relates the expected values of bounded functions to the TV distance (see, e.g., Niu et al. (2025)): EpX [f (X)] EpY [f (X)] sup f (x) dTV(pX , pY ). (5) When is finite, the KL divergence is defined as DKL(pX pY ) = (cid:88) xX pX (x) log pX (x) pY (x) . fundamental relationship between the KL divergence and the TV distance, originally due to Pinsker, states: dTV(PX , PY ) (cid:114) 1 DKL(PX , PY ). (6)"
        },
        {
            "title": "2 Problem Setup",
            "content": "Let θ denote the parameters of pretrained model pθ. We consider two post-training strategies, SFT and RL, on performance evidenced by training loss and reward. We investigate the commutativity of the two sequential post-training operations, SFT-then-RL and RL-then-SFT, wherein SFT either precedes or follows RL, as illustrated in Figure 2. 4 2.1 Supervised Fine-Tuning In the SFT stage, the pretrained language model pθ is trained with next-token prediction loss using high-quality task-specific data to adapt to task-specific knowledge. Typically, SFT data consist of pairs of input prompts and desirable outputs DSFT = {(xi, yi)}nSFT i=1 pDSFT. The weights θ are updated to minimize the negative log-likelihood for each token of the anticipated output, which is equivalent to the cross-entropy loss LSFT(pθ) := (cid:88) (cid:88) (x,y)DSFT j=1 log (pθ(yj x, y<j)) (7) such that θSFT = arg minθ LSFT(pθ). The resulting model pθSFT(yx) generates given prompt x. As the paradigm trains model to produce input-output mappings according to DSFT, it works well on in-distribution tasks, but may fail to generalize to out-of-distribution queries. We give the following lemma, which states that the SFT loss can be equivalently expressed using chunks and y. Lemma 1 The autoregressive training loss (7) has the equivalent expression LSFT(θ) := (cid:88) (x,y)DSFT"
        },
        {
            "title": "Proof",
            "content": "log (pθ(y x)) = ExpDSFT (x),ypDSFT (x)[ log pθ(y x)] LSFT(θ) = (cid:88) (x,y)DSFT = = (cid:88) (x,y)DSFT (cid:88) (x,y)DSFT log 1 pθ(x) log 1 pθ(x) (cid:89) j=1 (cid:89) j=1 pθ(x)pθ(yj x, y<j) pθ(yj x, y<j) (cid:89) i=1 pθ(xi x<i) log(pθ(y x))"
        },
        {
            "title": "2.2 Reinforcement Learning",
            "content": "To align models with human preferences, the Reinforcement Learning from Human Feedback (RLHF) pipeline (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022) trains reward model using preference data to encode desirable traits, then aligns the language model using policy gradient methods such as proximal policy optimization (PPO) (Schulman et al., 2017) and value-model-based RL such as PPO and variants DAPO (Yu et al., 2025), VAPO (Yue et al., 2025) to maximize reward. Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2025; Guo et al., 2025) is adopted when clear reward 5 function is possible while label data may be noisy or expensive. Instead of relying on humanlabeled data, direct binary feedback (true/false) can be obtained from formal verification tool, such as symbolic verifiers and rule-based tools. Using standard RL terminology, the language model pθ is referred to as policy, where the reference model πref (often from the SFT stage) serves as the initial policy preceding the RL stages (see (Razin et al., 2025)). Similarly to SFT, which trains the model on inputoutput pairs, RL aims at producing response that takes the highest value on reward function. To unify RLHF and RLVR, we model the RL data DRL = {xi, ypos i=1 pDRL(x, y) as prompt-response pairs, where given prompt x, ypos and yneg stand for positive (preferred/true) and negative responses annotated by humans or rule-based tools. An unknown ground truth reward }nRL , yneg rG(, ) : (cid:55) [R, R] is assumed to encode human preferences. When provided with an input x, the reward rG(x, y) evaluates the quality of the output y. The objective is to maximize the expected reward max θ Eypθ(x)[rG(x, y)]. When rG is not directly accessible, many methods, e.g., (Ziegler et al., 2019; Bai et al., 2022; Ouyang et al., 2022), train proxy reward model r(, ) through Bradley-Terry log-likelihood loss (Bradley and Terry, 1952) on RL data DRL. The optimization is further regularized with entropy so that the model does not drift too far from the original model. The policy pθ is updated using policy gradient methods (e.g. PPO, GRPO) to maximize the following RL objective JRL(θ) = ExpDRL ,ypθ(x)[r(x, y)] βExpDRL [DKL(pθ( x)πref ( x))], where the reward is bounded, r(x, y) Rmax, p(x) is fixed distribution over the prompts at test time, and θRL = arg maxθ JRL(θ). The closed-form solution is well known to be (e.g., see (Peng et al., 2019)) pθRL ="
        },
        {
            "title": "1\nZ(x)",
            "content": "πref (yx) exp (cid:18) r(x, y) β (cid:19) . (8) As β increases, the RL algorithm behaves more on-policy (Yan et al., 2025)."
        },
        {
            "title": "2.3 Assumptions",
            "content": "We make the following assumption regarding the SFT and RL data. In DSFT and DRL, each sample consists of pair (xi, yi), where is the input of the task, e.g., prompt, query, etc. This covers wide range of scenarios, such as question-answering, summarization, and code completion. In particular, we assume that the prompts are from the same distributions, so that the SFT data and RL data are from the same domains. Assumption 1 The sets of prompts {xi} in DSFT and DRL are sampled uniformly according to the prompt distribution q(x). 6 In particular, Akter et al. studied the allocation of reasoning data between pretraining and different post-training stages, revealing that pretraining benefits from diversity in reasoning patterns and that SFT is sensitive to data quality. We make the following bounded reward assumption during the RL stage, which is enforced in practice in standard algorithms such as PPO and GRPO. Assumption 2 Let r(, ) be the reward function during RL, then there exists Rmax > 0 such that r(x, y) Rmax (x, y) 2 (9)"
        },
        {
            "title": "3 SFT-then-RL Coupling",
            "content": "We first study the training loss of the SFT-then-RL pipeline illustrated in Figure 2(a). Let pθ denote pretrained base model upon which we perform SFT followed by RL. We denote the first SFT checkpoint as and the checkpoint obtained after the subsequent RL . process as θ(1) SFT θ(2) RL We use cross-entropy loss to indicate model performance, as in (Niu et al., 2024). The next theorem establishes non-decoupling property: even when the model has already converged under the first SFT (so the SFT objective exhibits negligible further loss reduction), the subsequent RL phase can still impair the SFT-induced performance. In particular, SFT and RL cannot be decoupled: any nontrivial improvement in the RL reward necessarily induces nontrivial degradation in the SFT loss. In particular, suppose that the first SFT stage achieves low loss (see Lemma 1), i.e., LSFT(p θ(1) SFT ) = Exq(x),ypDSFT (x)[ log θ(1) SFT (y x)] ϵSFT. Then, after the subsequent RL stage, we show that the SFT loss increases by positive amount: Exq(x),ypDSFT (x)[ log θ(2) RL (y x)] ϵSFT + δ(β), for some strictly positive δ(β) whenever the RL phase achieves nontrivial reward improvement. Consequently, the improvement attributable to RL is not orthogonal to the gains achieved during SFT: improving reward trades off against the SFT likelihood fit, so the two phases cannot be decoupled. Theorem 3.1 Suppose that the first SFT stage results in reference model that matches the SFT data: θ(1) SFT (y x) = pDSFT(y x) for q-a.e. x, (10) then the second RL phase degrades the SFT performance, i.e. Exq(x),ypDSFT (x)[ log θ(2) RL (y x)] = Exq(x),ypDSFT (x)[ log θ(1) SFT (y x)] + C1(β) (11) for some constant C1(β) 0. Proof The SFT-then-RL scheme admits πref = the maximizer is given by θ(1) SFT . θ(2) RL further maximizes JRL(θ), and (y x) = θ(2) RL 1 Zβ(x) θ(1) SFT exp( r(x, y) β ) (cid:2) exp(r(x, y)/β)(cid:3) according to Equation (8). Using (12), (x) with Zβ(x) = Eyp θ (1) SFT log θ(2) RL (y x) = log (y x) θ(1) SFT 1 β r(x, y) + log Zβ(x). Taking expectations gives LSFT(p θ(2) RL ) = Exq, ypDSFT (x)[ log (y x)] θ(1) SFT 1 β Exq, ypDSFT (x)[r(x, y)] + Exq[log Zβ(x)] = LSFT(p θ(1) SFT ) + C1(β), (12) (13) where C1(β) = Exq EypDSFT (x)[r(x, y)] Next, we show the nonnegativity of C1(β). For each fixed x, apply Jensens inequality log Zβ(x) 1 β . (cid:104) (cid:105) to the convex function exp(): EypDSFT (x) (cid:2)er(x,y)/β(cid:3) exp (cid:16) EypDSFT (x)[r(x, y)]/β (cid:17) . Taking logarithms on both sides and averaging over yields C1(β) 0."
        },
        {
            "title": "4 RL-then-SFT Coupling",
            "content": "θ(2) SFT In this section, we study the reward of the RL-then-SFT pipeline illustrated in Figure 2(b), where RL is performed on pretrained base model pθ followed by SFT. We denote the first and the checkpoint obtained after the additional SFT process as RL checkpoint as . We show the irreversibility of the RL and SFT processes. θ(1) RL We first give weak version of the result. Suppose that SFT starting from an RL policy is small update (e.g., early stopping, regularization, or limited steps). Under this condition, SFT cannot increase the RL reward by more than constant controlled by the distribution shift budget > 0. Proposition 1 Assume that the SFT update does not move too far from the RL policy in average conditional KL: (cid:104) Exq"
        },
        {
            "title": "DKL",
            "content": "(cid:16) θ(2) SFT ( x)p θ(1) RL (cid:17)(cid:105) ( x) B. Then, Exq,yp θ (2) SFT (x)[r(x, y)] Exq,yp θ (1) RL (x)[r(x, y)] + Rmax 2B. 8 (14) (15) Proof Fix any x. Let p2() = ( x) and p1() = y. Since r(x, y) Rmax, applying Equation (5) with (y) = r(x, y)/Rmax, the difference in conditional expected reward is controlled by total variation: ( x) be two distributions over θ(2) SFT θ(1) RL (cid:12) Eyp2[r(x, y)] Eyp1[r(x, y)] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2Rmax dTV(p2, p1), Next, applying Equation (6) yields, for each x, Eyp2[r(x, y)] Eyp1[r(x, y)] + Rmax (cid:112)2DKL(p2p1). Now, taking expectation over and applying Jensens inequality yields Exq (cid:2)(cid:112)DKL(p2p1)(cid:3) (cid:113) Exq (cid:2)DKL(p2p1)(cid:3) B. Substituting into (17) and averaging over gives Equation (15). (16) (17) Now, under additional assumptions on the optimality of the first RL checkpoint and bounded shift, we can give stronger version. The next theorem establishes nondecoupling property: if the model has already converged under the first RL, so that the RL objective exhibits negligible further reward gain, then the subsequent SFT phase suffers measurable drop in reward relative to RL-from-scratch. Consequently, the SFT step cannot be decoupled from the earlier RL optimization. In particular, define, for any conditional policy π( x), the expected reward functional J(π) Exq,yπ(x)[r(x, y)]. (18) Then, after the subsequent SFT stage, we would like to show J(p ). Hence, RL followed by SFT hurts RL performance, and the two stages cannot be decoupled without incurring reward deficit. ) < J(p θ(2) SFT θ(1) RL For this, we assume that the SFT change is bounded and that RL results in maximizer with quantitative curvature, so any deviation decreases the reward by at least some amount. Assumption 3 There exist 0 < < such that Exq (cid:104) DKL (cid:0)p θ(2) SFT ( x)p θ(1) RL ( x)(cid:1)(cid:105) A. (19) Theorem 4.1 Under Assumptions 3, the second SFT phase degrades the RL reward obtained in the first RL phase: for some constant C2 > 0. J(p θ(2) SFT ) J(p θ(1) RL ) Proof By Assumption 3, is within region near the local maximizer θ(2) SFT bounded by A. Therefore, λ(B) > 0 such that for every policy π satisfying (20) of J(π) θ(1) RL Exq (cid:104) DKL (cid:0)π( x)p θ(1) RL ( x)(cid:1)(cid:105) A, 9 the following KL-growth condition around θ(1) RL ( x) holds: J(π) J(p θ(1) RL ) λ(B)Exq (cid:104) DKL (cid:0)π( x)p ( x)(cid:1)(cid:105) . θ(1) RL Letting π = θ(2) SFT yields J(p θ(2) SFT ) J(p θ(1) RL ) λ(B)Exq (cid:104) DKL (cid:0)p θ(2) SFT (x)p θ(1) RL (x)(cid:1)(cid:105) (19) J(p θ(1) RL ) aλ(B), with C2 = aλ(B) > 0."
        },
        {
            "title": "5 Empirical Results",
            "content": "We conduct experiments to verify the non-decoupling properties using the Qwen3-0.6B (Team, 2025) model. We use the Corpus of Linguistic Acceptability (CoLA) dataset (Warstadt et al., 2019), which consists of 10,657 English sentences labeled grammatical or ungrammatical from published linguistics literature. We also prepare CoLA-style SFT dataset with instruction as the prompt key and output as the response key following the original train/test split. The task we consider is sentence acceptability classification task in which the model is prompted to judge the grammatical acceptability of sentence. We implement both SFT and RL with the VeRL framework (Sheng et al., 2025), running GRPO for RL with simple reward function that returns +1 when the decoded answer matches the ground-truth label and 1 otherwise."
        },
        {
            "title": "5.1 SFT-then-RL",
            "content": "In this experiment, we first perform SFT on the Qwen3-0.6B base model for 2 complete epochs with the CoLA-style SFT dataset. The resulting checkpoint then serves as the initialization for reinforcement-learning phase implemented with Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Figure 3a records the cross-entropy loss on the SFT test set throughout the combined pipeline. When RL begins, the loss increases abruptly and eventually exceeds the value observed for the original base model, behavior consistent with the bound established in Theorem 3.1."
        },
        {
            "title": "5.2 RL-then-SFT",
            "content": "In this experiment, we first perform RL with GRPO on the Qwen3-0.6B base model. Once the RL phase converges, we use that model as the initialization for an SFT phase for 2 complete epochs on the same CoLA-style SFT data. We evaluate the rewards using the same settings as the validation rollout from the RL stage (temperature = 0.6, topp = 0.95). The reward function is strict on the format in RL training. However, the SFT model can output labels in slightly different formats, 10 (a) (b) Figure 3: Experimental evidence of coupling. (a) SFT-then-RL: SFT loss climbs immediately once GRPO starts and eventually exceeds the base-model baseline. (b) RL-then-SFT: reward collapses as soon as SFT begins and falls below the basemodel level eventually. therefore assigning 1 even if the answer is semantically correct. To reduce the format sensitivity only in evaluation, we used robust evaluation which keeps the same scoring rule (+1 for correct label, 1 otherwise) while scanning the whole output for acceptable / unacceptable and use the last occurrence. We report the mean@1 reward (one output per prompt) for the entire RL-then-SFT pipeline on the RL test set in Figure 3b. The SFT steps overwrite the RL-tuned behavior and hurt label accuracy, consistent with Theorem 4.1, and sharp drop in reward can be observed. In particular, the base model (step 0), under robust evaluation, has mean@1 0.385, i.e. approximately 69.5% accuracy. The final SFT checkpoint has mean@1 0.343, i.e., approximately 67.2% accuracy, even lower than the base model."
        },
        {
            "title": "6 Conclusion",
            "content": "Our analysis of the two canonical post-training pipelines shows that supervised fine-tuning and reinforcement learning are inherently coupled: whichever order is adopted, the second stage degrades the performance achieved by the first. Theoretical guarantees suggest that this loss is consequence of optimizing mismatched objectives. Empirical results on the Qwen3-0.6B model corroborate these guarantees, revealing abrupt degradation in either cross-entropy or reward when the transition occurs. We hope that these theoretical insights will inform the development of new training strategies that better balance memorization and generalization to build more capable models."
        },
        {
            "title": "References",
            "content": "S. N. Akter, S. Prabhumoye, E. Nyberg, M. Patwary, M. Shoeybi, Y. Choi, and B. Catanzaro. Front-loading reasoning: The synergy between pretraining and post-training data. arXiv preprint arXiv:2510.03264, 2025. Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. J. Chen, T. Yu, H. Bai, L. Yao, J. Wu, K. Li, F. Mi, C. Tao, L. Zhu, M. Zhang, X. Li, L. Hou, L. Shang, and Q. Liu. The synergy dilemma of long-CoT SFT and RL: Investigating posttraining techniques for reasoning VLMs. arXiv preprint arXiv:2507.07562, 2025a. L. Chen, X. Han, L. Shen, J. Bai, and K.-F. Wong. Beyond two-stage training: Cooperative SFT and RL for LLM reasoning. arXiv preprint arXiv:2509.06948, 2025b. P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. T. Chu, Y. Zhai, J. Yang, S. Tong, S. Xie, D. Schuurmans, Q. V. Le, S. Levine, and Y. Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. B. He, Z. Qu, Z. Liu, Y. Chen, Y. Zuo, C. Qian, K. Zhang, W. Chen, C. Xiao, G. Cui, et al. JustRL: Scaling 1.5b LLM with simple RL recipe. arXiv preprint arXiv:2512.16649, 2025. M. Huan, Y. Li, T. Zheng, X. Xu, S. Kim, M. Du, R. Poovendran, G. Neubig, and X. Yue. Does math reasoning improve general LLM capabilities? understanding transferability of LLM reasoning. arXiv preprint arXiv:2507.00432, 2025. N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V. Miranda, A. Liu, N. Dziri, S. Lyu, Y. Gu, S. Malik, V. Graf, J. D. Hwang, J. Yang, R. L. Bras, O. Tafjord, C. Wilhelm, L. Soldaini, N. A. Smith, Y. Wang, P. Dasigi, and H. Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2025. Z. Liu, Z. Yang, Y. Chen, C. Lee, M. Shoeybi, B. Catanzaro, and W. Ping. AceReasonNemotron 1.1: Advancing math and code reasoning through SFT and RL synergy. arXiv preprint arXiv:2506.13284, 2025. 12 X. Niu, B. Bai, L. Deng, and W. Han. Beyond scaling laws: Understanding transformer performance with associative memory. arXiv preprint arXiv:2405.08707, 2024. X. Niu, B. Bai, N. Guo, W. Zhang, and W. Han. Ratedistortionperception trade-off in information theory, generative models, and intelligent communications. Entropy, 27(4): 373, 2025. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019. M. S. Pinsker. Information and information stability of random variables and processes. Holden-Day, 1964. N. Razin, Z. Wang, H. Strauss, S. Wei, J. D. Lee, and S. Arora. What makes reward model good teacher? an optimization perspective. arXiv preprint arXiv:2503.15477, 2025. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400711961. Q. Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. B. Wang, C. Lee, N. Lee, S.-C. Lin, W. Dai, Y. Chen, Y. Chen, Z. Yang, Z. Liu, M. Shoeybi, et al. Nemotron-cascade: Scaling cascaded reinforcement learning for general-purpose reasoning models. arXiv preprint arXiv:2512.13607, 2025. A. Warstadt, A. Singh, and S. R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625641, 2019. J. Yan, Y. Li, Z. Hu, Z. Wang, G. Cui, X. Qu, Y. Cheng, and Y. Zhang. Learning to reason under off-policy guidance. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, W. Dai, T. Fan, G. Liu, L. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 13 Y. Yue, Y. Yuan, Q. Yu, X. Zuo, R. Zhu, W. Xu, J. Chen, C. Wang, T. Fan, Z. Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        }
    ],
    "affiliations": [
        "Theory Laboratory Central Research Institute, 2012 Laboratories Huawei Technologies Co., Ltd."
    ]
}