{
    "paper_title": "SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree",
    "authors": [
        "Shuangrui Ding",
        "Rui Qian",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Yuwei Guo",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the \"error accumulation\" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner. In practice, we maintain a fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in J&F on long-term video object segmentation benchmarks such as SA-V and LVOS. The code is released at https://github.com/Mark12Ding/SAM2Long."
        },
        {
            "title": "Start",
            "content": "SAM2LONG: ENHANCING SAM 2 FOR LONG VIDEO SEGMENTATION WITH TRAINING-FREE MEMORY TREE Shuangrui Ding1 Rui Qian1 Yuhang Zang2 Yuhang Cao2 Yuwei Guo1 Xiaoyi Dong1,2 Pan Zhang2 Dahua Lin1 Jiaqi Wang 4 2 0 2 1 2 ] . [ 1 8 6 2 6 1 . 0 1 4 2 : r 1The Chinese University of Hong Kong 2Shanghai Artificial Intelligence Laboratory"
        },
        {
            "title": "ABSTRACT",
            "content": "The Segment Anything Model 2 (SAM 2) has emerged as powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the error accumulation problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in constrained tree search manner. In practice, we maintain fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Without introducing any additional parameters or further training, SAM2Long significantly and consistently outperforms SAM 2 on five VOS benchmarks. Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in &F on long-term video object segmentation benchmarks such as SA-V and LVOS. The code is released at https://github.com/Mark12Ding/SAM2Long."
        },
        {
            "title": "INTRODUCTION",
            "content": "The Segment Anything Model 2 (SAM 2) has gained significant attention as unified foundational model for promptable object segmentation in both images and videos. Notably, SAM 2 (Ravi et al., 2024) has achieved state-of-the-art performance across various video object segmentation tasks, significantly surpassing previous methods. Building upon the original SAM (Kirillov et al., 2023), SAM 2 incorporates memory module that enables it to generate masklet predictions using stored memory contexts from previously observed frames. This module allows SAM 2 to seamlessly extend SAM into the video domain, processing video frames sequentially, attending to the prior memories of the target object, and maintaining object coherence over time. While SAM 2 demonstrates strong performance in video segmentation, its greedy segmentation strategy struggles to handle complex video scenarios with frequent occlusions and object reappearance. In detail, SAM 2 confidently and accurately segments frames when clear visual cues are present. However, in scenarios with occlusions or reappearing objects, it can produce mask proposals that are highly variable and uncertain. Regardless of the frames complexity, uniform greedy selection strategy is applied to both scenarios: the mask with the highest predicted IoU is selected. Such greedy"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Comparison of occlusion handling and long-term compatibility between SAM 2 and SAM2Long. (a) When an occlusion occurs, SAM 2 may lose track or follow the wrong object, leading to accumulated errors. In contrast, SAM2Long utilizes memory tree search to recover when the object reappears. (b) The per-frame &F scores of the predicted masks are plotted at specific timestamps on the LVOS and SA-V datasets. SAM2Long demonstrates greater resilience to elapsed time compared to SAM 2, maintaining superior performance over longer periods. choice works well for the easy cases but raises the error potential for the challenging frames. Once an incorrect mask is selected into memory, it is uncorrectable and will mislead the segmentation of the subsequent frames. We show such an error accumulation problem in Figure 1 both qualitatively and quantitatively. The performance of SAM 2 progressively deteriorates as the propagation extends into the later temporal segment, highlighting its limitations in maintaining accurate tracking over time. To this end, we redesign the memory module of SAM 2 to enhance its long-term compatibility and robustness against occlusions and error propagation. Our improvement is completely free of additional training and does not introduce any external parameters, but simply unleashes the potential of SAM 2 itself. Our approach is motivated by the observation that the SAM 2 mask decoder generates multiple diverse masks, accompanied by predicted IoU scores and an occlusion score when handling challenging and ambiguous cases. However, SAM 2 only selects single mask as memory, sometimes disregarding the correct one. To address this, we aim to equip SAM 2 with multiple memory pathways, allowing various masks to be stored as memory at each time step, thereby improving predictions for subsequent frames. In particular, we introduce novel constrained tree memory structure, which maintains fixed number of memory pathways over time to explore multiple segmentation hypotheses with efficiently managed computational resources. At each time step, based on set of memory pathways, each with its own memory bank and cumulative score (accumulated logarithm of the predicted IoU scores across the pathway), we produce multiple candidate branches for the current frame. Then, among all the branches, we select the same fixed number of branches with higher cumulative scores and prune other branches, thereby constraining the trees growth. After processing the final frame, the pathway with the highest cumulative score is selected as the final segmentation result. Moreover, to prevent premature convergence on incorrect predictions, we select hypotheses with distinct predicted masks when their occlusion scores indicate uncertainty, in order to maintain diversity in the tree branches. This tree-like memory structure augments SAM 2s ability to effectively overcome error accumulation. Within each pathway, we construct an object-aware memory bank that selectively includes frames with confidently detected objects and high-quality segmentation masks, based on the predicted occlusion scores and IoU scores. Instead of simply storing the nearest frames as SAM 2 does, we filter out frames where the object may be occluded or poorly segmented. This ensures that the memory bank provides effective object cues for the current frames segmentation. Additionally, we modulate the memory attention calculation by weighting memory entries according to their occlusion scores, emphasizing more reliable entries during cross-attention. These strategies help SAM 2 focus on reliable object clues and improve segmentation accuracy with negligible computational overhead. As evidenced in Figure 1(a), our approach successfully resolves occlusions and re-tracks the recurring balloon, where SAM 2 fails."
        },
        {
            "title": "Preprint",
            "content": "We provide comprehensive evaluation demonstrating that SAM2Long consistently outperforms SAM 2 across six VOS benchmarks, particularly excelling in long-term and occlusion-heavy scenarios. For instance, on the challenging SA-V test set, SAM2Long-L improves the &F score by 5.3 points, and SAM2Long-S shows an impressive 4.7-point gain over the same size SAM 2 model on SA-V val set. Similar trends are observed on the LVOS validation set, where SAM2Long-S surpasses SAM 2-S by 3.5 points. These consistent improvements across different model sizes, including both SAM 2 and the more recent SAM 2.1 model weights, clearly demonstrate the effectiveness of our proposed method. Furthermore, as illustrated in Figure 1(b), the per-frame performance gap between SAM2Long and SAM 2 widens over time, showcasing SAM2Long excels in long-term tracking scenarios. With these results, we believe SAM2Long sets new standard for video object segmentation based on SAM 2 in complex, real-world applications, delivering superior performance without any additional training or external parameters."
        },
        {
            "title": "2.1 VIDEO OBJECT SEGMENTATION",
            "content": "Perceiving the environment in terms of objects is fundamental cognitive ability of humans. In computer vision, Video Object Segmentation (VOS) tasks aim to replicate this capability by requiring models to segment and track specified objects within video sequences. substantial amount of research has been conducted on video object segmentation in recent decades (Fan et al., 2019; Oh et al., 2019; Hu et al., 2018a; Oh et al., 2018; Perazzi et al., 2017; Wang et al., 2019; Hu et al., 2018b; Li & Loy, 2018; Bao et al., 2018; Zhang et al., 2019; Li et al., 2020; Johnander et al., 2019; Zhang et al., 2023; Ventura et al., 2019; Li et al., 2022; Wu et al., 2023; Wang et al., 2023; Qian et al., 2023; 2025; Ding et al., 2022; 2023b). There are two main protocols for evaluating VOS models (Pont-Tuset et al., 2017; Perazzi et al., 2016): semi-supervised and unsupervised video object segmentation. In semi-supervised VOS, the first-frame mask of the objects of interest is provided, and the model tracks these objects in subsequent frames. In unsupervised VOS, the model directly segments the most salient objects from the background without any reference. It is important to note that these protocols are defined in the inference phase, and VOS methods can leverage ground truth annotations during the training stage. In this paper, we explore SAM 2 (Ravi et al., 2024), for its application in semi-supervised VOS. We enhance the memory design of SAM 2, significantly improving mask propagation performance without requiring any additional training. 2.2 MEMORY-BASED VOS Video object segmentation remains an unsolved challenge due to the inherent complexity of video scenes. Objects in videos can undergo deformation (Tokmakov et al., 2023), exhibit dynamic motion (Brox & Malik, 2010), reappear over long durations (Hong et al., 2024; 2023), and experience occlusion (Ding et al., 2023a), among other challenges. To address the above challenges, adopting memory architecture to store the object information from past frames is indispensable for accurately tracking objects in video. Previous methods (Bhat et al., 2020; Caelles et al., 2017; Maninis et al., 2018; Robinson et al., 2020; Voigtlaender & Leibe, 2017) treat VOS as an online learning task, where networks are test-time tuned on the first-frame annotation. However, this approach was time-consuming due to test-time fine-tuning. Other techniques (Chen et al., 2018; Hu et al., 2018b; Voigtlaender et al., 2019; Yang et al., 2018; 2020; 2021b) use template matching, but they lack the capability of tracking under occlusion. More recent approaches have introduced efficient memory reading mechanisms, utilizing either pixel-level attention (Cheng et al., 2023; Zhou et al., 2024; Duke et al., 2021; Liang et al., 2020; Oh et al., 2018; Seong et al., 2020; Cheng & Schwing, 2022; Xie et al., 2021; Yang & Yang, 2022; Yang et al., 2021a) or object-level attention (Athar et al., 2023; 2022; Cheng et al., 2024). prominent example is XMem (Cheng & Schwing, 2022), which leverages hierarchical memory structure for pixel-level memory reading combined. Building on XMems framework, Cutie (Cheng et al., 2024) further improves segmentation accuracy by processing pixel features at the object level to better handle complex scenarios."
        },
        {
            "title": "Preprint",
            "content": "The latest SAM 2 (Ravi et al., 2024) incorporates simple memory module on top of the image-based SAM (Kirillov et al., 2023), enabling it to function for VOS tasks. However, by selecting only the temporally nearest frames as memory, SAM 2 struggles with challenging cases involving long-term reappearing objects and confusingly similar objects. we redesign SAM 2s memory to maintain multiple potential correct masks, making the model more object-aware and robust."
        },
        {
            "title": "2.3 SEGMENT ANYTHING MODEL",
            "content": "Segment Anything Model (SAM) (Kirillov et al., 2023) is recognized as milestone vision foundation model that can segment any object in an image using interactive prompts. Its impressive zero-shot transfer performance has shown great versatility in various vision tasks, including segmentation applications (Li et al., 2023; Ma et al., 2024; Xu et al., 2024), image editing (Gao et al., 2023) and object reconstruction (Lin et al., 2024). Building on SAM, SAM 2 (Ravi et al., 2024) extends its functionality to video segmentation through memory-based transformer architecture for real-time video processing. SAM 2s memory stores information about objects and past interactions, enabling it to generate segmentation masks across video frames more accurately and efficiently than previous methods. To further enhance SAM 2, we introduce constrained memory tree structure. This training-free design leverages the SAM 2s ability to generate multiple candidate mask proposals with predicted IoU and occlusion score, mitigating error accumulation during segmentation."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PRELIMINARY ON SAM 2 SAM 2 (Ravi et al., 2024) begins with an image encoder that encodes each input frame into embeddings. In contrast to SAM, where frame embeddings are fed directly into the mask decoder, SAM 2 incorporates memory module that conditions the current frames features on both previous and prompted frames. Specifically, for semi-supervised video object segmentation tasks, SAM 2 maintains memory bank at each time step 1: Mt = (cid:8)Mτ RKC(cid:9) τ , where is the number of memory tokens per frame, is the channel dimension, and is the set of frame indices included in the memory. In SAM 2, memory set stores up to of the most recent frames, along with the initial mask, using First-In-First-Out (FIFO) queue mechanism. Each memory entry consists of two components: (1) the spatial embedding fused with the predicted mask (generated by the memory encoder), and (2) the object-level pointer (generated by the mask decoder). After cross-attending to the memory, the current frames features integrate both fine-grained correspondences and object-level semantic information. The mask decoder, which is lightweight and retains the efficiency of SAM, then generates three predicted masks for the current frame. Each mask is accompanied by predicted Intersection over Union (IoU) score IoUt 0 and an output mask token. Additionally, the mask decoder predicts single occlusion score ot for the frame, where ot > 0 indicates object presence, ot < 0 indicates absence, and the absolute value ot depicts the models confidence. The mask with the highest predicted IoU score is selected as the final prediction, and its corresponding output token is transformed into the object pointer for use as the memory. 3.2 CONSTRAINED TREE MEMORY WITH UNCERTAINTY HANDLING To enhance SAM 2s robustness towards long-term and ambiguous cases, we propose constrained tree memory structure that enables the model to explore various object states over time with minimal computational overhead. We show the high-level pipeline in Figure 2. This tree-based approach maintains multiple plausible pathways and mitigates the effects of occlusions and erroneous predictions. 1In practice, SAM 2 stores more object pointers than spatial embeddings, as pointers are lighter. We assume equal numbers of both components solely for illustrative purposes, without altering the actual implementation."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: (a) The pipeline of constrained memory tree: At each time step t, we maintain multiple memory pathways, each containing memory bank and cumulative score Sp[t]. The input frame is processed through the mask decoder conditioned on the memory bank, generating three mask candidates for each pathway. The candidates with the highest updated cumulative scores Sp,k[t] are carried forward to the next time step. (b) Mask selection with uncertainty handling: When the maximum absolute occlusion score exceeds the threshold δconf (Certain), the high-scoring mask is selected. Otherwise (Uncertain), distinct mask candidates are picked to avoid incorrect convergence. Specifically, at each time step t, we maintain set of memory pathways, each with memory bank Mp and cumulative score Sp[t], representing possible segmentation hypothesis up to frame t. Conditioned on the memory bank of each pathway p, the SAM 2 decoder head generates three mask candidates along with their predicted IoU scores, denoted as IoUp,1 . This process expands the tree by branching each existing pathway into three new candidates. As result, there are total of 3P possible pathways at each time step. We then calculate the cumulative scores for each possible pathway by adding the logarithm of its IoU score to the pathways previous score: , and IoUp,3 , IoUp,2 t + ϵ), where ϵ is small constant to prevent the logarithm of zero. Sp,k[t] = Sp[t 1] + log(IoUp,k for = 1, 2, 3, However, continuously tripling the pathways would lead to unacceptable computational and memory costs. Therefore, to manage computational complexity and memory usage, we implement pruning strategy that selects the top pathways with the highest cumulative scores to carry forward to the next time step. This selection not only retains the most promising segmentation hypotheses but also constrains the tree-based memory, ensuring computational efficiency. Finally, we output the segmentation pathway with the highest cumulative score as the ultimate result. Compared to SAM 2, our approach introduces additional computation mainly by increasing the number of passes through the mask decoder and memory module. Notably, these components are lightweight relative to the image encoder. For instance, the image encoder of SAM 2-Large consists of 212M parameters while the total parameter of SAM 2-Large is 224M. Since we process the image encoder only once just as SAM 2 does, the introduction of memory tree adds negligible computational cost while significantly enhancing SAM 2s robustness against error-prone cases. Uncertainty Handling. Unfortunately, there are times when all pathways are uncertain. To prevent the model from improperly converging on incorrect predictions, we implement strategy to maintain diversity among the pathways by deliberately selecting distinct masks. That is, if the maximum absolute occlusion score across all pathways at time t, max({op p=1), is less than predefined uncertainty threshold δconf, we enforce the model to select mask candidates with unique IoU values. This is inspired by the observation that, within the same frame, different IoU scores typically correspond to distinct masks. In practice, we round each IoU score IoUp,k to two decimal places and only select those hypotheses with distinct rounded values. }P Overall, the integration of constrained tree memory with uncertainty handling offers balanced strategy that leverages multiple segmentation hypotheses to enhance robustness toward the long-term"
        },
        {
            "title": "Preprint",
            "content": "complex video and achieve more accurate and reliable segmentation performance by effectively mitigating error accumulation."
        },
        {
            "title": "3.3 OBJECT-AWARE MEMORY BANK CONSTRUCTION",
            "content": "In each memory pathway, we devise object-aware memory selection to retrieve frames with discriminative objects. Meanwhile, we modulate the memory attention calculation to further strengthen the models focus on the target objects. Memory Frame Selection. To construct memory bank that provides effective object cues, we selectively choose frames from previous time steps based on the predicted object presence and segmentation quality. Starting from the frame immediately before the current frame t, we iterate backward through the prior frames = {t 1, 2, . . . , 1} in sequence. For each frame i, we retrieve its predicted occlusion score oi and IoU score IoUi as reference. We include frame in the memory bank if it satisfies the following criteria: IoUi > δIoU and oi > 0, where δIoU is predefined IoU threshold. This ensures that only frames with confidently detected objects and reasonable segmentation masks contribute to the memory. We continue this process until we have selected up to frames. In contrast to SAM 2, which directly picks the nearest frames as the memory entries, this selection process effectively filters out frames where the object may be occluded, absent, or poorly segmented, thereby providing more robust object cues for the segmentation of the current frame. Memory Attention Modulation. To further emphasize more reliable memory entries during the cross-attention computation, we utilize the associated occlusion score ot to modulate the contribution of each memory entry. Assuming the memory set consists of frames plus the initial frame, totaling + 1 masks, we define set of standard weights std that are linearly spaced between lower bound wlow and an upper bound whigh: std = (cid:26) wlow + 1 (cid:27)N + (whigh wlow) . i=1 Next, we sort the occlusion scores in ascending order to obtain sorted indices = {Ii}N +1 that: i=1 such oI1 oI2 oIN +1. We then assign the standard weights to the memory entries based on these sorted indices: wIi = std , for = 1, 2, . . . , + 1. This assignment ensures that memory entries with higher occlusion scores, which indicate object presence with higher confidence, receive higher weights. Then, we linearly scale the original keys Mτ with their corresponding weights: (cid:102)Mτ = wτ Mτ , for τ I. Finally, the modulated memory keys (cid:102)Mt = {(cid:102)Mτ }τ are used in the memory modules crossattention mechanism to update the current frames features. By using the available occlusion scores as indicators, we effectively emphasize memory entries with more reliable object cues while introducing minimal computational overhead."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 DATASETS To evaluate our method, we select 6 standard VOS benchmarks and report the following metrics: (region similarity), (contour accuracy), and the combined &F. All evaluations are conducted in semi-supervised setting, where the first-frame mask is provided. The datasets used for testing are detailed as follows:"
        },
        {
            "title": "Preprint",
            "content": "SA-V (Ravi et al., 2024) is large-scale video segmentation dataset designed for promptable visual segmentation across diverse scenarios. It encompasses 50.9K video clips, aggregating to 642.6K masklets with 35.5M meticulously annotated masks. The dataset presents challenge with its inclusion of small, occluded, and reappearing objects throughout the videos. The dataset is divided into training, validation, and testing sets, with most videos allocated to the training set for robust model training. The validation set has 293 masklets across 155 videos for model tuning, while the testing set includes 278 masklets across 150 videos for comprehensive evaluation. LVOS v1 (Hong et al., 2023) is VOS benchmark for long-term video object segmentation in realistic scenarios. It comprises 720 video clips with 296,401 frames and 407,945 annotations, with an average video duration of over 60 seconds. LVOS introduces challenging elements such as long-term object reappearance and cross-temporal similar objects. In LVOS v1, the dataset includes 120 videos for training, 50 for validation, and 50 for testing. LVOS v2 (Hong et al., 2024) expends LVOS v1 and provides 420 videos for training, 140 for validation, and 160 for testing. This paper primarily utilizes v2, as it already includes the sequences present in v1. The dataset spans 44 categories, capturing typical everyday scenarios, with 12 of these categories deliberately left unseen to evaluate and better assess the generalization capabilities of VOS models. MOSE (Ding et al., 2023a) is challenging VOS dataset targeted on complex, real-world scenarios, featuring 2,149 video clips with 431,725 high-quality segmentation masks. These videos are split into 1,507 training videos, 311 validation videos, and 331 testing videos. VOST (Tokmakov et al., 2023) is semi-supervised video object segmentation benchmark that emphasizes complex object transformations. Unlike other datasets, VOST includes objects that are broken, torn, or reshaped, significantly altering their appearance. It comprises more than 700 high-resolution videos, captured in diverse settings, with an average duration of 21 seconds, all densely labeled with instance masks. PUMaVOS (Bekuzarov et al., 2023) is novel video dataset designed for benchmarking challenging segmentation tasks. It includes 24 video clips, each ranging from 13.5 to 60 seconds (28.7 seconds on average) at 480p resolution with varying aspect ratios. PUMaVOS focuses on difficult scenarios where annotation boundaries do not align with clear visual cues, such as half faces, necks, tattoos, and pimples, commonly encountered in video production. 4.2 MAIN RESULTS SAM2Long consistently improves SAM 2 over all model sizes and datasets. Table 1 presents an overall comparison between SAM 2 and SAM2Long across various model sizes on the SA-V validation and test sets, as well as the LVOS v2 validation set. In total, the table includes 8 model variations, covering SAM 2 and the latest SAM 2.1 across four model sizes. The average performance across 24 experiments yields &F score of 3.0. These results confirm that SAM2Long consistently outperforms the SAM 2 baseline by large margin. For instance, for SAM2Long-Large, achieves an improvement of 4.5 and 5.3 over SAM 2 on SA-V val and test sets. This pattern is also evident in the LVOS validation set, where SAM2Long delivers notable performance gains over SAM 2 for each model size. These results showcase the effectiveness of the training-free memory tree in long-time video segmentation scenarios. SAM2Long outperforms previous methods and excels in unseen categories. We also compare our proposed method, SAM2Long, with various state-of-the-art VOS methods on both the SA-V (Ravi et al., 2024) and LVOS (Hong et al., 2023; 2024) datasets, as shown in Table 2 and 3. Although SAM 2.1 already surpasses previous methods by large margin, SAM2.1Long pushes these limits even further. Specifically, our method achieves &F score of 81.1 on the SA-V validation set, 2.5-point improvement over SAM 2.1. For LVOS, SAM2.1Long attains &F scores of 83.4 and 85.9 on the v1 and v2 subsets, respectively, outperforming SAM 2.1 by 3.2 and 1.8 points. Notably, SAM2Long particularly excels in unseen categories, achieving and scores of 79.1 and 86.2. The significant improvements of 7.5 and 5.1 points over SAM 2 highlight its robust generalization capabilities. SAM2Long demonstrates versatility when handling videos with various challenges. In addition to the SA-V and LVOS datasets, we evaluate our proposed SAM2Long on other VOS benchmarks"
        },
        {
            "title": "Preprint",
            "content": "in Table 4. On the MOSE dataset (Ding et al., 2023a), which involves complex real-world scenes, SAM2.1Long achieves &F score of 75.2, surpassing SAM 2.1s score of 74.5. Given that the stronger SAM 2.1-L shows no improvement over the SAM 2-L model on the MOSE benchmark, our performance gain with SAM2.1Long is particularly notable. Similarly, on the VOST dataset (Tokmakov et al., 2023), which focuses on objects undergoing extreme transformations, SAM2.1Long shows improvement with &F score of 54.0, nearly 1-point gain over SAM2.1. On the PUMaVOS dataset (Bekuzarov et al., 2023), which challenges models with difficult visual cues, SAM2.1Long outperforms SAM 2.1 with score of 82.4 compared to 81.1, demonstrating its enhanced ability to handle subtle and ambiguous segmentation tasks. These results underscore that we have preserved the fundamental segmentation capabilities of SAM 2 while enhancing its long-term abilities, demonstrating the robustness and versatility of SAM2Long across range of VOS benchmarks. Table 1: Performance comparison on SA-V (Ravi et al., 2024) and LVOS v2 (Hong et al., 2024) datasets between SAM 2 and SAM2Long across all model sizes. We report the re-produced performance of SAM 2 using its open-source code and checkpoint. Method SAM2-T SAM2Long-T SAM2.1-T SAM2.1Long-T SAM2-S SAM2Long-S SAM2.1-S SAM2.1Long-S SAM2-B+ SAM2Long-B+ SAM2.1-B+ SAM2.1Long-B+ SAM2-L SAM2Long-L SAM2.1-L SAM2.1Long-L SA-V val SA-V test LVOS v2 val &F 73.5 77.0 (3.5) 75.1 78.9 (3.8) 73.0 77.7 (4.7) 76.9 79.6 (2.7) 75.4 78.4 (3.0) 78.0 80.5 (2.5) 76.3 80.8 (4.5) 78.6 81.1 (2.5) 70.1 73.2 71.6 75.2 69.7 73.9 73.5 75.9 71.9 74.7 74.6 76.8 73.0 77.1 75.1 77.5 76.9 80.7 78.6 82.7 76.3 81.5 80.3 83.3 78.8 82.1 81.5 84.2 79.5 84.5 82.0 84.7 &F 74.6 78.7 (4.1) 76.3 79.0 (2.7) 74.6 78.1 (3.5) 76.9 80.4 (3.5) 74.6 78.5 (3.9) 77.7 80.8 (3.1) 75.5 80.8 (5.3) 79.6 81.2 (1.6) 71.1 74.6 72.7 75.2 71.0 74.1 73.3 76.6 71.2 74.7 74.2 77.1 72.2 76.8 76.1 77. 78.0 82.7 79.8 82.9 78.1 82.0 80.5 84.1 78.1 82.2 81.2 84.5 78.9 84.7 83.2 84.9 &F 77.8 81.4 (3.6) 81.6 82.4 (0.8) 79.7 83.2 (3.5) 82.1 84.3 (2.2) 80.2 82.3 (2.1) 83.1 85.2 (2.1) 83.0 85.2 (2.2) 84.0 85.3 (1.3) 74.5 77.7 77.9 78.8 76.2 79.5 78.6 80.7 76.8 78.8 79.6 81.5 79.6 81.8 80.7 81.9 81.2 85.0 85.2 85.9 83.3 86.8 85.6 88.0 83.6 85.9 86.5 88.9 86.4 88.7 87.4 88.8 Table 2: Performance comparison with the-state-of-the-arts methods on SA-V dataset. Method STCN (Cheng et al., 2021) RDE (Li et al., 2022) SwinB-AOT (Yang et al., 2021a) SwinB-DeAOT (Yang & Yang, 2022) XMem (Cheng & Schwing, 2022) DEVA (Cheng et al., 2023) Cutie-base+ Cheng et al. (2024) SAM 2 (Ravi et al., 2024) SAM 2.1 (Ravi et al., 2024) SAM2Long (ours) SAM2.1Long (ours) SA-V val 57.4 48.4 46.4 56.6 56.3 51.5 58.3 72.9 75.1 74.7 77.5 &F 61.0 51.8 51.1 61.4 60.1 55.4 61.3 76.1 78.6 79.7 81.1 64.5 55.2 55.7 66.2 63.9 59.2 64.4 79.2 82.0 84.7 84.7 SA-V test 59.0 50.5 46.0 57.2 58.9 52.4 59.8 72.6 76.1 76.8 77.6 &F 62.5 53.9 50.3 61.8 62.3 56.2 62.8 76.0 79.6 80.8 81. 66.0 57.3 54.6 66.3 65.8 60.1 65.8 79.3 83.2 84.7 84.9 4.3 ABLATION STUDY We conduct series of ablation studies on the validation split of SA-V dataset and use SAM2-Large as default model size."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance comparison with state-of-the-art methods on validation set of LVOS dataset. Subscript and denote scores in seen and unseen categories. Unlike the results presented in Table 1, we use the evaluation code from the LVOS official repository. Method LWL (Bhat et al., 2020) CFBI (Yang et al., 2020) STCN (Cheng et al., 2021) RDE (Li et al., 2022) DeAOT (Yang et al., 2021a) XMem (Cheng & Schwing, 2022) DDMemory (Hong et al., 2023) SAM 2 (Ravi et al., 2024) SAM 2.1 (Ravi et al., 2024) SAM2Long (ours) SAM2.1Long (ours) LVOS v1 51.8 46.2 43.9 48.3 - 48.1 55.0 73.1 75.4 76.4 78.4 &F 56.4 51.5 48.9 53.7 - 52.9 60.7 77.9 80.2 81.3 83.4 60.9 56.7 54.0 59.2 - 57.7 66.3 82.7 84.9 86.2 88. &F 60.6 55.0 60.6 62.2 63.9 64.5 - 79.8 84.1 84.2 85.9 LVOS v2 Fs 64.3 59.2 64.0 64.1 69.0 69.1 - 86.6 87.4 89.2 88.6 Js 58.0 52.9 57.2 56.7 61.5 62.6 - 80.0 80.7 82.3 81.7 Ju 57.2 51.7 57.5 60.8 58.4 60.6 - 71.6 80.6 79.1 83.0 Fu 62.9 56.2 63.8 67.2 66.6 65.6 - 81.1 87.7 86.2 90.5 Table 4: The performance comparisons between SAM 2 and SAM2Long on other VOS benchmarks. All experiments use SAM2.1-Large model. Dataset MOSE (Ding et al., 2023a) VOST (Tokmakov et al., 2023) PUMaVOS (Bekuzarov et al., 2023) SAM 2.1 70.6 47.8 78.5 &F 74.5 53.1 81.1 78.4 58.3 83.7 SAM2.1Long 71.1 48.4 79. &F 75.2 54.0 82.4 79.3 59.6 85.1 Number of Memory Pathways . We ablate the number of memory pathways to assess their impact on SAM2Long in Table 5. Note that setting = 1 reverts to the SAM 2 baseline. Increasing the number of memory pathways to = 2 yields notable improvement, raising the &F score to 80.1. This result demonstrates that the proposed memory tree effectively boosts the models ability to track the correct object while reducing the impact of occlusion. Further increasing the number of memory pathways to = 3 achieves the best performance. However, using = 4 shows no additional gains, suggesting that three pathways strike the optimal balance between accuracy and computational efficiency for the SAM 2 model. In terms of speed, since we maintain fixed number of memory pathways at every time step, the processing speed remains efficient. Using three memory pathways slows down the model by only 18%, while yielding 4.5-point increase in performance. Iou Threshold δiou. The choice of the IoU threshold δiou is crucial for selecting frames with reliable object cues. As shown in Table 6, setting δiou = 0.3 yields the highest &F, indicating an effective trade-off between filtering out poor-quality frames and retaining valuable segmentation information. In contrast, having no requirement on mask quality and feeding all masks containing objects into memory (δiou = 0) decreases the score to 80.0, showing that unreliable frames with poor segmentation harm the SAM 2 model. Meanwhile, an overly strict selection (δiou = 0.9) degrades performance even more severely to 77.8, as it excludes too many potentially important neighboring frames, causing the model to rely on frames that are too far away from the current frame as memory. Uncertainty Threshold δconf. The uncertainty threshold δconf controls the selection of hypotheses under uncertain conditions. Our results in Table 7 indicate that setting δconf to 2 provides the highest &F score, indicating an optimal level for uncertainty handling. Lower values (e.g., 0.5) result in suboptimal performance, as they may prematurely commit to incorrect segmentation hypotheses, leading to significant performance drops due to error propagation. On the other hand, higher values (e.g., 5) do not further improve performance, suggesting that beyond certain threshold, the model does not benefit from additional mask diversity and can efficiently rely on the top-scoring masks when the segmentation is confident."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Ablation on number of pathways . Table 6: Ablation on IoU threshold δiou. &F 76.3 1 80.1 2 80.8 3 80.7 4 73.0 76.7 77.1 77.0 79.5 83.5 84.5 84.5 Speed 1 0.93 0.82 0.75 δiou &F 80.0 0 80.8 0.3 80.2 0.7 77.8 0.9 76.6 77.1 76.6 74.3 83.4 84.5 83.8 81.3 Table 7: Ablation on uncertainty threshold δconf. Table 8: Ablation on modulation [wlow, whigh]. δconf &F 80.4 0.5 80.8 2 80.5 76.7 77.1 76.9 83.7 84.5 84.1 [wlow, whigh] &F [1, 1] 80.2 [0.95, 1.05] 80.8 [0.9, 1.1] 80.5 76.5 77.1 76.9 83.8 84.5 84.1 Memory Attention Modulation [wlow, whigh]. We explore the effect of modulating the attention weights for memory entries using different ranges in Table 8. The configuration [1, 1] means no modulation is applied. We find that the configuration of [0.95, 1.05] achieves the best performance while increasing the modulation range to ([0.9, 1.1]) slightly decreases performance. This result indicates that slight modulation sufficiently emphasizes reliable memory entries. 4.4 VISUALIZATION We present qualitative comparison between SAM 2 and SAM2Long in Figure 3. SAM2Long demonstrates significant reduction in segmentation errors, maintaining more accurate and consistent tracking of objects across various frames. For example, in the second sequence of the second row, SAM 2 immediately loses track of the man of interest when occlusion happens. Although SAM2Long also loses track initially, its memory tree with multiple pathways enables it to successfully re-track the correct man later on. In another case, depicted in the third row where group of people is dancing, SAM 2 initially tracks the correct person. However, when occlusion occurs, SAM 2 mistakenly switches to tracking different, misleading individual. In contrast, SAM2Long handles this ambiguity effectively. Even during the occlusion, SAM2Long manages to resist the tracking error and correctly resumes tracking the original dancer when she reappears. In conclusion, SAM2Long significantly improves SAM 2s ability to handle object occlusion and reappearance, thereby enhancing its performance in long-term video segmentation."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce SAM2Long, training-free enhancement to SAM 2 that alleviates its limitations in long-term video object segmentation. By employing constrained tree memory structure with object-aware memory modulation, SAM2Long effectively mitigates error accumulation and improves robustness against occlusions, resulting in more reliable segmentation process over extended periods. Extensive evaluations on five VOS benchmarks demonstrate that SAM2Long consistently outperforms SAM 2, especially in complex video scenarios. Notably, SAM2Long achieves up to 5-point improvement in &F scores on challenging long-term video benchmarks such SA-V and LVOS without requiring additional training or external parameters. Although SAM2Long introduces significant improvements, there is still room for further enhancement. Future work could include fine-tuning the model on occlusion-heavy datasets using the memory architecture of SAM2Long. Additionally, exploring the semantic interactions between multiple objects within the same frame may offer valuable insights for more accurate segmentation, as the current method does not account for multi-object interactions in such scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Qualitative comparison between SAM 2 and SAM2Long, with GT (Ground Truth) provided for reference. blue box is used to highlight incorrectly segmented objects, while red box indicates missing objects. Best viewed when zoomed in."
        },
        {
            "title": "REFERENCES",
            "content": "Ali Athar, Jonathon Luiten, Alexander Hermans, Deva Ramanan, and Bastian Leibe. Hodor: Highlevel object descriptors for object re-segmentation in video learned from static images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 30223031, 2022. Ali Athar, Alexander Hermans, Jonathon Luiten, Deva Ramanan, and Bastian Leibe. Tarvis: unified approach for target-based video segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1873818748, 2023. Linchao Bao, Baoyuan Wu, and Wei Liu. Cnn in mrf: Video object segmentation via inference in cnn-based higher-order spatio-temporal mrf. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 59775986, 2018. Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, and Hao Li. Xmem++: Production-level video segmentation from few annotated frames. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 635644, 2023. Goutam Bhat, Felix Jaremo Lawin, Martin Danelljan, Andreas Robinson, Michael Felsberg, Luc Van Gool, and Radu Timofte. Learning what to learn for video object segmentation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 777794. Springer, 2020. Thomas Brox and Jitendra Malik. Object segmentation by long term analysis of point trajectories. In European conference on computer vision, pp. 282295. Springer, 2010. Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, and Luc Van Gool. One-shot video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 221230, 2017. Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, and Luc Van Gool. Blazingly fast video object segmentation with pixel-wise metric learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11891198, 2018. Ho Kei Cheng and Alexander Schwing. Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model. In European Conference on Computer Vision, pp. 640658. Springer, 2022. Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethinking space-time networks with improved memory coverage for efficient video object segmentation. Advances in Neural Information Processing Systems, 34:1178111794, 2021. Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13161326, 2023. Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, and Alexander Schwing. Putting the object back into video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 31513161, 2024. Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: new dataset for video object segmentation in complex scenes. In ICCV, 2023a. Shuangrui Ding, Weidi Xie, Yabo Chen, Rui Qian, Xiaopeng Zhang, Hongkai Xiong, and Qi Tian. Motion-inductive self-supervised object discovery in videos. arXiv preprint arXiv:2210.00221, 2022. Shuangrui Ding, Rui Qian, Haohang Xu, Dahua Lin, and Hongkai Xiong. Betrayed by attention: simple yet effective approach for self-supervised video object segmentation. arXiv preprint arXiv:2311.17893, 2023b. Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham Aarabi, and Graham Taylor. Sstvos: Sparse spatiotemporal transformers for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 59125921, 2021."
        },
        {
            "title": "Preprint",
            "content": "Deng-Ping Fan, Wenguan Wang, Ming-Ming Cheng, and Jianbing Shen. Shifting more attention to video salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 85548564, 2019. Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Editanything: Empowering unparalleled flexibility in image editing and generation. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 94149416, 2023. Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1348013492, 2023. Lingyi Hong, Zhongying Liu, Wenchao Chen, Chenzhi Tan, Yuang Feng, Xinyu Zhou, Pinxue Guo, Jinglun Li, Zhaoyu Chen, Shuyong Gao, et al. Lvos: benchmark for large-scale long-term video object segmentation. arXiv preprint arXiv:2404.19326, 2024. Ping Hu, Gang Wang, Xiangfei Kong, Jason Kuen, and Yap-Peng Tan. Motion-guided cascaded refinement network for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 14001409, 2018a. Yuan-Ting Hu, Jia-Bin Huang, and Alexander Schwing. Videomatch: Matching based video object segmentation. In Proceedings of the European conference on computer vision (ECCV), pp. 5470, 2018b. Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan, and Michael Felsberg. generative appearance model for end-to-end video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89538962, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023. Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan, and Dong Liu. Recurrent dynamic In Proceedings of the IEEE/CVF Conference on embedding for video object segmentation. Computer Vision and Pattern Recognition, pp. 13321341, 2022. Xiaoxiao Li and Chen Change Loy. Video object segmentation with joint re-identification and attention-aware mask propagation. In Proceedings of the European conference on computer vision (ECCV), pp. 90105, 2018. Yu Li, Zhuoran Shen, and Ying Shan. Fast video object segmentation using the global context module. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pp. 735750. Springer, 2020. Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33: 34303441, 2020. Jiehong Lin, Lihua Liu, Dekun Lu, and Kui Jia. Sam-6d: Segment anything model meets zero-shot 6d object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2790627916, 2024. Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024. K-K Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, and Luc Van Gool. Video object segmentation without temporal information. IEEE transactions on pattern analysis and machine intelligence, 41(6):15151530, 2018."
        },
        {
            "title": "Preprint",
            "content": "Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and Seon Joo Kim. Fast video object segmentation by reference-guided mask propagation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 73767385, 2018. Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 92269235, 2019. Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 724732, 2016. Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and Alexander Sorkine-Hornung. Learning video object segmentation from static images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 26632672, 2017. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Rui Qian, Shuangrui Ding, Xian Liu, and Dahua Lin. Semantics meets temporal correspondence: Self-supervised object-centric learning in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1667516687, 2023. Rui Qian, Shuangrui Ding, and Dahua Lin. Rethinking image-to-video adaptation: An object-centric perspective. In European Conference on Computer Vision, pp. 329348. Springer, 2025. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan, Fahad Shahbaz Khan, and Michael Felsberg. Learning fast and robust target models for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 74067415, 2020. Hongje Seong, Junhyuk Hyun, and Euntai Kim. Kernelized memory network for video object segmentation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXII 16, pp. 629645. Springer, 2020. Pavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the object in video object segmentation. In CVPR, 2023. Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Salvador, Ferran Marques, and Xavier Giro-i Nieto. Rvos: End-to-end recurrent network for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 52775286, 2019. Paul Voigtlaender and Bastian Leibe. Online adaptation of convolutional neural networks for video object segmentation. arXiv preprint arXiv:1706.09364, 2017. Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94819490, 2019. Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao, Yujia Xie, Lu Yuan, and Yu-Gang Jiang. Look before you match: Instance understanding matters in video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22682278, 2023. Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip HS Torr. Fast online object tracking and segmentation: unifying approach. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 13281338, 2019."
        },
        {
            "title": "Preprint",
            "content": "Qiangqiang Wu, Tianyu Yang, Wei Wu, and Antoni Chan. Scalable video object segmentation with simplified framework. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1387913889, 2023. Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping Zhang, and Wenxiu Sun. Efficient regional memory network for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12861295, 2021. Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu. Embodiedsam: Online segment any 3d thing in real time. arXiv preprint arXiv:2408.11811, 2024. Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos Katsaggelos. Efficient video object segmentation via network modulation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 64996507, 2018. Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. Advances in Neural Information Processing Systems, 35:3632436336, 2022. Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by foregroundbackground integration. In European Conference on Computer Vision, pp. 332348. Springer, 2020. Zongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object segmentation. Advances in Neural Information Processing Systems, 34:24912502, 2021a. Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by multiscale foreground-background integration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):47014712, 2021b. Jiaming Zhang, Yutao Cui, Gangshan Wu, and Limin Wang. Joint modeling of feature, correspondence, and compressed memory for video object segmentation. arXiv preprint arXiv:2308.13505, 2023. Lu Zhang, Zhe Lin, Jianming Zhang, Huchuan Lu, and You He. Fast video object segmentation via dynamic targeting network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 55825591, 2019. Junbao Zhou, Ziqi Pang, and Yu-Xiong Wang. Rmem: Restricted memory banks improve video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1860218611, 2024."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong"
    ]
}