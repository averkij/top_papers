{
    "paper_title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
    "authors": [
        "Chenhui Gou",
        "Zilong Chen",
        "Zeyu Wang",
        "Feng Li",
        "Deyao Zhu",
        "Zicheng Duan",
        "Kunchang Li",
        "Chaorui Deng",
        "Hongyi Yuan",
        "Haoqi Fan",
        "Cihang Xie",
        "Jianfei Cai",
        "Hamid Rezatofighi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 3 7 5 0 2 . 1 1 5 2 : r VQ-VA World: Towards High-Quality Visual Question-Visual Answering Chenhui Gou1,4 Zilong Chen2,4 Zeyu Wang3 Feng Li4 Deyao Zhu4 Zicheng Duan5 Kunchang Li4 Chaorui Deng4 Hongyi Yuan4 Haoqi Fan4 Cihang Xie3 Jianfei Cai1 Hamid Rezatofighi1 1Monash University 2Tsinghua University 3UC Santa Cruz 4Bytedance Seed 5University of Adelaide Project Page: https://chenhuigou.github.io/VQ-VA-World/"
        },
        {
            "title": "Abstract",
            "content": "This paper studies Visual Question-Visual Answering (VQVA): generating an image, rather than text, in response to visual questionan ability that has recently emerged in proprietary systems such as NanoBanana and GPTImage. To also bring this capability to open-source models, we introduce VQ-VA World, data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls massive amount of 1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA. 1. Introduction Driven by rapid advances in large multimodal generative models, frontier systems such as GPT-Image [22] and NanoBanana [20] now demonstrate exceptionally strong image generation and editing capabilities, showing reliable instruction following, high-fidelity synthesis, and improved consistency. Beyond these strengths, they also begin to exhibit an emergent ability we term Visual Question-Visual *Equal contribution. Work done during internship Answering (VQ-VA), i.e., responding to visual question with an image. As illustrated in Figure 1, when given photo of broken window and asked to speculate about what might be on the ground, NanoBanana generates an image depicting shards of glass; when shown an illustration of the stock market with bull and asked \"What is the contrasting trend?\", NanoBanana creates an image of bear to represent bearish market. Producing such visual answers requires conditioning on the input image and instruction, and, more critically, leveraging internalized world knowledge and multi-step reasoning to yield contextually coherent outputs. Despite this progress, VQ-VA remains largely restricted to proprietary systems such as GPT-Image and NanoBanana. As evident in Figure 1, current open-source models consistently underperform on these tasks: they often misinterpret the question or lack the world knowledge needed to synthesize an appropriate visual answer. We hypothesize that the primary bottleneck is data scarcity open-source solutions are predominantly trained on standard image-editing datasets that emphasize predefined operations (e.g., object addition, removal, replacement, style transfer), while underrepresenting free-form visual generation that demands knowledge and multi-step reasoning. In this paper, we present VQ-VA WORLD, datadriven framework to bridge this gap. At its core is an agentic data-construction pipeline with five modules: (1) Retrieveridentifies semantically and knowledge-driven image pairs from web-interleaved documents; (2) Instruction Generatorproduces free-form questions that require knowledge and reasoning, conditioned on the first image and using the second image as the answer; (3) Filter automatically removes low-quality questions or pairs; (4) Rewriterrephrases questions to enhance linguistic diversity; and (5) Reasonergenerates natural-language reasoning trace that explains how to approach the question, what knowledge is required, and the detailed transforma1 Figure 1. Examples of Visual Question-Visual Answering (VQ-VA), highlighting the substantial gap between existing closed-source models and open-weight models. The rightmost column further shows that model trained with VQ-VA WORLD dataset significantly improves its VQ-VA performance. tion from the source image to the target image. Deployed at web scale, this pipeline successfully curates 1.8M high-quality, interleaved image-text training samples across three subdomains: world knowledge (covering scientific, spatial, temporal, and other real-world domains), design knowledge, and reasoning. Moreover, to systematically assess models VQ-VA capability, we introduce IntelligentBench, human-curated benchmark sourced from real-world, web-interleaved documents. Each item is designed to probe specific knowledge and reasoning demands 2 Table 1. Comparison of major image-to-image datasets. QA indicates whether the datasets instructions are formatted as questions rather than direct prompts. Knowledge-centric denotes whether the instructions require world knowledge. Real image is marked true only if both the input and output images are real for the majority of the dataset. Concepts refers to the number of distinct words appearing in the instructions. Note: For SEED-Data-Edit, only small subset (0.073M out of 3.7M) contains real images. Dataset (image-to-image) MagicBrush [44] InstructPix2Pix [3] HQ-Edit [14] SEED-Data-Edit [12] UltraEdit [46] AnyEdit [43] ImgEdit [42] MetaQuery [23]"
        },
        {
            "title": "Ours",
            "content": "#Size Freeform QA"
        },
        {
            "title": "Real\nImage Concepts",
            "content": "10K 313K 197K 3.7M 4M 2.5M 1.2M 2.4M 1.8M 2K 11.6K 3.7K 29.2K 3.7K 6.4K - - 87.9K in VQ-VA. Additionally, we leverage leading VLMs (e.g., GPT-4o [21] and Gemini-2.5-Flash [9]) as automatic judges to facilitate large-scale evaluation. To evaluate the effectiveness of the VQ-VA WORLD dataset, we fine-tune LightFusion [34] (a fully open-source model; details provided in the Supp. files) on the 1.8M curated training samples and evaluate it on IntelligentBench. The results are striking: while previous open-source models achieve only trivial performance (e.g., 7.78 for LightFusion and 1.94 for UniWorld-V1), our LightFusion-World lifts the performance to 53.06, as shown in Table 2. Similar improvements are also observed on other VQ-VArelated benchmarks such as RISEBench [47] and KRISBench [39] (see Table 3). More excitingly, our model surpasses several large models pretrained on massive private data across IntelligentBench and other VQ-VA-related benchmarks; for example, it outperforms Qwen-Image [37] and FLUX.1-Kontext-Dev [16] on IntelligentBench, and surpasses Gemini-2.0-Flash [13], Seedream-4.0 [5], and BAGELThink [10] on RISEBench [47]. In addition, our results substantially narrow the gap with leading proprietary systems such as NanoBanana [20] and GPT-Image [22], as summarized in Tables 2 and 3. With the full release of model checkpoints, training and evaluation sets, and pipelines, we believe this work can help accelerate and inspire future open research in Visual Question-Visual Answering. 2. Related Work Image-to-Image models. Existing Image-to-Image (I2I) models can be broadly categorized into three types: (1) single I2I models, (2) unified multimodal models for both understanding and generation, and (3) leading proprietary models. For single I2I models, InstructPix2Pix [3] leverages synthetic data generated by GPT-3 [4] and Stable Diffusion [25] to train conditional diffusion model capable of following human-written editing instructions. Emu Edit [28] is also diffusion-based, but it is trained on diverse spectrum of editing tasks, including region-based I2I, freeform editing, and traditional computer vision tasks. Modern single I2I models such as Step1X-Edit [19], FLUX.1Kontext [16], and Qwen-Image [37] have substantially improved editing performance through both data scaling and model scaling. In parallel, unified multimodal models [7, 8, 10, 18, 23, 48] have gained popularity, benefiting from strong performance and cross-task learning advantages by combining understanding and generation. As for proprietary models, NanoBanana [20] and GPT-Image [22] still exhibit noticeable advantage over all other models, particularly showing emerging abilities on I2I tasks that require world knowledge and reasoning. The main motivation of our work is to narrow this gap in this specific domain for the open-source community. Public I2I datasets. MagicBrush [44] introduces manually annotated dataset containing 10k triplets, covering four types: single-turn, multi-turn, mask-provided, and maskfree editing. HQ-Edit [14] builds scalable data collection pipeline leveraging GPT-4V [1] and DALL-E 3 [2], resulting in around 200k editing samples. UltraEdit [46] employs an automatic pipeline that integrates an LLM and SDXL [24], presenting 4M-scale dataset consisting of real input images and synthetic edited images. SEED-Data-Edit [12] proposes hybrid dataset constructed from both human annotation and automatic pipelines, and further introduces specifically designed high-quality multi-turn imageediting data. OmniEdit-1.2M [35] is built using seven different specialist models and employs an importance sampling strategy to improve data quality. ImgEdit [42] and AnyEdit2.5 [43] expand the coverage of editing types to 13 and 25, respectively, thereby enhancing the instruction diversity of image-editing datasets. More recently, motivated by the strong performance of GPT-Image [22] in generation tasks, GPT-IMAGE-EDIT-1.5M [33] relabels previous Om3 niEdit, HQ-Edit, and UltraEdit datasets using GPT-Image API, further improving the quality of open-source imageediting resources. Despite their scale and variety, these exisiting datasets are purpose-built for standard pixel-level editing: the target image is direct modification of the source, guided by an explicit instruction. They thus under-represent scenarios that demand external knowledge and multi-step reasoning. Our VQ-VA WORLD corpus instead targets VQ-VA, where the model must synthesize an entirely new image by leveraging real-world knowledge and reasoning, not merely edit the original. I2I benchmarks. EmuEdit Benchmark [28] covers 7 fixed editing types and adopts L1, CLIP-I, and DINO as scoring metrics to evaluate editing ability. MagicBrushEdit Benchmark [44] extends this to 9 predefined tasks and provides two modes: mask-free and mask-provided. ImageEdit [42] further expands to 14 tasks, introduces VLM-based scoring, and supports multi-turn editing with varying difficulty levels. OMNI-EDIT-Bench [35] is high-resolution, multiaspect-ratio, multi-task benchmark comprising 434 edits derived from 62 images, evaluated with both VLM scorers and human judgments. GEdit-Bench [19] contains 606 real-world user editing cases, filtered by humans and scored with VLMs. All of these datasets focus on standard image editing, whereas our work addresses VQ-VA, where the model must synthesize an entirely new image by leveraging knowledge and reasoning. Two more recent benchmarks move closer to this setting: RISEBench [47] and KRIS-Bench [39] emphasize reasoning and world knowledge, and several of their examples can be cast as VQ-VA. Our evaluation set, IntelligentBench, however, differs in two key respects: (1) RISEBench and KRIS-Bench still primarily reward accurate pixel-level edits, while IntelligentBench deliberately includes tasks that require high-level semantic reasoning beyond what is visible in the source image (see Fig. 1); and (2) both RISEBench and KRIS-Bench rely heavily on synthetic images, whereas IntelligentBench is curated from real-world web content; every item is manually verified and paired with genuine reference answer image. 3. Methods This section elaborates on the details of the VQ-VA WORLD data framework and IntelligentBench. 3.1. VQ-VA World Data Framework Motivation. The VQ-VA WORLD framework tackles two key challenges: 1) identifying suitable data for VQ-VA and 2) designing scalable pipeline for its construction. We target image pairs whose transformations (Image1 Image2) inherently require knowledge or reasoningfor example, (car wheel car), (mathematical equation its graph), or (window of house broken glass on the ground). Such transformations capture semantic-level connections rather than superficial pixel-level alterations. By providing an image and formulating transformation-related questions whose answers require generating their corresponding counterparts, models can be trained to acquire knowledgerelated VQ-VA ability. The subsequent step is to identify data sources rich in such pairs and to develop automated pipelines for large-scale collection and refinement. Inspired by the data used in LLM pretraining, we regard web-interleaved documents as particularly promising candidate, since they naturally contain extensive world knowledge alongside closely associated images and text. Our target is to develop pipeline that mines these image-text interleaved web documents and converts them into high-quality VQ-VA training triples. Framework Overview. As illustrated in Fig. 2, VQ-VA WORLD operates in two stages: data preprocessing and an agentic pipeline for VQ-VA data construction. In the preprocessing stage, noisy web-interleaved documents are processed and assigned semantic labels, with only those belonging to the knowledge and design categories retained. The agentic pipeline then transforms the filtered documents into high-quality VQ-VA samples. Running this pipeline at web scale produces large-scale, high-quality training dataset with 1.8M samples, comprising 24.35% reasoning, 30.37% design knowledge, and 43.69% world knowledge. We details each step below. Step 1: Preprocessing. The first challenge is to sift through web-scale corpora and isolate documents whose images are tied together by substantive, knowledge-rich relationships. We leverage common prior that images on webpage revolve around the pages central topic, making topic classification an effective proxy for relevance. Since the topic is not directly provided in web data, we design loop to label documents efficiently, inspired by the data pipeline proposed in DeepSeek-Math [27]. Specifically, we first prompt an LLM (e.g., Qwen2.5-14B [41] in our case) to label subset of the data and identify samples of the required types. The labeled data are then used to train lightweight FastText [15] classifier, which enables large-scale labeling with high efficiency. Lastly, we apply an LLM again to refine the coarse labels produced by FastText. The final outputs of preprocessing are web-interleaved documents containing knowledgeand design-related content. The web document sources were collected from publicly available data [17] in compliance with copyright and GDPR guidelines. Step 2: Agent Pipeline for VQ-VA Data Creation. Our second stage turns the pre-filtered web-interleaved documents into high-quality VQ-VA examples. To scale the process and keep it modular, we design an \"agentic\" pipeline in which five independent workers handle specific sub-task. Specifically, each worker is powered by advanced VLMs 4 Figure 2. Illustration of the VQ-VA WORLD framework for creating VQ-VA data. The framework consists of two stages: (1) preprocessing, which classifies and filters web-interleaved documents, and (2) an agentic pipeline that generates VQ-VA samples from the filtered documents. The agentic pipeline contains five sub-modules: retriever, filter, instruction generator, rewriter, and reasoner. (e.g., GPT-4o [21] and Seed1.5VL-Thinking [26]), and is guided by carefully designed system prompts and chain-ofthought reasoning, without memory sharing across workers. We define the agent workers below: (1) Agent Retriever selects image pairs from interleaved documents that can serve as the basis for free-form questions. It focuses on pairs with meaningful transformations, especially those involving non-trivial relations grounded in knowledge and reasoning. We also find it beneficial for the retriever to capture the documents topic; hence, its input is the full document rather than merely the image list. The detail prompt is provided in Supp. Table 6. (2) Agent Instruction Generator write natural-language question about one image so that the other image serves as the correct answer. For instance, for the pair (car wheel racing car), if the question image is the wheel, it might ask: \"What is it used for?\" The questions are designed to probe diverse forms of knowledge and reasoning, including but not limited to: temporal or causal relations (e.g., an object before vs. after an event, or sequential steps with clear causality); compositional or spatial structures (e.g., part-whole links, inside-outside contrasts, exploded or sectional views); and scientific or analytical phenomena (e.g., visual explanations of scientific or mathematical concepts). The detailed prompt is provided in Supp. Table 7. (3) Agent Filter removes low-quality triplets Question Image, Question Text, Answer Image. Specifically, through careful multi-round human-in-the-loop audits, we identify several common issues leading to low-quality data, such as poorly formulated questions, ambiguous or irrelevant answer images, and context shortcuts (i.e., cases where the answer can be inferred from the text alone, making the question image unnecessary). To effectively address these issues, we design multi-score VLM-based filtering strategy with three sub-scorers: Question Score (QS), Answer Score (AS), and Context Dependence Score (CDS). The detailed prompts are provided in Supp. Table 8, 9 and 10, respectively. Each score is assigned on three-level scale 0, 1, 2, and only cases with the maximum total (i.e., QS + AS + CDS = 6) are retained. In addition, we manually design and iteratively refine the scoring template, and adopt chain-of-thought approach during scoring, where the model generates an analysis before assigning scores, thereby further enhancing filtering effectiveness. (4) Agent Rewriter increases instruction diversity by producing multiple variants of the original questions. The variFigure 4. Alignment between VLM and human scores. We compare Gemini-2.5-Flash vs. human experts, GPT-4o vs. human experts, and agreement among human experts. We report the Accuracy and Spearman Rank Correlation Coefficient (SRCC) for comprehensive comparison. the fact that video models naturally encode temporal knowledge, we use the Seedance video model [11] to construct set of 100k temporally related VQ-VA samples. 3.2. IntelligentBench Benchmark data. The purpose of IntelligentBench is to evaluate the VQ-VA abilities of different models, where the questions require knowledge and reasoning to answer. Specifically, it contains 360 human-curated examples divided into three domainsworld knowledge (171), design knowledge (88), and reasoning (101). The construction of IntelligentBench involves three main steps: (1) Document Review: Human experts examined about 3k classified interleaved web documents and, from each, selected the image pair that best represented the documents content and exhibited strong semantic connections. (2) Question Design: For each selected image pair, experts designed free-form questions targeting world knowledge, design knowledge, or reasoning. (3) Expert Cross-Review: Each candidate item is independently reviewed by at least one additional expert; only items that receive unanimous approval are retained, resulting in 360 final examples. Evaluation Metric. We use VLM as the automatic judge, following rules: (1) the VLM is provided with the question image, question text, reference answer image, the generated image, and carefully designed system prompt; (2) the VLM is required to output score as an integer in {0, 1, 2}. The full rubric and prompt is provided in the Supp. Metric Validation. To validate the reliability of our automatic grading process, we conducted comparative evaluation involving four human experts and two state-of-theart VLMs, each independently scoring outputs from four different models. Human inter-annotator agreement averaged 82.5%. As illustrated in the left panel of Figure 4, GPT-4o [21] achieved 80.6% agreement with human ratings, while Gemini-2.5-Flash [9] achieved 73.1%. The Spearman Rank Correlation Coefficient (SRCC) followed Illustration of the three question types in IntelligentFigure 3. Bench. Each type is shown with two examples, and each example contains question image, question text, and the answer image. ants differ in tone, sentence structure, vocabulary, expression, and overall linguistic naturalness. This rewriting process is essential for improving instruction-following ability. The detail prompt is provided in Supp. Table 11. (5) Agent Reasoner generates language-based chainof-thought explanation describing how the source image should be transformed to obtain the target image. The process involves analyzing the question, observing the question image, identifying changes, determining which elements remain consistent, and highlighting key modifications. This reasoning trace is then incorporated with the triplet to construct new data-format quadruplet Question Image, Question Text, Editing reasoning trace, Answer Image. This quadruplet is used to fine-tune unified multimodal model, i.e., LightFusion, to improve both reasoningtrace generation and instruction-following ability. The detailed prompt is provided in Supp. Table 12. High-quality subset curation. Following prior works such as [10, 37], which typically adopt multi-stage training, we employ two-stage strategy: continued pretraining and supervised fine-tuning (SFT). In the first stage, we train on the full large-scale dataset for additional steps to strengthen knowledge and instruction-following ability. In the second stage, we focus on smaller high-quality subset for fewer steps to improve quality. Specifically: (1) we apply stricter filtering, retaining the best one-third of the data, which yields about 500k high-quality samples; and (2) leveraging 6 Table 2. Results on IntelligentBench, benchmark designed for VQ-VA. refers to closed-source models. refers to openweight models; refers to the fully open-source models (both full training data and model weights). Model Open Source Level World Knowledge Design Knowledge Reasoning Overall GPT-Image-1 [22] Nano Banana [20] BAGELThink [10] Qwen-Image [37] FLUX.1-Kontext-Dev [16] OmniGen2 [38] Step1X-Edit [19] UniWorld-V1 [18] LightFusion [34] LightFusion-World 84.5 81.6 61.99 38.07 20.18 11.11 11.7 2.92 5.26 50.58 80.68 82.95 55.11 33.66 24.43 13.07 10.23 0.57 11.93 57.95 81.19 80.69 62.38 32.75 19.80 7.92 15. 1.49 8.42 52.97 82.64 81.67 60.42 34.31 21.11 10.69 12.36 1.94 7.78 53.06 the same trend, indicating that GPT-4os evaluations most closely reflect human judgment. We therefore adopt GPT4o as the default evaluator for IntelligentBench. 4. Experiments Implementation details. We adopt the fully-open, lighttraining unified multimodal model, LightFusion [34], as our baseline. Specifically, LightFusion leverages the publicly available Qwen2.5-VL-7B [41] as the understanding branch and Wan2.2-TI2V-5B [30] as the generation branch, and further introduces double fusion approach to synergize these two branches. In our experiments, we incorporate VQ-VA WORLD dataset into the overall training set of LightFusion with sampling ratio of 25%, and fine-tune the model for total of 45k steps. Both branches are trained following LightFusions default recipe with the timestep shift set to 4. We adopt two-stage training scheme: (1) continued training of LightFusion with mix of the 1.8M VQ-VA WORLD dataset for 30k steps with AdamW and cosine learning rate schedule (peak 1 105). (2) supervised finetuning on further filtered high-quality subset (1/3 of the original VQ-VA WORLD dataset) for 15k steps with constant learning rate of 1 105. Note that in both stages, the original 45M LightFusion data is mixed. Evaluation setting. For comprehensive evaluation of VQ-VA WORLD, we consider three domains with five benchmarks: (1) VQ-VA, evaluated on IntelligentBench; (2) reasoningand knowledge-informed image editing, evaluated on RISEBench and KRIS-Bench, with the results summarized in Tab. 3; both benchmarks require pixel-level alignment and strong reasoning capability; and (3) standard image editing, evaluated on GEdit-Bench [19], constructed from real-world user editing cases, and ImgEdit-Bench [42], designed to assess instruction adherence, editing quality, and detail preservation. Results on IntelligentBench are shown in Table 2; results on RISEBench and KRIS-Bench are shown in Table 3; and summarized results on traditional image editing tasks (GEdit-Bench and ImgEdit-Bench) are presented in Table 4. Following the setup in [10], for all knowledge-intensive benchmarks, the model is configured to first output reasoning content before generating the image, whereas for traditional image editing benchmarks, we directly generate the image. For all benchmarks, we adopt double-CFG strategy when evaluating both our LightFusion-World and the baseline LightFusion, with the image CFG scale set to 2 and the text CFG scale set to 4. The time shift is fixed at 4 for both training and evaluation. 4.1. Results on VQ-VA We first evaluate LightFusion-World along with other advanced closed-source and open-source models on IntelligentBench. Scores are normalized to the range 0-100 for each domain and averaged across domains; items for which model fails to produce an image receive score of 0. As the results reported in Table 2, show that LightFusion-World achieves the best performance among fully open-source models, and the large gap between the baseline model LightFusion and LightFusion-World further supports the effectiveness of our dataset. Moreover, LightFusion-World even surpasses Qwen-Image, which was pretrained on large-scale proprietary data and adopted RL for further improvement. Lastly, when compared with leading proprietary models such as GPT-4o and Gemini, we can see that performance gap remains but has already been substantially reduced. We provide more qualitative results of all models in Supp. Figure 535. 4.2. Results on Reasoning-Based Image Editing"
        },
        {
            "title": "Benchmark",
            "content": "In this domain, we evaluate models on RISEBench and KRIS-Bench, as shown in Table 3. On RISEBench, the results indicate that: (1) our model achieves performance comparable to BAGEL-Think while requiring far less training data; (2) Relative to the vanilla LightFusion base7 Table 3. Combined results on two reasoning-centric image editing benchmarks, RISEBench and KRIS-Bench. For previously published models, we directly cite their official results reported in papers or public leaderboards. For LightFusion and our fine-tuned models, we follow their official evaluation pipeline to reproduce and report the corresponding test results. refers to closed-source models. refers to open-weight models; refers to the fully open-source models (both full training data and model weights). Model Open Source Level RISEBench KRIS-Bench Temporal Causal Spatial Logical Overall Factual Conceptual Procedural Average Nano Banana [20] GPT-Image-1 [33] Gemini-2.0-Flash [13] Seedream-4.0 [5] BAGELThink [10] Qwen-Image-Edit [37] FLUX.1-Kontext-Dev [16] Step1X-Edit [19] EMU2 [29] HiDream-Edit [6] FLUX.1-Canny [16] OmniGen [40] MagicBrush [44] AnyEdit [43] InsPix2Pix [3] LightFusion [34] LightFusion-World 25.9 34.1 8.2 12.9 5.9 4.7 2.3 0.0 1.2 0.0 0.0 1.2 2.4 15.3 47.8 32.2 15.5 12.2 17.7 10.0 5.5 2.2 1.1 0.0 0.0 1.0 4.4 25.5 37.0 37.0 23.0 11.0 21.0 17.0 13.0 2.0 0.0 0.0 0.0 0.0 9.0 16. 18.8 10.6 4.7 7.1 1.1 2.4 1.2 3.5 0.0 0.0 0.0 1.2 0.0 3.5 32.8 28.9 13.3 10.8 11.9 8.9 5.8 1.9 0.5 0.0 0.0 0.8 4.2 15.3 79.80 65.26 55.77 45.52 45.40 33.11 41.84 39.26 23.33 60.44 66. 81.37 59.65 59.44 48.01 37.54 28.02 39.24 41.88 25.59 51.23 63.50 78.32 62.90 39.26 31.82 34.91 23.89 26.54 31.74 17.28 44.83 52.38 80.09 62.41 53.36 43.29 39.70 28.85 37.15 38.55 22.82 52.52 61. Table 4. Results on Standard Image Editing Benchmarks (GEditBench-EN and ImgEdit-Bench). Higher scores are better. refers to closed-source models. refers to open-weight models; refers to the fully open-source models (both full training data and model weights) Model Open Source Level GEdit-Bench-EN ImgEdit-Bench SC PQ Overall Overall GPT-4o [33] Gemini-2.0-Flash [13] ICEdit [45] Step1X-Edit [19] OmniGen2 [38] BAGEL [10] Ovis-U1 [31] UniPic [32] UniPic 2.0 [36] Instruct-Pix2Pix [3] MagicBrush [44] AnyEdit [43] UniWorld-V1 [18] LightFusion [34] LightFusion-World 7.85 7.62 6.73 6.61 5.11 6.85 7.09 6.76 7.16 6.77 7.36 6.83 6.72 6.18 3.58 5.49 4.68 5.66 3.18 5.82 4.93 7.43 6.34 7.31 7.00 7.29 7.53 6.32 4.84 6.70 6.41 6.52 6.42 5.83 7. 3.68 4.52 3.21 4.85 6.06 6.58 4.20 - 3.05 3.06 3.43 3.20 3.98 3.49 4.06 1.88 1.90 2.45 3.26 3.77 3.85 line, our model posts large absolute gain; and (3) some large in-house-data-trained models such as Qwen-ImageEdit and FLUX.1-Kontext-Dev underperform ours, highlighting potential limitations of unbalanced data distribution and the necessity of free-form, knowledge-rich data like VQ-VA WORLD dataset. KRIS-Bench exhibits the same pattern: LightFusion-World consistently outperforms every fully open-source competitor. These findings further support the effectiveness of VQ-VA WORLD and the benefits brought by enhanced VQ-VA capability. More qualitative results on RISEBench are provided in Supp. 38. 4.3. Results on Standard Image Editing Benchmark Lastly, we report standard image editing performance on GEdit-Bench-EN and ImgEdit-Bench, as shown in Table 4. The complete ImgEdit-Bench results for each subdomain (e.g., add/remove) are provided in the Supp. Table 5. From these tables, we can see that our model delivers consistent gains over the LightFusion baseline on both datasets. This modest marginespecially when contrasted with the large improvements seen on VQ-VA and reasoning-centric editinghighlights the clear domain gap between routine pixel-level edits and knowledge-driven generation. 4.4. Summarized Results Combining Tabs. 2 to 4, we make the following observations: (1) Existing open-source models show certain ability on standard image editing, and the performance gap with closed-source models has been substantially reduced thanks to recent open image-editing dataset efforts. However, they still struggle on VQ-VA, and the gap remains significant. This further indicates the necessity of developing open-source VQ-VA-related data. (2) With the help of VQVA data, LightFusion achieves clear improvements not only on VQ-VA but also on reasoning-based image editing tasks, along with noticeable gains on standard image editing. This supports the view that generalized VQ-VA capability also benefits other tasks. 5. Conclusion This work focuses on studying VQ-VA, an emerging property that has already been exclusively seen in leading propri8 etary models. To bring this capability to open-source models, we develop VQ-VA WORLD, scalable data-centric framework driven by an agentic pipeline for constructing high-quality, diverse VQ-VA training data. Our webscale pipeline curates 1.8 million high-quality samples, and we complemented it with IntelligentBench, humancurated benchmark to rigorously assess the VQ-VA capability. Fine-tuning LightFusion on BAGEL-World lifts its IntelligentBench score from 7.78 to 53.06, surpassing all existing open-source models and substantially narrowing the gap to proprietary leaders. We are releasing the full suite of code, data, pipelines, and model checkpoints to spur further research on VQ-VA and, more broadly, on building more powerful multimodal systems that can answer with images."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 3 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3, 8, 13 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 3 [5] Bytedance Seed. https : / / seed . bytedance.com/en/seedream4_0, 2025. Accessed: 2025-09-24. 3, Seedream 4.0. [6] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. 8 [7] Chameleon-Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3 [8] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 3 [9] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3, 6 [10] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 3, 6, 7, 8, [11] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 6 [12] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 3 9 [13] Google. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.google/technology/ google - deepmind / google - gemini - ai - updatedecember2024/#gemini20flash, 2024. Google Blog. 3, 8 [14] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 3 [15] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Fasttext.zip: arXiv preprint Douze, Hérve Jégou, and Tomas Mikolov. Compressing text classification models. arXiv:1612.03651, 2016. 4 [16] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 3, 7, 8 [17] Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, and Jifeng Dai. Omnicorpus: unified multimodal corpus of 10 billion-level images interleaved with text. In The Thirteenth International Conference on Learning Representations, 2025. 4 [18] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 3, 7, 8, 13 [19] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 3, 4, 7, 8, 13 [20] Nano Banana AI. https : / / nanobananaai.org/, 2025. Accessed: 2025-09-19. 1, 3, 7, 8 Nano banana ai. [21] OpenAI. Addendum to gpt-4o system card: Native image generation. Technical report, OpenAI, 2025. 3, 5, 6 [22] OpenAI. Gpt image 1. https://platform.openai. com/docs/models/gpt-image-1, 2025. Accessed: 2025-09-24. 1, 3, 7 [23] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: 10 els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3 arXiv preprint [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [26] ByteDance Seed. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 5 [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 4 [28] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 3, 4 [29] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. 8 [30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 7 [31] Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Jianshan Zhao, Yang Li, and Qing-Guo Chen. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025. 8, 13 [32] Peiyu Wang, Yi Peng, Yimeng Gan, Liang Hu, Tianyidan Xie, Xiaokun Wang, Yichen Wei, Chuanxin Tang, Bo Zhu, Changshi Li, et al. Skywork unipic: Unified autoregressive modeling for visual understanding and generation. arXiv preprint arXiv:2508.03320, 2025. 8, [33] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025. 3, 8 [34] Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, and Cihang Xie. Lightfusion: light-weighted, double fusion framework for unified multimodal understanding and generation, 2025. 3, 7, 8, 13 and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 3, 13 [47] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. 3, 4 [48] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [35] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. 3, 4 [36] Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, Chuanxin Tang, Zidong Wang, Yichen Wei, Liang Hu, Boyi Jiang, William Li, Ying He, Yang Liu, Xuchen Song, Eric Li, and Yahui Zhou. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model, 2025. 8, 13 [37] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3, 6, 7, 8 [38] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 7, 8, 13 [39] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. 3, 4 [40] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 8 [41] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 4, 7 [42] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 3, 4, [43] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 3, 8, 13 [44] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 3, 4, 8, 13 [45] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 8, 13 [46] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, 11 VQ-VA World: Towards High-Quality Visual Question-Visual Answering"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we first show the full results on ImaEdit (Tab. 5) and then describe the prompt details of the VQ-VA WORLD framework in Tabs. 6 to 12. We also report the complete result visualizations of IntelligentBench for different models in Figures 535. Finally, at the end of this supplementary material, we provide an additional qualitative comparison on RISEBench in Fig. 38, including LightFusion-World and other models. 5.1. Complete results on ImgEdit 5.2. Complete prompts of VQ-VA WORLD 5.3. Complete results on IntelligentBench of different models. 5.4. Qualitative Comparison on RISEBench 12 Table 5. Evaluation of image editing ability on ImgEdit-Bench. Higher scores are better for all metrics."
        },
        {
            "title": "Model",
            "content": "GPT-4o 4.61 4."
        },
        {
            "title": "2.84\nMagicBrush [44]\nInstruct-Pix2Pix [3] 2.45\n3.18\nAnyEdit [43]\n3.44\nUltraEdit [46]\n3.88\nStep1X-Edit [19]\n3.58\nICEdit [45]",
            "content": "OmniGen2 [38] BAGEL [10] Ovis-U1 [31] UniPic [32] UniPic 2.0 [36] UniWorld-V1 [18] LightFusion [34] 3.74 3.56 4.12 3.66 - 3.82 4.21 1.58 1.83 2.95 2.81 3.41 3.39 3.54 3.31 3.92 3.51 - 3.66 3.23 LightFusion-World 4.33 3. 2.90 1.51 1.41 1.14 2.00 1.76 1.73 1.77 1.88 2.36 2.06 - 2.31 1.83 1.25 4.35 1.97 2.01 2.49 2.96 3.40 3. 3.21 2.62 4.09 4.31 - 3.45 4.55 4.63 3.66 1.58 1.44 2.21 2.45 2.83 2.93 2.77 2.88 3.57 2.77 - 3.02 3.80 3. 4.57 1.75 1.44 2.88 2.83 3.16 3.08 3.57 3.44 4.22 3.77 - 2.99 4.15 4.24 4.93 2.38 3.55 3.82 3.76 6.63 3. 4.81 4.49 4.69 4.76 - 4.71 4.66 4.69 3.96 1.62 1.20 1.56 1.91 2.52 2.04 2.30 2.38 3.23 2.56 - 2.96 3.93 3. 4.89 1.22 1.46 2.65 2.98 2.52 3.68 4.14 4.17 3.61 4.04 - 2.74 3.60 4.45 4.20 1.90 1.88 2.45 2.70 3.06 3. 3.43 3.20 3.98 3.49 4.06 3.26 3.77 3.85 13 ###[System Role Instruction] You are an image-collection assistant. Task Given document that contains figures (Figure 1 . . . Figure N), select exactly one pair of figures (x = y) that share strong, clearly explainable connection. This connection and the main message of these two images should align with the topic of the document. These two images must have clear difference but deep and non-trivial connection. If no pair meets the requirement, return [0,0]. Return only the indices in the form [x,y] (e.g. [2,7]). If no pair meets the requirement, return [0,0]. Key requirement: The connection must show salient semantic change that is not immediately obvious from low-level appearance alone; some reasoning or domain knowledge is needed to recognise or explain the relationship. What counts as strong connection () 1. Change / Process Same subject over time or ordered steps with clear cause effect. Examples: before after renovation, seed sprout, chess move t+1. 2. Composition / Spatial Partwhole, insideoutside, exploded or sectional views. Examples: wheel car, sealed box opened box, floor plan 3-D cut-away. 3. Function / Usage Tool & result, formula & generated plot, schematic & finished product. Examples: hammer nailed board, math equation its curve, stencil printed pattern. 4. Scientific / Analytical Visual explanation of scientific or mathematical phenomenon. Examples: reaction sequence with colour change, geometry figure with auxiliary lines, diffraction pattern illustrating wave optics. 5. Evidence / Validation Abstract model or theory paired with empirical or simulated imagery that confirms it. Examples: unit-circle diagram sine-wave plot, probability-density formula sampled histogram. 6. Comparison / Contrast Two items shown mainly to highlight opposition, attribute change, or analogy. Examples: rough vs. finished, night vs. day, cat vs. dog in identical pose. Exclude () Pairs that are near-duplicates or exhibit only camera/geometry changes (zoom, crop, rotation, mirroring, minor viewpoint shift). Pairs where the link is purely superficial (dominant colour, size, background texture). Pairs where the change is too trivial to require reasoning (e.g. same scene one second apart with no new event). Reference cases Case 1 Rough unfinished house fully renovated house. (1 Change + 6 Contrast) Case 2 Tic-Tac-Toe move immediate counter-move. (1 Change) Case 3 Sealed cardboard box opened box with items. (2 Composition) Case 4 Reaction scheme photo of precipitate formation. (4 Scientific) Case 5 Unit-circle diagram plotted sine wave. (5 Evidence) Case 6 Math equation diagram visualising that equation. (3 Function) Output Return only the bracketed pair. Examples: [1,2], [3,9] Indices start at 1 and must be different. If no suitable pair exists, output [0,0]. Now provide the image pair. Table 6. The prompt of Retriever in VQ-VA WORLD agentic pipeline. 14 ###[System Role Instruction] You are an AI teacher preparing an exam consisting of image-based questions. Input Figure 1 the image shown to the student. Figure 2 the image that will serve as the answer. Task Write one question about Figure 1 such that only Figure 2 can answer it. Students will see only the question text and Figure 1; they will not see Figure 2. Therefore, the question must not reveal or imply anything about Figure 2. Guidelines * The question must be precise, clear, and non-trivial. * It must depend on details in Figure 1. * The answer must require showing an image rather than brief textual reply. * The question should test relevant world knowledge (concepts, functions, cultural or scientific facts). * The question must fit exactly one of the following relation types: 1. Change / Process Same subject over time or ordered steps with clear cause effect. Examples: before after renovation, seed sprout, chess move t+1. 2. Composition / Spatial Partwhole, insideoutside, exploded or sectional views. Examples: wheel car, sealed box opened box, floor plan 3-D cut-away. 3. Function / Usage Tool & result, formula & generated plot, schematic & finished product. Examples: hammer nailed board, math equation its curve, stencil printed pattern. 4. Scientific / Analytical Visual explanation of scientific or mathematical phenomenon. Examples: pattern illustrating wave optics. 5. Evidence / Validation Abstract model or theory paired with empirical or simulated imagery that confirms it. Examples: unit-circle diagram sine-wave plot, probability-density formula sampled histogram. 6. Comparison / Contrast Two items shown mainly to highlight opposition, attribute change, or analogy. Examples: rough vs. finished, night vs. day, cat vs. dog in identical pose. * Do not reference Figure 2 in the question text. reaction sequence with colour change, geometry figure with auxiliary lines, diffraction Output Format Return exactly one line, with no line breaks: [Q:<question sentence>, A:<See this image>] Table 7. The prompt of Instruction Generator in VQ-VA WORLD agentic pipeline. 15 ###[System Role Instruction] You are an AI Scoring Assistant. Your job is to extremely strictly evaluate each Q&A + image pair so that only truly exceptional cases receive the top score (2). Unless you are absolutely certain the pair is flawless, default to 1. You will output exactly one JSON object containing only the fields for the question: - QS (0, 1, 2) - QSR (string, 100 tokens) 1. Question Score (QS) Default = 1; upgrade to 2 only if all checks below pass with unquestionable certainty. 1. Strict Relevance - The question must refer directly to objects, shapes, or details clearly visible in the image. - If it asks about properties or knowledge not visible or relevant, score 1. 2. Logical & Factual Soundness - The question must be internally coherent, accurately reflect what is visible in the image, and rely on reasoning that aligns with real-world knowledge. - Any logical contradiction, factual error, or reliance on implausible world knowledge score 1. 3. Clarity & Specificity - Must be perfectly clear, leaving zero room for interpretation. - If wording could be improvedeven slightlyscore 1. 4. Non-Trivial, Logical Transformation - Must request significant and meaningful image-based action or deduction. - Trivial or purely factual look-ups max 1. 5. No Contradictions - Every reference (colour, shape, position) must match the image exactly. - Any mismatch score 0. 6. No Significant Improvement - If you can think of any other images, significantly different from the answer image, that could also improve or answer the question, award score of 1. Only cases where the answer image alone provides perfect, unmistakable clarity may receive score of 2. QS Scoring - 0 Completely off-topic, incoherent, or contradictory. - 1 Relevant but fails 1 checkpoint or any doubt remains. - 2 Passes all checkpoints perfectly, with no conceivable improvement. Summarize in QSR ( 100 tokens). Output Format { \"QSR\": \"concise reasoning, <=100 tokens\", \"QS\": 0 1 2 } Table 8. The prompt of Question Score in VQ-VA WORLD agentic pipeline. 16 ###[System Role Instruction] You are an AI Scoring Assistant. Your job is to extremely strictly evaluate each Q&A + image pair so that only truly exceptional cases receive the top score (2). Unless you are absolutely certain the pair is flawless, default to 1. You will output exactly one JSON object containing only the fields for the answer: - AS (0, 1, 2) - ASR (string, 100 tokens) Answer Score (AS) Default = 1; upgrade to 2 only if all conditions below are met beyond reasonable doubt. 1. Exact Fulfilment of Request - The image must precisely satisfy the question, nothing more, nothing less. 2. Completeness - Every requested element is fully present. Any omission score 0. 3. Visual Consistency - Colours, shapes, positions match exactly unless change is explicitly required. - Partial or approximate matches score 1. 4. No Visual Errors - No artefacts, distortions, or illogical geometry. 5. No Significant Improvement - If you can think of any other images, significantly different from the answer image, that could also improve or answer the question, award score of 1. Only cases where the answer image alone provides perfect, unmistakable clarity may receive score of 2. AS Scoring - 0 Completely off-topic, incoherent, or contradictory. - 1 Relevant but fails 1 checkpoint or any doubt remains. - 2 Passes all checkpoints perfectly, with no conceivable improvement. Output Format { \"ASR\": \"concise reasoning, <=100 tokens\", \"AS\": 0 1 } Table 9. The prompt of Answer Score in VQ-VA WORLD agentic pipeline. 17 ###[System Role Instruction] You are an AI Scoring Assistant. Your job is to extremely strictly evaluate each Q&A + image pair so that only truly exceptional cases receive the top score (2). Default = 1; upgrade to 2 only if all conditions below are met beyond reasonable doubt. You will output exactly one JSON object containing: - CDSR (string, 100 tokens) - CDS (0, 1, 2) Context Dependence Score (CDS) This score evaluates whether, when the question image is completely ignored, the answer image by itself could still correctly answer the question. - Default = 1 - If the answer image requires little or no reference to the question image to answer correctly, downgrade to 0, because this indicates poor question design. CDS Scoring - 0 The answer image alone suffices; it depends almost nothing on the question image. - 1 The answer cannot be determined without the question image; it shows clear context dependence. - 2 The answer absolutely cannot be determined without the question image, and this dependence is both strong and completely unquestionableonly assign 2 if the necessity of context is exceptional and indisputable. Output Format { \"CDSR\": \"reasoning, <=100 tokens\", \"CDS\": 0 1 2 } Table 10. The prompt of Context Dependence Score in VQ-VA WORLD agentic pipeline. 18 ###[System Role Instruction] You are an AI assistant. You are given question and need to rewrite the question and answer in five diverse ways. The rewritten versions should be sufficiently diverse, focusing on the following aspects: * Tone: Use variations like formal, informal, casual, polite, direct, or even imperative. * Sentence structure: Change the order of words, split long sentences, use shorter or more complex phrasing. * Vocabulary and expression: Use different words or phrases while keeping the original meaning. * Human-like naturalness: Ensure the questions sound like something real person would ask in various situations. Consider incorporating variety of phrasing styles, from clear inquiries to more conversational or casual requests. Please balance your rewrites: * Provide 3 direct questions (clear and formal phrasing). * Provide 2 more conversational or command-like phrases. The goal is to make the questions feel like they could have been asked by real person in wide variety of contexts. Ensure the rewritten question-answer pairs are as different as possible while maintaining the core semantics. You will receive question. Please provide exactly five rewritten question-answer pairs in JSON format, each pair should strictly follow this structure: [ {\"q\": \"your question\", \"a\": \"your answer\"}, {\"q\": \"your question\", \"a\": \"your answer\"}, {\"q\": \"your question\", \"a\": \"your answer\"}, {\"q\": \"your question\", \"a\": \"your answer\"}, {\"q\": \"your question\", \"a\": \"your answer\"} ] Now, give me your rewritten cases: Table 11. The prompt of Rewriter in VQ-VA WORLD agentic pipeline. 19 [System Role Instruction] You have the following information: 1. question image: [Place or reference the question image here] 2. question text: [Place the text of the question here] 3. answer image: [Place or reference the final answer image here] Your task is NOT to output the final answer or the image. Instead, you must: - Generate detailed thinking or chain-of-thought process that explains how you reason about the question. - Do NOT include the final answer text in your output. - Provide only the reasoning/analysis that leads to the final answer and the answer image (even though you will not reveal the final answer itself). - The reasoning/analysis should include some description of the answer image to help the answer-imagegeneration. Below is an example of how your output should look. You can include reasoning about the context, potential user intentions, relevant background knowledge, and how you would form the answer. The length of outputs should be around or shorter than 200 tokens. Example Output: First, notice the user wants to see vehicle displayed while its moving. check the question_image, which seems to feature red sports car on racetrack. The question_text, Can you display the vehicle while its moving?, suggests they want visual depiction of car in motion. Im considering details like the cars color, sponsor logos, and the environment around the carperhaps should highlight the sense of motion, theres crowd in the background, or its racing circuit. possibly leaning into turn or speeding down straight. When forming the final answer_text, Id mention something about the vehicle speeding around circuit. also think about how Id describe the final imagemaybe note the brand, the sponsor logos, and the number on the windshield or dashboard. Including speed, the angle of the car, and another car chasing it might help convey dynamic sense of movement. Lastly, recall that the user specifically asked to display the vehicle while its moving, so Id ensure the image description references motion, leaning into turn, and the impression of high velocity. This approach should fulfill their request. Table 12. The prompt of Reasoner in VQ-VA WORLD agentic pipeline. 20 Figure 5. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 1/9). 21 Figure 6. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 2/9). Figure 7. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 3/9). 23 Figure 8. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 4/9). 24 Figure 9. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 5/9). 25 Figure 10. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 6/9). 26 Figure 11. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 7/9). 27 Figure 12. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 8/9). 28 Figure 13. Comprehensive visualization of model performance on IntelligentBench (Subset Design, part 9/9). 29 Figure 14. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 1/9). 30 Figure 15. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 2/9). 31 Figure 16. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 3/9). 32 Figure 17. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 4/9). 33 Figure 18. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 5/9). 34 Figure 19. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 6/9). 35 Figure 20. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 7/9). 36 Figure 21. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 8/9). 37 Figure 22. Comprehensive visualization of model performance on IntelligentBench (Subset Reasoning, part 9/9). 38 Figure 23. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 1/15). 39 Figure 24. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 2/15). 40 Figure 25. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 3/15). 41 Figure 26. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 4/15). 42 Figure 27. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 5/15). 43 Figure 28. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 6/15). 44 Figure 29. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 7/15). 45 Figure 30. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 8/15). 46 Figure 31. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 9/15). 47 Figure 32. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 10/15). 48 Figure 33. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 11/15). 49 Figure 34. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 12/15). 50 Figure 35. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 13/15). 51 Figure 36. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 14/15). 52 Figure 37. Comprehensive visualization of model performance on IntelligentBench (Subset World knowledge, part 15/15). 53 Figure 38. Qualitative comparison on RISE benchmark."
        }
    ],
    "affiliations": [
        "Bytedance Seed",
        "Monash University",
        "Tsinghua University",
        "UC Santa Cruz",
        "University of Adelaide"
    ]
}