{
    "paper_title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
    "authors": [
        "Jianhui Chen",
        "Yuzhang Luo",
        "Liangming Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs."
        },
        {
            "title": "Start",
            "content": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Jianhui Chen * 1 Yuzhang Luo * 1 Liangming Pan 1 2 # jianhuichennlp@gmail.com Mechanistic-Data-Attribution"
        },
        {
            "title": "Abstract",
            "content": "While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted interventionremoving or augmenting small fraction of high-influence samplessignificantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce concurrent change in the models in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing principled methodology for steering the developmental trajectories of LLMs. 6 2 0 2 9 2 ] . [ 1 6 9 9 1 2 . 1 0 6 2 : r 1. Introduction The rapid advancement and widespread deployment of Large Language Models (LLMs) have transformed the landscape of artificial intelligence (Achiam et al., 2023; Yang et al., 2025; Guo et al., 2025). This progress has been acFigure 1. The Mechanistic Data Attribution (MDA) framework. MDA identifies interpretable LLM units and quantifies the influence of individual training samples on their functional behavior. This enables both the discovery of mechanistic training dynamics and precise data-level interventions to steer model development. *Equal contribution 1State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University 2Beijing Academy of Artificial Intelligence, Beijing, China. Correspondence to: Liangming Pan <liangmingpan@pku.edu.cn>. Preprint. January 30, 2026. companied by parallel surge in Mechanistic Interpretability (MI), field dedicated to reverse-engineering these neural networks into human-understandable algorithms (Elhage et al., 2021). Recent efforts have successfully identified specific interpretable units within Transformer-based LLMs, such as induction heads responsible for in-context copy1 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units ing and pattern completion (Olsson et al., 2022), knowledge neurons that store factual associations about specific entities (Dai et al., 2022), and monosemantic features disentangled via Sparse Autoencoders (SAEs) (Huben et al., 2023; Bricken et al., 2023). These findings effectively describe what model computes during inference, offering detailed anatomy of the models internal mechanisms. Despite these successes, current MI research remains predominantly static. While we can reverse-engineer what circuit computes, we lack the tools to discern the causal origins of these computations within the training corpus. Bridging this gap holds significant value for both the scientific understanding of large language models and their practical governance. From scientific perspective, identifying the data-driven origins of internal components provides causal lens to observe how specialized circuitssuch as those for logical reasoning (Hong et al., 2025) or factual recall (Nichani et al., 2025)are shaped by the statistical properties of the training corpus (Chan et al., 2022). Simultaneously, from practical standpoint, these insights enable precise data-level interventions. By filtering deleterious samples or augmenting high-leverage data, researchers can predictably modulate the emergence of specific mechanisms, offering more fine-grained control over internal representations compared to traditional data attribution methods (Koh & Liang, 2017; Grosse et al., 2023; Kou et al., 2025). To address this challenge, we introduce Mechanistic Data Attribution (MDA), novel methodological framework designed to trace the training origins of internal mechanisms, as shown in Figure 1. Unlike traditional Training Data Attribution (TDA) methods that typically focus on global model behavior (Koh & Liang, 2017; Grosse et al., 2023), MDA operates at the granularity of individual interpretable units, such as neurons, attention heads, or SAE features. By deriving specialized formulation of Influence Functions, our approach enables the precise computation of how specific training samples impact the functional properties of these units. This shift allows the analytical lens to move beyond descriptive analysis (this circuit exists) toward developmental tracing (this data distribution caused the formation of this circuit). To be specific, we make the following four contributions in this paper: Methodological Framework (Section 3): We propose the Mechanistic Data Attribution (MDA) framework and derive scalable, gradient-based approach leveraging Influence Functions. This framework enables the precise identification of training samples that most significantly impact the functional behavior of interpretable LLM units. distinct attention head types (induction and previous-token heads (Olsson et al., 2022)), we demonstrate that removing small fraction ( 10%) of high-influence samples from the total training set significantly hinders the emergence of targeted heads (e.g., Induction Heads). Conversely, repeating these specific samples accelerates their formation, whereas randomly removing or augmenting an equivalent volume of data yields no such effect. Mechanistic Insights (Section 5): Our investigation into the formation of induction heads reveals several key findings: 1) Data Composition: Noisy data characterized by highly repetitive patterns, predominantly sourced from LaTeX and XML, significantly accelerates the emergence of induction heads. 2) Transferability: High-influence samples generalize effectively across different induction heads, exhibiting significant overlap in their identified influential subsets. 3) Emergence Dynamics: Induction head formation is not driven by sparse sample subsets but develops steadily as training tokens accumulate, with high-influence samples primarily modulating the rate of this process. 4) In-Context Learning (ICL) Correlation: Enhancing induction head capabilities leads to concurrent improvement in in-context learning performance, and vice versa. This bidirectional coupling provides causal evidence supporting the hypothesis that induction heads serve as foundational mechanism for ICL (Olsson et al., 2022). Practical Application (Section 6): Leveraging these findings, we propose practical data augmentation pipeline. This pipeline utilizes LLMs to extract patterns from highinfluence samples and automatically generates code for data synthesis. Empirical results demonstrate that synthetic data generated via our smallest model generalizes effectively across various model sizes, consistently accelerating induction head formation. This provides scalable methodology for fine-grained control of model behavior. 2. Related Work 2.1. Mechanistic Interpretability Mechanistic Interpretability (MI) aims to reverse-engineer neural networks into functional circuits that implement specific algorithmic behaviors (Elhage et al., 2021). Conventional research predominantly adopts post-hoc paradigm, characterizing where and how circuitssuch as induction heads (Olsson et al., 2022)operate during inference. However, these analyses are largely static, treating internal mechanisms as fixed objects while overlooking their causal origins within the training corpus. Causal Validation (Section 4): We validate the causal effects of MDA through extensive data ablation and augmentation experiments during pre-training. Across four model scales in the Pythia family (Biderman et al., 2023) and two More recent work has begun to incorporate training dynamics, including studies of the developmental trajectory of induction heads (Tigges et al., 2024) and the temporal emergence of other mechanistic features (Ge et al., 2025). 2 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Several works further investigate induction head formation under controlled conditions: Singh et al. (2024) and Kawata et al. (2025) employ synthetic datasets and forward-pass interventions, while Aoyama et al. (2025) theoretically relates induction head emergence to the frequency and reliability of bigram repetitions. However, these approaches largely rely on simplified data distributions or controlled settings. In contrast, our work introduces complementary mechanistic interpretability paradigm that directly traces the emergence of internal circuits back to specific training examples in realistic models trained on natural, unstructured data, providing scalable framework for developmental tracing. 2.2. Training Data Attribution Training Data Attribution (TDA) identifies how specific training examples influence model behavior, primarily through Influence Functions (IF) (Koh & Liang, 2017). To overcome the computational costs of Hessian-inverse calculations in LLMs, recent advancements leverage scalable approximations like EK-FAC (George et al., 2018). Building on these, Grosse et al. (2023) utilized IF to explain output likelihood through MLP layers, while other works focus on general behavioral abilities (Kou et al., 2025; Li & Sen, 2025). However, the influence of training data on intermediate functional components remains largely unexplored. While recent studies have observed temporal correlations between specific data patterns and induction heads (Lee et al., 2025; Baker et al., 2025), they remain primarily observational. In contrast, we move beyond observation to conduct causal interventions that verify the impact of identified data on circuit formation. This provides new mechanistic insights into ICL and enables practical methodology for fine-grained training interventions. 3. Mechanistic Data Attribution Framework In this section, we first introduce some preliminaries and then formally present our proposed Mechanistic Data Attribution (MDA) framework. 3.1. Preliminary Transformers and Induction Heads. In Transformerbased Language Models, information flows via residual streams mediated by attention heads and MLP layers, many of which have been characterized as functionally interpretable units (Chen et al., 2025; Zhou et al., 2025). The most prominent among these are Induction Heads, which are considered critical components responsible for in-context learning capabilities (Olsson et al., 2022). Specifically, given previous context containing the sequence [A1][B1], an induction head operates by attending to the token [B1] upon receiving the current token [A2] (where [A2] = [A1]). This attention mechanism allows the model to copy the information from [B1] to predict the next token [B2] (where [B2] = [B1]), effectively implementing pattern completion operation. Let θ denote the model parameters. We represent specific component (e.g., an attention head h) by its corresponding subset of parameters θsub θ. Influence Functions and EK-FAC Approximation. Influence Functions (IF) provide classic statistical tool to estimate the effect of upweighting training sample ztrain on the loss of test sample ztest. The influence score is given by: I(ztrain, ztest) = θL(ztrain)H 1 θ θL(ztest) (1) where Hθ is the Hessian of the loss. Calculating the exact inverse Hessian is computationally prohibitive for LLMs. To scale this analysis, we employ the Eigenvaluecorrected Kronecker-Factored Approximate Curvature (EKFAC) method (George et al., 2018; Grosse et al., 2023). EK-FAC approximates the Hessian layer-wise using Kronecker products of covariance matrices, enabling efficient estimation of the Inverse-Hessian-Vector Product (IHVP) essential for attribution. 3.2. MDA Framework While standard Training Data Attribution (TDA) methods typically quantify the influence of training samples on the global model loss across the entire parameter space, we extend this paradigm by proposing the two-stage Mechanistic Data Attribution (MDA) framework (Figure 1). MDA enables the attribution of fine-grained, component-level behaviors to specific training data, allowing for more localized and mechanistic understanding of model development. Stage 1: Localizing Interpretable Units. The MDA framework is formally characterized by three-tuple (µ, π, fprobe). Specifically, we first define monitoring metric µ, which serves as quantitative indicator for identifying specific interpretable units (e.g., the prefix-matching score for induction heads (Olsson et al., 2022)). Guided by this metric, we localize the target unit and isolate its associated parameter subspace θsub with the subspace projection π. Building upon mechanistic understanding of the identified unit, we then design probing function fprobe (which may be identical to µ) along with corresponding evaluation dataset Dprobe to assess the functional efficacy of the target unit. summary of common design choices for (µ, π, fprobe) across attention heads, neurons, and SAE features in Table 3 (Appendix D). Stage 2: Unit-Specific Influence Calculation. To capture the contribution of data to the behavior of the target unit rather than generic token prediction, we replace the standard validation loss L(ztest) in Equation (1) with fprobe(θ, Dprobe). 3 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Combining with the specified θsub, the influence of training sample on the interpretable unit is (formal derivation in Appendix A.1): IMDA(ztrain, Dprobe) θsub L(ztrain) ˆH 1 θsub θsub fprobe(θ, Dprobe) (2) where ˆH 1 is the EKFAC-approximated inverse Hessian θsub computed exclusively within the subspace θsub. Algorithm 1 (Appendix D) provides the full procedural details for the MDA calculation. 4. Causal Validation: Data Influence on"
        },
        {
            "title": "Mechanistic Emergence",
            "content": "We verify the effectiveness of MDA through causal interventional study on the Pythia model suite (Biderman et al., 2023). We study the emergence of Induction Heads and Previous Token Heads (Olsson et al., 2022), two common mechanistic components identified in LLMs. We manipulate the pre-training corpus by removing or duplicating high-influence samples identified by MDA. This allows us to assess the extent to which these specific training instances are causally responsible for the formation of the targeted mechanistic circuits described above. 4.1. Experimental Setup Models and Target Units. We conduct our experiments on the first four sizes of the Pythia suite (14M, 31M, 70M, 160M) and analyze two well-studied interpretable units: Induction Heads and Previous Token Heads. Due to computational constraints, computing influence scores across the entire pre-training corpus is infeasible. Instead, we dedicate our attribution analysis to critical developmental window [tstart, tend] that encompasses the emergence of these heads (detailed in Appendix C). Within this window, we use the full sequence of training data without sampling; this approach ensures we capture sparse yet pivotal training examples that may be essential for triggering mechanistic emergence, which stochastic sampling might otherwise omit. The specifications of (µ, π, fprobe) for these heads are provided in Table 3 (Appendix D). Causal Validation. Although influence scores provide theoretical proxy for data importance, they do not inherently imply causality. To rigorously establish the causal link, we perform bidirectional experiments via counterfactual retraining to evaluate both the sufficiency and necessity of the identified samples. Specifically, we conduct two intervention experiment: 1) Data Augmentation: high influence samples ( 10% of all samples) are duplicated and inserted in specific training step; 2) Data Deletion: the gradients of high influence samples are masked during training. These experiments are localized within the [tstart, tend] windoweither from scratch or continued from official checkpoints. To ensure reproducibility, all configurations, including hyperparameters, data sequencing, and random seeds, strictly adhere to the original Pythia repository. Detailed experimental configurations are provided in Appendix E. 4.2. MDA Identifies Causally Effective Data As illustrated in Figure 2, Data Deletion (masking the topranked samples) results in consistent suppression or delayed emergence of both heads, whereas random exclusion yields negligible impactconfirming these samples are necessary for circuit development. Conversely, Data Augmentation triggers an accelerated phase transition comparing to random insertion baselines, demonstrating that the identified samples possess pivotal causal influence, providing the necessary signal to propel the emergence of the targeted mechanism. Together, these results establish robust causal link between the MDA-identified training samples and the internal development of the models functional circuits. further validation with an ablation-based metric also justifies the effectiveness of our framework (Appendix H). Furthermore, we observe that in most cases, models under both data augmentation and deletion regimes eventually converge to comparable saturation scores. This suggests that while specific samples significantly modulate the emergence rate, the ultimate formation of these heads is collective property of the general training distribution rather than being determined exclusively by sparse subset of samples. This finding aligns with the analysis of induction head development in Nanda et al. (2023), which suggests that induction circuits provide systematic loss reduction and thus receive consistent gradients across broad spectrum of training data. We further discuss this phenomenon in Section 5.3. Notably, we observe an early drop in induction head scores during the augmentation phase in the Pythia-70M and 160M models. We clarify that this does not constitute failure of MDA but serves as characteristic signal of accelerated emergence, phenomenon further explored in Appendix G. 5. Mechanistic Insights into Induction Head In this section, we demonstrate how MDA serves as complementary method to conventional post-hoc analysis, providing novel mechanistic insights into their formation and elucidates their functional coupling with the emergence of In-Context Learning (ICL) capabilities. 5.1. Distributional Patterns of High Influence Data We first examine the statistical distribution of induction head influence scores and the patterns of the top-ranked 4 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Figure 2. Causal validation of Mechanistic Data Attribution. Intervened retraining shows that targeted deletion and augmentation of high-influence samples (identified via MDA) significantly modulate the emergence of Induction and PT (Previous Token) heads. Head score is quantified via the metric from (Olsson et al., 2022). Note the clear gap between MDA-guided interventions and the random baselines across different model scales. data. Our analysis focuses on samples with positive scores in the Pythia-14M model and the full distribution is provided in Appendix I. Table 1. Representative High-Influence Training Samples. The top-ranked samples exhibit distinct repetitive structures across different domains. Type XML Code LaTeX Database Meaningless Sample Content (Truncated) <data>YnBsaX...</data> <key>ANSIBright...</key> <data>YnBsaX... public function matches($subject): return $subject instanceof... In the category of topological spaces ($mathbf{Top}$)... [\"44.2094250\",\"28.6460842\", \"Management\",\"Bachelor\"... AczgAczgAczgAczgAczgAczg AczgAczgAczgAczgAczgAczg bool { Power-Law Distribution of Influence. Across all evaluated model scales, influence scores consistently exhibit heavy-tailed distribution, as illustrated in Figure 3(a). This distribution adheres to distinct power-law, indicating that the emergence of mechanistic circuits is disproportionately driven by sparse subset of high-leverage training signals. Notably, the top 10% of samples account for approximately 50% of the total cumulative influence. This concentration of influence provides an empirical justification for the selective intervention strategy employed in Section 4. Highly Repetitive Patterns. Understanding the distributional properties that drive the emergence of specific mechanisms provides valuable empirical insights for optimizing model training (Chan et al., 2022). MDA offers principled lens to identify functional patterns within unstructured training data. qualitative inspection of samples with the highest positive influence scores  (Table 1)  reveals striking and previously under-explored finding: highly repetitive structuresincluding seemingly noisy or garbage sequencesact as primary catalysts for induction head formation. This observation aligns with the functional role of induction heads in predicting repetitive tokens within long-range contexts. For full examples, see Appendix L. 5.2. Transferability of Influential Data To determine whether the identified training signals are unitspecific or mechanism-general, we analyze the overlap of high-influence samples across different functional components in Pythia-14M. Specifically, we compare the top three induction heads against three arbitrary non-induction heads. As illustrated by the block-diagonal pattern in Figure 3(b), there is pronounced overlap in influential data among different induction heads, indicating they are driven by common set of mechanistic catalysts. In contrast, the overlap between induction and non-induction heads is notably lower. This dissociation confirms that MDA successfully isolates data specific to the induction mechanism, rather than merely identifying globally hard or high-loss samples. Furthermore, we observe that interventions (augmen5 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Figure 3. Distributional properties of high influence samples. a) Power-law distribution: The distribution of influence scores follows power-law, where the top 10% of samples contribute up to 50% of the total cumulative influence. b) Cross-head consistency: High-influence samples identified by induction heads (Ihead) within the Pythia-14M model exhibit significant overlap, yet remain distinct from those identified by non-induction heads (Nhead). c) Step uniformity: The identified high-influence samples are distributed uniformly throughout the training corpus, showing no significant temporal clustering. d) Induction head scores with high influence samples replaced with those from different steps. MDA-Repl @ [t1, t2] represents replaced by high influence sample in step t1 to t2. The random replacement baseline exhibits significant deviation from the MDA replacement, exceeding three standard errors (3σ). Pruned 95% means we randomly mask the gradient of 95% samples in training, while the induction head scores still show non-trivial increase. e) Induction scores differences of all heads from Pythia 14M. High influence samples from one head are generalizable to other induction heads (red squares). tation or deletion) using samples identified from single induction head effectively modulate the performance of other induction heads (Figure 3(e)). These results suggest that the identified data features are universally effective for the underlying mechanism itself, rather than being an artifact of specific unit. 5.3. Emergence Dynamics We first investigate the temporal dynamics of induction head formation by discretizing the training process into 100-step intervals. Surprisingly, we observe remarkable temporal homogeneity: both the influential samples and their corresponding scores are distributed uniformly across the entire training trajectory (Figure 3(c), Figure 8). This stands in stark contrast to the sharp phase transition observed in the induction scores (Figure 2), suggesting that the emergence of the mechanism is not tied to sudden influx of specific data during the transition window. To bridge this gap, we conducted series of cross-stage controlled interventions. We replaced high-influence samples in the emergence window (steps 14001500) with those identified from early (10001100), mid (13001400), and late (19002000) stages. As shown in Figure 3(d), highinfluence samples from any training interval demonstrate universal effectiveness, consistently outperforming random baselines. Notably, the induction mechanism continues to develop even when 95% of the training samples within the window are masked. These findings suggest that induction head formation follows steady accumulation model rather than being triggered by sparse subset of unique samples. Once training tokens reach critical threshold, the phase transition occurs spontaneously. Within this framework, high-influence samples provide higher signal density that shortens the required accumulation period. 5.4. Causal Link to In-Context Learning It has been widely acknowledged that Induction heads are usually correlated with In-context Learning (ICL) capabilities (Olsson et al., 2022). However, prior work has largely Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Figure 4. Validating the functional role of induction heads in ICL via data intervention. Under the same data augmentation and deletion settings used for induction heads, the concurrent shifts in ICL scores and induction head strength (grey dashes) provide causal evidence that these internal mechanisms are functionally coupled. relied on observational correlations. By leveraging the MDA framework, we provide direct causal evidence linking the development of induction heads to ICL capabilities. core insight is to automate the distillation of abstract structural patterns from the high-influence data identified by our framework and scale them up via procedural synthesis. General Metric for ICL capability. To rigorously quantify global ICL capability, we adopt the ICL Score metric proposed by Olsson et al. (2022), defined as the reduction in loss for late tokens compared to early tokens within long context window: ICL Score = L500 mean(L0:50). positive score indicates that the model is effectively utilizing the extended context (500 tokens) to improve prediction accuracy relative to shorter context (50 tokens). We evaluate this metric on the WikiText-2. MDA enables precise intervention in the formation of induction heads, providing rigorous means to verify their causal link to ICL capabilities. As illustrated in Figure 4, we observe striking alignment between the trajectories of induction scores and ICL performance: the suppression of induction head formation leads to simultaneous degradation in ICL scores. Conversely, the synchronized enhancement of induction scores results in corresponding boost in ICL proficiency. While we cannot entirely preclude the presence of latent confounders, MDA provides significantly more controllable experimental regime for mechanistic investigation compared to traditional observational studies. 6. Mechanistic Data Augmentation Synthesizing the insights from Section 5which established that induction heads are driven by specific structural motifs (e.g., frequent repetitions) rather than stochastic correlationswe propose practical approach for training enhancement: Mechanistic Data Augmentation. 6.1. Data Augmentation Pipeline We introduce three-step pipeline that translates post-hoc attribution results into an ante-hoc training strategy. Our Step 1: Influence-Guided Sample Selection: We employ the Pythia-14M model as mechanistic proxy to identify high-leverage training data within the corpus. By executing the MDA framework during the 14M models localized emergence window, we isolate the top-ranked = 2000 training samples that exhibit the highest influence on circuit formation. Step 2: Automated Pattern Distillation via LLM: To move beyond manual qualitative analysis, we leverage high-capacity Large Language Model (e.g., DeepSeekV3 (DeepSeek-AI et al., 2025)) to automatically extract latent structural motifs from the identified data. We utilize structured prompting strategy that tasks the LLM with analyzing batches of high-influence text and synthesizing them into rigorous JSON-formatted schemas. Step 3: Procedural Data Synthesis: Based on the extracted JSON schemas, we prompt the LLM to generate executable Python scripts, which are subsequently used to programmatically synthesize training examples. This pipeline ensures that the synthetic data maintains strict structural consistency with the target mechanism while providing sufficient diversity in surface patterns. Crucially, this approach bypasses the need for computationally expensive large-scale corpus mining. Detailed prompts for all generation stages are provided in Appendix J. 6.2. Synthetic Data Generalize Across Model Scales To assess the effectiveness and generalizability of our augmentation approach, we train four Pythia variants (14M, 31M, 70M, and 160M) by inserting mechanistic synthetic data during their localized emergence phases. This allows us to verify whether the synthetic data can consistently ac7 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Table 2. Changes in induction head scores across various model sizes after augmenting the training set with Pythia-14M-guided synthetic patterns. denotes augmenting with synthetic patterns from Ptyhia-160M. Model Baseline () Augmentation () 14M 31M 70M 160M 160M 0.432 0.472 0.304 0.508 0.508 0.485 0.523 0.352 0.558 0. +12.3% +10.8% +15.8% +9.84% +2.56% celerate functional formation across different model scales. We compare the induction head scores under synthetic augmentation against the baseline at the conclusion of the training interval. The results  (Table 2)  demonstrate that synthetic data consistently triggers and accelerates induction head formation across all model scales. This reinforces the hypothesis that structural motifs, rather than specific semantic content, serve as the primary causal drivers of the induction mechanism. Remarkably, synthetic data identified from the 14M model exhibited efficacy on the 160M model that surpassed the data derived from the 160M model itself. This finding provides robust evidence for the cross-model consistency of mechanistic drivers, suggesting that the structural curriculum required to catalyze induction heads is scaleinvariant. Such invariance validates the practical strategy of leveraging lightweight proxies to optimize the training of larger systems. The experimental details are provided in Appendix E.3. 6.3. Ablation Study on Insertion Strategy To isolate the factors driving the success of our strategy, we performed an ablation study on the Pythia-14M model, focusing on two critical dimensions: Insertion Quantity (N ) and Insertion Mode (Concentrated vs. Dispersed). By systematically varying these parameters, we characterize the optimal interventional regime required to maximize the acceleration of induction head formation. comprehensive specification of these experimental settings is provided in Appendix F. comparison between synthetic and natural data performance (Figure 5) reveals fundamental trade-off between mechanistic density and semantic diversity: 1) Small-Data Regime (N 50, 000): Synthetic data outperform natural data. For instance, at = 12, 500 and = 25, 000, the synthetic insertion triggers faster and sharper phase transition. This suggests that our synthetic templates possess higher causal densityevery sample is perfect structural example, whereas high influence real data may still contain noise. 2) Large-Data Regime (N = 100, 000): crossover occurs where natural data begins to outperform synthetic data. We attribute this to diversity exhaustion. Figure 5. Ablation of Insertion Strategy in Pythia 14M. Induction head formation is positively correlated with the quantity of mechanistic signals (N ). The performance gap between Dispersed and Concentrated modes reveals that the temporal density of interventions significantly modulates optimization stability, particularly for high-influence real-world samples. Since the synthetic data is generated from finite set of extracted patterns, scaling to 100, 000 samples likely introduces diminishing returns due to excessive structural redundancy. In contrast, the natural high-influence samples, while noisier, offer broader spectrum of lexical and syntactic variations, preventing overfitting to rigid template and supporting sustained capability growth. Regarding the insertion mode, Dispersed Insertion consistently outperforms Concentrated Insertion for natural data. By maintaining alignment with the original distribution, this approach minimizes optimization perturbations and avoids the optimization shock often induced by concentrated gradient bursts. Spreading the data thus acts as localized curriculum, facilitating stable mechanistic integration without disrupting concurrent feature acquisition. Interestingly, synthetic data exhibits divergent, scale-dependent results; we leave the investigation of this discrepancy to future work. 7. Conclusion We introduced Mechanistic Data Attribution (MDA), framework for tracing the causal origins of interpretable LLM mechanisms back to the training corpus. Our results demonstrate that the emergence of circuits, such as induction heads, is driven by identifiable data catalysts that generalize across model scales. By establishing causal link between these internal mechanisms and macro-level capabilities like ICL, MDA provides principled methodology for understanding Large Language Models. Furthermore, MDA provides foundation for mechanistic alignment, enabling researchers to steer or unlearn specific model behaviors by precisely targeting their causal data origins. Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units"
        },
        {
            "title": "Impact Statements",
            "content": "This work presents framework for Mechanistic Data Attribution (MDA) that traces the functional origins of LLM circuits back to their training data. The potential societal impacts of this research are twofold. First, in terms of AI safety and governance, MDA provides principled methodology for understanding how specific data distributions shape internal model behaviors. This enables more precise, data-level interventions to mitigate the emergence of biased or deleterious mechanisms, moving beyond superficial output-based filtering toward foundational transparency. Second, in terms of computational efficiency, our findings on data catalysts offer path toward more efficient pre-training by identifying high-leverage data patterns, potentially reducing the carbon footprint of training large-scale models. While such attribution tools could theoretically be repurposed for targeted data poisoning, the transparency provided by MDA serves as critical defensive layer, allowing researchers to audit and steer model development more responsibly."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https: //arxiv.org/pdf/2303.08774.pdf. Aoyama, T., Wilcox, E. G., and Schneider, N. Predicting the formation of induction heads, 2025. URL https: //arxiv.org/abs/2511.16893. Baker, G., Wang, G., Hoogland, J., and Murfet, D. Structural inference: Interpreting small language models with susceptibilities, 2025. URL https://arxiv.org/ abs/2504.18274. Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., OBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397 2430. PMLR, 2023. URL https://proceedings. mlr.press/v202/biderman23a.html. Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., DeniTowards monosemanson, C., Askell, A., et al. ticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023. URL https://transformer-circuits.pub/ 2023/monosemantic-features. Chan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill, F. Data distributional properties drive emergent incontext learning in transformers. Advances in neural information processing systems, 35:1887818891, 2022. URL https://dl.acm.org/doi/abs/10. 5555/3600270.3601641. Chen, J., Wang, X., Yao, Z., Bai, Y., Hou, L., and Li, J. Towards understanding safety alignment: mechanistic In The Thirty-ninth perspective from safety neurons. Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=AAXMcAyNF6. Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., Knowledge neurons in pretrained and Wei, F. the 60th Annual transformers. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84938502, 2022. URL https://aclanthology.org/2022. acl-long.581.pdf."
        },
        {
            "title": "In Proceedings of",
            "content": "DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., Zeng, W., Zhao, W., An, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Zhang, X., Chen, X., Nie, X., Sun, X., Wang, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Song, X., Shan, X., Zhou, X., Yang, X., Li, X., Su, X., Lin, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhu, Y. X., Zhang, Y., Xu, Y., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Yu, Y., Zheng, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Tang, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Wu, Y., Ou, Y., Zhu, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Zha, Y., Xiong, Y., Ma, Y., Yan, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Huang, Z., Zhang, Z., Xie, Z., Zhang, Z., Hao, Z., Gou, Z., Ma, Z., Yan, Z., Shao, Z., Xu, Z., Wu, Z., Zhang, Z., Li, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Gao, Z., 9 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units and Pan, Z. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL https://transformer-circuits. pub/2021/framework/index.html. Ge, X., Shu, W., Wu, J., Zhou, Y., He, Z., and Qiu, X. Evolution of concepts in language model pre-training, 2025. URL https://arxiv.org/abs/2509.17196. George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P. Fast approximate natural gradient descent in kronecker factored eigenbasis. Advances in neural information processing systems, 31, 2018. URL https://dl. acm.org/doi/10.5555/3327546.3327625. Grosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., Hubinger, E., Lukoˇsiute, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., and Bowman, S. R. Studying large language model generalization with influence functions, 2023. URL https://arxiv.org/abs/ 2308.03296. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025. URL https://www.nature.com/ articles/s41586-025-09422-z. Hong, G. Z., Dikkala, N., Luo, E., Rashtchian, C., Wang, X., and Panigrahy, R. implies b: Circuit analysis in LLMs for propositional logical reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=M0U8wUow8c. Huben, R., Cunningham, H., Smith, L. R., Ewart, A., and Sharkey, L. Sparse autoencoders find highly interIn The Twelfth pretable features in language models. International Conference on Learning Representations, 2023. URL https://openreview.net/pdf? id=F76bwRSLeK. Kawata, R., Song, Y., Bietti, A., Nishikawa, N., Suzuki, T., Vaiter, S., and Wu, D. From shortcut to induction head: How data diversity shapes algorithm selection in transformers. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https: //openreview.net/forum?id=n0QvMU2kON. Koh, P. W. and Liang, P. Understanding black-box predicIn International confertions via influence functions. ence on machine learning, pp. 18851894. PMLR, 2017. URL https://proceedings.mlr.press/v70/ koh17a.html. Kou, S., Tian, Q., Xu, H., Zeng, Z., and Deng, Z. Which data attributes stimulate math and code reasoning? an investigation via influence functions. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=b7uniOw0sZ. Lee, J. H., Smith, M., Adam, M., and Hoogland, J. Influence dynamics and stagewise data attribution, 2025. URL https://arxiv.org/abs/2510.12071. Li, L. and Sen, P. Unraveling the influence of training data and internal structures in large language models for enhanced explainability (student abstract). Proceedings of the AAAI Conference on Artificial Intelligence, 39(28):2940729409, Apr. 2025. doi: 10.1609/ aaai.v39i28.35268. URL https://ojs.aaai.org/ index.php/AAAI/article/view/35268. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=9XFSbDPmdW. Nichani, E., Lee, J. D., and Bietti, A. Understanding factual recall in transformers via associative memories. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=hwSmPOAmhk. Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, In-context learning and induction Y., Chen, A., et al. heads. arXiv preprint arXiv:2209.11895, 2022. URL https://arxiv.org/pdf/2209.11895. Singh, A. K., Moskovitz, T., Hill, F., Chan, S. C., and Saxe, A. M. What needs to go right for an induction head? mechanistic study of in-context learning cirIn Forty-first International cuits and their formation. Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=O8rrXl71D5. Tigges, C., Hanna, M., Yu, Q., and Biderman, S. LLM circuit analyses are consistent across training and scale. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=3Ds5vNudIE. 10 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Zhou, Z., Yu, H., Zhang, X., Xu, R., Huang, F., Wang, K., Liu, Y., Fang, J., and Li, Y. On the role of attention heads in large language model safety. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=h0Ak8A5yqw. 11 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units A. Theoretical Background A.1. Derivation of Influence Functions We adhere to the standard formalism of influence functions as introduced by Koh & Liang (2017). Let = (x, y) denote training sample from the input space and label space Y. Let L(z, θ) be the loss function for model parameterized by θ Θ Rp. Consider training dataset = {z1, . . . , zN }. The empirical risk minimizer ˆθ is given by: ˆθ = arg min θΘ"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 L(zi, θ). (3) To quantify the influence of specific training example on the model parameters, we consider perturbation where is upweighted by small constant ϵ. This corresponds to finding the minimizer of the perturbed empirical risk: ˆθϵ,z = arg min θΘ (cid:32)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:33) L(zi, θ) + ϵL(z, θ) . (4) The influence of the training point on the parameters is defined as the rate of change of the parameters with respect to ϵ at ϵ = 0: Iparams(z) def= dˆθϵ,z dϵ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 . (5) Since ˆθϵ,z is minimizer, the gradient of the perturbed objective must be zero. Assuming the loss function is twice differentiable and strictly convex in the neighborhood of ˆθ, the first-order optimality condition is: θ (cid:32) 1 (cid:88) i= L(zi, ˆθϵ,z) + ϵL(z, ˆθϵ,z) = 0. (cid:33) Let R(θ) = 1 (cid:80)N i=1 L(zi, θ) denote the empirical risk. The condition simplifies to: θR(ˆθϵ,z) + ϵθL(z, ˆθϵ,z) = 0. We perform first-order Taylor expansion of the gradient θR(ˆθϵ,z) around the original optimum ˆθ: θR(ˆθϵ,z) θR(ˆθ) + 2 θR(ˆθ)(ˆθϵ,z ˆθ). (6) (7) (8) Since ˆθ minimizes R(θ), we have θR(ˆθ) = 0. Let Hˆθ = 2 empirical risk. Substituting the expansion back into the optimality condition and keeping terms of order O(ϵ): θL(zi, ˆθ) denote the Hessian of the θR(ˆθ) = 1 i=1 2 (cid:80)N Hˆθ(ˆθϵ,z ˆθ) + ϵθL(z, ˆθ) 0. Solving for the parameter change θ = ˆθϵ,z ˆθ: Dividing by ϵ and taking the limit ϵ 0, we obtain the influence on parameters: ˆθϵ,z ˆθ ϵH 1 ˆθ θL(z, ˆθ). Iparams(z) = 1 ˆθ θL(z, ˆθ). (9) (10) (11) Finally, to measure the influence of training example on specific target function (θ) (e.g., the validation loss on test point ztest, or in our case, the component-specific capability score), we apply the chain rule: I(z, ) = df (ˆθϵ,z) dϵ = θf (ˆθ) dˆθϵ,z dϵ = θf (ˆθ)H 1 ˆθ θL(z, ˆθ). (12) This formulation allows us to estimate how upweighting training example affects any differentiable metric without retraining the model. In our methodology, represents the induction capability objective and the Hessian is restricted to the specific component subspace. 12 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units A.2. Scalable Approximation via EK-FAC Directly computing the Inverse-Hessian-Vector Product (IHVP) 1v is computationally intractable for LLMs, as the Hessian matrix for layer with dimensions din dout has size (dindout)2. To address this, we employ the Eigenvaluecorrected Kronecker-Factored Approximate Curvature (EK-FAC) (George et al., 2018; Grosse et al., 2023) method , tailored to the specific parameter subspaces of induction and previous token heads. Standard K-FAC Assumption. The K-FAC method approximates the Hessian of linear layer (where = x) by assuming independence between the input activations and the output gradients = yL. Under this assumption, the Hessian block for weight decomposes into the Kronecker product of the input covariance and the gradient covariance S: S, where = E[xx] and = E[gg]. (13) This allows efficient inversion via the identity (A S)1 = A1 S1, reducing complexity from O(d6) to O(d3). Eigenvalue Correction (EK-FAC). Standard K-FAC assumes that the eigenvectors of the Hessian are UA US and its eigenvalues are the Kronecker product of the eigenvalues of and S. However, this assumption is often inaccurate for neural networks, leading to poor curvature estimation. EK-FAC improves upon this by retaining the K-FAC eigenvector basis (which is generally good approximation) but correcting the eigenvalues. Let = UAΣAU be the eigendecompositions of the covariance matrices. EK-FAC estimates the diagonal of the Hessian in this Kronecker basis: and = USΣSU HEK-FAC = (UA US)Λ(UA US), (14) where Λ is diagonal matrix. The entries of Λ are estimated efficiently via Monte Carlo sampling using the exact per-sample gradients projected onto the K-FAC basis. This correction captures the true scale of the curvature along the principal directions, significantly improving the accuracy of influence estimation. Joint Subspace Approximation for Attention Heads. critical modification in our methodology is the handling of the Query (WQ) and Key (WK) matrices, particularly for previous token heads. These matrices do not operate in isolation; the attention mechanism Attention(Q, K, ) = softmax( QK )V relies on the inner product of their outputs. Treating dk WQ and WK as independent blocks (i.e., block-diagonal Hessian approximation) would enforce zero interaction term 2L WQWK = 0, ignoring the strong correlation between query and key updates. To capture these essential correlations, we perform EK-FAC on the concatenated joint subspace Wjoint = [WQ; WK] R(dq+dk)dmodel . Shared Input Covariance (A): Since both projections receive the same input (from the residual stream), the input covariance matrix Rdmodeldmodel is computed once and shared. Joint Gradient Covariance (S): The gradient covariance R(dq+dk)(dq+dk) is computed using the concatenated gradients gjoint = [gQ; gK]. Crucially, the off-diagonal blocks of this matrix capture the cross-covariance E[gQg K], effectively modeling the interaction between the query and key heads. This fused approach ensures that the influence scores reflect the coupled nature of the attention pattern formation, rather than treating the query and key projections as disjoint feature extractors. A.3. Component-Specific Influence Formulations Based on the derived influence framework and the EK-FAC approximation, we formally define the calculation of influence scores for the two specific types of mechanistic components investigated in this work. Case 1: Previous Token Heads (Attention Pattern Formation). The primary function of previous token head is to allocate attention mass to the immediately preceding token. This mechanism is governed solely by the interaction between the Query and Key projections, independent of the specific values being moved. 13 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Target Objective fprev (Averaged over Probes): To isolate the structural attention pattern from content-dependent interactions, we use batch of random sequences Dprobe = {x(m)}M m=1. We define the objective as the average attention probability mass assigned to the previous token position (t 1) across all positions and all sequences in the batch: fprev(θ) ="
        },
        {
            "title": "1\nM",
            "content": "(cid:32) (cid:88) m="
        },
        {
            "title": "1\nT − 1",
            "content": "T (cid:88) t=2 (cid:33) A(ℓ,h) t,t1(x(m)) . (15) Here, A(ℓ,h) t,t1(x(m)) denotes the attention score paid by the head to the token at 1 for the m-th sequence. By averaging over random content, we ensure the gradient fprev encourages the formation of the specific positional circuit (i.e., attending to 1 offset) regardless of the token identities. Parameter Subspace θprev: Since the attention pattern depends exclusively on WQ and WK, we define the active parameter subspace as the concatenation of these two matrices: θprev = vec([WQ; WK]) R2dmodeldk . Influence Score: The influence of training sample on the previous token head is computed as: Iprev(z) = θprevfprev(θ)H 1 EK-FAC(θprev)θprev L(z, θ). (16) (17) Here, the gradient θprev fprev captures how the weights must change to sharpen the attention onto the previous token, while the Hessian accounts for the curvature of the joint Q-K manifold. Case 2: Induction Heads (End-to-End Copying Mechanism). An induction head coordinates attention pattern formation (via Q, K) and content movement (via V, O). While the output is differentiable function of all four matrices via the chain rule, we adopt block-wise approximation for computational efficiency. Target Objective find (Averaged over Probes): To measure the generalized induction capability rather than the memorization of specific tokens, we construct set of synthetic sequences Dprobe = {x(m)}M m=1. Each sequence follows the structure [I, A, . . . , B, . . . , A], where and are distinct random tokens. The objective function is defined as the average log-likelihood of predicting the correct copy target across these sequences: find(θ) = 1 (cid:88) m=1 log Pθ(x(m) +1 = B(m) x(m) 1:T ). (18) Averaging gradients over multiple diverse probes ensures that the computed influence reflects the abstract mechanism of copying, reducing noise from token-specific embeddings. Block-wise Decomposition: This decomposition implies block-diagonal assumption for the Hessian. We justify this independence because we have already explicitly captured the strongest parameter couplingthe multiplicative query-key interactionwithin the joint θQK block, rendering the remaining second-order cross-correlations between the pattern and content pathways negligible for attribution purposes. Following this assumption, we decompose the parameter space into three orthogonal subspaces: θQK = vec([WQ; WK]) for pattern formation, θV = vec(WV ) for value content, and θO = vec(WO) for output projection. Aggregated Influence Score: The influence of training sample is the sum of the influence scores computed independently within these subspaces: Iind(z) = IQK(z) + IV (z) + IO(z). (19) Notably, we concatenate rather than multiply WQ and WK because the EK-FAC approximation is strictly derived for linear transformations of the form = x. The concatenated projection Wjoint = [WQ; WK] preserves this linearity with respect to the input x, allowing for valid Kronecker factorization of the curvature (A S). In contrast, formulating the influence in terms of the effective product matrix Wef = WK would render the attention scores quadratic with respect to the underlying parameters. This would violate the fundamental assumption of K-FAC, which relies on the gradient structure of linear layers, and would incorrectly model the optimization landscape of the actual trainable weights. 14 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Regarding WV and WO, although they theoretically form composite linear map (cid:80) A(WOWV x) due to the linearity of summation, we analyze them as distinct blocks for two reasons. First, from statistical perspective, WV operates on raw token embeddings, while WO operates on aggregated context vectors post-attention. Fusing them would force the curvature approximation to rely solely on token-level covariance, ignoring the significant distributional shift and rank reduction caused by the attention mixing mechanism. Second, from multi-head architecture perspective, WO acts as global integration interface that projects the heads subspace back to the residual stream. By maintaining its separation, we allow the Hessian to capture the specific curvature of this re-projection step, which is distinct from the feature extraction role of WV . B. From the perspective of Information geometry B.1. Proposition 1: Gradient Structure of Induction Heads Proposition 1. Consider simplified single-head attention mechanism where the induction capability is measured by the attention probability assigned to target induction token at index given query at index (where = k). The gradient of this objective with respect to the joint query-key parameter subspace θQK has rank-1 outer-product structure between the query-side and key-side signals, weighted by the softmax residual ([j = j] αtj). Proof. Let the pre-softmax attention score between query position and key position be denoted as stj. Under the joint parameterization WQK = [WQ; WK] and fixed input representations x, the score is given by the bilinear form: The attention probability αtj is obtained via the softmax function: stj = 1 dk (WQxt)(WKxj). αtj = exp(stj) i=1 exp(sti) (cid:80)t . We define the induction objective find as the log-likelihood of attending to the correct previous token j: find = log αtj = stj log (cid:88) i=1 exp(sti). (20) (21) (22) To find the influence direction, we compute the gradient with respect to the query parameter WQ (the derivation for WK is symmetric). Using the chain rule: Noting that we substitute this back: WQfind = = (cid:88) j=1 (cid:88) j=1 find stj stj WQ ([j = j] αtj) stj WQ . WQstj = 1 dk (WKxj) , WQfind = = = (WKxj ) αtj(WKxj) t (cid:88) j=1 WK xj αtjxj (cid:88) j=1 WK (xj Ejαt[xj]) . 1 dk 1 dk 1 dk 15 (23) (24) (25) (26) (27) (28) Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units If we consider the joint QK parameter block θQK, the resulting gradient inherits rank-1 outer-product form: query-side factor (proportional to xt or WQxt) multiplied by key-side residual factor (proportional to xj Ejαt[xj] or its mapped version under WK). In particular, when the softmax competition term is locally well-approximated as slowly varying (e.g., under small perturbations that do not substantially change the mass on competing keys), the ascent direction that tends to increase αtj is aligned with directions that increase stj relative to the other stjs. Under such local approximation, the component of the update that most directly increases the target score satisfies WQK stj vec(xt xj ) (up to the appropriate linear maps induced by WQ and WK). Thus, the influence score I(z) = indH 1L(z) (29) (30) can be interpreted as prioritizing training samples whose loss gradients L(z) have large projection onto the (whitened) directions emphasized by find within the QK block, i.e., directions spanned by rank-1 interactions between the query-side and key-side factors. Empirically, in many settings, such alignment is often associated with samples exhibiting token-to-token correspondences that resemble the probes inductive pattern (e.g., local repetitions or copy-like structures). B.2. Proposition 2: Influence as Riemannian Projection Proposition 2. The component-specific influence score I(z) can be expressed as an inner product between the capability gradient and the sample gradient on the Riemannian manifold of statistical distributions, equipped with the Fisher Information metric. Proof. Let = {pθ : θ Θ} be the manifold of probability distributions parameterized by the neural network weights θ. The local geometry of this manifold is defined by the Fisher Information Matrix (FIM), G(θ), which serves as the Riemannian metric tensor: G(θ) = ExD, ypθ(x) (cid:2)θ log pθ(yx)θ log pθ(yx)(cid:3) . (31) Under the standard regularity conditions discussed previously (e.g., in well-specified models and/or near likelihood optima, where the curvature of the negative log-likelihood is well-approximated by Fisher-type metrics), we use the approximation G. Standard Euclidean gradient descent updates parameters as θt+1 = θt ηL. However, the steepest descent direction on the probability manifold, which minimizes the KL-divergence, is given by the Natural Gradient : = G(θ)1L. (32) In our influence framework, we seek to quantify the impact of sample on the target capability . The first-order Taylor expansion of under perturbation in the direction of the natural gradient of the loss L(z) is: δf θf, δθEuclidean = (cid:10)θf, 1θL(z)(cid:11) θf G(θ)1θL(z). (33) (34) (35) We can rewrite this inner product using the Riemannian metric. Let u, vG = uGv denote the inner product on the tangent space TθP. The influence score becomes: I(z) (cid:10)θf, G1θL(z)(cid:11) = (cid:10)G1θf, G1θL(z)(cid:11) (cid:68) f, L(z) = (cid:69) . (36) (37) (38) Conclusion: The influence score is the negative inner product of the natural gradients of the mechanism probe and the training sample loss on the statistical manifold. By using EK-FAC to approximate G1, we approximately whiten the parameter space, which can improve conditioning and reduce sensitivity to certain parameter scalings, thereby emphasizing the intrinsic alignment between the probe gradient and sample gradients within the chosen metric. 16 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units C. Induction Attention Score and Formation Time Input construction. Given tokenized sequence x1:L, we form repeated input by concatenating identical copies of the same sequence: 1:L x(2) x(1) 1:L x(R) 1:L . We evaluate the attention pattern of target head (ℓ, h) on this repeated input and extract an induction-relevant stripe between consecutive repeats. Attention pattern. Let A(ℓ,h) [0, 1]T denote the attention pattern (post-softmax attention weights) of head (ℓ, h) at parameters θ, where = RL is the length of the repeated sequence. For given repeat index {2, . . . , R}, we consider the query positions in the r-th block and key positions in the (r 1)-th block. θ Stripe extraction (diagonal with offset). For each {2, . . . , R}, define the query range and key range Qr = {(r 1)L + 1, . . . , rL}, Kr1 = {(r 2)L + 1, . . . , (r 1)L}. We extract the attention sub-matrix Br = A(ℓ,h) θ [Qr, Kr1] [0, 1]LL, and summarize induction behavior by the mean attention mass on its diagonal stripe with fixed offset (e.g., offset = 1 for strictly repeated sequences): s(ℓ,h) (θ) = 1 (cid:88) (Br)i, i+, iI = 1, where = {1, . . . , } indexes valid diagonal entries. Intuitively, this measures whether tokens in the current repeat attend to the corresponding shifted positions in the previous repeat, which is characteristic of induction-style copying. Dataset-level induction attention score. We aggregate across repeats and across dataset of sequences to obtain single scalar score per head and checkpoint: s(ℓ,h) ind (θ) = ExD (cid:34) 1 1 (cid:88) r=2 (cid:35) s(ℓ,h) (θ) . This score is directly computed from attention patterns and does not require additional supervision beyond the input sequences. Since we focus on early-stage emergence, we use fixed repetition factor = 2 throughout. Concretely, we first sample base sequence length uniformly at random from the range [8, 20] (in tokens), then draw length-L token sequence from the model vocabulary, and finally duplicate it once to form length-2L repeated input. We compute the diagonal-stripe attention score on the cross-repeat block of the resulting attention pattern. We report the dataset-level score by averaging over 100 independently constructed repeated sequences. Definition of Formation Window. Let {θt}T t=0 denote the sequence of training checkpoints. We define the critical formation window for an induction head (ℓ, h) as the interval [tstart, tend] encompassing its phase transition. Specifically, tstart is identified as the checkpoint where the induction score s(ℓ,h) ind (θt) diverges from the early-training noise floor (empirically 0.1). Correspondingly, tend is defined as the point where the score reaches functional sufficiency threshold. In our experiments, this window captures the sharp ascent of the induction score from its initial baseline to stable level of 0.40.5, representing the distinct period during which the copy-paste mechanism is acquired. Temporal Localization of Phase Transition Mechanistic components in LLMs often exhibit distinct developmental trajectories, characterized by sudden phase transition rather than gradual improvement. Attributing data outside this critical developmental window introduces noise from unrelated model behaviors. Formally, for target mechanism M, we define monitoring metric µ(t) that quantitatively reflects the mechanisms maturity at training step t. By tracking µ(t) throughout the training trajectory, we identify the critical interval [tstart, tend] where the mechanism emerges most rapidly. Our influence analysis is strictly constrained to the model checkpoints within this window. 17 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Table 3. Instantiation of the MDA Framework for Target Mechanisms. We define distinct monitoring metrics µ, probe functions fprobe, and parameter subspaces θsub tailored to the specific nature of different interpretable units. Mechanism Monitoring Metric (µ) Probe Function (fprobe) Subspace Projection (π) Induction Heads Induction Score (Prefix matching score on validation set). The critical window is defined where this score rises sharply. Log-Likelihood of the correct copypaste token in synthetic repeated bigrams. Rationale: Measures the end-to-end information transmission capability. WQ, WK, WV, WO (Full Head) Rationale: Capture the coordination between attention pattern formation and value movement. Previous Token Heads Previous-Token Score (Average attention probability to token at 1). Attention Score allocated to the preceding token (t 1) on random sequences. Rationale: Isolates the specific attention pattern formation. WQ, WK (Query-Key Interaction) Rationale: The mechanism is primarily determined by local positional addressing. MLP Neurons Activation Score (Maximum or mean activation of neuron on trigger patterns). Neuron Activation on specific trigger inputs. Rationale: Directly measures the neuron selectively whether responds to its hypothesized feature. SAE Features Feature Activation Score (Average or peak activation of latent feature on trigger inputs). Reconstruction Fidelity or latent activation magnitude for feature k. Rationale: Tests whether the latent feature reliably encodes the targeted concept. Win[:, i], Wout[i, :] (Input/Output Weights of Neuron i) Rationale: These parameters fully determine how the neuron reads from and writes to the residual stream. Wenc[:, k], Wdec[k, :] (Encoder/Decoder Weights of Feature k) Rationale: These weights define how the feature is extracted from and injected into model activations. D. Detailed Framework Instantiation and Extensions In this section, we provide the precise specifications used to instantiate the MDA framework for the mechanisms analyzed in the main text. We also discuss how the framework can be generalized to other interpretable units. D.1. Instantiation for Attention Heads Table 3 summarizes the mapping between the abstract framework components and the concrete properties of Induction Heads and Previous Token Heads. D.2. Mechanistic Data Attribution (MDA) Algorithm 1 provides the full procedural details for the MDA calculation. E. Detailed Experimental Setup E.1. Model Training and Checkpointing To accurately capture the rapid phase transitions of mechanistic components, relying on standard open-source checkpoints (which are typically saved at coarse intervals, e.g., every 1000 or logarithmic steps) is insufficient. Therefore, we trained the first four sizes of the Pythia suite (14M, 31M, 70M, 160M) from scratch. We strictly followed the official architecture and training hyperparameters provided by Biderman et al. (2023) to ensure our models are representative of the standard Pythia suite. The primary difference lies in our checkpointing strategy: we saved model states at much higher frequency during the critical formation windows identified for each mechanism. All layer and head indices reported in this paper and the following tables follow 0-based indexing convention. Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Algorithm 1 Mechanistic Data Attribution (MDA) 1: Input: Pretrained Model parameters θ, Target Component Subspace θsub (e.g., WQK of Head L.H), Synthetic Probe Data Dsyn, Training Dataset Dtrain. 2: Output: Ranked Training Examples sorted by influence. 3: // Phase 1: Unit-Specific Curvature Estimation 4: Construct the EKFAC (via Equation (14))approximate inverse Hessian operator ˆH 1 θsub 5: Note: The curvature is strictly restricted to the parameters in θsub to filter out noise from other components. 6: // Phase 2: Compute Mechanism Influence Vector 7: Calculate the gradient of the mechanism-specific probe function on synthetic data: 8: gprobe θsubfprobe(Dsyn, θ) 9: Compute the Inverse Hessian Vector Product (IHVP) effectively projecting the probe direction onto the data manifold: 10: vIHV ˆH 1 θsub 11: // Phase 3: Score Training Data 12: for each training sample zi Dtrain do 13: estimated on subset of Dtrain. gprobe 14: 15: Compute gradient on training loss restricted to subspace: g(i) train θsub Ltrain(zi, θ) Calculate influence score (projection): si (g(i) train)vIHV 16: 17: end for 18: return Top-K samples with highest scores si E.2. Configuration for Mechanistic Data Attribution We provide the detailed hyperparameters used for the Mechanistic Data Attribution (MDA) framework in Table 4 (Induction Heads) and Table 5 (Previous Token Heads). The parameters include: Target Component: The specific Layer and Head index identified as the primary driver for the mechanism. EKFAC Configuration: The range of training steps [tstart, tend] and batch size used to estimate the covariance matrices ( ˆH 1). Analysis Scope: The total number of training samples (Num) scanned to compute influence scores. Intervention Settings: The specific training step where data augmentation was performed and the number of top-ranked samples (Top-K) selected for these interventions. E.3. Configuration for Mechanistic Data Augmentation For the Mechanistic Data Augmentation experiments described in Section 6, we generated synthetic datasets based on the patterns extracted from the 14M model. To ensure fair comparison, the position of synthetic data inserted was controlled. The specific insertion configurations are listed below: 14M: Insert 100,000 synthetic samples at step 900. 31M: Insert 20,000 synthetic samples at step 800. 70M: Insert 20,000 synthetic samples at step 700. 160M: Insert 10,000 synthetic samples at step 600. Note that for larger models (31M-160M), we used smaller volume of synthetic data compared to the natural data top-k insertion. This strict setting further validates the high causal density of the generated mechanistic patterns. As for the impact of the specific insertion quantity, we present detailed investigation in Appendix F. 19 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Table 4. Experimental Configuration for Induction Heads. The analysis covers the critical formation window specific to each model size. Influence scores were computed over comprehensive set of samples (Num) without downsampling. Model Layer Head Training Steps EKFAC Batch Size Analyzed Samples (Num) Selected Top-K Insertion Step 14M 1000-1999 31M 0-1199 70M 0-999 160M 0-799 For the 160M masking experiment, the exclusion count was set to 80,000. 1,024,000 1,228,800 1,024,000 819, 10 8 6 4 3 3 3 10 3 4 4 5 100,000 120,000 100,000 100,000 900 800 700 600 Table 5. Experimental Configuration for Previous Token Heads. Similar to induction heads, specific layers and heads were targeted based on the Previous-Token Score. Model Layer Head Training Steps EKFAC Batch Size Analyzed Samples (Num) Selected Top-K Insertion Step 14M 1,228,800 31M 1,126,400 70M 921,600 160M 716,800 For the 160M masking experiment, the exclusion count was set to 10,000. 0-1199 0-1099 0-899 0-699 2 0 3 10 10 8 6 4 2 3 3 4 100,000 110,000 80,000 70,000 500 800 600 500 E.4. Hardware Configuration All experiments were conducted on machines equipped with eight NVIDIA A100 GPUs. In total, approximately 800 GPU-hours were consumed for training and evaluation. F. Ablation Study on Insertion Dynamics F.1. Experimental Design We designed factorial experiment involving three data types, four quantity levels, and two scheduling strategies, resulting in total of 25 experimental runs (including the baseline): Data Types: 1. Real High-Influence: Top-ranked natural samples identified by MDA. 2. Synthetic Pattern-Based: Data generated via the pipeline described in Section 6.1. 3. Random Control: Randomly selected training samples. Insertion Quantities (N ): We tested four volume levels: 12,500, 25,000, 50,000, and 100,000 samples. Insertion Schedules: 1. Concentrated Injection (Burst): All samples are inserted immediately after step 900. 2. Dispersed Injection (Uniform): The samples are distributed uniformly across the interval from step 900 to 1400. G. Extended Training Dynamics and Window Selection In our main causal verification results (Section 4), the induction score trajectories occasionally exhibit slight fluctuations or minor decline after reaching their peak intensity. We devote this section to clarifying that this behavior is natural characteristic of the models optimization landscape rather than an artifact of our data interventions. 20 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Figure 6. Extended Training Dynamics of Pythia 14M. From this figure, we can more clearly observe that the critical window we defined is highly pronounced. After step 2000, the score begins to exhibit small-scale fluctuations. We believe that the induction circuit has already been formed at this stage, and subsequent changes involve other trade-offs. Therefore, this regime is not further considered in the present study. This also accounts for the slight decline observed in the the latter part of the curve in Figure 2. G.1. Post-Peak Fluctuations To provide context for the local behaviors observed in the critical window, we visualize the extended training trajectory of the 14M model, tracking the prefix matching score (Induction Score) from step 0 to step 3000 (Figure 6). As illustrated, the induction mechanism undergoes dramatic phase transition between step 1200 and 2000. However, after this saturation point, the score does not remain perfectly monotonic. Instead, it naturally exhibits minor undulations and stabilization phases. This phenomenon likely arises from the complex interplay between competing optimization objectives: as the model begins to focus on minimizing loss for other linguistic features (e.g., complex syntax or factual knowledge), the parameters associated with the induction circuit may undergo slight adjustments, leading to the observed fluctuations. Therefore, the minor drops observed in our intervention experiments are consistent with the baseline dynamics. G.2. Justification for the Critical Window Given the prohibitive computational cost of the Mechanistic Data Attribution (MDA) frameworkwhich requires constructing high-dimensional curvature estimations and computing per-sample gradientsit is intractable to perform dense influence analysis over the entire training lifecycle. Consequently, we strategically defined the Critical Window to encompass the period of maximum causal density: the phase transition where the mechanism originates. As verified by the extended trajectory, this window captures the most significant derivative of capability gain. Focusing our resources on this interval ensures that we identify the formative drivers of the mechanism, which is the primary research question of this work. H. Validation via Head-Specific Ablation Contribution In the main text, we primarily monitored the Prefix Matching Score to track the emergence of induction heads. While this metric effectively captures the formation of the attention pattern, it is observational. To rigorously verify that these attention patterns causally translate into correct next-token predictions, we introduced complementary interventional metric: Head-Specific Ablation Logit Contribution. H.1. Task Definition and Metric Calculation We evaluate the heads contribution on synthetic Induction Task designed to strictly test the copy-paste capability. Specifically, we construct sequences with the structure: [Prefix] [A C] [Gap] [A B] predict Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Figure 7. Logit differences of Pythia-14M on the synthetic task.Although logit difference and induction score are defined in fundamentally different ways, we observe striking consistency between them: their ordering is fully aligned with the previously observed ordering of the prefix matching score. We interpret this mutual agreement as strong evidence that our method is reliable and robust. where [A C] represents unique random token pattern, and the model must rely on the context to predict C. For target head (e.g., Layer 3 Head 3 for the 14M model), we quantify its contribution as the drop in the correct tokens logit when the head is functionally removed (zero-ablated). Let ℓ(Cx) be the logit of the correct token given input x. The ablation score is defined as: = ℓclean(Cx) ℓablated(Cx, 0) (39) positive indicates that head positively contributes to the correct prediction. H.2. Consistency of Results We tracked this ablation score throughout the training process across our experimental configurations. As shown in Figure 7, the trajectory of the Ablation Logit Contribution exhibits consistency with the observational Prefix Matching Score used in our main experiments. Both metrics capture the same phase transition interval and respond identically to our data interventions (Deletion and Augmentation). This alignment confirms that the Induction Score is robust proxy: the identified heads are not merely attending to the correct history but are the causal drivers pushing the correct logits for in-context learning. I. Full Distribution of Influence Scores In our main analysis, we focused primarily on the training examples with high positive influence scores, identifying them as the active drivers for the induction mechanism. In this section, to ensure comprehensive understanding, we present the full spectrum of influence scoresincluding samples with negative influence (opponents)using the 14M model as representative case study. I.1. Net Positive Drive We aggregated the influence scores for all training samples within the critical formation window. The global distribution of all samples via all suites is visualized in Figure 8. Crucially, we observe an asymmetry in magnitude: while there exists subset of data that exhibits negative influence (theoretically hindering the mechanism), the cumulative sum of positive influence substantially exceeds the absolute sum of negative influence: max(0, I(z)) > (cid:88) zD min(0, I(z)) (cid:88) zD 22 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Figure 8. The overall score distributions across all samples for all four models. All four models exhibit power-law behavior with an exponent close to 3. Moreover, the distributions are consistent across individual small bins, showing uniform pattern. This net positive drive provides the fundamental thermodynamic explanation for the circuits emergence: despite conflicting gradients from various samples, the dataset on aggregate provides dominant coherent signal favoring the formation of the induction heads. I.2. Robustness of Temporal Uniformity Furthermore, we examined the temporal properties of this distribution. Consistent with the uniformity observation in Section 5.3, we find that this net positive ratio is maintained steadily throughout the critical window. Even when analyzing the data at varying temporal granularities (i.e., using different bin sizes or continuous sliding windows, rather than the specific intervals used in the main text), the distribution of influence mass remains remarkably uniform. This indicates that the force driving the phase transition is continuous, constant pressure applied by the data distribution, rather than transient shock caused by specific anomaly batch. This goes along with the lottery ticket hypothesis for induction circuit proposed in Nanda et al. (2023). 23 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units J. Implementation Details of Mechanistic Data Augmentation Pipeline In this section, we detail the engineering pipeline used to operationalize the Mechanistic Data Augmentation strategy. The process is fully automated, converting raw high-influence training samples into executable data generation scripts via three-stage workflow: (1) Sample Mining, (2) Pattern Extraction, and (3) Generator Implementation. J.1. Stage 1: Mining High-Influence Samples We first extract the raw textual content of the samples identified by the MDA framework. As implemented in our data processing script, the procedure is as follows: 1. Ranking: We load the influence analysis results (Pickle format) from the 14M model and sort all training examples based on their projection scores in descending order. 2. Filtering: We select the Top-K (where =2000) samples that exhibit the strongest positive influence. 3. Decoding: We decode the corresponding token indices back into human-readable text strings. These raw texts serve as the seed data for pattern extraction. J.2. Stage 2: LLM-Driven Pattern Extraction To convert unstructured raw text into structured rules, we employ DeepSeek-V3 as pattern extraction engine. We feed the mined text samples into the LLM with specialized system prompt: System Instruction: You are an expert in linguistic pattern recognition. Analyze the provided text samples and extract their underlying structural templates. Ignore specific semantic content and focus on the fixed mechanistic structure. Output Requirement: Return valid JSON object with the following schema: { } \"pattern_id\": \"Unique identifier (e.g., p001)\", \"pattern_name\": \"Short descriptive name\", \"anchor_tokens\": [\"List of invariant strings, e.g., Chapter, Step\"], \"fields\": [ { } \"name\": \"Variable placeholder name used in template\", \"type\": \"Type of content (e.g., fixed_list, random_text)\", \"values_or_rules\": [\"List of options\"] or \"Description of generation rule\" ], \"template\": \"Global format string with placeholders (e.g., {anchor})\", \"length_control\": \"Constraints to match the original token length\" This step produces merged JSON registry containing definitions for all identified mechanistic templates. For 14M, it successfully extracted around 900 unique (deduplicated) patterns. J.3. Stage 3: Automated Generator Implementation In the final stage, we automatically convert the static JSON schemas into executable Python generation functions. This is achieved through meta-programming script that orchestrates the following steps: Meta-Prompting for Code Generation. We iterate through each pattern in the JSON registry and construct prompt for DeepSeek-V3. The prompt explicitly requires the model to write robust Python function that satisfies the schema constraints. The core prompt template used is: System: You are Python code generation expert. User: Please write Python generation function for the following data pattern. Requirements: 24 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units 1. Function Name: generate <pattern id> <name> 2. Fields Rule: Strictly generate data based on the values or rules and type defined in the fields list. 3. Template Structure: The output string must strictly follow the defined template. 4. Length Control: Implement looping logic to ensure the output length approximates target tokens. Pattern Definition JSON: <Insert JSON Content> K. Observations on Multi-Head Interaction and Circuit Evolution In our primary analysis, we focused on the single strongest induction head to establish clear causal link between training data and mechanism formation. However, induction circuits are rarely composed of isolated components; they often involve distributed set of heads working in concert. In this final section, we briefly discuss our preliminary observations regarding multi-head interactions and their evolution across model scales. K.1. Stability in Small Models (14M) For the 14M model, we extended our analysis to monitor secondary induction heads (those with lower but significant induction scores) under the same intervention settings. We observed that (Figure 9) while the magnitude of strengthening or weakening varies among these headssome are highly sensitive to data interventions while others are more resistanttheir functional identity remains stable. Heads originally classified as induction heads consistently retain their copy-paste behavior throughout the training and intervention processes, and non-induction heads remain functionally distinct. This suggests that in smaller architectures, the circuit topology is relatively static and rigid. K.2. Circuit Reconfiguration in Larger Models (160M) In contrast, divergent behavior emerges in larger models, such as the 160M. Here, we observed instances where specific heads, initially identified as part of the induction circuit, completely ceased to exhibit(the one that shows decrease of 0.26 in induction scores) induction behavior under certain interactive conditions or seemingly handed off their role to other components. This phenomenon implies that as model scale increases, the underlying circuit structure may undergo form of dynamic reconfiguration. The functional role of specific head is not as fixed as in the 14M model; instead, the circuit may evolve more fluid topology where responsibilities are redistributed among larger pool of redundant heads. While we hypothesize that this reflects sophisticated evolution in how larger models organize their internal mechanisms, detailed characterization of these complex multi-head dynamics remains outside the scope of this work. We present this observation as an open question to stimulate future research into the scaling laws of circuit topology. L. Qualitative Inspection of High-Influence Samples To provide concrete intuition regarding the data drivers identified by the MDA framework, we present qualitative inspection of the top-ranked training samples. As discussed in Section 5.1, our analysis indicates that induction heads are primarily driven by data containing long-range repetitive structures. Table 6 and Table 7 displays three representative high-influence examples explicitly mined from the 14M models training corpus. These samples, selected from the top 0.1% of the influence distribution, span diverse modalities including structured code (XML/Base64), raw binary-like sequences, and domain enumeration lists. Despite their superficial differences in format, they all share robust mechanistic signature: specific pattern or token sequence (highlighted in bold) appears in the context and is repeated after variable interval, providing strong copy-paste supervision signal for the induction circuit. Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Figure 9. The difference of induction scores among all heads for Pythia 14M and Pythia 160M. For the 14M model, the maximum difference is limited to around 0.1, and the overall direction of change is largely consistent across heads. In contrast, the 160M model exhibits form of compensatory behavior: while certain heads are significantly weakened, others are simultaneously strengthened. This suggests that, in larger models, interactions among heads are considerably more complex. 26 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Table 6. Representative High-Influence Training Samples. We showcase three actual top-ranked examples mined from the training corpus. They exhibit distinct long-range repetitive structures (highlighted in bold) across different modalities. Type XML Code Sample Content (Truncated) lc1dOU0NvbG9yohIUWE5TT2JqZWN0XxAPTlNLZXllZEFyY2hp dmVy0RcYVHJvb3SAAQgRGiMtMjc7QUhOW2KMjpCVoKmxtL3P0tcAAAAAAAABAQAAAAAA AAAZAAAAAAAAAAAAAAAAAAAA2Q== </data> <key>ANSIBrightBlueColor</key> <data> YnBsaXN0MDDUAQIDBAUGFRZYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3AS AAGGoKMHCA9VJG51bGzTCQoLDA0OVU5TUkdCXE5TQ29sb3JTcGFjZVYkY2xhc3NPEBww LjQwNzg0MzEzNzMgMC44MzUyOTQxMTc2IDEAEAKAAtIQERITWiRjbGFzc25hbWVYJGNs YXNzZXNXTlNDb2xvcqISFFhOU09iamVjdF8QD05TS2V5ZWRBcmNoaXZlctEXGFRyb290 gAEIERojLTI3O0FITltigYOFipWepqmyxMfMAAAAAAAAAQEAAAAAAAAAGQAAAAAAAAAA AAAAAAAAAM4= </data> <key>ANSIBrightCyanColor</key> <data> YnBsaXN0MDDUAQIDBAUGFRZYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3AS AAGGoKMHCA9VJG51bGzTCQoLDA0OVU5TUkdCXE5TQ29sb3JTcGFjZVYkY2xhc3NPEBww Ljc4MDM5MjE1NjkgMSAwLjk5MjE1Njg2MjcAEAKAAtIQERITWiRjbGFzc25hbWVYJGNs YXNzZXNXTlNDb2xvcqISFFhOU09iamVjdF8QD05TS2V5ZWRBcmNoaXZlctEXGFRyb290 gAEIERojLTI3O0FITltigYOFipWepqmyxMfMAAAAAAAAAQEAAAAAAAAAGQAAAAAAAAAA AAAAAAAAAM4= </data> <key>ANSIBrightGreenColor</key> declare(strict_types=1); namespace PhpSlangMatchWhen; class TypeOf extends AbstractWhen { /** * @param $subject * * @return bool */ public function matches($subject): bool { } } Logs return $subject instanceof $this->case; AF_22VFbhaqGevttreUjFreivprRRR,%object b__MGPA7naqebvq23OaFbhaqGevttreUjFreivprR0_AF_11OaVagresnprVAF_22VFbhaq GevttreUjFreivprRRR: .space __SIZEOF_POINTER__ .text .globl b__MA7naqebvq12FbhaqGevttre12thvqGbFgevatRCX12nhqvb_hhvq_fCpz .type b__MA7naqebvq12FbhaqGevttre12thvqGbFgevatRCX12nhqvb_hhvq_fCpz, %function b__MA7naqebvq12FbhaqGevttre12thvqGbFgevatRCX12nhqvb_hhvq_fCpz: nop .data .globl b__MGIA7naqebvq14OcFbhaqGevttreR .type b__MGIA7naqebvq14OcFbhaqGevttreR,%object b__MGIA7naqebvq14OcFbhaqGevttreR: .space __SIZEOF_POINTER__ .data 27 Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units Table 7. Representative High-Influence Training Samples. We showcase three actual top-ranked examples mined from the training corpus. They exhibit distinct long-range repetitive structures (highlighted in bold) across different modalities. Type LaTeX Sample Content (Truncated) Q: Simple example of product not preserving coequaliser in $mathbf{Top}$ In the category of topological spaces ($mathbf{Top}$), products do not always preserve colimits. If they did then $mathrm{Hom}_mathbf{Top} (-times X,S)$would be representable and hence $mathbf{Top}$ would be Cartesian closed(which itisnt). think that products do preserve coproducts, so it must be thattheres some coequaliser which products dont preserve. Im trying to understand why this is in more concrete terms, but Ive struggled to find simple example that can examine in detail. What are some simple spaces $A$, $B$ and $X$ and maps $f,g:Ato B$ in $mathbf{Top}$ such that the product of $X$ with the coequaliser is different from the coequaliser of the products? The same question for the category of locales is here. A: (Adapted from Ronald Browns Topology and Groupoids, Section 4.3 Example 4, Page 111.) Consider $mathbb{Z}$, $mathbb{Q}$ and $mathbb{R}$ with their usual topologies. Let $i:mathbb{Z}hookrightarrowmathbb{R}$ be the usual inclusion, and define $j :mathbb{Z}tomathbb{R}$ by $j(n) = i(n+1)$. Our example of failed preservation will be that the canonical map $$mathrm{coeq}(itimesmathbb{Q},jtimesmathbb{Q})tomathrm{coeq} (i,j)timesmathbb{Q}$$ is not homeomorphism. [\"44.2094250\",\"28.6460842\",\"Law\",\"Bachelor\",\"Romanian\",\"Spiru Haret University\",\"?we=module.fp.searchStudy0026ddpN=10904236920026igfm=goto Overview 0026iemuProgramStudiuId=14420026wtok=0026wtkps=hZDRDoIwDEXZe8 oa7d1q 9gTPwCDBI0IkQ0izH+u2BkOBcY29PbptTsFT86BlZ9IdSrHpWOQuoqmvjdxacIz hewNfe+ 5OpdSbvRSuvu+xSFNjTyEvDoiy782a71AiKjAEYc8cimoc7I7dOUBTaQDmdW0N k34vxrTjRn8IkOHciOYcWdU4T+pNMnUnwq1OSdAa1mtE4CZ0pcDYJiNKCJPo4iuZg8j1ZZ LIaLKALgiKk2AyAQILv59eaNrydtovqm5Rt82QPl8=0026wchk=e4dd399345f259e2ca15b 9b9ddd178e5e24c86ca\", \"Law\",\"Faculty of Legal and Economic Sciences\"], [\"44.2094250\",\"28.6460842\",\"Law\",\"Bachelor\",\"Romanian\",\"Spiru Haret University\",\"?we=module.fp.searchStudy0026ddpN=10904236920026igfm=goto Overview 0026iemuProgramStudiuId=14420026wtok=0026wtkps=hZDRDoIwDEXZe8 oa7d1q 9gTPwCDBI0IkQ0izH+u2BkOBcY29PbptTsFT86BlZ9IdSrHpWOQuoqmvjdxacIz hewNfe+ 5OpdSbvRSuvu+xSFNjTyEvDoiy782a71AiKjAEYc8cimoc7I7dOUBTaQDmdW0N k34vxrTjRn8IkOHciOYcWdU4T+pNMnUnwq1OSdAa1mtE4CZ0pcDYJiNKCJPo4iuZg8j1ZZ LIaLKALgiKk2AyAQILv59eaNrydtovqm5Rt82QPl8=0026wchk=e4dd399345f259e2ca15b 9b9ddd178e5e24c86ca\", \"Law\",\"Faculty of Legal and Economic Sciences\"], AczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgA czgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgA czgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgA czgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgA czgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgA czgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgAczgA czgAczgAczgAczgAczgAczgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAABABgAAAAAAAAMAAAAAAAAAAAAAAAAAAAAAAAAczgAczgAczgAc 28 Database Meaningless"
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence, Beijing, China",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}