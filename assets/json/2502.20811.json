{
    "paper_title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
    "authors": [
        "Xiao Wang",
        "Jingyun Hua",
        "Weihong Lin",
        "Yuanxing Zhang",
        "Fuzheng Zhang",
        "Jianlong Wu",
        "Di Zhang",
        "Liqiang Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, \\textbf{HAICBench} includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 1 1 8 0 2 . 2 0 5 2 : r HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models Xiao Wang 1* Jingyun Hua 1* Weihong Lin 1* Yuanxing Zhang 1 Fuzheng Zhang 1 Jianlong Wu Di Zhang 1 Liqiang Nie 1Kuaishou Technology"
        },
        {
            "title": "Abstract",
            "content": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, HAICBench includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/ datasets/KuaishouHAIC/HAIC."
        },
        {
            "title": "Introduction",
            "content": "Multi-modal large language models have notably showcased their preeminence across various video understanding tasks (Chen et al., 2024d; Li et al., 2024a; Wang et al., 2024a; Zhang et al., 2024b). Among these, human action understanding plays critical role in many downstream applications, e.g., human-computer interaction (Hayes, 2011), autonomous driving (Xu et al., 2021), embodied intelligence (Gupta et al., 2021), and human video generation (Wang et al., 2024b). *Equal contribution. Work done as an intern at Kuaishou Technology. Corresponding author. 1 recent study, ShareGPT4Video (Chen et al., 2024a) has demonstrated that high-quality and detailed video captions can improve MLLMs performance in video understanding. However, most existing works (Soomro et al., 2012; Xu et al., 2016; Krishna et al., 2017; Wang et al., 2019; Chen et al., 2024c) provide only coarse captions for human actions, insufficient for understanding fine-grained behaviors. MotionLLM (Chen et al., 2024b) introduces the MoVid dataset with finegrained action captions from MotionX (Lin et al., 2023). Nevertheless, this dataset mainly focused on single-person scenarios. For multi-person situations, MoVid only considered consistent group activities like group of people performing the Korean dance. comprehensive dataset is essential to enhance MLLMs understanding of detailed human actions and interactions in both singleand multi-person contexts, critical for tasks like emotional analysis, motivation prediction, and relationship modeling. There are two main challenges for building such datasets: (1) Action Video Accumulating. How to automatically accumulate large-scale videos featuring clear actions of multiple individuals. (2) Caption formatting. How to define caption format that can clearly distinguish different people and detail their behaviors and interactions respectively. To address the above challenges, we propose novel data generation pipeline composed of In the video accumulation stage, two stages. we accumulate videos from various domains featuring clear, meaningful human actions and identify their specific timestamps. This process is highly selective, with only about 1% of videos meeting our quality criteria after applying various In the caption annotation stage, we strategies. define caption format that uses human attributes to distinguish individuals and chronologically annotates detailed body actions and interactions for each person (see Figure 1). With this pipeline, Figure 1: Our standardized caption format presents each individuals detailed attributes, body actions, and interactions in chronological order, making it easier to distinguish individuals and comprehend their behaviors. we curate two datasets: HAICTrain (Human Action and Interaction Comprehension Training set) and HAICBench (a benchmark for evaluation). HAICTrain contains 126K videos accumulated from WebVid (Bain et al., 2021), annotated in our defined caption format by Gemini-1.5-Pro (Team et al., 2024). HAICBench includes 500 YouTube videos with human-annotated captions in the same format. Furthermore, we generate 1,400 multiplechoice QA pairs across five categorieshuman interaction, action detail, action sequence, count, and human attributeby prompting GPT-4o (Hurst et al., 2024) and Gemini-1.5-Pro. Note that all the above machine annotation results undergo review and refinement by human annotators. Experimental results indicate that utilizing HAICTrain for training can remarkably enhance the models human action understanding ability by 1.4%- 2.1%. Additionally, in MovieGenBench (Polyak et al., 2024), our post-trained model surpasses the original model in GSB score of 2.15 and 6.81 in HunyuanVideo (Kong et al.) and Wanx2.1 (Aliyun, 2023), respectively. Our contributions can be summarized as follows: large-scale videos with clear actions from the Internet and 2) generates standardized captions that distinguish individuals and detail their actions and interactions. We introduce two datasets: HAICTrain which includes 126K generated-then-verified high-quality video-caption pairs for training; and HAICBench comprising 500 human annotated video-caption pairs and 1,400 QA pairs, designed to evaluate MLLMs human action understanding comprehensively. Experiments demonstrate that training with HAICTrain significantly improves human action understanding in benchmarks including MVBench (Li et al., 2024c), PerceptionTest (Patraucean et al., 2024), ActivityNet-QA (Yu et al., 2019) and HAICBench. Furthermore, HAICTrain substantially improves text-tovideo generation on MovieGenBench."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video Caption Datasets We propose novel data annotation pipeline to provide data that can facilitate human action understanding, which 1) accumulates Most existing video captioning datasets prioritized general video understanding (Xu et al., 2016; Zhou et al., 2018; Wang et al., 2019; Miech et al., 2019; 2 Category Interaction Action Details Action Sequence Count Attribute Example What gesture does the middle-aged woman make while talking to the other man? (A) Gestures with both hands clasped in front of her (B) Claps her hands (C) Waves her hands in the air (D) Points at the desk What does the man in the black hat do with his right hand as he starts skateboarding? (A) He waves it in greeting (B) He points down the slope (C) He places it on his hip (D) He keeps it in his pocket What does the man in the gray cap do immediately after gripping the barbell? (A) He looks at the camera (B) He adjusts his hat (C) He bends down and lowers the barbell (D) He walks towards his front left and rubs his hands together How many times does the man clap his hands? (A) Three (B) Four times (C) Once (D) Twice What color is the cropped jacket worn by the young female character in the video? (A) Pink (B) White (C) Blue (D) Black Table 1: Task examples of HAICBench, showcasing comprehensive human action understanding across spatial (action details), temporal (sequence, count), and multi-human interaction (interaction and attribute) aspects. Bain et al., 2021; Zellers et al., 2021; Lei et al., 2021; Xue et al., 2022; Chen et al., 2024a,c; Wang et al., 2024d,c; Xiong et al., 2024; Wang et al., 2024b). These datasets included only subset of human videos, and the action captions tend to be coarse. Some datasets specifically focused on human-centric captioning, including ActivityNetCaptions (Krishna et al., 2017), Ego4D (Grauman et al., 2022), Kinetic-GEB (Wang et al., 2022), ActionHub (Zhou et al., 2024) and OpenHumanVid (Li et al., 2024b). Different from these datasets, our work focuses on finer-grained human actions and interaction understanding. 2.2 Video Understanding Benchmarks Traditional video understanding benchmarks have primarily honed in fixed set of classes (Soomro et al., 2012; Shahroudy et al., 2016; Carreira and Zisserman, 2017; Goyal et al., 2017). Recently, there has been shift towards open-set video understanding, including video captioning and question answering (Xu et al., 2016; Wang et al., 2019; Xiao et al., 2021; Li et al., 2024c). While benchmarks like NeXT-QA (Xiao et al., 2021), MVBench (Li et al., 2024c), and PerceptionTest (Patraucean et al., 2023) include action-related questions, they primarily focus on general action recognition rather than finegrained details. TemporalBench (Cai et al., 2024) evaluates event-level action understanding using different protocol that requires distinguishing correct captions from negative ones. Actionspecific benchmarks, such as ActivityNet-Captions (Krishna et al., 2017), TGIF-QA (Jang et al., 2019), MoVid-Bench (Chen et al., 2024b), EGOBody (Zhang et al., 2022), and GRAB (Taheri et al., 2020), address temporal sequences or humanobject interactions but often overlook humanhuman interactions. Our HAICBench provides more comprehensive evaluation of human action understanding, covering five key aspects detailed in Table 1. 2.3 Multi-modal Large Language Models integrated visual MLLMs generally process multi-modal input information and generate language output. Kosmos (Huang et al., 2023) introduced an end-to-end framework that inputs with LLM from cohesive training. Flamingo (Alayrac et al., 2022) and InstructBLIP (Dai et al., 2023) merged visual and linguistic features through crossattention and Q-Former module, respectively. MiniGPT-4 (Zhu et al., 2024) and LLaVA (Liu et al., 2023) simplified the integration by linearly projecting visual features directly into the LLM embedding space. Recent studies focused on different aspects to enhance the above early attempts in MLLMs. Cosmos-2 (Peng et al., 2024) and NeXT-GPT (Wu et al., 2024) have expanded MLLM applications to broader multi-modal tasks. LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVAOneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into unified input format. Figure 2: Our data generation pipeline. (a) The video accumulation stage collects videos featuring clear human actions from the Internet. Based on this, (b) HAICTrain is curated through Gemini-1.5-Pro re-captioning, and (c) HAICBench is created by LLM-assisted human annotation."
        },
        {
            "title": "3 HAIC Data Pipeline",
            "content": "In this section, we detail our data generation pipeline, as illustrated in Figure 2. 3.1 Action Video Accumulation This pipeline aims to accumulate human videos featuring clear human actions and sufficient details from large-scale videos. Metadata Filtering. We begin by discarding lowresolution videos and those without verbs in their descriptions using spaCy (Honnibal et al., 2020). The remaining videos are split into short clips with unique scenes using SceneDetect (Castellano, 2024). We keep clips between 5 and 20s long, as actions typically occur within single scene. Human Existence Filtering. We uniformly sample 16 frames from each clip and use the RTMPose (Jiang et al., 2023) object detector to identify humans. Only videos where all frames contain 1-5 humans and the total bounding box area covers at least 10% of the frames are retained, ensuring sufficient human detail. Human Action Filtering. Using RTMPose (Jiang et al., 2023), we detect human bounding boxes and 17 body keypoints at 1 fps. Tracklets are constructed based on the maximal IoU between frames. We then filter out videos with static humans by ensuring the L1 distance between all adjacent keypoints exceeds 0.085, with keypoint coordinates normalized by the video resolution. This filtering captures clear human actions. However, we observe that 15% of the filtered videos still contain static humans despite large keypoint L1 distances. This often results from image gallery videos. camera movements or In these cases, we empirically find keypoints approximately follow an affine transformation (e.g., translation, scaling, rotation, and shear mapping). Based on this insight, we developed strategy to filter these videos. Formally, let the keypoint vector in frame be Pt R317, where 17 is the number of keypoints and 3 corresponds to homogeneous coordinates (height, width, 1). We assume keypoints in these videos generally adhere to the affine transformation: Pt+1 = TPt, (cid:35) (cid:34) 1 , (1) = 4 where R22, R21 is the transformation coefficients, and = [0, 0]. We solve the following least squares problems: min A,t Pt+1 TPt2, (2) and retain only those samples with residual value > 0.0016. larger residual indicates greater deviation from Equation 1, suggesting that the unwanted videos mentioned above. Overall, the whole action video accumulation step yields 0.31% to 1.3% of human action videos, depending on the video source. 3.2 HAICTrain We chose the WebVid-10M dataset (Bain et al., 2021) as the video source for training due to its large scale and high vision quality. Initially, we apply the action video accumulation pipeline in Section 3.1 to collect action videos from the WebVid-10M. This process results in collection of 126K videos, representing 1.2% of the original dataset. Then, we employ Gemini-1.5-Pro (Team et al., 2024) to generate captions in the standardized format as depicted in Figure 1 referring to the videos and original captions. The specific prompts used for this process are detailed in Appendix A. We then employ an additional judgement to filter out failure cases that do not follow the pre-defined format or display low quality. This judgement ensures the quality of our data in HAICTrain. 3.3 HAICBench We develop an LLM-assisted human annotation pipeline to create the HAICBench, which evaluates the capabilities of MLLMs in human action understanding. Human annotators first craft video captions following the format in Figure 1. To enhance question diversity, we then adopt LLMs to generate QA pairs based on these captions, which are then verified by annotators. Video Caption Annotation. To avoid potential overlap with public benchmarks, we choose the newly proposed Omega-multimodal dataset (OMEGA Labs, 2024) as our benchmark video source, which comprises over 30 million 2-minute video clips. We apply our video accumulation stage in Section 3.1 to obtain some video clips, and manually check these clips to obtain 500 video clips. Then, human annotators are required to describe the number of human subjects, noting static attributes (gender, age, clothing, accessories) identification, followed by body for subject movements, action sequences, and interactions with others in chronological order. To ensure the quality of annotated captions, we train all annotators for one week. Besides, each video is first annotated by 1 annotator and then checked and made up missing points by another 3 ones. We do not follow previous works like VATEX (Wang et al., 2019) or MSR-VTT (Xu et al., 2016) to annotate multiple captions for one video, since our caption format is an exhaustive description for human actions in video. QA Pair Generation. Based on captions above, we prompt GPT-4o (Hurst et al., 2024) and Gemini-1.5-Pro (Team et al., 2024) to generate multiple-choice question-answer pairs about human interactions, action details, action sequences, count, and human attributes. The prompts share similar format with those in MVBench (Li et al., 2024c). See Appendix for details. Each question-answer pair is checked by 2 annotators and will be corrected if there are any mistakes. All options are shuffled to avoid potential bias. We get total of 500 video-caption pairs and 1,400 QA pairs for our HAICBench dataset. Detailed statistics are presented in Figure 3. Although each video clip focuses on single scene and is relatively short (less than 20 seconds), the captions are highly detailed, often exceeding 100 words. The word cloud analysis reveals that our captions provide comprehensive descriptions of actions."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Baselines. We selected LLaVA-Video-7B (Zhang et al., 2024b) as our baseline due to its minimal inductive bias in model architecture and superior performance. Furthermore, we enhanced its action understanding capability through post-training, resulting in LLaVA-Video-ActionPro-7B. Training dataset. To preserve the general capability of LLaVA-Video against catastrophic forgotten, we randomly selected 200K instruction pairs from its training set LLaVA-Video-178K for sample rehearsal (Zhang et al., 2024b) this subset is (Verwimp et al., 2021). Then, combined with our HAICTrain to form training set with 326K instruction pairs in total. Evaluation dataset. To evaluate human action 5 Figure 3: Statistics of HAICBench. Although videos are relatively short, the video captions are of high details including various action details and sequential actions. Model Frames Avg. Human Annotated Captions Gemini-1.5-Pro (Team et al., 2024) GPT-4o (Hurst et al., 2024) VideoLLaMA2-7B (Cheng et al., 2024) LongVA-7B (Zhang et al., 2024a) InternVL2-8B (Chen et al., 2024d) Qwen2VL-7B (Wang et al., 2024a) LLaVA-Video-7B (Zhang et al., 2024b) LLaVA-Video-ActionPro-7B - - 50* 64 64 64 64 64 64 96.7 41.5 40.3 21.5 16.2 29.1 28.7 30.7 35.7 Action Details 100.0 29.1 32.6 11.7 11.4 20.0 16.6 19.1 22.3 Action Sequence 94.3 28.7 28.3 18.0 13.0 18.3 21.7 16.0 17.0 Interaction Count Attribute 95.0 36.3 38.0 14.7 11.0 22.7 21.3 25.3 31.7 95.3 54.7 52.0 39.3 25.3 43.3 44.7 44.0 53.3 99.0 58.7 50.7 24.0 20.3 41.0 39.3 49.0 54.3 Table 2: Results on HAICBench in the caption evaluation setting. We adopted MLLMs to generate video captions, based on which we prompted an LLM to answer video questions. *The GPT-4o API supports maximum of 50 frames per video. understanding ability, we executed experiments on several action-related benchmarks, including MVBench, ActivityNet-QA, PerceptionTest, and our HAICBench. Note that we focused on questions related to human actions in these benchmarks. For MVBench, we performed comparison of all sub-tasks whose names contain Action, resulting in 7 types: Action Antonym, Action Count, Action Sequence, Action Prediction, Action Localization, Fine-grained Action, and Unexpected Action. For PerceptionTest, we also selected all questions whose tags are related to action. More details are presented in Appendix B. Evaluation metrics. For multiple-choice questions in MVBench, PerceptionTest, and HAICBench, we followed the prompting approach in LLaVA-1.5 (Liu et al., 2024) and used accuracy as the metric. For ActivityNet-QA which features open-ended questions, we followed the evaluation protocol in Video-ChatGPT (Maaz et al., 2024), utilizing GPT-4o-0513 to calculate accuracy. In terms of HAICBench, we implemented two evaluation (1) Standard Evaluation, where the settings: video and question are directly input into an MLLM to generate an answer; and (2) Caption Evaluation, where the model generates caption for each video, and an LLM (Gemini-1.5-Pro) answers questions based on this caption, thereby assessing caption ability based on QA accuracy. To avoid the LLM guessing answers when captions do not mention related contents, we asked Gemini1.5-Pro to refuse to answer those questions (see Appendix D). Implementation Details. We used LLaVA-Video7B as our initial model and fine-tuned it on the training dataset. All parameters were updated during training. The model was fine-tuned for one epoch with learning rate of 1e-5 for the LLM and 2e-6 for the vision encoder, with 256 batch size. We sampled 64 frames uniformly in both the training and evaluation stages. During generation, we take the greedy search without randomness. Our experiments utilized 128 NVIDIA A800-80GB GPUs. 4.2 Results on HAICBench Caption Evaluation Setting. We follow the caption evaluation setting of HAICBench, where MLLM baselines generate action-related captions for each video, and then the captions and questions are fed into Gemini-1.5-Pro for answers. The 6 Model Frames Avg. Gemini-1.5-Pro (Team et al., 2024) GPT-4o (Hurst et al., 2024) VideoLLaMA2-7B (Cheng et al., 2024) LongVA-7B (Zhang et al., 2024a) InternVL2-8B (Chen et al., 2024d) Qwen2VL-7B (Wang et al., 2024a) LLaVA-Video-7B (Zhang et al., 2024b) LLaVA-Video-ActionPro-7B - 50* 64 64 64 64 64 64 66.9 60.6 49.7 50.3 58.2 64.4 64.3 66.4 Action Details 60.3 59.1 35.4 43.1 41.4 61.1 53.7 56.6 Action Sequence 49.3 43.4 37.7 39.3 40.0 44.7 43.7 45.0 Interaction Count Attribute 75.7 64.0 48.3 56.0 61.7 65.3 67.3 68.7 66.7 54.0 62.7 37.3 68.0 71.3 70.7 76.0 82.7 82.3 64.3 75.7 80.0 79.7 86.3 86.0 Table 3: Results on HAICBench in the standard evaluation setting. *The GPT-4o API supports maximum of 50 frames per video. Base Model LLaVA-Video-178K 200K 126K HAICTrain 126K WebVid 126K MVB PerTest ANetQA HAICBench Training Dataset LLaVA-Video LLaVA-OneVision 59.4 60.0 62.1 58.8 57.8 59.3 60.6 58.8 58.1 58.3 59.9 55.8 47.9 55.7 56.9 55.8 61.8 63.8 65.2 62.1 57.1 61.8 62.7 60.4 64.3 64.7 66.4 62.2 61.6 62.2 64.1 61.8 Table 4: The gain from training with our high-quality human action captions is effective across several benchmarks. MVB, PerTest, and ANetQA denotes MVBench, PerceptionTest, and ActivityNet-QA, respectively. results are presented in Table 2. Our post-trained LLaVA-Video-ActionPro-7B achieves state-of-theart performance among open-source MLLMs, with 5% performance gain. These results show that training with HAICTrain can significantly improve the caption quality of human actions. Furthermore, using human annotated captions as input can achieve very high accuracy, which means there is still considerable room for improvement. Note that the accuracies of some classes are lower than 25% because we prompted the model not to answer the question if no clues are found in the caption. Standard Evaluation Setting. In this setting, videos and questions are directly fed into MLLMs for question answering. The results are presented in Table 3. Our post-trained LLaVA-VideoActionPro-7B outperforms LLaVA-Video by 2.1%, achieving state-of-the-art performance among open-source MLLMs. On the action-related subtasks, our model surpasses the baseline LLaVAVideo-7B model by 1.3%-5.3%. These findings highlight the effectiveness of our HAICTrain dataset for enhancing human action understanding. Additionally, Gemini significantly outperforms GPT-4o, which is the reason why we leverage it to generate the captions of our HAICTrain dataset. 4.3 Effectiveness of Better Captions In the training set, we combine our HAICTrain with 200K instruction pairs from LLaVA-Video178K (Zhang et al., 2024b) to prevent catastrophic forgotten. To assess the contribution of 126K video-caption pairs in HAICTrain, we replace them with another 126K samples randomly selected from LLaVA-Video-178K and execute post-training on two different MLLMs. The results are presented in Table 4. With the same amount of data, training with HAICTrain consistently yields better improvements across various action question-answering benchmarks, including MVBench, PerceptionTest, ActivityNetQA and HAICBench. We also try the same videos in HAICTain with their original captions in WebVid and find using these captions can not improve accuracy. All of these demonstrate that HAICTrain is better for the model to understand human actions. 4.4 Qualitative Analysis We conduct qualitative analysis of our posttrained model on HAICBench, as shown in Figure 4. The video features two human boxers fighting on the stage. As outlined in the figure, before post-training, the model can only provide coarse description of their actions. However, after 7 Figure 4: video caption example in HAICBench. post-training with HAICTrain, our model delivers detailed descriptions of the fighters, including their hair color, clothing, and positions in the cage. Besides, it presents clear sequence of actions, describing how the attack and defense progress over time. This also demonstrates the effectiveness of our caption format. By distinguishing the fighters based on detailed attributes, the caption facilitates better individual recognition, enhancing the viewers ability to follow the action. 4.5 Effectiveness for Text-video Generation To evaluate our methods effectiveness in textto-video generation, we use the MovieGenBench dataset (Polyak et al., 2024), consisting of 1,003 videos. We first take LLaVA-Video-7B and LLaVA-Video-ActionPro-7B to generate captions, which are then fed to HunyuanVideo (Kong et al.) and Wanx2.1 (Aliyun, 2023) to generate videos. Then, five human annotators assess the semantic relevance between videos generated by two captions, classifying them as Good, Same, or Bad. The annotators are asked to make decisions upon the original videos in MovieGen. We measure the GSB score, i.e., the percentage of Good and Same relative to that of Bad and Same. Results show that LLaVA-VideoActionPro-7B outperforms LLaVA-Video-7B in generating captions that lead to more semantically accurate videos. On HunyuanVideo, LLaVAVideo-ActionPro-7B achieves GSB score of 2.15, and on Wanx2.1, 6.81. LLaVA-Video-7B often produced verbose but less accurate captions, failing to capture the original videos core actions. These findings highlight our models superior ability to comprehend the actions in videos and relay captions that enable high-quality, semantically faithful text-to-video generation. More cases are presented in Appendix E."
        },
        {
            "title": "5 Conclusion",
            "content": "This study addresses the challenge of human action understanding by introducing novel twostage data annotation pipeline, combining data accumulation, human-machine annotation, and human verification. This process produces two datasets: HAICTrain, which significantly enhances human action understanding and generation, and HAICBench, comprehensive benchmark. Both datasets will be made open-source to advance research and applications in this field, promoting broader impact across human behavior understanding and generation."
        },
        {
            "title": "References",
            "content": "Although our work makes notable progress in human action understanding, it still has several limitations. Despite their comprehensive nature, our HAIC datasets may not cover the entire spectrum of human actions, particularly those involving complex interactions or cultural nuances. Furthermore, our work primarily focuses on visual and textual data, lacking integration with audio data, which could provide additional context for understanding. Future work should aim to incorporate audio and further refine the annotation process to address these limitations. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022. Flamingo: Visual Language In Advances in Model for Few-Shot Learning. Neural Information Processing Systems. Aliyun. 2023. Tongyi wanxiang. Accessed: 2025-0214. Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in Time: Joint Video and Image Encoder for End-to-End Retrieval. In International Conference on Computer Vision, pages 17081718. IEEE. Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, and Jianwei Yang. 2024. TemporalBench: Benchmarking Finegrained Temporal Understanding for Multimodal Video Models. ArXiv:2410.10818 [cs]. João Carreira and Andrew Zisserman. 2017. Quo Vadis, Action Recognition? New Model and the Kinetics Dataset. In Conference on Computer Vision and Pattern Recognition, pages 47244733. IEEE Computer Society. Brandon Castellano. 2024. PySceneDetect. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. 2024a. Sharegpt4video: Improving Video Understanding and Generation with Better Captions. ArXiv:2406.04325 [cs]. Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. 2024b. MotionLLM: Understanding Human Behaviors from Human Motions and Videos. ArXiv:2405.20340 [cs]. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, and others. 2024c. Panda-70m: Captioning 70m Videos with Multiple Cross-Modality Teachers. In Conference on Computer Vision and Pattern Recognition, pages 1332013331. IEEE. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min 9 Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024d. How Far are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites. ArXiv:2404.16821 [cs]. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. 2024. VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs. ArXiv:2406.07476 [cs]. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In Advances in Neural Information Processing Systems. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fründ, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. 2017. The \"Something Something\" Video Database for Learning and Evaluating Visual Common Sense. In International Conference on Computer Vision, pages 58435851. IEEE Computer Society. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina González, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Kolár, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbeláez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard A. Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. 2022. Ego4D: Around the World in 3, 000 Hours of Egocentric Video. In Conference on Computer Vision and Pattern Recognition, pages 1897318990. IEEE. Agrim Gupta, Silvio Savarese, Surya Ganguli, and Embodied Intelligence via Li Fei-Fei. 2021. Learning and Evolution. Nature communications, 12:5721. Gillian Hayes. 2011. The Relationship of Action Research to Human-Computer Interaction. ACM Transactions on Computer-Human Interaction, 18. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Nils Johan Bertil Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 2023. Language is not all you need: Aligning perception with language models. In Advances in Neural Information Processing Systems. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. arXiv preprint 2024. arXiv:2410.21276. Gpt-4o system card. Yunseok Jang, Yale Song, Chris Dongjoo Kim, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2019. Video Question Answering with Spatio-Temporal In Int. J. Comput. Vis., volume 127, Reasoning. pages 13851412. Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. 2023. Rtmpose: Real-time multi-person pose estimation based on mmpose. ArXiv:2303.07399 [cs]. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. 2017. Dense-Captioning Events in Videos. In International Conference on Computer Vision, pages 706715. IEEE Computer Society. Jie Lei, Tamara Berg, and Mohit Bansal. 2021. Detecting Moments and Highlights in Videos via Natural Language Queries. In Advances in Neural Information Processing Systems, volume 34. Neural Information Processing Systems Foundation. 10 Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. LLaVA-OneVision: Easy Visual Task Transfer. ArXiv:2408.03326 [cs]. Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. 2024b. Openhumanvid: large-scale high-quality dataset for enhancing arXiv preprint human-centric video generation. arXiv:2412.00115. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. 2024c. Mvbench: comprehensive multi-modal video understanding benchmark. In Conference on Computer Vision and Pattern Recognition, pages 2219522206. IEEE. Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. 2023. Motion-x: large-scale 3d expressive wholebody human motion dataset. Advances in Neural Information Processing Systems, 36:2526825280. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Improved Baselines with Visual Lee. 2024. In Conference on Computer Instruction Tuning. Vision and Pattern Recognition, pages 2628626296. IEEE. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. In Advances in Neural Information Processing Systems. Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, and Fahad Khan. 2024. Video-chatgpt: Towards detailed video understanding via large In Annual Meeting vision and language models. of the Association for Computational Linguistics, pages 1258512602. Association for Computational Linguistics. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. HowTo100M: Learning Text-Video Embedding by Watching Hundred Million Narrated In International Conference on Video Clips. Computer Vision, pages 26302640. IEEE. Inc. OMEGA Labs. 2024. Omega labs bittensor subnet: Multimodal dataset for agi rehttps://huggingface.co/datasets/ search. omegalabsinc/omega-multimodal. Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alexandre Fréchette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira. 2023. Perception Test: Diagnostic Benchmark In Advances in for Multimodal Video Models. Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. 2024. Grounding Multimodal Large Language Models to the World. In International Conference on Learning Representations. OpenReview.net. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, YenCheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. 2024. Movie gen: cast of media foundation models. Preprint, arXiv:2410.13720. Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. 2016. NTU RGB+D: large scale In 2016 dataset for 3d human activity analysis. IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 10101019. IEEE Computer Society. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. 2024. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. UCF101: dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402. Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. 2020. Grab: dataset of wholeIn European body human grasping of objects. conference on computer vision, pages 581600. Springer. 11 Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Eli Verwimp, Matthias De Lange, and Tinne Tuytelaars. 2021. Rehearsal revealed: The limits and merits of revisiting samples in continual learning. In International Conference on Computer Vision, pages 93859394. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv:2409.12191 [cs]. Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. 2024b. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. arXiv preprint arXiv:2410.08260. Xiao Wang, Jianlong Wu, Zijia Lin, Fuzheng Zhang, Di Zhang, and Liqiang Nie. 2024c. Video dataflywheel: Resolving the impossible data trinity in video-language understanding. arXiv preprint arXiv:2409.19532. Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, YuanFang Wang, and William Yang Wang. 2019. VaTeX: Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research. In International Conference on Computer Vision, pages 45804590. IEEE. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. 2024d. Internvid: large-scale video-text dataset for multimodal understanding and generation. In International Conference on Learning Representations. Yuxuan Wang, Difei Gao, Licheng Yu, Weixian Lei, Matt Feiszli, and Mike Zheng Shou. 2022. GEB+: benchmark for generic event boundary captioning, grounding and retrieval. In European Conference of Computer Vision, Lecture Notes in Computer Science, pages 709725. Springer. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. NExT-GPT: Any-to-Any Multimodal LLM. In International Conference on Machine Learning. OpenReview.net. Junbin Xiao, Xindi Shang, Angela Yao, and TatNExT-QA: Next Phase Seng Chua. 2021. of Question-Answering to Explaining Temporal In Conference on Computer Vision and Actions. Pattern Recognition, pages 97779786. Computer Vision Foundation / IEEE. Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, and Xihui Liu. 2024. Lvd-2m: longtake video dataset with temporally dense captions. arXiv preprint arXiv:2410.10816. Feiyi Xu, Feng Xu, Jiucheng Xie, Chi-Man Pun, Huimin Lu, and Hao Gao. 2021. Action recognition framework in traffic scene for autonomous driving IEEE Transactions on Intelligent system. Transportation Systems, 23. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. MSR-VTT: Large Video Description Dataset for In Conference on Bridging Video and Language. Computer Vision and Pattern Recognition, pages 52885296. IEEE Computer Society. Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. 2022. Advancing high-resolution video-language representation with large-scale video transcriptions. In Conference on Computer Vision and Pattern Recognition, pages 50265035. IEEE. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: dataset for understanding complex In AAAI web videos via question answering. Conference on Artificial Intelligence, pages 9127 9134. Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021. MERLOT: Multimodal Neural Script Knowledge Models. In Advances in Neural Information Processing Systems, pages 2363423651. Neural Information Processing Systems Foundation. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. 2024a. Long Context Transfer from Language to Vision. ArXiv:2406.16852 [cs]. Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. 2022. Egobody: Human body shape and motion of interacting people from head-mounted devices. In European conference on computer vision, pages 180200. Springer. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024b. Video Instruction Tuning With Synthetic Data. ArXiv:2410.02713 [cs]. Jiaming Zhou, Junwei Liang, Kun-Yu Lin, Jinrui Yang, and Wei-Shi Zheng. 2024. Actionhub: large-scale action video description dataset for zero-shot action recognition. ArXiv:2401.11654 [cs]. Luowei Zhou, Chenliang Xu, and Jason J. Corso. 2018. Towards automatic learning of procedures from web instructional videos. In AAAI Conference on Artificial Intelligence, pages 75907598. AAAI Press. 12 Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2024. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. In International Conference on Learning. OpenReview.net."
        },
        {
            "title": "A Automatic Annotation",
            "content": "Figure 5: Prompt for Gemini-Pro Re-captioning. In Section 3.2, we utilize Gemini-1.5-Pro to generate captions for HAICTrain in our defined standardized format. The prompt is detailed in Figure 5."
        },
        {
            "title": "B Evaluation datasets",
            "content": "In Section 4.1, we use action subsets from MVBench and PerceptionTest as our benchmarks. Here, we explain our subset selection process in detail."
        },
        {
            "title": "C QA Pair Generation",
            "content": "As briefly discussed in Section 3.3, we utilized large language model (LLM) to assist in generating QA pairs for HAICBench. The prompts used are as follows: for action interaction QA, see Figure 6; for action detail QA, see Figure 7; for action sequence QA, see Figure 8; for action count QA, see Figure 9; and for human attribute QA, see Figure 10. names include Prediction, Action \"action\": Count, Action For MVBench, we choose seven categories Action whose Sequence, Antonym, Action Localization, Action Fine-grained Action, and Unexpected Action. For PerceptionTest, we select questions tagged with any of the five labels related to human actions: recognition, Action Adversarial action, Distractor action, and Occlusion (Occluded interactions). counting, Action 14 Figure 6: Prompt for action interaction QA generation. 15 Figure 7: Prompt for action details QA generation. 16 Figure 8: Prompt for action sequence QA generation. 17 Figure 9: Prompt for action count QA generation. 18 Figure 10: Prompt for human attribute QA generation. Figure 11: Prompt caption evaluation setting. 19 the number of subjects. Figure 14 is an example of multiple subjects (the 129th sample in the dataset). It can be seen that in the caption, LLaVAVideo incorrectly identified the number of subjects and missed the positional relationship. Overall, the introduction of refined and formatted action description data has greatly improved the models ability to understand and retell movements. This is of great significance for current mainstream action understanding, anomaly recognition, motion generation, and text-to-video generation."
        },
        {
            "title": "F Potential Risks",
            "content": "The source videos of our HAICBench are from YouTube. YouTube inherently contains social biases, and viewing its videos as representative of the world can perpetuate hegemonic perspectives. The majority of popular YouTubers are men, and the platforms video practices often reflect gender biases. YouTube also faces issues with hate content, including radical alt-right and alt-lite material. These issues are exacerbated by the platforms recommendation algorithm. Even though we downloaded videos independently, filtering them by view count still subjects us to algorithmic influence. The popularity and monetization dynamics on YouTube shape and are shaped by broader cultural trends, affecting the style and content of uploaded videos."
        },
        {
            "title": "D Caption Evaluation Setting",
            "content": "In Section 4.1, we outline our caption evaluation setup, as illustrated in Figure 11. Initially, we prompt MLLMs to generate caption for each video using specific prompt. We then combine the generated caption with question to enable Gemini-1.5-Pro to produce answers. Case Study on Text-to-Video"
        },
        {
            "title": "Generation",
            "content": "We used LLaVA-Video and LLaVA-VideoActionPro to caption the reference videos (generated by MovieGen) in MovieGenBench. Then, we used these captions to generate videos, using the open source HunyuanVideo * and the closed source Wanx2.1 respectively. In the manual evaluation, we concluded that the captions generated by LLaVA-Video-ActionPro can give video that is more consistent and reasonable with the original video. Figure 12 is an example of single human subject (the first sample in the dataset). It can be seen that in the caption, LLaVAVideo lost the description of details, resulting in inconsistent generated content. Figure 13 is an example of single animated character subject (the 17th sample in the dataset). It can be seen that in the caption, LLaVA-Video incorrectly identified *The infer-steps parameter is set to 50. https://tongyi.aliyun.com/wanxiang/videoCreation 20 Figure 12: Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the first sample in MovieGenBench. The main subject in this case is one woman walking along the street. LLaVA-Video-ActionPro provides more detailed appearance of the woman than LLaVA-Video. 21 Figure 13: Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the 17th sample in MovieGenBench. The main subject in this case is one blue animated character. LLaVA-Video incorrectly identifies the main subject. 22 Figure 14: Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the 129th sample in MovieGenBench. The main subjects in this case are five cars lined in two rows. LLaVA-Video incorrectly identifies the number of the main subjects."
        }
    ],
    "affiliations": [
        "Kuaishou Technology"
    ]
}