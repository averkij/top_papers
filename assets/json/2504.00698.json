{
    "paper_title": "Command A: An Enterprise-Ready Large Language Model",
    "authors": [
        "Team Cohere",
        "Aakanksha",
        "Arash Ahmadian",
        "Marwan Ahmed",
        "Jay Alammar",
        "Yazeed Alnumay",
        "Sophia Althammer",
        "Arkady Arkhangorodsky",
        "Viraat Aryabumi",
        "Dennis Aumiller",
        "Raphaël Avalos",
        "Zahara Aviv",
        "Sammie Bae",
        "Saurabh Baji",
        "Alexandre Barbet",
        "Max Bartolo",
        "Björn Bebensee",
        "Neeral Beladia",
        "Walter Beller-Morales",
        "Alexandre Bérard",
        "Andrew Berneshawi",
        "Anna Bialas",
        "Phil Blunsom",
        "Matt Bobkin",
        "Adi Bongale",
        "Sam Braun",
        "Maxime Brunet",
        "Samuel Cahyawijaya",
        "David Cairuz",
        "Jon Ander Campos",
        "Cassie Cao",
        "Kris Cao",
        "Roman Castagné",
        "Julián Cendrero",
        "Leila Chan Currie",
        "Yash Chandak",
        "Diane Chang",
        "Giannis Chatziveroglou",
        "Hongyu Chen",
        "Claire Cheng",
        "Alexis Chevalier",
        "Justin T. Chiu",
        "Eugene Cho",
        "Eugene Choi",
        "Eujeong Choi",
        "Tim Chung",
        "Volkan Cirik",
        "Ana Cismaru",
        "Pierre Clavier",
        "Henry Conklin",
        "Lucas Crawhall-Stein",
        "Devon Crouse",
        "Andres Felipe Cruz-Salinas",
        "Ben Cyrus",
        "Daniel D'souza",
        "Hugo Dalla-Torre",
        "John Dang",
        "William Darling",
        "Omar Darwiche Domingues",
        "Saurabh Dash",
        "Antoine Debugne",
        "Théo Dehaze",
        "Shaan Desai",
        "Joan Devassy",
        "Rishit Dholakia",
        "Kyle Duffy",
        "Ali Edalati",
        "Ace Eldeib",
        "Abdullah Elkady",
        "Sarah Elsharkawy",
        "Irem Ergün",
        "Beyza Ermis",
        "Marzieh Fadaee",
        "Boyu Fan",
        "Lucas Fayoux",
        "Yannis Flet-Berliac",
        "Nick Frosst",
        "Matthias Gallé",
        "Wojciech Galuba",
        "Utsav Garg",
        "Matthieu Geist",
        "Mohammad Gheshlaghi Azar",
        "Seraphina Goldfarb-Tarrant",
        "Tomas Goldsack",
        "Aidan Gomez",
        "Victor Machado Gonzaga",
        "Nithya Govindarajan",
        "Manoj Govindassamy",
        "Nathan Grinsztajn",
        "Nikolas Gritsch",
        "Patrick Gu",
        "Shangmin Guo",
        "Kilian Haefeli",
        "Rod Hajjar",
        "Tim Hawes",
        "Jingyi He",
        "Sebastian Hofstätter",
        "Sungjin Hong",
        "Sara Hooker",
        "Tom Hosking",
        "Stephanie Howe",
        "Eric Hu",
        "Renjie Huang",
        "Hemant Jain",
        "Ritika Jain",
        "Nick Jakobi",
        "Madeline Jenkins",
        "JJ Jordan",
        "Dhruti Joshi",
        "Jason Jung",
        "Trushant Kalyanpur",
        "Siddhartha Rao Kamalakara",
        "Julia Kedrzycki",
        "Gokce Keskin",
        "Edward Kim",
        "Joon Kim",
        "Wei-Yin Ko",
        "Tom Kocmi",
        "Michael Kozakov",
        "Wojciech Kryściński",
        "Arnav Kumar Jain",
        "Komal Kumar Teru",
        "Sander Land",
        "Michael Lasby",
        "Olivia Lasche",
        "Justin Lee",
        "Patrick Lewis",
        "Jeffrey Li",
        "Jonathan Li",
        "Hangyu Lin",
        "Acyr Locatelli",
        "Kevin Luong",
        "Raymond Ma",
        "Lukas Mach",
        "Marina Machado",
        "Joanne Magbitang",
        "Brenda Malacara Lopez",
        "Aryan Mann",
        "Kelly Marchisio",
        "Olivia Markham",
        "Alexandre Matton",
        "Alex McKinney",
        "Dominic McLoughlin",
        "Jozef Mokry",
        "Adrien Morisot",
        "Autumn Moulder",
        "Harry Moynehan",
        "Maximilian Mozes",
        "Vivek Muppalla",
        "Lidiya Murakhovska",
        "Hemangani Nagarajan",
        "Alekhya Nandula",
        "Hisham Nasir",
        "Shauna Nehra",
        "Josh Netto-Rosen",
        "Daniel Ohashi",
        "James Owers-Bardsley",
        "Jason Ozuzu",
        "Dennis Padilla",
        "Gloria Park",
        "Sam Passaglia",
        "Jeremy Pekmez",
        "Laura Penstone",
        "Aleksandra Piktus",
        "Case Ploeg",
        "Andrew Poulton",
        "Youran Qi",
        "Shubha Raghvendra",
        "Miguel Ramos",
        "Ekagra Ranjan",
        "Pierre Richemond",
        "Cécile Robert-Michon",
        "Aurélien Rodriguez",
        "Sudip Roy",
        "Laura Ruis",
        "Louise Rust",
        "Anubhav Sachan",
        "Alejandro Salamanca",
        "Kailash Karthik Saravanakumar",
        "Isha Satyakam",
        "Alice Schoenauer Sebag",
        "Priyanka Sen",
        "Sholeh Sepehri",
        "Preethi Seshadri",
        "Ye Shen",
        "Tom Sherborne",
        "Sylvie Chang Shi",
        "Sanal Shivaprasad",
        "Vladyslav Shmyhlo",
        "Anirudh Shrinivason",
        "Inna Shteinbuk",
        "Amir Shukayev",
        "Mathieu Simard",
        "Ella Snyder",
        "Ava Spataru",
        "Victoria Spooner",
        "Trisha Starostina",
        "Florian Strub",
        "Yixuan Su",
        "Jimin Sun",
        "Dwarak Talupuru",
        "Eugene Tarassov",
        "Elena Tommasone",
        "Jennifer Tracey",
        "Billy Trend",
        "Evren Tumer",
        "Ahmet Üstün",
        "Bharat Venkitesh",
        "David Venuto",
        "Pat Verga",
        "Maxime Voisin",
        "Alex Wang",
        "Donglu Wang",
        "Shijian Wang",
        "Edmond Wen",
        "Naomi White",
        "Jesse Willman",
        "Marysia Winkels",
        "Chen Xia",
        "Jessica Xie",
        "Minjie Xu",
        "Bowen Yang",
        "Tan Yi-Chern",
        "Ivan Zhang",
        "Zhenyu Zhao",
        "Zhoujie Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency."
        },
        {
            "title": "Start",
            "content": "Command A: An Enterprise-Ready Large Language Model Cohere1 Abstract In this report we describe the development of Command A, powerful large language model purpose-built to excel at real-world enterprise use cases. Command is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency."
        },
        {
            "title": "1\nLarge Language Models (LLMs) are Artificial Intelligence (AI) models designed to understand and gener-\nate human-like text conditioned on the input they receive. Recent advancements have led to remarkable\nbreakthroughs in their ability to comprehend and produce human language with unparalleled accuracy and\nfluency. This progress has been instrumental in their widespread adoption across various real-world and\nenterprise environments, where they significantly boost operational efficiency and deepen understanding.",
            "content": "This technical report describes the development of Command and Command R7B, two LLMs designed to excel in real-world enterprise settings. Both the 111B parameter Command and Command R7B perform best-in-class across suite of established benchmarks for their respective model sizes. We also highlight key innovations and technical contributions including data and architectural optimisations, self-refinement algorithms, and model merging-based approach optimised to bring out expert-level performance across capabilities within single set of model weights, providing fast and efficient performance. Command is tailored for excellent performance in enterprise-relevant settings such as Retrieval Augmented Generation (RAG), where models can interact with, understand, and process information distributed across wide range of documents. As part of this focus, our models also excel in the multilingual setting, supporting 23 key languages of global business: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. Along with its impressive overall performance, achieving best-in-class results for any model in its size and efficiency range on common benchmarks such as MATH, Command outperforms across an extensive suite of human evaluation tasks as shown in Figure 1. Furthermore, Command achieves strong results on enterprise-relevant agentic benchmarks such as Taubench, as shown in Table 1. Command focuses on delivering competitive performance as efficiently as possible. With serving footprint of just two A100s or H100s, Command requires considerably less computational overhead than comparable models. This is of particular importance for privacy-preserving enterprise settings and on-premises deployments. Command can deliver tokens at rate of up to 156 tokens/sec which is 1.75x higher than GPT-4o and 2.4x higher than DeepSeek V3. 1Please cite this technical report as Cohere (2025). full author list can be found at the end of this document. Released as preprint on April 1, 2025 Figure 1: Head-to-head human evaluation win rates. All examples are blind-annotated by specially trained human annotators, assessing enterprise-focused accuracy, instruction following, and style."
        },
        {
            "title": "A\nd\nn\na\nm\nm\no\nC",
            "content": "85.5 80.0 90.9 50.8 51.7 63. 86.2 59.5 92.6 68.8 3 e e 88. 70.2 86.1 59.1 39.1 58.6 89. 53.1 92.2 69.8 0 7 3.3 Lla 86.0 77.0 92. 50.5 21.0 51.4 84.4 58.0 85. 62.5 4 - 85.7 68.5 83.8 53. 51.2 72.1 86.5 50.5 91.2 71. 7 a C 65.2 59.1 77.9 26.3 52. 72.0 42.2 69.6 48.1 8 3.1 Lla 71.1 51. 78.6 23.4 50.9 72.8 41.9 73. 49.2 8 al t"
        },
        {
            "title": "Minis",
            "content": "71.1 54.5 59.0 23.4 51.8 61. 33.2 62.0 36.8 Capability Benchmark Academic Agents Code Multilingual MMLU MATH IFEval GPQA Taubench BFCL MBPP+ Bird-SQL RepoQA NTREX Table 1: Command and Command R7B results on key academic, agentic, code and multilingual benchmarks, in comparison to relevant external models. We also release model weights to the research community to facilitate community-based exploration under CC-BY-NC License (Non-Commercial) with an acceptable use addendum. The model checkpoints are available on the HuggingFace model hub."
        },
        {
            "title": "2.1 Overview\nPre-training language models involves training a model on trillions of tokens of unlabelled text data to learn\ngeneral language patterns, syntax, and semantics, enabling it to generate contextually relevant responses.\nThis foundational step leverages self-supervised learning techniques, such as next-token prediction, to build\na versatile representation of language that can subsequently be fine-tuned for specific downstream tasks. Pre-\ntraining is computationally intensive but essential for achieving state-of-the-art performance across diverse\nnatural language processing applications.",
            "content": ""
        },
        {
            "title": "2.3 Model Architecture",
            "content": "Figure 2: Schematic of the Command model architecture. We use decoder-only Transformer architecture (Vaswani et al., 2017) as depicted in Figure 2. We highlight few key architectural decisions below: SwiGLU. The SwiGLU activation (Shazeer, 2020) demonstrates performance improvements over other activation functions. Interleaved attention layers. We use interleaved layers of sliding window attention and full attention in 3:1 ratio. Each sliding window layer uses Rotary Positional Embeddings (RoPE) (Su et al., 2021) and every full attention layer uses No Positional Embeddings (NoPE) (Kazemnejad et al., 2023). Further details can be found in Yang et al. (2025). GQA. We use grouped-query attention (GQA; (Ainslie et al., 2023)) to increase serving throughput. We use document masking to ensure that each individual sequence in batch can only attend to itself. Parallel transformer block. This shows equivalent performance but significant improvement in throughput compared to the vanilla transformer block. No bias. Similar to PaLM (Chowdhery et al., 2023), we do not employ bias terms, which improves training stability at larger scales. Input and output embeddings. We share the input and output embedding matrices, which provides large reduction in memory requirements due to our large vocabulary size. We do not observe any performance degradation across ablations."
        },
        {
            "title": "2.4.1 Distributed Training\nWe train all our models on our NVIDIA H100 GPU cluster using our internal JAX-based (Frostig et al.,\n2018) distributed training framework. Our framework leverages JAX’s GSPMD (Xu et al., 2021) as the\nbackbone to implement complex sharding strategies. We split the available GPUs into a mesh with four axes\nfor each of the following sharding schemes:",
            "content": "Data Parallel (DP) axis shards the activations along the batch dimension, which behaves as standard data parallel training when all GPUs are allocated to it. Fully Sharded Data Parallel (FSDP) axis shards both the activations along the batch dimension and model states along specified dimension. The model states are replicated across the data parallel axis to contain the communication costs to the FSDP submesh. Sequence Parallel (SP). Given the restrictions on critical batch-size of LLMs, scaling the number of GPUs in pure FSDP/DP scenarios is infeasible. We thus use sequence parallelism (Li et al., 2023b) to shard activations along the sequence dimension. The activations after the QKV projection are sharded along the heads dimension to remove communication costs during the attention computation. The attention outputs are sharded on the outer dimension, and the weight matrix of the final attention output transformation is sharded along the contracting dimension as in Megatron-style (Shoeybi et al., 2019) sharding. This allows us to operate using single all-gather and single reduce-scatter for the activations, while only gathering QKV and attention outputs along the FSDP axis. At the feed forward block, the FFN transformation is independent along the sequence axis, therefore there is no need for any activation communication. Moreover, since we use parallel attention and FFN block setup, we completely overlap the computation of the FFN expansion layer and the all-gather of the attention activations. The reduce-scatter after the attention block is further overlapped with the execution of the FFN reduction layer. Since all other major operations such as layer norm, input and output embedding layers are independent along the sequence axis, they incur no communications along the activations. Tensor Parallel (TP) axis for pure Megatron-style sharding, where two complementary matrix multiplications are sharded such that the activations are all-gathered before the first matrix multiplication (where the weight is sharded on the outer axis, resulting in the activations being sharded on the outer axis as well), and one all-reduce after the second matrix multiplication (to sum the partial outputs). Pure TP is desirable when moving activations between devices as it is much cheaper compared to moving weights, layout class sometimes referred to as weight-stationary (Pope et al., 2023). We use pure TP for fast decoding and in low batch-size scenarios. Our models are trained with varying combinations of the parallelism strategies mentioned above. During pre-training, since we are in the high batch-size and throughput regime, we opt for combination of DP, FSDP and SP to minimise activation communication. Furthermore, we can unroll the models forward loop to overlap the communication of the weights of the next layer with the execution of the current layer. We leverage Hopper-specific features such as FP8 tensor cores (Micikevicius et al., 2022) to further improve throughput. While many works have reported instability while training with FP8 precision for long training horizons (Fishman et al., 2025), we observe no such instability. In fact, we observe minimal run interventions due to loss spikes and optimisation instability. We keep our main weights and optimiser states in FP32 precision, and cast the model weights to BF16 or FP8 prior to the computation. We keep sensitive operations such as exponentials, softmaxes, layer norms, and output embeddings in FP32 precision, and run the attention computation in BF16 precision. While we do not observe any training instabilities with FP8 matmuls, we notice that there is small but non-trivial degradation in downstream performance if the entire training run is in FP8. To mitigate this effect, we first perform number of steps in BF16 precision, which brings performance back to the full BF16 trained models performance range. 4 Figure 3: Command goes through multiple post-training phases including two weighted model merging steps, and model polishing phase."
        },
        {
            "title": "3.1 Overview\nCommand A is post-trained using a novel decentralised approach to maximise and control its performance\nover a wide spectrum of domains and capability areas. More precisely, Command A is trained by alternating\ncentralised training stages, where a single model is fine-tuned, and decentralised training stages, where\nmultiple expert models are trained separately to maximise domain-wise performance before merging their\nparameters. Although the classic post-training paradigm involves training a single model sequentially with\nvarying data-mixtures (Dubey et al., 2024; Team et al., 2024), Command A is the first large-scale attempt\nto combine multiple parallel expert tracks, with parameter merging techniques playing a central role. This\nsection details the high-level post-training recipe of Command A, illustrated in Figure 3.",
            "content": "We divide the global Command post-training recipe into several sub-stages, each producing intermediary model artifacts: Instruct Model: We train an initial Instruct model with supervised learning on top of the base model to provide the core basic capabilities of the model. SFT Expert Models: We train six SFT experts on top of the Instruct checkpoint with specialised data mixtures to maximise capability-specific performance. SFT Soup Model: We merge the six model experts into Soup model with parameter-merging methods (see Section 3.4) to produce single SFT aggregate model. 5 RL Expert Models: We train six RL experts on top of the SFT Soup checkpoint using RL algorithms tailored to each domain, using pairwise comparisons or verifiable rewards. RL Soup Model: We merge the six RL experts into RL Soup model with parameter-merging methods to produce single RL aggregate model. Polished Model: We perform final stage on the RL Soup model to enhance human interaction performance by alternating between best-of-N methods, offline preference, and online RL algorithms. Six expert models are created at each expert stage: Code, Safety, RAG, Math, Multilingual, and General Long-Context expert. This approach allows us to adapt each experts training procedure, tailoring it to the specific capability or domain of interest. This allows fine-grained hyperparameter tuning, specialised data mixture optimisation, local optimisation (e.g., seed merging), and the capability-specific selection of the most appropriate algorithms. This becomes even more crucial during the RL stage, as different domains demand distinct RL techniques for example, verifiable rewards for Math and Code, or preference pairs for Safety and Multilingual. Our late-merging procedure allows us to re-balance Soup model performance posteriori without requiring additional training (3.4). From an organisational perspective, merging allows contributors to collaborate closely in parallel, fostering unique model development synergy. Overall, this decentralised training procedure maximises individual expert performance while controlling the final global model capacity, allowing us to optimise both model performance and efficiency. Finally, the model undergoes polishing phase to improve its writing style. First, we apply best-of-N supervised training stage to the RL Soup model. Then, we alternate between offline preference and online RL optimisation in ping-pong approach, iterating as required until we observe human preference performance plateau, to obtain the final Command model. In the following sections, we introduce the methods and algorithms that we use at various stages of the post-training process. We detail individual expert recipe considerations and provide further technical details on the merging techniques applied. Finally, we discuss key features of the model polishing phase."
        },
        {
            "title": "3.2.2.1 Preference Training with Self-refinement\nWe consider preference training methods for learning offline from preference datasets: Sequence Likelihood\nCalibration, or SLiC (Zhao et al., 2023), Direct Preference Optimisation, or DPO (Rafailov et al., 2024), and\nIdentity Preference Optimisation, or IPO (Azar et al., 2024). In addition to these conventional preference-\ntraining methods, our model-training pipeline incorporates our novel Self-improving Robust Preference Op-\ntimisation (SRPO) (Choi et al., 2025). This recently-developed approach represents a significant departure",
            "content": "6 from traditional preference training techniques, introducing novel mechanism for continuously enhancing model alignment and robustness. It amounts to solving the following min-max optimisation problem: min π max π ExEy1π(x),y2π(x,y1) [P (y2 y1x) β KL(ππrefx, y1) + β KL(ππrefx)] . This objective function aims to learn self-improvement policy π that can improve generations from π, according to the preference model without deviating too much from reference model πref, and at the same time at learning policy π of which generations cannot be improved by π. SRPOs novelty partly lies in its robustness: unlike classical methods, it does not depend on the sampling distribution of the preference dataset. This independence ensures greater generalisation and stability in varied deployment scenarios. Furthermore, SRPO uniquely enables iterative self-revision at inference time, process where the model sequentially refines its output: Given an initial prompt, the model first generates an initial completion using the generative policy π, followed by multiple sequential refinements through the self-refinement policy π, each progressively improving the quality and alignment of the final output. This iterative refinement capability is not present in conventional alignment pipelines, underscoring SRPOs innovative contribution."
        },
        {
            "title": "3.2.2.2 Optimising the Reward Model with RL\nWhen given a reward function, be it the reward model or a verifiable reward, we consider the classic KL-\nregularised reinforcement learning objective, J(π) = ExEy∼π(·|x)[R(x, y) − β KL(π||πref|x)]. In all settings\n(offline or online), we optimise it using the recent Contrastive Policy Gradient approach, or CoPG (Flet-\nBerliac et al., 2024). For a prompt x and k > 1 completions y1, . . . , yk of arbitrary origin, the corresponding\nloss to be minimised is",
            "content": "ℓ(x, y1, . . . , yk; π) = 1 1 (cid:88) i>j (cid:0)Rπ β(x, yi) Rπ β(x, yj)(cid:1)2 with Rπ β(x, y) = R(x, y) β ln π(yx) πref(yx) . The CoPG loss can be used in both the offline and online cases. In the online case, it can be used with replay buffer, possibly combined with additional datasets, or in pure on-policy fashion, in which case it is equivalent to Reinforce Leave-One Out, or RLOO (Ahmadian et al., 2024). Furthermore, Flet-Berliac et al. (2024) show that the gradient of this loss is form of (negative) off-policy policy gradient, not relying on importance sampling, clipping, or on an additional value network. It also comes with strong theoretical guarantees, notably estimating the KL-regularised optimal policy π even in the offline case, and generalising policy gradient and some preference training approaches. In the offline case, it can be used with any dataset, as long as there is more than one completion per prompt, and that we can compute the associated rewards. We mostly use CoPG offline and online on-policy."
        },
        {
            "title": "3.2.3 Reward Models\nWe train a Bradley-Terry reward model for use in online preference learning, evaluation, and data filtering.\nSimilar to Gunter et al. (2024), we use a cross-entropy loss with soft labels as targets.",
            "content": "We find that reward models tend to suffer from high memorisation, causing catastrophic collapse in performance on second epoch over the same data; so, we train the model in two stages. The first stage consists of approximately 4 million samples designated as lower quality and relabelled using an ensemble of reward models. Training is carried out for one epoch with batch size of 1024 with cosine learning rate schedule with peak of 4 105. The second stage consists of approximately 350,000 high quality samples, with labels derived from the strength of human preferences, ensembles of models (with labels inconsistent with human annotations moved to the first stage), or constant label value of 0.999 for gold-standard data and 0.5 for gold-standard tied pairs. This stage uses smaller batch size of 16, and lower maximum learning rate of 3 106. Both stages use packed data, where multiple pairs of preference data are encoded in single training sample for efficiency, using attention masking to avoid different (non-packed) samples influencing each other. The pairs are left-padded to align their <EOS_TOKEN>, and distributed to aim for 75% fill while keeping the number of samples per row as uniform as possible, ensuring an equal loss contribution. Our internal reward model scores 92.7% on RewardBench (Lambert et al., 2024), and achieves an average score of 72.3% on RMB (Zhou et al., 2024)."
        },
        {
            "title": "3.3 Capabilities",
            "content": "Instruction-Following"
        },
        {
            "title": "3.3.1.1 Data collection\nOur data collection approach can be divided based on the post-training method, i.e., SFT or preference\ntuning. To collect datasets that serve both of these, we primarily rely on synthetic prompt generation in\nconjunction with human annotation, and explore various sampling and filtering techniques (Bartolo et al.,\n2021). Specifically, we synthetically generate diverse sets of prompts covering a range of instructions tailored\nto individual domains (such domains are mostly enterprise-oriented) and generate two completions per\nprompt sampled with different temperatures. We then ask human annotators to provide discrete ratings for\nboth completions. This process is repeated over multiple turns, resulting in a multi-turn dataset. If the two\ncompletion ratings are not tied and the better completion does not obtain the highest possible rating, we\nask the annotator to improve the better completion.",
            "content": "SFT samples. SFT datasets are constructed using the human rewrites obtained from the process mentioned above, to ensure the highest completion quality. Preference pairs. We construct preference pairs directly from the obtained samples by considering completions with different ratings (including the human rewrites), with ties excluded. It is worth noting that the obtained preference samples are used to train both Command and our reward model itself. Iterative Reward-based Sample Refinement"
        },
        {
            "title": "3.3.1.3 Preambles\nA specific focus of the instruction-following post-training of Command A lies in the model’s ability to follow\npreamble (or system prompt) requirements. Preambles are designed to contain instructions that should apply\nto an entire conversation and potentially multiple model inputs, meaning that instead of having to repeat\ninstructions in every prompt, they can be defined directly in the preamble. Such system instructions could\nspecify what language the model should reply in (e.g., “Always respond in French.”), the desired format of\nmodel generations (e.g., “Always use JSON.”, “Do not generate Markdown.”), or the exclusion of specific words\nand phrases (e.g., “Do not use the word ‘LLM’ in your response.”). To train Command A to follow preamble\ninstructions, we develop methods based on synthetic data generation to create diverse preambles that are\nattached to prompts flowing into the above-described pipeline. The preambles are then taken into account\nwhen creating the respective completions and preferences, i.e., preamble-augmented data is used during\nboth SFT and preference tuning. During preamble generation, we aim to maximise instruction diversity to\nencourage robustness to a wide range of instructions at inference time.",
            "content": ""
        },
        {
            "title": "3.3.2.1 Agentic Tool-Use\nEmpowering LLMs with Tools. LLMs have demonstrated remarkable proficiency in leveraging external\ntools to enhance their capabilities (Schick et al., 2023). By generating API calls, models can execute specific\nactions—like performing calculations or retrieving information—to solve tasks more effectively. This process\ntypically involves providing a set of tool definitions in the model’s preamble. When faced with a task, the\nmodel selects and invokes the appropriate tools, and the results are fed back to inform its final response.",
            "content": "A prime example of this is Retrieval-Augmented Generation (RAG). In RAG (Lewis et al., 2020), the model has access to search tool (e.g. dense embedding index) to answer information-seeking queries. It generates search queries, retrieves relevant snippets from the selected knowledge source, and uses this context to craft well-informed response. Agents. For more intricate tasks, models may need to orchestrate multiple tools across several steps. This requires structured approach to halt generation, extract tool calls, execute them, and reintroduce results into the models workflowa process repeated until the task is resolved. We roughly follow the ReAct framework (Yao et al., 2022), widely adopted method for guiding LLMs through dynamic problem-solving. ReAct enables models to interleave reasoning and action: they first articulate their thought process, outlining plans and tool requirements, then either execute tool (via structured outputs like JSON) or deliver final answer. This iterative loop enables adaptive planning, reflection, and interaction with external systems, making it ideal for complex, multi-step tasks."
        },
        {
            "title": "3.3.2.2 Data and Training\nWe train our model on a combination of human-annotated and synthetically generated data. We collect\ndatapoints in multiple languages to directly supervise on, as well as datapoints with preference judgments\nfor multiple completions. The data covers areas of code tool execution, user-uploaded documents, and general\nAPI environments. Training consists of an SFT step on agentic datasets followed by offline preference tuning\nusing the Contrastive Policy Gradient loss (§3.2.2.2).",
            "content": "Data format. Each datapoint contains user prompt along with set of available tools and potentially custom model instructions that the user has provided to the model. The datapoint also contains reasoning step, where the model reasons about which tools to use to fulfil the user request and how to fill in the input parameters of the tools. This is followed by tool calls and tool outputs, which can be concurrent or sequential. The datapoint concludes with model response that includes citations to the tool outputs. Data collection. Annotation is performed by internal annotators specifically trained in ReAct-style data. All annotated data used for SFT is reviewed multiple times by different annotators to ensure correctness and 9 quality. For preference data, we use majority vote of at least 3 annotators to collect preference judgments. Synthetic data. We also generate synthetic training data containing whole trajectories of reasoning and tool calls. We verify the quality of the trajectories using internal LLMs-as-a-judge."
        },
        {
            "title": "3.3.3.2 Multilingual Data Annotation\nMultilingual data annotation is performed by internal and external multilingual annotators who are expertly\ntrained for annotation within various tasks. It can be divided into two distinct processes, i.e., regionally-\nrelevant data annotation and complex multilingual task annotation, which cover use cases for both SFT\nand preference tuning. For complex tasks such as domain-specific RAG, long-context reasoning, or agentic\ntool-use tasks, we conduct human annotation using two different approaches: 1) LLM-generated response\nwith human post-editing; and 2) manually annotated human data. The prior helps us scale the quantity of\ndata, while the latter helps develop high-quality multilingual data for tackling complex tasks. We develop\na customised in-house data annotation platform that can support both of these use cases. The high-quality\ndata generated from human annotations also helps to further improve the quality of the machine-generated\nresponses providing a positive feedback loop within the annotation process.",
            "content": "Multilingual Best-of-N. To further improve the multilingual quality of Command A, we conduct iterative synthetic data collection through multilingual best-of-N (Stiennon et al., 2020; Eisenstein et al., 2024). Using collection of high-quality prompts, we collect responses from all expert models and score them using our internal reward models and select the best response to be used in our iterative training. This approach is very similar to multilingual arbitrage where the model is trained on responses from diverse teacher models. Using this approach, we observe from human evaluation that LLMs can produce responses that are comparable or even better than the human-written gold label provided in many multilingual datasets."
        },
        {
            "title": "3.3.3.3 Training\nThe multilingual expert model is trained via both SFT and Preference Tuning (full details in Appendix B.2).\nWe find that training several models with the same configuration (but a different random seed) and uniformly\nmerging them gives a slight performance boost for the expert at the SFT stage, but does not help at the\npreference tuning stage.",
            "content": ""
        },
        {
            "title": "3.3.4.1 Data\nData Mixture. Our data mix focuses on 8 priority programming languages (Python, Java, C++, Rust,\nGo, JavaScript, TypeScript, COBOL) and 5 dialects of SQL (SQLite, MySQL, PostgreSQL, Microsoft T-\nSQL, Oracle PL/SQL). Across these languages, we target a wide range of tasks including code generation\n(i.e., NL-to-code), code translation, and code optimisation. Within these tasks, we include diverse domains\nsuch as interview-style questions; repository-level queries; and enterprise-specific demands (including high-\ndimensional arrays, complex debugging, data processing, and visualisation).",
            "content": "Prompts and completions are sourced from annotation campaigns and synthetic generation pipelines. We enrich prompt-completion pairs with additional information including execution feedback, explanations, paraphrases, stack traces, database context (Chang & Fosler-Lussier, 2023), diff patch formats, and unit-testing requirements. We prioritize candidate data with positive execution-based validation to filter erroneous or unverifiable code. This includes passing gold-standard unit tests or correct and error-free execution within database. We use multi-language code execution sandbox to evaluate code correctness in an isolated environment similar to Guo et al. (2024) and Team et al. (2025). During pretraining, we perform execution-based code data enrichment. We isolate self-contained functions and files and add print statements of some variables and generate synthetically valid input parameters. The resulting code is executed in sandbox and the output is appended to the enriched source code, adding several billion pretraining tokens. There is the added benefit that subset of code repositories can be formatted as very long document where files are linearised following graph traversal defined by import links. In the RL stage, we jointly optimise for code correctness and annotated preferences between code completions. This approach enhances both the functional accuracy of generated code and reduces edit times of technically correct but suboptimal or dispreferred generations. We quantify performance using the proportion of unit tests passed in our code execution sandbox, where reward of 1.0 indicates 100% test success and 0.2 represents 20% test success. When no valid code block is detected, we assign reward of 1.0 to explicitly penalize non-code output. We use synthetic unit-test generation ensuring all code completions have minimum of 4 tests per sample. Our synthetic test generation pipeline is similar to Zeng et al. (2025) with more robust unittest tests over assert statements. The preferred SQL completions are canonicalised using static query analysis. Beyond verifiable metrics, we incorporate DPO-style preference pairs (Rafailov et al., 2024) to optimise for code style conventions, documentation structure, and formatting consistency. Synthetic Data. We experiment with synthetic data pipelines for post-training data enrichment. As result, high proportion of our data are verified synthetic examples in many coding languages. For synthetic generation sources, we exploit our highest performing models for code, and generalist models for explanations and reasoning. We experiment with both novel data synthesis and conditional synthetic data augmentation. Our novel data synthesis efforts include generating examples taking inspiration from concepts, similar to StarCoder (Wei et al., 2024), and sampling pretraining programming queries. We explore pipelines where our synthesis is Python-only followed by translating code and unit tests into additional languages, or direct generation into any programming language. While the former is useful for generating parallel corpora and targeting Python benchmarks, the latter pipeline proves valuable for problems using absent or uncommon features of Python (e.g., multithreading or memory management for C++). We use our execution sandbox to verify all synthetic completions ensuring that any synthetic example teaches novel skill via verified code. This approach to data synthesis only improves performance for small models (i.e., data-based distillation from larger models). Novel synthesis methods yield negligible improvement for larger models, instead requiring human annotation and synthetic data augmentation to advance our most capable coding experts. We rely on synthetic data augmentation to diversify the style, syntax, and complexity of our code data. Our data augmentation pipeline includes prompt paraphrasing, injecting stricter requirements into prompts for more precise instruction following, and complexifying prompts similar to Magicoder (Wei et al., 2023). In verifiable scenarios, we use execution feedback to build data for code repair or translation, where iterative feedback provides guidance until the repaired code passes all tests. In similar scenario for SQL, repaired or translated SQL is adequate when it returns an equivalent answer from target language database. This offline pipeline can generate prompt-completion pairs, but we also cast this iterative process into multi-turn data to simulate conversational code repair. We also use synthetic augmentation to improve non-verifiable aspects of our data. This includes code explanations, markdown style formatting, technical precision, and global completion structure. We use reward modelling and majority voting to score these non-verifiable code completions. We also elicit feedback from human annotators to guide our data synthesis pipeline towards developer preferences for code style, structure, and explanations. This regularises against overfitting to the preferred style of any LLM judge, and our generations target style and structural features are actually preferred by human raters."
        },
        {
            "title": "3.3.4.2 Training\nThe code expert is trained in three stages (hyperparameters and full details in Appendix B.3):",
            "content": "Stage 1 is large-scale supervised learning, with the code data mixture described above. This stage includes data for all relevant tasks to optimise high level of coding capability. To mitigate variance in initialisation, learning trajectory, and performance on small evaluation datasets we use linear merging over the top seeds (Izmailov et al., 2018; Team et al., 2024; Yadav et al., 2024; Aakanksha et al., 2024; Khalifa et al., 2025) where is typically 2 or 3. We observe that this merged model is strictly superior initialisation for continued training with additional fine-tuning or RL. Stage 2 is supervised learning on only high-quality data. From the first stage fine-tuning, we further strengthen our code expert with additional fine-tuning on our highest-quality code generation datasets. We define high-quality as verifiable human or synthetic data from our best experts, or data rated highly by internal reward models. As before, we train multiple candidate models and merge across random seeds to produce the final SFT code expert. This secondary fine-tuning stage increased our key benchmark performance with negligible regression in tasks only present in stage 1 training (e.g., SQL query optimisation). Stage 3 is RL over scored or preference-pair data. We train the expert with the offline Contrastive Policy Gradient algorithm (3.2.2), to train with execution feedback and DPO-style preference pairs as described above. To ensure stable RL, we introduce three regularisation schemas. First, we repeat the Stage 2 highquality supervised learning and merging process on any non-code expert model (e.g., merge of multiple experts). CoPG on merged checkpoint was strictly more stable and yielded better results than RL on top of an individual SFT/merge. Second, we introduce hybrid cross-entropy loss function on top of CoPG to sample steps of typical supervised learning from the same Stage 2 data mix. Third, we use WARP-style merging (Ramé et al., 2024) to combine the final model trained with RL to the parent checkpoint. This hybrid approach ensures stable reinforcement learning optimisation to improve both our code experts for user preference, and improving our performance on intrinsic code generation capabilities."
        },
        {
            "title": "3.3.5 Math and Reasoning\nSophisticated reasoning abilities are a necessary competency area for generalisation in LLMs (Guo et al.,\n2025; Team et al., 2025; Toshniwal et al., 2024). We focus primarily on core mathematical reasoning as it\nis both intrinsically useful (e.g., in financial use cases) and yields out-of-distribution improvements in other\nknowledge-intensive tasks such as coding and data manipulation (Islam et al., 2023; Shao et al., 2024).",
            "content": ""
        },
        {
            "title": "3.3.5.2 Training\nSupervised Fine-Tuning. For SFT, we leverage synthetic mathematical reasoning datasets that have\nundergone extensive LLM-driven filtering for solution correctness. We find, similar to Toshniwal et al.\n(2024), that strict correctness cut-offs are not needed for optimal SFT performance.",
            "content": "Preference Tuning. We employ preference tuning following SFT across one of two datasets, dependent on the downstream model training stages: The first dataset is comprised of human-rated preferences on paired completions to reasoning prompts. The second, fully-synthetic dataset comprises correct and incorrect paired solutions to reasoning prompts. We find that, unlike in SFT, solution correctness is of critical importance in preference training (e.g., so that preferences are not accidentally inverted), and in the absence of humanwritten ratings, we use mixture of programmatic and model-driven verifiers to evaluate solution correctness. Merging. We find that using candidate models exhibiting maximal reasoning performance is sometimes detrimental to the cross-capability merge under particular merging strategies. We observe that optimal merged performance is achieved when first merging various reasoning-tuned and instruction-tuned expert models, and this yields sufficiently high-signal proxy for selecting Pareto-optimal candidates to merge with broader mix of models downstream. We employ this selection for our final set of candidate models, with the exact selection criteria along the Pareto frontier being dependent on downstream merging strategies. Training hyperparameters for SFT and preference tuning are in Appendix B.4."
        },
        {
            "title": "3.3.6 Long Context\nData. Given the complexity of human annotation for long-context tasks, we synthetically generate long-\ncontext examples. We sample from our long-context pretraining dataset and prompt Command R+ Refresh\nto generate question-answer pairs based on randomly selected fragments within 8,192 tokens (Xiong et al.,\n2024). To ensure high-quality, we use our reward model to select the best generation from a pool of candidates.\nThe selected question-answer pairs are then concatenated to the original samples to construct our synthetic\ndata.",
            "content": "Training. We perform one stage of SFT on top of the pretrained model, following similar approach to our cooldown phase. We use an interleaved training regime with datasets of 16k and 256k sequence lengths at 3:1 interleaving ratio. Hyperparameters are in Appendix B.5."
        },
        {
            "title": "3.3.7 Safety\nAI Safety focuses on quantifying and mitigating harms that can occur from using a given AI model, either\nto the end user, to the company deploying it, or to society at large. Harms can arise from a single piece of\ngenerated content (e.g. hate speech). They can also be distribution-based, which is the case when the model\nis biased towards certain groups. This section focuses on model safety at the instance level, that is, how we\ndecrease the risks stemming from single generative instances of a given model. We include a distribution-based\nevaluation (§4.6) and consider it to be a form of robustness (Seshadri & Goldfarb-Tarrant, 2025).",
            "content": "Coheres core Safety behaviour. We focus on practical safety considerations, driven both by model capabilities and deployment use cases. We consider two main settings in which our models can be deployed: The Default setting, in which the model is used entirely outside of Cohere (e.g. open weights release). In this scenario, we lack control of the preamble structure and deployment context. We ensure that the model behaves according to Coheres Core Safety behaviour in this general setting. The Enterprise setting, in which the model is deployed by Cohere to one of our enterprise partners. 13 Here the safety behaviour of the model is controllable by the preamble, to meet different enterprise needs exceeding Core Safety behaviour. The controllable behaviours are called \"Safety modes\". There are currently two safety modes; contextual, and strict. Our Core Safety behaviour focuses of five key areas where we want to prevent the purposeful propagation of harmful content online: Violence and hate, Misinformation, Self-harm, Child Sexual Exploitation and Abuse (CSEA) and Sexual content. In the default setting, we expect the model to be able to answer requests for information on those topics (covering factual elements such as statistics, educational content); however it should not generate any unsafe content, that is, supporting, encouraging or otherwise enabling harm. In the enterprise setting, the contextual mode is similar, but allows sexual content. The model behaviour can be made stricter by using the strict mode, which prevents the model from covering any topic related to our key focus areas, as well as from generating profanity."
        },
        {
            "title": "3.3.7.1 Data\nPretraining. We perform two stages of safety-related pretraining filtering: first, we remove known domains\nfor CSEA and sexual content, and second, we use a classifier to remove generally low quality content, including\nsexual content.",
            "content": "Post-training. In post-training, we use both SFT and preference datasets, with combination of manual and automated labels. Safety annotation is performed by internal annotators and specialist external vendors, who are specifically trained for our Safety concepts and tasks. Our close interaction with internal Safety annotators provides additional benefits due to the potentially distressing nature of the data. We increase the diversity of our post-training data via both LLM personas and LLM-based reformulations. We generate completions corresponding to different styles, identities and belief systems via diverse LLM personas. Additionally, we use our LLM to reformulate content (preserving overall semantics but changing form), thus increasing data diversity and making sure that the preferred completions are consistent with our refusal policy (in particular, the model should not apologise for refusing to generate unsafe content, which creates common dataset artifact (Chen & Goldfarb-Tarrant, 2025)). Balancing safety and refusal behaviour. Ensuring that the model cannot produce harmful content means that lot of training data shows refusal as the preferred behaviour. It is crucial to balance such data points with authorised user requests and compliant completions to prevent the model from over-relying on refusal as previously referred to in the literature as the balance between harmlessness and helpfulness (Bai et al., 2022). The balancing prompts can be split into two sets: user requests which are information requests on safety topics, and benign user requests with similar vocabulary and structure as unsafe prompts."
        },
        {
            "title": "3.3.7.2 Training\nImproving overall model safety means finding a fine balance between over- and under-refusal. We find it\ncrucial to split datasets in two: namely in their safety-increasing (where the model should refuse) and\nhelpfulness-inducing (where the model should answer) components. This allows us to balance these aspects\ndifferently during training. We use both SFT and offline preference tuning. We find offline preference tuning\ncrucial in limiting over-refusal, however, it is less efficient than SFT at making the model safer. We observe\nthis behaviour both on 8B models and 111B models, with the main difference between the two regimes being\nthe effect of regularisation, with larger models more prone to overfitting. Overall, the biggest impact on our\nmodel’s ability to respond safely and helpfully is achieved in the polishing process described in Section 3.5.",
            "content": "The Safety expert differs from other experts in that during the preference tuning stage we combine an offline preference loss with an equally weighted SFT loss. Preference tuning focuses on reinforcing helpfulness via helpfulness preference pairs, while SFT focuses on reinforcing safety via safety-inducing data. We find that IPO and DPO perform similarly, with SLiC showing worse trade-off between overand under-refusal, so we use IPO. Full details on SFT and preference hyperparameters are in Appendix B.6."
        },
        {
            "title": "3.4.1 Definition\nModel merging refers to the process of combining a set of model parameters θi for i ∈ [1, K], into a single\ncombined model θmerged = f (θ1, ..., θK), where f (·) is some merging function. The merging function can\nrange in complexity from simple averaging (Izmailov et al., 2018; Wortsman et al., 2022; Li et al., 2022) to\nmethods based on Fisher information (Matena & Raffel, 2022) and sign agreement between models (Yadav\net al., 2023; Yu et al., 2024). Model merging produces a single set of model parameters, resulting in faster\ninference than ensembling and lower memory requirements than runtime query routing.",
            "content": "Figure 4: Model merging allows teams to build domain expert models that excel at different capabilities independently. These experts are merged into single model that retains close-to-expert capability levels across multiple domains or capabilities."
        },
        {
            "title": "3.4.2 Merging Taxonomy\nWe here list the different merging techniques, each using different models and having different goals.",
            "content": "Expert merging. Expert merging refers to the process of combining set of models with different capabilities to produce single monolithic model with those capabilities. In this setting, the input models will likely be trained on various datasets and exhibit performance along single domain only. The aim is to produce single set of parameters that preserves as much of the individual expert performance as possible. Expert merging is core feature of the Command training pipeline, and we describe it in more detail in 3.4.3. Merging as Polyak averaging. Merging may be used to achieve form of Polyak averaging (Ruppert, 1988; Polyak & Juditsky, 1992). Here, the input models are checkpoints from different points along single training run, and merging acts as smoothing operation that reduces the effects of noise inherent in stochastic gradient-based optimisation. Seed merging. Merging may also reduce the effects of random seeds (e.g., for initialization or data loader ordering). Merging the final checkpoints from multiple equivalent training runs with different random seeds can reduce the risk of overfitting and lead to more robust model. Interpolation for capability recovery. We observe multiple instances of capability forgetting Kirkpatrick et al. (2017), whereby training an expert on one capability degrades performance on other capabilities. This is particular issue for long-context abilities since experts are generally trained on top of long-context capable model but with training schemes that use short context lengths. In this situation, merging an expert with the original base model can recover significant proportion of the original capability while retaining the new expert capability. This setting is closely related to the WARP approach (Ramé et al., 2024)."
        },
        {
            "title": "3.4.3 Expert Merging\nThe overall goal for an enterprise-ready LLM is a single monolithic model, with multiple capabilities. These\ncapabilities can sometimes be orthogonal (e.g., code and safety competencies have very different data distri-\nbutions) and may involve different scales of training data. For example, it is more straightforward to generate\nhigh volumes of synthetic data in more easily verifiable domains, such as code and reasoning, compared to\ndomains like safety or RAG, where human-annotated data is more prevalent. These differences introduce\ntechnical and operational challenges: how can we enable asynchronous development of model capabilities, and\njointly optimise for a range of capabilities with highly varied training dynamics?",
            "content": "Model merging enables multiple teams to work asynchronously on improving different capabilities, with their contributions merged together in parameter space. The capabilities exhibited by Command cover wide range of data scales, that would be non-trivial to combine into single dataset and optimisation strategy. Merging allows each team to separately optimise hyperparameters and data scale for peak performance on their capability of interest. Our final model was informed by 500 separate evaluation metrics, which would have been significantly less practical in more centralised organisational structure. Merging is computationally cheap, allowing us to quickly and easily rebalance the capabilities of the final model. We apply merging at two points in the overall training pipeline: firstly, to combine set of expert models trained using SFT into an initial SFT model soup; secondly, to combine set of experts that were trained using offline preference optimisation techniques on top of the SFT soup, giving an off-pref soup. At both stages, our aim is to jointly maintain as high proportion of the expert capability as possible while also allowing for rebalancing of the overall capabilities of the final model."
        },
        {
            "title": "3.4.3.2 Consistency is more important than optimality\nAll expert models are initialised from a common ‘general instruction following’ model, for two reasons.\nFirstly, some domain experts make use of special tokens (e.g., tool calls) whose embeddings otherwise remain\nuntrained. We find that selectively merging these embeddings only from checkpoints where they are trained\nis beneficial, but suboptimal. Using a shared generalised instruction-following model as initialisation for\neach expert and merging the special token embeddings as normal performs much better, even though these\nembeddings are likely to be lower quality. Secondly, we find that the post-training process generally degrades\nlong-context performance, and that this is challenging to recover. Starting from a generalised model that is\n‘long-context capable’ preserves long-context performance more easily throughout the training pipeline.",
            "content": "We find it valuable to include leave-one-out merges as part of the search process, to reveal instances where one expert model causes performance degradation of others, or collisions. To address this, we include small amount of cross-domain data in each experts training, to act as regulariser and ensure that each expert remains compatible with the other experts. We also observe that collisions can be caused by small inconsistencies in the style or formatting of the common data between experts. In combination this implies that maintaining some consistency between expert models is more important than absolute expert performance. 2For linear merging, the weights must sum to 1. Increasing the weight of one expert therefore requires reducing the weight of one or more of the other experts."
        },
        {
            "title": "3.5 Polishing\nModel merging provides a powerful mechanism for combining a diverse set of experts into a single model.\nHowever, combining experts trained to target specific capabilities does not guarantee the final model’s\nalignment with human preferences. To address this, we introduce a polishing phase as the final post-training\nstep. This phase serves two critical purposes: fixing any artifacts introduced during model merging and\naligning the final model with human preferences.",
            "content": "Unlike other specific capabilities such as coding or instruction-following, human alignment has crossdomain effect and influences every aspect of the models behaviour. The polishing phase ensures that the model adheres to human expectations, including tone and style, without sacrificing technical competence. Polishing is divided into three steps. First, we apply SFT on subset of our highest quality datasets. Second, we apply offline preference tuning, and finally, we apply online Reinforcement Learning from Human Feedback (RLHF). We find that ping-pong-style interleaving of offline and online preference learning helps improve alignment with human preferences while avoiding regressions and mitigating reward model hacking. Supervised Fine-Tuning (SFT). We employ best-of-N SFT approach (Stiennon et al., 2020) where we synthetically generate four candidate completions for each prompt. We leverage our reward model (3.2.3) trained on human preference data to rank these completions. We then apply SFT using the highest-ranked completions, ensuring that the model learns from the most highly rewarded responses. Preference Tuning. We use offline preference training to align our model with human preferences. We select completions with the highest reward scores as preferred completions, and use the completions with the lowest reward scores as dis-preferred. Additionally, we refine the dataset by filtering out prompts exhibiting low average reward. To further improve the models proficiency in instruction-following, mathematical reasoning, and coding, we incorporate domain-specific preference data into our training mixture. For instruction-following, completions that correctly adhere to all instructions are considered preferred, while completions failing to meet all instruction criterion are labelled as dis-preferred. To construct preference data for mathematical reasoning, we categorise completions that yield correct answers as preferred and those failing to produce accurate solutions as dis-preferred. Similarly, for code generation tasks, code snippets passing all unit tests serve as preferred completions, while those failing the tests are used as dis-preferred completions. We also filter these preference datasets by removing samples for which the preferred completions are assigned lower score than the dis-preferred completions by our reward model. We rely again on the SRPO loss due to its robustness and its self-refinement abilities (3.2.2.1). In our implementation of SRPO, following Grinsztajn et al. (2024), we average the log-likelihoods of preferred and dispreferred completions to control for variations in the completion length. Reinforcement Learning from Human Feedback (RLHF). To enhance the alignment of our model with human preferences, we further employ Reinforcement Learning from Human Feedback (RLHF). We use online CoPG (3.2.2.2) with two generations per prompt. The prompts used for RLHF training are derived from subset of those previously used during SFT, including reasoning, multilingual, coding, and preference-based tasks prompts. We regularize training using an auxiliary L2 loss with the reference policy, and an SFT loss using high-quality subset of post-training data. Area Benchmarks Academic, General Knowledge and Instruction Following (4.1) MMLU; MMLU-Pro; GPQA; IFEval; InFoBench Agents and Tool-Use (4.2) TauBench; BFCL. Multilingual (4.3) Code (4.4) MMMLU; NTREX; FLoReS; MGSM; mArenaHard (LLM-as-a-Judge); Language Confusion Benchmark; Al-Qasida; INCLUDE 44; mTauBench. LBPP; HumanEvalPack; MBPP+; Spider; Bird SQL; RepoQA; LiveCodeBench; BigCodeBench; SWE-Bench Diff Generation; Aider Polyglot; internal datasets. Math and Reasoning (4.5) MATH; AIME; LiveBenchMath; Waterloo; OpenQuant; FinanceBench; OmniMath. Safety (4.6) XSTest; internal datasets. Long-Context (4.8) Needle-in-a-Haystack; RULER; RulerQA. Table 2: Benchmark datasets used to evaluate Command models, grouped by area."
        },
        {
            "title": "4 Results\nWe report results from a diverse and extensive set of evaluations benchmarking the performance of Command\nA and Command R7B. We evaluate a broad range of capabilities using public academic datasets and internal\nevaluations. Table 2 gives an overview of the capability areas we focus on and the corresponding benchmarks.\nWe present a snapshot of results on a representative subset of these evaluations in Table 1 opening this report.\nFull details for each dataset are available in the corresponding sections.",
            "content": "We compare our models against open and closed models in similar parameter count ranges. Wherever possible, we show externally reported results with comparable evaluation settings. Where these are not available, we attempt to internally reproduce these results as faithfully as possible given the information provided publicly."
        },
        {
            "title": "4.1 Standard Benchmarks\nWhile our primary aim is to build a highly performant model for enterprise use cases (§4.7), we also measure\nperformance on standard academic datasets to evaluate baseline model knowledge and capabilities. Where\napplicable (MMLU, MMLU-Pro, GPQA), we follow the simple-evals implementation, including data, task\nsettings, prompting, and answer parsing. More details can be found in Appendix B.7.",
            "content": "Model Command GPT-4o DeepSeek V3 Llama 3.3 70B Instruct Llama 3.1 405B Instruct Mistral Large 2 Claude 3.5 Sonnet Gemini 2.0 Pro Command R7B Llama 3.1 8B Instruct Ministral 8B Gemma 2 9B Instruct Gemini 1.5 Flash-8B MMLU MMLU-Pro GPQA IFEval InFoBench 85.5 89.2 88.5 86.0 88.6 85.2 89.5 89.3 65.2 71.1 71.1 73.5 74.8 69.6 77.9 75.9 66.0 73.0 67.9 78.0 79.1 42.4 46.5 43.0 50.6 48. 50.8 53.6 59.1 50.5 49.0 48.6 65.0 64.7 26.3 23.4 23.4 31.3 31.6 90.9 83.8 86.1 92.1 88.6 83.8 90.2 87.3 77.9 78.6 59.0 74.4 88.0 94.9 94.0 94.3 92.8 93.9 93.3 93.9 92.2 85.6 90.1 88.3 87.2 88. Table 3: Results for Command and Command R7B on standard academic benchmarks. Note that for IFEval, Liu et al. (2024a) report only the prompt-level strict accuracy. We report the average of the promptand instruction-level strict accuracies for all other models (see Appendix B.7). We note that academic benchmarks have various limitations such as saturation, bias and alignment to real-world performance (Kiela et al., 2021). Human assessment of model capabilities can be undesirably 18 influenced by confounders (Hosking et al., 2024), be subject to idiosyncratic, conversational and demographic variance (Kirk et al., 2024), and demonstrates imperfect correlation to academic benchmarks (Schaeffer et al., 2025). Enterprise-relevant capabilities are often not well-represented in these benchmarks, so we augment our evaluations with enterprise-oriented signal (e.g. 4.2, 4.7), and human annotation based evaluation (4.11). Table 3 shows results on these selected benchmarks. Command is competitive across all benchmarks, generally outperforming similarly-sized models while remaining competitive with considerably larger and less-efficient models. On the instruction-following benchmarks, we observe that Command performs competitively across both IFEval and InFoBench. Specifically, it outperforms all similarly sized models on InFoBench and is outperformed only by Llama 3.3 70B Instruct on IFEval. We also note that Command represents substantial improvement over our previous Command R+ Refresh model."
        },
        {
            "title": "4.2 Agentic Tool Use",
            "content": "Model Command GPT-4o DeepSeek V3 ChatRAGBench StrategyQA Bamboogle DROP HotPotQA 72.9 66.6 40.3 76.7 81.2 73.8 76.0 76.0 70.4 91.1 89.5 85.7 92.1 92.1 90. Table 4: Standard RAG evaluations. Correctness is determined following the procedure in Verga et al. (2024) where panel of LLMs judges the models generation against reference answer. Model Command Llama 3.3 70B Instruct Mistral Large 2 Qwen 2.5 72B Instruct Claude 3.5 Sonnet Claude 3.7 Sonnet DeepSeek V3 GPT-4o Command R7B Llama 3.1 8B Instruct Gemma 2 9B Instruct Ministral 8B Qwen 2.5 7B Instruct BFCL Overall Live AST Multi-turn 63.8 51.4 58.5 63.5 56.5 58.3 58.6 72.1 52.2 50.9 51.6 51.8 53.7 80.5 62.8 69.9 79.0 78.9 78.4 68.4 79.8 69.2 61.1 68.0 64.9 67.4 25.5 6.9 23.8 24.6 41.0 48.4 18.6 47. 5.0 9.6 1.6 11.4 7.6 Table 5: BFCL Results. All numbers taken from official leaderboard. Where leaderboard entries exist for both function calling and prompted, we take the larger of the two reported values. Model Taubench Retail Taubench Airline Command Llama 3.3 70B Instruct Mistral Large 2 Llama 3.1 405B Instruct DeepSeek V3 GPT-4o Claude 3.5 Sonnet P@1 P@2 P@3 P@4 P@1 P@ P@3 P@4 60.0 6.2 53.3 29.1 54.8 60.6 69.2 49.8 5.7 37.8 17.5 41.2 49.0 57.6 44.1 5.49 29.0 12.8 34.1 42.4 50.9 40.4 5.3 23.1 10.4 30.4 37.7 46. 45.3 35.3 27.2 26.0 25.5 43.0 46.0 36.9 33.6 14.2 17.3 14.0 31.8 32.6 32.2 32.4 9.4 13.5 12.0 26.3 26.3 29.0 31.5 7.1 12.0 12.0 22.3 22.5 Table 6: Taubench Results. We follow the original experimental setup from Yao et al. (2024). Pass@k (P@k) evaluates models consistency; for example, Pass@4 is the probability that model answers the same question correctly 4 times. Scores are aggregated over 10 runs. . 19 Standard RAG Benchmarks. We evaluate on several RAG benchmarks that test the models ability to answer questions conditioned on source documents. In DROP (Dua et al., 2019) and HotPotQA-distractor (Yang et al., 2018), the model is given question and set of pre-retrieved relevant documents. Bamboogle (Press et al., 2022) and StrategyQA (Geva et al., 2021) are multi-hop question answering datasets where models must submit one or more sequential or parallel queries to search engine to gather documents and arrive at the answer. Finally, we show results averaged over the ten datasets in ChatRAGBench (Liu et al., 2024d) that cover variety of domains situated in multi-turn conversation. Results are shown in Table 4. Berkeley Function-Calling Leaderboard (BFCL). BFCL is one of the most widely used evaluations of LLM tool use / function calling capabilities and maintains an independently run leaderboard (Yan et al., 2024). Evaluations include simple single step tool calls, measures of tool irrelevance, and multi-turn subset which simulates much harder scenarios over long action trajectories. Results are shown in Table 5. Taubench. Taubench is complex agentic tool-use benchmark that simulates customer support agent in two settings: airline and retail (Yao et al., 2024). The agent model has access to set of tools for reading and writing to provided database and must help simulated user in accomplishing given task such as changing flight or returning product order. Results are shown in Table 6."
        },
        {
            "title": "4.3 Multilingual\nCommand A supports 23 key languages of global business: English, French, Spanish, Italian, German, Por-\ntuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indone-\nsian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. We evaluate performance on many of these\nlanguages (and beyond) on both academic and internal enterprise focused benchmarks, as well as public\nbenchmarks important for business use such as language consistency and steerability, and dialect awareness.",
            "content": "We assess the general multilingual capability of Command through machine translation via NTREX128 (Federmann et al., 2022), which contains human translated news domain documents, FLORES-200 (Team et al., 2022; Goyal et al., 2022); and multilingual mathematical reasoning (MGSM; Shi et al. (2022)). We further evaluate Command As understanding of regional contexts through INCLUDE (Romanou et al., 2025), large-scale region-specific evaluation suite in 44 languages. Results for machine translation on NTREX are shown in Table 7. We use the COMET-20 metric (Rei et al., 2020), one of the top performing MT metrics (Freitag et al., 2023). Rather than mark single winning models, we mark winning clusters of models by taking into account the effect size of the metric. model is in winning cluster if its score difference to the best model is smaller than 1.67 points. This threshold equates to 75% agreement with humans (Kocmi et al., 2024): humans will agree with automatic metric on 3 out of 4 pairwise system comparisons that have difference of 1.67 COMET-20. Further academic results (MGSM and INCLUDE-44) are in Appendix B.2. To evaluate more general and diverse capabilities, we ran an LLM-as-judge arena-like evaluation of responses to mArenaHard, dataset of 500 challenging queries from Chatbot Arena, originally in English, translated into 23 languages (Dang et al., 2024). As shown in Table 8, Command is preferred across all 23 languages versus Llama 3.3 70B Instruct, Llama 3.1 405B Instruct, and DeepSeek V3. We also conduct human-annotated arena-like evaluation. Figure 5 shows the results of an internal evaluation set consisting of 100 translated prompts 3 from English that focus on instruction-following ability. Command performs favourably in multilingual head-to-head human evaluations against comparable models across 9 priority languages. Command outperforms the Llama 3.3 70B Instruct and Llama 3.1 405B Instruct across all evaluated languages. Versus DeepSeek V3 and Mistral Large 2, Command is favoured across 8 of 9 languages. Notably, Command is favoured in Chinese compared to DeepSeek V3 and is favoured in French compared to Mistral Large 2. It is also favoured against GPT-4o on Arabic and Korean, and is competitive on Spanish, German, Italian, and Chinese. 3Models commonly had issues generating completions for one of the prompts across different languages, the win/tie/loss rates for these are based on 99 prompt-completion pairs 20 2 . 1 8 0 . 3 8 8 . 2 8 8 . 2 8 7 . 2 3 . 6 7 1 . 9 7 1 . 7 7 8 . 5 7 8 . 9 6 6 . 8 0 . 7 7 3 . 6 7 6 . 2 7 8 . 3 6 3 . 2 5 7 . 3 R . h k t o l o a d e f s 8 . 8 6 3 . 6 5 7 . 3 6 3 . 8 1 . 0 8 2 . 4 6 1 . 4 7 7 . 4 7 3 . 3 7 2 . 3 8 . 6 6 6 . 0 6 2 . 4 7 7 . 8 7 8 . 7 6 2 . 6 2 . 7 6 3 . 6 5 2 . 4 7 5 . 9 7 2 . 0 6 3 . 4 8 0 . 0 6 0 . 1 9 . 6 5 1 . 6 6 7 . 0 7 9 . 3 8 0 . 5 6 8 . 6 8 . 6 7 4 . 5 7 5 . 4 6 9 . 8 6 1 . 4 6 1 . 6 2 . 2 8 7 . 0 7 8 . 0 7 6 . 8 6 9 . 7 5 7 . 4 0 . 3 8 3 . 1 6 9 . 5 8 8 . 0 6 9 . 5 6 5 . 1 8 . 4 8 9 . 5 6 8 . 6 7 8 . 5 7 2 . 5 7 6 . 4 0 . 6 6 2 . 2 7 2 . 4 8 7 . 5 6 6 . 7 7 4 . 6 7 . 5 7 8 . 3 6 9 . 6 6 2 . 5 6 9 . 2 6 4 . 5 6 . 1 8 5 . 1 7 6 . 1 6 9 . 5 7 2 . 1 8 4 . 1 3 . 7 6 8 . 8 6 9 . 7 6 6 . 7 5 0 . 5 7 2 . 2 7 . 1 6 1 . 5 8 0 . 8 6 2 . 7 5 1 . 4 7 1 . 3 3 . 1 6 3 . 5 8 9 . 8 5 4 . 8 5 5 . 0 7 4 . 0 2 . 0 7 4 . 6 5 5 . 6 5 8 . 6 5 8 . 9 6 5 . 8 8 . 5 6 8 . 5 6 1 . 4 6 5 . 2 6 0 . 6 5 1 . 8 3 . 4 5 2 . 3 5 5 . 3 5 5 . 4 5 6 . 7 4 2 . 0 3 . 8 5 3 . 8 5 9 . 5 5 0 . 6 4 0 . 9 6 7 . 2 8 . 3 6 2 . 1 6 6 . 9 4 7 . 0 4 5 . 5 6 6 . 1 6 . 2 8 0 . 3 8 7 . 7 7 5 . 8 6 5 . 0 7 0 . 5 1 . 0 5 5 . 5 6 9 . 4 6 9 . 0 6 7 . 0 6 1 . 8 3 . 8 5 8 . 9 3 4 . 5 7 6 . 4 7 2 . 3 7 7 . 9 1 . 0 7 4 . 6 5 4 . 8 5 3 . 2 7 1 . 4 7 9 . 0 6 . 3 7 1 . 9 6 3 . 0 7 9 . 7 6 7 . 7 6 2 . 0 6 . 6 4 9 . 2 6 2 . 9 5 4 . 1 6 4 . 7 5 4 . 8 3 . 3 6 2 . 3 6 3 . 7 5 8 . 1 5 2 . 2 5 2 . 0 8 . 9 5 9 . 8 5 0 . 7 5 7 . 7 5 7 . 0 5 3 . 3 8 . 3 7 7 . 0 7 5 . 9 6 4 . 8 6 5 . 8 7 3 . 3 6 . 5 7 3 . 4 7 5 . 8 5 8 . 4 6 3 . 1 6 4 . 3 6 . 9 4 9 . 5 3 3 . 8 6 9 . 3 6 2 . 3 6 9 . 9 0 . 9 1 1 . 5 1 - 6 . 7 6 3 . 4 6 7 . 5 6 7 . 2 4 . 3 6 4 . 1 6 9 . 5 5 3 . 5 5 1 . 9 4 5 . 5 6 . 3 4 8 . 8 2 8 . 2 3 0 . 3 7 3 . 3 7 4 . 1 4 . 1 7 1 . 9 6 2 . 0 7 2 . 9 6 7 . 0 8 7 . 6 5 . 1 7 7 . 9 6 7 . 4 6 8 . 1 4 2 . 0 3 0 . 9 4 . 9 5 6 . 7 5 0 . 6 5 7 . 4 5 3 . 1 8 7 . 7 1 . 6 7 8 . 0 7 8 . 1 6 0 . 6 7 0 . 6 7 3 . 4 8 . 3 6 2 . 8 6 4 . 4 6 8 . 4 7 6 . 1 8 9 . 0 3 . 5 7 3 . 5 7 0 . 4 6 0 . 9 6 6 . 3 6 7 . 5 2 . 1 8 6 . 9 6 2 . 0 7 3 . 7 6 5 . 1 6 1 . 6 5 . 9 5 0 . 1 6 0 . 5 8 8 . 9 5 6 . 3 6 9 . 0 5 . 1 6 6 . 3 6 1 . 5 7 8 . 9 5 2 . 0 7 3 . 2 6 . 7 6 5 . 9 5 1 . 6 5 2 . 1 7 0 . 8 7 8 . 4 6 . 1 5 3 . 4 6 7 . 7 4 3 . 1 7 0 . 3 7 1 . 7 0 . 2 6 6 . 1 5 7 . 9 5 2 . 9 4 8 . 6 3 0 . 0 0 . 1 5 6 . 4 4 6 . 8 3 0 . 7 4 6 . 8 5 5 . 7 1 . 1 5 6 . 3 3 1 . 9 3 7 . 3 6 9 . 3 7 5 . 9 2 . 0 6 0 . 7 4 7 . 8 3 3 . 5 - 4 . 0 7 8 . 8 3 . 3 2 1 . 7 1 9 . 6 5 0 . 7 4 1 . 8 4 2 . 6 9 . 7 6 4 . 6 6 5 . 7 5 5 . 4 3 3 . 5 1 7 . 0 5 . 6 6 2 . 9 5 7 . 3 6 9 . 3 5 7 . 9 6 9 . 6 0 . 2 7 3 . 6 6 3 . 4 6 4 . 7 6 2 . 1 6 6 . 3 4 . 1 3 1 . 0 2 2 . 7 5 6 . 2 5 8 . 4 4 0 . 8 8 . 3 5 0 . 1 4 6 . 4 3 3 . 1 1 5 . 4 5 9 . 9 5 . 2 4 2 . 8 3 7 . 2 3 9 . 3 6 2 . 5 6 4 . 4 3 . 6 7 7 . 7 6 5 . 5 5 1 . 6 5 0 . 9 5 0 . 0 9 . 6 4 0 . 8 1 1 . 0 5 0 . 6 3 2 . 0 3 2 . 9 . 2 1 - 4 . 8 6 - 1 . 2 6 4 . 2 6 7 . 4 5 9 . 6 5 . 9 4 2 . 4 4 3 . 0 4 1 . 7 2 3 . 0 1 - 5 . 4 3 - 4 . 8 6 2 . 0 7 8 . 3 6 4 . 5 6 6 . 9 5 9 . 2 4 . 1 6 8 . 0 4 9 . 4 3 5 . 7 3 - 8 . 6 5 2 . 9 3 . 6 5 1 . 0 5 0 . 2 5 0 . 9 3 0 . 3 7 2 . 1 5 . 2 4 5 . 9 2 l 0 . 2 m r 5 . i t o 7 . 3 a 3 e e n o o 4 - 0 . 2 5 1 . 2 5 2 . 5 4 3 . 8 4 6 . 0 5 . 7 4 4 . 6 4 2 . 2 4 7 . 6 2 2 . 2 1 - u u n 2 5 . 2 Q r I 5 0 4 1 . 3 l u n 0 3 . 3 l 2 a r M 8 - l 5 . 1 m k 3 a c s 2 e u n 8 1 . 3 l 7 a C 8 t i 7 . 2 u u n 7 5 . 2 Q 21 n r m y . h o s ) 0 2 - O ( O e g v ."
        },
        {
            "title": "X\nE\nR\nT\nN",
            "content": "n r ) 0 2 - O ( t n e c : b . u l d e e l g n z u r t l a t i r f n e r . 4 . 4 7 6 . 2 8 0 . 0 8 0 . 9 7 7 . 9 0 . 7 7 7 . 6 7 9 . 9 7 8 . 8 7 6 . 4 8 9 . 4 9 . 4 7 6 . 1 8 9 . 1 8 0 . 4 8 1 . 6 7 6 . 1 4 . 6 7 8 . 2 7 7 . 8 7 4 . 1 8 3 . 7 7 2 . 1 8 . 8 7 2 . 8 7 1 . 2 7 6 . 0 7 6 . 3 7 0 . 8 5 . 2 8 6 . 7 7 3 . 6 7 3 . 9 7 7 . 2 8 2 . 1 7 . 1 8 1 . 1 8 2 . 5 8 3 . 9 7 4 . 8 7 7 . 2 6 . 3 7 7 . 8 7 2 . 1 8 4 . 7 7 4 . 0 8 9 . 6 2 . 0 7 5 . 4 6 6 . 7 6 2 . 4 6 5 . 5 6 7 . 1 9 . 4 6 8 . 4 6 2 . 1 7 6 . 6 6 9 . 2 6 8 . 6 6 . 5 6 0 . 8 6 0 . 1 6 6 . 8 6 8 . 3 6 1 . 4 5 . 8 6 4 . 4 6 8 . 4 6 4 . 8 6 7 . 4 5 7 . 5 2 . 5 5 4 . 0 5 0 . 2 5 9 . 1 5 2 . 4 5 2 . 0 7 . 2 5 2 . 3 5 4 . 6 5 8 . 1 5 9 . 3 5 6 . 1 0 . 4 5 7 . 3 5 7 . 2 5 7 . 2 5 5 . 2 5 5 . 4 9 . 5 5 5 . 4 5 5 . 4 5 9 . 8 7 7 . 8 7 9 . 5 4 . 3 5 r I 0 7 3 . 3 l . d m c s 5 0 1 . 3 l . d m 2 a r i . d m 3 e e . d m . d t e - o n a a a 3 2 s g t i a e A m C : 8 a Figure 5: Head-to-head human evaluations against comparable models. 22 Command GPT-4o Gemini 1.5 Pro Gemini 2.0 Flash Mistral Large MTaubench Retail MTaubench Airline Avg. 34.3 37.3 26.4 26.0 25.8 en 60.0 59.7 49.0 44.1 54. fr 36.5 41.7 28.4 29.6 30.0 ar 28.5 29.6 20.3 20.6 18.7 ja 24.4 28.7 16.2 17.1 11. ko Avg. en fr 22.0 26.7 18.8 18.8 14.4 43.4 45.2 41.4 33.7 27. 45.3 47.3 31.7 35.3 28.6 52.7 38.7 36.9 36.0 32.0 ar 47.3 50.7 46.0 34.0 30.9 ja 38.0 41.3 47.6 29.3 21. ko 33.8 48.0 45.0 34.0 21.9 Table 9: Multilingual Taubench Results: We follow the original experimental setup from Yao et al. (2024). We report the per language pass@1 (P@1) score. Scores aggregated over 3 runs. Command Command R+ Refresh Qwen 2.5 72B Instruct Turbo Claude 3.7 Sonnet Llama 3.3 70B Instruct Gemini 1.5 Pro DeepSeek V3 GPT-4o Mistral Large 2 Avg 93.0 95.5 93.0 91.8 91.3 90.6 90.6 88.9 75. ar de es fr hi id it ja ko pt ru tr vi zh 98.2 94.8 96.4 94.5 90.5 91.7 94.9 92.2 85.3 95.5 98.2 94.0 95.5 94.7 94.4 93.8 91.0 64.7 94.8 97.2 94.3 93.2 92.9 94.5 94.2 94.9 83.4 93.5 97.2 93.7 94.2 93.0 92.0 93.5 91.7 78. 94.6 96.5 94.1 93.6 98.2 93.5 92.2 91.9 86.9 84.2 89.0 86.6 78.9 92.1 82.9 82.6 80.9 65.0 94.9 97.5 94.0 94.0 93.3 93.3 92.8 90.4 69.6 93.2 96.2 91.3 93.6 83.3 86.1 85.5 85.3 82.6 93.4 94.7 93.0 93.9 78.9 90.6 91.5 87.4 76.1 89.6 91.6 87.9 84.1 91.2 86.7 85.3 87.0 75. 93.7 96.9 95.8 95.9 95.0 93.4 92.9 87.9 75.6 93.6 97.8 95.4 92.6 92.9 94.8 92.3 90.7 69.3 92.2 98.3 95.7 95.3 95.9 95.6 91.8 88.0 73.1 90.3 90.6 89.2 86.2 86.5 79.1 84.3 85.5 77.2 Table 10: Crosslingual line-level pass rate (LPR) from the Language Confusion Benchmark (Marchisio et al., 2024). Models are prompted in English with an instruction to reply in different language. LPR measures the percentage of answers with all lines in the requested language. Command Gemini 1.5 Pro GPT-4o Claude 3.7 Sonnet DeepSeek V3 Llama 3.3 70B Instruct Qwen 2.5 72B Instruct Turbo Mistral Large 2 Command R+ Refresh Monolingual Crosslingual 24.2 19.3 15.8 8.5 15.7 15.2 9.9 6.9 1.9 33.5 26.4 24.7 23.1 15.7 8.3 9.6 7.9 6.1 Table 11: ADI2 score over monolingual and crosslingual prompts in 4 Arabic dialects (Egyptian, Saudi, Syrian, Moroccan) from Robinson et al. (2024). Higher scores indicate greater desired dialect adherence. Beyond instruction-following, agentic capabilities are important for enterprise use. We evaluate Command on our own human translated version of τ -bench (Yao et al., 2024).4 As shown in Table 9, Command outperforms other widely-adopted LLMs agentic solutions such as Mistral Large 2 and Gemini 1.5 Pro, while being competitive with GPT-4o. The Language Confusion Benchmark (Marchisio et al., 2024) measures models ability to appropriately respond in the desired language of the user. In Table 10, we measure line-level pass-rate (LPR) on crosslingual prompts. Concretely, models are prompted with an English request and an instruction to reply in another language. LPR is the percentage of responses where all lines were in the users desired language. Command and its predecessor, Command R+ Refresh, perform very strongly across languages, with the highest and second highest aggregate scores. We measure Command As sensitivity to regional dialect in Table 11, which shows ADI2 scores over monolingual and crosslingual prompts in 4 Arabic dialects (Egyptian, Saudi, Syrian, Moroccan) from Robinson et al. (2024). Higher scores indicate more adherence to the desired Arabic dialect. We observe that Command 4The number may differ slightly from the official implementation due to extensions for our multilingual evaluation pipeline. 23 strongly outperforms comparison models in its ability to adhere to dialect."
        },
        {
            "title": "4.4 Code\nWe evaluate the code capabilities of Command A across code understanding, code editing, and SQL\ngeneration benchmarks.",
            "content": "Python Multi-language COBOL MBPP+ LiveCodeBench BigCodeBench LBPP(All) HE(All) HE Python RepoQA Command Command Expert Command Agentic Command R7B Command Refresh Command R+ Refresh 86. 87.0 72.0 74.3 78.8 Llama 3.3 70B Instruct 86.0 / 81.0 Mistral Large 2 Qwen 2.5 72B Instruct 84.7 88.6 Llama 3.1 405B Instruct 88.6 / 87.0 DeepSeek V3 90.0 26.9 24.9 32. 9.0 11.0 14.4 32.9 26.7 26. 29.3 33.5 45.4 47.4 59.7 30.9 34. 25.8 46.9 / 41.9 44.7 45.8 / 43.6 46.2 50.0 / 48. 51.5 50.8 65.4 21.9 24.7 25. 47.8 54.0 48.3 52.7 61.5 25. 29.8 55.7 64.6 76.2 77.5 50.7 54.7 54.4 75.5 7. 1.9 2.5 3.2 82.9 10.8 78. 76.7 6.3 3.2 83.5 15.2 35.4 34.2 43.7 46.2 46.8 55. 59.5 63.3 92.6 91.8 69. 73.2 77.0 85.6 88.0 83.2 90. 92.2 Table 12: Code Understanding Benchmarks across Python, Multi-language, and COBOL groups reporting 1-shot pass@1 and RepoQA reporting match accuracy. HE is HumanEval. All results are internal reproductions using an identical prompt except where / indicates external value first and internal reproduction second. Best score 1% is bolded. For BigCodeBench, we use 3 tool-use execution feedback tests. Command Command Expert Command R7B Command Refresh Command R+ Refresh Llama 3.3 70B Instruct Mistral Large 2 Qwen 2.5 72B Instruct Llama 3.1 405B Instruct DeepSeek V3 SWE-Bench Verified Aider Polyglot 26.8 23.4 3.6 11.6 17.0 29.4 30.0 33.0 33.4 42.0 / 45.8 14.7 8.9 2.7 1.8 2.2 8.4 16.0 8.0 13.8 49.6 / 51.6 Table 13: Code Editing Benchmarks. All results are internal reproductions using an identical prompt except where / indicates externally reported value first and internal reproduction second. Code Understanding evaluates code generation across multiple languages. For Python generation, we report on MBPP+ (Austin et al., 2021; Liu et al., 2024c), LiveCodeBench (Jain et al., 2024, Version 5 10/24-2/25), BigCodeBench (Zhuo et al., 2024, Instruct), and RepoQA (Liu et al., 2024b, 32K context length, threshold 0.8). For multi-language generation, we report HumanEval (Chen et al., 2021; Muennighoff et al., 2023) scores in Python, C++, Java, Javascript, Go, and Rust. We also extend our earlier uncontaminated Python benchmark, Less Basic Python Problems (Matton et al., 2024, LBPP), with parallel versions in C++, Java, Javascript, Go and Rust for uncontaminated generation evaluation across enterprise-critical programming languages. To assist in future advancements in COBOL understanding, we also develop parallel version of HumanEval in COBOL (i.e., HumanEval-COBOL). We evaluate direct generation of COBOL, and translation of COBOL 5We will release this dataset in an update to huggingface.co/datasets/CohereForAI/lbpp 24 Command Command Expert Command R7B Command Refresh Command R+ Refresh Llama 3.3 70B Instruct Mistral Large 2 Qwen 2.5 72B Instruct Llama 3.1 405B Instruct DeepSeek V3 Spider Bird Internal Dev 79. 85.5 78.1 76.5 82.0 81.1 78. 83.5 83.0 81.7 Test Dev Avg. SQLite PostgreSQL MySQL PL/SQL T-SQL 80.2 85.4 77. 78.1 81.7 84.8 76.3 83.8 59. 58.5 42.2 47.3 52.7 58.0 50. 50.1 86.7 59.4 55.3 56.1 34. 42.8 44.4 45.9 53.3 53.7 49. 48.7 49.3 27.3 36.7 40.7 41. 54.0 52.7 54.0 58.0 60.0 36. 48.0 47.3 48.0 54.7 54.7 58. 56.0 55.3 34.7 42.7 40.0 43. 50.7 56.7 50.7 58.7 58.0 38. 43.3 52.0 50.0 53.3 49.3 34. 55.3 58.0 35.3 43.3 42.0 46. 54.0 55.3 48.7 81.7 53.1 60. 56.7 66.0 60.7 58.7 62.0 Table 14: SQL Generation Benchmarks reporting execution accuracy against gold databases. All results are internal reproductions using an identical prompt. Avg. is the sample-weighted average across internal multi-dialect evaluation datasets. Best score 1% is bolded. to Python similar to Muennighoff et al. (2023). The translation setting tests model capability to update legacy codebases into modern language. Code Understanding metrics are outlined in Table 12. Sources of externally reported values are in Appendix B.3. Command provides strong Python and multi-language performance compared to similar and larger models. Table 26 details the complete performance for HumanEval and LBPP in all languages, highlighting competitive accuracy in many business-critical programming languages. In the hardest benchmarks, LiveCodeBench and BigCodeBench, Command surpasses many competitors and can be further improved with agentic tool-use discussed below. Command also leads in RepoQA performance compared to all competitors. Finally, Command offers state-of-the-art capabilities in COBOL for both direct generation, via HumanEval-COBOL, and translation from HumanEval-COBOL to HumanEval-Python. These strengths highlight that Command offers accurate code understanding in the complex environment of navigating legacy enterprise codebases. We also investigate the performance of Command as code agent using multi-hop tool use similar to the setup for RAG in Section 4.2. Command can now access code execution tool and receives feedback on code generation via execution results from gold-standard unit tests similar to Gehring et al. (2025). We evaluate 3 datasets in this regime: LiveCodeBench, BigCodeBench, and LBPP(all languages). In LiveCodeBench, we use the public unit tests for execution feedback and private tests for final evaluation. For BigCodeBench and LBPP, we simulate the unit-test split by using 3 unit tests for execution feedback and all remaining tests for final evaluation.6 Table 12 shows how using Command as an agent easily surpasses direct code generation across all datasetsachieving pass@1 gain over Command of +5.9% for LiveCodeBench, +14.3% for BigCodeBench, and +12.3% for LBPP across all languages. Notably, Command achieves 71.4% in LBPP-Python surpassing all competitors by 4.3% and surpasses all other models in the BigCodeBench leaderboard at the time of publication.7 Code Editing evaluates the model capability to generate precise code line-level changes to edit and update codebase. We evaluate our models on the SWEBench Verified Patch Generation task in Python (Jimenez et al., 2024), and the Aider Polyglot benchmark8 for multi-language code editing in Python, C++, Java, Javascript, and Rust. Table 13 demonstrates Command is competitively capable in repository-level understanding and solving pull-requests or building code fixes via patch generation. We note that these results are from post-hoc investigations into code-editing behaviour in our model as we did not target these functions 6As the prompt design for LBPP includes 3 unit-tests, this setup does not leak any further testing requirements to the model. 7The current best model is GPT-4o-2024-05-13 with 51.1 pass@1 https://bigcode-bench.github.io/ 8aider.chat/2024/12/21/polyglot.html 25 Figure 6: Code Performance against Model Size. Command provides state-of-the-art performance compared to models of similar size, and often significantly larger models. Command Agent improves even further to set new standard for performance at 111B size with tool-use in code. in developing Command A. Similar to our investigation into code agents described above, we share these results as early signposts for future objectives of code expert development. SQL Generation evaluates model capability in understanding user requests using partially observed database context. Understanding SQL and reasoning with databases is critical for Command to succeed as an enterprise model. We evaluate SQLite performance using Spider (Yu et al., 2018, Dev & Test) and the more recent Bird SQL benchmark (Li et al., 2023a, Dev). To ensure Command can accurately generate SQL in an enterprise database context, we also report results for an internal benchmark in SQLite, PostgreSQL, MySQL, Oracle PL/SQL, and Microsoft T-SQL. Performance on these dialects better reflects real usage of SQL to access commercial database systems. Table 14 demonstrates that Command offers state-of-the-art performance across multiple datasets. Command leads in both Spider Dev, and Bird to provide accurate SQL generation to solve challenging queries in even dirty database contexts. Across models of similar size, Command also demonstrates the strongest average performance across 5 enterprise-critical SQL dialects in our internal benchmark. This further punctuates the capability of Command in both academic and enterprise scenarios for SQL. We highlight the performance benefit of Command relative to size in Figure 6. Across 3 datasets, Command and Command Code Expert provide best-in-class performance, often surpassing similar and larger models. Command offers unique trade-off for enterprise capability in accurate code and SQL generation. Using Command as an agent for code further enhances the model for state-of-the-art capabilities across challenging benchmarks."
        },
        {
            "title": "4.5 Math and Reasoning\nWe evaluate the reasoning capability of our model on key mathematical reasoning benchmarks, and compare\nthis to publicly-reported metrics (where available) in Table 15. We find that Command A performs especially\nwell on mathematical benchmarks, and that merging models preserves reasoning performance (compared to\nreasoning-expert models) within a few percentage points across most benchmarks (§4.9).",
            "content": "26 Command GPT-4o Llama 3.3 70B Llama 3.3 405B Mistral Large 2 MATH (all) AIME (2024) GPQA (Diamond) 80.0 68.5 77.0 73.9 71.3 23.3 9.3 20.0 20.0 11.0 50.8 46.0 50.5 49.0 48.6 Table 15: Reasoning performance of Command compared to similarly-sized models. Benchmarks are MATH (Hendrycks et al., 2021), the 2024 AIME mathematics competition, and GPQA Diamond (Rein et al., 2023). Results for external models are taken from officially-reported sources, unless indicated with an asterisk (*), which denotes internal evaluation since official public results were not available. In our qualitative assessments, we also find that reasoning-expert models provide generalised gains in coding and structured data manipulation tasks, and that these are additive in the final Command model."
        },
        {
            "title": "4.6 Safety\nOur safety evaluation methodology combines human and automated assessments. Due to speed and cost\nconsiderations, we mainly rely on automated evaluations. We use human annotations as a baseline to ensure\nour automated evaluations align with human judgment. These are triply annotated by an internal team of\nspecialist safety annotators. To further strengthen the reliability of our automated evaluation, we assess the\nsuitability of evaluators based on their robustness to artifacts (Chen & Goldfarb-Tarrant, 2025).",
            "content": "We measure both absolute and relative safety. Absolute safety evaluation tests models with potentially eliciting prompts from the categories of our core safety behaviour (3.3.7), and then computes the rate of unsafe content in the model output, using an LLM-as-a-judge setup. The absolute safety aggregate score is the average of each categorical rate, where each category is weighted equally. Relative safety evaluation uses the same prompts, but considers how the safety of each response compares to the safety of another models response for the same prompt. If both responses are equally safe, the higher quality response is chosen as the winner. Relative safety is more challenging, so we rely on jury of LLM evaluators (Verga et al., 2024), which achieves human agreement scores of 77.7% and Cohens Kappa of 0.55 in relative safety evaluations. We also measure over-refusal rate; how frequently models refuse to answer prompt that should be answered. These prompts fall into two categories: word sense disambiguation and requests for information about safety topics. We use an LLM-as-a-judge setup, as we find refusal classification much easier task than safety, with very high accuracy and human agreement"
        },
        {
            "title": "4.6.1.1 Controllability\nIn Enterprise Safety, the notion of safety itself is context-dependent. Some core safety behaviour is consistent\nacross all contexts (§3.3.7.1), but much of it varies between different deployments. The boundaries of content\nthat an LLM should generate when used as an LLM-editor for a journalist are very different than the content\nboundaries of a customer service chatbot. Therefore, we evaluate the model’s ability to accurately condition\non different safety instructions, under our two safety modes: contextual and strict (§3.3.7.1). For each mode\nwe compose two evaluation sets: one that should always be answered (over-refusal evaluation) and one that\nshould always be refused (safety mode control), which allows us to optimise the trade-off between these two\nscenarios.9 Safety mode accuracy is the mean of these sets for a given mode.",
            "content": "9We note that the over-refusal evaluation set was created by red-teaming Command R+ Refresh. 27 Figure 7: The Pareto frontier between correctly answering and refusing for our enterprise safety modes. Figure 7 shows that the Command model is on the Pareto frontier between answering and refusing for both safety modes. Results for competitor models can be found in appendix Table 27. Each competitor targets different markets and behaviours, so we consider different modes to have effectively different competitors. In contextual mode, the relevant competitors are Mistral Large 2, Qwen 2.5 72B Instruct and Llama 3.3 70B Instruct, while in strict mode the relevant competitors are GPT-4o and Claude 3.5 Sonnet."
        },
        {
            "title": "4.6.1.2 Demographic Fairness\nLLMs are used in various hiring software systems in the market, and we evaluate demographic fairness in\nthis context. The model is tasked with summarising the suitability of resumes with respect to a given job\ndescription. We follow Seshadri & Goldfarb-Tarrant (2025) for both our method and our metric. We permute\nthe demographics of the resume and measure meaningful differences in generated summaries for candidates\nwhen their race or gender has changed. A perfect model would have no meaningful differences, i.e. would be\ninvariant to the perturbation. The bias metric is defined as the proportion of measurements (including reading\nease, subjectivity and regard, as outlined in Seshadri & Goldfarb-Tarrant (2025) for which the null invariance\nhypothesis is rejected when comparing the original and perturbed summaries. To account for variability in\ngenerations (Chen & Goldfarb-Tarrant (2025) observed this even at temperature 0), we generate responses\nusing each model five times per sample and plot the distribution of bias rates across all runs. The results\nfor gender and race are shown in Figure 8. We report with both Bonferroni (bonf) and Benjamini-Hochberg\n(bh) corrections to account for the multiple measurements on the same summaries and to allow the reader\nto select whichever correction is more applicable – bonf to minimise false positives (finding a demographic\nfairness issue when there is none), and bh to minimise false negatives.",
            "content": "We note two broad patterns across all models: models tend towards much stronger racial bias than gender bias, and smaller models tend to have greater bias than larger models. In particular, the Command models show impressive robustness to demographic perturbations. Command is entirely robust to gender perturbations and very resilient to race ones (only 1% failures). Command R7B similarly is entirely robust to gender in this evaluation, and competitive for small model at robustness to race, with around 4% failures. We dont observe significant gender bias for large models in this domain in our testing setup. Command A, Llama 3.3 70B Instruct, and Mistral Large 2 all exhibit minimal racial bias, each failing median of 1% of invariance tests, while Claude 3.5 Sonnet has the lowest, at 0%. Small models are significantly less robust. 28 Figure 8: Boxplots of gender and racial bias rates in model-generated resume summaries for Command (left) and Command R7B (right) compared to similarly sized models, respectively, using either Bonferroni or Benjamini-Hochberg correction. The Command models show impressive robustness to demographic perturbations. Command is robust to gender perturbations and very resilient to race ones (only 1% failures). Command R7B similarly is robust to gender in this evaluation, and competitive for small model at robustness to race, with around 4% failures. Most small models remain robust to gender, with the exception of Llama 3.1 8B Instruct and Ministral 8B, which fail 1-5% of invariance tests. Interestingly, Ministral 8B lacks robustness to gender, but is robust to race, whereas Mixtral lacks robustness to race, but is robust to gender. Overall, our models offer excellent coverage of robustness across different demographic categories, for multiple sizes. We note that, though generation does contribute, total demographic fairness in hiring pipeline is dominated by the retrieval stage (Seshadri & Goldfarb-Tarrant, 2025). Here we measure only the generation stage, but our embedding model for the retrieval stage is also the most robust to perturbations."
        },
        {
            "title": "4.6.2 Default Safety\nIn the default setting, we evaluate the safety of the model without a system preamble to simulate cases\noutside of Cohere’s API or enterprise contexts.",
            "content": "Command shows strong performance in various categories of unsafe content. As shown in Figure 9, Command significantly outperforms all competitors in relative safety evaluations. Additionally, it attains an absolute safety score of 70.4%, ranking third among large models, closely following Claude 3.5 Sonnet and Qwen 2.5 72B Instruct  (Table 16)  . It excels at avoiding violence and hate speech, with 89.7% safe response rate, and performs well in areas such as not generating CSEA (87.5%) and not promoting misinformation (67.9%) (Figure 15). While Command R7B shows lower overall performance, it still maintains notable presence in certain categories, such as avoiding violence and hate speech (76.3%) and not promoting CSEA (67.0%) (Figure 16). These results highlight the effectiveness of Command in mitigating unsafe content generation even in situations where we cannot add system preamble guardrails. Although the relative and absolute safety performance of Command may initially seem contradictory, this 29 (a) Large Models (b) Small Models Figure 9: Default relative safety performance. Winner is assigned by panel of LLM judges. When both responses are equally safe, the winner is chosen based on which response is higher quality. Relative Safety() Absolute Safety() Misinfo() Selfharm() CSEA() Sexual Content() Violence & Hate() Command Claude 3.5 Sonnet DeepSeek V3 GPT-4o Llama 3.1 405B Llama 3.3 70B Mistral Large 2 Qwen 2.5 72B Command R+ Refresh Command R7B Gemma 2 9B Llama 3.1 8B Qwen 2.5 7B Command Refresh 49.5 26.4 23.7 26.6 15.9 14.9 22.0 32.4 16.3 49.7 81.3 35.9 59.2 30.4 70.4 80.0 49.7 65.6 41.8 40.5 45.7 71.4 30. 58.2 87.3 63.6 71.5 31.3 67.9 76.9 50.0 76.9 42.3 50.0 57.7 61.5 42.3 50.0 76.9 57.7 65.4 38.5 61.2 90.4 37.0 69.9 28.8 42.5 37.0 60.3 21.9 50.7 82.2 58.9 50.7 26.0 87.5 98.5 74.1 33.7 63.0 63.0 74.8 86.3 45. 67.0 94.8 60.7 71.9 37.8 63.1 94.4 34.8 95.5 34.3 6.7 8.0 91.6 5.1 47.2 85.4 66.3 87.6 3.9 89.7 93.1 74.0 84.4 62.2 61.8 71.0 90.0 47.3 76.3 96.9 74.4 82.1 50.4 Table 16: Default safety performance of Command and Command R7B compared to similarly sized models across various categories of unsafe content. Relative safety is the winrate vs. Command A. Absolute safety score is computed as an average of safe response rates for all categories. Large models are shown in the top half of the table, while small models are shown in the bottom half. The top performing model for each size category is bolded in each column. As indicated by the upwards-pointing arrows, higher winrates and higher safe response rates correspond to better performance for each competitor. occurs because the relative safety evaluation considers the intersection of safety and quality. Critically, in the event that both models provide safe response, the relative safety evaluations then consider the winner to be the model that provides higher quality response. Rather than simply refusing to answer, Command engages meaningfully with queries that relate to potentially unsafe topics. Many other models, such as Claude 3.5 Sonnet, provide non-specific refusals. We also measure over-refusal rates for the default setting on the XSTest benchmark (Röttger et al., 2024). Command shows refusal rates under 3%, which is considerably better than other closed-source models, namely Claude 3.5 Sonnet and GPT-4o; and marginally better than open-access models such as Llama 3.3 30 Command Claude 3.5 Sonnet DeepSeek V3 GPT-4o Llama 3.1 405B Llama 3.3 70B Mistral Large 2 Qwen 2.5 72B Command R+ Refresh Command R7B Gemma 2 9B Llama 3.1 8B Qwen 2.5 7B Command Refresh XSTest Refusal () Partial Full Over-Refusal () XSTest Internal 1.1 3.6 0.8 5.6 1.2 1.2 0.8 0.4 3.6 4.0 2.8 6.4 0.8 3.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1. 0.8 4.0 0.0 0.0 0.0 1.1 3.6 0.8 5.6 1.2 1.2 0.8 0.4 4.8 4.8 6.8 6.4 0.8 3.2 7.1 4.1 1.2 7.1 2.4 3.5 2.4 1.2 5.3 11.8 24.1 8.8 9.4 2.4 Table 17: Over-refusal rate of Command and Command R7B, based on the XSTest benchmark, which distinguishes between full refusal and partial refusal. We sum both to obtain the over-refusal rate. Figure 10: Safety scores (rate of safe responses averaged over Misinformation and Violence and Hate) across 9 languages. The dashed lines show the English score. 70B Instruct (see Table 17). As XSTest is saturated, we also report default over-refusal based on our internal test set from red-teaming our previous model."
        },
        {
            "title": "4.6.3 Multilingual Safety\nWe evaluate safety on nine priority languages (results in Figure 10). The safety score is the rate of safe\nresponses measured on the same set of prompts across all languages, thus allowing for direct comparisons.\nThis set is the Misinformation and Violence and Hate categories from English, translated automatically,\nthen corrected and naturalised by multilingual annotators. We use LLM-as-a-judge evaluation. For some\nlanguages, completions are translated into English before being evaluated, and some are retained in the\noriginal language, based on which showed the best performance on a development set.",
            "content": "We also evaluate over-refusal via prompt set collected through red teaming with the multilingual annotators. The English over-refusal set was translated into each language, then refined and augmented with more natural 31 prompts. Command is on the Pareto frontier between safety and over-refusal in 5 languages (Japanese, Chinese, German, Korean and Arabic), whilst remaining competitive in all other measured languages."
        },
        {
            "title": "4.7 Enterprise-specific Benchmarks\nOur enterprise evaluation process focuses on testing model capabilities in generative and retrieval-augmented\nuse cases that mirror typical enterprise applications.",
            "content": "Generative use cases. We use rule-based checks and LLM-as-a-judge evaluations due to the complexity and nuances of the differing enterprise use cases. We show an example prompt for reference in Figure 11."
        },
        {
            "title": "Example Prompt",
            "content": "Create job ad for the role of Social Media Manager at the BuzzMedia, located in New York, NY. The ad should be creative and engaging to attract the best talent. It should include catchy and creative title, brief role overview, and list of at least 5 employee benefits offered by the company. The overview should be at least 50 words. Format the response as JSON object in the following format. { \"title\": <title>, \"role_overview\": <role_overview>, \"employee_benefits\": [ <Benefit 1>, <Benefit 2>, <Benefit 3>, <Benefit 4>, <Benefit 5> ] } Figure 11: Example prompt for an enterprise generative use case. We break down success criteria into rule-based and LLM-based checks: Rule-Based Checks. Checks for attributes like word count, valid JSON output, count of expected items, and similar. LLM-Based Checks. We use panel of judges, similar to the approach described in Verga et al. (2024), to evaluate more nuanced criteria like tone and natural language quality. The final per-example score is an average of the ruleand LLM-based scores. Our generative enterprise benchmark consists of 22 tasks covering use cases including chat and meeting summarisation, information extraction, and FAQ generation. summary of the results across all tasks can be found in Table 18. Command achieves the highest pass rate at 94.2% across all generative use cases. Command R7B scores 71.9%, which is the highest performance among similarly-sized models. RAG use cases. For enterprise RAG evaluation tasks, we assess question-answering use cases involving technical documentation and workplace policies, including internal rules and company benefits. These tasks often involve user queries on long document snippets that can exceed 10,000 tokens. Some questions require synthesizing information from multiple snippets, while others cannot be directly answered using the given documents. Our evaluation set includes ground-truth answers annotated by humans. To assess the performance of our models, we use two key evaluation metrics: Correctness: Measured using Llama Index Correctness, this evaluates the validity of models response. It ensures that generated answers align with the provided context and are factually correct. Answerability Accuracy: Assessed by LLM judges, this metric measures the models ability to discern between answerable and unanswerable questions. 32 Model Command Command R+ Refresh Claude 3.5 Sonnet v2 DeepSeek V3 GPT-4o Llama 3.3 70B Instruct Llama 3.1 405B Instruct Command R7B Gemma 2 9B Llama 3.1 8B Enterprise Pass Rate (%) 94. 87.4 84.2 81.3 79.1 65.2 77. 71.9 65.7 60.4 Table 18: Enterprise generative evaluation performance across all 22 tasks. Model Command Command R+ Refresh Claude 3.5 Sonnet v2 DeepSeek V3 GPT-4o Llama 3.3 70B Instruct Llama 3.1 405B Instruct Command R7B Gemma 2 9B Llama 3.1 8B Workplace Policies QA Technical QA 4. 4.08 4.63 4.52 4.50 4.45 4. 4.16 3.23 3.99 4.86 4.42 4. 4.64 4.81 4.78 4.62 4.48 4. 4.37 Avg. 4.73 4.25 4.72 4. 4.66 4.61 4.52 4.32 3.95 4. Table 19: Enterprise RAG evaluation performances of Command models. The table shows the LLama Index Correctness metric which ranges from 1-5. Best performances are marked in bold. Model Command Command R+ Refresh Claude 3.5 Sonnet v2 DeepSeek GPT-4o Llama 3.3 70B Instruct Llama 3.1 405B Instruct Command R7B Gemma 2 9B Llama 3.1 8B Answerable Acc. (%) Unanswerable Acc. (%) 96 78 89 94 95 94 86 90 91 95 92 86 88 90 76 90 91 Table 20: Enterprise RAG Answerable and Unanswerable Accuracy. These metrics ensure that the model generates accurate responses while demonstrating robust judgement in handling questions beyond the scope of the provided context. Table 19 shows the LLama Index Correctness performance on RAG use cases and Table 20 shows the Answerable and Unanswerable Accuracy across tasks. The accuracy across answerable and unanswerable questions is typically trade-off as we do not want the model to over-refuse when the question is actually answerable, given the context. Command has the best Llama Index Correctness Average at 4.73, Answerable Accuracy of 96% and an Unanswerable Accuracy of 91%, indicating that it responds when intended, while keeping hallucination rates low for unanswerable questions. Command R7B has the best Llama Index Correctness Average score at 4.32 33 compared to models of similar size, an Answerable Accuracy of 86% and an Unanswerable Accuracy of 76%."
        },
        {
            "title": "4.8 Long-Context Benchmarks\nTo assess long-context understanding capability, we employ two extensive long-context benchmark datasets:\nRULER (Hsieh et al., 2024) and LongBench-V2 (Bai et al., 2025). RULER comprises 13 distinct tasks,\nincluding retrieval, question-answering, multi-hop tracing and aggregation tasks. It is designed to evaluate a\nmodel’s ability to retrieve and reason over longer context inputs. The evaluation is conducted on sequences of\nup to 256k tokens. LongBench-V2 includes a diverse set of question-answering tasks spanning various levels\nof difficulty and multiple context types, such as single- and multi-document contexts, multi-turn dialogues,\ncode repositories, and long structured data.",
            "content": "4k 8k 16k 32k 64k 128k 256k Avg wAvg. wAvg. Command Mistral Large 2 Llama 3.1 70B Command R+ Refresh GPT-4o(11-20) Claude 3.5 Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) 97. 96.4 96.5 96.0 97.0 96.5 96. 96.0 96.9 96.3 95.8 95.1 92. 96.0 96.0 96.0 96.7 95.3 95. 94.0 89.0 95.7 96.0 95.1 95. 94.0 94.8 92.4 88.8 95.0 95. 95.7 93.3 85.9 88.4 85.4 88. 95.2 93.8 93.7 90.0 48.1 66. 64.6 - 93.8 91.7 86.0 84. - - - - - 91. 79.7 (128k) 95.0 86.0 89.6 87. - 95.4 94.9 93.8 (inc) 93. 79.5 85.5 83.4 - 95.0 94. 92.4 (dec) 96.1 92.5 93.7 92. - 95.8 95.6 95.1 Table 21: Results on the RULER long context benchmark. Not all models support contexts up to 256k. Command Mistral Large 2 Llama 3.1 70B Llama 3.1 405B Command R+ Refresh GPT-4o(11-20) Claude 3.5 Sonnet (10-22) Overall Short Medium Long Easy Hard 43.4 34.4 31.8 37.8 27.8 46. 40.7 44.1 41.7 37.2 39.7 36. 42.9 44.9 47.4 30.7 28.8 39. 23.7 52.1 41.3 34.3 29.6 28. 32.1 21.3 35.3 31.6 45.3 38. 35.4 39.0 30.2 52.1 45.8 42. 32.2 29.6 37.0 26.4 42.2 38. Table 22: LongBench-V2 results for Command A. Tables 21 and 22 highlight Command As exceptional long-context capabilities. Our hybrid architecture enables this level of performance while requiring significantly less KV cache memory compared to models with full attention architecture. For instance, at an 8k sequence length, Command requires only 75% of the KV cache memory used by Llama 3.3 70B Instruct, 23.8% of that used by Llama 3.1 405B Instruct, and 45.5% of that used by Mistral Large. At 128k sequence length, these decrease to 32.9%, 10.4%, and 19.9%, respectively. This reduction in KV cache memory usage can significantly decrease latency and memory consumption while enhancing throughput during inference, particularly for longer contexts."
        },
        {
            "title": "4.9.1 Expert performance is largely preserved\nWe find that model merging is an effective method for combining capabilities from a set of expert models\ninto a single model, and that linear merging is sufficient to preserve expert performance with only a 1.8%\naverage drop. Figure 12a shows the distribution of changes in score for the metrics tracked during merging.\nWe find that the overwhelming majority of metrics are preserved to within 2.5% of the best expert score, with\nsome metrics actually improving after merging. Figure 12b shows the average degree of metric preservation\nbetween expert and merge, grouped by domain. RAG and general performance are generally best preserved,\nwith code performance the least well preserved during merging.",
            "content": "34 (a) Distribution of metric values for the merged model, as percentage of the score for the best input expert. The overwhelming majority of metrics tracked are preserved to within 2.5% of the best expert score. (b) Average degree of metric preservation between expert and merge, by domain. Error bars denote standard deviation. RAG and general performance are best preserved, with code the least well preserved. Note that the higher RAG standard deviation is property of RAG metrics (rather than the merge) as TauBench has very high variance across runs. Figure 12 We additionally find that, at the 111B scale, model merging is very stable. Large changes to expert weights result in relatively small (2-3%) changes in domain performance, with very few candidates displaying catastrophic failures at any capability."
        },
        {
            "title": "4.10 Polishing\nThe polishing effort includes several phases to improve model style and overall alignment with human pref-\nerences. We use the same evaluation setting as the multilingual mArenaHard restricted to English (Dang\net al., 2024). In Figure 13, we show Command A win rates against GPT-4o (1120) according to a pool of\nLLM judges at each phase of the CoPG/SRPO ping-pong. While this method allows us to achieve high\nwin rates, we also observe that interleaving the two methods tends to increase overall training stability: a\npotential regression occurring at a particular phase is likely to be corrected by the next one.",
            "content": "In addition, polishing helps improve both the overall alignment with human preferences and recovery of any degraded scores during merging. For some of the metrics, it also improves the score over the expert models. Figure 14 provides the difference in metrics across many domains between the polished and the RL soup model, showing that some domains benefit significantly from polishing, with human preferences benefiting the most. 10Roughly 10-20x lower than was used for the main SFT training. 35 Figure 13: Command win rates against GPT-4o as the polishing phase progresses. Figure 14: Performance improvements across various domains during polishing between the inital RL soup model and the polished model."
        },
        {
            "title": "4.11 Human Evaluation\nWhile automated evaluation benchmarks provide quick feedback and allow for efficient hill-climbing in specific\ntask settings, automatically assessing the perceived quality of models remains challenging (Ni et al., 2024).\nTo validate broader model performance, our most promising model candidates are additionally evaluated by\nhuman annotators. This section describes our human evaluation setup, including details on the curation of\nour internal evaluation dataset, an overview of our annotation strategy, as well as the results for Command\nA in head-to-head evaluation against competitors.",
            "content": ""
        },
        {
            "title": "4.11.1 Evaluation Data\nChatbot Arena (Chiang et al., 2024) is a popular LLM benchmark that involves crowd-sourced human an-\nnotations. The framework relies on human preference judgments between two model-generated completions\nover user-provided prompts, from which the authors then derive Elo scores to build up-to-date leader-\nboards (Boubdir et al., 2024). While Chatbot Arena-like evaluation provides an extremely useful quality\nsignal on user-perceived model quality, it has several drawbacks for the efficient evaluation of internal model\ncandidates. Two challenges are the long ramp-up time required to provide performance signal on a new\nmodel (often requiring multiple thousands of ratings before producing a reliable Elo score), as well as the\nreliance on user-provided prompts that often end up skewing towards simpler topics. In order to provide a\nmore immediate and targeted feedback signal on human-perceived model performance, we instead curate a\nstatic collection of single-turn prompts for the internal evaluation of our models versus competitors.",
            "content": "Prompts in our internal collection are primarily curated from scratch by our pool of annotators to avoid accidental contamination for competitor models by re-using existing prompt collections. Instructions are specifically targeted towards creating complex real-world prompts that span broad range of typical LLMassisted tasks. For our purposes, we define complexity as function of the number of different asks contained in single prompt. As an example, asking for summary with certain length requirement would be assigned complexity score of 2. The first point for invoking particular task (summarisation), and the second point for additionally restricting the output (length constraint). We automatically filter prompts with the help of Command R+ Refresh along several axes, building up more comprehensive notion of prompt complexity. While the filtering may be overly strict in some instances, our findings show that the resulting pool of prompts is sufficiently difficult even for state-of-the-art models. We prune the final dataset to around 800 prompts, further subset into general (350 prompts), reasoning (150 prompts), and code (300 prompts). The general split focuses on general-purpose tasks that do not require deep technical expertise. This can include open-ended document generation or idea generation requests, and at most requires basic understanding of formatting languages such as CSV or JSON. The reasoning split includes more reasoningand math-heavy prompts that generally require undergraduate-level understanding of one STEM discipline. For the code split, we specifically instruct annotators to curate prompts across number of target programming languages, with focus on analysis and debugging rather than code generation."
        },
        {
            "title": "4.11.2 Annotation Methodology\nOur annotation process comprises a relatively straightforward pairwise preference evaluation setup. For each\nannotation task, we present annotators with one prompt and two completions from competing models.\nAnnotators are first tasked with evaluating the quality of each completion separately, assigning a score from\n1 (flawed) to 5 (perfect). They are then given the option to label common failure modes. Finally, annotators\nprovide their preference between the two completions. The choices correspond to “Completion A is much\nbetter”, “Completion A is slightly better”, “Tie”, “Completion B is slightly better”, and “Completion B is\nmuch better”. To avoid positional bias, we randomly shuffle the order in which completions are shown to\nannotators. We compare different models’ performance based on their win rate versus a fixed competitor\nmodel. We assign the win rate in a pairwise matchup as a single score, distributing ties, as:",
            "content": "winrate = wins + (0.5 ties) wins + ties + losses We group the strong and weak preferences for wins or losses together and find that computing win rate over the 5-point scale does not change model rankings, but helps annotators gain confidence in their given preference ratings. For complete pairwise evaluation of 800 samples across all three subsets, on average 65 annotators contribute to single evaluation run."
        },
        {
            "title": "4.11.3 Results\nTable 23 shows the results for pairwise human annotation runs against different competitor models. 11 Our\nresults show that Command A is competitive with frontier models on both general and reasoning subsets.",
            "content": "11We obtain completions for GPT-family models directly through the OpenAI API. We use the TogetherAI endpoints for Llama 3.3 and DeepSeek V3. 37 vs. GPT-4o GPT-4.5 Preview DeepSeek V3 Llama 3.3 70B Instruct Llama 3.1 405B Instruct Command Win Rate (%) General Reasoning 50.4 47.2 49.0 68.8 61.6 51. 30.7 49.3 71.7 64.0 Code 46. 38.3 54.7 63.4 61.6 Table 23: Win rate of human-annotated pairwise evaluation between Command and different competitor models on our internal test set. Win rates are from the perspective of Command (50% is tie, higher numbers mean Command wins by larger margin). Interestingly, we also observe that GPT-4.5-preview improves significantly over its predecessor on reasoningheavy prompts. Annotators strongly prefer Command over Llama 3.3 70B Instruct for all subsets, including code. Our analysis across evaluation results indicates that humans particularly prefer Command As ability to respect requests for particular formatting or style. To further illustrate the importance of polishing as critical step towards human preference optimisation (see Section 3.5), we compare the human evaluation results of an earlier internal checkpoint, result of the expert merging before polishing. Evaluating both our earlier checkpoint and the final model candidate against GPT-4o, we see substantial gains as reflected in human preference. For our final model, we manage an absolute improvement of more than seven percentage points in win rate on the general subset (43.2 50.4), 10 points on reasoning (41.4 51.4), and almost 17 points (30.0 46.8) on code. Overall, Command is significantly more preferred by human evaluators to models such as Llama 3.1 405B Instruct across all subsets, and is competitive with state-of-the-art models such as GPT-4o and DeepSeek V3 while being considerably more efficient."
        },
        {
            "title": "5 Conclusion\nThis technical report detailed the development of Command A, shared extensive performance evaluations\nacross many domains and languages, and shared additional results for Command R7B. Command A repre-\nsents a significant advancement in LLMs for enterprise, achieving best-in-class performance across a wide\nrange of tasks with optimal efficiency. Our models excel in enterprise-relevant tasks such as agentic workflows,\nmultilingual understanding and generation, and instruction-following. Key innovations introduced include\ndata and architectural optimisations, self-refinement algorithms, and a model merging-based approach that\nensures expert-level performance across diverse capabilities within a single model.",
            "content": "Command outperforms comparable models in both efficiency and computational overhead, requiring fewer resources for serving, making it easy to deploy on-premises or in private cloud environments on just two A100 or H100 GPUs, and delivering tokens at higher rate. The release of model weights under non-commercial license further facilitates community-based exploration and research. Command sets new standard for LLMs in enterprise applications, balancing performance, efficiency, and versatility and providing maximum performance for minimal compute. 38 References Aakanksha, Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, and Sara Hooker. Mix data or merge models? optimizing for performance and safety in multilingual contexts. In Neurips Safe Generative AI Workshop 2024, 2024. URL https://openreview.net/forum?id=L1Hxp8ktiT. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1224812267, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653 /v1/2024.acl-long.662. URL https://aclanthology.org/2024.acl-long.662. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. URL https: //arxiv.org/abs/2305.13245. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress, 2024. URL https://arxiv.org/abs/2405.15032. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021. URL https://arxiv.org/abs/2108.07732. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks, 2025. URL https://arxiv.org/abs/2412.15204. Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, and Douwe Kiela. Improving question answering model robustness with synthetic adversarial data generation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 88308848, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-mai n.696. URL https://aclanthology.org/2021.emnlp-main.696. Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model evaluation. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_f iles/paper/2024/hash/bfba8efb806a970455b83b852c9cf846-Abstract-Conference.html. Shuaichen Chang and Eric Fosler-Lussier. How to prompt llms for text-to-sql: study in zero-shot, singledomain, and cross-domain settings, 2023. URL https://arxiv.org/abs/2305.11853. 39 Hongyu Chen and Seraphina Goldfarb-Tarrant. Safer or luckier? llms as safety evaluators are not robust to artifacts, 2025. URL https://arxiv.org/abs/2503.09347. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openre view.net/forum?id=3MW8GKNyzI. Eugene Choi, Arash Ahmadian, Matthieu Geist, Olivier Pietquin, and Mohammad Gheshlaghi Azar. Selfimproving robust preference optimization. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=ZSdubdbOoi. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. URL http://jmlr.org/papers/v24/22-1144.html. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker. Aya expanse: Combining research breakthroughs for new multilingual frontier, 2024. URL https://arxiv.org/abs/2412.04261. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 23682378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 40 Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alexander Nicholas DAmour, Krishnamurthy Dj Dvijotham, Adam Fisch, Katherine Heller, Stephen Robert Pfohl, Deepak Ramachandran, Peter Shaw, and Jonathan Berant. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=5u1GpUkKtG. Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 news test references for MT evaluation of 128 languages. In Kabir Ahuja, Antonios Anastasopoulos, Barun Patra, Graham Neubig, Monojit Choudhury, Sandipan Dandapat, Sunayana Sitaram, and Vishrav Chaudhary (eds.), Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pp. 2124, Online, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.sumeval-1.4. Maxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling fp8 training to trillion-token llms, 2025. URL https://arxiv.org/abs/2409.12517. Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Eugene Choi, Bill Wu, Chris Cremer, Arash Ahmadian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, and Matthieu Geist. Contrastive policy gradient: Aligning LLMs on sequence-level scores in supervised-friendly fashion. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2135321370, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main.1190. Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 578628, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.51. URL https://aclanthology.org/2023. wmt-1.51. Roy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via high-level tracing, 2018. URL https://mlsys.org/Conferences/doc/2018/146.pdf. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025. URL https://arxiv.org/abs/2410.02089. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. doi: 10.1162/tacl_a_00370. URL https: //aclanthology.org/2021.tacl-1.21. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538, 2022. doi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.ta cl-1.30. Nathan Grinsztajn, Yannis Flet-Berliac, Mohammad Gheshlaghi Azar, Florian Strub, Bill Wu, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Olivier Pietquin, and Matthieu Geist. Averaging loglikelihoods in direct alignment. arXiv preprint arXiv:2406.19188, 2024. Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, 41 Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple Intelligence Foundation Language Models, 2024. URL https://arxiv.org/abs/2407.21075. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?i d=7W3GLNImfS. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models?, 2024. URL https://arxiv.org/abs/2404.06654. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali In The Eleventh International Conference on Learning Farhadi. Editing models with task arithmetic. Representations, 2023. URL https://openreview.net/forum?id=6t0Kwf8-jrj. Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, pp. 876885. Association For Uncertainty in Artificial Intelligence (AUAI), 2018. 42 Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= VTF8yNQM66. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 2489224928. Curran Associates, Inc., 2023. URL https://proceedings.neurip s.cc/paper_files/paper/2023/file/4e85362c02172c0c6567ce593122d31c-Paper-Conference.pdf. Muhammad Khalifa, Yi-Chern Tan, Arash Ahmadian, Tom Hosking, Honglak Lee, Lu Wang, Ahmet Üstün, Tom Sherborne, and Matthias Gallé. If you cant use them, recycle them: Optimizing merging at scale mitigates performance tradeoffs, 2025. URL https://arxiv.org/abs/2412.04144. Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 41104124, Online, June doi: 10.18653/v1/2021.naaclmain.324. URL 2021. Association for Computational Linguistics. https://aclanthology.org/2021.naacl-main.324. Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Michael Bean, Katerina Margatina, Rafael Mosquera, Juan Manuel Ciro, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale. The PRISM alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https: //openreview.net/forum?id=DFr5hteojx. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. Tom Kocmi, Vilém Zouhar, Christian Federmann, and Matt Post. Navigating the metrics maze: Reconciling score magnitudes and accuracies. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19992014, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.186 53/v1/2024.acl-long.110. URL https://aclanthology.org/2024.acl-long.110. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating Reward Models for Language Modeling, 2024. URL https://arxiv.org/abs/2403.13787. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrievalaugmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 94599474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/fi le/6b493230205f780e1bc26945df7481e5-Paper.pdf. 43 Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C.C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023a. Curran Associates Inc. Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. In First Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022, 2022. URL https://openreview.net/for um?id=SQgVgE2Sq4. Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23912404, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.134. URL https://aclanthology.org/2023.acl-long.134. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint 2412.19437, 2024a. Jiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, and Lingming Zhang. Repoqa: Evaluating long context code understanding. arXiv preprint arXiv:2406.06025, 2024b. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024c. Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv preprint arXiv:2401.10225, 2024d. Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101. Kelly Marchisio, Wei-Yin Ko, Alexandre Berard, Théo Dehaze, and Sebastian Ruder. Understanding and mitigating language confusion in LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 6653 6677, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.emnlp-main.380. Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, and Matthias Gallé. On leakage of code generation evaluation datasets. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1321513223, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-e mnlp.772. Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. Fp8 formats for deep learning, 2022. URL https: //arxiv.org/abs/2209.05433. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. 44 Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. Mixeval: Deriving wisdom of the crowd from LLM benchmark mixtures. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/b1f34d7b4a03a3d80be8e72eb430dd81-Abs tract-Conference.html. Ayomide Odumakinde, Daniel Dsouza, Pat Verga, Beyza Ermis, and Sara Hooker. Multilingual arbitrage: Optimizing data pools to accelerate multilingual progress, 2024. URL https://arxiv.org/abs/2408.1 4960. B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838855, 1992. doi: 10.1137/0330046. URL https://doi.org/10.1137/ 0330046. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. In D. Song, M. Carbin, and T. Chen (eds.), Proceedings of Machine Learning and Systems, volume 5, pp. 606624. Curan, 2023. URL https://proceedings.mlsys.org/paper_files/paper/2023/file/c4be71ab8d24cdfb45e3d06d bfca2780-Paper-mlsys2023.pdf. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. InFoBench: Evaluating instruction following ability in large language In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for models. Computational Linguistics: ACL 2024, pp. 1302513048, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.772. URL https://aclanthology.o rg/2024.findings-acl.772. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Alexandre Ramé, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, and Olivier Bachem. Warp: On the benefits of weight averaged rewarded policies, 2024. URL https://arxiv.org/abs/2406.16768. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. COMET: neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 26852702, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL https://aclanthology.org/2020.emnlp-main.213. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Nathaniel Robinson, Shahd Abdelmoneim, Kelly Marchisio, and Sebastian Ruder. Al-qasida: Analyzing llm quality and accuracy systematically in dialectal arabic. arXiv preprint arXiv:2412.04193, 2024. Angelika Romanou, Negar Foroutan, Anna Sotnikova, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Zeming Chen, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, 45 Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia soltani moakhar, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, and Antoine Bosselut. INCLUDE: Evaluating multilingual language understanding with regional knowledge. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=k3gCieTXeY. Paul Röttger, Hannah Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. XSTest: test suite for identifying exaggerated safety behaviours in large language models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 53775400, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653 /v1/2024.naacl-long.301. URL https://aclanthology.org/2024.naacl-long.301. David Ruppert. Efficient estimations from slowly convergent robbins-monro process. Technical report, Cornell University Operations Research and Industrial Engineering, 1988. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. Rylan Schaeffer, Punit Singh Koura, Binh Tang, Ranjan Subramanian, Aaditya Singh, Todor Mihaylov, Prajjwal Bhargava, Lovish Madaan, Niladri S. Chatterji, Vedanuj Goswami, Sergey Edunov, Dieuwke Hupkes, Sanmi Koyejo, and Sharan Narang. Correlating and predicting human evaluations of language models from natural language processing benchmarks, 2025. URL https://arxiv.org/abs/2502.18339. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 6853968551. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f65 8a906-Paper-Conference.pdf. Preethi Seshadri and Seraphina Goldfarb-Tarrant. Who does the giant number pile like best: Analyzing fairness in hiring contexts, 2025. URL https://arxiv.org/abs/2501.04316. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Noam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners, 2022. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. ArXiv, abs/1909.08053, 2019. URL https://api.semanticscholar.org/CorpusID:202660670. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. 46 Adam Sutton, Almog Simchon, Matthew Edwards, and Stephan Lewandowsky. You are what you read: Inferring personality from consumed textual content. In Jeremy Barnes, Orphée De Clercq, and Roman Klinger (eds.), Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, pp. 2838, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wassa-1.4. URL https://aclanthology.org/2023.wassa-1.4. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv. org/abs/2408.00118, 1(3), 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. NLLB Team, Marta Ruiz Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Alison Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon L. Spruit, C. Tran, Pierre Yves Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation. ArXiv, abs/2207.04672, 2022. URL https://api.semanticscholar.org/CorpusID:250425961. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Ahmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An instruction finetuned open-access multilingual language model. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1589415939, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.845. URL https://aclanthology.org/2024.acl-long.845. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/pap er/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations with panel of diverse models. arXiv preprint arXiv:2404.18796, 2024. Ke Wang, Nikolaos Dimitriadis, Guillermo Ortiz-Jiménez, François Fleuret, and Pascal Frossard. Localizing task information for improved model merging and compression. In International Conference on Machine Learning, 2024a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-Pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 47 Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120, 2023. Yuxiang Wei, Federico Cassano, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun Guha, and Lingming Zhang. Starcoder2-instruct: Fully transparent and permissive self-alignment for code generation, 2024. URL https://huggingface.co/blog/sc2-instruct. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2396523998. PMLR, 1723 Jul 2022. URL https://proceedings.ml r.press/v162/wortsman22a.html. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao In Kevin Duh, Helena Gomez, and Steven Ma. Effective long-context scaling of foundation models. Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 46434663, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl -long.260. URL https://aclanthology.org/2024.naacl-long.260. Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. GSPMD: general and scalable parallelization for ML computation graphs. CoRR, abs/2105.04663, 2021. URL https://arxiv.org/abs/2105.04663. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging: Resolving interference when merging models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=xtaX3WyCj1. Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, and Tsendsuren Munkhdalai. What matters for model merging at scale? arXiv preprint arXiv:2410.03617, 2024. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkel ey_function_calling_leaderboard.html, 2024. Bowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, and Acyr Locatelli. Rope to nope and back again: new hybrid attention strategy, 2025. URL https://arxiv.or g/abs/2501.18795. Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=Bx6qKuBM2AD. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 48 Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agentuser interaction in real-world domains, 2024. URL https://arxiv.org/abs/2406.12045. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: absorbing abilities from homologous models as free lunch. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 39113921, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis, 2025. URL https://arxiv.org/abs/2502.01718. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. RMB: Comprehensively Benchmarking Reward Models in LLM Alignment, 2024. URL https://arxiv.org/abs/2410.09893. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, Le Hou. 2023. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions, 2024. URL https: //arxiv.org/abs/2406.15877. 49 Authors Team Cohere: Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Björn Bebensee, Neeral Beladia, Walter BellerMorales, Alexandre Bérard, Andrew Berneshawi, Anna Bialas, Phil Blunsom, Matt Bobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Campos, Cassie Cao, Kris Cao, Roman Castagné, Julián Cendrero, Leila Chan Currie, Yash Chandak, Diane Chang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene Cho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin, Lucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel Dsouza, Hugo Dalla-Torre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Théo Dehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady, Sarah Elsharkawy, Irem Ergün, Beyza Ermis, Marzieh Fadaee, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac, Nick Frosst, Matthias Gallé, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado Gonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu, Shangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofstätter, Sungjin Hong, Sara Hooker, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi, Madeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara, Julia Kedrzycki, Gokce Keskin, Edward Kim, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wojciech Kryściński, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche, Justin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma, Lukas Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, Kelly Marchisio, Olivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot, Autumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Hemangani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James Owers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone, Aleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra Ranjan, Pierre Richemond, Cécile Robert-Michon, Aurélien Rodriguez, Sudip Roy, Laura Ruis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam, Alice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie Chang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev, Mathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su, Jimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren Tumer, Ahmet Üstün, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, Alex Wang, Donglu Wang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie, Minjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, and Zhoujie Zhao. A.1 Acknowledgements We would also like to acknowledge the contributions of the following people who helped make this work possible: Milad Alizadeh, Ellen Gilsenan-McMahon, Robert Li, Olivier Pietquin, Sebastian Ruder, Karina Waluk, Bill Wu, and James Zhou. We would also like to thank our talented team of internal annotators and model trainers. This technical report was written with the assistance of the models described in this technical report."
        },
        {
            "title": "B Appendices",
            "content": "Instruction-Following B.1 We use the following custom preamble override for internal IFEval evaluation for Command A: You are in non-interactive mode. Please be as comprehensive and accurate as possible and do not introduce your response and / or ask follow-up questions. B.2 Multilingual B.2.1 Expert Training Considerations Supervised Fine-Tuning (SFT). We employ all SFT datasets as mentioned in Section 3.3.3.1. To train the model, we use the Adam optimiser with peak learning rate of 2.5 105, cosine decay to 1.25 105, β1 = 0.9, β2 = 0.95 and weight decay of 0.1. We merge several models with the same configuration (but different random seed) at this stage. Preference Tuning. Following the SFT stage, we perform offline preference tuning using both humanannotated and synthetically generated multilingual preference data (with best-of-N or machine translation). We use DPO with the Adam optimiser, peak learning rate of 1.25 105 decaying to 1.25 106, β = 0.2 and use SFT regularisation with the same data mixture we used in the SFT stage. We do not merge over multiple seeds at this stage. B.2.2 Results We show additional results on MGSM in Table 24, and INCLUDE-44 in Table 25 highlighting the highly competitive multilingual capabilities of our models. Command DeepSeek V3 Claude 3.7 Sonnet Llama 3.3 70B Instruct Qwen 2.5 72B Instruct Turbo Mistral Large 2 Gemini 2.0 Flash GPT-4o Gemini 1.5 Pro Llama 3.1 405B Instruct Command R7B Gemma 2 9B Instruct Gemini 1.5 Flash-8B Claude 3 Haiku Ministral 8B Llama 3.1 8B Instruct Qwen 2.5 7B Instruct Turbo Avg. 90.1 92.3 92.1 91.5 90. 90.1 90.0 89.6 88.4 74.1 75. 80.7 80.3 76.9 76.1 70.4 70. de 92.0 92.4 93.6 92.8 92. 90.0 90.8 92.0 91.2 89.8 77. 83.1 79.9 77.9 81.0 73.9 81. es 92.4 96.0 96.4 91.6 94. 90.4 92.8 90.8 93.2 82.4 78. 85.9 80.7 77.9 80.6 67.1 75. fr 85.1 90.4 85.5 90.0 87.6 85. 83.1 83.1 77.5 7.7 71.5 76. 76.7 73.5 74.6 65.9 27.3 ja 87.1 89.6 89.2 89.6 87.1 90. 87.1 87.1 86.7 85.8 66.7 72. 79.1 74.7 64.4 62.7 72.7 ru 94.0 94.0 95.6 95.2 92.4 92. 95.2 92.0 92.4 90.3 79.9 86. 84.3 80.3 81.1 77.9 82.3 zh 90.0 91.6 92.4 90.0 89.6 91. 90.8 92.8 89.6 88.6 76.3 79. 80.7 77.1 74.7 75.1 81.5 Table 24: MGSM scores using simple-evals. : we did not train with the simple-evals template and our French outputs sometimes contain comma (used as decimal separator), which simple-evals counts as wrong. With our internal implementation of MGSM, we score 90.0% on French. 51 Command Claude 3.7 Sonnet Gemini 2.0 Flash GPT-4o DeepSeek Gemini 1.5 Pro Llama 3.1 405B Instruct Llama 3.3 70B Instruct Qwen 2.5 72B Instruct Turbo Mistral Large 2 Command R7B Gemini 1.5 Flash-8B Claude 3 Haiku Gemma 2 9B Instruct Qwen 2.5 7B Instruct Turbo Llama 3.1 8B Instruct Ministral 8B Avg. 74.3 81.0 78.8 78.7 76. 76.4 75.3 75.0 72.6 72.0 55. 66.0 64.0 61.7 59.9 54.9 50. ar 73.2 79.2 79.0 79.3 73. 72.5 70.7 71.7 70.5 68.3 58. 61.4 65.9 57.1 57.1 55.1 43. de 67.6 66.9 69.1 68.3 64. 66.2 71.7 65.5 64.0 66.2 51. 54.7 50.4 58.3 53.2 53.2 42. el es fa fr he hi id it ja ko nl pl pt ru 68.0 74.7 79.3 85. 61.3 73.4 77.6 84.2 75.5 87. 81.3 87.1 78.6 87.0 77.5 80. 69.2 78.8 64.6 63.3 63.5 65. 60.0 59.5 57.1 55.3 41.4 79. 82.3 83.1 79.4 77.4 75.2 77. 76.6 58.7 83.3 83.1 80.2 78. 77.8 78.5 70.5 74.9 53.6 67. 77.3 80.4 76.4 75.0 77.1 73. 72.4 71.5 69.3 47.2 72.7 81. 86.5 91.4 77.8 78.2 76.5 76. 76.4 76.4 73.1 72.2 58.4 89. 89.2 88.5 87.2 76.8 89.2 85. 87.0 69.2 86.8 93.4 91.2 93. 91.8 90.8 88.5 85.2 86.0 85. 61.9 72.4 73.6 79.4 73.8 71. 72.3 68.4 67.0 70.6 65.0 58. 86.0 85.8 83.7 86.2 85.8 83. 84.0 82.4 59.5 49.3 69.4 68. 65.4 67.8 79.4 77.4 64.0 74. 46.9 44.9 43.4 43.8 38.7 67. 64.0 63.5 59.2 57.0 68.0 66. 58.4 54.5 49.5 60.1 58.1 56. 49.2 46.6 67.1 63.6 63.5 60. 49.5 76.6 72.4 72.8 70.6 57. 75.0 73.7 71.9 58.3 57.3 62. 56.2 61.4 50.2 51.0 72.6 70. 69.7 63.7 58.4 71.8 72.5 66. 70.7 62.2 64.7 57.1 66.0 41. 61.7 54.2 55.3 49.3 33.5 41. 83.1 83.1 82.5 79.6 81.4 78. 76.9 79.5 64.0 71.2 71.5 65. 65.8 62.7 59.5 85.0 82.7 80. 80.4 84.7 86.5 71.9 74.3 71. 67.5 69.3 64.1 59.5 55.1 55. 79.1 78.9 77.9 75.7 75.8 77. 76.2 75.3 49.9 67.3 64.1 65. 61.5 59.5 49.2 73.7 74.8 72. 71.7 72.0 69.6 69.4 68.3 47. 62.1 62.3 57.8 52.5 49.1 51. tr 67.7 68.4 66.2 66.2 66. 67.7 71.9 73.0 64.8 65.3 62. 63.0 58.6 59.3 52.6 56.0 46. uk vi 76.5 87.1 70.7 80. 80.4 82.5 77.8 78.5 76.5 74. 71.3 71.8 58.9 66.5 69.3 66. 57.1 54.7 54.0 77.5 79.5 78. 75.1 77.6 76.4 70.0 66.2 52. 63.1 56.0 60.2 56.4 55.5 44. zh 75.8 80.7 78.3 79.3 83. 77.0 76.9 74.9 84.4 71.0 51. 65.9 61.3 56.5 72.8 53.6 50. Table 25: INCLUDE-44 scores for individual languages. B.3 Code B.3.1 Expert Training Considerations Stage 1 large-scale supervised learning. This expert is trained using Adam optimisation, learning rate peak of 5105, cosine decay to 5106, beta values (β1 = 0.9, β2 = 0.95), weight decay factor of 0.1, and gradient norm clipping at peak 1.0. Our regularisation is inspired by similar code expert training such as Qwen 2.5 Coder (Hui et al., 2024). Stage 2 supervised learning on only high-quality data. We follow similar schedule for fine-tuning of the merged model described in Stage 1. Stage 3 RL over scored or preference-pair data. We use constant learning rate of 2 106, regularisation parameter β of 0.06, and otherwise match the hyperparameters used for SFT. B.3.2 Results We additionally show breakdown of results across programming languages for HumanEval and Less Basic Python Problems (LBPP) in Table 26. HumanEval C++ Rust 73.2 81.1 Less Basic Python Problems (LBPP) JavaScript Java COBOL Avg. Python C++ Rust 41.6 25. 51.5 58.4 50.3 Command Command Expert Command Agentic Command R7B Command Refresh Command R+ Refresh Llama 3.3 70B Instruct Mistral Large 2 Qwen 2.5 72B Instruct Llama 3.1 405B Instruct DeepSeek V3 Avg. 76.2 77.5 50.7 54.7 54.4 75.5 82.9 78. 76.7 83.5 Python 85.4 86.6 61.0 67.1 72.0 80.5 / 85.4 94.5 86.6 / 86.6 89.0 / 88. Java JavaScript 86.0 76.2 52. 59.2 67.1 83.4 84.8 82.9 87.8 85. 64.6 79.9 53.7 64.6 62. 81.7 87.2 81.1 79.3 85.4 Go 72. 71.3 43.3 33.5 19.5 62. 72.6 71.3 69.5 77.4 76.8 54.9 56.1 54.3 75.0 86.0 76. 81.7 74.4 39.0 47.6 50.6 63. 72.6 72.0 53.7 70.7 29.8 7. 1.9 2.5 3.2 10.8 6.3 3. 15.2 50.8 65.4 21.9 24.7 25.6 47. 54.0 48.3 52.7 61.5 59.6 71.4 26. 33.5 38.5 55.3 62.7 59.6 59. 67.1 51.6 39.6 68.8 21.1 57.7 13.4 16. 21.5 32.9 36.2 32.9 38.9 25. 26.7 42.2 53.4 46.6 50.9 59. 50.0 53.2 63.9 23.4 22.2 30. 56.3 51.3 49.4 57.6 58.2 54. 68.0 29.4 26.8 28.1 56.2 52.9 60. 61.4 66.0 92.1 90.2 57.1 67. Go 50.3 46.6 62.7 17.4 23.6 8. 43.5 42.2 39.8 47.2 52.8 Table 26: Full pass@1 results for HumanEval and LBPP across Python, C++, Rust Java, Javascript, Go. and COBOL. All results are internal reproductions using an identical prompt except where / indicates an external value first, and footnote citation, and the internal reproduction second. Average (Avg.) results are sample-weighted average across Python, C++, Rust Java, Javascript and Go languages. Sources of externally reported numbers in all code-related tables in this report are as follows: Code Understanding Benchmarks  (Table 12)  Llama 3.3 70B Instruct: https://github.com/meta-llama/llama-models/blob/main/models/lla 52 ma3_3/MODEL_CARD.md, https://bigcode-bench.github.io/ Mistral Large 2: https://livecodebench.github.io/ Qwen 2.5 72B Instruct: https://bigcode-bench.github.io/ Llama 3.1 405B Instruct: https://github.com/meta-llama/llama-models/blob/main/models/lla ma3_3/MODEL_CARD.md DeepSeek V3: https://livecodebench.github.io/, https://bigcode-bench.github.io/ Code Editing Benchmarks  (Table 13)  DeepSeek V3: https://www.deepseek.com/ HumanEval and LBPP  (Table 26)  Llama 3.3 70B Instruct and Llama 3.1 405B Instruct: https://github.com/meta-llama/llama-mod els/blob/main/models/llama3_3/MODEL_CARD.md Qwen 2.5 72B Instruct: From Hui et al. (2024). B.4 Reasoning Supervised Fine-Tuning. We train using the Adam optimiser with peak learning rate of 2.5 105, cosine decay to 2.5 106, β1 = 0.9, β2 = 0.95, weight decay of 0.01, and gradient norm clipping peak at 1.0. Preference Tuning. We train using CoPG with the Adam optimiser, using learning rate of 2 106 with no decay, β1 = 0.9, β2 = 0.95, and gradient norm clipping peak at 1.0. B.5 Long Context Training is conducted using the Adam optimiser with peak learning rate of 2.5 105, cosine decay to 2.5 106, β1 = 0.9, β2 = 0.95, weight decay of 0.01, and gradient norm clipping peak at 1.0. B.6 Safety Command Claude 3.5 Sonnet DeepSeek V3 GPT-4o Llama 3.1 405B Llama 3.3 70B Mistral Large Latest Qwen 2.5 72B Command R+ Refresh Command R7B Gemini 2.0 Flash Gemma 2 9B Llama 3.1 8B Qwen 2.5 7B Command Refresh Contextual Over-Refusal Contextual Accuracy Strict Over-Refusal Strict Accuracy 8.3 10.1 3.6 21.3 5.9 3.6 8.9 10.1 8.9 14.2 26.6 39.1 4.1 18.3 9.5 75.4 82.5 66.6 77.8 70.3 60.9 70.6 72.2 61.8 60.9 69.7 72.5 70.7 70.3 63.1 10.2 10.4 1.7 10.4 5.8 4.0 8.7 1.2 12.7 7.5 14.7 14.5 6.9 8.7 10. 87.5 89.6 68.7 86.9 80.5 67.9 81.1 78.3 70.0 62.3 79.0 81.8 81.0 74.7 68.0 Table 27: Safety mode performance compared to similarly sized models. We pass the instructions for each safety mode to competitors as system message or first message to allow for the mode comparison. Note that the over-refusal set was developed by red-teaming Command R+ Refresh, and therefore is specifically challenging for Command models. The top performing model for each size category is bolded in each column. Higher accuracy and lower over-refusal rates correspond to better performance. 53 Figure 15: Absolute safety performance for large models in the default setting. Figure 16: Absolute safety performance for small models in the default setting. B.6.1 Expert Training Considerations Supervised fine-tuning. During the SFT stage, we train the safety expert using the Adam optimiser with β1 = 0.9, β2 = 0.95, and peak learning rate of 104 decayed to 105 with cosine schedule. Gradient norm is clipped to 1, and weight decay is weighted by 103. Offline preference tuning. We train the Safety expert using the same hyper-parameters as above, except the peak learning rate is 106 decayed to 107. We use IPO with KL regularisation parameter, β = 0.03. B.6.2 Results Table 27 provides additional results for the safety mode performance of the Command models, while Figures 15 and 16 show absolute safety performance for large and small models respectively in the default safety setting, further highlighting our models competitive safety performance. B.7 Evaluation on Standard Benchmarks We provide further details about how we measure performance on standard benchmarks: MMLU (Hendrycks et al., 2021) measures university-level academic knowledge across diverse range of subjects. We run 5-shot, with Chain-of-Thought (CoT). MMLU-Pro (Wang et al., 2024b) is an enhanced version of MMLU, designed to evaluate knowledge across diverse range of professional and academic domains, including law, medicine, and engineering. 54 We run 5-shot, with CoT. GPQA (Rein et al., 2023) measures graduate-level academic knowledge in specialized STEM topics. We run on 0-shot, with CoT, and we report results only on the diamond subset. IFEval (Zhou et al., 2023) measures instruction-following ability across 25 types of verifiable instructions (e.g. output length, keyword inclusion/exclusion, formatting). We compute the average of the prompt-level strict accuracy (i.e., the fraction of dataset prompts where all verifiable instructions are followed) and the instruction-level strict accuracy (i.e., the fraction of verifiable instructions that are followed, this allows partial credit as most prompts include multiple instructions). InFoBench (Qin et al., 2024) also measures instruction-following across five broad types of instructions: content, linguistic, style, format, and number. Each prompt in InFoBench is paired with set of yes-no evaluation questions, and we use GPT-4oto answer these questions. We compute the overall average accuracy across the fraction of correctly answered questions per prompt."
        }
    ],
    "affiliations": []
}