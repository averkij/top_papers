{
    "paper_title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "authors": [
        "Shuhang Chen",
        "Yunqiu Xu",
        "Junjie Xie",
        "Aojun Lu",
        "Tao Feng",
        "Zeying Huang",
        "Ning Zhang",
        "Yi Sun",
        "Yi Yang",
        "Hangjie Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow."
        },
        {
            "title": "Start",
            "content": "COGFLOW: BRIDGING PERCEPTION AND REASONING THROUGH KNOWLEDGE INTERNALIZATION FOR VISUAL MATHEMATICAL PROBLEM SOLVING Shuhang Chen1 Yunqiu Xu1 Junjie Xie1 Aojun Lu3 Tao Feng4 Zeying Huang2 Ning Zhang2 Yi Sun2 Yi Yang1 Hangjie Yuan1 1Zhejiang University 2Intelligent Learning 3Sichuan University 4Tsinghua University {sh.chen, jj.xie, yangyics, hj.yuan}@zju.edu.cn {imyunqiuxu, fengtao.hi, futuretrader}@gmail.com aojunlu@stu.scu.edu.cn jzxjeff@163.com sunyi@jzx100.cn 6 2 0 2 J 5 ] . [ 1 4 7 8 1 0 . 1 0 6 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present COGFLOW, novel cognitive-inspired three-stage framework that incorporates knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptioninternalizationreasoning. In line with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute new dataset MATHCOG for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed COGFLOW. Project page: https://shchen233.github.io/cogflow/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal large language models (MLLMs) are rapidly advancing and have been applied across various visionlanguage applications (Peng et al., 2024b; Xu et al., 2025b; Yue et al., 2024). However, existing MLLMs continue to struggle with challenging visual mathematical problems, resulting in low answer accuracy and inconsistent reasoning chains. Some early attempts (Wang et al., 2025a; Shen et al., 2025) adopt one-step reasoning framework that directly interleaves visual perception with reasoning in an unstructured manner, often resulting in both perceptual and reasoning errors. Another line of work (Chen et al., 2025a; Guo et al., 2025c) follows decoupled reasoning pipeline that explicitly separates the perception and reasoning parts, with the former focusing on visual recognition and the latter responsible for subsequent inference. Yet in practice, we observe that such pipeline often suffers from the reasoning drift issue, i.e., it tends to yield illogical or unwarranted reasoning steps that disregard perceptual evidence (see Figure 1). These observations motivate the development of new approach that not only achieves robust fine-grained recognition of matheCorresponding author."
        },
        {
            "title": "Preprint",
            "content": "(a) One-step framework yields unstructured reasoning, while decoupled pipeline modularly disentangles the flow. We adopt cognitiveinspired three-stage framework with knowledge internalization. Figure 1: The one-step reasoning framework (e.g., VLM-R1 (Shen et al., 2025)) often yields suboptimal results, while the decoupled reasoning pipeline (e.g., MathFlow (Chen et al., 2025a)) enhances perception yet still yields illogical reasoning steps that disregard visual evidence. In contrast, COGFLOW adopts cognitive-inspired three-stage framework to effectively mitigate reasoning drift. (b) Reasoning drift analysis across three representative pipelines, where higher precision indicates less reasoning drift. matical visual elements (e.g., diagrammatic primitives and symbols) but also faithfully incorporates extracted visual cues into subsequent reasoning. Inspired by cognitive science findings on knowledge internalization (Ryan & Connell, 1989; Landy et al., 2014; Wu et al., 2022), this paper introduces COGFLOW, novel three-stage visual mathematical reasoning framework that better mirrors the typical hierarchical structure of the human reasoning process. Concretely, after perception captures raw sensory input, an intermediate knowledge internalization stage transforms low-level perceptual signals into structured and semantically grounded knowledge representations (e.g., humans internalize the perceptual facts that the line segment AB is diameter and the point lies on the circle into the knowledge that ACB = 90) before high-level reasoning begins. As illustrated in Figure 1a, to ensure both accurate extraction of visual information and its faithful use in reasoning, COGFLOW explicitly models the hierarchical sequence of the human reasoning flow (i.e., ❶perception❷internalization❸reasoning) and holistically enhances all three stages in synchrony with it, where each improvement is tailored to the functional role of the corresponding stage in the human reasoning process. Unlike prior approaches (Jia et al., 2024; Chen et al., 2025a; Guo et al., 2025c) that decouple perception from reasoning trajectories (Ouyang et al., 2022) and enhance it with tailored tasks, COGFLOW first integrates perception enhancement into unified reinforcement learning (RL) framework through Synergistic Visual Rewards (SynVRs), enabling dynamic perceptionreasoning interaction and improving generalization. Specifically, SynVRs complementarily optimize the model from two distinct perspectives: (1) Visual Parameterized Reward (VPR) encoding normalized primitives (i.e., points, lines, and circles) and calculating the Euclidean distance in parameter space for precise and interpretable measurement; (2) Visual Semantic Reward (VSR) that extracts semantic embeddings (Xie et al., 2025) from re-rendered images (derived from textual perception outputs) and measures the cosine distance in semantic space to capture holistic style and layout consistency. Together, SynVRs ensure both local geometric fidelity and global perceptual coherence, forming trustworthy visual cues that serve as the foundation for effective visual mathematical reasoning. Notably, despite progress in perception enhancement, all prior efforts (Jia et al., 2024; Guo et al., 2025c; Wei et al., 2025) remain confined to accurate extraction of mathematical information from diagrams, while ignoring key question: are the extracted visual cues properly and faithfully integrated into subsequent reasoning? As illustrated in Figure 1b, our empirical findings reveal typical reasoning drift issue (i.e., the reasoning stage in existing methods often deviates from perceptual results), leading to reasoning chains that appear coherent yet conflict with the underlying visual evidence. To prevent such drift and improve interpretability, COGFLOW utilizes Knowledge Internalization Reward (IntlzR) that bridges the perception and reasoning stages by encouraging the model to generate structured and reasoning-ready outputs (i.e., knowledge-internalized representations (Ryan & Connell, 1989)) as more reliable foundation for subsequent reasoning."
        },
        {
            "title": "Preprint",
            "content": "Specifically, we curate positive trajectories integrating perception and reasoning processes with explicit internalization of perception primitives, and further derive five typical negative trajectories. Training with these trajectories enables the reward model to evaluate each response according to its fidelity to the internalized representation. IntlzR effectively improves the knowledge internalization stage, thereby reducing hallucinations and improving interpretability and robustness. In accordance with the hierarchical flow of human reasoning, we further improve multi-step visual reasoning beyond enhanced perception and knowledge internalization. Existing approaches either follow text-centric RL paradigm (Guo et al., 2025a) that is free from perceptual objectives (Chen et al., 2025b; Wang et al., 2025b), or overlook the structured dependency between perception and reasoning (Shen et al., 2025; Wang et al., 2025a). To ensure more stable reasoning in the presence of perceptual errors, COGFLOW introduces Visual-Gated Policy Optimization (VGPO) strategy that explicitly anchors the reasoning process in perceptual accuracy. In VGPO, visual gate is designed to adaptively filter perceptual trajectories through perceptual quality assessment, retaining only high-quality ones before subsequent reasoning trajectory generation. If low-quality perceptual trajectory is filtered out, the model regenerates alternative trajectories to obtain higher-quality response. Along with the proposed visual gate, VGPO integrates an outcome-supervised Inference Reward (Shao et al., 2024) for optimization, further strengthening multi-step visual reasoning. To facilitate research, we curate new MATHCOG dataset for model training, which contains three subsets and over 120K samples with high-quality perception-reasoning aligned annotations. We conduct extensive experiments on commonly used visual math problem-solving benchmarks (Chen et al., 2025a; Zhang et al., 2024; Lu et al., 2024; Qiao et al., 2025a; Xiao et al., 2024; Zou et al., 2025) to comprehensively evaluate COGFLOW. The results show that COGFLOW consistently outperforms state-of-the-art MLLMs with comparable model sizes. Notably, it achieves on-par or even better results compared to advanced closed-source MLLMs with much larger model sizes. The main contributions of this paper can be summarized as follows: All prior works neglect whether extracted visual cues are faithfully used in reasoning. To address this issue, we present COGFLOW, novel cognitive-inspired three-stage framework that faithfully simulates the hierarchical human reasoning flow: perceptioninternalizationreasoning. In line with human reasoning hierarchy, COGFLOW holistically enhances all three stages: SynVRs complementarily enhance accurate and complete diagram perception in parametric and semantic spaces; IntlzR improves the knowledge internalization ability for promoting faithful conversion of perceptual outputs into canonical context used for subsequent inference; VGPO employs visual gate to filter high-quality perception trajectories and enhances the stability of reasoning. To support model training, we curate new dataset MATHCOG with disentangled perception and reasoning annotations. Comprehensive experiments on multiple visual mathematical benchmarks validate that COGFLOW achieves substantial gains in both answer accuracy and reasoning quality."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Visual Mathematical Reasoning with MLLMs. Solving visual mathematical problems (e.g., geometry diagrams, algebraic plots, and etc) requires both strong reasoning ability and accurate interpretation of visual primitives and symbolic content (Yan et al., 2025; Qiao et al., 2025b; Zhang et al., 2025b). Most previous works are dedicated to improving the reasoning process, including chain-of-thought strategies (Xu et al., 2025a; Deng et al., 2024), tool-aided reasoning (Trinh et al., 2024; Chen et al., 2023b), test time scaling (Wang et al., 2025e; Hosseini et al., 2024), and reinforcement learning (Wang et al., 2025c; Jiang et al., 2025). Several recent works (Guo et al., 2025c; Jia et al., 2024; Wei et al., 2024) suggest that one of the major bottlenecks in visual mathematical reasoning is inaccurate visual comprehension. They typically decouple perception from reasoning, and strengthen perception either by designing specialized visual encoders (Zhang et al., 2025a) or by introducing auxiliary visual tasks (Chen et al., 2025a). However, prior works ignore key issue of whether correctly extracted visual cues are indeed faithfully incorporated into subsequent reasoning. Reinforcement Learning for Multimodal Reasoning. Traditional actorcritic methods, such as proximal policy optimization (Yu et al., 2022), are computationally expensive. lightweight alternative is group relative policy optimization (GRPO) (Guo et al., 2025a), which stabilizes advantage estimation using group baselines. While its variants (Zheng et al., 2025; Yu et al., 2025) have been"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of the proposed visual mathematical reasoning framework COGFLOW. Inspired by the canonical three-stage human reasoning flow, COGFLOW adopts hierarchical pipeline that integrates Synergistic Visual Rewards (SynVRs) for enhanced perception, Knowledge Internalization Reward (IntlzR) to bridge perception and reasoning, and Visual-Gated Policy Optimization (VGPO) with Inference Reward (InfR) to anchor multi-step reasoning in perceptual accuracy. widely explored, GRPO has also been extended to multimodal reasoning (Wang et al., 2025c; Shen et al., 2025; Zhou et al., 2025). Some extensions introduce hybrid reward formulations augmented with preference signals during training (Wang et al., 2025c; Pan et al., 2025), whereas others propose two-stage multimodal reinforcement learning paradigms, e.g., OVR (Wei et al., 2025). However, existing methods typically lack explicit mechanisms to strengthen the alignment between perception and reasoning, often leading to reasoning that is not firmly grounded in visual content."
        },
        {
            "title": "3 COGFLOW: A COGNITIVE-INSPIRED HIERARCHICAL FRAMEWORK",
            "content": "Inspired by the typical cognitive process of human reasoning (i.e., ❶perception❷internalization ❸reasoning), COGFLOW serves as visual mathematical reinforcement learning framework that explicitly implements the internalization stage (see Figure 2). Before training, we first curate MATHCOG to support the subsequent training (see Figure 10). Specifically, the training pipeline of COGFLOW consists of two sequential phases: supervised fine-tuning (SFT) phase and reinforcement learning (RL) phase. The SFT phase endows the base model with initial visual perception and basic reasoning skills based on the MATHCOG-SFT dataset. During the RL phase, we optimize the policy based on the MATHCOG-RL dataset under the Visual-Gated Policy Optimization (VGPO) framework to explicitly anchor the reasoning process in perceptual accuracy. Concretely, VGPO introduces visual gate to adaptively filter perceptual trajectories before reasoning trajectory generation. Furthermore, the rewards in VGPO are composed of three components: Synergistic Visual Rewards (SynVRs) for forming trustworthy perception, Knowledge Internalization Reward (IntlzR) for detecting reasoning drift and Inference Reward (InfR) for providing outcome-supervision. 3.1 FORMING TRUSTWORTHY PERCEPTION WITH SYNERGISTIC VISUAL REWARDS COGFLOW first constructs Synergistic Visual Rewards (SynVRs), enabling dynamic perceptionreasoning interaction and improving generalization. Specifically, the proposed SynVRs combine two complementary components: the Visual Parameterized Reward (VPR) and the Visual Semantic Reward (VSR), which respectively evaluate perceptual quality in the parametric and semantic spaces, thereby providing synergistic perception feedback integrated into RL training loops. Measuring Perceptual Accuracy in Parameter Space. As shown in Figure 3, VPR first converts structured visual information into parametric expressions. For example, the primitive Circle (0.01, 0.46, 7.68) is transformed into the Equation (x 0.01)2 + (y + 0.46)2 = 7.682. We then compute the cost matrix between GT Primitives and predicted Primitives and apply the Hungarian matching algorithm (Kuhn, 1955) to obtain the optimal one-to-one matching that minimizes the total cost SVPR. The VPR offers interpretable, geometry-aware semantic supervision"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Workflow of SynVRs. SynVRs consist of Visual Semantic Reward and Visual Parametric Reward, ensuring local geometric fidelity and global perceptual coherence respectively. Together, these two complementary visual rewards provide unified supervision mechanism for training robust and accurate visual perception. Figure 4: Pipeline of VGPO. VGPO introduces visual gate and multiple rewards to strengthen multi-step visual reasoning. Coupling perceptual quality control with outcome-based optimization promotes stability. that avoids the pitfalls of pixel-level noise or black-box embedding similarity. More details are provided in C.2. Capturing Holistic Layout and Style Consistency in Semantic Space. We render the predicted response into an image and compare it against the ground-truth rendering I, using frozen FGCLIP encoder (Xie et al., 2025) ϕ() with higher sensitivity to spatial features. The VSR score SVSR is the normalized cosine similarity. The higher values indicate closer agreement in global layout and style while preserving fine-grained geometric fidelity. Synergy of Complementary Visual Rewards. Finally, the SynVRs score can be formulated as SSynVRs = α exp (cid:18) (cid:124) 1 (cid:88) (i,j)H (cid:123)(cid:122) SVPR (cid:19) C(Pi, Gj) + (1 α) (cid:125) 1 + cos(cid:0)ϕ( I), ϕ(I)(cid:1) 2 (cid:123)(cid:122) SVSR (cid:125) (cid:124) , (1) where α [0, 1] balances local geometric precision (SVPR) and global visual consistency (SVSR). SSynVRs has two roles: (1) it acts as gate to prevent low-quality perceptions from propagating during policy generation, and (2) as training reward during RL to encourage accurate perception. 3.2 ALLEVIATING REASONING DRIFT WITH KNOWLEDGE INTERNALIZATION REWARD Failure Modes in Knowledge Internalization. Even with accurate perception, models often struggle to reliably internalize what they see. The empirical evidence suggests that such failures typically manifest in five forms: (1) omit or misbind primitives, losing essential elements or confusing their identifiers; (2) introduce nonexistent facts, fabricating geometric relations that are not present in the perceptual output; (3) invoke external theorems inappropriately, applying mathematical results that were never justified by the internalized structure; (4) contradict geometric constraints, producing inference steps that violate fundamental properties of the canonical representation; and (5) refer inconsistently to established elements, assigning shifting roles or properties to the same primitive across different reasoning steps. Therefore, to enhance knowledge internalization, it is crucial to mitigate these systematic failures. Rewarding Faithful Knowledge Internalization. To address these systematic weaknesses, we introduce the Knowledge Internalization Reward (IntlzR), trained reward model that evaluates whether each reasoning chain remains faithful to the internalized visual representation, thereby enforcing raw perception as the primary substrate of inference (e.g., reasoning drift). Within COGFLOW, we train the IntlzR to enhance internalization explicitly, supported by the curated MATHCOG-IntlzR (see C.3 for more details). This dataset is constructed from positivenegative pairs, with each positive matched to five corresponding negatives; positives are sampled from MATHCOG-SFT, and negatives are synthesized by injecting the five error types described above, providing fine-grained supervision for distinguishing grounded from unanchored reasoning. To bet-"
        },
        {
            "title": "Preprint",
            "content": "ter leverage diverse negative signals in IntlzR and adaptively emphasize the most challenging trajectories while improving gradient efficiency and training stability, we adopt Softmax-DPO (Chen et al., 2024a) for optimization: LSoftmax-DPO = log σ log (cid:88) j= exp (cid:0)s s+(cid:1) , = β(cid:2) log πθ(y x) log πref(y x)(cid:3), (2) where s+ denotes the score of the preferred trajectory, {s j=1 represent the corresponding scores of the dispreferred trajectories, β is the KL penalty coefficient and σ() denotes the sigmoid function. This formulation contrasts one positive trajectory against multiple negatives simultaneously, thereby providing denser and more informative supervision, implicitly emphasizing hard negatives through softmax weighting, and yielding more stable optimization with stronger robustness to unseen misalignment patterns. During RL, IntlzR is evaluated stepwise and aggregated across the chain, rewarding trajectories in which the extracted visual cues are faithfully and adequately integrated into subsequent reasoning. }m 3.3 STRENGTHENING MULTI-STEP VISUAL REASONING WITH VGPO Accurate perception and faithful internalization are necessary but not sufficient: the model must also produce long reasoning chains that are coherent, interpretable, and verifiably grounded. Hence, we introduce Visual-Gated Policy Optimization (VGPO), which integrates visual gate with grouplevel optimization to regularize the reasoning process. Concretely, as shown in Figure 4, for an input question we sample candidate trajectories yi(x) = (cid:0)yw (x) denotes the perception trajectories, i.e., structured parse of perceptual primitives and relations from the diagram, and yt (x)) denotes the reasoning trajectories conditioned on and yw (x))(cid:1). Here yw (x), yt (x, yw (x, yw (x). Visual Gate for Reliable Visually Grounded Reasoning. We introduce visual gate that scores each perception against the visual evidence and forwards only the most faithful parse to reasoning. (x) and the ground-truth ˆyw(x), while Ii and denote Specifically, given perception candidate yw their respective renderings, we define the perceptual accuracy score as: (cid:40)SVPR (cid:0) Ii, I(cid:1), SVSR (x), ˆyw(x)(cid:1) + SVSR (cid:0)yw (cid:0) Ii, I(cid:1), (x)(cid:1) = inference. training, (cid:0)yw Svis (3) Subsequently, the visual gate Γ() enforces perception quality by scoring each perception trajectory and accepting the first attempt whose score exceeds preset threshold. If no attempt passes within trials, the gate returns the attempt with the highest Svis: (cid:40) κ = min{ {1, . . . , } : Svis(yw M, i,k(x)) τ }, {k : Svis(yw {k : Svis(yw i,k(x)) τ } = , i,k(x)) τ } = , yw (x) = Γ(cid:0){yw i,k(x)}κ k=1 (cid:1) = arg max 1kκ Svis (cid:0)yw i,k(x)(cid:1), (4) (5) where κ is the stopping index, defined as the smallest whose score reaches acceptance threshold τ , yw i,k(x) is the k-th perception trajectory for input x. (x) is the perception trajectory selected by the visual gate; and yw Stabilizing Multi-Step Reasoning via Visual-Gated Policy Optimization. For each problem, we collect multiple candidate trajectories yi(x) and evaluate them under group-level reward that integrates (1) the Synergistic Visual Rewards RSynVRs for perceptual fidelity, (2) the Knowledge Internalization Reward RIntlzR for internalization faithfulness, and (3) an Inference Reward RInfR capturing answer correctness and output format, given by: = λSynVRs RSynVRs + λIntlzR RIntlzR + λInfR RInfR. Then the training optimization objective is formulated as: = Ei (cid:104) (cid:16) min ηi(θ)Ai, clip(cid:0)ηi(θ), 1 ϵ, 1 + ϵ(cid:1)Ai (cid:17)(cid:105) + βKL DKL (cid:0)πθ πref (cid:1), (6) (7)"
        },
        {
            "title": "Preprint",
            "content": "Model Table 1: Accuracy (%) and FlowVerse-style CoT-E (%) results on FlowVerse. All Text Centric Text Limited Text Plus Vision Dense Vision Centric Vision Primary CoT-E Acc CoT-E Acc CoT-E Acc CoT-E Acc CoT-E Acc CoT-E Acc CoT-E Claude-3.5-Sonnet GPT-4o GPT-4V MathFlow Gemini-2.5-pro GPT-5 GPT4V 55.5 56.9 64.2 64.2 64.5 68.2 37.8 InfiMM-Math-7B 46.3 InternVL2.5-8B Math-LLaVA-13B 39.3 MultiMath-7B 45.2 SVE-Math-Qwen2.5-7B 47.9 50.7 VLM-R1-7B COGFLOW-7B 66.0 45.1 49.7 58.7 59.5 56.2 59. 29.5 40.1 30.8 35.3 38.7 41.2 56.2 60.8 61.0 69.1 69.5 68.3 74.3 43.8 49.2 45.1 50.6 53.1 59.0 67.9 52.6 56.8 57.1 58.2 61.9 68.1 38.1 41.3 39.3 44.8 47.3 54.2 58.6 58.7 58.7 65.0 67.2 66.1 73. 40.6 40.5 44.4 49.9 53.4 57.9 67.3 50.3 54.4 55.0 57.4 60.8 66.7 36.7 38.4 37.4 42.9 45.8 49.8 58.3 64.0 62.2 72.0 71.1 68.9 77.0 46.1 49.6 - - - 65.5 68.1 58.3 58.2 61.4 64.1 64.1 69. 40.1 42.7 - - - 58.9 60.9 45.0 45.2 48.1 52.7 52.1 53.8 28.8 38.4 36.2 41.7 44.2 36.2 57.8 25.4 30.0 30.3 47.5 37.1 44.7 15.4 20.2 18.6 22.1 28.6 24.5 42.7 56.5 58.6 61.8 62.1 65.7 67. 39.6 41.0 41.7 47.2 48.9 46.1 68.2 48.0 52.6 46.3 57.1 57.9 61.7 30.3 35.9 35.9 40.4 44.2 37.8 61.1 48.1 54.1 42.0 60.4 57.0 60.3 26.1 35.8 37.0 39.7 45.8 30.6 66.7 Acc 45.2 51.0 36.7 57.0 54.6 57.5 23.2 33.9 34.2 38.8 42.0 26.1 63.5 Table 2: Accuracy (%) and MathVerse-style CoT-E (%) results on testmini set of MathVerse. Model All Text Dominant Text Lite CoT-E Acc CoT-E Acc CoT-E Acc CoT-E Acc CoT-E Text Only Vision Intensive Vision Dominant Vision Only Acc CoT-E Acc Acc CoT-E Qwen-VL-Plus Gemini-Pro Qwen-VL-Max GPT-4V MathFlow GPT4V SPHINX-MoE-56B InternLM-XC2-7B Math-LLaVA-13B MultiMath-7B SVE-Math-Qwen2.5-7B DVLR-14B SophiaVL-R1-7B COGFLOW-7B 21.3 35.3 37.2 54.4 56.7 25.8 25.9 - - - 48.1 48.8 53.9 11.8 23.5 25.3 39.4 43.8 15.6 16.5 20.1 26.9 31.4 - - 39.5 26.0 39.8 42.8 63.1 65.2 33.3 36.9 - - - 54.3 45.4 60. 15.7 26.3 30.7 54.7 51.1 22.2 22.3 22.8 34.8 37.6 - - 41.9 21.2 34.7 37.7 56.6 58.9 21.9 28.3 - - - 49.0 43.9 51.2 11.1 23.5 26.1 41.4 46.4 16.4 17.0 21.8 30.8 36.8 - - 37. 25.2 44.5 47.9 60.3 62.1 40.7 42.5 - - - - - 52.3 14.5 27.3 28.9 48.7 48.5 18.3 16.5 - - - - - 40.1 18.5 32.0 33.6 51.4 53.7 21.1 20.1 - - - 46.3 45.1 55. 9.0 23.0 24.1 34.9 40.3 14.8 15.7 21.1 28.1 34.9 - - 42.4 19.1 36.8 35.9 50.8 52.1 19.6 24.4 - - - 47.2 58.5 58.7 13.0 22.3 24.1 34.4 37.4 12.6 16.4 19.2 25.9 31.5 - - 44. 21.8 33.3 35.9 50.3 52.5 18.3 19.8 - - - 43.8 51.3 44.2 10.0 22.2 21.4 31.6 39.0 9.1 11.0 15.4 15.0 16.0 - - 26.3 yi(x) = (cid:0)yw (x), yt (x, Γ(yw (x)))(cid:1), ηi(θ) = (cid:0)yi(x) x(cid:1) (cid:0)yi(x) x(cid:1) , Ai = πθ πθold ri µgroup(r) σgroup(r) + ε , (8) where the Ei[] is the empirical average over candidates in the group; the Ai is the group-normalized advantage; ηi(θ) is the likelihood ratio w.r.t. the behavior policy πθold; clip() uses the PPO-style hyperparameter ϵ; µgroup and σgroup are the group mean and standard deviation of ri (with small stabilizer ε in the denominator); βKL is the KL penalty coefficient and DKL(πθπref ) denotes the forward KL to frozen reference policy (e.g., the SFT model), implemented as the token/stateaveraged KL in practice. By combining visual gate with group-level optimization, VGPO stabilizes long-horizon training and encourages the emergence of interpretable chain-of-thoughts. This mechanism ensures that COGFLOW converges toward the desired paradigm: first perceive correctly, then reason correctly."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Evaluation Benchmarks and Metrics. We conduct experiments on widely used benchmarks (i.e., FlowVerse (Chen et al., 2025a), MathVerse (Zhang et al., 2024), MathVista (Lu et al., 2024), WeMath (Qiao et al., 2025a), LogicVista (Xiao et al., 2024), and DynaMath (Zou et al., 2025)) with various visual mathematical reasoning tasks and different visual and textual complexity. Following common practice, we measure the accuracy (Acc) of the final answer and assess the reasoning ability from the intermediate reasoning process using the chain-of-thought evaluation (CoT-E). Baselines. We compare our method with wide range of closed-source MLLMs, such as GPT series (OpenAI, 2024; 2023; 2025), Gemini series (Team et al., 2025), DouBao-1.5-pro (Guo et al., 2025b), GLM-4.5V (Hong et al., 2025), Claude (Anthropic, 2024) and Qwen-VL-Plus (Bai et al., 2023), as well as open-source MLLMs, including InfiMM-Math (Han et al., 2025), In-"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Accuracy (%) results on MathVista. COGFLOW demonstrates consistent superiority. Table 4: COGFLOW shows competitive accuracy (%) results on more visual math benchmarks. Model All FQA GPS MWP TQA VQA Models WeMath LogicVista DynaMath GPT-4V Claude-3.5-Sonnet 67.7 Doubao-pro-1.5 49.9 43.1 50.5 57.5 65.2 38.0 - - - - - 79.5 77.7 88.9 86.0 82.3 62.0 25.1 19.1 48.7 3.6 25.0 28.7 G-LLaVA-7B VCAR-7B 33.7 30.9 34.6 38.7 37.3 28.5 SPHINX-Plus-56B 36.7 54.6 16.4 23.1 41.8 43.0 37.4 31.9 53.9 29.0 41.4 30.8 SVE-Math-7B MultiMath-7B 50.0 40.1 66.8 61.8 50.0 33.0 SophiaVL-R1-7B 71.3 ThinkLite-VL-7B 71.6 VL-Rethinker-7B 73.7 COGFLOW-7B 76.8 70.4 93.1 73.7 86.9 59.3 73.4 - - - - - - - - - - - - - - Claude-3.7-Sonnet GLM-4.5V Doubao-1.5-Pro GPT-5 Gemini-2.5-Pro Ovis-8B Qwen2.5-VL-8B InternVL3-8B Keye-VL-8B InternVL3.5-8B GLM-4.1V-9B COGFLOW-7B 49.3 68.8 65.7 71.1 78. 27.2 35.2 37.1 60.7 57.0 63.8 64.1 58.2 62.4 64.2 70.0 73.8 39.4 44.1 44.1 54.8 57.3 60.4 58.1 39.7 53.9 44.9 60.9 56.3 20.4 21.0 25.5 37.3 37.7 42.5 46.2 ternVL2.5 (Chen et al., 2024b), InternLM-XC2 (Dong et al., 2024), Math-LLaVA (Shi et al., 2024), MultiMath-7B (Peng et al., 2024a), SVE-Math (Zhang et al., 2025b), VLM-R1 (Shen et al., 2025), MathFlow (Chen et al., 2025a), DVLR (Guo et al., 2025c), VCAR (Jia et al., 2024), VLRethinker (Wang et al., 2025a), ThinkLite-VL (Wang et al., 2025d) and SPHINX-MoE (Liu et al., 2024a). Since some concurrent works have yet to release their models, we report the results provided in their original papers, some of which do not include category-wise performance. Implementation Details. We initialize COGFLOW with Qwen2.5-VL-7B (Bai et al., 2025) and train it on our curated MATHCOG dataset. For cold start, we first train COGFLOW on MATHCOGSFT subset for 2 epochs with learning rate of 1 105 and batch size of 64. Subsequently, COGFLOW is optimized using VGPO on MATHCOG-RL subset for 1 epoch with learning rate of 1 106 and batch size of 16. The IntlzR reward model is based on Qwen2.5-VL-3B and trained on MATHCOG-IntlzR subset for 3 epochs with learning rate of 7 106 and batch size 64. All our models are trained on 16 NVIDIA A100 GPUs. Please refer to for more detailed configures. 4.2 MAIN RESULTS As shown in Tables 14, COGFLOW achieves 66.0% accuracy on FlowVerse, 53.9% on MathVerse, 76.8% on Mathvista, 64.1% accuracy on WeMath and 46.2% on DynaMath, surpassing all opensource baselines by large margins. In terms of LogicVista, it delivers competitive performance, trailing only GLM-4.IV-9B. COGFLOWs improvements in visual perception are particularly significant, as evidenced by its superior performance on subsets where visual information dominates. For example, on FlowVerse it reaches 42.7% on Vision Dense, 55.6% on Vision Primary, and 61.1% on Vision Centric. On MathVerse, the gains are consistent across Vision Intensive (44.8%), Vision Dominant (42.1%), and Vision Only (25.7%). These results demonstrate that COGFLOW yields more accurate parsing of geometric primitives and relations, leading to stronger perceptionreasoning integration. COGFLOW also delivers higher-end-to-end problem-solving accuracy across all settings. The largest absolute gains appear in visually demanding subsets, where perception-anchored reasoning is essential. This demonstrates that COGFLOW not only perceives diagrams more accurately but also reasons over them more effectively. Figure 19 further shows that COGFLOW produces more structurally accurate and semantically consistent predictions compared to strong baselines. Please refer to D.12 for more detailed configurations. 4.3 ABLATION STUDIES Component Ablation. Table 5 reports the ablation results of COGFLOW. We observe that every component in our framework contributes positively to overall performance, which confirms the necessity of jointly addressing perception, internalization, and reasoning. Among all modules, VGPO emerges as the most influential, as it directly stabilizes long-horizon reasoning through visual gate and group-level optimization, thereby yielding the largest single-module gain."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Ablation of three proposed components (i.e., SynVRs, IntlzR and VGPO). The visual gate is always enabled during inference. SynVRs IntlzR VGPO FlowVerse MathVerse CoT-E Acc CoT-E Acc 57.4 48.7 63.2 54.7 62.7 53.5 63.4 54.8 64.4 55.1 66.0 56.2 48.2 35.6 50.5 36.9 49.9 36.2 50.8 37.3 52.1 38.0 53.9 39.5 Figure 5: Ablation analysis of SynVRs. Variants exhibit consistent improvements. (a) Impact of different error types, where All indicates all types are used. (b) Impact of DPO variants. Figure 6: Ablation analysis of the proposed Knowledge Internalization Reward. Analysis on Synergistic Visual Rewards. As shown in Figure 5, the model without any visual reward demonstrates reasonable performance. However, adding either VSR or VPR consistently improves both CoT-E and final accuracy across MathVerse and FlowVerse. The best results come from combining VSR and VPR, improving CoT-E by up to +3.0% and accuracy by +1.7% on MathVerse, and CoT-E by +2.2% and accuracy by +2.1% on FlowVerse over the baseline. These results confirm that VPR ensures geometric precision while VSR stabilizes the global layout, and their combination provides the most reliable perceptual supervision. Analysis on Knowledge Internalization Reward. As shown in Figure 6, removing any single Error type consistently degrades performance, indicating that each contributes complementary supervision for internalization. The largest drops arise when excluding omission/misbinding primitives or contradicting geometric constraints, highlighting that correctly binding primitives and respecting core geometric constraints are most critical for keeping reasoning tied to perception. Figure 6 compares different training strategies during training. Vanilla DPO with all categories offers moderate gains, whereas Softmax-DPO achieves the best results, reaching 66.0% / 56.2% on FlowVerse. This suggests that fine-grained, within-problem preference modeling better captures the nuances of perception-grounded reasoning. Analysis on Visual-Gated Policy Optimization. As shown in Figure 7, the distribution of visual reward values shifts steadily upward among different post-training methods, with VGPO producing both higher medians and more concentrated high-reward samples. This indicates that VGPO further enhances stability and fidelity by explicitly introducing visual-gated mechanism. To disentangle the effect of VGPO from the effect of the gate itself, we conducted an additional ablation of the visual gate, as shown in Figure 8. These results indicate that visual gate is also beneficial during inference, yielding consistent absolute gain of around 0.6-1% accuracy even when the model is trained without VGPO. Moreover, VGPO provides an additional and more substantial improvement, because it uses the visual gate during training to shape the policy itself, rather than only filtering outputs at test time. Error Type Analysis. As shown in Figure 9, we employ GPT-5 (OpenAI, 2025) to classify each response on the FlowVerse benchmark into one of four categories: Perception Error, Knowledge Internalization Error, Reasoning Error, and Correct. The first row covers specialized visual-math MLLMs (MultiMath-7B (Peng et al., 2024a), SVE-Math-7B (Zhang et al., 2025b)), an GRPO-style model (VLM-R1 (Shen et al., 2025)), the decoupled method (MathFlow-7B (Chen et al., 2025a)), and GPT-4o (OpenAI, 2024); the second row traces our variants from the SFT+GRPO baseline through +SynVRs, +IntlzR, and +VGPO to COGFLOW. We observe: (1) GPT-4os strong overall performance is driven by superior reasoning, yet its diagram perception lags behind specialized"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: The distribution of visual reward values among different post-training methods. higher concentration of values indicates stronger perceptual grounding achieved by the corresponding training strategy. Figure 8: Ablation analysis of visual gate. Training and Inference indicates visual gate is only used in training and inference phases respectively. Figure 9: Error-type analysis. We analyze error-type distributions for COGFLOW variants alongside specialized visualmath models, the GRPO-style model, and the decoupled method. The baseline is denoted by the SFT+GRPO setting. 7B models, indicating that strengthening perception and internalization remains essential; (2) Although methods such as MathFlow-7B and VLM-R1 reduce Perception Error and Reasoning Error to some extent, these methods do not meaningfully alleviate Knowledge Internalization Error; (3) +SynVRs chiefly reduces Perception Errors (2%), confirming that geometry-aware and semantic visual rewards improve perceptual fidelity; (4) +IntlzR primarily mitigates reasoning drift, reducing Knowledge-Internalization Error by 2% relative to the baseline; and (5) when all components are combined, COGFLOW minimizes every error type while maximizing the proportion of correct predictions. Furthermore, we conduct systematic case study (see D.13) to analyze the error compensation mechanism."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Faithful integration of visual perception into reasoning remains critical yet overlooked challenge in visual mathematical problem solving. In this paper, we observe prevalent reasoning drift issue and propose cognitive-inspired three-stage framework COGFLOW that explicitly models the hierarchical human reasoning flow from perception to knowledge internalization and finally to reasoning. By jointly enhancing perceptual fidelity, enforcing structured knowledge internalization, and anchoring multi-step reasoning to visual evidence, COGFLOW effectively mitigates reasoning drift and improves both reasoning accuracy and interpretability. Extensive experiments on commonly used visual mathematical benchmarks validate its consistent advantages over existing MLLMs."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. claude-3-5-sonnet system card, 2024. URL https://www.anthropic.com/ news/claude-3-5-sonnet. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Felix Chen, Hangjie Yuan, Yunqiu Xu, Tao Feng, Jun Cen, Pengwei Liu, Zeying Huang, and Yi Yang. Mathflow: Enhancing the perceptual flow of mllms for visual mathematical problems. arXiv preprint arXiv:2503.16549, 2025a. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. TMLR, 2025b. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023a. Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207, 2025c. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. TMLR, 2023b. Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, and In NeurIPS, Tat-Seng Chua. On softmax direct preference optimization for recommendation. 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen tau Yih, Shang-Wen Li, and Hu Xu. Meta clip 2: worldwide scaling recipe. In NeurIPS, 2025. Linger Deng, Yuliang Liu, Bohan Li, Dongliang Luo, Liang Wu, Chengquan Zhang, Pengyuan Lyu, Ziyang Zhang, Gang Zhang, Errui Ding, Yingying Zhu, and Xiang Bai. R-cot: Reverse chain-ofthought problem generation for geometric reasoning in large multimodal models. arXiv preprint arXiv:2410.17885, 2024. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024."
        },
        {
            "title": "Preprint",
            "content": "Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing HONG, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-LLaVA: Solving geometric problem with multi-modal large language model. In ICLR, 2025. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 2025a. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. Zixian Guo, Ming Liu, Qilong Wang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. In ICCV, Integrating visual interpretation and linguistic reasoning for math problem solving. 2025c. Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, and Quanzeng You. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. In Findings of EMNLP, 2025. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. In COLM, 2024. Mengzhao Jia, Zhihan Zhang, Wenhao Yu, Fangkai Jiao, and Meng Jiang. Describe-then-reason: Improving multimodal mathematical reasoning through visual comprehension training. arXiv preprint arXiv:2404.14604, 2024. Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, and Shikun Zhang. Vlm-r3: Region recognition, reasoning, and refinement for enhanced multimodal chain-of-thought. arXiv preprint arXiv:2505.16192, 2025. Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 1955. David Landy, Colin Allen, and Carlos Zednik. perceptual account of symbolic reasoning. Frontiers in Psychology, 2014. Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao. SPHINX-x: Scaling data and parameters for family of multi-modal large language models. In ICML, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024b. Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, and Hoifung Poon. X-reasoner: Towards generalizable reasoning across modalities and domains. arXiv preprint arXiv:2505.03981, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. OpenAI. GPT-4V(vision) system card, 2023. URL https://openai.com/research/ gpt-4v-system-card."
        },
        {
            "title": "Preprint",
            "content": "OpenAI. GPT-4o system card, 2024. URL https://openai.com/index/ gpt-4o-system-card/. OpenAI. GPT-5 system card, 2025. URL https://openai.com/research/ gpt-5-system-card. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, In and Ryan Lowe. Training language models to follow instructions with human feedback. NeurIPS, 2022. Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of visionlanguage models (vlms) via reinforcement learning. In MICCAI, 2025. Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024a. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. Grounding multimodal large language models to the world. In ICLR, 2024b. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? In ACL, 2025a. Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, Jie Wang, Chong Sun, Chen Li, and Honggang Zhang. We-math 2.0: versatile mathbook system for incentivizing visual mathematical reasoning. arXiv preprint arXiv:2508.10433, 2025b. Richard M. Ryan and James P. Connell. Perceived locus of causality and internalization: Examining reasons for acting in two domains. Journal of Personality and Social Psychology, 1989. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In EuroSys, 2025. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. In Findings of EMNLP, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2025."
        },
        {
            "title": "Preprint",
            "content": "Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. In NeurIPS, 2025a. Jiaqi Wang, Kevin Qinghong Lin, James Cheng, and Mike Zheng Shou. Think or not? selective reasoning via reinforcement learning for vision-language models. In NeurIPS, 2025b. Peiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, and Yahui Zhou. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025c. Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. In NeurIPS, 2025d. Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, Dacheng Tao, and Xipeng Qiu. Visuothink: Empowering lvlm reasoning with multimodal tree search. In ACL, 2025e. Haoran Wei, Youyang Yin, Yumeng Li, Jia Wang, Liang Zhao, Jianjian Sun, Zheng Ge, Xiangyu Zhang, and Daxin Jiang. Slow perception: Lets perceive geometric figures step-by-step. arXiv preprint arXiv:2412.20631, 2024. Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, and Vishal M. Patel. Open vision reasoner: Transferring linguistic cognitive behavior for visual reasoning. In NeurIPS, 2025. Zhiyong Wu, Wei Bi, Xiang Li, Lingpeng Kong, and Ben Kao. Lexical knowledge internalization for neural dialog generation. In ACL, 2022. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, and Yuhui Yin. Fg-clip: Fine-grained visual and textual alignment. In ICML, 2025. Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. In ICCV, 2025a. Yunqiu Xu, Linchao Zhu, and Yi Yang. Mc-bench: benchmark for multi-context visual grounding in the era of mllms. In ICCV, 2025b. Yibo Yan, Jiamin Su, Jianxiang He, Fangteng Fu, Xu Zheng, Yuanhuiyi Lyu, Kun Wang, Shen Wang, Qingsong Wen, and Xuming Hu. survey of mathematical reasoning in the era of multimodal large language model: Benchmark, method & challenges. In Findings of ACL, 2025. Jie Yang, Feipeng Ma, Zitian Wang, Dacheng Yin, Kang Rong, Fengyun Rao, and Ruimao Zhang. Wethink: Toward general-purpose vision-language reasoning via reinforcement learning. arXiv preprint arXiv:2506.07905, 2025. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mPLUG-Owl2: Revolutionizing multi-modal large language model with modality collaboration. In CVPR, 2024. Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. In NeurIPS, 2022."
        },
        {
            "title": "Preprint",
            "content": "Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale. In NeurIPS, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In ECCV, 2024. Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Shanghang Zhang, Peng Gao, and Hongsheng Li. MAVIS: Mathematical visual instruction tuning with an automatic data engine. In ICLR, 2025a. Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng, Piotr Koniusz, Kai Zou, Anton van den Hengel, and Yuan Xue. Open eyes, then reason: Fine-grained visual mathematical understanding in mllms. arXiv preprint arXiv:2501.06430, 2025b. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo, and Yaodong Yang. Generative rlhf-v: Learning principles from multi-modal human preference. In NeurIPS, 2025. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. In ICLR, 2025."
        },
        {
            "title": "TABLE OF CONTENTS",
            "content": "A General Discussions A.1 Ethics Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Reproducibility Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 The Use of Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Limitations and Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Details of MATHCOG Dataset B.1 Dataset License and Intended Use . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Data Curation . . B.3 Dataset Statistics . . B.4 Dataset Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Implementation Details C.1 Model Initialization . C.2 The Details of VPR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 The Details of IntlzR Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 The Details of InfR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Details of SFT and RL Training . . . . . . . . . . . . . . . . . . . . . . . . . . . C.6 Details of the Reasoning Drift Precision Metric . . . . . . . . . . . . . . . . . . . Additional Analysis D.1 Analysis on the Overall Effectiveness of COGFLOW . . . . . . . . . . . . . . . . . D.2 More Analysis on Synergistic Visual Rewards . . . . . . . . . . . . . . . . . . . . D.3 The Impact and Efficiency of The Vision Gate . . . . . . . . . . . . . . . . . . . . D.4 Analysis on Visual-Gated Policy Optimization . . . . . . . . . . . . . . . . . . . . D.5 Clarifying the Cognitive-Science Analogy . . . . . . . . . . . . . . . . . . . . . . D.6 Deeper insights into the mechanism of multi-stage reward structure . . . . . . . . . D.7 Sizes and Architectures Ablation. . . . . . . . . . . . . . . . . . . . . . . . . . . . D.8 Source of Performance Improvement . . . . . . . . . . . . . . . . . . . . . . . . . D.9 Ablations on Individual SynVRs Components . . . . . . . . . . . . . . . . . . . . D.10 Evaluating FG-CLIP for Similarity Reward . . . . . . . . . . . . . . . . . . . . . D.11 Further Analysis of Error Types . . . . . . . . . . . . . . . . . . . . . . . . . . . D.12 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.13 Analysis of Error Compensation Mechanism . . . . . . . . . . . . . . . . . . . . . 16 17 17 17 17 19 19 21 22 24 24 24 24 25 26 27 27 28 29 30 30 31 32 32 32"
        },
        {
            "title": "A GENERAL DISCUSSIONS",
            "content": "A.1 ETHICS STATEMENT Our study focuses on visual mathematical reasoning and does not involve human-subjects experiments, personally identifiable information, or biometric data. All benchmarks used are publicly available under their original licenses. We introduce MATHCOG (SFT/IntlzR/RL subsets) to support training. MATHCOG contains diagrammatic math items and corresponding solutions (including synthetic negatives generated by controlled perturbations of reasoning), andto the best of our knowledgecontains no sensitive or personal data. When reusing third-party content, we preserve original attribution, respect redistribution constraints, and release only the metadata/derivatives permitted by the source licenses. We do not identify any foreseeable ethical risks associated with this work. We disclose computing resources and carbon-related considerations in the appendix and follow common practices to reduce the footprint (mixed precision, batching, and early stopping). The authors declare no conflicts of interest beyond those listed on the title page. A.2 REPRODUCIBILITY STATEMENT To facilitate reproducibility, we have released the full COGFLOW, including code, configuration files, and training dataset. training and evaluation package for Data. We have released our constructed MATHCOG dataset (including MATHCOG-SFT, MATHCOG-IntlzR, and MATHCOG-RL subsets). We have also provided the construction scripts for MATHCOG-IntlzR, including the positivenegative pairing procedure and the five error-type perturbations used to synthesize negative trajectories. Training and Implementation. We have provided runnable training recipes for the two-stage pipeline (SFT followed by VGPO-based RL), including the base model initialization, optimizer settings, batch sizes, learning rates, training epochs, and hardware requirements. We have released the exact reward formulation and hyperparameters used by VGPO, including the reward weights (λSynVRs, λIntlzR, λInfR), the SynVRs mixing coefficient α, and the visual gate parameters. Evaluation Protocol. We have released evaluation scripts for all benchmarks used in the paper and report metrics consistent with the main text (answer accuracy and CoT-based evaluation). For baselines whose models or code are not publicly available, we have clearly indicated which numbers are taken from the original papers. Ablations and Diagnostics. We provide detailed ablation studies in 4.3, covering the contributions of SynVRs, IntlzR, VGPO, and the visual gate. We further report extended analyses of VGPO and the visual gate in C.6. All key training hyperparameters and configurations are specified in C, and the visual gate parameters are defined in 3.3. A.3 THE USE OF LARGE LANGUAGE MODELS large language model (i.e., ChatGPT-5) was used in two ways during the preparation of this paper: To aid in polishing the writing, including improvements to grammar, clarity, and readability. For retrieval and discovery, such as identifying and organizing related prior work. The model did not contribute to research ideation, experiment design, data analysis, or interpretation of results. Its use was limited to non-scientific assistance (e.g., language polishing, rephrasing, and formatting) under direct author supervision. All scientific content, methodological decisions, and conclusions are the sole responsibility of the authors. A.4 LIMITATIONS AND FUTURE WORKS While our approach demonstrates strong performance, it has certain limitations. Most notably, the training procedure is computationally demanding, requiring substantial resources to achieve stable improvements. This dependence on large-scale computation may hinder broader adoption, especially in resource-constrained settings."
        },
        {
            "title": "Preprint",
            "content": "For future work, we would like to expand our method (especially the method of obtaining visual primitives) to general scenes. While this paper is primarily focused on visual mathematical problems, the procedure for obtaining these primitives is domain-agnostic. The notion of primitives, as structured representation of visual information, naturally extends to natural scenes beyond the domain of shapes. Concretely, for natural images we could first leverage detection or segmentation models to obtain instance-level regions and their spatial extents. With the assistance of an LLM and lightweight human verification, these outputs can be normalized into unified primitive schema that encodes structured semantic features for each instance and their relationships. Such representation enables consistent knowledge-internalization step that transforms raw perceptual outputs into compact, reasoning-ready state, allowing downstream modules to operate over explicit and verifiable visual evidence. COGFLOWs strong performance on visual mathematical reasoning suggests that the pipeline generalizes beyond visual math problems. By explicitly decomposing the process into perception, knowledge internalization, and reasoningand training these stages to remain mutually consistentwe obtain broadly effective framework for visually grounded multimodal reasoning, with the potential to achieve strong performance when transferred to other visionlanguage domains."
        },
        {
            "title": "B ADDITIONAL DETAILS OF MATHCOG DATASET",
            "content": "Existing multimodal reasoning corpora provide abundant natural language annotations but rarely disentangle perception from reasoning (Zhang et al., 2025a; Gao et al., 2025). This gap makes it difficult to supervise models in way that enforces accurate perception and strengthens their ability to internalize visual content. To address this, we construct the MATHCOG dataset, which explicitly separates the watching (perception) and thinking (reasoning) stages. Furthermore, to support the different training phases of COGFLOW, we curate three tailored subsets: MATHCOG-SFT, MATHCOG-IntlzR, and MATHCOG-RL. B.1 DATASET LICENSE AND INTENDED USE The MATHCOG dataset will be released under the CC-BY-4.0 license, which permits redistribution and adaptation for both academic research and commercial use, provided that appropriate credit is given. The dataset is intended primarily for research on multimodal reasoning, visual perception, and mathematical problem solving. It is designed to facilitate studies on perceptionreasoning alignment in multimodal large language models. The dataset should not be used for purposes unrelated to research or education, including surveillance, profiling, or decision-making in sensitive domains such as healthcare or law enforcement. We encourage responsible use and proper citation in all derivative works. B.2 DATA CURATION B.2.1 DATA COLLECTION To construct MATHCOG, we first collect large pool of geometry-related visual math problems from existing corpora, including MAVIS (Zhang et al., 2025a), Geo170K (Gao et al., 2025), and LLaVACoT (Xu et al., 2025a). We then perform careful filtering process to obtain high-quality subset of 111,752 problems. Specifically, we remove (1) problems dominated by weak geometric relevance charts (e.g., histograms, pie charts) or artistic illustrations that deviate from geometric reasoning, and (2) diagrams with either excessively low resolution (< 64) or overly high resolution (> 2048), which would otherwise compromise model training consistency. Figure 10: Curation pipeline of MATHCOG. We label MATHCOG-SFT and MATHCOG-RL based on the raw data. To construct MATHCOG-IntlzR, we sample positive examples from MATHCOGSFT and generate five typical negative examples. Then, we adopt Softmax-DPO to train the IntlzR model."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Prompt configuration for refining the MATHCOG dataset. System: # Your Role: expert math geometry teacher ## Objective You will be provided with visual mathematics problem, along with its perception output (<WATCHING>) and raw solution (<THINKING>). Your task is to refine the solution such that the reasoning process explicitly internalizes the perceptual information before carrying out logical inference. ## Output Format The output format should strictly follow the example following: Question: XXX <WATCHING>: XXX <THINKING>: XXX Refined <THINKING>: XXX Now, here is the data you need to refine: Question: {question} <WATCHING>: {watching} <THINKING> (raw): {solution} Refined <THINKING>: B.2.2 MATHCOG-SFT AND MATHCOG-RL CURATION Visual Primitive Annotation. To construct perception-enhanced dataset, we developed preprocessing pipeline as illustrated in Figure 10. Since the raw dataset lacks sufficient perceptual information, we first curate additional geometric details from the original data. Formally, let denote the raw image. We apply OpenCV-based operators to extract set of primitive geometric elements: = Extract(I; θcv), (9) where θcv represents the parameters of the extraction process. We utilize the OpenCV library to detect pixel-coordinate-based visual primitives in the images from the collected data, including endpoints and intersections of line segments, as well as circular features. For circles, we directly extract center coordinates (x, y) and radius in pixel space. For points (e.g., vertices and character annotations), we similarly obtain precise pixel coordinates and establish optimal correspondences between detected visual elements and character labels via the Hungarian algorithm, thereby obtaining final mappings such as : (x1, y1) and : (x2, y2). To unify the representation, the extracted elements are normalized and remapped to fixed coordinate system, following the normalization scheme in prior work (Wei et al., 2024): ˆE = Normalize(E) 20 10, (10) where normalization is performed by dividing coordinates by the image width and height, and the subsequent linear transformation maps the results into the range [10, 10]. The complete pipeline can thus be summarized as: = Annotate(Normalize(Extract(I; θcv)) 20 10; H) . (11) This ensures that the final dataset not only captures essential geometric primitives but also provides high-quality, standardized representations suitable for subsequent training and evaluation, thus forming the perception part. Reasoning Annotation. Based on these visual primitives, we further construct reasoning trajectories from the solution rationales in the collected data. The goal is to (i) explicitly internalize the structured primitives into symbolic representation, and (ii) perform step-by-step logical inference over this internalized state to answer the question. These trajectories are first generated with the assistance of GPT-5 and are subsequently verified and refined by human annotators. The prompt is shown in Table 6. Finally, MATHCOG-RL is created by sampling 10,000 examples from this dataset, while MATHCOG-SFT is generated by sampling 100,000 examples. dedicated team of 31 professional annotators carried out this quality-assurance protocol over one-month period, ensuring high-fidelity and consistent ground-truth annotations."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: The illustration of data for Knowledge Internalization Reward. B.2.3 MATHCOG-INTLZR CURATION Building on MATHCOG-SFT, we construct MATHCOG-IntlzR as set of contrastive reasoning trajectories with explicit perturbations: We first sample 10,000 examples as positive samples from MATHCOG-SFT for MATHCOG-IntlzR. As mentioned earlier, we categorize the errors into five types: (1) omitting or misbinding primitives, (2) introducing nonexistent facts, (3) contradicting geometric constraints, (4) invoking external theorems inappropriately, and (5) referring inconsistently to established elements. The negative data for MATHCOG-IntlzR is constructed based on these five error types. The curation of MATHCOG-IntlzR is illustrated at the bottom of the Figure 10. We first use an LLM, such as GPT-5 (OpenAI, 2025), to generate negative trajectories conditioned on each positive trajectory by explicitly instructing it to modify the corresponding Thinking part, as specified in the prompt shown in Table 7. The generated candidates are then manually checked. For example, for the Type 1 error: Omit or misbind primitives in Figure 11, we input the designed prompt into the LLM and obtain the variant In any parallelogram, if the perimeter of triangle CDM is 5, . . . , which deliberately alters the relevant content in the Thinking part of the original positive example. Specifically, for every positive trajectory we generate five distinct negative counterparts, resulting in 10,000 positive and 50,000 negative trajectories for Softmax-DPO training. The easy/medium/hard labels characterize the relative difficulty for the model to detect and correct different types of reasoning drift errors, rather than the difficulty of constructing these cases. Easy errors are typically local and isolated (e.g., minor mistake in single visual primitive or reasoning step) and can often be corrected using limited contextual information. Medium errors involve multiple interacting primitives or steps and require the model to integrate information across broader context. Hard errors are globally entangled with the full reasoning chain, where correcting them demands coherent understanding of both the visual configuration and the multi-step logical structure of the solution. B.3 DATASET STATISTICS Table 8 provides the statistics for the MATHCOG dataset, detailing the number of problems and the distribution of data across different categories. The dataset consists of total of 121,730 problems, with 100,000 labeled for supervised fine-tuning (MATHCOG-SFT) and 10,000 labeled for reinforcement learning (MATHCOG-RL). Additionally, there are 10,000 positive samples and 50,000 negative samples used in the MATHCOG-IntlzR (Visual-Augmented Reward) subset. validation set contains 1,730 examples."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Prompt for error type generation in MATHCOG-IntlzR. Omit or Misbind Primitives: Starting from the correct reasoning, deliberately omit or misbind at least one basic geometric primitive (points, lines, circles, etc.), while keeping the overall reasoning fluent and locally plausible. Positive Data: {positive data} Introduce Nonexistent Facts: Starting from the correct reasoning, introduce at least one geometric or numerical fact that is not supported by the given figure or problem statement, but make the reasoning appear locally coherent and natural. Positive Data: {positive data} Contradict Geometric Constraints: Starting from the correct reasoning, modify the chain of thought so that at least one step violates the true geometric constraints (e.g., equal lengths, parallelism, angle measures), while preserving seemingly reasonable narrative. Positive Data: {positive data} Invoke External Theorems Inappropriately: Starting from the correct reasoning, inappropriately invoke at least one external theorem or formula whose preconditions are not satisfied, or whose use is not justified by the internalized structure, while keeping the explanation linguistically smooth. Positive Data: {positive data} Refer Inconsistently to Established Elements: Starting from the correct reasoning, alter the chain of thought so that references to previously established elements (points, lines, relationships) become inconsistent, such as swapping labels or changing properties across steps, but without breaking the overall fluency of the text. Positive Data: {positive data} Table 8: Statistics of MATHCOG. Statistic Total Problem - Number of MATHCOG-SFT - Number of MATHCOG-RL - Number of positive data of MATHCOG-IntlzR - Number of negative data of MATHCOG-IntlzR - Number of data for validation Maximum question length Maximum watching length Maximum thinking length Average question length Average watching length Average thinking length Number 121,730 100,000 10,000 10,000 50,000 780 1,322 1,288 186 463.03 527.79 Figure 12 shows the length of the various components. The maximum question length is 780, while the maximum lengths for the watching and thinking components are 1,322 and 1,288, respectively. On average, the question length is 186, with the watching and thinking components averaging 463.03 and 527.79, respectively. B.4 DATASET EXAMPLES Figure 13 illustrates three representative samples from the MATHCOG dataset (covering both SFT and RL subsets), explicitly demonstrating our unique data structure that disentangles perception from reasoning. Each sample is organized into four distinct components: the raw visual input (<IMAGE>), the problem statement (<PROBLEM>), the perception trajectory (<WATCHING>), the reasoning trajectory (<THINKING>), and the final answer (<ANSWER>). Specifically, the perception trajectory translates raw diagrammatic pixels into structured geometric primitives (Points, Lines, and Circles) with precise coordinates in visual parameterized space. For instance, in the leftmost example involving rectangle, the model identifies specific coordinates for vertices and (e.g., : (7.56, 3.5)) and the intersection point O. Similarly, in the center and"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Distribution of Word Length among QUESTION, WATCHING, THINKING. We present the distribution of word length, with the horizontal axis representing word length and the vertical axis depicting the corresponding probability distribution. Figure 13: Representative cases in the MATHCOG-SFT and MATHCOG-RL. right examples, the model explicitly parameterizes circles by identifying center coordinates and radii (e.g., Circle: (2.52, 0.48, 7.84)). The reasoning trajectory section then performs logical inference anchored in these extracted visual cues. For example, in the rightmost case, the reasoning process explicitly links the perceived fact that AB is the diameter (derived from the coordinates in <WATCHING>) to the geometric theorem that the angle subtended by diameter is 90, leading to the correct deduction that ACB = 905. This structured format ensures that the final <ANSWER> is derived from cognitive flow of perception internalization reasoning."
        },
        {
            "title": "C ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "C.1 MODEL INITIALIZATION As shown in Table 9, we initialize COGFLOW with 7B-scale pretrained MLLM backbone, Qwen2.5-VL-7B (Bai et al., 2025). All parameters of the vision encoder, perception modules, and language decoder are updated during training without freezing any component. C.2 THE DETAILS OF VPR The Visual Parameterized Reward (VPR) metric quantifies perceptual accuracy within the parameter space of geometric primitives. As depicted in Figure 3, VPR first translates structured visual information into parametric representations. Critically, such structured inputs are constrained to three primitive types: points, lines, and circles, which constitute the foundational elements of geometric constructions Trinh et al. (2024). For lines, parametric representations are derived by fitting straightline models to the input data; for circles, parameters are extracted directly as center coordinates and radii. Subsequently, an optimal one-to-one correspondence between predicted and ground-truth primitives is established via multi-class Hungarian algorithm. This matching minimizes the total assignment cost SVPR. The complete computational procedure is formalized in Algorithm 1. C.3 THE DETAILS OF INTLZR TRAINING As shown in Table 10, the Knowledge Internalization Reward (IntlzR) model is trained on the MATHCOG-IntlzR subset containing 10k pairs (one positive and five negatives per pair). We initialize the reward model with Qwen2.5-VL-3B (Bai et al., 2025) using the HuggingFace Transformers library, and train it for 3 epochs with batch size of 64 and learning rate of 7 106. It is worth noting that we adopt Softmax-DPO for training, with the objective function defined in Eq. 2, where the temperature parameter β is fixed to 1. IntlzR is implemented as reward model trained using Softmax-DPO on contrastive trajectory pairs generated in the MATHCOG-IntlzR subset. For each positive trajectory, we use an LLM to generate five negative trajectories by injecting one of the five structured reasoning-error types into the Thinking part, and all synthetic negatives are manually verified. Each trajectory is encoded into hidden representation h, and IntlzR learns scalar score RIntlzR(h) [0, 1] that reflects its consistency with the visual evidence. During training, IntlzR is optimized with the Softmax-DPO objective so that positive trajectories receive higher scores than their corresponding negative trajectories. C.4 THE DETAILS OF INFR Accuracy Reward. The accuracy reward evaluates whether the response is correct. Following Shao et al. (2024), we assess correctness solely based on the answer field: the reward is 1 if the answer is correct and 0 otherwise. IntlzR configuraTable 11: RL configurations. Table 9: SFT configurations. Setting Config epochs batch size base learning rate optimizer LR scheduler weight decay gradient clipping memory optimization ZeRO-2 1 64 1 105 AdamW Cosine 0.01 1.0 Table 10: tions. Config Setting epochs batch size Base learning rate optimizer β memory optimization ZeRO3 64 7 106 AdamW 1 24 Config Setting epochs batch size base learning rate trajectory sampling rollout generation precision gradient accumulation KL penalty coefficient memory optimization 1 16 1 106 8 vLLM bfloat16 enabled 0.001 ZeRO-"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: Multi-class Matching for VPR Input: Ground truth primitives = {G1, G2, . . . , Gm} with class labels {point, line, circle}, Predicted primitives = {P1, P2, . . . , Pn} with class labels {point, line, circle} Output: Optimal matching P, Total VPR cost SVPR = (cid:80) ; {point, line, circle} (i,j)H Pj Gi2 SVPR 0 // Primitive classes foreach class do Gk {Gi class(Gi) = k} Pk {Pj class(Pj) = k} if Gk > 0 and Pk > 0 then // Construct cost matrix Ck RGkPk for 1 to Gk do // GT primitives of class // Predicted primitives of class for 1 to Pk do Ck[i, j] ϕk(G(i) space ) ϕk(P (j) )2 // L2 distance in parameter end end // Apply Hungarian algorithm for optimal assignment Hk HUNGARIAN(Ck) Sk (cid:80) Ck[i, j] Hk SVPR SVPR + Sk // Returns matching pairs Hk Gk Pk // Class-specific cost (cid:80) Pj Gi (i,j)Hk end else if Gk > 0 then SVPR SVPR + λFN Gk end else if Pk > 0 then SVPR SVPR + λFP Pk end end return H, SVPR // Penalty for missed GT // Penalty for false positives Format Reward. The format reward verifies whether the response adheres to the required output schema: the model must produce JSON-style response, i.e., <WATCHING> ... </WATCHING> <THINKING> ... </THINKING> <ANSWER> ... </ANSWER>. It returns 1 if the response is compliant and 0 otherwise. Finally, we have the Inference Reward RInf R: RInf = RAcc + RF mt. (12) C.5 DETAILS OF SFT AND RL TRAINING For supervised fine-tuning (SFT), we utilize the MATHCOG-SFT subset, which consists of 100,000 curated samples. Both the MATHCOG-SFT and MATHCOG-RL splits are organized into three components<WATCHING>, <THINKING> and <ANSWER>. In our training pipeline, we first perform supervised fine-tuning (SFT) on the MATHCOG-SFT, where the model is trained on the Watching sequences together with the corresponding <THINKING> and<ANSWER> to strengthen its basic perceptual and reasoning abilities. The model is optimized for 1 epoch with batch size of 64 and learning rate of 1 105. We adopt AdamW as the optimizer, using cosine learning rate scheduler, weight decay of 0.01, and gradient clipping at 1.0 to ensure stable updates. For RL, the training is conducted on the MATHCOG-RL subset (10k samples) for 1 epoch, with learning rate of 1 106 and batch size of 16. For each input, we sample 8 candidate trajectories"
        },
        {
            "title": "Preprint",
            "content": "using temperature-controlled decoding. The rewards are computed by combining Synergistic Visual Rewards (SynVRs), Knowledge Internalization Reward (IntlzR), and Inference Reward (InfR), with their weights fixed to (1, 1, 1). We implement reinforcement learning under the VERL (Sheng et al., 2025) framework on 16 NVIDIA A100 GPUs, with distributed acceleration provided by vLLM. Mixed-precision training is conducted in bfloat16, and gradient accumulation is employed to simulate larger effective batch sizes. We adopt DeepSpeed ZeRO-2 for memory-efficient optimization, and the KL penalty coefficient is set to 0.001 to stabilize policy updates. C.6 DETAILS OF THE REASONING DRIFT PRECISION METRIC As illustrated in Figure 1, we assess the precision of reasoning drift: whether each reasoning step is faithfully grounded in the perceived structure. Concretely, generated solution is decomposed into steps {st}T t=1. From each step, we extract the referenced visual cues (primitives and relations) Rt by GPT-5 OpenAI (2025), and from the perception parse yw we obtain the set of perceived elements P. step is deemed grounded if its visual claims are entailed by the perception, formalized by consistency predicate, i.e., D(st) = 1. The precision of reasoning drift detection for response is given by Prec = 1 (cid:88) t=1 1(D(st) = 1), (13) which measures the fraction of flagged steps that are truly ungrounded; higher values indicate fewer false positives and better alignment between perception and reasoning."
        },
        {
            "title": "D ADDITIONAL ANALYSIS",
            "content": "D.1 ANALYSIS ON THE OVERALL EFFECTIVENESS OF COGFLOW Figure 14 compares the performance of the base model (Qwen2.5-VL-7B), the model after SFT, and our full framework (COGFLOW). The base model achieves 46.30/40.10 on CoT-E/Acc, while SFT brings moderate improvements to 50.70/42.90 (+4.40/+2.80; approximately +9.5%/+7.0%). Building upon this, COGFLOW, trained with visual rewards and reinforcement learning, achieves substantial gains of 66.01/56.22, outperforming SFT by +15.31/+13.32 (around +30.2%/+31.0%) and the base model by +42.6%/+40.2%. Notably, CoT-E consistently surpasses Acc across all settings, with the largest margin observed in COGFLOW (9.79 points), indicating that our approach not only improves final answer accuracy but also significantly enhances the reliability and coherence of intermediate reasoning chains. Finally, as shown in Table 12, COGFLOW attains an overall score of 76.8 (ALL), exceeding recent 7B-class multimodal reasoners and surpassing several general-purpose systems reported here (e.g., GPT-4o 63.8, Claude-3.5-Sonnet 67.7, Gemini-2.0-Flash 73.4). While Doubao-pro-1.5 reaches higher ALL (79.5), COGFLOW delivers the strongest category scores. The pronounced gains on geometry-centric GPS and text-heavy TQA align with our design: SynVRs stabilize fine-grained perceptual parsing, IntlzR enforces faithful internalization of visual cues, and VGPO optimizes reasoning under multi-signal feedback. Collectively, these components improve not only answer accuracy but also the consistency of see correctly internalize faithfully reason coherently across MathVistas diverse task types. D.2 MORE ANALYSIS ON SYNERGISTIC VISUAL REWARDS We assess perception fidelity by comparing the models structured output with the ground-truth primitives. Let pred and gt denote the predicted and gold sets of primitives (points/lines/circles). An LLM-based matcher sequentially aligns predicted elements to gt (typeand parameterconsistent), yielding match set M. We compute Precision = E pred , Recall = E gt , F1 = 2 Precision Recall Precision + Recall . (14) Figure 15 shows the F1 distribution under three settings (w/o VSR, w/o VPR, w/ VSR): removing either reward lowers perception fidelity, while enabling VSR produces the right-most, highestcentered distribution, indicating more primitives are correctly grounded; VPR provides complementary but weaker regularization. Figure 14: Effectiveness of COGFLOW. We compare the base model (Qwen2.5-VL7B) and its SFT-Model against COGFLOW. Figure 15: Perception F1 of SynVRs variants. The W/SynVRs achieves the best performance."
        },
        {
            "title": "Preprint",
            "content": "Table 12: Accuracy (%) results on MathVista dataset. Method Gemini-1.0-Pro (Team et al., 2025) GPT-4V (OpenAI, 2023) GPT-4o (OpenAI, 2024) Claude 3.5 Sonnet (Anthropic, 2024) Gemini-2.0-Flash (Team et al., 2025) Doubao-pro-1.5 (Seed et al., 2025) LLaVA-1.5-7B (Liu et al., 2024b) LLaMA-Adapter-V2-7B (Gao et al., 2023) TinyLLaVA-3B (Zhou et al., 2024) LLaVA-1.5-13B (Liu et al., 2024b) mPLUG-Owl2-7B (Ye et al., 2024) MiniGPT-v2-7B (Chen et al., 2023a) G-LLaVA-7B (Gao et al., 2025) VCAR (Jia et al., 2024) SPHINX-Plus (Liu et al., 2024a) SVE-Math-7B (Zhang et al., 2025b) MultiMath-7B (Peng et al., 2024a) X-REASONER (Liu et al., 2025) VL-Rethinker-7B (Wang et al., 2025a) ReVisual-R1 (Chen et al., 2025c) WeThink (Yang et al., 2025) Skywork-R1V-38B (Wang et al., 2025c) COGFLOW-7B All 47.7 49.9 63.8 67.7 73.4 79. 20.0 23.9 26.7 26.8 22.2 23.1 25.1 33.7 36.7 37.4 50.0 69.0 73.7 73.1 70.9 60.6 76.8 FQA GPS MWP TQA VQA 48.3 43.1 - - - 77.7 22.7 21.2 20.4 19.7 22.7 18.6 19.1 30.9 54.6 31.9 40.1 - - - - - 70.4 35.1 50.5 - - - 88.9 7.7 25.5 19.7 20.2 23.6 26.0 48.7 34.6 16.4 53.9 66.8 - - - - - 93.1 50.5 57.5 - - - 86.0 11.8 11.3 23.1 18.8 10.2 13.4 3.6 38.7 23.1 29.0 61.8 - - - - - 73. 65.8 65.2 - - - 82.3 26.6 32.3 44.9 45.6 27.2 30.4 25.0 37.3 41.8 41.4 50.0 - - - - - 86.9 42.5 38.0 - - - 62.0 33.0 31.8 31.8 36.9 27.9 30.2 28.7 28.5 43.0 30.8 33.0 - - - - - 59.3 Table 13: Ablation analysis of visual rewards and RL strategies. We compare GRPO with PPO and DAPO under consistent reward settings. Method Configuration FlowVerse CoT-E(%) FlowVerse Acc (%) DAPO Yu et al. (2025) PPO Yu et al. (2022) GRPO Shao et al. (2024) VGPO (ours) w/o SynVRs, w/o IntlzR SynVRs, w/o IntlzR SynVRs, IntlzR w/o SynVRs, w/o IntlzR SynVRs, w/o IntlzR SynVRs, IntlzR w/o SynVRs, w/o IntlzR SynVRs, w/o IntlzR SynVRs, IntlzR w/o SynVRs, w/o IntlzR SynVRs, w/o IntlzR SynVRs, IntlzR 56.1 59.7 62.8 55.9 58.2 61. 56.7 61.4 64.4 57.4 63.2 66.0 45.8 50.7 54.1 44.6 48.3 53.9 47.6 52.9 55.1 48.7 54.7 56. Training Time (h) 4.1 4.6 4.6 3.2 3.7 3.7 4.2 4.7 4.8 4.6 5.3 6.1 D.3 THE IMPACT AND EFFICIENCY OF THE VISION GATE The Impact of the Visual Gate in the Training Phase. To assess how the visual gate improves perceptual quality during VGPO training, we analyze the visual-reward scores produced by its conditional re-generation procedure. Figure 16 reports three visual rewards score distributions, each computed over the full training set but corresponding to different re-generation under visual gate with the threshold τ : (i) the first-attempt scores for all instances; (ii) the post-gating scores after one re-generation round, where only instances with Svis < τ are re-generated and use their secondattempt scores while the rest keep their first-attempt scores; and (iii) the post-gating scores after second re-generation round, where instances still below τ after the second attempt are re-generated again and use their third-attempt scores, while all others keep the earliest score at which they pass the gate. We visualize these distributions together with the threshold τ and compute the corresponding pass rates as the fraction of examples whose score exceeds τ at each stage. The results show"
        },
        {
            "title": "Preprint",
            "content": "Table 14: Analysis of Inference Schemes. We evaluate COGFLOW and the VLM-R1 baseline under these three settings from the FlowVerse with = 3. Model FlowVerse CoT-E Acc Inference Time (h / 1000 samples) 56.17 47.63 VLM-R1-7B (single-pass) 59.45 49.59 VLM-R1-7B (best-of-3 full) 64.51 55.22 COGFLOW-7B (single-pass) COGFLOW-7B (best-of-3 full) 66.04 56.17 COGFLOW-7B (visual gate) 66.03 56.24 2.77 5.54 2.72 5.52 3. Figure 16: The distribution of visual reward scores during training. Attempts with scores above the Threshold are accepted. that the score mass shifts upward and becomes increasingly concentrated above τ , indicating that the visual gate acts as an effective perceptual quality-control mechanism that improves perceptual grounding rather than arbitrarily resampling trajectories. Meanwhile, as evidenced by Table 13, enabling VGPO with the visual gate incurs only modest overhead of +1.3 training hours, which is acceptable for standard MLLM post-training. The Impact of the Visual Gate in the Inference Phase. Moreover, we add additional experiments to further analyze the impact of the visual gate at inference. Specifically, we compare three inference schemes: Single-pass: The model generates full trajectory (<WATCHING>, <THINKING>, <ANSWER>), which is identical to standard MLLM inference. Best-of-k full: The model generates full trajectories and selects the one whose final answer matches the ground truth. We note that this is not realistic inference strategy, as it relies on access to ground-truth answers. Visual gate (best-of-k perception): COGFLOW samples alternative perception trajectories, selects the one with the highest visual score, and then performs single reasoning pass conditioned on the selected perception. As shown in Table 14, to enable fair and controlled comparison, we evaluate COGFLOW and an open-source R1-style baseline (VLM-R1) under all three inference schemes with fixed = 3. We additionally report the average time required to process 1,000 examples (computed by selecting 1,000 samples and averaging the end-to-end inference time) for each setting. The results show that COGFLOW outperforms VLM-R1 under both the single-pass and best-of-3 full settings, indicating that the observed gains are not attributable to more aggressive sampling. Moreover, the visual gate (best-of-3 perception) achieves nearly the same accuracy as best-of-3 full responses while requiring substantially less inference time (2.72 vs. 3.06 vs. 5.52 h). Overall, these results support that our comparison is fair, and that COGFLOWs visual-gated inference provides more favorable computationperformance trade-off than naive best-of-k sampling over full responses. The Impact of in Visual Gate Finally, we ablate the perception sampling number used by the visual gate at both training time and inference time. Table 15 varies the training-time while keeping the inference-time gate fixed to = 3, whereas Table 16 varies the inference-time with the training-time gate fixed to = 3. Across both ablations, = 3 consistently yields favorable performancecomputation trade-off, achieving most of the attainable accuracy gains without incurring excessive additional cost. Accordingly, we adopt = 3 as the default setting for both stages. D.4 ANALYSIS ON VISUAL-GATED POLICY OPTIMIZATION Table 13 presents an ablation analysis across different reinforcement learning algorithms (DAPO (Yu et al., 2025), PPO (Schulman et al., 2017), GRPO (Shao et al., 2024), and our VGPO) under consis-"
        },
        {
            "title": "Preprint",
            "content": "Table 15: Analysis of perception sample number in the visual gate during training. Inference-time visual gate fixed to = 3. Table 16: Analysis of perception sample number in the visual gate during inference. Training-time visual gate fixed to = 3. 1 3 5 FlowVerse CoT-E Acc 64.40 66.03 66.60 55.13 56.24 56.52 Avg Training Time (h) 4.85 6.17 7.91 1 3 5 FlowVerse CoT-E Acc Avg Inference Time (h / 1000 samples) 64.51 66.03 66. 55.22 56.24 56.55 2.72 3.06 3.71 Figure 17: Scalability and broader applicability of COGFLOW. tent reward configurations based on the MATHCOG. Several clear trends emerge. First, the inclusion of visual perceptual reward (VPR) and Knowledge Internalization Reward (IntlzR) consistently improves performance across all methods, confirming their complementary roles in strengthening visual grounding. Second, GRPO outperforms both PPO and DAPO in all settings, highlighting its stability and effectiveness in geometry-oriented reasoning tasks. Most importantly, our proposed VGPO substantially surpasses all baselines, achieving 66.0%/56.2% accuracy on FlowVerse and MathVerse, respectively. This represents an improvement of over +6 points compared to GRPO with VPR and IntlzR, demonstrating the advantage of explicitly disentangling perception and reasoning optimization. Although VGPO incurs slightly higher training costs, the performance gain justifies the trade-off. D.5 CLARIFYING THE COGNITIVE-SCIENCE ANALOGY First, the watching stage in COGFLOW is not intended to correspond to internalization. Its function is limited to extracting low-level structured visual information from an imagesuch as points, lines, circles, spatial groupings, and relative configurations. These representations are still purely perceptual: they serve as normalized, noise-reduced description of the raw visual scene, but they do not yet constitute conceptual understanding or task-oriented reasoning. The subsequent internalization stage is distinct and is designed precisely to bridge perception and reasoning. It transforms the structured primitives produced in the watching stage into conceptually meaningful representation, integrating visual evidence with symbolic relations and problem-specific abstractions. Internalization therefore plays the role of aligning perceptual inputs with the evolving reasoning state. For instance: Type 1 (Omit or misbind primitives) errors stem from incorrect or incomplete internalization of otherwise correctly perceived primitives. They reflect failures in mapping the structured perceptual tokens into coherent conceptual statehence they lie at the interface between perception and reasoning. Type 4 (Invoke external theorems inappropriately) and Type 5 (Refer inconsistently to established elements) arise when the internalized conceptual representation is misinterpreted, misapplied, or inconsistently used within multi-step reasoning. These error types are characteristic of reasoning drift under an inadequately grounded internal state. D.6 DEEPER INSIGHTS INTO THE MECHANISM OF MULTI-STAGE REWARD STRUCTURE On the one hand, each reward is designed to target distinct failure mode in visual mathematical reasoning: Firstly, the Synergistic Visual Rewards (SynVRs), which comprise VPR and VSR, are designed to address perceptual failures. Figure 5 presents an ablation of the SynVRs components. The results show that adding either VPR or VSR alone consistently improves performance, and the best results are obtained when both are enabled. Figure 15 further reports the perception F1 scores"
        },
        {
            "title": "Preprint",
            "content": "for different SynVRs variants, indicating that removing either reward degrades perception fidelity. Finally, the error-type analysis in Figure 9 demonstrates that SynVRs effectively reduce perceptionrelated errors, thereby validating their effectiveness. Taken together, these results show that VPR and VSR provide complementary supervision signals: VPR enforces local geometric fidelity, while VSR encourages global perceptual coherence in terms of overall style and layout, and together they form trustworthy visual cues that serve as robust foundation for effective visual mathematical reasoning, which is consistent with the conclusions reported in prior studies (Guo et al., 2025c; Chen et al., 2025a). Secondly, the Knowledge Internalization Reward (IntlzR) is designed to alleviate reasoning drift by bridging the perception and reasoning stages, encouraging the model to produce structured, reasoning-ready outputs (i.e., knowledge-internalized representations (Ryan & Connell, 1989)) that provide more reliable foundation for subsequent reasoning. The ablation study on IntlzR (Figure 6) shows that removing any single error type consistently degrades performance, indicating that each error type provides complementary supervision. The largest drops occur when excluding the omission/misbinding primitives or contradicting geometric constraints error types, highlighting that correctly binding primitives and respecting core geometric constraints are most critical for keeping reasoning tied to perception. In addition, the error-type analysis in Figure 9 further demonstrates that IntlzR effectively reduces Knowledge Internalization Errors, which in turn leads to improved overall performance. Thirdly, the Inference Reward (InfR) ensures task-level correctness and proper output structure at the final reasoning stage (Shao et al., 2024). On the other hand, this multi-stage reward structure provides principled solution to the creditassignment problem: instead of relying on single sparse task-level reward, the stage-wise signals supply informative gradients at different points of the perceptioninternalizationreasoning pipeline. The ablation on COGFLOWs components  (Table 5)  shows that both SynVRs and IntlzR are essential for achieving strong performance, and that the best results are obtained when all rewards are enabled. In addition, the reasoning drift analysis across three representative pipelines (Figure 1b) demonstrates that COGFLOWs reasoning trajectories are more stable and better aligned with the visual information, which is consistent with the observed performance gains. Moreover, the errortype analysis in Figure 9 shows that, compared with the Baseline+VGPO setting, COGFLOW substantially reduces Perception Errors, knowledge-internalization errors, and Reasoning Errors. This indicates that the three rewards together form coherent cognitive paradigm: improved perception supports more reliable internalization, and IntlzR further aligns the reasoning trajectory with the visual evidence D.7 SIZES AND ARCHITECTURES ABLATION. To demonstrate the scalability and broader applicability of COGFLOW, we now explicitly evaluate COGFLOW across multiple backbone architectures and parameter scales, rather than only on Qwen2.5-VL-7B (see Figure 17). Specifically, we report results for Qwen2.5-VL (3B and 7B) and InternVL3.5 (2B, 4B, and 8B). The results show that: (1) COGFLOW consistently outperforms models of comparable size, and even with only 2B or 4B parameters it achieves competitive performance; (2) the performance gains are maintained or even amplified as model capacity increases; (3) the improvements hold for both the Qwen and InternVL families, indicating that the framework is not tied to specific architecture. These findings provide empirical evidence that COGFLOW is scalable and broadly applicable beyond single model configuration. D.8 SOURCE OF PERFORMANCE IMPROVEMENT Since MATHCOG is explicitly designed to support the training of COGFLOW, the method and the dataset are inherently coupled and cannot be treated as fully independent components. Nevertheless, for completeness, we conduct controlled analysis to further investigate to what extent the observed performance gains arise from the dataset itself versus from the proposed training framework. We conducted an ablation study in the Table 13 where we fix the training data to MATHCOG and vary only the post-training algorithm. Starting from the same base model, we compare standard PPO, DAPO and our proposed VGPO. The results show that all methods benefit from training on MATHCOG, indicating that VGPO consistently achieves the best performance among all posttraining methods under the same data. This suggests that the performance improvements cannot be attributed to the dataset alone, and that the COGFLOW framework and its multi-stage optimization play central role in the observed gain."
        },
        {
            "title": "Preprint",
            "content": "Table 17: Comparison of pre-trained models on FlowVerse. Results show that using FG-CLIP with ViT-L-14 backbone yields the best performance. MetaCLIP2 FG-CLIP ViT-B-16 ViT-L-14 ViT-BViT-L-14 CoT-E Acc CoT-E Acc CoT-E Acc CoT-E Acc 62.5 53.9 63.2 54. 63.3 54.6 63.8 55.3 Figure 18: Effect of varying α on FlowVerse performance. D.9 ABLATIONS ON INDIVIDUAL SYNVRS COMPONENTS We note that the Figure 5 already included ablations of each SynVRs component. However, we did not previously investigate how different mixing ratios between VPR and VSR affect performance. We therefore conduct an ablation study on the FlowVerse dataset (see Figure 18) by varying the weighting coefficient α while keeping IntlzR and the visual gate enabled. The results show that performance peaks at α = 0.6, indicating that balancing VPR and VSR yields the most effective supervisory signal. Moreover, the fact that α = 0 underperforms α = 1 suggests that parameterlevel supervision (VPR) contributes more substantially to perceptual fidelity than semantic similarity alone (VSR). D.10 EVALUATING FG-CLIP FOR SIMILARITY REWARD We further evaluate the FG-CLIP similarity module by replacing it with several alternative pretrained models (e.g., MetaCLIP2 (Chuang et al., 2025)) of comparable capacity. Across these variants, the FG-CLIP (ViT-L-14) configuration consistently achieves the best overall performance on FlowVerse, indicating that FG-CLIP provides stronger and more discriminative supervision signal for our VSR component than the alternative encoders D.11 FURTHER ANALYSIS OF ERROR TYPES From Figure 9, we observe that the Baseline+SynVRs setting leads to only modest reduction in perception-related errors, but more substantial reduction in reasoning-related errors (see also Figure 9 in the original submission). We explain this phenomenon as follows. First, we view this as natural consequence of the fact that perception, internalization, and reasoning are not independent modules: improvements in perception inevitably propagate to, and influence, the subsequent reasoning process. We conduct systematic case study to analyze the error compensation mechanism. In the illustrated example, the perception stage exhibits partial errors: the coordinates of point are misidentified, which in turn corrupts lines AB and AD. Nevertheless, during the subsequent reasoning stage, the model still produces the correct answer by accurately perceiving other visual primitives (e.g., points and C) and performing precise internalization. This analysis reveals cognitive processing pattern in which response generation proceeds sequentially through perception internalization reasoning, and suggests that strengthened perception and internalization capabilities can mitigate Reasoning Errors even in the presence of localized perception failures. In addition, among the Baseline+SynVRs, Baseline+IntlzR, and Baseline+VGPO configurations, Baseline+SynVRs already yields the largest reduction in Perception Errors, further demonstrating that SynVRs provide effective improvements to the perceptual component. D.12 CASE STUDIES As shown in Figure 19, consider circle with center and points A, B, on the circle, where ABO = 50. typical failure mode is primitive misbinding: one baseline incorrectly treats the given condition as AOB = 50 and then applies the inscribed-angle theorem to obtain ACB = 25. Another baseline perceives the condition correctly but makes an invalid geometric inference by asserting AOB = 2ABO, which yields ACB = 50. In contrast, with internalized primitives and visual rewards (VPR/IntlzR), our model preserves consistent grounding. Since OA = OB,"
        },
        {
            "title": "Preprint",
            "content": "Figure 19: An example of the solution generated by COGFLOW. Figure 20: Example solution generated by COGFLOW under partially erroneous perception. OAB is isosceles, so OAB = ABO = 50. It follows that AOB = 1802OAB = 80, and by the inscribed-angle theorem ACB = 1 AOB = 40, which is correct. This example 2 illustrates that disentangling perception from reasoning and supervising both stages with VPR/IntlzR mitigates primitive misbinding and discourages spurious theorem application. D.13 ANALYSIS OF ERROR COMPENSATION MECHANISM In this instance, perception contains partial errors: As shown in Figure 20, we conduct systematic case study to analyze the error compensation the coordinates of point are mechanism. misidentified, consequently corrupting lines AB and AD. Crucially, during subsequent reasoning, the model produces correct answers by accurately perceiving other visual primitives (e.g., points and C) and performing precise knowledge internalization. Our analysis reveals cognitive paradigm where response generation progresses sequentially through perceptioninternalizationreasoning. This demonstrates that enhanced perception and internalization capabilities can mitigate Reasoning Errors even when partial perception failures occur."
        }
    ],
    "affiliations": [
        "Intelligent Learning",
        "Sichuan University",
        "Tsinghua University",
        "Zhejiang University"
    ]
}