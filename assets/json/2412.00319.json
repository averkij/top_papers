{
    "paper_title": "Improving speaker verification robustness with synthetic emotional utterances",
    "authors": [
        "Nikhil Kumar Koditala",
        "Chelsea Jui-Ting Ju",
        "Ruirui Li",
        "Minho Jin",
        "Aman Chadha",
        "Andreas Stolcke"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states. To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative."
        },
        {
            "title": "Start",
            "content": "Nikhil Kumar Koditala Chelsea Jui-Ting Ju Ruirui Li Minho Jin Aman Chadha Andreas Stolcke Amazon Business, USA Amazon Alexa AI, USA Amazon GenAI, USA Amazon Search Experience Science, USA Amazon Web Services, USA 4 2 0 2 0 3 ] . [ 1 9 1 3 0 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "A speaker verification (SV) system offers an authentication service designed to confirm whether given speech sample originates from specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. noteworthy challenge faced by SV systems is their ability to perform consistently across range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to errors in downstream applications, such as authentication systems or emotion recognition applications. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states. To address this concern, we propose novel approach employing the CycleGAN framework to serve as data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving their unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing EER by as much as 3.64% relative. 1. Introduction Speaker verification (SV) systems are designed to discern whether given audio stream belongs to the speaker who has been previously enrolled, based on various acoustic characteristics of the speaker such as pitch, tone and intonation [1]. SV system typically consists of two phases an enrollment phase and verification phase. During the enrollment phase, the system extracts relevant features from enrolled speakers speech sample, whereas the verification phase employs statistical or machine learning models to compute distance metric between the current and previously enrolled speech samples [2]. The methodologies used for SV modeling have evolved over time. Earlier, techniques such as gaussain mixture models [3] and support vector machines [4] are prominent, which are later replaced by neural network based models [5, 6]. More details on our LSTM-based SV model are presented in Section 2.1. SV models can further be classified into text-dependent and text-independent systems [7]. Text-dependent models aim to verify the speaker based on predefined text, whereas textindependent models are independent of prior textual knowledge. SV systems acts as key component in various applications such as access control systems, Information structuring, telephonic banking and other voice based authentication systems [2] SV models can be used in conjunction with emotion prediction models to recognize particular speech segments and offer tone-related features to individual speakers. These applications encompass scenarios like recognizing individuals from emotional speech during criminal investigations [8], providing personalized support to individuals experiencing panic or frustration at helpline center [9], and in wearable emotion recognition devices to monitor psychological health [10]. The advent of SV systems has been cornerstone in the field of biometric security, offering seamless method for authenticating individuals in myriad of settings, from personal devices to secure access control systems. However, the nuanced nature of human speech, particularly when modulated by emotions, presents formidable challenge that current SV systems are ill-equipped to handle effectively. The variability introduced by emotional states in speech patterns can dramatically affect the accuracy of these systems, leading to potential security vulnerabilities and decreased user satisfaction. In real-world scenarios, speech is rarely devoid of emotional content. Individuals may express joy, frustration, or stress during interactions with voice-activated systems, leading to discrepancies between the enrolled neutral voice samples and the emotionally charged utterances during verification attempts. This discrepancy is not merely technical challenge but also barrier to accessibility and user trust in voice-based authentication technologies. The ability of an SV system to adapt to the emotional variability of speech is not just feature but necessity for ensuring the systems reliability and inclusivity. Moreover, the surge in remote work and digital communications has underscored the importance of robust and adaptable security measures. As voice-based platforms become increasingly integrated into our daily lives, the demand for SV systems that can navigate the complex landscape of human emotions has never been higher. The imperative to address this gap is further amplified by the ethical considerations of fairness and equity in technology deployment. Ensuring that SV systems are insensitive to emotional expressions is critical in preventing unintentional biases and promoting equal access across diverse user groups. Despite the clear need, the development of SV systems that can effectively process emotional speech is hampered by the scarcity of labeled emotional speech data. This limitation not only restricts the training of more adaptable models but also stifles innovation in creating more human-centric authentication solutions. The pursuit of overcoming these challenges through novel data augmentation techniques, such as the proposed CycleGAN framework, represents pivotal step towards realizing SV systems that are both secure and empathetic to the human condition. The integration of emotional intelligence into speaker verTable 1: Data distribution for training and evaluation of SV models. Speech is collected naturally from day-to-day activities, and can thus be proxy for the general emotion distribution. Dataset No. of Speakers No. of Utterances Neutral Calm Angry Happy Sad Training Evaluation 413 204 132128 55909 118948 (90%) 50860 (91%) 3487 (2.6%) 1194 (2.1%) 751 (0.6%) 494 (0.9%) 1344 (1%) 591 (1.1%) 7598 (5.7%) 2270 (4.1%) ification systems is not just an enhancement; it is critical evolution required to bridge the gap between human speechs dynamic nature and the static models that currently define SV technologies. By addressing the emotional variability in speech, we can unlock new dimensions of security, accessibility, and user experience that align with the nuanced complexities of human communication. Speakers may intentionally or unintentionally modulate their emotional state during verification, influenced by factors such as stress, urgency, or other physiological factors which may not be present during enrollment [11]. Notably, SV systems often suffer performance degradation when non-neutral tone is present in the speech [12]. Verification of speakers from emotional utterances is crucial for ensuring fairness and equity in SV systems. Failure to account for emotional variability can lead to inaccurate verification, potentially discriminating against individuals with diverse emotional expressions [13]. Gender related differences also affect the emotional state of speech, since it has been shown [14] that women experience and perceive emotions more extremely than men. SV on emotional speech can reduce these biases, and ensure equitable access to SV technologies for all users, aligning with the principles of fairness and ethical deployment in biometric systems [15]. Emotional primitives such as activation (indicating high or low energy) and valence (signifying positive or negative feeling) are frequently used to represent different emotions [16]. Activation aids in distinguishing between emotional states like angry or happiness (high energy) and sadness or calmness (low energy); valence helps differentiate between angry (negative feeling) and happiness (positive feeling) [17]. When person experiences anger, the speech often deviates from his/her neutral tone, resulting in significant tone mismatch. This presents major challenge for the SV systems when comparing such emotionally-inflected speech with the enrollment data. Addressing emotions in speaker recognition is crucial aspect that has been relatively unexplored. This approach has broader implications for improving the performance of speaker recognition models in various practical applications, especially in real-world scenarios where emotions play pivotal role in human communication. Moreover, building robust speaker recognition system is hindered by scarcity of emotional utterances data. Since we speak in neutral tone most of the time, only meagre fraction, 10% of the data, captures the emotional tone (cf. Table 1). Consequently, the error rates (false acceptance and false rejection) of our SV system, which is multi-layer LSTM network trained using generalized endto-end (GE2E) loss [18] is 1.3% higher for emotional utterances compared to neutral ones  (Table 4)  . To tackle the data sparsity issue, we propose to use emotional voice conversion techniques to supplement the training data. This involves generating emotional utterances from neutral ones by preserving the speakers identity, thereby enhancing the effectiveness of training for SV with emotional speech. Emotional voice conversion transforms audio samples of specific emotional tone into audio samples of different emotional tone without changing any linguistic content. Existing converters can be categorized into statistical-based or neural network-based approaches. Statistical-based approach includes Gaussian mixture models [19], non-negative matrix factorization [20], and partial least square regression [21]. These methods operate on low dimensional spectral features, and their performance is extremely sensitive to the quality of the input speech. Neural network-based approaches encompass restricted Boltzmann machines [22], feed-forward neural networks [23], and artificial neural networks [24]. These implementations rely on the availability of parallel training data, which means having dataset containing identical linguistic content spoken by the same speakers, but with various emotional expressions. Collecting such dataset is impractical, as it is not natural for speaker to repeat the same sentence with 4-5 different emotions. In recent developments, several approaches have emerged aiming to circumvent the need of parallel data in style-transfer tasks. These methods include Deep Bidirectional Long-ShortTerm Memory (DBLSTM) with i-vector [25], variational autoencoder [26, 27] and GANs [28, 29, 30]. Recently, Zhou et al. [31] introduced CycleGAN network to learn the emotional voice conversion task based on style transfer autoencoder. Furthermore, the authors explore the application of continuous wavelet transform (CWT) to enhance the efficacy of F0 conversion. This architecture eliminates the need for parallel training data or any external modules, and has emerged as viable option to synthesize emotional utterances that fit our need. We have trained two CycleGAN networks that transform neutral-tone utterances into angry-tone and happy-tone utterances while preserving their distinctive speaker characteristics. The synthetic emotional utterances are used to augment the training dataset used to learn the SV task. Results show that models trained with combination of authentic and synthetic emotional data have demonstrated relative reduction of 1.08% to 3.64% in equal error rate (EER) for emotional utterances. To summarize, the key contributions of our research can be outlined as follows: OUR CONTRIBUTIONS To the best of our knowledge, this is the first work that has applied CycleGAN as data augmentation technique for the SV task. The proposed method leads to improved SV performance on emotional speech owing to reduced performance gap between neutral and emotional utterances. Experimental results have shown that the trained CycleGAN networks are able to effectively preserve unique speaker characteristics during emotion conversion. 2. Methods Identity loss is defined in Equation (3) as follows, 2.1. Speaker Verification Model The SV model is multi-layer LSTM network which uses 40-dimensional Mel-spectogram as input [32] and outputs an n-dimensional d-vector or deep vector, which is the average of activations derived from the last hidden layer of the LSTM [33]. d-vector helps in speaker verification by encapsulating the neural embeddings of speakers voice characteristics. This network is trained using the generalized end-to-end (GE2E) loss proposed by Li et al. [18]. In the inference phase, the system takes pair of d-vectors one representing the speaker profile derived from enrollment data, and the other containing the speaker information of an utterance of interest. We use cosine similarity between these two d-vectors to verify if the pair corresponds to the same speaker. We acknowledge that the baseline model does not represent the state-of-the-art (SOTA). This deliberate choice was made to ensure that the SV model is functional within production-level constraints imposed to work on relatively smaller resources, be it on-device configurations or limited worker farms supporting cloud back-ends. 2.2. CycleGAN for Emotion Conversion The CycleGAN framework uses WORLD Vocoder [34] to extract speech features from an utterance. The emotion converter contains two components: one uses 24-dimensional Melcepstral coefficients (MFCCs) for spectrum conversion, and the other uses 10-dimensional Fundamental frequency (F0) features to handle prosody conversion are fed into separate CycleGAN network [31]. Similarly to other GAN architectures, CycleGAN incorporates generator for converting audio features from one emotional tone to another, and discriminator for discerning the real and the converted data. We use three different loss functions to train the CycleGAN framework: (i) adversarial loss, (ii) cycle-consistency loss, and (iii) identity loss. Adversarial loss, defined in Equation (1), measures the distinctness between the converted data and the original data, specifically, LADV (GXY , DY , X, ) = EyP (y) [DY (y)] (1) + ExP (x) [log (1 DY (GXY (x))] , where is the source utterance (with neutral tone) and is the target utterance (with an emotional tone). GXY is the Generator that converts source to target utterances, whereas DY is the Discriminator for target utterances. Adversarial loss only governs whether GXY follows the distribution of the target data but does not preserve the contextual information nor the speaker identity. Hence, cycleconsistency loss is used to guarantee the consistency of contextual information and speaker identity between and GXY . This loss encourages the generator to find an optimal pseudo pair (x, y) through circular conversion. Cycle-consistency loss is defined in Equation (2) as follows, LID (GXY , GY ) (3) =ExP (x) [GY (x) x] + EyP (y) [GXY (y) y] The final loss function is combination of the above three loss functions: L(GXY , GY , DX , DY , X, ) (4) = LADV (GXY , DY , X, ) + LADV (GY , DX , X, ) + λCY LCY (GXY , GY ) + λIDLID (GXY , GY ) To guide the learning process, we set λCY = 10 for all the iterations, whereas LID is only used for the first 104 iterations with λID = 5. 3. Experiment Settings 3.1. Data To train the CycleGAN model, we used non-parallel emotional utterances drawn from three distinct open source datasets: Emotional Speech Dataset [35], EmoV [36], and Ravdess [37]. Emotional Speech dataset consists of 350 utterances spoken by 10 English speakers in different emotional states. EmoV dataset is collection of utterances from 4 different speakers. Ravdess is an audio-visual dataset containing 1440 emotional utterances spoken by 24 professional actors. The aforementioned data is abundant in emotional content but lacks sufficient number of speakers. For SV models, we acquired recordings from everyday life through internal channels with proper consent. To be compliant with biometric regulations like General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), and Biometric Information Privacy Act (BIPA), we obfuscate the biometric data pertaining to our speakers. In order to generate the speaker labels, we first randomly sampled anonymized utterances. These selected utterances, along with the enrollment data corresponding to the same devices speakers, were then provided to multiple annotators, with minimum of 3 annotators independently assigning ground-truth labels. To minimize annotation errors, we adopted the decision agreed upon by the majority of annotators. We divided the data into training and evaluation subsets, with distinct sets of speakers. Within the training set, we randomly set aside 5% of the speakers as our validation set for early stopping [38]. Table 1 provides an overview of the distribution of speakers and utterances in the training and evaluation sets. 3.2. Model Training 3.2.1. Training Considerations We trained two separated CycleGAN networks, each dedicated to transforming neutral-tone utterances into either angry or happy tones. 2000 audio utterances per emotion are Table 2: Average cosine similarity between neutral tone and angry tone utterances of the same speaker. LCY (GXY , GY ) = ExP (x) +EyP (y) (cid:2)GY (GXY (x)) x1 (cid:3) (cid:2)GXY (GY (y)) y1 (cid:3) (2) Case Cosine Similarity Speaker 1 Speaker 2 Neutral vs. Authentic Angry Neutral vs. Synthetic Angry 0.51 0.10 0.65 0.06 0.53 0.12 0.65 0.09 Lastly, we use identity loss to preserve linguistic information. Table 3: Relative improvement in EER comparing the SV models trained with neutral utterances only (baseline) and with addition of synthetic angry and happy utterances. Training data contains sub-sampled speakers, with each speaker contributing 50-60 neutral utterances. Positive values signify an improvement in performance compared to the baseline. Experiment Configuration Overall Neutral Emotional Happy Angry Sad Calm Baseline 50 neutral Synthetic angry utterances 50 neutral + 10 angry 50 neutral + 20 angry 50 neutral + 50 angry 60 neutral + 20 angry - - 0.74% -0.28% -0.14% -0.65% -1.74% -3.77% 0.57% -0.20% - 0.29% 3.00% 1.64% 2.50% - - - - 4.38% -7.58% -1.46% 3.90% 10.18% 2.69% -1.17% 5.54% -1.42% 21.61% -5.12% 6.45% 11.60% -1.63% -2.34% 3.90% Synthetic happy utterances 50 neutral + 10 happy 50 neutral + 20 happy 0.61% 0.69% -1.42% 7.02% -1.46% 0.00% 1.44% 2.02% 4.57% 0.00% 7.02% -0.59% 3.90% 0.86% Synthetic angry + happy utterances 50 neutral + 5 angry + 5 happy 50 neutral + 10 angry + 10 happy 50 neutral + 20 angry + 20 happy -1.42% 4.32% -0.29% -1.36% 0.72% -0.71% 1.61% 1.30% 2.35% 4.38% 1.07% -0.59% 3.90% 5.80% 11.34% -2.05% 3.00% 0.31% -1.53% 2.57% 2.71% Table 4: Relative improvement in EER comparing the SV models trained with utterances directly collected from speakers (baseline) and with addition of synthetic angry and happy utterances. Each speaker contains varying numbers of neutral and emotional utterances. Positive values signify an improvement in performance compared to the baseline. Experiment Configuration Overall Neutral Emotional Happy Angry Sad Calm Performance gap (Emotional - Neutral) Baseline No synthetic data - - - - - - - Synthetic angry + happy utterances 5 angry + 5 happy 0.45% 0.81% 10 angry + 10 happy -0.31% -0.65% 15 angry + 15 happy 0.00% -1.18% 1.08% 2.63% 3.64% 6.16% 1.87% 0.90% 1.54% 6.16% 1.87% 0.60% -1.02% 6.16% 1.87% 0.90% -0.51% 1.30% 1.27% 1.05% 0.94% sampled from three different datasets i.e. Emotional Speech Dataset [35], EmoV [36], and Ravdess [37] are used to train the model. During the training phase of the CycleGAN network, the input consists of source (neutral) and target (emotional) utterances from same speaker, albeit containing different emotional expressions and even distinct linguistic content. The CycleGAN model concurrently learns forward and inverse mapping with the help of adversarial and cycle consistency losses. This approach aims to ascertain the optimal transformation between source (neutral) and target (emotional) utterances. We focus on synthesizing the angryand happy-tone utterances because these two emotional categories exhibit high energy levels and are situated farther apart from the neutral tone within the Activation-Valence spectrum. Throughout the adversarial training process, the generator and discriminator are competing with each other to learn the network. We expect the discriminator loss to stabilize, which means that the discriminator is not able to differentiate correctly between the generated samples and the real samples. However, the discriminator task is relatively easy at the beginning of the training since the generator has not learned to generate adequate data. This made the generator fail due to vanishing gradients [39]. To mitigate this issue, we adopt strategy that allows the generator head start in learning [40]. This method involves providing an initial advantage to the generator by refraining discriminator from training for the first train steps. With this technique, the generator is afforded distinct oppurtunity to undergo training and improve its performance without immediate interference from the discriminator. Consequently, this approach facilitates generators advancement ahead of discriminators active engagement, thereby mitigating the disparity in learning progress. Similarly, we observed an improvement of F0 model performance by using regularization techniques such as early stopping. We noted 16.5% improvement in consine similarity between source and prediction embeddings of same speaker. 3.2.2. Training Configuration and Hyperparameters We employed the generalized end-to-end (GE2E) [18] training framework, where we assembled mini-batch consisting of utterances. Specifically, these utterances were gathered from = 32 speakers, and each speaker contributes = 5 utterances. The features extracted from each of the training utterances xij (1 and 1 ) are fed into an LSTM network. During the training process, the GE2E loss pushes the centroid of embeddings closer to the true speaker and away from negative nearby speakers. We utilized the Adam optimizer [41] for training the model parameters, with learning rate η = 106, and an exponential decay rate β = 0.98 at every 10k iterations. 4. Results 4.1. Emotion Conversion As discussed in Section 3.2.1, we trained two different CycleGAN networks to transform neutral-toned utterances into those expressing anger and happiness. To assess the efficacy of this emotional modulation, we conducted comprehensive evaluation. Firstly, we utilized the cycleGAN model to synthesize angry utterances from neutral utterances. We then extracted speaker embeddings using the baseline SV model for three disances: (i) 50 neutral + 10 angry, (ii) 50 neutral + 20 angry, (iii) 50 neutral + 50 angry, and (iv) 60 neutral + 20 angry, where the notation 50 neutral + 10 angry signifies that within our training dataset, consisting of 192 speakers, we incorporated 50 neutral utterances along with 10 synthetic angry utterances for each speaker. Similarly, we trained experimental models by including synthetic happy utterances, viz. configuration (i) 50 neutral + 10 happy and (ii) 50 neutral + 20 happy, as well as experimental models including both synthetic happy and angry utterances as shown in Table 3. These models are evaluated on the same evaluation set described in Table 1, and we evaluate the overall performance together with the performance breakdown on different emotion categories. Table 3 summarizes the relative reduction on EER (EER) of the SV models trained with the inclusion of synthetic data. We would like to highlight that in accordance with the internal guidelines of our organization, we are restricted from disclosing the absolute EER. Therefore, we have presented the relative EER as an alternative measure. Overall, models trained with the inclusion of synthetic angryand happy-tone data have effectively reduced errors for emotional utterances (happy, angry, sad, and calm). However, this improvement is accompanied by decline in performance for neutral utterances, especially when synthetic angry-tone data is introduced. For instance, the *50 neutral + 20 angry* configuration has led to an improvement in EER of 3.00% on emotional utterances, but degradation of -0.65% on neutral utterances from the evaluation dataset. As we progressively increase the number of synthetic angry-tone utterances, we observe boost in performance for angry-tone utterances, but consistent degradation for neutral utterances. Table 3 shows that as we increase the synthetic angry-tone proportion from 10, 20, to 50 per speaker, we observed decline in neutral utterances from -0.28%, -0.65% to -3.77%. Yet, such degradation is less pronounced when increasing the number of synthetic happytone utterances. By adding 10, 20 synthetic happy utterances we noticed an improvement in overall EER by 0.61% (0.69% in neutral and 0.86% in emotional) and 1.44% (2.02% in neutral and 4.57% in emotional) respectively. This suggests that adding synthetic happy-tone data is more effective in learning the SV task compared to angry-tone data. There are two potential factors contributing to the observed performance gain. One possibility is that the synthetic data provides genuine signals to learn the SV task. The other possibility is the increase in data size. To disentangle these influences, we conducted an experiment with more utterances per speaker (cf. 60 neutral + 20 angry configuration in Table 3). If data size were the primary factor, we would expect better performance on this setting compared to others trained with fewer utterances. Contrary to this expectation, the overall performance was only superior to that of the 50 neutral + 20 angry configuration in Table 3. Similarly, when we introduced substantial number of synthetic angry-tone utterances (cf. 50 neutral + 50 angry configuration in Table 3), the model did not outperform others with fewer utterances. These findings suggest that data size is not the predominant factor, and synthetic data contains valuable signals that assist the learning of the SV task. Next, we delve into the impact of training the SV models with combination of authentic and synthetic emotional data on the entire training dataset. We incorporate utterances from all speakers. As presented in Table 4, the addition of modest quantity of synthetic data (cf. 5 angry + 5 happy configuration in Table 4) leads to relative error reduction of 0.81% for neutral utterances, and 1.08% for emotional utterances. Consistent Figure 1: t-SNE plot of utterances from single speaker in neutral tone (green), angry tone (blue), and converted angry tone (orange). tinct sets of utterances (i) neutral tone, (ii) angry tone, and (iii) converted angry tone via the CycleGAN network. We then used t-SNE [42] to project these embeddings into 2-dimensional space for visualization. To enhance the readability of the visualization, we opted to utilize data from single speaker exclusively. Figure 1 shows that neutral utterances form central cluster, while the angry-tone utterances scattered around the space. Notably, the synthetic utterances seamlessly integrate with the authentic angry tone utterances, indicating change in the emotional state of the utterances. Since the primary application of these synthetic emotional utterances lies in training SV models, critical aspect of the converter is preserving the speakers vocal characteristics. To evaluate the speaker information, we measure the cosine similarities based on speaker embeddings between the neutral and angry-tone utterances. Specifically, we computed the average and standard deviation of cosine similarities between neutral and real angry-tone utterances of the same speaker. This serves as baseline for the expected similarity between neutraland angry-tone utterances of single speaker. We then computed similar metrics between utterances in neutral and synthetic angry-tone of the same speaker. Table 2 shows that synthetic angry utterances exhibit higher cosine similarities with the neutral tone utterances compared to the authentic angry utterances. This suggests that our emotion converter effectively retains unique speaker characteristics. 4.2. Augmenting Emotional Data to Train the SV Model In this section, we explore the impact on the performance of the SV model by augmenting the training dataset with converted emotional data. In our preliminary investigation, we subsampled 192 active speakers from our training dataset that have sufficient number of neutral utterances. These neutral utterances are fed into the WORLD Vocoder [34] to extract F0 and MFCC features. We leveraged the trained CycleGAN models to convert the existing neutral prosody and spectrum features to synthesize angryand happy-tone utterances. The baseline model is trained with 50 neutral-tone utterances collected from each of these 192 speakers. The experimental models are trained with both neutral-tone (real) and emotional (synthesized) utterances in different proportions. We trained four experimental models using synthetic angry utterwith our previous findings, as we incrementally augment the number of synthetic emotional utterances, we continue to witness improvements for emotional utterances, albeit with slight degradation for neutral utterances. This approach of including synthetic emotional utterances enables us to narrow the error gap between neutral and emotional utterances, reducing it from 1.30% to 0.94%. 4.2.1. Resilience against Spoofing Attack Integration of synthetic utterances as data augmentation technique presents risk of spoofing attacks [43], which involves malicious use of audio utterances to deceive the SV model. Here, we specifically focused on media speech as proxy for spoofing, given its prevalence in everyday communication scenarios. The term media speech pertains to audio recordings sourced from various mediums such as television broadcasts or YouTube videos. It is crucial for SV models to differentiate media speech content from explicit user commands. Owing to our utilization of synthetic data for training, investigating whether this practice has had any adverse impact on the performance of SV models when applied to media speech data was necessary. For this experiment, we considered 3000 media speech utterances and we compared each of them against all the speakers in the evaluation data set. Despite adding synthetic utterances, we observed that the FAR value on media speech is less than the targeted 3% FAR in both Neutral and Emotional spectrum, which proves that adding synthetic data did not negatively affect the performance on media speech. Nonetheless, further research is imperative to fortify SV models against evolving spoofing techniques. 5. Conclusion In this work, we have introduced pioneering approach utilizing the CycleGAN framework to significantly improve SV systems through the innovative use of data augmentation. This technique uniquely maintains the individual vocal traits of speakers while generating synthetic emotional speech samples, thus enhancing the training process and improving model performance across various emotional scenarios. This integration of synthetic data marks pivotal improvement in the accuracy and reliability of SV models, addressing the critical need for systems that can adeptly handle the nuances of human speech influenced by emotional states. The implications of this advancement are profound, offering new possibilities for creating synthetic data that closely mimics wide range of acoustic conditions, from the specific properties of different recording devices to the nuances between near-field and far-field audio, and even the subtle differences in speaker gender. This opens the door to significantly improved speaker verification systems that can more accurately and effectively authenticate users under diverse conditions, making these systems more versatile and powerful tools for security and identification purposes. Furthermore, our findings underscore the importance of emotional variability in the context of SV systems, highlighting how addressing this variability directly contributes to the systems robustness, adaptability, and fairness. By enhancing the ability of SV systems to accurately recognize and verify speakers across broad spectrum of emotional expressions, we ensure more equitable access to authentication technologies. This focus on emotional intelligence within biometric systems aligns with an increasing demand for security solutions that are both advanced and attuned to the complexities of human behavior, ensuring more inclusive approach to authentication that respects and accommodates the diversity of user populations. Looking ahead, applications of our the potential CycleGAN-based data augmentation extend beyond the immediate scope of speaker verification. The same principles could be applied to enhance voice interaction systems in smart environments, support empathetic responses in AI-driven customer service, and even contribute to advancements in forensic analysis by enabling more accurate speaker identification under varied emotional conditions. The versatility of our approach paves the way for more empathetic and responsive AI systems and opens new avenues for research and development aimed at bridging the gap between current static models and the dynamic needs of real-world communication. 6. Acknowledgments We would like to thank Brecht Desplanques for his contribution in reviewing this work and providing valuable feedback. 7. References [1] Hamid Aghajan, Juan Augusto Wrede, and R.L-C Delgado, Human-Centric Interfaces for Ambient Intelligence, 01 2010. [2] Bimbot Frederic, Jean-Francois Bonastre, Corinne Fredouille, Guillaume Gravier, and Ivan MagrinChagnolleau, tutorial on text-independent speaker verification, EURASIP Journal on Advances in Signal Processing, 2004. [3] D.E. Sturim, D.A. Reynolds, E. Singer, and J.P. Campbell, Speaker indexing in large audio databases using anchor models, in 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221), 2001, vol. 1, pp. 429432 vol.1. [4] William M. Campbell, Joseph P. Campbell, Terry P. Gleason, Douglas A. Reynolds, and Wade Shen, Speaker verification using support vector machines and high-level features, IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 7, pp. 20852094, 2007. [5] Jee weon Jung, Hee-Soo Heo, Ju ho Kim, Hye jin Shim, Rawnet: Advanced end-to-end deep and Ha jin Yu, neural network using raw waveforms for text-independent speaker verification, in Interspeech, 2019. [6] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansfield, and Ignacio Lopz Moreno, Speaker diarization with lstm, in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 52395243. [7] Ravika Naika, An overview of automatic speaker verin Intelligent Computing and Inforification system, mation and Communication, Subhash Bhalla, Vikrant Bhateja, Anjali A. Chandavale, Anil S. Hiwale, and Suresh Chandra Satapathy, Eds., Singapore, 2018, pp. 603610, Springer Singapore. [8] Nilu Singh, Prof. Raees Khan, and Raj Shree Pandey, Applications of speaker recognition, Procedia Engineering, vol. 38, pp. 31223126, 12 2012. [9] Ali Bou Nassif, Ismail Shahin, Ashraf Elnagar, Divya Velayudhan, Adi Alhudhaif, and Kemal Polat, Emotional speaker identification using novel capsule nets model, Expert Systems with Applications, vol. 193, pp. 116469, 2022. [10] Lin Shu, Yang Yu, Wenzhuo Chen, Haoqiang Hua, Qin Li, Jianxiu Jin, and Xiangmin Xu, Wearable emotion recognition using heart rate data from smart bracelet, Sensors (Basel, Switzerland), vol. 20, 2020. [11] Purves D, Augustine GJ, and Fitzpatrick D, Physiological changes associated with emotion, Neuroscience. 2nd edition, 2001. [12] Raghavendra Pappagari, Tianzi Wang, Jesus Villalba, Nanxin Chen, and Najim Dehak, X-vectors meet emotions: study on dependencies between emotion and speaker recognition, in Proc. IEEE ICASSP, 05 2020, pp. 71697173. [13] Woan-Shiuan Chien and Chi-Chun Lee, Achieving fair speech emotion recognition via perceptual fairness, in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 15. [14] M.G.J. Swerts and E.J. Krahmer, Gender-related differences in the production and perception of emotion, in Proceedings of the international conference on spoken language processing (Interspeech 2008). 2008, pp. 334 337, ISCA. [15] Christian Rathgeb, Pawel Drozdowski, Dinusha C. Frings, Naser Damer, and Christoph Busch, Demographic fairness in biometric systems: What do the experts say?, IEEE Technology and Society Magazine, vol. 41, no. 4, pp. 7182, 2022. [16] Patricia E. G. Bestelmeyer, Sonja A. Kotz, and Pascal Belin, Effects of emotional valence and arousal on the voice perception network, in Soc Cogn Affect Neurosci, 2017, pp. 13511358. [17] Carlos Busso and Tauhidur Rahman, Unveiling the acoustic properties that describe the valence dimension, in Proc. Interspeech, 09 2012, pp. 11791182. [18] Li Wan, Quan Wang, Alan Papir, and Ignacio Moreno, Generalized end-to-end loss for speaker verification, in Proc. IEEE ICASSP, 04 2018, pp. 48794883. [19] Tomoki Toda, Alan Black, and Keiichi Tokuda, Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory, Audio, Speech, and Language Processing, IEEE Transactions on, vol. 15, pp. 2222 2235, 12 2007. [20] Daniel Lee and H. Sebastian Seung, Algorithms for nonnegative matrix factorization, in Advances in Neural Information Processing Systems, T. Leen, T. Dietterich, and V. Tresp, Eds. 2000, vol. 13, MIT Press. [21] Zhizheng Wu, Tuomas Virtanen, Eng Chng, and Haizhou Li, Exemplar-based sparse representation with residual compensation for voice conversion, Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 22, pp. 15061521, 10 2014. [22] Ling-Hui Chen, Zhen-Hua Ling, Li-Juan Liu, and Lirong Dai, Voice conversion using deep neural networks with layer-wise generative training, Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 22, pp. 18591872, 12 2014. [23] Srinivas Desai, Alan Black, B. Yegnanarayana, and Kishore Prahallad, Spectral mapping using artificial neural networks for voice conversion, Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, pp. 954 964, 08 2010. [24] Toru Nakashika, Ryoichi Takashima, Tetsuya Takiguchi, and Yasuo Ariki, Voice conversion in high-order eigen space using deep belief nets, in Proc. Interspeech, 08 2013, pp. 369372. [25] Jie Wu, Zhizheng Wu, and Lei Xie, On the use of i-vectors and average voice model for voice conversion without parallel data, in 2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2016, pp. 16. [26] Yuki Saito, Yusuke Ijima, Kyosuke Nishida, and Shinnosuke Takamichi, Non-parallel voice conversion using variational autoencoders conditioned by phonetic postein 2018 IEEE International riorgrams and d-vectors, Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 52745278. [27] Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang, Voice conversion from non-parallel corpora using variational auto-encoder, 2016. [28] Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang, Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks, arXiv e-prints, p. arXiv:1704.00849, Apr. 2017. [29] Berrak Sisman, Mingyang Zhang, Minghui Dong, and Haizhou Li, On the study of generative adversarial networks for cross-lingual voice conversion, in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 144151. [30] Takuhiro Kaneko and Hirokazu Kameoka, Cyclegan-vc: Non-parallel voice conversion using cycle-consistent adversarial networks, in 2018 26th European Signal Processing Conference (EUSIPCO), 2018, pp. 21002104. [31] Kun Zhou, Berrak Sisman, and Haizhou Li, Transforming spectrum and prosody for emotional voice conversion with non-parallel training data, in Odyssey Speaker and Language Recognition Workshop, 05 2020. [32] Ruirui Li, Chelsea Ju, Zeya Chen, Hongda Mao, Oguz Elibol, and Andreas Stolcke, Fusion of embeddings networks for robust combination of text dependent and independent speaker recognition, in Proc. Interspeech, 08 2021, pp. 45934597. [33] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier Gonzalez-Dominguez, Deep neural networks for small footprint text-dependent speaker verification, in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 40524056. [34] Masanori MORISE, Fumiya YOKOMORI, and Kenji Ozawa, World: vocoder-based high-quality speech synthesis system for real-time applications, IEICE Transactions on Information and Systems, vol. E99.D, pp. 18771884, 07 2016. [35] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li, Seen and unseen emotional style transfer for voice conversion with new emotional speech dataset, in Proc. IEEE ICASSP, 02 2021. [36] Adaeze Adigwe, Noe Tits, Kevin El Haddad, Sarah Ostadabbas, and Thierry Dutoit, The emotional voices database: Towards controlling the emotion dimension in voice generation systems, 06 2018. [37] Steven R. Livingstone and Frank A. Russo, The ryerson audio-visual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english, PLOS ONE, vol. 13, no. 5, pp. 135, 05 2018. [38] Lutz Prechelt, Early Stopping But When?, Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. [39] Martin Arjovsky and Leon Bottou, Towards principled methods for training generative adversarial networks, in International Conference on Learning Representations, 2017. [40] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, Generative adversarial nets, in Advances in neural information processing systems, 2014, pp. 26722680. [41] Diederik P. Kingma and Jimmy Ba, Adam: method for stochastic optimization, in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun, Eds., 2015. [42] Laurens van der Maaten and Geoffrey Hinton, Visualizing data using t-sne, Journal of Machine Learning Research, vol. 9, no. 86, pp. 25792605, 2008. [43] Danwei Cai, Zexin Cai, and Ming Li, Identifying source speakers for voice conversion based spoofing attacks on in ICASSP 2023 - 2023 speaker verification systems, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 15."
        }
    ],
    "affiliations": [
        "Amazon Alexa AI, USA",
        "Amazon Business, USA",
        "Amazon GenAI, USA",
        "Amazon Search Experience Science, USA",
        "Amazon Web Services, USA"
    ]
}