{
    "paper_title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
    "authors": [
        "Haoming Xu",
        "Shuxun Wang",
        "Yanqiu Zhao",
        "Yi Zhong",
        "Ziyan Jiang",
        "Ningyuan Zhao",
        "Shumin Deng",
        "Huajun Chen",
        "Ningyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 8 0 1 2 . 3 0 5 2 : r ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging Haoming Xu1 *, Shuxun Wang1 *, Yanqiu Zhao1 *, Yi Zhong1 *, Ziyan Jiang1 *, Ningyuan Zhao1 *, Shumin Deng2, Huajun Chen1, Ningyu Zhang1 1 Zhejiang University 2 National University of Singapore haomingxu2003@gmail.com, zhangningyu@zju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "This paper presents the ZJUKLAB teams submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIESMerging), combining two specialized models into more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research1."
        },
        {
            "title": "Introduction",
            "content": "Unlearning has emerged as critical technique in AI systems, enabling the selective removal of sensitive data, including copyrighted material and personal information, from trained models. As the International AI Safety Report (Bengio et al., 2025) emphasizes, unlearning plays vital role in mitigating privacy and copyright risks associated with extensive training datasets. However, it also acknowledges that current unlearning methods remain in- * Equal contribution Corresponding authors. 1 Code is available at https://github.com/ zjunlp/unlearn/tree/main/semeval25. adequate, which often fail to completely erase targeted data while potentially degrading model performance, thus limiting practical implementation. Specifically, existing unlearning methods often struggle with over-forgetting (excessive elimination of non-sensitive information) or underforgetting (incomplete removal of sensitive data). It is challenging to find optimal hyperparameters that balance performance across multiple evaluation dimensions, sometimes even impossible. To address these limitations, we propose novel unlearning system that leverages model merging to combine an over-forgetting model with an under-forgetting model, creating more effective unlearned model. It can produce superior results simply by merging two models with complementary biases. Our system achieved second place in SemEval2025 Task 4: Unlearning Sensitive Content from Large Language Models, with our 7B model attaining Task Aggregate Score of 0.944 and Aggregate Score of 0.487, demonstrating the effectiveness of our system in selectively removing sensitive content. Furthermore, our local experiments yielded almost perfect results with MIA Score of 0.501 and Aggregate Score of 0.806, while maintaining an exceptionally high Task Aggregate and comparable MMLU Avg.. We provide comprehensive analyses that validate our systems effectiveness and offer deeper insights into the unlearning process."
        },
        {
            "title": "2 Task Description",
            "content": "Datasets The dataset comprises forget set and retain set across three subtasks: (1) long-form synthetic creative documents, (2) short-form synthetic biographies with PII (names, phone numbers, SSN, emails, addresses), and (3) real documents from the target models training data. The organizers provide vanilla model (OLMo-7B-0724Instruct) (Groeneveld et al., 2024) which has been pretrained on all subtasks. Figure 1: Visualizing Unlearning via Model Merging. The vanilla model (top) initially assigns high probabilities to forget set (member) and low probabilities to holdout data (nonmember). We then merge two individually unlearned models: one exhibiting over-forgetting (middle left) and the other under-forgetting (middle right). Model merging aims to achieve balanced forgetting (bottom), effectively reducing the models confidence in predicting sensitive member data while preserving its performance on nonmember data. Evaluation Evaluation involves sentence completion and question answering across tasks. Key metrics include: Regurgitation Score (ROUGE-L for sentence completion), Knowledge Score (accuracy for QA), MIA Score (loss-based membership inference attack (Shi et al., 2024)), and MMLU Score (average accuracy on 57 STEM subjects). Task Aggregate is the harmonic mean of Regurgitation Scores and Knowledge Scores for each task. The overall Aggregate averages the Task Aggregate, MIA scores, and MMLU scores. For details about task description, please refer to the official paper (Ramakrishna et al., 2025)."
        },
        {
            "title": "3 Methodology",
            "content": "As illustrated in Figure 1, our unlearning system follows two phases. (1) the Training Phase develops two complementary models, each exhibiting strong performance. (2) the Merging Phase merges these models, leveraging their strengths to achieve effective and balanced unlearning."
        },
        {
            "title": "3.1 Training Phase",
            "content": "We train two models with identical objectives but different hyperparameters via Low-Rank Adaptation (LoRA) (Hu et al., 2021). Three components are included in the optimization process: Negative Preference Optimization (NPO) (Zhang et al., 2024a) on forget set, alongside Gradient Descent on Retain Set (GDR) and Kullback-Leibler Divergence Minimization on Retain Set (KLR). The composite objective is as follows: Ltotal = αLnpo + βLgdr + γLklr, (1) where Lnpo leverages the preference optimization to minimize probabilities of target tokens on forget data, while Lgdr and Lklr preserve retain data. The hyperparameters α, β, γ are set to balance forgetting and retention. Our aim is to train two complementary models that exhibit distinct strengths in metrics. Detailed formulations are shown in Appendix A.1."
        },
        {
            "title": "3.2 Merging Phase",
            "content": "After training, we apply TIES-Merging (Yadav et al., 2023) to combine the LoRA adapters of the two models. This involves three stages: Trimming: Preserving only the most significant parameters based on density threshold while zeroing out the rest. Electing: Creating unified sign vector that resolves parameter conflicts by identifying the dominant direction of change across models. Disjoint Merging: Averaging non-zero parameter values that align with the unified sign vector, ensuring that the merged model incorporates only changes contributing to the agreed direction, thus improving multitask performance. Environment Algorithm Aggregate Task Aggregate MIA Score/MIA AUC MMLU Avg. Online Local AILS-NTUA YNU Mr.Snuffleupagus ZJUKLAB (ours) NPO+GDR+KLR (model1) NPO+GDR+KLR (model2) Ours 0.706 0.470 0.376 0. 0.481 0.504 0.806 0.827 0.834 0.387 0.944 0.968 0.659 0.939 0.847 / 0.139 / 0.256 / 0.048 / 0.045 / 0.022 0.364 / 0.818 0.997 / 0.501 0.443 0.436 0.485 0. 0.431 0.491 0.480 Table 1: The online and local experiments results. Note that indicates over-forgetting, indicates under-forgetting, and signifies balanced forgetting, achieving raw MIA AUC close to 0.5. All metrics are detailed in 2."
        },
        {
            "title": "4 Experiments",
            "content": "Implementation We carried out our experiments using two NVIDIA A100-PCIE-40GB GPUs. The organizers supplied the local dataset for our local experiments and evaluated our code online using an additional unreleased dataset. Detailed configurations are provided in Appendix A.2. Main Results Table 1 presents the online results evaluated by the organizers and the local results evaluated by us. Our 7B model achieves an Aggregate score of 0.487 online, ranking second among 26 teams. The online MIA Score is less favorable, possibly due to dataset discrepancies between the online and local environments. However, local evaluations effectively validate the core principles of our system design. In training phase, model1 shows over-forgetting, achieving high Task Aggregate of 0.968 but low MIA Score of 0.022. In contrast, model2 shows under-forgetting, with lower Task Aggregate of 0.659 and higher MIA Score of 0.818. The merged model shows better performance, attaining Task Aggregate of 0.939 and MIA AUC of 0.501. This merging technique integrates the strengths of both models, preserving their high Task Aggregate and MMLU Avg. scores while successfully neutralizing their MIA scores, resulting in an almost ideal MIA score. These results highlight our systems ability to effectively aggregate the strengths of these biased models."
        },
        {
            "title": "5.1 Why NPO+GDR+KLR Works?",
            "content": "section analyzes This the effectiveness of NPO+GDR+KLR model (denoted as model1 in the training phase), trained on the local dataset. Performance Trajectory To understand performance trends, we evaluated model checkpoints Figure 2: Performance Curves: Regurgitation and Knowledge Scores During Training. Figure 3: NPO+GDR+KLR. Training Loss Curves of NPO and throughout training. As shown in Figure 2, both Regurgitation and Knowledge Scores initially decline concurrently for forget and retain sets (epochs 0-0.8). This suggests that, in the early stages of training, the optimization processes for both forgetting and retaining knowledge are proceeding in the same direction, causing simultaneous metric decrease. Subsequently, the Knowledge Score steadily trends upward, while the Regurgitation Score increases with noticeable oscillations. This indicates that the optimization directions of knowledge retention and knowledge forgetting are beginning to become different. The observed fluctuations Merging methods Agggregate Linear DARE-Linear DARE-TIES Magnitude Prune TIES 0.244 0.440 0.561 0.558 0.806 Table 2: Merging techniques comparison Figure 5: Performance for different density choices"
        },
        {
            "title": "5.2 Why Merge works?",
            "content": "To understand the efficacy of merging, we conduct comparative experiments on different merging techniques. As shown in Table 2, TIES-Merging outperforms others, and this effectiveness comes from its three fundamental operations: Trim, Elect, and Disjoint Merge. Firstly, for Trimming, we conduct ablation studies varying the density between 0.6, 0.8, and 1 (Figure 5) and observed that density of 0.8 yields the best results. This optimal density level retains essential parameters while removing redundant ones, effectively preserving the better performance of two models and achieving balanced forgetting. We hypothesize that lower densities (e.g., 0.6) excessively prune parameters vital for knowledge retention, leading to over-unlearning and reduced MIA score. Conversely, density of 1, by retaining all parameters, introduces redundancy and may incorporate influences from the less-unlearned model, resulting in suboptimal outcome and higher MIA score. Therefore, trimming with density of 0.8 strikes critical balance. Beyond trimming, TIES-Merging further enhances directional consistency through the Elect operation, which establishes parameter signs based on magnitude. Given the strong baseline performance of the individual models, this magnitude-based election ensures reliable convergence toward optimal directional consistency during merging. Finally, the DisFigure 4: Angle (θ) between Parameter Change Vectors: 165 . 0 , inal 0 , inal 165 in Regurgitation Score may stem from the tradeoff between learning and forgetting. Loss Dynamics Figure 3 compares the training loss curves of NPO and NPO+GDR+KLR models. Notably, the NPO+GDR+KLR loss curve displays oscillations in mid-training, likely caused by the similarity between forget and retain sets, hindering steady loss decline. Conversely, training with only the NPO loss function results in rapid convergence and smooth loss curve, further highlighting the conflict between NPO and regularization. Despite this, NPO+GDR+KLR achieves stable loss value in later training stages, demonstrating its ability to effectively balance forgetting and retention. 0 and inal Weight perspective Figure 2 shows performance trend with an initial decline followed by an increase. We identify this turning point as the inflection point (step 165). To understand optimization dynamics around this point, we analyzed the angle between flattened parameter change vectors across training phases (Figure 4), where s2 s1 be the parameter change vector from step s1 to s2. The angle between 165 is approximately 70-85 degrees. This suggests that the initial phase overemphasizes forgetting, while significant shift in optimization direction occurs after the inflection point, where the balance between forgetting and retention has gradually been established. Conversely, the angle between the initial direction (P 165 ) and the overall optimization direction (P inal ) approaches near orthogonality (90 degrees). This indicates that overall training does not consistently follow the initial direction, and the initial \"forgetting\" emphasis is balanced by later retention optimization. 165 0 joint Merging operation averages parameters with consistent elected signs and discards discordant ones. This strategic approach effectively mitigates over-unlearning and further enhances the merged models resistance to Membership Inference Attacks (MIA)."
        },
        {
            "title": "6 Rethinking Unlearning",
            "content": "6.1 Drawbacks: Over-forgetting Phenomena Despite demonstrating effectiveness, our system still exhibits over-forgetting. Firstly, the unlearned model exhibits model collapse, frequently generating repetitive characters (e.g., \"6 6 6\"). This phenomenon arises from the training process itself, the model may find suboptimal but easy shortcut: generating repetitive outputs to reduce loss. Specifically, Task 2 involves digit-heavy dataset, so the model will take this high-frequency option as their outputs. Secondly, we observed forgetting of generic knowledge. We analyze question patterns of forget set to construct 50 common knowledge questions (e.g., \"What is the capital of France?\"), finding significant Knowledge Score drop (0.88 0.35) against vanilla baseline. These drawbacks are also observed in some studies Mekala et al. (2025); Xu et al. (2025), highlighting fundamental weakness of this paradigm. By primarily focusing on suppressing the original tokens, the model is constrained from discovering meaningful alternative outputs and consequently falls into shortcut of generating meaningless responses. What is Lorette Fuchsias 6 6 6 6 6 6 6... Forget Set Case: Question: email address? Answer: Retain Set Case: Question: Fredericka Amber? Answer: 1969-12-21 Generic Knowledge Case: Question: Eiffel Tower located? Answer: In which city is the 6 6 6 6 6 6 6... What is the birth date of"
        },
        {
            "title": "6.2 Limitations of Unlearning Evaluation",
            "content": "ROUGE-based metrics primarily measure how closely response matches an expected output rather than exact knowledge unlearning. For instance, different long response might still inadvertently leak sensitive information like an email address, yet escape detection by ROUGE-L due to its focus on textual overlap rather than content semantics. In this competition, separate metrics have been introduced (i.e., Regurgitation Score and Knowledge Score). However, they remain susceptible to superficial textual variations, where minor rephrasing can mask underlying retention of knowledge, thus undermining their ability to accurately evaluate unlearning effectiveness. Similarly, MIA Scores like Min-k% prove insufficient. Although our method achieves an almost optimal MIA score of 0.501, it still generates repetitive outputs that deviate from the base models behavior. Some studies (Duan et al., 2024; Meeus et al., 2024) cast doubt on MIAs reliability for LLMs, pointing to potential temporal or domain discrepancies in datasets. In this competition, while the forget set and retain set are derived from Wikipedia after the deadline of OLMOs training, subtle distribution shifts may still persist. Our local test on OLMo-7B-0724Instruct-hf yields an MIA AUC of 0.46, slightly misaligned with the official optimal score of 0.5, further highlighting these inconsistencies."
        },
        {
            "title": "6.3 Rethinking Unlearning’s Objectives",
            "content": "Recent studies (Xu et al., 2024; Zhou et al., 2024; Thaker et al., 2024; Cooper et al., 2024; Barez et al., 2025) present critical analyses of generative AI unlearning. These studies collectively reveal three fundamental limitations: (1) current unlearning methods remain impractical, (2) evaluations fail to assess the generalization capability of unlearned models, and (3) benchmarks encourage model to overfit the training set, creating an illusory forgetting. The root challenge lies in the absence of clearly defined and universally applicable unlearnIn pragmatic sense, we should ing objective. not set too many objectives for unlearning, such as Resilient to Relearning Attacks (Fan et al., 2025). Rather than treating unlearning as an ultimate solution for privacy and copyright issues, future research should develop on-demand unlearning and robust evaluation to address policy concerns."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduce an unlearning system via model merging. By combining two complementary models, it effectively achieves balanced forgetting and excellent knowledge preservation."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank ZJUKLAB for their invaluable support and resources throughout this research project."
        },
        {
            "title": "References",
            "content": "Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan OGara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José Hernandez-Orallo, Mor Geva, and Yarin Gal. 2025. Open problems in machine unlearning for ai safety. Yoshua Bengio, Sören Mindermann, Daniel Privitera, Tamay Besiroglu, Rishi Bommasani, Stephen Casper, Yejin Choi, Philip Fox, Ben Garfinkel, Danielle Goldfarb, Hoda Heidari, Anson Ho, Sayash Kapoor, Leila Khalatbari, Shayne Longpre, Sam Manning, Vasilios Mavroudis, Mantas Mazeika, Julian Michael, Jessica Newman, Kwan Yee Ng, Chinasa T. Okolo, Deborah Raji, Girish Sastry, Elizabeth Seger, Theodora Skeadas, Tobin South, Emma Strubell, Florian Tramèr, Lucia Velasco, Nicole Wheeler, Daron Acemoglu, Olubayo Adekanmbi, David Dalrymple, Thomas G. Dietterich, Edward W. Felten, Pascale Fung, Pierre-Olivier Gourinchas, Fredrik Heintz, Geoffrey Hinton, Nick Jennings, Andreas Krause, Susan Leavy, Percy Liang, Teresa Ludermir, Vidushi Marda, Helen Margetts, John McDermid, Jane Munga, Arvind Narayanan, Alondra Nelson, Clara Neppel, Alice Oh, Gopal Ramchurn, Stuart Russell, Marietje Schaake, Bernhard Schölkopf, Dawn Song, Alvaro Soto, Lee Tiedrich, Gaël Varoquaux, Andrew Yao, Ya-Qin Zhang, Fahad Albalawi, Marwan Alserkal, Olubunmi Ajala, Guillaume Avrin, Christian Busch, André Carlos Ponce de Leon Ferreira de Carvalho, Bronwyn Fox, Amandeep Singh Gill, Ahmet Halit Hatip, Juha Heikkilä, Gill Jolly, Ziv Katzir, Hiroaki Kitano, Antonio Krüger, Chris Johnson, Saif M. Khan, Kyoung Mu Lee, Dominic Vincent Ligot, Oleksii Molchanovskyi, Andrea Monti, Nusu Mwamanzi, Mona Nemer, Nuria Oliver, José Ramón López Portillo, Balaraman Ravindran, Raquel Pezoa Rivera, Hammam Riza, Crystal Rugege, Ciarán Seoighe, Jerry Sheehan, Haroon Sheikh, Denise Wong, and Yi Zeng. 2025. International ai safety report. Huajun Chen. 2024. Large knowledge model: Perspectives and challenges. Data Intelligence, 6(3):587 620. Jiaao Chen and Diyi Yang. 2023. Unlearn what you want to forget: Efficient unlearning for llms. A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen, Matthew Jagielski, Katja Filippova, Ken Ziyu Liu, Alexandra Chouldechova, Jamie Hayes, Yangsibo Huang, Niloofar Mireshghallah, Ilia Shumailov, Eleni Triantafillou, Peter Kairouz, Nicole Mitchell, Percy Liang, Daniel E. Ho, Yejin Choi, Sanmi Koyejo, Fernando Delgado, James Grimmelmann, Vitaly Shmatikov, Christopher De Sa, Solon Barocas, Amy Cyphert, Mark Lemley, danah boyd, Jennifer Wortman Vaughan, Miles Brundage, David Bau, Seth Neel, Abigail Z. Jacobs, Andreas Terzis, Hanna Wallach, Nicolas Papernot, and Katherine Lee. 2024. Machine unlearning doesnt do what you think: Lessons for generative ai policy, research, and practice. Pala Tej Deep, Rishabh Bhardwaj, and Soujanya Poria. 2024. Della-merging: Reducing interference in model merging through magnitude-based sampling. Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. 2024. Do membership inference attacks work on large language models? Ronen Eldan and Mark Russinovich. 2023. Whos harry potter? approximate unlearning in llms. Chongyu Fan, Jinghan Jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, and Sijia Liu. 2025. Towards llm unlearning resilient to relearning attacks: sharpness-aware minimization perspective and beyond. Rohit Gandikota, Sheridan Feucht, Samuel Marks, and David Bau. 2024. Erasing conceptual knowledge from language models. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Preprint. Tianle Gu, Kexin Huang, Ruilin Luo, Yuanqi Yao, Yujiu Yang, Yan Teng, and Yingchun Wang. 2024. Meow: Memory supervised llm unlearning via inverted facts. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022. Knowledge unlearning for mitigating privacy risks in language models. Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2023. Knowledge unlearning for mitigating privacy risks in language models. In ACL (1), pages 1438914408. Association for Computational Linguistics. Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, and Tat seng Chua. 2025. Anyedit: Edit any knowledge encoded in language models. Zhenhua Liu, Tong Zhu, Chuanyuan Tan, and Wenliang Chen. 2024a. Learning to refuse: Towards mitigating privacy risks in llms. Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. 2024b. Towards safer large language models through machine unlearning. In Findings of the Association for Computational Linguistics: ACL 2024, pages 18171829, Bangkok, Thailand. Association for Computational Linguistics. Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. In Advances in Neural Information Processing Systems, volume 35, pages 2759127609. Curran Associates, Inc. Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Xiachong Feng, and Bing Qin. 2024. Unveiling entity-level unlearning for large language models: comprehensive analysis. Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, and Yves-Alexandre de Montjoye. 2024. Sok: Membership inference attacks on llms are rushing nowhere (and how to fix it). Anmol Reddy Mekala, Vineeth Dorna, Shreya Dubey, Abhishek Lalwani, David Koleczek, Mukund Rungta, Sadid A. Hasan, and Elita A.A Lobo. 2025. Alternate preference optimization for unlearning factual knowledge in large language models. In Proceedings of the 31st International Conference on Computational Linguistics, pages 37323752, Abu Dhabi, UAE. Association for Computational Linguistics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, pages 5372853741. Curran Associates, Inc. Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, and Rahul Gupta. 2025. Lume: Llm unlearning with multitask evaluations. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024. Detecting pretraining data from large language models. Yash Sinha, Murari Mandal, and Mohan Kankanhalli. 2024. Unstar: Unlearning with self-taught antisample reasoning for llms. George Stoica, Pratik Ramesh, Boglarka Ecsedi, Leshem Choshen, and Judy Hoffman. 2024. Model merging with svd to tie the knots. Pratiksha Thaker, Shengyuan Hu, Neil Kale, Yash Maurya, Zhiwei Steven Wu, and Virginia Smith. 2024. Position: Llm unlearning benchmarks are weak measures of progress. Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, and Ningyu Zhang. 2024. To forget or not? towards practical knowledge unlearning for large language models. Chenxi Wang, Ziwen Xu, Mengru Wang, Xiang Chen, Shumin Deng, and Ningyu Zhang. 2025. Overview of the nlpcc 2024 shared task 10: Regulating large language models. In Natural Language Processing and Chinese Computing, pages 253263, Singapore. Springer Nature Singapore. Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, and Ningyu Zhang. 2025. Relearn: Unlearning via learning for large language models. Jie Xu, Zihan Wu, Cong Wang, and Xiaohua Jia. 2024. Machine unlearning: Solutions and challenges. IEEE Transactions on Emerging Topics in Computational Intelligence, 8(3):21502168. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Ties-merging: Resolving interference when merging models. Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by partitioning gradients. In Findings of the Association for Computational Linguistics: ACL 2023, pages 60326048, Toronto, Canada. Association for Computational Linguistics. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024a. Negative preference optimization: From catastrophic collapse to effective unlearning. In First Conference on Language Modeling. Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, and Suhang Wang. 2024b. Does your llm truly unlearn? an embarrassingly simple approach to recover unlearned knowledge. Shiji Zhou, Lianzhe Wang, Jiangnan Ye, Yongliang Wu, and Heng Chang. 2024. On the limitations and prospects of machine unlearning for generative ai. Haomin Zhuang, Yihua Zhang, Kehan Guo, Jinghan Jia, Gaowen Liu, Sijia Liu, and Xiangliang Zhang. 2024. Uoe: Unlearning one expert is enough for mixture-of-experts llms."
        },
        {
            "title": "A Detailed Setup",
            "content": "A.1 Detailed formulas This section introduces detailed formulas in this paper. Negative Preference Optimization: The loss function penalizes the model for generating outputs with negative preferences while maximizing outputs with positive preferences: LNPO = 2 β EDf (cid:20) (cid:18) log σ β log πθ(yx) πref(yx) (cid:19)(cid:21) (2) where πθ(yx) is the models output distribution and πref(yx) is the reference distribution. Here, σ is the sigmoid function and β is regularization parameter. Gradient Descent on Retain Set (GDR): Minimizes the loss for samples in retain set by updating parameters in the direction to the gradient of the loss function: θt+1 = θt ηθL(θt, Dr) (3) where θt represents the model parameters at step t, η is the learning rate, and θL(θt, Dr) is the gradient of the loss function at step t, calculated on the retain set Dr. KL Minimization on Retain Set (KLR): Minimizes the Kullback-Leibler divergence between the models output distribution and target distribution on retain set: Lklr = (cid:88) πθ(yi) log πθ(yi) πtarget(yiDr) (4) where LKL is the loss, πθ(yi) is the models output distribution for the i-th output token yi, and πtarget(yiDr) is the target output distribution for the i-th token yi conditioned on the retain set Dr. A.2 Detailed Implementation Table 3 summarizes the complete configuration parameters used in our experiments."
        },
        {
            "title": "B Related Work",
            "content": "LLM Unlearning The topic of unlearning in large language models (Chen, 2024) has recently attracted significant attention in the literature. One approach to unlearning is Gradient Ascent (Jang et al., 2023), which aims to maximize the loss function to facilitate forgetting. Another method, Negative Preference Optimization (NPO) (Zhang et al., 2024a), builds upon Direct Preference Optimization (DPO) (Rafailov et al., 2023), offering an alternative strategy for model unlearning. Various unlearning techniques have been proposed, including those presented by (Lu et al., 2022; Eldan and Russinovich, 2023; Yu et al., 2023; Chen Parameter Model1 Model2 batch_size gradient_accumulation num_epochs lr max_length weight_decay seed ga_ratio gd_ratio gk_ratio LoRA_r LoRA_alpha LoRA_dropout 1 4 5 1 104 256 0.01 42 0.4 0.4 0.2 32 32 0.05 2 4 5 1 104 256 0.01 42 0.3 0.3 0.4 32 32 0. Table 3: Complete Hyperparameters Configuration. and Yang, 2023; Wang et al., 2025; Gandikota et al., 2024; Jiang et al., 2025; Liu et al., 2024b; Zhuang et al., 2024). An alternative strategy, referred to as locate-then-unlearn, is exemplified by KnowUnDo (Tian et al., 2024) and SURE (Zhang et al., 2024b), which focus on knowledge localization before executing the unlearning process. Additionally, data-driven methods for unlearning have also been introduced, such as those proposed by (Jang et al., 2022; Ma et al., 2024; Liu et al., 2024a; Gu et al., 2024; Sinha et al., 2024; Xu et al., 2025; Mekala et al., 2025). Model Merging Training model for each task can be costly, but model merging offers solution to these challenges by combining multiple pretrained models. Model merging strategies include parameter averaging (Linear), singular value decomposition (SVD) for low-rank alignment, and feature concatenation (CAT). Advanced variants like TIES (Yadav et al., 2023) trim redundant parameters and resolve sign conflicts, while TIESSVD (Stoica et al., 2024) integrates SVD for refined fusion. DARE methods(Yu et al., 2024), and methods like DARE-TIES, DARE-linear introduce parameter dropout and rescaling, with extensions (DARE-TIES-SVD, DARE-linear-SVD) combining SVD for structured compression. The magnitude-prune (Deep et al., 2024) removes lowimpact weights, and its SVD variant (magnitudeprune-SVD) is further compressed via low-rank decomposition."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Zhejiang University"
    ]
}