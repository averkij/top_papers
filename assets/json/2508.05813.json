{
    "paper_title": "Optimization-Free Style Transfer for 3D Gaussian Splats",
    "authors": [
        "Raphael Du Sablon",
        "David Hart"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler."
        },
        {
            "title": "Start",
            "content": "Optimization-Free Style Transfer for 3D Gaussian Splats Raphael Du Sablon and David Hart East Carolina University hartda23@ecu.edu 5 2 0 2 7 ] . [ 1 3 1 8 5 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or ﬁne-tuning the splat while incorporating style information or optimizing feature extraction network on the splat representation. We propose reconstructionand optimization-free approach to stylizing 3D Gaussian This is done by generating graph structure splats. the splat representation. across the implicit surface of feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler. 1. Introduction 3D Guassian Splats (3DGS) [26] have quickly gained high amounts of interest in the research community for their ability to capture real world scenes with incredibly high visual ﬁdelity. This is all done while maintaining an intrinsic representation of splats that are easily described and stored. This representation, however, is not conducive to simple editing and modiﬁcation techniques that are common to other 3D representations such as 3D meshes. One such modiﬁcation includes style transfer, the task of transferring the low-level artistic properties of style image onto content representation. While this area has deep history for image-to-image style transfer [4, 5, 11, 25, 33, 34], the process of stylizing onto 3DGS has only recently been explored. Some works have presented stylization options for 3DGS, but they rely on starting the reconstruction process from scratch while including style features in the process [10, 23]. Some methods have aimed to increase the stylization speed by not requiring reconstruction, using Style and 3DGS Stylized 3DGS Figure 1. 3D Gaussian splat with geometrically complex content and an image containing distinct artistic style (Left) and 3D Gaussian splat of the same content after style transfer using our approach (Right). post processing techniques instead. G-style [30] performs special ﬁne-tuning on an existing 3DGS to increase convergence speed. StyleGaussian [36] can operate on an existing 3DGS, but it does require learning content feature extraction network on the 3DGS before the stylization is possible. In short, none of the current works are able to perform style transfer on an existing 3DGS without reconstructing or reoptimizing the 3DGS. In this work, we present new approach for style transfer for 3DGS that does not require reconstructing the splat, ﬁnetuning network, or any other form of optimization. It does so by building graph structure across the implicit surface of the 3DGS. This graph is constructed in such way as to allow standard image convolution networks to be applied to the surface [15]. Then, standard pretrained image style transfer network can be used [33] and the colors on the graph can be interpolated back to the original splats contained within the 3DGS scene. An example result is given in Fig. 1. Our approach was designed while considering many possible variations and thorough ablation study on the algorithm is provided in this work. Our approach has some immediately apparent advantages. First, our approach does not require access to the original images from which the splat was constructed. Second, since the style colors are stored with the individual splats in scene, the stylization can be saved to the splat ﬁle and is not viewer dependent. Third, our approach is signiﬁcantly faster than approaches that require reconstructing 3DGS from scratch. Our approach is also primarily CPU-based, allowing it to run quickly even on standard consumer-grade hardware. Additionally, we will show that our approach maintains high visual quality that is comparable to other state-of-the-art approaches. In summary are contributions are as follows: novel surface-based approach for stylizing 3DGS scenes without additional optimization or training. An ablation on techniques to improve the quality of results. Qualitative and speed comparisons to state-of-the-art 3DGS stylization approaches. 2. Related Work 2.1. Neural Radiance Fields and Gaussian Splats In the various approaches to 3D rendering, the development of effective neural networks rapidly led to new iterations of and improvements on classical methods [31, 45, 52]. In particular, the ﬁeld saw revolution with the introduction of NeRFs, Neural Radiance Fields, introduced by [37]. NeRF is able to represent geometrically complex scenes involving difﬁcult optical phenomena as ﬁve-dimensional radiance ﬁelds. key insight of NeRF is to use neural network as mechanism for storing the learned visual information of scene. The differentiability of the rendering process allows for optimization to be performed and for loss function to be deﬁned on the ﬁnal 2D view of the representation. particular feature of this algorithm is that it allows for rendering new views of 3D scenes, process termed novel view synthesis. Decades-old novel view synthesis techniques do exist [2], but as with other areas, the ﬁeld saw marked improvement with the development of modern machine learning techniques [16, 47, 48, 52]. 3D Gaussian Splatting for Real-Time Radiance Field Rendering [26] builds on the success of radiance ﬁelds while eliminating the use of neural network at render time. In doing so, Gaussian splatting enabled real-time rendering of novel-view synthesis at high resolutions and qualities with simply described representation. 3D Gaussian Splats (or simply splats) are point-cloud-like primitive that is differentiable, unstructured, and easily projected to 2D for rendering. An optimization algorithm for ﬁne-tuning the properties of Gaussian splats and novel rasterizer allows for back-propagation while also being fast to render high quality scenes. Alpha-blending of the Gaussians as they are projected to 2D ensures good textural detail and smooth geometries, at least in regions of high splat density [18]."
        },
        {
            "title": "This unique approach is of high interest",
            "content": "in the research community because of its incredible visual quality for novel view synthesis of real scenes. Many works have followed that explore improving the reconstruction process and representation [9, 19, 54]. Some works have also explored pruning and compressing 3D Gaussian Splats [7, 32, 38, 39, 41] or even editing them [22, 42, 50], but the ability to modify such representations is still very limited given that the location of individual splats do not necessarily correlate with true geometry or semantic regions of an object. 2.2. Style Transfer While some classical approaches exist [17], modern imageto-image style transfer begins with the foundational work of Gatys et al. [11]. This original approach treated style transfer as an optimization problem, where the content image was modiﬁed until it optimally matched style image at the features extracted from VGG network [46]. In the decade since, many new approaches have emerged for performing style transfer. These include those that improved on the original optimization approach [12], feed-forward techniques [25], autoencoder-based reconstruction methods [33, 34], vision transformer models [5, 51], and, most recently, diffusion-based setups [4, 57]. In addition to style transfer for images, other works have applied style in other contexts, such as for video [20, 24, 43, 53], 360 images [15, 43], multi-view setups [6, 13], 3D meshes [8, 15, 49], and even ﬂuid simulation [27]. Speciﬁcally, our work builds on SelectionConv [14, 15], which uses graph to apply style to 3D meshes and other nonstandard domains. 2.3. Style Transfer for 3D Gaussian Splats There has been considerable attention paid to attempting to translate the success of image-to-image style transfer to the 3D realm of radiance ﬁelds and Gaussian splatting. Zhang et al. [56] presented an early method for applying style transfer techniques to 3D radiance ﬁelds, with others works to follow [3, 35]. For Gaussian splats, most approaches focus on stylizing the splats as the scene is being reconstructed. The work of Sahora et al. [44] builds on the AdaIN layer [21] for aligning content and style features during reconstruction. VGG features are used by Zhang et al. [55], which constructs the loss function in way that evolves the style spatially along the objects represented in the 3D Gaussian. Jain et al. [23] leverage segmentation models to allow for style transfers to select portions the 3D Gaussian that are being styled. different niche is ﬁlled by Galerne et al. [10], which applied new method of addressing scale to tackle style transfers to ultra-high resolution 3D Gaussians. In comparison to the works mentioned, some approaches have attempted to stylize without requiring reconstructing Figure 2. Overview of our approach. graph construction pipeline takes an existing Gaussian Splat scence and samples it as point cloud. Normal vectors are approximated and points in the point cloud are connected using K-Nearest-Neighbors approach. local planar approximation of the graph at each point in the graph allows for convolution on the splat using 2D image CNN weights. style transfer network is then converted into the graph space using SelectionConv [15]. The resulting graph is interpolated back to existing splat center to modify the color values, generating the ﬁnal stylized result. The pipeline is shown in both technical version (Top) and full splat visualization (Bottom). the scene from scratch. The method of Kovacs et al. [30] can ﬁne-tune an already existing reconstruction to incorporate style features. Liu et al. [36] used non-reconstructionbased approach and is most comparable to this work. They introduced KNN 3D CNN model to reduce operations that break multi-view consistency and it employs unique feature extraction network that associates VGG features with individual splats that is view-independent and does not require reconstruction. This approach, however, is still not optimization-free since it requires training the feature extraction network and potentially the stylization network. In comparison, our graph construction approach is well deﬁned and can be applied to any Gaussian splat scene with no additional ﬁne tuning or training. It can also be applied to splat even if the original camera views used for reconstruction cannot be obtained. 3. Methodology The proposed method of stylizing 3D Guassian Splat (3DGS) builds on the surface-based CNN called SelectionConv [14, 15] and the style transfer network of Li et al. [33] for its foundational building block. This relies on constructing properly oriented graph along the surface. An overview of our approach is show in Fig. 2. The graph construction process, along with various improvements designed for 3DGS representations are described in the following sections. 3.1. Graph Construction Interpolated SelectionConv is framework for convolutional neural network operations that can operate on meshes It allows for neural networks originally or surfaces [15]. conﬁgured for and trained on 2D images to operate on complex 3D meshes by representing the mesh as graph network. Points sampled along the surface of the mesh become the nodes in the graph and edges are connected based on proximity. The graph is then fed into modiﬁed network that mimics the operations and orientation of standard image convolution. Interpolation then handles the variations in the relations between nodes on the mesh or surface. In applying this approach to 3DGS, key difference is immediately apparent: meshes have points that are ﬁxed to the surface while splats can be scattered volumetrically throughout the object. Although this is the case, we have found that 3DGS representations tend to place most splats near the surface of the object. This creates psuedo implicit surface that does not vary drastically in depth and is sufﬁcient for the process of style transfer. Moving forward with this assumption, we take the following steps to construct an oriented graph. Point Cloud Sampling: By default, the centers of each individual splat are treated as points in point cloud. These become the nodes in the graph. If additional points are desired, they can be sampled from the 3D Gaussian distributions and undesired points can be removed using ﬁltering. More details are provided in Secs. 3.3 and 3.4. Normal Estimation: The graph network employed by Interpolated SelectionConv requires up vectors and accurate normal vectors in order to perform the interpolation and weight assignment. This allows the orientation of the cardinal and ordinal directions to remain consistent for the convolution operation on both the 3D surface and on an ordered 2D pixel grid. For the normal vectors, we ﬁnd that standard point cloud normal-estimation methods are sufﬁcient. In this work, the Ball-Pivoting sampling method of [1] is used via Open3Ds implementation [58]. KNN Edges: Edges for the graph are generated using the common K-Nearest-Neighbors approach. This connects the points in the sampled point cloud to each other based on proximity. For this work, we use K-value of 16 when constructing the graphs. Selection through Planar Projection: To determine the oriented direction for each edge, local planar projection is created using the previously deﬁned normal and an up vector. An arbitrary up vector can be used or potentially one can be derived from geometric information implied by the Gaussian splat if desired. Graham-Schimdt orthogonalization between the up vector and normal vector create locally planar axes for each node in the graph. This allows deﬁning the local oriented direction of each edge relative to each point. Completing this ﬁnal step for each node provides graph with all needed information for performing the stylization. 3.2. Stylization Once the graph is properly constructed, stylization is straightfoward. The Interpolated SectionConv [15] graph network copies the weights of the style transfer network of Li et al. [33] and applies the surface-based stylization to the graph. The style network of [33] was chosen for its ability to be mimicked by graph convolutions. To clarify the stylization process, we summarize the method and again address the issue of how surface-based approach is handled in volumetric graph. In [15], the convolution operation for each layer was deﬁned over the graph adjacency matrix as X(k+1) = SmX(k)Wm (1) where represents given direction, Sm is the adjacency matrix containing edges in that given direction, X(k) is the current values at each node in the graph, and Wm is the copied learned weights from standard image CNN, and X(k+1) is the output values at each node in the graph. To determine which Sm each edge will be part of, the normal vector at each node and the global up vector are aligned with the surface using Graham-Schimdt orthogonalization. This creates local axis at each node. Simple dot products are then used to determine which local direction each edge most closely aligns with. In this original formulation, the assumption is made that, after the axis alignment to the surface, edges are approximately locally planar and depth component of the edge is Figure 3. 3D Gaussian splat under two different style transfers using our approach. negligible. This assumption holds well for most high resolution meshes and surfaces. For Gaussian splats, such an assumption can not hold as strictly since points are not restricted to the surface. The 3D variation of edge values for points in 3DGS is much more drastic than for standard mesh. Even with such variation, however, we empirically ﬁnd that the locally planar assumption is sufﬁcient and the stylization still works well. The implicit surface that naturally occurs during the creation of 3DGS places points in way that meets the original assumptions of the Interpolated SectionConv graph network. After the stylization graph network runs, the outputs are the new color values for each node in the graph. These values become the base color values for the individual splats in the 3DGS representation and the stylization is complete. An example of the ﬁnal result is shown in Fig. 3. 3.3. Controlling Resolution through Sampling In image style transfer, the resolution of the content image compared to the style image can affect the quality of the stylization [12]. For 3DGS scene, however, comparing resolutions is difﬁcult to deﬁne given the nature of the unstructured data. Empirically, we ﬁnd that the using single point Figure 4. toy example 3D Gaussian splat scene (Left) and high resolution scene of toys (Right). The originally rendered scene (Top) is spatially misrepresented when selecting only the centers of the provided splats (Middle). higher density point cloud can be generated by extensive sampling within each Gaussian (Bottom). The graph created from this new point cloud provides higher quality results when fed into the style transfer network. per individual splat is usually insufﬁcient to provide highquality results. Thus, we generate higher density point cloud by sampling from each individual splat. The sampling is intuitive to deﬁne since each individual splat is already 3D normal distribution. If additional sampling is desired, the user speciﬁes the amount of novel points needed. New points are then generated by taking existing splats and sampling points from their 3D distributions. Splats with larger size and higher opacity are given precedence when sampling new points. This is visualized in Fig. 4. The new points are used in the graph construction process and for the stylization. The stylized results on the higher resolution graph are interpolated back to the original splat centers for the ﬁnal color assignment. This super sampling approach dramatically improves the details in the stylized results. 3.4. Improving Stylization Quality through Point"
        },
        {
            "title": "Filtering",
            "content": "Depending on the quality of the 3DGS representation, outlier splats can appear and may throw off the implicit surface. This noisy placement of splats comes from poor images, occluded views, or insufﬁcient training time. These kinds of outlier splats are visualized in Fig. 5. To improve the results, we implement basic ﬁltering algorithm that considers the distance of point from the average location of its neighFigure 5. 3D Gaussian splat with high level of noise with the edges of the implicit surface (Green) and sections of noise highlighted (Red). Noisy regions demonstrate strong tenancy towards falling outside the surface. bors. Splats that lie threshold distance from the average of their neighbors were considered outliers and removed from the graph construction process. This improved normal estimation and overall stylization results. More details about the ﬁltering process are provided in the supplemental material. 4. Results The output stylizations of our approach for multiple styles and 3DGS scenes are given in Fig. 6. Example 3DGS scenes were taken from the Tanks and Temples dataset [28], public repositories [29], and scans generated using the Scaniverse app [40]. High quality 3D Gaussian splats demonstrated excellent style transfer results, with good color alignment and excellent content preservation. Lower quality 3D Gaussian splats suffered at times from poor mapping of style elements to content and regions of sparse data. We compare our outputs with those of G-style [30] and StyleGaussian [36] since they are post reconstruction approaches like our method. Fig. 7 includes two different views of two different 3DGS scenes under style transfer. The outputs of G-style, Style Gaussian, and our approach are given. The outputs of all methods have comparable quality but G-style has the best content and style preservation overall, likely due to the ability of G-style to move splats during the ﬁne-tuning process. We note, however, that our approach has better color alignment than StyleGaussian in most cases, though our approach tends to have less sharp features. Our approach does not always have the Figure 6. Qualitative results of our approach. Example outputs are given for each of the style images along the left column and content 3DGS along the top row. top visual quality but we again note that our technique does not require the original training views nor does it require specialized renderer as it saves the output to standard splat ﬁle. Example stylized splat ﬁles are provided in the supplemental material. 4.1. Speed Comparisons Another advantage of our stylization method is its short runtime on consumer-grade hardware. We compare timing information of our approach to other state-of-the-art 3DGS stylization methods. Because the nature of the stylizations differs, we separate the speeds into four speed categories: reconstruction, preprocessing, stylization, and rendering speeds. Reconstruction time includes the optimization process that generates the individual Gaussians given the input images of the scenes (some approaches include the stylization in this step). Preprocessing time includes any step needed to prepare the 3DGS scene for stylization (e.g. the graph construction process in our approach). Stylization time is the runtime of the incorporated stylization network or stylization process. Rendering time accounts for the time needed to generate stylized image when viewing the scene. To make these comparisons, we conducted our own timing experiments on an NVIDIA RTX 4090 desktop machine on the train 3DGS scene from Fig. 6. The resultIf an algorithm can be ing speeds are shown in Table 1. used on an already exisiting 3DGS, this was also noted as preoptimized in the table. As is shown, our method performs faster than existing approaches, with both preprocessing and stylization steps being completed in under minute. As an additional point, our approach is particularly advantageous for stylizing splats on lower-end hardware, since the graph construction process is implemented on the CPU with only the simple CNN-based stylization optionally running on the GPU rather than the CPU. This gives speeds around 1 minute even when no GPU is available. In comparison, fast methods like G-style [30] reported stylization speeds of 20-28 minutes for 360 scenes on an NVIDIA L4 GPU in Google Colab. Our approach is viable option for stylization on standard consumer-based hardware. Style Original g-style [30] StyleGaussian [36] Ours Figure 7. Comparison of our approach to other state-of-the-art techniques for different 3DGS scenes, views, and styles. Table 1. Speed comparisons on the Train example from Fig. 6. Experiments were conducted on an NVIDIA RTX 4090 GPU with 24GB VRAM and an Intel i9 CPU. We also ran our method on an Mac M2 processor with no MPS acceleration to showcase the low hardware requirements for our stylization technique. *The SGSST time for this scene with similar hardware is self-reported in the original work [10]. The code for StyleGaussian is written to retrain the style transfer network for each individual 3DGS scene and these results are presented as such. Method SGSST [10] G-style [30] StyleGaussian [36] Ours Ours (CPU Only) Ours (Mac M2) Reconstruction *33 min Preoptimized Preoptimized Preoptimized Preoptimized Preoptimized Preprocessing - 4.5 min 4.08 min + 3hrs 34.47 sec 34.22 sec 22.07 sec Stylization Rendering Approx. Total (I/O included) - 8.7 min 0.1 sec 22.65 sec 56.65 sec 44.98 sec Real-time Real-time Real-time Real-time Real-time Real-time *33 min 12.5 min 4 min + 3hrs 1 min 1.5 min 1.25 min 4.2. Ablation: Increased Sampling In addition to the presented results, we demonstrate the beneﬁt of including our super sampling technique when constructing the point cloud. An example 3DGS stylization, with and without super sampling, is provided in Fig. 8. The high quality 3DGS shown has considerable intricate details (e.g. legible lettering, indentation around button boundaries, holes in the drip tray). If the super sampling method is not used, the stylization output loses much of the details that were present in the original splat. Additionally, the style transfer itself shows high degree of blurring and loss in variations. In comparison, when the super sampling method is used, the stylization output preserves many ﬁne details, including lettering and shapes, and has improved variation in colors and contrast. 4.3. Ablation: Random Normals Given that our technique operates on psuedo implicit surface, it is worth investigating the need for accurate normals at all during the stylization. In even geometrically simple objects, however, the need for accurate normals is immediately apparent. Fig. 9 shows an example stylization of 3DGS of simple object. To highlight the impact of good accuracy, style transfer with random normals"
        },
        {
            "title": "Without Sampling With Sampling",
            "content": "Figure 8. high quality 3D Gaussian splat (Left) after style transfer using point cloud created from selecting the centers of individual Gaussians (Middle) and using point cloud with Gaussian Sampling to reduce information loss (Right) demonstrating the effectiveness of applying Gaussian Sampling. Lettering detail shown in bottom row. was compared with style transfer with Ball-Pivoting calculated normals. In the randomized normals example, the style transfer showed the characteristic blurriness of imperfect style transfer, likely due to the convolution operator not being able to preserve global orientation. These kinds of artifacts are consistently present in all stylized 3DGS with randomized normals. The style transfer with calculated normals, meanwhile, greatly improves the stylization, with the decorative elements remaining distinct and different sections of the object showing distinct elements of the style. 5. Conclusion The presented method is both novel and effective for performing stylization on 3D Gaussian splats. Additionally, its surface-based approach can run on preoptimized 3DGS scenes and is fast even on standard consumer hardware. Limitations of this approach include the inability to modify geometry during the stylization. Removing such limitation could allow for stylized Gaussians to more closely match characteristic features of the style image. An additional limitation includes the reliance on psuedo implicit surface. While we empirically found, in the 3DGS scenes we tested, that splats always tend to optimize near the surface of an object, more quantitative tests would need to be conducted to verify the universality of this claim. Additional considerations would need to be made for using our approach when psuedo implicit surface is not present. Future work could include changing the underlying stylization network. We used the CNN-based stylization approach of Li et al. [33] because it was originally incorOriginal 3DGS and Style Image Style Transfer with Randomized Normals Style Transfer with Calculated Normals Figure 9. 3D Gaussian splat of simple object (Top). Accurately calculating normals is shown to be an important step of the style transfer since stylization with randomized normals (Middle) is excessively blurry compared to stylization with normals calculated with the Ball-Pivoting algorithm (Bottom). porated into Interpolated SelectionConv [15], the surfaceImprovements could occur based method we build on. by incorporating modern vision transformer or diffusionbased style transfer network. Such models would require considerable modiﬁcations to the underlying graph structure and could be an avenue for future research."
        },
        {
            "title": "References",
            "content": "[1] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Claudio Silva, and Gabriel Taubin. The ball-pivoting algorithm for surface reconstruction. IEEE transactions on visualization and computer graphics, 5(4):349359, 2002. 4 [2] Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 423432. Association for Computing Machinery, 1993. 2 [3] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, WeiSheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via implicit representation and hypernetwork. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 14751484, 2022. 2 [4] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting largescale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 87958805, 2024. 1, 2 [5] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: ImIn Proceedings of age style transfer with transformers. the IEEE/CVF conference on computer vision and pattern recognition, pages 1132611336, 2022. 1, 2 [6] Donal Egan, Martin Alain, and Aljosa Smolic. Light ﬁeld style transfer with local angular consistency. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 23002304, 2021. 2 [7] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps, 2023. [8] Jakub Fiˇser, Ondˇrej Jamriˇska, Michal Lukaˇc, Eli Shechtman, Paul Asente, Jingwan Lu, and Daniel Sykora. Stylit: illumination-guided example-based stylization of 3d renderings. ACM Trans. Graph., 35(4), 2016. 2 [9] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20796 20805, 2024. 2 [10] Bruno Galerne, Jianling Wang, Lara Raad, and Jean-Michel Morel. Sgsst: Scaling gaussian splatting styletransfer. arXiv preprint arXiv:2412.03371, 2024. 1, 2, 7 [11] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. 1, 2 [12] Leon Gatys, Alexander Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. Controlling perceptual factors in neural style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39853993, 2017. 2, 4 [13] David Hart, Jessica Greenland, and Bryan Morse. Style In 2020 IEEE Winter transfer for light ﬁeld photography. Conference on Applications of Computer Vision (WACV), pages 99108, 2020. [14] David Hart, Michael Whitney, and Bryan Morse. Selectionconv: convolutional neural networks for non-rectilinear image data. In European Conference on Computer Vision, pages 317333. Springer, 2022. 2, 3 [15] David Hart, Michael Whitney, and Bryan Morse. Interpolated selectionconv for spherical images and surfaces. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 321330, 2023. 1, 2, 3, 4, 8 [16] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):115, 2018. 2 [17] Aaron Hertzmann. Painterly rendering with curved brush strokes of multiple sizes. In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, page 453460, New York, NY, USA, 1998. Association for Computing Machinery. [18] Yueyu Hu, Ran Gong, Qi Sun, and Yao Wang. Low latency point cloud rendering with learned splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 57525761, 2024. 2 [19] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance ﬁelds. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 2 [20] Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. Real-time In 2017 IEEE Conference neural style transfer for videos. on Computer Vision and Pattern Recognition (CVPR), pages 70447052, 2017. 2 [21] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. 2 [22] Vishnu Jaganathan, Hannah Hanyun Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, and Zsolt Kira. Ice-g: Image conditional editing of 3d gaussian splats. arXiv preprint arXiv:2406.08488, 2024. 2 [23] Sahil Jain, Avik Kuthiala, Prabhdeep Singh Sethi, and Prakanshul Saxena. Stylesplat: 3d object style transfer with gaussian splatting. arXiv preprint arXiv:2407.09473, 2024. 1, [24] Ondˇrej Jamriˇska, ˇSarka Sochorova, Ondˇrej Texler, Michal Lukaˇc, Jakub Fiˇser, Jingwan Lu, Eli Shechtman, and Daniel Sykora. Stylizing video by example. ACM Trans. Graph., 38 (4), 2019. 2 [25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694711. Springer, 2016. 1, 2 [26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance ﬁeld rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 2 [27] Byungsoo Kim, Vinicius C. Azevedo, Markus Gross, and Barbara Solenthaler. Lagrangian neural style transfer for ﬂuids. ACM Trans. Graph., 39(4), 2020. 2 [28] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017. 5 [29] Vlad Kobranov. splats. https://huggingface.co/ VladKobranov/splats, 2024. Accessed: 2025-07-15. [30] [31] Aron Samuel Kovacs, Pedro Hermosilla, and Renata Raidou. G-style: Stylized gaussian splatting. In Computer Graphics Forum, page e15259. Wiley Online Library, 2024. 1, 3, 5, 6, 7 Aron Samuel Kovacs, Pedro Hermosilla, and Renata Raidou. Surface-aware mesh texture synthesis with pretrained 2d cnns. In Computer Graphics Forum, page e15016. Wiley Online Library, 2024. 2 [32] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance ﬁeld. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2171921728, 2024. 2 [33] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang. Learning linear transformations for fast image and video style transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3809 3817, 2019. 1, 2, 3, 4, 8 [34] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. Advances in neural information processing systems, 30, 2017. 1, [35] Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, and Eric Xing. Stylerf: Zero-shot 3d style transfer of neural radiance ﬁelds. 2023. 2 [36] Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, and Shijian Lu. Stylegaussian: Instant 3d style transfer with gaussian splatting. In SIGGRAPH Asia 2024 Technical Communications, pages 14. SIGGRAPH, 2024. 1, 3, 5, 7 [37] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [38] Wieland Morgenstern, Florian Barthel, Anna Hilsmann, and Peter Eisert. Compact 3d scene representation via selforganizing gaussian grids. arXiv preprint arXiv:2312.13299, 2023. 2 [39] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Smaller and faster gaussian splatting with vector quantization. arXiv preprint arXiv:2311.18159, 2023. 2 [40] Niantic. Scaniverse: 3D Scanner. Mobile App, 2021. Available at: https : / / apps . apple . com / us / app / scaniverse - 3d - scanner / id1541433223 and https://play.google.com/store/apps/ details?id=com.nianticlabs.scaniverse. 5 [41] Simon Niedermayr, Josef Stumpfegger, and Rudiger Westermann. Compressed 3d gaussian splatting for accelerated In Proceedings of the IEEE/CVF novel view synthesis. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1034910358, 2024. 2 [42] Francesco Palandra, Andrea Sanchietti, Daniele Baieri, and Emanuele Rodol`a. Gsedit: Efﬁcient text-guided editing of 3d objects via gaussian splatting, 2024. 2 [43] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox. Artistic style transfer for videos and spherical images. International Journal of Computer Vision, 126(11):11991219, 2018. 2 [44] Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Doand Daniel CrearXiv preprint minik Muhle, Tarun Yenamandra, mers. arXiv:2403.08498, 2024. Gaussian splatting in style. [45] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids. Advances in Neural Information Processing Systems, 35:3399934011, 2022. 2 [46] Karen Simonyan. Very deep convolutional networks arXiv preprint large-scale image recognition. for arXiv:1409.1556, 2014. [47] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24372446, 2019. 2 [48] Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3dstructure-aware neural scene representations. Advances in Neural Information Processing Systems, 32, 2019. 2 [49] Daniel Sykora, Ondˇrej Jamriˇska, Ondˇrej Texler, Jakub Fiˇser, Michal Lukaˇc, Jingwan Lu, and Eli Shechtman. StyleBlit: Fast example-based stylization with local guidance. Computer Graphics Forum, 38(2):8391, 2019. 2 [50] Cyrus Vachha and Ayaan Haque. Instruct-gs2gs: Editing 3d gaussian splats with instructions, 2024. 2 [51] Xiaolei Wu, Zhihao Hu, Lu Sheng, and Dong Xu. Styleformer: Real-time arbitrary style transfer via parametric style composition. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1459814607, 2021. 2 [52] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Instantmesh: Efﬁcient 3d Shenghua Gao, and Ying Shan. mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [53] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video In Proceedings of with artistic generation and translation. the Computer Vision and Pattern Recognition Conference (CVPR), pages 26302640, 2025. 2 [54] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19447 19456, 2024. 2 [55] Dingxi Zhang, Yu-Jie Yuan, Zhuoxun Chen, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, and Lin Gao. Stylizedgs: Controllable stylization for 3d gaussian splatting. arXiv preprint arXiv:2404.05220, 2024. 2 [56] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic radiance ﬁelds. In European Conference on Computer Vision, pages 717733. Springer, 2022. 2 [57] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1014610156, 2023. 2 [58] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: modern library for 3D data processing. arXiv:1801.09847, 2018. Optimization-Free Style Transfer for 3D Gaussian Splats - Supplemental Materials 1. Point Filtering In practical sense, individual Gaussian splats tend to lie along implicit geometric surfaces, or at the very least tend to cluster rather than occupying isolated points in space. In most situations, any Gaussian splats that are highly isolated are likely to be noisy data points or artifacts left over from the training steps when the representation was ﬁrst generated from image data. To reduce this noise, ﬁltering mechanism is implemented as preprocessing step to our graph construction pipeline. In addition to being visually unappealing, these noisy data potentially distort or even obscure content and reduce the accuracy of style transfers. With the assumption that most geometric objects tend to be contiguous and will be represented by splats that roughly follow along an implicit surface, we conclude that the more isolated point is, and the more distant it is from any implicit surface, the more likely it is to be noisy data point and candidate for ﬁltering out. Handily, both isolation from neighboring points and distance from an implicit surface can be calculated at the same time in our method. The ﬁltering method is as follows: the Gaussian splat is converted to sparse point cloud comprised of the central points of each individual Gaussian splat, which we deﬁne as set . For each , the KNN algorithm is used to determine the neighborhood about that point. In that neighborhood, the midpoint pm is calculated as the mean of all points in the neighborhood. The distance from the neighborhood average is then simply pm: = (p pm) where pm = 1 jN pj Figure 1 provides visual demonstration of this mathematical operation. The calculated distance for isolated points is much greater than the of points in or on the implicit surface or for points that are in tightly clustered groups. Figure 2 visualizes this phenomenon with toy example. The ﬁltering process then simply ﬁlters out the points with the top distances according to user deﬁned percentile. Filtering those points reduces the noise in the 3D Gaussian splat and better represents the implicit surface. This ﬁltering showed high degree of reliability in segregating between individual Gaussian splats centered on points on and along the surface of objects being represented and those with large offsets from that surface. Some well-trained 3D Gaussian splats from public repositories required no ﬁltering. Other publicly available 3DGSs, and in particular usergenerated Gaussian splats, do require ﬁltering. User-generated splats suffered from low training times and low-resolution reference images, and thus contained high percentage of artifacts and noise. To study the efﬁcacy of our ﬁltering approach, we examined two different cases. The ﬁrst was test case constructed by generating geometrically simple Gaussian splat containing noisy points. The second case involved user-generated Gaussian splats. For the ﬁrst case, programmatically-derived Gaussian splat made up of 5000 equal sized Gaussians distributed at equidistant points along the surface of unit sphere was generated. This was seeded with an additional 500 Gaussian splats centered on points offset from the surface. Offsetting was accomplished by randomly dispersing the points within 2x2x2 volume centered on the unit sphere. Our ﬁltering approach was applied to remove points not along the surface of the sphere. This obtained ﬁltering accuracy of 98.87%. Results are shown in Figure 3. In the second case, the Scaniverse mobile app was used to create 3D Gaussian splat of household object, in this case chair. This ﬁle was found to be particularly noisy and so was an excellent candidate for this study. Before and after ﬁltering results are presented in Figure 4. As is shown, isolated points are preferentially ﬁltered, while points along the implicit surface of the object, in this case chair, are not ﬁltered. One limitation of this approach, however, is the inability to ﬁlter points that are noisy but tightly clustered. 1 Figure 1. Within the point cloud derived from 3D Gaussian splat, points in neighborhood about some selected point are used to determine the mean point pm from which the distance is derived. Insofar as is on or within the implicit surface, should be small. The value of increases when is at greater distance from other points in its neighborhood, i.e when does not lie in or on the implicit surface. Figure 2. spherical point cloud with large number of points added at random distances from the surface to simulate noise (Left) and the same point cloud with cones representing the vectors between pm and (Right) demonstrating how the distance between pm and strongly segregates points close to the surface from those far from the surface. Figure 3. 3D Gaussian of sphere (Left) with large number of points added at random distances from the surface to simulate noise (Middle) and the same after our ﬁltering approach (Right) Original 3DGS Filtered 3DGS Original 3DGS as Point Cloud Filtered 3DGS as Point Cloud Figure 4. 3D Gaussian splat that contains large number of artifacts (Top) and noise both before (Left) and after (Right) being ﬁltered. Simpliﬁed representations of the 3DGS formed by creating point cloud with the central point of each individual Gaussian splat (Bottom) demonstrate how isolated points and points well-outside the implicit surface are ﬁltered."
        }
    ],
    "affiliations": [
        "East Carolina University"
    ]
}