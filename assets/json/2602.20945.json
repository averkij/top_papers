{
    "paper_title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
    "authors": [
        "Taiqiang Wu",
        "Zenan Zu",
        "Bo Zhou",
        "Ngai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization."
        },
        {
            "title": "Start",
            "content": "The Art of Efficient Reasoning: Data, Reward, and Optimization February 25, 2026 *, Zenan Xu Taiqiang Wu The University of Hong Kong, *, Bo Zhou , Ngai Wong LLM Department, Tencent chaysezhou@tencent.com, nwong@eee.hku.hk https://wutaiqiang.github.io/project/Art 6 2 0 2 4 2 ] . [ 1 5 4 9 0 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows twostage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization."
        },
        {
            "title": "Introduction",
            "content": "(LLMs), language models Large such as Qwen3 (Yang et al., 2025) and DeepSeek-R1 (Guo et al., 2025), have revolutionized the field of natural language processing (NLP) due to their superior reasoning capabilities. One key insight for such success is the consistently scaled Chainof-thought (CoT) thinking during inference (Snell et al., 2024). Despite the advancement, longer *Equal contributions. Corresponding authors. Figure 1: General pipeline for efficient reasoning via RL. The key is to promote short and accurate thinking trajectories via reward design. In this paper, we provide systematic insights ( ) considering data, reward, and optimization. CoT also introduces significant trade-offs, such as high latency for real-world deployments (Sui et al., 2025; Wu et al., 2025a). To address this issue, one mainstream method is to incentivize efficient reasoning via Reinforcement Learning (RL) with reward shaping (Ma et al., 2024; Kimi et al., 2025; Liu et al., 2025). As shown in Figure 1, the core idea is to incentivize efficient reasoning by allocating the rewards based on rollout length and correctness. For instance, one important principle is that shorter correct CoTs should receive higher rewards than longer correct CoTs (Yeo et al., 2025). However, previous methods are almost exclusively focused on reward design (Hou et al., 2025; Liu et al., 2025), while overlooking the broader training recipe, including data composition and optimization strategy. In this paper, we propose to systematically investigate the mechanics of efficient reasoning in unified experimental protocol. Our analysis reveals that the training dynamics follow two-stage paradigm, i.e., 1) length adaptation, where the model rapidly adapts to token constraints; and 2) reasoning refinement, where it optimizes performance within the length scope. For comprehensive observations, we advocate for more fine-grained metrics. Specifically, we propose to compare the length distribution conditioned on correctness for the training prompts. Meanwhile, for the downstream benchmarks, we argue to record the performance across wide spectrum of token budgets ranging from 2k to 32k. The effectiveness of different strategies is budget-dependent, exhibiting distinct or even contradictory behaviors. Importantly, we find that the learned length bias can be generalized across domains, i.e., training on mathematical prompts works well on the code task. Through extensive ablation studies, we further deconstruct the impact of data difficulty, rollout number, reward, and optimization strategies. Notably, we find that training on relatively easier prompts provides denser positive reward signal, which is essential for stable reasoning distillation. More rollouts contribute to better performance, but also bring heavier training costs. For the reward assignment, we compare the strategy to assign negative reward or mask corresponding rollouts. Moreover, we further explore the off-policy strategy with different staleness to speed up the reasoning refinement stage. We distill all the findings into valuable insights and practical guidelines, and evaluate them on the Qwen3 series LLMs. In summary, our contributions are as follows: We identify and characterize the two-stage paradigm for efficient reasoning, i.e., length adaptation followed by reasoning refinement. We introduce fine-grained metrics, providing more comprehensive understanding of training dynamics. We provide systematic exploration of the training recipe, offering practical insights into data, reward, and optimization that significantly improve the efficiency of CoT models."
        },
        {
            "title": "2 Preliminary",
            "content": "2.1 Experimental Setup Prompts. RL methods have been demonstrated as an effective way for reasoning. Given an input prompt from dataset D, the LLM policy πθ generates set of reasoning trajectories (rollouts) {y1, y2, . . . , yN }. The objective is to update πθ using policy gradients derived from reward signals upon rollouts. Data quality is critical for LLM reasoning (Guo et al., 2025). Therefore, we employ the popular DeepScaleR as training prompts1. 1Dataset link Reward Engineering. In standard outcomesupervised RL, the reward function focuses solely on correctness. Let I() denote the indicator function which returns 1 if the condition holds and 0 otherwise. The vanilla reward is defined as: Rvanilla(x, yi) = I(yi is correct). (1) To enforce efficient reasoning, we apply reward shaping to incentivize concise yet accurate rollouts. In this work, we select the truncation strategy as baseline with RT (x, yi) = I(yi is correct) I(L(yi) LT ), (2) where L(yi) denotes the token length of the i-th rollout, and LT represents the target length. We first compare this approach against various baselines such as Kimi-1.5 (Kimi et al., 2025) and Laser (Liu et al., 2025). Further details are provided in Appendix A. Evaluation Protocol. To capture the nuances of efficient reasoning, we advocate for more finegrained metrics as follows: Training dynamics: We monitor the length distribution conditioned on correctness to visualize how the model trades off verbosity for precision. Budget-aware benchmarking: For downstream tasks, we report performance across wide spectrum of inference token budgets (B {2k, 4k, 8k, 16k, 32k}). We report Pass@8 and Mean@8 metrics across standard mathematical reasoning benchmarks: AIME25 (AIME, 2025), AMC (AMC, 2025), MATH-500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), and Olympiad Bench (He et al., 2024). Additionally, we assess code generation capabilities using LiveCodeBench (LCB) (Jain et al., 2024). as Implementation. We Training use DeepSeek-R1-Distill-Qwen-1.5B the backbone model. RL training is performed using Group Relative Policy Optimization (GRPO) (Shao 6 et al., 2024). The learning rate is set to 1 10 with clip-high ratio of 0.28 following Yu et al. (2025). During the rollout phase, we use batch size of 128 and sample = 8 trajectories per prompt with maximum length of 16k (LR = 16k). The target length LT is 4k. Figure 2: Training dynamics of various reward shaping methods on DeepSeek-R1-Distill-Qwen-1.5B. All of them follow the two-stage paradigm. The behaviors are distinct when evaluated under different token budgets. 2.2 Two-stage Paradigm As illustrated in Figure 2, the training dynamics exhibit two-stage paradigm: Stage I: Length Adaptation. The optimization of constraint satisfaction dominates this initial phase. Driven by the length penalty, the model rapidly adjusts its output distribution to avoid zeroreward truncation. As shown in the Rollout Length curves, the average token consumption undergoes precipitous decline (e.g., from 6k to 2k), exhibiting an exponential decay pattern. Simultaneously, the Policy Entropy decreases significantly, indicating that the model is converging towards subspace of shorter and valid trajectories. Stage II: Reasoning Refinement. Once the rollout length stabilizes within the target budget, the training enters stationary phase regarding length, shifting focus to performance optimization. In this stage, the length curves plateau, demonstrating that the model has successfully adapted to the hard constraints on output length. Crucially, the performance metrics (e.g., Mean@8) continue to evolve or recover. At the same time, the Policy Entropy increases with such exploration. This indicates that the model is learning to increase the information density of each token to improve accuracy without violating the length budget. 2.3 Distinct Behaviors across Token Budgets Figure 2 also reveals that model behaviors are highly sensitive to the token constraints, exhibiting distinct and even contradictory trends. Under strict budget (2k), performance is dominated by length adaptation. Aggressive penalties (e.g., Kimi) excel here by forcing the model to fit the narrow context window. However, under generous budget (32k), such strategy suffers from reasoning collapse issue. In contrast, Laser exhibits U-shaped trajectory at 32k, which initially drops due to compression, but subsequently recovers through reasoning refinement. This decoupling phenomenon highlights critical trade-off: overoptimizing for efficiency can severely harm the upper-bound reasoning capability, necessitating the multi-budget evaluation protocol (Qi et al., 2025). Figure 3: Performance training on all prompts and easy/hard counterparts (rollout LR = 16k, target LT = 4k). 2.4 Generalization across Domain As detailed in Figures 8 and 9, the dynamic on code task (i.e., LiveCodeBench) is similar to other mathematical tasks, though the training prompts are math-related only. For example, on the LCB benchmark, the Kimi strategy performs best under 2k budget but is comparable at larger budgets, which is similar to other benchmarks. The similarity proves that learned length bias can be generalized across domains. Moreover, the following experiments, detailed in this paper, also show the same trend."
        },
        {
            "title": "3 Experiments and Guidelines",
            "content": "In this section, we systematically investigate the impact of training prompts, reward assignments, and optimization strategies. Since the performance is comparable, we employ the truncation for the following analysis. 3.1 Data: Prompt and Rollout Impact of Prompt Difficulty. The difficulty of training prompts plays pivotal role in determining the density of positive reward signals. To investigate this, we split DeepScaleR prompts based on the pass rate over = 8 rollouts into DeepScaleREasy (pass rate > 0.5) and DeepScaleR-Hard (pass rate 0.5). As illustrated in Figure 3, the training dynamics exhibit stark differences. Training exclusively on hard prompts results in catastrophic failure. The policy entropy spikes drastically, and the rollout length collapses prematurely. Consequently, downstream performance metrics (e.g., Mean@8 on AMC and Olympiad Bench) degrade significantly. This suggests that when the model struggles to generate correct answers, the RL signal becomes dominated by the length penalty on incorrect rollouts, leading to reasoning collapse. We attribute such an issue to the sparsity of positive samples, which leads to the overfitting on short output length. Conversely, training on the easier counterpart yields the most stable trajectory. The policy entropy remains low and stable, indicating consistent positive reinforcement. The rollout length adapts smoothly to the target budget. Crucially, despite training on easy prompts, the performance on relatively tough tasks (e.g., AIME25) is comparable to (or even slightly exceeding) training on the full dataset. Please refer to Appendix for results and analysis under more settings. Appendix further indisimply scaling . It is worth noting that while increasing improves performance, it also introduces heavier computational overhead. Please refer to Appendix for results under more settings. Insights towards Training Data: The key is to ensure sufficient and effective rewards. Training on easier prompts allows LLMs to focus on length reduction without compromising performance. Larger rollout would be better if computational resource allows. 3.2 Reward on Negative Rollouts Strategy Vanilla -I -L&C -L&C-S&I -L&C-L&I Correct Incorrect Short Long Short Long 1 1 1 1 1 0 0 - - - 0 - 0 - 0 0 - 0 0 - Table 1: Reward for different strategies on negative rollouts. denotes masking out. The art of RL is to utilize the negative signals. In the standard truncation strategy (denoted as Vanilla), both incorrect responses and overlong correct responses are treated as negative samples (R = 0). An alternative way is to mask these negative samples rather than setting the reward to 0. To investigate this, we conduct fine-grained ablation study by masking specific subsets of negative rollouts, as detailed in Table 1. As illustrated in Figure 5, the training dynamics reveal that different masking strategies exhibit distinct behaviors. In short, improper strategies will lead to unintended consequences, categorized into three distinct failure modes: 1) The trap that short is correct (-I, -L&C-S&I). When the correctness is coupled with length, the model will be misled by such incorrect causal relationships. For masking all incorrect rollouts (-I), we only penalize overlong and correct rollouts. The training signal only contains a) short and correct rollouts with positive reward and b) overlong and correct rollouts with negative reward. In this way, the model would hack this bias to generate short output. Figure 4: Performance with various rollouts using DeepScaleR-Easy. cates cases for reasoning collapse from baselines and paradigm shift from our strategy. Impact of Rollout Number . Given that the density of valid reward signals is crucial, we further investigate the impact of the rollout number . We set {8, 12, 16, 24} with rollout limit LR = 16k and target length LT = 4k on the DeepScaleR-Easy prompts. As shown in Figure 4, increasing yields observable benefits that significantly speed up the Length Adaptation phase. With larger N, it is easier to discover short and correct trajectories, promoting the length curve (e.g., = 24, red line) to decay faster than the baseline (N = 8, blue line), though all settings converge to similar length floor. Meanwhile, larger leads to more robust Reasoning Refinement stage. In mathematical benchmarks, the model recovers its reasoning capabilities faster and achieves higher asymptotic Mean@8. However, this advantage is task-dependent. On the LiveCodeBench (LCB) coding task, the performance gap between = 8 and = 24 is marginal, suggesting that the complexity of code generation may require distinct exploration strategies beyond Figure 5: Performance for various reward strategies on negative rollouts (rollout LR = 16k, target LT = 4k, = 24). We also visualize LR = 4k, LT = 4k for comparison. As shown by the orange line, the policy entropy explodes after 400 steps and rollout length collapses precipitously. The model abandons reasoning entirely to satisfy the length constraint. Meanwhile, when masking overlong correct and short incorrect rollouts (-L&C-S&I), the dynamics are similar. The training signal only contains a) short and correct rollouts with positive reward and b) overlong and incorrect rollouts with negative reward. 2) Short rollouts only. particularly interesting phenomenon occurs in the strategy -L&C-L&I (red line), where we mask all overlong trajectories. It means that the LLMs are optimized with short correct and short incorrect rollouts exclusively. In this setting, the overlong outputs are masked, without either positive or negative rewards. After 200 steps (reasoning refinement stage), the LLMs hack this and begin to generate overlong outputs. Interestingly, these outputs are almost incorrect, but the models do not collapse. 3) Do not penalize overlong but correct rollouts (-L&C). The -L&C strategy (green line) masks overlong but correct rollouts instead of penalizing them. On the downstream benchmarks, the LLMs will generate longer outputs and also outperform the vanilla baseline, indicating trade-off for length control and performance. Length of Negative Samples. Additionally, we compare these complex shaping strategies against simple baseline sampling at target length (i.e., LR = LT = 4k, brown line). Compared to Vanilla (LR = 16k, LT = 4k), the positive samples are roughly the same (correct rollouts that are less than 4k) while the negative samples are much shorter (4k vs. 6k). We can observe that it achieves the optimal Pareto frontier. We attribute such success to avoiding the harmful explicit length bias trap that short is correct. Typically, the positive rollouts are shorter than negative ones, which implictly encourges the model to be short yet accurate. Insights towards Reward on Negative Rollouts: Not penalizing overlong correct rollouts leads to higher performance, but also slightly longer outputs. Sampling at target length (LR = LT ) achieves better trade-off via avoiding the length trap. Figure 6: Performance for off-policy strategy with various staleness (i.e., 2,4,8,16). 3.3 Off-policy Optimization Off-policy optimization via introducing staleness can significantly accelerate the training process but also introduces instability (Zheng et al., 2025; Huang et al., 2025). Meanwhile, we can observe that the performance keeps growing at the reasoning refinement stage. Hence, we conduct experiments with varying staleness degrees {2, 4, 8, 16} using the robust setting derived from our previous guidelines (LR = LT = 4k, = 24). As illustrated in Figure 6, the results reveal trade-off between performance and stability. Firstly, the Length Adaptation stage is significantly shortened. As staleness increases, the rollout length decays more rapidly, allowing the model to satisfy the token budget earlier. After that, the model enters the Reasoning Refinement stage and learns quickly. As result, performance on downstream tasks shows that high-staleness models (e.g., = 16) achieve higher accuracy than the on-policy baseline. Contrary to findings that excessive off-policy updates lead to catastrophic model collapse, our experiments maintain high performance. We attribute this robustness to our optimized setup (easy prompts and large ), which ensures sufficient and effective reward signals. However, larger staleness also introduces potential instability risks: i) Entropy explosion: As shown in the Policy Entropy subplot, high staleness (e.g., 16) causes dramatic surge in entropy after 400 steps. For the on-policy baseline, the entropy remains almost the same as initialization. ii) Length rebound: The positive rollout length on training prompts under high staleness begins to drift upwards again, indicating that the policy model is struggling to balance efficiency and effectiveness. Insights towards Off-policy Optimization: Appropriate staleness can accelerate convergence without harming accuracy (before 800 steps), but also introduces latent instability, manifested as rising entropy and uncontrolled length drift. We suggest the onpolicy strategy, especially for larger LLMs, which are more fragile."
        },
        {
            "title": "4 Extensive Analysis",
            "content": "4.1 Evaluated on More LLMs To verify the universality of our derived to guidelines, we evaluation extend our Method Mean@8 Pass@8 Length Qwen3-0.6B Vanilla Ours (step 640) 13.33 24.58 Qwen3-1.7B Vanilla Ours (step 560) 35.00 38.75 26.67 36.67 60.00 60.00 14.9k 8.9k 17.7k 11.2k while, our approach improves the Pass@8 to 36.67 compared to the vanilla score of 26.67. similar trend is observed in larger models. For instance, on Qwen3-4B-Instruct-2507, our method maintains superior Pass@8 of 70.00 and increases the Mean@8 from 45.42 to 46.67, while drastically compressing the length from 9.1k to roughly 4.8k. Qwen3-4B-Instruct-2507 4.2 Case Study Vanilla Ours (step 1440) 45.42 46.67 66.67 70.00 9.1k 4.8k Qwen3-4B-Thinking-2507 Vanilla Ours (step 200) 75.83 76.25 Qwen3-8B Vanilla Ours (step 100) 65.83 67. 90.00 86.67 86.67 83.33 20.9k 16.0k 17.9k 12.8k To qualitatively validate the impact of our strategy, we compare the reasoning trajectories of the vanilla and optimized models. Please refer to Appendix for detailed examples and analysis. In short, our method not only encourages short outputs but also incentivizes the model to reorganize its CoTs into more streamlined and expert-like format. Qwen3-30B-A3B-Instruct-"
        },
        {
            "title": "5 Related Work",
            "content": "Vanilla Ours (step 600) 60.83 60.83 83.33 76.67 6.9k 5.1k 5.1 Efficient Reasoning Table 2: Performance on AIME25 for Qwen3 models. In Qwen3-8B, Qwen3-4B-Thinking-2507. limit with the target budget, including Qwen3-0.6B, the Qwen3 family, Qwen3-4B-Instruct-2507, Qwen3-1.7B, Qwen3-4B-Thinking-2507, and Qwen3-30B-A3B-Instruct-2507. Based insights, we strictly align the rollon our out including LR = LT = 8k for Qwen3-0.6B, LR = LT = 10k for Qwen3-1.7B, and LR = LT = 16k for particular, we set LR = 8k, LT = 6k for Qwen3-4B-Instruct-2507 and LR = 10k, LT = 8k for Qwen3-30B-A3B-Instruct-2507, aiming to incentivize shorter outputs better since original rollouts are already short and concise. Crucially, we maintain high reward density by sampling = 24 trajectories for each prompt from DeepScaleR-Easy. Also, we do not use the off-policy strategy due to the potential instability. The training batch sizes are 128, 128, 32, 32, 32, and 16, respectively. After training, we evaluate these compressed LLMs on AIME25 (AIME, 2025). As shown in Table 2, the experimental results demonstrate the robustness of our strategy across different model scales. Specifically, on the Qwen3-0.6B, our method significantly boosts the Mean@8 from 13.33 to 24.58, while reducing the average response length from 14.9k to 8.9k. MeanEfficient reasoning methods aim to mitigate the overthinking phenomenon (Wu et al., 2025b; Sui et al., 2025) and reduce the prohibitive inference costs associated with long-form CoT (Wu et al., 2025a; Cui et al., 2024). One prominent approach trains long CoTs to be short using SFT (Xia et al., 2025; Ma et al., 2025) or RL (Hou et al., 2025; Shen et al., 2025; Liu et al., 2025; Liang et al., 2025). Parallel research directions explore architectural innovations, such as reasoning within latent spaces (Hao et al., 2024; Su et al., 2025) or more efficient decoding (Sun et al., 2024; Xu et al., 2025). We refer the readers to Feng et al. (2025) for more details. Unlike works that propose novel architectures, we focus on the mechanics of RL-based efficiency optimization. 5.2 Reward Shaping Methods The philosophy for reward shaping is to incentivize short yet accurate rollouts via allocating training rewards (Weng, 2025). The first principle is to promote shorter responses and penalize longer responses among correct ones (Kimi et al., 2025; Hou et al., 2025; Aggarwal and Welleck, 2025). Meanwhile, we can also penalize longer for incorrect answers (Kimi et al., 2025). Contrary to that, Liu et al. (2025); Yeo et al. (2025) argue to promote longer incorrect rollouts to encourage exploration. Despite these advancements, existing studies often evaluate reward functions in isolation. In this paper, we select the simplest truncation strategy after comparison and conduct extensive experiments in unified protocol."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we first reveal that the training dynamics of efficient reasoning follow two-stage paradigm. Meanwhile, we advocate for more finegrained metrics. Based on these, we further deconstruct the impact of data difficulty, rollout number, reward on negative rollouts, and optimization strategies. All findings are distilled into valuable insights and practical guidelines. key finding is to train on relatively easier prompts for sufficient and effective rewards. The learned length bias for reasoning can be generalized across domains and difficulty levels. Evaluation of more LLMs up to 30B further demonstrates the robustness and generalization."
        },
        {
            "title": "Limitation and Future Work",
            "content": "In this work, we systematically investigate the mechanics of efficient reasoning. However, there are several limitations for future research. Domain diversity. In this paper, we train on DeepScaleR, containing mathematical reasoning prompts, and validate on math and coding benchmarks. One future work is to evaluate on more domains, such as creative writing. Another interesting topic is whether training on more diverse prompts contributes to better performance. Adaptive length. In this paper, we employ fixed rollout and target length (i.e., LR and LT ). One intuitive idea is to set the length adaptively based on the prompts and current LLMs. For target length LT , we conduct experiments on toy setting detailed in Appendix E. For adaptive rollout length LR, we can pre-compute and set the rollout engine correspondingly. Evaluate on larger models. In this paper, we conduct extensive experiments (about 0.2 million GPU hours) in unified protocol on the DeepSeek-R1-Distill-Qwen-1.5B. Moreover, we extend our evaluation to the Qwen3 family, such as Qwen3-30B-A3B-Instruct-2507. However, due to the limited GPUs, we do not evaluate on extremely large LLMs such as Qwen3-235B-A22B-Instruct-2507. We leave it for future work. More fine-grained supervision. In this paper, we focus on efficient reasoning via reward shapingbased RL training. The principle is to incentivize short yet accurate thinking trajectories. We do not apply fine-grained refinement to the CoTs. Meanwhile, we humans simplify the previous experience by making and employing useful tools, such as notebook and calculator. Therefore, how to create useful tools and reuse them during reasoning remains valuable topic."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks arXiv preprint with reinforcement learning. arXiv:2503.04697. AIME. 2025. Aime. In American invitational mathematics examination. AMC. 2025. Amc. In American mathematics competitions. Anonymous. 2025. Deepscaler: Effective RL scaling of reasoning models via iterative context lengthening. In Submitted to The Fourteenth International Conference on Learning Representations. Under review. Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, and 1 others. 2024. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 958979. Sicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, and 1 others. 2025. Deepseekr1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828 3850. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. 2025. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296. Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao LowLi, Yuhao Jiang, and Bo Zhou. 2025. probability tokens sustain exploration in reinforcement learning with verifiable reward. arXiv preprint arXiv:2510.03222. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Kimi, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and 1 others. 2022. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857. Tian Liang, Wenxiang Jiao, Zhiwei He, Jiahao Xu, Haitao Mi, and Dong Yu. 2025. Deepcompress: dual reward strategy for dynamically exploring and compressing reasoning chains. arXiv preprint arXiv:2510.27419. Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and Junxian He. 2025. Learn to reason efficiently with adaptive length-based reward shaping. arXiv preprint arXiv:2505.15612. Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, and Tze-Yun Leong. 2024. Highly efficient self-adaptive reward shaping for reinforcement learning. arXiv preprint arXiv:2408.03029. Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. 2025. Cot-valve: Lengthcompressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601. Penghui Qi, Zi-Yan Liu, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Optimizing anytime reasoning via budget relative policy optimization. ArXiv, abs/2505.13438. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian. 2025. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. 2025. Token assorted: Mixing latent and text tokens for improved language model reasoning. arXiv preprint arXiv:2502.03275. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, and 1 others. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024. Fast best-of-n decoding via speculative rejection. Advances in Neural Information Processing Systems, 37:3263032652. Lilian Weng. 2025. Why we think. lilianweng.github.io. Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, and Ngai Wong. 2025a. Revisiting model interarXiv preprint polation for efficient reasoning. arXiv:2510.10977. Yuyang Wu, Yifei Wang, Ziyu Ye, Tianqi Du, Stefanie Jegelka, and Yisen Wang. 2025b. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266. Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067. Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Jun Liu, Qika Lin, and Zhiyong Wu. 2025. ϕ-decoding: Adaptive foresight sampling for balanced inferencetime exploration and exploitation. arXiv preprint arXiv:2503.13288. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Haizhong Zheng, Jiawei Zhao, and Beidi Chen. 2025. Prosperity before collapse: How far can off-policy arXiv preprint rl reach with stale data on llms? arXiv:2510.01161."
        },
        {
            "title": "Engineering",
            "content": "For Kimi-1.5 (Kimi et al., 2025), they first define normalized length term: L(yi) = L(yi) Lmin Lmax Lmin , (3) where Lmax and Lmin are the maximum and minimum length within rollout group. Then the reward function can be rewritten as the sum of the correct reward and the incorrect penalty: RKimi(x, yi) = I(yi is correct) (1 + α(0.5 L(yi))) +I(yi is incorrect) min (0, α(0.5 L(yi))) . (4) For Laser (Liu et al., 2025), it can be viewed as base reward for correctness plus bonus if the length condition is met: RLaser(x, yi) = I(yi is correct) (1 + α I(L(yi) < LT )) . (5) They also propose variant to encourage exploration by adding an extra term to incentivize longer trajectories. Specifically, it adds an exploration bonus for incorrect rollouts: RLaser-D(x, yi) = I(yi is correct) (1 + α I(L(yi) < LT )) + I(yi is incorrect) (α I(L(yi) LT )) . (6) In this paper, we set α to 0.4. Figures 8 and 9 present the training dynamics on additional benchmarks, including AIME25, MATH-500, AMC (Figure 8), and Minerva Math, Olympiad Bench, LiveCodeBench (Figure 9). We can find that the training process strictly follows two-stage paradigm, i.e., the length adaptation phase followed by the reasoning refinement phase. The rollout length (top row) consistently decays before stabilizing. Meanwhile, the performance shows distinct behaviors across various token budgets. For instance, more aggressive strategy Kimi often exhibits gains at stricter budgets (2k), but suffers from stagnation or collapse at generous budgets (32k), whereas the truncation baseline maintains more balanced recovery. Without results under larger token budgets, the observations and conclusions will be biased. These extensive results reinforce the necessity of our proposed fine-grained evaluation protocol. In the main paper, we demonstrate that training on easy prompts allows LLMs to focus on length reduction without compromising performance, which is attributed to sufficient and effective rewards. Here, we provide details on all benchmarks and more settings. As illustrated in Figures 10 and 11, the results consistently validate our hypothesis regarding reward density under various settings. Across almost all benchmarks, the model trained exclusively on DeepScaleR-Hard (green line) exhibits significant instability. For instance, on AIME25 and Olympiad Bench, the performance often fluctuates violently or collapses entirely after the initial adaptation phase. Meanwhile, the model trained on DeepScaleREasy (orange line) consistently matches or rivals the performance of the model trained on the full dataset (blue line). Notably, on the relatively tough AIME25 and LCB, the performance is also comparable and even better, though we train on high-pass prompts. In short, the learned length bias for reasoning can be generalized across domain and difficulty."
        },
        {
            "title": "C Detailed Results for More Rollouts",
            "content": "Figures 12 and 13 indicate the results on more rollouts under two different settings (LR = 16k, LT = 4k and LR = 4k, LT = 4k). We observe three key phenomena: 1) Consistent benefits across settings. Regardless of the rollout length (LR), the advantage of scaling remains consistent. Whether allowing for long-context exploration (16k) or strictly constraining the search space (4k), increasing yields positive effect on the convergence and asymptotic performance. It indicates that increasing the density of the reward signal is robust strategy for efficient reasoning. 2) Task-dependent sensitivity. The performance improvement varies significantly across datasets. On difficult benchmarks such as AIME25 and LiveCodeBench (LCB), the performance gap between = 8 and = 32 is relatively narrow. This suggests that for problems requiring complex multistep reasoning or code synthesis, simply increasing the number of rollouts is insufficient if the base model lacks the fundamental capability to solve the problem. Conversely, on relatively easier or intermediate tasks like AMC and MATH-500, larger leads to substantial performance separation on Mean@8. 3) Mean@8 vs. Pass@8. While Mean@8 shows significant improvement with larger , the Pass@8 metric often remains stagnant or improves only marginally. Pass@k measures the ability to generate at least one correct solution under trials. It implies that scaling does not necessarily enable the model to solve new problems that were previously unsolvable. Meanwhile, Mean@k measures the expected correctness. The rise in Mean@8 indicates that scaling effectively reduces the variance of the policy. In conclusion, increasing for training prompts primarily acts as stabilizer, reducing the variance of the policy on solvable problems rather than expanding the upper-bound. However, it also brings challenges, i.e., increased computing overheads."
        },
        {
            "title": "D Case Study",
            "content": "Easy and hard prompts. Tables 3 and 4 indicate two examples with corresponding responses when trained on DeepScaleR-Easy and DeepScaleRHard. On DeepScaleR-Hard, the model suffers from the reasoning collapse issue, leading to overshort outputs. One behavior is to omit the doublechecking process. We attribute such an issue to the sparsity of positive samples, which leads to the overfitting on length. Meanwhile, the group normalization also amplifies such an effect (larger advantage value). For instance, the advantage of positive samples in group {1, 0, 0, 0} is larger than group {1, 1, 1, 0}. Vanilla and our strategy. Tables 5 and 6 indicate two examples with corresponding responses from vanilla and our strategy. For our method, we set LR = LT = 4k, = 24 with staleness of 16. As observed in the vanilla response, the model exhibits typical conversational redundancy: it frequently uses filler phrases (e.g., \"Hmm,\" \"Let me think\"), re-states the problem premise unnecessarily, and performs verbose arithmetic decomposition. In contrast, the model trained with our efficient reasoning strategy undergoes fundamental stylistic shift. It eliminates conversational fluff and directly adopts dense and mathematically formal structure. Reasoning transitions from hesitant narrative to precise symbolic derivation (e.g., integrating formulas such as = 1 3 Bh directly into the calculation flow). It demonstrates that our method not only encourages short outputs but also actively incentivizes the model to reorganize its CoTs into more streamlined and expert-like format."
        },
        {
            "title": "E Adaptive Target Length LT",
            "content": "Figure 7: Performance on Qwen3-0.6B when adaptively setting target length LT as 90-th qualities. For adaptive target length, we set LT as the 90th quantile of correct rollouts. As shown in Figure 7, the strategy of adaptive target leads to short answers and thus better performance on 4k budget. We attribute it to more negative signal since 10% of correct answers are always negative samples. However, on 32k budget, the performance is comparable or even worse. Also, there are lots of spikes on the KL divergence curve, indicating rapidly changing gradients and potential instability. We leave it for future work to explore more settings. Figure 8: Training dynamics of various reward shaping methods on AIME25, MATH-500, and AMC. Figure 9: Training dynamics of various reward shaping methods on Minerva Math, Olympiad Bench, and LiveCodeBench. Figure 10: Performance training on various training prompts (rollout LR = 16k, target LT = 8k). Figure 11: Performance training on various training prompts (rollout LR = 4k, target LT = 4k). Figure 12: Performance training on various rollouts (LR = 16k, LT = 4k). Figure 13: Performance training on various rollouts (LR = 4k, LT = 4k). Question Let be the region in the complex plane consisting of all complex numbers that can be written as the sum of complex numbers z1 and z2, where z1 lies on the segment with endpoints 3 and 4i, and z2 has magnitude at most 1. What integer is closest to the area of R? Lets think step by step and output the final answer within boxed{} Hard Easy To determine the area of the region R, first identify the segments involved. The first segment runs from 3 to 4i, and the second segment encompasses all points within unit circle centered at the origin. By translating these segments into the complex plane and applying appropriate transformations, calculate the combined area as the sum of their respective areas. To find the area of the region in the complex plane: Understanding the Components: Segment from 3 to 4i: This is line segment in the complex plane with endpoints at 3 (which is (3, 0)) and 4i (which (3)2 + (4)2 = 5. Region defined by z2: This is disk (or circle) centered is (0, 4)). The length of this segment is at the origin with radius 1. Calculating the Area: The region is the set of all points = z1 + z2, where z1 lies on the segment from 3 to 4i and z2 lies within the unit disk. This effectively translates the unit disk to every point along the segment z1 and 2 = π, the total area of is π. sums the areas. Since the unit disk has an area of π (1) Finding the Closest Integer: The area is approximately 3.1416, so the closest integer is 3. Final Answer: 3 (Incorrect) Okay, so need to find the area of the region in the complex plane. consists of all complex numbers that can be written as = z1 + z2, where z1 lies on the segment with endpoints 3 and 4i, and z2 has magnitude at most 1. Then, have to find the integer closest to this area. First, let me visualize this. The region is the Minkowski sum of the segment from 3 to 4i and the disk of radius 1. So, if think about it, should be rectangle with semicircles on both ends. But let me break it down step by step. The segment from 3 to 4i: In the complex plane, 3 is the point (3,0) and 4i is the point (0,4). So this is straight line connecting (3,0) to (0,4). The length of this segment can be calculated, but maybe we dont need it right away. z1 lies on this segment. So z1 can be parameterized. Let me define z1 as point on the line segment between 3 and 4i. Lets parameterize z1 with parameter from 0 to 1. So, z1(t) = (3 3t) + (4i 4it) = (3(1 t), 4(1 t)). Wait, actually, in complex terms, if let vary from 0 to 1, when t=0, z1 = 3, and when t=1, z1 = 4i. Alternatively, z1 can be expressed as 3 + (4i 3)i, where is parameter from 0 to 1. Hmm, maybe thats more complicated. Alternatively, the equation of the line from (3,0) to (0,4). The slope is (4 0)/(0 3) = 4/3. So equation is = (4/3)x + b. When = 3, = 0 0 = (4/3)(3) + => 0 = 4 + => = 4. So equation is = (4/3)x + 4. So, for any point z1 on the segment, z1 can be written as + yi where and satisfy = (4/3)x + 4, and ranges from 0 to 3? Wait, no, the segment goes from (3,0) to (0,4). So when decreases from 3 to 0, increases from 0 to 4. So, for any z1 on the segment, we can write z1 = (x, y) where = (4/3)x + 4, [0, 3]. Alternatively, parametric equations: Lets define as parameter from 0 to 1. z1(t) = (3 3t, 0 + 4t) because at t=0, we get (3,0), and at t=1, we get (0,4). So in complex terms, z1 = (3 3t) + (4t)i. Then, z2 is complex number with z2 1, which is disk of radius 1 centered at the origin. Since is the Minkowski sum of the segment and the disk, it is like sliding the disk along every point of the segment. So geometrically, its rectangle with length equal to the length of the segment and width 2 (diameter of the disk). But with semicircular ends because the disk is only of radius 1. Wait, actually, no. The Minkowski sum of line segment and disk is called an \"offset\" of the disk by the segment. So its rectangle with length equal to the segment length and width equal to twice the radius (since each end contributes semicircle). But in this case, the radius is 1, so each end is semicircle. Wait, but the segment is from (3,0) to (0,4), so the Minkowski sum would be the set of all points within distance 1 from the segment. So its like rectangle with length equal to the segment length and width 2, but with semicircular ends of radius 1. Therefore, the area would be the area of the rectangle plus the area of the two semicircles. (3 + 4) = 5. The width is Area of rectangle: length * width. The length is the distance between 3 and 4i, which is 2 (since radius is 1, diameter is 2). So area of rectangle is 5 2 = 10. Area of two semicircles: Each semicircle has area (1/2)π(1), so two of them is π. Therefore, total area is 10 + π 10 + 3.1416 13.1416. So the area is approximately 13.1416, so the closest integer is 13. Wait, but let me verify again. The Minkowski sum of line segment and disk is indeed rectangle with length equal to the segments length and width equal to 2r, and with two semicircular ends. So thats correct. The length of the segment is 5, as calculated before. So area is 2 = 10 + π. So yes, the area is 10 + π 13.1416. Therefore, the integer closest to the area is 13. 5 2 + π (1) Final Answer 13 Double check: The region consists of all complex numbers (z) that can be written as = z1 + z2, where z1 lies on the segment with endpoints (3) and (4i), and z2 has magnitude at most (1). The segment from (3) to (4i) in the complex plane is straight line connecting the points ((3, 0)) and ((0, 4)). This line has length of (5) (calculated using the distance formula). The region (R) is the Minkowski sum of this segment and the disk of radius (1). This results in shape that is rectangle with length (5) and width (2) (since the radius of the disk is (1)), plus two semicircular ends each with radius (1). The area of the rectangle is (5 2 = 10). The area of the two semicircular ends is (π 12 = π). Therefore, the total area of (R) is 10 + π 10 + 3.1416 13.1416. The integer closest to the area of (R) is 13 . (Correct) Table 3: Case comparison when trained on DeepScaleR-Easy and DeepScaleR-Hard using truncation strategy. Question Hard Easy The volume of cone is given by the formula = 1 3 Bh, where is the area of the base and is the height. The area of the base of cone is 30 square units, and its height is 6.5 units. What is the number of cubic units in its volume? Lets think step by step and output the final answer within boxed{} To find the volume of the cone, Ill use the formula = (1/3) * * h. The area of the base, B, is 30 square units, and the height, h, is 6.5 units. First, Ill calculate one-third of the base area: (1/3) * 30 = 10. Next, Ill multiply this result by the height: 10 * 6.5 = 65. Therefore, the volume of the cone is 65 cubic units. To find the volume of the cone, we can use the formula: = 1 3 square units - ( ) is the height = 6.5 units Plugging in the values: = 1 30 6.5 3 First, calculate one-third of 30: 1 3 Then multiply by 6.5: 10 6.5 = 65 Therefore, the volume of the cone is: 65 cubic units. (Correct) Where: - ( ) is the area of the base = 30 30 = Okay, so need to find the volume of cone. The formula given is = (1/3)Bh, where is the area of the base and is the height. Hmm, let me recall what each part means. First, the area of the base, which is B. The problem says the area of the base is 30 square units. So is 30. Then, the height is 6.5 units. Got that. So plugging these values into the formula: = (1/3) * * h. So that would be (1/3) * 30 * 6.5. Let me compute that step by step. First, can compute 30 multiplied by 6.5. Let me do that: 30 * 6.5. Hmm, 30 * 6 is 180, and 30 * 0.5 is 15, so adding those together gives 195. So 30 * 6.5 is 195. Then, take that result and multiply by (1/3). So (1/3) * 195. Dividing 195 by 3. Well, 3 goes into 195 sixty-five times because 360 is 180, and 35 is 15, so 60 + 5 = 65. So (1/3)*195 is 65. So = 65 cubic units. Let me just double-check that. Alternatively, maybe can compute it differently. Since = (1/3) * * h, with = 30 and = 6.5. Compute * first: 30 * 6.5. As above, thats 195. Then multiply by 1/3: 195 / 3 = 65. Yep, same result. So the volume is 65 cubic units. think thats correct. Final Answer The volume of the cone is 65 cubic units. Double check: The volume of cone is given by the formula = 1 area of the base is 30 square units, and the height is 6.5 units. First, we compute the product of ( ) and ( ): 30 6.5 = 195 Next, we multiply this result by 1 Thus, the volume of the cone is 65 cubic units. (Correct) 3 Bh, where is the area of the base and is the height. The 195 = 65 3 : 1 3 Table 4: Case comparison when trained on DeepScaleR-Easy and DeepScaleR-Hard using truncation strategy. The model suffers from the reasoning collapse issue when trained on DeepScaleR-Hard, e.g., the double check part is skipped for shorter length. Question Compute ( 31 28). Lets think step by step and output the final answer within boxed{} Vanilla Okay, so need to compute the binomial coefficient (( 31 28)). Hmm, binomial coefficients are those numbers that come up in combinations, right? They represent the number of ways to choose subset of items from larger set, disregarding the order. First, remember that the formula for binomial k) = n! 31! coefficient is (( with these values: ( k!(nk)! ). So, in this case, (n = 31) and (k = 28). Let me write down the formula 31 28) = 28!(3128)! Simplifying the denominator: 31 - 28 = 3, so it becomes 31! 28!3! Hmm, but calculating factorials for such large numbers seems tedious. Maybe theres way to simplify this without computing the entire factorials. remember that sometimes, in binomial coefficients, you can simplify by canceling out terms in the numerator and denominator. Let me think about how to do that. Since (31! = 31 30 29 28!), can rewrite the numerator as: 31! = 31 30 29 28! 31 28) = 31302928! So substituting back into the equation: ( Oh, see! The (28!) terms cancel out in numerator and denominator: = 313029 Now, (3! = 321 = 6), so substituting that in: = 313029 Alright, now need to compute this. Let me compute the numerator first: 31 multiplied by 30 is 930, and 930 multiplied by 29. Hmm, 930 times 29. Let me do that step by step: - 930 20 = 18,600 - 930 9 = 8,370 Adding them together: 18,600 + 8,370 = 26,970 So the numerator is 26,970. The denominator is 6. So now, 26,970 divided by 6. Lets do that: 26,970 6. Dividing step by step: - 6 goes into 26 four times (4x6=24), remainder 2. - Bring down the 9: 29. 6 goes into 29 four times (4x6=24), remainder 5. - Bring down the 7: 57. 6 goes into 57 nine times (9x6=54), remainder 3. - Bring down the 0: 30. 6 goes into 30 five times. So putting it all together: 4, 4, 9, 5. So its 4,495. Wait, let me double-check that division because might have messed up the steps. Alternatively, can write 26,970 6 as (26,000 + 970) 6. 26,000/6 = 4,333.333... 970/6161.666... Adding them together: 4,333.333...+161.666... = 4,495. 28!3! 3! 6 Yep, that matches. So the result is 4,495. Therefore, (( 31 28) = 4, 495). Final Answer The value of 31 28)) is boxed4495. To compute the binomial coefficient (( 31 28)), we use the formula: ( (( k) = n! k!(nk)! Substituting (n = 31) and (k = 28): ( = 31! 28!3! We simplify the factorials by recognizing that (31! = 31 30 29 28!): ( Since (3! = 6), we have: = 313029 Dividing by the denominator: 26970 6 6 Calculating the numerator: 31 30 = 930][930 29 = 26970 = 4495 Thus, the value of (( 31 28)) is ( 4495 ). Ours To compute the binomial coefficient (( 31 28)), Ill start by recalling that the binomial coefficient formula 31 28) = ( 31 3 ) = n! k) = is given by: ( k) = ( k!(nk)! Here, (n = 31) and (k = 28). Alternatively, since (( k)), perhaps can simplify the expression by choosing the smaller (k) value to make calculation easier. 31 3 ) Calculating (31 - 28 = 3), which means: ( 31 3 )): Using the formula: ( Now compute (( 31 30 29 28!): 31302928! 313029 6 3!28! Simplifying, since (31! = Calculate the denominator (3! = 6): Therefore, Computing numerator: 31 30 = 930; 930 29 = ? Compute: 930 29: Breakdown: = 930 (30 - 1) = 930 30 - 930 1 = 27900 - 930 = 26970 Then divide by 6: 26970 6 = ? Calculate: 26970 6 = 4495 Wait perhaps: 6 4495 = 26970, yes. Therefore, the value becomes 4495. Thus, = 313029 31! 3!(313)! = 31! 3!28! 3! 4495 To compute (( 31 k) = ( 28)), we use the property (( k)): ( 31 28) = ( 31 3 ) Compute (( 31 3 )): Using the formula, ( 31 3 ) = 31! 3!(313)! 26970/6 = 4495 Thus, = 313029 321 Calculating: 31 30 29 = 26970 Divide by (6) : Table 5: Case for vanilla and our trained LLM. Both answers are correct. 31! 28!(3128)! 31 28) = 31 28) = 31302928! 28!3! = 313029 3! Question Vanilla Ours The volume of cone is given by the formula = 1 3 Bh, where is the area of the base and is the height. The area of the base of cone is 30 square units, and its height is 6.5 units. What is the number of cubic units in its volume? Lets think step by step and output the final answer within boxed{} Okay, so need to find the volume of cone. The formula given is = (1/3)Bh, where is the area of the base and is the height. Hmm, lets break this down step by step. First, know the area of the base is 30 square units. Thats B. Then, the height is 6.5 units, so is 6.5. The formula is straightforward, but should make sure plug the values correctly. So, can write it out as = (1/3) * * h. Substituting the given values, that becomes = (1/3) * 30 * 6.5. Let me compute this step by step. First, let me calculate 1/3 of 30. Hmm, 30 divided by 3 is 10. So, 1/3 * 30 = 10. Now, need to multiply that result by 6.5. So, 10 * 6.5. Let me think, 10 * 6 is 60, and 10 * 0.5 is 5, so 60 + 5 is 65. Therefore, 10 * 6.5 is 65. So, putting it all together, = 65 cubic units. That should be the volume of the cone. Wait, let me double-check to make sure didnt make any calculation errors. 1/3 of 30 is indeed 10, correct. Then, 10 multiplied by 6.5. Yes, 6.5 is the same as 13/2, so 10(13/2) is (10/1)(13/2) = (10*13)/2 = 130/2 = 65. Yep, thats right. So, think did that correctly. The volume is 65 cubic units. Final Answer The volume of the cone is 65 cubic units. The volume of cone is given by the formula = 1 the height. Given: - The area of the base (B = 30) square units. - The height (h = 6.5) units. We substitute these values into the formula: = 1 3 First, calculate 1 3 1 30 = 10 3 Next, multiply the result by 6.5: 10 6.5 = 65 Thus, the volume of the cone is ( 65 ) cubic units. 3 Bh, where (B) is the area of the base and (h) is 30 6.5 30: To find the volume of the cone, Ill use the formula (V = 1 3 Bh). Given that the base area (B) is 30 square units and the height (h) is 6.5 units. First, multiply these values together: (30 6.5 = 195) Then, take one-third of that product: ( 1 195 = 65) Therefore, the volume of the cone is 65 cubic 3 units. Given: - Area of the base ((B)) = 30 square units - Height ((h)) = 6.5 units The formula for the 3 Bh Plugging in the given values: = 1 volume (V) of cone is: = 1 Calculate: 30 6.5 = 195 Then, = 195 3 = 65 Answer: ( 65 ) cubic units. 30 6. 3 Table 6: Case II for vanilla and our trained LLM. Both answers are correct."
        }
    ],
    "affiliations": [
        "Tencent",
        "The University of Hong Kong"
    ]
}