{
    "paper_title": "ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning",
    "authors": [
        "Yuzhou Huang",
        "Ziyang Yuan",
        "Quande Liu",
        "Qiulin Wang",
        "Xintao Wang",
        "Ruimao Zhang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges in this task: 1) the identity decoupling problem, where directly adopting existing customization methods inevitably mix attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training such a model that represents and decouples various concepts well. To address these challenges, we introduce ConceptMaster, an innovative framework that effectively tackles the critical issues of identity decoupling while maintaining concept fidelity in customized videos. Specifically, we introduce a novel strategy of learning decoupled multi-concept embeddings that are injected into the diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To further overcome the scarcity of high-quality MCVC data, we carefully establish a data construction pipeline, which enables systematic collection of precise multi-concept video-entity data across diverse concepts. A comprehensive benchmark is designed to validate the effectiveness of our model from three critical dimensions: concept fidelity, identity decoupling ability, and video generation quality across six different concept composition scenarios. Extensive experiments demonstrate that our ConceptMaster significantly outperforms previous approaches for this task, paving the way for generating personalized and semantically accurate videos across multiple concepts."
        },
        {
            "title": "Start",
            "content": "ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning Yuzhou Huang1,2,3 Xintao Wang2 Ruimao Zhang1 Ziyang Yuan2,4 Quande Liu2 Qiulin Wang2 Pengfei Wan2 Di Zhang2 Kun Gai2 1Sun Yat-sen University 2Kuaishou Technology 3The Chinese University of Hong Kong, Shenzhen https://yuzhou914.github.io/ConceptMaster/ 4Tsinghua University 5 2 0 2 8 ] . [ 1 8 9 6 4 0 . 1 0 5 2 : r Figure 1. We propose ConceptMaster, Multi-Concept Video Customization (MCVC) method that could create high-quality and conceptconsistent customized videos based on given multiple reference images without additional test-time tuning. Our ConceptMaster is capable of handling diverse customized scenarios, including but not limited to 1) multiple persons, 2) persons with livings, 3) persons with stuffs, 4) multiple livings, 5) livings with stuffs and 6) persons with both livings and stuffs."
        },
        {
            "title": "Abstract",
            "content": "Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains significant challenge. We identify two key challenges in this task: 1) the identity decoupling problem, where directly adopting existing customization methods inevitably mix attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training such model that represents and decouples various concepts well. To address these challenges, we introduce ConceptMaster, an innovative framework that effectively tackles the critical issues of identity decoupling while maintaining concept fidelity in customized videos. Specifically, we introduce novel strategy of learning decoupled Corresponding author KwaiVGI, Kuaishou Technology Work done during an internship at multi-concept embeddings that are injected into the diffusion models in standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To further overcome the scarcity of high-quality MCVC data, we carefully establish data construction pipeline, which enables systematic collection of precise multi-concept video-entity data across diverse concepts. comprehensive benchmark is designed to validate the effectiveness of our model from three critical dimensions: concept fidelity, identity decoupling ability, and video generation quality across six different concept composition scenarios. Extensive experiments demonstrate that our ConceptMaster significantly outperforms previous approaches for this task, paving the way for generating personalized and semantically accurate videos across multiple concepts. 1 1. Introduction Diffusion-based text-to-video generation models, trained on extensive text-video data pairs, have demonstrated remarkable success in generating high-quality videos from textual inputs [2, 3, 6, 11, 15, 18, 26, 41, 52, 56, 58, 64, 71]. These advancements have sparked increasing interest in personalizing video generation through user-defined concepts. Some methods have been proposed to produce customized videos using additional image guidance, with proven effectiveness for the customization on object [25], human [17], style [37], etc. Existing approaches to concept customization primarily fall into two methodological categories: tuning-based solutions and pretrain-based methods. The tuning-based solutions [9, 13, 16, 30, 50] typically first optimize model parameters (e.g., variants of LoRAs [22] or fully training latent diffusion models [50]) each time for customized concepts and then incorporate them for inference. However, these methods are computationally time-consuming and often require the manual collection of multiple reference samples, rendering them impractical in most time-sensitive and user-friendly scenarios. Conversely, pretrain-based methods [8, 14, 17, 25, 33, 35, 6062, 67] aim to integrate visual embeddings into diffusion models at training time in datadriven manner, enabling personalization without additional test-time parameter tuning. Despite their progress, how to adopt these approaches to simultaneously process multiple concepts in videos to keep both concept fidelity and factorization in feed-forward approach remains challenging. In this paper, we study the unsolved challenging problem of Multi-Concept Video Customization (MCVC) without test-time tuning, which introduces two critical difficulties: 1) The identity decoupling problem, unlike single-concept processing, the MCVC task demands not only representing every concept individually based on the given multiple references, but also precisely differentiating the attributes across them in generated videos. Simply adopting existing pretrain-based approaches often leads to the conflation of visual concepts, inadvertently blending attributes from distinct individuals. This issue becomes more pronounced when dealing with concepts that contain similar attributes. naive composite method is to firstly apply multi-concept image customization based on multiple references, and then input the generated image into image-to-video (I2V) models [68] for animation. However, this approach will be simultaneously subject to the representation and decoupling capability of two models, easily resulting in inferior generation quality and concept consistency. As illustrated in Fig. 2, both these two solutions can neither represent each concept well, nor clearly decouple them across visual appearances, resulting in unacceptable customized videos. 2) The scarcity of suitable high-quality MCVC datasets. Ideally, training such customization model requires extensive Figure 2. Directly applying single-concept method cannot handle the MCVC task, while the naive solution by combining multiconcept image generation and image-to-video generation models can also hardly create satisfactory customized results. videos featuring diverse concepts, accompanied by precise textual descriptions and reference images of each entity. Current data sources significantly fall short of these requirements, while how to collect large-scale, pairwise videoentity data remains extremely challenging due to the accurate extraction of multiple concepts contained in videos across vast diversity of both visual and textual concepts. To overcome these challenges, we propose ConceptMaster (see Fig. 1), an effective MCVC method that could effectively maintain the fidelity of multiple concepts well and address the identity decoupling problem, even for highly similar concepts. Unlike previous approaches to integrate visual embeddings with textual counterparts [25, 33, 35], or directly aggregate visual embeddings as whole into diffusion models [17, 61, 67], our key insight is to learn the decoupled multi-concept embeddings and inject into diffusion transformer models in standalone manner. Specifically, the process includes: 1) Extracting comprehensive visual embeddings from given reference images, where we initially extract dense visual tokens by the CLIP image encoder [45], and integrate learnable query transformer (Q-Former) network [34] to better represent comprehensive visual embeddings and align with the diffusion model space. 2) Incorporating visual representation with corresponding text description of every concept, where we propose the Decouple Attention Module (DAM) to conduct intra-pair attention to separately bind the extracted visual embedding with corresponding textual embedding for each concept, the process could effectively capture semantic differences across multiple concepts while maintain conceptspecific uniqueness. 3) Introducing novel multi-concept embeddings injection strategy, where we firstly composite the multi-concept embeddings and then inject them using an individual Multi-Concept Injector (MC-Injector), which 2 is standalone cross-attention layer, into the diffusion transformer models without affecting the original textual crossattention. This strategy separates the functionality of the original textual cross-attention from the learning process of the newly injected composite multi-concept embeddings, effectively enhancing the representation of multiple identities. The designed ConceptMaster could efficiently create high-fidelity customized videos during inference without additional parameter tuning, which significantly provides the potential for the practicality of real-world applications. Furthermore, to address the scarcity of suitable MCVC data, we carefully establish data collection pipeline, which could collect high-quality MCVC data that precisely extract entity images and corresponding text descriptions of diverse concepts in videos. By utilizing this pipeline, we collect over 1.3 million video-entity pairs spanning diverse conceptual domains, including humans, livings, and various object categories. To further facilitate the evaluation, we introduce Multi-Concept Benchmark (MC-Bench) to comprehensively validate this task from 1) concept fidelity, 2) effectiveness of identity decoupling, and 3) video generation quality across six distinct multi-concept composition scenes. Overall, our key contributions can be summarized: We propose ConceptMaster, novel multi-concept video customization framework to personalize video generaIt effectively adtion based on user-defined concepts. dresses the identity decoupling problem while ensuring every concept fidelity, even for highly similar concepts. We present novel strategy of learning the decoupled multi-concept embeddings and injecting into the diffusion models without influencing the original text crossattention, which guarantees both representation and decoupling of multiple concepts in customized videos. We introduce dedicated data collection pipeline that enables the creation of high-quality multi-concept videoentity pairs across diverse concepts, and we collect more than 1.3 million suitable MCVC data for the task. We introduce MC-bench, comprehensive benchmark including six distinct multi-concept composition scenarios for evaluation. We rigorously assess customized videos on concept fidelity, effectiveness of identity decoupling, and video quality. Extensive quantitative and qualitative experiments demonstrate that ConceptMaster significantly outperforms the previous state-of-the-art methods. 2. Related Work 2.1. Foundation Text-to-Video Diffusion Models The rapid development of text-to-video (T2V) models has been phenomenal. Early works in T2V diffusion models such as AnimateDiff [15], VideoCrafter [6] and ModelScope [58] are mainly based on latent diffusion models [48] with UNet backbones [49]. By using transformers [55] as the backbone of diffusion models, such as Diffusion Transformers (DiT) [44], SORA [3], and other transformer-based variants [20, 31, 70], advanced T2V models have scaled parameters and demonstrated impreslong-range, and sive capabilities in generating realistic, physically consistent videos. This advancement significantly expands the possibilities for content generation. 2.2. Image-based Concept Customization Customization in diffusion models enables users to provide reference images to generate results retain the given identities. These customization methods are primarily categorized into tuning-based and pretrain-based approaches. Early represented tuning-based methods [13, 50] are designed to online-optimize word embeddings or weights of diffusion models when new reference images are provided by users, which are constrained by consuming time and manually collecting training samples. Pretrain-based methods [8, 14, 33, 35, 60, 62, 65, 67] usually train an encoder on certain concept datasets to learn visual representation for conditional diffusion generation process. Some works primarily focus on general-domain concept customization [8, 14, 33, 62, 67], while others mainly aim at human face identity scenarios [35, 60, 65]. While aforementioned methods mainly customized single provided concept, the problem also extends to process multiple references. For example, CustomDiffusion [30] optimizes additional multiple key-value pairs in cross-attention. SSR-Encoder [69] aligns query inputs with image patches and preserves fine features of the subjects. MS-Diffusion [61] pretrains grounding resampler and generates images with bounding box layout guidance. These approaches remarkably promote the development of image customization. 2.3. Video-based Concept Customization Pretrained-based multi-concept customized video generation raises little attention. Preliminary methods [17, 23, 25, 63] predominantly focus on single-concept scenarios. DreamVideo [63] employs tuning-based approach to simultaneously customize identities and motion. Videobooth [25] simply utilizes Grounded-SAM [29, 39, 47] to extract foreground information and tags from the first frame of each video from WebVid dataset [1] including nine categories as training data, and further trains coarse-to-fine visual embedding on that data. In contrast, ID-Animator [17] leverages the CelebV dataset [72] to construct face identity dataset, and integrates pre-trained IP-Adapter [67] with AnimateDiff [15] for joint optimization. However, neither the data collection methods nor the proposed models targeting for single-concept customization are sufficient to directly transfer into multi-concept scenarios. ConceptMaster, on the other hand, could solve the challenging MCVC task well in feed-forward manner, we believe that Con3 ceptMaster has substantially promoted the development of video customization and paved the way for its future. 3. Preliminary: Diffusion Transformer Models for Text-to-Video Generation Transformer-based text-to-video diffusion models demonstrate huge potential on video content generation. Our ConceptMaster is built upon transformer-based latent diffusion model, which employs 3D Variational Autoencoder (VAE) [27] to transform videos from the pixel level to latent space. Each basic transformer block consists of 2D spatial self-attention, 3D spatial-temporal self-attention, text cross-attention, and feed-forward network (FFN). The text prompt embedding ctext for cross-attention is obtained by T5 encoder ET 5 [46]. We use Rectified Flow [12, 40] to define probability flow ordinary differential equation (ODE), which transfers the clean data z0 to noised data zt with straight path zt = (1 t)z0 + tϵ at timestep t, where ϵ is normal gaussian noise. The diffusion transformer output directly parameterizes the vΘ(zt, t, ctext) to regress velocity (z1 z0) with the Flow Matching objective [36]: LLCM = Et,z0,ϵvΘ(zt, t, ctext) (z1 z0)2 2. (1) 4. ConceptMaster 4.1. Multi-Concept Video Customization Problem: Given caption describing video, along with set of concept images {Xii = 1 . . . } and their corresponding labels {Yii = 1 . . . } (e.g., man and woman with their respective images), where represents the number of distinct concepts, the task of MultiConcept Video Customization (MCVC) aims to generate high-quality videos that incorporate all image-defined visual concepts while aligning them with the given descriptive caption . Each concept should maintain its identity as the provided images while precisely expressing its semantic behavior as described in the caption. Overview: To achieve this goal, we first meticulously design data collection pipeline, resulting in the creation of dataset comprising over 1.3 million high-quality MCVC samples. These training videos provide precise information about each entitys image and corresponding text description. Additionally, we incorporate several existing singleconcept image and video datasets to further enhance the concept representation. Afterwards, in order to generate videos that could effectively maintain the fidelity of each concept and decouple multiple visual representation, we firstly extract thoughtful visual embeddings of given reference images, and then design the Decouple Attention Module (DAM) to perform intra-pair attention across paired image-label features, achieving multi-modal representation for each identity. Subsequently, we combine every multimodal concept embedding into the composite ones, and further introduce Multi-Concept Injector (MC-Injector) in cross-attention manner to embed the multi-modal composite representation into the diffusion transformer models, where the composite features serve as keys and values. In Fig. 3, we demonstrate the overview framework of our proposed ConceptMaster. 4.2. Injecting the Decoupled Multi-Concept Embeddings Visual Concept Representation Extraction. To enable the model to process multiple concepts with high fidelity, we need to obtain reasonable visual representation from the concept images {Xii = 1 . . . }. We opt to use the CLIP image encoder Eimg [45] to extract the last layer output as dense visual tokens with shapes 16 16 768, i.e., {fifi = Eimg(Xi), = 1 . . . }. These tokens have demonstrated more complete visual representation of image conditions [51, 62, 67]. However, directly applying these dense visual tokens in diffusion generation often achieves inadequate alignment with representation space of diffusion models, results in unsatisfactory visual fidelity. To prevent such trivial visual conditions injection and achieve better alignment with the diffusion transformer context, we integrate learnable Q-Former architecture Q, which comprises stacked cross-attention layers and FFN [33, 34, 66]. We utilize the dense visual tokens as key-value corpus and employ the Q-Former to query these tokens {xixi = Q(fi), = 1 . . . }, thereby extracting comprehensive visual semantic representation. Decoupling Intra-Pair Embeddings. After obtaining the appropriate visual representation, we integrate the corresponding text labels to create multi-modal concept representation. While previous works [35, 65] directly combine the visual representation with the corresponding word from the caption embedding ctext = Etext(T ), we hope to fully leverage the textual label information associated with related image to enhance the representation specific to each concept. Therefore, unlike these approaches, we employ T5-encoder ET 5 to encode each concept label individually to obtain the text representation {yiyi = ET 5(Yi), = 1 . . . }. Subsequently, we introduce the Decouple Attention Module (DAM) to fuse each pair of the visual and text label embedding {(xi, yi)i = 1 . . . }. The DAM operation can be formulated as: Qi = WQ xi; Ki = WK yi; Vi = WV yi, Attention(Qi, Ki, Vi) = Softmax( ci = FFN(Attention(Qi, Ki, Vi)) QiK ) Vi, (2) where WQ, WK, and WV are projection matrices, is the embedding dimension, and FFN is two-layer multilayer perceptron (MLP) with GLUE [57] as the middle acti4 Figure 3. Overview of the framework of our proposed ConceptMaster. vation function. With the designed DAM, every visual representation could integrate its corresponding textual label to serve as the multi-modal representation for the diffusion transformer models. Composite Multi-Concept Representation Injection. After obtaining the multi-modal representation of each pair {cii = 1 . . . }, we firstly concatenate all concept embeddings into composite one, where is the dimension of concept embedding: IDs = Concat(c1, . . . , cN ), IDs RN c (3) Additionally, we design Multi-Concept Injector (MCInjector) to encode the composite multi-concept embeddings into the diffusion transformer models. Specifically, the MC-Injector is an additional specialized cross-attention layer integrated within each transformer block, positioned after the original text cross-attention layer. The additional standalone cross-attention layer can effectively learn the concepts without interference of the original text crossattention. Comparing with merging the composite embeddings into the original text cross-attention layer, our experiments in Sec. 5.4 indicate that by interleaving the MCInjector with the original one could achieve both better decoupling ability and visual fidelity on generated videos. Finally, the specific diffusion process assisted by the composite embeddings IDs can be formulated as: LLCM = Et,z0,ϵvΘ(zt, t, ctext, IDs) (z1 z0)2 2. (4) 4.3. MC-Oriented Video Data Construction Training good MCVC model requires high-quality MCoriented video data. Previous studies [25, 42, 61] heavily relied on the state-of-the-art open-set object detection methods, such as Grounding-DINO [39], to obtain bounding boxes for each concept based on its text label. They then employ the segmentation model SAM [29] to extract 5 masks using the bounding boxes as input. However, the simplistic method is far insufficient for our objectives, as Grounding-DINO, equipped with the CLIP text encoder, often perform poorly in distinguishing similar concepts, especially those that have high visual appearance or textual semantic similarity. Additionally, incorporating lowquality videos or data not suitable for customization task in training can adversely affect the quality of generated videos. Consequently, in order to collect high-quality and large-scale MCVC data, we carefully design the data collection pipeline into two levels: 1) Fast elimination of unsuitable videos, we filtering out low-quality videos that are not unsuitable for the task with time efficiency and low resources. 2) Fine-grained identity information extraction, we guarantee the accuracy of extracted identity reference images and corresponding text labels. We finally collect more than 1.3 million MCVC data for our ConceptMaster. Fig. 4(a) demonstrates the overview of our dataset collection pipeline. Additionally, we randomly sample 2000 samples from Panda-2M [7], and we count the success rate of collect videos. Fig. 4(b) demonstrates our designed data pipeline is significantly better than simply using GroundedSAM. More discussions could be found in appendix. Fast elimination of unsuitable videos. Scene transition detection and low-quality video elimination. We initially collect more than 6.4 million videos from Internet as sources. To ensure the basic attributes of our video data are maintained at high standard, we initially use PySceneDetect [5] to filter out videos that contain scene transitions to maintain the temporal coherence in videos. We also remove videos with low optical flow scores [54] to guarantee the dynamic integrity. Additionally, videos with low light contrast are excluded. Video captioning and concept labels extraction. We employ Qwen2-VL [59] to produce accurate and concise captions for videos. To extract potential concept entity textual description from the caption, we define taxonFigure 4. (a) The overview of multi-concept data collection pipeline. When dealing with complex scenarios that contain concepts with high visual appearance or textual semantic similarity, our data pipeline could still extract precise entity images and corresponding labels, while simply exploit previous methods like Grounded-SAM would introduce large number of errors and it is difficult to remove these errors through subsequent processing. (b) The success rate of testing videos comparison between Grounded-SAM and our data pipeline. omy of 120 classes, with each class encompassing several sub-words (for instance, the class dog includes sub-words such as dog, puppy and beagle). we utilize SpaCy [21] to extract nouns from the captions, ensuring that these nouns fall within the predefined set of sub-words. The extracted nouns serve as the textual input for text-guided detection and segmentation algorithms. Elimination of unsuitable videos for MCVC task. Since most videos are unsuitable for video customization, we hope to quickly exclude those clearly cannot meet our requirements with minimal resource expenditure and time consuming. For each video, we uniformly sample 10% of the frames and use the extracted nouns to identify entity boxes through text-guided Grounding-DINO. Simultaneously, we apply Non-Maximum Suppression (NMS) to filter out duplicate boxes and remove boxes that are either too large or too small (e.g., areas smaller than 10% or larger than 90% of the video frame size). Subsequently, we classify each box using CLIP, eliminating any box if the label classified by CLIP is inconsistent with the original one. If all the boxes are eliminated through this process, the corresponding video will be excluded. Fine-grained identity information extraction. Accurate visual and textual labels extraction. To accurately extract the region and text label of each identity, we employ the same frame sampling strategy and use LISA [32], an MLLM-based [38] segmentor, input by both text prompts and images with strong visual reasoning capabilities, to extract entity masks. LISA provides highly accurate segmentation results, even for similar visual appearance and textual semantics. Those masks are either too large or too small, or with high degree of fragmentation are removed. We then derive box regions from these masks and remove any misclassified ones through CLIP classification. Additionally, we use FaceAnalysis1 1https://github.com/deepinsight/insightface to detect all regions belonging to the person class, retaining only those that contain face regions (i.e., removing humans where only the body parts are visible). 4.4. Joint Training with Auxiliary Datasets In addition to the MCVC data we have constructed, we also utilize auxiliary datasets to enhance concept representation. We reproduce the single-concept image dataset from BLIPDiffusion [33] (around 300k) for high-specificity concept enhancement. Furthermore, we incorporate the singleconcept video dataset CelebV [72] (about 60k) to improve human representation. The data sampling ratio of our built data, BLIP-Diffusion and CelebV is 8:1:1. 5. Experiments 5.1. Experimental Setup Implementation Details. The implementation details of ConceptMaster can be found in the supplementary material. Evaluation Metrics. In order to comprehensively evaluate the methods for the MCVC task, we consider from three different dimensions: 1) Concept fidelity, where we exploit the commonly used CLIP-I and CLIP-T [45] to globally evaluate that if the generated videos match the given reference images and video captions. 2) Decoupling ability, where we firstly utilize OWLv2 [43] to detect the box area of each concept in generated videos, and then we compute CLIP-I and DINO-I [4] scores between the original concept images and specific box areas in generated videos, and CLIP-T between specific box areas and detected labels for local evaluation. 3) Video generation quality, where we adopt the motion smoothness, dynamic degree, aesthetic quality and imaging quality collected by VBench [24] to evaluate the quality of generated videos. Figure 5. Qualitative comparison on multi-concept customization. When compared to several different methods to conduct the MCVC task, our approach clearly demonstrates superior capabilities on concept fidelity, identity decoupling and caption semantic consistency. Methods CustomDiffusion SSR-Encoder IP-Adapter MS-Diffusion DreamBooth ConceptMaster (Ours) Concept Fidelity Decoupling Ability Video Quality CLIP-T CLIP-I CLIP-T CLIP-I DINO-I Motion Smoothness Dynamic Degree Aesthetic Quality 27.986 24.572 27.943 28.519 18.887 29.461 13.648 13.829 14.072 14.703 8.23 17.585 19.371 19.857 14.829 21.060 12.781 22.378 0.557 0.518 0.564 0.525 0.499 0.573 0.643 0.659 0.539 0.666 0.562 0.678 0.531 0.517 0.394 0.528 0.44 0. 0.592 0.598 0.551 0.584 0.517 0.618 0.964 0.962 0.977 0.976 0.991 0.987 Imaging Quality 0.686 0.631 0.701 0.704 0.632 0.709 Table 1. Quantitative comparison with different methods on MC-Bench, where bold represents the best result and underline represents the second best result. 5.2. MC-Bench Evaluation In order to comprehensively evaluate the performances of the MCVC models on diverse scenarios, we establish Multi-Concept Benchmark (MC-Bench), where we collect videos from the Internet sources by the same MCVC data collection pipeline mentioned in section 4.3, and we only leave the videos containing multiple concepts, including 1) multiple persons, 2) persons with livings, 3) persons with stuffs, 4) multiple livings, 5) livings with stuffs and 6) persons with both livings and stuffs. The number of samples for each scene is 50, 50, 50, 50, 50, and 33 respectively (Total 283). It is worth being noticed that we carefully guarantee there is no any overlap between the training videos and the constructed MC-Bench. 5.3. Comparing with other methods Comparing with naive solutions. We compare several open-sourced multi-concept image customization methods [30, 61, 67, 69], combining with the image-to-video (I2V) generation model I2VGen-XL [68], as naive solution for the MCVC task with our ConceptMaster. According to the quantitative and qualitative results in Tab. 1 and Fig. 5, we can see that our ConceptMaster has clear advantages on customizing multiple concepts in videos. The naive solution will be subject to the decoupling and Figure 6. Comparison with tuning-based method DreamBooth. representation ability of both two models, the instructionfollowing capability of I2V models could further influence the quality of generated videos. In contrast, the end-to-end video customization models could achieve better results. Comparing with tuning-based methods. We compare the classical tuning-based method DreamBooth [50], where we segment the original videos from MC-Bench into multiple clips for few-shot training examples and we train DreamBooth on our text-to-video models. The quantitative and qualitative results are demonstrated in Tab. 1 and Fig. 6, where we can see that Dreambooth could hardly solve the identity mixing problem. Additionally, the tuning-based methods always require users to manually collect few-shot training samples for additional parameter-tuning, which not only can be time-consuming, but collecting same concept in different scenes as training samples is cumbersome, the 7 Methods Concept Fidelity Decoupling Ability Video Quality Merge Textual and Visual Embeddings IP-Adapter-like ConceptMaster (Ours) 0.367 0.402 0.623 Table 2. Quantitative comparison of different multi-concept embeddings injection manner on MC-Bench, bold represents the best result and underline represents the second best result. All the methods we compared have been trained on the same data as that used by ConceptMaster. CLIP-T CLIP-I CLIP-T CLIP-I DINO-I Motion Smoothness Dynamic Degree Aesthetic Quality 29.418 26.812 29.461 Imaging Quality 0.666 0.653 0. 3.748 13.946 17.585 17.063 17.477 22.378 0.995 0.986 0.987 0.549 0.528 0.573 0.606 0.609 0.678 0.569 0.558 0. Methods Without Q-Former Without DAM Concate-MLP Self-Attn DAM (Ours) Concept Fidelity Decoupling Ability Video Quality CLIP-T CLIP-I CLIP-T CLIP-I DINO-I Motion Smoothness Dynamic Degree Aesthetic Quality 27.595 26.463 27.576 27.679 29. 16.881 16.481 15.125 16.436 17.585 19.256 19.718 20.535 21.946 22.378 0.512 0.537 0.583 0.603 0.618 0.277 0.394 0.546 0.581 0.623 0.589 0.583 0.643 0.641 0.678 0.98 0.985 0.984 0.986 0. 0.548 0.526 0.554 0.575 0.573 Imaging Quality 0.631 0.665 0.696 0.705 0.709 Table 3. Quantitative comparison of the design choice of Q-Former and DAM modules on MC-Bench, bold represents the best result and underline represents the second best result. All the methods we compared have been trained on the same data as that used by ConceptMaster. Figure 7. Different injection methods of multi-concept references. situation is more intense for multi-concept video customization task. Therefore, our ConceptMaster is obviously more practical for real-world applications. 5.4. Multi-Concept Embeddings Injection Manner In section 4.2, our ConceptMaster introduces standalone MC-Injector to integrate the multi-concept multi-modal Some previrepresentation into the diffusion models. ous methods, represented as BLIP-Diffusion [33] and IPAdapter [67], the former combines visual embeddings with textual caption embeddings as the whole condition representation, while the latter encodes the whole image as visual embeddings and aggregates into models by decoupled cross-attention layer. However, in multi-concept scenarios, merging multi-modal features as the whole conditions can make it challenging to distinguish the semantic meanings among different identities. In addition, integrating all visual concepts on one image is not the optimal choice for multi-concept representation, especially when they contain similar visual appearances. We conduct the above two integration approaches of the multi-concept embeddings on our text-to-video generation models with the same training data for ConceptMaster. In Tab. 2 and Fig. 7, we can see that when customizing multiple concepts, both these two methods can hardly maintain the concept fidelity and deal with the identity decoupling problem. Additionally, when merging the textual and visual embeddings, the instruction Figure 8. Demonstration of the effectiveness of the Q-Former and DAM modules. following ability is also unsatisfactory. The generated video has sense of splicing, and the dynamic degree is significantly reduced, as the original text cross-attention layer is influenced by the additional visual embeddings. Therefore, our designed ConceptMaster adopts the optimal solution to inject the decoupled multi-concept embeddings into the diffusion models. 5.5. Ablation Study In section 4.2, our ConceptMaster proposes to firstly utilizes Q-Former network to integrate the dense visual tokens extracted by CLIP image encoder into the comprehensive visual embeddings. While after simply replacing the Q-Former by an MLP layer, the generated videos cannot capture the appearances of given images, as in Fig. 8, and the quantitative metrics also largely drop in Tab. 3. In addition, in order to demonstrate the effectiveness of the designed DAM module, which is the intra-pair attention module based on paired visual embeddings and textual descriptions representation, we conduct several variants include: 1) Removing the DAM module, where only the extracted visual embeddings are further injected into the diffusion models, and the textual descriptions are unused. 2) Replacing the intra-pair attention by firstly concatenating the visual and textual embeddings along channel dimension (double the channel dimension), and integrating to the original channel dimension by an MLP layer. 3) Replacing the intra-pair cross-attention by self-attention, the features conduct selfattention are obtained by directly adding the visual and textual embeddings. In Tab. 3 and Fig. 8, we can see that the proposed DAM module is the optimal design. Cooperating the textual descriptions is significant, which not only enhances the uniqueness of each concept representation, but also assists the alignment of the multi-concept embeddings and the original diffusion model space. Furthermore, fusing visual and textual embeddings by the MLP layer is less comprehensive as it does not involve sufficient token-level interaction between them, lowering the concept representation and instruction-following in generated results. Additionally, the cross-attention operation is better than selfattention, which maintains consistent visual appearances in videos, while more artifacts (i.e., the cats head is split open) would be created by the self-attention operation. 6. Conclusion In this paper, we introduce ConceptMaster, an innovative framework that effectively addresses the critical issues of identity decoupling while maintaining concept fidelity when customizing multiple identities in videos. ConceptMaster introduces novel strategy for learning decoupled multi-concept embeddings and injecting them into diffusion models in standalone manner. This strategy ensures the quality of customized videos with multiple identities, even for highly similar visual concepts. To further address the scarcity of high-quality multi-concept video-entity data, we have established meticulous data construction pipeline. This pipeline enables the systematic collection of precise multi-concept video-entity data across diverse concepts. Additionally, we have designed comprehensive benchmark to validate the effectiveness of our model from three critical dimensions across six different concept composition scenarios. Extensive experiments demonstrate that ConceptMaster significantly outperforms previous approaches, paving the way for generating personalized and semantically accurate videos across multiple concepts."
        },
        {
            "title": "References",
            "content": "[1] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 3 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2, 3 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 6 [5] Brandon Castellano. PySceneDetect. 5 [6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 2, 3 [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 5, 2 [8] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level imIn Proceedings of the IEEE/CVF Conage customization. ference on Computer Vision and Pattern Recognition, pages 65936602, 2024. 2, [9] Jooyoung Choi, Yunjey Choi, Yunji Kim, Junho Kim, and Custom-edit: Text-guided image editarXiv preprint Sungroh Yoon. ing with customized diffusion models. arXiv:2305.15779, 2023. 2 [10] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024. 1 [11] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. 2 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen9 Or. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 3 [14] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023. 2, 3 [15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3 [16] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact paarXiv preprint rameter space for diffusion fine-tuning. arXiv:2303.11305, 2023. 2 [17] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 2, [18] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity arXiv preprint video generation with arbitrary lengths. arXiv:2211.13221, 2022. 2 [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1 [20] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for and Jie Tang. text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3 [21] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-strength Natural Language Processing in Python. 2020. 6 [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2 [23] Yuzhou Huang, Yiran Qin, Shunlin Lu, Xintao Wang, Rui Huang, Ying Shan, and Ruimao Zhang. Story3d-agent: Exploring 3d storytelling visualization with large language models. arXiv preprint arXiv:2408.11801, 2024. 3 [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [25] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image In Proceedings of the IEEE/CVF Conference prompts. on Computer Vision and Pattern Recognition, pages 6689 6700, 2024. 2, 3, 5 [26] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Texttoimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 2 [27] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [28] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3, 5, 1 [30] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 2, 3, 7 [31] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 3 [32] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 6 [33] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 4, 6, 8 [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2, 4 [35] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. 2, 3, 4 [36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [37] Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang, Xintao Wang, Yujiu Yang, and Ying Shan. Stylecrafter: Enhancing stylized text-to-video generation with style adapter. arXiv preprint arXiv:2312.00330, 2023. 2 [38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 6 [39] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3, 5, 1 [40] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 4 [41] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion modIn Proceedings of els for high-quality video generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1020910218, 2023. 2 [42] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 5 [43] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. Advances in Neural Information Processing Systems, 36, 2024. 6 [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 4, 6, 1 [46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [47] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 3, 1 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. 3 [50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2, 3, 7 [51] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85438552, 2024. [52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [53] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1 and Stefano Ermon. arXiv preprint [54] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 5 [55] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [56] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 2 [57] Alex Wang. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. 4 [58] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2, 3 [59] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5 [60] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2, [61] Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. 2, 3, 5, 7 [62] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. 2, 3, 4 [63] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. 3 [64] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 2 [65] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 3, 4 [66] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2025. 4 [67] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 4, 7, 8 [68] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 2, 7 [69] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. 3, 7, 2 [70] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 3 [71] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. [72] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebvhq: large-scale video facial attributes dataset. In European conference on computer vision, pages 650667. Springer, 2022. 3, 6 12 ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning"
        },
        {
            "title": "Supplementary Material",
            "content": "We provide the following contents in supplementary materials: 1. Introduction of our text-to-video diffusion transformer models. 2. Implementation Details of ConceptMaster. 3. Discussions on Comparison between Our Data Collection Pipeline and Grounded-SAM. 4. Comparison Methods Implementation. 5. More Discussions on Multi-Concept Embeddings Injection. 6. More Discussions on Ablation Study. 7. More Qualitative Results Demonstration. 1. Introduction of our text-to-video diffusion transformer models We utilize transformer-based latent diffusion model as the foundational text-to-video (T2V) generation model, as depicted in Fig. 9. Initially, we employ 3D Variational Autoencoder (3D-VAE) to transform videos from the pixel space into latent space, upon which we build transformer-based video diffusion model. Unlike previous models that rely on UNets or transformers with an additional 1D temporal attention module for video generation, our approach addresses the limitations of spatiallytemporally separated designs, which often do not yield optimal results. We replace the 1D temporal attention with 3D self-attention, allowing the model to more effectively perceive and process spatiotemporal tokens. This results in high-quality and physically coherent video generation model. Specifically, before each attention or feed-forward network (FFN) module, we map the timestep to scale and apply RMSNorm to the spatiotemporal tokens. 2. Implementation Details of ConceptMaster Implementation Details. We train ConceptMaster using our proprietary transformer-based text-to-video diffusion models. Initially, we employ the CLIP image encoder [45] as the external vision encoder to extract visual features from reference images. Subsequently it is followed by the stacked cross-attention and FFN layers, collectively referred to as the Q-Former, and an additional cross-attention layer for the DAM module. During training, we drop the video captions and reference conditions (both paired images and text descriptions) with probabilities of 50% and 33% for classifier-free guidance [19], respectively. We freeze the 3D spatiotemporal self-attention layer and fine-tune the other Figure 9. Overview framwork of our base text-to-video generation models. parameters of the transformer to enhance video dynamics. Consequently, the entire transformer backbone, except for the 3D spatiotemporal layer, the Q-Former, and the DAM module, are jointly optimized. Additionally, inspired by NaViT [10], we adopt the similar strategy of padding the videos to the same height and width with effective attention masks within each batch during training, and the training video segments consist of 77 frames, corresponding to duration of 5 seconds at 15 frames per second (fps). The training process employs the Adam optimizer [28] and is conducted on 64 NVIDIA H800 GPUs, with learning rate set to 5 106 and global batch size of 256. During inference, we utilize 100 DDIM steps [53] and set the CFG scale to 7.5, and the inference videos are uniformly resized to resolution of 384 672 pixels. 3. Discussions on Comparison between Our Data Collection Pipeline and GroundedSAM Previous studies typically exploit open-set object detection and segmentation methods, represented by Groundedto extract concepts information in SAM [29, 39, 47], source images or videos. However, we claim that the simplistic method is far insufficient for our objectives, since Grounding-DINO is built on top of the CLIP text encoder, which often perform poorly in distinguishing similar concepts, especially those that have high visual appearance or textual semantic similarity (see Fig 4(a)). Additionally, we need to guarantee the quality in high standard of the source data as well as the extracted concept information (e.g., appropriate size and completeness) to train the model for our task. We carefully design the data collection pipeline into two levels including: 1) Fast elimination of unsuitable 1 videos and 2) Fine-grained identity information extraction. In order to evaluate the effectiveness of our dataset pipeline, we first randomly sample 2000 samples from Panda-2M [7], and we count the success rate of collect videos. The standard for success rate statistics is based on whether there are errors in the extracted concepts information from video (i.e., wrong classification label for the concepts). success is defined as the absence of any errors, while the presence of any error is considered failure. We additionally hire 20 experienced workers in data construction to manually evaluate the results of the constructed evaluation samples from Panda-2M. We present the success rate statistics in Fig. 4(b). The results indicate that the success rate of our designed data pipeline is significantly higher than that achieved by simply using Grounded-SAM. Grounded-SAM exhibits success rate of only 52%, which poses challenges in training robust model capable of representing and decoupling multiple concepts in generated videos, especially when dealing with MCVC data that contains numerous errors. Consequently, our constructed data pipeline is essential for high-quality MCVC data collection. We also hope that our data collection process could provide inspiration for future MCVC works. 4. Comparison Methods Implementation We supplement the implementation details of the compared methods for the MCVC task. Comparing with naive solutions. We compare several open-sourced multi-concept image customization methods, including CustomDiffusion [30], SSR-Encoder [69], IPAdapter [67] and MS-Diffusion [61], combining with the image-to-video (I2V) generation model I2VGen-XL [68], as naive solution for the MCVC task with our ConceptMaster. While SSR-Encoder, IP-Adapter and MS-Diffusion do not need additional training and can be viewed as feedforward customization models, CustomDiffusion requires users to manually few-shot examples for additional parameter training. Therefore, we collect the few-shot training samples from videos, where we randomly sample 15 frames as training images for each concept. Additionally, we follow its usage requirements, where we label specific character for each concept (i.e., s1* an elephant is playing with s2* dog on the road). In addition to Tab. 1 and Fig. 5 in the main paper, we demonstrate more quantitative results in Fig. 10. Comparing with aforementioned methods, our end-to-end video customization models ConceptMaster could achieve better generation results more practicality than two-stage solutions. Comparing with tuning-based methods. We compare the classical tuning-based method DreamBooth [50], where we segment the original videos into multiple clips for few-shot training examples and we train DreamBooth on our textto-video models. We also follow its requirements to label specific character for each concept (i.e., s1* man and s2* woman dancing on city street). In addition to Tab. 1 and Fig. 6 in the main paper, we demonstrate more quantitative results in Fig. 11. Comparing with DreamBooth, our ConceptMaster not only achieves significant advantages on both generated results and practicality, but also has better dynamic degree since DreamBooth also demonstrates poor dynamics under multi-concept scenarios. 5. More Discussions on Multi-Concept Embeddings Injection We demonstrate more quantitative results between these three different multi-concept embeddings injection methods in Fig. 12. Our key insight is to inject the represented multi-concept embeddings into the diffusion models in standalone cross-attention layer. While previous methods that the most representative ones include 1) BLIPDiffusion [33], which combines visual and textual caption embeddings as the whole condition representation. 2) IPAdapter [67], which encodes the whole image as visual embeddings and aggregates into models by decoupled crossattention layer. These two technical thoughts are widely adopted in previous image and video concept customization, which are different from our core insight. In addition to Tab. 1 and Fig. 5 in the main paper, we demonstrate more quantitative results in Fig. 12, where both these two methods can hardly deal with the identity decoupling problem with there are similar concepts (e.g., man and girl). Even when concepts have huge semantic differences (i.e., woman and dog), both two methods cannot maintain the concept fidelity of each concept. Additionally, when merging the visual and textual embeddings together as the conditions, the instruction following ability becomes very bad and the generated videos have the copy-paste effects. Therefore, our ConceptMaster adopts the most suitable manner of the injection of the multi-concept embeddings, which could represent and decouple multiple identities well. 6. More Discussions on Ablation Study We demonstrate more quantitative results of the effectiveness of the Q-Former and DAM modules in Fig. 13. Initially, our ConceptMaster proposes to firstly utilizes QFormer network to integrate the dense visual tokens extracted by CLIP image encoder into the comprehensive visual embeddings. Additionally, we introduce the DAM module, which conducts the intra-pair attention module based on paired visual embeddings and textual descriptions representation. These designs demonstrate effective capability of decoupling and representing of multiple given references. While we also conduct several ablation architectures: 1) Replacing the Q-Former by an MLP layer, in 2 Fig. 13 we can see that the visual appearances of provided man and red jacket could no longer be captured by the generation results. Therefore, the Q-Former is significant to assist the representation of comprehensive visual embeddings. 2) Removing the DAM module, where only the extracted visual embeddings are further injected into the diffusion models, and the textual descriptions are unused. Similar phenomenon could be observed in Fig. 13 as the concepts could not keep their fidelity. This is because the absence of adequate text label representation fails to effectively represent and differentiate the uniqueness of multiple concepts, and it also results in poor alignment with the original diffusion space. 3) Replacing DAM by firstly concatenating the visual and textual embeddings along channel dimension, and downsampling the dimension to the original one by an MLP layer. While the insufficient representation would lead to the inharmonious combination of multiple concepts. Since when concatenating along the channel dimension and then downsamling by MLP, the tokens could not conduct interaction among them. As in Fig. 13, the way that the man dressed in red jacket is unreasonable. 4) Replacing the intrapair cross-attention by self-attention, where we firstly add the visual and textual embeddings and then conduct the selfattention operation. According to the qualitative and quantitative results in Tab. 3 and Fig. 8 in main paper and Fig. 13, the the cross-attention operation is better than self-attention, as the latter easily leads to more artifacts and inharmonious movements. Therefore, our proposed Q-Former and DAM modules would be the best designated architectures to simultaneously represent and decouple multiple references, and could create high-quality customized videos. 7. More Qualitative Results Demonstration Our ConceptMaster could create high-quality and conceptconsistent customized videos based on given multiple reference images in diverse scenarios, including but not limited to 1) multiple persons, 2) persons with livings, 3) persons with stuffs, 4) multiple livings, 5) livings with stuffs and 6) persons with both livings and stuffs. We demonstrate more qualitative results including these scenes in Fig. 14 and Fig. 15. Figure 10. More Qualitative comparison on multi-concept customization between ConceptMaster and naively combining the multi-concept image customization with image-to-video generation models. Figure 11. More Qualitative comparison on multi-concept customization between ConceptMaster and DreamBooth. 4 Figure 12. More Qualitative comparison on different injection methods of multi-concept references. Figure 13. More Qualitative comparison the effectiveness of the Q-Former and DAM modules. Figure 14. More qualitative results of ConceptMaster on diverse scenarios (1/2). 6 Figure 15. More qualitative results of ConceptMaster on diverse scenarios (2/2)."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Sun Yat-sen University",
        "The Chinese University of Hong Kong, Shenzhen",
        "Tsinghua University"
    ]
}