{
    "paper_title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search",
    "authors": [
        "Yiming Jia",
        "Jiachen Li",
        "Xiang Yue",
        "Bo Li",
        "Ping Nie",
        "Kai Zou",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 2 8 5 0 1 . 3 0 5 2 : r VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search k,tYiming Jia*, nJiachen Li, mXiang Yue, zBo Li, xPing Nie, yKai Zou, kWenhu Chen kUniversity of Waterloo, tUniversity of Toronto, nUC Santa Barbara, mCMU, zNUS, xIndependent, yNetmind.ai {yiming.jia@mail.utoronto.ca, wenhuchen@uwaterloo.ca} https://tiger-ai-lab.github.io/VisualWebInstruct"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack In this work, of high-quality and diverse training data. we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct novel approach that leverages search engine to create diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through pipeline of content extraction, filtering and synthesis, we build dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Prostd (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs reasoning capabilities for complex multimodal tasks. 1. Introduction Vision-Language Models (VLMs), such as Llava [33] and Gemini [49], are designed to process multimodal inputs, including images, videos, and text. While VLMs have recently demonstrated significant progress in straightforward perceptual tasks such as VQA [3], DocVQA [41], *Published during an internship at University of Waterloo Figure 1. Overview of our automated data curation approach and major experimental results. and VizWiz [20], they often struggle with more complex tasks such as MMMU [62], MathVista [38], and MEGABench [8], which require multi-step, deliberate reasoning [15, 54]. One major bottleneck for existing VLMs is the scarcity of reasoning-focused training datasets. Current multimodal reasoning datasets exhibit several limitations: (1) Many datasets, such as FigureQA [25], MapQA [6], GeoQA [7], and ChartQA [40], focus narrowly on specific types of scientific images. (2) Some datasets rely on synthetic images generated through predefined rules, such as CLEVR [24] and Geo170K [16], which often result in poor generalization to real-world visual reasoning tasks. (3) Other training datasets, such as AI2D [26] and ScienceQA [45], are relatively small and simplistic, primarily covering elementary-level visual knowledge. Due to these limitations, VLMs fail to acquire diverse reasoning skills, leading to slower progress on reasoning-intensive benchmarks compared to language models. 1 Dataset Size Source & Domains Coverage ScienceQA [45] IconQA [37] Geo170K [16] CLEVR [24] FigureQA [25] ChartQA [40] Math360V [47] Mulberry [60] Llava-CoT [57] Elementary and high school science 21K 107K Abstract diagrams and visual reasoning 170K Synthesized from LLMs 700K Synthesized from rules 1.3M Synthesized from rules 23K Charts from Staista, Pew, etc Science Q&A, diagrams, K-12 Exam Visual reasoning, diagrams Geometry Shapes Bar, Line, Pie Charts 260K FigureQA [25], CLEVR [24], IconQA [37], etc 260K Geo3K [36], IconQA [37], ChartQA [40], ScienceQA [45], etc Geo, Figure, Medical, K-12 Exam 100K ChartQA [40], AI2D [26], GeoQA [7], CLEVR [24], etc Geo, General VQA, K-12 Exam Math reasoning, diagrams VISUALWEBINSTRUCT 906K Internet (Homework Website, Forums, etc) All Above + College Exams Table 1. Comparison between our dataset and the existing datasets. VISUALWEBINSTRUCT is the most diverse dataset with very broad coverage of disciplines and image types. Given the difficulty of human annotation, we draw inspiration from WebInstruct [64] to mine naturally existing reasoning-focused instruction data from the internet. While WebInstruct retrieves reasoning-focused text data from Common Crawl1, their approach is infeasible for the multimodal domain due to two key challenges: (1) the lack of comparable large-scale multimodal dataset, and (2) the unreliability of current multimodal retrieval models. To address these challenges, we leverage commercial web image search tools, such as Google Image Search, which offer high coverage and accuracy. We begin by collecting approximately 30,000 seed images across multiple disciplines, including Accounting, Chemistry, Mathematics, and Physics. These images serve as queries for Google Image Search [67] to identify websites containing similar images. We then download the HTMLs from these websites and extract their accessibility trees, which are processed by an LLM to extract QA pairs (if any) for an initial dataset. However, we found that over half of the extracted questions lack annotated answers due to three primary reasons: (1) these websites do not provide answers, (2) some require membership to access, and (3) some necessitate user interaction to reveal the answers. To address this, we use GPT-4o [22] to synthesize multiple candidate solutions for each question, filtering for consistency among responses. Finally, we align the selected answers with the content from original webpage to remove potential inaccurate ones. Through this sophisticated process, we construct VISUALWEBINSTRUCT, dataset containing approximately 900K QA pairs, where 40% are visual QA pairs associated with 163,743 unique images, while the remaining 60% are text-only QA pairs. Most of them are exam-like problems requiring deliberate reasoning. Table 1 compares VISUALWEBINSTRUCT with other datasets in terms of source and coverage. Our dataset comprises highly diverse, human-created scientific images 1https://commoncrawl.org/ spanning multiple disciplines and levels of complexity. Its broad coverage and increased difficulty make it particularly well-suited for improving VLM performance on realworld tasks requiring multi-step reasoning. To evaluate the effectiveness of VISUALWEBINSTRUCT, we perform supervised fine-tuning on MAmmoTH-VL [19] and LlavaOV-mid [28]. Comprehensive evaluations across seven visual reasoning benchmarks, including MMMU [62], MathVista [38], and Dyna-Math [70], demonstrate substantial performance gains. When fine-tuning LlavaOV-mid, we observe an absolute improvement of 10 20 percentage points across these benchmarks. When fine-tuning MAmmoTH-VL, our model MAmmoTH-VL2 achieves state-of-the-art performance (within the 10B parameter range) on several benchmarks, including MMMUPro-std (40.7%), MMVet (64.5%), MathVerse (42.6%), and Dyna-Math (55.7%). MAmmoTH-VL2s average performance across seven benchmarks surpasses strong competitors such as InternVL2.5 [9] and Phi-4-Mini [1], underscoring the effectiveness of VISUALWEBINSTRUCT in enhancing VLMs reasoning capabilities. Our contributions can be summarized as follows: We propose scalable pipeline for acquiring high-quality multimodal reasoning data from the internet, ensuring both scalability and quality. We introduce VISUALWEBINSTRUCT, diverse and comprehensive multimodal instruction dataset, which we will publicly release to the research community. We develop MAmmoTH-VL2, 7B-parameter visionlanguage model fine-tuned on VISUALWEBINSTRUCT, achieving state-of-the-art performance among models of comparable size and excelling in complex reasoning tasks requiring multi-step deliberation with visual context. In the following sections, we will first talk about how we mine the data from the Internet in section 2 and then talk about how to refine it in section 3. Finally, we show our experimental results in section 4. 2 Figure 2. Comprehensive Pipeline for VISUALWEBINSTRUCT Dataset Generation. The workflow illustrates our multi-stage approach for creating high-quality multimodal instruction data. Stage 1: starting with seed images, we leverage Google Image search to identify relevant webpages, which are processed into accessibility trees. The raw QA pairs are extracted from the trees and refined through post-processing step to ensure the vadality the data. Stage 2: we first generat multiple synthesized answers for consistency filtering, then align these with original web-sourced content to enhance the accuracy of the answers. 2. Stage 1: Mining Data from the Internet Our data mining pipeline follows systematic approach to extract image-rich QA pairs from the internet. We begin with approximately 30K scientific images as seed data spanning multiple disciplines. We employ Google Image Search to identify visually similar content, gathering 758,490 unique URLs. After filtering out irrelevant domains, we construct accessibility trees for the relevant websites to extract meaningful content, preserving both textual and visual information while eliminating non-essential elements. We then leverage the Gemini 1.5 Flash model in two-stage process: first to automatically extract QA pairs from the accessibility trees and then to filter these pairs based on comprehensive quality criteria, including question validity and image relevance, ensuring the educational value and integrity of the final dataset. 2.1. Seed Data collecting Due to the limited availability of image-rich QA datasets and the predominant focus on mathematics in existing datasets, creating comprehensive QA dataset that incorporates diverse subjects and abundant visual content is essential. Our seed dataset consists of approximately 30,000 images, which were crawled from Stemez2 in compliance with copyright regulations. These images span multiple disciplines, including mathematics, physics, accounting, chemistry, engineering, and biology, ensuring both subject diversity and visual richness. 2.2. Google Image Searching Using the seed images, we conducted Google Image searches to find visually similar content across the web. Leveraging Google Lens (Figure 3), we collected approxi2https://stemez.com/subjects/science/ mately 60 URLs per image, resulting in total of 1,747,634 URLs containing visually similar content. Many websites with non-permissive licenses implement anti-crawling mechanisms, and we ensured compliance by avoiding data collection from such sources. We applied rigorous deduplication and filtering, removing URLs from domains unlikely to contain educational content (e.g., video platforms and image repositories). This refinement yielded 758,490 unique, high-quality URLs for further processing. By using images as primary search keys, we ensured strong visual and contextual connections between the collected data and our seed dataset, effectively preserving the original distribution while significantly expanding its coverage. Figure 3. Example of Google Lens search functionality for circle geometry problems. 2.3. Accessibility Tree Building After filtering out irrelevant domains, we processed the HTML content of each remaining URL to construct accessibility trees that capture essential textual and visual information. As illustrated in Figure 4, our implementation focuses on extracting meaningful text content and image elements while filtering out non-essential components such as navigation menus, advertisements, and auxiliary elements. We developed tree-based structure where each node represents either textual content or an image, preserving the hierarchical relationships present in the original HTML while removing unnecessary markup and styling information. The resulting accessibility trees provide clean, hierarchical representation of each webpages content, making subsequent QA pair extraction more efficient and reliable. Figure 4. Example of an accessibility tree structure extracted from an educational website. 2.4. QA Pairs Extraction After constructing accessibility trees, we prompt the Gemini 1.5 Flash model to identify and extract high-quality QA pairs from webpage content. We designed structured prompt instructing the model to extract complete question text, identify relevant question-related images, and extract comprehensive solution details while preserving mathematical notations and step-by-step explanations. This approach maintains the educational integrity of the extracted content by preserving its original formatting, mathematical expressions, and logical structure, ensuring technical accuracy throughout the extraction process. Through this method, we extracted total of 421,320 raw QA pairs from the webpages, with approximately 60% containing images. We then implemented post-processing stage using the Gemini 1.5 Flash model to ensure dataset quality by evaluating both textual content and images. Our evaluation framework assessed two key criteria: question validity and meaningfulness, as well as the relevance and clarity of question-related images. By prompting Gemini to verify whether images are properly referenced, clear, visible, and contribute to understanding the question, we established strict validation criteria for retaining QA pairs. This post-processing step significantly improved dataset quality by removing incomplete, unclear, or irrelevant content while preserving educational integrity and effectiveness. Our analysis shows that out of 421,320 processed pairs, 361,015 (85.7%) were valid, while 60,305 were filtered out as invalid. Similarly, out of 449,859 total images processed, 331,818 (73.76%) were deemed valid and relevant to their corresponding questions. 3. Stage 2: Dataset Refinement After Stage 1, we obtain large amount of raw data from the Internet. However, this data contains notable level of noise. For instance, more than half of the questions lack corresponding answers due to various issues, such as (1) membership requirements, (2) interaction requirements, and (3) the absence of an answer. Thus, second round of refinement is necessary to further improve the dataset quality. 3.1. Answer Refinement We implemented comprehensive refinement process to ensure consistency and quality in our dataset. This step was critical in addressing potential variations or inconsistencies in the extracted answers, thereby creating high-fidelity dataset for model training. Our refinement methodology leveraged GPT-4os capabilities in two-stage process. First, for each question and its associated images, we prompted GPT-4o [22]3 to generate four different answer variations. This approach allowed us to obtain multiple perspectives on each question. Next, we employed GPT-4o as an LLM judge to determine whether the synthesized responses aligned with each other. As illustrated in Figure 5, we evaluated whether the conclusions were mutually consistent across these responses. This evaluation was particularly important for questions in domains such as mathematics and physics, where precision and correctness are paramount. Only when more than half of the synthesized responses demonstrated consistency did we retain the question along with the consistent responses. This rigorous consistency check served as an additional quality filter, ensuring that our dataset contained highly accurate and unambiguous answers that could be reliably used for model training. Through this refinement process, we successfully created dataset in which all responses were systematically 3We compared GPT-4o and Gemini-1.5 and found that GPT-4os outputs were significantly more reliable. Therefore, we adopted GPT-4o. generated by GPT-4o, ensuring consistent style and level of quality throughout the collection. The resulting dataset comprises 1.04 million QA pairs spanning multiple disciplines, representing one of the largest collections of consistency-verified multimodal instruction data available. 3.2. Answer Alignment The final step in our quality assurance process involved answer alignment to further enhance accuracy. While the previous refinement step generated consistent answers using GPT-4o, we recognized the importance of validating these against authoritative content from the original web sources. Figure 5. Illustration of our consistency checking methodology. In this step, we used Gemini-2.0-Flash to measure the alignment between GPT-generated responses and the original extracted answers, if available. In cases where the comparison indicated inconsistency, we preserved the original web-sourced answer. Conversely, when the Gemini model determined strong alignment between the generated and web-sourced answers, we retained the GPT-generated version. Through this alignment process, we combined the consistency of model-generated content with the authority of original educational materials in balanced manner. 3.3. Dataset Statistics The statistics presented in Table 2 illustrate the distribution of knowledge domains in our dataset, VISUALWEBINSTRUCT. While the major categories are shown in the table, the Others category (6.60%) comprises General Knowledge (2.45%), Computer Science (2.25%), Biology (1.40%), and humanities subjects, including Language/Literature (0.25%), Social Sciences (0.20%), and Arts (0.05%). This distribution reflects the datasets strong quantitative orientation while ensuring sufficient breadth. Table 3 summarizes the statistics after each step of the VISUALWEBINSTRUCT pipeline, showing the data progression through two main stages. Our approach effectively scaled the initial 30,000 seed images into comprehensive multimodal instruction dataset containing 900K instruction data. The final dataset includes 347,313 image-associated QA pairs (approximately 38% of the total) supported by 163,743 unique images. We also conducted thorough decontamination checking to ensure our training dataset does not contain any data from the evaluation benchmarks, thereby maintaining the integrity of our experimental results."
        },
        {
            "title": "Percentage Num of QA Pairs",
            "content": "Math Physics Finance Chemistry Engineering Others 62.50% 14.50% 7.25% 4.80% 4.35% 6.60% 566K 132K 66K 43K 39K 60K Table 2. Distribution of Categories in VISUALWEBINSTRUCT 4. Experiments We detail the training and evaluation details of our experiments in this section. 4.1. Experimental Setup For our experiments, we directly fine-tuned an existing MAmmoTH-VL checkpoint on our VISUALWEBINSTRUCT dataset. We refer to our resulting model as MAmmoTH-VL2. The architecture consists of language tower based on Qwen2.5-7B-Instruct [59], vision tower using SigLip [65], and projector module connecting these components, following Llava-OneVision [28, 33]. To enhance data diversity, we employed data mixing strategy that combined our VISUALWEBINSTRUCT dataset with modified LLaVA-CoT data [57] (with CoT prompting tags removed) in 9:1 ratio, resulting in approximately 900K samples from VISUALWEBINSTRUCT and 100K samples from the modified LLaVA-CoT dataset. This mixing strategy empirically improved our models performance across diverse visual reasoning tasks. We employed supervised fine-tuning (SFT) approach with batch size of 256. The learning rate was set to 1 105 for the language model and projector components, while the vision encoder was fine-tuned with lower rate of 2 106 to preserve its pre-trained visual recognition capabilities. The model was trained for single epoch, which proved sufficient given the high quality and diversity of our dataset. Input images were processed at resolution of 384 384 with appropriate adjustments for varied aspect ratios. We limited input sequences to maximum of 8,192 tokens to accommodate detailed reasoning chains while maintaining computational efficiency. This fine-tuning approach enabled MAmmoTH-VL2 to leverage the strong multimodal reasoning foundation of MAmmoTH-VL while enhancing its performance on our targeted visual reasoning tasks that require multi-step deliberation with visual context. Processing Stage Total QA Pairs Image-Associated QA Unique Questions Total Images Unique Images Stage 1: Mining Data from the Internet Seed Data Collection QA Pairs Extraction Post-Processing - 421,320 361, - 248,643 159,059 - 421,320 361,015 30,000 552,269 331,818 Answer Refinement Answer Alignment 1,041,598 906,160 407,218 347, 257,201 257,201 577,455 475,099 Stage 2: Dataset Refinement 30,000 362,728 212,530 167,493 163,743 Table 3. Statistics of different milestones in the data processing pipeline of VISUALWEBINSTRUCT. Model Size MMMU MMMU-Pro MMMU-Pro MathVista MMVet MathVerse Dyna-Math Avg val standard vision testmini test testmini test GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet - - - Molmo Llava-OV Llama-3.2-Inst Qwen2-VL MAmmoTH-VL InternVL2.5 Phi-4-mini DeepSeek-VL2 Llava-CoT-L Llava-CoT-M LlamaV-o1 Mulberry Insight-V MM-Eureka MAmmoTH-VL2 over SoTA 8B 7B 11B 7B 7B 7B 5.6B 27B 11B 7B 11B 7B 8B 8B 7B 69.1 59.1 68.3 45.3 48.8 50.7 52.1 50.8 55.8 55.1 51.1 50.1 51.4 49.1 55.0 50.2 49.2 54.7 -1. Closed-sourced Models 54.0 49.4 55.0 49.7 65.8 48.0 63.8 63.9 67.7 Open-source General Vision-Language Models 28.3 29.5 33.0 37.0 33.2 38.2 39.7 31.4 31.6 33.0 31.5 36.8 30.7 - 40.7 +1.0 18.9 18.7 23.7 26.9 25.3 30.4 31.2 24.3 20.4 23.7 22.4 23.6 20.5 - 26.3 -4.9 51.6 63.2 51.5 58.2 66.0 64.4 62.4 62.8 54.8 63.8 54.4 63.1 59.9 67.1 68.1 +2.1 76.2 64.0 75. 58.0 58.6 59.3 62.0 62.3 62.8 60.5 - 60.3 58.6 63.6 60.9 60.8 60.7 64.5 +0.9 50.2 41.2 44.2 18.9 26.2 31.6 28.2 34.2 39.5 37.6 - 30.2 39.4 - 31.0 28.7 40.4 42.6 +3.1 63.7 64.8 60. 41.6 40.3 40.5 42.1 44.7 49.8 51.4 - 44.8 48.3 - 45.1 47.8 - 55.7 +4.3 61.0 58.3 59.9 37.5 40.8 41.5 43.8 45.2 48.7 48.6 - 41.7 45.5 - 45.0 42.6 - 50.4 +1.7 Table 4. Evaluation Results of our model and other baseline models. Most of the baseline results are taken from other papers. The best and second-best results across all open-source models are highlighted in bold and underlined, respectively. 4.2. Evaluation Setup To assess the capabilities of MAmmoTH-VL2, we conducted comprehensive evaluation across multiple multimodal benchmarks that specifically test visual reasoning and knowledge application. Our evaluation framework focuses on benchmarks that require complex reasoning with visual context. We evaluate our model on seven key benchmarks that collectively provide comprehensive assessment of multimodal reasoning capabilities: MMMU [62]: Tests multimodal understanding across university-level domains, requiring integration of visual and textual information. MMMU-Pro [63]: Advanced versions of MMMU with more challenging problems and more distractor options that require sophisticated visual reasoning. MathVista [38]: Evaluates mathematical reasoning with visual inputs, testing the models ability to process visual information for solving complex math problems. MMVet [61]: Assesses general multimodal understanding across diverse tasks and contexts. MathVerse [68]: Focuses on mathematical reasoning with visual components and relies less on text hints, requiring complex visual reasoning. Dynamath [70]: Tests dynamic mathematical reasoning capabilities with visual context. For all evaluations, we used greedy decoding in zeroshot setting to ensure fair comparison with existing models. We categorize the comparison models into three groups: closed-source models (GPT-4o, Gemini-1.5-Pro, Claude-3.5-Sonnet), open-source vision-language models 6 (Qwen2-VL [53], LLaVA-OV [28], Molmo [13], etc.), and reasoning-enhanced vision-language models (LLaVACoT [57], Mulberry [60], etc). We include Llava-CoT-L, which is trained from Llama-3.2 [18] and Llava-CoT-M, which is trained from MAmmoTH-VL [19]. To ensure standardized and reproducible evaluations, we employed LMMsEval [66], comprehensive evaluation framework for multimodal language models. For all evaluations, we used greedy decoding in zero-shot setting to ensure fair comparison with existing models. Our approach allows for direct comparison with models of comparable size, providing insights into the value of the VISUALWEBINSTRUCT dataset. Performance is reported using accuracy scores for each benchmark, with an average score across all benchmarks to indicate overall model capability. 4.3. Experimental Results Here we evaluate our results from different perspectives. Quantitative Results The table 4 presents the performance of MAmmoTH-VL2 compared to various multimodal models across seven benchmarks. Our analysis reveals several important findings regarding the effectiveness of models fine-tuned on VISUALWEBINSTRUCT. Overall Performance. MAmmoTH-VL2 achieves an average accuracy of 50.4% across all benchmarks, outperforming other open-source vision-language models of comparable size (7B-11B parameters). This represents significant improvement over standard vision-language models like Qwen2-VL (43.8%), LLaVA-OV (40.8%), and Molmo It even beats the very recent model like In- (37.5%). ternVL2.5 [9] and Phi-4-mini-Multimodal [1]. Mathematical Reasoning Capabilities. MAmmoTH-VL2 demonstrates particularly strong performance on mathematical reasoning tasks. On MathVista, our model achieves 68.1% accuracy, surpassing all the open-source and closedsource models. The models performance on MathVerse (42.6%) and Dyna-Math (55.7%) further confirms its enhanced capability for visual reasoning. Complex Reasoning Tasks. On MMMU-Pro-std with 10 options, MAmmoTH-VL2 achieves 40.7% accuracy, showing significant improvement over other 7B models such as LLaVA-OV (29.5%) and Qwen2-VL (37.0%). This demonstrates that our approach effectively enhances the models ability to perform complex reasoning across diverse domains beyond mathematics. Gap with Larger and Closed-Source Models. While MAmmoTH-VL2 outperforms open-source models of comparable size, there remains gap with closed-source models such as GPT-4o, Gemini-1.5-Pro, and Claude-3.5-Sonnet. This indicates potential for further improvements through scaling or enhanced training methodologies. Comparison with Reasoning-Enhanced Models. Among the reasoning-enhanced vision-language models like LlavaCoT, Mulberry [60], LlamaV-o1 [51] and Insight-V [14], MAmmoTH-VL2 demonstrates competitive performance, achieving results comparable to or better than specialized models like LLaVA-CoT and Mulberry. For instance, on MMMU-Pro Vision, our model achieves 26.3% accuracy, outperforming LLaVA-CoTMs 23.7%. Notably, other reasoning-enhanced models often utilize complex methodologies in either the training or inference stage to enhance their chain-of-thought abilities, which makes the development process and deployment more complicated. In contrast, MAmmoTH-VL2 achieves much better reasoning capabilities through our straightforward fine-tuning approach on VISUALWEBINSTRUCT, offering simpler yet effective solution compared to the other approaches. These results confirm that fine-tuning on VISUALWEBINSTRUCT significantly enhances the models reasoning capabilities. The consistent performance improvements across diverse benchmarks from non math-related and math-related domains demonstrate the effectiveness of our approach in developing more capable multimodal reasoning models. We believe our dataset can be utilized to augment future vision-language models. 4.4. Ablation Study The ablation study in Table 5 demonstrates the impact of different training datasets and their combinations on model performance across multiple visual reasoning benchmarks. Two base models were evaluated: Llava-OV-mid and MAmmoTH-VL. For Llava-OV-mid, the baseline starts at 26.3% average score across benchmarks. Training with Llava-CoT data improves this to 33.6%, while training on VISUALWEBINSTRUCT yields an even better 38.1%, with with MMVet performance notably jumping from 32.1% to 57.6%. The combined training approach (VISUALWEBINSTRUCT +LlavaCoT) achieves the best overall performance at 39.7%. The stronger MAmmoTH-VL model begins with an average score of 45.4%. Training with VISUALWEBINSTRUCT improves the average to 49.0%, showing gains across multiple benchmarks, particularly in MMMU-Pro vision and Dyna-Math tests. As with Llava-OV-mid, the combined training approach works best, reaching 50.4% average score, with notable improvements in MMMU (54.7%), MMMU-Pro standard (40.7%), and Dyna-Math (55.7%). The key findings indicate strong data complementarity between VISUALWEBINSTRUCT and Llava-CoT, with their combination consistently delivering the best results. We also observe that weaker base models show larger relative improvements from training. Overall, the ablation study confirms that our VISUALWEBINSTRUCT dataset significantly boosts model performance across all benchmarks, demonstrating its effectiveness in enhancing visual reasoning capabilities regardless of the base model. 7 Training Data MMMU MMMU-Pro MMMU-Pro MathVista MMVet MathVerse Dyna-Math Avg val standard vision testmini test testmini test - Llava-CoT Ours Ours+Llava-CoT - Llava-CoT Ours Ours+Llava-CoT 40.1 40.8 45.3 47.6 50.8 51.4 52.6 54. Training from LLava-OV-mid 12.2 14.6 20.9 20.9 36.0 45.7 43.9 48.8 Training from MAmmoTH-VL 25.3 24.6 29.0 26.3 66.0 63.8 65.9 68. 32.1 47.5 57.6 51.7 62.3 58.7 61.8 64.5 21.2 25.8 31.5 31.6 34.8 35.2 38.6 40.7 18.1 27.2 27.4 34.9 34.2 39.4 39.4 42. 24.4 33.9 40.3 42.3 44.7 48.3 55.7 55.7 26.3 33.6 38.1 39.7 45.4 45.9 49.0 50.4 Table 5. Ablation Results of our experiments. We show experimental results from different backbones to show the impact of consistency filtering and data mixing with Llava-CoT. For each base model, the best performance is highlighted in bold. 5. Related Works 5.1. Multimodal Instruction Data Creating high-quality multimodal datasets remains significant challenge in advancing MLLMs. Current approaches face critical limitations, particularly in balancing quality and scale. Human-annotated datasets provide high-precision, contextually appropriate data [13, 42, 48, 58] but suffer from prohibitive costs and scalability constraints. Meanwhile, methods leveraging existing academic datasets [34, 52] offer more cost-effective alternatives but lack the diversity and reasoning complexity needed for advanced multimodal reasoning tasks. This limitation is particularly evident in the scarcity of large-scale, reasoningfocused multimodal datasets that can be efficiently produced. Our work addresses these challenges by proposing novel, scalable methodology for constructing multimodal instruction datasets that maintain both the quality and reasoning complexity. 5.2. Multimodal Large Language Models Multimodal Large Language Models (MLLMs) have advanced AI by integrating text and visual processing capabilities. While proprietary models such as GPT-4o [22] and Gemini [49, 50] achieve state-of-the-art performance, they remain inaccessible to the broader research community. To address this gap, connector-based approaches [12, 30] have emerged, linking visual encoders to language models through lightweight projection modules. Recent open-source MLLMs, such as LLAMA [18], LLaVA [27, 32], MiniGPT-4 [69], and Deepseek-VL [35], have contributed to advancements in vision-language understanding. Additionally, Qwen-VL [53] and InternVL [9] have demonstrated strong performance through efficient design and diverse pre-training. Meanwhile, various approaches have been developed to enhance MLLM reasoning capabilities, including neural symbolic methods [2, 10], optimized visual encoding strategies [23, 31], plan-based prompting [39, 43], structured reasoning frameworks [57], and sequential instruction tuning [21]. Despite these advancements, these models face critical challenge: the scarcity of publicly available large-scale visual reasoning datasets necessary for enhancing model reasoning capabilities [4]. Our work addresses this supervised fine-tuning data bottleneck while building on the connector-training paradigm, aiming to bridge the gap between proprietary and open-source multimodal models to foster more accessible vision-language systems. 5.3. Chain-of-Thought in Large Language Models Chain-of-Thought (CoT) prompting [55] has revolutionized how large language models tackle complex reasoning challenges. This technique enables LLMs to navigate difficult problemsincluding commonsense scenarios [17, 46] and logical puzzles [29, 56]by following explicit reasoning pathways. At its core, CoT methodically decomposes complex questions into manageable sequential steps, creating structured framework that guides models toward systematic solutions [11]. Evidence consistently demonstrates significant improvements in reasoning performance through this approach. Notable advancements include Prism [44], which implements distinctive dual-stage architecture that separates initial perception from subsequent reasoning operations, and MSG [5], which pioneered the forced Chain-of-Thought methodologyestablishing foundational paradigm shift in structured prompting approaches that continues to shape current research. 6. Conclusion In this paper, we explore the possibility of constructing large-scale multimodal reasoning datasets without relying on human annotation. We are the first paper to utilize Google Image Search for mining high-quality visual reasoning dataset. Our approach has been highly effective to achieve state-of-the-art performance on 5 out of 7 evaluated In the future, we plan to work on multiple benchmarks. round of search to further expand the dataset size."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, YenChun Chen, Yi ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, and Xiren Zhou. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, 2025. 2, 7 [2] Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang, and Kazuhito Koishida. Neuro-symbolic visual reasoning: Disentangling visual from reasoning, 2020. 8 [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 1 [4] Tianyi Bai, Hao Liang, Binwang Wan, Yanran Xu, Xi Li, Shiyu Li, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Ping Huang, Jiulong Shan, Conghui He, Binhang Yuan, and Wentao Zhang. survey of multimodal large language model from data-centric perspective, 2024. 8 [5] Franz Louis Cesista. Multimodal structured generation: Cvprs 2nd mmfm challenge technical report, 2025. 8 [6] Shuaichen Chang, David Palzer, Jialin Li, Eric FoslerLussier, and Ningchuan Xiao. Mapqa: dataset for arXiv preprint question answering on choropleth maps. arXiv:2211.08545, 2022. 1 [7] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning, 2022. 1, [8] Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024. 1 [9] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 7, 8 [10] Minkyu Choi, Harsh Goel, Mohammad Omama, Yunhao Yang, Sahil Shah, and Sandeep Chinchali. Towards NeuroSymbolic Video Understanding, page 220236. Springer Nature Switzerland, 2024. 8 [11] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. Navigate through enigmatic labyrinth survey of chain of thought reasoning: Advances, frontiers and future, 2024. 8 [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. [13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris CallisonBurch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. 7, 8 [14] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. 7 [15] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36:7075770798, 2023. 1 [16] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-llava: Solving geometric problem with multi-modal large language model, 2023. 1, 2 [17] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies, 2021. 8 [18] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv preprint et al. arXiv:2407.21783, 2024. 7, 8 The llama 3 herd of models. [19] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. 2, 7 [20] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from In Proceedings of the IEEE conference on blind people. computer vision and pattern recognition, pages 36083617, 2018. 1 [21] Hanxu Hu, Simon Yu, Pinzhen Chen, and Edoardo M. Ponti. Fine-tuning large language models with sequential instructions, 2024. 8 [22] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 4, [24] Justin [23] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding, 2024. 8 Johnson, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. 1, 2 Bharath Hariharan, [25] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. 1, 2 [26] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. 1, [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 8 [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 5, 7 [29] Hao Li, Jinfa Huang, Peng Jin, Guoli Song, Qi Wu, and Jie Chen. Weakly-supervised 3d spatial reasoning for text-based visual question answering. IEEE Transactions on Image Processing, 32:33673382, 2023. 8 [30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. 8 [31] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm, 2024. 8 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 10 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 5 [34] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too, 2023. 8 [35] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world visionlanguage understanding, 2024. 8 [36] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. 2 [37] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram underarXiv preprint standing and visual language reasoning. arXiv:2110.13214, 2021. [38] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 1, 2, 6 [39] Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, and Houqiang Li. Textcot: Zoom in for enhanced multimodal text-rich image understanding, 2024. 8 [40] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 1, 2 [41] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 1 [42] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu H`e, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. Mm1: Methods, analysis & insights from multimodal llm pretraining, 2024. 8 [43] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought prompting for large multimodal models, 2024. [44] Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Prism: framework for decoupling and assessing the capabilities of vlms, 2024. 8 [45] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. 1, 2 [46] Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, and Dan Roth. Commonsense reasoning for natural language In Proceedings of the 58th Annual Meeting of processing. the Association for Computational Linguistics: Tutorial Abstracts, pages 2733, Online, 2020. Association for Computational Linguistics. 8 [47] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models, 2024. 2 [48] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023. 8 [49] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, [50] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 8 [51] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamavo1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 7 [52] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 8 [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 7, 8 [54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1 [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. [56] Siheng Xiong, Yuan Yang, Ali Payani, James Kerce, and Faramarz Fekri. Teilp: Time prediction over knowledge graphs via logical reasoning, 2024. 8 [57] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason stepby-step, 2025. 2, 5, 7, 8 [58] Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu Huang. Vision-flan: Scaling human-labeled tasks in visual instruction tuning, 2024. 8 [59] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 5 [60] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and Dacheng Tao. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search, 2024. 2, 7 [61] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning, pages 5773057754. PMLR, 2024. [62] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 1, 2, 6 [63] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2409.02813, 2024. 6 [64] Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 37:9062990660, 2025. 2 [65] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 5 [66] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. 7 [67] Lei Zhang and Yong Rui. Image searchfrom thousands to billions in 20 years. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 9 (1s):120, 2013. 2 [68] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186, 2024. 6 [69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. 8 11 [70] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. In The Thirteenth International Conference on Learning Representations, 2024. 2, 6 7. Supplementary Material 7.1. Image Number Distribution 7.2. MAmmoTH-VL2 Training Configuration"
        },
        {
            "title": "Model Architecture",
            "content": "Qwen/Qwen2.5-7B-Instruct google/siglip-so400m-patch14-384 Base Language Model Vision Encoder Vision-Language Connector MLP-based projector (2-layer with GELU) Vision Select Layer Patch Merge Type Starting Checkpoint -2 (second-to-last layer) spatial unpad MAmmoTH-VL"
        },
        {
            "title": "Training Epochs\nBatch Size\nMaximum Sequence Length\nLearning Rate\nVision Tower Learning Rate\nWeight Decay\nWarmup Ratio\nLR Scheduler",
            "content": "1 256 8,192 tokens 1e-5 (language and projector) 2e-6 0.0 0.03 Cosine"
        },
        {
            "title": "Tunable Components",
            "content": "Enabled Enabled Enabled Enabled Enabled (inductor)"
        },
        {
            "title": "Data Processing",
            "content": "anyres max 4 Image Aspect Ratio (1x1),...,(6x6) Image Grid Pinpoints Group by Modality Enabled Image Start/End Tokens Disabled Disabled Image Patch Token Enabled Lazy Preprocessing"
        },
        {
            "title": "Primary Dataset\nAdditional Dataset\nPrompt Template",
            "content": "VisualWebInstruct LLaVA-CoT (9:1 ratio) qwen"
        },
        {
            "title": "Optimization",
            "content": "Distributed Training TF32 Precision Mixed Precision TF32 Precision DeepSpeed Zero-3 Enabled BF16 Enabled 7.3. Prompt for Each Stage"
        },
        {
            "title": "QA Pairs Extraction",
            "content": "\"\"\"Analyze this webpage content and extract questions, images, and complete solution details in Markdown format. Please format your response as follows: **Question 1:** [complete question text] **Images:** * [First image URL if available] * [Second image URL if available] [continue for each additional image...] **Solution:** [Copy the complete solution text from the webpage, including all steps, explanations, and calculations] **Images in Solution:** * [First image URL if available] * [Second image URL if available] [continue for each additional image...] [repeat for each additional question...] Requirements: - Keep the complete solution text exactly as shown in the webpage - Use Markdown formatting throughout the response - Mark missing content as \"Not found\" - For images, include URL only - For multiple questions, number them sequentially - Do not summarize or modify the solution text - Preserve all mathematical notations and formulas - Keep all step-by-step explanations intact - Preserve all line breaks and indentation in solution text - If there is no question in the content, mark it as \"Not found\" - If the webpage is empty or missing, return nothing Webpage content: {Accessibility Tree} \"\"\""
        },
        {
            "title": "QA Pairs Validation",
            "content": "complete question text complete solution text \"\"\"Please analyze this question-answer pair and its images: Question: Solution: Your tasks: 1. 2. - Properly referenced in the question - Clear and visible - Actually helps understand the question Determine if the question is meaningful and valid. For the question images (if any), determine if each is: For the solution images (if any), determine if each is: 3. - Helps explain the solution Notes: - Image indices start from 0 (e.g., first image is index 0, second is index 1, etc.) - Images should be marked as valid if they show the actual content being discussed - Images should be marked as invalid only if they are: * Completely irrelevant to the question/solution * Corrupted or unreadable * Duplicate or redundant Question Images: [Images loaded here] Solution Images (starting new section, indexes reset to 0): [Images loaded here] Please respond in this exact format: QUESTION VALID: [yes/no] ANALYSIS: [Brief explanation of why the question is valid/invalid] QUESTION IMAGES: [comma-separated list of valid image indices starting from 0] QUESTION IMAGES REASON: [Brief explanation for each image decision] SOLUTION IMAGES: [comma-separated list of valid image indices starting from 0] SOLUTION IMAGES REASON: [Brief explanation for each image decision] CRITICAL RESPONSE FORMAT INSTRUCTIONS: - You MUST respond using EXACTLY this format with no additional text - Use ONLY numeric indices for images, starting from 0 - If no images are valid, use an empty string - Be precise and use actual numbers - Always use numeric indices (0,1,2...) - Use empty string for no images (e.g., \"SOLUTION IMAGES: \") - Do not add explanatory text in the indices field \"\"\""
        },
        {
            "title": "Answer Alignment",
            "content": "What is 2 + 2? gptanswer realanswer \"\"\"Given the question and the provided image(s), compare these two answers and determine if they are aligned. Question: question GPTs Answer: Real Answer: Example of Aligned Answers: Question: GPT Answer: Real Answer: 4 Example of Misaligned Answers: Question: GPT Answer: 2x + 1 Real Answer: 2x Are these answers aligned? brief explanation on the second line.\"\"\" Respond with just Yes or No on the first line. What is derivative of x2?"
        },
        {
            "title": "Provide",
            "content": "16 7.4. Example of QA Pair"
        }
    ],
    "affiliations": [
        "CMU",
        "Independent",
        "NUS",
        "Netmind.ai",
        "UC Santa Barbara",
        "University of Toronto",
        "University of Waterloo"
    ]
}