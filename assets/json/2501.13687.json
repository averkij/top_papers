{
    "paper_title": "Question Answering on Patient Medical Records with Private Fine-Tuned LLMs",
    "authors": [
        "Sara Kothari",
        "Ayush Gupta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Healthcare systems continuously generate vast amounts of electronic health records (EHRs), commonly stored in the Fast Healthcare Interoperability Resources (FHIR) standard. Despite the wealth of information in these records, their complexity and volume make it difficult for users to retrieve and interpret crucial health insights. Recent advances in Large Language Models (LLMs) offer a solution, enabling semantic question answering (QA) over medical data, allowing users to interact with their health records more effectively. However, ensuring privacy and compliance requires edge and private deployments of LLMs. This paper proposes a novel approach to semantic QA over EHRs by first identifying the most relevant FHIR resources for a user query (Task1) and subsequently answering the query based on these resources (Task2). We explore the performance of privately hosted, fine-tuned LLMs, evaluating them against benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by 0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we examine advanced aspects of LLM usage, including sequential fine-tuning, model self-evaluation (narcissistic evaluation), and the impact of training data size on performance. The models and datasets are available here: https://huggingface.co/genloop"
        },
        {
            "title": "Start",
            "content": "QUESTION ANSWERING ON PATIENT MEDICAL RECORDS WITH PRIVATE FINE-TUNED LLMS 5 2 0 2 3 2 ] . [ 1 7 8 6 3 1 . 1 0 5 2 : r Sara Kothari Department of Computer Science Stanford University sarako@stanford.edu Ayush Gupta Genloop Labs, Inc. Delaware, USA founder@genloop.ai ABSTRACT Healthcare systems continuously generate vast amounts of electronic health records (EHRs), commonly stored in the Fast Healthcare Interoperability Resources (FHIR) standard. Despite the wealth of information in these records, their complexity and volume make it difficult for users to retrieve and interpret crucial health insights. Recent advances in Large Language Models (LLMs) offer solution, enabling semantic question answering (QA) over medical data, allowing users to interact with their health records more effectively. However, ensuring privacy and compliance requires edge and private deployments of LLMs. This paper proposes novel approach to semantic QA over EHRs by first identifying the most relevant FHIR resources for user query (Task1) and subsequently answering the query based on these resources (Task2). We explore the performance of privately hosted, fine-tuned LLMs, evaluating them against benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by 0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we examine advanced aspects of LLM usage, including sequential fine-tuning, model self-evaluation (narcissistic evaluation), and the impact of training data size on performance. The models and datasets are available here: https://huggingface.co/genloop Keywords LLMs Private Edge Fine-Tuning FHIR Question Answering Medical"
        },
        {
            "title": "1 Introduction",
            "content": "The Fast Healthcare Interoperability Resources (FHIR) [1] standard, developed by HL7, provides consistent framework for exchanging electronic health records (EHRs) across healthcare systems, enabling improved interoperability. FHIRs structured, machine-readable format supports the Question Answering on Patient Medical Records with Private Fine-Tuned LLMs seamless transfer of complex healthcare data, making it central to modern health information exchange. Recent mandates requiring the use of HL7 FHIR Release 4 to support API-enabled services reflect shift toward greater data transparency and patient autonomy[2]. Regulatory changes, particularly the Anti-Information Blocking provisions of the 21st Century Cures Act [3], also emphasize the need for patient-centric access to health data. Platforms like Apple Health now integrate FHIR data, giving patients unprecedented access to their medical records and creating new opportunities to transform complex data into actionable health insights. [4] Despite these advances in interoperability, patients often face significant challenges when interacting with their own health data. Complex medical terminology, language barriers, and limited health literacy make it difficult for many individuals to understand their conditions, diagnoses, or treatment plans [5]. These challenges are not just inconveniencesthey can lead to delays in care, misunderstandings of critical information, and ultimately, poorer health outcomes. Large Language Models (LLMs) [6] provide paradigm for human readability of their health records. LLMs, characterized by their ability to process vast amounts of unstructured data and generate human-like text, are transforming the landscape of healthcare informatics. Their inherent capacity to perform natural language understanding (NLU) and generation enables them to extract, summarize, and interpret complex medical information. In the context of this paper, LLMs serve as bridge between raw FHIR (Fast Healthcare Interoperability Resources) data and end-users, translating intricate clinical terminology into plain language that patients and healthcare providers can easily comprehend. By leveraging Large Language Models (LLMs), healthcare providers have the opportunity to simplify and personalize health data. Such systems can empower patients, making their medical information more accessible and helping them better understand their health, which in turn may enhance patient engagement and adherence to treatment plans. This approach also has the potential to reduce healthcare inefficiencies by offering quicker, more intuitive access to relevant medical records. However, the application of LLMs for such tasks faces significant barriers, particularly with respect to data privacy and security. Sharing personal health information (PHI) with cloud-hosted, generalpurpose LLMs risks violating key privacy regulations, such as HIPAA [7], the California Consumer Privacy Act (CCPA)[8], and the Biometric Information Privacy Act (BIPA, Illinois) [9]. In the healthcare domain, ensuring the confidentiality of sensitive patient data is paramount. For this reason, edge-deployed, privately hosted LLMs present more viable solution, allowing healthcare systems to maintain control over patient data while still benefiting from the powerful capabilities of LLMs. Thanks to open-source models like Metas LLaMA series [10, 11] and Mistrals open source model series [12], it is now feasible to self-host LLMs for specific applications. However, these smaller models often fall short in the accuracy required for sophisticated healthcare tasks such as semantic question answering. Fine-tuning with techniques like LoRA [13, 14, 15] emerges as critical step to address this gap with computational efficiency. By customizing smaller, domain-specific models to the nuances of the target task, fine-tuning enhances both the accuracy and performance of these 2 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs models. This adaptation not only makes them suitable for demanding healthcare applications but also ensures they remain efficient and privacy-compliant, aligning with the requirements of edge deployment in sensitive domains like healthcare. In this paper, we break down the task of semantic question answering over medical records into two stages: (1) retrieving the most relevant FHIR resources given users medical query, and (2) answering the query based on the retrieved resources. We fine-tune smaller, open-source models for each stage, ensuring that they are well-suited to the unique challenges of medical data processing. To facilitate this process, we generate synthetic patient data and utilize larger general-purpose models to create task-specific synthetic datasets (data collection). These datasets are then refined (data refinement), followed by training multiple models to identify the best-performing configurations (training), and finally, evaluating their performance against established benchmarks (evaluation). The resulting models are deployed in privacy-compliant setup. Our experiments reveal that fine-tuned smaller models significantly outperform larger general models like GPT-4, particularly in terms of accuracy, efficiency, and privacy compliance. This work demonstrates that edge-deployed, fine-tuned LLMs can offer practical solution for healthcare providers seeking to implement patient-centric semantic question answering without sacrificing data privacy. Beyond the core tasks, this study explores several important aspects of LLM behavior in healthcare contexts: 1. The impact of sequential fine-tuning on multi-task performance. 2. The tendency of LLMs to exhibit narcissistic behavior by favoring their own outputs. 3. The influence of dataset size on fine-tuning effectiveness, with implications for resourceconstrained environments. We discuss these results in Section 5 (Results and Discussion) 5 and highlight key insights for future research in Section 6 (Conclusion, Limitations, and Future Work) 6."
        },
        {
            "title": "2 Related Work",
            "content": "The application of Large Language Models (LLMs) to patient medical data processing has garnered significant attention in recent research. Notably, [16] demonstrated the efficacy of leveraging LLMs to convert clinical texts into FHIR resources with an impressive accuracy rate of over 90 percent, thereby streamlining the processing and calibration of healthcare data and enhancing interoperability. [17] introduced multi-agent workflow utilizing the Reflexion framework, which employed ICD-10 codes from medical reports to generate patient-friendly letters with an accuracy rate of 94.94%. These studies did not involve direct querying of Electronic Medical Records (EMRs). Recently, [18] introduced \"LLM on FHIR\", an open-source mobile application that leverages GPT-4 to allow users to interact with their health records, demonstrating impressive accuracy and relevance in delivering comprehensible health information to patients. [19] developed on top of [18] to replace GPT-4 with fine-tuned LLMs. They divided the FHIR querying process into three tasks: filtering, 3 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Figure 1: Complete Approach Diagram summarization, and response. They compared against Meditron[20], family of medical-adapted Llama-2 models, and Llama 2 Base models. Their approach was shown performing better than Llama 2 Base models but underperforming Meditron."
        },
        {
            "title": "3 Approach",
            "content": "To approach this task, we break query processing in 2 stages, similar to how retrieval augmented generation [21, 22], is performed. 1. Task 1: Identifying the FHIR resources from the patients medical record that are relevant to given natural language patient query. Each patient will have numerous FHIR resources in their patient record, only some of which are relevant to the patient query. We formulate the problem as binary classification problem i.e. given query q, and FHIR resource r, the problem is setup as F(q, r) = I{0, 1} 2. Task 2: Answering the medical query of the patient using the FHIR resources that were identified as relevant to the query i.e. generating the response based on (query, list of relevant resources) pairs. Figure 1 1 outlines the approach. The main inputs are the user query and the EHR records in the FHIR format. The EHR records can be both relevant or irrelevant when received. The Task 1 classifies relevant FHIRs and help us pick the necessary records to generate the answer for the users medical question in Task 2. LLMs are the intelligence modules for Task 1 and Task 2. In our approach, we fine-tune the top-performing models available at the time of experimentation (Llama-3.1-8B, and Mistral-NeMo) for each of these tasks. We also compare the results switching them with GPT-4 (SOTA model at the time), and Meditron-7b [20] (SOTA medical domain model at the time). More details on the experiments, including dataset generation, are covered in Section 4 (Experiment Details). The Results are discussed in Section 5 (Results and Discussion) Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Figure 2: Data Preparation Flow"
        },
        {
            "title": "4.1 Dataset",
            "content": "We employed Synthea [23], an open-source synthetic patient generator, to produce HL7 FHIRformatted medical records (JSON files). we started with generating data of 50 patients for Task 1 and an additional 450 patients for Task 2. Each file was processed to retain only patient-relevant resources, excluding entries such as \"SupplyDelivery\". Only the following resource types were included: \"Procedure, \"Medication\", \"MedicationRequest\", \"Encounter\", \"ImagingStudy\", \"Immunization\", \"Device\", \"CarePlan\", \"ExplanationOfBenefit\", \"AllergyIntolerance\", \"Observation\", \"Condition\", \"DiagnosticReport\" We ran custom script to filter and retain only those segments of each FHIR resource that are likely to be relevant to patient queries. The script eliminated non-essential details and sections from which meaningful medical information could not be extracted. This pre-processing step was designed to eliminate irrelevant or redundant information. Figure 2 2 shows the complete data preparation flow. More details on creating the training pairs for Task 1 LLM fine-tuning and Task 2 LLM fine-tuning are covered in the following sections. 4.1.1 Task 1 Data Preparation The following additional steps were followed for Task 1 Data Preparation 1. Data Selection: We randomly selected batch of 10 FHIR resources from each patient record. 2. Query Generation: GPT-4 was employed to generate natural language query based on one or more of the 10 selected FHIR resources. We utilized prompt engineering to ensure the generation of realistic, simple, and non-technical queries. The specific prompt used has been provided in Appendix A.1. 5 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs 3. Labeling: GPT-4 labeled the resources as \"relevant\" if they were used to generate the query and \"irrelevant\" if they were not. 4. Dataset Creation: This process was repeated 10 times per patient, resulting in total of 5,000 JSON files containing queries, patient IDs, relevance label, and resource. For LLM fine-tuning, the inputs are the query and resource, and the output is relevance label. We used 95-5 training-test split for the dataset. 4.1.2 Task 2 Data Preparation For Task 2, we generated 450 additional patient records and processed them through the same method as Task 1 Data Preparation. That gave us total of 500 patient records, with the corresponding relevance label for each resource,. 1. Answer Generation: For each query, we have relevant and irrelevant FHIR resources. For the task of answer generation, we are interested in only the relevant resources. Therefore, each input example for Task 2 is pair of (query, [list of relevant resources]). Since we have 10 queries per patient record, we have 5,000 such examples. For each of them, GPT-4 was used to create relevant answer. The generated answered were reviewed for accuracy along with their context. Finally, we had 5,000 examples of (query, [list of relevant resources], answer). The specific prompt used for GPT-4 based synthetic data generation has been provided in Appendix A.2. 2. Dataset Creation: We conducted 98-2 train-test split, resulting in 4900 examples in the training dataset and 100 examples in the test dataset. We also randomly sub-sampled 500 examples from the 4900 example training dataset to create another smaller training dataset to determine the effect of training dataset size on model performance (details covered in following sections)."
        },
        {
            "title": "4.2 Fine-tuning",
            "content": "Fine-tuning [13, 14, 15] is technique by which the original model is modified to adapt to the specific task of concern. Full fine-tuning of LLMs is compute intensive process. For reference, 8B parameter model will require around 100GB GPU VRAM, that is available with exclusive GPU machines like H200 costing more than $10,000 per month on rent and hardly available to purchase. Parameter-Efficient Fine-Tuning (PEFT) [13] gives us computationally efficient technique to perform fine-tuning with marginal performance difference. It adds additional trainable parameters to the LLM, that work in conjunction with the original pre-trained weights to give desired results. In our experiments, we use variant of PEFT that provides even better memory efficiency called QLoRA [15]. This technique allows to load the original model weights in 4-bit quantization reducing memory usage, while training additional weights through the PEFT approach. For our experiments, we used LoRA rank of 16, and trained for 5 epochs. We conducted our fine-tuning and inference experiments on an NVIDIA A100 40GB GPU. More model training configs are available with the model cards on our HF page here: https://huggingface.co/genloop 6 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs"
        },
        {
            "title": "4.3 Experiments",
            "content": "We additionally conduct the following experiments to ascertain how fine-tuned smaller LLMs perform against the general LLMs, and also empirically understand their behaviour: 1. Experiment 1 - Comparison of Base Models, Fine-tuned Models, Meditron, and GPT-4 Performance: We compare the performance of pretrained models, models fine-tuned on specific tasks using custom datasets, and GPT-4 (and GPT-4o) to understand how finetuning improves task-specific performance and how these models stack up against state-of-the-art model like GPT-4 or GPT-4o. 2. Experiment 2 - Analyzing the Effect of Training Dataset Size on Model Performance: In this experiment, we assess how varying the size of the training dataset impacts model performance. 3. Experiment 3 - Impact of Sequential Fine-tuning on Task Retention: Through this experiment, we explore cross-task knowledge transfer. Specifically, how fine-tuning model on second task affects its ability to retain knowledge from the first task, assessing whether the model \"forgets\" how to perform the original task after being adapted to new one, and vice versa. Moreover, it investigates whether being initially trained on first task with knowledge overlap improves or hinders performance on the second task. 4. Experiment 4 - Investigating Self-Preference and Self-Recognition Bias in LLM-asa-Judge Evaluation: This experiment analyzes potential biases in large language models when they act as evaluators, specifically focusing on whether the model exhibits preference for its own outputs or recognizes and favors its own responses over others in an evaluation context. For Task 1, we conducted Experiment 1 and Experiment 3. 1. For Experiment 1, we fine-tuned Llama 3.1 Base, Llama 3.1 Instruct, Mistral NeMo Instruct, and Mistral NeMo Base using the 5,000-example dataset created for Task 1. We benchmarked these models against GPT-4 and Meditron-7b. The pretrained Llama 3.1 and Mistral Nemo Base models were our baseline. 2. For Experiment 3, we explored two approaches. First, we fine-tuned Llama 3.1 Base on Task 2 using 4,900-example dataset, followed by additional fine-tuning on Task 1. In the second approach, we reversed the order by first fine-tuning Llama 3.1 Base on Task 1, then further fine-tuning the resulting model on Task 2. We did an experiment with an extended prompt for the second approach, in order to instruct the model on the classification requirements. We evaluated all these models on this binary classification task using F1, Recall, Precision, and Accuracy, placing greatest emphasis on F1. For Task 2, we conducted all four experiments. In all these experiments, we used the METEOR score [24] as the evaluation metric. We chose METEOR as it incorporates stemming and synonymy 7 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs matching, in addition to exact word matching, and aligns closely with human judgment at the sentence/segment level. 1. For Experiment 1, we fine-tuned Llama 3.1 Base and Mistral NeMo Base using dataset of 4,900 examples and benchmarked their performance against GPT-4o and Meditron 7B. 2. For Experiment 2, we fine-tuned Llama 3.1 Base and Mistral NeMo Base using smaller dataset of 500 examples. We compared these models performance to those fine-tuned in the first experiment (4,900 examples) to assess the effect of dataset size on model performance. 3. For Experiment 3, we conducted sequential finetuning on both Llama 3.1 Base and Mistral NeMo Base models, which were initially fine-tuned on Task 1. We further fine-tuned these models on Task 2 explore task transfer and task retention. Additionally, we fine-tuned the Llama 3.1 Base model, which had been first fine-tuned on Task 2 with 4,900 examples, on Task 1 using 5,000 examples, and then evaluated its performance on Task 2 to assess knowledge transfer across tasks. 4. For Experiment 4, we performed an LLM-as-a-judge evaluation involving the top two performing models from the previous tasks using Claude Sonnet and GPT-4o as judge LLMs. We initially asked GPT-4o to evaluate and select the better response between the two models without knowing which model generated which response. After this, we disclosed which response was generated by GPT-4 and asked GPT-4o to choose again. We then conducted the same blind evaluation process with Claude. In both cases, the reference answer from the dataset was provided, and the evaluation criteria included the following: (a) Relevance: Does the answer accurately address the patient query using the relevant FHIR resources? (b) Groundedness: Is the answer based on factual, evidence-based information? (c) Completeness: Does the answer cover all aspects of the patient query comprehensively? (d) Quality: Is the answer well-written, clear, and useful for the patient? (e) Conciseness: Is the answer succinct while still being informative? (f) Closeness to Reference: How closely does the answer match the content and intent of the reference answer?"
        },
        {
            "title": "5.1.1 Experiment 1",
            "content": "The results for Task 1 Experiment 1 are shown in Figure 3 3. It can be seen that fine-tuning improves model performance significantly compared to the pre-trained baselines. LLaMA 3.1 Base (Fine-tuned) achieved the highest accuracy of 98.82%, with strong performance in both precision (96.97%) and recall (94.12%), leading to the strongest F1 score of 95.52%. This result highlights the models effective learning and adaptation to Task 1 after fine-tuning, surpassing both GPT-4 and 8 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Figure 3: Task 1 Experiment 1 Results Figure 4: Task 1 Experiment 1 F1 Scores Meditron-7B. With 8B parameters, this model is approximately 0.45% the size of GPT-4 (which has 1760B parameters). On the other hand, LLaMA 3.1 Base (Pretrained), serving as baseline, only achieved 54.33% accuracy, while Mistral NeMo Base (Pretrained) had 54.51% accuracy, reflecting their underperformance prior to fine-tuning. Another interesting point to note is that llama 3.1 base model has 8B parameters compared to Mistral NeMo with 12B parameters, but is able to give better results. Lastly, GPT-4, with an accuracy of 98.43% and an F1 score of 95%, was comparable to the finetuned models but lacked the same level of precision, highlighting the strength of task-specific fine-tuning. Meditron-7B, though specialized medical model, underperformed significantly in this task with only 29.02% accuracy. The models are further compared illustratively in Figure 4 4. 5.1.2 Experiment 3 The compiled results for this experiment of sequential training on Task 1 is presented in Figure 5 5. As expected, LLM trained on Task 1 and then on Task 2, loses its practice on Task 1 and gives inferior results. The F1 Score drops from 95.52 for Llama 3.1 Base FT model to 31.62 for Llama 3.1 Base model fine-tuned on Task 1 and then Task 2, as 67% drop in F1. Understandably, Task 2 is more verbose than the classification problem in Task 1. So we conducted another experiment 9 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Figure 5: Task 1 Experiment 3 Results Figure 6: Task 2 Experiment 1 Results extending the prompt to instruct the classification into one word \"relevant\" or \"irrelevant\". This increased the F1 score to 56.62, 78% improvement. This helps understand that fine-tuned model is still receptive to instructions in the prompt. Fine-tuning the LLM on Task 2 and Task 1 yields similar results as the Llama 3.1 Base directly fine-tuned on Task 1."
        },
        {
            "title": "5.2 Task 2 Results",
            "content": "5.2.1 Experiment 1 Figure 6 6 compiles the results from Experiment 1 on Task 2. We see that fine-tuned Llama 3.1 8B Base and Mistral NeMo 12B models surpass their base versions by 200% and 136%, respectively. Mistral NeMo Base fine-tuned on the 4,900 examples achieved the highest performance with METEOR score of 0.5333, while GPT-4 scored lower at 0.375223. Meditron scored 0.07 on the same task. 5.2.2 Experiment 2 For Experiment 2, we compared the Llama 3.1 and Mistral Nemo models trained on 4,900 examples against training on 500 examples. As can be seen in Figure 7 7, Mistral NeMo Base fine-tuned with 4900 examples outperformed its counterpart fine-tuned with 500 examples by 4.55% in METEOR score. Similarly, LLaMA Base fine-tuned with 4900 examples showed 4.39% higher METEOR score compared to Mistral NeMo Base fine-tuned with 500 examples. We conclude that an increase in the training dataset size definitely improves model performance. However, deeper study is required on more size variations and the limit to such improvement. 10 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Figure 7: Task 2 Experiment 2 Results 5.2.3 Experiment 3 The compiled results for this experiment of sequential training on Task 2 are presented in Figure 8 8. For Experiment 3, we observed differing effects of fine-tuning strategies between Mistral NeMo-based and LLaMA-based models. For Mistral NeMo, directly fine-tuning on Task 2 resulted in better performance than sequential fine-tuning on Task 1 followed by Task 2. Mistral NeMo Base FT-4900 (1) outperformed Mistral NeMo Sequential FT-4900 (Task 1 Task 2) (2), and Mistral NeMo Base FT-500 (4) similarly outperformed Mistral NeMo Sequential FT-500 (Task 1 Task 2) (6). However, in the case of LLaMA 3.1-based models, the opposite trend was observed: sequential fine-tuning led to better results. LLaMA Sequential FT-4900 (Task 1 Task 2) (3) outperformed LLaMA Base FT-4900 (5), and LLaMA Sequential FT-500 (Task 1 Task 2) (7) surpassed LLaMA Base FT-500 (8). Thus, the effect of sequential fine-tuning varies across models, and no conclusive pattern emerged. The reverse fine-tuning approach, where Mistral NeMo (9) was fine-tuned on Task 2 first and then Task 1, resulted in notably low METEOR score of 0.16375. Similarly, the Llama-based model (10) showed similar results, with score of 0.05841. This suggests that the model may have \"unlearnt\" important aspects of Task 2 during subsequent fine-tuning on Task 1."
        },
        {
            "title": "5.2.4 Experiment 4",
            "content": "For Experiment 4, we selected the two best performing models from Task 2 Experiment 3 above - Mistral NeMo Sequential FT-4900 (Task 1 -> Task 2), Mistral NeMo Base FT-4900 - and compared it against GPT-4 response with LLM-as-a-judge evaluation. The judge LLMs were chosen to be GPT-4o and Claude Sonnet. The first evaluation was conducted without disclosing the identity of the models generating the response. This is marked as the \"blind\" evaluation. The second evaluation 11 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Figure 8: Task 2 Experiment 3 Results Figure 9: LLM-as-a-judge Win Rate Results was conducted with the identities revealed. Judgment from Claude Sonnet was used as benchmark number. Minimal self-recognition bias was detected during the blind GPT-4o evaluation. GPT-4o gave itself only few more wins than when evaluated by Claude. In the non-blind evaluation, where GPT-4o knew which response it had generated, significant self-bias appeared, as GPT-4o awarded itself more wins. The actual win-rate dropped by 13 points in non-blind evaluation. This experiment highlights the \"narcissistic\" bias LLMs tend to have. Figure 9 9 demonstrates the point difference."
        },
        {
            "title": "6.1 Conclusion",
            "content": "Here are the conclusions from the four experiments conducted, specifically addressing our use cases and the two tasks outlined in this paper: 12 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Experiment 1: Fine-tuning significantly boosts model performance for task-specific applications. Remarkably, these fine-tuned models outperformed GPT-4 while requiring only fraction of its size and cost. This demonstrates that fine-tuning open-source LLMs is highly efficient solution for our tasks. Additionally, such models can be hosted privately, addressing critical concerns around patient privacy and data security. Experiment 2: Expanding the size of the training dataset consistently improved model performance across all tasks, underscoring the importance of data volume in optimizing model accuracy and generalization. Experiment 3:The effectiveness of sequential fine-tuning strategies varies depending on the model architecture. Furthermore, fine-tuning model on one task and subsequently on another leads to significant drop in performance on the first task. Experiment 4: Non-blind evaluations by GPT-4 resulted in strong bias, where GPT-4 showed clear preference for its own outputs. This is consistent with recent research. [25] noted that LLMs often act as \"narcissistic evaluators,\" inflating their evaluation scores when assessing their own outputs. Similarly, [26] discussed how self-recognition drives this self-preference. However, our study found that this self-recognition bias was more limited unless GPT-4 was explicitly told it was evaluating its own response."
        },
        {
            "title": "6.2 Limitations",
            "content": "One of the key limitations of our work is that we have used synthetic patient data instead of real patient data. This could impact the generalizability of our findings, as synthetic data may not capture the full complexity and variability present in actual patient records."
        },
        {
            "title": "6.3 Future Work",
            "content": "In future work, we plan to investigate multi-task learning (MTL) strategies and continual pretraining (CPT) to improve the performance of models querying Fast Healthcare Interoperability Resources (FHIR) data. By integrating these approaches, we aim to develop single model capable of efficiently handling both tasks simultaneously."
        },
        {
            "title": "References",
            "content": "[1] Index - FHIR v5.0.0. [2] CMS Interoperability and Patient Access https://www.cms.gov/priorities/key-initiatives/ (CMS-9115-F) Final Rule CMS cms.gov. burden-reduction/interoperability/policies-and-regulations/ cms-interoperability-and-patient-access-final-rule-cms-9115-f. 26 Dec. 2024). (accessed Question Answering on Patient Medical Records with Private Fine-Tuned LLMs [3] An act to accelerate the discovery, development, and delivery of 21st century cures, and for other purposes. Accession Number: PLAW-114publ255, PLAW-114publ255 Call Number: AE 2.110:, AE 2.110/3:, AE 2.110:114-255, AE 2.110:, AE 2.110/3:, AE 2.110:114-255 Source: DGPO, DGPO. [4] Healthcare - Health Records apple.com. https://www.apple.com/healthcare/ health-records/. (accessed 26 Dec. 2024). [5] Hilal Al Shamsi, Abdullah G. Almutairi, Sulaiman Al Mashrafi, and Talib Al Kalbani. Implications of language barriers for healthcare: systematic review. 35(2):e122. [6] Large Language Models: Survey arxiv.org. https://arxiv.org/abs/2402.06196. [Accessed 26-12-2024]. [7] Office for Civil Rights (OCR). Health information privacy. Last Modified: 2024-0419T18:24:31-0400. [8] California consumer privacy act (CCPA) state of california - department of justice - office of the attorney general. [9] Biometric information privacy act (BIPA) ACLU of illinois. [10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. [11] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat 14 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, 15 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. [12] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 27902799. PMLR, 2019. [14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. [15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. 16 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs [16] Yikuan Li, Hanyin Wang, Halid Z. Yerebakan, Yoshihisa Shinagawa, and Yuan Luo. FHIR-GPT: Enhancing health data interoperability with large language models. Pages: 2023.10.17.23297028. [17] Malavikha Sudarshan, Sophie Shih, Estella Yee, Alina Yang, John Zou, Cathy Chen, Quan Zhou, Leon Chen, Chinmay Singhal, and George Shih. Agentic LLM workflows for generating patient-friendly medical reports. [18] Paul Schmiedmayer, Adrit Rao, Philipp Zagar, Vishnu Ravi, Aydin Zahedivash, Arash Fereydooni, and Oliver Aalami. LLM on FHIR demystifying health records. [19] Michael Brockman, Nina Boord, and Hamed Hemkat. SUPaHOT: Universally scalable and private method to demystify FHIR health records. [20] Zeming Chen, Alejandro Hernández-Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. Meditron-7b: Scaling medical pretraining for large language models, 2023. [21] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey, 2024. [22] Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue. Retrieval-augmented generation for natural language processing: survey, 2024. [23] Jason Walonoski, Mark Kramer, Joseph Nichols, Andre Quina, Chris Moesel, Dylan Hall, Carlton Duffett, Kudakwashe Dube, Thomas Gallagher, and Scott McLachlan. Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record. 25(3):230238. [24] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. [25] Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. LLMs as narcissistic evaluators: When ego inflates evaluation scores. [26] Arjun Panickssery, Samuel R. Bowman, and Shi Feng. LLM evaluators recognize and favor their own generations. 17 Question Answering on Patient Medical Records with Private Fine-Tuned LLMs"
        },
        {
            "title": "A Appendix",
            "content": "A.1 GPT-4 Prompt for Query Generation: Task 1 To generate natural language queries, we used the following GPT-4 prompt: Come up with query that this patient might Pretend you are patient curious about an aspect of your medical history. have regarding their medical data. At least one or more medical data points from the given set of FHIR resources should be Make the question realistic, sufficient to answer the query. simple, and non-technical. For example, What are my current medicines? or When was my last shot? complications of my last heart procedure?; or What were the Generate an output in the JSON format below corresponding to each of the 10 inputted resources after generating 1 query based on one or more of the 10 given FHIR resources: {10_resources}. The relevance should be relevant if the resource was used by the model for the particular query, and irrelevant if not. The resource_label should be natural language label generated Condition Cardiac for each of the 10 resources in the format: Arrest 06-19-2018. Therefore, the output should be the 10 JSON formatted files per resource, with patient_id being the same throughout, relevance can be either relevant or irrelevant if it wasnt used to generate the query. generated from the 10 resources. So the query label will be the same for all 10. Use the following format for the output: Only one query has to be json [ { \"resource\": \"{{resource}}\", \"query\": \"relevance\": \"patient_id\": \"resource_label\": } \"{{relevance}}\", \"{patient_id}\", \"{{query}}\", ] \"{{resource_label}}\" A.2 GPT-4 Prompt for Answer Generation: Task 2 You are knowledgeable and helpful medical assistant. the given query using the list of relevant FHIR resources provided to you. {query}, Resources: Query: {resources}"
        },
        {
            "title": "Answer",
            "content": ""
        }
    ],
    "affiliations": [
        "Department of Computer Science Stanford University",
        "Genloop Labs, Inc. Delaware, USA"
    ]
}