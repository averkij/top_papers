{
    "paper_title": "Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition",
    "authors": [
        "Zheyang Xiong",
        "Ziyang Cai",
        "John Cooper",
        "Albert Ge",
        "Vasilis Papageorgiou",
        "Zack Sifakis",
        "Angeliki Giannou",
        "Ziqian Lin",
        "Liu Yang",
        "Saurabh Agarwal",
        "Grigorios G Chrysos",
        "Samet Oymak",
        "Kangwook Lee",
        "Dimitris Papailiopoulos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term \"task superposition\". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of \"LLMs as superposition of simulators\", and raise questions about the mechanisms enabling simultaneous task execution."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 3 0 6 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "EVERYTHING EVERYWHERE ALL AT ONCE: LLMS CAN IN-CONTEXT LEARN MULTIPLE TASKS IN SUPERPOSITION Zheyang Xiongw, Ziyang Caiw, John Cooperw, Albert Gew, Vasilis Papageorgiouw Zack Sifakisw, Angeliki Giannouw, Ziqian Linw, Liu Yangw, Saurabh Agarwalw Grigorios Chrysosw, Samet Oymakm, Kangwook Leew, Dimitris Papailiopoulosw,ms wUniversity of Wisconsin-Madison, mUniversity of Michigan, msMicrosoft Research"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during single inference call, capability we term task superposition. We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of LLMs as superposition of simulators, and raise questions about the mechanisms enabling simultaneous task execution."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with one of the most intriguing being in-context learning (ICL). ICL enables LLMs to perform tasks during inference without the need to fine-tune for that particular task, simply by providing few examples within the input prompt. This ability has sparked significant interest in the research community, as it suggests that LLMs can adapt to novel tasks on-the-fly, using the capabilities that they acquired during pretraining, and the context provided. While ICL has been extensively studied from both theoretical and empirical perspectives, many aspects of its underlying mechanisms remain elusive. In this work, we study surprising phenomenon related to ICL that, to the best of our knowledge, has not been thoroughly studied before: LLMs can perform multiple distinct ICL tasks simultaneously, in single inference call, capability we refer to as task superposition, with examples shown in Figure 1. Our study suggests that pretrained autoregressive LLMs such as Llama (Touvron et al., 2023) or GPT-3.51 (Brown et al., 2020) display superposition of tasks purely in-context. When presented with multiple in-context examples from different tasks, in the same prompt, the models can generate outputs that correspond to solutions for all these individual tasks. For instance, given examples of addition and translation, the model can concurrently produce correct answers for both tasks, as well as the composition of these tasks (e.g., the result of addition translated into another language). Figure 1 illustrates this phenomenon. In Figure 1a (left), given in-context examples of addition in different languages and the query 91 + 83 , the model generates probabilities for the correct sum in various languages, demonstrating its ability to perform addition and translation concurrently. Email: <zheyang@cs.wisc.edu>. Correspondence: <dimitris@papail.io>. Code available at: github.com/edixiong/task-superposition 1In particular, gpt-3.5-turbo-instruct."
        },
        {
            "title": "Preprint",
            "content": "(a) (left) Two-digit addition in variety of languages. (right) Naming the capital of given country name, naming the continent of given country name or capitalizing the country name. (b) (left) Tasks copy(op1), copy(op2) and op1+op2. (right) First or last letter in upper or lower case. Figure 1: LLMs can perform task superposition. (a) Llama-3 70B and (b) GPT-3.5 Turbo are each presented with two sets of tasks. For each set of tasks, we show an example prompt such that all except the last row are in-context examples of one of the tasks and the last row is the query. We provide 20 in-context task examples for each task in the prompt with order randomized and provide the probabilities of outputs when correctly performing each task on the query. This discovery aligns and lends further support to the view of LLMs as superposition of simulators (Janus, 2022; Shanahan et al., 2023; Nardo, 2023) and the Bayesian perspective of ICL proposed by Xie et al. (2022). While not mathematically rigorous formulation, we can conceptualize the output of an LLM as weighted sum of conditional probabilities across possible tasks: (cid:88) P(outputtask, prompt)P(taskprompt). P(outputprompt) task In this conceptual model, P(outputprompt) represents the probability distribution over possible outputs given the input prompt, task can be thought of as latent variable representing different capabilities the model might possess (e.g., arithmetic, translation, sentiment analysis), P(outputtask, prompt) represents the output probability distribution if the model was specifically attempting to solve single task, based on the test example in the prompt, and P(taskprompt) represents the models inferred probability that the prompt specifies particular task. Although this mental model is an over-simplification of how an LLM operates, it offers clean conceptual framework for the task superposition phenomenon we observe. Our findings lend support to the idea that LLMs can simultaneously maintain and utilize multiple task distributions, resulting in outputs that reflect combination of relevant tasks. Our Contributions: Our study makes several key contributions:"
        },
        {
            "title": "Preprint",
            "content": "1. Through extensive empirical investigation and theoretical results, we demonstrate that task superposition is prevalent across various pretrained LLM families (GPT3.5, LLama-3, Qwen). 2. We empirically show that task superposition can emerge, even if we train on one task at time. 3. We provide theoretical construction showing that Transformers models are indeed capable of task superposition, and have the capacity to implement multiple tasks in parallel. 4. We explore how LLMs internally compose task vectors (Hendel et al., 2023) during superposition, and show how convex combinations of task vectors can reproduce the superposition effect. 5. We show that larger models can solve more tasks in parallel and more accurately reflect the distribution of in-context tasks. We believe that our findings offer new insights into the latent capabilities of LLMs and raise questions about the mechanisms enabling simultaneous task execution. We believe this work sheds more light on the ICL capabilities of frontier language models, and offers glimpse on potential applications of task superposition in practical settings."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Theory and practice of in-context learning. There is rich literature which formalizes in-context learning under diverse definitions. For example, prior works study in-context learning through Bayesian framework for task retrieval (Xie et al., 2022; Panwar et al., 2023; Zhang et al., 2023), martingales (Falck et al., 2024), optimizers (Akyurek et al., 2023; Oswald et al., 2023; Dai et al., 2022) and more (Reddy, 2024; Olsson et al., 2022). Other works confirm the theoretical framing of in-context learning by using it to implement variety of algorithms and methods (Zhou et al., 2023; Ahn et al., 2023; Giannou et al., 2023; Wu et al., 2024; Laskin et al., 2022; Zhou et al., 2022), or to approximate general-purpose computing machines (Giannou et al., 2023; Wei et al., 2022). To bridge the gap between theory and practice, many works have used these theoretical insights to study in-context learning behaviors, such as in many-shot in-context learning, (Agarwal et al., 2024), long-context (Li et al., 2024), or eliciting personas (Choi & Li, 2024). Other works study the factors that influence how well models can learn through context, such as task diversity (Raventos et al., 2023; Chan et al., 2022), the balance between pre-training priors and in-context (Wei et al., 2023; Lin & Lee, 2024), in-context labels (Min et al., 2022; Lyu et al., 2022), and the in-context format (Lampinen et al., 2022). In-context learning has also been proposed as means of fine-tuning to improve non-language tasks (Dinh et al., 2022). The development of new architectures such as state space models (Gu & Dao, 2023) has further motivated studying whether in-context learning is prevalent in alternative architectures such as Mamba (Park et al., 2024; Grazzi et al., 2024; Zeng et al., 2024) or in looped transformers (Yang et al., 2023). Steering models through in-context learning has been growing area of interest. Recent work has hypothesized that in-context learning can be encapsulated by high-dimensional description of task, which can be used to replace, (Hendel et al., 2023) compose (Todd et al., 2024) or augment (Liu et al., 2024) the latent states of model, in order to alter its default behavior. Task vectors can be combined via arithmetic operations to solve variety of tasks (Ilharco et al., 2023). Prior work has also been investigating the power of tokens in defining task (Bai et al., 2024). Other definitions of superposition. Our findings on superposition are inspired by notions of language models as multiverse generators (Reynolds & McDonell, 2021; moire, 2021). One consequence of LLMs as superposition of tasks is that the outputs may collapse to unintended simulacra, behavior known as the Waluigi effect (Nardo, 2023). Superposition has been defined in various related contexts of learning models. Feature superposition (Elhage et al., 2022) refers to neural networks ability to represent multiple learned concept in single neuron. Though our discovery of task superposition describes the same abstract idea, we stress that it is distinct from feature superposition because task superposition is most apparent in the final output of model. Feature superposition is microscopic-level observation whereas task superposition is macroscopic-level observation. Superposition is also described as way to store multiple models in single set of parameters (Cheung et al., 2019), processing multiple inputs simultaneously (Shen et al., 2024a; Murahari et al.,"
        },
        {
            "title": "Preprint",
            "content": "(a) Addition in original numerical form and in different languages as in Figure 1a (left). (b) Capital name, continent name and capitalization as in Figure 1a (right). (c) copy(op1), copy(op2), and op1+op2 as in Figure 1b (left). (d) First or last letter in upper or lower cases as in Figure 1b (right). Figure 2: Distributions of probabilities for correct outputs for each task in box plots, where 0/25/50/75/100-percentiles are shown and region between 25 and 75 percentiles is colored. For every set of tasks, we tested with 100 prompts and for each prompt, every task has 20 random in-context task examples with order randomized like in Figure 1. Category other is the sum of probabilities of all other outputs. Gray dashed line in each figure is the ideal probability if we assume the model perfectly calibrates its output distribution to the distribution of in-context task examples. With uniform distribution of task examples, the dashed lines are at 0.25 (4 tasks setting) and 0.33 (3 tasks setting). 2022). In our work, we demonstrate task superposition directly as result of language pre-training, without the necessity of additional adapters or decoding strategies."
        },
        {
            "title": "3 LLMS ARE A SUPERPOSITION OF MULTIPLE IN-CONTEXT LEARNERS",
            "content": "In this section, we want to investigate if existing pre-trained models exhibit superposition of multiple tasks and whether this phenomenon is common (i.e., whether we can observe this phenomenon on variety set of tasks and different families of LLMs)."
        },
        {
            "title": "Preprint",
            "content": "Finding 1: LLMs can in-context learn multiple tasks in superposition when provided with prompts of mixture of task examples. We denote by the number of tasks and consider four different settings of task mixtures. 1. Numerical addition and addition in English, French or Spanish (K = 4). Example prompt is shown in Figure 1a (left). 2. Given name of country, name the capital, continent or capitalize the country name (K = 3). Example prompt is shown in Figure 1a (right). 3. Given input {op1}@{op2}, copy op1, op2 or add op1 and op2 (K = 3). Example prompt is shown in Figure 1b (left). 4. Given word, output first letter or last letter in lower or upper cases (K = 4). Example prompt is shown in Figure 1b (right). We provide GPT-3.5 (Brown et al., 2020), Llama-3 70B (AI@Meta, 2024) and Qwen-1.5 72B (Bai et al., 2023a) with prompts of uniform mixture of tasks (each task has 20 random examples in the prompt ordered randomly). For each prompt consisting of in-context task examples (e.g., 11 + 26 37 for the first task in the first setting) and query (e.g., 91 + 83 ), we calculate the probabilities of outputs when correctly performing each task on the query and plot the distribution of probabilities for each task in Figure 2. Details on calculating the probabilities is in Appendix B. Figure 2 reveals that in all four sets of tasks, all models have non-negligible median values of probabilities for at least two tasks. This indicates that the models can in-context learn multiple tasks in superposition when provided with prompts of mixture of task examples. We can also observe that, even though every task in prompt has an equal number of in-context examples (20 examples), LLMs do not calibrate their output distribution perfectly with the in-context task example distribution and they still have bias on what task to perform. For example, Figure 2a shows that Llama-3 70B prefers performing numerical addition over addition in other languages, Qwen-1.5 72B prefers addition in English while GPT-3.5 does not have strong preference over single task. On the other hand, in Figure 2b GPT-3.5 has strong preference over the capital task. Additionally, some tasks are harder than other tasks. For example, in Figure 2d, all models assign near-zero probability for task answers of last letter and last letter cap. The category other has relatively high values, indicating high noise when prompted with in-context examples of this setting. In contrast, in Figure 2c, category other has very small values, indicating that all models most of the time would correctly assign the output probabilities to the correct answers."
        },
        {
            "title": "4 TASK SUPERPOSITION IN MODELS TRAINED FROM SCRATCH",
            "content": "In Section 3 we investigated task superposition in pre-trained LLMs at inference time. In this section, we further investigate how task superposition emerges in LLMs during training. Specifically, if we train the model to in-context learn one task at time, can it perform task superposition when provided with prompts containing examples of multiple tasks? To answer this question, we train small GPT-2 model (12 heads, 12 layers and 86 million parameters) (Radford et al., 2019) to learn family of retrieval tasks. The input has the form {ch1}{ch2}{ch3}{ch4}{ch5}{ch6}{ch7}{ch8} where ch1, ..., ch8 are distinct single characters. We consider 8 retrieval tasks ret1, ..., ret8 where ret1 is to output ch1 and so on. The model is trained to in-context learn one task (retrieve one of {ch1, ..., ch8}) at time in training. Namely, during training, the model is only provided with text data such that each prompt only contains in-context examples of single randomly chosen task (and different prompts can correspond to different tasks). Concretely, for each sample, we randomly select task {ret1, ..., ret8} and inputs x(1), ..., x(m), where each x(j) is an eight-character long string. We then form the sequence = [x(1), gt(x(1)), ..., x(m), gt(x(m))] where gt(x(j)) is the output of performing task on x(j). We train the model Mθ parametrized by θ using ICL training. In particular, we minimize the following objective:"
        },
        {
            "title": "Preprint",
            "content": "(a) Trained on retrieval tasks and tested on prompts with mixture of in-context examples of ret2 and ret6. (b) Trained on addition tasks and tested on prompts with mixture of in-context examples of plus2 and plus6. Figure 3: We consider two different training settings of tasks: (a) given an eight-character length string as input, consider ret1, ..., ret8 where ret1 is to retrieve the first character and so on; and (b) given two-digit integer as an input, consider plus0, ..., plus9 where plus0 is to add 0 on the input and so on. The model in-context learn one task at time during training. After training, for each setting, we select two tasks ad we provide the model with prompts containing in-context examples from these two tasks and vary the mixture ratio λ such that the in-context task example distribution for two tasks is [λ, 1 λ]. We plot λ on x-axis and the output probabilities of task answers for each task on y-axis. min θ Es 1 1 m1 (cid:88) j=1 CE(Mθ(sj x(j+1)), gt(x(j+1))) , (1) where sj x(j+1) [x(1), gt(x(1)), ..., x(j), gt(x(j)), x(j+1)] and CE is the cross-entropy loss. After training, we provide the model with prompts containing in-context examples of two tasks (in particular, we choose ret2 and ret6) and see if the model performs task superposition. We vary the proportion of in-context examples of two tasks and plot the output distributions in Figure 3a. Similarly, we consider second setting involving 10 tasks. Given two digit integer input num, task plus0 outputs num, task plus1 outputs num + 1 and so on, up to task plus9. The model is trained to in-context learn one of plus0,..., plus9 at time, following the procedure above. During inference time, the model is tested with prompts containing mixture of in-context examples from tasks plus2 and plus6. We vary the mixture ratio and show the output distributions in Figure 3b. Finding 2: Transformers can in-context learn multiple tasks in superposition even if trained to in-context learn one task at time. Remarkably, from Figure 3a and 3b, GPT-2 trained from scratch that in-context learns one task at time can generalize to simultaneously performing multiple tasks and calibrate the output probabilities according to the in-context task example distribution when provided with mixture of in-context examples. For example, in Figure 3a at the mixture ratio λ = 0.5, meaning that 50 percent of the examples in the prompt is from task ret2 and the other 50 percent comes from task ret6, we can see the output probabilities for task answers of ret2 and ret6 being roughly [0.5, 0.5]. We can also observe similar behavior in Figure 3b."
        },
        {
            "title": "5 TRANSFORMERS HAVE THE CAPACITY TO PERFORM TASK SUPERPOSITION",
            "content": "In this section, we explore whether Transformers have the inherent expressivity to perform multiple tasks in superposition with single inference call. To this end, we provide theoretical construction"
        },
        {
            "title": "Preprint",
            "content": "of Transformer which, given the ability to implement multiple tasks, performs task superposition depending on the examples given in-context. Theorem 1. seven layer transformer with embedding dimension O(d + log(mn)) with heads per attention layer can perform tasks on vectors of dimension in superposition, with weighting based on different in-context examples each of length . The proof of Theorem 1 is provided in Appendix D.4. Note that while this does not guarantee that training Transformer will actually find these parameters, it does indicate that Transformers are expressive enough to perform task superposition at test time. Below we outline the main ideas used in the proof. 1 , . . . , x(j) n2, =, y(j))m Prediction based on multiple tasks. Assume that we are given in-context samples (x(j) j=1 where = represents specific value used only for preceding the label, and set of different Transformers TFi which can implement the different desired tasks, where each deterministic task is denoted as gi(x(j)) with [k] and [m], i.e. y(j) = gi(x(j)) for some task dependent on sample j. Using the weights of each TFi, we can compute outputs of the following form: . . . x(j) 1 0 . . . ... 0 . . . . . . x(j) n2 = y(j) 0 0 0 . . . ... ... ... 0 0 0 . . . . . . . . . . . . . . . x(j) 1 0 . . . ... 0 . . . . . . x(j) n2 = 0 . . . ... 0 . . . y(j) 0 g1(x(j)) y(j)1 ... ... 0 gT (x(j)) y(j)1 . . . . . . . . . We use the l1 norm to aggregate the prediction, in case that the task is multi-dimensional. These differences are used to identify tasks, as gi(x(j))y(j)1 0 for y(j) coming from task i. Different heads at each layer in the model are used to execute each of the tasks in parallel using the weights from TFi. In Appendix we construct tasks where an arbitrary function gi(x(j) ) is implemented using ReLUs for some fixed that is task-specific. 1:l1) x(j) k)i = gk(x(j) Creating task identifiers. Having the differences between the implemented function and the label, we first use the ReLUs to clean up the vectors vk so that only the positions in each vector that are associated with task are maintained and the rest are set to 12. We thus create the vectors (v ) = 1 otherwise. Now we use ReLUs to threshold and create 10} which identify the task, i.e.,these are task identifiers. an indicator vectors 1{gk(x(j) Notice that if the task is correctly predicted then the difference should be close to 0 (up to some error), while if the task is not identified the corresponding value would not be 0; the rest of the rows would be 1. We have created one vector for each task, which has 1 in the position of the corresponding task if the task was identified in the context. 1 and (v 1:l1)x(j) Averaging and task superposition. As last step, we average all the task identifiers and place the result in the last column, in which the next prediction will happen. We then use the averaged task identifier to weight the prediction of each task based on it, as in task superposition. If the task has been identified multiple times in the context, it would be assigned higher weight/probability."
        },
        {
            "title": "6 TASK SUPERPOSITION THROUGH THE LENS OF TASK VECTORS",
            "content": "While in Section 5 we provide an existential result by constructing Transformer that performs task superposition and shows that task superposition is well within the expressive power of Transformers, we would like to further investigate how task superposition manifest in pretrained LLMs internally. In this section we explore the underlying mechanisms that LLMs employ during task superposition. In particular, we focus our empirical study on task vectors (Hendel et al., 2023) where the detailed implementation is in Appendix C. Task vectors are vectors in the embedding space and are found to encode the algorithm that model internally implements to solve task given in-context demonstrations. 2This step is not mandatory, but it ensures that we have no values over which we have no control. We leave as future work an error analysis on how these values could affect the task identifiers."
        },
        {
            "title": "Preprint",
            "content": "(a) copy(op1) / copy(op2) / op1+op2 (b) to fr / to de / to it Figure 4: Task vectors of Llama-3 8B projected onto two axes chosen by LDA for two sets of tasks: (a) copy(op1), copy(op2) and op1+op2 and (b) to fr, to de and to it. For tasks t1, t2, t3, we use P(t1)/P(t2)/P(t3) to denote different levels of task mixtures, e.g., 0.50/0.50/0.00 represents the case where the in-context task examples are 50% t1, 50% t2 and 0% t3. We want to investigate if there is any relation between the task vectors of each individual task and the task vectors of mixture of task examples in the prompt. To this end, we consider two sets of tasks: (a) copy(op1), copy(op2) and op1+op2 as in Figure 1b (left). (b) Given two-digit integer, task to fr translates it to French, task to de translates it to German and task to it translates it to Italian. For each set of tasks, we collect the task vectors for each individual task and task vectors extracted from prompts that contain examples of different tasks. In Figure 4, we project task vectors along two axes chosen by linear discriminant analysis (LDA). Finding 3: LLMs internally combine task vectors during task superposition. Interestingly, we observe that the locations of task vectors of mixture of tasks strongly correlate with the locations of task vectors for each individual task and the in-context task example distribution (the mixture ratio for examples of different tasks). For example, if the prompt includes an equal number of in-context examples from each task, the task vectors are roughly centered in the middle; if the prompt only contains in-context examples of two tasks, then the task vectors roughly lie on the connecting line between task vectors of two individual tasks. We argue that this observation is indicative of the fact that, when prompted with mixture of in-context task examples, LLMs internally combine task vectors. As we observe signs that LLMs internally compose task vectors, we want to further investigate whether we can reproduce the task superposition phenomenon by patching in convex combination of task vectors. For example, for tasks copy(op1) and copy(op2), we first extract the corresponding task vectors Vcopy(op1) and Vcopy(op2) on Llama-3 8B using the method described in Appendix C. We then make convex combination of the two task vectors with parameter λ that controls the ratio: Vinterpolate,λ = λ Vcopy(op1) + (1 λ) Vcopy(op2). Finding 4: Convex combinations of task vectors produce task superposition. For new query (in this scenario in the form {op1}@{op2}=), we patch the vector Vinterpolate,λ into the model at the task vector layer. We calculate the model output probabilities that correspond to each task while we vary λ. For each λ, we use 100 different queries and plot the average probabilities in the top row of Figure 5. As comparison, in the bottom rows of Figure 5, we plot the corresponding output probabilities when providing the models with prompts containing mixture of task examples where the mixture ratio is controlled by λ."
        },
        {
            "title": "Preprint",
            "content": "n a r i t - (a) Tasks: copy(op1) and copy(op2) (b) Tasks: translate to de and to it Figure 5: On Llama-3 8B, we vary the proportion, λ, between two tasks and observe how the output probabilities for the correct answers change. The proportion λ is varied in two ways: (1) in the top row, we plot the output from patching in convex combination of task vectors for two tasks. (2) in the bottom row, we plot the output from mixed proportion of in-context examples for the two tasks. Subplot (a) shows the output probabilities from mixing two copy tasks and (b) shows the probabilities from mixing two translate tasks. In top row of Figure 5, we observe that patching convex combinations of task vectors into the model produces task superposition. We would also like to point out that in Figure 5b, although irrelevant outputs sum up to large probability, the task answers for two tasks to de and to it in most cases will still be the top-2 answers. Comparing the top rows and the bottom rows, we can see that top rows (the scenario of interpolating task vectors of individual tasks) have larger probabilities of irrelevant output (category other). Task vector interpolation also produces less of linear relationship between λ and the output probabilities. This shows that while convex combinations of task vectors are sufficient for producing task superposition, the convex combination does not fully explain task superposition. We leave it to future work to investigate other mechanistic explanations of task superposition."
        },
        {
            "title": "7 TASK SUPERPOSITION CAPABILITIES AS THE MODEL SCALES",
            "content": "Finding 5: Within the same LLM family, bigger models can solve more tasks in parallel and better calibrate to ICL distribution. We want to further investigate how models task superposition capabilities change as the model size scales. In particular, we investigate two questions: 1) whether larger models can perform more tasks in-context and 2) whether larger models can align their output distribution more closely with the distribution of task examples provided in the prompt. We chose the Qwen-1.5 model family since it contains several model sizes ranging from 0.5B to 14B parameters. We first introduce quantity which captures the capability of model to perform multiple tasks. Given prompt that contains examples of tasks, we define to be the number of these tasks whose correct answers appear among the models top-K most likely outputs. Note that K. To see how close the model align the output distribution with the distribution of task examples, we use KL-divergence defined below: KL(PD) = P(x) log (cid:18) P(x) D(x) (cid:19) , (cid:88) xX (2)"
        },
        {
            "title": "Preprint",
            "content": "where is the models probabilities on the outputs when correctly performing each task on the query and is the in-context task example distribution. For example the prompt in Figure 1a (left) gives = [0.5217, 0.1316, 0.1110, 0.2169, ...] and = [0.25, 0.25, 0.25, 0.25, 0, ...]. We consider the setting of = 6 different tasks: given an input of the form {num} where num is two-digit integer, we consider 6 tasks that output (1) num itself, (2) negation of num, (3) num + 1, (4) num 1, (5) num 2 and (6) num2. We choose the number of in-context examples = 60 (each task has 10 examples) and configure the prompt with three different in-context task example distributions D1, D2 and D3. In particular, D1 is the uniform distribution, D2 has probability 0.5 on the third task and 0.1 on other tasks, and D3 is distribution with probabilities alternating between 0.25 and 0.083. For each in-context task examples distribution Di, we generate 100 prompts and for each prompt we calculate the probabilities of outputs when correctly performing each task. The average values of and KL-divergence under three distributions are shown in Figure 6. (a) (the number of tasks whose correct answers appear in top-K most likely outputs). (b) KL divergence. Figure 6: (a) Average number of tasks completed, r, and (b) KL divergence for Qwen-1.5 model family under ICL distributions D1, D2 and D3 where D1 is the uniform distribution, D2 has probability 0.5 on the third task and 0.1 on other tasks, and D3 is distribution with probabilities alternating between 0.25 and 0.083. In Figure 6a, we can observe that bigger models have higher values (except for task distribution D2, 4B model has slightly lower than that of the 1.8B model). This shows bigger models will have more correct answers of tasks show up in their top-K probable outputs and therefore they can solve more tasks at the same time. In Figure 6b, we can see that for larger models like Qwen-1.5 7B and Qwen-1.5 14B, the KL-divergence values are small, and for each model, the differences between KL-divergence values under in-context task example distributions D1, D2 and D3 are small. This indicates that bigger models can better calibrate their output distribution to the in-context task example distribution."
        },
        {
            "title": "8 LIMITATIONS AND FUTURE DIRECTIONS",
            "content": "One limitation of our work is the current gap between the demonstrated capability of LLMs to perform task superposition and its practical application in real-world scenarios. While we have shown that LLMs possess the capacity to execute multiple tasks simultaneously, conventional decoding algorithms are not equipped to fully leverage this capability. This limitation stems from what we term generation collapse, phenomenon where, after the first token is generated, the model tends to converge on predicting tokens for single task, effectively negating its ability for multi-task execution. This collapse presents substantial challenge in harnessing the full power of task superposition. It highlights critical area for future research: developing decoding strategies that can maintain the models multi-task state throughout the generation process. Recent work by Shen et al. (2024b) offers some hope that this direction may be fruitful, by proposing superposed decoding algorithm. Their method efficiently generates multiple streams of tokens from single inference pass by utilizing superposed token embeddings. While this approach represents significant step forward, it also highlights the potential for further innovation in this area."
        },
        {
            "title": "9 CONCLUSION",
            "content": "We report on the discovery of task superposition, which is the ability of LLMs to simultaneously solve distinct tasks from in-context examples. Task superposition is present in variety of pretrained models, and becomes more accurate at predicting the distribution of tasks as the model size increases. We also find evidence that while displaying task superposition, models internally mix the task vectors of each individual task. We hope that our findings will contribute to understanding in-context learning mechanisms and enhance our knowledge of LLMs overall."
        },
        {
            "title": "REFERENCES",
            "content": "Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning, 2024. Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models, May 2023. URL http: //arxiv.org/abs/2211.15661. arXiv:2211.15661 [cs]. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection, 2023b. Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, Marc-Antoine Rondeau, Sanxing Chen, Yang Gao, and Jackie Chi Kit Cheung. Identifying and analyzing task-encoding tokens in large language models. (arXiv:2401.11323), February 2024. doi: 10.48550/arXiv.2401.11323. URL http: //arxiv.org/abs/2401.11323. arXiv:2401.11323 [cs]. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadIn Adford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. vances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:1887818891, 2022. Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition of many models into one, June 2019. URL http://arxiv.org/abs/1902.05522. arXiv:1902.05522 [cs]."
        },
        {
            "title": "Preprint",
            "content": "Hyeong Kyu Choi and Yixuan Li. Picle: Eliciting diverse behaviors from large language models with persona in-context learning. (arXiv:2405.02501), May 2024. doi: 10.48550/arXiv.2405.02501. URL http://arxiv.org/abs/2405.02501. arXiv:2405.02501 [cs]. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn incontext? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Shashank Rajput, Michael Gira, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. arXiv preprint arXiv:2206.06565, 2022. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy Models of Superposition, September 2022. URL http://arxiv.org/abs/2209.10652. arXiv:2209.10652 [cs]. Fabian Falck, Ziyu Wang, and Chris Holmes. Is in-context learning in large language models bayesian? martingale perspective. (arXiv:2406.00793), June 2024. URL http://arxiv. org/abs/2406.00793. arXiv:2406.00793 [cs, stat]. Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers, 2023. Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba capable of in-context learning? (arXiv:2402.03170), April 2024. doi: 10.48550/arXiv.2402.03170. URL http://arxiv.org/abs/2402.03170. arXiv:2402.03170 [cs]. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. (arXiv:2312.00752), December 2023. doi: 10.48550/arXiv.2312.00752. URL http://arxiv. org/abs/2312.00752. arXiv:2312.00752 [cs]. Roee Hendel, Mor Geva, and Amir Globerson. In-Context Learning Creates Task Vectors, October 2023. URL http://arxiv.org/abs/2310.15916. arXiv:2310.15916 [cs]. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing Models with Task Arithmetic, March 2023. URL http://arxiv.org/abs/2212.04089. arXiv:2212.04089 [cs]. Janus. Simulators, 2022. vJFdjigzmcXMhNTsx/. URL https://www.lesswrong.com/posts/ Andrew Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022. Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning with algorithm distillation. arXiv preprint arXiv:2210.14215, 2022. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. (arXiv:2404.02060), April 2024. doi: 10.48550/arXiv.2404.02060. URL http://arxiv.org/abs/2404.02060. arXiv:2404.02060 [cs]. Ziqian Lin and Kangwook Lee. Dual Operating Modes of In-Context Learning, February 2024. URL http://arxiv.org/abs/2402.18819. arXiv:2402.18819 [cs]. Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. (arXiv:2311.06668), February 2024. doi: 10.48550/arXiv.2311.06668. URL http://arxiv.org/abs/2311.06668. arXiv:2311.06668 [cs]."
        },
        {
            "title": "Preprint",
            "content": "Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. Z-icl: Zero-shot in-context learning with pseudo-demonstrations. arXiv preprint arXiv:2212.09865, 2022. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning (arXiv:2202.12837), October 2022. doi: 10.48550/arXiv.2202.12837. URL http: work? //arxiv.org/abs/2202.12837. arXiv:2202.12837 [cs]. moire. Language models are multiverse generators, January 2021. URL https://generative. ink/posts/language-models-are-multiverse-generators/. Vishvak Murahari, Carlos E. Jimenez, Runzhe Yang, and Karthik Narasimhan. Datamux: Data multiplexing for neural networks. (arXiv:2202.09318), November 2022. doi: 10.48550/arXiv.2202. 09318. URL http://arxiv.org/abs/2202.09318. arXiv:2202.09318 [cs]. Cleo Nardo. The waluigi effect (mega-post), 2023. URL https://www.lesswrong.com/ posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers Learn In-Context by Gradient Descent. In Proceedings of the 40th International Conference on Machine Learning, pp. 3515135174. PMLR, July 2023. URL https://proceedings.mlr.press/v202/von-oswald23a.html. ISSN: 2640-3498. Madhur Panwar, Kabir Ahuja, and Navin Goyal. In-context learning through the bayesian prism, June 2023. URL https://arxiv.org/abs/2306.04891v2. Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? comparative study on in-context learning tasks. (arXiv:2402.04248), April 2024. doi: 10.48550/arXiv.2402.04248. URL http://arxiv.org/abs/2402.04248. arXiv:2402.04248 [cs]. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. The effects of pretraining task diversity on in-context learning of ridge regression. In ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo), 2023. Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=aN4Jf6Cx69. Laria Reynolds and Kyle McDonell. Multiversal views on language models, February 2021. URL http://arxiv.org/abs/2102.06391. arXiv:2102.06391 [cs]. Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language models. Nature, 623(7987):493498, 2023. Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, and Aditya Kusupati. Superposed decoding: Multiple generations from single autoregressive inference pass. (arXiv:2405.18400), May 2024a. doi: 10. 48550/arXiv.2405.18400. URL http://arxiv.org/abs/2405.18400. arXiv:2405.18400 [cs]. Ethan Shen, Alan Fan, Sarah Pratt, Jae Sung Park, Matthew Wallingford, Sham Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, and Aditya Kusupati. Superposed decoding: Multiple generations from single autoregressive inference pass. arXiv preprint arXiv:2405.18400, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. Function vectors in large language models. (arXiv:2310.15213), February 2024. doi: 10.48550/ arXiv.2310.15213. URL http://arxiv.org/abs/2310.15213. arXiv:2310.15213 [cs]. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: case study on approximating turing machines with transformers. Advances in Neural Information Processing Systems, 35:1207112083, 2022. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In International Conference on Learning Representations (ICLR), 2024. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. (arXiv:2111.02080), July 2022. doi: 10.48550/arXiv.2111. 02080. URL http://arxiv.org/abs/2111.02080. arXiv:2111.02080 [cs]. Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023. Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, and Kangwook Lee. Can mllms perform text-to-image in-context learning? (arXiv:2402.01293), April 2024. doi: 10.48550/arXiv.2402. 01293. URL http://arxiv.org/abs/2402.01293. arXiv:2402.01293 [cs]. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does incontext learning learn? bayesian model averaging, parameterization, and generalization. (arXiv:2305.19420), October 2023. doi: 10.48550/arXiv.2305.19420. URL http://arxiv. org/abs/2305.19420. arXiv:2305.19420 [cs, stat]. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? study in length generalization. arXiv preprint arXiv:2310.16028, 2023."
        },
        {
            "title": "A NOTATIONS",
            "content": "Notation ℓ gi() x(j) y(j) sm () Description Number of tasks Length of tasks output layer ℓ for model Number of in-context examples Length of each in-context example Token vocabulary Operation performed by Task Data for example Label for example in-context examples Model (predictor) Positional encodings"
        },
        {
            "title": "B IMPLEMENTATION DETAILS ON CALCULATING PROBABILITIES",
            "content": "In this section we provide details on how we calculate probabilities of different outputs given prompt in our setting. Notations. Let be the token vocabulary, be an LLM, be the tokenizer. We use ... to represent string, <...> to represent single token where the content within the angle brackets is an integer representing tokens index in vocabulary. For example, token <266> corresponds to at. We use [<...>, ..., <...>] to represent sequence of tokens. Given tokenizer, we use two functions tok() and detok() to tokenize strings and detokenize tokens. For example tok(superposition) = [<9712>,<3571>] and detok([<16>,<10>,<16>,<28>,<17>]) = 1+1=2. In our in-context learning setting, an input string consists of in-context examples (separated by the delimiter n) and query. For example, an example prompt can be 1+1=2n2+2=4n3+3=. We view an LLM as next-token predictor and there is corresponding P() such that given sequence of tokens [v1, ..., vM ] where vj V, P(u [v1, ..., vM ]) measures the probability of the next token being where V. Measuring the probabilities of task answers. Let be the input prompt. For example, in the example in Figure 1a (left), the prompt is 11+26->37n33+13->quarante-sixn ...30+25->fiftyfiven91+83->. We consider four tasks: 1) numerical addition, 2) addition in English, 3) addition in French and 4) addition in Spanish. The corresponding task answers (the output of correctly performing task on the query) are 174, one hundred and seventy-four, cent soixante-quatorze and ciento setenta cuatro, respectively. We want to measure the probability of each task answer. Let be task answer in string. Let [v1, ..., vM ] := tok(I) and let [u1, ..., uN ] := tok(o). Then the probability of the task answer given prompt can be calculated as P(u1 [v1, ..., vM ]) (cid:89) j=2 P(uj [v1, ..., vM , u1, ..., uj1]). (3)"
        },
        {
            "title": "C IMPLEMENTATION DETAILS ON TASK VECTORS",
            "content": "We use the task vector definition from Hendel et al. (2023). For example, for task copy(op1) in Figure 1b (left), the procedure to collect the task vector consists of 1. Collect dataset of 100 ICL sample prompts. Each prompt consists of = 60 in-context examples of particular task and query x(m+1). Each task example (x(j), y(j)) follows the form {op1}@{op2}={op1}, where x(j) has the form {op1}@{op2}= and y(j) is performing task copy(op1) on x(j), namely op1. 2. For each prompt = [x(1), y(1), ..., x(m), y(m), x(m+1)] in the dataset, we feed into the transformer model , and extract the feature (which is vector) at the last = token in layer ℓ. Call this vector (s; ℓ). Then we average (s; ℓ) across all prompt to get v(ℓ) for layer ℓ. 3. Now for each layer ℓ we have vector v(ℓ). We run forward pass with one query in the form {op1}@{op2}= and we patch in v(ℓ) at the = token position in layer ℓ, simulating the effect of complete context. We repeat this process 100 times for different query and get an accuracy accℓ of performing task copy(op1) with vector v(ℓ). 4. The task vector layer ℓ is selected by and we define the task vector Vcopy(op1) := = v(ℓ). ℓ = arg max ℓ accℓ, Here we record the task vector layer where task vectors are extracted in Section 6. Task copy(op1), copy(op2), op1+op2 to de(op1), to fr(op1), to it(op1) Task vector layer 14 Table 1: Task vector layer for various tasks considered in Section 6."
        },
        {
            "title": "D CONSTRUCTION DISPLAYING SUPERPOSITION",
            "content": "In this section we construct Transformer that is performing superposition of multiple tasks at inference. For this purpose, we first construct Transformer that copies from n-tuple in-context examples the i-th one, as well as any function using the ReLU layers. We then create indicator vectors, for each task, which show whether specific task is present in-context or not. As last step, we combine these indicator vectors to create the superposition of different tasks. Notice that using the parallel heads of the transformer architecture we can process each task independently until the last step in which the predictions are combined. D.1 OVERVIEW Here we provide brief overview of how the construction is implemented, while latter we provide the corresponding details. 1 , . . . , x(j) n2, =, y(j))m Prediction based on multiple tasks. Assume that we are given in-context samples (x(j) j=1 where = represents specific value used only for preceding the label, and set of different Transformers TFi which can implement the different desired tasks, where each deterministic task is denoted as gi(x(j)) with [k] and [m], i.e. y(j) = gi(x(j)) for some task dependent on sample j. Using the weights of each TFi, we can compute outputs of the following form: . . . x(j) 1 0 . . . ... 0 . . . . . . x(j) n2 = y(j) 0 0 0 . . . ... ... ... 0 0 0 . . . . . . . . . . . . . . . x(j) 1 0 . . . ... 0 . . . . . . x(j) n2 = 0 . . . ... 0 . . . y(j) 0 g1(x(j)) y(j)1 ... ... 0 gT (x(j)) y(j)1 . . . . . . . . . We use the l1 norm to aggregate the prediction, in case that the task is multi-dimensional. These differences are used to identify tasks, as gi(x(j))y(j)1 0 for y(j) coming from task i. Different heads at each layer in the model are used to execute each of the tasks in parallel using the weights from TFi. In Appendix we construct tasks where an arbitrary function gi(x(j) ) is implemented using ReLUs for some fixed that is task-specific. 1:l1) x(j) k)i = gk(x(j) Creating task identifiers. Having the differences between the implemented function and the label, we first use the ReLUs to clean up the vectors vk so that only the positions in each vector that are associated with task are maintained and the rest are set to 13. We thus create the vectors (v ) = 1 otherwise. Now we use ReLUs to threshold and create 10} which identify the task, i.e. these are task identifiers. an indicator vectors 1{gk(x(j) Notice that if the task is correctly predicted then the difference should be close to 0 (up to some error), while if the task is not identified the corresponding value would not be 0; the rest of the rows would be 1. We have created one vector for each task, which has 1 in the position of the corresponding task if the task was identified in the context. 1 and (v 1:l1)x(j) Averaging and task superposition. As last step, we average all the task identifiers and place the result in the last column, in which the next prediction will happen. We then use the averaged task identifier to weight the prediction of each task based on it, as in task superposition. If the task has been identified multiple times in the context, it would be assigned higher weight/probability. D.2 TASK IDENTIFICATION The first task for performing task superposition based on in-context examples is to define set of tasks that the model is able to implement. First, the outputs of tasks need to be identified. 3This step is not mandatory, but it ensures that we have no values over which we have no control. We leave as future work an error analysis on how these values could affect the task identifiers"
        },
        {
            "title": "Preprint",
            "content": "Lemma 1. Consider the following input x(1) 1 0 0 . . . y(j1) x(j) 1 0 0 . . . 0 0 . . . = . . . x(j) n2 = y(j) x(j+1) 0 0 . . . 0 0 . . . 0 0 1 0 1 , . . . . . . . . . where x(j) Rd1 before the positional encodings are added, with one additional dimension that represents if the symbol is an equals symbol. Then, 1-layer transformer with single attention head and embedding dimension O(d + log(mn)) can output = (cid:20) x(1) 1 0 . . . y(j1) x(j) 1 0 1 . . . . . . x(j) n2 = y(j) x(j+1) 1 0 . . . 0 1 (cid:21) . . . . . . Proof. With positional encodings appended, let the input have the following structure: = x(1) 1 0 pn+1 pn x(j) 1 0 x(j) 2 0 . . . . . . . . . pjn+1 pjn+2 pjn+1 . . . pjn x(j) n2 0 y(j) . . . 0 . . . pjn+n . . . pjn+n2 pjn+n1 . . . pjn+n3 pjn+n2 pjn+n1 = 1 x(j+1) 1 0 p(j+1)n+1 p(j+1)n . . . . . . . . . . . . (4) To rotate the second row one position to the right, use the following matrices. WQ = [0 0 0 I] WK = [0 0 CI 0] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 WV = The pair WQ and WK attend tokens to the token directly to the right. The value matrix simply filters only the second row in-place. second head can used to clear the original 1s, resulting in = x(1) 1 0 pn+1 pn x(j) 2 0 x(j) 1 0 . . . . . . . . . pjn+1 pjn+2 pjn+1 . . . pjn x(j) n2 0 y(j) . . . 1 . . . . . . pjn+n2 pjn+n1 pjn+n . . . pjn+n3 pjn+n2 pjn+n1 = 0 x(j+1) 1 0 p(j+1)n+1 p(j+1)n . . . . . . . . . . . . , (5) as desired. Implementation of functions. To illustrate set of operations that could be implemented with transformer, we consider approximating functions as sums of ReLUs; we use result from Bai et al. (2023b), which we present below. Definition 1 (Definition 12 in Bai et al. (2023b)). function : Rk is (ϵ, R, M, C)- approximable by sum of ReLUs, if there exists an (M,C)-sum of ReLUs function cm C, max m[M ] am1 1, am Rk+1, cm fM,C(x) = (cid:88) m=1 cmReLU(a m[x; 1]) with (cid:88) (cid:12) (cid:12)g(x) f(M,C)(x)(cid:12) m=1 (cid:12) ϵ. such that supx[R,R]k] Definition 2 (Definition A.1 in Bai et al. (2023b)). We say function : Rk is (R, Cl)-smooth if for = (k 1)/2 + 2, 24 on [R, R]k and (cid:13)ig(x)(cid:13) (cid:13) (cid:12)xj1,...,xjig(x)(cid:12) (cid:12) (cid:12) Li (cid:13) = sup x[R,R]k max j1,...,ji[k] sup x[R,R]k for all = 0, 1, 2 with max0is LiRi Cl. 4C denotes that function is times differentiable with continuous i-th derivative."
        },
        {
            "title": "Preprint",
            "content": "Proposition 1 (Proposition A.1 in Bai et al. (2023b)). For any ϵ > 0, 1, Cl > 0, we have that: Any (R, Cl)-smooth function, : is (ϵ, R, M, C)-approximable by sum of ReLUs (Definition 1) with C(k)C 2 Lemma 2. For any function : Rk that is (R, Cl)-smooth, there exists transformer with two layers, one head and width O(log(n) + d), where satisfies the requirements of Prop. 1, such that given as input log(1 + Clϵ)/ϵ2. = x(1) 1 0 . . . y(j1) x(j) 1 0 1 . . . 0 0 . . . n2 = y(j) x(j+1) . . . x(j) 1 0 . . . 0 0 . . . 0 0 0 0 1 , . . . . . . . . . it outputs = x(1) 1 . . . . . . . . . y(j1) ) y(j1) g(x(j1) x(j) 1 . . . x(j) n2 = . . . . . . y(j) g(x(j) ) y(j) x(j+1) 1 . . . . . . . . . where g(x) g(x) ϵ and for some [1, . . . , 2]. Proof. We consider that the positional encodings are added in the input and we have = x(1) 1 0 0 1 pn+1 pn+1s x(j) 1 0 0 1 pjn+1 x(j) . . . 2 0 . . . 0 . . . 1 . . . . . . pjn+2 . . . pjn+1s pjn+2s x(j) n2 0 0 1 pjn+n2 x(j+1) . . . 1 0 . . . 0 . . . 1 . . . . . . p(j+1)n+1 . . . pjn+n2s pjn+n1s pjn+ns p(j+1)n+1s = 0 0 1 pjn+n1 y(j) 1 0 1 pjn+n (6) where we fix some positional encodings pk where pl by some threshold for = l. The encodings used here are the binary representations of {1, 1}log(mn). Further, we consider 1s in the positions with the results of the task to differentiate the context of the task and the result of the task. Define = i, the distance between the result and the associated value in the context. pk is larger than In the first layer, we use the MLPs to create according to Proposition 1 = x(1) 1 0 g(x(1) 1 ) 0 1 pn+1 pn+1s x(j) 1 0 g(x(j) 1 ) 0 1 pjn+1 x(j) . . . 2 0 . . . g(x(j) 2 ) . . . 0 . . . 1 . . . pjn+2 . . . . . . pjn+1s pjn+2s x(j) n2 0 g(x(j) n2) 0 1 pjn+n2 x(j+1) . . . 1 0 . . . g(x(j+1) . . . 1 0 . . . 1 . . . p(j+1)n+1 . . . . . . pjn+n2s pjn+n1s pjn+ns p(j+1)n+1s = 0 g(=) 0 1 pjn+n y(j) 1 g(y(j)) 0 1 pjn+n ) The next operation is shift of the sequence of g()s to the right by s. This will align the desired output g(x(j) ) with the observed output y(j). Consider the following weight matrices (7) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . WQ = [. . . WK = [. . . WV = 0 0 I] 0 CI 0] 0 0 0 . . . 0 0 0 0 . . . 0 0 0 . . . 0 ... ... ... ... 0 0 0 . . . 0 (8) (9) (10) for some large constant to decrease error from the softmax attending to the incorrect tokens. This produces (within small error induced by using softmax) 20 (11) (12) (13) (14)"
        },
        {
            "title": "Preprint",
            "content": "(X σS(X WQX)i,j = n+ipns+j WQX)i,j = 1{n+i=ns+j} = 1{i=js} WV = . . . . . . . . . . . . 0 0 g(x(j) 1 ) ... 0 0 0 g(x(j) 2 ) ... 0 . . . . . . . . . . . . 0 0 g(x(j) n2) ... 0 0 g(=) ... 0 0 0 g(y(j)) ... 0 . . . . . . . . . . . . . . . . . . . . . 0 0 g(x(j) ) ... 0 WV XσS(X WQX) = + WV XσS(X WQX) = 0 ... 0 ... . . . . . . . . . 0 0 . . . 0 0 0 . . . 0 . . . ... ... . . . 0 0 . . . 0 0 x(j) . . . 2 0 . . . . . . 0 . . . 1 . . . . . . pjn+2 . . . pjn+1s pjn+2s x(j) 1 0 0 1 pjn+1 . . . x(j) n1 0 0 1 pjn+n y(j) . . . 1 . . . g(x(j) ) . . . 0 . . . 1 . . . . . . pjn+n . . . pjn+n2s pjn+n1s pjn+ns = 0 0 1 pjn+n1 . . . . . . . . . . . . . . . . . . . . . Each matrix above only shows the slice that contains the j-th in-context example. This is repeated for each of the other in-context examples. As final step with an MLP, subtract row 1 from row 3 to achieve the following output: (15) (16) x(j) 1 0 0 1 pjn+ x(j) . . . 2 . . . 0 . . . . . . 0 . . . 1 pjn+2 . . . . . . pjn+1s pjn+2s x(j) n1 0 0 1 pjn+n2 . . . = . . . 0 . . . . . . 0 . . . 1 pjn+n1 . . . . . . pjn+n2s pjn+n1s y(j) 1 g(x(j) ) y(j) 0 1 pjn+n pjn+ns . . . . . . . . . . . . . . . . . . . . . (17) Copy Tasks As has been experimentally investigated, the situation where specific position within the context is copied as the label can be easily implemented by setting g(x) = x. The dependence on the subscript within the construction is what allows the position copied to vary. D.2.1 IDENTIFYING IF TASKS OUTPUT MATCHES THE IN-CONTEXT EXAMPLE Lemma 3. three layer transformer with ReLU MLPs and embedding dimension O(d + log(mn)) can calculate the proportion of in context examples that come from specific task, where is the number of in-context examples, each of length and dimension d. Proof. We now have matrix of the following form."
        },
        {
            "title": "Preprint",
            "content": "x(j) 1 0 0 1 pjn+1 x(j) . . . 2 0 . . . . . . 0 . . . 1 . . . . . . pjn+2 . . . pjn+1s pjn+2s . . . = 0 . . . . . . 0 . . . 1 . . . . . . pjn+n1 . . . pjn+n1s (x(j) y(j) 1 ) y(j) 0 1 pjn+n pjn+ns . . . . . . . . . . . . . . . . . . . . . (18) If the task is correct, than (x(j) function approximation error. First, we find the L1-norm of (x(j) calculating z1 for arbitrary z, we can use ) y(j) 0, with some small error coming from softmaxs and ) y(j) using an MLP. For z1 = (cid:88) i= ReLU(zi) ReLU(zi) (19) which can be done in single 1-layer MLP. Thus, we have x(j) 1 0 0 1 pjn+ x(j) . . . 2 0 . . . . . . . . . 0 . . . 1 . . . pjn+2 . . . . . . pjn+1s pjn+2s . . . = 0 . . . . . . . . . 0 . . . 1 . . . pjn+n1 . . . . . . pjn+n1s (x(j) (x(j) y(j) 1 ) y(j) ) y(j)1 0 1 pjn+n pjn+ns . . . . . . . . . . . . . . . . . . . . . . . . (20) Notice that if some task has different dimension than another task, the extra rows would be zero and will not affect the result. For clarity, we set all values in the 1 row to 1s. These will cause the following ˆδ in the following set these to 0. This operation can be omitted as the construction handles these trash values at later layer. Let represent the value of the flag in the second row marking the vectors and let represent the values in the row with (x(j)) y(j)1. The following ReLUs set the * values to 1. + 1 ReLU(x Cb) ReLU(Cb + 1) (21) for some large constant C. When = 0, this reduces to + 1 = 1, and when = 1, this reduces to + 1 1 = x, as desired. x(j) 1 0 1 0 1 pjn+1 x(j) . . . 2 0 . . . . . . 1 . . . 0 . . . 1 . . . . . . pjn+2 . . . pjn+1s pjn+2s . . . = 0 . . . . . . 1 . . . 0 . . . 1 . . . . . . pjn+n1 . . . pjn+n1s (x(j) (x(j) y(j) 1 ) y(j) ) y(j)1 0 1 pjn+n pjn+ns . . . . . . . . . . . . . . . . . . . . . . . . (22) Now define thresholding function ˆδ(z) that satisfies ˆδ(0) = 1 and ˆδ(z) = 0 for >> 0. One such function used here is ˆδC(z) = ReLU(1 Cz) (23)"
        },
        {
            "title": "Preprint",
            "content": "for some constant C, where larger captures narrower neighborhood of 0. However, slight change needs to be added to ˆδC. In the same row as (x(j)) y(j) are many values that need to be discarded. Let be the bit for the current column marking if the column contains an or y. We use instead ˆδC(b, z) = ReLU(b Cz) (24) This will be zero whenever = 0 and 0. We then have as output x(j) 1 0 1 0 0 1 pjn+1 x(j) . . . 2 . . . 0 . . . . . . 1 . . . 0 . . . 0 . . . 1 pjn+2 . . . . . . pjn+1s pjn+2s . . . = . . . 0 . . . . . . 1 . . . 0 . . . 0 . . . 1 pjn+n1 . . . . . . pjn+n1s y(j) 1 (x(j) ) y(j) (x(j) ) y(j)1 ˆδC(f (x(j) ) y(j)1) 0 1 pjn+n pjn+ns Importantly, ˆδC(f (x(j) when () disagrees by more than 1 in L1-norm. )y(j)1) = 1 when () is the correct task and ˆδC(f (x(j) . . . . . . . . . . . . . . . . . . . . . . . . . . . (25) )y(j)1) = 0 Lastly, for the next step in the construction, we need to average these soft indicators ˆδ to see how common is within the context. This is done with an attention layer. Let WQ select the row with all 1s multiplied by some large constant C, and let WK select the row with flags for results y. Then K WQX = σS(X WQX) . . . . . . ... 0 0 ... 0 ... 0 0 ... 0 ... ... 0 0 0 0 ... ... 0 . . . 0 . . . ... ... ... ... . . . . . . ... 0 0 ... 0 ... 0 0 ... . . . 0 1/m 1/m . . . ... ... ... 0 0 ... 0 ... 0 0 ... 0 1/m 1/m ... ... (26) (27) where 1/m will appear in every row corresponding to result y. Let the value matrix select the row containing ˆδ(f (x(j) y(j))1). Denote = 1 ˆδ(f (x(j) y(j))1). Without causal masking, we would have as output (cid:80)m j="
        },
        {
            "title": "Preprint",
            "content": "x(j) 1 0 1 0 0 1 pjn+1 x(j) . . . 2 0 . . . . . . 1 . . . 0 . . . . . . 0 . . . 1 . . . . . . pjn+2 . . . pjn+1s pjn+2s . . . = 0 . . . . . . 1 . . . 0 . . . . . . 0 . . . 1 . . . . . . pjn+n1 . . . pjn+n1s y(j) 1 (x(j) ) y(j) (x(j) ) y(j)1 ˆδC(f (x(j) ) y(j)1) 0 1 pjn+n pjn+ns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (28) However, with causal masking, we can only guarantee that will appear in the columns containing the most recent example being queried. Thankfully, this is all that is needed. D.3 TASK EXECUTION Lemma 4. two layer transformer, with embedding dimension O(d + log(mn)) can perform task and weight its output by the proportion of examples of that task seen within the context. Now that the proportions of each task have been identified in the context, the task itself needs to be executed for the new example being queried. To simplify notation, let the input to this step be = . . . x(m) . . . . . . . . . . . . . . . 1 0 1 x(m) 2 0 1 . . . x(m) n2 = . . . . . . 0 0 . . . . . . 1 1 . . . (29) Following the same process as outlined above, although with slightly different positional encodings, calculate (x(m)) and place that result in the final column being decoded. These need to be added at the beginning of the construction, but are only introduced here for clarity. . . . x(m) . . . . . . . . . . . . . . . . . . . . . . . . 1 0 1 0 0 x(m) 2 0 1 0 0 . . . x(m) n2 . . . . . . . . . 0 . . . . . . 1 . . . 0 . . . 1 . . . x(m) n1 (x(m)) 0 1 1 (30) We will transform the row containing all to be able to approximately multiply by (x(m)). Using the second to last row, perform 1 p. Using the last two rows, clear out the rest of that row and fill it with for some large constant C. We then have 1 x(m) 2 . . . x(m) . . . x(m) n2 . . . . . . . . . C . . . . . . . . . 0 . . . . . . . . . . . . 1 . . . . . . 0 . . . . . . 1 . . . . . . 0 1 0 0 0 1 0 0 24 x(m) n1 1 (x(m)) 0 1 1 0 (31)"
        },
        {
            "title": "Preprint",
            "content": "Further, use the second-to-last row to clear out all in the rows below. 1 x(m) 2 . . . x(m) . . . x(m) n2 . . . . . . . . . C . . . 0 . . . . . . 0 . . . . . . . . . . . . 1 . . . . . . 0 . . . . . . 1 . . . . . . 0 0 1 0 0 0 0 1 0 0 x(m) n1 1 (x(m)) 0 1 1 0 (32) These previous operations can all be done in single MLP. Lastly, use an attention layer where WK selects the row with the Cs, WQ selects the row with all 1s, and WV selects the (x(m)). For the last token xL, WQxL = σS(X WQxL) ... C ... 1 ... ... 1 [1] = = ... C ... 1 ... 0 0 ... 1 1+e12p 1 1 1+e12p WV xLσS(X K WQxL) = xL + WV xLσS(X WQxL) = 1 ... 0 1+e12p )f (x(m)) 1+e12p 0 + (1 0 ... 1 1 x(m) n1 1 1+e12p (x(m)) 0 1 1 (33) (34) (35) (36) 1+e12p is approximately p, especially Importantly, we are left with around 1 2 . This multiplication can also be calculated more accurately with approximations using ReLUs or sigmoids, but for brevity and following experimental evidence of sigmoid shape in task superpositions, these options are ommited. 1+e12p (x(m)). The factor 1"
        },
        {
            "title": "Preprint",
            "content": "D.4 SUPERPOSED TASKS WITH PARALLEL HEADS The above construction works for single task, where the output is weighted by the proportions of the task within the context. To complete the construction of transformer that does superposition of tasks, each of these models needs to be placed within the same overall transformer. This is described here. Let there be collection of tasks {ti}T i=1 which can be executed by transformers with model weights represented by subscripts (i). With the input to each transformer being (i), the overall input matrix is given by vertically stacking these matrices. = X1 X2 ... XT 1 XT Similarly, define each MLPs weights and biases as = diag(W1, . . . , WT ) = (37) (38) b1 ... bT This puts every MLP to be independent of each other. Lastly, we need to change the attention layers. This requires the use of one head per task. In each of the following, (i) is weight matrix for head i, (W )i is the weight matrix for task in its individual transformer, and each matrix below is in the i-th block. (i) = ... 0 (WV )i 0 ... (i) = (i) = ... 0 (WK)i 0 ... ... 0 (WQ)i 0 ... (39) In all, this model executes multiple tasks in superposition by using parallel streams of heads that each performs single task. Task identification can happen through the same mechanism as task execution by comparing the output of the task on each in context example with the true output. For context related tasks, there needs to be positional encodings that allow for looking back fixed number of tokens. For context agnostic tasks, wide MLP can be used to approximate arbitrary non-linear transformations of the input. Each of these tasks only require small number of layers, significantly smaller than those of modern LLMs. It may be possible that LLMs do certain tasks with different combinations of layers. Also, if we take the feature from each parallel stream, this creates the following task identifier. (40) = p1 p2 ... pT Interpolating between the pure tasks, represented by unit vectors, different amounts of each task will appear in the superposition in roughly equal proportions to those found in v. Lastly, we restate this construction formally."
        },
        {
            "title": "Preprint",
            "content": "Theorem 1. seven layer transformer with embedding dimension O(d + log(mn)) with heads per attention layer can perform tasks on vectors of dimension in superposition, with weighting based on different in-context examples each of length . Proof. Using in succession each of Lemma 1, Lemma 2, Lemma 3, and Lemma 4, transformer with the desired properties can execute tasks in parallel. Lemma 1 identifies positions within the context that contain the labels y. Lemma 2 then uses function approximation to perform arbitrary tasks within the architecture, which are then used by 3 to find the proportions of each task and aggregate them into single task identifier. Lastly, Lemma 4 uses this task identifier to create weighted sum of outputs from the different tasks based on their in-context proportions. Remark. Transformers of greater depth than seven layers can also represent this construction by setting the weights in all other layers for the non residual part to zero."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "University of Michigan",
        "University of Wisconsin-Madison"
    ]
}