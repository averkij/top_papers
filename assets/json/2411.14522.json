{
    "paper_title": "GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI",
    "authors": [
        "Tianbin Li",
        "Yanzhou Su",
        "Wei Li",
        "Bin Fu",
        "Zhe Chen",
        "Ziyan Huang",
        "Guoan Wang",
        "Chenglong Ma",
        "Ying Chen",
        "Ming Hu",
        "Yanjun Li",
        "Pengcheng Chen",
        "Xiaowei Hu",
        "Zhongying Deng",
        "Yuanfeng Ji",
        "Jin Ye",
        "Yu Qiao",
        "Junjun He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 2 2 5 4 1 . 1 1 4 2 : r GMAI-VL & GMAI-VL-5.5M: Large Vision-Language Model and Comprehensive Multimodal Dataset Towards General Medical AI Tianbin Li1*, Yanzhou Su1*, Wei Li1,2 Bin Fu1,3, Zhe Chen1,4, Ziyan Huang1,2 Guoan Wang1,5, Chenglong Ma1,6, Ying Chen1,7, Ming Hu1,8, Yanjun Li1,5, Pengcheng Chen1,9, Xiaowei Hu1, Zhongying Deng1,10, Yuanfeng Ji11, Jin Ye1,8, Yu Qiao1, Junjun He1 1Shanghai AI Laboratory 2Shanghai Jiao Tong University 3Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences 5East China Normal University 8Monash University 4Nanjing University 7Xiamen University 9University of Washington 6Fudan University 10University of Cambridge 11Stanford University"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, general medical vision-language model with progressively three-stage training strategy. This approach significantly enhances the models ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-ofthe-art results across wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL. *Equal contribution. Work done as an intern in Shanghai AI Lab. Corresponding author. 1 (LVLMs) have Large-scale Vision-Language Models rapidly evolved in recent years, effectively integrating visual perception with language understanding by leveraging large-scale multimodal data, which enables them to capture complex visual and textual patterns and drive significant advancements in image recognition, natural language processing, and multimodal tasks. With the advancement of multimodal integration technologies, the demand for highprecision processing of diverse data types in the medical field has become increasingly critical. The ability to effectively integrate and analyze various data modalities, such as medical images, clinical text, and structured clinical records, is pivotal for achieving accurate and comprehensive diagnostic and treatment decisions. However, existing LVLMs, such as GPT-4 [1], are limited in medical applications due to their lack of domainspecific knowledge, highlighting the need for specialized solutions that effectively integrate medical expertise. Addressing this challenge requires constructing comprehensive medical vision-language dataset and developing domain-specific models. For the medical dataset, it should provide high-quality medical knowledge, including the following three aspects: Comprehensive medical tasks. To enhance the models applicability across various medical scenarios, the dataset should cover wide range of medical contexts, such as disease types, symptoms, and treatments. Comprehensive task coverage can improve the models generalization ability and increase its reliability in real-world applications. However, existing models often focus on specific domains [28, 35, 41, 65, 72], limiting their broader applicability. Expanding the datasets scope will further enhance the models utility in clinical practice. Rich multimodal representation. well-rounded medical multimodal dataset should encompass various modalities, including different medical imaging types (such as CT, MRI, and X-rays) and diverse forms of textual data (such as medical records and imaging reports). This would allow models to better integrate multi-source information and improve their analytical capabilities. However, existing methods tend to focus on single type of medical imagery [33, 50, 70], limiting the models adaptability to diverse clinical scenarios. more diverse multimodal dataset would provide foundation for developing more comprehensive models, better suited to the complexity of realworld medical environments. High-quality image-text data. High-quality training data is essential for optimizing model performance, especially in medical applications. Ideally, image-text data for medical contexts should consist of large, well-curated collection of medical images paired with precise textual descriptions. This data would enhance the models understanding of key medical concepts, such as diagnosis, treatment, and clinical workflows, ultimately contributing to improved clinical outcomes. While progress has been made with datasets sourced from platforms like PubMed [39, 54, 70], these datasets suffer from inconsistent data quality, imprecise alignment between images and text, and lack of standardization, which limits their effectiveness. Based on above observations, we propose an annotationguided data generation methodology for developing comprehensive multimodal medical datasets. The methodology begins by collecting large-scale, open-source medical imaging datasets and extracting key annotations, such as modality, task type, labels, and bounding boxes. visionlanguage model (e.g., GPT-4o) is then used to transform these datasets, covering tasks like lesion detection, segmentation, and disease diagnosis, into high-quality image-text pairs for training LVLMs. To ensure data quality, extracted image information is incorporated into the prompt design, improving model performance across various clinical tasks. This results in comprehensive multimodal dataset with 5.5M samples, named GMAI-VL-5.5M, which supports the development of general medical LVLMs. Fig. 1(a) illustrates the sources, departments, modalities, task types, and instruction formats of the constructed dataset. With the constructed GMAI-VL-5.5M dataset, we develop general medical vision-language model, GMAI-VL. To enhance its integration of visual and linguistic features and its instruction-following abilities, three-stage training strategy is proposed in this paper. Specifically, we sequentially implement shallow and deep alignments in the first two stages, gradually building associations between visual (medical images) and language (medical texts) elements from basic features to high-level semantics. Next, we fine-tune the model with cross-modal instructions, improving its understanding of visual-language interactions and instruction-following in complex tasks. With this strategy, GMAI-VL shows strong performance in medical tasks like visual question answering and medical image diagnosis, providing solid foundation for advancing multimodal models in the medical field. The contributions of this work are as follows: We build an annotation-guided data generation methodology to construct GMAI-VL-5.5M, comprehensive vision-language dataset. This dataset offers extensive coverage of medical tasks, diverse multimodal representations, and high-quality image-text pairs, providing robust foundation for model training. Leveraging GMAI-VL-5.5M, we design GMAI-VL, versatile medical vision-language model. Our proposed three-stage training strategy enhances its ability to integrate visual and linguistic features, significantly improving its performance in instruction-following and generalization across wide range of medical tasks. GMAI-VL outperforms existing models in multimodal question-answering tasks, including PMC-VQA and VQA-RAD. It sets new benchmarks on OmniMedVQA, GMAI-MMBench, and Health & Medicine track of MMMU. Specifically, GMAI-VL achieves an average score of 88.48% on OmniMedVQA, 62.43% on the GMAI-MMBench test set, and 51.3% on the Health & Medicine track of MMMU. 2. Related Work Large-scale medical vision-language datasets of high quality and multiple modalities are the basis of Large Vision-Language Models (LVLMs) in the medical domain. While natural language and vision datasets are easily accessible online, biomedical datasets often focus on text or images only and many of them are limited to specific tasks or modalities, thus with unsatisfactory generalization ability. Notable datasets like MIMIC-CXR [33] and CheXpert [12] have advanced radiology models but are restricted to single image modality (X-ray), which hinders their use as generalpurpose medical LVLMs. To address this, researchers have begun scraping public sources like PubMed and textbooks to construct large-scale vision-language datasets. Examples include datasets proposed in LLaVA-Med [39], MedFlamingo [54], and PubMedVision [14], with PubMedVision optimizing LLaVA-Med dataset for higher-quality medical data. In addition to scraping, open-source image datasets with annotations can also be converted into imagetext pairs for model training. Specifically, image information like modalities and annotations are input into large language models, e.g., GPT series, to generate text paired with the corresponding image. Some popular examples in2 Figure 1. Overview of GMAI-VL and GMAI-VL-5.5M. (a) illustrates the sources, departments, modalities, task types, and instruction formats of the GMAI-VL-5.5M dataset. (b) Architecture of GMAI-VL, integrating Vision Encoder, Projector, and Large Language Model. (c) Three-stage training process of GMAI-VL, including shallow alignment, deep alignment, and instruction tuning with corresponding data sizes and training components. The flame symbol denotes the training part, while the snowflake symbol indicates frozen part. clude the datasets constructed in RadFM [70], MedDr [28], MedTrinity-25M [71], ChiMed-VL [48], BiomedGPT [79], Med-Gemini [58], and Med-PaLM [59]. These efforts usually suffer from either limited modalities, data sources, or task coverage. Thus, their dataset quality needs further improvement. To this end, we propose to construct comprehensive medical vision-language dataset with extensive coverage of medical tasks, diverse multimodal representations, and high-quality image-text pairs, forming robust foundation for model training. Medical vision-language models are usually based on general-purpose Large Vision-Language Models (LVLMs). Most of them adapt LVLMs to specific medical applications using specialized medical datasets. For instance, MedFlamingo [54] enhances OpenFlamingo-9B using 0.8 million interleaved and 1.6 million paired medical image-text data, highlighting the critical need for multimodal data in medical image analysis and automated report generation tasks. RadFM [70] improves PMC-LLaMA [69] by leveraging 16 million radiology images with text descriptions from diverse sources. Similarly, Med-PaLM [68] adapts PaLM-E [25] to the medical domain with approximately one million medical data samples, achieving state-of-theart performance in diagnostic support and medical knowledge Q&A. LLaVA-Med [39] utilizes large-scale biomedical figure-caption dataset extracted from PubMed Central to enhance LLaVA [66, 67] to better understand biomedical images and facilitate open-ended conversational interactions. Med-Gemini [58] leverages long-format questionanswering datasets to improve the multimodal and longcontextual capabilities of the baseline Gemini model, enabling superior performance in complex medical Q&A and multimodal reasoning tasks. Additionally, HuatuoGPTVision [14] and MedDr [28] build medical multimodal datasets to adapt general-purpose LVLMs like LLaVA and InternVL to various medical modalities, including radiology, pathology, dermatology, and endoscopy. Previous studies usually focus on constructing medical datasets to adapt general-purpose LVLMs but pay less attention to the adaptation strategies. However, naive training/adaptation strategies may not successfully adapt general-purpose LVLMs to the medical data, due to the large gap between the natural image-text pairs and the medical ones. Moreover, these strategies can hardly align the broad imaging modalities and various types of medical text (e.g., prescriptions, radiology reports, and electronic health records) to obtain generalizable features, thus limiting the models performance. Our work thus proposes novel three-stage training strategy to better integrate the visual and language features to enhance generalization ability. 3 3. GMAI-VL-5.5M: Comprehensive Multimodal Dataset With rapid advancements in medical vision-language models (VLMs), constructing high-quality datasets is crucial for developing general-purpose models. Unlike previous approaches that primarily rely on published literature, our method utilizes specialized medical datasets to create more robust, high-quality resource. We build GMAI-VL5.5M, comprehensive medical vision-language dataset that aggregates data from diverse open-source and proprietary sources. Covering 13 medical imaging modalities and 18 specialties, it addresses wide range of medical imaging tasks. This dataset enhances the models ability to process complex medical information, advancing precision medicine and intelligent diagnostics. 3.1. Data Curation Data collection. To construct comprehensive multimodal medical dataset, we sourced 219 datasets from diverse platforms. Fig. 1(a) highlights key data sources, such as Kaggle, Grand Challenge, and Huggingface, which facilitate extensive data collection. These datasets cover diverse imaging modalities, including fundus, CT, MRI, and ultrasound (US), and span variety of medical tasks, such as diagnosis, severity assessment, and organ recognition. They also encompass multiple clinical specialties, including pathology, dermatology, ophthalmology, otolaryngology, and oncology, further enhancing their diversity. Data preprocess. After data collection, we apply preprocessing workflow to extract 2D medical images from the videos and 3D medical volumes. For data cleaning, we first extract key annotation information from classification data, including modality, department, and labels for each image, discarding instances with missing or unclear annotations. For segmentation data, we follow the SAMed2D-20M [75] preprocessing method, filtering out lowquality images and labels, and converting them into detection datasets. The converted segmentation data is then processed together with the original detection dataset, with key annotation information extracted and linked to each image. Finally, the preprocessed data are then standardized and organized into structured format: <image, modality, label, department, bbox [optional]>. Data generation. Subsequently, the data are categorized into two primary types: classification datasets and detection/segmentation datasets. Each category is further refined using specific prompts tailored for large model training. For data generation, large vision-language models (GPT-4o) are employed to produce detailed image descriptions and corresponding instruction-following data through annotation-guided data generation methodology. For classification datasets, detailed descriptions of the entire imFigure 2. The prompt-driven data generation pipeline comparing without-annotation-guided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g., <image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the without-annotationguided approach often results in lower-quality outputs. Figure with complete prompt and response is provided in Supp. Mat.. age are generated, while for detection datasets, the focus is on specific regions enclosed by bounding boxes, providing comprehensive functional analyses of these areas. Notably, the segmentation dataset was transformed into detection dataset using external bounding boxes, and data generation followed detection dataset protocols. Furthermore, to improve the models multilingual capability, we translated portion of English image-text data into Chinese. Incorporating multilingual data helps to enhance the generalization capabilities of domain-specific multimodal models. The resulted data is utilized for medical Visual Question Answering (VQA) tasks, forming the comprehensive VQA dataset, named GMAI-VL-5.5M. The detailed pipeline for generating prompt-driven data is illustrated in Fig. 2. As depicted in Fig. 1(a), it contains six kinds of instrcuction-following formats, including image-level captions, region-level captions, free instructions, dialogue, visual perception and text-only tasks. The specific composition of GMAI-VL-5.5M can be found in Supp. Mat.  (Table 6)  . These formats enable VLMs to better understand and process complex visual and textual information in medical contexts. The GMAI-VL-5.5M dataset significantly en4 Table 1. Comparison of various medical multimodal datasets, including details on the dataset size, modality type, language, data traceability, and sources of information. Datasets Data Size Modality Language Traceability Data Source PathVQA [29] MIMIC-CXR [33] quilt-1M [32] MedDr VQA [28] PMC-OA [43] PMC-VQA [80] LLaVA-Med VQA [39] ChiMed-VL [48] PMC-CaseReport [70] PubMedVision [14] 32.7k 227k 1M 197k 1.65M 413k 56,702 1.05M 438k 1.29M Pathology X-Ray Pathology Multimodal Multimodal Multimodal Multimodal Multimodal Multimodal Multimodal EN EN EN EN EN EN EN CN EN EN&CN GMAI-VL-5.5M (ours) 5.5M Multimodal EN&CN Textbooks Hospital YouTube & PubMed 13 medical datasets PubMed PubMed PubMed PubMed PubMed PubMed 219 specialized medical imaging datasets hances the models cross-modal reasoning ability, enabling it to handle complex multimodal inputs in real clinical scenarios. The richness of the instruction formats allow the model to progress from basic question answering to advanced medical image analysis, ultimately providing strong support for clinical diagnosis and decision-making. 3.2. Data Property Data statistics. These datasets encompass diverse medical imaging tasks and modalities, forming solid foundation for developing and evaluating medical LVLMs. Fig. 4 (in Supp. Mat.) illustrates the distribution of modalities, tasks, clinical departments, and specific medical challenges represented within the collected datasets. This visualization highlights the extensive diversity and coverage of our data collection efforts. After careful standardization and integration, these datasets form the core of our comprehensive medical image-text dataset, GMAI-VL-5.5M, which serves as crucial resource for advancing precision medicine and intelligent diagnostic systems. Accuracy and reliability of generated data. The accuracy and reliability of the generated data are primarthe reliaily addressed from two key aspects. bility of the data source is ensured by utilizing highquality datasets from reputable sources. The data used for generation primarily comes from professional medical challenges (such as Kaggle and GrandChallenge) and publicly available, peer-reviewed datasets. This guarantees the integrity and accuracy of the data at its source. Second, the data generation process is rigorously controlled to ensure accuracy and reliability. Although GPT was used for data generation, prompts were meticulously crafted to include essential annotations, minimizing errors and hallucinations. As illustrated in Fig. 2, the proposed annotation-guided approach incorporates specific annotations (e.g., <image, modality, label, department, bbox [optional]>) to produce precise, high-quality descriptions. Compared to the withoutFirst, annotation-guided approach, which often results in less detailed outputs, the annotation-guided method delivers more professional, pathology-specific descriptions, significantly enhancing the reliability of the generated data. Compared with other medical multimodal datasets. The GMAI-VL-5.5M dataset, as highlighted in Table 1, stands out due to its unmatched scale, encompassing over 5.5 million samples from more than 219 specialized medical imaging datasets. Unlike other datasets listed, GMAIVL-5.5M supports wider variety of modalities and languages, making it truly global resource that caters to diverse clinical needs. Additionally, GMAI-VL-5.5M emphasizes traceability of its data, ensuring high standard of clinical relevance and reliability. This comprehensive and diverse dataset is critical for pushing the boundaries of medical multimodal research, enabling more effective training of LVLMs that can generalize across multiple medical tasks and scenarios, thereby driving innovations in precision medicine and intelligent diagnostics. 4. GMAI-VL: General Medical Vision-"
        },
        {
            "title": "Language Model",
            "content": "4.1. Architecture The GMAI-VL model is vision-language model built upon the LLaVA architecture [39, 45], incorporating three key components: large language model (LLM), vision encoder, and projector (MLP), as illustrated in Fig. 1(b). These components are designed to work together seamlessly, enabling the model to deliver exceptional performance in medical applications. We adopt InternLM2.5-7B [63] as our language processing module, which offers outstanding reasoning capabilities. With context window up to one million tokens, it can handle complex medical tasks and generate coherent, accurate responses. Its support for advanced instructionfollowing makes it particularly effective in addressing intri5 cate medical queries, thereby enhancing the models ability to understand and respond to wide range of instructions. For vision processing, GMAI-VL employs CLIP-based vision encoder [55], which transforms visual inputs into high-dimensional feature representations. CLIPs strong performance in aligning image and text representations ensures that medical image features are accurately extracted and effectively integrated with linguistic information, significantly enhancing the models ability to handle multimodal medical data. The MLP, as projector, serves as bridge between the vision encoder and the LLM, optimizing high-dimensional outputs and further enhancing feature representation. The seamless integration of these components enables GMAIVL to excel in processing and understanding complex medical multimodal data. 4.2. Optimization Strategy As illustrated in Fig. 1(c), the training process of the GMAIVL model is divided into three stages: shadow alignment, deep alignment, instruction tuning, respectively. The detailed hyper-parameter settings can be found in Supp. Mat.  (Table 8)  . To enhance the training of GMAI-VL, we supplement our GMAI-VL-5.5M dataset with additional medical datasets, ultimately forming an 11.7M dataset for training. This supplemented data increases the diversity of training data, exposing the model to wider range of medical scenarios and visual-language patterns, enhancing its generalization to complex clinical tasks and ensuring robustness in real-world applications. Fig. 5 (in Supp. Mat.) provides complete distribution of the utilized datasets during the training stage. The detailed data proportions for each training stage are detailed in Table 7 (in Supp. Mat.). Stage I: Shadow alignment. In the shallow alignment phase, we utilize large-scale medical image-text dataset comprising approximately 11.7 million image-text pairs, sourced from combination of publicly accessible datasets and proprietary in-house data. To achieve shallow alignment, we freeze both the large language model and the vision encoder, optimizing only the projector. With this optimization stage, the model establishes an initial alignment between medical images and their corresponding textual descriptions. All input images are resized to 336 336 pixels, and the training objective is to minimize the cross-entropy loss of the text tokens. Stage II: Deep alignment. Most vision encoders in multimodal models are pre-trained on natural images, so it is essential to address the domain gap between medical and natural images during the deep alignment stage. To achieve this, we fine-tune both the vision-language projector and the vision encoder, enhancing the alignment between the visual features of medical images and the language models feature space. Stage III: Instruction tuning. At this stage, we fine-tune our GMAI-VL model (including the vision encoder, the language model, and the projector components) by instruction tuning to enhance its instruction-following and dialogue capabilities. The multimodal instruction tuning data is primarily derived from the training data in previous stages, with filtering process to select high-quality and more suitable data for fine-tuning. Specifically, we manually reviewed subset of each dataset and labeled it as either high quality or low quality. High-quality datasets were used for tuning, while most low-quality ones were excluded, except for small portion retained for diversity. This approach allowed efficient filtering without reviewing every sample. Additionally, we incorporate medical text dialogue data to ensure the models versatility in handling various dialogue scenarios. Thus, our instruction tuning data comprises approximately ten million samples. 5. Experimental Results To evaluate our model, we employed several established multimodal medical benchmarks, each targeting specific aspects of medical image understanding and question answering. Below is brief overview of the benchmarks used in our experiments: Traditional medical VQA benchmarks: Traditional multimodal medical question-answering benchmarks, such as VQA-RAD [38], SLAKE [44], and PMCVQA [80], span various imaging modalities tasks, primarily assessing the models ability to extract information from medical images and answer clinical questions. They evaluate the models performance in understanding medical imaging and integrating multimodal information. OmniMedVQA: OmniMedVQA [31] provides rich dataset of paired medical images and text, designed to evaluate the models ability to recognize and understand fundamental medical imaging concepts, focusing on cross-modal reasoning and information integration. GMAI-MMBench: GMAI-MMBench [16] focuses on assessing the models ability to identify fine-grained objects in complex clinical scenarios, challenging its capacity to handle long-context tasks and accurately recognize and reason over detailed medical features. MMMU Health & Medicine track: The Health & Medicine track of the MMMU [78] benchmark spans wide range of medical fields, derived from university exams, quizzes, and textbooks. It evaluates the models reasoning ability in complex medical scenarios and the specialized knowledge in health and medicine. 5.1. Medical VQA Benchmarks The performance of various VLMs on popular medical VQA benchmark datasets is summarized in Table 2, inTable 2. Results on Traditional Medical VQA Benchmarks. The highest performance in each column is highlighted in red, and the second-highest performance is highlighted in blue. Model VQA-RAD SLAKE PMC-VQA Avg. Med-Flamingo [54] RadFM [70] LLAVA-Med-7B [39] Qwen-VL-Chat [6] Yi-VL-34B [77] LLAVA-v1.6-7B [46] LLAVA-v1.6-13B [46] LLAVA-v1.6-34B [46] HuatuoGPT-Vision-7B [14] GMAI-VL(w/o our data) GMAI-VL(ours) 45.4 50.6 51.4 47.0 53.0 52.6 55.8 58.6 63. 62.3 66.3 43.5 34.6 48.6 56.0 58.9 57.9 58.9 67.3 74.5 66.3 72.9 23.3 25.9 24.7 36.6 39.5 35.5 36.6 44.4 52.7 39.0 54.3 37.4 37.0 41.6 46.5 50.5 48.7 50.8 56.8 63. 55.9 64.5 cluding VQA-RAD[38], SLAKE[44], and PMC-VQA[80]. Our model, GMAI-VL, demonstrates strong performance, achieving the highest score on the VQA-RAD[38] dataset with 66.3%, outperforming other models such as HuatuoGPT-Vision-7B. This result highlights GMAI-VLs superior capability in handling radiological image questionanswering tasks. For the PMC-VQA[80] dataset, GMAIVL achieves 54.3%, and 72.9% on SLAKE[44], demonstrating its capability in handling medical VQA tasks across diverse modalities. GMAI-VL demonstrates performance across multiple benchmarks, showcasing its versatility in medical image understanding and question-answering. competitive Table 3. Comparison of performance between representative LVLMs and GMAI-VL on OmniMedVQA across five different question type. The best performance in each column is highlighted in red, and the second-best performance is highlighted in blue. Abbreviations: MR = Modality Recognition, AI = Anatomy Identification, DD = Disease Diagnosis, LG = Lesion Grading, OBA = Other Biological Attributes. AI MR DD 25.00 25.84 28.41 25.40 37.49 LG OBA Overall 28.28 Model Random Guess MiniGPT-4 [81] LLaVA [45] LLaMA Adapter v2 [27] InstructBLIP [20] BLIP-2 [40] Qwen-VL-Chat [6] mPLUG-Owl2 [76] LLaVa-NeXT [46] DeepSeek-VL [49] Yi-VL [77] InternVL2-40B [18] Open-Source LVLMs 36.98 32.68 24.19 20.45 26.14 52.30 35.27 11.80 9.77 24.70 58.45 38.18 29.12 23.73 30.97 72.35 39.90 32.01 43.80 47.91 57.48 49.83 46.21 30.52 73.52 33.69 10.95 16.27 6.71 41.68 78.01 48.52 39.68 20.56 59.36 68.23 46.74 41.21 18.43 39.57 74.01 51.94 45.46 21.06 29.04 59.56 44.81 48.97 32.93 24.63 96.76 64.25 76.28 76.50 76.27 Medical Special Model MedVInT-TE [80] 62.62 41.03 40.57 12.17 45.17 LLaVA-Med [39] 48.41 27.96 23.72 16.10 21.94 Med-Flamingo [54] 26.74 25.10 23.80 28.04 16.26 RadFM [70] 27.45 21.65 23.75 16.94 20.05 91.37 51.62 65.56 73.18 74.52 MedDr [28] HuatuoGPT-Vision-34B [14] 95.06 75.67 66.51 72.83 74.92 Our Model GMAI-VL(w/o our data) GMAI-VL(ours) 96.40 80.97 79.14 70.29 75.66 98.64 92.95 88.7 87.21 82. 79.96 88.48 27.59 22.86 35.08 41.14 50.77 20.29 48.44 45.57 48.76 47.28 78.70 43.83 27.82 23.82 23.48 68.27 73.23 5.2. OmniMedVQA broader applications in medical question answering. Table 3 summarizes the performance of various large vision-language models (LVLMs), including our proposed GMAI-VL, across five question types: Modality Recognition, Anatomy Identification, Disease Diagnosis, Lesion Grading, and Other Biological Attributes. GMAI-VL demonstrates outstanding accuracy across multiple tasks, achieving 98.64% in Modality Recognition, 92.95% in Anatomy Identification, and 88.71% in Disease Diagnosis. It outperforms both open-source LVLMs and medicalspecific models, underscoring its capability to accurately identify anatomical structures and diagnose diseases from visual data. In Lesion Grading, GMAI-VL attained the highest score of 87.21%, and it also delivers strong performance of 82.95% in Other Biological Attributes, showcasing its versatility across diverse biological contexts. With an average accuracy of 88.48%, the highest among all evaluated models, GMAI-VL excels not only in general medical question-answer tasks but also in complex reasoning requiring domain-specific knowledge, surpassing models like HuatuoGPT-Vision-34B and InternVL2-40B. These results verify our GMAI-VL is leading model in multimodal medical image understanding, setting new benchmark for medical VQA tasks. Its consistent top performance across question types highlights its potential for 5.3. GMAI-MMBench The GMAI-MMBench benchmark is comprehensive medical multimodal benchmark designed to evaluate models on range of clinical visual question-answering (VQA) tasks. Table 4 presents the results of various LVLMs, including open-source LVLMs and commercial models, evaluated on the val and test sets across multiple clinical tasks. Notably, GMAI-VL excels in specific tasks such as abnormality recognition (73.78%), biological variation recognition (63.06%), and clinical disease diagnosis (66.67%). These results demonstrates the models strong ability in understanding and interpreting complex clinical images. In comparison to other models, GMAI-VL consistently achieves either the best or second-best performance across most tasks. For instance, it ranks first in 16 out of 20 categories, including key tasks such as AR (Attribute Recognition) and DD (Disease Diagnosis), where it achieved scores of 75.26% and 67.14%, respectively, suggesting GMAIVLs strength in understanding medical scenarios. Overall, GMAI-VL establishes new benchmark in various clinical VQA tasks, demonstrating its potential as reliable and versatile tool in medical multimodal applications. Table 4. Results on the val and test sets of GMAI-MMBench for clinical VQA tasks. The full names of the evaluated tasks can be found in Table 5 in literature [16]. The best model in each category is highlighted in red, while the second-best model is indicated in blue. Model Name Overall (val) Overall (test) AR BVR CR DD IQG MR NT OR-A OR-HN OR-P OR-T SG SAR SIR SWR Random 25.70 25.94 38.20 22.73 22.92 22.72 24.06 26.66 27.13 27.00 20.00 24.75 21.37 22.93 22.33 21.18 32.43 24.23 21.39 23. Random Guess Flamingo v2 [4] VisualGLM-6B [22] InstructBLIP-7B [20] Qwen-VL [6] Yi-VL-6B [77] ShareGPT4V-7B [15] LLAVA-V1.5-7B [45] XComposer2 [24] LLAVA-InternLM-7b [19] InternVL-Chat-V1.5 [18] InternVL-Chat-V1.2 [17] LLAVA-InternLM2-7b [19] DeepSeek-VL-7B [49] MiniCPM-V2 [73] Claude3-Opus [2] Qwen-VL-Max [6] GPT-4V [1] Gemini 1.0 [62] Gemini 1.5 [56] GPT-4o [1] Med-Flamingo [54] LLaVA-Med [39] Qilin-Med-VL-Chat [48] RadFM [70] MedDr [28] GMAI-VL(w/o our data) GMAI-VL(ours) 25.58 29.58 31.80 34.80 34.82 36.71 38.23 38.68 38.71 38.86 39.52 40.07 41.73 41. 32.37 41.34 42.50 44.38 47.42 53.53 12.74 20.54 22.34 22.95 41.95 54.99 61.74 Open-Source LVLMs 37.74 21.50 20.62 22.00 22.41 27.29 25.91 27.45 18.00 28.79 25.16 40.16 33.92 24.92 25.22 24.21 32.99 29.96 29.53 21.20 37.88 30.32 42.12 26.92 24.92 28.09 21.65 34.58 31.58 29.23 22.40 30.30 28.95 37.05 37.24 35.85 28.98 24.81 43.60 24.70 30.12 19.20 44.44 29.68 41.66 39.16 26.62 30.23 31.88 38.01 26.72 24.93 25.20 37.37 29.58 43.96 37.59 21.54 37.57 18.80 43.26 32.39 27.30 22.80 43.43 29.47 45.45 34.27 30.92 41.32 21.65 44.68 34.01 27.74 23.60 43.43 28.00 41.89 37.59 33.69 40.79 22.26 45.87 36.44 32.94 27.20 58.59 26.11 36.36 36.54 32.62 38.10 30.68 46.53 34.82 28.19 25.20 48.99 28.11 43.84 44.58 34.00 33.99 31.28 45.59 33.20 38.28 32.40 42.42 31.89 41.66 44.06 27.38 38.46 34.29 46.99 33.60 34.42 21.20 47.98 30.63 39.82 37.94 30.62 35.24 29.77 48.97 34.01 25.96 20.80 53.03 30.95 38.43 47.03 42.31 37.03 26.47 51.11 33.20 31.16 26.00 44.95 36.00 40.74 43.01 36.46 37.57 27.82 51.08 28.74 29.08 26.80 47.47 37.05 Proprietary LVLMs 1.61 39.51 34.31 31.66 12.63 39.26 28.74 30.86 22.40 37.37 25.79 32.68 44.58 31.38 40.79 10.68 50.53 32.79 44.36 29.20 51.52 41.37 29.92 48.95 44.00 37.39 12.93 52.88 32.79 44.21 32.80 63.64 39.89 42.12 45.10 46.46 37.57 20.45 53.29 35.22 36.94 25.20 51.01 34.74 55.33 38.87 48.07 30.00 76.26 51.05 43.50 38.32 61.01 57.08 49.02 46.62 61.45 46.56 56.38 34.00 75.25 53. 56.12 51.23 47.58 2.26 Medical Special Model 13.43 12.15 6.62 9.23 10.14 11. 6.67 9.26 24.51 17.83 17.08 19.86 15.04 19.81 20.24 21.51 13.20 15.15 20.42 29.57 19.41 16.46 23.79 15.79 24.19 21.86 16.62 13.64 24.00 27.16 20.63 13.23 19.14 20.45 24.51 23.48 22.85 15.60 16.16 14.32 41.20 50.70 37.85 29.87 28.27 52.53 36.03 31.45 29.60 47.47 33.37 18.18 6.38 7.20 8.00 26.34 30.45 30.95 36.05 34.31 36.70 37.96 39.20 39.11 39.73 40.01 40.45 43.43 42. 32.44 42.16 44.08 44.93 48.36 53.96 11.64 19.60 22.06 22.93 43.69 22.13 24.80 27.47 31.87 31.20 37.33 42.13 36.40 40.53 42.80 42.80 42.67 58.13 46.40 41.07 58.00 54.13 59.60 75.87 69.47 18.27 23.73 14.67 24.93 51.33 22.00 22.00 34.61 22.88 20.44 27.43 13.33 29.88 33.11 19.62 19.16 37.43 23.00 24.82 32.88 19.81 21.64 26.57 25.00 31.18 30.26 21.54 20.10 26.86 32.33 30.59 36.71 24.81 23.18 31.43 22.00 31.76 34.98 24.42 25.06 30.00 29.00 35.06 33.41 22.12 23.61 29.14 43.67 37.29 32.06 23.46 27.80 32.86 33.33 36.00 34.08 26.73 24.12 29.71 27.00 36.82 34.76 23.27 24.72 32.57 27.67 35.88 35.59 23.85 24.98 28.00 32.00 39.88 32.43 21.73 24.38 38.00 36.33 47.29 34.91 18.08 25.49 39.43 25.33 46.59 35.89 22.31 23.44 31. 29.33 33.18 31.31 21.35 23.87 4.00 30.67 41.65 26.95 25.00 24.64 39.14 37.00 50.59 27.55 23.08 25.75 37.43 34.00 50.00 36.64 23.65 23.87 35.43 46.33 62.24 20.57 27.69 30.54 40.57 48.67 65.88 33.93 22.88 29.51 39.43 8.47 5.19 11.00 11.53 12.16 11.43 17.67 19.65 21.70 19.81 14.11 20.86 12.67 15.53 26.13 24.42 17.37 25.71 17.33 21.53 29.73 17.12 19.59 31.14 32.67 44.47 35.14 25.19 25.58 32.29 56.23 62.43 51.26 61.05 53.79 44.39 44.51 62.60 40.80 57.42 35.20 79.50 61.31 75.26 59.66 67.24 56.86 54.29 67.14 42.80 79.97 41.60 75.00 60. 77.81 75.48 53.60 69.29 35.39 35.77 29.71 44.86 53.33 58.12 42.09 72.31 37.40 59.14 Our Model 5.4. MMMU Health & Medicine track The MMMU benchmark, widely recognized standard for evaluating multimodal models, was used to assess the performance of our proposed GMAI-VL model on the Health & Medicine track. The experimental results in Table 5 show the models performance across five key categories: Basic Medical Science (BMS), Clinical Medicine (CM), Diagnostics and Laboratory Medicine (DLM), Pharmacy (P), and Public Health (PH). GMAI-VL performs strongly across multiple categories, achieving top scores in DLM (43.3%), (50.0%), and PH (53.3%), surpassing competitive models like LLaVA-v1.6 and HuatuoGPT-Vision-7B. These results highlight the models proficiency in handling complex tasks requiring diagnostic reasoning, pharmaceuIn BMS, tical knowledge, and public health expertise. GMAI-VL scores 50.0%, achieve the best performance, demonstrating the models the capacity of understanding medical knowledge. In CM, the model achieves 60.0%, remaining competitive with other leading models. These results underscore the models ability in processing both clinical and foundational medical information effectively. Overall, GMAI-VL achieves an average score of 51.3% across the Health & Medicine track, which is top performance among other models, verifying its versatility in specialized medical domains. Table 5. Performance on the val set for the MMMU Health & Medicine track. This track is divided into five categories: BMS (Basic Medical Science), CM (Clinical Medicine), DLM (Diagnostics and Laboratory Medicine), (Pharmacy), and PH (Public Health). The best performance in each column is highlighted in red, and the second-best performance is highlighted in blue. Model BMS CM DLM PH MMMU Health & Medicine 33.6 30.2 23.3 29.3 25.8 Med-Flamingo [54] RadFM [70] 31.6 28.6 26.7 26.2 26.8 33.8 32.3 26.7 40.7 43.3 LLaVA-Med-7B [39] 32.7 20.6 19.3 29.6 33.3 Qwen-VL-Chat [6] 48.1 55.6 36.7 35.4 31.3 Yi-VL-34B [77] 46.4 43.4 30.0 29.6 26.7 LLaVA-v1.6-7B [45] LLaVA-v1.6-13B [45] 53.6 46.7 33.3 22.2 40.0 HuatouGPT-Vision-7B [14] 50.0 63.3 36.7 48.1 53.3 GMAI-VL(w/o our data) GMAI-VL(ours) 43.3 56.7 43.3 46.7 40.0 50.0 60.0 43.3 50.0 53.3 28.4 27.9 38.6 31.7 48.2 33.1 39.3 50.3 46.0 51.3 5.5. Ablation Study on GMAI-VL-5.5M Dataset We further analyzed the effectiveness of the proposed GMAI-VL-5.5M dataset. By removing GMAI-VL-5.5M from the training set, we observed significant performance drop, as shown in Table 2, Table 3, Table 4 and Table 5. In the VQA-RAD, SLAKE, and PMC-VQA benchmarks, removing GMAI-VL resulted in an average performance improvement of 8.6%. For the OmniMedVQA, GMAI-MMBench, and MMMU Health & Medicine track, the improvements were 8.52%, 6.75%, and 5.3%, respectively. These results demonstrate that the GMAI-VL5.5M dataset provides highly accurate and reliable medical knowledge, particularly in recognizing and understanding multimodal medical data, which significantly boosts model performance. This underscores the datasets diversity, comprehensiveness, and its ability to effectively complement other datasets in complex medical tasks. 6. Conclusion In this study, we develop GMAI-VL, large visionlanguage model, along with GMAI-VL-5.5M, comprehensive multimodal medical dataset aimed at advancing (GMAI). GMAI-VL-5.5M, which general medical AI image analysis datasets converts hundreds of medical into high-quality image-text pairs through annotationguided data generation, enables GMAI-VL to effectively address wide range of clinical Experimental results show that GMAI-VL-5.5M significantly enhances GMAI-VLs performance on diverse clinical tasks, achieving state-of-the-art results across several key benchmarks. tasks."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 8 [2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. 8 [3] American Society of Retina Specialists ASRS. Home - Retina Image Bank, 2024. Accessed: 2024-09-11. 5 [4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. [5] Azure99. Blossom orca v3. https://huggingface. co/datasets/Azure99/blossom-orca-v3, 2024. 5 [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 7, 8 [7] Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, et al. Coig-cqia: Quality is all you need for chinese instruction fine-tuning. arXiv preprint arXiv:2403.18058, 2024. 5 [8] Asma Ben Abacha, Sadid Hasan, Vivek Datla, Dina Demner-Fushman, and Henning Muller. Vqa-med: Overview of the medical visual question answering task at imageclef 2019. In Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes. 9-12 September 2019, 2019. 5 [9] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia, 2024. [10] Ray Bernard. Leetcode dataset, 2023. 5 [11] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pages 15111520, 2022. 5 [12] Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang, Zhihong Chen, Maya Varma, Steven QH Truong, Chu The Chuong, and Curtis Langlotz. Chexpert plus: Hundreds of thousands of aligned radiology texts, images and patients. arXiv preprint arXiv:2405.19538, 2024. 2, 5 [13] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 5 [14] Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. 2, 3, 5, 7, 8 [15] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 8, 5 [16] Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, et al. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. arXiv preprint arXiv:2408.03361, 2024. 6, [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 8 [18] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 7, 8 [19] XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/ xtuner, 2023. 8 [20] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. 7, 8 [21] Dina Demner-Fushman, Marc Kohli, Marc Rosenman, Sonya Shooshan, Laritza Rodriguez, Sameer Antani, George Thoma, and Clement McDonald. Preparing collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304310, 2016. 5 [22] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 8 [23] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling highquality instructional conversations, 2023. 5 [24] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form textimage composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 8 [25] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 3 [26] GAIR. Lima: Less is more for alignment, 2023. 5 [27] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 7 [28] Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. MedDr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024. 1, 3, 5, 7, [29] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. 5 [30] Xinyue Hu, Gu, An, Zhang, Liu, Kobayashi, Harada, Summers, and Zhu. Medical-diff-vqa: largescale medical dataset for difference visual question answering on chest x-ray images, 2023. 5 [31] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2217022183, 2024. 6 [32] Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. Advances in neural information processing systems, 36, 2024. 5 [33] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Roger Mark, and Steven Horng. Mimic-cxr, deidentified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. 2, 5 [34] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. [35] Manav Nitin Kapadnis, Sohan Patnaik, Abhilash Nandy, Sourjyadip Ray, Pawan Goyal, and Debdoot Sheet. Serpentvlm: Self-refining radiology report generation using vision language models. arXiv preprint arXiv:2404.17912, 2024. 1 [36] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 5 [37] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498517. Springer, 2022. 5 [38] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 6, 7, 5 [39] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 5, 7, 8 [40] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 7 [41] Jiajia Li, Zhouyu Guan, Jing Wang, Carol Cheung, Yingfeng Zheng, Lee-Ling Lim, Cynthia Ciwei Lim, Paisan Ruamviboonsuk, Rajiv Raman, Leonor Corsino, et al. Integrated image-based deep learning and language models for primary diabetes care. Nature Medicine, pages 111, 2024. [42] Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Weiyun Wang, Zhe Chen, et al. Seeing and understanding: Bridging vision with chemical knowledge via chemvlm. arXiv preprint arXiv:2408.07246, 2024. 5 [43] Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 525536. Springer, 2023. 5 10 [44] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 16501654. IEEE, 2021. 6, 7, 5 [45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 5, 7, 8 [46] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 7 [47] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. [48] Junling Liu, Ziming Wang, Qichen Ye, Dading Chong, Peilin Zhou, and Yining Hua. Qilin-med-vl: Towards chinese large vision-language model for general healthcare. arXiv preprint arXiv:2310.17956, 2023. 3, 5, 8 [49] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 7, 8 [50] Ming Lu, Bowen Chen, Drew FK Williamson, Richard Chen, Igor Ivy Liang, Tong Ding, Guillaume Jaume, Odintsov, Long Phi Le, Georg Gerber, et al. visuallanguage foundation model for computational pathology. Nature Medicine, 30(3):863874, 2024. 2 [51] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 5 [52] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 5 [53] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [54] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. 2, 3, 7, 8 [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [56] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 8 [57] Johannes Ruckert, Louise Bloch, Raphael Brungel, Ahmad Idrissi-Yaghir, Henning Schafer, Cynthia Schmidt, Sven Koitka, Obioma Pelka, Asma Ben Abacha, Alba G. Seco de Herrera, et al. Rocov2: Radiology objects in context version 2, an updated multimodal image dataset. Scientific Data, 11 (1):688, 2024. 5 [58] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416, 2024. 3 [59] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620 (7972):172180, 2023. [60] Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, and Hannaneh Hajishirzi. Medicat: dataset of medical images, captions, and textual references. arXiv preprint arXiv:2010.06000, 2020. 5 [61] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Stanford alpaca: An instruction-following Hashimoto. llama model. https://github.com/tatsulab/ stanford_alpaca, 2023. 5 [62] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 8 [63] InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. 5 [64] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. 5 [65] Omkar Chakradhar Thawakar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Khan. Xraygpt: Chest radiographs summarization using large medical vision-language models. In Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, pages 440448, 2024. 1 [66] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3 [67] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [68] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. NEJM AI, 1(3):AIoa2300138, 2024. 3 [69] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2(5):6, 2023. 3 [70] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023. 2, 3, 5, 7, 8 [71] Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, et al. Medtrinity-25m: large-scale multimodal dataset with multigranular annotations for medicine. arXiv preprint arXiv:2408.02900, 2024. 3 [72] Dong Xue* Xin Yan. Mindchat: Psychological large language model. https://github.com/XDLab/ MindChat, 2023. 1 [73] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. LLaVA-UHD: an lmm perceiving any aspect ratio and highresolution images. arXiv preprint arXiv:2403.11703, 2024. 8 [74] Jianxin Yang. Firefly( ):          . https://github.com/yangjianxin1/Firefly, 2023. [75] Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. arXiv preprint arXiv:2311.11969, 2023. 4 [76] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language arXiv preprint model with modality collaboration. arXiv:2311.04257, 2023. 7 [77] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 7, 8 [78] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 6 [79] Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian Davison, Hui Ren, et al. generalist visionlanguage foundation model for diverse biomedical tasks. Nature Medicine, pages 113, 2024. 3 [80] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. 5, 6, [81] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 7 12 GMAI-VL & GMAI-VL-5.5M: Large Vision-Language Model and Comprehensive Multimodal Dataset Towards General Medical AI"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Expanded Data Generation Example In this section, we present the full version of Fig. 2, which provides detailed example of the data generation process, as shown in Fig.3. This expanded illustration includes the complete prompt and the corresponding detailed responses, enabling clear comparison between data generation methodologies with and without annotation guidance. The detailed comparison allows for more intuitive understanding of the distinctions between both methods. 8. Details of GMAI-VL-5.5M In Table. 6, we provide sub-datasets information of the multimodal dataset GMAI-VL-5.5M we have constructed. Based on the different data forms introduced in the paper, we have categorized the data into five distinct sub-datasets. These include GMAI-MM-Caption-1.7M, GMAI-MM-Percept-1.3M, GMAI-MM-Instruct-0.9M, and GMAI-Text-0.6M. Each GMAI-Text-Single-1M, image sub-dataset corresponds to specific components: caption data, free instruction data, visual perception data, text-only, and conversation. Furthermore, Fig. 4 presents comprehensive distribution of the data across different modalities, original tasks, departments, and clinical task within this multimodal dataset, highlighting its extensive richness and diversity. 9. Training Data Overview In this section, we provide comprehensive overview of all datasets utilized for training the GMAI-VL model. The details include the dataset names, their corresponding categories, the amount of data used for training, and the proportion of training data allocated to each dataset during the three phases of model training. Fig. 5 visualizes the distribution of all our training data, showing the proportion of each category and subcategory. Table. 7 summarizes the datasets employed, highlighting their respective categories and sizes. It is important to note that for certain datasets, we performed data cleaning and bilingual translation. As result, the dataset sizes reported here may differ from the official numbers. 10. Model Training Settings The table. 8 presents the training settings for GMAI-VL across three stages, detailing key hyperparameters. The train diagram as illustrated in Fig. 6. Stage (Shallow alignment). During this stage, both the large language model (LLM) and the vision encoder are frozen, while the MLP remains trainable. The learning rate is set to 1e3 with cosine decay schedule, using AdamW as the optimizer. Input size is 336 336, and the total batch size is 32 8 2. Here, 32 refers to the number of GPUs used, 8 represents the micro-batch size per GPU, and 2 is the number of gradient accumulation steps. Notably, we use soft packing technique, which allows each sample to actually contain multiple sequencesestimated at over 2 per sample. Stage II (Deep alignment). The LLM remains frozen, while the vision encoder and MLP are unfrozen and trainable, allowing for deeper feature alignment. The learning rate is lowered to 1e4, the total batch size is 32 4 4, and other settings remain consistent with Stage I. Stage III (Instruction tuning). In this stage, all components of the model are trainable, allowing for comprehensive fine-tuning. The learning rate is further reduced to 1e5, the total batch size is 32 4 4, while other parameters, including optimizer, remain unchanged across stages. Each stage utilizes DeepSpeed for mixed-precision training (bf16) on 32 A100 (80GB) GPUs. To accelerate the training rate of the model, we utilize the soft packing method in xtuner*, which packs multiple sequences into each sample to optimize memory usage and efficiency.. 11. Results Fig. 7 illustrates several examples of our model on various tasks including image description, disease diagnosis, diagnostic report generation, free question answering, etc. These examples cover diverse range of medical imaging modalities and tasks, highlighting the models versatility and general capabilities within medical domain. Furthermore, GMAI-VL supports both Chinese and English, thereby facilitating bilingual functionality and expanding its applicability in diverse clinical settings. *https://github.com/InternLM/xtuner 1 Figure 3. The full version of Fig. 2 in the main text illustrates the complete of data generation pipeline comparing without-annotationguided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g., <image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the withoutannotation-guided approach often results in lower-quality outputs."
        },
        {
            "title": "Dataset",
            "content": "Sub-Dataset Name"
        },
        {
            "title": "Description",
            "content": "Table 6. Sub-Dataset Details for GMAI-VL-5.5M GMAI-MM-Caption-1.7M curated set of detailed medical image captions. GMAI-MM-Instrunct-0.9M diverse set of instructions for medical image analysis. GMAI-VL-5.5M"
        },
        {
            "title": "Size",
            "content": "1.7M 0.9M GMAI-MM-Percept-1.3M dataset of labels for medical image classification and segmentation. 1.3M GMAI-Text-Single-1M set of single-round medical dialogues on patient queries GMAI-Text-Multi-0.6M dataset of multi-turn medical conversations on various topics. 1.0M 0.6M (a) Modality distribution (b) Original task distribution (c) Department distribution (d) Clinical task distribution Figure 4. Distribution of GMAI-VL-5.5M across tasks, modalities, departments, and clinical tasks. (a) Original Task Distribution: The dataset includes 2D Classification (50.4%), 3D Segmentation (30.3%), 2D Segmentation (12.7%), and 2D Detection (6.6%). (b) Modality Distribution: In addition to CT (26.8%) and MR (24.7%), X-ray (12.6%), Pathology (11.2%), and less common modalities like Dermoscopy (3.5%), Microscopy (2.4%), and PET (0.2%) are represented. (c) Department Distribution: While Orthopedic Surgery (12.9%) and General Surgery (10.3%) are the top contributors, departments like Endocrinology (1.3%), Infectious Diseases (0.8%), and Urology (0.7%) also provide data. (d) Clinical Task Distribution: Besides Disease Diagnosis (40.4%) and Organ Recognition (16.0%), tasks such as Muscle Recognition (3.3%), Nervous Tissue Recognition (1.5%), and Microorganism Recognition (1.2%) are included. Note: The distribution statistics shown here pertain only to the multimodal components of GMAI-VL-5.5M. 3 Figure 5. Distribution of all our training data. The inner ring represents major categories, each depicted in different color. The outer ring corresponds to the subcategories within each major category. The size of each segment is proportional to the amount of data, as indicated in the legend, where the data volume for each subcategory is also provided. Figure 6. Diagram of the three-stage training process. 4 Table 7. List of datasets used in our model. We employ large collection of image-text data and instruction data for training stage."
        },
        {
            "title": "Size",
            "content": "ratio in stage 1&2 ratio in stage 3 ALLaVA[13] ShareGPT4V[15] GMAI-MM-Caption-1.7M PubMedVision[14] MedICaT[60] MPx-Single[70] PMC-OA[43] QUILT-1M[32] Retina Image Bank[3] CheXpertPlus[12] MIMIC-CXR[33] OpenI[21] GeoQA+[11] AI2D[36] SynthDoG[37] ChartQA[51] MMChemExam[42] LLaVA-Instruct-150K[45] DVQA[34] DocVQA[52] GMAI-MM-Percept-1.3M GMAI-MM-Instruct-0.9M PubMedVision[14] LLaVA-Med-60k[39] PMC-Inline[70] VQA-Med-2019[8] Medical-Diff-VQA[30] PathVQA[29] PMC-CaseReport[70] PMC-VQA[80] ROCOV2[57] SLAKE[44] VQA-RAD[38] blossom orca[5] COIG-CQIA[7] Cosmopedia-100k[9] ShareGPT4V[15] Orca-Math[53] Leetcode[10] LogiQA[47] Lima[26] Open Hermes 2.5[64] Firefly[74] UltraChat[23] Alpaca-Instruct-52K[61] GMAI-Text-Single-1M GMAI-Text-Multi-0.6M - 468k 102k 1.7M 1.3M 173k 31k 1.3M 643k 22k 223k 486k 7k 72k 12k 29k 18k 219k 157k 200k 10k 1.3M 0.9M 1.28M 56k 288k 3.2k 260k 2.6k 109k 251k 60k 0.6k 0.3k 20k 14.8k 33k 26k 379k 1.7k 12.7k 83k 200k 189k 189k 49k 1.0M 649k 15.7M 5 100.0% 100.0% 50.0% 100.0% 100.0% 5.0% 100.0% 30.0% 100.0% 75.0% 100.0% 100.0% 100.0% 10.0% 0.0% 100.0% 0.0% - 100.0% - Table 8. Training settings of GMAI-VLs stage I, stage II, and stage III."
        },
        {
            "title": "Stage III",
            "content": "freeze LLM freeze MLP freeze Vision Encoder packing type learning rate learning rate schedule optimizer optimizer hyper-parameters input size total batch size drop rate numerical precision GPUs for training True False True soft packing 1e-3 cosine decay AdamW 1 = 0.9, 2 = 0.999 336x336 32x8x2 0.0 DeepSpeed bf16 32xA100 (80G) True False False soft packing 1e-4 cosine decay AdamW 1 = 0.9, 2 = 0.999 336x336 32x4x4 0.0 DeepSpeed bf16 32xA100 (80G) False False False soft packing 1e-5 cosine decay AdamW 1 = 0.9, 2 = 0.999 336x336 32x4x4 0.0 DeepSpeed bf16 32xA100 (80G) Figure 7. Examples of our model in various medical tasks, including image description, disease diagnosis, diagnostic report generation, free question answering, etc. The results demonstrate the models versatility across different imaging modalities and tasks, with support for both Chinese and English."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Fudan University",
        "Monash University",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences",
        "Stanford University",
        "University of Cambridge",
        "University of Washington",
        "Xiamen University"
    ]
}