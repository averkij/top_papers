{
    "paper_title": "HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models",
    "authors": [
        "Haoran Li",
        "Yingjie Qin",
        "Baoyuan Ou",
        "Lai Xu",
        "Ruiwen Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 4 4 4 0 2 . 5 0 5 2 : r Preprint. HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models Haoran Li1, Yingjie Qin2, Baoyuan Ou2, Lai Xu2, Ruiwen Xu2 1Carnegie Mellon University 2Xiaohongshu Inc."
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the longcontext capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs) [1, 2, 3, 4, 5] have achieved remarkable success in multimodal tasks, including visual question answering [6, 7], image captioning [8, 9], and cross-modal retrieval [10, 11]. However, VLMs suffer from significant performance degradation in long-context scenarios, particularly long videos [12, 13, 14, 15]. In such settings, VLMs even struggle with simple tasks like object counting and temporal localization [16, 17], revealing critical limitation in their ability to model extended spatial-temporal dependencies. This limitation substantially hinders their real-world deployment, where input length often exceeds the context window they have been pretrained on. Rotary Position Embedding (RoPE) [18] has been widely adopted for length generalization in text-based LLMs [19, 20, 21]. Specifically, RoPE incorporates positional information by partitioning the query and key vectors into 2-dimensional pairs and rotating each pair at unique frequency that decreases as the dimensional index increases. Despite its advantages, directly applying 1D RoPE fails to capture the intricate spatial-temporal dependencies in videos. Several methods have been proposed to extend 1D RoPE for multimodal inputs [2, 22, 23, 23]. Among these, the most common approach is to allocate different frequencies to encode different positional components, as shown in the upper plots of Figure 1. For example, M-RoPE [2] allocates the highest frequencies for temporal modeling (t), and the remaining low frequencies for spatial modeling (x, y). In contrast, VideoRoPE [23] proposes to allocate the lowest frequencies for temporal dimensions (t), and further applies fixed scaling factor to scale the temporal indices of visual tokens, as shown in the lower plots of Figure 1. Despite their improved performance, two significant challenges remain unsolved. Firstly, current methods mainly rely on heuristics rather than 1 Preprint. Figure 1: Comparison of our HoPE and existing methods. Upper plots illustrate the frequency allocation strategies in different RoPE variants. Here, frequency decreases along the diagonal. (d) HoPE sets the lowest frequencies to zero for reliable long-range semantic modeling. Lower plots demonstrate different temporal scaling mechanisms. (d) HoPE proposes dynamic and bidirectional scaling to learn temporal dynamics at multiple scales, facilitating robustness to various video speeds. theoretical analysis to determine the frequency allocation strategy. Second, applying fixed scaling factor for all videos is suboptimal in real-world scenarios, where videos proceed with different speeds and demonstrate significant variance in information densities. To address these challenges, we first conduct an in-depth theoretical analysis of what ideal properties multimodal RoPE should hold in Section 3. Our analysis reveals that: (1) the flattening operation in vanilla RoPE inherently violates spatial-temporal locality prior, which is crucial in video modeling; (2) despite diverse frequency allocation strategies, existing multimodal RoPE variants fail to reliably capture semantic similarities over extended contexts; (3) temporal scaling of visual tokens should include both compression and expansion to accommodate varying video speeds in real-world scenarios. Guided by these insights, we propose HoPE, Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. As shown in Figure 1, HoPE first introduces hybrid frequency allocation strategy to facilitate long-range semantic modeling. In this strategy, higher frequencies, which are more sensitive to positional differences and better at capturing local features, are allocated for spatial positions (x, y) in an interleaved manner. Meanwhile, the lower frequencies are directly set to zero and allocated for temporal modeling (t) to enable reliable semantic modeling. Moreover, HoPE develops dynamic temporal scaling mechanism for lengthy visual tokens. This mechanism not only enhances VLMs robustness to various video speeds, which are common in real-world scenarios, but also offers flexible scaling during inference across diverse context lengths. Main Contributions. (1) To our best knowledge, we are the first to provide theoretical analysis of how different frequency allocation strategies impact the performance of multimodal RoPE. These findings can be further utilized for the design and analysis of future multimodal RoPE variants. (2) Guided by our analysis, we propose HoPE, which consists of hybrid frequency allocation strategy for reliable semantic modeling in long-context scenarios, and dynamic temporal scaling mechanism for robust and flexible temporal comprehension. (3) Extensive experiments on four video benchmarks demonstrate that HoPE consistently outperforms existing RoPE variants, achieving improvements of 22.23% and 8.35% on long video retrieval and long video understanding tasks, confirming its effectiveness. 2 Preprint."
        },
        {
            "title": "2 Preliminaries",
            "content": "Rotary Position Embedding (RoPE). Current Transformer-based LLMs rely on Positional Encodings (PEs) to incorporate sequential information into the attention mechanism. Among various PEs, Rotary Position Embedding (RoPE) [18] has emerged as dominant approach for long-context modeling in text-based LLMs [19, 20, 21]. The key to RoPEs success lies in its ability to encode relative position information through an absolute positional encoding approach, ensuring both effectiveness and efficiency. Consider query and key vectors with dimensions (where is an even number), RoPE partitions the dimensions into d/2 pairs, e.g., qn = [q ]. Each pair of dimensions is assigned unique rotation angle θi = b2i/d, {0, 1, . . . , d/2 1}, where is pre-defined frequency base and set to 10, 000 by default [18]. This rotation can be achieved through rotation matrix as follows: (1) ; . . . ; (d/21) (0) ; (cid:18) r(i) = cos θi sin θi cos θi sin θi (cid:19) . (1) The overall rotation matrix Rn is constructed by concatenating each pairs rotation matrix along the diagonal to form block-diagonal matrix, i.e., Rn = diag(r(0), r(1), . . . , r(d/21)) Rdd. Therefore, during attention computation, the attention score1 An,m between the n-th query qn and m-th key km is: An,m = (qnRn)(kmRm) = qnRnmk m, where the rotation matrix Rnm can be formulated as: Rnm = cos θ0(n m) sin θ0(n m) cos θ0(n m) sin θ0(n m) ... ... 0 0 0 0 . . . 0 0 ... cos θ(d/21)(n m) sin θ(d/21)(n m) cos θ(d/21)(n m) sin θ(d/21)(n m) 0 0 ... (2) . (3) It can be observed that through pairwise attention computation, the final rotation matrix naturally incorporates the relative position information (n m) between the query-key pair. No Positional Encoding (NoPE). Despite the popularity of RoPE, several works have pointed out that the causal attention mechanism in current decoder-only LLMs implicitly learns absolute positional information [25, 26, 27]. This motivates the development of No Positional Encoding (NoPE). Specifically, the causal attention mask enforces An,m = 0 for all > n, ensuring that each token only attends to itself and previous tokens. Under this constraint, the attention score with NoPE is simple dot product between the query vector qn and the key vector km, i.e., An,m = qnk m, providing no explicit positional information to Transformers."
        },
        {
            "title": "3 Analysis",
            "content": "In this section, we conduct comprehensive theoretical analysis on multimodal RoPE variants, aiming to answer the following questions: (1) Is vanilla RoPE enough for long-context VLMs? (2) How do different frequency allocation strategies impact semantic modeling in long-range multimodal contexts? (3) How should we assign the temporal indices for text and visual tokens? 1Here, we omit the softmax function and 1/ scaling in standard Transformer [24] for simplicity. 3 Preprint. 3.1 Vanilla RoPE Fails in Spatial-Temporal Structure Several recent VLMs [1, 28, 29, 15, 5] still use vanilla RoPE for multimodal inputs. In their approach, each video frame is first encoded by vision encoder (e.g., ViT [30]) and then flattened into sequence of patch-level tokens. These visual tokens will be treated equally as text tokens for positional encoding, with each token only incorporating 1D temporal information. We show in Proposition 3.1 that this approach, while easy to implement, distorts spatial-temporal localities and fundamentally limits VLMs ability to model extended spatial-temporal dependencies. Proposition 3.1 (1D RoPE violates spatial-temporal locality priors). Given any query at position (t, x, y) and relative distance of 1 in spatial or temporal dimensions, the flattening operation in 1D RoPE distorts the relative distance with magnitude dependent on the frame resolution. We provide the proof in Appendix A.1. This mismatch between positional encoding and the 3D structure of videos creates distorted attention patterns, making it difficult for models to learn meaningful spatial-temporal relationships essential for video-related tasks. Conclusion 1: Directly applying vanilla RoPE for multimodal long-context inputs inherently fails to capture their rich spatial-temporal dependencies. 3.2 Current Multimodal RoPEs Are Unreliable in Long-Range Semantic Modeling To capture the spatial-temporal structure of multimodal inputs, recent VLM, Qwen2-VL [2] has introduced Multimodal Rotary Position Embedding (M-RoPE). Concretely, MRoPE partitions the 128-dimensional rotary embedding into three distinct groups: the first 32 dimensions for temporal information (t), the subsequent 48 dimensions for horizontal spatial information (x), and the final 48 dimensions for vertical spatial information (y), i.e., Rt,x,y = diag(Rt, Rx, Ry). While this approach realizes naive extension for RoPE, fundamental question remains to be answered: How do different frequency allocation strategies impact the performance of multimodal RoPE? This question arises from the fact that in RoPE, different dimensions carry unique frequencies (θi = b2i/d, {0, 1, . . . , d/2 1}), as shown in Equation 3. Therefore, different strategies exist for frequency allocation in multimodal RoPE. As shown in Figure 1, M-RoPE allocates the highest frequencies for t, intermediate frequencies for x, and the lowest frequencies for y. In contrast, VideoRoPE [23] proposes to assign the lowest frequencies for temporal modeling (t) and high frequencies to spatial dimensions (x, y). Their empirical justification stems from attention pattern analysis, which reveals that dimensions encoded with the lowest frequencies exhibit more pronounced attention sink phenomenon [31], which has proven to be effective in long-context modeling. However, we argue that using the lowest frequencies for temporal modeling is still unreliable in capturing semantic similarities in extended multimodal contexts. Specifically, we first introduce semantic preference, property where attention mechanisms should prioritize semantically similar tokens regardless of their relative distance, and formally define this concept in Definition 3.1. Definition 3.1 (Semantic Preference). For any query vector and semantically similar key vector that can be expressed as = + δ where δ is zero-mean perturbation, the attention score with RoPE should satisfy: q,k,δ[qRtxyk qRtxyk] 0, where is the key vector of semantically unrelated token. This preference should hold regardless of the relative distance (t, x, y) between the query and key. (4) Then, we show in Theorem 3.1 that all frequency allocation strategies of current multimodal RoPEs, including selecting the highest/lowest frequencies for temporal modeling, are unreliable in maintaining the semantic preference property over extended contexts. This limitation arises because, with sufficiently long contexts, even the lowest frequencies can produce arbitrary rotations, ultimately undermining semantic preference. We provide the proof in Appendix A.2. 4 Preprint. Theorem 3.1. Let = [x1, x2, . . . , xL] be an input sequence, and let RoPE use any fixed set of temporal frequencies (e.g., highest or lowest). Then there exists critical length Lc such that for all Lc, the semantic preference property (Definition 3.1) is violated. Conclusion 2: There exists different frequency allocation strategies to extend vanilla RoPE to multimodal RoPE. However, we prove that none of these strategies can reliably maintain the semantic preference property over sufficiently long context. 3.3 How to Assign Position Index for Multimodal Inputs? Currently, most VLMs [1, 2, 3, 4, 14, 15] adopt the same temporal stride for video frames and text tokens, as shown in Figure 1. However, this approach overlooks the inherent difference in information densities between text and visual tokens. To address this issue, VideoRoPE [23] proposes to apply fixed scaling factor (implemented as 2) to adjust the temporal indices of visual tokens, achieving better empirical performance. However, this rigid scaling approach lacks the flexibility needed for diverse real-world videos, which naturally vary in pace and information density. more ideal approach would incorporate both temporal compression and expansion capabilities, allowing the model to learn multi-scale temporal relationships, thereby enabling more robust temporal modeling. Conclusion 3: Temporal index scaling is beneficial to balancing multimodal information. However, current methods lack flexibility and bidirectionality in temporal scaling."
        },
        {
            "title": "4 HoPE: Hybrid of Position Embedding for Vision-Language Models",
            "content": "To address the above challenges, we propose HoPE, Hybrid of Position Embedding designed to improve the long-context capability of VLMs. As illustrated in Figure 1 and Figure 2, HoPE first introduces hybrid frequency allocation (HFA) strategy to better preserve the semantic preference property (Definition 3.1) in long-context modeling. Under this strategy, spatial information will be encoded with higher frequencies to capture local semantics, while the lowest frequencies will be set to zero (as in NoPE [25]) to facilitate long-range semantic modeling. Second, HoPE develops dynamic temporal scaling (DTS) mechanism to enhance VLMs robustness to various video speeds and enable flexible inference under diverse context lengths. We will detail these strategies as follows: 4.1 Hybrid Frequency Allocation Strategy To extend vanilla RoPE to multimodal scenarios, common approach is to allocate different frequencies to encode different positional components (t, x, y). For example, M-RoPE [2] assigns the highest frequencies for temporal modeling and lower frequencies for spatial encoding. In contrast, VideoRoPE [23] allocates the lowest frequencies for temporal modeling and achieves better empirical results. However, in Theorem 3.1, we theoretically prove that despite using lower frequencies is more ideal for semantic modeling, none of these frequency allocation strategies can maintain the ideal semantic preference property (Definition 3.1) over extended contexts. To provide stronger theoretical guarantee for the semantic preference property, we propose hybrid frequency allocation strategy. As shown in Figure 1, we encode spatial information (x, y) with high frequencies, as high frequencies are more sensitive to positional differences and thereby better at capturing local semantics [23, 32]. Following existing work [23], and are encoded in an interleaved manner to prevent biased spatial encoding. More importantly, unlike existing methods [18, 23, 2], we directly set the lowest frequencies to zero (as in NoPE [25]) to provide stronger guarantee for the semantic preference property (Definition 3.1), as shown in Figure 2. Specifically, for = 128, we interleave and positions in the first 96 dimension of the rotation matrix and set the frequencies in the remaining 32 dimensions to zero, which corresponds to an identity matrix: 5 Preprint. (a) High frequencies for temporal modeling in M-RoPE. (b) Low frequencies for temporal modeling in VideoRoPE. (c) Zero frequencies for temporal modeling in HoPE (ours). Figure 2: Multimodal RoPEs use different frequencies for temporal modeling. M-RoPE uses the highest frequencies which are suboptimal for long-context modeling. VideoRoPE utilizes the lowest frequencies for more stable semantic modeling. Our HoPE, by using zero frequencies for temporal modeling, establishes the upper bound for semantic modeling capabilities across all strategies. Rx,y = diag( cos θ0 sin θ0 0 0 . . . 0 0 0 0 x sin θ0 cos θ0 0 0 . . . 0 0 0 0 0 0 cos θ1 sin θ1 . . . 0 0 0 0 sin θ1 cos θ1 . . . 0 0 0 0 0 y . . . 0 0 0 0 . . . . . . cos θ46 . . . sin θ46 0 . . . 0 . . . sin θ46 cos θ46 0 0 x 0 0 0 0 . . . 0 0 0 0 . . . 0 0 cos θ47 sin θ47 0 0 0 0 . . . 0 sin θ47 cos θ47 y , I32) We now provide theoretical analysis of how this hybrid strategy helps the attention mechanism to capture long-range semantic similarities. Specifically, with Definition 3.1 and Theorem 3.1, we first formalize the condition under which semantic preference is preserved in multimodal RoPE. Lemma 4.1 (Necessary Condition for Semantic Preference). For multimodal RoPE with rotation matrix Rt,x,y = diag(Rt, Rx, Ry) (where t, x, do not denote their sequential order in the matrix), by Theorem 3.1, the semantic preference property (Definition 3.1) holds if the following condition is satisfied for all possible relative distances: 2σ2cos(t θi) + iix 2σ2cos(x θi) + iiy 2σ2cos(y θi) 0, iit (5) where σ2 denotes the variance of each component in the query/key vector, it, ix, iy denotes dimensions allocated to t, x, and y, respectively, and {0, 1, . . . , 1}, {0, 1, . . . , H}, {0, 1, . . . , W} represent the relative distances in these dimensions. This lemma establishes clear theoretical criterion for maintaining semantic preference with multimodal RoPE. It directly follows from our analysis in Theorem 3.1 and Appendix A.2, providing the foundation for our proposed method. Based on this condition, we now prove how our hybrid frequency allocation strategy provides stronger guarantees for semantic preference property over extended context. Specifically, HFA set θi = 0 for all it. Hence, the temporal terms in Equation 5 reduce to iit 2σ2 1, noting that iit 2σ2 1 iit 2σ2 cos(t θi) holds for any alternative choice of temporal frequencies θi. Adding the identical spatial terms on both sides, we obtain: iit iit 2σ2 1 + iix 2σ2 cos(x θi) + iiy 2σ2 cos(y θi) 2σ2 cos(t θi) + iix 2σ2 cos(x θi) + iiy 2σ2 cos(y θi). 6 (6) Preprint. This shows that our HFA strategy, by setting the lowest frequencies to zero, dominates any other choice of temporal frequencies and provides stronger guarantee for preserving the semantic preference property under long-context scenarios, as in Theorem 4.1. Theorem 4.1. For multimodal position embeddings with dimensions allocated across temporal (it), and spatial dimensions (ix, iy), setting θi = 0 for all temporal dimensions it maximizes the semantic preference guarantee in Definition 3.1 compared to any alternative frequency allocation strategy, particularly for extended context lengths. Another interesting finding is that, if we set it = d/4, ix = iy = d/8 and θi = 0, it, for any context length and spatial size x, y, semantic preference property invariably holds, as Equation 5 reduces to d/81 2σ2(2 + cos(x θ2i) + cos(y θ2i+1)) 0. However, the empirical results of this approach are inferior to our proposed HoPE, probably due to the decreased number of frequencies for spatial modeling. More discussions are provided in Appendix B.4. i=0 4.2 Dynamic Temporal Scaling Mechanism Considering the distinct information densities of text and visual tokens, HoPE introduces dynamic temporal scaling mechanism that adjusts the temporal strides of visual inputs. Specifically, we first define set of scaling factors, e.g., Γ = {0.5, 0.75, 1, 1.25, 1.5}, which includes both stretching (γ > 1) and compressing (γ < 1) operations. During training, scaling factor γ is randomly selected from Γ and applied to each video. This allows the model to learn temporal relationships at multiple scales, making it more robust to variations in video speed, which are common in real-world scenarios. Consider multimodal input (text, video, text) of length Lt, Lv, and Le, respectively. The position indices (t, x, y) for each token with our dynamic scaling factor γ are: (t, x, y) = (l, l, l), 0 < Lt Lt + γ(l Lt), Lt + γ(l Lt) + 2 , Lt + γ(l Lt) + 2 , Lt < Lt + Lv . (7) (γ 1)Lv + l, (γ 1)Lv + l, (γ 1)Lv + , Lt + Lv < Lt + Lv + Le During inference, scaling factors can be flexibly selected from the set to accommodate videos of different lengths. It is worth noting that unlike existing methods, which do not consider temporal scaling for visual tokens [1, 2, 4, 5] or just apply fixed and unidirectional scaling factor for both training and testing [23], our methods not only help the model learn temporal relationships at multiple scales, but also offer flexibility during inference to accommodate various context lengths."
        },
        {
            "title": "5 Experiment",
            "content": "In this section, we evaluate the performance of HoPE on four video benchmarks across long video understanding and long video retrieval tasks, aiming to validate its effectiveness in multimodal long-context modeling. Additionally, we conduct ablation studies to investigate the individual contribution of each strategy to overall performance. 5.1 Experimental Setups Implementation Details. We utilize Qwen2-1.5B and Qwen2-7B [33] as the backbone models. By integrating these models with vision encoders from Qwen2-VL-2B/7B-Instruct [2], we obtain Qwen2-2B/7B-Video, respectively. During training, we adopt batch size of 7 Preprint. Table 1: Performance comparison on long video understanding task across MLVU, LongVideoBench, and Video-MME benchmarks. The training context length for all methods is set to 8k, while we report the performance on 8k, 16k, 32k, and 64k to assess the length generalization ability of different methods. The best results are bold, while the second best results are underlined. Method 8k 16k 32k 64k 8k 16k 32k 64k 8k 16k 32k 64k MLVU LongVideoBench Video-MME Qwen2-2B-Video Vanilla RoPE M-RoPE VideoRoPE HoPE (Ours) 55.10 53.26 54.75 54.89 55.21 53.69 55.19 56.36 54.36 54.73 54.00 55.70 39.06 40.63 42.19 45.12 51.57 50.81 52.17 52.31 50.29 52.26 52.02 52. 51.00 51.30 51.31 51.66 34.21 44.74 36.84 46.27 50.70 51.44 50.89 51.79 51.48 51.22 50.52 51.87 51.44 51.52 50.56 51.69 20.31 23.44 15.63 26. Qwen2-7B-Video Vanilla RoPE M-RoPE VideoRoPE HoPE (Ours) 59.75 59.70 60.40 61.09 61.13 61.68 61.82 63.48 61.03 62.46 62.51 63.85 34.38 46.88 45.31 50. 51.17 52.27 52.89 54.11 50.31 53.29 53.13 55.09 51.29 53.49 53.82 55.34 39.47 50.00 47.37 51.22 56.70 56.81 57.51 57.74 57.96 57.77 59.00 59. 57.99 58.37 59.13 59.44 26.13 23.43 26.52 27.34 128, learning rate of 1e-5(2B)/2e-5(7B) with cosine scheduler. Following the instruction tuning settings in Qwen2-VL [2], we set the maximum video frames to 128 and the video sampling rate to 2. The training context length is set to 8k, with the entire training process taking approximately 304 GPU hours on machines equipped with H800-80GB GPUs. During evaluation, the minimum tokens per frame is set to 144. Training Data. We train the models on subset of LLaVA-Video-178k [34], which consists of 178k videos ranging from 0 to 3 minutes and 5M instruction samples, including captions, free-form and multiple-choice question answering. Our selected subset includes 30k videos with durations under 2 minutes and 3k videos with durations between 2 and 3 minutes, resulting in roughly 300k pairs. Baselines. We compare HoPE with the following RoPE-variants: 1) vanilla RoPE [18], the standard approach in long-context LLMs, 2) M-RoPE [2], famous RoPE extension in Qwen2-VL for multimodal inputs, 3) VideoRoPE [23], specialized RoPE variant designed for video-related tasks. Evaluation Benchmarks. We evaluate HoPE across four video benchmarks for long video understanding and long video retrieval tasks. For long video understanding, we utilize LongVideoBench [35], Video-MME [36], and MLVU [37]. For long video retrieval, we employ V-NIAH (Visual Needle-In-A-Haystack) [12]. For detailed benchmark descriptions, please refer to Appendix B.1. 5.2 Results on Long Video Understanding In this section, we provide comprehensive comparison of HoPE and different RoPE variants in long video understanding. From Table 1, we observe that: (1) HoPE consistently outperforms all baselines across nearly all benchmarks, context lengths, and backbone sizes. Specifically, under the 7B model scale and 32k context lengths, HoPE surpasses vanilla RoPE by 2.82, 4.05, 1.45 on MLVU, LongVideoBench, and Video-MME, respectively. This confirms its effectiveness and generalizability in multimodal long-context modeling. (2) The effectiveness of HoPE scales with backbone size. For instance, when the size of the backbone LLM increases from 2B to 7B, HoPEs performance gain on LongVideoBench (32k) significantly increases from 0.66 to 4.05 compared to vanilla RoPE. Notably, the performance gap between different methods on the 2B scale is less significant, probably due to the limited capabilities of the backbone LLM. (3) For context lengths under 64k, performance on Video-MME drops substantially, while the impact on MLVU and LongVideoBench is 8 Preprint. Figure 3: Performance on long video retrieval task (V-NAIH). The black dotted line represents the training context length. Each frame corresponds to 144 tokens. less pronounced. This suggests that extrapolating to extreme context lengths (e.g., up to 8x) remains highly challenging. 5.3 Results on Long Video Retrieval We evaluate HoPE against other RoPE variants on V-NIAH [12] to demonstrate the superiority of our method in long video retrieval, where VLMs are required to identify specific frames in video to answer the question. Figure 3 demonstrates that multimodal RoPEs significantly outperform vanilla RoPE, supporting our claim in Proposition 3.1 that the flattening operation in vanilla RoPE hinders spatial-temporal modeling. Furthermore, HoPE achieves better extrapolation than M-RoPE and VideoRoPE, confirming its effectiveness in multimodal long-context modeling. Quantitative results in Table 3 show that HoPE surpasses the best baseline by significant margin of 22.23%. 5.4 Ablation Studies Table 2: Ablation results on Video-MME from 8k to 64k. Here, HFA: hybrid frequency allocation, DTS: dynamic temporal scaling. We conduct series of ablation experiments to evaluate the impact of each component in HoPE and summarize the results in Table 2. According to the results, we observe that : (1) The 3D structure effectively improves the performance of vanilla RoPE in multimodal contexts, supporting our Proposition 3.1. (2) Based on the 3D structure, the hybrid frequency allocation (HFA) strategy further enhances long-range semantic modeling, achieving an average improvement of 1.69 across all context lengths. (3) The dynamic temporal scaling (DTS) mechanism facilitates VLMs robustness to varying video speeds in real-world scenarios, yielding further performance gain. By combining the above strategies, our HoPE achieves the best results in multimodal long-context modeling. More ablations on test-time scaling factor selection are provided in Appendix B.3. 56.70 57.96 57.99 26.13 Vanilla RoPE 56.81 57.77 58.37 23.43 + 3D structure + 3D + HFA 57.66 59.19 59.33 26.98 + 3D + HFA + DTS 57.74 59.33 59.44 27. Method 64k 32k 16k 8k"
        },
        {
            "title": "6 Related Work",
            "content": "Position Embeddings in LLMs. Rotary Position Embedding (RoPE) has become common choice in LLMs [19, 20, 21]. As discussed in Section 2, RoPE achieves this success through 9 Preprint. rotating query and key vectors, encoding relative position information through an absolute positional encoding approach. Despite its success, several works have pointed out that No Position Embedding (NoPE) still works for decoder-only LLMs, arguing that the causal attention mechanism implicitly learns absolute position information [25, 26, 38]. These works even suggest NoPE outperforms RoPE in out-of-distribution (OOD) scenarios. However, this observation remains unexplored in multimodal settings, where positional encoding strategies may have different implications for cross-modal interactions. Multimodal Position Embeddings in VLMs. In current VLMs [1, 2, 3, 4, 5], images are processed by vision encoders and flattened into 1D tokens. Some of these VLMs [1, 4, 5] still use vanilla RoPE to encode position information, which distorts spatial-temporal locality. Recently, Qwen2-VL [2] has proposed M-RoPE, which extends RoPE by assigning different frequencies to encode different positional components. VideoRoPE [23] further refines this method by adjusting frequency allocation strategies and applying fixed scaling factor for the temporal indices of visual tokens. In contrast, our work theoretically analyzes how different frequency allocation strategies impact the performance of multimodal RoPE. Based on our analysis, we propose HoPE, which combines RoPE and NoPE in the multimodal scenario for length generalization and demonstrates strong empirical performance."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper theoretically analyzes the limitations of current multimodal RoPE variants. Our analysis reveals that: (1) vanilla RoPE inherently fails in spatial-temporal modeling; (2) keeping all frequencies in multimodal RoPE is unreliable in capturing long-range semantic similarities; (3) temporal scaling of lengthy visual tokens should include both compression and expansion to accommodate various video speeds. Consequently, we introduce HoPE, hybrid of position embedding designed to enhance the long-context capabilities of VLMs. HoPE proposes hybrid frequency allocation strategy to facilitate long-range semantic modeling, and dynamic temporal scaling mechanism to enhance VLMs robustness to varying video speeds in real-world scenarios. Experimental results on long video understanding and long video retrieval tasks demonstrate that HoPE consistently outperforms existing methods across diverse context lengths and backbone sizes, confirming its effectiveness."
        },
        {
            "title": "References",
            "content": "[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [2] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [3] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [4] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 1304013051, 2024. [5] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. [6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 10 Preprint. [7] Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. MMed-RAG: Versatile multimodal RAG system for medical vision language models. In The Thirteenth International Conference on Learning Representations, 2025. [8] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. Advances in Neural Information Processing Systems, 36:4092440943, 2023. [9] Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, Chengjie Wang, and Feng Zheng. Transferable decoding with visual entities for zero-shot image captioning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 31363146, 2023. [10] Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. Generative cross-modal retrieval: Memorizing images in multimodal language models for retrieval and beyond. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1185111861, 2024. [11] Yabing Wang, Le Wang, Qiang Zhou, Zhibin Wang, Hao Li, Gang Hua, and Wei Tang. Multimodal llm enhanced cross-lingual cross-modal retrieval. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 82968305, 2024. [12] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [13] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. [14] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [15] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1258512602, 2024. [16] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, et al. Needle in multimodal haystack. Advances in Neural Information Processing Systems, 37:2054020565, 2024. [17] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tuny Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics, 2025. [18] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [20] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [21] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 11 Preprint. [22] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In European Conference on Computer Vision, pages 289305. Springer, 2024. [23] Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. Videorope: What makes for good video rotary position embedding? arXiv preprint arXiv:2502.05173, 2025. [24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [25] Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 13821390, 2022. [26] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. [27] Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuan-Jing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. In Findings of the Association for Computational Linguistics ACL 2024, pages 1402414040, 2024. [28] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual In Proceedings of the 2023 Conference on language model for video understanding. Empirical Methods in Natural Language Processing: System Demonstrations, pages 543553, 2023. [29] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. [30] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [31] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [32] Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veliˇckovic. Round and round we go! what makes rotary positional encodings useful? In The Thirteenth International Conference on Learning Representations, 2025. [33] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [34] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [35] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. [36] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [37] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 12 Preprint. [38] Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veliˇckovic. Round and round we go! what makes rotary positional encodings useful? arXiv preprint arXiv:2410.06205, 2024. 13 Preprint."
        },
        {
            "title": "A Proofs",
            "content": "In this section, we provide detailed proofs for the theoretical statements presented in this paper. A.1 Vanilla RoPE Fails in Spatial-Temporal Structure Proposition 3.1. Given any query at position (t, x, y) and relative distance of 1 in spatial or temporal dimensions, the flattening operation in 1D RoPE distorts the relative distance with magnitude dependent on the frame resolution. Proof. Consider video of shape W, where each token at position (t, x, y) is flattened by (t, x, y) = tHW + xW + y. Now consider two types of local neighbors: 1. Spatial neighbors within the same frame: Let (t, x, y) and (t, + 1, y) be adjacent in the spatial dimension. Then, (t, + 1, y) (t, x, y) = ((x + 1)W + y) (xW + y) = W. (8) Note that relative distance of 1 in becomes after flattening, which grows linearly with the frame width. 2. Temporal neighbors at the same spatial position: Let (t, x, y) and (t + 1, x, y) be adjacent in time. Then, (t + 1, x, y) (t, x, y) = (t + 1)HW + xW + (tHW + xW + y) = HW. (9) For 1-frame shift in time, the index difference becomes HW, which grows with spatial resolution. In both cases, spatially or temporally adjacent tokens are mapped to indices with significant differences. Since vanilla RoPE incorporates positional information based on these 1D index differences, such flattening leads to distorted spatial-temporal relationships. A.2 Semantic Preference Property We now prove that the frequency allocation strategies in current multimodal RoPEs are unreliable in capturing semantic similarities over extended contexts, as defined in Definition 3.1. Definition 3.1. (Semantic Preference). For any query vector and semantically similar key vector that can be expressed as = + δ where δ is zero-mean perturbation, the attention score with RoPE should satisfy: q,k,δ[qRtxyk qRtxyk] 0, where is the key vector of semantically unrelated token. This preference should hold regardless of the relative distance (t, x, y) between query-key pairs. (10) Firstly, we use Lemma A.1 to show why using lower frequencies for temporal modeling is more ideal in multimodal RoPE. Intuitively, larger rotation angles (frequencies) are more likely to produce negative cosine similarity values between semantically related tokens under long-context scenarios. Lemma A.1. Let be drawn uniformly from {0, 1, . . . , 1}, and define P(θ) = 1 (cid:12){ : cos(θ t) < 0}(cid:12) (cid:12) (cid:12). Then for any > 1: 1. If 0 < θ < π 2(L1) , then P(θ) = 0. Preprint. 2. For θ π 2(L1) , P(θ) is non-decreasing in θ. 3. lim θ P(θ) = 1 2 . Proof of Lemma A.1. 1. No negative region for small θ. If 0 < θ < π {0, . . . , 1} we have 2(L1) , then for every 0 θ < θ (L 1) < π/2, so cos(θ t) > 0. Hence P(θ) = 0. 2. Monotonicity once the first zero enters. As soon as θ π 2(L1) , the point satisfying θ = π/2 lies in {0, . . . , 1}. Each further increase in θ extends the interval of length θ(L 1), adding more half-periods of cosine. Each added half-period contains exactly one (cid:12) (and hence P(θ)) negative region of length π. Therefore the count can only stay the same or increase, up to O(1/L) rounding errors on the discrete grid. 3. Limit to one half for large θ. For large θ, the values {θ }L1 =0 become equidistributed mod 2π. Since the negative region { mod 2π cos < 0} has total length π over each 2π-cycle, one finds (cid:12){ : cos(θ) < 0}(cid:12) (cid:12) P(θ) = lim θ π 2π = 1 2 . We can now prove the frequency allocation strategies in current multimodal RoPE cannot reliably maintain the semantic preference property, i.e., semantically similar tokens should receive higher attention than semantically unrelated pairs. Theorem 3.1. Let = [x1, x2, . . . , xL] be an input sequence, and let RoPE use any fixed set of temporal frequencies (e.g., highest or lowest). Then there exists critical length Lc such that for all Lc, the semantic preference property (Definition 3.1) is violated. Proof. We first recall the definition of multimodal RoPE, where the rotation matrix is partitioned to encode different dimensions: Rt,x,y = diag(Rt, Rx, Ry), where Rt, Rx, and Ry are rotation matrices applied to temporal, horizontal spatial, and vertical spatial dimensions, respectively, with each dimension carrying frequency of θi = b2i/d, {0, . . . , d/2 1}. Note that the (t, x, y) ordering is purely notational and does not constrain the actual dimension allocation strategy. Assume that each component of the query vector is independently and identically distributed with mean µ and variance σ2. We denote key vector that is semantically similar to as = + δ, where δ is zero-mean perturbation. The semantically unrelated key vector is independently drawn with the same distribution as q. Let t, x, denote relative temporal and spatial distances between the query and each key. According to Definition 3.1, 15 Preprint. the semantic preference property requires that: q,k,δ[qRt,x,y(q + ffi) qRt,x,yk] q,k[qRt,x,yk] q,k,δ[qRt,x,yk qRt,x,yk] = = Eq[qRt,x,yq] = Eq[qRt,x,yq] µ2Rt,x,y 2(µ2 + σ2)cos(t)θi + = iix iit 2µ2cos(x)θi + 2µ2cos(t)θi + iiy iix 2(µ2 + σ2)cos(x)θi + iiy 2µ2cos(y)θi iit = iit 2σ2cos(t θi) + iix 2σ2cos(x θi) + iiy 2σ2cos(y θi) 0, 2(µ2 + σ2)cos(y)θi (11) where it, ix, iy denote dimensions allocated to encode temporal (t), horizontal spatial (x), and vertical spatial (y) information. To satisfy the semantic preference property (Definition 3.1), the expected attention between query and its semantically similar key should remain higher than that for an unrelated key, regardless of their relative distance. This implies the following condition must hold universally: 2σ2cos(t θi) + iix 2σ2cos(x θi) + iiy 2σ2cos(y θi) 0, iit (12) {0, 1, . . . , 1}, {0, 1, . . . , H}, {0, 1, . . . , W}. Now consider long-context scenario, where H, W, we can now theoretically prove that why VideoRoPE [23] (using lowest frequencies for t) is better than M-RoPE (using highest frequencies for t) in maintaining semantic preference property in long contexts. Simply, by Lemma A.1, we show that when the context length is sufficiently large, the probability that cos(t θi) leads to negative values becomes higher when θi becomes larger. Therefore, lower frequencies, which rotate less, are less likely to violate the semantic preference property. However, despite using the lowest frequencies, VideoRoPE still fails to guarantee that the semantic preference property holds for all context lengths (Equation 12). Let VideoRoPE allocate only the smallest frequency to the temporal dimensions, instead of the = it smallest frequencies: so that in Equation (12) the temporal sum reduces to: θmin = b2( 2 1)/d, 2σ2 it cos(t θmin). Here, under the reasonable assumption that semantically related tokens co-occur in nearby spatial positions across frames, the spatial sums in Equation 12 remains non-negative. Thus we only consider the temporal sum in Equation 12: 2σ2it cos(t θmin). Now pick any context length so large that there exists {0, 1, . . . , 1} with θmin (cid:0) π 2 , 3π 2 (cid:1). Such indeed exists as soon as θmin (L 1) > π 2 , i.e. for any For that choice of t, we have > Lc = π 2 θmin + 1. cos(t θmin) < 0, 16 Preprint. Figure 4: Illustration of V-NIAH, which consists of randomly inserted needle image, haystack video, and specific question related to the needle. and hence the left-hand side of Equation (12) becomes 2σ2 it cos(t θmin) < 0. This single counterexample (t, x, y) violates the semantic preference condition, since no further temporal frequencies are available to rescue the sum. Therefore, despite using the lowest frequencies for temporal modeling, VideoRoPE still fails to guarantee the semantic preference property. In conclusion, all frequency allocation strategies in current multimodal RoPEs fail to maintain the semantic preference property in Definition 3.1, completing the proof."
        },
        {
            "title": "B Further Experimental Details",
            "content": "In this section, we provide further details of our experiments, including benchmark descriptions, experimental settings, and further results. B.1 Detailed Benchmark Description In this subsection, we provide detailed descriptions for the video benchmarks we used in the experiments, i.e., LongVideoBench [35], Video-MME [36], MLVU [37], and V-NIAH [12]. LongVideoBench is comprehensive benchmark for evaluating Vision-Language Models on long video understanding tasks. Unlike traditional video benchmarks that focus on short clips under one minute, this dataset features videos ranging from 8 seconds to 1 hour across diverse sources including everyday life, movies, knowledge, and news. The benchmark encompasses 17 fine-grained question categories organized into two levels: perception and relation. In our experiment, questions that are free from subtitles are retained. Video-MME is full-spectrum evaluation benchmark of Vision-Language Models in video analysis, spanning 6 primary visual domains with 30 subfields to ensure generalizability. It features temporal diversity by incorporating both short- (2 minutes), medium- (4-15 minutes), and long-term videos (30-60 minutes), ranging from 11 seconds to 1 hour. MLVU is high-quality benchmark designed to evaluate the video understanding capabilities of Vision-Language Models. The temporal duration of videos within MLVU spans from 3 minutes to 2 hours, covering genres such as movies, life records, and egocentric videos. In our experiment, we evaluate all methods on the following multiple-choice tasks: Action Count, Action Order, Topic Reasoning, Ego Reasoning, Needle QA, Plot QA, and Anomaly Recognition. V-NIAH is challenging benchmark designed to evaluate VLMs ability to identify specific frames within long videos. In this task, needle image is inserted into haystack video, and the VLMs are required to answer specific questions about this needle image, as shown in Figure 4. Following the settings in V-NIAH [12], we utilize haystack video 17 Preprint. with 1-hour duration (3,000 frames). The needle image is inserted at 20% depth intervals (e.g., frame depth of 0% would place the needle image at the very beginning of the video.) Table 3: Quantitative performance of different RoPE variants on V-NIAH. Here, we report the average accuracy across different context lengths and frame depths. Vanilla RoPE M-RoPE VideoRoPE HoPE (ours) V-NIAH 21.00 47.11 52.00 63.56 B.2 Quantitative Results on V-NIAH Here, we provide the quantitative results of different RoPE variants on long video retrieval task in Table 3. It can be observed that our HoPE demonstrates 22.23% improvement compared to the best baseline, justifying its effectiveness in multimodal long-context modeling. B.3 Ablation Studies on Test-Time Scaling Factor Selection In this subsection, we conduct further experiments to investigate how different scaling factors γ in our dynamic temporal scaling mechanism impact the performance of videorelated tasks. We summarize the results on V-NIAH and LongVideoBench in Table 4. Our main observations are as follows: Long video retrieval generally prefers smaller scaling factors. As shown in Table 4, when we utilize smaller scaling factors γ during inference, the performance on V-NIAH improves. We attribute this to the substantial length of 1-hour videos (3,000 frames), which far exceeds the training length (128 frames). In these cases, it is more effective to compress the visual information rather than further expand already lengthy content. Therefore, we set γ = 0.75 for long video retrieval. Long video understanding benefits from larger scaling factors with longer context lengths. In contrast to retrieval, we find that long video understanding is relatively insensitive to the choice of scaling factor when the input context length is close to the training length. However, as the input length increases, employing larger scaling factors (γ > 1) results in better performance. We hypothesize that larger scaling factors prevent the visual information from becoming overly compressed, thus preserving more temporal details required for complex understanding tasks. Moreover, when the context length is extremely long (e.g., 64k), performance becomes consistent across different scaling factors (γ {0.5, 0.75, 1.0, 1.25}). This suggests that VLMs may face comprehension bottleneck when provided with excessive information, regardless of how temporal information is scaled. In our experiment, we set γ = 1.5 for long video understanding. Table 4: Ablation studies on scaling factor selection. Scaling Factor fl LongVideoBench V-NIAH 0.50 0.75 1.00 1.25 1.50 8k 16k 32k 64k 54.48 54.36 54.11 54.11 54.11 54.29 54.97 54.48 54.84 55.09 54.36 54.72 54.97 55.70 55.34 52.63 52.63 52.63 52.63 51. 60.89 63.56 62.67 62.67 61.78 18 Preprint. B.4 Ideal Condition for Semantic Preference As discussed after Theorem 4.1, the semantic preference property (Definition 3.1) invariably holds for any context length and spatial size x, when we set it = d/4, ix = iy = d/8 and θi = 0, it, since Equation 5 reduces to: d/81 i= 2σ2(2 + cos(x θ2i) + cos(y θ2i+1)) 0. In our original HoPE implementation, the frequencies allocated to t, x, are 16, 24, 24, respectively, with the lowest 16 frequencies for set to zero. For this proposed variant (HoPE-X), we redistribute these allocations to 32, 16, 16 for t, x, y, respectively, while setting the lowest 32 frequencies for to zero. To evaluate the comparative effectiveness of these configurations, we conduct further experiments on LongVideoBench. Table 5: Performance comparison between HoPE-X and HoPE. Method LongVideoBench 8k 16k 32k 64k HoPE-X 52.68 54.11 HoPE 52.73 55.09 53.01 55. 46.32 51.22 Table 5 demonstrates that HoPE consistently outperforms HoPE-X across diverse context lengths. We deduce that the inferior performance of HoPE-X is due to its decreased dimensions allocated for spatial modeling. While this configuration helps to maintain semantic preference property, it negatively impacts HoPE-Xs ability to model local features. Therefore, it is necessary to keep adequate dimensions for spatial modeling in multimodal RoPE."
        },
        {
            "title": "C Limitations",
            "content": "Due to computing resources constraints, we conducted experiments only on 2B and 7B model scales. Although our experiments show that the performance gain of our proposed HoPE increases as backbone size increases from 2B to 7B, it would be interesting to investigate whether this boost scales to larger model sizes, e.g., 13B or even 72B. We plan to extend our method to larger models given more computing resources."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Xiaohongshu Inc."
    ]
}