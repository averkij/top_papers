{
    "paper_title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
    "authors": [
        "Tongyu Wen",
        "Guanting Dong",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch."
        },
        {
            "title": "Start",
            "content": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents Tongyu Wen wentongyu@ruc.edu.cn Renmin University of China Beijing, China Guanting Dong dongguanting@ruc.edu.cn Renmin University of China Beijing, China Zhicheng Dou dou@ruc.edu.cn Renmin University of China Beijing, China 6 2 0 2 J 8 ] . [ 1 8 8 8 4 0 . 1 0 6 2 : r Abstract Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents overall effectiveness. To mitigate this issue, we introduce SmartSearch, framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design three-stage curriculum learning framework. This framework guides the agent through progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch. CCS Concepts Information systems Information retrieval; Language models; Question answering. Keywords Search Agent, Information Retrieval, Large Language Models, Process Reward, Query Refinement ACM Reference Format: Tongyu Wen, Guanting Dong, and Zhicheng Dou. 2026. SmartSearch: Process Reward-Guided Query Refinement for Search Agents. In Proceedings of the 49th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 26), July 2024, 2026, Melbourne Naarm, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. SIGIR 26, Melbourne Naarm, Australia 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2026/06 https://doi.org/XXXXXXX.XXXXXXX Figure 1: An example from ASearcher [14] dataset demonstrating how low-quality intermediate search queries lead to unexpected retrieval results and derail the entire trajectory. Australia. ACM, New York, NY, USA, 16 pages. https://doi.org/XXXXXXX. XXXXXXX"
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) have shown strong performance\nacross a variety of tasks [1, 2, 6, 45, 46], including translation [58, 64],\nsummarization [41, 66], and question answering [23, 38]. However,\nchallenges remain, particularly with issues like hallucinations [17,\n35] and the absence of recent or field-specific knowledge, which\nmay result in inaccurate or outdated answers. Retrieval-augmented\ngeneration (RAG) [3, 15, 19] has been introduced to address these\nchallenges by incorporating external knowledge to complement\nthe modelâ€™s internal knowledge [36, 54]. However, static RAG faces\nlimitations in its ability to handle more complex, dynamic, and deep\nexploration tasks.",
            "content": "Recently, LLM-based search agents have proven to be promising method [4, 21, 28, 29, 39, 40]. These agents can autonomously and iteratively invoke external search tools, thereby addressing more challenging knowledge-intensive problems that demand adaptive retrieval and in-depth reasoning. Current research on search agents has made considerable progress in optimizing the reasoning paradigms of search agents through methods like prompt engineering [28] and fine-tuning [4, 21, 39, 40]. However, they often overlook SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al. the quality of intermediate search queries during reasoning, yet low-quality queries can lead to unexpected retrieval results or even derail the entire trajectory. Figure 1 illustrates how minor inaccuracies in an intermediate search query (e.g., omitting actor) can lead search agent to retrieve and accept unexpected information, ultimately resulting in an incorrect answer. This highlights the critical role that search query quality plays in the deep information-seeking process. Some studies [9, 52, 59, 67] have attempted to incorporate process rewards into search agent training. However, they tend to focus more on shaping better reasoning behavior rather than improving the quality of intermediate search queries, and existing efforts [52] on intermediate search queries remain preliminary and ineffective. Furthermore, research [20, 42] has shown that existing training paradigms often prioritize information utilization, persistently neglecting the optimization of retrieval patterns. This undoubtedly impedes the search agents ability to achieve deep and reliable information retrieval, thereby compromising its overall effectiveness. Such issues highlight the need for methods that specifically focus on optimizing query quality during training. In this work, we present SmartSearch, framework that optimizes search query quality through the guidance of process rewards, thereby enhancing the deep information-seeking capabilities of search agents. Specifically, SmartSearch incorporates two key mechanisms: (1) Process rewards: To provide fine-grained supervision for the quality of each search query, we introduce Dual-Level Credit Assessment, which comprises two complementary components. The first one is rule-based assessment for query novelty, which detects redundancy by checking whether the retrieved documents contain excessive overlap with previous rounds. The second one is model-based evaluation for query usefulness, which judges whether the query intent is necessary and whether the retrieved results provide the expected answer. This mechanism outputs both numerical scores and textual feedback, which serve as guidance for subsequent query refinement. (2) Query refinement: To further promote the optimization of query generation during training, the agent first generates complete search trajectory, then identifies low-quality search rounds according to the numerical scores from the process rewards. Subsequently, we employ model to refine those queries under the textual guidance provided by the process rewards, after which the search agent continues generating from the refined queries. To improve the efficiency of query assessment and refinement, smaller model is trained for scoring and refinement, reducing computational cost while maintaining effectiveness. Building on the foundation of the two mechanisms, we introduce three-stage curriculum learning framework. The framework guides the search agent through progression from imitation and alignment to generalization, enabling it to progressively internalize the ability to enhance query quality under the guidance of process rewards. (1) Query Quality Screened Imitation Learning: The initial stage leverages Supervised Fine-Tuning (SFT) to guide the search agent during its early learning of information retrieval and utilization. The training data is filtered based on both final answer correctness and query quality measured by the process rewards. It ensures the model to learn from trajectories that not only lead to correct answers but also maintain high-quality search processes. (2) Query Generation Alignment: In this stage, the search agent cultivates advanced query generation capabilities through Direct Preference Optimization (DPO). We employ the query refinement mechanism to generate comparative data, with process rewards and outcome rewards jointly defining which trajectories are of higher quality. (3) Query-Aware Policy Optimization: The final stage utilizes Reinforcement Learning (RL) to further strengthen its integrated capabilities of information retrieval and utilization. During the rollout phase, the query refinement mechanism is employed, with the process rewards incorporated into the reward function. To thoroughly assess the capabilities of SmartSearch, we perform experiments on four challenging knowledge-intensive tasks and two web exploration tasks. Experimental results indicate that SmartSearch consistently surpasses all baselines in overall performance and exhibits strong generalization to open-web settings. Additionally, we perform range of ablation studies and quantitative analyses to comprehensively validate SmartSearchs effectiveness. Our findings highlight the critical contribution of our two key mechanisms and three curriculum learning stages, as well as their superiority in terms of search efficiency, search query quality, and other dimensions. To summarize, the primary contributions of this study include: (1) We present pioneering focus that optimizes the quality of intermediate search queries through process reward guidance, thereby improving the information-seeking ability of search agents. (2) We propose SmartSearch, framework that incorporates two key mechanisms: process rewards and query refinement, to enable process reward-guided search refinement. (3) We design three-stage, query-oriented curriculum learning framework that guides the agent through progression from imitation and alignment to generalization, progressively internalizing the ability to improve query quality. (4) Experiments across six challenging benchmarks demonstrate that SmartSearch consistently surpasses existing baselines, and further quantitative analyses confirm significant improvements in both search efficiency and query quality."
        },
        {
            "title": "2 Related Works\n2.1 LLM-based Search Agents\nLLMs have demonstrated strong performance across various tasks\n[1, 2, 6, 45, 46], yet challenges like hallucinations [17, 35] and static\nparametric knowledge remain. Nowadays, LLM-based search agents\nhave emerged as a promising solution [4, 21, 28, 29, 39, 40]. This\nadvanced paradigm enables models to autonomously and iteratively\ninvoke external tools, effectively tackling challenging knowledge-\nintensive problems. Research on search agents has progressed\nthrough methods including prompt engineering and fine-tuning.\nEarly prompt-based methods [25, 28, 31, 48] focused on carefully\ndesigned prompts and structured workflows to steer the agentâ€™s\nbehavior. However, these methods donâ€™t fundamentally enhance\nthe modelâ€™s underlying capabilities, leading many studies to shift to-\nwards fine-tuning-based approaches. A prominent line of work has\ndemonstrated that SFT [12, 13, 18, 30] on expert trajectories enables\nagents to learn through imitation and yields promising performance.\nBuilding upon this foundation, recent studies [4, 10, 21, 39, 40] have\nemployed RL to further advance search agent capabilities. However,\nexisting methods tend to overlook intermediate search query qual-\nity, which can lead to unexpected retrieval results or even derail",
            "content": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia the entire trajectory. Moreover, research [20] indicates that current training paradigms tend to prioritize information utilization, which can lead to stagnation in information retrieval abilities. Thus, we present framework designed to optimize the quality of intermediate search queries under the guidance of process rewards, thereby enhancing the overall performance of search agents."
        },
        {
            "title": "3 Preliminaries\n3.1 Task Formulation\nWe adopt ReAct [62] as the framework for the search agent. Given a\nuser query ğ‘, the search agent, guided by an LLM policy ğœ‹ğœƒ , interacts\nwith an external search tool through several iterations of Thought-\nAction-Observation to gather information and ultimately generate\nan answer. Specifically, during each iteration, the search agent\nstarts by engaging in thinking to generate a â€œThoughtâ€ according\nto the existing context. It then produces the next â€œActionâ€, which\ninvolves querying the search tool. The agent subsequently waits\nfor the environment to return the â€œObservationâ€, consisting of the\nTop-K retrieved document fragments for the search query. The\niteration concludes when the search agent has gathered sufficient\ninformation required to address the userâ€™s question and selects the\nâ€œfinal answerâ€ as the action. A complete trajectory over ğ‘‡ iterations\nis denoted as:",
            "content": "ğ»ğ‘‡ = (ğ‘, ğœ0, ğ‘0, ğ‘œ0, . . . , ğœğ‘–, ğ‘ğ‘–, ğ‘œğ‘–, . . . , ğœğ‘‡ , ğ‘ğ‘‡ ). (1) Here, ğœğ‘– , ğ‘ğ‘– , and ğ‘œğ‘– correspond to the Thought, Action, and Observation of the ğ‘–-th iteration. In iteration ğ‘¡, the LLM policy ğœ‹ğœƒ (ğ‘, ğ‘¡ ğ»ğ‘¡ 1) produces the thought ğœğ‘¡ and action ğ‘ğ‘¡ , which is conditioned on the entire history of prior context ğ»ğ‘¡ 1."
        },
        {
            "title": "3.2 Agentic Reinforcement Learning",
            "content": "Policy Optimization. In the context of Agentic RL [65], Group Relative Policy Optimization (GRPO) [37] is typically employed for policy optimization. In our approach, we also employ GRPO during the Query Aware Policy Optimization stage, with specific focus on the augmentation of the rollout and reward modules to optimize the quality of intermediate search queries. Specifically, GRPO optimizes the policy model through maximization of the objective function below: ğ½GRPO (ğœƒ ) = E(ğ‘,ğ‘)D,{ğ‘œğ‘– }ğœ‹ğœƒ (cid:34) 1 ğº ( ğ‘) old clip (ğ‘Ÿğ‘¡ (ğœƒ ), 1 ğœ–, 1 + ğœ–) Ë†ğ´ğ‘– ğº ğ‘–=1 (cid:19) 1 ğ‘œğ‘– ğ‘œğ‘– ğ‘¡ = (cid:18) ğ‘Ÿğ‘¡ (ğœƒ ) Ë†ğ´ğ‘–, min ğ›½ ğ·KL (ğœ‹ğœƒ ğœ‹ref) (cid:35) . (2) In this formulation, for each input pair (ğ‘, ğ‘) drawn from the dataset D, ğº trajectories {ğ‘œğ‘– }ğº ( ğ‘). The importance weight ğ‘Ÿğ‘¡ (ğœƒ ) is defined as: ğ‘–=1 are generated from the old policy ğœ‹ğœƒold ğ‘Ÿğ‘¡ (ğœƒ ) = ğœ‹ğœƒ (ğ‘œğ‘–,ğ‘¡ ğ‘, ğ‘œğ‘–,<ğ‘¡ ) (ğ‘œğ‘–,ğ‘¡ ğ‘, ğ‘œğ‘–,<ğ‘¡ ) ğœ‹ğœƒold . The normalized advantage score Ë†ğ´ğ‘– is denoted as: Ë†ğ´ğ‘– = ğ‘Ÿğ‘– mean({ğ‘Ÿ ğ‘— }ğº std({ğ‘Ÿ ğ‘— }ğº ğ‘—=1) ğ‘—=1) . (3) (4) Here, ğ‘Ÿğ‘– denotes the scalar reward for the ğ‘–-th rollout. Furthermore, agentic RL typically masks observations originating from the external environment during loss computation, thereby preventing unstable training. Reward Design. As discussed earlier, in agentic RL, each rollout corresponds to scalar reward ğ‘Ÿ . Prior research [4, 21, 39, 40] predominantly relies on combining two key types of rewards: the outcome reward ğ‘Ÿoutcome, reflecting the trajectorys answer correctness, and the format reward ğ‘Ÿformat, assessing the trajectorys structural correctness. These rewards are typically weighted and combined using simple hyperparameter ğœ† as follows: ğ‘Ÿ = ğ‘Ÿoutcome + ğœ† ğ‘Ÿformat. (5) In some recent works [9, 59, 60], process rewards have been incorporated into the reward function to provide fine-grained feedback on intermediate steps. The reward function is then extended to include the process rewards, with composite reward incorporating both the outcome reward and the process rewards, while the format reward is weighted by hyperparameter: ğ‘Ÿ = ğ‘Ÿcomposite + ğœ† ğ‘Ÿformat. Here, ğ‘Ÿcomposite is computed as the aggregation of multiple step-wise process rewards and the final outcome reward ğ‘Ÿoutcome: (6) ğ‘Ÿcomposite = ğ‘“ (ğ‘Ÿ process , ğ‘Ÿ process , . . . , ğ‘Ÿ process ğ‘› 1 2 where ğ‘› represents the total steps in the trajectory, and ğ‘Ÿ process denotes the process reward for the ğ‘–-th step. The aggregation function ğ‘“ combines these individual rewards, and its specific form may vary across different works. ğ‘– , ğ‘Ÿoutcome), (7) SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al. Figure 2: An overview of the two key mechanisms in SmartSearch: the process rewards (a) and the query refinement (b)."
        },
        {
            "title": "4.2 Process Reward for Assessing Query Quality\nIn this section, we introduce the process rewards mechanism to\nassess the quality of each query, providing both numerical scores\nand textual feedback. These outputs guide the subsequent query\nrefinement, and play a key role within the three-stage curricu-\nlum learning framework by selecting trajectories with high-quality\nsearch processes and providing finer-grained supervision signals,\nwhich will be introduced later.",
            "content": "Design Principles. Our assessment of search query quality is guided by comprehensive set of three fundamental principles: Query Novelty: The query should avoid redundancy with previous queries and introduce novel information. Intent Necessity: The querys search intent must be necessary for progressing toward the final answer. Retrieval Relevance: The retrieved documents should align with the search intent, effectively containing the expected information or answer. These principles are well-motivated and collectively capture the essential aspects of high-quality query, while also being readily applicable via either rule-based checks or simple model judgments. Dual-Level Credit Assessment. We operationalize these principles through Dual-Level Credit Assessment, which consists of two complementary components. (1) Rule-based Evaluation: The first is rule-based evaluation for query novelty, which identifies redundant queries by measuring the document overlap between the current and previous search rounds. Formally, for the ğ‘¡-th step, the novelty score Snovel and its corresponding textual explanation novel are defined as: ğ‘¡ ğ‘¡ (cid:17) (cid:16) = Snovel ğ‘¡ , novel ğ‘¡ (cid:40)(0, the query is redundant), (1, the query is novel), Here, ğ¾ is threshold hyperparameter, and ğ‘‚ğ‘¡ represents the number of documents retrieved at step ğ‘¡ that share the same content with those retrieved in any previous step, defined as: if ğ‘‚ğ‘¡ > ğ¾, if ğ‘‚ğ‘¡ ğ¾ . (8) ğ‘› ğ‘‚ğ‘¡ = I(ğ·ğ‘¡ ğ‘– ğ‘¡ 1 (cid:216) ğ‘› (cid:216) ğ·ğ‘  ğ‘— ), (9) ğ‘ =0 ğ‘– refers to the ğ‘–-th document retrieved at step ğ‘¡, and I() is ğ‘—=1 ğ‘–=1 where ğ·ğ‘¡ the indicator function. Suseful ğ‘¡ (2) Model-based Evaluation: The second component is modelbased evaluation for query usefulness, which assesses the necessity of the query intent and checks whether the retrieved results provide the expected answer. For the ğ‘¡-th step, the evaluation score Suseful and its corresponding textual explanation useful = LLMeval (ğ‘, ğ‘, ğ»ğ‘¡ ), (10) where LLMeval is the model used for evaluation, ğ‘ is the users query, ğ‘ denotes the golden answer, and ğ»ğ‘¡ indicates the trajectory up to step ğ‘¡. The score Suseful is set to 1 if the query meets the criteria, and 0 otherwise, while the explanation useful is directly parsed from the models output. To enhance efficiency, we employ smaller model fine-tuned via SFT for both scoring and the subsequent query refinement task. Specifically, we input task-specific prompts into are defined as: , useful ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ SmartSearch: Process Reward-Guided Query Refinement for Search Agents SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Figure 3: The overall framework of query-oriented three-stage curriculum learning, including Query Quality Screened Imitation Learning, Query Generation Alignment, and Query Generation Alignment. more powerful teacher model and use its outputs as annotation labels. The smaller model is then trained on these prompt-output pairs, enabling it to achieve effective performance at reduced computational cost. More details about the model will be introduced in Section 5.1. Finally, the overall assessment score Sğ‘¡ and its corresponding textual explanation Tğ‘¡ are derived by aggregating the evaluations for query novelty and usefulness. The overall score is determined by logical conjunction of the component scores: Sğ‘¡ = (cid:40) 1, 0, = 1 Suseful ğ‘¡ if Snovel ğ‘¡ otherwise. = 1, (11) The final explanation is synthesized by concatenating the textual feedback from both components: Tğ‘¡ = novel ğ‘¡ useful ğ‘¡ , (12) where denotes the concatenation operator."
        },
        {
            "title": "4.3 Process Reward-Guided Query Refinement\nThis section introduces the query refinement mechanism, which\nis designed to promote the optimization of query generation. It\nis achieved by systematically identifying and refining low-quality\nqueries, then regenerating subsequent search steps from these re-\nfined points. This mechanism serves a pivotal function within the\nthree-stage curriculum learning framework by generating compar-\native data for training and acting as a rollout strategy.",
            "content": "Formally, this process can be represented as follows. The search agent starts by generating complete trajectory ğ»ğ‘‡ , represented as (ğ‘, ğœ0, ğ‘0, ğ‘œ0, . . . , ğœğ‘–, ğ‘ğ‘–, ğ‘œğ‘–, . . . , ğœğ‘‡ , ğ‘ğ‘‡ ). Each search query in this trajectory is then evaluated by the process rewards mechanism, yielding sequence of scores (S0, S1, . . . , Sğ‘‡ 1) and corresponding textual explanations (T0, T1, . . . , Tğ‘‡ 1) with Equation (11) and (12). For each low-quality query ğ‘ğ‘– where the score Sğ‘– = 0, refinement step is triggered. The refined query ğ‘ ğ‘– is generated by language model as follows: ğ‘ ğ‘– = LLMrefine (ğ‘, ğ»ğ‘–, Tğ‘– ). (13) Here, LLMrefine is the same lightweight SFT-tuned model introduced earlier, ğ‘ is the users original query, ğ»ğ‘– is the trajectory history up to step ğ‘–, and Tğ‘– is the textual feedback diagnosing the quality issue for the low-quality query ğ‘ğ‘– . The search agent subsequently regenerates the search process from this refined query ğ‘ ğ‘– , yielding ğ‘‡ , ğ‘– , . . . , ğœ ğ‘–, ğ‘œ ğ‘‡ , represented as (ğ‘, ğœ0, ğ‘0, ğ‘œ0, . . . , ğœğ‘–, ğ‘ new trajectory ğ» ğ‘ ğ‘‡ ). The primary distinction between the initial and revised trajectories originates from the refined query ğ‘ ğ‘– , resulting in different reward for ğ‘ğ‘– and ğ‘ ğ‘– , thereby promoting the optimization of query generation within the curriculum learning framework. Specifically, to enable the model to effectively refine the lowquality search query based on the textual feedback provided by the process rewards mechanism, we distill key empirical insights into set of actionable guidelines based on thorough analysis of representative cases: If the textual feedback indicates that the query is redundant or unnecessary, the refined query should serve for more necessary intent and eliminate redundancy. If the textual feedback indicates that the retrieved results do not contain the expected information, the model should strategically reformulate the query to better capture the target content. This SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al. reformulation may involve switching between complete semantic question and keyphrase-based query, or adaptively adding or removing information from the original query."
        },
        {
            "title": "4.4 Query-Oriented Training Framework\nThis section presents a three-stage curriculum learning framework\nthat integrates the two preceding mechanisms, enabling the agent\nto progressively internalize the ability to improve query quality\nthrough a progression from imitation, to alignment, and ultimately\nto generalization. The following paragraphs detail the three pro-\ngressive stages: Query Quality Screened Imitation Learning, Query\nGeneration Alignment, and Query Aware Policy Optimization.",
            "content": "Stage-1: Query Quality Screened Imitation Learning. In this stage, we employ SFT to guide the model in its initial learning of information retrieval and utilization. critical step in SFT is the selection of high-quality trajectories for training. Following common practice, we begin by selecting trajectories that yield correct final answers and adhere to the proper format, thus guiding the model towards correct patterns from the outset. However, many trajectories, despite yielding correct final answers, contain low-quality intermediate search queries. Learning from such trajectories could lead the model to pick up suboptimal behaviors, thereby impairing its overall performance. To address this, we further leverage process rewards to selectively retain only those trajectories comprised entirely of high-quality intermediate search queries, i.e., ğ‘¡ [0, . . . ,ğ‘‡ ], Sğ‘¡ = 1. This ensures that the trajectories comprising our final training dataset not only yield correct final answers but also exhibit high-quality intermediate search queries. We then apply the standard SFT objective, which is formulated as: LSFT (ğœƒ ) = E(ğ‘,ğ‘¦)D [log ğ‘ƒğœƒ (ğ‘¦ ğ‘)] , (14) where ğ‘ is the users original query, ğ‘¦ is the agents high-quality response, and ğœƒ denotes the model parameters. Stage-2: Query Generation Alignment. In this stage, the search agent cultivates advanced query generation capabilities through DPO training. Unlike common approaches that directly generate trajectories from scratch, we employ the query refinement mechanism when constructing comparative data. For each users query ğ‘, the search agent first generates an initial trajectory ğ‘¦0. Following this, each low-quality query within ğ‘¦0 is refined and the search agent regenerates subsequent search steps from the refined query, producing sequence of trajectories ğ‘¦1, . . . , ğ‘¦ğ‘›, where ğ‘› is the number of low-quality queries. This process ensures that for given input ğ‘, the key differences among the candidate trajectories ğ‘¦0, ğ‘¦1, . . . , ğ‘¦ğ‘› originate specifically from the refined queries, thereby directly promoting the optimization of query generation. Next, for each users query ğ‘, we choose one positive sample ğ‘¦ğ‘¤ and one negative sample ğ‘¦ğ‘™ among the corresponding candidate trajectories ğ‘¦0, ğ‘¦1, . . . , ğ‘¦ğ‘›. Diverging from approaches that rely only on the correctness of the final answer, our selection criteria incorporate both the final-answer correctness and the quality of intermediate search queries, guided by the following principles: trajectory with correct final answer is preferred over one with an incorrect answer. Among trajectories with correct final answers, those with fewer low-quality (i.e., Sğ‘¡ = 0) queries are preferred. Among trajectories with incorrect final answers, those containing more high-quality (i.e., Sğ‘¡ = 1) queries are preferred. We then optimize the model using the standard DPO objective: LDPO (ğœƒ ) = E(ğ‘,ğ‘¦ğ‘¤,ğ‘¦ğ‘™ )D (cid:18) (cid:20) log ğœ ğ›½ log ğœ‹ğœƒ (ğ‘¦ğ‘¤ ğ‘) ğœ‹ref (ğ‘¦ğ‘¤ ğ‘) (cid:19) (cid:21) ğ‘) ğœ‹ğœƒ (ğ‘¦ğ‘™ ğ‘) ğœ‹ref (ğ‘¦ğ‘™ ğ›½ log . (15) Here, ğ‘ is the users original query, ğ‘¦ğ‘¤ is the positive sample, ğ‘¦ğ‘™ is the negative sample, ğ›½ represents the hyperparameter, ğœ refers to the sigmoid function, ğœƒ is the model parameters, and ğœ‹ref indicates the reference model, which is initialized to ğœ‹ğœƒ and kept frozen during training. Stage-3: Query Aware Policy Optimization. In the final stage, we further enhance the search agents integrated capabilities of information retrieval and utilization through Query Aware Policy Optimization. Specifically, we train it on curated set of challenging questions that remained unresolved after multiple sampling trials. Unlike the standard GRPO algorithm that generates ğº independent trajectories from scratch, our method employs the query refinement mechanism as its rollout strategy. For each users query, the search agent first generates an initial trajectory ğ‘¦0 and then expands it into ğ‘¦0, ğ‘¦1, . . . , ğ‘¦ğ‘› through sequential refinement and regeneration. Different from the Query Generation Alignment stage, we retain at most ğ‘€ trajectories from this set to avoid too many trajectories sharing common prefix, thereby ensuring behavioral diversity and promoting the holistic improvement of the agents capabilities. If the total number of trajectories collected remains less than ğº, we repeat this generation-and-expansion process, until complete set of ğº trajectories is obtained. For reward design, we integrate process supervision into the reward function. Following Eq. (6), our reward function is: ğ‘Ÿ = ğ‘Ÿcomposite + ğœ† ğ‘Ÿformat, (16) where ğœ† is weighting coefficient, ğ‘Ÿformat {0, 1} indicates the correctness of the output format, and ğ‘Ÿcomposite defined in Eq. (7) integrates both outcome and process reward as follows: (cid:40) ğ‘Ÿcomposite = max(ğ‘Ÿoutcome ğ›¾ ğ‘›wrong, ğœ™min), min(ğ‘Ÿoutcome + ğ›¾ ğ‘›correct, ğœ™max), ğ‘Ÿoutcome = 1, ğ‘Ÿoutcome = 0. (17) Here, ğ‘Ÿoutcome {0, 1} denotes the final answers correctness, ğ‘›wrong and ğ‘›correct represent the number of low- (i.e., Sğ‘¡ = 0) and highquality (i.e., Sğ‘¡ = 1) queries respectively, ğ›¾ is scaling factor for process rewards, and ğœ™min, ğœ™max bound the influence of process rewards. This reward design incentivizes the agent not only to prioritize final answer correctness but also to refine its search process by reducing low-quality queries in successful trajectories. Moreover, even when unable to provide final correct answer, the agent is motivated to generate more high-quality queries that may progressively approach the solution. We then optimize the model using the standard GRPO objective introduced in Section 3.2. SmartSearch: Process Reward-Guided Query Refinement for Search Agents SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Table 1: Performance comparison of SmartSearch and existing approaches on four knowledge-intensive benchmarks, with bold for the best and underlined for the runner-up. Numbers in () indicate the improvement compared with the runner-up. Method 2WikiMQA HotpotQA Bamboogle Musique Average EM F1 EM EM F1 EM F1 Prompt-based Approaches Direct Inference CoT IRCoT RAG Search-o1 19.3 18.1 20.0 22.5 20. 24.7 24.9 27.2 31.4 29.4 RL Approaches with Outcome Rewards ReSearch ZeroSearch R1-Searcher Search-R1 29.4 29.2 29.8 27.3 36.7 36.5 37.1 35.5 14.6 12.8 19.3 24.3 22.0 28.5 27.5 27.0 31. 24.5 24.0 28.0 36.7 33.6 40.8 39.1 38.7 41.1 4.0 14.4 16.8 7.2 28.8 12.8 14.4 31.2 29.4 11.6 25.5 25.9 17.2 36.1 22.9 25.4 39.2 38. 2.3 2.2 5.8 4.5 5.1 10.0 10.4 8.0 9.3 7.9 7.8 12.7 12.2 12.6 17.3 18.2 16.4 16.6 EM 10.1 11.9 15.5 14.6 19. 20.2 20.4 24.0 24.5 F1 17.2 20.6 23.5 24.4 27.9 29.4 29.8 32.9 33.0 RL Approaches with Process Rewards 30.4 ReasonRag 31.2 PPR StepSearch 36.8 SmartSearch (Ours) 45.3(24%) 52.3(21%) 40.7(7%) 52.4(4%) 44.8(22%) 56.1(16%) 19.1(15%) 27.8(12%) 37.5(25%) 47.2(19%) 11.3 14.7 16. 41.7 50.3 45.9 18.6 22.0 24.9 27.6 29.4 30.1 39.1 39.4 48.4 43.2 41.8 38.9 35.7 38.4 39. 32.2 38.1 35.1 36.5 33.7 32."
        },
        {
            "title": "5 Experiments\n5.1 Experimental Setup",
            "content": "Dataset. We comprehensively assess SmartSearchs performance through experiments on two types of benchmarks: (1) knowledgeintensive tasks, including 2WikiMultihopQA [16], HotpotQA [61], Bamboogle [33], and Musique [47], and (2) web exploration tasks, including GAIA [32] and WebWalker [55]. (cid:205)ğ‘ ğ‘–=1 Metrics. For consistent comparison with previous studies, we use the widely adopted Exact March (EM) and word-level F1 score to assess the answers correctness. To assess search efficiency, we follow prior work [5] and employ the Search Efficiency metric, defined ğ¹ğ‘– as: ğ‘†ğ¸ = 1 ğ‘‡ğ‘– . Here, ğ‘ represents the dataset size, ğ¹ğ‘– denotes ğ‘ the F1 score for sample ğ‘–, and ğ‘‡ğ‘– represents the search call count for sample ğ‘–. Additionally, to assess search query quality, we introduce (cid:1) (cid:0)ğ¶perfect + ğ¶partial the Search Quality metric, defined as: ğ‘†ğ‘„ = 1 ğ‘ where ğ‘ represents the dataset size, ğ¶perfect denotes the number of samples where the final answer is correct and all intermediate search queries are of high quality, and ğ¶partial denotes the number of samples where the final answer is incorrect but the trajectory contains high-quality intermediate search queries. In particular, we ğ‘ ğ¶partial, define the Perfect Rate as 1 which contribute to the overall Search Quality metric from two different aspects. ğ‘ ğ¶perfect and the Partial Rate as 1 Table 2: Performance comparison of SmartSearch and baselines on web exploration tasks, with bold for the best. Method Search-o1 Search-R1 StepSearch SmartSearch GAIA WebWalker Average EM 4.7 6.3 9.4 13. F1 8.0 9.8 12.5 16.7 EM 6.3 7.5 9.1 11.5 F1 18.3 21.6 25.8 31. EM 5.5 6.9 9.3 12.5 F1 13.2 15.7 19.2 23.9 Implementation Details. Qwen2.5-3B-Instruct serves as the base model in SmartSearch and other baselines. For local search, we utilize the 2018 Wikipedia dump [24] provided by FlashRAG [22] as the knowledge corpus, and employ E5-base-v2 [50] as the retriever to obtain top 5 documents. For web search, the Serper API is employed to retrieve the top 10 document snippets. Our training is conducted under the LLaMA-Factory and VERL frameworks, using Asearcher-Base as the training dateset. Additionally, to improve efficiency, we train smaller student model, Qwen2.53B-Instruct, to perform scoring and query refinement, with training labels annotated by the teacher model, Qwen3-32B. Baselines. We compare SmartSearch with several representative baselines, which are classified into three categories: (1) promptbased approaches, including Direct Inference, CoT [53], IRCoT [48], RAG [27], and Search-o1 [28]. (2) RL approaches with outcome rewards, including ReSearch [4], ZeroSearch [40], R1-Searcher [39], and Search-R1 [21]. (3) RL approaches with process rewards, including PPR [59], ReasonRag [67], and StepSearch [52]."
        },
        {
            "title": "5.2 Main Result",
            "content": "Overall Performance. Table 1 presents the main results, demonstrating that SmartSearch consistently surpasses existing approaches across four datasets, and yielding several important insights. (1) Prompt-based approaches exhibit limited performance. Direct Inference and CoT, which are based entirely on the models internal knowledge, achieve an average EM of only around 10%, SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al. Table 3: Ablation study results for the two core mechanisms in SmartSearch across all three stages of curriculum learning training framework, with bold for the best results of each stage. Method Stage 1 SmartSearch w/o process rewards Stage 2 SmartSearch w/o process rewards w/o query refinement Stage 3 SmartSearch w/o process rewards w/o query refinement Standard GRPO 2WikiMQA HotpotQA Bamboogle Musique Average EM F1 EM F1 EM EM F1 EM F1 38.2 33.8 41.4 40.2 39. 45.3 43.3 44.1 43.6 45.3 40.6 48.7 47.4 46.7 52.3 50.6 51.2 50.8 35.3 32.5 37.9 36.5 35. 40.7 40.0 39.9 39.0 45.5 42.8 48.5 47.2 46.1 52.4 51.5 51.6 50.6 38.4 36.0 39.2 37.6 36. 44.8 39.2 41.6 40.8 51.0 48.7 51.8 50.1 49.6 56.1 51.6 54.2 52.5 14.7 12.6 15.4 14.4 14. 19.1 17.9 17.5 15.9 21.6 20.8 23.6 22.3 22.9 27.8 26.7 26.4 24.8 31.7 28.7 33.5 32.2 31. 37.5 35.1 35.8 34.8 40.9 38.2 43.2 41.8 41.3 47.2 45.1 45.9 44.7 highlighting the inherent challenges of LLMs, including hallucinations and static parametric knowledge. RAG and IRCoT yield gain of around 5% in average EM, demonstrating the necessity of integrating external knowledge. Among prompt-based methods, Search-o1 attains the highest performance, reaching an average EM score of 19.2%, reflecting the effectiveness of search agents. However, it still lags behind other fine-tuning-based approaches. (2) Incorporating process rewards effectively enhances RL training. While outcome-driven RL methods such as ReSearch, Search-R1, and R1-Searcher improve performance over promptbased approaches, indicating the benefits of RL for LLM-based search agents, they often remain inferior to RL approaches that integrate both outcome and process rewards by margin of around 5% in both average EM and F1 score. This underscores that reward signals based solely on final outcomes result in sparse feedback. Such sparse feedback provides insufficient guidance for intermediate steps and leads to unstable optimization, thereby highlighting the critical importance of fine-grained supervision. (3) Optimizing the quality of intermediate search queries significantly improves overall performance. Existing methods often overlook the quality of intermediate search queries, which can lead to stagnation in information retrieval abilities. By explicitly optimizing the quality of intermediate search queries under the guidance of process rewards, SmartSearch enhances the search agents overall effectiveness, achieving more than 7% improvement in both average EM and F1 score compared with other processsupervised RL methods. Generalization to Web Search Scenarios. As discussed earlier, SmartSearch is trained solely on Wikipedia-based local search. To evaluate its generalization ability to web search, we test it against several baseline models on two demanding web exploration tasks, GAIA and WebWalker. As demonstrated in Table 2, SmartSearch surpasses existing approaches across both datasets, achieving an average F1 score increase of nearly 5%. This indicates that while SmartSearch optimizes the quality of intermediate search queries Figure 4: F1 score training dynamics for different algorithms. in the local search setting, it also exhibits strong generalization capabilities in the web search environment, maintaining robust performance despite the difference in settings."
        },
        {
            "title": "5.3 Ablation Study\nTo further examine the impact of SmartSearchâ€™s two key mecha-\nnismsâ€”process rewards and query refinement, we conduct exten-\nsive ablation studies across all three training stages. The results are\nsummarized in Table 3.",
            "content": "For Stage 1, we compare our configuration with baseline that filters the training data exclusively according to whether the final answer is correct. The results indicate that incorporating queryquality filtering enables the model to achieve superior performance with only 60% of the training data, highlighting the importance of learning from trajectories with high-quality search processes. For Stage 2, we compare our method with two alternatives: (1) directly generating full trajectories without query refinement, and (2) determining preference based exclusively on the final answer SmartSearch: Process Reward-Guided Query Refinement for Search Agents SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia generates more precise queries, reducing ineffective or failed search rounds and, as result, improving overall search efficiency. Figure 5: Left: Search query quality comparison. Right: Search efficiency comparison. correctness. Ablation results underscore the essential contribution of both mechanisms in this satge, particularly the query refinement mechanism, underscoring the significance of promoting the optimization of query generation. For Stage 3, we compare our algotithm with three variants: GRPO baseline, version that only incorporates the process rewards into the reward function, and version that only applies the query refinement mechanism during rollout. As depicted in Figure 4, we demonstrate F1 score curves of various RL algorithms during training. The results demonstrate that our algorithm reliably outperforms all alternatives. Notably, integrating process rewards into the reward function yields significant gains, illustrating the crucial role of fine-grained supervision for the quality of each query."
        },
        {
            "title": "5.4 Quantitative Analyses\nTo comprehensively assess the effectiveness of the SmartSearch\nframework, we perform multiple quantitative experiments. The\nfollowing analyses demonstrate its superiority in four key aspects:\nintermediate query quality, search efficiency, the effectiveness of\nthe process reward model, and the effectiveness-efficiency trade-off.",
            "content": "Search Query Quality Analysis. To assess whether SmartSearch improves the quality of intermediate search queries, we compare the Search Quality metric across various methods. As presented in Figure 5 (a), SmartSearch achieves the highest Search Quality, with the highest values for both Perfect Rate and Partial Rate, which contribute to the overall Search Quality metric. This indicates that SmartSearch effectively enhances the quality of intermediate search queries. Specifically, the search agent reduces ineffective searches while striving to generate perfect trajectories where all queries are of high quality. Even when unable to provide final correct answer, the agent makes more attempts to generate high-quality queries that edge closer to the correct solution. Search Efficiency Analysis. The results in previous sections have shown that SmartSearch outperforms all baselines in accuracy. We now further evaluate whether it also achieves superior search efficiency. To this end, we compare the search efficiency metrics across multiple methods, and as shown in Figure 5 (b), SmartSearch outperforms all other methods in this regard. This suggests that by optimizing the quality of intermediate search queries, SmartSearch Figure 6: Left: Overlap between the scores assigned to queries by the student model, teacher model, and human annotations. Right: Effectiveness and efficiency tradeoff in SmartSearch. Process Reward Model Analysis. The process reward model plays crucial role in our approach by providing fine-grained supervision for the quality of each query and guiding subsequent query refinement. To assess the effectiveness of our process reward model, we randomly choose 100 trajectories. For each search query in these trajectories, we compare the scores annotated by the teacher model, the student model, and human annotators. Figure 6 (a) illustrates the overlap between the scores assigned to each intermediate search query by these three sources. The results reveal that the teacher model achieves nearly 90% overlap with human annotations, demonstrating its effectiveness in labeling the training data. After training, the student model achieves over 85% overlap with the teacher model, indicating the effectiveness of our finetuning. Finally, the student model shows over 80% overlap with human annotations, result that is entirely acceptable, striking good balance between scoring accuracy and efficiency. Effectiveness-Efficiency Trade-off. To validate the suitability of the lightweight student model as both LLMeval and LLMrefine, we conduct experiments using more powerful teacher model in these roles. In this experiment, effectiveness is defined as the average F1 score, while efficiency refers to the average time (s) required to apply the process rewards and query refinement mechanisms to each sample. As shown in Figure 6 (b), using more powerful model as both LLMeval and LLMrefine indeed improves the agents effectiveness, underscoring the importance of the process rewards and query refinement mechanisms within our training framework. However, this gain in average F1 score is modest, remaining below 1%, whereas the time required to process each sample increases by nearly five times. This result demonstrates clear trade-off between effectiveness and efficiency, and the decision to use the lightweight student model as both LLMeval and LLMrefine proves to be sensible one. This choice achieves an optimal balance between effectiveness and efficiency, ensuring effective query optimization while avoiding excessive computational costs. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al."
        },
        {
            "title": "6 Conclusion\nIn this work, we introduce SmartSearch, a framework designed to\noptimize the quality of intermediate search queries through two\nkey mechanisms: (1) Process rewards, which provide fine-grained\nsupervision for the quality of each query through Dual-Level As-\nsessment. (2) Query refinement, which promotes the optimization\nof query generation by selectively refining low-quality queries and\nregenerating subsequent search rounds from these refined points.\nBuilding on the foundation of the two mechanisms, we design a\nthree-stage curriculum learning framework that guides the agent\nthrough a progression from imitation and alignment to generaliza-\ntion, enabling it to progressively internalize the ability to enhance\nquery quality. Experiments across four challenging benchmarks\ndemonstrate that SmartSearch consistently surpasses existing base-\nlines, with further quantitative analyses confirming significant gain\nin both search efficiency and query quality.",
            "content": "References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 18771901. [3] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1775417762. [4] Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, et al. 2025. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470 (2025). [5] Yifei Chen, Guanting Dong, and Zhicheng Dou. 2025. Toward Effective ToolIntegrated Reasoning via Self-Evolved Preference Learning. arXiv preprint arXiv:2509.23285 (2025). [6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1113. [7] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. [n. d.]. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training. In Forty-second International Conference on Machine Learning. [8] Tri Dao. 2024. FLASHATTENTION-2: FASTER ATTENTION WITH BETTER PARALLELISM AND WORK PARTITIONING. In 12th International Conference on Learning Representations, ICLR 2024. [9] Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, et al. 2025. Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward. arXiv preprint arXiv:2508.12800 (2025). [10] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning. arXiv preprint arXiv:2505.16410 (2025). [11] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. 2025. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 (2025). [12] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Ji-Rong Wen, and Zhicheng Dou. 2025. Understand what LLM needs: Dual preference alignment for retrieval-augmented generation. In Proceedings of the ACM on Web Conference 2025. 42064225. [13] Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, et al. 2025. Cognitive kernel-pro: framework for deep research agents and agent foundation models training. arXiv preprint arXiv:2508.00414 (2025). [14] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. [n. d.]. Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL. In First Workshop on Multi-Turn Interactions in Large Language Models. [15] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997 (2023). [16] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics. 66096625. [17] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 138. [18] Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. 2025. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 70647074. [19] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning. CoRR (2025). [20] Yi Jiang, Lei Shen, Lujie Niu, Sendong Zhao, Wenbo Su, and Bo Zheng. 2025. QAgent: modular Search Agent with Interactive Query Understanding. arXiv preprint arXiv:2510.08383 (2025). [21] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. [n. d.]. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning. ([n. d.]). [22] Jiajie Jin, Yutao Zhu, Zhicheng Dou, Guanting Dong, Xinyu Yang, Chenghao Zhang, Tong Zhao, Zhao Yang, and Ji-Rong Wen. 2025. Flashrag: modular toolkit for efficient retrieval-augmented generation research. In Companion Proceedings of the ACM on Web Conference 2025. 737740. [23] Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and Davood Rafiei. 2023. Evaluating Open-Domain Question Answering in the Era of Large Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 55915606. [24] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.. In EMNLP (1). 67696781. [25] Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. 2023. Instructerc: Reforming emotion recognition in conversation with retrieval multi-task llms framework. CoRR (2023). [26] Yongqi Leng, Yikun Lei, Xikai Liu, Meizhi Zhong, Bojian Xiong, Yurong Zhang, Yan Gao, Yao Hu, Deyi Xiong, et al. 2025. DecEx-RAG: Boosting Agentic RetrievalAugmented Generation with Decision and Execution Optimization via Process Supervision. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track. 14121425. [27] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems 33 (2020), 94599474. [28] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366 (2025). [29] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yongkang Wu, Ji-Rong Wen, Yutao Zhu, and Zhicheng Dou. 2025. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776 (2025). [30] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Ye Qi, and Zhicheng Dou. 2025. Retrollm: Empowering large language models to retrieve fine-grained evidence within generation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1675416779. [31] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. [n. d.]. OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning. In ICLR 2025 Workshop on Foundation Models in the Wild. [32] GrÃ©goire Mialon, ClÃ©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. [n. d.]. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. [33] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023. 56875711. [34] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 35053506. [35] Vipula Rawte, Amit Sheth, and Amitava Das. 2023. survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922 (2023). [36] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in SmartSearch: Process Reward-Guided Query Refinement for Search Agents SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Neural Information Processing Systems 36 (2023), 6853968551. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [38] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. 2023. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617 (2023). [39] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. CoRR (2025). [40] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. 2025. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588 (2025). [41] Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan Nestor, Ali Soroush, Pierre Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin Rousseau, et al. 2023. Evaluating large language models on medical evidence summarization. NPJ digital medicine 6, 1 (2023), 158. [42] Zhengwei Tao, Haiyang Shen, Baixuan Li, Wenbiao Yin, Jialong Wu, Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Liwen Zhang, et al. 2025. Webleaper: Empowering efficiency and efficacy in webagent via enabling info-rich seeking. arXiv preprint arXiv:2510.24697 (2025). [43] Kimi Team. [n. d.]. KIMI K2: OPEN AGENTIC INTELLIGENCE. ([n. d.]). [44] Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face (2024). [45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [47] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question Composition. Transactions of the Association for Computational Linguistics 10 (2022), 539554. [48] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers). 1001410037. [49] Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, and Zhenzhe Ying. 2025. Information Gain-based Policy Optimization: Simple and Effective Approach for Multi-Turn LLM Agents. arXiv preprint arXiv:2510.14967 (2025). [50] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022). [51] Yiding Wang, Zhepei Wei, Xinyu Zhu, and Yu Meng. 2025. Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents. arXiv preprint arXiv:2510.04695 (2025). [52] Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. 2025. StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization. arXiv preprint arXiv:2505.15107 (2025). [53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [54] Tongyu Wen, Chenglong Wang, Xiyuan Yang, Haoyu Tang, Yueqi Xie, Lingjuan Lyu, Zhicheng Dou, and Fangzhao Wu. 2025. Defending against Indirect Prompt Injection by Instruction Detection. CoRR abs/2505.06311 (2025). arXiv:2505.06311 doi:10.48550/ARXIV.2505. [55] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. [n. d.]. WebWalker: Benchmarking LLMs in Web Traversal. In Workshop on Reasoning and Planning for Large Language Models. [56] Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Kaiyu He, Xinya Du, and Zhiyu Chen. 2025. HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation. arXiv preprint arXiv:2510.07794 (2025). [57] Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, et al. 2025. Rag-gym: Optimizing reasoning and search agents with process supervision. arXiv preprint arXiv:2502.13957 (2025). [58] Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. paradigm shift in machine translation: Boosting translation performance of large language models. arXiv preprint arXiv:2309.11674 (2023). [59] Peiran Xu, Zhuohao Li, Xiaoying Xing, Guannan Zhang, Debiao Li, and Kunyu Shi. 2025. Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks. arXiv preprint arXiv:2509.25598 (2025). [60] Zhichao Xu, Zongyu Wu, Yun Zhou, Aosong Feng, Kang Zhou, Sangmin Woo, Kiran Ramnath, Yijun Tian, Xuan Qi, Weikang Qiu, et al. 2025. Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation. arXiv preprint arXiv:2510.13272 (2025). [61] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing. 23692380. [62] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. [63] Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, Yang Katie Zhao, and Mingyi Hong. 2025. Reinforcing multi-turn reasoning in llm agents via turn-level credit assignment. In ICML 2025 Workshop on Computer Use Agents. [64] Biao Zhang, Barry Haddow, and Alexandra Birch. 2023. Prompting large language model for machine translation: case study. In International Conference on Machine Learning. PMLR, 4109241110. [65] Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. 2025. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547 (2025). [66] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori Hashimoto. 2024. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics 12 (2024), 3957. [67] Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, et al. 2025. Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning. arXiv preprint arXiv:2505.14069 (2025). [68] Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, and Limin Liu. 2025. R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning. arXiv preprint arXiv:2506.04185 (2025)."
        },
        {
            "title": "A Prompt Templates",
            "content": "Prompt for SmartSearch. You are helpful assistant that can solve the given question step by step with the help of the wikipedia search tool. Given question, you need to first think about the reasoning process in the mind and then provide the answer. During thinking, you can invoke the wikipedia search tool to search for fact information about specific topics if needed. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags respectively, and the search query and result are enclosed within <search> </search> and <result> </result> tags respectively. For example, <think> This is the reasoning process. </think> <search> search query here </search> <result> search result here </result> <think> This is the reasoning process. </think> <answer> The final answer is answer here </answer>. In the last part of the answer, the final exact answer is enclosed within boxed{ } with latex format. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al. Prompt for scoring. Prompt for Refining. You are query-evaluation assistant. Your task is to assess the quality of search agents query of the current search round according to the users question, the golden answer and the agents search process up to the current search round. If the agents query intent of the current search round is necessary , and the corresponding query result includes the answer for the query, the score for query should be 1. Otherwise, the score for the query should be 0. The details of the assessment are in the Evaluation Guideline, please read it carefully. ### Users question {question} ### Golden answer {answer} ### Agents search process up to the current search round {context} ### Evaluation Guideline 1. Identify the agents query intent of the current search round accurately. 2. The query result doesnt need to solve the users question directly; but it must include the information that address the agents query intent completely, related entities alone is not enough. 3. The intended entity and the one found in the query result must be exactly the same, otherwise, the score should be 0. ### Output Format: <answer> score for the query </answer> <explanation> explanation for the score </explanation> You are query-refine assistant. Your task is to refine search agents query of the current search round within <search> </search> according to the users question, the agents search process up to the current search round and the issues of the query. The details of the refinement are in the Refine Guideline, please read it carefully. ### Users question {question} ### Agents search process up to the current search round {context} ### Issues of the query {explanation} ### Refine Guideline 1. The refined query is meant to replace the query of the current round, so dont rely on any query result within <result> </result> from the current round when refining the query. 2. If the issues of the query indicate that the query intent is unreasonable, the refined query should serve for more necessary and actionable query intent. 3. The refined query can be expressed as complete semantic question or keyphrase-based query, and you may add or remove information from the original query. All depends on which option best serves the agents query intent, ensuring that the query result contains the answer to the agents query intent. ### Output format: <search> refined query </search> <explanation> explanation for the refined query </explanation> Implementation Details In the Query Quality Screened Imitation Learning stage, we employ ARPO-14B [11] as the policy model for trajectory sampling. The trajectories obtained through this sampling process are then used to fine-tune the Qwen2.5-3B-Instruct model through SFT, resulting in the SFT model. The training is conducted with learning rate of 7e-6 over total of 3 epochs, and the maximum input length during training is set to 16384 tokens. We utilize DeepSpeed ZeRO-3 [34] and FlashAttention2 [8] to accelerate training, with total batch size of 64 and applying BF16 precision. In the Query Generation Alignment stage, we perform DPO training using trajectories generated by the SFT model as positive and negative samples, resulting in the DPO model. This process involves LoRA fine-tuning with learning rate of 7e-6 trained for 3 epochs, and the maximum input length during training is set to 10000 tokens. As in the previous stage, we leverage DeepSpeed ZeRO-3 and FlashAttention2 for efficient training, with total batch size of 32 and BF16 precision. In the Query Aware Policy Optimization stage, we focus on curated set of challenging questions that remained unresolved after four sampling trials. Through RL, the DPO model is further optimized to produce the final SmartSearch model. The training is SmartSearch: Process Reward-Guided Query Refinement for Search Agents SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia conducted with learning rate of 1e-6, where each sample undergoes 8 rollouts to explore different trajectories. The total training batch size is 64, with PPO mini-batch size of 16. We set the maximum output length to 8192 tokens and limit the number of tool calls to 5 during each rollout. During the Inference stage, we set the maximum output length to 16384 tokens and limit the number of tool calls to 10. Case Study To better demonstrate the effectiveness of SmartSearch, as well as the process reward and query refinement mechanisms, we conducted case study. As shown in Table 4, when handling users question, SmartSearch first comprehends the question and performs preliminary planning to form an accurate search intent. Based on this intent, the model formulate precise search query and successfully retrieves the desired information. It is also noteworthy that our model effectively utilizes its internal knowledge, generating only necessary search queries, which further enhances the search efficiency of SmartSearch. Table 5 illustrates how process reward mechanism provides both numerical scores and textual explanations for each search query within the models output. The scores accurately reflect the quality of the search queries, while the explanations offer detailed feedback. For low-quality queries, the explanations clearly identify the reasons for their subpar performance, providing crucial guidance for subsequent refinement. As presented in Table 6, query refinement mechanism refines the low-quality queries based on the explanations provided by the process reward and regenerates the subsequent steps from the refined query. The initial query fails to retrieve the expected information, causing the entire trajectory to deviate from the correct path. In contrast, the refined query successfully retrieves the desired information and leads to the correct answer. The comparison between the two trajectories helps the model better concentrate its optimization on search queries, highlighting the effectiveness of this mechanism. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al. Table 4: Example of SmartSearch output, including the question, golden answer, and model output. Example of SmartSearch Output Question: Douglas D. Scott is an American archaeologist most notable for his work at the site of battle that occurred in what year? Golden Answer: 1876 Model Output: <think> Okay, so need to figure out the year of the battle that Douglas D. Scott worked on. Let me start by recalling what know about Douglas D. Scott. Hes an American archaeologist, so maybe hes known for excavating significant site. The question mentions battle site, so should look for his notable archaeological sites related to battles. First, Ill try to search for Douglas D. Scotts main archaeological sites. Maybe hes famous for specific battle site. Let me start with general search query. </think> <search> Douglas D. Scott notable archaeological sites </search> <result> result: Douglas D. Scott by Ronald K. Wetherington and Frances Levine, pp 134152, University of Oklahoma Press, Norman. Douglas D. Scott Douglas D. Scott is an American archaeologist most notable for his work at the Little Bighorn in the mid-1980s. Working with Richard Fox, Melissa Connor, Doug Harmon, and staff and volunteers from the National Park Service, Scott worked to sketch out field methodology that has enabled archaeologists to systematically investigate battlefields. This work is internationally recognized as constituting great step forward in our ability to interpret battlefields archaeologically, regardless of the extent of the historical record. At the Little Bighorn, the ... </result> <think> Hmm, looking at the search results, the first entry mentions Douglas D. Scotts work at the Little Bighorn in the mid-1980s. The other entries dont seem to mention battle site. The Little Bighorn is well-known battle, so thats probably the one. The question asks for the year of the battle. The Little Bighorn was major battle during the American Indian Wars, specifically between the Lakota, Northern Cheyenne, and Arapaho tribes against the U.S. Army. The Battle of the Little Bighorn took place on June 25, 1876. Let me confirm the exact year. The search result says mid-1980s for his work at Little Bighorn. Since the battle itself was in 1876, the year would be 1876. dont see any conflicting information here. The other sites mentioned are not battles but other archaeological sites. So the answer should be 1876. </think> <answer> The final answer is 1876 . </answer> SmartSearch: Process Reward-Guided Query Refinement for Search Agents SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Table 5: Example of process reward, including the question, golden answer, model output, and process reward for each query within the model output. Example of Process Reward Question: An Annapolis Story stars which American stage, film, and television actor born on February 15, 1914? Golden Answer: Kevin McCarthy Model Output: <think> To solve this question, need to find out which American stage, film, and television actor born on February 15, 1914, stars in the movie An Annapolis Story. Step 1: Identify the actor who stars in An Annapolis Story. Step 2: Determine which of that actors birthdate matches February 15, 1914. Lets start with Step 1: Step 1: Identify the actor who stars in An Annapolis Story. Ill use query to search for information about the stars of An Annapolis Story: </think> <search> stars in An Annapolis Story </search> <result> result:An Annapolis Story An Annapolis Story An Annapolis Story (alternative titles: The Blue and Gold and Navy Air Patrol) is 1955 American drama film directed by Don Siegel and starring John Derek, Diana Lynn and Kevin McCarthy.A product of the newly formed Allied Artists company, An Annapolis Story, despite having the Siegel Touch, suffered from its low budget. Brothers Tony (John Derek) and Jim Scott (Kevin McCarthy) enroll as midshipmen at the United States Naval Academy in Annapolis. Jim, the older one, looks after the more impulsive Tony and helps him pass difficult test so he can play football in the ... </result> <think> From the search results, can see that An Annapolis Story is 1955 American drama film starring John Derek, Diana Lynn, and Kevin McCarthy. Step 2: Determine which of that actors birthdate matches February 15, 1914. Ill use query to search for information about the birthdate of Kevin McCarthy: </think> <search> birthdate of Kevin McCarthy </search> <result> result: Kevin McCarthy (California politician) Kevin McCarthy (California politician) Kevin Owen McCarthy (born January 26, 1965) is an American politician serving as the House Majority Leader since 2014 and U.S. Representative for Californias 23rd congressional district since 2007. The 23rd district, numbered as the 22nd district from 2007 to 2013, is based in Bakersfield and includes large sections of Kern County and Tulare County as well as part of the Quartz Hill neighborhood in northwest Los Angeles County. member of the Republican Party, he was formerly chairman of the California Young Republicans and the Young Republican National Federation. McCarthy worked as district director for ... </result> <think> From the search results, can see that Kevin McCarthy was born on January 26, 1965. Therefore, the American stage, film, and television actor born on February 15, 1914, who stars in An Annapolis Story is not Kevin McCarthy. However, the question might be referring to another actor. ... </think> Process Reward: Score1: 1 Explanation1: The search intent is necessary, and the search results included the cast list for the movie An Annapolis Story. Score2: 0 Explanation2: The search intent is necessary, but the search results did not include the birth date of actor Kevin McCarthy. Instead, they contained information about politician Kevin McCarthy. Therefore, the score is 0. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Trovato et al. Table 6: Example of query refinement, including the question, golden answer, model output, refined query, and the regenerated subsequent steps. Example of Query Refinement Question: An Annapolis Story stars which American stage, film, and television actor born on February 15, 1914? Golden Answer: Kevin McCarthy Original Model Output: <think> To solve this question, need to find out which American stage, film, and television actor born on February 15, 1914, stars in the movie An Annapolis Story. Step 1: Identify the actor who stars in An Annapolis Story. Step 2: Determine which of that actors birthdate matches February 15, 1914. Lets start with Step 1: Step 1: Identify the actor who stars in An Annapolis Story. Ill use query to search for information about the stars of An Annapolis Story: </think> <search> stars in An Annapolis Story </search> <result> result: An Annapolis Story An Annapolis Story An Annapolis Story (alternative titles: The Blue and Gold and Navy Air Patrol) is 1955 American drama film directed by Don Siegel and starring John Derek, Diana Lynn and Kevin McCarthy.A product of the newly formed Allied Artists company, An Annapolis Story, despite having the Siegel Touch, suffered from its low budget. Brothers Tony (John Derek) and Jim Scott (Kevin McCarthy) enroll as midshipmen at the United States Naval Academy in Annapolis. Jim, the older one, looks after the more impulsive Tony and helps him pass difficult test so he can play football in the ... </result> <think> From the search results, can see that An Annapolis Story is 1955 American drama film starring John Derek, Diana Lynn, and Kevin McCarthy. Step 2: Determine which of that actors birthdate matches February 15, 1914. Ill use query to search for information about the birthdate of Kevin McCarthy: </think> <search> birthdate of Kevin McCarthy </search> <result> result: Kevin McCarthy (California politician) Kevin McCarthy (California politician) Kevin Owen McCarthy (born January 26, 1965) is an American politician serving as the House Majority Leader since 2014 and U.S. Representative for Californias 23rd congressional district since 2007. The 23rd district, numbered as the 22nd district from 2007 to 2013, is based in Bakersfield and includes large sections of Kern County and Tulare County as well as part of the Quartz Hill neighborhood in northwest Los Angeles County. member of the Republican Party, he was formerly chairman of the California Young Republicans and the Young Republican National Federation. McCarthy worked as district director for ... </result> <think> From the search results, can see that Kevin McCarthy was born on January 26, 1965. Therefore, the American stage, film, and television actor born on February 15, 1914, who stars in An Annapolis Story is not Kevin McCarthy. However, the question might be referring to another actor. ... </think> Query Refinement <search> birthdate of Kevin McCarthy </search> <search> birthdate of Actor Kevin McCarthy </search> Regenerate Subsequent Steps <search> birthdate of Actor Kevin McCarthy </search> <result> result: Kevin McCarthy (actor) Kevin McCarthy (actor) Kevin McCarthy (February 15, 1914 September 11, 2010) was an American actor who gave over 200 television and film performances. He is best remembered for portraying the male lead in the horror science fiction film Invasion of the Body Snatchers (1956). Following several television guest roles, McCarthy gave his first credited film performance in Death of Salesman (1951), portraying Biff Loman to Fredric Marchs Willy Loman. The role earned him Golden Globe Award and nomination for the Academy Award for Best Supporting Actor. McCarthy was born in Seattle, Washington, the son of Roy ... </result> <think> The search results confirm that Kevin McCarthy was an American actor born on February 15, 1914. He starred in An Annapolis Story (1955) and had long career in film, television, and stage. The question asks for an actor who starred in An Annapolis Story and was born on February 15, 1914. Kevin McCarthy fits all these criteria. </think> <answer> ğ¾ğ‘’ğ‘£ğ‘–ğ‘› ğ‘€ğ‘ğ¶ğ‘ğ‘Ÿğ‘¡â„ğ‘¦ </answer>"
        }
    ],
    "affiliations": [
        "Renmin University of China"
    ]
}