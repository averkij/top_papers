{
    "paper_title": "TimeBill: Time-Budgeted Inference for Large Language Models",
    "authors": [
        "Qi Fan",
        "An Zou",
        "Yehan Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies."
        },
        {
            "title": "Start",
            "content": "TimeBill: Time-Budgeted Inference for Large Language Models Qi Fan, An Zou, Yehan Ma* Shanghai Jiao Tong University {fanqi666, an.zou, yehanma}@sjtu.edu.cn 5 2 0 2 6 2 ] . [ 1 9 5 8 1 2 . 2 1 5 2 : r Abstract Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or drop in response performance. In this paper, we propose TimeBill, novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the endto-end execution time of LLMs. Following this, we develop time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies."
        },
        {
            "title": "Introduction",
            "content": "With the development of Large Language Models (LLMs), LLMs have been widely applied in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation. For instance, Autoware.Flex (Song et al. 2024) utilizes LLMs to translate natural language instructions into format that autonomous driving systems can understand during the driving process. DriveGPT4 (Xu et al. 2024) employs LLMs to perceive the driving environment and generate driving decisions along with corresponding explanations. In these scenarios, hard or firm deadlines may be introduced, especially when LLMs are involved in decision-making, control, or safety-critical tasks. The inference of LLMs should be completed within specific time budget while ensuring the performance of the response. Therefore, it is essential to model the end-to-end *Corresponding author Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: An example of different inference strategies. The vanilla inference may overrun and miss the deadline, resulting in incomplete output. The As-Fast-As-Possible (AFAP) strategy will degrade the response performance, while timebudgeted inference improves the response performance under timing constraints. execution time of LLM inference and balance between inference efficiency and response performance under the given time budget. However, applying LLMs in hard real-time systems faces numerous challenges. Unlike Convolutional Neural Networks (CNNs), LLMs exhibit significant uncertainty in the end-to-end execution time due to the auto-regressive generation process (Brown et al. 2020). Since the end-to-end execution time of LLMs is closely related to the number of generated tokens, namely response length, accurate modeling requires fine-grained prediction of response length. Several predictors rely on coarse-grained classification (Qiu et al. 2024; Jin et al. 2023), further increasing the error in modeling. Moreover, the response length is influenced by series of factors such as input content and the LLM itself (Perez-Ramirez, Kostic, and Boman 2025). Different input content can lead to various response lengths, while even with the same input, different LLMs may generate responses with diverse lengths. Therefore, fine-grained predictor well-aligned with the target LLM to be deployed is crucial for accurately modeling the end-to-end execution time of LLMs. Existing efficient inference methods for LLMs can be categorized into two types: offline and online. Offline efficient inference methods, including quantization (Xiao et al. 2023; Lin et al. 2024; Frantar et al. 2023) and pruning (Frantar and Alistarh 2023; Ma, Fang, and Wang 2023), compress the model before deployment to reduce resource consumption. However, they cannot adjust according to the time budget at runtime. Online methods are orthogonal to the offline ones, involving key-value (KV) cache eviction (Xiao et al. 2024; Li et al. 2024a; Xiao et al. 2025) and quantization (Hooper et al. 2024; Liu et al. 2024). Furthermore, the impact of KV cache eviction varies across different tasks (Li et al. 2024a), and different tasks may also have different time budgets. fixed KV cache eviction ratio lacks the flexibility to handle such diverse scenarios. For instance, higher ratio helps meet time budgets but harms response performance, while lower ratio results in overruns. Therefore, it is vital to develop an efficient inference method that dynamically adjusts the KV cache eviction ratio based on the time budget, enabling LLMs to complete inference on time while maintaining response performance. In this paper, we propose TimeBill, time-budgeted inference framework for LLMs. As shown in Fig. 1, different from the As-Fast-As-Possible (AFAP) strategy, TimeBill balances inference efficiency and response performance. Beginning with presenting the problem formulation of timebudgeted inference for LLMs, we introduce fine-grained response length predictor (RLP) and workload-guided execution time estimator (ETE) to accurately predict the endto-end execution time of LLMs. Furthermore, we develop time-budgeted efficient inference method, which adapts the KV cache eviction ratio based on the execution time prediction and the time budget. Finally, we conduct series of experiments and demonstrate the effectiveness of the TimeBill framework. The contributions are fourfold. We present the problem formulation of time-budgeted inference for LLMs and introduce novel framework named TimeBill, which balances the timing performance of inference and response performance. We construct fine-grained response length predictor (RLP), providing precise response length prediction of the target LLM to be deployed. We propose workload-guided execution time estimator (ETE) with analytical modeling and profiling integrated, offering accurate end-to-end execution time estimation. We develop time-budgeted efficient inference mechanism, which effectively adjusts the KV cache eviction ratio based on the execution time prediction and time budget, thereby improving task completion rate and maintaining response performance."
        },
        {
            "title": "2.1 Execution Time Estimation\nRecently, real-time inference has been studied in determin-\nistic DNNs, ensuring strict time constraints in time-critical\napplications. Chen et al. (Chen et al. 2024) map the fully-\nconnected DNNs to a segmented task model on heteroge-\nneous platforms. Kang et al. (Kang et al. 2021) establish re-\nsponse time analyses of layered DNNs and introduce quanti-\nzation and runtime layer migration to reduce inference time.\nHowever, unlike the fixed-structure deterministic DNN, the\nend-to-end execution time of LLMs exhibits uncertainty due",
            "content": "to the auto-regressive generation (Brown et al. 2020), which makes the execution time dependent on the dynamic response lengths. series of response length predictors have been proposed. For instance, PiA (Zheng et al. 2023) uses fine-tuning or prompt engineering to enable the target LLM to predict its response length before answering the question. ProxyModel (Qiu et al. 2024) builds 5-class classifier based on BERT (Devlin et al. 2019) to predict which bucket the response length will fall into. S3 (Jin et al. 2023) uses DistilBERT (Sanh et al. 2019) to construct 10-class classifier. However, BERT-based predictors struggle with long input content. On the other hand, coarse-grained predictors cannot provide precise predictions for response time estimation. Therefore, fine-grained response length predictor that can handle long input sequences is crucial to perform accurate response time prediction. In addition, some works explore predicting execution time based on the execution characteristics of LLMs. For example, RLM-ML (Imai et al. 2024) and LLMStation (He et al. 2025) combine the roofline model with machine learning to predict execution time through data collection and optimization. BestServe (Hu et al. 2025) uses an adaptive roofline model for prediction. However, ML-based execution time prediction methods lack interpretability and are not friendly to online prediction."
        },
        {
            "title": "2.2 Efficient LLM Inference",
            "content": "Due to the high computational resource usage and inference latency, LLMs face challenges in real-time applications. To this end, series of efficient inference methods for LLMs have been proposed, which are mainly divided into two categories: offline and online methods. Offline methods compress the model before deployment for lower resource consumption and time cost during inference. For example, SmoothQuant (Xiao et al. 2023) quantizes both weights and activations, while AWQ (Lin et al. 2024) and GPTQ (Frantar et al. 2023) perform weight-only quantization. In addition, SparseGPT (Frantar and Alistarh 2023) and LLM-Pruner (Ma, Fang, and Wang 2023) apply pruning to the model weights. However, when facing varying time costs due to the dynamic response time during runtime, these methods are unable to adjust according to the time budgets. Online methods achieve efficient inference primarily through runtime eviction and quantization of the key-value (KV) cache. For instance, StreamingLLM (Xiao et al. 2024), SnapKV (Li et al. 2024a), and DuoAttention (Xiao et al. 2025) discard less important parts of the KV cache. Meanwhile, KVQuant (Hooper et al. 2024) and KIVI (Liu et al. 2024) quantize the KV cache to 4 bits or lower. Although online methods allow runtime adjustments, they overlook the time budgets, which may lead to overruns or inaccurate responses. Therefore, an efficient inference method that accounts for the time budget while maintaining the response performance is essential."
        },
        {
            "title": "3.1 Time-Budgeted Inference Problem for LLMs\nThe inference process of LLMs consists of two phases,\nnamely the prefill phase and the decoding phase (Zhong\net al. 2024). During the prefill phase, LLMs process the in-\nput prompt x and produce the first output token ˆy0. LLMs\nperform autoregressive generation in the subsequent decod-\ning phase, which comprises N −1 sequential decoding steps,\nas shown in Fig. 2. In each decoding step, LLMs generate a\nnew output token ˆyi based on the previously generated to-\nkens, where i ∈ [1, N − 1] is the index of the decoding step.\nIn time-critical systems with hard deadline constraints\n(hard real-time systems), inference that exceeds the time\nbudget T is considered a system failure (Goh and Ander-\nson 2024). Therefore, the goal of time-budgeted inference\nfor LLMs is to optimize the response performance while en-\nsuring that inference completes within the time budget. To\nbe more specific, the time-budgeted inference for LLMs can\nbe formulated as",
            "content": "max θ s.t. M(ˆy(θ), y) te2e(x, θ) Nmax. (1a) (1b) (1c) Given LLMs, the execution time and response performance of generating ˆyi are affected by series of configuration factors θ in the decoding phase, such as the KV cache eviction ratio α. The objective in (1a) is to configure θ at runtime for each LLM inference in order to optimize the response performance metric M() of generated response content ˆy(θ) = (ˆy0, ˆy1, . . . , ˆyi, . . . , ˆyN 1) compared with the ground truth = (y0, . . . , yN 1). And the end-to-end execution time is te2e that should stay within the given time budget , which is described as constraint (1b). The inference process completes when termination token is generated or the maximum generation length Nmax is reached, i.e., Nmax in (1c)."
        },
        {
            "title": "3.2 TimeBill Framework\nChallenges of the time-budgeted inference problem for\nLLMs described in Prob. (1) are:\nChallenge 1 [Run-time execution time te2e Estimation]:\nte2e largely depends on the response length N , which can-\nnot be established until the inference is done. Therefore, the\nrun-time response length prediction is the first challenge. In\naddition, given the response length prediction, how to accu-\nrately estimate te2e is another challenge.\nChallenge 2 [Run-time LLM configuration θ]: How to map\nthe execution time estimation and decoding phase configu-\nration θ is the first challenge. Furthermore, how to configure\nthe θ at runtime to optimize the response performance M(·)\nwithin a certain time budget T is another challenge.",
            "content": "We propose the TimeBill framework to address the above challenges. The overview of the TimeBill framework is presented in Fig. 2. 1 The fine-grained RLP based on Small Language Model (SLM) is proposed to predict response lengths of the target LLM inference, which will be Figure 2: The overview of the TimeBill framework. introduced in Sec. 4.1. 2 Given the predicted response length, the workload-guided ETE is constructed by integrating FLOPs analysis and profiling to offer accurate endto-end execution time estimation, which will be introduced in Sec. 4.2. 3 Given the time budget and estimated execution time for each LLM inference, we develop timebudgeted efficient inference mechanism based on the KV cache eviction. The mechanism establishes the optimal KV cache eviction ratio to maximize the response performance M() within the time budget, which will be described in Sec. 5."
        },
        {
            "title": "4 Fine-grained Execution Time Prediction",
            "content": "for LLM According to Challenge 1 mentioned in Sec. 3.2, the accurate end-to-end execution time prediction is the prerequisite for time-budgeted LLM inference. This section introduces the fine-grained RLP to predict the response length , and the workload-guided ETE to estimate the end-to-end execution time te2e."
        },
        {
            "title": "4.1 Fine-grained Response Length Predictor\nPredictor Design To provide accurate end-to-end execu-\ntion time estimation, we develop a fine-grained RLP. We de-\nfine the response length prediction as a classification task\ninstead of a regression task, since predicting the exact re-\nsponse length is challenging. Hence, the RLP needs to de-\ntermine which bucket the response length will fall into,\nwhere the bucket size is fixed at B, as shown in Fig. 3.\nDue to the limited context length of BERT (Devlin et al.\n2019), it is difficult to process longer inputs. Therefore, the\nRLP Predict(·) is based on a Small Language Model\n(SLM) to better process long input prompts, which has sig-\nnificantly fewer parameters than the target LLM. The ar-\nchitecture of RLP is shown in Fig. 3, which consists of\nan embedding layer, L decoder layers, and a classifica-",
            "content": "d (cid:17) account for the majority of FLOPs (Narayanan et al. 2021), since there is no matrix multiplication in the embedding and Norm layer, we only analyze the FLOPs from the CausalAttention and FeedForward layers and the language modeling head LMHead. We suppose the number of FLOPs of each layer in the prefill phase and decoding step is denoted as LayerName prefill-phase and LayerName decoding-step, respectively. (cid:16) QKM The main computation of the CausalAttention layer , is CausalAttention(Q, K, ) = softmax where is the hidden size, denotes element-wise multiplication, and is the causal attention mask (Vaswani et al. 2017). In the prefill phase, both and have length Nx, where Nx is the length of the input prompt x. Therefore, CausalAttention is quadratic in Nx. In the decoding step, prefill-phase only the last generated token needs processing, so the length of is 1 and the length of is Nkv, which denotes the current length of the KV cache. As result, CausalAttention is linear in Nkv. Similarly, the FeedForward layer handles the input prompt consisting of Nx tokens during the prefill phase, and processes the last generated token in each decoding step. Since the FeedForward layer processes each input token independently (Yu et al. 2022), FeedForward is linear in Nx, while FeedForward is constant and deterdecoding-step mined solely by the hyperparameters of the model (e.g., hidden size, intermediate size). The language modeling head LMHead takes the hidden state of the last input token as the input and produces the logits of the next token. Therefore, the FLOPs of the LMHead layer are solely related to the model architecture, regardless of the inference stage, namely prefill-phase and LMHead LMHead Therefore, the number of FLOPs in the prefill phase fprefill-phase is quadratic in Nx with linear and constant terms included, while that in the decoding step fdecoding-step is linear in Nkv, namely decoding-step are constant. decoding-step prefill-phase + LMHead stage + CausalAttention fstage = FeedForward stage (3) stage where stage is prefill phase or decoding step. Since the number of FLOPs and the corresponding execution time share the same form (Chowdhery et al. 2023), we can derive the estimated execution time of the prefill phase ˆtprefill-phase with respect to Nx (the length of input prompt x), and that of the i-th decoding step ˆti kv as follows: decoding-step with respect to , ˆtprefill-phase(x) = aN 2 ˆti decoding-step(N kv) = pN kv + q, + bNx + (4a) (4b) where a, b, c, p, and are corresponding coefficients. In the next subsection, we will present profiling-based and datadriven approach to establish these coefficients. Furthermore, we observe in Eq. (4b) that kv largely affects the execution time of each decoding step, the derivation and configuration of which will be discussed in detail. Profiling-based Fitting Although FLOPs-based analytical modeling characterizes computational workload in terms of Nx and kv, actual execution time depends on implementation and hardware (Chowdhery et al. 2023). To this end, we propose profiling-based fitting method to estimate the execution time. To be more specific, we establish coefficients Figure 3: The overview of the proposed fine-grained response length predictor (RLP). tion head. Each decoder layer includes four layers in sequence: an RMSNorm layer (Zhang and Sennrich 2019), CausalAttention layer, another RMSNorm layer, and an FFN with SwiGLU layer (Shazeer 2020). Since the response length largely depends on the target LLM (Perez-Ramirez, Kostic, and Boman 2025), we employ the knowledge distillation approach to better align the RLP with the target LLM. As shown in Fig. 3, for given input prompt xj, we collect the actual length Nj of the response generated by the target LLM, where is the index of the data item. Therefore, we construct the training dataset consisting of pairs (xj, Nj ), where Nj represents the target bucket index, namely the classification label. According to the model card, the maximum generation capacity of the target LLM is Nmodel tokens, which exceeds the maximum generation length Nmax specified during runtime, namely Nmodel Nmax. Therefore, there are Nmodel/B buckets for classification during the training process. According to the constraint (1c), we perform postprocessing to limit the maximum predicted response length by Nmax. Assuming the predicted bucket index is ˆn = Predict(x), indicating the response length is between (ˆn 1)B and ˆnB, and ˆnB is reported as the predicted response length. The predicted response length ˆN after postprocessing can be obtained as follows: ˆN = min(Nmax, ˆnB) = min(Nmax, Predict(x)B) (2)"
        },
        {
            "title": "4.2 Workload-guided Execution Time Estimator\nEnd-to-end execution time estimation is essential in hard\nreal-time systems (Goh and Anderson 2024). Modeling the\nworst-case execution time (WCET) during system design\nensures that each LLM inference meets deadlines. In this\nsection, we develop a workload-guided ETE, integrating\nfloating point operations (FLOPs) -based analytical model-\ning and profiling-based fitting.",
            "content": "FLOPs-based Modeling We adopt FLOPs-based modeling to analyze the relationship between computational workload and WCET, providing theoretical support for profilingbased fitting. Since most modern Transformer-based LLMs (Grattafiori et al. 2024; Yang et al. 2024) employ an architecture comprising an embedding layer, series of decoder layers, and language modeling head. The architecture of each decoder layer is Norm-CausalAttention -Norm-FeedForward. Given that matrix multiplications in Eq. (4) using data-driven approaches based on execution time profiling, given certain LLM models and hardware platforms. Taking the prefill phase as an example, the actual execution time tprefill-phase(Nx) for given Nx can be measured. Hence, dataset with pairs (Nx, tprefill-phase(Nx)) can be obtained to derive the coefficients a, b, and in Eq. (4a). Wellestablished fitting methods, such as the Least Squares (LS), can be applied. The same applies to the decoding step. End-to-end Execution Time Prediction According to Eq. (4b), the execution time of each decoding step depends on kv. Therefore, we explore the impact of the KV cache eviction ratio α on the execution time. Since KV cache eviction occurs after the prefill phase and before the decoding phase, fraction α of the KV cache produced in the prefill phase will be evicted. As result, in the i-th decoding step, the length of the KV cache is kv(x, α) = (1 α)Nx + 1. (5) Given Eqs. (4b) and (5), the estimated execution time of the i-th decoding step becomes ˆti decoding-step(x, α) = p((1 α)Nx + 1) + q. (6) According to the predicted response length ˆN , which is obtained according to Eq. (2), the estimated execution time of the decoding phase is the sum of the execution time of all decoding steps, i.e., ˆtdecoding-phase(x, α, ˆN ) = ˆN 1 (cid:88) i=1 ˆti decoding-step(x, α). (7) The end-to-end execution time prediction consists of the estimated execution time of the prefill and decoding phases. Thus, we estimate the execution time based on Eqs. (4a) and (7), i.e., ˆte2e(x, α, ˆN ) = ˆtprefill-phase(x) + ˆtdecoding-phase(x, α, ˆN ). (8) Furthermore, considering WCET, we introduce the pessimistic factor k, 1, to ˆN . Hence, the pessimistic predicted response length becomes ˆN . According to constraint (1c), the pessimistic predicted response length is ˆNW = min(k ˆN , Nmax). Based on Eq. (8), we can estimate the WCET of executing LLM inference as ˆtWCET(x, α, ˆNW) = ˆtprefill-phase(x) + ˆtdecoding-phase(x, α, ˆNW). (9) We will evaluate the impacts of in the evaluation."
        },
        {
            "title": "5.1 Time-Budgeted LLM Inference Mechanism\nAccording to Challenge 2 mentioned in Sec. 3.2, the config-\nuration factors θ need to be adjusted at runtime due to the\nvariances of input and response length. Therefore, we de-\nvelop a time-budgeted efficient inference mechanism based\non the KV cache eviction. Consequently, in the following\nanalysis, we target the KV cache eviction ratio α as the con-\nfiguration factor.",
            "content": "Since the response performance metric M() is unknown until the inference is completed, which poses challenge for maximizing the objective (1a) in Prob. (1). In general, the response performance is non-increasing as the KV cache eviction ratio α increases (Xiao et al. 2025). Thus, the objective function can be transformed from maximizing the response performance to minimizing the KV cache eviction ratio α. Additionally, according to the constraint (1b), the end-toend execution time te2e should stay within the time budget , which cannot be established until the inference is completed as well. To this end, we use the predicted worst-case execution time ˆtWCET in Eq. (9) as conservative prediction of te2e. Additionally, the overall overhead of executing Predict() and ˆtWCET prediction cannot be ignored, which we denote as tPredict(x). Since ˆtWCET prediction is the prerequisite for establishing α, the tPredict(x) can be measured directly. Therefore, the constraint (1b) is transformed to tPredict(x) + ˆtWCET(x, α, ˆNW) . Therefore, we can obtain the converted problem of timebudgeted LLM inference, i.e., min α s.t. α tPredict(x) + ˆtWCET(x, α, ˆNW) 0 α αmax. (10a) (10b) (10c) Since an excessively large α may degrade M() significantly, the maximum eviction ratio αmax is introduced in constraint (10c). Substituting Eqs. (6), (7), and (9) into Eq. (10b), we can derive the optimal α by solving Prob. (10). (cid:32) α = min αmax, 1 ˆtprefill-phase(x) tPredict(x) pNx( ˆNW 1) ˆNW 2 2pNx pNx + + (cid:33) , (11) where ˆtprefill-phase(x) is defined in Eq. (4a). To summarize, given the input prompt and optional maximum generation length Nmax, the coefficients in Eq. (4), and the pessimistic factor k, the optimal KV cache eviction ratio α for the timebudgeted efficient inference can be established to optimize the response performance within the time budget ."
        },
        {
            "title": "5.2 System Deployment\nAs shown in Fig. 4, for each LLM inference, upon the input\nprompt x arriving, Nx is known. Subsequently, an offline-\ntrained RLP Predict(·) is utilized to predict ˆN according",
            "content": "Figure 4: The timeline of TimeBill, where incoming arrows represent inputs (e.g., x1, Nx1) , and outgoing arrows represent outputs (e.g., ˆy1, α 1). to Eq. (2). Based on ˆN , k, and the offline-obtained coefficients in Eq. (4), ˆtWCET is estimated through Eq. (9). At the same time, the LLM inference begins the prefill phase, which takes tprefill-phase. After the prefill phase, α can be determined through Eq. (11). α percent of the KV cache is evicted, and the retained KV caches are utilized during the subsequent decoding phase. After tdecoding-phase, the inference is completed, and the response ˆy is acquired. Since ˆtWCET is the prerequisite for establishing α, ˆN needs to be obtained before the decoding phase. Therefore, Predict() and ˆtWCET prediction can be executed in parallel with the prefill phase of the LLM on other processors, such as CPU or GPU. If tPredict tprefill-phase, the negative impact of tPredict on response performance can be eliminated, i.e. tPredict = 0 in Eq. (11). Therefore, in this work, we integrate our ETE in Sec. 4 with prompt compression (Li et al. 2024b). Note that the overhead of Eq. (9) is ignorable compared with Predict(). Similar to Eq. (4a), the execution time of Predict() can be modeled as ˆtPredict(xp) = apN 2 + bpNp + cp, where xp is the input of the predictor and Np is the length of xp. Given ˆtprefill-phase obtained through Eq. (4a), an upper bound of Np can be derived by solving ˆtPredict(xp) ˆtprefill-phase(x). Accordingly, prompt compression is performed to compress the input prompt into shorter xp. Note that the time budget can vary across the inferences, e.g., T1 = T2 in Fig. 4. Since ˆN , ˆtWCET, and α are established for each inference, this can be handled naturally by TimeBill."
        },
        {
            "title": "6.1 Experimental Setup\nWe utilize Qwen2.5-7B-Instruct (Yang et al. 2024) as the\ntarget LLM with a context length of 32,768 tokens and a\nmaximum generation capacity Nmodel = 8,192 tokens. The\ntest dataset is LongBench (Bai et al. 2024), and the response\nperformance score is evaluated using the official evaluation\nmetrics, such as F1 score, ROUGE-L (Lin 2004), and Leven-\nshtein distance. The KV cache eviction is performed using\nSnapKV (Li et al. 2024a). The experiments are conducted\non a server equipped with Intel(R) Xeon(R) Platinum 8350C\nand an NVIDIA A40 GPU.",
            "content": "TimeBill Implementation We employ Qwen2.5-0.5BInstruct (Yang et al. 2024) as the SLM to build the proposed RLP Predict() with default 512 buckets (the number of buckets will be discussed in Sec. 6.2). To avoid training on the test set, the prompts from the Arena-Human-Preference100k dataset (Tang, Chiang, and Angelopoulos 2025; Chiang et al. 2024) are used to construct the dataset for training the Predict(). The execution time of the target LLM is profiled to build the ETE, where Nx and Nkv range from 0 to 32,768. The default pessimistic factor is set to 5 (the value of will be discussed in Sec. 6.5). The maximum KV cache eviction ratio αmax is set to 95%. Approaches Vanilla inference (Vanilla), where the target LLM is directly used for inference. KV cache eviction with fixed α (Devoto et al. 2024; Zhang et al. 2023), where α is to 25%, 50%, 75%, 95%, respectively. We denote them as α = x% in this section. set Activation-aware Weight Quantization (AWQ), where the model weights is quantized to 4 bits (Lin et al. 2024). Our proposed TimeBill. Overrun Strategies When an inference job is about to overrun and miss the deadline, the hard real-time system will apply an overrun strategy. We apply two of the most commonly used overrun strategies (Goh and Anderson 2024), Kill. The current job will be terminated and considered incomplete. The response is regarded as empty since it misses the deadline. Skip-Next. Skip the next few jobs until the current job completes. The skipped jobs are considered incomplete and do not any produce response. The average response performance score across all data items in the test set and the completion rate are reported, where the completion rate is defined as the percentile of the number of completed jobs to the total number of jobs."
        },
        {
            "title": "6.2 Efficacy of the Response Length Predictor\nWe compare our fine-grained RLP Predict(·) with BERT-\nbased predictors, including ProxyModel (Qiu et al. 2024)\nand S3 (Jin et al. 2023). We test the Predict(·) with\ndifferent bucket sizes (B = 16, 32, 64), which corre-\nspond to 512, 256, and 128 buckets, respectively. In addi-\ntion, we test Predict(·) modeled in a regression man-\nner. We evaluate the prediction error using metrics includ-\ning Mean Absolute Error (MAE), Root Mean Square Error\n(RMSE), and R-squared (R2), where the ground-truth re-\nsponse lengths are collected using the target LLM. As shown\nin Tab. 1, Predict(·) outperforms ProxyModel and S3.\nThe Predict(·) based on regression modeling performs\npoorly, indicating predicting exact response length directly\nis challenging. Predict(·) with 512 buckets achieves the\nbest performance. As the number of buckets increases, the\ngranularity of Predict(·) becomes finer, and the perfor-\nmance improves.",
            "content": "Methods Ours ProxyModel #Buckets Reg 128 512 5 S3 10 MAE 64.21 48.95 44.15 42. 105.72 108.96 RMSE 103.30 87.57 78.63 78.13 136.79 148.91 0.516 0.652 0.719 0.723 0.152 -0.004 Table 1: The efficacy of different response length predictors. Figure 5: Fitted curves for estimating ˆtprefill-phase, ˆtdecoding-step. Figure 6: The performance of estimating ˆte2e and ˆtWCET. Figure 7: The average scores and completion rates of different approaches under Kill and Skip-Next."
        },
        {
            "title": "6.3 Performance of the Execution Time Estimator",
            "content": "We first evaluate the performance of estimating the ˆtprefill-phase and ˆti decoding-step using Mean Absolute Percentage Error (MAPE). The MAPEs are 1.22% and 1.69% for the prefill phase and the decoding step, respectively, as shown in Fig. 5. Furthermore, we evaluate the performance of the end-to-end ETE, where Predict() with 512 buckets is utilized. The results of α = 0 and Nmax = 64 are presented in Fig. 6. We can see that ˆte2e is close to the actual te2e, while ˆtWCET effectively provides an upper bound of te2e, demonstrating the effectiveness of the proposed ETE."
        },
        {
            "title": "6.4 Comparison with Baselines",
            "content": "We conduct experiments on six different time budgets, ranging from 5 to 10 in one-second increments. The average response performance scores and completion rates under the Kill and Skip-Next strategies are shown in Fig. 7. We can see that TimeBill achieves the state-of-the-art performance in average score and maintains competitive task completion rate. Since the Vanilla often exceeds the time budget, it performs poorly with low task completion rate and average score. For KV cache eviction with fixed α, as α increases, the task completion rates increase, while the average score first increases and then decreases. This is because when α is small, the benefit gained from allowing more inferences to finish within outweighs the loss in response performance, causing the average score to increase as α increases. However, when α is large, it degrades the response performance significantly. Furthermore, AWQ performs slightly better than the Vanilla. Moreover, TimeBill is orthogonal to it and can be effectively integrated with quantization. In contrast, TimeBill balances the inference efficiency and response performance, thereby achieving the highest average response performance scores among tested approaches while attaining similar task completion rate as α = 95%. Figure 8: The average scores and completion rates with different pessimistic factors under the overrun strategy Kill, where the time budget = 5 s."
        },
        {
            "title": "7 Conclusion\nIn this paper, we propose TimeBill, a novel time-budgeted\ninference framework for LLMs. We present the problem\nformulation of time-budgeted inference for LLMs. We in-\ntroduce a fine-grained RLP and a workload-guided ETE,\nenabling accurate end-to-end execution time prediction\nfor LLMs. We develop a time-budgeted efficient\ninfer-\nence method and provide the corresponding deployment.\nThrough extensive experiments, we demonstrate the state-\nof-the-art performance of TimeBill in improving both re-\nsponse performance and completion rate.",
            "content": "Acknowledgments This work is supported in part by the NSF of China under Grants 62473254 and 62202287, and in part by the Open Research Fund of Peng Cheng Laboratory under Grant 2025KF1B0010. References Bai, Y.; Lv, X.; Zhang, J.; Lyu, H.; Tang, J.; Huang, Z.; Du, Z.; Liu, X.; Zeng, A.; Hou, L.; et al. 2024. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding. In Annual Meeting of the Association for Computational Linguistics, 31193137. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877 1901. Chen, J.; Zou, A.; Xu, Y.; and Ma, Y. 2024. Scenic: Capability and scheduling co-design for intelligent controller on heterogeneous platforms. In 2024 IEEE Real-Time Systems Symposium (RTSS), 114. IEEE. Chiang, W.-L.; Zheng, L.; Sheng, Y.; Angelopoulos, A. N.; Li, T.; Li, D.; Zhu, B.; Zhang, H.; Jordan, M.; Gonzalez, J. E.; et al. 2024. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning. Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 41714186. Devoto, A.; Zhao, Y.; Scardapane, S.; and Minervini, P. 2024. Simple and Effective 2 Norm-Based Strategy for KV Cache Compression. arXiv preprint arXiv:2406.11430. Frantar, E.; and Alistarh, D. 2023. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, 1032310337. PMLR. Frantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2023. OPTQ: Accurate Quantization for Generative Pre-trained In International Conference on Learning Transformers. Representations. Goh, J.; and Anderson, J. H. 2024. Towards Principled Budget Enforcement in Real-Time Systems. In 2024 IEEE RealTime Systems Symposium (RTSS), 256266. IEEE. Grattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Vaughan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. He, Y.; Yang, H.; Lu, Y.; Klimovic, A.; and Alonso, G. 2025. Resource Multiplexing in Tuning and Serving Large Language Models. In 2025 USENIX Annual Technical Conference (USENIX ATC 25), 16391655. Hooper, C.; Kim, S.; Mohammadzadeh, H.; Mahoney, M. W.; Shao, Y. S.; Keutzer, K.; and Gholami, A. 2024. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37: 12701303. Hu, X.; Zeng, T.; Yuan, X.; Song, L.; Zhang, G.; and He, B. 2025. BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures. arXiv preprint arXiv:2506.05871. Imai, S.; Nakazawa, R.; Amaral, M.; Choochotkaew, S.; and Chiba, T. 2024. Predicting LLM Inference Latency: Roofline-Driven ML Method. In Annual Conference on Neural Information Processing Systems. Jin, Y.; Wu, C.-F.; Brooks, D.; and Wei, G.-Y. 2023. S3: Increasing GPU Utilization during Generative Inference for Higher Throughput. Advances in Neural Information Processing Systems, 36: 1801518027. Joseph, M.; and Pandya, P. 1986. Finding response times in real-time system. The Computer Journal, 29(5): 390395. Kang, W.; Lee, K.; Lee, J.; Shin, I.; and Chwa, H. S. 2021. Lalarand: Flexible layer-by-layer cpu/gpu scheduling for real-time dnn tasks. In 2021 IEEE Real-Time Systems Symposium (RTSS), 329341. IEEE. Li, Y.; Huang, Y.; Yang, B.; Venkitesh, B.; Locatelli, A.; Ye, H.; Cai, T.; Lewis, P.; and Chen, D. 2024a. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37: 2294722970. Li, Z.; Liu, Y.; Su, Y.; and Collier, N. 2024b. Prompt compression for large language models: survey. arXiv preprint arXiv:2410.12388. Lin, C.-Y. 2004. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, 7481. Lin, J.; Tang, J.; Tang, H.; Yang, S.; Chen, W.-M.; Wang, W.-C.; Xiao, G.; Dang, X.; Gan, C.; and Han, S. 2024. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6: 87100. Liu, Z.; Yuan, J.; Jin, H.; Zhong, S.; Xu, Z.; Braverman, V.; Chen, B.; and Hu, X. 2024. Kivi: tuning-free asymmetric 2bit quantization for kv cache. In International conference on machine learning. Ma, X.; Fang, G.; and Wang, X. 2023. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36: 2170221720. Narayanan, D.; Shoeybi, M.; Casper, J.; LeGresley, P.; Patwary, M.; Korthikanti, V.; Vainbrand, D.; Kashinkunti, P.; Bernauer, J.; Catanzaro, B.; et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the international conference for high performance computing, networking, storage and analysis, 1 15. Perez-Ramirez, D. F.; Kostic, D.; and Boman, M. 2025. CASTILLO: Characterizing Response Length DistribuarXiv preprint tions of Large Language Models. arXiv:2505.16881. Zheng, Z.; Ren, X.; Xue, F.; Luo, Y.; Jiang, X.; and You, Y. 2023. Response length perception and sequence scheduling: An llm-empowered llm inference pipeline. Advances in Neural Information Processing Systems, 36: 6551765530. Zhong, Y.; Liu, S.; Chen, J.; Hu, J.; Zhu, Y.; Liu, X.; Jin, X.; and Zhang, H. 2024. DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving. In USENIX Symposium on Operating Systems Design and Implementation, 193210. Qiu, H.; Mao, W.; Patke, A.; Cui, S.; Jha, S.; Wang, C.; Franke, H.; Kalbarczyk, Z. T.; Basar, T.; and Iyer, R. K. 2024. Efficient interactive llm serving with proxy arXiv preprint model-based sequence length prediction. arXiv:2404.08509. Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. Shazeer, N. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202. Song, Z.; Lv, M.; Ren, T.; Xue, C. J.; Wu, J.-M.; and Guan, N. 2024. Autoware. Flex: Human-Instructed Dynamically Reconfigurable Autonomous Driving Systems. arXiv preprint arXiv:2412.16265. Tang, K.; Chiang, W.-L.; and Angelopoulos, A. N. 2025. Arena Explorer: Topic Modeling Pipeline for LLM Evals & Analytics. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Vestal, S. 2007. Preemptive scheduling of multi-criticality systems with varying degrees of execution time assurance. In 28th IEEE international real-time systems symposium (RTSS 2007), 239243. IEEE. Xiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and Han, S. 2023. Smoothquant: Accurate and efficient post-training In International quantization for large language models. conference on machine learning, 3808738099. PMLR. Xiao, G.; Tang, J.; Zuo, J.; Guo, J.; Yang, S.; Tang, H.; Fu, Y.; and Han, S. 2025. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. In International Conference on Learning Representations. Xiao, G.; Tian, Y.; Chen, B.; Han, S.; and Lewis, M. 2024. Efficient streaming language models with attention sinks. In International Conference on Learning Representations. Xu, Z.; Zhang, Y.; Xie, E.; Zhao, Z.; Guo, Y.; Wong, K.- Y. K.; Li, Z.; and Zhao, H. 2024. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. IEEE Robotics and Automation Letters. Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; et al. 2024. Qwen2. 5 Technical Report. arXiv preprint arXiv:2412.15115. Yu, G.-I.; Jeong, J. S.; Kim, G.-W.; Kim, S.; and Chun, B.-G. 2022. Orca: distributed serving system for TransformerBased generative models. In USENIX Symposium on Operating Systems Design and Implementation, 521538. Zhang, B.; and Sennrich, R. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32. Zhang, Z.; Sheng, Y.; Zhou, T.; Chen, T.; Zheng, L.; Cai, R.; Song, Z.; Tian, Y.; Re, C.; Barrett, C.; et al. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36: 3466134710."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University"
    ]
}