{
    "paper_title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability",
    "authors": [
        "Yarden Bakish",
        "Itamar Zimerman",
        "Hila Chefer",
        "Lior Wolf"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 8 3 1 2 0 . 6 0 5 2 : r Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability Yarden Bakish Tel-Aviv University Itamar Zimerman Tel-Aviv University Hila Chefer Tel-Aviv University Lior Wolf Tel-Aviv University"
        },
        {
            "title": "Abstract",
            "content": "The development of effective explainability tools for Transformers is crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available. https://github.com/YardenBakish/PE-AWARE-LRP"
        },
        {
            "title": "Introduction",
            "content": "Explainable AI (XAI) is increasingly vital in deep learning (DL), where models often achieve remarkable performance but operate as opaque black boxes [8, 19]. This lack of transparency reduces trust, limits user engagement, and complicates troubleshooting, thereby restricting the use of DL models in applications where decision-making transparency is essential. Consequently, developing XAI techniques for DL models has become an important research domain [37]. This task, however, is challenging, due to the inherent complexity of these models, which cannot be easily represented by simple functions. Transformer-based architectures, which have become dominant in DL, present additional challenges for explainability due to their large scale, often containing billions of parameters. To address this, researchers have developed various attribution methods specifically designed for Transformers [16, 2, 3, 1]. Among these, model-specific XAI techniques have gained prominence, providing explanations based on the models parameters, internal representations, and overall architecture. The most effective model-specific XAI techniques, and the current state-of-the-art for Transformer explainability, are LRP-based, such as [2]. LRP is well-established attribution technique that explains models predictions by propagating relevance scores backward through the network, redistributing activation values based on predefined propagation rules. Unlike gradient-based methods, which often suffer from issues like vanishing gradients or numerical instabilities, LRP provides more stable and precise way to trace how information flows through each layer. Preprint. Under review. Recently, several refinements have been proposed to improve the stability and faithfulness of LRP rules for Transformers, leading to more robust and reliable interpretability techniques. Notable examples include [16, 3] and [2], which introduce custom rules for propagating LRP through attention mechanisms, layer normalization, and other key components. Despite these advancements, we identify critical gap in this extensive line of work: all existing LRP-based methods for Transformers overlook the need for PE-aware LRP rules and do not propagate attribution through positional encoding. This omission results in the loss of key aspect of relevancy related to positional and structural concepts, limiting the ability to provide faithful and comprehensive explanations. (a) (b) Figure 1: (a) Explainability heatmaps by the stateof-the-art Attention-LRP method (AttnLRP) [2]. (b) The LRP heatmap obtained directly from our PEaware LRP rules. The relevancy captured by the PE is less fragmented and captures more of the object. To mitigate this problem, we propose Positional-Aware LRP (PA-LRP), novel technique that significantly improves upon previous methods through two fundamental modifications: (i) Reformulating the input space of the Transformer explainability problem to incorporate positional information. Instead of relying solely on the vocabulary space in NLP and the patch space in vision, we define the input space as set of position-token pairs. (ii) Introducing the first LRP rules specifically designed to propagate relevance across standard positional encoding (PE) layers, including learnable PE, Rotary PE [41], and others. To enhance stability and faithfulness, our rules are further improved through techniques such as reparameterization of PE layers, linearization, and defining an appropriate sink for positional relevance to ensure that position-associated information is properly absorbed, which we validate to be crucial for precise propagation. Moreover, we provide complementary theoretical analysis, proving that our rules both satisfy the conservation property, and able to derive explanations more faithfully. Our main contributions consist of the following: (i) We identify critical gap in current LRP-based XAI techniques for Transformers: they overlook the attribution of positional encodings (PE). This omission results in violation of the conservation property for input-level PE, as shown in Lemma 3.1, and leads to unfaithful heatmaps when handling positional features, as demonstrated in Lemma 3.3. We empirically validate that this omission is critical limitation by significantly outperforming existing methods, and demonstrating that in certain cases, assigning relevance to PE alone can surpass standard state-of-the-art Transformer explainability techniques, showing that this signal is significant. Additionally, the obtained signal is complementary and distinct from the non-positional signal, better capturing spatial, positional, and structural relationships, as shown in Figure 1. (ii) We introduce PA-LRP, theoretically grounded and PE-aware technique for assigning relevance in Transformers. As shown in Tables 1 3, PA-LRP significantly outperforms previous methods across both fine-tuned classifiers and zero-shot foundation models, in both NLP and vision tasks. (iii) Providing an open-source and user-friendly implementation of our method, along with demos and practical examples, to facilitate adoption by the broader research and practitioner community."
        },
        {
            "title": "2 Background and Related Work",
            "content": "In this section, we describe the scientific context for discussing LRP-based Transformer explainability, along with the necessary terminology and symbols needed to describe our method. 2.1 Positional Encoding in Transformers Transformer-based [45] architectures rely on self-attention, which computes contextual relationships between tokens using: Attention(X) = Softmax (cid:19) (cid:18) QK dk V, = XWQ, = XWK , = XWV (1) where, K, Q, represent key, query, and value matrices respectively, dk is the embedding dimension, and WQ, WK, WV are learnable linear projection matrices. This attention mechanism is duplicated over several heads and is wrapped by standard DL peripherals such as Layer Normalization, FFNs, 2 and skip connections, forming the core structure of Transformer model by: = LayerNorm (X + Attention(X)) , = LayerNorm (cid:0)X + FFN(X )(cid:1) . (2) where FFN applies two-layer linear transformation with activations in the middle. Transformers operate on sets of tokens rather than ordered sequences, making them permutationinvariant by design. Unlike architectures with built-in order sensitivity such as RNNs [23, 27], Transformers require explicit positional encoding (PE) to capture sequence structure. PE can be introduced at different stages of the model: it can be added to token embeddings at the input layer, as seen in learnable PE and sinusoidal PE [45], or integrated within the attention mechanism at each layer, as employed in Rotary PE (RoPE) [41] and Alibi [34]. The key insight of this paper is that while PE is well known for its important role in the forward pass [20], its crucial role in propagation-based XAI methods, such as LRP, has been largely overlooked, leading to violations of conservation and the loss of significant relevance, which often carries distinctive positional and structural meanings. Learnable PE. learn position representations directly from data. This approach offers flexibility and adaptability. Learnable PE represents positions as trainable parameters, allowing the model to Sinusoidal PE. Sinusoidal PE, as applied by the original Transformer model [45], encodes positions using sine and cosine functions with different non-trainable frequencies. Because it is based on absolute positions, it is less effective in tasks where relative positional information is more important. Rotary PE (RoPE). RoPE [41] incorporates positional information by rotating token embeddings in structured manner, enabling the model to naturally encode relative positions. Specifically, each key and query vector is transformed using per-position block-diagonal rotation matrix. Unlike learnable or sinusoidal PEs, RoPE encodes relative positional relationships through the multiplication of rotation matrices. Due to its effectiveness, many popular foundation models, including SAM2 [36],Pythia [13], LLaMA [44], Qwen [11], Gemma [42], and others are built on top or RoPE. Other PE techniques, such as ALiBi [34] and relative PEs [39, 35], are described in Appendix A. 2.2 Model-Specific XAI and LRP Methods for explaining neural models have been extensively studied in the context of DNNs [51, 30], particularly in NLP [6, 48] and computer vision [38, 29], and across various architectures including transformers [15, 28], RNNs [7, 51], CNNs [24, 50], state-space models [25, 5], and others. widely adopted strategy for this task is the use of model-specific techniques, which exploit the internal architecture and parameters of neural models to generate explanations. One notable method in this category is LRP [9, 32], which propagates relevance scores, denoted by R(), backwards through the network by redistributing activation values. Propagation relies on predefined rules and interactions between tokens. LRP. LRP is an evolution of gradient-based methods, such as Input Gradient [40, 10], which often suffer from issues like numerical instabilities and gradient shattering [12]. LRP enhances backpropagation rules by enforcing two key principles: (i) the conservation property, which ensures that the total relevance is preserved across layers. Namely, for layer , where = (X), the relevance of the output R(Y ) is equal to the relevance of the input R(X). (ii) The prevention of numerical instabilities during propagation. To achieve these goals, LRP rules are often derived from the Deep Taylor Decomposition principle [31], redistributing relevance scores at each layer based on the first-order Taylor expansion of the layers function. 2.3 XAI for Transformers The first model-specific XAI methods for Transformers were based on attention maps [14, 18], leveraging attention scores to quantify the contribution of each token to others across layers. Building on this approach, Abnar and Zuidema [1] introduced the attention rollout technique, which aggregates attention matrices across multiple layers to provide more holistic explanation. However, Jain and Wallace [26] later demonstrated that attention-based techniques can be misleading, as attention scores do not always correlate with gradient-based feature importance measures or actual model behavior. To address these limitations, Chefer et al. [16] developed hybrid XAI method that combines LRP scores with attention maps, marking breakthrough in the field by improving attribution fidelity. Alternatively, an extensive body of work focuses on model-agnostic methods [33, 28, 17], however, these approaches often exhibit lower performance compared to model-specific techniques. 3 Purely LRP-based XAI methods for Transformers were first introduced in [46] and later refined by Ali et al. [3], who developed custom LRP rules tailored for LayerNorm and attention layers to preserve conservation properties and ensure numerical stability. More recently, Achtibat et al. [2] further improved this approach by designing more faithful propagation rules for self-attention, achieving state-of-the-art performance in Transformer explainability. To the best of our knowledge, this represents the most advanced technique in the field and serves as our primary baseline. Interestingly, despite extensive research in this area, none of these approaches propagate relevance through PE layers. This omission leads to loss of significant relevance associated with positional and structural features, ultimately resulting in less faithful and holistic attributions."
        },
        {
            "title": "3 Method",
            "content": "In this section, we describe our PE-aware LRP rules. We first revise the input space used in the Transformer explainability problem in Section 3.1. Then, building upon this formulation, we define our custom LRP rules in Section 3.2 and Section 3.3. Finally, in Section 3.4, we prove that our PE-aware LRP rules are theoretically grounded. 3.1 Reformulating the Input Space To comprehensively attribute positional information, we must define sink that absorbs PE-associated relevance. To this end, we reformulate the explainability problem for Transformers. Given sequence of embedded tokens of length L, denoted by E1, . . . , EL, where is the embedding size, previous methods traditionally define the input space as = {Ei [L], Ei RD} . (3) In contrast, we reformulate the input space as token-position pairs, with positional features defined separately for each layer, as follows: Figure 2: Visualization of our method for propagating PE-associated relevance. Purple arrows indicate the forward path, while blue arrows represent the LRP propagation rules. Dashed arrows denote custom PE-aware rules defined in our method. (cid:110)(cid:16) = Ei, (Pi,1, Pi,2, . . . , Pi,K) (cid:17) [L], Ei RD, Pi,1, . . . , Pi,K RD(cid:111) , (4) where is the dimension of the positional embeddings, and is the number of layers. Thus, in our formulation, each token in the input space consists of two ingredients: one representing the semantic embedding Ei for all token indices [L], and the other representing the per-layer positional embedding Pi,k for all layers [K]. The use of separate per-layer sinks for positional relevance ensures that the omission of certain positional features in one layer does not obscure or override essential features captured in other layers, and that important positional attributions are not discarded. Building upon the formulation of Eq. 4, the next two sections define LRP rules that enable stable propagation of relevance across PE components. In Section 3.2, we present our rules for input-level PE, while Section 3.3 outlines our rules for attention-level PE. 3.2 LRP-rules for Input Level PE We begin with the simplest form of positional encodinglearnable PEand then demonstrate that other input-level PEs can be reparameterized in similar manner. Since PE is incorporated only at the input layer, we assume for brevity that Pi is vector rather than matrix, namely Pi = Pi,1. We also tie the embedding dimensions of both the semantical and positional vectors (D = D). Learnable PE. This layer learns positional information during training through positional embedding matrix RLD where represents the embedding dimension of positional information, 4 and denotes the maximum sequence length. For each sample, the positional and semantic embeddings are summed to obtain the final input representation. Formally, the combined embedding for the token at position is given by is the i-th row of the positional embedding matrix . Thus, we can propagate relevance from the input of the first transformer block of the i-th token zi denoted by R(zi), to the positional component by using the standard LRP-ϵ rule for addition [2]: (5) PA-LRP for input-level PE : R(P + Ei where . ) = R(zi) + Ei + ϵ Sinusoidal PE. This method encodes position information via unique vector of sine and cosine values constructed by: Sinusoidal PE(i)[2d] = sin (cid:19) (cid:18) 10000 2d , Sinusoidal PE(i)[2d + 1] = cos (cid:19) (cid:18) 10000 2d . (6) Thus, the values derived from Eq. 6 can be used to reparameterize the positional embedding matrix , replacing the learned vectors with their corresponding sine and cosine values. Such reparameterization eliminates the need to propagate gradients through non-linear functions such as sine and cosine, improving efficiency and stability. 3.3 LRP-rules for Attention-level PE For attention-level PE, we focus on describing the PA-LRP rules for RoPE [41] as representative example. For the derivation of the PA-LRP rules for ALiBi [34], we refer the reader to Appendix B. At each layer k, RoPE modifies the queries (Q) and keys (K) matrices before computing the attention scores. This modification is done by multiplying each key and query vector by position-dependent rotation matrix Ri,k RDD where [L]. The rotation matrix is block-diagonal matrix defined as follows: [L], [K] : Ri,k = i cos θ(1) sin θ(1) ... 0 0 is defined as θ(m) sin θ(1) cos θ(1) ... 0 0 . . . . . . . . . . . . . . . 0 0 ... cos θ(D/2) sin θ(D/2) 0 0 ... sin θ(D/2) cos θ(D/2) (7) 2(m1) . = iωm, where ωi = 10000 where each rotation angle θ(m) Note that in RoPE, as in other attention-level positional encodings, the positional information is represented by matrix Ri,k, not vector. Accordingly, we assume: Pi,k = Flattening(Ri,k), = D2 . Thus, we can propagate relevance from the matrix R(Ri,k) to the vector R(Pi,k) by flattening it: R(Pi,k) = Flattening(R(Ri,k)) . (9) Now, key remaining step is to define how relevance should be propagated to Ri,k. The RoPE computation is executed before computing the attention scores, transforming the per-position queries and keys as follows: (8) [L] : Qi = Ri,kQi, Ki = Ri,kKi, RoPE Attention(X) = Softmax (cid:33) (cid:32) KT dk . Our formulation builds on top of AttnLRP [2], which propagates relevance over the queries and keys K, resulting in their corresponding relevance scores R( Q),R( K). To propagate relevance from these matrices to the rotation matrices Ri,k, we apply the LRP rule for matrix multiplication employed by Achtibat et al. [2] separately to the key and query matrices, and then sum the resulting terms to produce final attribution map per attention layer, as follows: [L] : R(Ri,k) = 1 2 R( Qi) + 1 2 R( Ki) . (10) Finally, we aggregate the relevance scores across layers and across feature dimension D, summing only the positive contributions, following strategy similar to that proposed by Chefer et al. [16] and Xiong et al. [47]: (cid:88) Ri = Ei[d]+ + (cid:88) (cid:88) Pi,k[d]+ , (11) d[D] k[K] d[D] 5 where Ri is the final relevance for token i, and ()+ denotes the ReLU function, which filters out negative values. Overall Method. Our PA-LRP rules allow us to assign relevance to the positional part of the input space. For the non-positional part, we use the same rules as defined in AttnLRP [2]. It is worth noting that, although our rules in Eqs.5,9,10, and 11 are built on top of the AttnLRP framework, they are not limited to it. Our input-level PE rules can be decoupled and applied to any LRP method, while the attention-level PE rules can be integrated with alternative formulations, as long as they propagate relevance through the attention matrices and preserve the connection between PE and the computational graph. As result, similar to other LRP methods, our approach can produce explainability maps with computational efficiency comparable to single backward pass. We further clarify that although our method introduces several modifications in the forward path and input space, it does not require any changes to the transformer itself. Instead, these modifications propose an equivalent forward path that allows us to better define the propagation rules. 3.4 Theoretical Analysis To support our PA-LRP rules, we provide theoretical evidence demonstrating that they satisfy the key LRP criteria. First, the following two lemmas prove that our proposed LRP rules satisfy the conservation property. Lemma 3.1. For input-level PE transformers, the conservation property is violated when disregarding the positional embeddings relevancy scores. Lemma 3.2. For attention-level PE transformers, our PE-LRP rules satisfy the conservation property. Next, we present lemma based on key example illustrating that existing methods exhibit low faithfulness. In particular, we show that in simplified settings, applying standard LRP techniques without incorporating position-aware LRP rules leads to unfaithful explanations in tasks that heavily depend on positional features. Lemma 3.3. For attention-level PE transformers, current LRP attribution rules achieve low faithfulness, especially when considering positional features. The proofs and examples are detailed in Appendix E."
        },
        {
            "title": "4 Experiments",
            "content": "To assess the effectiveness of our PA-LRP rules, we conduct comprehensive set of experiments in both the NLP and vision domains. First, in Section 4.1, we perform perturbation tests with both zero-shot foundation models and finetuned classifiers, as well as performing an ablation study. Next, in Section 4.2, we conduct perturbation and segmentation tests in the Vision domain using DeiT [43]. We begin by describing our baselines, ablation variant, and evaluation metrics: Baseline and Ablation Variant. Our primary baseline for comparison is AttnLRP [2], as it represents the SoTA in general transformer XAI, and our method builds on top of it for non-positional components. The key distinction between our approach and this baseline (as well as other LRP-based methods) is our ability to attribute relevance to positional information. Our composite approach that balances both positional and non-positional relevance is denoted as PA-LRP, or ours. Additionally, to isolate the effect of the positional encoding, we introduce an ablation variant denoted by PE Only, which directly measures the relevance assigned to positional components at the input space using our custom attribution rules. Although empirical evaluation of attribution methods is inherently challenging, we validate our PA-LRP method using perturbation and segmentation tests. Below, we describe these metrics: Perturbation Tests. Perturbation tests are split into two metrics: positive and negative perturbations, which differ in the order in which pixels or tokens are masked. In positive perturbation, pixels or tokens are masked in descending order of relevance. An effective explanation method identifies the most influential regions, leading to noticeable drop in the models score (measured in comparison to the predicted or target class) as these critical areas are gradually removed. In negative perturbation, masking begins with the least relevant elements and progresses toward the more important ones. Table 1: Perturbation Tests in NLP. Evaluation of LLaMa-2 7B, Quantized LLaMa-2 7B, and Tiny-LLaMa, all finetuned on IMDB, on pruning and generation perturbation tasks. AttnLRP [2] is the LRP baseline. The metrics used are AUAC (area under activation curve, higher is better) and AU-MSE (area under the MSE, lower is better). Model Method Generation Pruning AUAC AU-MSE AUAC AU-MSE LLaMa-2 7B LLaMa-2 7B LLaMa-2 7B AttnLRP PE Only Ours LLaMa-2 7B Quantized AttnLRP LLaMa-2 7B Quantized PE Only LLaMa-2 7B Quantized Ours Tiny-LLaMa-2 7B Tiny-LLaMa-2 7B Tiny-LLaMa-2 7B AttnLRP PE Only Ours 0.779 0.771 0.796 0.774 0.758 0.785 0.803 0.788 0.806 7.629 6.792 6.521 11.348 10.730 10.137 8.065 3.918 4. 0.777 0.771 0.790 0.767 0.758 0.778 0.792 0.788 0.805 6.548 6.823 6.325 10.067 10.774 9.685 4.030 3.947 4. reliable explanation should keep the models prediction stable, demonstrating robustness even when unimportant components are masked. Following [3, 52], in both Vision and image domains, the final metric is quantified using the AreaUnder-Curve (AUC), capturing model accuracy relative to the percentage of masked pixels or tokens, from 10% to 90%. Segmentation Tests. For attribution methods in vision, segmentation tests are set of evaluations used to assess the quality of models ability to distinguish foreground from background in an image. These tests compare the labeled segmentation image, which indicates whether each pixel belongs to the background or the foreground, with the explainability map, after it has been binarized using thresholding technique. Then, several metrics are computed over both images: (i) Pixel Accuracy: The percentage of correctly classified pixels, measuring how well the predicted segmentation aligns with the ground truth. (ii) Mean Intersection-over-Union (mIoU): The ratio of the intersection to the union of the predicted and ground-truth segmentation maps, averaged across all images. (iii) Mean Average Precision (mAP): metric that considers precision and recall trade-offs at different thresholds, providing robust assessment of segmentation quality. 4.1 Results in NLP For experiments in the NLP, we first present results for perturbation tests, including an ablation study. For our tests, we adopt settings defined in [4, 52] and we present qualitative results in Appendix. D. Perturbation Tests for Finetuned Models. We conduct perturbation tests on three LLMs, finetuned on the IMDB classification dataset: LLaMa 2-7B [44], LLaMa 2-7B Quantized, and Tiny-LLaMa [49]. The results presented in Table 1 demonstrate that our method achieves better scores than the LRP baseline across all metrics . In particular, our approach improves the AU-MSE score in the generation scenario by 14.5% for LLaMa 2-7B, 10.6% for LLaMa 2-7B Quantized, and 51.41% for Tiny-LLaMa. Perturbation Tests in Zero-Shot Settings. We use LLaMa 3-8B [21] to evaluate explainability performance in zero-shot setting. The results presented in Table 3 showcase the superiority of our method across all metrics. (i) Multiple-Choice Question Answering (MCQA): our approach improves, on both generation and pruning scenarios, the AUAC score by approximately 3.2%, and AU-MSE score by approximately by 7.7%. (ii) Next Token Prediction: our approach improves the AUAC score by approximately 0.5% on both generation and pruning scenarios, and AU-MSE score by approximately by 3% on both scenarios. In contrast to MCQA, the Wikipedia dataset consists relatively long texts, making shifts in relevancy distributions less critical to the models prediction. Ablation. To better understand the contribution of our PA-LRP rules, we conduct perturbation tests for the method that attributes solely position-associated relevance. The results are presented in the second, fourth, and sixth rows of Table 1, and second row of Table 3, and are denoted by PE-Only. Surprisingly, this method produces results similar to the AttnLRP baseline, Table 2: Ablation Study: Analyzing the contribution of the multi-sink mechanism via perturbation tests in NLP. The evaluation was conducted on the IMDB dataset. Method Generation Pruning AUAC AU-MSE AUAC AU-MSE Ours w/o Multi-Sink 0.796 0.759 6.521 7.124 0.790 0.758 6.325 7. 7 Table 3: Perturbation Tests in NLP (Zero-Shot). Evaluation of LLaMa-3 8B in zero-shot on generation and pruning perturbation tasks for both multiple-choice question answering and Next-Token Prediction (NTP) settings. Metrics reported are AUAC (area under activation curve, higher is better) and AU-MSE (area under MSE, lower is better). AttnLRP refers to the LRP baseline [2]. for generation and for pruning. Multiple-Choice Question Answering Next Token Prediction Method G. AUAC G. AU-MSE P. AUAC P. AU-MSE G. AUAC G. AU-MSE P. AUAC P. AU-MSE AttnLRP PE Only Ours 0.365 0.374 0.377 66.399 61.014 61.285 0.354 0.364 0.368 68.856 63.141 63.424 0.559 0.557 0.562 41.704 40.538 40. 0.559 0.556 0.561 42.003 40.800 40.735 demonstrating the importance of PE-associated relevance, which carries significant part of the signal. Notably, in Table 1, this variant achieves the best score on the AU-MSE metric for Tiny-LLaMA, reducing the error by 50% compared to AttnLRP [2]. Moreover, in Table 2, we ablate the contribution of our multi-sink approach, which relies on drawing solely positive contributions across layers, as we aim to prevent the loss of positional relevance due to influence of negative contributions in final layers. We evaluate explainability performance for binary classification of LLaMa-2-7B, using the same perturbation metrics, and report that the multi-sink approach improves the results by 7%. 4.2 Results for Vision Transformers For vision models, we present both quantitative and qualitative analysis. Qualitative Analysis. For qualitative analysis, we visualize the explainability maps obtained from our method, the AttnLRP [2] baseline, and the ablation variant that focuses exclusively on PEassociated relevance, denoted by PE Only. Additional examples with larger images are presented in appendix C. Figure 3 presents comparative visualization of these maps. The results reveal three notable trends. (i) Effectiveness of PE-associated relevance: The maps from the ablation variant perform at the same level as the AttnLRP baseline. This finding highlights the strength of our method in identifying important signals that previous works have overlooked, underscoring the importance of our PA-LRP rules. (ii) The uniqueness of PE-associated relevance: The attributed signal derived solely from positional-associated relevance captures unique relationships, exhibiting clearer spatial and structural patterns. In particular, relevance is distributed across the entire object, especially in the snake, bird, and shark examples. In contrast, the baseline method, which does not propagate relevance through PEs, produces sparser pattern that does not focus on the entire object but instead is highly selective to specific patches. One possible explanation is that positional-associated relevance better captures concepts related to position, structure, order, and broader regions within the image. (iii) The importance of balancing: It is evident that the maps obtained from the PE-associated method and the baseline are complementary, and their combination, extracted via our approach, provides the most robust explanations. 8 (a) (b) (c) (d) Figure 3: Results of different explanation methods for DeiT. (a) Input image. (b) PA-LRP (ours), which includes PE attribution. (c) PE only LRP, (d) AttnLRP [2], which does not attribute relevancy to PE. Quantitative Analysis. Here, we present our quantitative results through perturbation and segmentation tests. Perturbation Tests in Vision. The results for perturbation tests are shown in Table 4, where we compare our method against the attention LRP baseline and an ablation variant that focuses solely on maps obtained for positionally associated relevance (PE only). Experiments are conducted using three model sizes: Tiny, Small, and Base. Table 4: Perturbation Tests for DeiT Variants on ImageNet. AUC results for predicted class. Higher (lower) is better for negative (positive). Table 5: Segmentation performance of DeiT variM. for ants on ImageNet segmentation [22]. model. Higher is better. M. Size Method Negative Positive M. Size Method Pixel Accuracy mIoU Predicted Target Predicted Target Base AttnLRP Base Ours Small AttnLRP Small Ours Tiny AttnLRP Tiny Ours 52.185 54.970 50.662 53. 43.832 50.126 47.516 50.174 45.105 47.948 37.499 42.567 10.784 9.918 10.511 9. 2.796 3.579 8.032 9.237 9.761 8.477 2.503 3.214 Base AttnLRP Base Ours Small AttnLRP Small Ours Tiny AttnLRP Tiny Ours 72.204 72.698 72.114 73. 74.815 76.613 50.100 51.400 50.000 51.700 52.850 55.920 Notably, our method outperforms the baseline by significant margin. For instance, in negative perturbation of the predicted class, our method improves the performance by an average of 3.97 points across the three model sizes. However, in positive perturbation, our method lags behind the baseline in half of the cases, though by small margin of at most 1.2 points."
        },
        {
            "title": "5 Discussion: The Role of Attributing PEs",
            "content": "Our theoretical and empirical analysis suggests that both semantic and positional relevance are complementary, and combining them is essential to provide precise explanations. LRP-type attribution creates pixel-level heatmaps, but can we characterize and identify which concepts are attributed mainly by positional relevance versus semantic relevance? We may expect, for example, that objects that are usually placed in specific contexts (boats on water, airplanes in the sky) would display more significant PE component. Much of this position context is relative. RoPE, for example, captures relative position through the matrix multiplication of two position-dependent rotation matrices, which plays fundamental role in capturing spatial features in vision tasks (e.g., objects spanning across multiple patches) and when modeling relationships between words in the same sentence in NLP. In such cases, our PA-LRP rules can effectively attribute positional features, that are largely ignored by standard LRP methods."
        },
        {
            "title": "6 Conclusions",
            "content": "This paper explores the importance of assigning LRP scores to positional information, crucial component of Transformers and LLMs. Our theoretical and empirical analysis demonstrates that positional-associated relevance carries unique type of significance and can drastically improve XAI methods for attention models. Regarding limitations, we emphasize that our work focuses on designing new custom LRP rules to propagate relevance through PEs, leveraging the insight that this aspect has been previously overlooked. However, we do not extend this insight to redesign or systematically revisit existing LRP rules. Such redesign could offer an opportunity to empirically and theoretically establish improved LRP rules for attention mechanisms and Transformer models."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This work was supported by grant from the Tel Aviv University Center for AI and Data Science (TAD). This research was also supported by the Ministry of Innovation, Science & Technology ,Israel (1001576154) and the Michael J. Fox Foundation (MJFF-022407)."
        },
        {
            "title": "References",
            "content": "[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020. [2] Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, and Wojciech Samek. Attnlrp: attention-aware layer-wise relevance propagation for transformers. In Proceedings of the 41st International Conference on Machine Learning, pages 135168, 2024. [3] Ameen Ali, Thomas Schnake, Oliver Eberle, Grégoire Montavon, Klaus-Robert Müller, and Lior Wolf. Xai for transformers: Better explanations through conservative propagation. In International conference on machine learning, pages 435451. PMLR, 2022. [4] Ameen Ali, Idan Schwartz, Tamir Hazan, and Lior Wolf. Video and text matching with conditioned embeddings. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 15651574, 2022. [5] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024. [6] Leila Arras, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. Explaining recurrent neural network predictions in sentiment analysis. In Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 159168, 2017. [7] Leila Arras, José Arjona-Medina, Michael Widrich, Grégoire Montavon, Michael Gillhofer, Klaus-Robert Müller, Sepp Hochreiter, and Wojciech Samek. Explaining and interpreting lstms. Explainable ai: Interpreting, explaining and visualizing deep learning, pages 211238, 2019. [8] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion, 58:82115, 2020. [9] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015. [10] Baehrens, Schroeter, Harmeling, Kawanabe, Hansen, and K-R Müller. How to explain individual classification decisions. Journal of Machine Learning Research, 2010. [11] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [12] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International conference on machine learning, pages 342350. PMLR, 2017. [13] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. [14] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [15] Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 397406, 2021. 10 [16] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 782791, 2021. [17] Qisen Cheng, Jinming Xing, Chang Xue, and Xiaoran Yang. Unifying prediction and explanation in time-series transformers via shapley-based pretraining. arXiv preprint arXiv:2501.15070, 2025. [18] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning. What does bert look at? an analysis of berts attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, page 276. Association for Computational Linguistics, 2019. [19] Arun Das and Paul Rad. Opportunities and challenges in explainable artificial intelligence (xai): survey. arXiv preprint arXiv:2006.11371, 2020. [20] Philipp Dufter, Martin Schmitt, and Hinrich Schütze. Position information in transformers: An overview. Computational Linguistics, 48(3):733763, 2022. [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [22] Matthieu Guillaumin, Daniel Küttel, and Vittorio Ferrari. Imagenet auto-annotation with segmentation propagation. International Journal of Computer Vision, 110:328 348, 2014. URL https://api.semanticscholar.org/CorpusID:1005559. [23] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 17351780, 1997. [24] Rami Ibrahim and Omair Shafiq. Explainable convolutional neural networks: taxonomy, review, and future directions. ACM Computing Surveys, 55(10):137, 2023. [25] Farnoush Rezaei Jafari, Grégoire Montavon, Klaus-Robert Müller, and Oliver Eberle. Mambalrp: Explaining selective state space sequence models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [26] Sarthak Jain and Byron Wallace. Attention is not explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 35433556, 2019. [27] Michael Jordan. Serial order: parallel distributed processing approach. In Advances in psychology, volume 121, pages 471495. Elsevier, 1997. [28] Enja Kokalj, Blaž Škrlj, Nada Lavraˇc, Senja Pollak, and Marko Robnik-Šikonja. Bert meets shapley: Extending shap explanations to transformer-based classifiers. In Proceedings of the EACL hackashop on news media content analysis and automated report generation, pages 1621, 2021. [29] Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William Freeman, Phillip Isola, Amir Globerson, Michal Irani, et al. Explaining in style: training gan to explain classifier in stylespace. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 693702, 2021. [30] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai: review of machine learning interpretability methods. Entropy, 23(1):18, 2020. [31] Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and KlausRobert Müller. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:211222, 2017. [32] Grégoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and KlausRobert Müller. Layer-wise relevance propagation: an overview. Explainable AI: interpreting, explaining and visualizing deep learning, pages 193209, 2019. [33] Edoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel Gallagher, and Georg Groh. ShapIn Proceedings of the 29th based explanation methods: review for nlp interpretability. international conference on computational linguistics, pages 45934603, 2022. [34] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations. [35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [36] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [37] Wojciech Samek, Thomas Wiegand, and Klaus-Robert Müller. Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296, 2017. [38] Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618626, 2017. [39] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018. [40] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In International conference on machine learning, pages 31453153. PMLR, 2017. [41] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [42] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [43] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR, 2021. [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [45] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [46] Elena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 11261140, 2021. [47] Haoyi Xiong, Xuhong Li, Xiaofei Zhang, Jiamin Chen, Xinhao Sun, Yuchen Li, Zeyi Sun, and Mengnan Du. Towards explainable artificial intelligence (xai): data mining perspective, 2024. URL https://arxiv.org/abs/2401.04374. [48] Tingyi Yuan, Xuhong Li, Haoyi Xiong, Hui Cao, and Dejing Dou. Explaining information flow inside vision transformers using markov chain. In eXplainable AI approaches for debugging and diagnosis., 2021. [49] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. 12 [50] Quan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: survey. Frontiers of Information Technology & Electronic Engineering, 19(1):2739, 2018. [51] Yu Zhang, Peter Tiˇno, Aleš Leonardis, and Ke Tang. survey on neural network interpretability. IEEE Transactions on Emerging Topics in Computational Intelligence, 5(5):726742, 2021. [52] Itamar Zimerman, Ameen Ali Ali, and Lior Wolf. Explaining modern gated-linear RNNs via unified implicit attention formulation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=wnT8bfJCDx."
        },
        {
            "title": "A Background for Additional PEs",
            "content": "In this appendix, we introduce additional PEs beyond those presented in Section 2. Relative Positional Bias (RPB). Similar to Alibi, RPB [35] modifies the attention scores by introducing learnable bias term that depends on the relative distance between query and key tokens. For query at position and key at position j, the attention scores are adjusted as follows: Ai,j = Ai,j + B(i j) (12) where B(i j) is learned bias function that depends only on the relative position difference (i j), rather than the absolute positions. Attention with Linear Biases (ALiBi). ALiBi [34] is positional encoding method designed to help transformers generalize to longer sequences when trained on shorter ones. Instead of using explicit positional embeddings, ALiBi modifies attention scores directly by introducing learned linear bias that penalizes attention weights based on token distance. Specifically, for query token at position j, Alibi adjusts the attention scores as follows: i,j = Ai,j + m(i j) (13) where is learned or predefined slope that controls how quickly attention strength decays with distance. Different attention heads can use different slopes, enabling some heads to focus more on local interactions while others capture long-range dependencies. PA-LRP Rules for Alibi Recall the main modification in the ALiBi computation: i,j = Ai,j + Pi,j, where Pi,j = m(i j) (14) i,j), we define specialized rules to propagate relevancy from Adopting the same approach presented for RoPE, given the relevancy scores of i,j, denoted by R(A i,j to the positional terms of ALiBi at each layer, namely, indices and j. We begin by distributing the relevancy scores between Ai,j and Pi,j, using the standard LRP-ϵ rule for addition, giving us: R(Pi,j) = Pi,j i,j) R(A Ai,j + Pi,j + ϵ (15) We proceed to propagate the relevancy scores R(Pi,j) to the positional encoding and in similar fashion to our rules for RoPE. We make the following observations: (i) is constant, resulting in 100% of the relevancy to propagate from Pi,j to j. (ii) Since we are using auto-regressive models, we get that > j, allowing us to ignore the absolute value function (iii) The standard LRP-ϵ rule for addition applies the same of subtraction, as we can express as + (j), and also = (1) j, and since 1 is constant, we propagate the entire relevancy to j. That gives us: R(i) = R(Pi,j) + (j) + ϵ , R(j) = R(j) = R(Pi,j) + (j) + ϵ (16) From hereon we adhere to our PA-LRP rules, aggregating the relevance scores of the positional terms across all layers as employed in Section 3.3."
        },
        {
            "title": "C Additional Qualitative Results in Vision",
            "content": "In addition to Figure 3, we provide more examples in Figures 4 - 6. As previously explained, PEassociated relevance better highlights the entire object, and overcomes the issue of over-consideration of the foreground, where extremely high relevancy scores are produced for patches which are more concerned with semantics or common patterns, like birds beak in the first row in Figure 5. 14 Figure 4: Additional Qualitative Results In Vision. Results of different explanation methods for DeiT. (a) The input image. (b) PA-LRP (ours), which include PE relevancy attribution. (c) PE only LRP, (d) AttnLRP [2], which does not attribute relevancy to PE. 15 Figure 5: Additional Qualitative Results In Vision. Results of different explanation methods for DeiT. (a) The input image. (b) PA-LRP (ours), which include PE relevancy attribution. (c) PE only LRP, (d) AttnLRP [2], which does not attribute relevancy to PE. Figure 6: Additional Qualitative Results In Vision. Results of different explanation methods for DeiT. (a) The input image. (b) PA-LRP (ours), which include PE relevancy attribution. (c) PE only LRP, (d) AttnLRP [2], which does not attribute relevancy to PE."
        },
        {
            "title": "D Additional Qualitative Results in NLP",
            "content": "We present qualitative results for NLP in Figure 7. It can be seen that our method demonstrates better results in highlighting tokens crucial for prediction, along with their surrounding context, emphasizing its superiority to draw relevancy based on both semantics and positionally. In (b) we see that the amount of artifacts is reduced drastically, with more relevancy channeled to the tokens essential for prediction (\"They should have been giving tribute to Branagh for bringing us one of the greatest films of all time\"). 18 <s> Great just great ! The West Coast got \" Dir ty \" Harry Cal la han , the East Coast got Sh ark . urt Reyn olds plays Sh ark in \" Sh ark Machine \" and enjoyed every minute of it . Play ing ma ver ick arc ot ics cop in Atlanta , is just what everyone wants . Instead of susp ension , he sent to vice squad . Like in the irty Harry mov ies or any other cop mov ies , the captain is always going to be the erk . When was kid , was curious what that movie meant \" Sh ark Machine \". Well knew who played Sh ark , wonder what his machine was . It was his GROUP of fellow ops . After un cover ing the murder , he goes all out to find the per . When it turns out to be big time mob oss , Sh ark doesn play around . When he gets the other prost itute into safety , Sh ark ights back hard and good despite losing finger to the th ug . And also like the part where the bad gets blow out of the building through plate glass window . That was the OM ! andy raw ford \" St reet Life \" really put the movie in the right ood , and the movie itself is really great hit to me , AL WA YS ! ating 4 out of 5 stars . <s> Great just great ! The West Coast got \" Dir ty \" Harry Cal la han , the East Coast got Sh ark . urt Reyn olds plays Sh ark in \" Sh ark Machine \" and enjoyed every minute of it . Play ing ma ver ick arc ot ics cop in Atlanta , is just what everyone wants . Instead of susp ension , he sent to vice squad . Like in the irty Harry mov ies or any other cop mov ies , the captain is always going to be the erk . When was kid , was curious what that movie meant \" Sh ark Machine \". Well knew who played Sh ark , wonder what his machine was . It was his GROUP of fellow ops . After un cover ing the murder , he goes all out to find the per . When it turns out to be big time mob oss , Sh ark doesn play around . When he gets the other prost itute into safety , Sh ark ights back hard and good despite losing finger to the th ug . And also like the part where the bad gets blow out of the building through plate glass window . That was the OM ! andy raw ford \" St reet Life \" really put the movie in the right ood , and the movie itself is really great hit to me , AL WA YS ! ating 4 out of 5 stars . <s> Great just great ! The West Coast got \" Dir ty \" Harry Cal la han , the East Coast got Sh ark . urt Reyn olds plays Sh ark in \" Sh ark Machine \" and enjoyed every minute of it . Play ing ma ver ick arc ot ics cop in Atlanta , is just what everyone wants . Instead of susp ension , he sent to vice squad . Like in the irty Harry mov ies or any other cop mov ies , the captain is always going to be the erk . When was kid , was curious what that movie meant \" Sh ark Machine \". Well knew who played Sh ark , wonder what his machine was . It was his GROUP of fellow ops . After un cover ing the murder , he goes all out to find the per . When it turns out to be big time mob oss , Sh ark doesn play around . When he gets the other prost itute into safety , Sh ark ights back hard and good despite losing finger to the th ug . And also like the part where the bad gets blow out of the building through plate glass window . That was the OM ! andy raw ford \" St reet Life \" really put the movie in the right ood , and the movie itself is really great hit to me , AL WA YS ! ating 4 out of 5 stars . (a) <s> went to see Ham let because was in between jobs . figured 4 hours would be great , ve been fan of Bran agh ; Dead Again , Henry . was completely over wh el med by the direction , acting , cinemat ography that this film captured . Like other reviews the 4 hours passes swift ly . Bran agh doesn play Ham let , he is Ham let , he was born for this . When watch this film constantly trying to find fault , ve looked at the go of and haven noticed them . How he was able to move the camera in and out of the Hall with all the mirror is mystery to me . This movie was shot in 7 0 mil . It shame that Columbia hasn released ides creen version of this on HS . own DVD player , and take this over itan ic any day . So Columbia if you re listening put this film out the way it should be watched ! And don know what happened at the sc ars . This should have swe pt Best Picture , Best ctor , Best irection , best cinemat ography . What films were they watching ? felt sorry for Bran agh at the sc ars when he did ribute to Shakespeare on the screen . They should have been giving ribute to Bran agh for bringing us one of the greatest films of all time . <s> went to see Ham let because was in between jobs . figured 4 hours would be great , ve been fan of Bran agh ; Dead Again , Henry . was completely over wh el med by the direction , acting , cinemat ography that this film captured . Like other reviews the 4 hours passes swift ly . Bran agh doesn play Ham let , he is Ham let , he was born for this . When watch this film constantly trying to find fault , ve looked at the go of and haven noticed them . How he was able to move the camera in and out of the Hall with all the mirror is mystery to me . This movie was shot in 7 0 mil . It shame that Columbia hasn released ides creen version of this on HS . own DVD player , and take this over itan ic any day . So Columbia if you re listening put this film out the way it should be watched ! And don know what happened at the sc ars . This should have swe pt Best Picture , Best ctor , Best irection , best cinemat ography . What films were they watching ? felt sorry for Bran agh at the sc ars when he did ribute to Shakespeare on the screen . They should have been giving ribute to Bran agh for bringing us one of the greatest films of all time . <s> went to see Ham let because was in between jobs . figured 4 hours would be great , ve been fan of Bran agh ; Dead Again , Henry . was completely over wh el med by the direction , acting , cinemat ography that this film captured . Like other reviews the 4 hours passes swift ly . Bran agh doesn play Ham let , he is Ham let , he was born for this . When watch this film constantly trying to find fault , ve looked at the go of and haven noticed them . How he was able to move the camera in and out of the Hall with all the mirror is mystery to me . This movie was shot in 7 0 mil . It shame that Columbia hasn released ides creen version of this on HS . own DVD player , and take this over itan ic any day . So Columbia if you re listening put this film out the way it should be watched ! And don know what happened at the sc ars . This should have swe pt Best Picture , Best ctor , Best irection , best cinemat ography . What films were they watching ? felt sorry for Bran agh at the sc ars when he did ribute to Shakespeare on the screen . They should have been giving ribute to Bran agh for bringing us one of the greatest films of all time . (b) Figure 7: Qualitative Results in NLP. Both groups (a) and (b) present results from different explanation methods for the same example obtained from the IMDB benchmark. In each group, the first row represents the AttnLRP baseline, followed by the PE-only variant in the middle, and finally, our maps at the end."
        },
        {
            "title": "E Proofs of Lemmas",
            "content": "Lemma 3.1. For input-level PE transformers, the conservation property is violated when disregarding the positional embeddings relevancy scores. Proof of Lemma 3.1. Let be our input representation to the first transformer layer, such that = + E, where and are the token and positional embeddings, respectively. Let be the number of layers in our transformer. Following the conservation property, the sum of the relevancy scores at any given layer should uphold: (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) R(L) = R(l) = R(0) = RZ = (RE + RP ) (17) When ignoring RP , we get the final relevancy attribution map Rinput, such that: (cid:88) (cid:88) R(l) = (RE + RP ) = RE = Rinput (18) directly violating the conservation property rule Lemma 3.2. For attention-level PE transformers, our PE-LRP rules satisfy the conservation property. Proof of Lemma 3.2. Let be the number of layers in our Transformer, and the sequence length. We denote R(l) as the relevancy score of the output at layer l. Beginning with R(M ) as the the models output propagating relevancy backwards to achieve the final explanation map for the input embeddings RE, we assume that the standard LRP method does not violate conservation, i.e: [M ] : R(M ) = R(l) = RE (19) Recall that for our PA-LRP formulation, we achieve the final explanation map by summing together the semantic attribution RE, achieved by the standard LRP rules, and the positional relevancy R(l) distributed across the absorbing sinks at each attention layer [M ], giving us the final relevancy map RE + (cid:80) . We aim to prove the following: R(l) R(M ) = RE + (cid:88) R(l) Each attention layer in the transformer is computed using rotary attention: [L], [K] : Qi = Ri,kQi, Ki = Ri,kKi, RoPE Attention(X) = Softmax (20) (cid:33) (cid:32) KT dk . (21) Notice that any computation in this layer which involves more than one tensor, is matrix multiplication function. Adopting the existing baseline, we use the uniform relevance propagation rule, distributing the relevancy evenly between components. Thus, the relevancy scores of Q, K, V, , with denoting the rotation matrix, is equal, and added together to the relevancy of the attention layers output. The absorbing sink mechanism results in the following: R(0) = RE, [M ] : R(l) = R(l1) + R(l) (22) Following this recursion we would get the exact same result as Eq. 20 Lemma 3.3. For attention-level PE transformers, current LRP attribution rules achieve low faithfulness, especially when considering positional features. Proof of Lemma 3.3. We define basic learning problem which relies solely on positional features, proving that existing LRP-based explanation methods which dont propagate relevance through positional encodings, will not produce faithful explanations. Let us assume we use an auto-regressive transformer model (e.g GPT), with single causal self-attention with Alibi PE, and the Value projection replaced by an affine transformation (instead of linear layer). Also, for brevity, let us consider scalar input tokens with sequence length of = 2, denoted by x1, x2. The final model uses the following keys (K), queries (Q), and values (V ): [1, 2] : Qi = WQXi, , Ki = WKXi, Vi = WV Xi + (23) We apply the Alibi self-attention mechanism, and obtain the final output = (O1, O2): A(i, j) = QiKj + mh(i j), mh = 1, O2 = A2,1V1 + A2,2V2 (24) To prevent the semantic representation from affecting the prediction, an optimal solution to this (cid:19) (cid:18)0 0 . For the Value projection, we problem will assign zeros to WQ, WK, namely: = = assume: = Wv + b, with Wv = (cid:19) (cid:18)0 0 , = (cid:19) . (cid:18)0 0 Relevance propagation. Following our settings, we get: = (cid:19) (cid:18)0 0 1 0 , giving us: Attention(Q, K, ) = = (cid:18)0 1 (cid:19) 0 0 (25) For the backwards relevancy propagation, the relevancy scores of Attention are distrusted between and based on the standard Gradient Input. Regardless, we now consider how relevancy scores of both terms RV , Rscore are propagated back to the input x. RV Rx. recall that Wv are assigned with zeros. Given that the fundamental ϵ-LRP rule for affine transformations ignores the bias term completely and uses the weights Wv as measure of weighting the relevancy scores, we get that zero relevancy scores are produced for both tokens. RA Rx. Following the standard LRP rules, the positional terms would be considered constant, and therefor, 100% of the distribution would be directed to the queries and keys. Given that WQ, WK are assigned with zeros, we again get zero relevancy scores being propagated to x. Given that the relevancy scores propagated back from the attention layer are all assigned with zeros, we will get final attribution map of zeroes, indicating the same level of impact for all tokens. This of course, yields an unfaithful explanation. In contrast, our method makes positional terms attributable, maintaining relevancy scores that would otherwise be zeroed out due to existing rules."
        },
        {
            "title": "F Conservation Percentage Results",
            "content": "We measure the sum of relevance for DeiT model at different capacities: Tiny, Small, and Base. The figure provides clear visualization for the violation of the conservation property, with PE relevancy constituting 16.75%, 22.39%, and 9.22% out of the total relevancy for Tiny, Small, and Base DeiT models, respectively. Figure 8: We assess both the positional relevance and the non-positional relevance for DeiT models at different capacities, visualizing the violation of conservation rule, with high non-negligible ratio between the entire relevance in the models ours and positional-associated relevance PE Only."
        }
    ],
    "affiliations": [
        "Tel-Aviv University"
    ]
}