{
    "paper_title": "GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis",
    "authors": [
        "Ashish Goswami",
        "Satyam Kumar Modi",
        "Santhosh Rishi Deshineni",
        "Harman Singh",
        "Prathosh A. P",
        "Parag Singla"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) generation has seen significant progress with diffusion models, enabling generation of photo-realistic images from text prompts. Despite this progress, existing methods still face challenges in following complex text prompts, especially those requiring compositional and multi-step reasoning. Given such complex instructions, SOTA models often make mistakes in faithfully modeling object attributes, and relationships among them. In this work, we present an alternate paradigm for T2I synthesis, decomposing the task of complex multi-step generation into three steps, (a) Generate: we first generate an image using existing diffusion models (b) Plan: we make use of Multi-Modal LLMs (MLLMs) to identify the mistakes in the generated image expressed in terms of individual objects and their properties, and produce a sequence of corrective steps required in the form of an edit-plan. (c) Edit: we make use of an existing text-guided image editing models to sequentially execute our edit-plan over the generated image to get the desired image which is faithful to the original instruction. Our approach derives its strength from the fact that it is modular in nature, is training free, and can be applied over any combination of image generation and editing models. As an added contribution, we also develop a model capable of compositional editing, which further helps improve the overall accuracy of our proposed approach. Our method flexibly trades inference time compute with performance on compositional text prompts. We perform extensive experimental evaluation across 3 benchmarks and 10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach not only improves the performance of the SOTA models, by upto 3 points, it also reduces the performance gap between weaker and stronger models. $\\href{https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$"
        },
        {
            "title": "Start",
            "content": "GraPE : Generate-Plan-Edit Framework for Compositional T2I Synthesis Ashish Goswami1, Satyam Kumar Modi1*, Santhosh Rishi Deshineni1*, Prathosh A. P2, Harman Singh1, Parag Singla1 1IIT-Delhi, 2IISc Bangalore ashish.goswami@scai.iitd.ac.in, parags@cse.iitd.ac.in 4 2 0 2 8 ] . [ 1 9 8 0 6 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) generation has seen significant progress with diffusion models, enabling generation of photo-realistic images from text prompts. Despite this progress, existing methods still face challenges in following complex text prompts, especially those requiring compositional and multi-step reasoning. Given such complex instructions, SOTA models often make mistakes in faithfully modeling object attributes, and relationships among them. In this work, we present an alternate paradigm for T2I synthesis, decomposing the task of complex multi-step generation into three steps, (a) Generate: we first generate an image using existing diffusion models (b) Plan: we make use of Multi-Modal LLMs (MLLMs) to identify the mistakes in the generated image expressed in terms of individual objects and their properties, and produce sequence of corrective steps required in the form of an edit-plan. (c) Edit: we make use of an existing text-guided image editing models to sequentially execute our edit-plan over the generated image to get the desired image which is faithful to the original instruction. Our approach derives its strength from the fact that it is modular in nature, is training free, and can be applied over any combination of image generation and editing models. As an added contribution, we also develop model capable of compositional editing, which further helps improve the overall accuracy of our proposed approach. Our method flexibly trades inference time compute with performance on compositional text prompts. We perform extensive experimental evaluation across 3 benchmarks and 10 T2I models including DALLE-3 and the latest SD-3.5Large. Our approach not only improves the performance of the SOTA models, by upto 3 points, it also reduces the performance gap between weaker and stronger models. The code is available at https://dair-iitd.github.io/GraPE/ 1. Introduction Diffusion models [45], have revolutionized the space of generating photo-realistic images with multiple works Figure 1. Illustration of GraPEs capability to align the image with the input prompt. Left: Generations from various state-of-the-art diffusion models [5, 41, 46], along with their inaccuracies. Right: Images produced by GraPE, in completely training free manner. showing their superior performance compared to their older competitors such as GANs, VAEs and Flow-based models [14, 34, 43]. Of specific interest has been their ability to generate images from text. Despite the success, it has been observed that SOTA diffusion models still struggle to generate accurate images for instructions which involve multistep compositional reasoning, often resulting in errors over object attributes as well as their interactions with other objects in the image [9, 26, 40]. Figure 1 shows an example of text-prompt and the corresponding images generated by 3 of the SOTA text-to-image generation models. Several reasons have been pointed for the relative performance on such complex instructions, including lack of appropriate training data, and use of weak text-encoders to name few. This has naturally limited their applicability for the task of automated and reliable image generation in real life scenarios. To make matters worse, image-editing has been left even further behind, with existing models hardly capable of correctly handing complex edit tasks. To overcome these challenges, recent work has seen several credible attempts to address the challenge of generating images faithful to complex instructions. Broadly, these can be divided in two categories (1) those requiring fine-tuning of existing generation models, examples include [8, 24, 27, 51, 54] (2) and those which are based only on adapting the inference procedure, examples include [1, 9, 16, 31]. While these approaches have been able to show some improvements over the base diffusion models, the problem remains far from solved. We survey the existing related works in detail in Section 2. In this work, we present novel approach which is based on the following observations: Instead of performing the image synthesis for complex compositional instruction in one go, we can redefine the task into iterative refinement of partially faithful generated image. Our approach can be described as pipeline of the following 3 steps: (a) Generate: Initial generation using existing T2I models (b) Plan: Identification of errors in the form of series of simple edits (largely over individual objects and their relations) required to fix the original generation; the edits are expressed in the form of sequential editing plan generated via the use of MLLM (c) Edit carrying out the edit plan in sequential manner over the original generation to get the desired image. We refer to our approach as GeneRAte-Plan-Edit (GraPE). To the best of our knowledge, we are the first ones to decompose the task of fine-grained generation as series of simpler (atomic) edits which are preceded by partially correct generation. We perform series of experiments to evaluate the efficacy of our approach over three benchmark datasets. We compare with 10 different T2I models, and show improvement in each case, with improvements up-to significant 20+ points in some cases. We also note that we see more significant improvement in weaker (typically smaller) generation models, resulting in narrowing of performance gap between smaller and larger SoTA models. We also perform ablations showing the value of our object-centric approach for generating edit plans. Along with this, significant boost in performance is shown when we use the compositional image editor that we develop, as part of our pipeline. We also present detailed error analysis and point to limitations of our model in our experimental analysis. The contributions of our paper can be summarized as follows: (a) We propose simple yet effective approach for improving the performance of T2I models via generate-planedit pipeline. (b) Our approach is guided by object-centric edit plans, and exploits the power of editing models which can handle compositions well. (c) Our approach is modular, and can be used with any existing base generation or editing model (d) We perform series of experiments demonstrating the efficacy of our approach over large number of T2I models, and also present detailed insights into the working of model. 2. Related Work Compositional Image Generation: T2I models struggle with compositional image generation, such as generating objects with correct attributes, adhering to object relations, etc. [4, 17, 21]. Several works aim to improve the compositional image generation capabilities of T2I diffusion models. Some prior works intervene on the cross-attention maps in diffusion models which are responsible for incorporating information form the text into the image. [1, 9] strengthen the attention activations of neglected concepts in the image during the generative process using inference time losses, primarily targeting the reduction of catastrophic neglect and incorrect attribute binding. [27, 51] proposes training models focused on alignment of Cross-attention maps for each token with pseudo-ground truth segmentation maps. [16] uses linguistic structures and structured representations of text such as constituency tree, for manipulating cross-attention representations. However, all these works often focus on narrow compositional concepts, such as only attribute binding or reducing catastrophic neglect of objects, than being general purpose method for aligning images with text. [50] uses MLLMs as an agent with an extensive set of tools for decomposing and planning the generation process into multiple steps. [57] uses an LLM for planning the generation process and suggesting subregions for region-wise diffusion, however, it may struggle with either with decomposing into or merging of these regions, since their re-weighting and sampling method is different from what diffusion generative models are trained for. [12, 18, 31, 35] utilize LLMs for generating intermediate representations such as layouts, panels or blobs followed by generating using specialized models that require such complex annotations to be trained. [11, 19, 32, 56, 64] utilize LLMs such as Gemma, LLaMA , T5-XXL [39, 47, 49] as text-encoders for improved text representations compared to contrastively trained models like CLIP [38] that are largely agnostic to word order [28, 59]. Orthogonally, we use MLLMs for planning the sequential editing process based on the initial generated image, and use general purpose generator and editor models in their native fashion. Instruction Guided Image Editing: Editing images with human written instructions have seen significant interest in the community [6, 15, 29, 44, 6062]. Among recent work, InstructPix2Pix [6] leverages training free editing model, Prompt-to-Prompt [22] along with GPT3 [7] to produce paired image data with editing instructions. MagicBrush [60] is manually annotated real world image editing dataset that improves InstructPix2Pix. [29] release Aurora, for action and reasoning centric image editing. In our work we explore the use of general-purpose image editors for aligning images generated via T2I models to their complex text prompts. We also develop compositional image editor leading to further improved performance. 3. Method In this section, we outline our Generate-Plan-Edit (GraPE) framework. Let us introduce some notation. Let be textual instruction. In the task of T2I synthesis, given an instruction , our goal is to be able to generate an image Io which satisfies various requirements expressed via the instruction . While most existing techniques take the approach of directly generating Io via , they often result in various kinds of inaccuracies, due to the complexity of the instruction. We are motivated by the observation that the task of T2I synthesis can be broken down into simpler steps of first generation, followed by identification of errors, and sequence of corrective edits, each of which is simple and object specific in nature. Accordingly, we propose the following generation pipeline. We first generate an initial image Ig using SOTA generative model, G. This Ig may have mistakes, or inaccuracies with respect to the intent expressed in . We then make use of an existing MLLM (P) to identify the mistakes in Ig, as difference between image and textual descriptions in and Ig, respectively, for each object of interest, and corrective steps suggested, in the form of an edit plan expressed as (Te1 , Te2 , , Ten ), where each Tek is an edit instruction fixing some aspect of the Ig so that it can be aligned with the original prompt . Note that the number of edits is instance specific, and is part of the output produced by P. Finally, we make use of an editing model which inputs the current image Iek along with an edit instruction, Tek+1 and produces next image in the sequence Iek+1 = E(Iek , Tek+1) with {1, 2, 1}. Note that Ig = Ie0 and Ien = Io. This algorithm comprises of 3 broad steps: (a) Generate: Generate the image for textual instruction (b) Plan: Identify mistakes and propose correction plan (c) Edit: Perform the sequence of edits based on the corrective plan. We now describe each step in detail. 3.1. Image Generation For our frameworks initial step, we create base image Ig, which serves as foundational input for the subsequent processes involved in planning and editing. This image Ig and its corresponding text-prompt are considered as primary inputs for the next steps, which allows our framework to operate in plug-and-play manner, regardless of the T2I model used in generation. 3.2. Multi-Modal Planner The planner is key component of GraPE, with its role being executed by Multi-Modal Large Language Model (MLLM) that assesses the initial image, Ig, for potential object misalignment relative to the textual description provided in the prompt , . This discrepancy analysis acts similarly to the Chain-of-Thought prompting technique [52], designed to enhance reasoning by encouraging the planner to break down the assessment process into object level steps. Prompting Style: The prompting style used with the MLLM play crucial role in its ability to generate clear and concise plans across images from diverse domains. Unlike existing approaches [31, 57], which typically rely on LLMs to create abstract or structured representationssuch as bounding boxes, layouts or blobseither in few-shot manner or through explicit training on annotated data [63], our approach takes streamlined alternative that bypasses the complexity of these strategies. Our method focuses on leveraging the inherent strengths of MLLMs, focusing on tasks where they already excel, such as image captioning and language comprehension, which reduces the need for intricate representations enabling our framework to maintain simplicity and generalization across domains. By combining these strengths with carefully designed few-shot examples, we guide the planner to produce object-centric, structured outputs without necessitating additional, specialized training. The planners output is organized into four key steps, explained below using the example in Fig 2. Analyzing Textual Elements: The goal here is to extract high level object-attribute pairs from the text-prompt, focusing on relationships among the described objects. As shown in Fig. 2, the key exacted entities are Tiny Dog, White Car and plate of sushi. Analyzing Image Elements: Next, the planner generates detailed object-level analysis of Ig, which is akin to creating comprehensive image caption limited to objectlevel information. As seen in our example, MLLM effectively finds the objects present in the image in context of the text-prompt, i.e Tiny Dog and White Car and nonexistence of sushi, scattered oranges. Error Identification: The extracted entities from both modalities extracted in the prior steps are compared and descriptive summary of the identified errors is generated, this step not only grounds the mistakes but also provides Figure 2. Proposed GraPE framework, given text prompt is used to generate an initial image from T2I model, Ig which is then fed into MLLM based planner along with the text prompt which identifies the objects that are misaligned in the image and outputs set of edit plans guided by few-shot prompting. The plans are executed as series of edits over the initial image to produce the final image an interpretable way into planners reasoning. Feedback Generation: Finally, leveraging its extensive knowledge, the MLLM generates actionable feedback in the form of editing instructions to re-align image elements with the textual description. This step ensures that output aligns with the prompts intent. As in Figure 2, the MLLM plans to first replaces the objects (orange slices) on the plate with sushi and adds oranges around the dog in two distinct steps. This structured approach not only enhances the interpretability and accuracy of the planners outputs but also provides user-friendly, transparent process for refining image alignments in GraPE. The plans are extracted from MLLM output using regex matching of Feedback header. 3.3. Text Guided Image Editing In our framework, the editing model plays crucial yet straightforward role: it iteratively refines the generated image (Ig) based on the plans extracted from the structured output produced by the MLLM in the planning phase. This design ensures flexibility, allowing for the use of any pretrained model as plug-and-play component. Compositional Image Editing T2I models based on CLIP [38], and by extension, editing models built on it are limited by CLIPs compositional reasoning capabilities including understanding word order in text and object relations in images [28, 48, 59]. We hypothesize and show for the first time that enhancing word order understanding in the text space and training the model on high-quality objectcentric and reasoning-oriented data is key to induce compositionality in image editors. For this, we introduce PixEdit, text-guided image editing model based on PixArt-Sigma [11] and trained on the reasoning-centric dataset used by AURORA [29]. With the T5-XXL [39] language model as its text encoder, PixEdit demonstrates enhanced performance on compositional and spatially complex edits, benefiting from both robust language comprehension and reasoning-centric training data. Providing gains over all existing image-editing models for the generation task in our GraPE framework. Note that PixEdit is developed to be general-purpose image editor, with enhanced compositional and object centric editing capabilities, and is not specific to the generation task we tackle. For additional details about PixEdit refer Supplementary Section 9. 3.4. Implementation Details We utilize frontier multi-modal model, GPT-4o as multimodal planner for its exceptional image-understanding and instruction following capabilities, both of which are essential for generating high-quality plans. While GraPE works with any strong image editing model, for best results we use our developed PixEdit and the recently proposed AURORA model [29] as our editors due to their ability to perform object-centric and reasoning-oriented edits on real images, supported by training on an editing dataset derived from videos and simulators. See Supplementary Section 9 for more details on implementation. Text Prompt Generated Image Edit Step 1 Edit Step 2 Edit Step 3 porcelain pot with tulips and metal can with orchids and glass jar with sunflowers Three chairs are arranged in row. gray duck is positioned on the right side of the rightmost chair. skyline in background. green bench, red car, blue bowl, and pink apple. Remove sunflowers from image. Add glass jar on table. Put sunflowers in the glass jar. Remove the duck from the leftmost chair. Remove the duck from the middle chair. Position gray duck on the right side of the rightmost chair. Table 1. Iterative results by applying GraPE. on images generated by SD3.5 and SDXL, these images are edited via the proposed PixEdit editing model. Please see 8 for additional details on the plans generated for these images. Change the blue apple to blue bowl. Change the red apple to pink apple. Figure 3. Experimental results showcasing the maximum gain in DSG score by GraPE with both AURORA and PixEdit as editing models. The figure presents both DSG and DSG (w/o dependency) scores. The percentage gain is measured over DSG scores. 4. Experiments 4.1. Experimental Settings Models: To evaluate the effectiveness of GraPE, we conduct comprehensive assessment across 10 state-of-the-art text-to-image (T2I) models of varying sizes and capabilities. These models include: Stable Diffusion v1.5 [42], Structure Diffusion [16], Stable Diffusion V2.1 [42], LMD [31], SD-XL [37], PixArt-α [10], DeepFloyd IF [2], PlaygroundV2.5 [30], Dalle 3 [5], and the latest Stable DiffuMethod Concept K= Concept K=3 Concept K=5 Concept K=7 Base +GraPEPixEdit Base +GraPEPixEdit Base +GraPEPixEdit Base +GraPEPixEdit Stable-Diffusion v1.5 [42] Structure Diffusion [16] Stable Diffusion v2.1 [42] LMD [31] SD-XL [37] PixArt-α [10] DeepFloyd IF [2] PlaygroundV2.5 [30] Dalle3 [5] Stable Diffusion v3.5 Large [46] 0.808 0.009 0.823 0.002 0.833 0.002 0.855 0.004 0.848 0.010 0.813 0.010 0.883 0.009 0.908 0.010 0.947 0.002 0.927 0.005 0.892 0.002 0.885 0.004 0.868 0.005 0.873 0.002 0.877 0.005 0.872 0.010 0.915 0.000 0.955 0.004 0.953 0.002 0.948 0.002 0.606 0.018 0.606 0.002 0.639 0.014 0.711 0.008 0.708 0.018 0.668 0.011 0.680 0.016 0.737 0.023 0.832 0.012 0.815 0.002 0.752 0.002 0.723 0.005 0.737 0.002 0.773 0.006 0.780 0.007 0.722 0.002 0.765 0.007 0.792 0.009 0.861 0.003 0.817 0.002 0.497 0.010 0.542 0.014 0.579 0.012 0.643 0.011 0.635 0.014 0.649 0.011 0.663 0.014 0.658 0.015 0.812 0.014 0.803 0.003 0.689 0.002 0.703 0.006 0.687 0.005 0.725 0.009 0.729 0.004 0.742 0.002 0.745 0.002 0.721 0.002 0.832 0.004 0.831 0. 0.450 0.005 0.447 0.001 0.466 0.002 0.591 0.002 0.520 0.003 0.507 0.001 0.583 0.002 0.540 0.003 0.728 0.006 0.759 0.004 0.610 0.005 0.571 0.002 0.626 0.002 0.668 0.009 0.628 0.002 0.625 0.002 0.662 0.006 0.640 0.005 0.737 0.004 0.784 0.005 Table 2. Results on Concept-mix benchmark GraPEPixEdit sion v3.5 Large [46]. This selection encompasses diverse range of recent T2I models, allowing for thorough evaluation of GraPEs robustness across different architectures and configurations. Benchmarks: 1. T2I Compbench [26]: We evaluate the compositional generation capabilities of GraPE using this wellestablished benchmark, specifically designed to assess performance in this area. This includes compositional challenges across categories such as shape, color, texture, spatial, non-spatial, and complex. We utilize subset of 100 prompts, sampled uniformly across all six categoriesshape, color, texture, spatial, non-spatial, and complexto ensure comprehensive assessment of the models performance across diverse compositional challenges. 2. ConceptMix [55] : This benchmark evaluates models across varying levels of controllable compositionality. Using the official ConceptMix codebase, we generate 100 prompts for each 1, 3, 5, 7. Each prompt includes at least one object paired with additional visual conceptssuch as color, number, shape, size, style, spatial arrangement, and textureoffering diverse and rigorous assessment of the models ability to handle increasingly complex visual compositions. 3. Flickr-Bench : Flickr-30k [36] is widely used benchmark for evaluating the generation quality of T2I models using metrics such as FID [23]. The human-generated prompts in this dataset offer well-balanced mix of compositionality and realism, making it suitable for assessing our methods performance on general-purpose tasks beyond compositionality. To this end, we sample 100 prompts from the datasets test split for evaluation. Evaluation Metrics: Many of the SOTA automated evaluation metrics belong to the QA pair generation and Visual Question-Answering(VQA) framework. Prior works [13, 21, 25, 53, 58] have demonstrated its effectiveness and correlation with human-predictions when judging semantic accuracies. The ability to generate object/attribute centric yes/no questions over compositional text prompt allows for fine-grained assessment of image elements using MLLMs. We therefore follow [13] to generate grounded binary questions for T2I-Compbench and Flickr-Bench and utilize ConceptMixs existing set of questions that are generated simultaneously with the prompts and utilize GPT-4o as the choice VQA model following [55]. 4.2. Results GraPE Improves Compositional T2I Synthesis GraPE can be used with any given back-bone in plug-n-play manner, and substantially improves performance across the board for all of the 10 T2I models tested in this study, including SoTA models like SD3.5 [46]. Results on T2I Compbench and Flickr-Bench are present in Figure 3. Results on ConceptMix are present in Table 2. On T2I Compbench we see improvements ranging from 17.6% in the case of SD1.5 to nearly 2% for the latest SD3.5 Large model. On ConceptMix as well, we see large performance gains across prompt complexities K={1, 3, 5, 7}. For prompts with higher complexity (K=7), we see gains ranging from 3.1% to 35.5% across models. GraPE is more effective with complex T2I prompts. As seen in Table 2, the absolute performance difference between generation from base model vs our method increases as we go towards more complex prompts (from K=1 to K=7). For e.g., for SD XL, the gains compared to the base model increase from 2.9% for K=1 to 10.8% for K=7, and this observation remains constant across T2I models. exceeds performance on general T2I tasks GraPE Gains achieved by GraPE are not limited to compositional or complex prompts as is the case in T2I Compbench and ConceptMix datasets, rather it also generalizes to more widely used general T2I benchmarks such as Flickr-Bench. Fig. 3 shows that GraPE leads to large performance gains ranging from 2 to 23% across different models. GraPE reduces the gap between models of varied T2I capabilities GraPE leads to larger absolute performance increase for relatively less capable base diffusion models. This closes the gap between performance of such models with more capable ones. For e.g., for ConceptMix dataset (K=3), the difference between SD-1.5 and SD-3.5 reduces from 20.9% to 6.5% when comparing the base generations vs generations from GraPE respectively. scales well with more compute Our approach GraPE can be viewed from lens of using extra inference time compute (calls to the editing model) for creating images better aligned with complex prompts, achieving better performance on the T2I Synthesis task. GraPE flexibly trades off performance with the amount of compute (or the number of edits at inference time). See Fig. 4 for results on four representative models. This also shows that each planning step of GraPE is important on average for improving on the T2I synthesis task gradually. For additional results, including comparisons with other recent approaches, see Supplementary Section 8. 4.3. Ablations Naive Planner This section highlights the importance of object and attribute centric decomposition of plans to generate effective editing instructions. We modify the few-shot structure of the MLLM planner to skip the decomposition into image and text elements and prompt it to identify errors and suggest editing plans given the text-prompt and generated image. We refer to the pipeline thus constructed as GraPEnaive. Table 3 presents the results of both GraPE and GraPEnaive on K=7 subset of ConceptMix benchmark for subset of models. See Supplementary Section 7 for more ablations. Method Stable Diffusion v2.1 [42] LMD [31] SD-XL [37] PlaygroundV2.5 [30] Stable Diffusion v3.5 Large [46] Concept K=7 GraPEnaive GraPE 0.618 0.622 0.598 0.622 0.724 0.626 0.668 0.628 0.640 0.784 Table 3. Results on Concept-mix benchmark (K=7 subset). Comparing both GraPEnaive and GraPE (Using PixEdit) 4.4. Analysis and Insights We conduct detailed examination of each component in GraPE, focusing specifically on identifying and analyzing failure cases in both the MLLM planner and the editing model. For this purpose, we curate dataset of 120 imageprompt pairs, uniformly sampled across all 10 models and Figure 4. Trend of GPT-QA score with increasing number of editing steps. Figure 5. Average Steps per Plan for ConceptMix Benchmark across the models. 6 benchmarks. This dataset is carefully evaluated by six human annotators, compensated well above the national average hourly wage. Q1 Q2 Type Yes No No Partial Full Yes No GPT-4o 8.7 Qwen-VL-72B 25.4 74.5 26.9 25.4 74.5 35.7 30. 55.5 89.4 10.5 43.0 85.9 14.0 Table 4. Results from human study on MLLM planners. We present results with GPT-4o based planner used in GraPE and an open-source alternative Qwen-VL-72B [3] MLLM planner Accuracy For each image-prompt pair, along with the plan generated by GraPE, evaluators were asked to answer three multiple-choice questions: Q1: Does the generated image accurately and completely match the content described in the text prompt? Q2: To what extent does executing the proposed plan improve the images alignment with the text prompt? Q3: Are the instructions in the plan indivisible, meaning each instruction is fundamental action that cannot be further subdivided? Questions Q1 and Q3 require binary answers, with yes (1) or no (0) responses. Q2 offers three response options: no improvement, partial improvement and full alignment. This structured evaluation allows us to assess the accuracy and granularity of the MLLM planner in relation to image-prompt alignment. Our analysis reveals that Qwen-VL-72B as planner has around 18% more cases of no improvement compared to GPT-4o, largely due to generating considerable number of empty plans. This suggests that while Qwen-VL-72B may trail GPT-4o in visual capabilities it often fails to identify discrepancies, but tends to produce solid plan when it does recognize them, as evident from Table 4. We also carry out the annotation process on smaller model, Qwen-VL-7B which in contrast to the above planners performed poorly, generating approximately 100 empty plans out of 120 images, indicating significant limitations in both identifying discrepancies and generating actionable plans. Error Analysis of Editing Model We qualitatively take look at the failure cases of the editing model (PixEdit) specifically utilizing the above dataset to gather subset of plans which are deemed to fully align the image with the text prompt by the majority of evaluators (based on Q2 earlier). This subset consists of about 46 plans in which any errors in the final image solely result from the shortcomings of the image-editing model. We specifically look at images in which correct, partial or incorrect edits were performed by the model. Out of these 14 were completely correct, 17 partially correct, and 15 were incorrect. Within the correct ones, the distribution of correct add, remove and modify was 12, 0 and 2, respectively. Within the partially correct ones, the distribution of correct add, remove and modify was 11, 2 and 4, respectively. Figure 6 provides some representative examples. Fixing these issues, possibly through an RL based mechanism for incorporating feedback is direction for future work. This also points to the fact that the primary reason for any errors originates from the editing model, rather than because of the planner. Multi-Modal Planner as Evaluation Metric Can planning help evaluating T2I generative models? For this, we look at the analysis and mistake identification capabilities of the planner. Specifically, we post-process the plannergenerated output filtering out only the textual-image element comparison and pass this back to the planner which is provided zero-shot system prompt. This prompt directs the planner to generate score on 1-100 scale identifying the alignment based on the objects, their relationships and attributes. When compared with LLMScore [33], We observed moderate correlation of 0.423 and 0.404 when tested using Spearman and Pearsons test respectively. This shows the future potential of our MLLM planner to be used Figure 6. Results illustrating failure cases of Image-Editing model in conjunction with SoTA evaluation metrics to further improve them. Number of planning steps In Fig. 5, we see gradual increase in the average number of plans as we increase the prompt complexity from K=1 to K=7 using the ConceptMix benchmark. This shows our planner is able to recognize the complexity of the prompt and generate longer plan in response. We also see that on average, more capable T2I models like SD3.5 require fewer editing steps compared to weaker models such as SD1.5. Additional qualitative examples of our approach are shown in Supplementary Section 10. 5. Discussion and Limitations Summary: In this paper, we have looked at the task of finegrained image generation for compositional prompts. Unlike most exiting methods, which try to achieve this task in one-go, we take different route, and decompose the problem in 3-steps: (a) Generate an initial image based on the prompts (b) Identify the mistakes and plan out corrective steps by anaylzing the difference between textual and visual elements for each object (c) Execute the corrective steps via an editing model in sequential manner. The ability of our model to break the corrective steps into multiple simple edits is key to the success of our approach. Extensive experimentation on multiple datasets shows that our method can significantly improve the generation quality over 10 different models, including SoTA approaches. Limitations and future work: Limitations of our approach include its inability to handle certain kinds of complex prompts, and its dependence on pre-trained MLLM in the planning step. Future work includes working with more complex prompts, integrating our approach with models which explicitly change the architecture to better align with text, and incorporating corrective feedback using an RL based framework and extending to image editing tasks."
        },
        {
            "title": "References",
            "content": "[1] Aishwarya Agarwal, Srikrishna Karanam, Joseph, Apoorv Saxena, Koustava Goswami, and Balaji Vasan Srinivasan. A-star: Test-time attention segregation and retention for text-to-image synthesis, 2023. 2 [2] DeepFloyd Lab at StabilityAI. DeepFloyd IF: novel stateof-the-art open-source text-to-image model with high degree of photorealism and language understanding. https: //www.deepfloyd.ai/deepfloyd-if, 2023. Retrieved on 2023-11-08. 5, 6, 4 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 7 [4] Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20041 20053, 2023. 2 [5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Improving image generation with better Yufei Guo, et al. captions. Computer Science, 2(3):8, 2023. 1, 5, 6, 4 [6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023. 3, 2 [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. [8] Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, and Yezhou Yang. Getting it right: Improving spatial consistency in text-to-image models, 2024. 2 [9] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023. 2 [10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 5, 6, 4 [11] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. 2, 4 [12] Jaemin Cho, Abhay Zala, and Mohit Bansal. Visual programming for text-to-image generation and evaluation. In NeurIPS, 2023. 2 [13] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi PontTuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation, 2024. [14] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. ArXiv, abs/2105.05233, 2021. 1 [15] Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, and Graham W. Taylor. Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 3 [16] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis, 2023. 2, 5, 6, 4 [17] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, TrainingXin Eric Wang, and William Yang Wang. free structured diffusion guidance for compositional text-toimage synthesis. In The Eleventh International Conference on Learning Representations, 2023. 2 [18] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffusion for accurate instruction following, 2024. [19] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, Renrui Zhang, Junlin Xi, Wenqi Shao, Zhengkai Jiang, Tianshuo Yang, Weicai Ye, He Tong, Jingwen He, Yu Qiao, and Hongsheng Li. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers, 2024. 2 [20] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing, 2024. 2 [21] Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and Yezhou Yang. Benchmarking spatial relationships in text-to-image generation, 2023. 2, 6 [22] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3 [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. 6 [24] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. [25] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering, 2023. 6 Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. 2, 4 [26] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation, 2023. 2, 6 [27] Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. Comat: Aligning text-to-image diffusion model with imageto-text concept matching, 2024. 2 [28] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Text encoders bottleneck compositionality in contrastive visionlanguage models, 2023. 2, [29] Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, and Siva Reddy. Learning action and reasoning-centric image editing from videos and simulations, 2024. 3, 4, 1, 2 [30] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. 5, 6, 7, 4 [31] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llmgrounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models, 2024. 2, 3, 5, 6, 7, 4 [32] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models, 2024. 2 [33] Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. LLMScore: Unveiling the power of large language models in text-to-image synthesis evaluation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 8 [34] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. ArXiv, abs/2102.09672, 2021. [35] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image genIn International eration with dense blob representations. Conference on Machine Learning (ICML), 2024. 2 [36] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models, 2016. 6 [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 5, 6, 7, 1, 4 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 2, 4 [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. 2 [41] Recraft AI. Recraft-V3. https://www.recraft.ai/ ai-image-generator, 2024. [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 5, 6, 7, 4 [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic textto-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022. 1 [44] Harman Singh, Poorva Garg, Mohit Gupta, Kevin Shah, Ashish Goswami, Satyam Modi, Arnab Mondal, Dinesh Khandelwal, Dinesh Garg, and Parag Singla. Image manipulation via multi-hop instructions - new dataset and In Proceedweakly-supervised neuro-symbolic approach. ings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 29753007. Association for Computational Linguistics, 2023. 3 [45] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. 1 [46] stability.ai. Stable diffusion 3.5. https://stability. ai/news/introducingstablediffusion35, 2024. 1, 6, 7, 4 [47] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao [56] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformer, 2024. 2 [57] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In International Conference on Machine Learning, 2024. 2, 3, [58] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving textimage alignment evaluation, 2023. 6 [59] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it?, 2023. 2, 4 [60] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing, 2024. 3 [61] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. Hive: Harnessing human feedback for instructional visual editing. arXiv preprint arXiv:2303.09618, 2023. [62] Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang, Honglak Lee, and Irfan Essa. Text as neural operator: Image manipulation by text instruction. In Proceedings of the 29th ACM International Conference on Multimedia, pages 1893 1902, 2021. 3 [63] Xinyu Zhang, Mengxue Kang, Fei Wei, Shuang Xu, Yuhe Liu, and Lin Ma. Tie: Revolutionizing text-based image editing for complex-prompt following and high-fidelity editing, 2024. [64] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-next: Making lumina-t2x stronger and faster with next-dit, 2024. 2 Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. 2 [48] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visiolinguistic compositionality. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 52285238, 2022. 4 [49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and finetuned chat models, 2023. 2 [50] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. arXiv preprint arXiv:2407.05600, 2024. 2 [51] Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-to-image diffusion with token-level supervision, 2024. [52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. 3 [53] Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajic, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset, and Aida Nematzadeh. Revisiting text-to-image evaluation with gecko: On metrics, prompts, and human ratings, 2024. 6 [54] Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model, 2023. 2 [55] Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. Conceptmix: compositional image generation benchmark with controllable difficulty, 2024. 6 GraPE : Generate-Plan-Edit Framework for Compositional T2I Synthesis"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Algorithm: We present the algorithm showcasing the sequence of steps in GraPE below. Algorithm 1 Generate-Plan-Edit : GraPE Require: Text Prompt: , T2I Model: G, MLLM: P, Editing Model: and Few-Shot Examples: (cid:2)E1 Ep #Generate Ig G(T ) Initial Generated Image (cid:3) #Plan {Te1, Te2, Ten , } P(cid:0)Ig, T, (cid:2)E1 Ep (cid:3)(cid:1) #Edit Ie0 Ig for = 1, , n-1 do Iek+1 E(Iek , Tek+1 ) Intermediate Edited Image end for Ieo Ien 7. Ablations: Final Edited Image We provide the system prompt and an in-context prompting example in Fig. 9 for the GraPEnaive pipeline used in section 4.3. The planner follows the system prompt to generate reasoning and editing instructions from the given image and text prompt, without the proposed decomposition into image and text based elements. 8. Additional and Detailed Results: Comparison with SOTA LLM-Based Approaches We compare the performance of GraPE against RPG [57], recent LLM-based image generation method. RPG leverages an LLM to decompose complex image generation tasks into simpler subtasks within localized subregions and employs complementary-regional-diffusion to coherently merge the generated subregions into unified image. The comparison is conducted on the ConceptMix benchmark, with RPG using SD-XL [37] as its base model, while GraPE employs the PixEdit editing model. Results are summarized in Table 5. Our evaluation reveals that RPG fails to generate valid plans for over 50% of the samples in the comparison, primarily due to frequent parsing errors. For the remaining samples, GraPE demonstrates significant improvements in GPT-QA accuracy over images generated by RPG. Specifically, GraPE achieves gains of 21.5%, 3.0%, 18.0%, and 20.3% for K=1, K=3, K=5, and K=7, respectively. Table for T2I-Bench and Flickr-Bench Table 10, 11 presents the absolute values of GraPE with PixEdit and AURORA [29] as editing models, for the graphs in Fig. 3. Results on ConcetpMix Benchmark using GraPE with AURORA Table 9 presents the numbers on ConceptMix Dataset using GraPE with AURORA editing model, this table complements Table 2, where PixEdit was used as the editing model. Performance on Editing Benchmarks We employ two established benchmarks to evaluate and compare the editing performance of PixEdit with other models. The first benchmark, AURORA-BENCH, consists of image-edit instruction pairs sourced from eight distinct datasets. This benchmark enables the assessment of the discriminative editing capabilities of models across wide variety of images. Performance is measured using an automated metric called DiscEdit [29]. The second benchmark we evaluate is the MagicBrush Test-set, which comprises 1K editing turns, including both single-turn and multi-turn edits. For quantitative analysis on this benchmark, we utilize standard metrics such as L1, L2, and CLIP/DINO similarity scores, providing comprehensive evaluation of the editing performance. The results in Table 6 indicates strong discriminative capability in PixEdit compared to baselines. Table 7 shows that PixEdit is great at single turn edits and follows closely with baselines in multi-turn edits. 9. Implementation Details: 9.1. Prompts and Few-shot Examples: Figure 7 and Figure 8 illustrate the full system prompt and selection of few-shot examples employed in the MLLMbased planner for GraPE. 9.2. Hyperparameters Generation To generate images using various diffusion models, we adhere to the default hyperparameters specific to each model, such as the number of inference steps and the sampling method. All images are generated with fixed random seed of 0 to ensure reproducibility. Figure 7. System-prompt used with GraPEs MLLM Planner Method Concept K=1 Concept K=3 Concept K=5 Concept K= Base +GraPEPixEdit Base +GraPEPixEdit Base +GraPEPixEdit Base +GraPEPixEdit RPG [57] 0.6960.015 0.845 0.008 0.694 0. 0.715 0.007 0.583 0.002 0.688 0.005 0.388 0.007 0.467 0.005 Table 5. Comparison of RPG [57] and GraPE on ConceptMix Model WhatsUp Something AG Kubric CLEVR MagicBrush AURORA PixEdit 0.472 0.565 0.566 0.371 0.548 0.613 0.477 0.583 0.606 0.392 0.592 0.400 0.400 0.450 0.600 Solver++ in combination with the PixEdit model to generate edited images, ensuring reproducibility by using fixed random seed of 0. Table 6. Performance comparison of PixEdit with other models on AURORA-BENCH using DiscEdit computed as specified in [29]. Note: The baseline scores are taken from [29] Planning We utilize GPT-4o with sampling temperature set to 0, ensuring deterministic outputs for both planning and Visual Question Answering (VQA) tasks. Editing For image editing, we use AURORA with 50 inference steps and its default sampler as described in its paper. Additionally, we employ 14 inference steps with DPM9.3. PixEdit: This section details the implementation of PixEdit model. We chose PixArt-sigma [11] as our backbone T2I model which we convert into an editing checkpoint. We follow similar training strategy as used in [6, 29] i.e adding additional channels in the initial convolution layer of the diffusion model and use the randomly sampled noise concatenated with input image as input to de-noise into editedimage. We started with the SEED-EDIT [20] dataset for initial pre-training stage. We skip the automatically generated data and use the 147K sample subset of this dataset that are either procured from the web or manually created to enFigure 8. Selective Few shot examples used in GraPEs MLLM Planner Settings Methods L1 L2 CLIP-I DINO CLIP-T Single-turn Multi-Turn MagicBrush AURORA PixEdit MagicBrush AURORA PixEdit 0.0788 0.0754 0.0719 0.0921 0.0904 0.0892 0.0274 0.0270 0. 0.0327 0.0334 0.0357 0.8978 0.9105 0.9082 0.8777 0.8887 0.8729 0.8313 0.8594 0.8661 0.7996 0.8272 0.8068 0.2973 0.2981 0. 0.3020 0.3005 0.2947 Table 7. Comparison of methods across Single-turn and Multi-turn settings on Magic-Brush Test set. Note: The numbers were reproduced using the publicly available codebases of the respective baselines with the same random seed. sure high quality pre-training data. The model is trained for 32K steps with an effective batch size of 256 at this stage. We further fine-tune this model for 42K steps on the mixture of editing dataset proposed in [29] to create PixEdit. 9.3.1. Analysis of Improvement in PixEdit PixEdit introduces two significant advancements over AURORA: Upgraded Backbone Diffusion Model: Transitioning from SD1.5 to PixArt-Sigma. Refined Pre-Training Data: Replacing the InstructPix2Pix dataset with the real-only subset of the Seed-Edit dataset for pre-training. To isolate the impact of change (2), we compare PixEdit with SD1.5 adapted for editing using the same Seed-Edit dataset and evaluate their performance on the = 7 subset of the ConceptMix benchmark. Results in Table 8 show that SD1.5 adapted with Seed-Edit data, and subsequently finetuned using the Aurora data-mixture, performs slightly better than AURORA alone on average. This highlights the advantage of incorporating real-world, complex pre-training data. PixEdit, leveraging its more powerful PixArt-Sigma backbone and compositional text-encoder, achieves the best performance on average. This underscores the benefits of combining an advanced diffusion model with improved text-image alignment for editing tasks. Figure 9. System-prompt and few-shot example format for GraPEnaive Method Stable-Diffusion v1.5 [42] Structure Diffusion [16] Stable Diffusion v2.1 [42] LMD [31] SD-XL [37] PixArt-α [10] DeepFloyd IF [2] PlaygroundV2.5 [30] Dalle3 [5] Stable Diffusion v3.5 Large [46] Concept K=7 GraPEAURORA GraPESD1.5 0.598 0.003 0.612 0.003 0.613 0.009 0.625 0.005 0.624 0.010 0.624 0.006 0.638 0.006 0.611 0.002 0.718 0.006 0.765 0.003 0.618 0.000 0.596 0.005 0.588 0.003 0.649 0.004 0.615 0.008 0.614 0.006 0.672 0.007 0.722 0.004 0.719 0.007 0.761 0.007 GraPEPixEdit 0.610 0.005 0.571 0.002 0.626 0.002 0.668 0.009 0.628 0.002 0.625 0.002 0.662 0.006 0.640 0.005 0.737 0.004 0.784 0.005 Table 8. GPT-QA scores using GraPE with different editing models SD1.5 vs AURORA vs PixEdit. Note GraPESD1.5 represents the editing model based on SD1.5 trained with refined pre-training data and fine-tuned on Aurora data-mixture. 10. Additional Qualitative Examples: Errors due to Planner vs Editing Model We try to understand the broader context, consider hypothetical set of 100 plans (randomly sampled from the human study on MLLM planners). We look at the breakup of these 100 plans and how the planner and editing-model fare on them. 74.5% of the plans are deemed to have generation errors, 55.5% (41.3 plans in absolute terms) of which are completely corrected at the planner level (considering GPT-4o) indicating strong alignment between the generated plan and the text prompt. We then note that about one-third of these ( 30.5% or 12.5 images) are successfully edited to produce completely correct final images, showcasing the editing models ability to translate plans into precise visual outputs. The remaining two-thirds of the aligned plans ( 69.5% or 28.8 images) fall short, with errors entirely attributable to the editing model. This underscores that while the planner demonstrates robust performance in generating accurate and contextually aligned plans, the editing model remains the predominant source of errors. Editing Errors in Perfect Plans The subset of 46 edit plans showing full improvement as per the majority of human annotators chosen for qualitative error analysis predominantly involved plans requiring add actions, with relatively fewer plans focusing on remove or modify actions. This can also be seen from the breakdown of the corresponding final edited images on the basis of mentioned actions. This distribution reflects the natural bias of generative models which often fail to identify certain textual elements and the image generated then has errors which can be fixed by add edit actions. Plans showing no or partial improvement Finally, considering the plans which show no/partial improvement we note that distinct patterns emerge providing further insights into the limitations of the planner. The plans with no improvement are primarily empty plans generated by the planner, where it fails to propose any actionable edits to address the misalignment between the text prompt and the image. On the other hand, the plans with partial improvement typically exhibit incomplete details, where some, but not all, elements of the prompt are addressed. small subset of these also includes hallucinations, where the planner mistakenly assumes the presence of an object in the image that does not actually exist. These observations highlight areas where the planner could be enhanced, particularly in terms of improving its ability to identify actionable edits and reducing cases of overor under-specification in the generated plans. Method Concept K=1 Concept K=3 Concept K=5 Concept K=7 Base GraPEAURORA Base GraPEAURORA Base GraPEAURORA Base GraPEAURORA Stable-Diffusion v1.5 [42] Structure Diffusion [16] Stable Diffusion v2.1 [42] LMD [31] SD-XL [37] PixArt-α [10] DeepFloyd IF [2] PlaygroundV2.5 [30] Dalle3 [5] Stable Diffusion v3.5 Large [46] 0.808 0.009 0.823 0.002 0.833 0.002 0.855 0.004 0.848 0.010 0.813 0.010 0.883 0.009 0.908 0.010 0.947 0.002 0.927 0.005 0.863 0.005 0.895 0.004 0.875 0.014 0.907 0.008 0.893 0.006 0.883 0.008 0.898 0.005 0.945 0.004 0.950 0.004 0.927 0.002 0.606 0.018 0.606 0.002 0.639 0.014 0.711 0.008 0.708 0.018 0.668 0.011 0.680 0.016 0.737 0.023 0.832 0.012 0.815 0.002 0.717 0.024 0.734 0.021 0.719 0.012 0.765 0.005 0.740 0.013 0.717 0.012 0.735 0.016 0.783 0.012 0.809 0.007 0.812 0.004 0.497 0.010 0.542 0.014 0.579 0.012 0.643 0.011 0.635 0.014 0.649 0.011 0.663 0.014 0.658 0.015 0.812 0.014 0.803 0. 0.688 0.010 0.698 0.015 0.694 0.009 0.721 0.012 0.692 0.018 0.711 0.022 0.713 0.007 0.709 0.006 0.783 0.015 0.792 0.002 0.450 0.005 0.447 0.001 0.466 0.002 0.591 0.002 0.520 0.003 0.507 0.001 0.583 0.002 0.540 0.003 0.728 0.006 0.759 0.004 0.598 0.003 0.612 0.003 0.613 0.009 0.625 0.005 0.624 0.010 0.624 0.006 0.638 0.006 0.611 0.002 0.718 0.006 0.765 0.003 Table 9. Results on Concept-mix benchmark - GraPEAURORA Method Base GraPEAURORA GraPEPixEdit DSG (w/o dep) DSG DSG (w/o dep) DSG DSG (w/o dep) DSG Stable-Diffusion v1.5 [42] Structure Diffusion [16] Stable Diffusion v2.1 [42] LMD [31] SD-XL [37] PixArt-α [10] DeepFloyd IF [2] PlaygroundV2.5 [30] Dalle3 [5] Stable Diffusion v3.5 Large [46] 0.679 0.006 0.714 0.004 0.701 0.001 0.782 0.001 0.805 0.002 0.710 0.004 0.822 0.002 0.788 0.002 0.932 0.003 0.871 0.003 0.654 0.007 0.697 0.004 0.688 0.000 0.777 0.002 0.795 0.001 0.699 0.006 0.811 0.004 0.775 0.004 0.924 0.003 0.864 0.004 0.758 0.008 0.760 0.017 0.757 0.012 0.805 0.005 0.844 0.004 0.758 0.006 0.840 0.008 0.821 0.011 0.913 0.014 0.876 0. 0.746 0.009 0.741 0.014 0.742 0.014 0.790 0.006 0.829 0.005 0.744 0.009 0.819 0.011 0.801 0.011 0.910 0.015 0.866 0.007 0.788 0.006 0.794 0.006 0.787 0.002 0.836 0.008 0.839 0.003 0.796 0.006 0.852 0.001 0.825 0.006 0.934 0.003 0.888 0.003 0.769 0.006 0.777 0.008 0.772 0.004 0.826 0.008 0.828 0.002 0.788 0.006 0.840 0.001 0.808 0.006 0.928 0.002 0.880 0.003 Table 10. GPT-QA scores on T2I Comp-Bench, GraPEX refers to using GraPE with editing model Method Base GraPEAURORA GraPEPixEdit DSG (w/o dep) DSG DSG (w/o dep) DSG DSG (w/o dep) DSG Stable-Diffusion v1.5 [42] Structure Diffusion [16] Stable Diffusion v2.1 [42] LMD [31] SD-XL [37] PixArt-α [10] DeepFloyd IF [2] PlaygroundV2.5 [30] Dalle3 [5] Stable Diffusion v3.5 Large [46] 0.703 0.004 0.725 0.012 0.720 0.009 0.682 0.006 0.695 0.004 0.735 0.006 0.831 0.003 0.789 0.005 0.891 0.004 0.881 0.004 0.640 0.005 0.648 0.014 0.656 0.007 0.640 0.012 0.613 0.003 0.663 0.005 0.792 0.003 0.735 0.007 0.857 0.005 0.852 0.004 0.800 0.016 0.816 0.003 0.800 0.010 0.752 0.008 0.775 0.010 0.792 0.005 0.842 0.006 0.806 0.010 0.890 0.002 0.905 0. 0.748 0.017 0.766 0.002 0.740 0.010 0.697 0.008 0.720 0.011 0.733 0.005 0.803 0.007 0.759 0.012 0.847 0.001 0.873 0.004 0.787 0.005 0.802 0.005 0.821 0.001 0.744 0.002 0.818 0.005 0.811 0.011 0.877 0.003 0.833 0.005 0.904 0.004 0.909 0.003 0.742 0.006 0.732 0.006 0.757 0.001 0.697 0.004 0.758 0.007 0.767 0.011 0.843 0.005 0.789 0.004 0.873 0.007 0.884 0.003 Table 11. GPT-QA scores on Flick-Bench, GraPEX refers to using GraPE with editing model Figure 10. Results illustrating failure cases of generated Edit Plans Figure 11. More results illustrating failure cases of Editing Models"
        }
    ],
    "affiliations": [
        "IISc Bangalore",
        "IIT-Delhi"
    ]
}