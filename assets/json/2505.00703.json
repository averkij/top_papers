{
    "paper_title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT",
    "authors": [
        "Dongzhi Jiang",
        "Ziyu Guo",
        "Renrui Zhang",
        "Zhuofan Zong",
        "Hao Li",
        "Le Zhuo",
        "Shilin Yan",
        "Pheng-Ann Heng",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 3 0 7 0 0 . 5 0 5 2 : r T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT Dongzhi Jiang1, Ziyu Guo2, Renrui Zhang1, Zhuofan Zong1, Hao Li3 Le Zhuo1,3, Shilin Yan, Pheng-Ann Heng2, Hongsheng Li1 1CUHK MMLab 2CUHK MiuLar Lab 3Shanghai AI Laboratory {dzjiang, ziyuguo, renruizhang}@link.cuhk.edu.hk hsli@ee.cuhk.edu.hk Equal Contribution Project Leader"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models have demonstrated how chain-ofthought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, novel reasoning-enhanced text-to-image generation model, powered by RL with bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1."
        },
        {
            "title": "Introduction",
            "content": "The emergence of advanced Large Language Models (LLMs) [45, 47, 63, 72], such as OpenAI o1 [48] and DeepSeek-R1 [17], has demonstrated considerable reasoning capabilities across domains including mathematics [1, 20, 42] and coding [6, 2, 23]. Through reinforcement learning (RL) [55, 56], these models analyze problems progressively with comprehensive Chain-of-Thought (CoT) [66, 27, 19, 25, 77, 18] before providing answers, significantly enhancing output accuracy. The CoT reasoning strategies have also been extended to the visual domain. Recent Large Multi-modal Models (LMMs) [5, 43, 75, 78] have adapted the paradigm to accommodate the visual understanding task [40, 77, 26]. These advanced LMMs can jointly process images and their associated textual queries, performing step-by-step analyses of visual details and integrating them with reasoning steps to derive final answers. Concurrently, CoT-like reasoning has been initially investigated in the visual generation task, particularly in autoregressive text-to-image generation. The pioneering work, Image Generation with CoT [19], regards the progressive generation of the image tokens as kind of CoT analogous to that of the text tokens, and proposes to optimize this intermediate process to enhance the image quality. Figure 1: The Illustration of CoT in Image Understand and Generation Tasks. In the image understanding task, the CoT is the textual reasoning process. In the autoregressive visual generation task, we identify two levels of CoT: the semantic-level and token-level CoT. The semantic-level CoT is the high-level planning prior to the image generation, in the form of text. The token-level CoT is the intermediate patch-by-patch generation process, focusing on the local pixel details within patch, in the form of image tokens. Despite these advances, the exploration of CoT for image generation remains preliminary. Unlike image understanding, image generation requires the complex interpretation of cross-modal alignment and the synthesis of fine-grained visual details. To address these challenges, we identify two distinct levels of CoT reasoning that can be leveraged to enhance image generation, as illustrated in Fig. 1: Semantic-level CoT is the textual reasoning about the image to generate, which is introduced prior to the image generation. The semantic-level CoT designs the global structure of the image, e.g., the appearance and location of each object. In case the prompt requires reasoning shown in Fig. 2, the semantic-level CoT also helps to deduce the objects to generate. Optimizing the semantic-level CoT could explicitly manage the planning and reasoning of the prompt before the subsequent image tokens generation, making the generation easier. Token-level CoT is the intermediate patch-by-patch generation process of the image, as originally introduced in [19]. This process could be viewed as form of CoT as it outputs each subsequent token conditioned on all previous tokens within discrete space, similar to the textual CoT. Unlike semantic-level CoT, token-level CoT focuses on low-level details like pixel generation and maintaining visual coherence between adjacent patches. Optimizing the token-level CoT can enhance both the generation quality and the alignment between the prompt and the resulting images. Despite recognizing these two levels of CoT, critical question remains unaddressed: How can we enhance and coordinate them for text-to-image generation? Current mainstream generative models [58, 61, 53, 28] are trained exclusively on generation targets, lacking the explicit textual understanding required for semantic-level CoT reasoning. Although introducing separate model (e.g., an LLM) specifically for prompt interpretation [9] is technically feasible, this approach would significantly increase computational costs, complexity, and deployment challenges. Recently, trend has arisen to merge visual understanding and generation within single model. Building upon LMMs, these unified LMMs (ULMs) [67, 70, 79, 7] could not only understand the visual inputs but also generate images from text prompts. However, their two capabilities are still decoupled, typically pre-trained in two independent stages, with no clear evidence that the understanding capabilities can 2 Figure 2: Visualization of the Image Generation Process of T2I-R1. All the prompts need reasoning or contain an uncommon scenario. We observe that T2I-R1 successfully deduces the true intention behind the prompt or provides sensible imagination for the uncommon scenario (highlighted in the text) to produce satisfying result compared with the baseline model, Janus-Pro. benefit generation. Given these potentials and issues, we start from ULM and enhance it to unite both the semantic-level and token-level CoT into one framework for text-to-image generation. To fulfill our target, we introduce BiCoT-GRPO, an RL method to jointly optimize the two levels of CoT for ULM. We opt for RL instead of supervised fine-tuning (SFT) for two reasons: First, the ULM has possessed the fundamental ability needed for the semantic-level and token-level CoT; our goal is only to elicit the fusion of these two abilities by guiding the models self-exploration. Second, RL methods have proven highly effective for enhancing reasoning capabilities, which are essential for both levels of CoT. Specifically, we first instruct the ULM to imagine and plan the image based on the prompt to obtain the semantic-level CoT. Then, we feed it into the ULM as the condition for the subsequent image generation for token-level CoT. We simultaneously generate multiple images from each prompt and then compute group-relative reward to optimize both levels of CoT within the same iteration. Unlike understanding tasks, where clearly defined rules for rewards exist, image generation lacks such standardized rules. Therefore, we propose to utilize an ensemble of diverse vision experts [68, 64, 38, 19] as reward models. This reward design serves two critical purposes: it evaluates generated images from multiple dimensions to ensure reliable quality assessment, while also functioning as regularization method to prevent the ULM from hacking single reward model. Through the proposed reasoning strategies, we obtain T2I-R1, the first reasoning-enhanced text-toimage model combining the semantic-level and token-level CoT. Empirical results show that our approach outperforms baseline models by 13% and 19% improvements on the T2I-CompBench and WISE benchmark, and even surpasses the previous state-of-the-art model FLUX.1. Qualitative analysis reveals that our method empowers the model to generate more human-aligned results by reasoning about the true intentions behind the prompt and demonstrates enhanced robustness when dealing with uncommon scenarios. Our contributions are summarized as follows: 1. We identify dual-level reasoning process in the autoregressive image generation task by introducing the semantic-level and token-level CoT, which decouple high-level image planning from low-level pixel generation for more reliable generation. 2. We develop BiCoT-GRPO, new reinforcement learning framework that jointly optimizes both levels of CoT reasoning, seamlessly integrating the understanding capabilities of ULMs for image generation. For reward modeling, we investigate robust reward system utilizing an ensemble of vision experts. 3. Our resulting model, T2I-R1, that incorporates both levels of CoT using BiCoT-GRPO, demonstrates significant performance improvements and surpasses FLUX.1 across multiple established benchmarks."
        },
        {
            "title": "2 Related Work",
            "content": "Unified Generation and Understanding LMM. Recently, the effort to unify image generation and understanding in single LMM has attracted much attention. Building upon large language models (LLMs), it is natural for the LMMs to understand the image and output the text [46, 30, 81, 16, 76]. However, the method of how to generate an image from LMM is still under exploration. The image generation method diverges into different branches. One line of the method relies on an exterior image generation model to complete generation [11, 60, 59, 34, 62, 13, 80, 29]. The generator often utilizes text-to-image diffusion models [53, 49] due to its powerful generation capability. To deliver the generation information, the LMM passes either the implicit conditional feature or the explicit image prompt to the generator. For example, EMU [60] first trains the LMM to output CLIP [51] image features identical to that input to the LMM. Then, pretrained UNet [54] of Stable Diffusion [53] receives the output feature as the condition to generate an image. Another line of the method seeks to train the LMM to generate discrete tokens produced by VQGAN [12] to eliminate the need for an additional generator. [65, 32] directly adopts the VQGAN encoder as the image tokenizer for LMM. However, the VQGAN encoder is only pretrained on the image reconstruction task and thereby generates visual tokens less helpful for image understanding. To improve the understanding capability, [67, 7, 41, 36] proposes to tackle the understanding and generation tasks with different vision encoders separately. The CLIP encoder deals with image input for understanding, while the VQGAN encoder is responsible for generation. Moreover, some works [69, 50, 57] attempt to empower the vision encoder with both the understanding and the generation capability. VILAU [69] trains vision encoder with both the contrastive loss [51] for text-image understanding and reconstruction loss [12] for image detail preserving. Thanks to the joint pretraining, the vision encoder could generate text-aligned discrete visual tokens. The LMM is then trained to receive the discrete tokens for image understanding and predict them for image generation. Reinforcement Learning for Large Reasoning Models. The emergence of OpenAI o1 [48] has gained tremendous attention in developing the reasoning capability of large language models. Later, DeepSeek-R1 [17] proposes rule-based reward and GRPO training method. The introduced method instructs the model to perform an extensive reasoning process before generating the final answer. The reward only focuses on the correctness of the final answer and the following of the pre-defined format. Recently, number of works have applied this method to multi-modal large language models [5, 43, 73, 75, 10, 22] with task-specific rewards like correctness and IoU [39]. This training paradigm largely helps various reasoning-intensive tasks [52, 26, 18] like mathematical problem-solving [20, 42, 40, 77, 78] and code generation [6, 2, 23]."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary Recently, the employment of reinforcement learning has been the dominant approach to elicit the reasoning capability of the large models. [56] introduces GRPO, enhancing PPO by eliminating the value function and estimating the advantage in group-relative manner. For specific prompt-answer pair (p, a), group of individual responses {oi}G i=1 is sampled from the old policy πθold. Each response is then input to reward function to obtain the individual reward Ri. Then, the advantage of the i-th response is calculated by normalizing the rewards {Ri}G i=1 of the group: Ai = Ri mean({Ri}G std({Ri}G i=1) i=1) . (1) GRPO adopts clipped objective similar to PPO. Besides, KL penalty term between the current policy πθ and the reference model πθref is directly added in the loss function: JGRPO(θ) = (q,a)D,{oi}G (cid:34) 1 i=1 oi (cid:80)G i=1πθold (q) oi (cid:88) (cid:32) (cid:16) min (cid:88) ri,t(θ) ˆAi, clip i=1 t=1 4 (cid:16) ri,t(θ), 1 ε, 1 + ε (cid:17) (cid:17) ˆAi (cid:33)(cid:35) βDKL(πθπref) , (2) Figure 3: Framework of BiCoT-GRPO. In step 1, we instruct the model to generate the semanticlevel CoT based on the image prompt. In step 2, images are generated conditioned on both the image prompt and semantic-level CoT, with the intermediate generation process serving as token-level CoT. The resulting images are evaluated by an ensemble of vision experts to obtain rewards. We generate images from each prompt to compute the group-relative reward and perform GRPO training. where ri,j(θ) is the ratio between the probabilities of πθ and πθold for outputting the current token: ri,j(θ) = πθ(oi,j q, oi,<j) πθold(oi,t q, oi,<j) . (3) In text reasoning tasks like mathematical problem solving, the model is instructed to follow the pre-defined template to output the reasoning process and final answer. The reward functions are rule-based rewards that only check the correctness of the final answer and the output format. 3.2 Semantic-level and Token-level CoT In the autoregressive text generation tasks of LLMs and LMMs, CoT occurs in the textual reasoning format. However, in autoregressive image generation tasks, we identify two distinct types of CoT that could enhance the image generation at different abstraction levels: Semantic-level CoT. Semantic-level CoT is defined as the textual reasoning that precedes image generation, serving as an overall semantic planning stage for the intended image. This process mirrors human artistic creation: when given brief prompt, an artist first thinks about the scene construction, considering object attributes, spatial relationships, and interactions. In addition to the planning for common prompts, we also observe the semantic-level CoT benefits two other scenarios. If the prompt does not directly depict the object to generate, the semantic-level CoT can reason about the true intention from the users prompt, providing more aligned images. As illustrated in Fig. 2, the semantic-level CoT reasons that the flower cultivated in the country where Amsterdam is located is tulip. Without this semantic-level CoT, Janus-Pro fails to provide valid results. Additionally, the semantic-level CoT demonstrates importance when handling unusual or potentially ambiguous scenes. In the bottom example of Fig. 2, when given the prompt pig on the bottom of train, semanticlevel CoT introduces the action lying for the pig, creating more sensible scenario. In contrast, direct generation without this interpretive imagination creates significant confusion for Janus-Pro. Formally, each semantic-level CoT si is composed of si text tokens {si,1, si,2, ..., si,si}. 5 Token-level CoT. Unique to the image generation task, token-level step-by-step thinking exists in the image generation process. The generation of image tokens much resembles chain of thought: the image tokens are generated patch by patch, where the current patch is generated based on the previous ones. We define the sequential generation of image tokens as token-level CoT. This process parallels how an artist progressively fills canvas, with the generated patches forming visual reasoning chain that maintains coherence across the image. This chain of patches is later reshaped to 2D grid Rhwc and input to an image decoder to obtain the image. Unlike semantic-level CoT, which addresses global planning, token-level CoT focuses on local details and visual coherence across the image space. Formally, each token-level CoT ti consists of image tokens {ti,1, ti,2, ..., ti,M }, where represents the resolution of the generated image, i.e., = w. 3.3 BiCoT-GRPO GRPO has been proven to be highly effective for exploring the reasoning capability of the LLMs and LMMs. To accommodate both semantic-level and token-level CoT in image generation, we propose BiCoT-GRPO, where the model reasons twice in single generation process. We instruct the model to first perform semantic-level CoT for global planning, and then dive into the local details by performing token-level CoT. However, compared with the task of text generation, great pipeline challenge is posed for incorporating two levels of CoT for image generation. Limited by the training paradigm, most current ULMs cannot generate interleaved images and text themselves. manual signifier is often needed to instruct the model on which task to perform, either text generation or image generation. For Janus-Pro to generate an image, which is the ULM we use in this work, we need to manually concatenate an image start token (<img_start>) to explicitly instruct the model to start generating image tokens. To tackle this problem, we propose novel pipeline to facilitate ULM in generating images with two levels of CoT, as shown in Fig. 3. Specifically, our pipeline is composed of two-step generation process. The first step is to generate the semantic-level CoT. We input the image prompt and instruct the model to imagine and reason about the details of the image to generate semantic-level CoT {si}G i=1. The second stage focuses on the token-level CoT generation. We input the image prompt, the generated semantic-level CoT in the first stage, and the image start token to the ULM for generating image tokens {ti}G i=1. Then, the image tokens are input to the image decoder to obtain the image I. Since there exist two types of CoT in our method, first the semantic-level CoT and then the token-level CoT. Each response oi is composed of two parts, namely oi = (si, ti). In this sense, the ri,j(θ) is converted to: ri,j(θ) = πθ(oi,j q, oi,<j) πθold (oi,j q, oi,<j) = πθ(si,j q,si,<j ) πθold (si,j q,si,<j ) , πθ(ti,j q,si,ti,<j ) πθold (ti,j q,si,ti,<j ) , 0 si si < si + (4) Then, we update the ULM by maximizing Equation 2. In practice, we incorporate the token-level policy gradient loss in [74], where the loss term is normalized over all the generated tokens to balance the reward on overly long semantic-level CoT. 3.4 Ensemble of Generation Rewards Unlike DeepSeek-R1 with the rule-based reward, assessing the images based on pre-defined rules is infeasible. The assessment of the image includes various aspects, including the aesthetic appeal and objects existence, attributes, and relationships. Considering the complexity, we introduce an ensemble of vision experts to judge the generated image from multiple aspects. Meanwhile, the use of multiple reward functions also serves as regularization method to prevent the ULM from hacking into specific reward model. As shown in Fig. 4, the ensemble contains the following experts: Human Preference Model. Human preference models (HPMs), such as HPS [68] and ImageReward [71], are trained to simulate human aesthetic preferences. These models are developed using datasets of human rankings on synthetic images, where annotators evaluate and compare generated outputs. During inference, these models assess both the aesthetic quality and prompt alignment of Figure 4: Illustration of the Ensemble of Generation Rewards. We use GPT-4o mini to extract the objects and their attributes before training. Each specialized reward model receives customized information inputs for the reward calculation. We take the average of all the rewards as final reward. generated image, producing composite human preference score RHPM. This expert provides holistic reward signal from general perspective. Object Detector. Another option of the reward model is an object detector, e.g., GroundingDINO [38] and YOLO-world [8]. These open-vocabulary detection models accept an image along with object queries as input and output both the spatial positions and confidence scores for detected objects. This kind of vision expert serves as an ideal tool to evaluate the objects existence and relationship concerning space and numbers. For implementation, we extract all objects obji i=1 from the training image prompts, where represents the total number of objects. We then query the object detector to identify these objects within the generated image. For each object, we assign binary existence score (1 if detected, 0 otherwise) and average these scores across all objects in the prompt. If the prompt contains spatial relationship, we further leverage the detected location to validate its correctness. We calculate the relative distance and intersection over union (IoU) between the objects for the spatial score Rspatial. If the number of the object nobji is specifically pointed out in the prompt, we compare the number with the detected number of the object ˆnobji. The reward from the object detector RDet is determined as: RDet = (cid:80)K αRspatial + (1 α) 1 1 1 I(nobji = ˆnobji), I(obji detected), (cid:80)K i=1 i= (cid:80)K i=1 I(obji detected), if spatial relationship in the prompt, if number in the prompt, else, where Rspatial is 1 if the relative distance between the objects are larger than threshold and the direction is right. If the direction is wrong, the reward is 0. Otherwise, we use the IoU as the spatial reward. We set α as 0.6 to encourage the correctness of the spatial relationship. Visual Question Answering Model. The visual question answering (VQA) models are trained to answer questions based on the image input. The VQA models include earlier models prior to LLM, e.g., BLIP [33] and GIT [64], and LMMs like LLaVA [35]. We leverage these models to judge the existence and attributes of the objects. For example, if the image prompt is red dog and yellow cat, we first reformat each individual object obji with its attribute as question to the VQA model, i.e., red dog? and yellow cat?. Then, we record the probability for the model to answer Yes as 7 Figure 5: Visualization Results. We provide the image generation results of the same prompt from four models: base model, the model with only semantic-level CoT optimized, the model with only token-level CoT optimized, and the model with both levels of CoT optimized. Yes and No as No. The reward for prompt is calculated as: RVQA = 1 (cid:88) Yes Yes + i No . (5) Output Reward Model. Lastly, we also employ the output reward model (ORM) proposed in [19] as reward model. The ORM is fine-tuned from an LMM (e.g., LLaVA-OneVision [30]) specifically for evaluating the alignment between the prompt and the image. The fine-tuning is to instruct the model to output Yes if the image perfectly aligns with the image and output No otherwise. Therefore, we calculate RORM using the methodology similar to RVQA, except that we input the whole image prompt to the ORM instead of reformatting the prompt. We can choose one or multiple reward functions illustrated above, and take the average as the final reward for specific sample. The detailed experiments of reward model in shown in Table 3."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup Training Settings. Our training dataset comprises text prompts sourced from the training set of T2I-CompBench [21] and [19], totaling 6,786 prompts with no images. Prior to training, we use GPT4o mini to extract the objects and their attributes from the prompts to facilitate computing the rewards. We use Janus-Pro-7B as the base model. We use learning rate of 1e-6 and beta of 0.01. For the reward model, we choose HPS [68] as the human preference model, GroundingDINO [38] as the object detector, and GIT [64] as the VQA model. For the ORM, we finetune LLaVA-OneVision-7B in the same manner as [19]. 8 Table 1: T2I-CompBench Result. The best score is in blue , with the second-best score in green . Model Attribute Binding Object Relationship Complex Color Shape Texture Spatial Non-Spatial StructureDiffusion [14] Composable Diffusion [37] Attend-and-Excite [3] PixArt-α [4] CoMat [24] SD-v1.5 [53] SD-XL-base-1.0 [49] FLUX.1 [28] Show-o [70] Show-o + PARM [19] EMU3 [65] Janus-Pro-7B (Baseline) [7] T2I-R1 (Ours) Diffusion Models 0.4218 0.3299 0.4517 0.4927 0.5329 0.3713 0.4687 0.5718 0.4900 0.3645 0.5963 0.6477 0.6468 0.4186 0.5299 0.6922 AutoRegressive Models 0.41 0.56 0.5706 0.3528 0. 0.46 0.66 0.7164 0.4936 0.7243 0.4990 0.4063 0.6400 0.6690 0.7827 0.3758 0.5879 0.7407 0.56 0.75 0.7544 0.6359 0.8130 0.1386 0.0800 0.1455 0.2064 0.2428 0.1165 0.2131 0.2863 0.20 0.29 - 0.2061 0.3378 0.3111 0.2980 0.3109 0.3197 0.3187 0.3112 0.3119 0. 0.30 0.31 - 0.3085 0.3090 0.3355 0.2898 0.3401 0.3433 0.3680 0.3047 0.3237 0.3703 0.29 0.37 - 0.3559 0.3993 Table 2: WISE Result. The best score is in blue , with the second-best score in green . Model Cultural Spatio-Temporal Natural Science Time Space Biology Physics Chemistry Overall PixArt-Alpha [4] playground-v2.5 [31] SD-v1-5 [53] SD-XL-base-0.9 [49] FLUX.1-dev [28] Emu3 [65] Show-o [70] VILA-U [69] Janus-1.3B [67] Janus-Pro-7B (Baseline) [7] T2I-R1 (Ours) 0.45 0.49 0.34 0.43 0.48 0.34 0.28 0.26 0.16 0.30 0.56 Diffusion Models 0.50 0.58 0.35 0.48 0.58 0.48 0.55 0.32 0.47 0.62 0.49 0.43 0.28 0.44 0.42 AutoRegressive Models 0.45 0.40 0.33 0.26 0.37 0.55 0.48 0.48 0.37 0.35 0.49 0. 0.41 0.30 0.35 0.28 0.36 0.54 0.56 0.48 0.29 0.45 0.51 0.45 0.46 0.39 0.30 0.42 0.55 0.34 0.33 0.21 0.27 0.35 0.27 0.30 0.23 0.14 0.26 0.30 0.47 0.49 0.32 0.43 0. 0.39 0.35 0.31 0.23 0.35 0.54 Benchmark. We test on T2I-CompBench [21] and WISE [44] to validate the effectiveness of our method. T2I-CompBench comprises 6,000 compositional text prompts evaluating three categories (attribute binding, object relationships, and complex compositions) and six sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). WISE consists of 1,000 text prompts spanning three categories (cultural common sense, spatial-temporal reasoning, and natural science) for evaluating world knowledge of the text-toimage models. To correctly generate an image, the model needs to reason about what the exact object or scenario is depicted in the prompt. We slightly modify the reasoning instruction on the WISE benchmark for more aligned results. We follow the official evaluation setting of the two benchmarks. 4.2 Main Results We compare T2I-R1 with leading text-to-image diffusion and autoregressive models on the T2ICompBench and WISE benchmarks (in Table 1 and 2). We also provide the qualitative results in Fig. 5. Our method demonstrates substantial improvements over the baseline model, with average enhancements of 13% and 19% on T2I-CompBench and WISE, respectively. On T2I-CompBench, the most significant gains appear in attribute binding, with an average improvement of 19%. For the 9 Table 3: T2I-CompBench Results with Different Reward Models. Det stands for object detector. Model Janus-Pro-7B - - - - - T2I-R1 - Reward Model Attribute Binding Object Relationship HPM Det VQA ORM Color Shape Texture Spatial Non-Spatial Complex Visual Quality - - - - - - - - - - - - - - - - - - - 0.6359 0.8134 0.7422 0.8171 0.7819 0.8210 0.8130 0.7599 0.3528 0.6048 0.5140 0.6019 0.5638 0.6074 0.5852 0.5742 0.4936 0.7311 0.6494 0.7307 0.7010 0.7440 0.7243 0.6902 0.2061 0.2383 0.3044 0.2969 0.3301 0.3189 0.3378 0.2796 0.3085 0.3012 0.3100 0.3088 0.3103 0.3076 0.3090 0. 0.3559 0.3899 0.3872 0.4052 0.3959 0.4005 0.3993 0.3921 - - - 0.218 1.775 1.942 2.063 - WISE benchmark, improvements are more evenly distributed across categories. When compared to the more powerful state-of-the-art diffusion models, T2I-R1 achieves superior or comparable results across both benchmarks. Notably, on T2I-CompBench, our method leads in five of six subtasks, with an exceptional performance in the spatial subtask (0.3378), surpassing previous SOTA results by over 5%. Similarly, for WISE, T2I-R1 excels in four of seven subtasks and achieves the highest overall score of 0.54, outperforming the robust FLUX.1-dev by 4%. Remarkably, our approach consistently achieves the leading results across all subtasks in both benchmarks when compared to other autoregressive models. Remarkably, the improvement on T2I-Compbench benefits from the planning ability brought by the semantic-level CoT, which designs the complex scenarios before generation. While the enhancement of WISE is due to the reasoning capability from the semantic-level CoT, which deduces the true object or place depicted behind the prompt. The token-level CoT plays an important role in faithfully following the design to generate the image and ensure its visual appeal. As shown in Fig. 5, without the semantic-level CoT, the model fails to fully understand what the object or place is to generate, providing unaligned results. When lacking token-level CoT, we observe multiple artifacts in the images, showcasing relatively low image quality. 4.3 Reward Analysis In this section, we experiment with the choice of reward functions and their combinations. We hope to provide some insights into how to choose the reward functions and combine them. Our results are shown in Table 3. We first experiment with the individual reward model. As shown in the table, HPM (H) demonstrates superior performance in attribute binding but shows limited effectiveness in supervising object relationships, likely due to its weak relation comprehension capabilities. The object detector (O) yields the least improvement in attribute binding among all other tested reward models, which aligns with expectations since our detector-based reward functions do not explicitly evaluate attributes. Any improvements observed stem solely from enhanced object existence ratios in the prompts. We observe that VQA model (V) and ORM (O) are both effective reward models with distinct strengths: the VQA model excels at improving attribute binding, while ORM demonstrates superior performance in object relationships. Then we experiment with multiple reward models. We start from the composition of HPM and object detector (H + O), and progressively incorporate other reward models. Our findings indicate that both the HPM-object detector combination (H + O) and the three-model integration of HPM, object detector, and VQA (H + + V) deliver balanced and satisfactory results across both attribute binding and relationship modeling tasks. To obtain the optimal choice of reward models, we conduct human study to evaluate the visual quality. Specifically, we select four options of reward models (V, O, + O, and + + V) to generate an image from the same prompt. Then we ask humans to rank the four images and score them according to the rank (rank 1 for 3 points, rank 2 for 2 points, and so on). We randomly choose 30 prompts from each of the subtasks from the T2I-CompBench. The result is shown in the visual quality column in Table 3. We observe that ensemble rewards achieve better visual quality, with + + obtaining slightly superior results. This improvement could be attributed to the implicit regularization provided by multiple rewards, preventing overfitting to single reward model. Conversely, individual reward models fail to provide satisfactory quality despite high benchmark scores. To ensure visual appeal, we adopt the ensemble of three reward models (H + + V) for our final model. 10 Table 4: Ablation Experiments on the Effectiveness of the Two Levels of CoT. Model Janus-Pro-7B - - T2I-R1 Optimized CoT T2I-CompBench WISE Diversity Semantic-level Token-level Color Shape Texture Culture Spatio-Temporal Science 0.6359 0.8082 0.7752 0.8130 0.3528 0.5684 0.5849 0.5852 0.4936 0.7219 0.7451 0. 0.3000 0.4900 0.3500 0.5600 0.4232 0.5599 0.4732 0.5855 0.3467 0.4367 0.3900 0.4633 6.976 8.177 6.255 8.203 Figure 6: Visualization Result of the Image Diversity of Single Prompt. We showcase the result of only token-level CoT optimized and both semantic-level and token-level CoT optimized. 4.4 Ablation Study We validate the effectiveness of incorporating both semantic-level and token-level CoT. We first validate the effectiveness of the semantic-level CoT by comparing it with baseline method that generates images using only the token-level CoT optimized with the GRPO method. This is the default text-to-image generation setting in Janus, whose result is shown in the third row in Table 4. Comparing the third and fourth row in the table, we find that semantic-level CoT generally brings performance improvements across both benchmarks tested. We witness particularly significant gain on the WISE benchmark. This enhanced performance can be attributed to the textual reasoning capabilities inherent in semantic-level CoT. As illustrated in Fig. 5, our method could first clearly reason about the objects or phenomena described in the prompt through semantic-level CoT. This effectively decouples the reasoning and generation processes and thereby facilitates superior results. We also observe that training solely with token-level CoT substantially reduces the diversity of generated images, as demonstrated in Fig. 6. To quantify this effect, we evaluate image diversity by reusing the generated images from T2I-CompBench, where each prompt generates ten images. We compute the Vendi Score [15] across the ten images for each prompt. Results indicate that GRPO training without semantic-level CoT decreases the diversity score, whereas incorporating semantic-level CoT significantly improves diversity through varied textual planning. We also consider another situation: the semantic-level CoT is incorporated in image generation, but GRPO only optimizes the semantic-level CoT without the token-level CoT. This can be viewed as 11 only enhancing the models high-level planning capabilities. The second row of Table 4 presents the result. The results show that optimizing semantic-level CoT exclusively yields smaller improvements compared to the joint optimization approach. Additionally, we find that optimizing both CoT types produces images with much better aesthetic quality compared with optimizing semantic-level CoT only. This indicates the necessity to jointly optimize both levels of CoT."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce T2I-R1, the first reasoning-enhanced text-to-image model powered by bi-level CoT reasoning process. We identify both the semantic-level CoT for high-level planning and the token-level CoT for patch-by-patch generation. We further integrate them through our proposed BiCoT-GRPO, reinforcement learning framework incorporating two levels of CoT within the same training step. By leveraging ULM capable of both visual understanding and generation, our approach eliminates the need for separate specialized models while achieving significant performance improvements, +13% on T2I-CompBench and +19% on the WISE benchmark, surpassing even FLUX.1. Our qualitative analysis demonstrates that T2I-R1 better understands complex prompts, reasons about user intentions, and handles uncommon scenarios with greater robustness, establishing new paradigm for reasoning-centric generative systems."
        },
        {
            "title": "References",
            "content": "[1] Amini, A., Gabriel, S., Lin, P., Koncel-Kedziorski, R., Choi, Y., Hajishirzi, H.: Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319 (2019) [2] Austin, J., Odena, A., Nye, M.I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C.J., Terry, M., Le, Q.V., Sutton, C.: Program synthesis with large language models. CoRR abs/2108.07732 (2021) [3] Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., Cohen-Or, D.: Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG) 42(4), 110 (2023) [4] Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., Li, Z.: Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis (2023) [5] Chen, L., Li, L., Zhao, H., Song, Y., Vinci: R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V (2025), accessed: 2025-02-02 [6] Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H.P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., Zaremba, W.: Evaluating large language models trained on code. CoRR abs/2107.03374 (2021) [7] Chen, X., Wu, Z., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C.: Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811 (2025) [8] Cheng, T., Song, L., Ge, Y., Liu, W., Wang, X., Shan, Y.: Yolo-world: Real-time openvocabulary object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1690116911 (2024) [9] Datta, S., Ku, A., Ramachandran, D., Anderson, P.: Prompt expansion for adaptive textto-image generation. In: Ku, L.W., Martins, A., Srikumar, V. (eds.) Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long 12 Papers). pp. 34493476. Association for Computational Linguistics, Bangkok, Thailand (Aug 2024). https://doi.org/10.18653/v1/2024.acl-long.189, https://aclanthology.org/2024. acl-long.189/ [10] Deng, Y., Bansal, H., Yin, F., Peng, N., Wang, W., Chang, K.W.: Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement (2025), https: //arxiv.org/abs/2503. [11] Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al.: Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499 (2023) [12] Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1287312883 (2021) [13] Fang, R., Duan, C., Wang, K., Li, H., Tian, H., Zeng, X., Zhao, R., Dai, J., Li, H., Liu, X.: Puma: Empowering unified mllm with multi-granular visual generation. arXiv preprint arXiv:2410.13861 (2024) [14] Feng, W., He, X., Fu, T.J., Jampani, V., Akula, A., Narayana, P., Basu, S., Wang, X.E., Wang, W.Y.: Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032 (2022) [15] Friedman, D., Dieng, A.B.: The vendi score: diversity evaluation metric for machine learning. arXiv preprint arXiv:2210.02410 (2022) [16] Gemini Team, G.: Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023) [17] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025) [18] Guo, Z., Zhang, R., Chen, H., Gao, J., Jiang, D., Wang, J., Heng, P.A.: Sciverse: Unveiling the knowledge comprehension and visual reasoning of lmms on multi-modal scientific problems. arXiv preprint arXiv:2503.10627 (2025) [19] Guo, Z., Zhang, R., Tong, C., Zhao, Z., Gao, P., Li, H., Heng, P.A.: Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926 (2025) [20] Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., Steinhardt, J.: Measuring mathematical problem solving with the math dataset. NeurIPS (2021) [21] Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.: T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems 36, 7872378747 (2023) [22] Huang, W., Jia, B., Zhai, Z., Cao, S., Ye, Z., Zhao, F., Hu, Y., Lin, S.: Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749 (2025) [23] Jain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., Stoica, I.: Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR abs/2403.07974 (2024), https://doi.org/10.48550/arXiv.2403.07974 [24] Jiang, D., Song, G., Wu, X., Zhang, R., Shen, D., Zong, Z., Liu, Y., Li, H.: Comat: Aligning textto-image diffusion model with image-to-text concept matching. arXiv preprint arXiv:2404.03653 (2024) [25] Jiang, D., Zhang, R., Guo, Z., Li, Y., Qi, Y., Chen, X., Wang, L., Jin, J., Guo, C., Yan, S., et al.: Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621 (2025) 13 [26] Jiang, D., Zhang, R., Guo, Z., Li, Y., Qi, Y., Chen, X., Wang, L., Jin, J., Guo, C., Yan, S., et al.: Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621 (2025) [27] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. Advances in neural information processing systems 35, 2219922213 (2022) [28] Labs, B.F.: Flux. https://github.com/black-forest-labs/flux (2024) [29] Lei, J., Zhang, R., Hu, X., Lin, W., Li, Z., Sun, W., Du, R., Zhuo, L., Li, Z., Li, X., et al.: Imagine-e: Image generation intelligence evaluation of state-of-the-art text-to-image models. arXiv preprint arXiv:2501.13920 (2025) [30] Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., Li, C.: Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 (2024) [31] Li, D., Kamko, A., Akhgari, E., Sabet, A., Xu, L., Doshi, S.: Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245 (2024) [32] Li, H., Tian, C., Shao, J., Zhu, X., Wang, Z., Zhu, J., Dou, W., Wang, X., Li, H., Lu, L., et al.: Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding. arXiv preprint arXiv:2412.09604 (2024) [33] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International Conference on Machine Learning. pp. 1288812900. PMLR (2022) [34] Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., Liu, S., Jia, J.: Mini-gemini: Mining the potential of multi-modality vision language models. arXiv: 2403.18814 (2024) [35] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2023) [36] Liu, J., Chen, H., An, P., Liu, Z., Zhang, R., Gu, C., Li, X., Guo, Z., Chen, S., Liu, M., et al.: Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model. arXiv preprint arXiv:2503.10631 (2025) [37] Liu, N., Li, S., Du, Y., Torralba, A., Tenenbaum, J.B.: Compositional visual generation with composable diffusion models. In: European Conference on Computer Vision. pp. 423439. Springer (2022) [38] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., yue Li, C., Yang, J., Su, H., Zhu, J.J., Zhang, L.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. ArXiv abs/2303.05499 (2023) [39] Liu, Y., Peng, B., Zhong, Z., Yue, Z., Lu, F., Yu, B., Jia, J.: Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520 (2025) [40] Lu, P., Bansal, H., Xia, T., Liu, J., yue Li, C., Hajishirzi, H., Cheng, H., Chang, K.W., Galley, M., Gao, J.: Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. ArXiv abs/2310.02255 (2023) [41] Ma, Y., Liu, X., Chen, X., Liu, W., Wu, C., Wu, Z., Pan, Z., Xie, Z., Zhang, H., Zhao, L., et al.: Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975 (2024) [42] MAA: American invitational mathematics examination - aime. In: American Invitational Mathematics Examination - AIME 2024 (February 2024), https://maa.org/ math-competitions/american-invitational-mathematics-examination-aime [43] Meng, F., Du, L., Liu, Z., Zhou, Z., Lu, Q., Fu, D., Shi, B., Wang, W., He, J., Zhang, K., et al.: Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365 (2025) 14 [44] Niu, Y., Ning, M., Zheng, M., Lin, B., Jin, P., Liao, J., Ning, K., Zhu, B., Yuan, L.: Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265 (2025) [45] OpenAI: Chatgpt. https://chat.openai.com (2023) [46] OpenAI: GPT-4V(ision) gpt-4v-system-card system card (2023), https://openai.com/research/ [47] OpenAI: Hello gpt-4o. https://openai.com/index/hello-gpt-4o/ (2024) [48] OpenAI: Introducing openai o1, 2024. (2024), https://openai.com/o1/ [49] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023) [50] Qu, L., Zhang, H., Liu, Y., Wang, X., Jiang, Y., Gao, Y., Ye, H., Du, D.K., Yuan, Z., Wu, X.: Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069 (2024) [51] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning (2021), https://api.semanticscholar.org/CorpusID:231591445 [52] Rein, D., Hou, B.L., Stickland, A.C., Petty, J., Pang, R.Y., Dirani, J., Michael, J., Bowman, S.R.: GPQA: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022 (2023) [53] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1068410695 (2022) [54] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. pp. 234241. Springer (2015) [55] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017) [56] Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., Guo, D.: Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024) [57] Song, W., Wang, Y., Song, Z., Li, Y., Sun, H., Chen, W., Zhou, Z., Xu, J., Wang, J., Yu, K.: Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324 (2025) [58] Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., Yuan, Z.: Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525 (2024) [59] Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T., Wang, X.: Generative multimodal models are in-context learners. arXiv: 2312.13286 (2023) [60] Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., Wang, X.: Generative pretraining in multimodality. arXiv: 2307.05222 (2023) [61] Tian, K., Jiang, Y., Yuan, Z., Peng, B., Wang, L.: Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems 37, 8483984865 (2024) [62] Tong, S., Fan, D., Zhu, J., Xiong, Y., Chen, X., Sinha, K., Rabbat, M., LeCun, Y., Xie, S., Liu, Z.: Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164 (2024) [63] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023) [64] Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., Wang, L.: Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100 (2022) [65] Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al.: Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869 (2024) [66] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35, 2482424837 (2022) [67] Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., et al.: Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848 (2024) [68] Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341 (2023) [69] Wu, Y., Zhang, Z., Chen, J., Tang, H., Li, D., Fang, Y., Zhu, L., Xie, E., Yin, H., Yi, L., et al.: Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429 (2024) [70] Xie, J., Mao, W., Bai, Z., Zhang, D.J., Wang, W., Lin, K.Q., Gu, Y., Chen, Z., Yang, Z., Shou, M.Z.: Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528 (2024) [71] Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagereward: learning and evaluating human preferences for text-to-image generation. In: Proceedings of the 37th International Conference on Neural Information Processing Systems. pp. 1590315935 (2023) [72] Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Fan, Z.: Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024) [73] Yang, Y., He, X., Pan, H., Jiang, X., Deng, Y., Yang, X., Lu, H., Yin, D., Rao, F., Zhu, M., et al.: R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615 (2025) [74] Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., et al.: Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 (2025) [75] Zhang, J., Huang, J., Yao, H., Liu, S., Zhang, X., Lu, S., Tao, D.: R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937 (2025) [76] Zhang, R., Han, J., Liu, C., Zhou, A., Lu, P., Qiao, Y., Li, H., Gao, P.: Llama-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In: ICLR 2024 (2024) [77] Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.W., Gao, P., et al.: Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024 (2024) 16 [78] Zhang, R., Wei, X., Jiang, D., Zhang, Y., Guo, Z., Tong, C., Liu, J., Zhou, A., Wei, B., Zhang, S., et al.: Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739 (2024) [79] Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M., Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., Levy, O.: Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039 (2024) [80] Zong, Z., Jiang, D., Ma, B., Song, G., Shao, H., Shen, D., Liu, Y., Li, H.: Easyref: Omnigeneralized group image reference for diffusion models via multimodal llm. arXiv preprint arXiv:2412.09618 (2024) [81] Zong, Z., Ma, B., Shen, D., Song, G., Shao, H., Jiang, D., Li, H., Liu, Y.: Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046 (2024)"
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "CUHK MiuLar Lab",
        "Shanghai AI Laboratory"
    ]
}