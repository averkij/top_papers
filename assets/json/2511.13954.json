{
    "paper_title": "A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition",
    "authors": [
        "Nilay Kumar",
        "Priyansh Bhandari",
        "G. Maragatham"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer."
        },
        {
            "title": "Start",
            "content": "Preprint Brain Wave Encodes Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition Nilay Kumar , Priyansh Bhandari , G. Maragatham Department of Computational Intelligence, SRM Institute of Science and Technology, KTR {nl9459, pr6479, maragatg}@srmist.edu.in 5 2 0 N 7 1 ] . - [ 1 4 5 9 3 1 . 1 1 5 2 : r AbstractHuman emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEGbased emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive intercortical multi-head attention blocks that construct an electrode electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous stateof-the-art methods across all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer. three datasets, over all Index TermsElectroencephalography (EEG), Emotion Recognition, Transformers, Inter-Cortical Attention Mechanism, BrainComputer Interface (BCI), Affective Computing I. INTRODUCTION Emotions are complex psychophysiological responses to internal or external stimuli and are deeply embedded in human cognition, shaping how individuals feel, form mental states, and react emotionally to their surroundings [1][3]. These responses can be expressed through either non-physiological modalities, such as facial expressions, gestures, speech, and language [4][6], which are often subject to voluntary control and may mask what person is truly experiencing, or through Corresponding author: Nilay Kumar (nl9459@srmist.edu.in). physiological modalities, including heart rate, skin conductance, and especially neural activity [7], [8], which provide more direct and involuntary window into how people respond to stimuli and how emotions evolve internally over time. Quantifying and recognizing emotions plays crucial role in healthcare and clinical contexts, helping in the understanding of emotional dysregulation and impaired affective expression, which are important for the diagnosis and treatment of medically diagnosed conditions, as defined by the DSM-5 [9] and ICD-11 classification systems [10], including neuropsychiatric conditions (e.g., depression, anxiety) and broader neurodevelopmental and psychotic disorders (e.g., autism spectrum disorder, schizophrenia) [11], [12]. To quantify and enable such modelling, emotions can be modelled using either discrete or dimensional models. Discrete models can classify emotions into only fixed set of labels (e.g., happiness, sadness, anger, fear, disgust, surprise) [13]. Dimensional models are more general and can describe any emotion by expressing them along three linear scales, Valence, Arousal, and Dominance (VAD) [14], [15], which aligns more closely with how emotions are naturally experienced and support representations that generalize across individuals and contexts [16]. As result, any emotion represented using discrete model can be plotted within the VAD dimensional model, shown in Figures 1 and 2. Valence represents the degree of pleasantness, Arousal represents the level of alertness or activation, and Dominance represents the sense of control over emotion. To capture raw and involuntary neural activity, various braincomputer interface (BCI) techniques are used like functional magnetic resonance imaging (fMRI), magnetoencephalography (MEG), and nearinfrared spectroscopy (NIRS) [17], [18]. However, electroencephalography (EEG) is still the most widely used technique because of its non-invasive nature, cost-effectiveness, and high temporal resolution, making it well-suited for real-time emotion monitoring. Traditionally, emotion recognition using electroencephalographic (EEG) signals has relied on manually extracting features through domain-specific preprocessing and feature extraction techniques, subsequently passing them to shallow machine learning models. The extracted features are generally Preprint Fig. 1. Visual representation of wide range of emotions in the threedimensional ValenceArousalDominance (VAD) space. Fig. 2. Individual emotion representations within the three-dimensional ValenceArousalDominance (VAD) space, showcasing the unique characteristics of each emotion. derived from one of three primary domain representations: (i) time-domain (e.g., Hjorth parameters [19], Higher Order Crossing (HOC) [20]), (ii) frequency-domain (e.g., Differential Entropy (DE) [21], Power Spectral Density (PSD) [22]), and (iii) timefrequency domain (e.g., Discrete Wavelet Transform (DWT) [23], Continuous Wavelet Transform (CWT) [24]). Using these representations, features were manually handcrafted and fed into shallow classifiers. For example, Li et al. manually extracted 18 time-domain features (9 Hjorth parameters and 9 nonlinear features) from gamma-band EEG signals after applying Common Spatial Pattern (CSP) filtering and used linear Support Vector Machine (SVM) for emotion recognition [25]. Patil et al. applied Empirical Mode Decomposition (EMD) to denoise EEG signals and extracted features using HOC for emotion recognition [26]. Shi et al. extracted features across five frequency bands using DE and passed them to SVM for final classification [21], and similarly, Duan et al. extracted features using DE from multichannel EEG data and passed them into hybrid k-Nearest Neighbors (k-NN) and SVM classifier [27]. However, over the past decade, deep neural networks (DNNs) have steadily outperformed traditional machine learning models across range of fields, including computer vision [28], natural language processing [29], and humancomputer interaction [8]. similar progression was observed in the domain of EEG-based emotion recognition, when Zheng and Lu introduced Deep Belief Network (DBN) [30], one of the first deep learning models for affective emotion recognition. The DBN learned hierarchical representations from DE features and outperformed previous state-of-the-art machine learning approaches. In the following years, subsequent studies, such as those by Huang and Zhao [31] and Rudakov [32], made use of Convolutional Neural Networks (CNNs) [28] to capture spatial dependencies across EEG electrodes. By projecting signals into structured 2D topographies or frequency-based brain maps, CNNs enabled more effective modeling of spatial information. In parallel with the development of CNN-based approaches, the sequential and autoregressive nature of EEG signals led studies such as those by Zhang et al. [33] and Ma et al. [34] to explore Recurrent Neural Networks (RNNs) [35] and Long Short-Term Memory (LSTM) [36] architectures, as these were better suited for capturing temporal dependencies and tracking emotional state transitions over time. Naturally, to capture both the spatial and temporal richness of EEG signals, studies such as those by Shen et al. [37] and Yang et al. [38] introduced hybrid architectures combining CNNs and RNNs (or LSTMs). During the development of such models, studies such as those by Song et al. [39] and Yin et al. [40] made use of Graph Neural Networks (GNNs) [41] to more accurately ground the spatial structure of EEG data neurophysiologically, effectively modeling neural and inter-electrode dependencies as compared to the fixed-grid representations used in earlier approaches. In recent years, studies have turned to Transformer and attention-based models [29] for their ability to capture longrange temporal dependencies and leverage non-recurrent architecture that enables global context access, parallel computation, and more efficient training. Studies such as those by Tao et al. [42] apply spatial and temporal attention to identify informative electrodes and time steps for emotion classification. Xiao et al. [43] extend this by adding frequency-band attention to weigh spectral features. Liu et al. [44] employ dual channel attention within 3D CNN to emphasize key spatio-temporal patterns. While these and other EEG-based emotion recognition models leverage attention mechanisms to highlight salient features within individual domains, such as spatial (electrode), spectral (frequency band), or temporal (time slice), they fundamentally treat each electrodes signal as an independent input stream, and attention in these architectures primarily functions as filtering mechanism, adaptively weighting channels or segments based on their relative informativeness for the task. However, none of the existing studies or model architectures take into account inter-electrode interactions, which are more neurologically grounded in how emotions emerge and are processed across cortical regions, and which can lead to more accurate and robust affective state recognition. Preprint To explore and address this issue, we introduce RBTransformer, Transformer-based neural network architecture that models these inter-cortical neural interactions in the latent space for EEG-based emotion recognition. RBTransformer first converts raw EEG signals into Band Differential Entropy (BDE) tokens, which are then passed through an Electrode Identity Embedding layer, which allows the model to retain awareness of each electrodes unique identity and ordering. These representations are then passed through stack of Inter-Cortical Multi-Head Attention Blocks, which allow each electrode to directly interact with every other electrode using an electrode electrode attention matrix. And finally, these features are passed through classification head to get the final class prediction. This architecture mimics the recurrent exchange of information across cortical regions, allowing RBTransformer to capture both inter-cortical dependencies and localized temporal dynamics, without relying on handcrafted features or sequential modeling at all. In brief, the primary contributions of this paper are as follows: 1) We introduce RBTransformer, Transformer-based architecture that explicitly models inter-cortical neural interactions through dedicated multi-head attention mechanism. By enabling structured communication between EEG electrodes and incorporating frequency-aware BDE tokens along with Electrode Identity Embeddings, the model captures both localized saliency and global interregional dependencies without relying on handcrafted features or explicit temporal modeling. 2) We conduct extensive experiments and demonstrate that RBTransformer achieves state-of-the-art performance across all three benchmark datasets, SEED, DEAP, and DREAMER, across all dimensions (Valence, Arousal, and Dominance for DEAP and DREAMER), under both binary and multi-class emotion classification settings. 3) We also present t-SNE visualizations and confusion matrices to demonstrate that the model effectively segregates emotional classes in the latent space and maintains consistent discriminative performance across classes. The rest of the paper is organized as follows: Section II reviews related work. Section III describes data preprocessing steps and model architecture for RBTransformer. Section IV outlines datasets, evaluation metrics, and training configurations. Section presents the results of extensive experiments and highlights the effectiveness of the proposed model. Finally, Section VI concludes the paper. II. METHODOLOGY In this section, we present the end-to-end pipeline used to preprocess the raw EEG data for RBTransformer and provide detailed explanation of RBTransformers model architecture. A. Dataset Preprocessing We use the SEED [30], DEAP [8], and DREAMER [45] datasets for our experiments and apply consistent preprocessing pipeline across all three datasets. The complete EEG Fig. 3. Preprocessing pipeline applied across EEG datasets for RBTransformer. preprocessing pipeline is presented in Fig. 3, and the entire workflow is explained as follows. Each dataset is made up of continuous multichannel EEG recordings, where each recording includes both baseline segment XB and trial stimulus-induced response segment XT , as shown in Eq. (1). = [XB, XT ] (1) For each recording i, the pair (XBi, XT i) corresponds to the baseline segment XBi RC LB and the trial segment XT RC LT , where is the number of EEG channels, and LB, LT are the respective time points in the baseline and trial segments. Next, these baseline and trial signals are chunked into smaller windows. For the trial segments, sliding window of size 512 with stride of 117 is applied to each XT i. Each XT is chunked into overlapping windows of size 512, denoted as XT = {X (1) , (2) }, where each (j) RC 512. To chunk baseline segments, the average signal is computed per trial as shown in Eq. (2): , . . . , (M ) XBi ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) j=1 (j) Bi (2) Here, each XBi is chunked into non-overlapping winBi }, RC 128. This process yields dows of size 128, denoted as XBi = {X (1) where each (j) Bi Bi , . . . , (M ) Bi , (2) Preprint set of averaged and chunked baseline signals XB = { XB1, XB2, . . . , XBn}. To reduce inter-subject and inter-channel variability, z-score normalization is applied independently to both the trial chunks (j) RC 512 and the averaged baseline segments XBi RC 128. Let RC denote either trial chunk (j) or baseline segment XBi, where = 512 for trial chunks and = 128 for baseline segments. The normalization is performed per channel {1, 2, . . . , }, shown in equations Eqs. (3)(5): µc ="
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) t=1 [c, t] σc = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) t=1 (M [c, t] µc)2 (3) (4) ˆM [c, t] = [c, t] µc σc , {1, . . . , L} (5) Here, µc and σc are the mean and standard deviation across the time points in channel c, and ˆM RC denotes the normalized matrix with zero mean and unit variance per channel. This process yields the set of normalized trial chunks ˆXT = { ˆX (1) } and the set of normalized baseline segments ˆXB = {ˆXB1, ˆXB2, . . . , ˆXBn}, which serve as standardized inputs for the subsequent feature extraction stage. 1 , . . . , ˆX (M ) 1 , ˆX (2) Band Differential Entropy (BDE) [30] is then calculated for each normalized trial chunk and the corresponding normalized baseline segment. Bandpass filters are used to extract sub-band signals corresponding to four EEG frequency bands: Theta (4 8 Hz), Alpha (813 Hz), Beta (1330 Hz), and Gamma (30 45 Hz). The differential entropy for each frequency band and channel is computed using the following formulation, shown in Eq. (6): BDE(b) = (cid:16) log 1 2 2πe Var (cid:16)ˆS (b)[c] (cid:17)(cid:17) (6) [c] or baseline segment ˆX (b) Here, ˆS (b)[c] denotes the filtered signal in frequency band for channel c, which may correspond to either trial segment ˆX (j,b) Bi [c]. Var() represents the sample variance of the sub-band signal, capturing the frequency-specific energy distribution across EEG channels. This yields trial BDE feature tokens (j) RC 4 and baseline BDE feature tokens Fi RC 4. The full sets are denoted as = {F (1) } and = { F1, F2, . . . , Fn}, 1 respectively. , . . . , (M ) , (2) 1 Finally, to eliminate subject-specific trends, remove static background activity, and isolate emotion-related neural activity, baseline correction is applied by subtracting the baseline BDE feature token Fi RC4 from the corresponding trial BDE feature token (j) RC4, as shown in Eq. (7): = (j) (j) Fi (7) Here, (j) BDE token for tively, { (1) , . . . , (M ) 1 and inference for RBTransformer. RC4 denotes the final baseline-corrected trial. CollecF = }, which serves as the input for training this yields the fully preprocessed dataset the jth chunk of , (2) 1 the ith B. Model Architecture The RBTransformer model is an attention-based architecture tailored for EEG-based emotion recognition. It operates on the baseline-normalized Band Differential Entropy (BDE) tokens, where each token encodes the deviation of each electrodes spectral energy from its baseline across canonical EEG bands. These inputs are passed through the following sequence of processing components, depicted in Figure 4, which illustrates the complete architecture layout from BDE token input to classification output. (1) BDE Feature Projection Layer: batch of preprocessed BDE tokens RBC4 is passed through shared linear projection layer that maps each 4-dimensional BDE vector into dmodel -dimensional embedding vector for all electrodes C, as shown in Eq. (8): = Wproj + bproj (8) Here, dot product is computed between each 4dimensional BDE token and the shared projection matrix Wproj R4dmodel , followed by the addition of learnable bias vector bproj Rdmodel , which results in each BDE token being transformed into richer representational space of dimension dmodel . (2) Electrode Identity Embedding Layer: To support inter-cortical modeling, the attention mechanism relies on fixed electrode-to-electrode structure, which is achieved by assigning unique learnable embedding to each electrode to retain its spatial information. Accordingly, after BDE projection, learnable identity embedding Eidentity R1Cdmodel is added to the projected tensor , broadcast across the batch, as shown in Eq. (9): = + Eidentity (9) which is then followed by Dropout for regularization, shown in Eq. (10): = Dropout(Z ) (10) (3) Inter-Cortical Attention Mechanism: Following the addition of electrode identity embeddings, the intermediate features then enter the models core module, the Inter-Cortical Attention Mechanism. For this, RBTransformer uses stack of attention blocks, each made up of Multi-Head SelfAttention module and Feedforward Network, both of which are explained in detail in the following sections. (I) Multi-Head Self-Attention (MHSA): The Electrode Identity Embedded tokens are then passed through the MultiHead Self-Attention (MHSA) module, which serves as the Preprint Fig. 4. Schematic architecture diagram of RBTransformer implementing inter-cortical attention. core component for modeling the inter-cortical neural interactions in the latent space, simulating how different regions of the brain exchange information. In this, input tokens are first passed through normalization layer, which helps to stabilize the training by ensuring that the features are scaled consistently across the embedding dimension. This is shown in Eq. (11). ˆZ = LayerNorm( ) (11) The normalized tokens RBCdmodel are then linearly projected into three learnable representations, queries (Q), keys (K), and values (V). This operation is performed independently across attention heads, each of dimensionality dhead . Each head maintains its own set of QKV projections, Q, , RBHCdhead , allowing the model to capture multi-perspective inter-regional dependencies from the same neural signal simultaneously. This linear projection is computed via dot product between the normalized input tokens and learnable weight matrix Wqkv Rdmodel 3Hdhead , as shown in Eq. (12). (12) [Q, , ] = ˆZ Wqkv After this comes the inter-cortical attention mechanism, which computes an attention matrix of shape (electrodes electrodes). This attention matrix Attention RBHCC enables each electrode to attend to every other electrode, effectively capturing inter-regional neural interactions. It is calculated by taking the dot product between the Queries (Q) and Keys (K) matrices, scaled by the square root of the head dimension dhead , as shown in Eq. (13). These attention weights are applied to the values (V ) to obtain the head-specific outputs. The outputs from all heads are concatenated and projected back to the original embedding dimension using linear transformation Wout RH dhead dmodel , as shown in Eq. (14): Zattn = Concat(Attention ) Wout (14) Finally, residual connection is applied, along with dropout to produce the final output of the MHSA regularization, module ZMHSA RBC dmodel , as given in Eq. (15): ZMHSA = + Dropout(Zattn ) (15) (II) Feedforward Network (FFN): The output of the MHSA block ZMHSA, is then passed through Feedforward Network (FFN), which enhances the models ability to capture complex local patterns and non-linear relationships within each signal. Specifically, the FFN applies stack of linear transformations and dropout layers to project the input representation and promote regularization, while GELU activation introduces non-linearity to better model complex local dependencies. This sequence refines the representation at each electrode. Finally, residual connection adds the FFN output back to the original input ZMHSA, helping preserve the original information flow and mitigate vanishing gradients during training, as shown in Eqs. (16)(17). ZFFN = Dropout(cid:0)Wout Dropout(cid:0) GELU (cid:0)Win ZMHSA + bin (cid:1)(cid:1) + bout (cid:1) ZAttnBlock = ZMHSA + ZFFN (16) (17) Attention = Softmax (cid:19) (cid:18) QK dhead (13) Here, ZAttnBlock RBC embed_dim represents the output of one attention block. Win Rembed_dimhidden_dim and Preprint Fig. 5. Electrode layouts for (a) SEED [30] with 62 electrodes, (b) DEAP [8] with 32 electrodes, and (c) DREAMER [45] with 14 electrodes. Blue circles indicate active electrodes used in each dataset; Grey circles denote unused electrodes. Wout Rhidden_dimembed_dim are learnable weight matrices, while bin Rhidden_dim and bout Rembed_dim are the corresponding learnable bias vectors. (4) Classification Head: Finally, after the input is processed through multiple attention blocks, it enters the classification head, which converts the contextualized electrode representations into emotion predictions. The output tensor ZAttnBlockFinal is first aggregated using Global Average Pooling (GAP) across the electrode dimension. The resulting pooled vector, ZGAP RBembed_dim , is then normalized via LayerNorm (LN) to obtain ZLN RBembed_dim , which is subsequently passed through linear layer to produce the final class logits, ylogits RBnum_classes , as shown in Eqs. (18)(20): ZGAP ="
        },
        {
            "title": "1\nC",
            "content": "C (cid:88) ZAttnBlock [:, i, :] i=1 ZLN = LayerNorm(ZGAP) ylogits = Wlogits ZLN + blogits (18) (19) (20) Here, Wlogits Rembed_dimnum_classes and blogits Rnum_classes are the learnable weights and bias matrices of the final classification head. III. EXPERIMENTAL SETUP In this section, we describe the datasets used for training and benchmarking, the metrics used for performance evaluation, and the training configuration for RBTransformer. A. Datasets We perform experiments on three standardized EEG emotion recognition benchmarks, SEED [30], DREAMER [45], and DEAP [8]. The details of each dataset are explained as follows. (1) SEED: The SEED dataset [30] contains EEG recordings from 15 subjects (7 male, 8 female) with an average age of 23.3 years. Each participant completed three sessions spaced at least one week apart to assess intra-subject stability. During each session, they viewed 15 emotionally evocative movie clips, five each for positive, neutral, and negative emotions, and labeled their emotional responses accordingly. EEG signals were recorded using 62-channel setup following the international 1020 system, originally sampled at 1000 Hz and later downsampled to 128 Hz. Band Differential Entropy (BDE) features were extracted across five canonical frequency bands: delta, theta, alpha, beta, and gamma. detailed electrode layout used in SEED is shown in Figure 5.(a). (2) DEAP: The DEAP dataset [8] comprises EEG and peripheral physiological recordings from 32 participants (17 male, 15 female) aged between 19 and 37 years. Each participant watched 40 one-minute music video clips and rated their emotional responses along four dimensions: Valence, Arousal, Dominance, and liking. EEG signals were recorded using 32-channel BioSemi ActiveTwo system and downsampled to 128 Hz after preprocessing. The electrode layout used in DEAP is shown in Figure 5.(b). (3) DREAMER: The DREAMER dataset [45] comprises EEG and ECG recordings from 23 participants (14 male, 9 female) aged between 22 and 33 years. Participants watched 18 audio-visual movie clips and provided self-assessments of their emotional states using continuous ratings for Valence, Arousal, and Dominance. EEG signals were recorded using 14-channel Emotiv EPOC device at sampling rate of 128 Hz. Band Differential Entropy (BDE) features were extracted from overlapping time windows to capture temporal dynamics. The electrode layout used in DREAMER is shown in Figure 5.(c). B. Metrics To evaluate the performance of RBTransformer, we report the following classification metrics: Accuracy, Precision, Recall, and F1-score [46]. (1) Accuracy: Accuracy is the proportion of correct predictions across all samples, denoted in Eq. 21: Accuracy = + P + + + (21) Here, , , , and represent true positives, true negatives, false positives, and false negatives, respectively. (2) Precision: Precision [46] is the proportion of true positives out of all predicted positives, denoted in Eq. 22: Preprint Precision = P + (22) (3) Recall: Recall [46], also known as Sensitivity or True Positive Rate, is the proportion of actual positives correctly identified by the model, denoted in Eq. 23: Recall = P + (23) (4) F1-score: The F1-score [46] is the harmonic mean of Precision and Recall, balancing both in single metric: F1 -score = 2 Precision Recall Precision + Recall (24) C. Training Configurations RBTransformer was trained using the PyTorch framework [47] on an NVIDIA P100 GPU. We used the AdamW optimizer [48] for total of 300 epochs with an L2 regularization (weight decay) of 1 103. cosine annealing learning rate scheduler [49] dynamically adjusted the learning rate from 1 103 to 1 106 throughout the training. The model was trained using Cross-Entropy loss with labelsmoothing [50] set to 0.12. Batch size was set to 256 for the first 150 epochs and reduced to 64 for the remaining epochs. To handle class imbalance, SMOTE [51] was applied only to the training folds. 5 fold cross-validation was carried out to evaluate the performance across all experiments which were tracked using Weights & Biases (WandB) [52]. IV. RESULTS AND DISCUSSION In this section, RBTransformer is extensively evaluated on the SEED, DEAP, and DREAMER datasets under subjectdependent setting for both binary and multi-class classification tasks, and results are compared with existing state-of-the-art models. An extended metric evaluation is carried out for model performance validation, and to analyze class-wise predictions and visualize RBTransformers ability to distinguish and segregate class clusters in the latent space, confusion matrices and t-SNE plots are presented, respectively, across all datasets and dimensions for both classification settings. Finally, an ablation study is carried out to assess the impact of individual model components. A. Subject-Dependent Emotion Recognition Evaluation For performance evaluation, RBTransformer is assessed under subject-dependent setting, in which the model is trained and evaluated separately on data from the same subjects. The entire dataset, which comprises EEG recordings across all subjects and trials is first preprocessed, and then is divided into training and validation sets using an 8020 trainval split. 5 fold cross-validation is then applied, where each fold maintains the subject-dependent setting. This means that within each fold, data from the same subjects can appear in both the training and validation splits, but always as distinct subsets. Same procedure is applied consistently across all three datasets, along all three dimensions, for both binary and multiclass classification tasks. B. Binary-Class Classification Evaluation Results RBTransformer is first evaluated under binary-class classification setting across DEAP and DREAMER, along their three affective dimensions: Valence, Arousal, and Dominance. For binary classification, the labels are converted as follows: DEAP labels (19) are split into \"High\" (above 5) and \"Low\" (5 or below), while DREAMER labels (15) are split into \"High\" (above 3) and \"Low\" (3 or below). The performance of RBTransformer is compared against the existing state-ofthe-art models, and the detailed results for both datasets under binary-class classification setting are listed below. TABLE BINARY-CLASS CLASSIFICATION PERFORMANCE COMPARISON ON DEAP DATASET DEAP Dataset Model Valence Arousal Dominance ACRNN GANSER 4D-CRNN BiDCNN CLDTA DFCN RACNN 4D-ANN TRPO-NET TDMNN RBTransformer (Ours) 93.72 3.21 93.86 94.22 2.61 94.38 2.61 94.58 1.40 94.59 96.65 2.65 96.90 1.65 97.87 1.89 98.08 2.13 99.84 0. 93.38 3.73 94.38 94.58 3.69 94.72 2.56 94.11 2.10 95.32 97.11 2.01 97.39 1.75 98.08 1.83 98.25 2.85 99.83 0.05 94.78 98.33 1.55 99.82 0.06 TABLE II BINARY-CLASS CLASSIFICATION PERFORMANCE COMPARISON ON DREAMER DATASET DREAMER Dataset Model Valence Arousal Dominance GANSER DGCNN DFCN RACNN ACRNN BiDCNN TRPO-NET TDMNN RBTransformer (Ours) 85.28 86.23 12.29 93.15 95.55 2.18 97.93 1.73 98.35 0.87 98.86 0.57 99.45 0.91 99.61 0.05 84.16 84.54 10.18 91.30 97.01 2.74 97.98 1.92 98.66 1.46 98.97 0.49 99.51 0.79 99.74 0.06 85.02 10.25 92.04 98.23 1.42 99.01 0.96 98.93 0.69 99.79 0. Experimental results under the binary-class classification setting are reported in Table and Table II for the DEAP and DREAMER datasets, respectively, and are reported as mean accuracy standard deviation (ACC STD). On DEAP dataset, RBTransformer achieves ACC STD values of 99.84% 0.02, 99.83% 0.05, and 99.82% 0.06 along its Valence, Arousal, and Dominance dimensions, improving the previous state-of-the-art results by 1.76% and 1.58% on Valence and Arousal, respectively. Similarly, on DREAMER dataset, RBTransformer achieves ACC STD values of 99.61% 0.05, 99.74% 0.06, and 99.79% 0.04 along Preprint its Valence, Arousal, and Dominance dimensions, improving the previous state-of-the-art results by 0.16% and 0.23% on Valence and Arousal, respectively. C. Multi-Class Classification Evaluation Results Similarly, RBTransformer is also assessed on multi-class classification tasks across SEED, DEAP, and DREAMER. SEED is inherently multi-class dataset with three emotion classes: Positive, Neutral, and Negative. For DEAP and DREAMER, the original continuous emotion ratings across three primary emotional dimensions, Valence, Arousal, and Dominance, are converted into discrete classes. Specifically, DEAP is converted into nine classes (1 to 9) for each dimension, while DREAMER is converted into five classes (1 to 5). RBTransformers performance is directly compared against the existing state-of-the-art models on the task of multi-class classification, and the detailed results for each dataset are listed below. TABLE III MULTI-CLASS CLASSIFICATION PERFORMANCE COMPARISON ON SEED DATASET SEED Dataset the SEED, DEAP, and DREAMER datasets, respectively, and are reported as mean accuracy standard deviation (ACC STD). On SEED dataset, RBTransformer achieves an ACC STD of 99.51% 0.02, improving the previous state-of-theart results by 1.80%. On DEAP dataset, it achieves ACC STD values of 99.87% 0.04, 99.84% 0.04, and 99.87% 0.05 along its Valence, Arousal, and Dominance dimensions, respectively, improving the previous state-of-the-art results by 2.10%, 2.24%, and 1.99%. Similarly, on DREAMER dataset, RBTransformer achieves ACC STD values of 99.54% 0.06, 99.55% 0.04, and 99.60% 0.05 along its Valence, Arousal, and Dominance dimensions, improving the previous state-of-the-art results by 1.18%, 1.36%, and 1.20%, respectively. D. Ablation Study To evaluate the contribution of different architectural components, training configurations, and regularization choices for RBTransformer, five ablation experiments are carried out on the DREAMER dataset along the Arousal dimension for the binary classification task. Results are reported as the mean and standard deviation over five cross-validation folds. In the first ablation, the Inter-Cortical Attention mechanism is removed from the model to assess its impact. Model DGCNN 4D-CRNN CLDTA SST-EmotionNet 4D-ANN TDMNN 3DCANN GANSER RBTransformer (Ours) Accuracy 90.40 8.49 94.74 2.32 95.09 4.48 96.02 2.17 96.25 1.86 97.20 1.57 97.35 97.71 99.51 0.02 TABLE IV MULTI-CLASS CLASSIFICATION PERFORMANCE COMPARISON ON DEAP DATASET DEAP Dataset Model Valence Arousal Dominance TRPO-NET RBTransformer (Ours) 97.63 2.38 99.87 0.04 97.74 2.26 99.84 0.04 97.88 2.24 99.87 0. TABLE MULTI-CLASS CLASSIFICATION PERFORMANCE COMPARISON ON DREAMER DATASET DREAMER Dataset Model Valence Arousal Dominance TRPO-NET RBTransformer (Ours) 98.18 0.97 99.54 0.06 98.37 0.93 99.55 0.04 98.40 0.80 99.60 0.05 Fig. 6. Ablation study for binary classification on the DREAMER dataset (Arousal dimension). Top: Impact of inter-cortical attention. Bottom: Impact of training and regularization components including ADASYN, SMOTE with label smoothing, weight decay, and dropout. Experimental results under the multi-class classification setting are reported in Table III, Table IV, and Table for As shown in Figure 6.(a), this leads to sharp drop of around 30% across all four metrics, highlighting the critical Preprint TABLE VI EXTENDED METRIC EVALUATION (ACCURACY, PRECISION, RECALL, F1-SCORE), MEAN SD ON SEED, DEAP, AND DREAMER DATASETS (BINARY AND MULTI-CLASS CLASSIFICATION) Metric SEED Accuracy (%) Precision (%) Recall (%) F1-score (%) 99.51 0.02 99.51 0.02 99.51 0.02 99.51 0.02 Binary-Class Classification Arousal 99.83 0.05 99.83 0.06 99.83 0.06 99.83 0.06 Valence 99.84 0.02 99.84 0.02 99.83 0.02 99.83 0.02 Dominance 99.82 0.06 99.81 0.07 99.81 0.06 99.81 0.07 Valence 99.87 0.04 99.87 0.06 99.86 0.03 99.86 0.04 Multi-Class Classification Arousal 99.84 0.04 99.84 0.05 99.84 0.05 99.84 0.04 Dominance 99.87 0.05 99.86 0.06 99.87 0.04 99.87 0.05 Valence 99.61 0.05 99.58 0.05 99.60 0.06 99.59 0.06 Binary-Class Classification Arousal 99.74 0.06 99.61 0.12 99.67 0.07 99.64 0.09 Dominance 99.79 0.04 99.63 0.06 99.72 0.09 99.68 0.06 Valence 99.54 0.06 99.54 0.06 99.54 0.05 99.54 0.05 Multi-Class Classification Arousal 99.55 0.04 99.49 0.06 99.46 0.09 99.47 0.07 Dominance 99.60 0.05 99.56 0.05 99.58 0.14 99.57 0.09 DEAP DREAMER t-SNE visualization of RBTransformers learned feature representations on SEED, DEAP, and DREAMER for both binary and multi-class classification Fig. 7. tasks across all dimensions (Valence, Arousal, and Dominance). importance of the Inter-Cortical Attention mechanism. Figure 6.(b) presents the remaining four ablations: the second ablation replaces SMOTE with ADASYN; the third disables both SMOTE and label smoothing; the fourth removes weight decay; and the fifth removes dropout. In all cases, the performance ranks lower than the final RBTransformer configuration, demonstrating the effectiveness and necessity of the chosen architectural and training design decisions. E. Extended Metric Evaluation for Performance Validation While accuracy provides general indication of performance, it may overlook models class-wise prediction capability. So, to assess the robustness and generalizability of RBTransformer in depth, an extended evaluation of metrics, including Precision, Recall, and F1-Score is carried out. Precision reflects the models ability to avoid false positives, Recall captures how well it retrieves relevant instances, and F1-Score balances the two. Evaluating these metrics helps assess whether the model performs reliably across all emotional classes, including less frequent ones. The extended metric evaluation is carried out on the SEED, DEAP, and DREAMER datasets across the three affective dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), for both binary and multi-class classification tasks. Results are reported as the mean and standard deviation over five crossvalidation folds and include Accuracy, Precision, Recall, and F1-Score. As shown in Table 1, RBTransformer demonstrates balanced, stable, and consistently high performance across all metrics, datasets, and corresponding dimensions, supporting its overall accuracy and ensuring that the observed accuracy is not driven by biased learning or class imbalance. Preprint Fig. 8. Confusion matrices of RBTransformer predictions on SEED, DEAP, and DREAMER for both binary and multi-class classification across all dimensions (Valence, Arousal, and Dominance). F. Visualization of Learned Feature Representations by RBTransformer G. Confusion Matrix Analysis To visualize RBTransformers ability to classify unseen EEG signals, its high-dimensional operational feature space is projected into two dimensions using the t-SNE (t-Distributed Stochastic Neighbor Embedding) algorithm. As observable from Figure 7, RBTransformer demonstrates strong performance across all datasets: SEED shows clear separation of Positive, Neutral, and Negative classes; in binary classification, DEAP and DREAMER form distinct groupings for High and Low classes across all dimensions, Valence, Arousal, and Dominance; and in multi-class settings, DEAPs nine and DREAMERs five emotion classes also form well-segregated clusters across these same dimensions, validating the models ability to capture and differentiate emotional states in the learned feature space. In addition to plotting t-SNE plots for visualizing RBTransformers ability to distinguish between different class clusters in the latent space, we plot confusion matrices for all the datasets to further analyze the class-wise prediction capability of RBTransformer in detail. The confusion matrices are generated for all three datasets, SEED, DEAP, and DREAMER, across all three affective dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification tasks, shown in Figure 8. As observable from the plots, RBTransformer achieves very high alignment between predicted and true labels, with most values concentrated along the principal diagonal. On average, the model achieves class-wise accuracy of around 99.50%, with only minor misclassifications appearing in off-principaldiagonal entries, providing quantitative and visual confirmaPreprint tion of RBTransformers strong class-wise performance across all datasets and affective dimensions under both classification settings. [12] R. Adolphs, How should neuroscience study emotions? by distinguishing emotion states, concepts, and experiences, Soc. Cogn. Affect. Neurosci., vol. 12, no. 1, pp. 2431, 2017. [13] P. Ekman, An argument for basic emotions, Cogn. Emot., vol. 6, no. V. CONCLUSION In this paper, we propose RBTransformer, Transformerbased neural network architecture that simulates and captures inter-cortical neural interactions in latent space for effective EEG-based emotion recognition. RBTransformer first converts raw EEG signals into Band Differential Entropy (BDE) tokens that are then passed into an Electrode-Identity Embedding, which allows the model to retain spatial awareness of each electrodes position and ordering. These intermediate features are then passed through an Inter-Cortical Attention module, which is made up of successive Inter-Cortical Multi-Head Attention blocks, each comprised of electrode electrode attention matrix, allowing each electrode to interact directly with every other electrode in the latent space, and feedforward network. These attention blocks are stacked on top of one another, which mimics the recurrent exchange of signals between distinct brain regions, allowing RBTransformer to capture both inter-regional spatial dependencies and localized temporal dependencies, without relying on any handcrafted features or explicit temporal sequence modeling. Experimental results demonstrate that RBTransformer achieves state-of-theart performance, outperforming all existing models, across all benchmarks, DEAP, DREAMER, and SEED, across all dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class EEG-based emotion recognition tasks."
        },
        {
            "title": "REFERENCES",
            "content": "[1] W. James, What is an emotion? Mind, vol. 9, no. 34, pp. 188205, 1884. [2] S. Schachter and J. E. Singer, Cognitive, social, and physiological determinants of emotional state, Psychol. Rev., vol. 69, no. 5, pp. 379 399, 1962. [3] C. E. Izard, J. Kagan, and R. B. Zajonc, Eds., Emotion, Cognition, and Behavior. Cambridge, UK: Cambridge University Press, 1984. [4] P. Ekman and W. V. Friesen, Constants across cultures in the face and emotion, J. Pers. Soc. Psychol., vol. 17, no. 2, pp. 124129, 1971. [5] S. Poria, E. Cambria, R. Bajpai, and A. Hussain, review of affective computing: From unimodal analysis to multimodal fusion, Inf. Fusion, vol. 37, pp. 98125, 2017. [6] S. K. DMello and J. Kory, review and meta-analysis of multimodal affect detection systems, ACM Comput. Surv., vol. 47, no. 3, pp. 136, 2015. [7] R. A. Calvo and S. DMello, Affect detection: An interdisciplinary review of models, methods, and their applications, IEEE Trans. Affect. Comput., vol. 1, no. 1, pp. 1837, 2010. [8] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras, Deap: database for emotion analysis using physiological signals, IEEE Trans. Affect. Comput., vol. 3, no. 1, pp. 1831, 2011. [9] American Psychiatric Association, Diagnostic and Statistical Manual of Mental Disorders, 5th ed. Washington, DC: American Psychiatric Association, 2013. 34, pp. 169200, 1992. [14] J. A. Russell, circumplex model of affect, J. Pers. Soc. Psychol., vol. 39, no. 6, pp. 11611178, 1980. [15] A. Mehrabian, Pleasure-arousal-dominance: general framework for describing and measuring individual differences in temperament, Curr. Psychol., vol. 14, no. 4, pp. 261292, 1996. [16] L. F. Barrett and J. A. Russell, The structure of current affect: Controversies and emerging consensus, Curr. Dir. Psychol. Sci., vol. 8, no. 1, pp. 1014, 1999. [17] K. A. Lindquist, T. D. Wager, H. Kober, E. Bliss-Moreau, and L. F. Barrett, The brain basis of emotion: meta-analytic review, Behav. Brain Sci., vol. 35, no. 3, pp. 121143, 2012. [18] H. Doi, S. Nishitani, and K. Shinohara, Nirs as tool for assaying emotional function in the prefrontal cortex, Front. Hum. Neurosci., vol. 7, p. 770, 2013. [19] B. Hjorth, Eeg analysis based on time domain properties, Electroencephalogr. Clin. Neurophysiol., vol. 29, no. 3, pp. 306310, 1970. [20] P. C. Petrantonakis and L. J. Hadjileontiadis, Emotion recognition from eeg using higher order crossings, IEEE Trans. Inf. Technol. Biomed., vol. 14, no. 2, pp. 186197, 2009. [21] L.-C. Shi, Y.-Y. Jiao, and B.-L. Lu, Differential entropy feature for eeg-based vigilance estimation, in 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEE, 2013, pp. 66276630. [22] S. M. Redwan, M. P. Uddin, A. Ulhaq, M. I. Sharif, and G. Krishnamoorthy, Power spectral density-based resting-state eeg classification of first-episode psychosis, Sci. Rep., vol. 14, no. 1, p. 15154, 2024. [23] N. Kumar, K. Alam, and A. H. Siddiqi, Wavelet transform for classification of eeg signal using svm and ann, Biomed. Pharmacol. J., vol. 10, no. 4, pp. 20612069, 2017. [24] M. Ende, A. K. Louis, P. Maass, and G. Mayer-Kress, Eeg signal transform techniques, in Nonlinear analysis by continuous wavelet Analysis of Physiological Data, H. Kantz, J. Kurths, and G. MayerKress, Eds. Berlin, Heidelberg: Springer, 1998, pp. 213219. [25] K.-E. Ko, H.-C. Yang, and K.-B. Sim, Emotion recognition using eeg signals with relative power values and bayesian network, International Journal of Control, Automation and Systems, vol. 7, no. 5, pp. 865870, 2009. [26] P. C. Petrantonakis and L. J. Hadjileontiadis, Emotion recognition from eeg using higher order crossings, IEEE Transactions on Information Technology in Biomedicine, vol. 14, no. 2, pp. 186197, 2010. [27] R.-N. Duan, J.-Y. Zhu, and B.-L. Lu, Differential entropy feature for eeg-based emotion classification, in 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER). IEEE, 2013, pp. 8184. [28] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, Backpropagation applied to handwritten zip code recognition, Neural Comput., vol. 1, no. 4, pp. 541551, 1989. [29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017. [30] W. Zheng and B. Lu, Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks, IEEE Trans. Auton. Mental Dev., vol. 7, no. 3, pp. 162175, 2015. [31] D. Huang, S. Chen, C. Liu, L. Zheng, Z. Tian, and D. Jiang, Differences first in asymmetric brain: bi-hemisphere discrepancy convolutional neural network for eeg emotion recognition, Neurocomputing, vol. 448, pp. 140151, 2021. [32] E. Rudakov, L. Laurent, V. Cousin, A. Roshdi, R. Fournier, A. Nait-Ali, T. Beyrouthy, and S. A. Kork, Multi-task cnn model for emotion recognition from eeg brain maps, in Proc. 4th Int. Conf. Bio-Engineering for Smart Technologies (BioSMART), 2021, pp. 14. [33] T. Zhang, W. Zheng, Z. Cui, Y. Zong, and Y. Li, Spatialtemporal recurrent neural network for emotion recognition, IEEE Trans. Cybern., vol. 49, no. 3, pp. 839847, 2018. [10] World Health Organization, International Classification of Diseases, 11th Revision (ICD-11), 11th ed. Geneva, Switzerland: World Health Organization, 2018. [34] J. Ma, H. Tang, W.-L. Zheng, and B.-L. Lu, Emotion recognition using multimodal residual LSTM network, in Proc. 27th ACM Int. Conf. Multimedia (MM), 2019, pp. 176183. [11] P. A. Kragel and K. S. LaBar, Decoding the nature of emotion in the [35] J. L. Elman, Finding structure in time, Cogn. Sci., vol. 14, no. 2, pp. brain, Trends Cognit. Sci., vol. 20, no. 6, pp. 444455, 2016. 179211, 1990. Preprint [36] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural Comput., vol. 9, no. 8, pp. 17351780, 1997. [37] F. Shen, G. Dai, G. Lin, J. Zhang, W. Kong, and H. Zeng, Eeg-based emotion recognition using 4d convolutional recurrent neural network, Cogn. Neurodyn., vol. 14, pp. 815828, 2020. [38] Y. Yang, Q. Wu, M. Qiu, Y. Wang, and X. Chen, Emotion recognition from multi-channel eeg through parallel convolutional recurrent neural network, in Proc. Int. Joint Conf. Neural Netw. (IJCNN), 2018, pp. 17. [39] T. Song, W. Zheng, P. Song, and Z. Cui, Eeg emotion recognition using dynamical graph convolutional neural networks, IEEE Trans. Affect. Comput., vol. 11, no. 3, pp. 532541, 2018. [40] Y. Yin, X. Zheng, B. Hu, Y. Zhang, and X. Cui, Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm, Appl. Soft Comput., vol. 100, p. 106954, 2021. [41] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, The graph neural network model, IEEE Trans. Neural Netw., vol. 20, no. 1, pp. 6180, 2008. [42] W. Tao, C. Li, R. Song, J. Cheng, Y. Liu, F. Wan, and X. Chen, Eegbased emotion recognition via channel-wise attention and self attention, IEEE Trans. Affect. Comput., vol. 14, no. 1, pp. 382393, 2020. [43] X. Xiao, X. Chen, Y. Wang, T. Ma, X. Zhang, and X. Chen, Multifrequency band and temporal-spatial based attention neural network for eeg emotion recognition, Sensors, vol. 20, no. 21, p. 5986, 2020. [44] X. Liu, X. Yang, Y. Du, W. Zeng, Q. Chen, Y. Zhou, C. Li, and Y. Xu, Emotion recognition from multichannel eeg data through dual channel 3d cnn model, Frontiers in Neuroscience, vol. 16, 2022. [45] S. Katsigiannis and N. Ramzan, Dreamer: database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices, IEEE J. Biomed. Health Inform., vol. 22, no. 1, pp. 98107, 2017. [46] D. M. W. Powers, Evaluation: From precision, recall and f-measure informedness, markedness and correlation, arXiv preprint to roc, arXiv:2010.16061, 2020. [47] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: An imperative style, highperformance deep learning library, in Advances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019. [48] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in Proceedings of the 7th International Conference on Learning Representations (ICLR), 2019. [49] , SGDR: Stochastic gradient descent with warm restarts, in Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017. [50] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 28182826. [51] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, SMOTE: Synthetic minority over-sampling technique, Journal of Artificial Intelligence Research, vol. 16, pp. 321357, 2002. [52] L. Biewald, Experiment tracking with weights and biases, 2020, [Online]. Available: https: from wandb.com. software available //www.wandb.com/"
        }
    ],
    "affiliations": [
        "Department of Computational Intelligence, SRM Institute of Science and Technology"
    ]
}