{
    "paper_title": "Prior Prompt Engineering for Reinforcement Fine-Tuning",
    "authors": [
        "Pittawat Taveekitworachai",
        "Potsawee Manakul",
        "Sarana Nutanong",
        "Kunat Pipatanakul"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 7 5 1 4 1 . 5 0 5 2 : r Prior Prompt Engineering for Reinforcement Fine-Tuning Pittawat Taveekitworachai1, Potsawee Manakul1, Sarana Nutanong2, Kunat Pipatanakul 1SCB 10X R&D, SCB 10X, SCBX Group, Thailand 2School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Thailand pittawat@scb10x.com, kunat@scb10x.com, potsawee@scb10x.com, snutanon@vistec.ac.th"
        },
        {
            "title": "Abstract",
            "content": "This paper investigates prior prompt engineering (pPE) in the context of reinforcement finetuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior promptthe instructions prepended to queries during training to elicit behaviors such as step-by-step reasoningremains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategiesreasoning, planning, code-based reasoning, knowledge recall, and null-example utilizationinto corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as powerful yet understudied axis for RFT."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in reasoning models mark significant step forward in improving language model (LM) performance by allocating additional compute budget at test time. common approach to developing such models is reinforcement finetuning (RFT), which incentivizes an LM to perform extended reasoning during inference by using re1 ward signalsbased on the correctness of generated answersduring training. Current studies have explored various components of the RFT pipeline, including objective functions and training algorithms (Liu et al., 2025; Yu et al., 2025; Yeo et al., 2025; Yue et al., 2025), data domains and curricula (Xie et al., 2025; Wei et al., 2025; Su et al., 2025; Hu et al., 2025), reward functions and shaping (Yeo et al., 2025; Su et al., 2025; Hu et al., 2025), and the influence of inherent behaviors across different LM families and model sizes (Liu et al., 2025; Zeng et al., 2025a; Gandhi et al., 2025). However, despite these improvements for various components of the RFT pipeline, one critical aspect remains understudied: the design of the prompt. To scope our study, we separate prompt used during RFT into two main parts of the prompt used during RFT: the instruction and the task content (see Figure 2). The instruction part of the prompt guides the model to exhibit desired behaviors (e.g., step-by-step reasoning). We refer to this section as the prior prompt, and it is the main focus of this study. Additional examples of prior prompts used in existing work are available in Appendix C. Although some studies briefly mention the importance of the prior prompt for training stability and performance impact (Xie et al., 2025; Zeng et al., 2025a), there has been lack of extensive investigation into how different prior prompting approaches used during RFT affect the resulting model behaviors. Therefore, this study centers on the following question: Can different prior prompt engineering approaches guide language models to internalize distinct behaviors when applied during RFT? The breadth of inference-time prompt engineering (iPE) approaches demonstrates their effectiveness at eliciting different behaviors (i.e., generation patterns) from LMs (Kojima et al., 2022), ultimately leading to varying performance impact. For example, chain-of-thought prompting (CoT) (Wei et al., 2022b) elicits step-by-step reasoning Figure 1: Five generated responses from five distinct models post-RFT with different pPE approaches<think>, <plan>, <code>, <examples>, and <knowledge>. Each pPE approach is inspired by corresponding iPE paradigm: chain-of-thought, plan-and-solve, program-of-thought, null-shot, and generated knowledge prompting, respectively. conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. User: Let the circles k1 and k2 intersect at two distinct points and B, and let be common tangent of k1 and k2, that touches k1 and k2 at and , respectively. If AM and = 2AM , evaluate B. Assistant: Figure 2: The prompt used during RFT by DeepSeek-AI et al. (2025). The prior prompt is highlighted in yellow. Non-highlighted content is task content. before final answer; plan-and-solve prompting (PS) (Wang et al., 2023) produces high-level plan prior to problem solving; and program-of-thought prompting (PoT) (Chen et al., 2023) induces codebased reasoning. These examples show that different iPE approaches not only elicit distinct behaviors (e.g., reasoning, planning, coding) but also yield different performance outcomes. Inspired by iPE, we introduce the term prior prompt engineering (pPE) to denote approaches for modifying the prior prompt in RFT. Just as iPE guides behavior during inference, we conjecture that pPE can shape model behavior during training. By combining the varied elicitation induced by pPE with RFTs incentivization mechanism, the resulting models may exhibit diverse behaviors and achieve different levels of performance impact. In this paper, we study the effects of various pPE approaches on model behaviors and performance impact after RFT. We select five representative iPE approaches based on their distinct elicited behaviorsreasoning, planning, coding, knowledge recall, and null example utilizationand translate them into corresponding pPE approaches. We employ these five pPE approaches to train Qwen2.5 7B into five distinct models with RFT using math-only training data. We then compare each RFT-trained model to its corresponding iPE-only baseline. We evaluate our models using both quantitative and qualitative methods. Quantitatively, we measure performance on mathematical reasoning, coding, and question-answering benchmarks (e.g., AIME2024, GPQA Diamond, and HumanEval+) to assess impact on in-domain and out-of-domain tasks. Qualitatively, we employ modified behavior-classification framework from Gandhi et al. (2025) to quantify differences in model behaviors. To test generalization, we replicate our experiments at smaller scales on Qwen2.5 3B, Qwen2.5 Coder 7B, and Llama 3.1 8B. We find that all pPE-trained models surpass their corresponding iPE-only baselines. Among pPE approaches, the null-example utilization approach which exhibits behavioral similarities to the reasoning approachachieves the largest improvement on GPQA Diamond and the highest average performance gain across tasks. Figure 1 illustrates the five models trained with different pPE strategies, each demonstrating distinct behavioral styles and indicating that pPE can incentivize diverse behaviors. Our contributions are as follows: We introduce the concepts of prior prompt and prior prompt engineering (pPE) as critical yet previously understudied aspects of RFT. We demonstrate that different pPE strategies elicit distinct behaviors, including variations in performance impact, response structure, verbosity, and behavior types. We propose an updated systematic behavior classification approach to quantify both cognitive and elicited behaviors, revealing how different pPE approaches shape model behavior."
        },
        {
            "title": "2 Prior Prompt Engineering for\nReinforcement Fine-Tuning",
            "content": "Our main question in this study is whether different pPE approaches can lead an LM to internalize distinct behavioral styles after RFT. If different pPE approaches indeed yield different behaviors, this could provide simple meansby only changing the prior promptto train models for specialized behaviors beyond reasoning (e.g., plan generation, code-based reasoning, or knowledge generation). To answer this question, we select five representative iPE approaches and translate them into pPE approaches. We then apply standard RFT setup to train five distinct models, differing only in their pPE approach and format reward (see Section 3). The overall process and distinctions between iPE and pPE are depicted in Figure 3. We evaluate each model quantitatively and qualitatively to assess performance changes and behavioral differences. Quantitative evaluation uses established benchmarks for mathematical reasoning, coding, and question answering. For qualitative evaluation, we adapt the framework of Gandhi et al. (2025) to classify each post-RFT models behavior into one of four cognitive categories and five pPEspecific categories. We also apply these evaluations to the base model at inference time with different iPE approaches, to further compare iPE and pPE. To explore the impact of pPE approaches on prior prompts, we select five representative iPE approaches based on their differences in behavioral elicitation when used to prompt an LM: 1. Reasoning: Chain-of-thought prompting (CoT) (Wei et al., 2022b) elicits an LM to generate step-by-step reasoning before producing final answer. This iPE approach is mapped to <think> in pPE and is the most commonly used in RFT studies, resulting in reasoning models (DeepSeek-AI et al., 2025; Xie et al., 2025). This serves as our baseline for comparison. 2. Planning: Plan-and-solve prompting (PS) (Wang et al., 2023) elicits the model to first generate plan (e.g., numbered steps) and then execute that plan, yielding improvements over standard CoT. The planning approach is mapped to <plan> in pPE. We expect the post-RFT model to generate plan before providing an answer. 3. Code-based reasoning: Program-of-thought prompting (PoT) (Chen et al., 2023) elicits structured reasoning through code by asking model to generate relevant code for problem solving. PoT has shown strong performance on math and logic tasks, especially with codepretrained models such as CodeLlama (Rozière et al., 2024), Qwen2.5-Coder (Hui et al., 2024), and StarCoder 2 (Lozhkov et al., 2024). This iPE approach is mapped to <code> in pPE. We expect the post-RFT model to generate code and comments that solve the given task. 4. Knowledge recall: Generated knowledge prompting (Liu et al., 2022) asks the model to recall or synthesize relevant knowledge before answering, simulating form of self-retrieval and improving performance on commonsense benchmarks. This approach is mapped to <knowledge> in pPE. We expect the post-RFT model to recall definitions, theorems, or formulas before proceeding to final answer. 5. Null-example utilization: Null-shot prompting (Taveekitworachai et al., 2024) prompts the model to utilize non-existent in-context examples relevant to the question, exploiting inductive biases without providing real demonstrations. It maps to <examples> in pPE, and we expect the post-RFT model to generate or reference illustrative examples relevant to query. With these five distinct pPE approaches for eliciting different behaviors in LMs during RFT, we expect not only differences in performance impact and post-RFT behaviors but also in training dynamics, such as average response length or per-step reward trajectories."
        },
        {
            "title": "3.1 Prior Prompts",
            "content": "To prepare our prior prompts, we adapt the template of Xie et al. (2025). For each iPE approach, we modify the instruction in the template (e.g., plan, recall relevant knowledge, write required code) and update the corresponding tag <x></x> as introduced in Section 2. These same templates are used for evaluating iPE-prompted models. The full prior prompt templates are given in Appendix D.1."
        },
        {
            "title": "3.2 Training",
            "content": "We follow standard RFT setup similar to DeepSeek-AI et al. (2025). Specifically, we use Group Relative Policy Optimization (GRPO) with 3 Figure 3: Left: iPE approaches are applied to prompt during inference, before inputting it into an LM, to elicit desired behaviors in the response. Right: pPE approaches are translated from iPE approaches and applied to the prior prompt to elicit desired behaviors during training. pretrained base LM. Our training stack is OpenRLHF v0.6.4 (Hu et al., 2024) for policy optimization and vLLM v0.8.2 (Kwon et al., 2023) for rollout generation. We train using prompts from the STILLv3 dataset (Chen et al., 2025), which contains approximately 30K mathematical problems and is used to train reasoning model. We note that the use of math-only training datasets is common in the existing literature (Liu et al., 2025; Yeo et al., 2025; Yu et al., 2025). Additional details and hyperparameters are listed in Appendix D.3. Our reward function comprises two equally weighted components (summing to 1.0): (1) accuracy, which assesses whether the model produces the correct final answer; and (2) format, which assesses whether the models output follows the expected format<x></x> followed by <answer></answer>, where is one of {think, plan, code, knowledge, examples}. The expected format is updated dynamically to match the pPE approach. Additional details on the reward function are available in Appendix D.2. For our main experiments, we use Qwen2.5-7B (Qwen et al., 2025) as the base model. All five pPE variants are trained with the same settings, differing only in the pPE approaches. We also use Qwen2.57B, prompted at inference with each iPE approach, as our comparison baseline. To assess generalization, we run scaled-down experiments due to budget constraints: Model size: Qwen2.5-3B trained with <think> and <plan>; Model family: Llama 3.1-8B (Grattafiori et al., 2024) with <think> and <plan>, and Qwen2.5-Coder-7B (Hui et al., 2024) with <think> and <code>. We select <plan> as the main comparator because of its distinct behaviors, which can be easily observed during and after RFT. <code> is selected for the code-specialized model to compare differences with the base model."
        },
        {
            "title": "3.3 Evaluation",
            "content": "We evaluate all models and prompting methods via quantitative and qualitative analyses. Quantitative benchmarks We report average accuracy across benchmarks for: Mathematical reasoning: AIME2024 (AIME) (Li et al., 2024), AMC12 2223 (AMC) (Li et al., 2024), and MATH-500 (MATH) (Hendrycks et al., 2021); Coding: HumanEval+ (HE+) base and extra sets (Liu et al., 2023); Question answering: GPQADiamond (GPQA) (Rein et al., 2024). See Appendix D.4.1 for additional details. Qualitative analysis We analyze differences across: (1) Training dynamics, (2) Average response length, (3) Ratio of four fundamental cognitive behaviors (Gandhi et al., 2025), and (4) Ratio of behavior patterns specific to each of the five pPE categories. Four fundamental cognitive behaviors are (i) Verification: identifying errors; (ii) Backtracking: proposing an alternative approach; (iii) Subgoal setting: generating intermediate steps; and (iv) Backward chaining: reasoning from the result to inputs. For (3) and (4), we employ the LM-based classification framework of Gandhi et al. (2025) to automatically classify model responses. Further details are in Appendix D.4.2."
        },
        {
            "title": "4 Results and Findings",
            "content": "In this section, we present and discuss results from our experiments, as described the setup in Section 3. Our objective is to answer the core question posed earlier: whether and how different pPE approaches can guide LMs to internalize distinct behaviors during RFT. To address this question, we examine three key aspects: 1. Performance impact: Do different pPE approaches lead to measurable improvements over 4 the baseline and their iPE counterparts? Do they result in distinct performance gains across tasks, or do they converge to similar outcomes? 2. Behavioral differences: Do different pPE approaches induce differences in fundamental cognitive behaviors and elicited generation patterns? Do the behavioral profiles of pPE-trained models align with those observed under iPE? 3. Generalization: How well do pPE approaches generalize across model sizes and families? The following subsections address each of these aspects in detail. Additional and detailed results, including results from the generalization study, are presented in Appendix E."
        },
        {
            "title": "4.1 Performance Impact",
            "content": "Model AIME AMC GPQA MATH HE+ Avg. Qwen2.5-7B 13.33 37.35 24.24 55. 72.60 40.62 iPE Think Plan Code Knowledge Examples pPE Think Plan Code Knowledge Examples 10.00 10.00 13.33 20.00 16.67 20.00 20.00 16.67 16.67 20.00 31.33 30.12 26.51 25.30 32.53 43.37 44.58 46.99 37.35 43.37 24.24 24.24 24.24 24.24 24.24 28.28 24.75 25.25 21.72 30. 56.00 51.20 51.40 59.60 56.80 73.20 69.60 66.20 71.00 71.20 75.00 73.80 72.00 72.00 0.00 70.10 68.90 78.00 73.20 72.60 39.31 37.87 37.50 40.23 26.05 46.99 45.57 46.62 43.99 47. Table 1: Benchmark accuracy (%) of Qwen2.5-7B when prompted with different iPE or RFT with different pPE approaches across five benchmarks. Bold indicates the best performance per column; underlined indicates the second best per column. Table 1 shows the performance of Qwen2.5-7B after being prompted with different iPE approaches or fine-tuned using RFT with different pPE approaches across the benchmarks. Notably, we observe that all iPE approaches result in lower average performance compared to the base model. For example, under the null-example utilization approach, iPE fails to generate parsable code during the evaluation of HE+, resulting in 0.00. In contrast, all post-RFT modelsregardless of the pPE approachachieve performance improvements over the base model. The commonly used reasoning approach, i.e., <think>, serves as strong baseline in pPE and delivers substantial gains. However, the null-example utilization approach, which underperforms in the iPE setting, yields the highest average performance improvement (+6.98 points) after RFTsurpassing the reasoning (<think>) approach (+6.37 points). This suggests that there exist approaches beyond stepby-step reasoning that are able to elicit different behaviorsand these behaviors can lead to larger performance gains. Another notable observation is that, while the null-example iPE approach fails on HE+, its pPE counterpart maintains strong performance on that benchmark. Conversely, the knowledge recall approach, which achieves the best performance among iPE variants, results in the lowest gains when used as pPE approach. These findings indicate that performance trends observed in the iPE setting do not directly translate to the pPE setting, emphasizing the different mechanisms at play in inference-time prompting versus RFT. Furthermore, just as iPE methods show varied effectiveness across benchmarks, different pPE approaches exhibit diverse performance profiles. While the code-based reasoning pPE approach understandably achieves the highest gain on HE+ (a coding benchmark), it also unexpectedly performs best on AMC (a math benchmark). In contrast, the knowledge recall pPE approach fails to deliver meaningful gains on GPQA, question-answering benchmark, and even underperforms compared to the base model. These results suggest that the impact of pPE is more nuanced than simply aligning domain-specific prompt with domain-specific task. We leave further investigation into the mechanisms behind these dynamics to future work."
        },
        {
            "title": "4.2 Behavioral Differences",
            "content": "Figure 4: \"Reward progression of Qwen2.5-7B during RFT exhibits similar trendsan initial climb followed by fluctuationsregardless of the pPE approaches. Training dynamics Figures 4 and 5 show the reward and average response length dynamics during RFT for each pPE approach, respectively. From the figures, we observe that all pPE approaches result in very similar dynamics in both reward progression and average response length. The reward dynamics exhibit steep increase during the initial 5 that reasoning and null-example utilization iPE approaches already elicit long responses from the base model. Additionally, after RFT with any of the pPE approaches, the average response length generally increases. However, two exceptions stand out: the reasoning and null-example utilization pPE approachesboth of which achieve the highest performance improvements. This indicates that different pPE approaches result in different post-RFT model behaviors in terms of response length. Notably, the null-example utilization pPE approach produces the shortest average responses while achieving the highest overall performance gains, making it the most efficient model in terms of test-time compute budget. These findings suggest that pPE not only affects the qualitative behavior of post-RFT models, but also their efficiency. This highlights the potential of pPE as tool for influencing efficiency of post-RFT models. Four fundamental behaviors Figure 6 shows the ratio of four fundamental cognitive behaviors in responses from the quantitative evaluation, both when prompted with iPE approaches and after RFT with pPE approaches. We observe that backward chaining is the most prominent behavior across all modelsregardless of whether iPE or pPE is usedand is already present in the base model. In general, iPE approaches increase the frequency of this behavior, while pPE approaches tend to reduce it, with the exception of the planning approach. More broadly, we find that pPE approaches tend to decrease the overall presence of all four fundamental cognitive behaviors compared to iPE. Importantly, the ratio of these behaviors does not correlate well with final model performance. However, these ratios remain useful for highlighting how fundamental cognitive behaviors shift post-RFT, and for differentiating between pPE approaches based on their behavioral profiles. We speculate that this behavior classification frameworkoriginally developed to analyze reasoning models (Gandhi et al., 2025)may not generalize well to models trained with different pPE paradigms, which may incentivize different forms of fundamental behavior set, beyond those captured by the current classification framework. Five elicited behaviors Figure 7 shows the change in the frequency of five elicited behaviors when models are prompted or trained using iPE or pPE approaches, relative to the base model under zero-shot prompting. We observe that most iPE Figure 5: Evolution of the average response length for Qwen2.5-7B during RFT, showing an initial drop followed by gradual recovery across pPE approaches. steps (up to around step 20), which likely reflects the model learning to produce correctly formatted responses. Beyond this point, the training enters steadier phase in reward growth, coinciding with increased in response length. This suggests that the model begins to experiment with utilizing larger token budget to achieve higher reward, as seen in the upward trend in response length and fluctuations in reward. These observations lead us to conclude that different pPE approaches exert minimal influence on training dynamics. Instead, other factorssuch as the training algorithm (Yu et al., 2025; Liu et al., 2025), the base model family (Zeng et al., 2025a), and hyperparameter choiceslikely play more significant role in shaping training dynamics. Furthermore, given that the training dynamics are largely similar across pPE variants, yet the resulting models differ substantially in performance and behavior, we argue that reward and response length dynamics are poor indicators of final outcomes. Model AIME AMC GPQA MATH Avg. Qwen2.5-7B 1416.80 1352.54 534.29 841.74 1036. iPE Think Plan Code Knowledge Examples pPE"
        },
        {
            "title": "Think\nPlan\nCode\nKnowledge\nExamples",
            "content": "2512.17 1662.57 641.07 1406.30 2274.17 2042.70 1685.17 1657.47 2015.57 1136.20 1367.69 644.90 953.51 1237.22 1316.12 1024.96 1085.47 836.28 1082.96 831.98 534.29 534.29 534.29 534.29 534.29 476.86 476.47 492.44 587.10 442. 804.85 540.98 635.09 780.31 752.60 612.10 601.18 690.42 626.45 685.79 1304.75 845.69 690.99 989.53 1219.30 1039.16 962.07 919.15 1078.02 774.11 Table 2: Average response length, i.e., number of tokens, of Qwen2.5-7B when prompted with different iPE or RFT with different pPE approaches. Average response length Table 2 shows the average response length, i.e., the average number of generated tokens, computed from responses produced during quantitative evaluation. We observe 6 Figure 6: Ratio of the four fundamental cognitive behaviorsbacktracking, backward chaining, subgoal setting, and verificationacross different prompting (iPE) and RFT (pPE) approaches with Qwen2.5-7B. Backward chaining dominates across setups, especially under iPE. anticipated behavior. Instead, the model may discover more effective behavior patterns during RFT, independent of the pPE approach. Qualitative behaviors We present qualitative examples of generated responses in Appendix F. We observe that post-RFT models are generally able to produce behaviors aligned with the pPE approaches. For example, the planning pPE approach results in models that generate numbered list of steps to solve the problem and then execute them. The reasoning pPE approach leads to stepby-step reasoning, while the knowledge recall pPE approach elicits definitions and formulas relevant to solving the task. Interestingly, the null-example utilization pPE approach somewhat resembles the behavior of the reasoning pPE model, despite its differences in performance trends. We also observe that Qwen2.5-7B tends to prefer natural language reasoning over code-based reasoning during RFT. Specifically, under the code-based reasoning pPE approach, the model frequently generates natural language reasoning, followed by statement such as: <code> # We dont need to write any Python code since the problem is solved analytically. </code> This stands in contrast to the code-specialized model, which, as shown in the qualitative examples, relies more heavily on code generation as part of its problem-solving process. These behavioral differences among post-RFT models suggest that RFT with different pPE approaches can be used to steer models toward exhibiting distinct, desired behaviorssimilar to RLHF (Ouyang et al., 2022). For instance, it is possible to train plan-generating Figure 7: Ratio of five elicited behavior categories reasoning, planning, code-based reasoning, knowledge recall, and null-example utilizationas observed when behaviors are elicited through prompting with different iPE approaches (left) and after RFT with different pPE approaches (right). approacheswith the exception of reasoning and planningelicit high levels of reasoning, planning, and knowledge recall behaviors. However, this trend does not hold post-RFT. Instead, we find that post-RFT models exhibit the greatest increase in behavior instances corresponding to their specific pPE approach, except for null-example utilization approach. For example, the planning pPE approach results in the highest number of planning behavior instances, and similarly for knowledge recall and reasoning. Another notable observation is that only the code-based reasoning iPE and pPE approaches elicit higher number of code-based behavior instances, which is expected. Finally, we find that each pPE approach results in different distribution of the five elicited behavior types. Notably, the null-example utilization pPE approach yields the lowest number of knowledge recall instances, yet still achieves the highest performance gains on GPQA. Furthermore, it also exhibits the lowest number of null-example behavior instancesin contrast to its iPE counterpart and our expectations. This suggests that pPE approach may not always result in the model exhibiting the 7 model by applying RFT with prior prompt that elicits plan generation. However, for such model to be effective, the plan must not only be valid but also executable in way that achieves high reward. In this context, the reward signal serves as proxy for plan quality. We further discuss the implications, extensions, and applications of pPE for RFT in Appendix B."
        },
        {
            "title": "4.3 Generalization",
            "content": "Model AIME AMC GPQA MATH HE+ Avg. Qwen2.5 3B 13.33 24.10 9.60 49.40 62. 31.85 iPE"
        },
        {
            "title": "Think\nPlan",
            "content": "pPE"
        },
        {
            "title": "Think\nPlan",
            "content": "13.33 13.33 22.89 15.66 9.60 9.60 37.20 35.20 61.00 60.40 28.80 26. 10.00 0.00 12.05 0.00 11.11 7.07 28.40 0.00 61.00 59.10 24.51 13. Llama 3.1-8B 0.00 1.20 0.00 5.00 31. 7.58 iPE Think Plan pPE Think Plan 3.33 6. 3.33 3.33 3.61 4.82 6.02 2.41 0.00 0.00 0.00 0.00 8.00 6. 31.70 31.70 9.33 10.00 9.60 8.40 32.30 32.90 10.25 9.41 Qwen2.5-Coder-7B 6.67 15.66 25.25 27.60 81.10 31. iPE Think Code pPE Think Code 0.00 3.33 10.84 10. 25.25 25.25 22.20 12.00 75.60 78.00 26.78 25.88 13.33 13.33 37.35 18. 26.26 34.85 68.80 26.80 78.70 75.00 44.89 33.61 Table 3: Benchmark accuracy (%) of Qwen2.5-3B, Llama 3.1-8B, and Qwen2.5-Coder-7B when prompted with different iPE or RFT with different pPE approaches across five benchmarks. Bold indicates the best performance per column under the same base model; underlined indicates the second best per column under the same base model. Performance impact Table 3 shows the performance of Qwen2.5-3B, LLaMA 3.1-8B, and Qwen2.5-Coder-7B, which serve as representative models for our generalization studies. Additional accompanying resultsincluding training dynamics and behavior classificationare available in Appendix E. We observe that the reasoning pPE approach, i.e., <think>, is consistently robust across model families and sizes. This is likely due to its alignment with behavioral patterns already familiar to models from prior fine-tuning on CoT-like data (Chung et al., 2024). In contrast, smaller or weaker model families show limited success with non-reasoning pPE approaches, aligning with findings from (Zeng et al., 2025a) that such models benefit less from reasoning RFT. Behavioral differences We also observe instances of reward hacking when RFT is applied with the planning pPE approach in Qwen2.5-3B and LLaMA 3.1-8B. In these cases, the models output only correctly formatted responses in order to maximize the format reward, while neglecting further exploration of the accuracy reward (see Figures 25 to 28). Another notable observation is that the code-specialized Qwen2.5-Coder-7B model is more effective at exhibiting code-based reasoning behaviorsunder both iPE and pPEcompared to Qwen2.5-7B. This illustrates how different model families can influence the behaviors exhibited after RFT (Zeng et al., 2025a). Nevertheless, both reasoning and code-based reasoning pPE approaches improve performance over the baseline, although the reasoning pPE approach achieves the highest performance. Still, pPE demonstrates measurable success over iPE in steering model behavior postRFT, as illustrated in Figures 34 to 36. These findings suggest that pPE generalizes reliably in stronger, behaviorally aligned models, while less capable models are more prone to reward hacking or fail to internalize the intended behaviors."
        },
        {
            "title": "5 Conclusions",
            "content": "This paper investigates the impact of pPE in the context of RFT by evaluating five pPE approaches inspired by iPE: reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization. While these approaches often degrade performance when applied only at inference time (iPE), incorporating them during RFT (pPE) consistently improves performance relative to the base model. In particular, null-example utilization proves more effective than the reasoning approach for enhancing downstream task performance. Beyond these performance impact, different pPE approaches induce distinct behavioral patterns in the fine-tuned models. For example, models trained with the planning pPE approach tend to exhibit plan-and-solve behavior, i.e., generating list of steps before execution. Finally, we explore the generalization of pPE approaches across model sizes and families. We hope this study will inspire further research into the role of pPE in RFT, especially given the extensive literature on iPE."
        },
        {
            "title": "Limitations",
            "content": "Due to computational resource constraints, we were unable to conduct experiments with larger 8 model sizes, larger datasets, or higher number of steps. As result, the behavioral trends observed in this study remain inconclusive for larger models where emergent abilities (Wei et al., 2022a)known to appear only at scalemay lead to different outcomes. While we believe many of our findings will generalize across model sizes (as is often the case in iPE studies), this assumption remains to be validated. In contrast, smaller models may not capture the complexity or expressiveness of their larger counterparts due to their lower capacity and the limited potential of pPE, similar to iPE. Additionally, we fixed the training data domain (mathematics), the reinforcement learning algorithm (GRPO), and other experimental configurations to isolate the effect of pPE approaches. Future work should investigate how different domains, RL algorithms, and reward schemes interact with pPE. We conjecture that, as with iPE, once model demonstrates the ability to exhibit structured behaviors, those behaviors will generalize across architectures and settings. However, further studies are necessary to confirm this generalization in broader context of RFT."
        },
        {
            "title": "Ethical Considerations",
            "content": "Prompting language models to elicit specific behaviors is inherently unpredictable due to their stochastic nature (Bengio et al., 2000). RFT, which aims to amplify specific generation patterns for improved performance, may also unintentionally reinforce undesirable or unsafe behaviorsespecially those that were already latent in the pretrained model. We strongly recommend integrating established alignment techniques (Grattafiori et al., 2024; Bai et al., 2022; Dai et al., 2024) and safety measures (Zeng et al., 2025b; Inan et al., 2023), as prior studies (DeepSeek-AI et al., 2025; Seed et al., 2025) have shown that such safeguards remain effective even after RFT. As with iPE, models in dynamic or open-ended environments are vulnerable to misuse. For example, they may be exposed to malicious prompts (Liu et al., 2024) or poisoned data (Zhao et al., 2025) during RFT, leading to unexpected or concerning behaviors. Furthermore, the pPE framework proposed in this study can be extended to alignment and safetyfocused training, similar to recent efforts in deliberative alignment (Guan et al., 2025). To mitigate risks, we recommend safeguards such as prompt auditing, robust reward design, safe rollout filtering, and post-training alignment stepsespecially when applying RFT in safety-critical or user-facing applications."
        },
        {
            "title": "References",
            "content": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, and 32 others. 2022. Constitutional AI: Harmlessness from AI Feedback. Preprint, arXiv:2212.08073. Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000. Neural Probabilistic Language Model. In Advances in Neural Information Processing Systems, volume 13. MIT Press. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language Models are In Advances in Neural InforFew-Shot Learners. mation Processing Systems, volume 33, pages 1877 1901. Curran Associates, Inc. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. Transactions on Machine Learning Research. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. 2025. An Empirical Study on Eliciting and Improving R1-like Reasoning Models. Preprint, arXiv:2503.04548. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, and 16 others. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2024. Safe RLHF: Safe Reinforcement Learning from Human Feedback. In The Twelfth International Conference on Learning Representations. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, 9 Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Preprint, arXiv:2501.12948. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs. Preprint, arXiv:2504.11536. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. 2025. Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs. Preprint, arXiv:2503.01307. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The Llama 3 Herd of Models. Preprint, arXiv:2407.21783. Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, and Amelia Glaese. 2025. Deliberative Alignment: Reasoning Enables Safer Language Models. Preprint, arXiv:2412.16339. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework. Preprint, arXiv:2405.11143. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in Neural Information Processing Systems. Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip H. S. Torr, Fahad Shahbaz Khan, and Salman Khan. 2025. LLM Post-Training: Deep Dive into Reasoning Large Language Models. Preprint, arXiv:2502.21321. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 23, page 611626, New York, NY, USA. Association for Computing Machinery. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, and 1 others. 2024. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9. Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31543169, Dublin, Ireland. Association for Computational Linguistics. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. In Thirtyseventh Conference on Neural Information Processing Systems. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. 2025. OpenReasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model. Preprint, arXiv:2503.24290. Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2024. Prompt Injection attack against LLM-integrated Applications. Preprint, arXiv:2306.05499. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, and 5 others. 2024. Qwen2.5-Coder Technical Report. Preprint, arXiv:2409.12186. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. 2023. Llama Guard: LLMbased Input-Output Safeguard for Human-AI Conversations. Preprint, arXiv:2312.06674. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding R1-Zero-Like Training: Critical Perspective. Preprint, arXiv:2503.20783. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, and 47 others. 2024. StarCoder 2 and The Stack v2: The Next Generation. Preprint, arXiv:2402.19173. 10 Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1104811064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, and 21 others. 2025. 2 OLMo 2 Furious. Preprint, arXiv:2501.00656. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. Preprint, 2025. arXiv:2412.15115. Qwen2.5 Technical Report. Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen, Haibo Ding, and 2 others. 2025. Systematic Survey of Automatic Prompt Optimization Techniques. Preprint, arXiv:2502.16923. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: Graduate-Level Google-Proof Q&A Benchmark. In First Conference on Language Modeling. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, and 7 others. 2024. Code Llama: Open Foundation Models for Code. Preprint, arXiv:2308.12950. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2025. Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications. Preprint, arXiv:2402.07927. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxiPreprint, Alec Radford, and Oleg Klimov. 2017. mal Policy Optimization Algorithms. arXiv:1707.06347. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2024. Quantifying Language Models Sensitivity to Spurious Features in Prompt Design or: How learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations. ByteDance Seed, :, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, and 255 others. 2025. Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning. Preprint, arXiv:2504.13914. Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, 623(7987):493498. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. Preprint, arXiv:2402.03300. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. 2025. Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains. Preprint, arXiv:2503.23829. Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. 2024. Struc-bench: Are large language models good at generating complex structured tabular data? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 1234, Mexico City, Mexico. Association for Computational Linguistics. Pittawat Taveekitworachai, Febri Abdullah, and Ruck Thawonmas. 2024. Null-shot prompting: Rethinking prompting large language models with hallucination. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1332113361, Miami, Florida, USA. Association for Computational Linguistics. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26092634, Toronto, Canada. Association for Computational Linguistics. 11 Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent Abilities of Large Language Models. Transactions on Machine Learning Research. Survey Certification. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. 2025. SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution. Preprint, arXiv:2502.18449. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning. Preprint, arXiv:2502.14768. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of Draft: Thinking Faster by Writing Less. Preprint, arXiv:2502.18600. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying Long Chain-of-Thought Reasoning in LLMs. Preprint, arXiv:2502.03373. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. DAPO: An Open-Source LLM Reinforcement Learning System at Scale. Preprint, arXiv:2503.14476. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, and 8 others. 2025. VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks. Preprint, arXiv:2504.05118. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. 2025a. SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild. Preprint, arXiv:2503.18892. Wenjun Zeng, Dana Kurniawan, Ryan Mullins, Yuchi Liu, Tamoghna Saha, Dirichi Ike-Njoku, Jindong Gu, Yiwen Song, Cai Xu, Jingjing Zhou, Aparna Joshi, Shravan Dheep, Mani Malek, Hamid Palangi, Joon Baek, Rick Pereira, and Karthik Narasimhan. 2025b. ShieldGemma 2: Robust and Tractable Image Content Moderation. Preprint, arXiv:2504.01081. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, and Chen Ma. 2025. Survey on TestTime Scaling in Large Language Models: What, How, Where, and How Well? Preprint, arXiv:2503.24235. Pinlong Zhao, Weiyao Zhu, Pengfei Jiao, Di Gao, and Ou Wu. 2025. Data Poisoning in Deep Learning: Survey. Preprint, arXiv:2503.22759."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Reinforcement Fine-Tuning (RFT) Reinforcement learning (RL) has become common post-training method for large language models (LLMs) (Kumar et al., 2025). One prominent RL approach is reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), where reward modeltrained on human preference comparisonspredicts scalar scores for model outputs. This enables optimization of model behavior toward human-aligned responses, typically using Proximal Policy Optimization (PPO) (Schulman et al., 2017). recent shift in RL for LLM post-training is the introduction of reinforcement learning with verifiable rewards (RLVR), also known as reinforcement fine-tuning (RFT). First introduced by OLMo et al. (2025), RFT replaces the reward model with taskspecific, rule-based reward functions for domains with verifiable answers such as mathematics, logic, and code. This not only improves performance but also eliminates the need to train separate reward model and maintain it during training. RFT gained widespread attention following the release of DeepSeek-R1-Zero (DeepSeek-AI et al., 2025), which extends the RLVR paradigm by incorporating two key modifications: (1) replacing PPO with Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to eliminate the need for separate value model, reducing compute cost; and (2) introducing prior prompt to elicit reasoning behavior during training. While the former has received significant attention, the latterprior promptremains largely understudied. The core components of RFT include: (1) the RL algorithm, (2) base language model, (3) training dataset, (4) reward function, and (5) prior prompt. Recent studies have explored improvements in RL algorithms (e.g., Dr. GRPO (Liu et al., 2025), DAPO (Yu et al., 2025), VAPO (Yue et al., 2025)), base model effects (Zeng et al., 2025a; Gandhi et al., 2025), and expanding verifiable tasks such as logic (Xie et al., 2025), coding (Wei et al., 2025), and function calling (Feng et al., 2025). However, the role of prior prompts has received little attention. Aside from one study noting their effect on training stability (Xie et al., 2025; Hu et al., 2025), prompt design in RFT remains significantly underexplored. Given the importance of prompting in inference-time settings (iPE), we argue that pPE deserves focused study as core axis of RFT. A.2 Inference-Time Prompt Engineering (iPE) iPE has seen rapid development since the introduction of ChatGPT (Sahoo et al., 2025). iPE refers to techniques for prompting LLMs to produce desirable outcomes. prominent direction in iPE is reasoning-centric prompting, with the seminal work being chain-of-thought (CoT) prompting (Wei et al., 2022b), which uses in-context examples to demonstrate multi-step reasoning. This was later extended by zero-shot CoT (Kojima et al., 2022) prompting, where simple phrase like Lets think step by step. is sufficient to elicit similar behavior from capable LLMs. Since then, many variants have emerged to elicit range of intermediate reasoning patternsbeyond just step-by-step reasoning. These include prompting for planning (Wang et al., 2023), code generation (Chen et al., 2023), knowledge recall (Liu et al., 2022), and hallucination induction (Taveekitworachai et al., 2024). We note conceptual parallel between iPE and pPE: both initially focused on reasoning, but iPE has since broadened to include diverse useful behaviors (Sahoo et al., 2025). Motivated by this, our study extends RFT by incorporating range of iPE-inspired prompting strategies as prior prompts. We aim to investigate whether these paradigms, when moved from inference to training time, yield corresponding behavioral changes during RFT."
        },
        {
            "title": "B Additional Discussions",
            "content": "B.1 Domain generalization We observe that, although only mathematical problems is used during training, performance improvements often extend to other domainsas seen in GPQA and HE+ in Tables 1 and 3. This demonstrates the robustness of the RFT approach in general and suggests that RFT may function more as mechanism for discovering useful generation patterns than for infusing the model with new knowledge. We expect broader performance generalization to emerge with more diverse training data, such as by incorporating code or logic problems. B.2 pPE for RFT As demonstrated in this study, the importance of pPE in RFT is analogous to the role of iPE for LMs. Prompts play critical role in conditioning the base models generation, which in turn affects the trajectories sampled during RFTultimately leading to distinct post-training behaviors. This also implies that properties known to affect LMs during inference, such as sensitivity to prompt wording (Kojima et al., 2022; Shanahan et al., 2023), formatting (Sclar et al., 2024; Tang et al., 2024), and prompt order (Taveekitworachai et al., 2024; Min et al., 2022), can similarly influence RFT outcomes. However, our results in Section 4.1 suggest that insights from iPE do not directly translate to RFTreinforcing the need for targeted study of pPE. That said, the models instructionfollowing capabilities can still be leveraged to incentivize distinct behavioral patterns through carefully designed prior prompts. B.3 Beyond Reasoning Models We discuss here several promising directions enabled by pPE, inspired by advances in iPE: Not only think, plan, code, and recall knowledge The reasoning trace itself can be an important vehicle for interpretability and user trust (Wei et al., 2022b). Given that we can elicit distinct reasoning styles, we may tailor them to user preferences or application requirements. For instance, an LLM could reason in self-talk style using <dialogues> tags, or imitate specific style using few-shot demonstrations (Brown et al., 2020). Recent work on chain-of-draft prompting (Xu et al., 2025) shows that natural language constraints can guide the model to produce shorter but still effective reasoning traces. Such behavior can likely be transferred into pPE settings, especially with models that possess strong instruction-following capabilities. The breadth of iPE research suggests many additional stylesbeyond those we studied herecould be explored and reinforced via pPE. 13 Dynamic pPE As shown in Table 1, different pPE approaches excel in different domains. Dynamically selecting the prior prompt based on the task or question difficulty could further enhance performance. This idea aligns with the test-time scaling paradigm (Zhang et al., 2025), which advocates allocating more resources to harder inputs. Furthermore, prior prompts could become part of the RL optimization process, akin to automatic prompt search (Ramnath et al., 2025). While such approaches increase system complexity, they offer path toward more adaptive and robust behaviors. Structured thinking Instead of using single behavior tag (e.g., <think>), we may extend to multi-tag structures (e.g., combining <plan> and <code>) to guide the model through more structured multi-phase reasoning processes. This may be especially beneficial in tasks requiring distinct reasoning modes at different stages (e.g., planning followed by execution). Incentivizing behaviors through verifiable rewards Consider model trained with the <plan> prompt. During training, it learns to produce useful plan inside the <plan> tag before solving the problem in <answer>. Because final task accuracy is used as reward, this implicitly incentivizes the model to generate effective intermediate content. Thus, verifiable rewards can act as surrogate signal for training behaviorssuch as planning or codingwithout direct supervised signals. This logic extends to other prompts: if we stop generation after </plan> or </code>, we can repurpose these models to act as plan generators or code synthesizers. This strategy opens up broader class of behavioral specialization, where useful intermediate behaviors can be extracted and repurposed for downstream applicationsall trained indirectly via RFT."
        },
        {
            "title": "C Prior Prompt Examples",
            "content": "In this section, we provide additional two examples of prior prompts used in existing RFT studies to elicit reasoning behavior during RFT. These are Figures 8 and 9."
        },
        {
            "title": "D Additional Experimental Setup Details",
            "content": "This section provides additional implementation details of our experimental setup, including prior prompt templates in Appendix D.1, training scripts reward function design in in Appendix D.3, 14 Logic-RL Prior Prompt You are helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to solve logical reasoning problem. After thinking, when you finally reach conclusion, clearly state the identity of each character within <answer> </answer> tags. i.e., <answer> (1) Zoey is knight, (2) ... </answer>. Figure 8: The prompt used during RFT by Logic-RL (Xie et al., 2025). Open-Reasoner-Zero Prior Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>. And your final answer will be extracted automatically by the boxed{} tag. {{user_prompt}} Assistant: <think> Figure 9: The prompt used during RFT by OpenReasoner-Zero (Hu et al., 2025). Appendix D.2, and evaluation details in Appendix D.4."
        },
        {
            "title": "Code Prompt",
            "content": "D.1 Prior Prompts This section presents the full set of prior prompts used in our experiments, as described in Section 3. Each prompt was designed to elicit different behavioral styles from the model. These prompts are: <think> (Figure 10, for step-bystep reasoning), <plan> (Figure 11, for planning), <code> (Figure 12, for reasoning through code), <knowledge> (Figure 13, for recalling relevant facts), and <examples> (Figure 14, for utilizing null-examples)."
        },
        {
            "title": "Think Prompt",
            "content": "You are helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to solve mathematical reasoning problem. After thinking, when you finally reach conclusion, clearly state the final answer in boxed{} within <answer> </answer> tags. You always answer mathematically and do not state the choice. Figure 10: The <think> prior prompt, inspired by chainof-thought (CoT) prompting (Wei et al., 2022b), encourages the model to reason step by step before concluding with an answer."
        },
        {
            "title": "Plan Prompt",
            "content": "You are helpful assistant. The assistant first plans about the reasoning process in the mind and then provides the user with the answer. The plan and answer are enclosed within <plan></plan> and <answer></answer> tags, respectively, i.e., <plan> detailed plan here </plan><answer> answer here </answer>. Now the user asks you to solve mathematical reasoning problem. After planning, when you finally reach conclusion, clearly state the final answer in boxed{} within <answer> </answer> tags. You always answer mathematically and do not state the choice. Figure 11: The <plan> prior prompt, based on plan-andsolve prompting (Wang et al., 2023), asks the model to explicitly lay out plan before solving the problem. You are helpful assistant. The assistant first writes required code used to solve the problem and then provides the user with the answer. The code and answer are enclosed within <code></code> and <answer></answer> tags, respectively, i.e., <code> detailed code here </code><answer> answer here </answer>. Now the user asks you to solve mathematical reasoning problem. After coding, when you finally reach conclusion, clearly state the final answer in boxed{} within <answer> </answer> tags. You always answer mathematically and do not state the choice. Figure 12: The <code> prior prompt encourages the model to reason through code, inspired by program-ofthought (PoT) prompting (Chen et al., 2023)."
        },
        {
            "title": "Knowledge Prompt",
            "content": "You are helpful assistant. The assistant first recalls relevant knowledge used to solve the problem and then provides the user with the answer. The knowledge and answer are enclosed within <knowledge></knowledge> and <answer></answer> tags, respectively, i.e., <knowledge> comprehensive knowledge here </knowledge><answer> answer here </answer>. Now the user asks you to solve mathematical reasoning problem. After recalling knowledge, when you finally reach conclusion, clearly state the final answer in boxed{} within <answer> </answer> tags. You always answer mathematically and do not state the choice. Figure 13: The <knowledge> prior prompt elicits factual recall relevant to the problem before beginning reasoning, inspired by generated knowledge prompting (Liu et al., 2022)."
        },
        {
            "title": "Examples Prompt",
            "content": "You are helpful assistant. The assistant first lists relevant examples used to solve the problem and then provides the user with the answer. The examples and answer are enclosed within <examples></examples> and <answer></answer> tags, respectively, i.e., <examples> relevant examples here </examples><answer> answer here </answer>. Now the user asks you to solve mathematical reasoning problem. After listing examples, when you finally reach conclusion, clearly state the final answer in boxed{} within <answer> </answer> tags. You always answer mathematically and do not state the choice. D.2 Reward Design We design our reward function with two equally weighted components: (1) an accuracy reward Figure 14: The <examples> prior prompt draws on null-shot prompting (Taveekitworachai et al., 2024) to encourage the model to provide illustrative examples before answering. 15 and (2) format reward. This setup follows the approach introduced by Xie et al. (2025). While some studies suggest the format reward may not be necessary (Zeng et al., 2025a), we find that it is crucial in our setting to ensure the model outputs are well-structured. Accuracy: The accuracy reward is based on the models predicted answer, extracted from the content enclosed in boxed{}. We use the math-verify1 package (Apache License 2.0) to check for mathematical equivalence with the ground-truth answer. If the answer is equivalent, the model receives reward of 0.5; otherwise, it receives 0.0. Format: We adopt relaxed version of the format reward from the open-r12 (Apache License 2.0) implementation. The reward is given if the response includes exactly one pair of the expected XML tags (e.g., <think>...</think> followed by <answer>...</answer>), and the content satisfies basic XML structure constraints, even if the tags are not the only elements in the string. This constraint discourages generation of multiple or malformed tag pairs. If the response satisfies these constraints, reward of 0.5 is granted; otherwise, it receives 0.0. The total reward is the sum of these two components, yielding final reward in the range [0, 1]. This balanced reward design helps incentivize both correct and well-structured responses during RFT. D.3 Training Setup and Hyperparameters We use the training script illustrated in Figure 15 for training the models as described in Section 3.2. All training runs use single node equipped with 8xH100 GPUs. Across all experiments presented in this paper, we utilized total of 78 GPU-hours. We note that both OpenRLHF and vLLM are available under the Apache License 2.0. The Qwen2.5-7B model used in our main experiments is distributed under the Apache License 2.0. For our generalization studies, the Qwen2.5-3B model is distributed under the Qwen Research License, the Llama 3.1-8B model under the Llama 3.1 Community License Agreement, and the Qwen2.5Coder-7B model under the Apache License 2.0. All of these licenses permit use for research purposes. 1https://github.com/huggingface/Math-Verify 2https://github.com/huggingface/open-r"
        },
        {
            "title": "OpenRLHF Training Script",
            "content": "python3 -m openrlhf.cli.train_ppo_ray --ref_num_nodes 1 --ref_num_gpus_per_node 8 --actor_num_nodes 1 --actor_num_gpus_per_node 8 --vllm_num_engines 8 --vllm_tensor_parallel_size 1 --colocate_all_models --vllm_enable_sleep --vllm_gpu_memory_utilization 0.5 --pretrain \"Qwen/Qwen2.5-7B\" --remote_rm_url \"reward_function.py\" --micro_train_batch_size 1 --train_batch_size 64 --micro_rollout_batch_size 8 --rollout_batch_size 64 --n_samples_per_prompt 8 --enable_prefix_caching --max_epochs 1 --prompt_max_len 1024 --max_samples 10000 --generate_max_len 4096 --init_kl_coef 1e-6 --gamma 1.0 --use_kl_loss --kl_estimator k3 --advantage_estimator group_norm --zero_stage 2 --bf16 --actor_learning_rate 5e-7 --prompt_data \"user/stillv3\" --prompt_split \"train\" --input_key \"query\" --label_key \"answer\" --apply_chat_template --normalize_reward --adam_offload --gradient_checkpointing --flash_attn --packing_samples Figure 15: Training script using the OpenRLHF for RFT. This script specifies the model, dataset, GRPO algorithm, reward configuration, and other relevant hyperparameters. D.4 Evaluation In this section, we provide additional details on quantitative and qualitative evaluation, mentioned in Section 3.3. D.4.1 Quantitative Analysis We evaluate the performance of each trained model using both inand out-of-domain benchmarks. During evaluation, we consistently prepend the same prior prompt used during training to elicit the trained behaviors. We evaluate once with fixed random seed using pass@1 accuracy. The benchmarks are: Mathematical reasoning: AIME24 (Li et al., 2024), AMC12 2223 (Li et al., 2024), and MATH-500 (Hendrycks et al., 2021) are benchmarks used for evaluating mathematical reasoning and serve as our primary in-domain evaluations. Coding: HumanEval+ (base and extra) (Liu et al., 2023) is used to evaluate general coding ability and serves as an out-of-domain probe. Knowledge-based question answering: GPQADiamond (Rein et al., 2024) evaluates factual knowledge and complex reasoning. We include it to assess whether math-centric training with different prior prompts can elicit behaviors associated with knowledge recall. While this ability may appear unrelated to solving math problems, it can be useful for recalling definitions, theorems, or formulas relevant to given problem. Additional metadata of these evaluation benchmarks, along with our training set, is available in Table 4. D.4.2 Qualitative Analysis To investigate whether different pPE approaches lead to distinct behavioral patterns after RFT, we assess the following aspects: Training dynamics and response length We analyze whether different pPE approaches result in distinct training dynamics across models. In addition, we compute the average number of tokens in generated responses. Four fundamental cognitive behaviors of reasoning models Gandhi et al. (2025) identify four fundamental cognitive behaviors commonly exhibited by reasoning models. These behaviors are considered core components of what makes model capable of reasoning: (1) Verification: Identifying errors in intermediate results, (2) Backtracking: Abandoning the current approach and trying alternatives, (3) Subgoal setting: Breaking problems down into smaller, more manageable steps, and (4) Backward chaining: Reasoning backward from the expected answer to the given inputs. Following their methodology, we use an LLMbased classifier to detect the presence of each behavior in model outputs. While the original study used gpt-4o-mini, we employ more recent model, gpt-4.1-mini-2025-04-14, for classification. Our goal is to compare the distribution of these behaviors across models trained or prompted using different iPE/pPE strategies. Prompts used for classification are provided in Figures 16 to 19."
        },
        {
            "title": "Verifications Classification Prompt",
            "content": "You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem} The response the model used is the following: # Response {completion} the response contains any Evaluate whether answer-verification steps. An example of an answer-verification step is: This sequence results in 1, which is not equal to 22 and Since 25 is not equal to 22 for explicit verification and Too high! or This works! for implicit verification. We want to mark instances where the response explicitly checks the current result. If you find any answer-verification steps, please count them and provide the count as between the tags <count> </count>. If the response does not contain any answer-verification steps, please provide count of 0 as <count>0</count>. Figure 16: Prompt used to identify verification behavior explicit checking or validation of intermediate resultsin the models reasoning. Five pPE-specific behaviors We further adapt the same classification approach to evaluate whether the target behavior elicited by each pPE (e.g., reasoning, planning, coding, knowledge recall, or example generation) is present in model responses. This is treated as binary classifica-"
        },
        {
            "title": "License",
            "content": "Training dataset STILLv3 (Chen et al., 2025)"
        },
        {
            "title": "Train",
            "content": "29925 N/A N/A Evaluation benchmark Math AIME24 (Li et al., 2024) AMC12 2223 (Li et al., 2024) Math MATH-500 (Hendrycks et al., 2021) Math Code HumanEval+ (Liu et al., 2023) QA GPQA-Diamond (Rein et al., 2024)"
        },
        {
            "title": "Test\nTest\nTest\nTest\nTest",
            "content": "30 83 500"
        },
        {
            "title": "Number\nNumber\nNumber\nCode\nMC",
            "content": "N/A N/A MIT License Apache 2.0 CC BY 4.0 Table 4: Overview of the training dataset and evaluation benchmarks. We note that all datasets are available for the purposes used in this study."
        },
        {
            "title": "Backtracking Classification Prompt",
            "content": "You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem}"
        },
        {
            "title": "Subgoal Settings Classification Prompt",
            "content": "You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem} The response the model used is the following: The response the model used is the following: # Response {completion} # Response {completion} Evaluate whether the response contains any backtracking behavior, where the model realizes path wont work and explicitly goes back to try different approach. Count the number of distinct backtracking instances and provide the count between the tags <count> </count>. If the response does not contain any backtracking behavior, please provide count of 0 as <count>0</count>. Figure 17: Classification prompt used to detect instances of backtrackingwhen model revises or abandons previous approachin its reasoning trace. Evaluate whether the response contains any backward-chaining behavior, where the model starts from the target and works backwards to the initial problem. An example of backward-chaining when the target is 24 and the numbers are 12 and 2 is: \"Lets work backwards from the target. 24/2 = 12. So, 12*2=24.\" and if the target is 22 and the numbers are 25 and 3 is: \"Since the target is 22, and 22 + 3 = 25, ...\". Count the number of distinct backward-chaining instances and provide the count between the tags <count> </count>. If the response does not contain any backward-chaining behavior, please provide count of 0 as <count>0</count>. Figure 18: Prompt used to detect subgoal setting behavior, where the model breaks problem into smaller, intermediate steps."
        },
        {
            "title": "Backward Chaining Classification Prompt",
            "content": "You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem}"
        },
        {
            "title": "Think Classification Prompt",
            "content": "You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem} The response the model used is the following: The response the model used is the following: # Response {completion} Evaluate whether the response contains any explicit subgoal setting, where the model breaks down the problem into smaller, intermediate goals. An example of subgoal setting is: \"First, Ill try to get close to target/2, then...\". Count the number of distinct subgoals set and provide the count between the tags <count> </count>. If the response does not contain any subgoal setting, please provide count of 0 as <count>0</count>. # Response {completion} the response contains interEvaluate whether nal step-by-step logical reasoning to reach conclusion. Look for clear sequences of deductive or mathematical steps that reflect the assistant \"thinking through\" the problem logically. If you find any reasoning steps, please return If the response does not <result>YES</result>. contain any reasoning steps, please return <result>NO</result>. Figure 19: Classification prompt used to identify backward chainingreasoning from the goal back to known factswithin models response. Figure 20: Prompt used to classify whether the model is exhibiting reasoning aligned with the <think> prompt (step-by-step logical reasoning). tion task, assessing the presence or absence of the expected behavior per response. Classification prompts are provided in Figures 20 to 24. For all analyses, we exclude HumanEval+ due to missing raw responses from the evaluation program. We also omit very small number of responses (less than 0.05% of the total) due to response parsing errors from gpt-4.1-mini-2025-04-14."
        },
        {
            "title": "Plan Classification Prompt",
            "content": "ou are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem} The response the model used is the following:"
        },
        {
            "title": "E Additional Results",
            "content": "# Response {completion} This section presents additional results from the generalization studies discussed in Section 4.3. Training dynamics for the three language models are provided in Appendix E.1. Results for the classification of the four fundamental behaviors and the five elicited behavior categories are available in Appendix E.2 and Appendix E.3, respectively. In addition, we provide an alternative visualization of performance across benchmarks from the main experiments on Qwen2.5-7B, showing changes over the baseline in Table 5. Evaluate whether the response contains structured plan or approach for solving the problem before executing calculations. Look for high-level steps, strategies, or intentions stated clearly (e.g., \"First will factor this... then will check for...\", \"1. Conduct hypothesis testing.\"). If you find any solution planning, please return <result>YES</result>. If the response does not contain any solution planning, please return <result>NO</result>. Figure 21: Prompt used to identify whether the model is engaging in explicit planning, consistent with the <plan> prompting style."
        },
        {
            "title": "Examples Classification Prompt",
            "content": "You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem} The response the model used is the following: # Response {completion} Evaluate whether the response includes written code (in any programming language) used to compute or check the solution. Code may be accompanied by brief comments or used directly to reason. If you find any code implementation, please return <result>YES</result>. If the response does not contain any code implementation, please return <result>NO</result>. You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem} The response the model used is the following: # Response {completion} Evaluate whether the response provides one or more illustrative examples (worked out or referenced) that help explain the problem or solution process. These may be from simpler or analogous problems used to derive or validate the answer. If you find any example usage, please return If the response does not <result>YES</result>. contain any example usage, please return <result>NO</result>. Figure 22: Prompt used to determine whether codebased reasoning patterns, encouraged by the <code> prompt, are present in the model output. Figure 24: Prompt used to classify whether the model is generating illustrative examples, as intended by the <examples> prompting approach."
        },
        {
            "title": "Knowledge Classification Prompt",
            "content": "You are helpful assistant that analyzes mathematical reasoning. Here is response that language model generated while trying to solve the following problem the goal of accurately answer the problem. # Problem {problem} Model iPE Think Plan Code Knowledge Examples pPE Think Plan Code Knowledge Examples AIME AMC GPQA MATH HE+ Avg. -3.33 -3.33 +0.00 +6.67 +3.34 +6.67 +6.67 +3.34 +3.34 +6.67 -6.02 -7.23 -10.84 -12.05 -4.82 +6.02 +7.23 +9.64 +0.00 +6. +0.00 +0.00 +0.00 +0.00 +0.00 +4.04 +0.51 +1.01 -2.52 +6.57 +0.40 -4.40 -4.20 +4.00 +1.20 +17.60 +14.00 +10.60 +15.40 +15.60 +2.40 +1.20 -0.60 -0.60 -72.60 -2.50 -3.70 +5.40 +0.60 +0. -1.31 -2.75 -3.13 -0.40 -14.58 +6.37 +4.94 +6.00 +3.36 +6.97 The response the model used is the following: Table 5: Absolute change in accuracy (green = gain, red = drop) relative to the zero-shot Qwen2.5-7B base model. # Response {completion} Evaluate whether the response recalls relevant theorems, or concepts before solving the facts, problem. This includes definitions, properties, or known formulas used to justify or support the solution path. If you find any background knowledge, please return <result>YES</result>. If the response does not contain any background knowledge, please return <result>NO</result>. Figure 23: Prompt used to detect knowledge-recall behavior, as in <knowledge> approach. Figure 25: Reward progression for Qwen2.5-3B during RFT. 20 Figure 26: Evolution of the average response length for Qwen2.5-3B during RFT. Figure 27: Reward progression for Llama 3.1-8B during RFT. Figure 28: Evolution of the average response length for Llama 3.1-8B during RFT. Figure 29: Reward progression for Qwen2.5-Coder-7B during RFT. 21 Figure 30: Evolution of the average response length for Qwen2.5-Coder-7B during RFT. E.1 Training Dynamics and Average"
        },
        {
            "title": "Response Length",
            "content": "Figures 25 to 30 present the reward and responselength dynamics during RFT for Qwen2.5-3B, Llama 3.1-8B, and Qwen2.5-Coder-7B, respectively. Model AIME AMC GPQA MATH Avg. Qwen2.5 3B 1249.00 1297.70 455.85 981. 996.12 iPE Think Plan pPE Think Plan 1764.63 1349. 1156.81 1270.47 455.85 455.85 754.45 631.23 1032.94 926.67 124.37 9.00 492.70 9. 412.17 274.95 182.44 9.00 302.92 75.49 Llama 3.1-8B 8.03 11. N/A 128.93 49.40 iPE Think Plan pPE Think Plan 6122.33 4562.97 4347.72 5150.96 N/A N/A 4177.73 4452.76 4882.59 4722. 22.70 21.10 23.23 20.37 N/A N/A 25.84 53.94 23.92 31.80 Qwen2.5-Coder-7B 3856.83 2947.86 1510.95 3570.15 2971.45 iPE Think Code pPE Think Code 2601.03 779.67 748.31 337.59 1510.95 1510. 917.51 265.43 1444.45 723.41 1850.67 164.72 1045.12 254.41 592.05 311.49 758.67 152. 1061.63 220.82 Table 6: Average response length, i.e., number of tokens, of Qwen2.5-3B, Llama 3.1-8B, and Qwen2.5-Coder-7B when prompted with different iPE or RFT with different pPE approaches across four benchmarks. Finally, Table 6 reports the average response length, i.e., average number of tokens in responses for the generalization experiments. E.2 Four Fundamental Cognitive Behaviors In this subsection, we present the results of four fundamental cognitive behavior classifications from the generalization studies for Qwen2.5-3B, Llama 3.1-8B, and Qwen2.5-Coder-7B, shown in Figures 31 to 33, respectively. In addition, we provide detailed tables of the exact behavior counts for the main experiment in Table 7, previously visualized in Figure 6, and for the generalization studies in Table 8, previously visualized in Figures 31 to 33. Model Backtrack. Back.Chain. Subgoal.Set. Veri. Qwen2.5-7B iPE"
        },
        {
            "title": "Think\nPlan\nCode\nKnowledge\nExamples",
            "content": "pPE"
        },
        {
            "title": "Think\nPlan\nCode\nKnowledge\nExamples",
            "content": "30 31 60 66 79 114 27 32 42 63 46 1707 1542 2202 2394 2794 3159 1354 2450 1628 1180 9 195 15 6 31 56 30 16 11 19 14 8 201 156 223 274 453 150 110 262 148 Table 7: Occurrence counts of the four fundamental cognitive behaviorsbacktracking (Backtrack.), backward chaining (Back.Chain.), subgoal settings (Subgoal.Set.), and verifications (Veri.)in Qwen2.5-7Bs responses under different iPE and pPE approaches. Counts are obtained via the LM-based classification framework of Gandhi et al. (2025). Bold marks the highest count and underlined marks the lowest count per column. E.3 Five Elicited Behaviors In this subsection, we present the results of five elicited cognitive behavior classifications from the generalization studies for Qwen2.5-3B, Llama 3.18B, and Qwen2.5-Coder-7B, shown in Figures 34 to 36, respectively. Figure 34: Behavior alignment heatmaps for Qwen2.5 3B: iPE on the left, RFT on the right. Figure 31: Ratio of the four fundamental cognitive behaviorsbacktracking, backward chaining, subgoal setting, and verificationacross different prompting (iPE) and RFT (pPE) approaches with Qwen2.5-3B. Figure 32: Ratio of the four fundamental cognitive behaviorsbacktracking, backward chaining, subgoal setting, and verificationacross different prompting (iPE) and RFT (pPE) approaches with Llama 3.1-8B. Figure 33: Ratio of the four fundamental cognitive behaviorsbacktracking, backward chaining, subgoal setting, and verificationacross different prompting (iPE) and RFT (pPE) approaches with Qwen2.5-Coder7B. Figure 35: Behavior activation patterns for Llama 3.1 8B under iPE (left) and RFT (right). In addition, we provide detailed tables of the exact behavior counts for the main experiment in Table 9, previously visualized in Figure 7, and for the generalization studies in Table 10, previously visualized in Figures 34 to 36. 22 Model Backtrack. Back.Chain. Subgoal.Set. Veri. Qwen2.5-3B 51 Model Code Examples Knowledge Plan Think Qwen2.5-3B iPE"
        },
        {
            "title": "Think\nPlan",
            "content": "pPE"
        },
        {
            "title": "Think\nPlan",
            "content": "Llama 3.1-8B iPE"
        },
        {
            "title": "Think\nPlan",
            "content": "pPE"
        },
        {
            "title": "Think\nPlan",
            "content": "Qwen2.5-Coder-7B iPE"
        },
        {
            "title": "Think\nCode",
            "content": "pPE"
        },
        {
            "title": "Think\nCode",
            "content": "37 13 16 6 0 0 1 1 0 25 6 6 57 2 1611 646 1220 604 31 18 14 2 0 908 420 959 2172 10 5 13 12 0 2 4 6 0 7 3 17 21 12 91 125 46 42 7 5 0 0 0 644 24 53 313 Table 8: Occurrence counts of the four fundamental cognitive behaviorsbacktracking (Backtrack.), backward chaining (Back.Chain.), subgoal settings (Subgoal.Set.), and verifications (Veri.)in Qwen2.5-3B, Llama 3.1-8B, and Qwen2.5-Coder-7B responses under different iPE and pPE approaches. Counts are obtained via the LMbased classification framework of Gandhi et al. (2025). Bold marks the highest count and underlined marks the lowest count per column under the same base model. Figure 36: Behavior heatmaps for Qwen2.5-Coder 7B: iPE (left) vs. RFT (right). Model Code Examples Knowledge Plan Think Qwen2.5-7B 193 iPE Think Plan Code Knowledge Examples pPE Think Plan Code Knowledge Examples 31 38 704 88 68 34 23 602 14 21 77 55 51 60 112 68 57 58 61 57 626 652 711 635 616 1002 1168 1112 662 658 640 720 622 733 1137 1155 1136 741 759 688 722 704 716 659 1130 1289 1304 764 757 739 760 746 Table 9: Occurrence counts of the five elicited behaviorscode, examples, knowledge, plan, and think in Qwen2.5-7Bs responses under different iPE and pPE approaches. Bold marks the highest count and underlined marks the lowest count per column. iPE"
        },
        {
            "title": "Think\nPlan",
            "content": "pPE"
        },
        {
            "title": "Think\nPlan",
            "content": "Llama 3.1-8B iPE"
        },
        {
            "title": "Think\nPlan",
            "content": "pPE"
        },
        {
            "title": "Think\nPlan",
            "content": "3 0 1 0 0 4 3 0 0 Qwen2.5-Coder-7B 16 iPE"
        },
        {
            "title": "Think\nCode",
            "content": "pPE"
        },
        {
            "title": "Think\nCode",
            "content": "0 232 0 1142 53 27 15 8 0 9 8 1 0 54 18 43 103 21 545 630 535 491 788 30 71 120 4 3 513 650 920 22 9 28 32 2 532 423 570 26 36 152 172 5 3 525 669 355 426 299 328 382 409 1179 1247 60 1324 191 Table 10: Occurrence counts of the five elicited behaviorscode, examples, knowledge, plan, and think in Qwen2.5-3B, Llama 3.1-8B, and Qwen2.5-Coder7B responses under different iPE and pPE approaches. Bold indicates the best performance per column under the same base model; underlined indicates the second best per column under the same base model."
        },
        {
            "title": "F Qualitative Examples",
            "content": "To illustrate the behavioral differences induced by each pPE approach, we present qualitative outputs from models trained with different prior prompts in response to shared math problem. Examples of model responses for each pPE approach are shown in Figures 38 to 42, using the shared prompt in Figure 37."
        },
        {
            "title": "Example Prompt",
            "content": "How many positive whole-number divisors does 196 have? Figure 37: The shared prompt used to probe all models in this qualitative comparison. The question requires the model to determine the number of positive wholenumber divisors of 196."
        },
        {
            "title": "G Declaration of AI Assistance",
            "content": "We utilized ChatGPT for grammatical checking and LaTeX support of the content presented in this study but did not use it for the initial draft of this study. Cursor was utilized for trivial and boilerplate code completion during data analysis. We declare that all content presented and code utilized in this study has been reviewed and edited by the authors. Qwen2.5 7B RFT with Think Qwen2.5 7B RFT with Plan <think> To determine the number of positive whole-number divisors of 196, we first need to find its prime factorization. 1. Start by dividing 196 by the smallest prime number, which is 2: 196 2 = So, 196 = 2 98. 2. Next, divide 98 by 2: 98 2 = 49 <plan> 1. Identify the prime factorization of 196. 2. Use the formula for finding the number of divisors from the prime factorization. </plan> <answer> To find the number of positive whole-number divisors of 196, we first need to determine its prime factorization. 1. Prime factorization of 196: - 196 is even, so it is divisible by 2: 196 2 = 98. - 98 is also even, so it is divisible by 2: 98 2 = 49. - 49 is not even, but it is divisible by 7: 49 7 = 7. - 7 is prime number. So, 196 = 2 2 49. So, the prime factorization of 196 is 22 72. 3. Now, 49 is not divisible by 2, so we move to the next smallest prime number, which is 3. However, 49 is not divisible by 3 either. The next prime number is 5, and 49 is not divisible by 5. The next prime number is 7: 49 7 = 7 So, 196 = 2 2 7 7. 4. We can write this in exponential form: 196 = 22 72 2. Using the formula for finding the number of divisors from the prime factorization: If number has prime factorization of pe1 1 pe2 , then the number of divisors of is given by (e1 + 1)(e2 + 1) . . . (ek + 1). 2 . . . pek the exponents in the prime factorizaFor 196, tion are 2 and 2. Therefore, the number of divisors is (2 + 1)(2 + 1) = 3 3 = 9. the number of positive whole-number Thus, divisors of 196 is 9 . </answer> To find the number of divisors, we use the formula for the number of divisors based on the If number has prime prime factorization. factorization of pe1 , then the number of divisors D(n) is given by: 2 pek 1 pe2 Figure 39: Response from Qwen2.5 7B trained with the <plan> prior prompt. The model outlines structured plan before proceeding to execution. D(n) = (e1 + 1)(e2 + 1) (ek + 1) For 196, the exponents in the prime factorization are 2 and 2. Therefore, the number of divisors is: D(196) = (2 + 1)(2 + 1) = 3 3 = 9 the number of positive whole-number diSo, visors of 196 is 9. </think> <answer> 9 </answer> Qwen2.5 Coder 7B RFT with Code <code> divisors = [] number = for in range(1, number + 1): if number % == 0: divisors.append(i) </code> <answer>The number 196 has 9 positive wholenumber divisors.</answer> Figure 38: Response from Qwen2.5 7B trained with the <think> prior prompt. The model demonstrates stepby-step reasoning throughout its solution. Figure 40: Response from Qwen2.5-Coder 7B trained with the <code> prior prompt. The model uses Python code to assist in its reasoning process. 24 Qwen2.5 7B RFT with Knowledge <knowledge> To determine the number of positive whole-number divisors of 196, we first need to find its prime factorization. The prime factorization of number is the product of prime numbers that equals the original number. Lets find the prime factorization of 196: 1. 196 is even, so it is divisible by 2: 196 2 = 98 So, we have 196 = 2 98. 2. 98 is also even, so it is divisible by 2: 98 2 = 49 So, we have 196 = 2 2 49. 3. 49 is not even, but it is divisible by 7: 49 7 = 7 So, we have 196 = 2 2 7 7. 4. 7 is prime number, so we stop here. The prime factorization of 196 is 22 72. Qwen2.5 7B RFT with Examples <examples> To determine the number of positive whole-number divisors of 196, we first need to find its prime factorization. 1. Start by dividing 196 by the smallest prime number, which is 2: So, 2 is prime factor. 2. Divide 98 by 2 again: 196 2 = 98 98 2 = So, 2 is prime factor again. 3. Now, 49 is not divisible by 2, so we move to the next smallest prime number, which is 3. However, 49 is not divisible by 3 either. The next prime number is 5, but 49 is not divisible by 5. The next prime number is 7: 49 7 = 7 So, 7 is prime factor. Finally, 7 is prime number itself, so we divide it by 4. 7: 7 7 = 1 So, 7 is prime factor again. Thus, the prime factorization of 196 is: 196 = 22 72 To find the number of positive divisors, we use the formula that involves adding 1 to each of the exponents in the prime factorization and then multiplying the results: To find the number of positive whole-number divisors, we use the formula for finding the number of divisors from the prime factorization. If number has prime factorization of ek pe1 1 pe2 , then the number of positive divisors of is given by: 2 (2 + 1)(2 + 1) = 3 3 = 9 (e1 + 1)(e2 + 1) (ek + 1) Therefore, 196 has 9 positive whole-number divisors. </knowledge> <answer> 9 </answer> Figure 41: Response from Qwen2.5 7B trained with the <knowledge> prior prompt. The model first recalls definitions and relevant facts before solving the problem. For 196, 2. Therefore, the number of positive divisors is: the exponents in the prime factorization are 2 and (2 + 1)(2 + 1) = 3 3 = 9 So, 196 has 9 positive whole-number divisors. </examples> <answer> 9 </answer> Figure 42: Response from Qwen2.5 7B trained with the <examples> prior prompt. The model introduces illustrative examples to support its reasoning."
        }
    ],
    "affiliations": [
        "SCB 10X R&D, SCB 10X, SCBX Group, Thailand",
        "School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Thailand"
    ]
}