{
    "paper_title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
    "authors": [
        "Siyu Yuan",
        "Zehui Chen",
        "Zhiheng Xi",
        "Junjie Ye",
        "Zhengyin Du",
        "Jiecao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%)."
        },
        {
            "title": "Start",
            "content": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Siyu Yuan*1,2, Zehui Chen*2, Zhiheng Xi1,2, Junjie Ye1,2, Zhengyin Du2 and Jiecao Chen2 *Equal contributions, 1Fudan University, 2ByteDance Seed 5 2 0 2 0 2 ] A . [ 1 5 2 4 1 1 . 1 0 5 2 : r Abstract: Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive and agentic environments. Existing work primarily focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is notoriously difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions solely based on correctness, our approach leverages Monte Carlo Tree Search (MCTS) to construct training samples that recover correct trajectories from erroneous ones. key challenge of agent task reflection lies in the necessity for timely revision rather than waiting until the end of rollout to revise errors. To address this, we introduce model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that this approach continuously improves the models ability to recover from errors and enables earlier/timely error correction. Extensive experiments on three representative interactive and agentic environments show that the proposed framework effectively equips agents to identify and correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%). Code is available at https://github.com/bytedance/Agent-R. 1. Introduction Large Language Models (LLMs) have become foundational tools in solving complex tasks across interactive and agentic environments (Wang et al., 2022; Yao et al., 2022; Deng et al., 2023; Zhang et al., 2024b; Chen et al., 2024c). These LLM-based agents are increasingly employed in scenarios requiring capabilities such as autonomous decision-making, error correction, and task optimization (Prasad et al., 2024; Liu et al., 2024b). Despite the impressive performance of the existing methods, their reliance on behavior cloning from stronger experts poses significant limitations: due to the adoption of all-correct trajectories for training, it struggles to proactively self-correct errors, leading to cascading failures and suboptimal task performance (Zhang et al., 2024a; Xie et al., 2024). This limitation arises from an inability to effectively detect errors or revise trajectories dynamically once errors occur, highlighting the need for methods emphasizing timely revision capabilities. Previous work has proposed methods relying on explicit error signals or reward functions for selfcorrection. However, these methods mainly focus on single-turn scenarios, such as code repair (Kim et al., 2024; Chen et al., 2024b), tool use (Olausson et al., 2023), and mathematical reasoning (Kumar et al., 2024; Havrilla et al., 2024). In contrast, tasks in interactive and agentic environments usually involve multi-turn interactions and do not reveal explicit error signals until reaching the terminal state. Additionally, unlike mathematical reasoning (Xi et al., 2024b), designing high-quality reward functions to critique intermediate actions in long interactive trajectories remains difficult. Corresponding author(s): syyuan21@m.fudan.edu.cn, lovesnow@mail.ustc.edu.cn Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Figure 1: Illustration of language agents struggling with error correction in trajectory generation. These errors can cause agents to enter loops, hindering recovery in long trajectories and resulting in suboptimal outcomes. Agent-R enables agents to effectively detect and address errors in real-time, handling long-horizon tasks and avoiding loops with greater self-reflection capabilities. critical bottleneck in enhancing error recovery in interactive and agentic environments is the lack of step-level reflection data. Traditional approaches to collecting these datasets involve labor-intensive annotation processes, which are both time-consuming and costly (Lin et al., 2024; Zeng et al., 2024b; Zheng et al., 2024). Without robust reflection data, models face challenges in identifying and correcting their own errors, limiting their utility as intelligent agents. Constructing reflection datasets is thus essential for building agents capable of self-reflection and better decision-making. However, how to automatically construct such training samples is non-trivial. significant challenge of agent task reflection lies in the necessity for timely revision rather than waiting until the end of rollout to revise errors. If corrections are applied only at the end of the trajectory, the delayed revisions prevent agents from learning to detect and address errors as they occur, undermining their capacity for real-time self-reflection. Furthermore, delayed revisions may leave catastrophic errors unaddressed, particularly those occurring early in the trajectory. To address these challenges, we propose Agent-R, novel framework designed to enable LLM-based agents to perform on-the-fly reflection and self-improvement. Unlike previous reward-based approaches, which directly penalize or reward actions based solely on outcome correctness (Song et al., 2024b; Xiong et al., 2024; Shi et al., 2024; Putta et al., 2024), Agent-R introduces dynamic self-training framework that revises errors at the step level. By leveraging Monte Carlo Tree Search (MCTS) (Kocsis and Szepesv√°ri, 2006), Agent-R dynamically constructs training samples that recover correct trajectories from erroneous ones, effectively guiding the agent to navigate complex decision spaces. Specifically, Agent-R identifies the most suitable revision step (based on the current actor model) in an incorrect trajectory and connects it with the subsequent correct trajectory, enabling real-time recovery instead of rolling out to the end of the trajectory. This dynamic revision process not only enhances the agents reflection ability but also mitigates the risk of concatenating inconsistent or incoherent trajectories, which can occur with naive correction strategies. To further explore the scalability of Agent-R, we iteratively refine both its error correction capabilities and its dataset construction process. This iterative approach enables the agent to continuously improve its performance, correcting errors earlier in trajectories and reducing the likelihood of entering erroneous loops. Extensive experiments across three diverse and representative interactive environments (Wang et al., 2 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training 2022; Yao et al., 2022; Prasad et al., 2024) demonstrate that Agent-R significantly outperforms baselines by providing language agents with self-correction capability similar to human decision-making. Notably, training with revision trajectories surpasses the use of expert trajectories. Additionally, AgentR enables agents to more effectively identify and correct erroneous actions while avoiding loops, limitation observed in previous methods trained on expert trajectories. Overall, the contributions of this paper can be summarized as follows: We are the first to highlight the critical challenge of error correction in interactive environments for language agents. This limitation is overlooked by prior methods. We introduce Agent-R, novel iterative framework that empowers language agents with selfreflection capabilities in interactive environments. By leveraging MCTS and model-guided reflection mechanism, Agent-R dynamically constructs and revises trajectories, enabling agents to improve their behavior through autonomous self-correction without relying on human or expert model supervision. We conduct extensive experiments across three diverse interactive environments, demonstrating that Agent-R significantly outperforms baselines. By leveraging self-correction, Agent-R enables agents to effectively detect and address errors in real-time, avoiding loops and handling longhorizon tasks with greater robustness. 2. Preliminary 2.1. Task Formulation In this paper, we focus on tasks with partial observations in interactive environments. Following prior work (Song et al., 2024b; Qiao et al., 2024), these tasks can be formulated as Partially Observable Markov Decision Process (POMDP): (ùí∞, ùíÆ, ùíú, ùí™, ùíØ , ‚Ñõ). The instruction space ùí∞ provides task descriptions and their corresponding requirements. ùíÆ represents the state space, ùíú is the action space, and ùí™ is the observation space. The transition function ùíØ : ùíÆ ùíú ùíÆ is determined by the environment, while the reward function ‚Ñõ : ùíÆ ùíú [0, 1] specifies the reward. For language agent, ùí∞, ùíÆ, ùíú, and ùí™ are represented in natural language. At each time step ùë°, the historical trajectory ùúèùë° is defined as: ùúèùë° = (ùëé1, ùëú1, . . . , ùëéùë°, ùëúùë°) ùúãùúÉ(ùúèùë°ùë¢) where (ùëé1, ùëú1, . . . , ùëéùë°, ùëúùë°) is sequence of actions and observations, and ùëúùë° is the observation obtained after executing action ùëéùë°. The language agent with parameter ùúÉ is the actor model ùúãùúÉ. At each time step ùë° + 1, the actor model should generate ùëéùë°+1 based on ùúèùë° and instruction ùë¢, i.e., ùëéùë°+1 ùúãùúÉ(ùúèùë°, ùë¢). We adopt the ReAct approach (Yao et al., 2023b), generating rationales before each action to enhance performance. The task ends when it is successfully completed, or the maximum number of rounds is reached. Then, the final reward ùëü(ùúè ) [0, 1] is then given by the interactive environments. Thus, at the terminal time step ùëá , the probability distribution of the entire trajectory ùúèùëá is represented as: (1) ùúãùúÉ(ùúèùëá ùë¢) = ùëá ùë°=1 ùúãùúÉ(ùëéùë°ùúèùë°1, ùë¢). (2) 2.2. Monte Carlo Tree Search Monte Carlo Tree Search (MCTS) is decision-making algorithm widely used in complex decision processes (Kocsis and Szepesv√°ri, 2006; Browne et al., 2012). It builds search tree and simulates 3 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Figure 2: The framework of Agent-R consists of two phases. In Phase I, we adopt MCTS and model-guided reflection mechanism to construct revision trajectories. In Phase II, the agents are trained using the collected revision trajectories. These two phases can be repeated iteratively. rs is the revision signal, ùë° is the transition point between the bad and good trajectories, and ùêø(ùúÉ) is the loss function to be optimized. outcomes to estimate the value of actions. MCTS operates by iteratively developing decision tree through four key stages: Selection: Uses the Upper Confidence bound for Trees (UCT) strategy (Kocsis and Szepesv√°ri, 2006) to select the next node for expansion. Expansion: Based on the selected node, generates and adds new nodes to the tree. Simulation: Performs multiple rollouts from the newly added node to terminal node, which could be either terminal state or predetermined maximum tree depth. Backpropagation: Updates node values based on the results of the simulations. 3. Method In this section, we present Agent-R in detail, which consists of two core phases, as illustrated in Figure 2. Phase I: Model-Guided Reflection Trajectory Generation (S 3.1), which utilizes Monte Carlo Tree Search (MCTS) to dynamically generate revision trajectories, transforming erroneous trajectories into corrected ones by identifying the most suitable reflection step (for current policy model). Phase II: Iterative Self-Training with Revision Trajectories, where agents are iteratively trained on the dynamically constructed revision trajectories. Such an iterative training manner enables us to explore the scalability of Agent-R, where agents progressively learn harder (earlier) revision steps based on its current policy and improve their decision-making process, avoiding cascading errors and loops. 3.1. Phase I: Model-Guided Reflection Trajectory Generation Reflection Trajectory Definition We first define four types of trajectories that play central role in Agent-R: initial trajectories, bad trajectories, good trajectories, and revision trajectories. These are described as follows1: 1The detailed formal definitions and mathematical formulations for each type of trajectory are shown in Appendix 4 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training 1, ùëúùëñ 1, . . . , ùëéùëñ ùë°, ùëúùëñ ùë°): An initial trajectory represents the initial Initial Trajectory, denoted as ùúè ùëñ = (ùëéùëñ sequence of actions and observations. Bad Trajectory, denoted as ùúè ùëè = (ùúè ùëñ, ùëéùëè ): bad trajectory extends the initial ùë°+1, . . . , ùëéùëè ùëáùëè trajectory ùúè ùëñ with sequence of suboptimal actions and observations, leading to an erroneous or less-rewarding outcome. ùë°+1, ùëúùëè , ùëúùëè ùëáùëè Good Trajectory, denoted as ùúè ùëî = (ùúè ùëñ, ùëéùëî ùë°+1, ùëúùëî ùë°+1, . . . , ùëéùëî ùëáùëî , ùëúùëî ùëáùëî ): good trajectory builds upon the initial trajectory ùúè ùëñ, involving optimal or high-reward actions and observations. Revision Trajectory, denoted as ùúè ùëü: revision trajectory is constructed by correcting bad trajectory with good trajectory. Formally, it is defined as: ùúè ùëü = (ùúè ùëñ, ùëéùëè ùë°+1, ùëúùëè ùë°+1, . . . , ùëéùëè ùë°, ùëúùëè ùë°, rs, ùëéùëî ùë°+1, ùëúùëî ùë°+1, . . . , ùëéùëî ùëáùëî , ùëúùëî ùëáùëî ), (3) where ùë° represents the transition point between the bad and good trajectory segments, and rs is the revision signal marking the transition. The revision signal rs is brief communication OK., which facilitates such as: Assistant: [reflection on the error] Human: the agents reflection on its past actions. The interactive environment will provide the final reward after the trajectory has terminated. Based on the final reward, we define constraints to ensure that bad trajectories can be effectively corrected into good ones and that the quality of the revision trajectory matches that of the good trajectory. The reward conditions are as follows: ùëü(ùúè ùëè) < ùõΩ < ùëü(ùúè ùëî) 1, ùõº < ùëü(ùúè ùëî) = ùëü(ùúè ùëü), (4) where ùõΩ is threshold separating the rewards of bad and good trajectories, and ùõº represents lower bound for high-quality trajectories, encouraging revisions that are of consistently high quality. ùëü(ùúè ùëî) = 1 means that this good trajectory is Optimal Trajectory. Trajectory Collection with MCTS To efficiently explore and reflect errors within the trajectory space, we employ Monte Carlo Tree Search (MCTS) to collect revision trajectories. This method systematically searches through possible action paths, ensuring diversity in the generated trajectories while balancing exploration and exploitation. Starting from the initial root node ùë†0, which corresponds to the user-provided instruction ùë¢, MCTS iteratively performs four key stages: selection, expansion, simulation, and backpropagation. As shown in Figure 2, during the simulation stage, default rollout policy is used to sample future actions. To improve reward estimation and ensure diversity in collected trajectories, multiple rollouts are performed for each simulation. In the selection phase, we balance exploration and exploitation using the UCT criterion: ùëà ùê∂ùëá (ùë†) = ùëÑ(ùë†) + ùëêuct log ùëÅùëù(ùë†) ùëÅ (ùë†) , (5) where ùëÑ(ùë†) is the average reward of state ùë†, ùëÅ (ùë†) the number of visits to state ùë†, ùëÅùëù(ùë†) is the total visit count of the parent node of ùë†, and ùëêuct is constant that controls the exploration-exploitation trade-off. When terminal node is reached, either due to terminal state or exceeding predefined maximum depth, MCTS generates trajectory from the root node to the terminal node and obtains the final reward for the trajectory from the environment. By performing multiple rollouts, we collect diverse set of trajectories. Good and bad trajectories share the same initial trajectory and begin to diverge 5 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training after step ùë°, where their quality is determined by the final reward given by environments. Good trajectories achieve higher rewards, while bad trajectories fall below threshold. To construct the revision trajectories, we first manually design ten different revision thoughts for rs.2 During the construction process, one of these thoughts is randomly sampled to connect the segments of bad and good trajectories. Following Kumar et al. (2024), simple baseline, referred to as the Direct-Revision Trajectory, sets the transition point ùë° as the terminal step of the bad trajectory (ùë° = ùëáùëè), meaning that the good trajectory is directly appended to the end of the bad trajectory. While this approach is simple, it omits key distinction of agent task reflection: timely revision is necessary for most agent tasks instead of waiting until the end of rollout to revise errors. If corrections are applied only at the end of the trajectory, the agent cannot self-reflect and learn to detect and reflect errors as they occur. Moreover, the direct-revision trajectory fails to address catastrophic errors that may occur early in the trajectory. Transition Point Determination with Actor Model To address the above problem, we propose model-guided revision trajectory construction approach inspired by human cognitive processes. In this approach, the language agent evaluates each action within its self-generated bad trajectories to identify errors based on the actor models current capability, which is in line with the reflection on its actions. Specifically, when the agent detects an incorrect action ùëéùë°*, the transition point is set to ùë° = ùë°*, and the bad trajectory is truncated at this point. The agent then constructs revision trajectory by combining the corrected portion of the bad trajectory with the corresponding segment of the good trajectory. This approach is fundamentally interactive and leverages the agents self-awareness to actively refine its behavior in real-time. Figure 3 in our experiments demonstrates the superiority of such transition point determination over the direct-revision approach. The prompt used for the model-guided transition point determination is provided in Appendix A. It not only ensures that the revisions are grounded in the agents own learned dynamics but also lays the foundation for scalable self-improvement from weaker to stronger behaviors (discusses in 3.2). 3.2. Phase II: Iterative Self-Training with Revision Trajectories In this phase, the goal is to train language agents using self-generated revision trajectories collected through MCTS. The agent uses its own rollouts to identify and correct errors dynamically through self-reflection, progressively improving its policy based on its own experiences during these rollouts. Although training on revision trajectories enables the agent to develop self-reflection capabilities, relying solely on these trajectories may initially hinder the agents ability to identify the correct trajectory. common solution in previous works is to train the model using both optimal and revision trajectories. However, this still suffers from the cold-start problem, where the agent begins with little to no knowledge of the environment and must depend on trial and error. This results in limited number of optimal trajectories discovered early in training. To address this issue, we propose mixing revision trajectories with good trajectories during training. Over the course of self-training, we gradually increase the value of ùõº in Equation 4, which encourages the good trajectories to progressively converge toward optimal trajectories. This enables the agent to improve both its error reflection capabilities and its ability to identify the correct trajectory over time. Additionally, following the strategy from AgentTuning (Zeng et al., 2024a), we combine agent 2The ten revision thoughts are listed in Appendix A. 6 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Table 1: Statistics of three agent interactive environments. Due to different selection thresholds, MCTS can collect different numbers of revision data and good data for the training phase. Dataset Simulation # MCTS w/ Iter 1 MCTS w/ Iter 2 MCTS w/ Iter 3 Revision Good Revision Good Revision Good Test Size WebShop SciWorld TextCraft 300 200 200 5500 8000 6000 600 2500 2000 8500 4200 7000 1800 1800 3200 9000 5400 2000 1200 4000 200 200 100 trajectory datasets with general datasets, such as ShareGPT3, to improve generalization capabilities. This approach has been shown to outperform training exclusively on agent tasks and enhances the agents ability to generalize across broader range of environments. We adopt this strategy in our framework. Specifically, for the base model ùúãùúÉ, the objective is to find the optimal policy ùúãùúÉ(ùúè ùë¢) that minimizes the loss function ùêø(ùúÉ): ùêø(ùúÉ) = ùúÇ {E(ùúè ùëî,ùë¢)ùíüGood [log ùúãùúÉ(ùúè ùëî ùë¢)] + E(ùúè ùëü,ùë¢)ùíüRevision + (1 ùúÇ) E(ùë•,ùë¶)ùíügeneral [log ùúãùúÉ(ùë¶ ùë•)] , [ log ùúãùúÉ(rs, ùúè ùëî (ùë°>ùë°) ùë¢, ùúè ùëè (ùë°ùë°)) ] } (6) ùë°=1 log ùúãùúÉ(ùëéùë° ùúèùë°1, ùë¢). where (ùë•, ùë¶) is the input-output pair from the general dataset, ùúÇ is the mixture ratio between ùíüRevision + ùíüGood and ùíügeneral and log ùúãùúÉ(ùúè ùë¢) = ùëá To further explore the stability of our approach, we iteratively refine both its error correction capabilities and its dataset construction process. Specifically, at each iteration, we re-collect model-guided revision trajectories based on the current actor model and perform SFT to optimize Equation 6. As the agent trains over multiple iterations, it builds enhanced self-reflection capabilities and progressively transforms from weaker, error-prone behaviors to stronger, more efficient performance in interactive environments. This process supports the agents ability to dynamically recover from errors and learn from its experiences, facilitating its reflection on actions and continuous performance enhancement. 4. Experiment In this section, we conduct extensive experiments on three interactive environments to demonstrate the effectiveness of Agent-R. 4.1. Interactive and Agentic Environments Following previous work (Xi et al., 2024a; Prasad et al., 2024), we conduct experiments on three types of representative interactive environments: WebShop (Yao et al., 2022), which is an interactive web environment for online shopping. It contains 12k instructions and offers over one million real products from amazon.com. Agents can click buttons on the webpage or perform searches using the search engine. ScienceWorld (Wang et al., 2022), which is scientific, text-based environment designed to evaluate agents scientific reasoning abilities. It includes 30 types of scientific tasks at the standard elementary science curriculum level. TextCraft (Prasad et al., 2024), which is text-based environment for crafting Minecraft items. It constructs crafting tree based on Minecrafts recipes. Each task provides target item and 3https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered 7 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training list of crafting commands generated by the tree. Agents receive reward of 1 when they successfully craft the target item. 4.2. Experiment Setting We conduct our experiments on AgentGym (Xi et al., 2024a), an interactive platform that includes diverse agent environments. Data Split The statistical details of these three interactive environments are shown in Table 1. To create the revision trajectories, we randomly sample 300 simulations from WebShop, 200 from SciWorld, and 200 from TextCraft to conduct MCTS. We set the distinguishable gap ùõΩ = 0.2 between bad and good trajectories in Equation 4. As mentioned in 3.1, considering the cold-start problem for agents in these challenging environments, the initial threshold for good trajectories is set relatively low and gradually increases in later iterations. Specifically, we conduct Agent-R for three iterations. We set ùõº = 0.5 for iteration 1, ùõº = 0.7 for iteration 2, and ùõº = 1.0 for iteration 3. By iteration 3, the good trajectories converge toward optimal trajectories. Following AgentTuning (Zeng et al., 2024a), we use ShareGPT as the general dataset ùê∑general and set ùúÇ = 0.2 in Equation 6. Following Xi et al. (2024a), we select 200 tasks for the WebShop test set, 200 for the SciWorld test set, and 100 for the TextCraft test set. MCTS Settings For all simulations in the three interactive environments, during the trajectory self-generation stage, we sample ùëò = 8 rollouts for each Monte Carlo estimation. The depth ùëë is set to 20 for all tasks. In the expansion phase, the temperature of the LLMs is set to 1. At each depth, the LLMs generate 4 candidate actions as new child nodes. In the selection phase, to balance exploration and exploitation, we set ùëêuct = 0.25 for UCT in Equation 5. Training Settings We perform Agent-R for three iterations and present the results of the third iteration in Table 2. We set the epoch number to 3 for the first iteration and 1 for subsequent iterations to avoid over-fitting. Detailed training settings are provided in Appendix C.1. Our main backbone model is the instruct version of LLama-3.1-8B, i.e., Llama-3.1-8B-Instruction (Dubey et al., 2024). Baselines Following Xi et al. (2024a), we select closed-source models, i.e., GPT-3.5-Turbo (OpenAI, 2022), GPT-4-Turbo (OpenAI, 2023), GPT-4o (gpt-4o-2024-08-06) (OpenAI, 2023), Claude 3 (Anthropic, 2024), and DeepSeek-Chat (Liu et al., 2024a). We also select open-source models like Llama2-Chat (Touvron et al., 2023) and agents trained on expert trajectories, i.e., AgentLM (Zeng et al., 2024a) and Agent-Flan (Chen et al., 2024d). Additionally, we compare ETO (Song et al., 2024b), which first applies SFT to base agent using behavioral cloning and then uses DPO (Rafailov et al., 2023) to fine-tune the model with contrastive pairs of good and bad trajectories. We also report the results of Llama-3.1-8B-Instruct trained on direct-revision trajectories mixed with good trajectories over three iterations for comparison (w/ Direct-Revision). Evaluation Metrics Following Xi et al. (2024a), for SciWorld and WebShop, we use the average final reward as the evaluation metric. For TextCraft, we use the success rate as the evaluation metric. In all three environments, the maximum number of rounds is set to 100. Detailed evaluation settings are provided in Appendix C.1. 8 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Method WebShop SciWorld TextCraft Average DeepSeek-Chat (Liu et al., 2024a) Claude-3-Haiku (Anthropic, 2024) Claude-3-Sonnet (Anthropic, 2024) GPT-3.5-Turbo (OpenAI, 2022) GPT-4-Turbo (OpenAI, 2023) GPT-4o (OpenAI, 2023) Close-sourced Models 11.00 5.50 1.50 12.50 15.50 25.48 Open-sourced Models Llama2-Chat-13B (Touvron et al., 2023) AgentLM-7B (Zeng et al., 2024a) AgentLM-13B (Zeng et al., 2024a) Agent-FLAN (Chen et al., 2024d) 1.00 36.50 39.50 40.35 Llama-3.1-8B-Instruct (Dubey et al., 2024) w/ ETO (Song et al., 2024b) w/ Direct-Revision w/ Agent-R 19.65 52.80 54.44 63.91 16.80 0.83 2.78 7.64 14.38 46.91 0.83 2.75 10.68 28.64 14.36 67.55 66. 23.00 0.00 38.00 47.00 77.00 64.00 0.00 0.00 4.00 16.00 26.00 75.00 66.00 16.93 2.11 14.09 22.38 35.63 45.46 0.61 13.08 18.06 28.33 20.00 65.12 62. 70.23 78.00 70.71 Table 2: Results of three interactive environments. We train Llama-3.1-8B on revision trajectories collected from Agent-R for three iterations and compare its performance with various models. 4.3. Main Result The overall results for the three interactive environments are shown in Table 2. We find that: 1. Agent-R significantly improves the performance of language agents in interactive environments. This approach outperforms both advanced closed-source models (e.g., GPT-4o) and agents trained on expert trajectories (e.g., AgentLM and Agent-Flan). The results highlight the critical role of revising erroneous trajectories during the learning process rather than solely relying on expert data. 2. Trajectories constructed by Agent-R, through dynamic self-reflection and early error detection, lead to faster recovery and more stable learning. This early intervention prevents the propagation of errors, resulting in higher-quality models and better performance than direct-revision methods. 3. Although adopting contrastive learning methods (e.g., ETO) improves language agent performance in interactive environments, these methods do not equip agents with self-reflection capabilities. This limitation is analyzed in 4.4. Additional case study analyses for Agent-R are provided in Appendix C.3, illustrating specific instances of error recovery and trajectory correction. 4.4. Findings with Analysis Finding 1: Training with trajectories from Agent-R can outperform using optimal trajectories. To further investigate the significance of revision trajectories constructed by Agent-R, we compare our approach with Llama-3.1-8B-Instruct trained on direct-revision trajectories combined with good trajectories (w/ Direct-Revision Trajectory) for each iteration. Additionally, we include two other ablated variants: w/ Optimal Trajectory: Llama-3.1-8B-Instruct trained only on self-generated optimal trajectories (ùëü(ùúè ) = 1). w/ Optimal + Good Trajectory: Llama-3.1-8B-Instruct trained on optimal trajectories combined with the good trajectories from iteration 1 in Agent-R. The results in Figure 3 show that: Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Figure 3: Results of different training trajectories under different iterations on three interactive environments. WebShop SciWorld TextCraft Average Method GPT-4o AgentLM-7B AgentLM-13B Agent-FLAN w/ Optimal w/ Optimal+ w/ ETO 24.10 20.96 23.20 19.37 31.38 25.03 35.78 35. 0.00 0.22 0.00 Llama-3.1-8B 1.15 0.00 3.47 8.22 0.00 0.00 0.00 0.00 0.00 0. # Iter1 # Iter2 # Iter3 # Iter1 # Iter2 # Iter3 Llama-3.1-8B w/ Direct-Revision Trajectories Llama-3.1-8B w/ Trajectories from Agent-R 40.26 40.35 42.02 46.51 47.51 48. 19.09 27.45 35.27 40.82 45.22 48.79 11.35 22.97 29.73 32.43 35.14 43.24 22.63 6.99 7.81 6. 10.84 8.34 13.08 23.57 30.26 35.67 39.92 42.62 46.75 Table 3: The revision results of different methods. w/ Optimal+ means the golden trajectories for training Llama-3.1-8B are generated from GPT-4o. 1. Iterative SFT with trajectories from Agent-R gradually enhances the models capabilities. It is worth noting that this process relies entirely on self-play to collect and construct trajectories, offering insights and potential future directions for the autonomous evolution of language agents. 2. Mixing revision trajectories with optimal trajectories in the training process significantly improves performance. This highlights the importance of iterative reflection and error recovery in dynamic environments. 3. Adding only good trajectories to optimal ones decreases performance. This may be because good trajectories introduce noise, as it is not always completely correct (ùëü(ùúè ) 1 ). Although revision trajectory also contains noise, it provides information on how to recover from errors. This additional information offsets the noise and improves performance. Finding 2: Agent-R can effectively provide language agents with self-reflection capabilities. To further explore the self-reflection capabilities of language agents trained with Agent-R, we first collect 10 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Figure 4: Average count of repeated action lengths for different training trajectories and different iterations in three interactive environments. all failure trajectories (ùëü(ùúè ) = 0) from the test set results of Llama-3.1-8B-Instruct. Then, given these failure trajectories, we randomly select time step ùë°, truncate the trajectories at that time, and then ask language agents to continue generating actions after ùë° and evaluate whether they can correct these failures to achieve better final rewards. In this revision setting, the maximum number of rounds for all three environments is set to 50. We compare Agent-R with self-generated optimal trajectories (ùëü(ùúè ) = 1, w/ Optimal), GPT-4o-generated optimal trajectories (w/ Optimal+), contrastive pairs of good and bad trajectories (w/ ETO) and direct-revision mixed with good trajectories. The results in Table 3 indicate that: 1. Training with Agent-R revision trajectories outperforms direct-revision methods, emphasizing the importance of incorporating early reflection during the trajectory rather than waiting until the end. This on-the-fly correction mechanism is crucial for timely error recovery. 2. Although training on expert trajectories using SFT or with contrastive pairs of good and bad trajectories using DPO can significantly enhance agent performance in interactive environments, it does not provide satisfactory self-reflection capabilities. If an agent gets stuck in loop or executes incorrect actions, it cannot identify and correct the error. 3. While GPT-4o performs worse than AgentLM and AgentEvol on the test set, it shows better selfreflection capabilities, demonstrating its strong general abilities. Finding 3: Training with revision trajectories helps agents more easily identify and correct erroneous actions. As mentioned in 3.1, to construct revision trajectories, we ask language agents to identify the first erroneous action in the bad trajectories and then concatenate the correct trajectories after this point. To determine if agents trained with iterative SFT can more effectively and quickly detect the first error, we measure the revision length. This is the number of actions from the start of the bad trajectory to the first identified error. We report the average revision length for different iterations across the three interactive environments. Figure 5: Average revision length of different iterations on three interactive environments. 11 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training The results in Figure 5 indicate that Agent-R significantly improves the agents ability to recognize when an error occurs within trajectory. Additionally, the shorter average revision length indicates that training with revised trajectories from Agent-R, where error correction is integrated dynamically and earlier in the trajectory, enables the model to more efficiently adjust its path. Moreover, this improvement reflects the enhanced reflection ability of the actor model, which could better serve as critical component to assist other models, direction worth exploring in future work. Finding 4: Training with revision trajectories helps agents avoid getting stuck in loops. To understand why revision trajectories are more effective than self-generated optimal ones, we measure the length of repeat action sequences in test set trajectories under different training settings. We report the average count, which reflects how often agents repeat the same action sequence when stuck in loop. For example, sequence length of 2 with an average count of 5 means the agent repeats the same sequence of two actions 5 times on average. The results in Figure 4 show that although agents can obtain optimal trajectories through MCTS, these trajectories may contain repeated or noisy middle actions, causing the agent to get stuck in dead loops, which negatively impacts performance. Compared to training with optimal trajectories, training with trajectories from Agent-R significantly reduces the occurrence of dead loops. This indicates that agents trained with revision trajectories are more likely to explore new actions and avoid getting trapped in local loops. These findings emphasize the importance of revision trajectories in enhancing exploration and overall performance. Finding 5: Multi-task training is more effective strategy for Agent-R. We compare our multi-task training approach with single-task training, where good and revised trajectories are collected separately for each task using Agent-R. The results in Figure 6 show that multitask training is more effective way to train language agents in interactive environments. Additionally, the trajectories collected by Agent-R are better suited for multi-task training, further enhancing performance. The detailed results of three environments with further analysis are shown in Appendix C.2. 5. Related Work Figure 6: Comparison of different training methods on three interactive environments. Agent Learning in Interactive Environments Previous approaches to agent learning in interactive environments can be categorized into three strategies: 1) Prompt-based Strategy, which uses humanwritten prompts to guide LLMs in summarizing experiences (e.g., constructing and refining sets of transferable skills (Nottingham et al., 2024; Sarch et al., 2024) or helpful hints (Chen et al., 2024a; Majumder et al., 2023; Zhao et al., 2024; Yuan et al., 2024)) during exploration. These summarized experiences are incorporated into the models memory to enhance its knowledge and improve performance. 2) Inference-time Search Strategy, which employs various search algorithms, such as Tree-of-Thought (Yao et al., 2023a; Light et al., 2024; Koh et al., 2024; Hao et al., 2023) and Monte Carlo Tree Search (MCTS) (Zhao et al., 2023), to identify optimal trajectories during inference. This strategy leverages prior knowledge in LLMs to enable more efficient and effective 12 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training search processes (Zhao et al., 2023). 3) Training-based Strategy, which employs Supervised FineTuning (SFT) (Xi et al., 2024a; Qiao et al., 2024) or Direct Preference Optimization (DPO) (Zhai et al., 2024; Putta et al., 2024; Song et al., 2024b; Xiong et al., 2024) to train LLMs. The training data can be collected from expert models (Xi et al., 2024a; Song et al., 2024a) or generated through exploration using MCTS (Zhai et al., 2024; Putta et al., 2024; Patel et al., 2024). Self-Correction for Large Language Models Self-correction is highly desirable capability of large language models (LLMs), but current LLMs have consistently been found to perform it ineffectively (Xi et al., 2023; Kamoi et al., 2024; Pan et al., 2024). Some studies on self-correcting LLMs depend on prompt-engineering (Madaan et al., 2023; Shinn et al., 2023), but often fail to achieve significant intrinsic self-correction, and can even degrade performance (Huang et al., 2023; Tyen et al., 2024; Xie et al., 2024; Zhang et al., 2024a). To address these issues, several approaches propose collecting revision data from human annotators (Scheurer et al., 2023), expert models (Du et al., 2024), or self-generated samples (Welleck et al., 2023; Kumar et al., 2024). However, these methods primarily focus on tasks such as code repair (Kim et al., 2024; Chen et al., 2024b), tool use (Olausson et al., 2023), and math (Kumar et al., 2024; Havrilla et al., 2024). These tasks provide explicit error signals, e.g., code error messages (Chen et al., 2024b) or tool-use parameters (Gou et al., 2024), or rely on guidance from critical models (Wang et al., 2024). Additionally, existing methods are limited to single-turn scenarios and do not account for longer trajectories. In this paper, we explore self-correction within interactive environments where language agents lack explicit error signals and rely on observations. 6. Conclusion In this paper, we address the critical challenge of error correction for language agents operating in interactive environments, limitation that remains unresolved by existing approaches relying on expert trajectories. To tackle this issue, we introduce Agent-R, an iterative self-training framework that empowers agents to dynamically reflect and correct their actions in interactive and agentic environments. By leveraging Monte Carlo Tree Search (MCTS) and model-guided reflection construction, Agent-R allows for timely revision of erroneous trajectories, significantly improving agents ability to recover from errors in real time. Experiments across three interactive environments show that Agent-R not only enhances error correction capabilities but also prevents looping and suboptimal behaviors, leading to superior performance compared to baseline methods. Furthermore, integrating self-generated revision trajectories into training improves the agents overall performance and supports the development of more intelligent, self-reflective agents. These findings open exciting avenues for future work, particularly in refining the role of self-correction as critical function in agent-based systems."
        },
        {
            "title": "References",
            "content": "Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143, 2012. Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, and Xiaofei He. Automanual: Gen13 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training erating instruction manuals by llm agents via interactive environmental learning. arXiv preprint arXiv:2405.16247, 2024a. Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=KuPixIqPiq. Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. Mindsearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183, 2024c. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 93549366, Bangkok, Thailand, August 2024d. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL https: //aclanthology.org/2024.findings-acl.557. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and In Thirty-seventh Conference on Yu Su. Mind2web: Towards generalist agent for the web. Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https:// openreview.net/forum?id=kiYqbO3wqw. Chengyu Du, Jinyi Han, Yizhou Ying, Aili Chen, Qianyu He, Haokun Zhao, Sirui Xia, Haoran Guo, Jiaqing Liang, Zulong Chen, et al. Think thrice before you act: Progressive thought refinement in large language models. arXiv preprint arXiv:2410.13413, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=Sx038qxjek. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.507. URL https://aclanthology.org/2023.emnlp-main. 507. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. Glore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can LLMs actually correct their own mistakes? critical survey of self-correction of LLMs. Transactions of the Association Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training for Computational Linguistics, 12:14171440, 2024. doi: 10.1162/tacl_a_00713. URL https: //aclanthology.org/2024.tacl-1.78. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36, 2024. Levente Kocsis and Csaba Szepesv√°ri. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer, 2006. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, and Ziniu Hu. Strategist: Learning strategic skills by llms via bi-level tree search. arXiv preprint arXiv:2408.10635, 2024. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809, 2024. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024a. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=zAdUB0aCTQ. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S37hOerQLB. Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. Clin: continually learning language agent for rapid task adaptation and generalization. arXiv preprint arXiv:2310.10134, 2023. Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, and Roy Fox. Skill set optimization: Reinforcing language model behavior via transferable skills. arXiv, 2024. URL https://arxiv.org/abs/2402.03244. Theo Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Is self-repair silver bullet for code generation? In The Twelfth International Conference on Learning Representations, 2023. OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. 15 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Transactions of the Association for Computational Linguistics, 12:484506, 2024. doi: 10.1162/tacl_a_00660. URL https://aclanthology.org/2024.tacl-1.27. Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris CallisonBurch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309, 2024. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. ADaPT: As-needed decomposition and planning with language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 42264252, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.264. URL https://aclanthology.org/2024.findings-naacl.264. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=j6kJSS9O6I. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id=HPuSIXJaa9. Gabriel Herbert Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, and Katerina Fragkiadaki. VLM agents generate their own memories: Distilling experience into embodied programs of thought. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=5G7MRfPngt. J√©r√©my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023. Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, and Fuli Feng. Direct multi-turn preference optimization for language agents. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 23122324, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.138. URL https://aclanthology.org/2024.emnlp-main. 138. Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2(5):9, 2023. Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. AgentBank: Towards generalized LLM agents via fine-tuning on 50000+ interaction trajectories. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 21242141, Miami, Florida, USA, November 16 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.116. URL https://aclanthology.org/2024.findings-emnlp.116. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Explorationbased trajectory optimization of LLM agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75847600, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.409. URL https://aclanthology. org/2024.acl-long.409. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. LLMs cannot find reasoning errors, but can correct them given the error location. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1389413908, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10. 18653/v1/2024.findings-acl.826. URL https://aclanthology.org/2024.findings-acl. 826. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 510. URL https://aclanthology.org/2024.acl-long.510. Ruoyao Wang, Peter Jansen, Marc-Alexandre C√¥t√©, and Prithviraj Ammanabrolu. ScienceWorld: Is your agent smarter than 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1127911298, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.775. URL https://aclanthology.org/2022. emnlp-main.775. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=hH36JeQZDaO. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864, 2023. 17 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024a. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, et al. Enhancing llm reasoning via critique models with test-time and training-time supervision. arXiv preprint arXiv:2411.16579, 2024b. Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, and Yanghua Xiao. Revealing the barriers of language agents in planning. arXiv preprint arXiv:2410.12409, 2024. Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Watch every step! LLM agent learning via iterative step-level process refinement. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15561572, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 93. URL https://aclanthology.org/2024.emnlp-main.93. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=R9KnuFlvnU. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https:// openreview.net/forum?id=5Xc1ecxO1h. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id= WE_vluYUL-X. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoagent: Towards automatic multi-agent generation via evolutionary algorithms. arXiv preprint arXiv:2406.14228, 2024. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. AgentTuning: Enabling generalized agent abilities for LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 30533077, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-acl.181. URL https://aclanthology.org/2024.findings-acl.181. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, et al. Mr-ben: meta-reasoning benchmark for evaluating system-2 thinking in llms. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. Yuanzhao Zhai, Tingkai Yang, Kele Xu, Feng Dawei, Cheng Yang, Bo Ding, and Huaimin Wang. Enhancing decision-making for llm agents via step-level q-value models. arXiv preprint arXiv:2409.09345, 2024. 18 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, and Minlie Huang. Understanding the dark side of llms intrinsic self-correction. arXiv preprint arXiv:2412.14959, 2024a. Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, and Jiangjie Chen. TimeArena: Shaping efficient multitasking language agents in time-aware simulation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38943916, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.215. URL https://aclanthology.org/2024.acl-long.215. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1963219642, 2024. Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. In RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. URL https://openreview.net/forum?id=tED747HURfX. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. 19 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training A. Prompt Template of Revision Trajectory Table 4 shows the prompt of Agent-R to determine the transition point. Ten different revision thoughts are shown in Table 5. B. Trajectory Definition We define four types of trajectories: initial trajectories, bad trajectories, good trajectories, and revision trajectories. Below, we provide formal definitions and mathematical formulations for each type of trajectory: Initial Trajectory (ùúè ùëñ): An initial trajectory represents the initial sequence of actions and observations given the user-provided instruction ùë¢. Formally, it is expressed as: where ùëéùëñ ùëò and ùëúùëñ ùëò ùúè ùëñ = (ùëéùëñ 1, ùëúùëñ 1, . . . , ùëéùëñ ùë°, ùëúùëñ ùë°), (7) denote the ùëò-th action and the corresponding observation in the trajectory. Bad Trajectory (ùúè ùëè): bad trajectory extends the initial trajectory ùúè ùëñ with sequence of suboptimal actions and observations. It is represented as: ùúè ùëè = (ùúè ùëñ, ùëéùëè ùë°+1, ùëúùëè ùë°+1, . . . , ùëéùëè ùëáùëè , ùëúùëè ùëáùëè ), (8) where ùëáùëè denotes the terminal step of the bad trajectory. Good Trajectory (ùúè ùëî): good trajectory builds upon the initial trajectory ùúè ùëñ but involves optimal or high-reward actions and observations. It is given by: ùúè ùëî = (ùúè ùëñ, ùëéùëî ùë°+1, ùëúùëî ùë°+1, . . . , ùëéùëî ùëáùëî , ùëúùëî ùëáùëî ), (9) where ùëáùëî denotes the terminal step of the good trajectory. Revision Trajectory (ùúè ùëü): revision trajectory synthesizes components from both bad trajectory ùúè ùëè and good trajectory ùúè ùëî. Specifically, it starts with the bad trajectory up to step ùë° and transitions to the good trajectory thereafter. Formally, it is defined as: ùúè ùëü = (ùúè ùëñ, ùëéùëè ùë°+1, ùëúùëè ùë°+1, . . . , ùëéùëè ùë°, ùëúùëè ùë°, rs, ùëéùëî ùë°+1, ùëúùëî ùë°+1, . . . , ùëéùëî ùëáùëî , ùëúùëî ùëáùëî ), (10) where ùë° ùëáùëè denotes the transition point between the bad and good trajectories. rs is the revision signal, which does not involve any environment-altering action but serves as marker for the revision process. In our settings, rs is single-turn conversation: Assistant: [revision thought] Human: OK.\". C. Experiment Details C.1. Training and Evaluation Details All experiments are conducted using eight A100-80GB GPUs. Services for different environments are deployed on separate ports of the same server. The training process employed an iterative SFT approach. For the first iteration, the number of epochs is set to 3 to ensure sufficient learning. In subsequent iterations, the number of epochs is reduced to 1 to mitigate overfitting. The learning rate for iterative SFT is 2ùëí5, with 3% warm-up and 20 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Table 4: Prompt templates of determining transition point in Agent-R. You are good verifier of interactive environments. You will be given history log that memorizes an agent interacting with the environment to solve task. The format of the log is ### Action: Action Observation: Observation ### Log: Task Description: {task description} {history log} Current Action: {node action} Current Observation: {node observation} You need to verify whether the current action is good or bad or uncertain. - good action is one that is greatly helpful to solve the task. - bad action is one that is greatly harmful to solve the task. - An uncertain action is one that is neither good nor bad. You cannot judge based on the current information. You must give reasons first and then give the response with the format: Judgment: <Good or Bad or Uncertain> Table 5: Ten revision thoughts to construct revision trajectories. Revision Thoughts Thought 1: realize my approach was flawed. need to revise it. Thought 2: took the wrong actions. need to identify the right path. Thought 3: My actions were incorrect. must adjust my strategy. Thought 4: see an error in my actions. need to fix it. Thought 5: My judgment was incorrect. need to rethink it. Thought 6: overlooked something important. need to address it. Thought 7: recognize my mistake. Lets find better solution. Thought 8: recognize my failure. need to learn and move forward. Thought 9: My decision was wrong. should reevaluate. Thought 10: made an error. must determine how to correct it. cosine scheduler. The AdamW optimizer is used with weight decay of 0, and maximum gradient clipping norm of 1 is applied to prevent gradient explosion. The batch size is set to 1 per device, with sequence length of 8,196 and an accumulative count of 16, adjusted based on sequence parallelism. Following the settings in AgentGym (Xi et al., 2024a), the evaluation temperature is set to 0. For models not fine-tuned on expert and revision trajectories, few-shot evaluation approach is used. For models fine-tuned on expert and revision trajectories, zero-shot evaluation approach is employed. C.2. Multi-task training v.s. Single-task Training We compare our multi-task training approach to single-task training, where good and revised trajectories are collected separately for each task using Agent-R. The results in Table 6 demonstrate that multi-task training outperforms single-task training when using trajectories generated by Agent-R. Multi-task training leverages cross-task learning to enhance performance across datasets, especially in later iterations, highlighting its scalability and effectiveness with additional training cycles. Trajectories from Agent-R appear particularly beneficial for multi-task training, potentially providing richer Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Table 6: Detailed Results of different training methods on three interactive environments. Dataset Iteration Method WebShop SciWorld TextCraft Average Direct-Revision + Good Trajectory Trajectory from Agent-R 1 2 3 1 3 Single Multi Single Multi Single Multi Single Multi Single Multi Single Multi 48.06 42.66 55.56 46.93 58.82 54.44 49.80 45.84 56.34 52. 60.66 63.91 64.52 63.10 64.52 64.23 65.51 66.65 64.02 66.71 65.62 68. 67.22 70.23 57.00 48.00 63.00 64.00 71.00 66.00 60.00 64.00 71.00 76. 76.00 78.00 56.53 51.25 61.03 58.39 65.11 62.36 57.94 58.85 64.32 65. 67.96 70.71 and more diverse data for generalization. However, single-task training may remain advantageous for simpler or well-separated tasks, especially in earlier iterations. C.3. Case Study Error Correction in Trajectory Generation As mentioned in 4, training with trajectories from Agent-R significantly enhances the performance of language agents in interactive environments. Table 7 presents examples of historical records before and after the adaptive transition point. These examples demonstrate that language agent trained on our trajectories can effectively recover from incorrect trajectories and identify the correct action. Adaptive Transition Point Evaluation As shown in Figure 5, we find that iterative SFT allows language agents to more easily recognize when they are on bad trajectory with erroneous actions. We further analyze the results of adaptive transition point evaluation to examine errors in trajectory actions. Examples in Table 8 categorize these erroneous actions into three types: Invalid Actions: Actions that are not logically permissible or executable in the current environment or context, often violating the task or system constraints. Mismatch Observation: Actions that contradict the observed evidence or fail to align with the current state of the environment or task requirements. Irrelevant Actions: Actions that do not contribute to achieving the task goal are often unrelated or misaligned with the tasks specified objectives. The examples demonstrate that language agents trained with trajectories from Agent-R are better equipped to identify and avoid erroneous actions. These findings indicate that Agent-R enhances agents ability to differentiate between permissible, aligned, and task-relevant actions, resulting in improved decision-making and trajectory optimization in complex environments. 22 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Table 7: Example of error correction from the trajectory generated by Llama-3.1-8B, trained on trajectories from Agent-R during the third iteration. The search results in WebShop are omitted due to their length. WebShop Example 1: Assistant: Thought: think should search for light blue shorts in specific size and price range to get more accurate results. Action: search[light blue 36w 30l price:40] Human: [Search Result] Assistant: Thought: went in the wrong direction with my search. need to reset and try again. Human: ok. Assistant: Thought: need to back to search. Action: click[Back to Search] Human: [Search Result] Example 2: Assistant: Thought: think the second option is bit off-topic, but its good price, so Ill consider it. Action: click[B08226NDZW] Human: [Search Result] Assistant: Thought: recognize my mistake. Lets find better solution. Action: wait Human: ok. Assistant: Thought: think the search query is not specific enough, so Ill try to refine it by adding more keywords. Action: search[machine wash mens dress shirts polyester heathered cotton classic fit men 3t price:50.00] Human: [Search Result] Sciworld Example 1: Assistant: Thought: should open door to kitchen. Action: open door to kitchen Human: The door is now open. Assistant: Thought: took the wrong steps. need to identify the right path. Action: wait Human: ok. Assistant: Thought: should go to kitchen. Action: go to kitchen Human: You move to the kitchen. Example 2: Assistant: Thought: think should try to determine the temperature of the unknown substance B. Action: use thermometer on unknown substance Human: the thermometer measures temperature of 19 degrees celsius Assistant: Thought: misunderstood the situation. Time to reassess. Action: wait Human: ok. Assistant: Thought: think should check the unknown substance again to make sure the temperature is accurate. Action: use thermometer on unknown substance Human: the thermometer measures temperature of 16 degrees celsius TextCraft Example 1: Assistant: Thought: should check if can get blackstone directly from the environment or the inventory. Action: inventory Human: Inventory: You are not carrying anything. Assistant: Thought: recognize my mistake. Lets find better solution. Action: wait Human: ok. Assistant: Thought: should go get the blackstone from the environment. Action: get 3 blackstone Human: Got 3 blackstone Example 2: Assistant: Thought: can use the quartz slab to craft something else that might be useful. Action: craft quartz wall using 4 quartz slab Human: Could not find enough items to craft minecraft:quartz_wall Assistant: Thought: realize chose poorly. must change direction. Action: wait Human: ok. Assistant: Thought: think should try to get more quartz to craft the quartz wall. Action: get 2 quartz slab Human: Got 2 quartz slab 23 Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training Table 8: Example results from the evaluation of adaptive transition points. These judgments are all generated by Llama-3.1-8B, trained on trajectories from Agent-R during the third iteration. WebShop Judgment (Invalid Actions): The action is bad because it does not match the original search criteria. Therefore, Judgment: Bad Judgment (Mismatch Observation): However, the product does not match the description of machine wash mens dress shirt with polyester heathers, heathers cotton, cotton heather, needle sleeve, classic fit, and color heather grey, as specified in the task description. Therefore, Judgment: Bad Judgment (Irrelevant Actions): The action is bad because it is not relevant to the task of finding mens shirts with specific characteristics. Therefore, Judgment: Bad Sciworld Judgment (Invalid Actions): There is no orange mentioned in the environment, so this action is invalid. Its not helpful action, but rather an invalid input. Therefore, Judgment: Bad Judgment (Mismatch Observation): The current task is to determine if the aluminum foil is electrically conductive. The agent has already connected the aluminum foil to the battery and the green light bulb, and observed that the light bulb turned on. This suggests that the aluminum foil is electrically conductive. Moving the aluminum foil to the yellow box would mean that the agent is giving up on the possibility of the aluminum foil being conductive, which is not in line with the current observation. Therefore, Judgment: Bad Judgment (Irrelevant Actions): The agent is not using the art studios resources effectively, as the cup containing red paint is not related to the task of creating green paint. Therefore, Judgment: Bad TextCraft Judgment (Invalid Actions): The agent is stuck in an infinite loop of trying to execute an invalid action, which is not helpful in solving the task. Therefore, Judgment: Bad Judgment (Mismatch Observation): The action is not good because it is not possible to craft dried kelp block with 9 dried kelp. The observation suggests that the agent lacks the necessary items to craft the block. This action is actually harmful to the task of crafting the block. Therefore, Judgment: Bad Judgment (Irrelevant Actions): The agent is trying to craft diamond boots, which requires 4 diamonds. The agent has already obtained 4 diamonds, which is sufficient to craft the boots. However, the agent is trying to craft diamond block instead, which requires 9 diamonds. This action is not helpful in achieving the goal of crafting diamond boots, as it will waste the 4 diamonds the agent already has. Therefore, Judgment: Bad"
        }
    ],
    "affiliations": [
        "ByteDance",
        "Fudan University"
    ]
}