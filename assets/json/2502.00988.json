{
    "paper_title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback",
    "authors": [
        "Kanika Goswami",
        "Puneet Mathur",
        "Ryan Rossi",
        "Franck Dernoncourt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors."
        },
        {
            "title": "Start",
            "content": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback Kanika Goswami IGDTUW, Delhi India"
        },
        {
            "title": "Franck Dernoncourt\nAdobe Research\nUSA",
            "content": "5 2 0 2 3 ] . [ 1 8 8 9 0 0 . 2 0 5 2 : r Abstract Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including: (1) Query Planning Agent that breaks down complex user requests into executable steps; (2) Code Generation Agent that converts pseudocode into executable Python code; and three retrieval feedback agents(3) Numeric Feedback Agent, (4) Lexical Feedback Agent, and (5) Visual Feedback Agentthat leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving 4-6% improvement on the MatPlotBench dataset, leading to enhanced user trust in LLMgenerated visualizations and improved novice productivity due to reduction in debugging time needed for plot errors. CCS Concepts Information systems Multimedia content creation. Keywords Agentic Generation, Multimodal Retrieval Feedback, LLM Agents ACM Reference Format: Kanika Goswami, Puneet Mathur, Ryan Rossi, and Franck Dernoncourt. 2018. PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym XX). ACM, New York, NY, USA, 5 pages. https://doi.org/XXXXXXX. XXXXXXX Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1\nScientific data visualization is essential for transforming raw data\ninto visual representations, enabling the communication of complex\ninformation, the analysis of patterns in large datasets, and the for-\nmulation of evidence-based narratives. Professional data analysts\nand researchers have access to several sophisticated toolkits (e.g.\nMatplotlib, Seaborn and Plotly) to aid in the creation of diverse visu-\nalizations. However, novice users often face significant challenges\nwhen attempting to produce informative and accurate visualizations\nfrom raw data. These challenges stem from the difficulty of interpret-\ning user requirements, choosing the appropriate visualization tool\nfrom the vast array of available options, and mastering the technical\nintricacies of graph plotting. Consequently, there is a growing need\nfor more intuitive automated systems that support users with limited\ntechnical expertise to generate high-quality infographics.",
            "content": "Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in assisting wide range of complex tasks, such as software development[14], web navigation[8], and document editing[10], due to their proficiency in code generation[17], functioncalling[16], tool utilization[13, 19], and self-reflective learning[7, 20]. Recent advancements in LLM-based agents have opened new avenues for exploring their potential in automating the generation of scientific data visualizations based on user-defined queries, potentially lowering the barriers for novice users and enhancing accessibility to advanced data visualization techniques. Modern code-focused LLMs, such as CodeLlama[17], WizardCoder[9], and OpenAI GPT4 have shown strong coding proficiency in Python to generate 2D and 3D chart figures via in-context learning. However, they struggle to faithfully adhere to user specifications due to hallucinations in code generation. AI-assisted data visualization often requires iterative refinement, where users critique intermediate plot outputs and adjust code generation to meet specific customization goals aligns with user expectations in terms of visual appearance, text label clarity, and precise data representation. To address these challenges, we propose PlotGen (see Fig. 1), novel multi-agent framework designed to generate accurate scientific data visualizations based on user specifications by leveraging multimodal LLMs to iteratively debug the intermediate figures output by code LLM via multimodal self-reflection. PlotGen consists of three unique multimodal feedback agents that provide sensory feedbackvisual, lexical, and numericalto the code generation agent, helping to rectify errors during the plotting process and better fulfill user queries through self-reflection. The system orchestrates multiple LLM agents, including: (1) Query Planning Agent that decomposes complex user requests into sequence of executable steps Conference acronym XX, June 0305, 2018, Woodstock, NY Kanika Goswami, Puneet Mathur, Ryan Rossi, and Franck Dernoncourt Figure 1: PlotGen generates accurate scientific data visualizations based on user specifications by orchestrating multimodal LLMs: (1) Query Planning Agent that breaks down complex user requests into executable steps; (2) Code Generation Agent that converts pseudocode into executable Python code; and three code retrieval feedback agents(3) Numeric Feedback Agent, (4) Lexical Feedback Agent, and (5) Visual Feedback Agentthat leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual aesthetics of generated plots via self-reflection. using chain-of-thought prompting; (2) Code Generation Agent that transforms user data into plot by converting pseudocode into executable Python code; (3) Numeric Feedback Agent that ensures data rows and columns are accurately plotted and that the appropriate plot type is selected; (4) Lexical Feedback Agent that verifies the accuracy of textual elements, such as titles, subtitles, axis labels, tick marks, and legend values, based on user requirements; and (5) Visual Feedback Agent that checks that visual aspects of the chart, such as color schemes, layout, entity placement, and overall aesthetics, align with user specifications. Extensive experiments to benchmark the performance of our proposed PlotGen against strong baselines show that PlotGen improves the end-to-end task of scientific data visualization by 10-12% across different LLM settings. Our main contributions are: PlotGen, novel multi-agent LLM framework that generates accurate scientific data visualizations based on user specifications by utilizing multimodal LLMs to iteratively refine intermediate plot outputs produced by code LLM, via visual, lexical, and numerical self-reflection. Extensive experiments demonstrate that PlotGen significantly outperforms strong baselines by 4-6% on the MatPlotBench dataset across various LLM configurations. Qualitative user studies further indicate that PlotGen enhances user trust in LLM-generated scientific visualizations and helps novice analysts increase productivity by reducing time spent debugging plotting errors."
        },
        {
            "title": "2 Related Work\nCode LLMs: With the advent of ChatGPT, several propriety LLMs\nlike GPT-3.5, GPT-4, and Claude Sonnet-3.5 have emerged with",
            "content": "increasingly strong code generation abilities. Moreover, numerous open-source LLMs such as CodeLlama [18], DeepSeekCoder [6], [9], [23] have also come out that are on par in producing executable code. LLM-based Data Visualization: Several previous works have attempted to automate data visualization generation from natural language. [5] was the first attempt to use LSTM to convert JSON data into Vega-Lite visualizations. [4] explored use of LLMs to generate visualization code. Recent works studied the utility of LLMs like ChatGPT for generating charts from ambiguous natural language [3, 21]. [2] expanded this line of work to include multimodal LLMs for chart plotting. [25] tried to involve human feedback to refine the LLM generated plots via reinforcement learning. [26] proposed framework to provide visual feedback to LLMs for iterative refinement. Our work is different from existing works as it explores the use of multimodal feedback via LLM self reflection to resolve errors related to numeric values, lexical labeling and visual aesthetics. LLM Agents: Recent years have seen proliferation of frameworks that utilize Large Language Models (LLMs) to test their applications in practical scenarios [11, 15, 27, 28]. The development of OpenAgents [24] marked the introduction of an accessible platform that implements LLM-powered agents for daily use through three specialized components: Data Agent, Plugins Agent, and Web Agent. groundbreaking simulation system that replicates human behavioral patterns was developed by [12], enabling software agents to computationally reproduce authentic human actions and interactions. In the gaming realm, Voyager [22] emerged as the pioneer LLM-controlled autonomous agent within Minecraft, engineered to continuously discover its surroundings, develop diverse abilities, and generate novel discoveries without human intervention. The software development sphere saw innovation through ChatDev [14], PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback Conference acronym XX, June 0305, 2018, Woodstock, NY which established virtual software company operated through chat interfaces and adhering to waterfall development principles. Building upon these advances, our research investigates how LLM-based agents can contribute to scientific data visualization, an essential domain for modern researchers."
        },
        {
            "title": "3 Methodology\nTask Description: Given a user request 𝑟 , specifying the visual-\nization requirements including chart type, spatial arrangement of\nchart entities, and aesthetic preferences, along with a data table\n𝐷 = {𝑑1,1, 𝑑1,2...𝑑𝑛𝑥𝑚 } as inputs, the scientific data visualization\ntask should output a figure 𝑣. We propose PlotGen to automate\nthis challenging task that orchestrates a multi-agent LLM framework\nconsisting of five agents as follows:\n(1) Query Planning Agent: Query Planning Agent uses an LLM\nto breakdown a complex user request into a sequence of executable\nsteps via chain-of-thought prompting. Each step in the thought chain\ncorresponds to an explicit instruction to the code LLM responsi-\nble for generating the visualization. It highlights the programming\nlanguage required and intermediate programming steps that specify\nthe function calls, parameters, and return types. The output also\ninstructs the code LLM on the data file and its format provided for\nvisualization, along with the visual characteristics desired by the\nuser.\n(2) Code Generation Agent: The Code Generation Agent is the\nmost crucial component of the framework as it is responsible for\ngenerating an error-free executable code that can transform the user\nprovided data into a scientific visualization as per user request. We\nuse specific code LLMs such as GPT-3.5, GPT-4o for this purpose.\nHowever, code LLMs are prone to programming errors that may\nhinder the final plot generation. Hence, we include a self-reflection\nstep in the code generation agent by iterating on the debugger error\nresponse in case of failed code execution. The debugger error mes-\nsage is passed as feedback to the code LLM to fix issues related to\nsyntax, library imports, function arguments, and data formatting. To\nprevent an infinite loop of iteration over coding errors, we restrict\nthe number of iterations of self-debugging.\nMultimodal Feedback: Humans tend to repeatedly refine their\ndrafts to reach satisfactory outputs based on sensory feedback. We\nhypothesize that the code generation agent should also have the\nability to receive external feedback to rectify mistakes during the\nplotting process to better fulfill the user’s queries. Some of the errors\nmay be difficult to diagnose in code but become apparent when\nobserved visually through \"eyes\", reading out the textual phrases,\nor asking probing questions about the data points. To this end, we\nintroduce Multimodal Feedback Agents that provide visual, lexical,\nand numerical feedback to the code agent for self-reflection and\nimproving the final figure output.\n(3) Numeric Feedback Agent: The numeric agent is responsible for\nensuring the accuracy of underlying data in the figure by ensuring all\ndata rows and columns are appropriately plotted and the right kind\nof plot has been selected for visualization. Numeric feedback agent\nuses GPT-4V to de-render the draft figure to get back the underlying\ndata from the plot. It then compares the de-rendered data with the\noriginal data set to check for any discrepancies between the two.\nIf the figure has the correct plot, the two data collections should",
            "content": "show similar numeric trends across rows and columns even if the corresponding numerical values are not exactly same. Further, the numeric feedback agent predicts if the draft figure type matches the users expectation. In case the data trends dont match or the plot type is inconsistent with the user request, the agent provides text feedback to the code agent to fix the error by refining the code used to generate the plot. (4) Lexical Feedback Agent: Chart figures contain labels that help comprehend the plotted data. The textual markers corresponding to these labels are passed as arguments during the code generation step. The lexical feedback agent is responsible for ensuring the textual markers such as titles, subtitles, tick labels, axis descriptions, and legend values are accurate as per the raw data and user requirements. Lexical feedback agent uses GPT-4V to \"read\" and verify the textual values of chart labels with the corresponding ground truth values present in the data sheet. In case the chart labels dont match, it provides feedback to the code generation agent to fix them in the next iteration. (5) Visual Feedback Agent: Visual feedback agent is responsible to ensure that visual aspects in chart figures such as color schemes, layout, placement of chart entities, and aesthetics are aligned as per user requirements. To this end, visual feedback agent uses GPT-4V to observe the draft figure similar to how human \"sees\" the figure and provides textual feedback to the code generation agent for draft refinement. All three of the feedback agents are allowed to provide their respective feedback sequentially. We iterate each feedback agent repeatedly until satisfactory results are achieved in that domain or the maximum trial count runs out."
        },
        {
            "title": "4 Experiments\nMultimodal LLMs: We experiment with GPT-4V [1] for multi-\nmodal feedback agents.\nCode LLMs: For code generation agent in PlotGen, we exper-\niment with both closed source (GPT-3.5, GPT-4) as well as open\nsource LLMs (Magicoder-SDS-6.7B [23] and WizardCoder-Python33B-\nV1.1 [9]) . We set the decoding temperature to 0 for all code LLMs\nand use their respective APIs for closed source LLMs.\nDataset: We use MatPlotBench [26] as benchmark dataset to assess\nthe effectiveness of our proposed framework. This corpus contains\n100 high-quality user queries and raw data mapped to ground-truth\nvisualizations figures close to real-world scenarios.\nBaselines: We benchmark PlotGen against several strong base-\nlines: (1) Direct Decoding: The LLM directly generates the code\nfor rendering the visualization. (2) Zero-Shot Chain-of-thought\nprompting: The LLM is prompted to inference with the zero-shot\nchain of thought mechanism based on the user description. (3) Mat-\nPlotAgent: We benchmark against MatPlotAgent [26] which utilizes\nGPT-4 as code LLM and GPT-4V for visual feedback. We exper-\niment all baselines with various code LLM backbones - GPT-3.5,\nGPT-4, Magicoder-SDS-6.7B [23], and WizardCoder-Python33B-\nV1.1 [9].\nEvaluation: Following [26], we use LLM-as-a-judge automatic\nscoring metric between 0 to 100 to evaluate model-generated vi-\nsualizations with corresponding ground-truth as a reference. The\nauthors show that automatic evaluation scores provided by GPT-4V",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Kanika Goswami, Puneet Mathur, Ryan Rossi, and Franck Dernoncourt Method Dir. Decoding 0-shot CoT MatplotAgent PlotGen WizardCoder-33B Magicoder-6.7B 36.94 35.81 45.96 48.82 38.49 37.95 51.70 57. GPT-3.5 38.03 37.14 47.51 53.25 GPT-4 48.86 45.42 61.16 65.67 Table 1: Performance of PlotGen compared with baselines on MatPlotBench. Figure 2: User Evaluation are sufficiently reliable due to their strong correlated with human evaluation results. Libraries: We use MatPlotlib to generate final output from LLM code output. We use Huggingface library to run open-source code LLMs."
        },
        {
            "title": "5 Results and Discussion\nPerformance Evaluation: Table 1 compares the performance of\nPlotGen with baseline methods on the MatPlotBench dataset. We\nobserve that PlotGen significantly outperforms strong baselines\nacross both open-source and closed source code LLMs. The di-\nrect decoding methods show degraded performance due to their\ninability to correct common plotting mistakes in text labels and\nvisual appearance. We observe that naive zero-shot CoT mecha-\nnism does not significantly enhance the performance of code LLMs.\nWhile MatPlotAgent shows better performance, it struggles to han-\ndle complex charts that require fine-grained numerical accuracy and\nprecise visual-textual alignment of chart labels with plot compo-\nnents. PlotGen effectively captures user requirements, translates\nthem into error-free code, and reduces errors caused by the lack\nof multimodal perception in baseline methods. PlotGen shows\nbest performance with GPT-4 backbone, where inclusion of lexi-\ncal, numerical, and visual feedback helps it to recover from various\nconcurrent errors during the plotting process, demonstrating the\neffectiveness of our multi-agent approach.\nAblation Study: We conduct an ablation study to verify the ef-\nfectiveness of each feedback agent in PlotGen. Ablation of the\nLexical Feedback agent causes 5-7% performance deterioration as\nit is unable to iteratively refine textual details such as titles, subti-\ntles, tick labels, and legend text, which is crucial for clarity in data\ninterpretation. The Numerical Feedback agent helps reduce ambi-\nguity in data plotting by verifying the underlying data trends and\ncorresponding chart types. Its usefulness is particularly pronounced\nin co-located charts with complex legend hierarchies that are prone",
            "content": "to mispredictions due to LLM hallucinations. We observed severe performance drop (10-15%) across all code LLMs in the absence of the Visual Feedback agent, which plays critical role in maintaining visual aesthetic quality as per user requirements. Without the visual feedback, PlotGen runs into common plotting mistakes related to color schemes, scaling, layout, tick labels and legend mapping. User Evaluation: Five participants evaluated 200 randomly sampled user requests from the MatPlotBench dataset to study the usefulness and accuracy of the plots generated by PlotGen. The evaluation results demonstrated strong positive reception, with participants rating the attributions as Completely Accurate (40.5%) or Somewhat Accurate (24.5%) for natural language based plot generation in PlotGen. Plots were found to be more \"Completely Inaccurate\" in 10.6% of the cases. Participants described the outputs useful in reducing time spent debugging plotting errors."
        },
        {
            "title": "6 Conclusion\nIn this paper, we introduced PlotGen, a novel multi-agent frame-\nwork designed to automate the process of scientific data visualization\nthrough a multimodal self-reflection. By coordinating multiple LLM-\nbased agents responsible for query planning, code generation, and\nmultimodal feedback, PlotGen addresses the challenges faced by\nnovice users in generating accurate visualizations according to user\npreferences. PlotGen significantly improves the quality of LLM-\ngenerated plots by 10-12% compared to strong baselines, enhancing\nuser trust, reducing debugging time, and improving accessibility.\nFuture work will extend PlotGen beyond traditional table data vi-\nsualization to explore its application in real-time environments such\nas interactive dashboards, Virtual Reality simulations and visual arts.",
            "content": "References [1] 2023. GPT-4 Technical Report. 257532815 https://api.semanticscholar.org/CorpusID: [2] Matthew Berger and Shusen Liu. 2024. The Visualization JUDGE: Can Multimodal Foundation Models Guide Visualization Design Through Visual Perception? PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback Conference acronym XX, June 0305, 2018, Woodstock, NY (2023). https://api.semanticscholar.org/CorpusID:256697342 [20] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. ArXiv abs/2303.11366 (2023). https://api.semanticscholar.org/CorpusID:257636839 [21] Yuan Tian, Weiwei Cui, Dazhen Deng, Xinjing Yi, Yurun Yang, Haidong Zhang, and Yingcai Wu. 2024. Chartgpt: Leveraging llms to generate charts from abstract natural language. IEEE Transactions on Visualization and Computer Graphics (2024). [22] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. Trans. Mach. Learn. Res. 2024 (2023). https://api.semanticscholar.org/CorpusID:258887849 [23] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source Code Is All You Need. ArXiv abs/2312.02120 (2023). https: //api.semanticscholar.org/CorpusID:265609970 [24] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. 2023. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634 (2023). [25] Yupeng Xie, Yuyu Luo, Guoliang Li, and Nan Tang. 2024. HAIChart: Human and AI Paired Visualization System. arXiv preprint arXiv:2406.11033 (2024). [26] Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi, and Maosong Sun. 2024. MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization. ArXiv abs/2402.11453 (2024). https: //api.semanticscholar.org/CorpusID:267750198 [27] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems 35 (2022), 2074420757. [28] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 (2023). arXiv preprint arXiv:2410.04280 (2024). [3] Liying Cheng, Xingxuan Li, and Lidong Bing. 2023. Is GPT-4 Good Data Analyst? ArXiv abs/2305.15038 (2023). https://api.semanticscholar.org/CorpusID: 258866019 [4] Victor Dibia. 2023. LIDA: Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Danushka Bollegala, Ruihong Huang, and Alan Ritter (Eds.). Association for Computational Linguistics, Toronto, Canada, 113126. https://doi.org/10.18653/v1/2023.acl-demo. [5] Victor C. Dibia and Çagatay Demiralp. 2018. Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent NeuIEEE Computer Graphics and Applications 39 (2018), 3346. ral Networks. https://api.semanticscholar.org/CorpusID:4706694 [6] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence. ArXiv abs/2401.14196 (2024). https://api.semanticscholar.org/CorpusID:267211867 [7] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards Mitigating LLM Hallucination via Self Reflection. In Conference on Empirical Methods in Natural Language Processing. https://api.semanticscholar. org/CorpusID:266176951 [8] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. 2024. AutoWebGLM: Large Language Model-based Web Navigating Agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 52955306. [9] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. ArXiv abs/2306.08568 (2023). https://api.semanticscholar.org/CorpusID:259164815 [10] Puneet Mathur, Alexa Siu, Varun Manjunatha, and Tong Sun. 2024. DocPilot: Copilot for Automating PDF Edit Workflows in Documents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). 232246. [11] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted question-answering with human feedback. ArXiv abs/2112.09332 (2021). https: //api.semanticscholar.org/CorpusID:245329531 [12] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology. 122. [13] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large Language Model Connected with Massive APIs. ArXiv abs/2305.15334 (2023). https://api.semanticscholar.org/CorpusID:258865184 [14] Cheng Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ChatDev: Communicative Agents for Software Development. In Annual Meeting of the Association for Computational Linguistics. https: //api.semanticscholar.org/CorpusID:270257715 [15] Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, et al. 2023. Webcpm: Interactive web search for chinese long-form question answering. arXiv preprint arXiv:2305.06849 (2023). [16] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. ArXiv abs/2307.16789 (2023). https://api.semanticscholar.org/CorpusID: [17] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish Bhatt, Cristian Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. ArXiv abs/2308.12950 (2023). https://api.semanticscholar.org/CorpusID:261100919 [18] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [19] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. ArXiv abs/2302."
        }
    ],
    "affiliations": [
        "IGDTUW, Delhi India"
    ]
}