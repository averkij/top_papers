{
    "paper_title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates",
    "authors": [
        "Kaiyu Yue",
        "Vasu Singla",
        "Menglin Jia",
        "John Kirchenbauer",
        "Rifaa Qadri",
        "Zikui Cai",
        "Abhinav Bhatele",
        "Furong Huang",
        "Tom Goldstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small \"surrogate models\" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder."
        },
        {
            "title": "Start",
            "content": "Zero-Shot Vision Encoder Grafting via LLM Surrogates Kaiyu Yue Vasu Singla Menglin Jia"
        },
        {
            "title": "University of Maryland",
            "content": "Meta https://github.com/facebookresearch/zero 5 2 0 2 8 2 ] . [ 1 4 6 6 2 2 . 5 0 5 2 : r Figure 1. Zero-shot vision encoder grafting via small language surrogate (srgt) model to trigger the target LLM to perform visual understanding task without any additional training. Figure 2. Reducing full decoder training cost with our surrogatetrained encoder for Llama-70B in VLMs. Hollow (cid:155) indicates the average score of the surrogate-trained encoder on the left."
        },
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) typically pair modestly sized vision encoder with large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, potential promising strategy is to first train the vision encoder using small language model before transferring it to the large one. We construct small surrogate models that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, process we call zero-shot grafting1 when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by 45% when using Llama-70B as the decoder. Most modern auto-regressive VLMs are built by extracting visual features from images using an encoder like CLIP [37] or SigLIP [47, 52], and placing these features into 1We define zero-shot grafting as plugging vision encoder trained on surrogate model directly into its target LLM without additional training. In contrast, transferring involves further fine-tuning after grafting. the context window of an LLM. The image features must be aligned with the representation space of the LLM, and this is achieved by training the entire pipeline end-to-end. The cost of such training is often severely dominated by the language model. For example, plugging CLIP (approx 400M parameters) into Llama-70B [9] results in pipeline where vision encoder training occupies almost none of the required memory and computation. In this paper, we explore methods of performing encoder alignment using relatively small lightweight language models, and transferring the resulting features to large language model. We train small surrogate language models with the same representation space as larger target LLM. After training the vision encoder on this small surrogate model, we can then transfer it to the larger model, either directly (grafting) or with fine-tuning. major focus of our work is on understanding how to construct small surrogate models that accurately mock larger target LLMs. Our method of creating such small models stems from analyzing the internal prediction dynamics of LLMs, particularly how predictions evolve across layers. This analysis reveals two distinct phases in the prediction trajectory, separated by clear transition point. We construct our small models by preserving the layers that participate in the early feature extraction phase of inference, and condensing all other layers. Since the small model inherits its shallow parameters from the target LLM, it shares the same embedding space as the original larger model and can effectively stand in as its surrogate. Our surrogate model has two major advantages: Zero-shot grafting capability. Vision features trained on smaller and less resource-intensive surrogate can be directly used by the larger target LLM without any fine-tuning, as depicted in Figure 1. This zero-shot grafting demonstrates these surrogate-trained encoders effectively trigger visual understanding in target LLMs. Fast-converging VLM training. The encoders trained on surrogate models can be further fine-tuned with the full-size target LLM. Since they are already aligned with the LLMs embedding space, they achieve high performance with comparatively little full-scale training. Our experiments show 45% cost reduction for full decoder training with Llama70B, as shown in Figure 2, highlighting the efficiency of our surrogate-trained encoders. layer index and is the hidden dimension. The final hidden states XL1 are passed through normalization layer and the final linear layer RV to produce the logits, where is the vocabulary size. The probability distribution for the predicted next token can be computed for all positions: = softmax (norm(XL1 )W ) RN . (1)"
        },
        {
            "title": "The probability for the next output token at each individual\nposition is",
            "content": "p = P[ 1, t[1 ]] RN 1, (2) where P[ 1, t[1 ]] shifts one position forward and indexes by up to the second-to-last position, aligning each tokens probability with its following token in the sequence. For each layers hidden states Xℓ, we compute the intermediate probability distribution qℓ following the same procedure:"
        },
        {
            "title": "Table of Main Contents",
            "content": "qℓ = softmax (norm(Xℓ )W ) [ 1, t[1 ]]. (3) Section 1: We detail the method of constructing our surrogate model, providing analysis that demonstrates how we discovered, developed, and validated our approach through experimental ablations. Section 2: We show our surrogate models for giant LLMs like Llama-70B, producing encoders with strong zeroshot grafting ability, which can also accelerate the full decoder training of giant language models for VLMs. 1. Building Surrogate Models In this section, we present our approach for building small surrogate models for target LLMs. First, we analyze the LLMs hidden features to identify the critical transition point between shallow and deep information processing layers. Next, we observe that the second/deep phase of inference contributes very little to encoder transferability, and observe that image features transfer well between models when they share their early/shallow processing layers. Finally, we validate these findings and propose to construct surrogate models by preserving the early-phase layers while replacing late-phase layers with translator. 1.1. Analyzing the Prediction Trajectory For target LLM and input array2 of text token IDs ZN , we trace the evolution of features over forward pass of the model. By propagating these tokens through all transformer layers, we obtain intermediate hidden states RN from each layer, where ℓ [0, L1] denotes the Xℓ 2Bold capital letters denote matrix X, and bold lower-case letters column vector x. X[i, j] refers to the element at row and column in matrix X. All non-bold letters represent scalars. To capture the trajectory of evolving predictions, we calculate the KL divergence between the normalized layer-wise distribution qℓ and the final distribution p: DKL(qℓ p) = 1 (qℓ log qℓ ), (4) where 1 RN 1 is vector of ones, log is applied elementwise. Eq. (4) quantifies the deviation of each layers prediction from the final model output, offering insight into how much each layers distribution shifts along the prediction trajectory. This measure enables deeper understanding of each layers role in shaping the models eventual output distribution. In Figure 3, we plot Eq. (4) across different layers of the Llama-3B, 8B, and 70B3 models by feeding4 300 random samples from GenQA [5]. To demonstrate the same curve pattern in different model family, Gemma-2B is also included. Each model displays distinct phase transition where the curves abruptly coalesce and then monotonically converge to the final distribution. For example, in Llama8B, this point appears to occur around layer 17 whereas for Llama-70B it is closer to layer 40. We speculate that this point marks transition in the type of position-wise information processing occurring in the model, where the internal states shift from early phase before the transition point to the late phase after it. The layers in the early phase 3Unless stated otherwise, each model mentioned refers to its latest instruct version. For example, Llama-3B indicates Llama-3.2 3B, Llama70B represents Llama-3.1 70B, and Gemma-2B denotes Gemma-2 2B. 4One concern about this teacher-forced manner is ablated in Sec. A.7. Figure 3. The trajectory of prediction across different layers of Llama-3B, 8B, and 70B, and Gemma-2B from different model family. The arrow marks the transition point where the trajectories of 300 random samples converge. process information from individual token embeddings and combine simple representations together to form higher order concepts, then layers in the late phase converge towards specific next-token prediction. Figure 4. Replacing layers with translator. Despite the relative size in the illustration, our translator is simply an identical transformer layer inherited from the target LLM. The translator bypasses many network layers, and is initialized from the shallowest original layer that it replaced. 1.2. Studying the Transition Phases To test our hypothesis on the transition point, we experiment with Llama-3B5 by replacing consecutive layers of each phase with single transformer layer called translator (terminology adopted from [3]), as depicted in Figure 4. From Llama-3Bs 28 layers, we preserve the first (ℓ = 0) and last (ℓ = 27) layers while replacing two groups of eleven layers each with translator : layers from ℓ = 1 to 11 indicated as (1, 11) for the early phase before Llama-3Bs transition point (ℓ = 16 in Figure 3) and (16, 26) for the late phase after it. Each is 2B small model. Next, we examine the two transition phases by evaluating vision encoders trained on models (1, 11) and (16, 26). To understand their differences and how they affect encoder transferability to the target LLM, we construct two LLaVAlike VLMs using these small models as decoders. We employ two-stage training approach to conduct the initial experiments: 1) First, we simultaneously pre-train vision adapter (a twolayer MLP) and the translator on 1M instructions6, combin5Initial experiments with Gemma-2B showed similar results. In later sections we adapt the method to 70B model. 6This differs from the typical pre-training of vision adapters, which use captions rather than instructions. ing LLaVA-1.5-665K [27] vision-language instructions and random GenQA [5] 500K text instructions, for one epoch. 2) Then, we fine-tune the encoder (ViT-L/14@336px) and vision adapter with the frozen decoders on the LLaVA-1.5665K instructions for one epoch. s e m a a e h a a n a q b o e 60.7 73.0 71.1 52.7 70.6 77.1 78.9 39.2 26.6 42.5 50.3 27.7 53.5 66.6 57.5 32.4 58.9 57.2 54.8 38.5 64.3 67.6 78.2 32.6 56.9 57.3 57.5 40.7 64.3 70.1 79.9 35.2 small model Llama-3B ( 1, 11) (16, 26) (16, 26) Table 1. Accuracy (%) of small models for Llama-3B on text benchmarks. is control experiment added later in the study. Evaluating decoders. After fine-tuning translators in the first stage, we evaluate models (1, 11) and (16, 26) on text benchmarks7  (Table 1)  . The first row is the baseline performance of Llama-3B. The second and third rows show the performance of the decoders with earlyand late-phase layers replaced, respectively. significant performance drop occurs when replacing early-phase layers, underscoring their critical role in understanding and generation. Evaluating encoders. During the second stage, encoders are fine-tuned with small models (1, 11) and (16, 26). We also train an encoder with the full-size Llama-3B as our baseline, listed in the first row of Table 2. For each model, (1, 11) and (16, 26), we report two results: a) performance with their respective encoders, and b) performance with these encoders zero-shot grafted to Llama-3B. For case b), since Llama-3B is never trained on vision-language instructions, it cannot consistently follow special instructions in benchmarks like MME [10] and POPE [26] that expect yes or no answers by prompting with single word or phrase. For these benchmarks, we prompt the model with binary prompts, directing it to answer with yes or no to ensure measurable responses. 7To ease the benchmarking, we evaluate our instruct models on the same benchmarks as the non-instruct models, i.e., base models, and report accuracy produced by log-likelihood. Figure 5. Qualitative results on zero-shot grafting capability of encoders trained with small models for Llama-3B. For comparison, we also include responses from the encoder trained with Llama-3B and the fine-tuned Llama-3B. The encoder trained on (16, 26) achieves strong zero-shot transfer to Llama-3B. Response is sampled with greedy decoding. denotes plugging into B. n r E r b P e E - M i - a c r e B encoder fine-tuned on Llama-3B model ( 1, 11) zero-shot grafting model (16, 26) zero-shot grafting model (16, 26) zero-shot grafting 81.7 63.2 2.7 70.4 80.1 84.4 22.3 Table 2. Accuracy (%) of encoders fine-tuned by small models for Llama-3B on VLM benchmarks. indicates control experiment added later in the study. 1028 599 540 923 1022 1162 714 24.1 14.3 6.9 20.6 23.1 25.0 11.2 41.8 0.6 9.2 45.4 47.4 50.3 30.1 42.9 37.2 26.3 42.7 56.6 48.1 34.2 54.2 25.8 25.3 53.2 53.1 59.8 40.7 Table 2 clearly shows that the encoder trained with earlyphase layers preserved model (16, 26) outperforms the one with early-phase layers discarded model (1, 11). Remarkably, performance improves further when the encoder fine-tuned on (16, 26) is zero-shot grafted to Llama-3B, as shown in the third row block. This improvement highlights that the encoder trained with (16, 26) can produce image features that are interpretable by Llama-3B. In Figure 5, we present qualitative results showcasing the zero-shot grafting cabability of the encoders trained via (1, 11) and (16, 26). The responses enhance the above results that replacing the early-phase layers causes the encoder to fail in generating image features that are directly interpretable by the full-size Llama-3B. Are early layers the most critical for encoder transfer? The shallow phase of inference plays crucial role in transferring pre-trained encoder to the full-size target LLM. To concretely verify this observation, we conduct control . run based on (16, 26), in which we unfreeze every other layer before the translator and train them alongside it during the first stage. This control experiment is designed to disrupt the original early-phase parameters, allowing us to completely assess their impact on encoder transferability. We denote this modified model as (16, 26) First, back in Table 1, the last row indicates that fine-tuning additional layers alongside the translator leads to better performance on text benchmarks. However, in Table 2, when , we actually evaluating the encoder trained on (16, 26) observe huge loss of zero-shot grafting ability. This suggests that modifying early-phase parameters in (16, 26) enhance performance on both text and VLM benchmarks when evaluated through itself, but fails to preserve the encoders zero-shot grafting capability as the embedding space of (16, 26) drifts away from the target model. How many early-phase layers should be preserved? If retaining the original early-phase parameters is necessary, the next question is how many layers to preserve for effective zero-shot grafting. In other words, we seek to confirm the transition point in Figure 3 as the optimal starting point for layer removal and translator insertion. To ensure generalization, we conduct this ablation with Llama-8B, where the transition point is around layer ℓ = 17. To validate the transition point, we create three small models by progressively reducing the replaced layers before the transition point: (6, 30), (12, 30), and (17, 30). As shown in Table 3, the performance of these three models on text benchmarks corroborates our findings from Llama-3B, demonstrating that early layers are indeed important. Keeping more early layers leads to better performance, with the best achieved by the model (17, 30). Figure 6. Qualitative results on zero-shot grafting capability of encoders trained with surrogate models for Llama-8B. For comparison, we also include responses from the encoder trained with Llama-8B and the fine-tuned Llama-8B. More early-phase layers preserved lead to stronger zero-shot grafting capability. Responses are sampled with greedy decoding. denotes plugging into B. s e u y c g l c d g w p o a o p 68.4 80.5 79.8 61.8 77.3 81.5 85.4 44.8 25.5 31.8 36.8 24.7 50.9 58.8 61.3 25.8 25.4 42.9 40.5 29.0 59.6 62.6 69.4 29.6 66.8 61.2 59.3 44.8 70.9 71.0 69.3 34.2 small model Llama-8B ( 6, 30) (12, 30) (17, 30) Table 3. Accuracy (%) of small models for Llama-8B on text benchmarks. n r E r b P e E - M i - a c r e B encoder fine-tuned by Llama-8B model ( 6, 30) zero-shot grafting model (12, 30) zero-shot grafting model (17, 30) zero-shot grafting 84.7 73.3 20.6 77.9 81.7 81.3 83.4 Table 4. Accuracy (%) of encoders fine-tuned by small models for Llama-8B on VLM benchmarks. 1165 583 767 983 1022 1041 1044 57.5 25.8 30.9 26.9 50.7 55.4 56.1 23.2 8.9 13.4 13.6 20.5 20.9 25.2 44.9 8.6 - 0.43 45.4 49.7 53.5 47.6 22.4 28.1 29.1 47.9 42.0 56.8 We evaluate the zero-shot grafting capability of the encoders trained with these three models to Llama-8B in Table 4. Performance improves with more preserved early layers, showing particularly strong results when retaining all early-phase layers before the transition point in (17, 30). Figure 6 depicts qualitative example demonstrating the zero-shot grafting capability of three trained encoders. As expected, the encoder trained by the model (6, 30) fails to generate readable image features for Llama-8B as most of the early-phase layers are removed. The encoder trained by (12, 30) performs better, but its image features lack fine-grained detail (e.g., no couch in response), which explains the zero-shot performance gap in Table 4. The encoder trained by (17, 30) generates more detailed and accurate image features, achieving the best zero-shot grafting response among the three, which covers the cats color, expression, eye direction, position, the presence of couch, and even the atmosphere. In summary, our entire analysis of the prediction trajectory reveals: The early phasea plays pivotal role in the encoders transferability to the target LLM. Retaining the original parameters of the early phase is critical for maintaining the encoders zero-shot grafting capability. The transition point in Figure 3 is good starting point for late-phase removal and translator insertion. aWe believe the early phase of LLMs has potential beyond building surrogate models, enabling more creative applications. Based on these three key findings, now we define the model (16, 26) as our surrogate model for Llama-3B, the model (17, 30) as our surrogate for Llama-8B, by inheriting the early-phase layers and replacing the late-phase layers with translator, which can be fine-tuned with small set of text instructions, e.g., 500K for one epoch. 2. Generalizing to Giant Models Having validated our approach at relatively small scales, we now expand our methodology to construct surrogate models for giant LLMs Llama-70B. In this section, our exFigure 7. Qualitative results on strong zero-shot grafting ability of surrogate-trained encoder for Llama-70B, which produces finegrained image features to trigger Llama-70B to perform complex visual understanding tasks. Response is sampled with greedy decoding. periments demonstrate two key advantages of our surrogate approach: First, surrogates can bring strong zeroshot grafting ability to encoders, enabling them to trigger target LLMs to perform visual understanding tasks without additional training. Second, training target LLM decoders on surrogate-trained encoders significantly reduces cost by providing warm start for fine-tuning. 2.1. Surrogate for Llama-70B We analyze the prediction trajectory for Llama-70B in Figure 3 to identify the transition point that marks the end of token processing, which occurs around layer ℓ = 40. Then we keep the first (ℓ = 0) and last layer (ℓ = 79), insert translator at ℓ = 40, and remove the late phase from ℓ = 41 to ℓ = 78, to build 37B surrogate (40, 78). Text benchmark results of this surrogate are shown in Table 5, and VLM benchmark results in Table 6. Table 6 shows the performance of the encoder trained using surrogate (40, 78) on VLM benchmarks, highlighting significant improvement through zero-shot grafting. These experiments show that our approach can be scaled up to giant models, holding the same principles of early phase preservation. 2.1.1. Results: Zero-shot Grafting In Table 7, with encoder-only training, our surrogate outperforms the full-size Llama-70B on most VLM benchmarks, except for VisWiz. This demonstrates the effectiveness of our surrogate models. The last row shows the performance of zero-shot grafting the surrogate-trained encoder into Llama-70B. Notably, the performance of zeroshot grafting surpasses the full-size Llama-70B decoder training on some benchmarks by big margin, demonstrating that our surrogate-trained encoder effectively prompts LLaMA-70B to handle complex visual understanding tasks. Figure 8. Qualitative OCR results on strong zero-shot grafting ability of surrogate-trained encoder for Llama-70B. The input image size is 3362. s e m a a n a r n o a q b o e 82.6 86.9 83.4 71.2 85.4 83.7 89.1 47.6 80.8 70.4 67.3 56.6 77.9 73.9 86.9 37. small model Llama-70B surrogate-37B Table 5. Accuracy(%) of surrogate model for Llama-70B on text benchmarks. n r M a E P n E - M i - a e m r e encoder fine-tuned on Llama-70B surrogate-37B zero-shot grafting 83.4 84.8 86.1 Table 6. Accuracy (%) of encoder fine-tuned by surrogate for Llama-70B on VLM benchmarks. 1294 1287 1315 58.8 59.5 60. 45.6 54.2 59.7 27.0 29.6 37.4 59.8 64.2 64.1 Figure 7 presents qualitative results showcasing the strong zero-shot grafting capability of our surrogate-trained encoder, including questions about creativity, negation, and reasoning. Additionally, Figure 8 demonstrates its effectiveness on OCR tasks, showing that our surrogate models are able to squeeze robust and detailed visual information into encoders. training method Llama-70B decoder Llama-70B encoder surrogate-37B encoder zero-shot grafting cog perc MME POPEbinary f1 acc. MMEbinary SEED-Bench MM LLaVA MMB -Vet perc cog all 327 1545 345 1524 84.9 83.1 84.8 82.9 63.6 68.9 43.7 35.5 285 1294 288 1321 83.4 82.6 82.7 81.2 59.8 65.4 38.6 27.0 312 1329 291 1250 85.5 83.9 86.3 85.0 65.9 71.1 46.2 28.8 295 1348 303 1298 86.8 86.1 87.0 86.4 65.4 70.7 45.3 32.8 -Wild 67.5 45.6 54.3 68. en 71.8 58.8 63.1 65.6 POPE f1 acc. img vid CV-Bench avg 3d 2d 61.8 73.3 67.5 62.2 59.4 60.8 64.7 64.0 64.3 63.2 67.2 65. GQA VisWiz 53.0 47.4 22.7 40.0 62.4 54.4 56.5 51.9 Table 7. Accuracy (%) for Llama-70B on VLM benchmarks. The bold numbers indicate the best performance between the full-size decoder training and our surrogate-trained encoder by zero-shot grafting. special clarification for LLaVA-Wild is in Sec. A.13. cog acc. perc MME POPE f1 POPEbinary f1 acc. method X% score 0.5127 baseline 0.5153 0.6277 0.6444 0.6538 0.6259 0.6612 0.6701 0.6704 SEED-Bench MM LLaVA MMB avg. MMEbinary -Vet all perc cog 301 1178 333 1139 74.5 73.1 74.3 73.9 46.0 49.2 34.1 17.7 255 1061 277 1116 71.4 61.1 71.6 61.3 52.2 55.5 39.5 20.6 314 1447 316 1399 85.5 84.8 85.5 84.8 59.8 64.8 40.9 30.8 353 1511 358 1515 84.8 83.1 84.4 82.4 62.4 67.9 41.5 32.4 327 1545 345 1524 84.9 83.1 84.8 82.9 63.6 68.9 43.7 35.5 295 1348 303 1298 86.8 86.1 87.0 86.4 65.4 70.7 45.3 32.8 340 1404 342 1430 87.3 86.8 84.9 82.7 67.1 72.7 45.9 37.6 369 1435 361 1486 86.7 85.6 86.4 84.9 67.2 72.8 46.0 38.8 374 1449 349 1490 87.9 87.7 86.8 85.5 67.0 72.0 47.8 38.9 -Wild 30.4 41.3 57.9 64.5 67.5 68.9 69.7 70.5 69. en 45.6 58.3 66.8 70.9 71.8 65.6 70.9 73.2 73.9 10 20 30 60 100 - 10 20 30 grafting ours img vid CV-Bench avg 3d 2d 52.4 61.7 57.1 56.6 66.8 61.7 60.6 71.1 65.9 61.5 72.2 66.8 61.8 73.3 67.5 63.2 67.2 65.2 66.6 70.6 68.6 65.7 74.8 70.3 66.6 72.8 69. GQA VisWiz 43.9 38.5 55.1 48.1 52.9 40.0 57.9 52.4 49.2 45.4 47.5 57.1 61.3 62.4 51.9 60.1 60.6 61.4 Table 8. Convergence comparison with using X% of training data between baseline and our surrogate training approach for Llama-70B decoder training. The gray row indicates the training hours reported in the Table 9 with 20% of training data for ours. See Table A.1 for extra columns with additional benchmarks. While the surrogate-trained encoder enables zero-shot conversion of the giant LLM into VLM, its performance still lags behind that of full-size decoder training. What benefits can we expect from this surrogate-trained encoder? Next, we demonstrate that it can accelerate training convergence and improves the performance of full-size decoder training. 2.2. Reducing Full Decoder Training Cost In the previous sections, we conduct the experiments with two-stage training strategy, where we simultaneously train the vision adapter in encoder and the translator in decoder during the first stage, and then fine-tune the encoder atop the surrogate in the second stage. Currently, we are interested in training the full-size decoder, which is the final third training stage. First, we introduce the training setup, and recipes are introduced in Sec. A.9. Models. As in previous sections, we use the CLIP-L/14 encoder with an input image size of 3362. The vision adapter is two-layer MLP, consisting of consecutive linear layers with GELU activation in between. Notably, we maintain fixed vision adapter size across all model scales, unlike prior works [6, 24] that scale it with model size. This design choice ensures that variations in adapter size do not introduce unknown effects on the encoders zero-shot grafting capability, allowing for controlled initial study. For state-of-the-art performance, however, the vision adapter can be scaled up with the model size. The decoders are our surrogate-37B, i.e., (40, 78), and full-size Llama-70B. Data. In the third training stage, the training data is still the same as in the previous two stages the LLaVA-1.5-665K [27] instructions (without text-only samples). This choice is based on the following considerations: 1) The first training stage focuses on the adapter and translator. Commonly, vision adapters are trained on captions instead of instructions, but we found no significant difference in experimental outcomes. Thus, to simplify training, we merge the training of the vision adapter and translator into single stage using vision-language and text-only instructions. When forwarding the text-only instructions, gradients backpropagated to the vision adapter are zero. 2) The second stage trains encoders with surrogates, aiming to efficiently compress data into encoders while preparing to transfer knowledge to the full-size decoder. To ensure consistency, we use the same training data for the second and third stages. It is recommended to use larger and more diverse datasets for those two stages. 2.2.1. Results: Convergence and Training Cost In Table 8, we compare performance of the typical baseline method and our surrogate training approach across different percentages of training data used for training decoders. The baseline trains Llama-70B with the original CLIP encoder, while ours trains it with our surrogate-trained encoder (the third row in Table 7). First, the gray row represents the performance of zero-shot grafting the surrogatetrained encoder to Llama-70B, which nearly matches the baseline with 30% of the data. Second, after training on just 10% of data, our approach achieves nearly the same performance as the baseline with 100% of the data, except for MME. For other benchmarks, our 10% performance even outperforms the final baseline result. With continued training, performance remains unchanged, suggesting saturation after 20% of the data. In Figure 2, we plot the normalized average score of each X% data utilization for our method and baseline. We also visualize Table 9 in Figure 2 for direct comparison of training hours for each training stage. First, our pretraining time is longer than the baseline because we train both the vision adapter and the translator with additional text instructions. However, the key advantage of our surrogate training approach is seen in the decoder training, which is the real bottleneck in common methods. With 20% of the data used for our decoder training, we achieve training cost reduction of 45%. This reduction is the minimum, as our performance in the 10% case already exceeds the final result of the baseline, as shown in Table 8. method baseline ours # gpu 128 128 pre. 7.01 9.36 ft. enc. 0.00 4.25 ft. dec. 27.88 5.56 total hours 34.79 19.17 Table 9. Training hours comparison between baseline and our surrogate approach for training VLMs with Llama-70B, including the time for pretraining (pre.), fine-tuning encoder (ft. enc.), and fine-tuning decoder (ft. dec.). Checkpoint loading and saving times are excluded. More details in Sec. A.8. 3. Related Work Overview Understanding LLMs is key topic in mechanistic interpretability [39]. [1] uses linear classifiers (probes) to understand the dynamics of intermediate layers in neural networks. For LLMs, [36] directly employs the output embedding matrix as probe to classify layer-wise representations, illustrating how input tokens shift from current positions to next ones. Tuned Lens [3] extends this idea with trainable probe for broader applicability to modern LLMs. [41] conceptualizes transformer layers as painters that iteratively refine representations and suggests that middle layers share the same representation. In contrast, we identify two distinct transition phases in LLMs. The shared representation in middle layers suggests redundancy. [41] further concludes that some middle layers can be removed without significant performance drop. Prunning LLMs largely is based on such insight of redundancy. Notably, both [12] and [34] found that deep layers are not essential and can be removed. Interestingly, our surrogate models also replace deep layers in the late phase. However, our method differs in how we identify the transition point and in our objective. Unlike prunning, which aims to remove layers while preserving performance, our focus is on the efficiency of surrogate models for encoder transferability. While our surrogate models consistently underperform compared to their target LLMs, they serve distinct purpose in producing efficient encoders for VLMs. Our surrogate-trained encoders can directly prompt target LLMs to generate the expected responses without any finetuning. This zero-shot grafting ability aligns with the concept of steering LLMs, lightweight alternative to finetuning LLMs [14, 20]. Prior works show that language models can be guided to perform specific tasks without extensive fine-tuning. Similarly, in our case, image features from surrogate-trained encoders act as steering tokens, enabling target LLMs to interpret visual content and answer various complicated questions. This capability provides warm start for further decoder fine-tuning, helping to mitigate the expensive training cost of VLMs [2, 6, 25, 46, 48, 49]. The costs surged as decoder sizes scale from relatively small models (3B, 8B) to much larger ones, such as 70B [24], 110B [22, 28]. Additionally, increasing the number of image tokens for high-resolution inputs further escalates the computational burden. LoRA [16] could be applied for training VLMs. While LoRA improves efficiency, it underperforms full fine-tuning, especially in giant LLMs, when applied with small rank (e.g., 8) and alpha (e.g., 32) to query and key decoder layers common practice in LLM training. Closing this gap needs applying LoRA to entire transformer layers with large rank and alpha (e.g., rank 128 with alpha 256 as in [24] for 13B decoder training). Then LoRA takes about the same time as full decoder fine-tuning. This limitation likely explains why current VLMs still rely on full decoder fine-tuning. Critically, contrasting to our surrogate training approach, LoRA does not accelerate convergence. See more in Sec. A.2. Additionally, the idea of using small models to train encoders before applying them to larger decoders has been depicted in [18]. However, this work is not directly related to ours, as it employs progressive multi-stage training strategy to just scale up model size and refine image processing from coarse to fine. No further details are provided on this method, leaving it unclear how it reduces costs. In contrast, our approach provides well-defined framework for constructing efficient surrogate models specifically tailored for any target LLM. Plus, we plug the surrogate-trained encoders directly into their target LLMs, converting them into VLMs without any fine-tuning to perform complex visual understanding tasks. Further, with our surrogate-trained encoders, the decoder needs only few full-scale fine-tuning steps to achieve the desired performance. 4. Conclusion In this work, we show that vision encoders trained with our surrogate models can accelerate VLM training. We also note that our surrogate models are not limited to vision encoders. The main limitation of our approach is the need for well-designed surrogate, which ideally should be small. Although our layer-dropping strategy works in principle for any LLM, resulting models are still half the size of their target LLMs, for example, our surrogate-37B for Llama70B. This underscores the practical value of surrogate models and highlights the need for ways to create them more efficiently and with better compression. A. Appendix A.1. Extra Columns for Main Tabel 8. avg. ChartQA Text Doc Info AI method X% score VQA VQA overall human aug. VQA 2D 16.1 24.7 63.6 baseline 15.4 25.9 64.8 16.5 25.6 64.9 13.1 20.7 61.2 22.0 28.9 64.2 17.8 28.9 65.7 19.6 29.4 66.3 0.2751 36.9 16.5 0.3041 46.8 22.2 0.3105 47.8 22.6 0.2821 46.2 21.1 0.3512 51.5 29.5 0.3376 52.3 28.7 0.3468 53.1 29.1 17.8 20.2 21.1 19.1 25.8 22.7 23.7 16.9 17.8 18.8 16.1 23.9 20.3 21. 30 60 100 - 10 20 30 grafting ours Table A.1. Extra columns for Table 8 with additional benchmarks. A.2. Comparing with LoRA It is reasonable to ask whether applying LoRA [16] to full decoder training could reduce training costs enough to eliminate the need for our surrogate training approach. We evaluate this by applying LoRA to the full Llama-70B decoder training with the same setup as the experiments in Section 2.2. LoRA is applied to query and key layers in all transformer blocks, with rank 8 and alpha 32, following common configuration [16] for tuning LLMs. Each training step takes an average of 14.2 seconds, including data loading and optimizer steps. Indeed, LoRA training is faster than full parameter fine-tuning, as shown in Figure 2. However, LoRA cannot reduce training costs as effectively as our surrogate training approach because it does not improve convergence speed. This happens for two reasons: 1) LoRA is designed for fine-tuning an already well-trained model, where the target distribution is mostly aligned. For example, fine-tuning language model on new text corpus. In contrast, VLM training aims to transform language model from text-only space to vision-language space. Since LoRA updates only few parameters, it has no sufficient capacity to adapt the language model to vision features, leading to suboptimal performance. 2) LoRA reduces trainable model parameters, but not the total training steps. Conversely, our surrogate training approach speeds up the decoder training convergence by the surrogate-trained encoder, reducing overall training steps. By addressing optimization efficiency rather than just parameter count, our method is fundamentally more effective. A.3. Language Degradation in Decoders common issue in VLM training is that the language decoder underperforms on text benchmarks after training on vision-language instructions. This degradation is due to the models focus on vision-language tasks, which can lead to loss of language understanding. typical solution is to mix the text-only corpus with vision-language instructions during training to mitigate this issue. In our surrogate training approach, we examine whether it can help mitigate this degradation in this ablation. s e m a a e h a n o a q b o e 82.6 86.9 83.4 71.2 85.4 83.7 89.1 47.6 82.5 86.6 84.2 71.9 85.3 84.1 89.7 47.8 79.4 84.8 78.9 69.5 84.6 83.6 87.5 50.2 a f p - model Llama-70B ours baseline Table A.2. Accuracy(%) of 70B decoders from the final training stage of our surrogate approach (ours) and baseline method on text benchmarks. In Table A.2, we compare the accuracy of the 70B decoders from the final training stage of our surrogate approach and the baseline method on text benchmarks. Our decoder retains language performance, matching or even slightly surpassing the original Llama-70B. In contrast, the baseline method suffers significant drop in performance on most benchmarks, including MMLU, HellaSwag, ARC, Winogrande, and BoolQ. This is because our surrogate approach fine-tunes the fullsize decoder with few steps on our surrogate-trained encoder, which is already aligned with the LLMs embedding space. This alignment prevents the decoders representation from drifting too far, preserving its language understanding. Figure A.1. The trajectory of prediction across different layers of 1B, 4B, 12B, and 27B instruct models from Gemma-3. The arrow marks the transition point where the trajectories of 300 random samples converge. A.4. Prediction Trajectory of Gemma-3 Family In Figure A.1, we plot the prediction trajectory across different layers of Gemma-1B, 4B, 12B, and 27B instruct models from Gemma-3 [43] using 300 random sequences. Their early phases are instable and spiky, which may be MME on (22, 35) grafting encoder ft. MMEbinary cog perc cog 236 920 284 278 1026 272 Info AI f1 Bench -Vet Wiz VQA VQA QA VQA 2D 10.7 43.1 8.9 48.1 12.7 7.7 20.6 50.1 16.3 56.5 44.3 11.5 Table A.3. Accuracy (%) of encoder trained on Qwen3-4B surrogate (22, 35) and zero-shot grafted to full-size Qwen3-4B on VLM benchmarks. We report only all for SEED-Bench, avg for CV-Bench, and overall for ChartQA to save space. POPEbinary f1 acc. 78.0 80.0 78.0 75.1 81.1 81.6 78.1 80.9 SEED MM LLaVA MMB CVGQA VisText Doc Chart Bench 44.4 54.0 -Wild 27.9 44.4 en 20.8 49.4 perc 888 8.5 10.1 41.6 32.9 5.4 7.6 POPE acc. Figure A.2. The trajectory of prediction across different layers of 7B, 14B, 32B, and 72B instruct models from Qwen2.5. The arrow marks the transition point where the trajectories of 300 random samples converge. Figure A.3. The trajectory of prediction across different layers of 4B, 8B, 14B, and 32B instruct models from Qwen3. The arrow marks the transition point where the trajectories of 300 random samples converge. due to the sliding window attention. But this instability does not affect our method, as noted in Section 1.1, where our intial proof-of-concept experiments were conducted on Gemma-2-2B. Their trajectories still converge at transition point, marking the shift between early and late phases of the model. We highlight the transition point for each model. As the figure shows, the early phase becomes increasingly spiky as model size grows in the Gemma-3 family, while the late phase remains smooth and stable. A.5. Prediction Trajectory of Qwen Family In Figure A.2 and Figure A.3, we plot the prediction trajectory across different layers of Qwen2.5 [44] and Qwen3 [45] families. The difference is that the transition point occurs slightly later in the model, especially for larger models, compared to Llama and Gemma families. This will cause the surrogate model to be larger than the Llama and Gemma one, for example, the Qwen2.5-72B surrogate model will have 56B parameters. While this proposed surrogate model is faster and more cost-effective than the fullsize 72B model, particularly for extended training, it still remains large for surrogate we ideally expect. This limitation has been discussed in Section 4. A.6. Ablation on Method Generalizability To validate the generalizability of our method, we run an ablation on the fresh new Qwen3-4B. According to the prediction trajectory in Figure A.3, we build 2.8B surrogate (22, 35) and train the last eight CLIP encoder layers with it under the same experimental settings. Table A.3 shows the encoders results on VLM benchmarks: the first row is with the surrogate; the second row shows improvements with zero-shot grafting to full-size Qwen3-4B (without thinking mode), except for GQA. Most improvements are significant. Thus, our method can generalize well to any pretrained LLM. A.7. Ablation on Teacher-Forced Feeding In Section 1.1, the curves in Figure 3 are produced using teacher-forced feeding to obtain the models predictions. Specifically, we feed 300 randomly sampled pairs of question and response into the model and examine the intermediate feature dynamics. potential concern is that this teacher-forced manner with real-world text samples may not accurately represent the models natural feature dynamics, as it assumes perfect reconstruction of the sequence. To validate our findings, we prompt models with 300 random questions and allow them to predict responses through greedy sampling. Then we feed these predictions back into the models and find that the resultant curves remain consistent with those obtained from the forced-prediction approach in Figure 3. A.8. Training Time for Llama-70B The bottleneck in training Llama-70B is not only the GPU card, but also the network bandwidth for communication. In our experiments, we use 128 A100-80G GPUs with AWS EFA network. We shard the 70B parameters across 16 GPU ranks, and the replica group size is 8 using PyTorch FSDP [53]. The batch size is also 128, which means each training step requires communication among all GPUs for forward and backward propagation, without gradient accumulation. The NCCL communication is AWS EFA8. For reference of training full 70B model, the average forward time is 4.5 seconds, and the average backward time is 15.7 seconds. Thus, the total average time of each training step is 20.5 seconds, including the data loading time and optimizer step. A.9. Training Recipes In the analysis experiments of Section 1, we train the entire encoder with surrogate models for one epoch during the second training stage. In contrast, the main experiments in Section 2 follow different setting: only the last eight layers of the encoder are trained for one epoch, using either surrogate-37B or full-size Llama-70B, while the remaining layers are frozen. This setting is also applied to the baseline method. Aside from this difference, all other training recipes remain the same. Next, we provide detailed description of the training recipes. We apply the chat template specific to each LLM, including any special chat tokens, to the input conversations. For instance, the dialog shown in Figure 6 starts as raw text; after applying the chat template, it becomes: In VLM training, cross-entropy loss for next-token prediction is typically applied only to the green tokens in responses. The special token eot id marks the end of conversation turn, while all other tokens are masked out. However, we found that during encoder-only training, the loss should also be applied to the blue special tokens. Without this adjustment, the encoder struggles to properly follow question instructions and generate desired responses, both in zero-shot grafting senarios and when paired with surrogate models. For Llama-70B, along with their surrogates, we fully fine-tune all parameters during decoder training. Hyper-parameters are in Table A.4. A.10. Evaluation Benchmarks We evaluate VLMs following LLaVA-1.5 [27] with additional benchmarks, including MME [10], POPE [26], LLaVA-in-the-Wild [29], SEED-Bench [21], MM-Vet [50], MMBench [30], TextVQA [40], GQA [17], DocVQA [32], ChartQA [31], InfoVQA [33], AI2D [19], Viz-Wiz [13] using lmms-eval toolkit [23]. We also evaluate models on the vision-centric benchmark CV-Bench [46]. For language models, we evaluate them on MMLU [15], HellaSwag [51], ARC [8], PIQA [4], Winogrande [38], BoolQ [7], and OpenBookQA [35] using lm-harness toolkit [11]. The few-shot setting and the type of reported accuracy for text benchmarks in lm-harness is shown in Table A.5. w l s r n a r m n o a p o q b o 10 5 - number of shots acc. type 5 norm norm norm - Table A.5. Few-shot setting for text benchmarks in lm-harness. For accuracy type, norm refers to length-normalized accuracy. 0 norm - 0 norm 25 0 0 A.11. Surrogate Training for Smaller Models For interested readers, we share our experience applying our surrogate training approach to smaller language models, such as Llama-3B and 8B, compared with Llama-70B. Before diving in, our key takeaway is: our surrogate training approach is most effective for giant LLMs. The larger the LLM decoder and the training data scale in VLMs, the greater the cost reduction our method achieves. Applying this approach to relatively small LLMs is unnecessary, two reasons: a) Their training costs are already affordable nowadays. b) It introduces additional hyper-parameters with minimal cost savings. This section serves purely as discussion, as our experiments revealed some interesting yet unverified observations. We share these findings to provide insight into potential limitations and edge cases of our method, which may inform future research. When applying our surrogate training approach to Llama3B and 8B, we observe performance degradation in the third training stage, particularly on benchmarks requiring short answer (e.g., yes or no in MME and POPE) or single word/phrase (e.g., GQA and VizWiz). The initial thought is that we may encounter the overfitting issue since we use the same LLaVA-1.5-665K instructions for all the training stages. 8Introduction of Amazon Elastic Fabric Adapter (EFA). After further investigation, we find that the performance stage-1 recipes encoder decoder adapter in encoder translator in decoder trainable parameters adapter + translator encoder + adapter encoder + adapter + decoder 2e-5 for 70B, 5e-5 for others stage-2 CLIP-L/14-336px Llama-3.2-3B, 3.1-8B, 3.1-70B instruct version Linear GELU Linear transformer layer stage-3 5e-5 learning rate batch size 1e-4 256 instruction datasets GenQA[5]-500K LLaVA-1.5-665K 128 LLaVA-1.5-665K (ShareGPT-40K not included) translator layer index image input size image augmentation number of A100-80G warmup ratio lr scheduler optimizer weight decay gradient clip epochs precision PyTorch FSDP gradient accumulation activation checkpointing 16 for Llama-3B, 17 for Llama-8B, and 40 for Llama-70B fixed size of 3362 pixels pad to input size with per-channel mean pixel value 16 (Llama-3B, -8B) and 128 (Llama-70B) 3% of total batch iterations cosine annealing with lr min = 0 AdamW(β1 = 0.9, β2 = 0.999, eps = 1e-8) 0 max norm = 1.0 with 2-norm 1 bfloat16 enabled enabled enabled Table A.4. Training hyper-parameters, recipes and settings. Figure A.4. Dynamic loss weights for balancing loss contributions from different response lengths in global batch. Multiple curves represent different maximum response lengths in batch, gradually increasing from 30 to 300. loss contributions from different response lengths within batch. Asume the length of the i-th response is Li and the total number of responses in global batch is across all ranks. The dynamic loss weight for the i-th response is calculated as: wi = ( maxj{1,...,N } Lj log Li ord ) , wi wi j=1 wj)/( j=1 Lj) ( . We then scale the loss for all tokens in the i-th response by wi. Figure A.4 illustrates how this weighting adjusts based on the maximum response length in batch. In our experiments, we set ord = 0.5. With this adjustment, we solve the performance degradation issue and achieve comparable performance to the baseline method, as shown in the left subfigure of Figure A.5. We believe dynamic loss weighting is not the only solution. The issue could be addressed by using more diverse, larger-scale datasets in the second and third stages, unlike the single small set of vision-language instructions used in our experiments. A.12. Distilled Model as Surrogate In some LLM families, such as Llama [9] and Gemma [42], they distill small models (Llama-1B) with large ones (Llama-8B). We investigate whether these distilled models can serve as surrogates for training encoders. The core issue with using distilled models as surrogates is that they have different embedding dimensions from the larger models. So additional training stage is required to align the embedding dimensions, e.g., with linear layer. We conduct the experiments on Llama-1B and 8B, treating Llama-1B as surroFigure A.5. Training cost comparison among our surrogate training approach, baseline method, and training with distill models as surrogates for Llama-3B and 8B. degradation is not due to overfitting. Instead, our surrogate is highly effective at training encoders with strong zero-shot grafting capability in the second training stage. Then in the third stage, the encoder triggers the target LLM to generate responses that already align with the training data distribution, resulting in lower-than-expected loss values, especially for short answers or single words/phrases. This issue arises from how loss is mean-reduced in batch by dividing the total loss by the number of tokens involved (e.g., green tokens in responses). As result, the loss from short answers is overwhelmed by that of longer answers, such as open-ended responses with hundreds of tokens, leading to insuffcient gradient updates for short answers. To address this issue and verify our hypothesis, we adjust the loss calculation to consider the number of tokens in responses. Specifically, we multiply the loss for each response group by dynamic loss weight to balance the input question Which one is larger, the cat or the dog? 1 is larger than 10, is it correct? Happy birthday to you! love you. 1, 2, 3, 4. Count the next three numbers. What is 100 + 13 = ? Calculate 2 * 10.2 = ? Calculate 1 + 10 = ? Is 8.11 > 8.3? response of model (1, 16) The cat is larger than the dog. No, 10 is not larger than 1. Happy birthday to you! Im glad to hear that. Im here to help you with any questions or tasks you may have. 1, 2, 3, 4 100 + 13 = 113 2 * 10.2 = 20.4 1 + 10 = 11 No, 8.11 is not greater than 8.3. Yes, 8.11 > 8.3. response of model (16, 24) The dog is larger. The statement is not true. The number 1 is less than the number 10. Thank you for the birthday greetings! am happy to celebrate with you. am an AI language model and do not have personal feelings or emotions. However, can provide you with information and assistance in your own way. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 100 + 13 = 125 2 * 10.2 = 22.4 1 + 10 = 20 Table A.6. Simple ability assessment for two small models of Gemma-2B. The model (1, 16) means the trained translator replaces the early transition phase, while (16, 24) replaces the late phase. Response is sampled with greedy decoding. gate model. The extra training stage is added between the second and third stages, where we fix the surrogate-trained encoder and other parameters, only training the linear layer to align the embedding dimensions. In the right subfigure of Figure A.5, we compare the performance of the distilled model as surrogate with our surrogate training approach using the same training setup, including the ord = 0.5 dynamic loss weighting. The distilled model as surrogate performs worse than our surrogate training approach, indicating that distilled models are not ideal surrogate choices for our method. A.13. Performance Drop on LLaVA-Wild Our models and the baseline model both score lower on the LLaVA-in-the-Wild [29] than the official scores reported in LLaVA-1.5 [27], appearing in Tables 7 and 8, as well as in the ablation studies in Section 1. This drop is expected due to change in the GPT-4 API model version in the juding process. LLaVA-1.5 uses GPT4-0314, which has been deprecated. Instead, the evaluation toolkit lmms-eval uses GPT4-0613, which systematically results in lower scores across all models. For example, LLaVA-1.5-7B drops from 65.3 to 59.6, and LLaVA-1.5-70B from 72.8 to 66.1. For more details, please refer to README.md9 in lmms-eval repository. A.14. Different Abilities in Two Transition Phases In our initial experiments with Gemma-2-2B for Section 1, we observe that different abilities emerge in the two transition phases of the model. Gemma-2B has 26 layers and its transition point is at layer 16, as shown in Figure 3. We construct two small models by replacing transition phases with translator: (1, 16) to replace the early phase, and (16, 24) to replace the late phase. As in the ablation studies of Section 1, we train the translator for one epoch in the first training stage. When we 9lmms-eval - Comprehensive evaluation results of LLaVA family models. prompt these two small models with the same question, we find that (16, 24) and (1, 16) exhibit different abilities in their responses. In Table A.6, we show the responses of both models to set of questions. The first block includes simple questions that test basic common sense reasoning, factual recall, and conversational coherence. Model (16, 24) performs well on these questions, in which the early phase is preserved. It can correctly answers that the dog is larger than the cat, provides coherent response to the birthday greeting, and appropriately declines the love confession as an AI model. Additionally, it follows the instruction to count the next numbers but misinterprets how many to include. In contrast, model (1, 16) struggles with these questions, in which the late phase is preserved. This suggests that: a) Common sense reasoning, factual knowledge, and conversational abilities are primarily stored in the early-phase parameters. b) Despite training the translator in (1, 16), it may not fully recover the models knowledge, possibly due to its limited capacity with fewer parameters than the full model. The second block of questions tests the models ability to perform basic arithmetic calculations and comparisons. Conversely, (1, 16) correctly handles even floating-point multiplication, while (16, 24) fails entirely. This suggests that arithmetic computation and numerical comparison are primarily handled by the late-phase parameters. A.15. Potential Use Cases Our surrogate training approach may also benefit large encoder training, instead of the full-size decoder, in the third stage. For example, InternViT [6] trains large encoders (6B) with LLMs. Using surrogate to align the encoder beforehand could provide warm start for continued training with the frozen full-size LLM, reducing overall cost. This is feasible since the loss function remains consistent (with contrastive term), and the encoder is strongly aligned with the full-size LLM via the surrogate like in our experiments."
        },
        {
            "title": "Acknowledgements",
            "content": "We sincerely appreciate Metas support in providing GPUs for our experiments. The authors at University of Maryland were supported by DAPRA TIAMAT, the ONR MURI program, the National Science Foundation (IIS-2212182), and the NSF TRAILS Institute (2229885). Additional commercial support was provided by the Amazon Research Award program, Open Philanthropy, and Capital One Bank."
        },
        {
            "title": "Disclaimer",
            "content": "The image-based training data was used only to train vision encoders to produce image features, not generative components."
        },
        {
            "title": "References",
            "content": "[1] Guillaume Alain and Yoshua Bengio. Understanding InterIn ICLR, mediate Layers Using Linear Classifier Probes. 2017. 8 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: Visual Language Model for Few-Shot Learning. In NeurIPS, 2022. 8 [3] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting Latent Predictions From Transformers With the Tuned Lens. arXiv preprint arXiv:2303.08112, 2023. 3, 8 [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about Physical Commonsense in Natural Language. In AAAI, 2020. 11 [5] Jiuhai Chen, Rifaa Qadri, Yuxin Wen, Neel Jain, John Kirchenbauer, Tianyi Zhou, and Tom Goldstein. GenQA: Generating Millions of Instructions from Handful of Prompts. arXiv preprint arXiv:2406.10323, 2024. 2, 3, 12 [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, InternVL: Scaling up Vision Foundation Lewei Lu, et al. Models and Aligning for Generic Visual-Linguistic Tasks. In CVPR, 2024. 7, 8, [7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In ACL Anthology, 2019. 11 [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. 11 [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. 1, 12 [10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. MME: Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394, 2023. 3, 11 [11] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. Framework for Few-Shot Language Model Evaluation, 2024. 11 [12] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel Roberts. The Unreasonable Ineffectiveness of the Deeper Layers. In ICLR, 2025. 8 [13] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. VizWiz Grand Challenge: Answering Visual Questions from Blind People. In CVPR, 2018. [14] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji. Word Embeddings Are Steers for Language Models. In ACL Anthology, 2024. 8 [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. In ICLR, 2021. 11 [16] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. 8, 9 [17] Drew Hudson and Christopher Manning. GQA: New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In CVPR, 2019. 11 [18] InternVL2. InternVL2: Better than the BestExpanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy. https : / / internvl.github.io/blog/2024-07-02-InternVL2.0/, 2024. [19] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. Diagram Is Worth Dozen Images. In ECCV, 2016. 11 [20] Md Kowsher, Nusrat Jahan Prottasha, and Prakash Bhat. Propulsion: Steering LLM with Tiny Fine-Tuning. In ACL Anthology, 2024. 8 [21] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. SEED-Bench: Benchmarking MultiIn CVPR, modal LLMs with Generative Comprehension. 2024. 11 [22] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. LLaVA-Next: Stronger Llms Supercharge Multimodal Capabilities in the Wild. https://llavavl.github. io/blog/2024-05-10-llava-next-stronger-llms/, 2024. 8 [23] Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. LMMs-Eval: Accelerating the Development of Large Multimodal Models. https: //github.com/EvolvingLMMs-Lab/lmms-eval, 2024. [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. LLaVA-OneVision: Easy Visual Task Transfer. arXiv preprint arXiv:2408.03326, 2024. 7, 8 [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML, 2023. 8 [26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating Object Hallucination in Large Vision-Language Models. In EMNLP, 2023. 3, 11 [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning. In CVPR, 2024. 3, 7, 11, 13 [28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-Next: Improved Reasoning, Ocr, and World Knowledge. https: //llavavl.github.io/blog/20240130llavanext/, 2024. 8 [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In NeurIPS, 2024. 11, 13 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is Your Multi-modal Model an All-around Player? In ECCV, 2025. 11 [31] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: Benchmark for Question Answering About Charts With Visual and Logical Reasoning. In ACL Findings, 2022. [32] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: Dataset for VQA on Document Images. In WACV, 2021. 11 [33] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis InfographKaratzas, Ernest Valveny, and CV Jawahar. icVQA. In WACV, 2022. 11 [34] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. ShortGPT: Layers in Large Language Models are More Redundant Than You Expect. arXiv preprint arXiv:2403.03853, 2024. 8 [35] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can Suit of Armor Conduct Electricity? New In ACL AnDataset for Open Book Question Answering. thology, 2018. 11 [36] nostalgebraist. interpreting GPT: the logit lens. https: //www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens, 2020. 8 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In ICML, 2021. 1 [38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. In ACM Communications, 2021. 11 [39] Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. Open arXiv preprint Problems in Mechanistic Interpretability. arXiv:2501.16496, 2025. 8 [40] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards VQA Models That Can Read. In CVPR, 2019. 11 [41] Qi Sun, Marc Pickett, Aakash Kumar Nain, and Llion arXiv preprint Jones. Transformer Layers as Painters. arXiv:2407.09298, 2024. 8 [42] Gemma Team. Gemma-2. Kaggle, 2024. 12 [43] Gemma Team. Gemma-3. Kaggle, 2025. 9 [44] Qwen Team. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115, 2025. 10 [45] Qwen Team. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025. [46] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: Fully Open, Vision-Centric Exploration of Multimodal LLMs. In NeurIPS, 2025. 8, 11 [47] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features. arXiv preprint arXiv:2502.14786, 2025. 1 [48] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191, 2024. 8 [49] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. CogVLM: Visual Expert for Pretrained Language Models. In NeurIPS, 2024. 8 [50] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. In ICML, 2024. 11 [51] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can Machine Really Finish Your Sentence? In ACL Anthology, 2019. [52] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Sigmoid Loss for Language Image PreLucas Beyer. Training. In ICCV, 2023. 1 [53] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, ChienChin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. arXiv preprint arXiv:2304.11277, 2023."
        }
    ],
    "affiliations": [
        "Meta"
    ]
}