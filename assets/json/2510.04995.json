{
    "paper_title": "Power Transform Revisited: Numerically Stable, and Federated",
    "authors": [
        "Xuefeng Xu",
        "Graham Cormode"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Power transforms are popular parametric techniques for making data more Gaussian-like, and are widely used as preprocessing steps in statistical analysis and machine learning. However, we find that direct implementations of power transforms suffer from severe numerical instabilities, which can lead to incorrect results or even crashes. In this paper, we provide a comprehensive analysis of the sources of these instabilities and propose effective remedies. We further extend power transforms to the federated learning setting, addressing both numerical and distributional challenges that arise in this context. Experiments on real-world datasets demonstrate that our methods are both effective and robust, substantially improving stability compared to existing approaches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 5 9 9 4 0 . 0 1 5 2 : r Power Transform Revisited: Numerically Stable, and Federated Xuefeng Xu University of Warwick xuefeng.xu@warwick.ac.uk Graham Cormode University of Warwick g.cormode@warwick.ac.uk October 7, 2025 Abstract Power transforms are popular parametric techniques for making data more Gaussian-like, and are widely used as preprocessing steps in statistical analysis and machine learning. However, we find that direct implementations of power transforms suffer from severe numerical instabilities, which can lead to incorrect results or even crashes. In this paper, we provide comprehensive analysis of the sources of these instabilities and propose effective remedies. We further extend power transforms to the federated learning setting, addressing both numerical and distributional challenges that arise in this context. Experiments on real-world datasets demonstrate that our methods are both effective and robust, substantially improving stability compared to existing approaches."
        },
        {
            "title": "1 Introduction",
            "content": "Power transforms are widely used to stabilize variance and reduce skewness, and are described in textbooks as foundational tools for data preprocessing. Among them, the Box-Cox (Box and Cox, 1964) and Yeo-Johnson (Yeo and Johnson, 2000) transforms are the most prominent, and are frequently employed in statistical analysis and machine learning pipelines (Ruppert, 2001; Fink, 2009). Despite their popularity, directly implementing these mathematical functions leads to serious numerical instability issues, producing erroneous outputs or even causing program crashes. This is not theoretical concern: as we show in Table 1, simple inputs can easily provoke such failures. The issue has been identified in prior work such as the MASS package (Venables and Ripley, 2002), but only partial solutions have been proposed. Recently, this issue was raised by Marchand et al. (2022). Unfortunately, their analysis includes some incorrect claims and unsound solutions. As shown in Figure 3, their method fails to identify optimal parameters and can crash even on simple datasets. very recent study by Barron (2025) also mentions numerical issues, but their remedy relies on simple replacement of the numerical function, which remains insufficient to ensure stability. Consequently, this foundational question has been unanswered until now. In this paper, we provide comprehensive analysis of numerical instabilities in power transforms. Our solution include log-domain computation, careful reformulation of critical expressions, and constraints on extreme parameter values, all of which are essential to ensure robust numerical behavior. Guided by our theoretical analysis, we also construct adversarial datasets that systematically trigger overflows under different floating-point precisions. Such data can occur naturally in real-world settings (e.g., Figure 11 in the Appendix) or be deliberately introduced by adversarial clients in federated learning scenarios. We further extend power transforms to the federated setting, where distributed data introduces unique challenges. We design numerically stable one-pass variance aggregation method for computing the negative log-likelihood (NLL), enabling robust and efficient optimization of transformation parameters across multiple clients with minimal communication overhead. Extensive experiments on real-world datasets validate the effectiveness of our approach. Compared to existing implementations, including those of Marchand et al. (2022), our methods consistently achieve superior stability in both centralized and federated settings. Our contributions are: 1 Figure 1: Transformation functions for different λ. 1. We conduct comprehensive analysis of numerical instabilities in power transforms, together with recipes for constructing adversarial datasets that expose these weaknesses. 2. We propose remedies addressing each source of instability, yielding numerically stable algorithms in both centralized and federated learning settings. 3. We perform extensive experimental validation on real-world datasets, demonstrating the effectiveness and robustness of our methods. The rest of the paper is organized as follows. Section 2 reviews preliminaries. Section 3 explains the root causes of instability, while Section 4 presents our remedies. Section 5 extends the approach to federated learning. Experimental results are presented in Section 6, further issues are discussed in Section 7, and Section 8 concludes."
        },
        {
            "title": "2.1 Power Transform",
            "content": "The Box-Cox (BC) transformation (Box and Cox, 1964) is widely used power transform requiring strictly positive inputs (x > 0): ψBC(λ, x) = (cid:40) xλ1 λ ln if λ = 0, if λ = 0. (1) The Yeo-Johnson (YJ) transformation (Yeo and Johnson, 2000) extends BC to accommodate both positive and negative values: ψYJ(λ, x) = (x+1)λ1 λ ln(x + 1) (x+1)2λ1 λ2 ln(x + 1) if λ = 0, 0, if λ = 0, 0, if λ = 2, < 0, if λ = 2, < 0. (2) The parameter λ controls the shape of the transformation; when λ = 1, the mapping is nonlinear and alters the distribution. Figure 1 shows the transformation curves for various λ. 2 Figure 2: Tree-structured variance aggregation. The parameter λ is typically estimated by minimizing the negative log-likelihood (NLL): NLLBC = (1 λ) NLLYJ = (1 λ) (cid:88) i=1 (cid:88) i=1 ln xi + ln σ2 ψBC , sgn(xi) ln(xi + 1) + 2 ln σ ψYJ , (3) (4) where σ2 ψ = Var[ψ(λ, x)]. Both NLL functions are strictly convex (Kouider and Chen, 1995; Marchand et al., 2022), guaranteeing unique optimum. SciPy (Virtanen et al., 2020) estimates λ using Brents method (Brent, 2013), derivative-free optimizer with superlinear convergence."
        },
        {
            "title": "2.2 Numerically Stable Variance Computation",
            "content": "Variance is computed as 1 data, which is inefficient for large datasets or streaming data. The equivalent one-pass form, i=1(xi x)2, where is the sample mean. This requires two passes over the (cid:80)n 1 (cid:88) i= x2 1 n2 (cid:32) (cid:88) (cid:33)2 xi , i=1 (5) is well known to be numerically unstable (Ling, 1974; Chan et al., 1983). numerically stable one-pass approach (Chan et al., 1982) maintains (n, x, s) triples, where = (cid:80)n x)2. Merging two partial aggregates (n(A), x(A), s(A)) and (n(B), x(B), s(B)) is done as: i=1(xi n(AB) = n(A) + n(B), x(AB) = x(A) + (x(B) x(A)) n(B) n(AB) , s(AB) = s(A) + s(B) + (x(B) x(A))2 n(A)n(B) n(AB) . (6) (7) (8) This method can be applied sequentially, but tree-structured aggregation (Figure 2) minimizes error accumulation and improves numerical stability."
        },
        {
            "title": "2.3 Federated Learning",
            "content": "Federated learning (Kairouz et al., 2021) is distributed paradigm where multiple clients collaboratively train model without sharing raw data, thereby preserving privacy. Each client computes local updates from its own data and transmits them to central server, which aggregates the information to update the global model. Communication efficiency is key concern, as excessive exchanges can be costly; methods such as FedAvg (McMahan et al., 2017) are commonly used to reduce communication overhead. 3 Figure 3: ExpSearch fails to find the true optimum λ. Left: data [0.1, 0.1, 0.1, 0.101], λ 361; Right: data [10, 10, 10, 9.9], λ 358."
        },
        {
            "title": "3 Understanding Numerical Instabilities",
            "content": "This section examines numerical instabilities in power transforms. We begin by explaining why such instabilities matter and how they can disrupt optimization. We then introduce adversarial datasets for testing implementation robustness. Numerical instabilities in power transforms arise in the computation of the NLL functions defined in Equations (3) and (4). These instabilities can disrupt optimization, producing suboptimal or even incorrect results. In practice, this means that the output of the method becomes unusable, causing bugs or failures in downstream tasks. Moreover, these issues cannot be resolved by simple quick fixes, as the instability is fundamental to the underlying computation and requires careful redesign for robust operation. As illustrated in Figure 3, the Exponential Search method (Marchand et al., 2022)1 fails even on simple datasets. The true optimum λ can be quite large in magnitude, causing numerical overflow when evaluating the power functions, e.g., 0.1361 or 10358. As result, ExpSearch will crash and return values that diverge significantly from the true optimum. To better understand these issues, we first summarize key properties of the BC transformation ψ(λ, x) in Theorem 3.1; the proof is intricate and deferred to Appendix A. Analogous results for YJ can be found in Yeo (1997); Yeo and Johnson (2000). Theorem 3.1. The Box-Cox transformation ψ(λ, x) defined in (1) has the following properties: 1. ψ(λ, x) 0 for 1, and ψ(λ, x) < 0 for < 1. 2. ψ(λ, x) is convex in for λ > 1 and concave in for λ < 1. 3. ψ(λ, x) is continuous function of (λ, x). 4. If ψ(k) = kψ(λ, x)/λk then, for 1. ψ(k) = (cid:40) [xλ(ln x)k kψ(k1)]/λ if λ = 0, (ln x)k+1/(k + 1) if λ = 0. ψ(k) is continuous in (λ, x) and ψ(0) ψ(λ, x). 5. ψ(λ, x) is increasing in both λ and x. 6. ψ(λ, x) is convex in λ for > 1 and concave in λ for < 1. 1There is mismatch between the sign formula (Equation 3) and the ExpUpdate method (Algorithm 2) in (Marchand et al., 2022). The sign formula is NLLYJ/λ (not ln LYJ/λ), so = 1 in ExpUpdate should instead be = 1. Alternatively, one could add negative sign in Equation 3 and keep ExpUpdate unchanged. 4 Table 1: Adversarial datasets causing negative or positive overflow under different floating-point precisions. Transf. Overflow"
        },
        {
            "title": "Adversarial data",
            "content": "BC YJ BC YJ BC YJ"
        },
        {
            "title": "Negative\nPositive\nNegative\nPositive",
            "content": "[0.1, 0.1, 0.1, 0.101] [10, 10, 10, 9.9] [-10, -10, -10, -9.9] [10, 10, 10, 9.9] [0.1, 0.1, 0.1, 0.10001] [10, 10, 10, 9.999] [-10, -10, -10, -9.999] [10, 10, 10, 9.999] [0.1, 0.1, 0.1, 0.100001] [10, 10, 10, 9.9999] [-10, -10, -10, -9.9999] [10, 10, 10, 9.9999] λ -361.15 357.55 -391.49 393.49 -35936.9 35933.3 -39524.8 39526. -359353.0 359349.0 -395283.0 395285.0 Extreme Value Max Value (Precision) -3.87e+358 9.96e+354 -1.51e+407 1.51e+407 -2.30e+35932 5.85e+35928 -2.29e+41158 2.292e+41158 -2.74e+359347 6.99e+359343 -6.47e+411640 6.47e+411640 1.80e+308 (Double) 1.19e+4932 (Quadruple) 1.61e+78913 (Octuple) These properties help pinpoint instability sources and guide the construction of adversarial datasets that trigger numerical overflow  (Table 1)  under various floating-point precisions. Section 4 presents numerically stable formulation to address these issues. In summary, there is simple recipe that can lead to numerical overflow: Avoiding Zero Points. By Theorem 3.1.1, ψ(λ, 1) = 0 for all λ. Thus, = 1 should be avoided. Choosing all > 1 ensures ψ(λ, x) > 0 and may lead to positive overflow; similarly, choosing all < 1 can yield negative overflow. Datasets containing both > 1 and < 1 may be less likely to trigger overflow, and constructing simple adversarial examples in this regime is more challenging. Extreme skewness. For λ > 1, ψ(λ, x) is convex and increasing in (Theorem 3.1.2 and 3.1.5), stretching the right tail more than the left. Thus, left-skewed data tends to push λ > 1 toward positive overflow after transformation; conversely, right-skewed data tends to push λ < 1 toward negative overflow. Extreme skewness drives λ far from 1. Small variance. Tightly clustered data also drives λ to extreme values, as the transformation must expand interpoint distances to approach normality. For > 1, ψ(λ, x) is convex and increasing in λ (Theorem 3.1.5 and 3.1.6), so large λ favors positive overflow; for < 1, it is concave and increasing, so small λ favors negative overflow. Combining extreme skewness with small variance (achieved by inserting duplicate values) is particularly effective at producing overflow. As we show experimentally in Section 6, such conditions naturally occur in real datasets or can be deliberately created by adversarial clients to poison training."
        },
        {
            "title": "4 Numerically Stable Power Transform",
            "content": "This section presents numerically stable approach for power transforms, addressing different sources of instability and then introduces remedies. Modern computers use finite-precision floating-point arithmetic (e.g., IEEE 754 standard (IEEE, 2019)), which limits the range of representable values. As shown in Table 1, power transforms can generate extreme values that exceed these limits, resulting in numerical overflow. Therefore, direct computation of the NLL functions is prone to instability. Beyond the representation issue, the choice of optimization method also plays critical role. In particular, derivative-based methods, such as Exponential Search (Marchand et al., 2022), are prone to instability, whereas derivative-free methods (e.g., Brents method) can achieve stable optimization. We detail these observations below. Derivative-based methods are unstable. Consider Exponential Search, which requires the first-order 5 (a) Eq. (10) vs. (11) (b) Eq. (11) vs. (12) Figure 4: Comparison of NLL curves using different equations. Data used: [10, 10, 10, 9.9]. derivative of the NLL function. For the Box-Cox transformation, this derivative is NLLBC λ = 1 σ ψBC (cid:32) (cid:88) i=1 ψBC(λ, xi) ψ(1) BC(λ, xi) 1 n (cid:88) i=1 ψBC(λ, xi) (cid:33) ψ(1) BC(λ, xi) (cid:88) i=1 (cid:88) i=1 ln xi (9) Here, terms such as ψBC(λ, x), ψ(1) computed reliably. Consequently, derivative-based optimization is not numerically stable. BC(λ, x) (Theorem 3.1.4), and σ2 ψBC can easily overflow and cannot be Derivative-free methods are stable. By contrast, derivative-free methods rely only on evaluating the NLL function itself, which can be stabilized. In particular, the NLL function involves only ln σ2 ψ, which can be represented in floating-point format and efficiently computed using log-domain methods (Haberland, 2023) (see Appendix B). Modifying the Variance Computation. While log-domain computation mitigates overflow, additional instabilities remain. Specifically, the computation of ln σ2 ψBC requires careful reformulation: ln σ2 ψBC = ln Var[(xλ 1)/λ] = ln Var(xλ/λ) = ln Var(xλ) 2 ln λ (10) (11) (12) Equation (11) removes the constant 1/λ, which avoids catastrophic cancellation (Weckesser, 2019). For example, with λ < 14 and > 1, we have xλ 0, and the subtraction xλ 1 leads to severe precision loss. Therefore, computing the variance after the transformation becomes very unstable. The same occurs for large λ with < 1. This instability, illustrated in the left panel of Figure 4, shows large fluctuations in the NLL curve when keeping the constant term, which will disrupts optimization whenever the evaluated λ lies in such regions. Further, as λ 0, computing xλ/λ yields extreme values, destabilizing the variance computation. Factoring out λ, as in (12), restores stability. The right panel of Figure 4 demonstrates this improvement, where the NLL curve becomes smooth after factoring out λ. Bounding extreme values. Finally, the optimal λ may still yield extreme transformed values  (Table 1)  . To prevent this, we impose constraints during optimization. Since the transformation is monotone in both λ and (Theorem 3.1.5), we restrict the transformed data to lie within [yB, yB] by bounding λ according to xmax and xmin: min λ s.t. NLLBC(λ, x) λ ψ1 λ ψ1 BC(xmax, yB) if xmax > 1, BC(xmin, yB) if xmin < 1. (13) Figure 5: Comparison of NLL curves using the naive and pairwise aggregation. Data are synthetic Gaussian samples from (104, 103) with 100 points. Here, ψ1 BC is the inverse Box-Cox transform, expressed via the Lambert function (Corless et al., 1996): ψ1 BC(x, y) = 1 ln (cid:16) x1/y ln (cid:17) . (14) Since the Lambert function has two real branches (k = 0 for (x) 1, and = 1 for (x) 1), we use the = 1 branch in overflow cases (x > 1, λ 1 or < 1, λ 1), where (cid:16) x1/y ln (cid:17) = (λ + ) ln λ ln = ln(xλ) 1. (15) (16) The same approach applies to the Yeo-Johnson transformation (see Appendix C)."
        },
        {
            "title": "5 Federated Power Transform",
            "content": "This section extends power transforms to the federated learning setting. We begin by explaining the challenges of federated NLL evaluation, then show how textbook variance computation can cause numerical instability in this context. Finally, we introduce numerically stable variance aggregation method that enables reliable NLL evaluation under federation. In federated learning, the objective is to find the global optimum λ that minimizes the NLL across multiple clients, each holding its own local dataset. Standard federated optimization methods such as FedAvg do not apply here: each client may have different local optimum λ , and averaging them does not in general recover the global optimum. For heterogeneous data distributions, the averaged value may diverge significantly from the true global solution. Consequently, the server must evaluate the NLL function on aggregated statistics and apply derivative-free optimizer (e.g., Brents method) to locate the global optimum. To reduce communication rounds, one-pass variance computation is preferred. However, this approach introduces severe numerical instabilities, which directly affect both NLL evaluation and the optimization of λ. To illustrate this, we compare two approaches: the naive one-pass method (Equation (5), also used in Marchand et al. (2022)) and our numerically stable aggregation procedure (Algorithm 1). As shown in Figure 5, the naive method produces large fluctuations in the NLL curve, disrupting optimization. In contrast, our stable approach yields smooth curves and enables reliable optimization. For numerically stable federated NLL computation with Box-Cox, each client sends four values to the server: the local sum c, sample size n, mean y, and sum of squared deviations for the transformed data (see line 5 in client part). The server aggregates these (n, y, s) triplets using the queue-based procedure in Algorithm 1 (line 4 in server part), which ensures numerical stability compared to naive sequential aggregation. 7 Algorithm 1 Variance Aggregation 1: Input: Queue containing (n(j), y(j), s(j)) 2: while > 1 do 3: 4: 5: 6: end while 7: return the final (N, , S) Dequeue (n(A), y(A), s(A)), (n(B), y(B), s(B)) Compute (n(AB), y(AB), s(AB)) using Pairwise Enqueue (n(AB), y(AB), s(AB)) back into Pairwise: 1: Input: (n(A), y(A), s(A)) and (n(B), y(B), s(B)) 2: n(AB) n(A) + n(B) 3: δ y(B) y(A) 4: y(AB) y(A) + δ n(B) n(AB) 5: s(AB) s(A) + s(B) + δ2 n(A)n(B) n(AB) 6: return (n(AB), y(AB), s(AB)) Algorithm 2 Federated NLL Comptation (Box-Cox) Client Part: 1: Input: λ and local data xi (size n) 2: (cid:80) (cid:40) ln xi xλ ln xi (cid:80) 3: yi if λ = 0, if λ = 0. yi and (cid:80) 4: 1 5: Send: (c, n, y, s) to server i(yi y)2 Server Part: 1: Input: λ 2: Collect (c(j), n(j), y(j), s(j)) from clients 3: Enqueue (n(j), y(j), s(j)) into 4: Compute (N, , S) from using Algorithm 1 5: if λ = 0 then 6: 7: end if 8: ln σ2 ψ ln ln 9: NLLBC (1 λ) (cid:80) 10: return NLLBC ln ln 2 ln λ c(j) + 2 ln σ2 ψ Table 2: Dataset statistics. Datasets # Row # Col"
        },
        {
            "title": "Adult\nBank\nCredit",
            "content": "33K 45K 30K 14 16 24 Figure 6: ROC curves after applying different transforms (LDA). We also incorporate the numerically stable strategies from Section 4, namely log-domain computation and modified variance formulations (see line 3 in client part and line 6 in server part). The case of Yeo-Johnson is more involved, since it handles both positive and negative inputs; details are deferred to Appendix D."
        },
        {
            "title": "6 Empirical Evaluation",
            "content": "Our experiments contain two parts: (1) evaluating the effect of power transforms on downstream tasks, and (2) testing the numerical stability of our methods. Code is available at https://github.com/xuefeng-x u/powertf. All experiments were performed on an Apple MacBook M3 (16GB RAM), completing within 1 hour."
        },
        {
            "title": "6.1 Downstream Effectiveness",
            "content": "We first evaluate the impact of power transforms on downstream classification tasks using three datasets: Adult (Becker and Kohavi, 1996), Bank (Moro et al., 2012), and Credit (Yeh, 2009). Dataset statistics are shown in Table 2. Each dataset is split into 80% training and 20% testing. Since features may contain negative values, we apply the Yeo-Johnson transformation to each feature, estimating λ from the training set. We compare against two baselines: (1) Standardization (STD: zero mean and unit variance), and (2) Raw data without any transformation. We then train three classifiers on the transformed features: Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Logistic Regression (LR). Since LDA and QDA assume normally distributed inputs, power transforms are expected to improve performance. Figures 6 shows ROC curves and AUC scores on test set (additional plots are in Figure 9 in the Appendix). Power transforms consistently outperform the baselines, although the improvements are modest. recent study by Eftekhari and Papyan (2025) applied power transforms to hidden layers of deep neural networks, showing that improved Gaussianity leads to higher classification accuracy. This further confirms the effectiveness of power transforms. We next study the effect of varying λ on AUC scores. This tests whether the estimated λ is indeed optimal for downstream tasks. Since each dataset has multiple features, we select the feature with the highest 9 Figure 7: Effect of varying λ on AUC scores (LDA). Table 3: Dataset statistics and unstable features detected. Datasets # Row # Col # Inst. Transf. ExpSearch Linear Blood Cancer Ecoli 748 569 336 House 1460 31 8 80 1 3 3 BC YJ BC YJ BC YJ BC YJ lip, chg YearRemodAdd, YrSold Street, YearRemodAdd, YrSold Monetary ID, area1, area3 lip, chg YrSold YrSold mutual information with the label and ignore other features, apply the Yeo-Johnson transform with varying λ. Figures 7 shows AUC as function of λ (additional results are in Figure 10 in the Appendix). Interestingly, λ (marked with vertical dashed line) does not always yield the absolute highest AUC (e.g., Adult dataset). However, it consistently provides competitive results across datasets, while deviating from it often reduces performance. This highlights the importance of numerically stable estimation of λ, as instability can significantly degrade downstream outcomes."
        },
        {
            "title": "6.2 Numerical Experiments",
            "content": "We now test numerical stability of power transforms on four datasets: Blood (Yeh, 2008), Cancer (Wolberg et al., 1995), Ecoli (Nakai, 1996), and House (Montoya and DataCanary, 2016). The first three were previously identified by Marchand et al. (2022) as unstable cases; House is new addition with features exhibiting extreme skewness. We benchmark three methods: (1) ExpSearch, derivative-based optimization method (Marchand et al., 2022) vs. our derivative-free approach (based on Brents method); (2) Linear-domain computation with our proposed log-domain method. (3) Naive one-pass variance computation vs. our pairwise computation in federated learning. Table 3 summarizes dataset statistics and nine features we identified with numerical issues. Histograms in Figure 11 (in the Appendix) show these features are highly skewed, sometimes binary, and often extremely imbalanced. These match our analysis in Section 3, and align with our recipe for constructing adversarial datasets  (Table 1)  . Importantly, this shows that such pathological data naturally occur in practice, underscoring the need for stability in power transforms. Next, we observe that seven features cause ExpSearch to fail. Instead of finding λ, it returns either boundary of the search interval or an arbitrary value. Figures 8 and 12 (in the Appendix) illustrate this. The left plot shows the true NLL curve and the optimum λ (found by Brents method). ExpSearchs λ is far 10 Figure 8: Comparison of Brents method and ExpSearch. from optimal. The middle and right plots show the modified derivative used by ExpSearch versus the true derivative. Both are either unstable (returning arbitrary values) or overflow (returning boundary solutions). This explains ExpSearchs failures. We then compare linear-domain versus log-domain computation. Eight features exhibit failures in the linear domain. Figure 13 (in the Appendix) shows that linear-domain NLL curves often overflow, preventing discovery of the optimum if λ lies in the overflow region. By contrast, log-domain computation yields smooth and stable curves, reliably identifying λ. This confirms the robustness of our approach. Finally, we evaluate federated NLL computation. We split each dataset into 100 clients and compare our variance aggregation method with naive one-pass variance computation. Figure 14 (in the Appendix) shows that our method produces smooth NLL curves, while the naive method introduces spikes that can disrupt optimization. This highlights the necessity of numerically stable aggregation in federated settings."
        },
        {
            "title": "7 Discussion",
            "content": "Communication cost is common concern in federated learning, consisting of two components: (1) the size of each message per round and (2) the total number of communication rounds. In our method, each client only needs to send four numbers to the server, while the server sends back single number, making the per-round communication negligible. For the number of rounds, we adopt Brents method, which converges superlinearly. In practice, convergence typically requires 20-30 rounds, depending on the dataset, which is acceptable in most settings. One possible extension is to reduce the number of rounds by increasing message size, since the server can evaluate the NLL at multiple points per round. For example, after identifying bracket that contains the minimum, grid search could be applied. As shown in Figure 15 (in the Appendix), communication rounds can be reduced to under 10 with grid size of 1K. This trade-off between message size and number of rounds can be tuned for specific applications. Privacy is key concern in federated learning. Approaches such as Secure Aggregation (Bonawitz et al., 2017), Trusted Execution Environments (TEE) (Sabt et al., 2015), and Secure Multiparty Computation (SMPC) (Lindell, 2020) can ensure that only aggregated statistics are revealed to the server, not individual client data. Our method uses pairwise variance aggregation for numerical stability. To integrate with SMPC, the division operations in Equations (7) and (8) can be handled by treating the sample count as public value, which is standard in federated learning and also adopted by Marchand et al. (2022). Preserving privacy during iterative optimization without revealing intermediate results in each round is stronger requirement and we leave it for future work."
        },
        {
            "title": "8 Conclusion",
            "content": "Numerical issues have long been challenge in scientific computing, as mathematically equivalent expressions can yield vastly different results under finite-precision arithmetic. In this paper, we addressed the numerical instability of power transforms, widely used technique for data normalization. We conducted detailed analysis of the sources of instability and proposed numerically stable approachs that combines log-domain computation, reformulated expressions, and bounding strategies. We further extended power transforms to the federated learning setting, introducing numerically stable variance aggregation method suitable for distributed data. Our empirical results demonstrate the effectiveness and robustness of our methods in both centralized and federated scenarios. We believe that our work not only makes power transforms more reliable in practice, but also provides insights that can be applied to broader range of numerical stability problems in scientific computing."
        },
        {
            "title": "References",
            "content": "Barron, J. T. (2025). power transform. CoRR, abs/2502.10647. Becker, B. and Kohavi, R. (1996). Adult. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5XW20. Bonawitz, K. A., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H. B., Patel, S., Ramage, D., Segal, A., and Seth, K. (2017). Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS 2017, Dallas, TX, USA, October 30 - November 03, 2017, pages 11751191. ACM. Box, G. E. P. and Cox, D. R. (1964). An analysis of transformations. Journal of the Royal Statistical Society: Series (Methodological), 26(2):211243. Brent, R. P. (2013). Algorithms for Minimization Without Derivatives. Dover Publications, Incorporated. Chan, T. F., Golub, G. H., and LeVeque, R. J. (1982). Updating formulae and pairwise algorithm for computing sample variances. In COMPSTAT 1982 5th Symposium held at Toulouse 1982, pages 3041, Heidelberg. Physica-Verlag HD. Chan, T. F., Golub, G. H., and Leveque, R. J. (1983). Algorithms for computing the sample variance: Analysis and recommendations. The American Statistician, 37(3):242247. Corless, R. M., Gonnet, G. H., Hare, D. E. G., Jeffrey, D. J., and Knuth, D. E. (1996). On the lambertw function. Advances in Computational Mathematics, 5(1):329359. Eftekhari, D. and Papyan, V. (2025). On the importance of gaussianizing representations. In Forty-second International Conference on Machine Learning. Fink, E. L. (2009). The faqs on data transformation. Communication Monographs, 76(4):379397. Haberland, M. (2023). Bug: fix overflow in stats.yeojohnson. https://github.com/scipy/scipy/pull/18 852#issuecomment-1657858886. Accessed: August 14, 2025. IEEE (2019). Ieee standard for floating-point arithmetic. IEEE Std 754-2019 (Revision of IEEE 754-2008), pages 184. Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Nitin Bhagoji, A., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., DOliveira, R. G. L., Eichner, H., El Rouayheb, S., Evans, D., Gardner, J., Garrett, Z., Gascon, A., Ghazi, B., Gibbons, P. B., Gruteser, M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., Konecny, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri, M., Nock, R., Ozgur, A., Pagh, R., Qi, H., Ramage, D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S. U., Sun, Z., Suresh, A. T., Tram`er, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F. X., Yu, H., and Zhao, S. (2021). Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 14(12):1210. 12 Kouider, E. and Chen, H. (1995). Concavity of box-cox log-likelihood function. Statistics & Probability Letters, 25(2):171175. Lindell, Y. (2020). Secure multiparty computation. Communications of the ACM, 64(1):8696. Ling, R. F. (1974). Comparison of several algorithms for computing sample means and variances. Journal of the American Statistical Association, 69(348):859866. Marchand, T., Muzellec, B., Beguier, C., Ogier du Terrail, J., and Andreux, M. (2022). Securefedyj: safe feature gaussianization protocol for federated learning. In Advances in Neural Information Processing Systems, volume 35, pages 3658536598. Curran Associates, Inc. McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. y. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 12731282. PMLR. Montoya, A. and DataCanary (2016). House prices - advanced regression techniques. https://kaggle.com /competitions/house-prices-advanced-regression-techniques. Kaggle. Moro, S., Rita, P., and Cortez, P. (2012). Bank Marketing. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5K306. Nakai, K. (1996). Ecoli. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5388M. Ruppert, D. (2001). Statistical analysis, special problems of: Transformations of data. In Smelser, N. J. and Baltes, P. B., editors, International Encyclopedia of the Social & Behavioral Sciences, pages 1500715014. Pergamon, Oxford. Sabt, M., Achemlal, M., and Bouabdallah, A. (2015). Trusted execution environment: What it is, and what it is not. In 2015 IEEE Trustcom/BigDataSE/ISPA, volume 1, pages 5764. Venables, W. N. and Ripley, B. D. (2002). Modern Applied Statistics with S. Springer, New York, fourth edition. ISBN 0-387-95457-0. Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy 1.0 Contributors (2020). SciPy 1.0: Fundamental algorithms for scientific computing in python. Nature Methods, 17(3):261272. Weckesser, W. (2019). Bug: stats: Fix boxcox llf to avoid loss of precision. https://github.com/scipy/s cipy/pull/10072. Accessed: August 14, 2025. Wolberg, W., Mangasarian, O., Street, N., and Street, W. (1995). Breast Cancer Wisconsin (Diagnostic). UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5DW2B. Yeh, I.-C. (2008). Blood Transfusion Service Center. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5GS39. Yeh, I.-C. (2009). Default of Credit Card Clients. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C55S3H. Yeo, I. and Johnson, R. A. (2000). new family of power transformations to improve normality or symmetry. Biometrika, 87(4):954959. Yeo, I.-K. (1997). new family of power transformations to reduce skewness or approximate normality. PhD thesis, The University of Wisconsin-Madison. 13 Proof of Theorem 3.1 1. For 1, we have (cid:40) xλ 1 0 xλ 1 0 if λ > 0, if λ < 0. (17) When λ = 0, ln 0 for 1. Hence ψ(λ, x) 0 for all λ whenever 1. Similarly, for 0 < < 1, we have (cid:40) xλ 1 < 0 xλ 1 > 0 if λ > 0, if λ < 0. (18) (19) When λ = 0, ln < 0 for 0 < < 1. Hence ψ(λ, x) < 0 for all λ whenever 0 < < 1. 2. The second order partial derivative of ψ with respect to is (cid:40) 2ψ(λ, x) x2 = (λ 1)xλ2 1/x2 if λ = 0, if λ = 0. Therefore, 2ψ(λ,x) x2 > 0 when λ > 1 and 2ψ(λ,x) x2 < 0 when λ < 1. 3. Its clear that ψ(λ, x) is continuous for λ and except λ = 0. We just need to prove its continuous at λ = 0. By LHopitals rule, we have xλ 1 λ lim λ0 = lim λ0 xλ ln 1 = ln 4. We prove this by induction. Let = 1, then for λ = 0 ψ(1)(λ, x) = xλλ ln (xλ 1) λ2 = xλ ln ψ(0)(λ, x) λ For λ = 0, by LHopitals rule, we have ψ(1)(0, x) = lim λ0 ψ(1)(λ, x) = lim λ0 xλλ ln xλ + 1 λ2 xλ(ln x)2 = lim 2 λ0 = (ln x)2/2 Assume that this hold for = where 1, then for = + 1 and λ = 0 ψ(n+1)(λ, x) = = = xλ(ln x)n nψ(n1)(λ, x) λ λ [xλ(ln x)n+1 nψ(n)(λ, x)]λ [xλ(ln x)n nψ(n1)(λ, x)] λ2 xλ(ln x)n+1 (n + 1)ψ(n)(λ, x) λ For λ = 0, by LHopitals rule, we have ψ(n+1)(0, x) = lim λ0 ψ(n+1)(λ, x) xλ(ln x)n+1 (n + 1)ψ(n)(λ, x) λ xλ(ln x)n+2 (n + 1)ψ(n+1)(λ, x) = lim λ0 = lim λ0 = (ln x)n+2 (n + 1) lim λ0 ψ(n+1)(λ, x) 14 (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) Since (29) is equal to (32), ψ(n+1)(0, x) = limλ0 ψ(n+1)(λ, x) = (ln x)n+2/(n+2). Thus, the recurrence relation holds for all 1 and λ. 5. The partial derivative of ψ with respect to is ψ(λ, x) = (cid:40) xλ1 1/x if λ = 0, if λ = 0. so ψ(λ,x) > 0. Therefore, ψ is increasing in x. The partial derivative of ψ with respect to λ is ψ(λ, x) λ = (cid:40) xλ(ln xλ1)+1 λ2 (ln x)2/2 if λ = 0, if λ = 0. (33) (34) Let = xλ > 0 and f1(y) = y(ln 1) + 1, we have unique minimum at = 1 and f1(y) > f1(1) = 0. Thus ψ(λ,x) 1(y) = ln y, 1 (y) = 1/y > 0. Thus f1(y) has the λ > 0. Therefore, ψ is increasing in λ. 6. The second order partial derivative of ψ with respect to λ is 2ψ(λ, x) λ2 = (cid:40) xλ[(ln xλ)22 ln xλ+2]2 λ3 (ln x)3/3 if λ = 0, if λ = 0. (35) Let = xλ > 0 and f2(y) = y[(ln y)2 2 ln + 2] 2, we have f2(y) > 0 when > 1 and f2(y) < 0 when < 1 since f2(y) is increasing in y. The relationship between x, λ and y, f2(y) are as follows 2(y) = (ln y)2 > 0 and f2(1) = 0. Thus > 1, λ > 0 > 1, f2(y) > 0 > 1, λ < 0 < 1, f2(y) < 0 0 < < 1, λ < 0 > 1, f2(y) > 0 0 < < 1, λ > 0 < 1, f2(y) < 0 (cid:41) (cid:41) f2(y)/λ3 > 0 f2(y)/λ3 < (36) Therefore, 2ψ(λ,x) λ2 > 0 when > 1 and 2ψ(λ,x) λ2 < 0 when 0 < < 1. Log-domain Computation Log-domain computation refers to performing numerical operations in the logarithmic space rather than on raw values. This technique is particularly effective at avoiding numerical overflow and underflow, especially when working with exponential functions, as in the case of power transforms. Central to this approach is the Log-Sum-Exp (LSE) function: LSE(x1, . . . , xn) = ln (cid:88) i=1 exp(xi) = ln (cid:88) i= exp(xi c) + (37) where = maxi xi. This ensures numerically stable computation by shifting values before exponentiation. Using the LSE operator, the logarithmic mean is computed as ln µ = ln (cid:32) 1 (cid:88) i=1 (cid:33) xi = LSE(ln x1, . . . , ln xn) ln Similarly, the logarithmic variance is expressed as ln σ2 = ln (cid:88) (xi µ)2 ln = LSE (2 ln(x1 µ), . . . , 2 ln(xn µ)) ln i=1 (38) (39) 15 The computation of ln(xi µ) requires extra care. stable way is to rewrite the difference using the LSE trick (Haberland, 2023): ln(xi µ) = ln (exp(ln xi) + exp(ln µ + πi)) = LSE(ln xi, ln µ + πi) (40) where the πi term is the imaginary part that handles sign differences for negative values. Numerically Stable Yeo-Johnson To extend numerical stability to the Yeo-Johnson transformation, we must handle both positive and negative inputs. Unlike Box-Cox, the constant term cannot be eliminated when both positive and negative values are present. We therefore consider the following two cases separately: 1. Data entirely positive (or entirely negative): Apply log-domain computation as in Box-Cox, removing the constant term and factoring out λ (for positive) or (λ 2) (for negative). 2. Data contains both positive and negative values: Use the full piecewise definition of Yeo-Johnson (Equation (2)) with log-domain computation, but do not remove the constant term. In practice, this case does not exhibit instability. As in the Box-Cox case, extreme values of λ can be constrained during optimization. Here, we use = 0 as the reference point since ψYJ(λ, 0) = 0: min λ s.t. NLLYJ(λ, x) λ ψ1 λ ψ1 YJ (xmax, yB) if xmax > 0, YJ (xmin, yB) if xmin < 0. Here ψ1 YJ is the inverse Yeo-Johnson transformation, expressed using the Lambert function: ψ1 YJ (x, y) = 1 1 ln(x+1) (cid:16) 2 1 + 1 ln(1x) (cid:17) (x+1)1/y ln(x+1) (cid:17) (cid:16) (1x)1/y ln(1x) if 0, if < 0. For overflow cases, we employ the = 1 branch of the Lambert function. As for > 0 and λ 1: (cid:16) (x+1)1/y ln(x+1) (cid:17) = (λ + 1 ) ln(x + 1) λ ln(x + 1) 1. For < 0 and λ 1: (cid:16) (1x)1/y ln(1x) (cid:17) = (λ + 1 2) ln(1 x) (λ 2) ln(1 x) 1. (41) (42) (43) (44) Federated NLL Computation for Yeo-Johnson In the federated setting, the Yeo-Johnson transformation requires additional care due to its piecewise definition for positive and negative inputs. The main challenges are: (1) clients need to use different formulas to achieve numerical stability based on their local data (all-positive, all-negative, or mixed); (2) the server must correctly aggregate statistics from these heterogeneous clients. First, instead of reporting only the total number of samples, each client separately transmits the counts of positive and negative values. This enables the server to identify whether the clients dataset is all-positive, all-negative, or mixed, and to aggregate contributions accordingly. Second, depending on the case, clients apply different formulas for the transformed data. In the allpositive (or all-negative) case, constant term can be safely omitted. For mixed data, however, the full piecewise definition of the transform function must be used to preserve correctness of the variance. Finally, the server aggregates the statistics and computes the variance of the transformed data. It then applies correction step to adjust the mean, compensating for constant term that clients may have omitted. The complete procedure is summarized in Algorithm 3. 16 All positive All negative Mixed data All positive All negative Mixed data Algorithm 3 Federated NLL Computation (Yeo-Johnson) Client Part: 1: Input: λ and local data xi (size n, with positive size n+ and negative size n) 2: (cid:80) 3: if = 0 then sgn(xi) ln(xi + 1) (cid:40) 4: yi (xi + 1)λ ln(xi + 1) 5: else if n+ = 0 then (cid:40) 6: yi if λ = 0, if λ = 0. (xi + 1)2λ ln(xi + 1) if λ = 2, if λ = 2. yi ψYJ(λ, xi) (Equation (2)) 7: else 8: 9: end if 10: 1 11: Send: (c, n+, n, y, s) to server yi and (cid:80) i(yi y)2 (cid:80) Server Part: ln S+ ln S+ 2 ln λ Enqueue (n+(j), y(j), s(j)) into Q+ Enqueue (n(j), y(j), s(j)) into Enqueue (n+(j) + n(j), y(j), s(j)) into 1: Input: λ 2: Collect (c(j), n+(j), n(j), y(j), s(j)) from clients 3: if n(j) = 0 then 4: 5: else if n+(j) = 0 then 6: 7: else 8: 9: end if 10: Compute (N +, +, S+) from Q+ using Algorithm 1 11: if λ = 0 then 12: 13: end if 14: Compute (N , , S) from using Algorithm 1 15: if λ = 2 then 16: 17: end if 18: Compute (N , , S) from using Algorithm 1 19: if (N > 0 or > 0) and λ = 0 then 20: 21: else if (N + > 0 or > 0) and λ = 2 then 22: 23: end if 24: Enqueue (N +, +, S+), (N , , S), (N , , S) into 25: Compute (N, , S) from using Algorithm 1 26: ln σ2 ψ ln ln 27: NLLYJ (1 λ) (cid:80) 28: return NLLYJ ln ln 2 ln 2 λ ( 1)/(λ 2) + ( + 1)/λ c(j) + 2 ln σ2 ψ 17 Add constant term back for mixed data"
        },
        {
            "title": "E Supplementary Plots",
            "content": "Figure 9: ROC curves after applying different transforms (QDA and LR). 18 Figure 10: Effect of varying λ on AUC scores (QDA and LR). 19 Figure 11: Histogram of features that have numerical issues. Figure 12: Comparison of Brents method and ExpSearch. 21 Figure 12: Comparison of Brents method and ExpSearch. 22 Figure 13: Comparison of log and linear domain computation. Figure 14: Comparison of pairwise and naive variance aggregation. 24 Figure 15: Number of communication rounds vs. grid size."
        }
    ],
    "affiliations": [
        "University of Warwick"
    ]
}