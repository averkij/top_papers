{
    "paper_title": "VGGT-X: When VGGT Meets Dense Novel View Synthesis",
    "authors": [
        "Yang Liu",
        "Chuanchen Luo",
        "Zimo Tang",
        "Junran Peng",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 9 1 5 2 . 9 0 5 2 : r Preprint. Under review. VGGT-X: WHEN VGGT MEETS DENSE NOVEL VIEW SYNTHESIS Yang Liu1,2, Chuanchen Luo4, Zimo Tang3, Junran Peng5 (cid:0), & Zhaoxiang Zhang 1,2 (cid:0) 1 NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences 2 University of Chinese Academy of Sciences 3 Huazhong University of Science and Technology 4 Shandong University {liuyang2022, zhaoxiang.zhang}@ia.ac.cn, u202315173@hust.edu.cn chuanchen.luo@sdu.edu.cn, jrpeng4ever@126.com 5 University of Science and Technology Beijing Figure 1: Reconstruction and Novel View Synthesis results. In part (a), we extend VGGT to handle dense multi-view inputs and incorporate an efficient global alignment, yielding highly accurate predictions. Part (b) demonstrates that eliminating redundant VRAM usage enables inference throughput over 1000 images without compromising performance. The VGGT here denotes VGGT with the elimination of redundant intermediate features. Finally, part (c) illustrates that, with an appropriate joint pose and 3DGS optimization strategy, photorealistic rendering can be realized."
        },
        {
            "title": "ABSTRACT",
            "content": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structurefrom-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveal that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initializationsensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-theart results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github. io/vggt-x.github.io/. 1 Preprint. Under review."
        },
        {
            "title": "INTRODUCTION",
            "content": "Novel View Synthesis (NVS) reconstructs 3D scene from multi-view images to render photorealistic novel views. Implicit representations like Neural Radiance Fields (NeRF) (Mildenhall et al., 2021) set new standard in rendering fidelity, while recent explicit 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) revolutionizes the area by enabling realistic rendering with real-time speed. Both families, however, typically require tens of minutes to train and depend on accurate initialization from external sensors or reconstruction pipelines (e.g., COLMAP (Schonberger & Frahm, 2016)), which incurs expensive hardware or additional minutes-to-hours overhead (Li et al., 2025). Recent 3D Foundation Models (3DFMs) offer promising alternative by dramatically accelerating components of the pipeline. For example, VGGT can infer camera poses and depth for 200 images in 10s (Wang et al., 2025a), and Anysplat produces 3DGS from 64 views in 5s (Jiang et al., 2025a), suggesting orders-of-magnitude speedups over classic pipelines. Yet these methods are largely demonstrated in sparse-view regimes (tens of images), leaving open the question: what would happen if 3DFMs are applied to dense NVS? This work investigates applying 3DFMs to dense NVS and identifies two central obstacles. First, 3DFM computation and memory cost increase dramatically with the number of views (e.g., VRAM of VGGT rises from 5.6 GB to 40.6 GB when input rises from 20 to 200 views (Wang et al., 2025a)), making direct dense inference of 3DGS properties infeasible on commercial GPUs. Second, even when used as drop-in replacement for traditional reconstruction pipelines like COLMAP, 3DFM outputs exhibit higher noise levels. Such noise undermines the learning of initialization-sensitive 3D primitives and leads to significant degradation in rendering quality. To remove the obstacles and explore the answer to the question, we take VGGT (Wang et al., 2025a) as representative 3DFM and pursue two directions. On the 3DFM side, we remove redundant feature caching, reduce numeric precision, and adopt batched frame-wise operations to losslessly scale VGGT inference to 1,000+ images (see part (b) of Fig. 1). On the 3DGS side, we study the effect of initializing 3DGS with VGGT outputs. Tab. 4 shows substantial degradation under naıve initialization. We further investigate whether the mitigation strategy exists. We propose an efficient adaptive global alignment under epipolar constraints to refine VGGT predictions. Besides, we adopt MCMC-3DGS (Kheradmand et al., 2024) and joint pose optimization to increase robustness to noisy initialization, along with point-cloud initialization strategy through comparative analysis. Through these approaches, we largely mitigate the fidelity gap and obtain state-of-the-art rendering under COLMAP-free settings. We also analyze remaining discrepancies with COLMAP-initialized training, including overfitting and generalization problems, and offer concrete directions for stronger 3DFMs and more robust NVS training. In summary, our contributions are fourfold: We identify and analyze the key problems that prevent current 3DFMs from scaling to dense NVS. We explore and reveal how the key problems can be alleviated by introducing VGGT-X, memory-efficient VGGT implementation combined with an adaptive global alignment and 3DGS training practices tailored to imperfect initialization. We analyze the residual gap to COLMAP-initialized pipelines and provide insights to strengthen future 3DFMs and NVS training. Extensive experiments confirm our state-of-the-art performance in both pose estimation and COLMAP-free NVS."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 NOVEL VIEW SYNTHESIS Novel view synthesis (NVS) seeks to generate photorealistic images from novel viewpoints given set of input images captured from different perspectives of 3D scene. This task fundamentally relies on reconstructing faithful 3D representation of the scene. landmark in this field is Neural Radiance Fields (NeRF) (Mildenhall et al., 2021), which employs multi-layer perceptrons (MLPs) 2 Preprint. Under review. to implicitly encode scene geometry and appearance. Subsequent works have advanced NeRF along multiple directions, including improved reflectance modeling (Verbin et al., 2022; Attal et al., 2023), anti-aliasing techniques (Barron et al., 2021; 2022), and acceleration of both training and inference (Zhang et al., 2023; Muller et al., 2022; Yu et al., 2021). More recently, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has emerged as powerful alternative, offering substantial efficiency gains while preserving high rendering quality. Building on this foundation, recent research has extended 3DGS to large-scale scene reconstruction (Lin et al., 2024; Liu et al., 2024; 2025), compact storage and transmission (Fan et al., 2024a; Lee et al., 2024), and artifact mitigation (Yu et al., 2024; Ye et al., 2024; Radl et al., 2024). Despite these advances, NVS methods still require accurate camera parameters, and 3DGS in particular remains highly sensitive to the quality of the initial point cloud. Inaccurate poses or noisy geometry often result in visual artifacts and geometric misalignments in the synthesized views. 2. 3D FOUNDATION MODELS 3D foundation models aim to infer fundamental 3D attributessuch as camera parameters, point clouds, depth maps, point tracks, or even neural radiance fieldsdirectly from image collections. Current approaches are broadly instantiated through two architectural paradigms: diffusion-based models (Ho et al., 2020) and feed-forward ViT-based models (Dosovitskiy et al., 2021). Based on input types, the 3D foundation models can be categorized into 4 types (Cong et al., 2025). (i) For uncalibrated image pairs, DUSt3R (Wang et al., 2024b) and its successors (Leroy et al., 2024; Fan et al., 2024b; Zhang et al., 2025a; Ye et al., 2025; Lu et al., 2025; Smart et al., 2024; Chen et al., 2025) predict point clouds (with auxiliary properties such as confidence) within the coordinate frame of the first camera. Through additional correspondence matching and reprojection loss optimization, these local geometries can be aligned into consistent global frame (Duisterhof et al., 2025). (ii) For unordered multi-view image collections, models such as (Yang et al., 2025; Wang et al., 2025a;c; Fang et al., 2025) employ interand intra-view cross-attention to directly produce globally consistent poses and geometry. (iii) For image streams, models like Spann3R (Wang & Agapito, 2025) and CUT3R (Wang et al., 2025b) predict next-frame geometry by leveraging current features and temporal memory, while diffusion-based approaches (Team et al., 2025; Jiang et al., 2025b; Xu et al., 2025) cast geometry estimation as conditional generative process. (iv) For uncalibrated sparse views, FLARE (Zhang et al., 2025b) adopts cascaded, feed-forward pipeline that first regresses camera poses and then conditions global geometry and appearance estimation. Despite rapid progress, most existing models incur substantial computational overhead and exhibit degraded performance when scaled to hundreds or thousands of images. Our study aims to address this gap and provide new insights into the development of scalable 3D foundation models. 2.3 3D RADIANCE FIELD LEARNING WITH POSE OPTIMIZATION To mitigate the dependence on accurate camera poses, recent NVS approaches have explored variety of strategies. widely adopted solution is the joint optimization of camera parameters alongside the neural radiance field, often complemented by multi-view correspondence losses (Wang et al., 2021; Jeong et al., 2021). Methods such as NoPe-NeRF (Bian et al., 2023) and SPARF (Truong et al., 2023) incorporate depth supervision, whereas (Bian et al., 2024; Huang et al., 2025b) employ MLPs to regress pose updates, enhancing robustness and exploiting global scene context. NeRF-based techniques further investigate strategies to mitigate sub-optimal convergence caused by high-frequency positional embeddings (Lin et al., 2021; Chng et al., 2022; Xia et al., 2022). In the context of 3DGS, MCMC-3DGS (Kheradmand et al., 2024) enhances robustness to initialization by reformulating the Gaussian Splatting update mechanism, while (Fu et al., 2024; Chen et al., 2024; Ji & Yao, 2025) perform incremental local geometry reconstruction and pose refinement for unposed image sequences. More recently, approaches leveraging 3D foundation models or tracking models (Huang et al., 2025a;b; Shi et al., 2025) have been proposed to efficiently obtain high-quality initializations of poses and geometry. Despite these advances, notable performance gap remains compared to COLMAP-initialized optimization, and scaling these methods to large image collections remains largely unexplored. Our work aims to advance this frontier, providing insights into training photorealistic neural radiance fields from imperfectly registered poses and point clouds. Preprint. Under review."
        },
        {
            "title": "3.1 PRELIMINARY",
            "content": "Figure 2: Overall pipeline of our model. 3D Gaussian Splatting (Kerbl et al., 2023) models 3D scene using collection of ellipsoids parameterized by 3D Gaussian distributions, i.e., = {Gi = 1, . . . , NG}. Each Gaussian is associated with learnable attributes, including its center µi R31, covariance matrix Σi R33, opacity σi [0, 1], and spherical harmonics (SH) features fi R316 for view-dependent appearance modeling. The covariance matrix is further decomposed into scaling matrix Si and rotation . For given pixel p, the color cp is obtained via alpha matrix Ri, such that Σi = RiSiSi blending. Given ground-truth image I, the optimization of 3DGS is driven by the total loss Ltotal, defined as the weighted combination of the L1 loss L1 and the D-SSIM loss LSSIM. To mitigate underor over-reconstruction, 3DGS employs heuristic adaptive density control strategy guided by the view-space position gradient densify = L/µi. Gaussians with gradients exceeding predefined threshold are either cloned or split. We refer readers to the original paper (Kerbl et al., 2023) for additional details. Ri 3DGS-MCMC (Kheradmand et al., 2024) improves 3DGS in both rendering fidelity and robustness to noisy initialization. The key insight is that the optimization of 3DGS can be reformulated as Stochastic Gradient Langevin Dynamics (SGLD) update: λlr GEII[Ltotal(G; I)] + λnoise ϵ, (1) where λlr and λnoise denote hyperparameters that control the learning rate and the magnitude of stochastic exploration, respectively, and ϵ represents noise sampled for exploration. To mitigate the dependency on precise initialization, we adopt 3DGS-MCMC as our baseline for NVS. 3.2 MEMORY-EFFICIENT VGGT IMPLEMENTATION As illustrated in Fig. 2, the network structure of VGGT comprises three main components: per-frame DINO-based patch embedding extractor, stacked transformer layers alternating between global and frame-wise attention (i.e., AA layers), and decoder for camera parameter regression and dense predictions (Wang et al., 2025a). Although VGGT contains 24 AA layers, only the output features from layers 4, 11, 17, and 23 are utilized for dense prediction. To eliminate redundancy, we discard intermediate outputs from other layers, thereby reducing VRAM consumption. This modification increases image throughput from 150 to 600 images, and we refer to this variant as VGGT. Another source of redundancy lies in data precision. While automatic mixed precision is enabled, the majority of operations and tensor storage still default to Float32. We observe that switching to BFloat16, except for MLP in heads, introduces no noticeable degradation in performance. In contrast, it reduces the peak GPU memory usage by up to 74%, leading to substantial improvement in inference throughput. Moreover, since both DINO feature extraction and frame-wise attention involve only intra-frame computation, frames can be processed asynchronously. Consequently, input images can be divided into N/S chunks, which are sequentially processed. By selecting an appropriate chunk size S, peak memory usage in these modules can be effectively controlled. For convenience, this version is named as VGGT. 4 Preprint. Under review."
        },
        {
            "title": "3.3 CAMERA PARAMETERS GLOBAL ALIGNMENT (GA)",
            "content": "After the feedforward inference of VGGT, we obtain estimated camera parameters {Kn, Rn, tn}N , where for the n-th camera, Kn denotes the intrinsic matrix, while Rn and tn represent the rotation matrix and translation vector of the extrinsic matrix, respectively. These parameters can be refined using image correspondences by minimizing the epipolar distance loss: LEG = (cid:88) (cid:88) wk em,k / (cid:88) (cid:88) wk, em,k = kFmxk, (2) where em,k is the epipolar distance for the k-th correspondence in the m-th image pair, and xk are the corresponding keypoints, and Fm is the fundamental matrix derived from the paired cameras. The weights wk reflect the reliability of each correspondence, making their estimation crucial for effective optimization. Not all 2 image pairs have overlapping fields of view. Following (Jeong et al., 2021), we restrict candidate pairs to those with view angles below certain threshold. For these pairs, VGGTs tracking head can provide correspondences and confidence scores. However, as shown in Tab. 3, these predictions are insufficiently reliable for camera refinement. We therefore adopt XFeat (Potje et al., 2024), recent neural feature matcher known for its efficiency. While XFeat provides accurate matches, it does not supply correspondence weights wk. Using VGGTs depth confidence as proxy also proves suboptimal (cf. Tab. 3). To address this issue, we propose an adaptive weighting strategy. Intuitively, when both the 3D foundation model and the matching model provide reliable estimates, most em,k values should cluster near zero, and such correspondences should be assigned higher weights. Conversely, correspondences with large em,k are more likely to be outliers and should be down-weighted. The Global Alignment panel in Fig. 2 illustrates typical histogram of em,k with the x-axis limited to [0, 20]. As observed, em,k exhibits long-tail distribution, which aligns naturally with this intuition. Accordingly, we first compute em,k using VGGT-predicted camera parameters as defined in Eq. (2), and then estimate the adaptive weights as: wk = (cid:18) (em,k) Avg(f (em,k)) (cid:19)α , (3) where is the probability density function approximated via histogram, Avg(f (em,k)) denotes the average density over all em,k, and α is empirically set to 0.5. As validated in Tab. 3, this weighting scheme enables more efficient convergence during camera optimization. Finally, we adapt the learning rate to different convergence regimes. When VGGTs initialization is accurate, small learning rate suffices for fine alignment. However, in challenging cases, such setting fails to provide adequate updates. To adaptively control learning, we use the median epipolar distance as an indicator and adjust the learning rate according to the following empirical rule: lr = lr0, lr1, lr2, if Median(em,k) < b1, if b1 < Median(em,k) < b2, if Median(em,k) > b2, (4) where lr0, lr1, lr2 and the bounds b1, b2 are specified in Sec. 4.1. As shown in Tab. 3, this adaptive strategy is critical for ensuring robust convergence in camera parameter optimization. 3.4 3DGS TRAINING WITH IMPERFECT POSES The global alignment procedure in Sec. 3.3 substantially improves the accuracy of estimated camera parameters, thereby facilitating convergence of 3DGS training. Nonetheless, the performance gap relative to COLMAP remains, which is detrimental for initialization-sensitive models such as vanilla 3DGS (cf. Tab. 4). To mitigate this issue, we adopt MCMC-3DGS, which offers improved robustness under noisy or imperfect poses. Preprint. Under review. Figure 3: Qualitative comparison of rendering results. 3DGS here means 3DGS trained with COLMAP initialization, and is mainly for reference. Here, Apple is from CO3Dv2 dataset, Garden and Stump are from MipNeRF360 dataset, Ignatius and Caterpillar are from TnT dataset. In addition, we adopt joint optimization scheme in which residual camera poses are optimized alongside Gaussian parameters under photometric supervision. Concretely, we estimate the residual translation tn R3 and residual rotation rn R6. Following (Zhou et al., 2019), the 6D rotation representation rn is converted into residual rotation matrix Rn R33, which is then applied to refine Rn. In addition, we leverage the correspondence weights introduced in Sec. 3.3 to select reliable initialization points, providing stronger starting configuration for 3DGS training. As shown in Tab. 4, this strategy leads to consistently improved performance."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets & Metrics. We evaluate our model on widely used multi-view reconstruction benchmarks, including MipNeRF360 (Barron et al., 2022), Tanks and Temple (TnT) (Knapitsch et al., 2017), and CO3Dv2 (Reizenstein et al., 2021), with maximum image sequence lengths of 311, 1106, 202 and scene numbers of 9, 5, 5, respectively. MipNeRF360 is employed for our ablation studies. We follow (Wang et al., 2025a) for pose and point map estimation. Pose accuracy is evaluated using the standard AUC@30 metric, which integrates Relative Translation Error (RTE) and Relative Rotation Error (RRE). RTE and RRE compute the relative angular errors in translation and rotation for each image pair. We note that AUC@30 is not order-invariant and introduce minor modification to address this; further details are provided in the Sec. B. Point map quality is measured using Chamfer Distance, alongside accuracy and completeness metrics. For multi-view reconstruction, we adhere to the dataset splits and training view resolutions reported in prior works (Kerbl et al., 2023; Fu et al., 2024). Rendering quality is assessed via PSNR, SSIM, and LPIPS. For computational efficiency, we report both runtime and VRAM usage measured on 40G A100 GPU. Implementation Details. In our experiments, the chunk size for the frame-wise operation described in Sec. 3.2 is set to 128. For the global alignment procedure in Sec. 3.3, the angle-of-view threshold is set to 30 degrees. The learning rates lr0, lr1, and lr2 are configured to 5104, 1103, and 1 102, respectively, while the parameters b1 and b2 are set to 2.5 and 7.5. The maximum number of correspondences per image pair is limited to 4096, and the optimization is run for 300 iterations. When extracting COLMAP results, only matched points with weights exceeding 0.3 are retained. During MCMC-3DGS training, the maximum number of Gaussians per scene is matched to that of the vanilla 3DGS to ensure fairness. Pose embeddings are initialized with learning rate of 6 Preprint. Under review. Table 1: Comparison with SOTA methods on rendering quality. means initialized with COLMAP. Note that for fairness, 3RGS is also trained on predictions from our VGGT with GA. The best performance of each part is in bold. MipNeRF360 Tanks and Temple CO3Dv2 Model 3DGS MCMC MCMC CF-3DGS HT-3DGS 3RGS Ours SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS 0.8148 0. 0.5484 0.2344 0.3796 0.7128 0.7821 27.39 27.91 22.19 12.38 14.79 25.39 26.40 0.1849 0.1536 0.2822 0.7186 0.6691 0.2158 0.1774 0.8509 0. 0.6789 0.3914 0.4508 0.7497 0.8419 24.85 25.76 21.42 12.19 13.83 21.47 24.77 0.1550 0.1391 0.2778 0.6082 0.5850 0.3002 0.1676 0.9379 0. 0.7121 0.6110 0.8326 0.8781 0.9105 32.58 33.21 25.71 20.18 28.28 31.07 31.85 0.0954 0.0968 0.2008 0.4354 0.2298 0.1283 0.1128 Table 2: Comparison with SOTA methods on pose estimation. The units for RRE and RTE are degrees. Note that for fairness, 3RGS is also trained on predictions from our VGGT with GA. The best performance of each part is in bold. OOM here means fail to run on all scenes due to Out-of-Memory error. MipNeRF360 Tanks and Temple CO3Dv2 Model RRE RTE AUC@30 RRE RTE AUC@30 RRE RTE AUC@30 17.18 MASt3R-Sfm π3 3.244 VGGT 1.094 VGGT, +GA 0.678 104.0 CF-3DGS 93.69 HT-3DGS 0.605 3RGS 0.601 Ours 10.25 3.470 1.759 0.686 56.45 56.55 0.484 0.484 0.718 0.889 0.951 0.986 0.001 0.003 0.991 0.992 21.02 14.10 OOM OOM 1.891 2.034 1.479 1.783 55.20 110.9 51.87 100.0 6.762 4.855 1.259 1.738 0.687 OOM 0.953 0.967 0.006 0.010 0.846 0.971 11.72 0.924 3.035 2.002 15.2 12.30 1.972 1.984 15.32 1.719 4.659 2.811 21.5 12.25 2.583 2. 0.618 0.956 0.841 0.906 0.336 0.501 0.911 0.909 1 104 and decayed exponentially by factor of 0.1, while the learning schedule for other 3DGS attributes follows (Kheradmand et al., 2024). During rendering quality assessment, we would freeze trained Gaussians and tune the pose embedding for the test view and minimize the photometric loss, following practice of (Huang et al., 2025b). The tuning iteration is 10,000 for TnT and 5,000 for other datasets, while the other setting of learning schedule aligns with the training progress. Baselines. For 3D key attributes prediction, we compare the performance of MASt3R-SfM (Duisterhof et al., 2025), π3 (Wang et al., 2025c), and VGGT (Wang et al., 2025a). For MASt3R-SfM, we employ the retrieval mode in scene graph construction to achieve balance between accuracy and efficiency. For COLMAP-free 3DGS training, we consider CF-3DGS (Fu et al., 2024), HT-3DGS (Ji & Yao, 2025), 3RGS (Huang et al., 2025b), and MCMC-3DGS (Kheradmand et al., 2024). To ensure fair comparison, we replace the initial poses and point cloud in 3RGS with our globally aligned, higher-accuracy results. 4.2 COMPARISON WITH SOTA METHODS In Tab. 1, we compare rendering performance against recent advances and include results with COLMAP initialization as an upper-bound reference. Our model achieves state-of-the-art performance, as further confirmed by the qualitative results in Fig. 3, which show that our method more effectively suppresses blurry artifacts and floaters while preserving fine-grained textures. It is worth noting that the rendering quality of CF-3DGS on CO3Dv2 is noticeably worse than reported in its original paper, likely due to reproducibility issues documented in its repository1. In Tab. 2, we compare pose estimation accuracy. The results demonstrate that both our global alignment and joint optimization strategies consistently improve performance, surpassing all previous 1https://github.com/NVlabs/CF-3DGS/issues/7 7 Preprint. Under review. approaches that jointly optimize poses and 3DGS. We also evaluate the pose accuracy of 3D foundation models and provide trajectory visualizations in Fig. 4. Our model exhibits closer alignment with ground-truth trajectories, achieving the highest accuracy on MipNeRF360 and TnT, and ranking second on CO3Dv2."
        },
        {
            "title": "4.3 ABLATION",
            "content": "Table 3: Ablation on model components in pose and point map estimation. The experiments are conducted on MipNeRF360 (Barron et al., 2022). -XFeat here means replacing XFeat with tracking predicted by VGGT itself. - PDF Weight means using confidence predicted by VGGT to replace adaptive weight proposed in Sec. 3.3. Computation costs are evaluated on 40G A100. Model RRE() RTE() AUC@30 Acc. Comp. Overall T(min) Mem.(GB) Pose Estimation Point Map Estimation Cost VGGT VGGT VGGT VGGT, +BA VGGT, +GA - XFeat - Adaptive LR - PDF Weight - Rand Order OOM 1.090 1.094 0.640 0.652 2.096 0.732 2.705 0.681 OOM 1.740 1.759 0.392 0.643 2.250 0.751 2.970 0. OOM 0.951 0.951 0.994 0.988 0.920 0.984 0.892 0.986 OOM OOM 0.051 0.064 0.063 0.050 0.037 0.064 0.069 0.220 0.064 0.108 0.068 0.039 0.184 0.037 0.058 0.040 OOM 0.058 0.057 0. 0.054 0.202 0.051 0.083 0.054 OOM 0.98 1.29 157 1.78 4.46 1.78 1.88 1.78 OOM 28.87 9.66 24.26 11.12 13.49 11.12 11.12 11.12 Figure 4: Qualitative comparison of estimated trajectories. Here we also report the Root Mean Square Error (RMSE) of the Absolute Trajectory Error (ATE) (in meters) (Matsuki et al., 2024). The color bar indicates trajectory distance. We recommend zooming in for better details. First, we ablate the effect of different modules on 3D key attribute estimation. The primary reduction in computational overhead comes from redundant feature elimination and precision adjustment, which together lower VRAM usage by 83% on MipNeRF360. Batched attention further reduces memory by over 1 GB when scaling to more than 800 images. Combined these modifications together, the inference throughput is pushed to 1000+ images, as shown in Fig. 1. Noticeably, these optimizations have only negligible impact on prediction accuracy, as indicated in Tab. 3. Second, we examine strategies to enhance VGGT output quality. Replacing XFeat with the VGGT tracking head decreases AUC@30 by 6.8 points and increases Chamfer Distance by nearly fourfold. Similarly, leveraging VGGT-derived depth confidence to reweight XFeat correspondences results 8 Preprint. Under review. Table 4: Ablation on model components in multi-view reconstruction. The experiments are conducted on MipNeRF360 (Barron et al., 2022). The best performance of each metric is in bold. Initialization Train Set Test Set Model Pose COLMAP 3DGS MCMC COLMAP +Pose Opt. COLMAP Point Map COLMAP COLMAP COLMAP VGGT Rand. 500K 3DGS VGGT, +GA Rand. 500K 3DGS VGGT, +GA Rand. 500K MCMC +Pose Opt. VGGT, +GA Rand. 500K +Pose Opt. VGGT, +GA Rand. 500K +Pose Opt. VGGT, +GA Filtered. 500K +Pose Opt. VGGT, +GA Matched Points VGGT, +BA +Pose Opt. VGGT, +BA SSIM PSNR LPIPS SSIM PSNR LPIPS 0.8869 0.9041 0.9042 0.7284 0.7538 0.8178 0.8965 0.8965 0.8794 0.8966 0.8948 29.58 30.17 30.19 23.95 25.00 26.41 29.25 29.25 28.85 29.59 29.23 0.1427 0.1231 0.1228 0.2830 0.2471 0.1974 0.1229 0.1229 0.1473 0.1314 0. 0.7194 0.8357 0.8359 0.5321 0.5675 0.5563 0.7731 0.7731 0.7620 0.7821 0.7765 27.39 27.91 27.95 21.10 22.23 22.37 26.28 26.28 25.88 26.40 26.33 0.1849 0.1536 0.1537 0.3466 0.3058 0.2795 0.1823 0.1823 0.2005 0.1774 0. Figure 5: Bad case analysis. The blue and red histograms respectively correspond to rotation and translation residual distribution. The right part shows blurry artifacts caused by inaccurate poses. in substantial performance degradation. In contrast, incorporating our adaptive learning rate yields consistently higher accuracy. Moreover, we observe that with permutation-equivariant AUC@30, random input order still yields slight performance gain, consistent with the findings of (Wang et al., 2025c). Besides, we also scale the official Bundle Adjustment (BA) strategy to hundreds of images by applying our architectural optimizations to VGG-SfM (Wang et al., 2024a). While this achieves higher accuracy, it requires over two hours to complete, and as shown in Tab. 4, its initialization does not improve NVS quality, confirming the superior efficiency of our strategy. Finally, we ablate design choices for training high-quality 3DGS. As shown in Tab. 4, MCMC is more effective than vanilla 3DGS under imperfect initialization, and pose optimization proves essential for stable convergence and high rendering quality. Among initialization strategies, point clouds derived from high-confidence correspondences achieve the best performance. Limited by pages, we put additional ablations in Tab. 5 in the Appendix. 4.4 DISCUSSION Although our model achieves state-of-the-art performance, noticeable gap remains compared to 3DGS trained with COLMAP initialization, as shown in Tab. 1. Interestingly, Tab. 4 reveals that on the training set, our model even surpasses COLMAP-initialized 3DGS in rendering quality, yet its performance on the test set lags behind, suggesting clear overfitting issue. This highlights the inherently ill-posed nature of the problem. And without reliable initialization, the optimization process is prone to getting trapped in local minima of the highly non-convex loss landscape. We also experimented with adding depth supervision (cf. Tab. 5), but found little improvement. Besides, as illustrated in Tab. 2, even after joint optimization, pose accuracy still falls short of COLMAP. We further compare the learned camera pose residuals with ground-truth. The visualization is included in Fig. 5. We observe that while most residuals cluster near zeroindicating accurately predicted posesthe model struggles to sufficiently correct poses with large deviations. 9 Preprint. Under review. Another noteworthy finding in Tab. 2 is that although VGGT substantially outperforms π3 on MipNeRF360, it is surpassed on CO3Dv2 by considerable margin. This discrepancy suggests that the generalization ability of 3D foundation models remains an open challenge."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "In this paper, we investigated the potential of applying 3D Foundation Models to dense novel view the poor scalability in computational overhead and synthesis and identified two key challenges: insufficient prediction accurarcy for subsequent radiance field fitting. To address these obstacles, we introduced VGGT-X, which integrates memory-efficient VGGT implementation, adaptive global alignment, and robust 3DGS training strategies. Our approach substantially narrows the performance gap with COLMAP-initialized counterparts. Beyond these improvements, our analysis also sheds light on the remaining limitations and outlines promising directions for advancing both 3DFMs and NVS frameworks. We hope our findings provide valuable insights toward building fast, reliable, and fully COLMAP-free dense NVS systems."
        },
        {
            "title": "REFERENCES",
            "content": "Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew OToole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with ray-conditioned sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1661016620, 2023. Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 58555864, 2021. Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54705479, 2022. Jia-Wang Bian, Wenjing Bian, Victor Adrian Prisacariu, and Philip Torr. Porf: Pose residual field for accurate neural surface reconstruction. In ICLR, 2024. Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 41604169, 2023. Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. Yu Chen, Rolandos Alexandros Potamias, Evangelos Ververas, Jifei Song, Jiankang Deng, and Gim Hee Lee. Zerogs: Training 3d gaussian splatting from unposed images. arXiv preprint arXiv:2411.15779, 2024. Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. Gaussian activated neural radiance fields for high fidelity reconstruction and pose estimation. In European Conference on Computer Vision, pp. 264280. Springer, 2022. Wenyan Cong, Yiqing Liang, Yancheng Zhang, Ziyi Yang, Yan Wang, Boris Ivanovic, Marco Pavone, Chen Chen, Zhangyang Wang, and Zhiwen Fan. E3d-bench: An end-to-end benchmark for 3d geometric foundation models. ICCV, 2025. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 10 Preprint. Under review. Bardienus Pieter Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Revaud. MASt3r-sfm: fully-integrated solution for unconstrained structure-frommotion. In International Conference on 3D Vision 2025, 2025. URL https://openreview. net/forum?id=5uw1GRBFoT. Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang, et al. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. Advances in neural information processing systems, 37:140138140158, 2024a. Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, et al. Large spatial model: End-to-end unposed images to semantic 3d. Advances in neural information processing systems, 37:4021240229, 2024b. Xianze Fang, Jingnan Gao, Zhe Wang, Zhuo Chen, Xingyu Ren, Jiangjing Lyu, Qiaomu Ren, Zhonglei Yang, Xiaokang Yang, Yichao Yan, et al. Dens3r: foundation model for 3d geometry prediction. arXiv preprint arXiv:2507.16290, 2025. Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2079620805, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Guichen Huang, Ruoyu Wang, Xiangjun Gao, Che Sun, Yuwei Wu, Shenghua Gao, and Yunde Jia. Longsplat: Online generalizable 3d gaussian splatting from long sequence images. arXiv preprint arXiv:2507.16144, 2025a. Zhisheng Huang, Peng Wang, Jingdong Zhang, Yuan Liu, Xin Li, and Wenping Wang. 3r-gs: Best practice in optimizing camera poses along with 3dgs. arXiv preprint arXiv:2504.04294, 2025b. Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 58465854, 2021. Bo Ji and Angela Yao. Sfm-free 3d gaussian splatting via hierarchical training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2165421663, 2025. Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, et al. Anysplat: Feed-forward 3d gaussian splatting from unconstrained views. arXiv preprint arXiv:2505.23716, 2025a. Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction, 2025b. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023. Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Yang-Che Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 3d gaussian splatting as markov chain monte carlo. Advances in Neural Information Processing Systems, 37:8096580986, 2024. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):113, 2017. Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2171921728, 2024. Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024. Jiahao Li, Haochen Wang, Muhammad Zubair Irshad, Igor Vasiljevic, Matthew R. Walter, Vitor Campagnolo Guizilini, and Greg Shakhnarovich. Fastmap: Revisiting structure from motion through first-order optimization. https://arxiv.org/abs/2505.04612, 2025. 11 Preprint. Under review. Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 57415751, 2021. Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, et al. Vastgaussian: Vast 3d gaussians for large scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 51665175, 2024. Yang Liu, Chuanchen Luo, Lue Fan, Naiyan Wang, Junran Peng, and Zhaoxiang Zhang. Citygaussian: Real-time high-quality large-scale scene rendering with gaussians. In European Conference on Computer Vision, pp. 265282. Springer, 2024. Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, and Zhaoxiang Zhang. Citygaussianv2: Efficient and geometrically accurate reconstruction for large-scale scenes. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=a3ptUbuzbW. Jiahao Lu, Tianyu Huang, Peng Li, Zhiyang Dou, Cheng Lin, Zhiming Cui, Zhen Dong, Sai-Kit Yeung, Wenping Wang, and Yuan Liu. Align3r: Aligned monocular depth estimation for dynamic videos. CVPR, 2025. Hidenobu Matsuki, Riku Murai, Paul HJ Kelly, and Andrew Davison. Gaussian splatting slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1803918048, 2024. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):115, 2022. Guilherme Potje, Felipe Cadar, Andre Araujo, Renato Martins, and Erickson R. Nascimento. Xfeat: Accelerated features for lightweight image matching. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 26822691, 2024. doi: 10.1109/CVPR52733. 2024.00259. Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, and Markus Steinberger. Stopthepop: Sorted gaussian splatting for view-consistent real-time rendering. ACM Transactions on Graphics (TOG), 43(4):117, 2024. Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), Oct 2021. doi: 10.1109/iccv48922.2021.01072. URL http://dx.doi.org/10.1109/ iccv48922.2021.01072. Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Dongbo Shi, Shen Cao, Lubin Fan, Bojian Wu, Jinhui Guo, Renjie Chen, Ligang Liu, and Jieping Ye. Trackgs: Optimizing colmap-free 3d gaussian splatting with global track constraints. arXiv preprint arXiv:2502.19800, 2025. Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. 2024. URL https://arxiv.org/abs/ 2408.13912. Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, and Tong He. Aether: Geometric-aware unified world modeling. ICCV, 2025. 12 Preprint. Under review. Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural In Proceedings of the IEEE/CVF Conference on radiance fields from sparse and noisy poses. Computer Vision and Pattern Recognition, pp. 41904200, 2023. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, and Pratul SriniIn 2022 vasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 54815490. IEEE, 2022. Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. 3DV, 2025. Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2168621697, 2024a. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025a. Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1051010522, 2025b. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. 2024b. Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning. arXiv preprint arXiv:2507.13347, 2025c. Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021. Yitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool. Sinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. arXiv preprint arXiv:2210.04553, 2022. Tian-Xing Xu, Xiangjun Gao, Wenbo Hu, Xiaoyu Li, Song-Hai Zhang, and Ying Shan. Geometrycrafter: Consistent geometry estimation for open-world videos with diffusion priors. ICCV, 2025. Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern forward pass. Recognition (CVPR), June 2025. Botao Ye, Sifei Liu, Haofei Xu, Li Xueting, Marc Pollefeys, Ming-Hsuan Yang, and Peng Songyou. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. ICLR, 2025. Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, and Yong Dou. Absgs: Recovering fine details in 3d gaussian splatting. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 10531061, 2024. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for In Proceedings of the IEEE/CVF International real-time rendering of neural radiance fields. Conference on Computer Vision, pp. 57525761, 2021. Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Aliasfree 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1944719456, June 2024. Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. ICLR, 2025a. 13 Preprint. Under review. Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views, 2025b. URL https://arxiv.org/abs/2502. 12138. Yuqi Zhang, Guanying Chen, and Shuguang Cui. Efficient large-scale scene representation with hybrid of high-resolution grid and plane features. arXiv preprint arXiv:2303.03003, 2023. Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation In Proceedings of the IEEE/CVF conference on computer representations in neural networks. vision and pattern recognition, pp. 57455753, 2019. 14 Preprint. Under review."
        },
        {
            "title": "A ADDITIONAL ABLATIONS",
            "content": "Table 5: Additional ablation on model components in multi-view reconstruction. The experiments are conducted on MipNeRF360 (Barron et al., 2022). This table showcases the aborted model designs. The best performance of each metric is in bold. The Baseline denotes MCMC-3DGS equipped with pose embedding. The modification of each following row is independent of the others. Initialization Train Set Test Set Model Pose Point Map SSIM PSNR LPIPS SSIM PSNR LPIPS Ours Baseline Ours MLP Ours depth 2 pose lr Ours Ours Epi. Loss VGGT* VGGT* Epi. Loss Matched Points Matched Points Matched Points Matched Points Matched Points 0.8966 0.8753 0.8851 0.9044 0.8964 0.8499 29.59 28.60 28.85 30.02 29.55 27.21 0.1314 0.1436 0.1415 0.1243 0.1318 0.1806 0.7821 0.7492 0.7628 0.7740 0.7795 0.6440 26.40 25.83 25.94 26.16 26.31 23. 0.1774 0.1934 0.1954 0.1737 0.1780 0.2572 Here we provide additional ablation studies in Tab. 5. We experimented with design choices like MLP-based pose embedding learning (Huang et al., 2025b), epipolar loss during 3DGS training, and depth supervision. But none of them bring clear benefits to the rendering quality. We also tried to double learning rate and encourage to learn broader distribution, but it turns out to aggregate the overfitting phenomenon. Moreover, the last row of Tab. 5 shows that integrating global alignment into GS training, rather than treating it as separate process, leads to suboptimal results. Therefore, we adopt global alignment as an independent component. PERMUTATION-EQUIVARIANT AUC@30 In this section, we analyze why the conventional AUC@30 metric is sensitive to the input image order and propose simple yet effective modification to address this issue. AUC@30 first computes relative poses for all 2 image pairs. Comparing the relative poses from ground truth and predictions, the relative rotation and translation errors can be derived for AUC@30 calculation. Specifically, for two images indexed by and (with < j) and their corresponding extrinsics, the relative pose is computed as: Eij = Ei 1Ej = (cid:18)Ri Ri 1 0 ti (cid:19) (cid:18)Rj 0 (cid:19) tj 1 (cid:18)Ri = Rj Ri 0 (cid:19) (tj ti) 1 . (5) However, if the image order is permuted and precedes i, the relative pose becomes: Eji = (cid:18)Rj Ri Rj 0 (cid:19) (ti tj) . (6) (tj ti) = While the orthogonality of Ri and Rj ensures that Rj (ti tj). Consequently, the relative translation angleand hence AUC@30is sensitive to the Rj ordering of input images, which can lead to differences exceeding five points. To mitigate this, we include both Eij and Eji in the relative pose sequence instead of only Eij. This modification preserves the relative rotation error while introducing permutation equivariance to relative translation error and AUC@30, resulting in more robust and fair evaluation of pose estimation accuracy. Rj, it is clear that Ri Ri = Ri"
        },
        {
            "title": "C LARGE LANGUAGE MODEL USAGE",
            "content": "We used LLMs solely as writing assistant to improve grammar, clarity, and conciseness of the manuscript. The research ideas, technical contributions, experiments, and analyses were entirely conceived and conducted by the authors. No content was generated by LLMs beyond language refinement, and all scientific claims and results are the sole responsibility of the authors."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences",
        "Shandong University",
        "University of Chinese Academy of Sciences",
        "University of Science and Technology Beijing"
    ]
}