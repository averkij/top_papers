{
    "paper_title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities",
    "authors": [
        "Junyan Zhang",
        "Yubo Gao",
        "Yibo Yan",
        "Jungang Li",
        "Zhaorui Hou",
        "Sicheng Tao",
        "Shuliang Liu",
        "Song Dai",
        "Yonghua Hei",
        "Junzhuo Li",
        "Xuming Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community."
        },
        {
            "title": "Start",
            "content": "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLMs Instruction-Following Capabilities Junyan Zhang1,*, Yubo Gao1,*, Yibo Yan1,2, Jungang Li1, Zhaorui Hou1, Sicheng Tao1, Shuliang Liu1,2, Song Dai1, Yonghua Hei1, Junzhuo Li1,2, Xuming Hu1,2, 1The Hong Kong University of Science and Technology (Guangzhou) 2The Hong Kong University of Science and Technology {junyanzhang0317, yubogao1015}@gmail.com, xuminghu@hkust-gz.edu.cn 5 2 0 2 7 2 ] . [ 1 1 9 1 1 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how finetuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HEXAINST, carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, novel analytical framework comprising three key contributions: ❶ method for identifying these sparse components, ❷ an evaluation of their functional generality and uniqueness, and ❸ systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) fine-tuning has significantly enhanced the ability of LLMs to comprehend user intent, follow instructions, and align with human preferences, thereby boosting their performance across diverse tasks (Prakash et al., 2024; Dang et al., 2024; Yan et al., 2025; Zhang et al., 2023). However, precisely how these fine-tuning processes alter the internal computational mechanisms of models to achieve superior instruction following remains crucial yet elusive scientific *Equal contribution. Corresponding author. 1 question. To shed light on these mechanisms, this study adopts an interpretability perspective. Figure 1: Comparison of research focuses between Language-Specific Neurons (a) and Instruction-Specific Neurons & Experts in dense LLMs & MoE models (b). Prior works in neuron-level interpretation (Tang et al., 2024; Huo et al., 2024; Kojima et al., 2024; Wang et al., 2022b; Huang et al., 2024) has successfully identified X-specific neurons crucial for storing factual knowledge (Dai et al., 2021), processing specific languages (Tang et al., 2024), recalling domain information (Huo et al., 2024), and ensuring model safety (Chen et al., 2024). These specific neurons make up small proportion, but they are crucial to the models corresponding capabilities, as illustrated in Figure 1 (a). In the field of circuit analysis, it has been demonstrated that there exist several important circuits that store specific knowledge for particular tasks, such as indirect object identification (Wang et al., 2022a) and color object identification (Merullo et al., 2023). Inspired by these findings, we posit central hypothesis: Does the remarkable instruction-following capability exhibited by instruction-tuned models also stem from certain sparse components? In this work, we conduct systematic investigation into the activation mode of two types of sparse components: Instruction-Specific Neurons and Instruction-Specific Experts within two popular open-source LLM families (LLaMA and Mistral) and one Mixture-of-Experts (MoE) model family (Qwen-MoE), as shown in Figure 1 (b). Therefore, we first propose meticulously curated and balanced instructional HEXAINST dataset comprising six instruction categories. We further propose SPARCOM, novel sparse component analysis framework with three key steps. ❶ We identify Instruction-Specific Neurons and InstructionSpecific Experts within LLMs, enabling precise localization of these sparse components. These neurons and experts are primarily responsible for processing and executing instructions. ❷ We evaluate the generality and uniqueness of the distribution of the identified Instruction-Specific Neurons and Experts, providing robust methodology for assessing their functional characteristics. ❸ We perform an alteration comparison, analyzing the differences of Instruction-Specific Neurons and Experts in the same model before and after fine-tuning. Also, we examine the distribution patterns of InstructionSpecific Neurons across different layers and propose three-stage framework for understanding the internal mechanism. Ultimately, we have obtained several significant findings, offering new insights into how fine-tuning shapes the internal mechanisms of LLMs. Contributions can be summarized as follows1: We propose meticulously curated and balanced HEXAINST dataset comprising six instruction categories for our in-depth analysis. We present SPARCOM, novel framework designed to identify and analyze instructionspecific neurons and experts in LLMs. We explore the generality and uniqueness of these specialized sparse components and uncover how fine-tuning shapes LLMs through them, revealing their distribution and activation patterns."
        },
        {
            "title": "2.1 Preliminaries",
            "content": "Dense LLMs The conventional decoder-only transformer model takes an input sequence of tokens = (t1, t2, . . . , tn), where n, and transforms it into an output probability distribution = (y1, y2, . . . , yn), with RnV . Let x(l) (t) Rdmodel denote the residual stream activation for the token at position at the start of the l-th layer. The transformation at each layer includes 1Code and dataset will be released upon acceptance. two main components: Attention Mechanism and Feed-Forward Network (FFN): = x(l) x(l) + Attn(l)(x(l) 1:i). x(l+1) = x(l) + FFN(l)(x(l) ). In FFN component, we do the following: (1) (2) (cid:32) (cid:18) = Wdown act_fn Wgate_up_proj x(l) (cid:19) [0 : dmid] (cid:18) Wgate_up_proj x(l) (cid:19) (cid:33) [dmid : 2dmid] , (3) where Wgate_up_proj and Wdown are learnable weight matrices. The gate_up_proj is used to project the input into higher-dimensional space, dividing it into gating part and an up-projection part. Then, the gating part passes through an activation function to determine the flow of information. To streamline the representation, the bias terms have been omitted from the formulation. MoE models In MoE models, the FFN is redesigned with gating network to select multiple experts per token, incorporating load balancing mechanism to optimize performance and computational load while ensuring all experts contribute fairly. Specifically, the FFN layer is replaced with the following formula: (cid:88) = gj FFNj(x(l) ), (4) jTop-k(g) where is the weight vector output by the gating network. In our implementation, we use the QwenMoE models."
        },
        {
            "title": "2.2 Related Work",
            "content": "MoE Models The MoE approach employs individual, independent experts to handle different tasks (Jacobs et al., 1991; Jordan and Jacobs, 1994). In recent years, with the advancement of LLMs, MoE has emerged as an effective method for significantly scaling up model capacity while minimizing computational overhead, thereby attracting increasing attention from both academia and industry (Huang et al., 2025; Cai et al., 2025; Lin et al., 2024). The core idea behind MoE is to introduce conditional computation: instead of applying the same parameters to all inputs, different inputs are 2 Figure 2: The SPARCOM framework, which comprises three elements, aims for the identification & evaluation of sparse components. ISNs and ISEs denote Instruction-Specific Neurons and Instruction-Specific Experts. processed by different parts of the model. This allows for scalable and efficient model growth (Shi et al., 2024; Li et al., 2025a; Yuan et al., 2025; Yang et al., 2025). LLM Fine-tuning While foundational pretrained language models possess extensive knowledge and certain reasoning capabilities (Ke et al., 2025; Yan et al., 2024b,a), they often struggle to directly meet diverse and specific user needs (Kumar et al., 2025; Wei et al., 2025; Li et al., 2025b). To bridge this gap, researchers widely explore finetuning techniques to adapt models to general domains (Han et al., 2024; Wang et al., 2024a,b). This typically involves two key components. Supervised fine-tuning enables the model to learn and generalize across wide range of general instructions (Wei et al., 2022; Wang et al., 2023; Shen et al., 2024), while alignment with human preferences ensures that the models behavior is refined to better adhere to human values and expectations (Christiano et al., 2023; Stiennon et al., 2022; Rafailov et al., 2024; Wang et al., 2024d; Ji et al., 2024). Neuron Analysis In recent years, mechanistic interpretability has gained prominence (Marks et al., 2024; Yao et al., 2024; Cambria et al., 2024; Bilal et al., 2025; Mumuni and Mumuni, 2025; Wu et al., 2024), with neuron analysis emerging as powerful approach for uncovering the internal mechanisms of LLMs (Wang et al., 2024c; Song et al., 2024). Recent studies in this area have successfully identified neurons that are either language-specific or domain-specific, thereby uncovering the specialized roles that certain neurons play within these models (Tang et al., 2024; Huo et al., 2024; Chen et al., 2024). In our work, we apply neuron analysis techniques to specific instructions, with particular focus on understanding how activated neurons before and after instruction tuning."
        },
        {
            "title": "3 SPARCOM Framework",
            "content": "SPARCOM introduces an innovative sparse components analysis framework composed of three core steps, as depicted in Figure 2. The initial step focuses on pinpointing Instruction-Specific Neurons (ISNs) and Instruction-Specific Experts (ISEs) in LLMs, allowing for accurate detection of these sparse, task-aligned components. Leveraging this identification, the second step systematically analyzes the generality and uniqueness of the discovered neurons and experts, establishing rigorous approach to evaluating their functional properties. In the third step, we analyze how the activation distributions of these sparse components 3 change before and after model fine-tuning. 3.1 Sparse Components Identification ISEs Identification Inspired by the language activation probability entropy proposed by Tang et al. (2024), we developed method to identify ISNs. Specifically, for each instruction I, we perform the following procedure. The activation of neurons mainly focuses on the Feed-Forward Network component. We feed into the LLM and record the activation value of each neuron in j-th gate_up_proj layer after applying the activation function: At ij(I) = (cid:16) gate_up_proj(x(j) (cid:17) (I) , (5) where () denotes the activation function applied to the layers output of the token across the instruction sequence length . The activation frequency is empirically estimated by the likelihood that the neurons activation value exceeds zero: pij(I) ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 [At ij(I) > 0]. (6) Subsequently, the activation frequency of each neuron in response to given instruction is calculated. This process involves flattening all neurons across every layer and computing their respective activation frequencies. threshold is then established to identify the top ϵ percentile of these frequencies. Neurons exceeding this threshold are designated as ISNs, reflecting their heightened propensity for activation in response to the specific instruction: S(I) = {(i, j) pij ϵ} (7) For MoE models, we use similar ways to calculate the activation frequency: peij(I) ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:2)At eij(I) > 0(cid:3) [e Ejt] , (8) where represents the index across all experts and Ejt represents the top-k activated experts of token in j-th layer."
        },
        {
            "title": "The ISNs of MoE models can be calculated as",
            "content": "the following: S(I) = {(e, i, j) peij ϵ}. (9) ISEs Identification In MoE models, for each token, the routing mechanism selects the top-k experts from the output probability distribution, thereby determining the chosen experts. We name the activated experts as ISEs. 3.2 Sparse Components Evaluation Distribution of ISNs: Overlaps and Differences We have already identified the ISNs, and we want to explore whether there is overlap in the distribution of these ISNs among instructions of the same type, and whether there are significant differences between different types of instructions: Sim(m1, m2) = 1 Nm1Nm2 (cid:88) J(S(Im1n1), S(Im2n2)), n1m1 n2m2 (10) where, Sim(m1, m2) represents the similarity of ISNs between two types of instructions, m1 and m2 represent the types of instructions, and n1 and n2 represent the indices of the specific instructions within their respective types, represents jaccard similarity. For MoE models, we use the similar methods to calculate. This metric reflects the overlaps and differences in the distribution of ISNs and between same-type and different-type instructions. Distribution of ISEs: Overlaps and Differences For instruction I, we collect the activation frequencies of all experts across each layers: fej(I) ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 xejt, fej(I) [0, 1] (11) where xejt represents whether the t-th token in the j-th layer activates the e-th expert. After this, we flatten the obtained activation probability matrix into one-dimensional vector: (I) = [fej(I)], (12) Given two instructions I1 and I2, along with their activation frequency vectors (I1) and (I2), calculate the pearson correlation coefficient between them: rI1I2 = (cid:104) (cid:80) (I1)[i] (I1) (cid:105) (cid:104) (cid:105) (I2)[i] (I2) (cid:114) (cid:80) (cid:104) (I1)[i] (I1) (cid:105)2 (cid:114) (cid:80) 1 (cid:104) (I2)[i] (I2) (cid:105)2 . (13) Next, compute the average pearson correlation coefficient for all pairs of instructions from these two different or identical types: Corrm1m2 = 1 Nm1Nm2 Nm1(cid:88) Nm2(cid:88) rIm1n1 Im2n2 . n1=1 n2= (14) This metric reflects the correlation in the distribution of activated experts between same-type and different-type instructions."
        },
        {
            "title": "Comparison",
            "content": "In this chapter, we evaluate alterations in ISNs and ISEs from three perspectives. Hierarchy Distribution of ISNs We visualize the ISNs across layers of the model before and after fine-tuning, based on the computed results. Alterations in ISNs We compare the jaccard similarity of distribution changes of the same instruction of activated neurons in the model before and after fine-tuning. * denotes that the calculation is between the model before and after fine-tuning: (Touvron et al., 2023), Mistral (Jiang et al., 2023), and Qwen (Bai et al., 2023). For fine-tuned models, we examined multiple versions of LLaMA-2Chat, specifically the 7B and 13B variants, along with Mistral-7B-Instruct-v0.1 and Qwen1.5-MoEA2.7B-Chat. For the vanilla models, we selected LLaMA-2-7B, LLaMA-2-13B, Mistral-7B-v0.1, and Qwen1.5-MoE-A2.7B. Datasets Our work requires dataset containing various types of instructions, which must be clear and precise, and preferably free of any extraneous information that could cause interference. However, current instruction datasets are unevenly distributed across tasks, particularly lacking summarization and classification instructions, as well as AI-generated ones. We construct balanced dataset HEXAINST with 1,200 instances across six instruction categories: classification (CLS), code (CODE), generalqa (QA), generation (GEN), math (MATH), summarization (SUM). Each category contains 100 AI-generated and 100 human-curated instructions to control for source variability. Specifically, the dataset is compiled through two primary sources. Synthetic data: Generated via DeepSeek R1 with constrained meta-prompts. Natural data: Built upon public benchmarks: Classification: FLAN Collection (Longpre et al., 2023) Code: HumanEval (Chen et al., 2021) GeneralQA: TriviaQA (Joshi et al., 2017) Generation: Alpaca (Taori et al., 2023) Sim(m) = 1 N (cid:88) n=1 (S(Imn), S(Imn)) . Math: Math-500 (Lightman et al., 2023) (15) Summarization: FLAN Collection (Longpre Alterations in ISEs Similarly, we compute the average pearson correlation coefficient for all pairs of the same instruction in the model before and after fine-tuning: Corrm = 1 N (cid:88) n=1 ImnImn. (16)"
        },
        {
            "title": "4.1 Experimental Setups",
            "content": "Models We conducted our study primarily on three families of publicly available LLMs: LLaMA et al., 2023) For natural data, we extract instructions using regex pattern matching followed by expert validation and refinement. Synthetic instructions are cross-checked against training data of public LLMs to prevent contamination. The balanced design (100 synthetic plus 100 natural per category) enables disentangling neuron activation patterns from data source biases. All data have undergone manual post-validation to ensure quality. The details of post-validation can be found in B. Examples of each type of instructions can be found in C. 5 havior and characteristics of instruction-specific components in LLMs. 5.1 Generality and Uniqueness of Sparse Components We propose that the ISNs and ISEs identified through SPARCOM framework can be categorized into two types: general and unique ISNs, and general and unique ISEs. Generality As shown in Figure 3, we believe that the overlapping neurons between different instruction types mainly belong to general ISNs. These neurons are responsible for processing general instruction language and encode the common functions or conceptual elements required for instruction processing, or handle parts unrelated to the specific content of the instructions. For example, although there are clear semantic and expressive differences between different types of instructions, overlap in certain vocabulary is inevitable. This also leads to the overlap of general neurons across different instruction types. The high overlap of ISNs between classification and summarization likely reflects their intrinsic connection and shared skill requirements. Similarly, in Figure 4, there is also correlation in the activation of experts between different instruction types, although it is much weaker compared to the correlation within the same type of instructions. We believe that these general experts may be responsible for handling general instructions and responding to potential overlaps in tokens across different instructions. Uniqueness As illustrated in Figure 3, all models exhibit notably darker coloration along the diagonal, particularly for instruction types such as classification, summarization, code, and math. This indicates high overlap of ISNs among instructions of the same type. Despite considerable variations in vocabulary and syntax within the same category of instructions, pronounced similarity is observed in their representations. We contend that this observation provides compelling evidence for the uniqueness and specialized functionality of ISNs. This finding highlights the ability of these neurons to recognize and process the core elements of instructions, with limited influence from superficial differences in expression. As shown in Figure 4, the activation of experts within the same type of instructions also exhibits Figure 3: Overlaps and differences in ISNs distribution across same-type and different-type instructions on LLaMA-2-Chat-7B, LLaMA-2-Chat-13B, Mistral-7BInstruct-v0.1, and Qwen1.5-MoE-A2.7B-Chat. Figure 4: Overlaps and differences in ISEs distribution across same-type and different-type instructions on Qwen1.5-MoE-A2.7B-Chat. Implementation Details We use the vllm (Kwon et al., 2023) and Transformer library to obtain For and hook the internal states of LLMs. Qwen1.5-MoE-A2.7B-Chat and Qwen1.5-MoEA2.7B model, we use the default settings, which involve selecting four dynamic experts for each token from pool of sixty experts based on the gating networks scores."
        },
        {
            "title": "5 Results and Insights",
            "content": "Through experiments, we conducted three key steps of our framework: Sparse Components Identification, Sparse Components Evaluation, and Sparse Components Alteration Comparison. Based on the results, we derived meaningful insights and significant findings regarding the be6 Figure 5: Hierarchy distribution of ISNs across different layers. The upper part includes LLaMA-2-Chat-7B, LLaMA-2-Chat-13B, Mistral-7B-Instruct-v0.1, and Qwen1.5-MoE-A2.7B-Chat models. The down part includes LLaMA-2-7B, LLaMA-2-13B, Mistral-7B-v0.1, and Qwen1.5-MoE-A2.7B models. significantly higher correlation, particularly prominent in classification, code, and math tasks, which demonstrates the uniqueness of ISEs. We believe this also supports the hypothesis that different experts in MoE model specialize in distinct skills. Through the design of the load-balancing loss, the MoE model ensures that different experts develop unique capabilities to handle instructions from different categories."
        },
        {
            "title": "5.2 Features of Sparse Components",
            "content": "Similarity of Processing Instructions According to Figure 5, the overall trend of the distribution of ISNs by layer remains largely unchanged before and after model fine-tuning, particularly evident in the LLaMA-2-7B, LLaMA-2-13B, and Qwen1.5-MoE-A2.7B. This observation suggests that the fundamental logic with which each model processes instructions does not undergo significant changes through fine-tuning. However, following fine-tuning, these models exhibit an increase in the number of more capable and specialized ISNs. These enhanced neurons enable the models to handle wider variety of tasks and generate more accurate and contextually appropriate responses. Another key similarity appears across different instruction types. Despite the substantial diversity among instructions, the distribution patterns of ISNs follow remarkably consistent trends in all tested models, especially in LLaMA-2-7B, LLaMA-2-13B, and Mistral-7B-v0.1 series. This suggests that LLMs likely rely on shared computational mechanism for processing instructions, i.e., one where the underlying neural activation patterns remain stable regardless of instruction type. Understanding ISNs Working Mechanism Inspired by Zhao et al. (2024), for both types of models, we propose three-phase mechanistic framework to elucidate ISNs operational principles. For non-MoE models, in the early stage, the number of ISNs is significantly large, as this phase involves the encoding and processing of the shallow concepts of diverse instructions. In the intermediate stage, instructions are further generalized and understood by the language model, leading to sharp reduction in the number of ISNs. In the final Model CLS CODE QA GEN MATH SUM LLaMA-7B Series LLaMA-13B Series Mistral-7B Series 0. 0.55 0.43 Qwen-MoE-2.7B Series 0.44 0.62 0. 0.43 0.53 0.59 0.56 0.42 0. 0.56 0.51 0.38 0.53 0.62 0. 0.42 0.51 0.57 0.53 0.41 0. Table 1: Jaccard similarity coefficient in ISNs of the same instruction following fine-tuning, illustrating the Alterations in ISNs. Model CLS CODE QA GEN MATH SUM Qwen-MoE-2.7B Series 0.91 0. 0.94 0.92 0.91 0.92 Table 2: Pearson correlation coefficient in ISEs of the same instruction following fine-tuning, illustrating the Alterations in ISEs. pattern is consistent across two different instruction types. The overlapping neurons that inherently respond to instructions undergoes further refinement during fine-tuning. Additionally, new ISNs emerge post-fine-tuning. These newly formed and refined neurons work in tandem to establish more precise instruction-to-response mappings, demonstrating enhanced functional specialization and contributing to improved performance. This aligns closely with insights proposed by Prakash et al. (2024). According to Table 2, it can be observed that before and after fine-tuning, the same instructions still activate experts with high degree of correlation, indicating strong linear relationship. This suggests that, from the perspective of experts, their responses remain highly consistent before and after fine-tuning. The underlying architecture and decision-making process of the model remain relatively stable, meaning that fine-tuning has not significantly altered the models reliance on different experts. This is also reflected in F. Instead, the ISNs of the experts may have played more significant role in the improved performance."
        },
        {
            "title": "6 Conclusion",
            "content": "This study provides novel insights into how instruction tuning shapes LLMs through sparse components. By introducing the HEXAINST dataset and SPARCOM framework, we systematically identify and analyze ISNs and ISEs, revealing their unique distribution patterns and activation behaviors. Our findings advance the understanding of fine-tuning mechanisms, demonstrating how targeted modifications to sparse components significantly enhance instruction-following capabilities. This work opens new directions for interpretability research and efficient model optimization. Figure 6: Venn-bar diagram illustrating the distribution changes of activated ISN numbers for two example instructions from code and math categories, using LLaMA-2-Chat-7B series. stage, the number of neurons specific to particular instructions increases sharply again. These ISNs facilitate the generation of corresponding outputs by continuously decoding content into the relevant output tokens. This observation aligns with the insights proposed by Huo et al. (2024). For MoE models, the early stage, which extends from the shallowest layers to the intermediate layers, sees continuous increase in the number of ISNs, enriching the representation of instructions as more experts participate in the processing. Our hypothesis is that MoE models require more steps to continuously understand and process the content of instructions. In the middle stage, the number of ISNs decreases. Similarly, in the final stage, there is another sharp increase, enabling the model to generate the corresponding outputs."
        },
        {
            "title": "5.3 Alterations in Sparse Components",
            "content": "Following Fine-tuning As shown in Table 1, the activation patterns of specific neurons in response to the same instruction within the same model exhibit significant changes before and after fine-tuning. This provides additional validation for the effectiveness of our identification of ISNs. As illustrated in Figure 6, the changes in activation patterns before and after finetuning in LLaMA-2-7B are layer-specific: the increase in ISNs is primarily observed in the early layers (responsible for initial instruction parsing) and late layers (involved in output generation), and this"
        },
        {
            "title": "Limitations",
            "content": "Our study primarily investigates the characteristics of LLMs in processing different instructions from mechanistic explainability perspective. While we have identified certain Instruction-Specific Neurons and Experts, developing effective strategies to leverage these components to enhance the models instruction-following capabilities and task-solving performance is an important direction for future work. Furthermore, this paper focuses on limited set of six representative instructions. To gain more comprehensive understanding, future work will explore larger and more diverse datasets to identify broader range of Instruction-Specific Neurons and Experts, thereby enhancing the generalizability of our findings."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Ahsan Bilal, David Ebert, and Beiyu Lin. 2025. Llms for explainable ai: comprehensive survey. arXiv preprint arXiv:2504.00125. Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. Deep reinforcement learning from human preferences. Preprint, arXiv:1706.03741. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2021. Knowledge neuarXiv preprint rons in pretrained transformers. arXiv:2104.08696. Yunkai Dang, Mengxi Gao, Yibo Yan, Xin Zou, Yanggan Gu, Aiwei Liu, and Xuming Hu. 2024. Exploring response uncertainty in mllms: An empirical evaluation under misleading scenarios. arXiv preprint arXiv:2411.02708. Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. 2024. Parameter-efficient finetuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608. Kaichen Huang, Jiahao Huo, Yibo Yan, Kun Wang, Yutao Yue, and Xuming Hu. 2024. Miner: Mining the underlying pattern of modality-specific neurons in multimodal large language models. arXiv preprint arXiv:2410.04819. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2025. survey on mixture of experts in large language models. IEEE Transactions on Knowledge and Data Engineering. Zihao Huang, Qiyang Min, Hongzhi Huang, Defa Zhu, Yutao Zeng, Ran Guo, and Xun Zhou. 2025. Ultra-sparse memory network. Preprint, arXiv:2411.12364. Erik Cambria, Lorenzo Malandri, Fabio Mercorio, Navid Nobani, and Andrea Seveso. 2024. Xai meets llms: survey of the relation between explainable ai and large language models. arXiv preprint arXiv:2407.15248. Jianhui Chen, Xiaozhi Wang, Zijun Yao, Yushi Bai, Lei Hou, and Juanzi Li. 2024. Finding safety neurons in large language models. arXiv preprint arXiv:2406.14144. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Jiahao Huo, Yibo Yan, Boren Hu, Yutao Yue, and Xuming Hu. 2024. Mmneuron: Discovering neuron-level domain-specific interpretation in multimodal large language model. arXiv preprint arXiv:2406.11193. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. 1991. Adaptive mixtures of local experts. Neural computation, 3(1):7987. Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Tianyi Alex Qiu, Juntao Dai, and Yaodong Yang. 2024. Aligner: Efficient alignment by learning to correct. Advances in Neural Information Processing Systems, 37:9085390890. Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv, abs/2310.06825. Michael Jordan and Robert Jacobs. 1994. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214. 9 Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, arXiv:1705.03551. Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, Peifeng Wang, Silvio Savarese, et al. 2025. survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems. arXiv preprint arXiv:2504.09037. Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka, and Yutaka Matsuo. 2024. On the multilingual ability of decoder-based pre-trained language models: Finding and controlling language-specific neurons. arXiv preprint arXiv:2404.02431. Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Fahad Shahbaz Khan, and Salman Khan. 2025. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. 2025a. Uni-moe: Scaling unified multimodal llms with mixture of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025b. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning, et al. 2024. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947. Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. 2024. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2023. Circuit component reuse across tasks in transformer language models. arXiv preprint arXiv:2310.08744. Fuseini Mumuni and Alhassan Mumuni. 2025. Explainable artificial intelligence (xai): from inherent explainability to large language models. arXiv preprint arXiv:2501.09967. Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. 2024. Fine-tuning enhances existing mechanisms: case study on entity tracking. arXiv preprint arXiv:2402.14811. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Preprint, arXiv:2305.18290. Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi. 2024. Tagllm: Repurposing general-purpose llms for specialized domains. arXiv preprint arXiv:2402.05140. Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, and Ming Jin. 2024. Time-moe: Billion-scale time series foundation models with mixture of experts. arXiv preprint arXiv:2409.16040. Ran Song, Shizhu He, Shuting Jiang, Yantuan Xian, Shengxiang Gao, Kang Liu, and Zhengtao Yu. 2024. Does large language model contain task-specific neurons? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 71017113. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. Learning to summarize from human feedback. Preprint, arXiv:2009.01325. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, and Ji-Rong Wen. 2024. Language-specific neurons: The key to multilingual capabilities in large language models. arXiv preprint arXiv:2402.16438. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. Preprint, arXiv:2301.13688. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Yibo Yan, Shen Wang, Jiahao Huo, Hang Li, Boyan Li, Jiamin Su, Xiong Gao, Yi-Fan Zhang, Tianlong Xu, Zhendong Chu, et al. 2024b. Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection. arXiv preprint arXiv:2410.04509. Yibo Yan, Shen Wang, Jiahao Huo, Jingheng Ye, Zhendong Chu, Xuming Hu, Philip Yu, Carla Gomes, Bart Selman, and Qingsong Wen. 2025. Position: Multimodal large language models can significantly advance scientific reasoning. arXiv preprint arXiv:2502.02871. Haoqi Yang, Luohe Shi, Qiwei Li, Zuchao Li, Ping Wang, Bo Du, Mengjia Shen, and Hai Zhao. 2025. Faster moe llm inference for extremely large models. arXiv preprint arXiv:2505.03531. Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, and Huajun Chen. 2024. Knowledge circuits in pretrained transformers. arXiv preprint arXiv:2405.17969. Yichao Yuan, Lin Ma, and Nishil Talati. 2025. Moelens: Towards the hardware limit of high-throughput moe llm serving under resource constraints. arXiv preprint arXiv:2504.09345. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792. Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. 2024. How do large arXiv language models handle multilingualism? preprint arXiv:2402.18815. Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, and Dianhui Chu. 2024a. survey on data selection for llm instruction tuning. arXiv preprint arXiv:2402.05123. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022a. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593. Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, and Fei Yang. 2024b. Parameter-efficient fine-tuning in large models: arXiv preprint survey of methodologies. arXiv:2410.19878. Weixuan Wang, Barry Haddow, Minghao Wu, Wei Peng, and Alexandra Birch. 2024c. Sharing matters: Analysing neurons across languages and tasks in llms. arXiv preprint arXiv:2406.09265. Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. 2022b. Finding skill neurons in pre-trained transformer-based language models. arXiv preprint arXiv:2211.07349. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. Preprint, arXiv:2212.10560. Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham Mehrotra, Xiang-Bo Mao, Sitaram Asur, et al. 2024d. comprehensive survey of llm alignment techniques: arXiv preprint Rlhf, rlaif, ppo, dpo and more. arXiv:2407.16216. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. Preprint, arXiv:2109.01652. Ting-Ruen Wei, Haowei Liu, Xuyang Wu, and Yi Fang. 2025. survey on feedback-based multi-step reasoning for large language models on mathematics. arXiv preprint arXiv:2502.14333. Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, Yucheng Shi, Fan Yang, Tianming Liu, Xiaoming Zhai, Wenlin Yao, Jundong Li, Mengnan Du, et al. 2024. Usable xai: 10 strategies towards exploiting explainability in the llm era. arXiv preprint arXiv:2403.08946. Yibo Yan, Jiamin Su, Jianxiang He, Fangteng Fu, Xu Zheng, Yuanhuiyi Lyu, Kun Wang, Shen Wang, Qingsong Wen, and Xuming Hu. 2024a. survey of mathematical reasoning in the era of multimodal large language model: Benchmark, method & challenges. arXiv preprint arXiv:2412.11936. 11 AI-Generated Instructions Dataset Post-Validation Prompt of Instruction Generation Classification: You are an expert instruction generator specialized in crafting diverse SENTIMENT CLASSIFICATION tasks. Please produce exactly 100 distinct instructions. Your output must meet the following requirements: the lengths of the instructions themselves should vary from short to long and they should cover different domains. Code: You are an expert instruction generator specialized in crafting diverse CODE tasks. Please produce exactly 100 distinct instructions. Your output must meet the following requirements: the lengths of the instructions themselves should vary from short to long and they should cover different levels. GeneralQA: You are an expert instruction generator specialized in crafting diverse GENERALQA tasks. Please produce exactly 100 distinct instructions. Your output must meet the following requirements: the lengths of the instructions themselves should vary from short to long and they should cover different domains. Generation: You are an expert instruction generator specialized in crafting diverse GENERATION tasks. Please produce exactly 100 distinct instructions. Your output must meet the following requirements: the lengths of the instructions themselves should vary from short to long and they should cover different domains. Math: You are an expert instruction generator specialized in crafting diverse MATH tasks. Please produce exactly 100 distinct instructions. Your output must meet the following requirements: the lengths of the instructions themselves should vary from short to long and they should cover different levels. Summarization: You are an expert instruction generator specialized in crafting diverse SUMMARIZATION tasks. Please produce exactly 100 distinct instructions. Your output must meet the following requirements: the lengths of the instructions themselves should vary from short to long and they should cover different domains. In the post-validation process, we engaged two graduate students and one PhD student majoring in computer science. Among them, the two graduate students served as junior annotators, and the PhD student served as the senior annotator. We provided compensation of 1000 Chinese RMB to each of the three annotators. In the initial phase of the validation process, the comprehensive review of the content was carried out by junior annotators, with focus on error checking. This included verifying whether the instructions contained obvious grammatical or spelling errors, identifying any redundancies, detecting potential classification errors, and ensuring that the instructions were expressed clearly and unambiguously. For the aspects of classification accuracy and clarity of the instructions, we collected suggestions from both annotators. In cases where discrepancies arose, these differences were flagged and subsequently referred to our senior annotator, who was responsible for making the final adjudication. Below, we present the specific evaluation forms provided to the annotators: Dataset Post-Validation Process Please answer the following questions. Instruction: Write Java application to perform matrix multiplication on two-dimensional arrays. Instruction Type: Code 1. Whether the instructions contain obvious grammatical or spelling errors? 2. Whether the instructions contain any redundancies? 3. Whether the instructions exist potential classification errors? 4. Whether the instructions are expressed clearly and unambiguously?"
        },
        {
            "title": "C Dataset Example",
            "content": "Concrete examples are provided in Table 3 for each of the six instruction types."
        },
        {
            "title": "D Implementation Details",
            "content": "To mitigate the excessive computational time required for generation, for the Jaccard similarity calculations and Pearson correlation coefficient involved in Figures 3 and 4, we conducted calculations on total of 300 randomly sampled instances. 12 Instruction Type Natural Instruction AI-Generated Instruction Classification Code GeneralQA Generation Math In this task, you are given text from tweets. Your task is to classify given tweet text into two categories: 1) positive, and 2) negative based on its content. From list of integers, remove all elements that occur more than once. Keep order of elements left the same as in the input. Can you evaluate the emotional response in this online marketplace comment? Write Java application to perform matrix multiplication on two-dimensional arrays. Who was the first female presenter of Blue Peter? Who is the ancient Egyptian deity associated with the afterlife? Come up with slogan to describe new lipstick product. Generate set of ethical guidelines for the use of AI in the healthcare industry. If 4 daps = 7 yaps, and 5 yaps = 3 baps, how many daps equal 42 baps? Determine the median of the set: [22, 17, 31, 28, 19]."
        },
        {
            "title": "Summarization",
            "content": "Given the background description of some cooking related query, summarize the question into title. Generate brief synopsis of book on the impact of the digital revolution on traditional media and journalism. Table 3: Dataset examples."
        },
        {
            "title": "Vanilla Models",
            "content": "Fine-tuned Models LLaMA-2-7B LLaMA-2-Chat-7B LLaMA-2-13B LLaMA-2-Chat-13B Mistral-7B-v0. Mistral-7B-Instruct-v0.1 Qwen1.5-MoE-A2.7B Qwen1.5-MoE-A2.7B-Chat Table 4: Vanilla and fine-tuned model names. Under this setup, each small square in the figures requires 2500 corresponding computations. Vanilla and Fine-tuned Models We list the specific names and provide links for all vanilla and fine-tuned models in Table 4 and 5."
        },
        {
            "title": "F Experts Activation Counts",
            "content": "For each instruction category, we count the total number of times each expert is activated out of two hundred instructions, and identify the top five most frequently activated experts by ID in Table 6. It can be observed that there is minimal variation in the top five experts across different types of instructions. Specifically, for the six types of instructions, total of 25 experts remained in the top five. 13 Model Link LLaMA-2-7B LLaMA-2-Chat-7B LLaMA-2-13B LLaMA-2-Chat-13B Mistral-7B-v0.1 Mistral-7B-Instruct-v0.1 Qwen1.5-MoE-A2.7B Qwen1.5-MoE-A2.7B-Chat https://huggingface.co/meta-llama/Llama-2-7b-hf https://huggingface.co/meta-llama/Llama-2-7b-chat-hf https://huggingface.co/meta-llama/Llama-2-13b-hf https://huggingface.co/meta-llama/Llama-2-13b-chat-hf https://huggingface.co/mistralai/Mistral-7B-v0.1 https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat Table 5: Model links. Model Task Category Classification Qwen-MoE-2.7B-Chat Qwen-MoE-2.7B Expert ID Activation Counts (k) Expert ID Activation Counts (k) 11 12.9 11 13.5 52 12.8 52 13. 25 12.3 25 12.6 58 11.9 36 12.5 36 11.8 35 12. 51 11.8 51 11.7 17 11.8 27 11.6 Code 27 11. 19 11.6 GeneralQA 52 11.1 17 11.4 19 11.1 7 11. 52 6.6 52 6.5 3 6.0 42 6.1 58 6.0 3 6. 51 5.8 58 5.9 Model Task Category Generation Qwen-MoE-2.7B-Chat Qwen-MoE-2.7B Expert ID Activation Counts (k) Expert ID Activation Counts (k) 52 6.0 52 6.1 51 6. 51 5.9 11 5.6 11 5.6 25 5.2 25 5.4 21 5. 21 5.3 17 11.7 42 11.9 51 11.6 52 11.7 Math 19 11.4 19 11.7 Summarization 52 11.4 7 11.5 42 11. 22 11.5 11 11.5 11 12.0 52 10.5 52 10.9 51 10. 25 10.5 25 10.1 19 9.9 42 5.8 53 5.7 19 9. 51 9.8 Table 6: Top five experts activation counts by instruction types. The highlighted experts represent those that remain among the top five most frequently activated experts before and after fine-tuning."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}