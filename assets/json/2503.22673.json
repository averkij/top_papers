{
    "paper_title": "ActionStudio: A Lightweight Framework for Data and Training of Large Action Models",
    "authors": [
        "Jianguo Zhang",
        "Thai Hoang",
        "Ming Zhu",
        "Zuxin Liu",
        "Shiyu Wang",
        "Tulika Awalgaonkar",
        "Akshara Prabhakar",
        "Haolin Chen",
        "Weiran Yao",
        "Zhiwei Liu",
        "Juntao Tan",
        "Juan Carlos Niebles",
        "Shelby Heinecke",
        "Huan Wang",
        "Silvio Savarese",
        "Caiming Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 2 3 7 6 2 2 . 3 0 5 2 : r ActionStudio: Lightweight Framework for Data and Training of Large Action Models Jianguo Zhang, Thai Hoang, Ming Zhu, Zuxin Liu, Shiyu Wang, Tulika Awalgaonkar, Akshara Prabhakar, Haolin Chen, Weiran Yao, Zhiwei Liu, Juntao Tan, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong Salesforce AI Research {jianguozhang, thai.hoang,mingzhu, zuxin.liu}@salesforce.com"
        },
        {
            "title": "Abstract",
            "content": "Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data to facilitate research in the community. GitHub: github.com/SalesforceAIResearch/xLAM"
        },
        {
            "title": "Introduction",
            "content": "Action models are becoming increasingly critical for enabling autonomous agents to operate effectively across complex, multi-step tasks in diverse environments-from personal productivity assistants to real-world industrial automation systems. While recent open-source initiatives have advanced the development of large action models [15], scalable infrastructure for efficient agentic data processing and model training remains underdeveloped. central challenge lies in the nature of agentic training data, which often comprises long-horizon trajectories with tool interactions, observations, and user feedback originating from varied environments. While prior work has attempted to address data standardization [4, 5], existing solutions usually rely on instruction-following templates that abstract away tool use [3, 6] or adopt fixed formats that may not generalize across tasks. Moreover, the data conversion and quality control processes required to turn raw agent trajectories into training-ready datasets are seldom open-sourced, which limits reproducibility and adaptation to new domains. On the training side, general-purpose frameworks such as Transformers [7] and LLAMA-Factory [8] have played an important role in Large language model (LLM) development. However, these Co-first Authors. Core Contributor. Corresponding Authors. technical report for the ActionStudio framework. Figure 1: Framework of ActionStudio. frameworks are primarily designed and optimized for standard LLM workflows, requiring substantial customization to support agent-specific training. Even high-performing open-source models like xLAM [5] have not fully released their implementation code. This creates barriers for researchers and developers aiming to build agentic systems in real-world settings. To address these challenges, we present ActionStudio, lightweight and efficient data and training framework for large action models. Designed for production use, ActionStudio combines lightweight yet powerful backend with extensible support for supervised fine-tuning (SFT) and preference-based learning (e.g., DPO) with diverse setups, including LoRA training, quantization, and multi-node distributed training. Our framework is developed for real-world scalability while remaining flexible for academic experimentation. To support this, we also provide collection of ready-to-train datasets for speed-up model training. Furthermore, we demonstrate the effectiveness of ActionStudio through superior model performances on both public and realistic industry benchmarks. The key contributions of our work are: We introduce lightweight and extensible training framework specifically designed for large action models, supporting scalable and modular training workflows. We implement data pipeline that automates the ingestion, filtering, and conversion of diverse agent trajectories. We also release collection of high-quality, ready-to-train datasets in unified format to support streamlined model training. We demonstrate ActionStudios effectiveness across public and realistic industry benchmarks, showing its utility for real-world agent applications and research."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Agent Data While proprietary models often restrict data accessibility, the research community has significantly advanced open-source initiatives by releasing extensive agent datasets [1, 912, 4, 6, 13]. However, due to the inherent complexity and heterogeneous nature of agent trajectories across different environments, datasets often vary widely in format, creating substantial hurdles for industrial-scale model development. Recent efforts such as Lumos [3], AgentOhana [4], and xLAM [5] have aimed to standardize datasets into unified formats to reduce errors and simplify training. Nonetheless, these initiatives have not fully open-sourced the data conversion and automation pipelines, limiting widespread adoption and scalability. 2.2 Action Models Beyond proprietary model APIs, significant progress has been made toward developing open-source large action models, specifically tailored for complex agent-oriented tasks [2, 14, 5, 15]. These models have achieved impressive performance on benchmarks, showing the growing capability of open-source initiatives. While established frameworks like Transformers [7] and LLAMA-Factory [8] facilitate general language model training, specialized frameworks designed explicitly for fine-tuning large action models remain limited. Our work contributes directly to addressing this gap by providing 2 an efficient, lightweight training pipeline within ActionStudio, significantly reducing the complexity and resource requirements associated with training high-performing large action models."
        },
        {
            "title": "3 Framework",
            "content": "To support the development of high-performing large action models (LAMs), we present ActionStudio, comprehensive framework that unifies data pre-processing and model training under single, modular system. ActionStudio is composed of two core pipelines: data pipeline for standardizing and preparing diverse agentic data sources, and training pipeline for fine-tuning large language models on agent tasks at various scales. 3.1 Data Pipeline The data pre-processing pipeline in ActionStudio is designed to convert diverse agentic data sources into standardized, training-ready formats. It includes four major components: data collection, format unification, quality filtering, and format conversion. This modular structure ensures the framework is extensible, scalable and compatible with wide range of agent environments and models. Data Collection. To support agentic model training, we compile diverse set of high-quality datasets from multiple agent environments and domains, including function calling, tool-use, and robotic agent trajectories. The datasets vary in structure and components, which poses significant challenges for LAM training. Format Unification. Given the heterogeneity of data formats across various sources, we introduce Unified Format 2.0, an upgraded version of prior data schema [5]. Unified Format 2.0 is designed to be compatible with both Alpaca-like [16] \"(input, output)\" data and ShareGPT-like [17] dialogue format, as well as general chat-style LLM training, aligning with the application of widely adopted LLM chat templates. It modularizes agent trajectories into well-defined components such as task instruction, available tools, user-agent conversation which includes tool calls and tool execution results. It simplifies downstream processing and ensures data consistency across varied sources. An example of trajectory in Unified Format 2.0 is shown in Figure 2. We also release selected collection of curated datasets in Unified Format 2.0 alongside the ActionStudio framework to facilitate an efficient and seamless start to LAM training. Quality Filtering. Following prior work [18, 5], we leverage LLMs to automatically evaluate and filter trajectory quality in order to mitigate the cost of manual annotation. Trajectories are scored on dimensions such as correctness, hallucinations, tool use appropriateness, and overall response quality. Despite recent improvements in LLM evaluators, we observed tendency toward median scores and challenges in catching subtle hallucinations. To address this, we apply in-context examples and fine-tune critic models to better align with human evaluations. Our current setup achieves approximately 70% agreement with human judges and provides reliable filtering at industrial scale. Format Conversion. Once data has been unified and filtered, we convert it into training-ready datasets by slicing the trajectory into turns, and then apply Jinja-based chat templates from the tokenizer of models to train. ActionStudio also supports real-time chat template application and error checking via built-in data verifier. 3.2 Training Pipeline To support training agentic models under industry context, we need powerful backend that support diverse efficient training methods, yet straightforward for customization and debugging. This allows us to effectively experiments with newest technologies at different scales. We have implemented our training pipeline using PyTorch [19] with model architectures from Transformers [7], distributive training engines from DeepSpeed [20] and Accelerate [21]. 3.2.1 Model Handler It is essential to load and store the model efficiently during training process. For this, we utilized the Transformers library, employing AutoModelForCausalLM and AutoTokenizer to support wide range of models. For scalable distributed training, we integrated DeepSpeed and Accelerator to 3 shard model parameters, gradient, and optimizers, enabling efficient multi-GPU training across single or multiple pods. To optimize resource usage, especially for large models, we incorporated 4-bit quantization via BitsAndBytes [22], significantly reducing memory and computation needs without sacrificing performance. Additionally, we implemented LoRA training using the Peft library [23] for parameter-efficient fine-tuning, focusing on low-rank updates to minimize resource consumption. This unified framework offers flexibility, scalability, and efficiency, catering to diverse training scenarios and constraints. 3.2.2 Data Handler To ensure flexibility and control in preparing data for agentic model training, we developed an agent-focused pipeline to handle data, aim to process and organize training data efficiently while maintaining adaptability for various agentic tasks. Below are the key components of the pipeline: Training data parsing Using datasets formatted under Unified Format 2.0 along with provided chat template, the handler parses instances into training-ready structures. Instances containing invalid or incomplete data, such as missing fields or mismatched templates that frequently occur in real-world industrial settings, are identified and excluded. Valid data is organized into two main structures: \"input\" (containing prompts and context) and \"chosen\" (representing desired outputs). For DPO (Direct Preference Optimization) [24] trainings, an additional \"reject\" field is included to indicate undesirable outputs. Interleave datasets Training frequently involves combining multiple datasets, possibly with different weights. Our system simplifies this process by allowing users to conveniently specify dataset names and the desired number of training iterations in YAML file. The system then automatically calculates the appropriate sample ratios and generates mixed dataset that combines the provided datasets accordingly. Data Verifier Given the complexity of processing industry-level agent data trajectories, potential errors during conversion are mitigated through robust Data Verifier. It checks that all data samples conform to predefined format requirements, identifying deviations and providing corresponding warnings for users. Additionally, the verifier supports visualization of processed samples, helping to detect potential alignment issues or edge cases in data trajectories and enabling effective debugging. Modular Design We designed highly modular and lightweight procedure to iterate over data during training, enabling exceptional customizability for each sub-process. This includes: 1) preparing training-ready agentic data, 2) filtering bad data (e.g., incorrect formats or excessive lengths), 3) encoding components of data, 4) applying loss masking if required, and 5) performing post-processing tasks such as merging, padding, and creating attention masks. The modular structure enables users to easily customize each component to meet their specific requirements, such as implementing custom prompt loss masking strategies or adding additional data filtering layers. This adaptable framework empowers users to fine-grain the process to align with their unique objectives. 3.2.3 Training Handler Training methods To develop high-performing agentic models, prior research typically employs two-step process: Supervised Instruction Fine-Tuning (SFT), followed optionally by Human Preference Alignment (HPA). For SFT, we constructed custom training pipeline using PyTorch [25], Accelerate [21], and DeepSpeed [20]. This setup grants us full control over the training process, enabling us to fine-tune critical optimization details, such as configuring gradient checkpointing and optimizing model, data, or optimizer sharding, to fully utilize GPU resources. For HPA, we noted that training is typically much faster due to the significantly smaller dataset sizes compared to those used in pretraining or SFT. Consequently, we chose to implement Direct Preference Optimization (DPO) using the TRL Trainer [26]. Distributed Training Our framework supports training across single GPUs, multiple GPUs within the same pod, and multiple GPUs across different pods within the same network. We leverage Accelerate for communication in distributed training and DeepSpeed ZeRO for parallelism and sharding. Specifically, ZeRO 2 allows us to shard the optimizer states and gradient, while ZeRO 3 further shards model parameters for improved efficiency under distributed training setup. To prevent compatibility issues when library versions are updated, we implemented custom logic to manually extract and correctly save distributed model weights, reducing the risk of errors. Optimizations To efficiently facilitate large-scale agentic model training in industry settings, our pipeline incorporates additional optimizations, including DeepSpeeds model parameters and optimizer offloading to CPU, gradient checkpointing, and Flash Attention 2 [27]. Monitoring & Logging To monitor training progress such as tracking of losses, learning rate schedules and GPU utilization, we integrate Weights & Biases (wandb) [28] into the pipeline. We created straightforward access to detailed training metrics by simply configuring login credentials within the training scripts, ensuring effective management of the training process."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Model Training Utilizing the ActionStudio framework, we conducted supervised fine-tuning for xLAM [5] new series on selected open-source models including Mistral series [29, 30] and Llama 3 series [31]. To showcase ActionStudios effectiveness across various sizes, we fine-tune models ranging from smaller-scale models such as Mistral variants to larger-scale models like Llama-3.1-70b-inst and Mixtral-8x22b-inst. We set the sequence length between 8k and 16k, the batch size between 32 and 96 and the learning rate between 2e-6 and 2e-5, employing cosine learning rate scheduler with 5% warm-up steps. Smaller models were fine-tuned on single NVIDIA H200 pod, while larger models were fine-tuned on both single and more H200 pods for comparison. Each H200 pod is equipped with 8 NVIDIA H200 GPUs, each having 141GB of memory. 4.2 Benchmarks To demonstrate the effectiveness of ActionStudio, we select two benchmarks, NexusRaven and CRM Agent Bench. NexusRaven [32] provides diverse benchmark for function calling, comprising 318 test examples across 65 distinct APIs. The dataset is curated through structured pipeline that mines function definitions, docstrings, and execution contexts from open-source corpora. LLMs are then prompted to generate natural language queries, Chain-of-Thought (CoT) traces, and hard-negative function candidates to enhance evaluation difficulty. NexusRaven specifically evaluates model performance using precision, recall, and F1-score metrics for both function retrieval and argument inference tasks, offering comprehensive assessment framework for function calling capabilities. The CRM Agent Benchmark 4 is proprietary evaluation developed by Salesforce. It assesses proficiency of AI models across critical and real CRM agent scenarios, focusing on accuracy in agent topic identification, accurate generation of function calls, and creation of contextually appropriate free-text responses. This benchmark emphasizes realistic business use cases by incorporating several hundred real CRM data points with expert assessments, providing valuable insights into the practical utility and reliability of language models for commercial deployment. Importantly, for all experiments, we fine-tune models exclusively on public datasets, ensuring that no customer data from any companies is utilized during training. 4.3 NexusRaven Table 1 shows the comparative performance of various models evaluated on NexusRaven. ActionStudio-trained models consistently outperform baseline and prominent commercial models. In particular, our fine-tuned xLAM-Mixtral-8x22b-inst-exp model achieves the highest overall F1-score (0.969), reflecting strong precision and recall, significantly surpassing commercial alternatives such 4https://www.salesforceairesearch.com/crm-benchmark 5 Model xLAM-Llama-3.3-70b-inst-exp xLAM-Llama-3.1-70b-inst-exp xLAM-Mixtral-8x22b-inst-exp xLAM-Mistral-latest-12b-inst-exp xLAM-Mistral-7b-inst-exp Llama-3.3-70b-inst [31] Mistral-latest-12b-inst [30] Llama-3.1-70b-inst [31] GPT-4o [33] DeepSeek-r1-671b [34] Mistral-7b-inst [29] Mixtral-8x22b-inst [30] GPT-4 [35] Papi 0.950 0.940 0.969 0.953 0.884 0.917 0.906 0.907 0.943 0.837 0.814 0.758 0.894 Rapi 0.953 0.943 0.969 0.956 0.884 0.934 0.940 0.915 0.840 0.840 0.827 0.786 0.635 F1api 0.951 0.942 0.969 0.954 0.884 0.925 0.923 0.911 0.889 0.838 0.821 0.772 0. Table 1: Performance comparison on NexusRaven. The best-performing result is indicated in bold, while the second and third-best results are marked with underline. as GPT-4 and GPT-4o. Similarly, other fine-tuned models also exhibit robust performance. These results show the effectiveness of ActionStudios pipeline in enhancing function-calling capabilities. 4.4 CRM Agent Bench Topic Acc Function Call Acc Free Text Acc Average Acc xLAM-Llama-3.3-70b-inst-exp xLAM-Llama-3.1-70b-inst-exp xLAM-Mixtral-8x22b-inst-exp xLAM-Mistral-latest-12b-inst-exp xLAM-Mistral-7b-inst-exp DeepSeek-r1-671b [36] o1-preview [37] Llama-3.3-70b-inst [31] GPT-4-turbo [35] Llama-3.1-70b-inst [31] Mixtral-8x22b-inst [30] GPT-4o-mini [33] Mistral-latest-12b-inst [30] Mistral-7b-inst [29] 0.98 0.96 0.98 0.98 0.95 0.82 0.98 0.99 0.99 1.0 0.98 0.94 0.96 0.99 0.79 0.77 0.75 0.64 0.49 0.83 0.75 0.72 0.60 0.62 0.65 0.42 0.18 0. 0.83 0.86 0.82 0.78 0.74 0.94 0.81 0.80 0.92 0.82 0.60 0.81 0.70 0.63 0.87 0.86 0.85 0.80 0.73 0.86 0.85 0.84 0.83 0.81 0.74 0.72 0.61 0.60 Table 2: Accuracy on the CRM Agent Benchmark. The best-performing result is indicated in bold, while the second and third-best results are marked with underline. Table 2 illustrates our performance on the Salesforce CRM industry Agent Benchmark. Our xLAMLlama-3.3-70b-inst-exp model achieves the highest overall performance with an average accuracy of 0.87, surpassing the base Llama-3.3-70b-inst model (0.84). This reflects balanced capabilities across all three dimensions. Similarly, the xLAM-Llama-3.1-70b-inst-exp shows robust performance. Furthermore, our fine-tuned xLAM-Mixtral-8x22b-inst-exp significantly outperforms its base, Mixtral-8x22b-inst, by 11%. Models such as xLAM-Mistral-7b-inst-exp also exhibit marked improvements of 13% compared to their instruct baselines, indicating the efficacy of targeted fine-tuning in enhancing smaller model capabilities. Additionally, our models also outperform strong baseline models such as o1-preview and GPT-4-turbo, demonstrating superior adaptability and effectiveness. These findings highlight ActionStudios capability to train versatile and reliable models that are essential for practical and real CRM applications. 4.5 Ablation Study Table 3 compares Mixtral-8x22b models fine-tuned on processed versus raw data using the Salesforce CRM Agent Benchmark. The xLAM-Mixtral-8x22b-inst-exp (processed) model significantly 6 w/ FT on ActionStudio Processed Data w/ FT on Raw Data Mixtral-8x22b-inst 0.98 0.98 0. 0.75 0.44 0.65 0.82 0.75 0.60 0.85 0.72 0.74 Topic Function Call Free Text Average Table 3: Accuracy on the CRM Agent Benchmark. outperforms its raw-data counterpart, achieving higher function call accuracy (0.75 vs. 0.44) and free-text accuracy (0.82 vs. 0.75), resulting in superior overall accuracy (0.85 vs. 0.72). Importantly, the raw data fine-tuning model (xLAM-Mixtral-8x22b-inst-exp) degrades performance compared to the baseline model, particularly in function call accuracy (0.44 vs. 0.65). In contrast, the processed model outperforms the baseline across all metrics. These results demonstrate that while naive finetuning with raw agent data can harm model capabilities, ActionStudios data preprocessing pipeline effectively filters out noisy examples, producing robust and reliable performance on real industry scenarios. Training Setup Llama 3.1 8B (BS/GPU=6, Seq=8k) Mixtral 8x7B (BS/GPU=8, Seq=4k) Mixtral 8x22B (BS/GPU=8, Seq=4k) Mixtral 8x22B (BS/GPU=1, Seq=32k) Quantized + LoRA LoRA FT 1 pod FT 2 pods FT 4 pods 79,306 46,193 14,703 15,236 76,766 47,404 14,654 15, 64,179 33,661 OOM OOM 125,097 71,858 OOM OOM 224,192 137,146 44,438 46,861 Table 4: Training throughput (tokens/second) for various setups using our ActionStudio framework. LoRA indicates training with LoRA on one pod, while FT refers to Full Training. Quantized models use NF4, whereas others use BF16. OOM denotes insufficient memory for the specified setup. Batch size and sequence length per GPU are abbreviated as BS/GPU and Seq, respectively. 4.6 Training Efficiency To demonstrate the training efficiency of ActionStudio framework, we conducted supervised finetuning experiments on three model sizes, Llama 3.1 8B, Mixtral 8x7B, and Mixtral 8x22B, representing small, medium, and large models, respectively. For LoRA models, we applied fine-tuning to the q_proj,v_proj,k_proj,o_proj with lora_r 32 and lora_a 64. The average training throughput, in tokens per second, is summarized in Table 4. As highlighted in the table, our framework delivers outstanding training throughput across all configurations. Specifically, NF4 quantized fine-tuning for LoRA achieved throughput rates of 79,306 tokens/sec for Llama 3.1 8B, 46,193 tokens/sec for Mixtral 8x7B, 14,703 and 15,236 tokens/sec for Mixtral 8x22B. Similarly, BF16 fine-tuning delivered comparable throughput with 76,766, 47,404, 14,654, and 15,430 tokens/sec, respectively. Notably, full fine-tuning, which updates all model weights, was only 16% to 30% slower than LoRA on single pod, demonstrating the efficiency of our framework even for computationally intensive setups. Our framework also demonstrated excellent scalability. Throughput consistently doubled when scaling from one pod to two, trend that held true when scaling further to four pods. Additionally, our framework supports long-context training without any performance degradation. Here, Mixtral 8x22B achieves slightly improved performance when trained with batch size of 1 and sequence length of 32k, compared to batch size of 8 and sequence length of 4k. These results demonstrate the efficiency and scalability of ActionStudio for training large-scale agentic models, positioning it as powerful tool for advancing model development."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced ActionStudio, lightweight and flexible framework for training large action models. By integrating structured data preprocessing, advanced fine-tuning, and distributed training, ActionStudio simplifies agentic model development. Evaluations on NexusRaven and the Salesforce CRM Agent Benchmark, which specifically reflects realistic industry scenarios, demonstrated its effectiveness and practical value for robust agentic model solutions."
        },
        {
            "title": "6 Limitations",
            "content": "While ActionStudio provides practical and extensible framework for developing robust large action models for industry agent scenarios, few limitations remain. The current implementation primarily focuses on text-based and function-calling scenarios, with future support planned for multimodal and embodied environments. Additionally, although the framework includes standardized formats and robust data processing pipeline, model effectiveness still depends on the quality and diversity of input datasets, which can vary across use cases. Despite these challenges, ActionStudio significantly lowers the barrier to LAM development and offers scalable foundation for further innovation in industry and research."
        },
        {
            "title": "References",
            "content": "[1] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023. [2] Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing natural language and code for language agents. arXiv preprint arXiv:2310.06830, 2023. [3] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs. arXiv preprint arXiv:2311.05657, 2023. [4] Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. Agentohana: Design unified data and training pipeline for effective agent learning. arXiv preprint arXiv:2402.15506, 2024. [5] Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, et al. xlam: family of large action models to empower ai agent systems. arXiv preprint arXiv:2409.03215, 2024. [6] Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024. [7] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 3845, 2020. [8] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. [9] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [10] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023. [11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [12] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 8 [13] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. arXiv preprint, 2024. [14] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. ICLR, 2024. [15] Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920, 2024. [16] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instructionfollowing model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. [17] Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Silvio Savarese, and Caiming Xiong. Dialogstudio: Towards richest and most diverse unified dataset collection for conversational ai. arXiv preprint arXiv:2307.10172, 2023. [18] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. [19] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). ACM, April 2024. [20] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [21] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. [22] Tim Dettmers. bitsandbytes. https://github.com/bitsandbytes-foundation, 2021. [23] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https: //github.com/huggingface/peft, 2022. [24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290, 2023. [25] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. 9 [26] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. [27] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [28] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from wandb.com. [29] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [30] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [31] Aaron Gratt., Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [32] Venkat Krishna Srinivasan, Zhen Dong, Banghua Zhu, Brian Yu, Damon Mosk-Aoyama, Kurt Keutzer, Jiantao Jiao, and Jian Zhang. Nexusraven: commercially-permissive language model for function calling. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. [33] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [34] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [35] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [36] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [37] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Unified Format 2.0 To support diverse agentic tasks in model-friendly way, we introduce Unified Format 2.0, an upgraded version of the format used in prior work. While Unified Format 1.0 was designed to modularize agentic trajectories for general-purpose processing, it lacked alignment with the messagepassing format expected by modern LLM APIs. Unified Format 2.0 is designed to be natively compatible with modern chat-based LLM APIs and HuggingFace-style chat templates, significantly reducing the effort required to convert raw data into model-ready training samples. An example of trajectory in Unified Format 2.0 is shown in Figure 2. Unified Format 1.0 [5] introduced modular schema for representing agentic interaction data, including fields such as task_instruction, query, tools, and list of steps that capture tool calls, intermediate thoughts, user follow-ups, and observations. While effective for general processing and augmentation, this format was not directly aligned with the message-based structure expected by most open-source LLMs, requiring non-trivial conversion logic to adapt the data for training. An example showcasing Unified Format 1.0 is presented in Figure 4. In contrast, Unified Format 2.0 structure is based on the conversational schema commonly used in APIs like OpenAI and HuggingFace. It replaces the steps field with conversation list, where each entry is message with specific role (e.g., system, user, assistant, or tool). Tool calls are now explicitly represented inside assistant messages using tool_calls field, and tool responses are mapped to messages with the role tool, linking back via tool_call_id. This structure is more compatible with LLM APIs and chat templates, which removes the need for custom scripts to flatten or restructure training samples. Figure 3 demonstrates that with Unified Format 2.0, the training format can be flexibly changed by applying the corresponding chat template from the tokenizer. In contrary, Figure 5 shows the fixed training format from Unified Format 1.0, which remains unchanged across different models, requiring substantial effort in data format conversion for both training phase and deployment phase. A.2 Evaluation Details for NexusRaven To evaluate function-calling performance on the NexusRaven benchmark, we follow two-step process: (1) parsing tool calls from the models output, and (2) computing precision, recall, and F1 scores by comparing the parsed predictions to ground-truth annotations. Tool Call Parsing. NexusRaven includes custom parser designed to extract structured tool call information from raw model outputs. Since different models may follow varying output formats (e.g., enclosing tool calls in special tags, including JSON blocks with markdown fencing, or appending irrelevant tokens), the parser applies series of heuristics to sanitize the output and isolate the tool call content. It then parses the cleaned text into structured format containing: 1) the function/tool name, 2) the arguments as dictionary of key-value pairs, and 3) an optional tool call ID. The parser handles edge cases such as missing fields, extraneous formatting tags (e.g., <think>, <tool_call>), and malformed JSON. Metric Computation. After extracting the predicted and ground-truth tool calls, we compute evaluation metrics at the API level level, which includes the Precision (P_api), Recall (R_api), and F1 Score (F1_api). P_api and R_api calculate the proportion of predicted tool calls whose function name matches one in the ground-truth, and the proportion of ground-truth tool calls that were successfully predicted, respectively. The F1_api is the harmonic mean of API precision and recall. This enables consistent and scalable comparison of function-calling capabilities across models, while maintaining tolerance to minor formatting differences. 11 { \" q _ j o _ \" : \" id \" , \" task_instruction \" : \" ... \" , \" tools \" : [ { \" type \" : \" function \" , \" function \" : { \" name \" : \" get_sleep_stats \" , \" description \" : \" Get the user sleep statistics for specified time period . \" , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" user_id \" : { \" type \" : \" string \" , \" description \" : \" Unique identifier of the user whose sleep statistics will be retrieved . \" , } , } , \" required \" : [ \" user_id \" , ] } } } , ] , \" conversation \" : [ \" role \" : \" user \" , \" content \" : \" would like to get my sleep statistics from last night . \" { } , { \" role \" : \" assistant \" , \" content \" : \" \" , \" tool_calls \" : [ { } \" type \" : \" function \" , \" function \" : { \" name \" : \" get_sleep_stats \" , \" arguments \" : { \" user_id \" : \" 1 2 3 4 \" , } } , \" id \" : \" 8 0 8 3 8 0 8 0 6 \" ] } , { \" role \" : \" tool \" , \" name \" : \" get_sleep_stats \" , \" content \" : { \" data \" : { \" message \" : \" ... \" } } , \" tool_call_id \" : \" 8 0 8 3 8 0 8 0 6 \" } , { \" role \" : \" assistant \" , \" content \" : \" Your sleep statistics from last night has been retrived successfully . \" } ] } Figure 2: Unified format 2.0 for function calling data. 12 Prompt: <begin_of_text><start_header_id>system<end_header_id> Environment: ipython Cutting Knowledge Date: December 2023 Today Date: 26 Jul 2024 <eot_id><start_header_id>user<end_header_id> Given the following functions, please respond with JSON for function call with its proper arguments that best answers the given prompt. Respond in the format {\"name\": function name, \"arguments\": dictionary of argument name and its value}. Do not use variables. { \"type\": \"function\", \"function\": { \"name\": \"get_sleep_stats\", \"description\": \"Get the users sleep statistics for specified time period.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"string\", \"description\": \"Unique identifier of the user whose sleep statistics will be retrieved.\" } }, \"required\": [ \"user_id\" ] } } } would like to get my sleep statistics from last night.<eot_id> Output: [{\"name\": \"get_sleep_stats\", \"arguments\": {\"user_id\": \"1234\"}}] Figure 3: Example prompt and output for function-calling from unified format 2.0, by applying Llama-3.1-70BInstruct chat template. 13 { \" q _ j o _ \" : \" id \" , \" task_instruction \" : \" ... \" , \" ew_ sho t_ exa mpl es \" : [ ] , \" query \" : \" The task or the question that the user provides . \" , \" tools \" : [ { \" name \" : \" api_name 1 \" , \" description \" : \" description of this api \" , \" parameters \" : { \" param 1 \" : { \" type \" : \" string \" , \" description \" : \" \" , } , } } , ] , \" steps \" : [ { \" thought \" : \" thinking and / or planning process \" , \" tool_calls \" : [ { } \" name \" : \" api_name 1 \" , \" arguments \" : { \" argument 1 \" : \" xxx . \" , \" argument 2 \" : \" xxx \" } ] , \" step_id \" : 1 , \" next_observation \" : \" observations or feedbacks from the environment / APIs after execution function . \" \" user_input \" : \" User follow up input at this turn if any . \" } , ] , } Figure 4: Unified format 1.0 for function calling data. 14 Prompt: [BEGIN OF TASK INSTRUCTION] Based on the previous context and API request history, generate an API request or response as an AI assistant. [END OF TASK INSTRUCTION] [BEGIN OF AVAILABLE TOOLS] [ { \"name\": \"get_fire_info\", \"description\": \"Query the latest wildfire information\", \"parameters\": { \"location\": { \"type\": \"string\", \"description\": \"Location of the wildfire.\", \"required\": true, }, \"radius\": { \"type\": \"number\", \"description\": \"The radius (in miles) around the location.\", } }, },... ] [END OF AVAILABLE TOOLS] [BEGIN OF FORMAT INSTRUCTION] Your output should be in the JSON format, which specifies list of function calls. The example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make tool_calls an empty list \"[]\". {\"thought\": \"the thought process, or an empty string\", \"tool_calls\": [{\"name\": \"api_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}}]} [END OF FORMAT INSTRUCTION] [BEGIN OF QUERY] Can you give me the latest information on the wildfires occurring in California? [END OF QUERY] [BEGIN OF HISTORY STEPS] [ { }, \"thought\": \"Sure, what is the radius (in miles) around the location of the wildfire?\", \"tool_calls\": [], \"step_id\": 1, \"next_observation\": \"\", \"user_input\": \"User: Let me think... 50 miles.\" ] [END OF HISTORY STEPS] Output: {\"thought\": \"\", \"tool_calls\": [{\"name\": \"get_fire_info\", \"arguments\": {\"location\": \"California\", \"radius\": 50}}]} Figure 5: Example prompt and output for function-calling from unified format 1.0."
        }
    ],
    "affiliations": [
        "Salesforce AI Research"
    ]
}