{
    "paper_title": "Improving Video Generation with Human Feedback",
    "authors": [
        "Jie Liu",
        "Gongye Liu",
        "Jiajun Liang",
        "Ziyang Yuan",
        "Xiaokun Liu",
        "Mingwu Zheng",
        "Xiele Wu",
        "Qiulin Wang",
        "Wenyu Qin",
        "Menghan Xia",
        "Xintao Wang",
        "Xiaohong Liu",
        "Fei Yang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Yujiu Yang",
        "Wanli Ouyang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop a systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing a large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, a multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From a unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models by extending those from diffusion models. These include two training-time strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and Flow-DPO demonstrates superior performance compared to both Flow-RWR and standard supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs. Project page: https://gongyeliu.github.io/videoalign."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 1 9 3 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Improving Video Generation with Human Feedback",
            "content": "Jie Liu1,3,5* Gongye Liu2,3* Jiajun Liang3 Ziyang Yuan2,3 Xiaokun Liu3 Mingwu Zheng3 Xiele Wu3,4 Qiulin Wang3 Wenyu Qin3 Menghan Xia3 Xintao Wang3 Xiaohong Liu4 Fei Yang3 Pengfei Wan3 Di Zhang3 Kun Gai3 Yujiu Yang2(cid:66) Wanli Ouyang1,5 1The Chinese University of Hong Kong 2Tsinghua University 3Kuaishou Technology 4Shanghai Jiao Tong University 5Shanghai AI Laboratory Equal contribution Project Leader (cid:66) Corresponding author Abstract Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models by extending those from diffusion models. These include two trainingtime strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inferencetime technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and FlowDPO demonstrates superior performance compared to both Flow-RWR and standard supervised fine-tuning methods. Additionally, FlowNRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs. Project page: https: //gongyeliu.github.io/videoalign. 1. Introduction Recent advances in video generation have yielded powerful models (Kong et al., 2024; Polyak et al., 2024; Kuaishou, 1 2024; Brooks et al., 2024), capable of producing videos with convincing details and relatively coherent motion. Despite these notable achievements, current video generation systems still face persistent challenges, including unstable motion, imperfect alignment with user prompts, unsatisfactory visual quality, and insufficient adherence to human preferences (Zeng et al., 2024). In language modeling and image generation, learning from human preference (Ouyang et al., 2022; Zhou et al., 2023; Wallace et al., 2024) has proved highly effective in improving the response quality and aligning generative models with user expectations. However, applying such preference-driven alignment strategies to video generation remains in its infancy. One key obstacle is the lack of large-scale, high-quality preference data. pioneering work (He et al., 2024) introduces human-rated video preference dataset, and concurrent studies (Wang et al., 2024b; Liu et al., 2024b; Xu et al., 2024a) have contributed additional annotations. Yet, these datasets primarily focus on videos generated by earlier open-source models, which are often of relatively low quality, contain artifacts, or remain limited in duration. With video generation techniques rapidly advancing, the gap between such datasets and the capabilities of state-of-the-art video diffusion models (VDMs) has become increasingly pronounced. second challenge arises from the internal mechanisms of cutting-edge video generation models. Many modern systems employ rectified flow (Liu et al., 2022; Lipman et al., 2022), predicting velocity rather than noise. Recent studies (Wang et al., 2024b; Zhang et al., 2024a; Liu et al., 2024b; Xu et al., 2024a) have tested DPO (Rafailov et al., 2024; Wallace et al., 2024) and RWR (Peng et al., 2019; Lee et al., 2023; Furuta et al., 2024) on diffusion-based video generation approaches. However, adapting existing alignment methods to these flow-based models introduces new questions. While concurrent work (Domingo-Enrich et al., 2024) extended Diffusion-DPO to flow-based image generation, the aligned flow-based models in these explorations underperformed their unaligned counterparts. Improving Video Generation with Human Feedback To address these limitations, we present comprehensive investigation into aligning advanced flow-based video generation models with human preferences. First, we select 12 advanced video generation models and curate new large-scale multi-dimensional preference dataset of approximately 182k annotated examples. The dataset encompasses three critical dimensionsVisual Quality (VQ), Motion Quality (MQ), and Text Alignment (TA)to better capture how users evaluate generated videos. We then developed multi-dimensional video reward model, systematically examining how annotations and different design choices affect its performance. Subsequently, we aggregated these multi-dimensional rewards into comprehensive metric to capture overall human preferences. Furthermore, from unified reinforcement learning perspective that maximizes reward with KL regularization, we introduce three alignment algorithms for flow-based models by extending those from diffusion models. These include two training-time strategiesDirect Preference Optimization for Flow (FlowDPO) and Reward Weighted Regression for Flow (FlowRWR)and an inference-time technique, Flow-NRG. Our derivation reveals that the parameter β (which modulates the KL divergence constraint) in Flow-DPO is dependent on the time step, resulting in Flow-DPO performing poorly on certain tasks. However, we found that by simply fixing β, Flow-DPO becomes the most effective alignment approach in our evaluations. Flow-NRG is an efficient inference-time alignment algorithm that applies reward guidance directly to noisy videos during the denoising process.It allows users to apply arbitrary weightings to multiple alignment objectives during inference, eliminating the need for extensive retraining to meet personalized user requirements. Our contributions can be summarized as follows: Large-Scale Preference Dataset: We create new 182ksized human-labeled video generation preference dataset from 12 video generation models, capturing VQ, MQ, and TA dimensions. Multi-Dimensional Reward Modeling: We propose and systematically study multi-dimensional video reward model, investigating how different design decisions influence its rewarding efficacy. VideoGen-RewardBench: To more accurately assess video reward models, we have annotated prompt-video pairs from VideoGen-Eval. This dataset includes wide variety of prompts and videos generated by modern VDMs. The resulting benchmark consists of 26.5k video pairs, each accompanied by preference label. Flow-Based Alignment: From unified reinforcement learning perspective, we introduce three alignment algorithms for flow-based models: two training-time strategies (Flow-DPO, Flow-RWR) and one inference-time technique (Flow-NRG). Our experiments show that FlowDPO outperforms other methods when β is fixed. Additionally, Flow-NRG, using lightweight reward model for noisy videos, enables users to apply custom weightings to multiple alignment objectives during inference. 2. Related Works Evaluation and Reward Models. Evaluation models and reward models play pivotal role in aligning generative models with human preferences. Earlier approaches and benchmarks (Huang et al., 2023; Liu et al., 2024c; Huang et al., 2024) relied on metrics like FID (Heusel et al., 2017) and CLIP scores (Radford et al., 2021) to assess visual quality and semantic consistency. Recent works (Wu et al., 2023a; Xu et al., 2024b; Kirstain et al., 2023; Zhang et al., 2024b; Liang et al., 2024a) have shifted towards utilizing human preference datasets to train CLIP-based models, enabling them to predict preference scores with improved accuracy. With the advent of large vision-language models (VLMs) (Achiam et al., 2023; Wang et al., 2024a), their powerful capabilities in visual understanding and text-visual alignment make them natural proxy for reward modeling. common approach involves replacing the token classification head of VLMs with regression head that predicts multi-dimensional scores for diverse evaluation tasks. Two main learning paradigms have emerged based on the type of human annotation. The first paradigm relies on point-wise regression, where the model learns to fit annotated scores (He et al., 2024) or labels (Xu et al., 2024a) directly. Another paradigm focuses on pair-wise comparisons, leveraging Bradley-Terry (BT) (Bradley & Terry, 1952) loss to model relative preferences, which is largely unexplored for video reward model. Beyond these methods, some works (Wang et al., 2024b) also leverage the intrinsic reasoning capabilities of VLMs through VLM-as-a-judge (Li et al., 2023; Lin et al., 2024b), where VLMs are adopted to generate preference judgments or scores in textual format through instruction tuning. Despite these promising advances, most existing video reward models primarily focus on legacy video generation models, typically from the pre-Sora (OpenAI, 2024) era, which are constrained by short video durations and relatively low quality. Furthermore, the design and technical choices underlying the vision reward models remain underexplored. Our work seeks to address these limitations by focusing more on modern video generation models and investigating broader range of reward modeling strategies. Alignment for Image & Video Generation. In large language models, Reinforcement Learning from Human Feedback (RLHF) improves alignment with human preferences, enhancing response quality (Ouyang et al., 2022; Jaech et al., 2024). Similar methods have been applied to image generation, using reward models or human preference data to align pretrained models. Key approaches include: (1) direct 2 Improving Video Generation with Human Feedback Figure 1. Overview of Our Video Alignment Paradigm. (a) Human Preference Annotation (Sec. 3.1). We construct dataset of 182k (prompt, video A, video B) triplets, collecting preference annotations on Visual Quality (VQ), Motion Quality (MQ), and Text Alignment (TA) from human evaluators. (b) Reward Mode Training (Sec. 3.2). We train VLM-based reward model using the Bradley-Terry-Model-with-Ties formulation. (c) Video Alignment (Sec. 4). We adapt alignment techniques DPO, RWR, and reward guidance to flow-based video generation models and provide comprehensive comparison of their effectiveness. backpropagation with reward signals (Prabhudesai et al., 2023; Clark et al., 2023; Xu et al., 2024b; Prabhudesai et al., 2024); (2) Reward-Weighted Regression (RWR) (Peng et al., 2019; Lee et al., 2023; Furuta et al., 2024); (3) DPO-based policy optimization (Rafailov et al., 2024; Wallace et al., 2024; Dong et al., 2023; Yang et al., 2024a; Liang et al., 2024b; Yuan et al., 2024; Liu et al., 2024b; Zhang et al., 2024a; Furuta et al., 2024); (4) PPO-based policy gradients (Schulman et al., 2017) (Black et al., 2023; Fan et al., 2024); and (5) training-free alignment (Yeh et al., 2024; Tang et al., 2024; Song et al., 2023). These methods have successfully aligned image generation models with human preferences, improving aesthetics and semantic consistency. Our work applies the DPO algorithm (Rafailov et al., 2024; Wallace et al., 2024) to flow matching in video generation. Concurrent work (Domingo-Enrich et al., 2024) also explores similar things in image generation. However, they reports worse performance for the DPO-aligned model compared to the unaligned one. We argue that the originally derived Flow-DPO algorithm imposes stronger KL constraint at lower noise steps, resulting in suboptimal performance. In contrast, using constant KL constraint significantly improves performance on certain tasks. There has also been work on aligning video generation models using feedback (Furuta et al., 2024; Li et al., 2024). However, these approaches rely on image reward models (Kirstain et al., 2023; Wu et al., 2023a). Concurrent works (Yuan et al., 2024; Liu et al., 2024b; Zhang et al., 2024a) focus on aligning diffusion-based video generation models by annotating videos generated from earlier opensource models. Our work differs by exploring alignment techniques for advanced flow-based video generation. 3. VideoReward 3.1. Human Preference Data Collection Existing human preference datasets for video generation (Liu et al., 2024c; Huang et al., 2024; He et al., 2024; Wang et al., 2024b; Xu et al., 2024a) primarily consist of videos generated by earlier open-source models, which are often characterized by low quality, noticeable artifacts, and short video durations. As advancements in video diffusion models (VDMs) continue to elevate the state-of-the-art, the gap between current preference datasets and the capabilities of modern VDMs has become increasingly pronounced. To bridge this gap, we focus on developing reward model tailored for the latest VDMs. Inspired by Zhang et al. (2024b), we collect diverse set of prompts from the internet, organiz3 Improving Video Generation with Human Feedback Table 1. Statistics of the collected training dataset. We utilized 12 text-to-video models to generate total of 108k videos from 16k unique prompts. This process ultimately resulted in 182k annotated triplets, each consisting of prompt paired with two videos and corresponding preference annotations. T2V Model Date #Videos #Anno Triplets Resolution Duration Pre-Sora-Era Models Gen2 (Runway, 2023) Modern Models SVD (Blattmann et al., 2023) Pika 1.0 (Labs, 2023) Vega (VegaAI, 2023) Pixverse v1 (PixVerse, 2024) HiDream (HidreamAI, 2024) Dreamina (Capcut, 2024) Luma (LumaLabs, 2024) Gen3 (Runway, 2024) Kling 1.0 (Kuaishou, 2024) Pixverse v2 (PixVerse, 2024) Kling 1.5 (Kuaishou, 2024) 23.06 23.11 23.12 23.12 24.01 24.01 24.03 24.06 24.06 24.06 24.07 24. 6k 6k 6k 6k 6k 0.3k 16k 16k 16k 6k 16k 7k 13k 13k 13k 13k 13k 0.3k 68k 57k 55k 33k 58k 28k 768 1408 576 1024 720 1280 576 1024 768 1408 768 1344 720 1280 752 1360 768 1280 384 672 576 1024 704 4s 4s 3s 4s 4s 5s 6s 5s 5s 5s 5s 5s Figure 2. Statistics of our training data. ing them into 8 meta-categories (animal, architecture, food, people, plants, scenes, vehicles, objects). These prompts are further expanded using GPT-4o (Achiam et al., 2023). filtering process is then applied to exclude unsuitable prompts (e.g., repetitive, irrelevant to motion, unsafe content). Subsequently, we employ our internal prompt rewriter to refine and enhance the collected prompts, resulting in final dataset of 16k detailed prompts. Leveraging these prompts, we select 12 text-to-video generation models with various capabilities, producing 108k videos and constructing 182k triplets, each comprising prompt and two corresponding videos generated by different video generation models. Comprehensive statistics of the dataset are provided in Tab. 1 and Fig 2. To annotate these text-video triplets, we hire annotators to perform pairwise assessments across three critical dimensions: Visual Quality (VQ), Motion Quality (MQ), and Text Alignment (TA). Details are available in appendix C. For each dimension, annotators are presented with prompt and two generated videos, and asked to provide choices indi4 Improving Video Generation with Human Feedback cating their preference (A wins/Ties/B wins). Each sample is independently assigned to three annotators to ensure robustness. In cases of disagreement, an additional annotator will be engaged to resolve conflicts. This effort results in dataset of 182k high-quality, multi-dimensional preference annotations, which were utilized for training the reward model. In addition to pairwise annotations, we also conduct pointwise annotations using similar procedure, where annotators rate each video on Likert scale from 1 to 5 for the same three dimensions (i.e., VQ, MQ, TA). This dual annotation setup enables us to explore the advantages and limitations of pairwise preferences versus pointwise scores as annotation strategies. In addition to the training data, we construct validation set comprising 13k annotated triplets, following similar annotation protocol. The prompts in the validation set are strictly excluded from the training set to ensure generalization. The validation set serves as reliable benchmark to monitor the convergence during training. 3.2. Reward Model Learning Following prior works that leverage Vision-Laguage Models (VLMs) for related tasks (He et al., 2024; Wang et al., 2024b; Xu et al., 2024a), we adopt Qwen2-VL-2B (Wang et al., 2024a) as the base model for our video reward framework. While existing studies have demonstrated the effective application of reward models in both evaluation (Wu et al., 2023b;a; He et al., 2024) and optimization (Lee et al., 2023; Wallace et al., 2024; Xu et al., 2024b; Prabhudesai et al., 2024), the underlying design choices remain insufficiently explored. We start with Bradley-Terry style reward model as our baseline, exploring how technical decisions affect the video reward models performance. Score Regression v.s. Bradley-Terry. We first explore the merits of adopting Bradley-Terry pairwise comparison approach compared to pointwise score regression approach for training reward models. The BT model (Bradley & Terry, 1952) uses pairwise log-likelihood loss to capture the probability that one video is preferred over another, enabling the prediction of rewards based on the relative preferences. Conversely, the pointwise regression model directly estimates the absolute quality scores by minimizing the MSE loss. The loss functions for these methodologies are formally defined as follows: LBT = (y,xw 0 ,xl 0)D (cid:2)log (cid:0)σ (cid:0)r(xw 0 , y) r(xl (cid:2)r(x0, y) z2(cid:3) Lreg = E(y,x0,z)D In these formulations, (y, xw 0) denotes pairwise annotations, where is the input prompt and xw 0 are the two generated videos with corresponding preference annotations. Meanwhile, (y, x0, z) represents pointwise 0 and xl 0 , xl 0, y)(cid:1)(cid:1)(cid:3) (1) (2) annotations, with being the assigned quality score. The function signifies the optimized reward function. Given that our training dataset includes both pointwise scores and pairwise preferences, annotated by the same group of annotators on the same dataset, this setup enables direct comparison between the two paradigms. To conduct this comparison, we train both types of reward models on the training data with varying proportions of annotated data. Their performance is assessed by evaluating the average accuracy of the best-performing checkpoints on the validation set. Notably, for the BT reward model, we exclude samples labeled as Ties and only utilize samples annotated as Wins or Wins since the BT framework is not well-suited to handle tied data. The comparison results are illustrated in Fig. 3. Figure 3. Accuracy comparison between the BT and regression reward models across varying training data fractions (log scale). We observe that as the dataset size increases, both the Bradley-Terry reward model and the regression reward model exhibit notable improvements in accuracy, with the BT model consistently outperforming the regression model. This disparity can be attributed to the inherent characteristics of pointwise and pairwise annotations. Pairwise annotations are inherently more effective at capturing relative differences between samples. For instance, even when two videos receive the same score in pointwise annotations, annotators can still discern their relative quality in pairwise comparisons. This ability grants the BT reward model distinct advantage when the dataset size is limited. As the dataset expands, the performance gap between the two models narrows, as the regression model benefits from the increased data to learn more accurate ranking relationships. Additionally, the annotation process itself varies in complexity between the two approaches. Pairwise preference labels are generally easier for annotators to agree upon compared to assigning precise scores in pointwise annotations. Recent studies (Wang et al., 2024d;c) have demonstrated that with carefully curated pointwise annotations, regression models can achieve performance levels comparable to BT reward models. We posit that as both the quantity and quality of the data improve, the performance differences between these approaches become negligible. While in the context of our 5 Improving Video Generation with Human Feedback current training data, the BT reward model emerges as the better-performing choice. Ties Matters. While the BT model is commonly used to capture human preferences from chosen-rejected pairs, the importance of tie annotations is often overlooked. Inspired by recent works (Liu et al., 2024a), we use the BradleyTerry model with ties (BTT) (Rao & Kupper, 1967), an extension of BT that accounts for tied preferences. For 0 and x2 given prompt and two generated videos x1 0, the BTT model defines the probabilities of each possible preference as follows: (xA 0 = xB 0 y) = (xA 0 xB 0 y) = (xB 0 xA 0 y) = (θ2 1)er(xA 0 ,y)(cid:17) (cid:16) 0 ,y)er(xB θer(xA 0 ,y) 0 ,y) + er(xB 0 ,y) + θer(xB 0 ,y)(cid:17) (cid:16) er(xA er(xA 0 ,y) 0 ,y) + θer(xA er(xB 0 ,y) 0 ,y) + er(xB 0 ,y) 0 ,y) er(xB θer(xA (3) (4) (5) where θ is parameter that controls the tendency toward ties, with larger θ indicating higher probability of ties. We empirically set θ = 5.0 and optimize the BTT model by minimizing the negative log-likelihood (NLL): LBTT = (y,xA 0 ,xB 0 )D (cid:104) 1(xA 0 xB 0 ) log (xA 0 xB 0 y)+ 1(xB 1(xA 0 xA 0 = xB 0 ) log (xB 0 ) log (xA 0 xA 0 = xB 0 y)+ (cid:105) 0 y) (6) We train both the Bradley-Terry reward model and the Bradley-Terry-With-Tied reward model under the same setting and visualize the distribution of = r(xA 0 , y) r(xB 0 , y) on the validation set using the best-performing checkpoints. As shown in the boxplot in Fig. 4, while the BT model effectively manages chosen/rejected pairs, it struggles to accurately differentiate tied pair. large portion of ties exhibit larger than certain chosen/rejected pairs, underscoring the BT models limitations in handling ambiguous cases. In contrast, the BTT model learns more generalized decision boundary, enhancing it to distinguish tied pairs from clear chosen/rejected pairs based on r. This improvement stems from the explicit modeling of ties in Eq. 3, which allows the model to incorporate this oftenoverlooked category, resulting in more robust performance. Separation for Decoupling. When adopting LLMs/MLLMs for multi-attribute reward modeling, common practice (Ouyang et al., 2022; Touvron et al., 2023; He et al., 2024) involves extracting the last token from the final layer and applying trainable linear projection to predict scores Figure 4. Visualization of the distribution for the BT reward model (Left) and the BTT reward model (Right). The BTT model effectively distinguishes tie pairs from chosen/rejected pairs. across multiple dimensions. While this approach enables efficient multi-dimensional scoring, it inherently induces coupling among dimensions, particularly between contextagnostic and context-aware attributes. For example, such model may inadvertently rely on the input prompt when evaluating visual quality, causing the same video sample to receive different visual quality scores depending on its prompt. Although this coupling may be inherited from the correlations within the annotated data, it compromises the interpretability of reward scores and introduces challenges for the evaluation or optimization of individual dimensions. To decouple context-agnostic dimensions from contextual influences, we introduce separate special tokens for each dimension. Specially, three unique query tokens are added to represent the scores across the three evaluation dimensions. In the input template of our reward models, the contextagnostic tokens (i.e., VQ, MQ) are placed after the video but before the prompt, while the context-aware special tokens (i.e., TA) are positioned after the prompt. We then extract the feature representations from the final layer corresponding to each special tokens position and apply shared linear head to predict the associated dimension scores. Owing to the autoregressive models causal attention mechanism, the context-agnostic tokens exclusively attend to the video content, whereas the context-aware tokens attend to both the video and the prompt. This strategy effectively separates the focus of each token, enabling more robust performance of each dimension and independent evaluation for context-agnostic dimensions. We provide full version of our input prompt template in appendix B. 4. Video Alignment Let x0 q(x0) denote real data sample drawn from the true data distribution, and let x1 p(x1) denote noise sample, where x0, x1 Rd. Recent advanced imagegeneration models (e.g., (Esser et al., 2024)) and videogeneration models (e.g., (Kong et al., 2024)) adopt the Rectified Flow framework (Liu et al., 2022), which defines the Improving Video Generation with Human Feedback noised data xt as xt = (1 t) x0 + x1, where ϵpred and vpred refer to predictions either from the model pθ or the reference model pref. By substituting Eq. 9 into Eq. 8, we obtain the final Flow-DPO loss LFD(θ): for [0, 1]. Then we can train transformer model to directly regress the velocity field vθ(xt, t) by minimizing the Flow Matching objective (Lipman et al., 2022; Liu et al., 2022): L(θ) = Et, x0q(x0), x1p(x1) (cid:2) vθ(xt, t)2(cid:3), where the target velocity field is = x1 x0. 4.1. Flow-DPO 0 , xl Consider fixed dataset = {y, xw 0}, where each sample consists of prompt and two videos xw 0 and xl 0 generated by reference model pref, with human annotations indicating that xw 0 xl 0 is preferred over xl 0). The goal of RLHF is to learn conditional distribution pθ(x0 y) that maximizes reward model r(x0, y), while controlling the regularization term (KL-divergence) from the reference model pref via coefficient β: 0 (i.e., xw max pθ EyDc,x0pθ(x0y) [r(x0, y)] β DKL [pθ(x0 y) pref(x0 y)] (7) In this context, pref corresponds to pref,0 and pθ corresponds to pθ,0. For the sake of simplicity, the timestep subscripts are omitted. However, RLHF is usually unstable and resource intensive (Rafailov et al., 2024). To address this issue, Diffusion-DPO aligns diffusion models with human preferences by directly solving the above RLHF objective (Eq. 7) analytically. It interprets alignment as classification problem, and optimizes policy to satisfy human preferences by supervised training. For simplicity, we omit the conditioning prompt in the following equations. The Diffusion-DPO objective LDD(θ) is given by: (cid:34) (cid:32) log σ (cid:16) β 2 ϵw ϵθ(xw , t)2 ϵw ϵref(xw , t)2 (cid:0)ϵl ϵθ(xl t, t)2 ϵl ϵref(xl t, t)2(cid:1)(cid:17) (cid:33)(cid:35) (8) = (1 t) where 0 + ϵ, ϵ (0, I). The superscript indicates either (for the preferred data) or (for the less preferred data). The expectation is taken over samples {xw 0} and the noise schedule t. In Rectified Flow, we relate the noise vector ϵ to velocity field v. Specifically, Lemma A.1 in Appendix shows that 0 , xl ϵ ϵpred(x , t)2 = (1 t)2v vpred(x , t)2, (9) (cid:34) (cid:32) log σ βt 2 (cid:16) vw vθ(xw , t)2 vw vref(xw , t)2 (cid:0)vl vθ(xl t, t)2 vl vref(xl t, t)2(cid:1)(cid:17) (cid:33)(cid:35) , (10) 0 , xl where βt = β (1 t)2 and the expectation is taken over samples {xw 0} and the noise schedule t. Intuitively, minimizing LFD(θ) guides the predicted velocity field vθ closer to the target velocity vw of the preferred data, while pushing it away from vl (the less preferred data). The strength of this preference signal depends on the differences between the predicted errors and the corresponding reference errors, vw vref(xw t, t)2. We provide pseudo-code of Flow-DPO in Appendix E. , t)2 and vl vref(xl Discussion on βt. The parameter βt governs the strength of the KL divergence constraint, determining the extent to which the model can diverge from the base reference model (Rafailov et al., 2024; Wallace et al., 2024). Note that from our derivation, βt = β(1 t)2. As the timestep approaches 1, βt approaches zero, resulting in relatively weak KL constraint. Conversely, as approaches 0, βt converges to the original β, enforcing stronger KL constraint. This scheduling strategy causes the model to prioritize alignment at higher noise levels. Our experiments reveal that this degrades performance, while using constant βt provides better results. Similar phenomena have been observed in DDPMs (Ho et al., 2020) standard diffusion training objective, where discarding the weighting in denoising score matching leads to improved sample quality, as discussed in Section 3.4 of their paper. We provide more detailed discussion of this in Section 5.2. 4.2. Flow-RWR Drawing inspiration from the application of Rewardweighted Regression (RWR) (Peters & Schaal, 2007) in diffusion models (Lee et al., 2023; Furuta et al., 2024), we propose counterpart for flow-based models based on expectation-maximization. Starting from the general KLregularized reward-maximization problem in Eq. 7, prior work (Rafailov et al., 2024; Peters & Schaal, 2007) shows that its optimal closed-form solution can be written as pθ(x0 y) = (cid:18) 1 1 Z(y) β β r(x0, y)(cid:1) is the pref(x0 y) exp(cid:0) 1 where Z(y) = (cid:80) partition function. Following (Furuta et al., 2024), we can pref(x0 y) exp r(x0, y) , (11) (cid:19) x0 Improving Video Generation with Human Feedback obtains the RWR loss by M-step: LRWR(θ) = EyDc (cid:2) 1 Z(y) exp( r(x0, y) β ) log pθ(x0 y)(cid:3). In practice, we omit the intractable normalization Z(y) and β, and we employ Eq. 37 in Wallace et al. (2024) as simplified upper bound of the negative log-likelihood. This yields the approximate form LRWR(θ) = Ey,x0,ϵ,t (cid:2)exp(r(x0, y))ϵ ϵθ(xt, t, y)2(cid:3). (12) Substituting Eq. 9 into Eq. 12 then provides the final RWR objective for velocity prediction: LRWR(θ) = E(cid:2)exp(r(x0, y))v vθ(xt, t, y)2(cid:3), (13) where is the ground-truth velocity, and vθ(xt, t, y) represents the predicted velocity under parameters θ. We omit (1 t)2 as in Sec. 4.1. By applying RWR in this way, our flow-based model learns to generate velocity fields that emphasize high-reward samples, paralleling how diffusionbased models align samples via reward-weighted likelihood maximization. 4.3. Noisy Reward Guidance Recall that Eq. 7 has closed-form solution as presented in Eq. 11. This expression indicates that the goal of RLHF is to transform the original distribution pref(x0 y) into the new target distribution pθ(x0 y). Since the constants β and can be absorbed into r(x0, y), Eq. 11 becomes pθ(x0 y) pref(x0 y) [exp(r(x0, y))]w, (14) where controls the strength of the reward guidance. Notably, this formulation aligns with the concept of classifier guidance (Dhariwal & Nichol, 2021; Song et al., 2020), so we refer to it as reward guidance. As we proved in Appendix A.2, for Rectified Flow, if we set the marginal velocity field to vt(xt y) = vt(xt y) 1 r(xt, y), (15) the marginal distribution is modified from pt(xt y) to pt(xt y) pt(xt y) [exp(r(xt, y))]w. When = 0, the resulting samples are drawn from Eq. 14. We provide pseudo-code in Appendix E. To producing meaningful reward values for noised videos xt, we train time-dependent reward function. We use the same dataset = {y, xw, xl} described in Section 3.2, where xw and xl represent two videos with different preference levels. In order to train our reward function, we assume that applying 8 the same noise to both videos preserves their preference relationship. Formally, for each pair (xw, xl), we define xw = (1 t) x0 + xw 1 , 1, with xw = (1 t) x0 + xl xl xl t. (16) Here, x0 is the original video, while xw 1 are the noised versions corresponding to the more preferred and less preferred videos, respectively. We then adopt the BT model to learn the reward function from these noised videos. Specifically, we minimize the following loss: 1 and xl LBT (θ) = E(cid:2)log(cid:0)σ(cid:0)rθ(xw , t) rθ(xl t, t)(cid:1)(cid:1)(cid:3), (17) where the expectation is taken over (xw, xl, y) D, (0, 1). In this way, the reward function rθ(, t) learns to preserve the relative preference between noised videos at any noise level t. However, advanced video generation models typically work in latent space and rely on VAE (Kingma, 2013) to decode the latent representation back into video. In Eq. 15, computing the gradient of the reward with respect to xt would require first decoding the latent representation into video and then passing the gradient through the large VAE decoder back to the latent space, incurring high computational cost and memory usage. To address this challenge, we propose training the reward model directly within the latent space using pretrained VDM. Since the VDM has already been trained on noisy latent representations, we can develop lightweight, timedependent reward function by utilizing only the initial layers of the pretrained model rather than the entire VDM. Subsequently, we apply Equation 15 at each inference step to implement reward-guided generation. 5. Experiments In this section, we empirically evaluate our reward model and reinforcement learning alignment algorithms on Textto-Video (T2V) tasks. 5.1. Reward Learning Training Setting. We utilize Qwen2-VL-2B (Wang et al., 2024a) as the backbone for our reward model and train it with BTT loss. To finetune the model, LoRA (Hu et al., 2021) is applied to update all linear layers in the language model, while the vision encoders parameters are fully optimized. The training process is conducted with batch of 32 and learning rate of 2 106, with the model trained over two epochs. This setup requires approximately 72 A800 GPU hours. Several observations were made during training. First, higher video resolution and more frames generally improved the reward models performance. Second, using stable sampling interval instead of fixed frame number Improving Video Generation with Human Feedback Table 2. Preference accuracy on GenAI-Bench and VideoGen-Eval. w/ Ties indicates that accuracy is calculated with ties included (Deutsch et al., 2023), and w/o Ties excludes tied pairs when calculating accuracy. * denotes that for LiFT, ties prediction are randomly converted to chosen/rejected with 0.5 probability due to large number of ties produced by the model. Bold: best performance. Method GenAI-Bench VideoGen-RewardBench Overall Accuracy Overall Accuracy VQ Accuracy MQ Accuracy TA Accuracy w/ Ties w/o Ties w/ Ties w/o Ties w/ Ties w/o Ties w/ Ties w/o Ties w/ Ties w/o Ties Random VideoScore (He et al., 2024) LiFT* (Wang et al., 2024b) VisionRewrd (Xu et al., 2024a) Ours 33.67 49.03 37.06 51.56 49.41 49.84 71.69 58.39 72.41 72.89 41.86 41.80 39.08 56.77 61.26 50.30 50.22 57.26 67.59 73. 47.42 47.41 47.53 47.43 59.68 49.86 47.72 55.97 59.03 75.66 59.07 59.05 59.04 59.03 66.03 49.64 51.09 54.91 60.98 74.70 37.25 37.24 33.79 46.56 53.80 50.40 50.34 55.43 61.15 72. Table 3. Ablation study on reward model type, seprate tokens and data augmentation. Bold: Best Performance. Variants RM Type Separate Tokens Overall Accuracy Overall Accuracy VQ Accuracy MQ Accuracy TA Accuracy GenAI-Bench VideoGen-RewardBench w/ Ties w/o Ties w/ Ties w/o Ties w/ Ties w/o Ties w/ Ties w/o Ties w/ Ties w/o Ties II III IV Regression BT BTT BTT 48.28 47.74 48.27 49.41 71.13 71.21 70.89 72.89 58.39 61.22 61.50 61. 70.16 72.58 73.39 73.59 54.23 52.33 60.52 59.68 73.61 77.10 76.31 75.66 61.16 59.43 64.64 66.03 65.56 73.50 72.40 74.70 52.60 53.06 53.55 53. 70.95 71.62 72.12 72.20 significantly enhanced the accuracy of motion quality evaluations, particularly when trained with videos of varying durations. In practice, we sample videos at 2 fps, with resolution of approximately 448 448 pixels during the training process. The original aspect ratio is preserved to ensure visual quality. Evaluation. To assess the performance of our reward model in evaluating T2V generation, we employ comprehensive evaluation framework designed to ensure fair comparison across different models and mitigate potential domain biases. We utilize two primary benchmarks for evaluation. (1). VideoGen-RewardBench: Derived from the third-party prompt-video dataset VideoGen-Eval (Zeng et al., 2024), VideoGen-RewardBench is specifically designed to evaluate modern T2V models. To address the absence of human annotations in VideoGen-Eval, we manually construct 26.5k triplets and hire annotators to provide pairwise preference labels, following the procedure outlined in Sec. 3.1. Additionally, annotators assess the overall quality between the two videos, serving as universal label for reward models across various dimensions. (2). GenAIBench (Jiang et al., 2024b): : Serving as complementary benchmark, GenAI-Bench features short videos (2 seconds in duration) generated by pre-Sora-era T2V models. This benchmark enables us to evaluate our reward models capability in assessing earlier-generation models, despite their limited representation in our training set. We summarize the key difference between VideoGen-RewardBench and GenAI-Bench in Appendix D.1. We compare our reward model against existing baselines, including VideoScore (He et al., 2024), as well as two concurrent works: LiFT (Wang et al., 2024b), and VisionReward (Xu et al., 2024a). Consistent with prevailing practices in LLMs (Lambert et al., 2024), we employ pairwise accuracy as primary metric. Inspired by VisionReward (Xu et al., 2024a), we report two accuracy metrics: ties-included accuracy (Deutsch et al., 2023) and ties-excluded accuracy. We calculate the overall accuracy on GenAI-Bench and both overall and dimension-specific (VQ, MQ, TA) accuracy on VideoGen-RewardBench. Additional details of the evaluation across different methods are provided in the Appendix D.2. Main Results. Tab 2 presents the pairwise accuracy of various reward models on GenAI-Bench (representing preSora-era models) and VideoGen-RewardBench (representing modern T2V models). VideoScore is primarily designed to evaluate videos from earlier-generation models. It performs well on GenAI-Bench but exhibits close-to-random predictions on VideoGen-RewardBench, reflecting its inability to generalize to the improved capabilities of modern T2V models. LiFT adopts the VLM-as-a-judge approach to train its reward model, but it struggles with pairwise differentiation. This is because LiFT tends to assign similar scores to both videos in pair. When calculating its ties-excluded accuracy, we randomly convert ties prediction to chosen/rejected categories with 0.5 probability, indicating the model fail to distinguish between video qualities. VisionReward demonstrates competitive performance on GenAI-Bench but shows diminished performance on VideoGen-RewardBench, particularly in ties-included ac9 Improving Video Generation with Human Feedback Table 4. Multi-dimensional alignment with VQ:MQ:TA = 1:1:1. Bold: Best performance. Although Flow-DPO with timestep-dependent β achieves high VQ and MQ reward win rates, it exhibits significant reward hacking. In contrast, Flow-DPO with constant β achieves high VQ, MQ, and TA scores while avoiding reward hacking. Method VBench VideoGen-Eval TA-Hard Total Quality Sementic VQ MQ TA VQ MQ TA VQ MQ TA Pretrained SFT Flow-RWR Flow-DPO (βt = β(1 t)2) Flow-DPO 83.19 82.31 82.27 80.90 83.41 84.37 83.13 83.19 81.52 84.19 78.46 79.04 78.59 78.42 80.26 50.0 51.28 51.55 87.78 93.42 50.0 65.21 63.9 82.36 69. 50.0 52.84 53.43 51.02 75.43 50.0 61.27 59.05 88.44 90.95 50.0 76.13 69.7 91.23 81.01 50.0 46.35 48.35 28.14 68.26 50.0 57.75 61.97 84.29 77.46 50.0 76.06 78.87 83.10 71. 50.0 57.75 55.71 38.03 73.24 curacy for visual quality and motion quality. This shortfall can be attributed to the significant improvements in visual fidelity and motion smoothness exhibited by modern T2V models, which VisionReward struggles to effectively assess. In contrast, our method VideoReward outperforms all other models on VideoGen-RewardBench, showcasing its effectiveness in evaluating outputs from state-of-the-art T2V models. Moreover, despite being trained on distinct dataset, our approach also achieves comparable performance to VisionReward on GenAI-Bench, highlighting its strong generalization across different eras of T2V models. Ablation Study. As discussed in Sec. 3.2, we have highlighted several key design choices for our reward model and demonstrated their effectiveness on the validation set. To further prove these findings, we conduct an ablation study on the evaluation benchmark, providing quantitative supplement to our analysis. Results are demonstrates in Tab. 3. Regarding the type of reward model, we compare three variants: the regression-style reward model, the BradleyTerry reward model, and the Bradley-Terry-With-Tied reward model. The BT reward model shows slight advantage over the regression-style reward model, likely due to inherent strengths of pairwise annotations. Pairwise annotations more accurately capture the ranking relationships within the same context and demonstrate greater tolerance to potential noise in data annotations. The BTT reward model achieves comparable performance to the BT model on tiesexcluded accuracy, but exhibits significant improvement in ties-included accuracy. This improvement stems from the BTT models explicit modeling to tied pairs, enabling it to learn more robust decision boundary that effectively captures neutral relationships in ambiguous cases. Beyond the benefits of disentangling rewards attribute as discussed in Sec. 3.2, we observe that adopting separate tokens rather than shared last token further enhances the models overall performance. This design allows the model to better represent distinct aspects of the reward, thus improving alignment with human preferences. 5.2. Video Alignment Training Setting. Our pretrained model pref is transformer-based latent flow model trained using rectified flow. In all alignment experiments, we applied LoRA to fine-tune the transformer models linear layers, as our findings indicate that full parameter fine-tuning can degrade the models performance or potentially lead to model collapse. For supervised fine-tuning (SFT), we utilize only the chosen data. In the RWR experiments, we first calculate the mean and variance of the rewards across all training samples and normalize the rewards to have mean of 0 and variance of 1. For the VDM reward model, we extract and concatenate the features from the first 20 transformer blocks of our VDM. These concatenated features are then fed into three fully connected heads, which output three-dimensional reward. All parameters of the first 20 transformer blocks are trained. For the training dataset, following the approach of Rafailov et al. (Rafailov et al., 2024), we employ VideoReward as the ground-truth reward model to simulate human feedback and relabel our training dataset. Consequently, T2V models trained on this synthetically relabeled dataset can be reliably evaluated using VideoReward , ensuring fair assessment of their performance. More experimental details, including LoRA architectures, VDM reward model and final hyperparameter settings, are provided in the Appendix F. Evaluation. We evaluate performance using both automatic and human assessments. The automatic evaluation comprises the win rate, as determined by VideoReward , and the Vbench score. The win rate is calculated by having VideoReward assign scores to outputs from both the pretrained and aligned models, then determining the proportion of instances where the aligned models reward exceeds that of the pretrained model. Since VideoReward generates continuous floating-point scores, ties do not occur. Vbench serves as fine-grained benchmark for evaluating T2V models from two main perspectives: Quality and Semantic. The Quality aspect focuses on the perceptual quality of the synthesized video without considering the input condition, corresponding to our VQ and MQ metrics. The Semantic aspect assesses whether the synthesized video aligns with the 10 Improving Video Generation with Human Feedback Figure 5. Visual comparison of videos generated by the original pretrained model and the Flow-DPO aligned model. VideoGen-Eval and TA-Hard prompts for evaluation. We find that the standard Vbench and VideoGen-Eval prompts are relatively straightforward in terms of text alignment, as the pretrained model consistently generates well-aligned video content. To create more rigorous assessment, we have developed additional challenging prompts (TA-Hard) that feature combinations of two subjects and incorporate uncommon actions. For example: talking apple with eyes and mouth, and singing banana with legs hosting talent show in vibrant theater. We provide prompt subset of TA-Hard in Appendix H. Multi-dimensional Alignment. We employ linear scalarization (Li et al., 2020), defined as = 1 3 (rvq + rmq + rta), to consolidate multi-dimensional preferences into single score. For each prompt y, the reward model annotates preferences between two generated videos, such that xw is preferred over xl if r(xw, y) > r(xl, y). This process generates preference dataset = {(y, xw, xl)}. As presented in Tab. 4, Flow-DPO with constant β achieves Figure 6. Human evaluation of Flow-DPO aligned model vs. pretrained model on VideoGen-Eval, which contains 400 prompts. user-provided guiding condition, corresponding to TA. For human evaluation, each sample is independently reviewed by two annotators to ensure reliability. In cases of disagreement, third annotator is consulted to resolve conflicts. We employ same random seeds to generate videos for each method, ensuring fair comparison. We employ Vbench, 11 Improving Video Generation with Human Feedback Table 5. Single-dimensional alignment with TA. Bold: Best performance. Flow-DPO with constant β is the most effective method, achieving best performance without reward hacking. Method VBench VideoGen-Eval TA-Hard Total Quality Semantic TA Pretrained SFT Flow-RWR Flow-DPO (βt = β(1 t)2) Flow-DPO 83.19 82.71 82.40 82.35 83.38 84.37 83.48 83.36 83.00 84.28 78.46 79.62 78.58 79.75 79.80 50.00 52.88 59.66 63.67 69.09 TA 50.00 53.81 49.50 55.95 65.49 TA 50.00 64.79 66.20 71.83 84.51 significant improvements over the pretrained model across three dimensions and outperforms both SFT and Flow-RWR. Conversely, Flow-DPO with timestep-dependent β exhibits reduced performance in the TA task compared to the pretrained model. Although it maintains high reward win rates in VQ and MQ, we observe severe reward hacking issues in the generated videos. In contrast, Flow-DPO with constant β also achieves high VQ and MQ scores without encountering reward hacking problems. This suggests that Flow-DPO with timestep-dependent β struggles to effectively learn the TA task compared to VQ and MQ tasks. We will discuss this issue in detail in Section 5.2 & 5.2. Fig. 6 & 10 demonstrates that the aligned model using Flow-DPO outperforms the pretrained model in human evaluations for both VideoGen-Eval and TA-Hard prompts. Single-dimensional Alignment. We also investigate the ability of different methods to align the T2V model on specific task: TA. As shown in Tab. 5, Flow-DPO with constant β achieves best performance across all datasets. Table 6. Reward guidance using multi-dimensional rewards on VideoGen-Eval prompts. The weighted reward guidence allows users to apply arbitrary weightings to multiple alignment objectives during inference to meet personalized user requirements. VQ VQ:MQ:TA MQ TA 0:0:1 0.1:0.1:0.8 0.1:0.1:0.6 0.5:0.5:0 60.56 66.50 68.94 86.43 46.48 63.73 67.59 93. 70.42 60.86 53.28 26.65 Reward Guidance. We also employ linear scalarization (Li et al., 2020) to compute weighted sum of rewards for each dimension and backpropagate the gradients to the noised latent space. Tab. 6 demonstrates that reward guidance can improve the pretrained model in the corresponding dimensions when using the respective dimensions reward to guide the generation process. Furthermore, the weighted reward approach simultaneously enhances performance across all dimensions. Ablation on β. We meticulously adapted diffusion-DPO to flow-based models, resulting in Equation 10, where βt = β(1 t)2. As illustrated in Fig. 7, under various β settings, 12 Figure 7. Accuracy of time-dependent βt vs. constant β for TA: Flow-DPO with constant β consistently outperforms the timestepdependent β across various settings. Table 7. Reward guidance using only MQ rewards on TA-Hard. The reward model trained with noised latents guides the generation effectively, while the model trained on cleaned latents fails to provide meaningful gradient guidance for noised latents. VDM w/o noise VDM w/ noise VQ 37.1 66.2 MQ 38.6 74.6 TA 52.9 32.4 Flow-DPO with constant β consistently outperforms FlowDPO with timestep dependent β. We note that none of the results in the figure exhibit reward hacking issues. Timestep dependent β may cause uneven training across different timesteps (Hang et al., 2023), since T2V models utilize shared model weights for various noise levels. Ablation on Adding Noise to the VDM Latent Reward Model. Tab. 7 presents the results of applying reward guidance exclusively with MQ rewards on TA-Hard prompts. The reward model trained with noised latents effectively guides the generation process, whereas the reward model trained solely on cleaned latents fails to provide meaningful gradient guidance for intermediate noised latents. Consequently, the performance after guidance is even worse than without guidance. Following the methodology of Yu et al. (2023), we also attempted to directly predict the corresponding x0 from the intermediate xt and then use r(x0, y) to backpropagate gradients. However, we found that this method cannot generate normal videos. 6. Conclusion We have constructed large-scale preference dataset with 200k human preference annotations, encompassing visual quality, motion quality, and text alignment for modern video generation models. Building on this dataset, we develop VideoReward, sophisticated multi-dimensional video reward model, and establish the VideoGen-RewardBench Improving Video Generation with Human Feedback benchmark, facilitating more precise and fair evaluation of video generation models. From unified reinforcement learning perspective, we introduce three alignment algorithms for flow-based video generation models, demonstrating their effectiveness. Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., and Shan, Y. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73107320, 2024. Limitations & Future Work In our experiments, excessive training with Flow-DPO led to significant deterioration in model quality, despite improvements in alignment across specific dimensions. To prevent this decline, we employed LoRA training. Future work can explore the simultaneous use of high-quality data for supervised learning during DPO training, aiming to preserve video quality while enhancing alignment. Additionally, our algorithms have been validated on text-to-video tasks; future work can extend the validation to other conditional generation tasks, such as image-to-video generation. Moreover, our VideoReward model is vulnerable to reward hacking, wherein human assessments indicate marked decrease in video quality, yet the reward model continues to assign high scores. This issue arises because the reward function is differentiable, making it susceptible to manipulation. Future research should focus on developing more robust reward models, potentially by incorporating uncertainty estimates and increasing data augmentation. Additionally, there is potential to apply more RLHF algorithms, such as PPO, to flow-based video generation tasks."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Black, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A. Video generation models as world simulators. 2024. Capcut. Dreamina. https://dreamina.capcut.com/aitool/home, 2024. Clark, K., Vicol, P., Swersky, K., and Fleet, D. J. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Deutsch, D., Foster, G., and Freitag, M. Ties matter: Metaevaluating modern metrics with pairwise accuracy and tie calibration. arXiv preprint arXiv:2305.14324, 2023. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Domingo-Enrich, C., Drozdzal, M., Karrer, B., and Chen, R. T. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., Shum, K., and Zhang, T. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P., Ghavamzadeh, M., Lee, K., and Lee, K. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Furuta, H., Zen, H., Schuurmans, D., Faust, A., Matsuo, Y., Liang, P., and Yang, S. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. Hang, T., Gu, S., Li, C., Bao, J., Chen, D., Hu, H., Geng, X., and Guo, B. Efficient diffusion training via min-snr In Proceedings of the IEEE/CVF weighting strategy. International Conference on Computer Vision, pp. 7441 7451, 2023. He, X., Jiang, D., Zhang, G., Ku, M., Soni, A., Siu, S., Chen, H., Chandra, A., Jiang, Z., Arulraj, A., et al. Videoscore: Building automatic metrics to simulate finegrained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. 13 Improving Video Generation with Human Feedback Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. HidreamAI. Hidreamai. https://www.hidreamai.com/, 2024. Kuaishou. Kling ai. https://klingai.kuaishou.com/, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hong, W., Wang, W., Ding, M., Yu, W., Lv, Q., Wang, Y., Cheng, Y., Huang, S., Ji, J., Xue, Z., et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2icompbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jiang, D., He, X., Zeng, H., Wei, C., Ku, M., Liu, Q., and Chen, W. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024a. Jiang, D., Ku, M., Li, T., Ni, Y., Sun, S., Fan, R., and Chen, W. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024b. Kingma, D. P. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. Labs, P. Pika 1.0. https://pika.art/, 2023. Lambert, N., Pyatkin, V., Morrison, J., Miranda, L., Lin, B. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Lee, K., Liu, H., Ryu, M., Watkins, O., Du, Y., Boutilier, C., Abbeel, P., Ghavamzadeh, M., and Gu, S. S. Aligning textto-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Li, J., Sun, S., Yuan, W., Fan, R.-Z., Zhao, H., and Liu, P. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. Li, J., Long, Q., Zheng, J., Gao, X., Piramuthu, R., Chen, W., and Wang, W. Y. T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. arXiv preprint arXiv:2410.05677, 2024. Li, K., Zhang, T., and Wang, R. Deep reinforcement learning for multiobjective optimization. IEEE transactions on cybernetics, 51(6):31033114, 2020. Liang, Y., He, J., Li, G., Li, P., Klimovskiy, A., Carolan, N., Sun, J., Pont-Tuset, J., Young, S., Yang, F., et al. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1940119411, 2024a. Liang, Z., Yuan, Y., Gu, S., Chen, B., Hang, T., Li, J., and Zheng, L. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2024b. Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., and Han, S. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26689 26699, 2024a. Lin, Z., Gou, Z., Liang, T., Luo, R., Liu, H., and Yang, Y. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809, 2024b. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 14 Improving Video Generation with Human Feedback Liu, J., Ge, D., and Zhu, R. Reward learning from preference with ties. arXiv preprint arXiv:2410.05328, 2024a. Liu, R., Wu, H., Ziqiang, Z., Wei, C., He, Y., Pi, R., and Chen, Q. Videodpo: Omni-preference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024b. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Liu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H., Liu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22139 22149, 2024c. LumaLabs. Dream machine. https://lumalabs.ai/dreammachine, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Rao, P. and Kupper, L. L. Ties in paired-comparison experiments: generalization of the bradley-terry model. Journal of the American Statistical Association, 62(317): 194204, 1967. Runway. Gen-2: Generate novel videos with text, images or video clips. https://runwayml.com/research/gen-2, 2023. Runway. Gen-3. https://runwayml.com/, 2024. OpenAI. Video generation models as world simulators. https://openai.com/index/video-generation-modelsas-world-simulators, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable arXiv preprint off-policy reinforcement arXiv:1910.00177, 2019. learning. Peters, J. and Schaal, S. Reinforcement learning by rewardweighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pp. 745750, 2007. PixVerse. Pixverse. https://pixverse.ai/, 2024. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Prabhudesai, M., Goyal, A., Pathak, D., and Fragkiadaki, K. Aligning text-to-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. Prabhudesai, M., Mendonca, R., Qin, Z., Fragkiadaki, K., and Pathak, D. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. Song, J., Zhang, Q., Yin, H., Mardani, M., Liu, M.-Y., Kautz, J., Chen, Y., and Vahdat, A. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pp. 3248332498. PMLR, 2023. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Tang, Z., Peng, J., Tang, J., Hong, M., Wang, F., and Chang, T.-H. Tuning-free alignment of diffusion models with direct noise optimization. arXiv preprint arXiv:2405.18881, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. VegaAI. Vegaai. https://www.vegaai.net/, 2023. Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Purushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik, N. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. 15 Improving Video Generation with Human Feedback Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Yu, J., Wang, Y., Zhao, C., Ghanem, B., and Zhang, J. Freedom: Training-free energy-guided conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2317423184, 2023. Wang, Y., Tan, Z., Wang, J., Yang, X., Jin, C., and Li, H. Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024b. Yuan, H., Chen, Z., Ji, K., and Gu, Q. Self-play fine-tuning of diffusion models for text-to-image generation. arXiv preprint arXiv:2402.10210, 2024. Zeng, A., Yang, Y., Chen, W., and Liu, W. The dawn of video generation: Preliminary explorations with sora-like models. arXiv preprint arXiv:2410.05227, 2024. Zhang, J., Wu, J., Chen, W., Ji, Y., Xiao, X., Huang, W., and Han, K. Onlinevpo: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024a. Zhang, S., Wang, B., Wu, J., Li, Y., Gao, T., Zhang, D., and Wang, Z. Learning multi-dimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 80188027, 2024b. Zheng, Q., Le, M., Shaul, N., Lipman, Y., Grover, A., and Chen, R. T. Guided flows for generative modeling and decision making. arXiv preprint arXiv:2311.13443, 2023. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Zhou, Z., Liu, J., Yang, C., Shao, J., Liu, Y., Yue, X., Ouyang, W., and Qiao, Y. Beyond one-preference-forall: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708, 2023. Wang, Z., Bukharin, A., Delalleau, O., Egert, D., Shen, G., Zeng, J., Kuchaiev, O., and Dong, Y. Helpsteer2preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024c. Wang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert, D., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024d. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023a. Wu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2096 2105, 2023b. Xu, J., Huang, Y., Cheng, J., Yang, Y., Xu, J., Wang, Y., Duan, W., Yang, S., Jin, Q., Li, S., et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024a. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024b. Yang, K., Tao, J., Lyu, J., Ge, C., Chen, J., Shen, W., Zhu, X., and Li, X. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024a. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Yeh, P.-H., Lee, K.-H., and Chen, J.-C. Training-free diffusion model alignment with sampling demons. arXiv preprint arXiv:2410.05760, 2024. 16 Our Appendix consists of 8 sections: Improving Video Generation with Human Feedback Section provides detailed derivations and lemma proofs for Flow-DPO, Flow-RWR, and Reward Guidance. Section provide the input template used for reward model. Section explains details of human annotaion and guidelines. Section provides details about reward model evaluation, including comparison of the two evaluation benchmarks, evaluation procedures of all methods, and the metrics employed. Section provides pseudo-code of Flow-DPO and Flow-NRG. Section provides hyperparameters for our reward modelinf and alignment algorithms. Section offers more experimental results. Section provides part of prompts used in our TA-Hard dataset. A. Details of the Derivation A.1. Relation beween Velocity and Noise Lemma A.1. Let X0 be real data sample drawn from the true data distribution and X1 be noise sample, where X0, X1 Rd. Define vt(xt X0, X1) to be the conditional velocity field specified by Rectified Flow (Liu et al., 2022), and let vpred (xt) be the predicted marginal velocity field. Then the L2 error of the noise prediction is related to the L2 error of the velocity field prediction by X1 pred 1 (xt, t)2 = (1 t)2 (cid:13) (cid:13)vt(xt X0, X1) vpred (xt)(cid:13) 2 (cid:13) . Proof. The Rectified Flow is time-dependent flow ψ : [0, 1] Rd Rd for [0, 1], defined by By definition, the marginal velocity field vt(xt) is ψ(X0, X1) = (1 t) X0 + X1. vt(xt) = E(cid:2)vt(Xt X0, X1) (cid:12) = E(cid:2) ψ(Xt X0, X1) (cid:12) = E(cid:2)X1 X0 (cid:12) Xt = xt (cid:3) (cid:12) Xt = xt (cid:3) (cid:3) (cid:12) (cid:12) Xt = xt (cid:104) X1 (cid:0)(1 t) X0 + X1 1 (cid:12) (cid:12) (cid:12) Xt = xt E[ X1 Xt = xt ] xt 1 (cid:104) X1 xt 1 (cid:105) . = = = (cid:1) (cid:12) (cid:12) (cid:12) Xt = xt (cid:105) Meanwhile, the conditional velocity field vt(xt X0, X1) is given by vt(xt X0, X1) = X1 xt 1 . Substituting equation 20 into equation 19, we obtain Assuming that Consequently, (cid:13) (cid:13)vt(xt X0, X1) vt(xt)(cid:13) 2 (cid:13) = (cid:13) (cid:13)X1 E[ X1 Xt = xt ](cid:13) 2 (cid:13) (1 t)2 . pred 1 (xt, t) = E[ X1 Xt = xt ] and vpred (xt) = vt(xt). (cid:13) (cid:13)X1 pred 1 (xt, t)(cid:13) 2 (cid:13) = (1 t)2 (cid:13) (cid:13)vt(xt X0, X1) vpred (xt)(cid:13) 2 (cid:13) . 17 (18) (19) (20) Improving Video Generation with Human Feedback A.2. Reward as Classifier Guidance We begin by citing lemma from the Guided Flows paper (Zheng et al., 2023) Lemma A.2. Let pt(xy) be Gaussian Path defined by scheduler (αt, σt), i.e., pt(xx0) = (xαtx0, σ2 I) where Rk is conditioning variable, then its generating velocity field vt(xy) is related to the score function log pt(xy) by where vt(xy) = atx + bt log pt(xy), at = αt αt , bt = ( αtσt αt σt) σt αt . Seed Appendix A.61 of the Guided Flows paper (Zheng et al., 2023) for detailed derivations."
        },
        {
            "title": "If we define",
            "content": "vt(xty) = vt(xty) + w[atxt + btr(xt, y) vt(xty)] = vt(xty) + w[atxt + bt log exp(r(xt, y)) vt(xty)] = (1 w)vt(xty) + w[atxt + bt log exp(r(xt, y))] = atxt + bt [(1 w) log pt(xty) + log exp(r(xt, y))] = atxt + bt log pt(xty) (21) (22) (23) (24) (25) (26) (27) where pt(xty) pt(xty)1w[exp(r(xt, y))]w. We change our goal from sampling from the distribution pt(xty) to sampling from the distribution pt(xty). We note, however, that this analysis shows that Reward Guided Flows are guaranteed to sample from q(y) at time = 1 if the probability path pt(y) is close to the marginal probability path (cid:82) pt(x1)q(x1y)dx1, but it is not clear to what extent this assumption holds in practice. This also mens that pt(xty) is also marginal gaussian path defined by pt(xx1) = (xαtx1, σ2 I). Simlirly, if we define vt(xty) = vt(xty) + wbtr(xt, y) = vt(xty) + wbt log exp(r(xt, y)) = atxt + bt [log pt(xty) + log exp(r(xt, y))] = atxt + bt log pt(xty) (28) (29) (30) (31) where pt(xty) pt(xty)[exp(r(xt, y))]w. We change our goal from sampling from the distribution pt(xty) to sampling from the distribution pt(xty). Reward Guidance for Rectified Flow. Rectified Flow (Liu et al., 2022) is also Gaussian path defined by where x1 is from normal Gaussian distribution. Then xt = (1 t)x0 + tx1 where αt = 1 t, σt = t. Then we get pt(x x0) = (x(1 t)x0, t2I) at = 1 1 , bt = 1 . Eq. 24 becomes Eq. 29 becomes vt(xty) = vt(xty) + w[ 1 1 xt + 1 r(xt, y) + vt(xty)]. vt(xty) = vt(xty) 1 r(xt, y). We use Eq. 24 or Eq. 29 to guide inference through reward model. (32) (33) (34) (35) Improving Video Generation with Human Feedback B. Input Template for Reward Model"
        },
        {
            "title": "Full Input Template",
            "content": "[VIDEO] You are tasked with evaluating generated video based on three distinct criteria: Visual Quality, Motion Quality, and Text Alignment. Please provide rating from 0 to 10 for each of the three categories, with 0 being the worst and 10 being the best. Each evaluation should be independent of the others. **Visual Quality:** Evaluate the overall visual quality of the video, with focus on static factors. The following sub-dimensions should be considered: - **Reasonableness:** The video should not contain any significant biological or logical errors, such as abnormal body structures or nonsensical environmental setups. - **Clarity:** Evaluate the sharpness and visibility of the video. The image should be clear and easy to interpret, with no blurring or indistinct areas. - **Detail Richness:** Consider the level of detail in textures, materials, lighting, and other visual elements (e.g., hair, clothing, shadows). - **Aesthetic and Creativity:** Assess the artistic aspects of the video, including the color scheme, composition, atmosphere, depth of field, and the overall creative appeal. The scene should convey sense of harmony and balance. - **Safety:** The video should not contain harmful or inappropriate content, such as political, violent, or adult material. If such content is present, the image quality and satisfaction score should be the lowest possible. Please provide the ratings of Visual Quality: <VQ reward> END **Motion Quality:** Assess the dynamic aspects of the video, with focus on dynamic factors. Consider the following sub-dimensions: - **Stability:** Evaluate the continuity and stability between frames. There should be no sudden, unnatural jumps, and the video should maintain stable attributes (e.g., no fluctuating colors, textures, or missing body parts). - **Naturalness:** The movement should align with physical laws and be realistic. For example, clothing should flow naturally with motion, and facial expressions should change appropriately (e.g., blinking, mouth movements). - **Aesthetic Quality:** The movement should be smooth and fluid. The transitions between different motions or camera angles should be seamless, and the overall dynamic feel should be visually pleasing. - **Fusion:** Ensure that elements in motion (e.g., edges of the subject, hair, clothing) blend naturally with the background, without obvious artifacts or the feeling of cut-and-paste effects. - **Clarity of Motion:** The video should be clear and smooth in motion. Pay attention to any areas where the video might have blurry or unsteady sections that hinder visual continuity. - **Amplitude:** If the video is largely static or has little movement, assign low score for motion quality. Please provide the ratings of Motion Quality: <MQ reward> END **Text Alignment:** Assess how well the video matches the textual prompt across the following sub-dimensions: - **Subject Relevance** Evaluate how accurately the subject(s) in the video (e.g., person, animal, object) align with the textual description. The subject should match the description in terms of number, appearance, and behavior. - **Motion Relevance:** Evaluate if the dynamic actions (e.g., gestures, posture, facial expressions like talking or blinking) align with the described prompt. The motion should match the prompt in terms of type, scale, and direction. - **Environment Relevance:** Assess whether the background and scene fit the prompt. This includes checking if real-world locations or scenes are accurately represented, though some stylistic adaptation is acceptable. - **Style Relevance:** If the prompt specifies particular artistic or stylistic style, evaluate how well the video adheres to this style. - **Camera Movement Relevance:** Check if the camera movements (e.g., following the subject, focus shifts) are consistent with the expected behavior from the prompt. Textual prompt - [PROMPT] Please provide the ratings of Text Alignment: <TA reward> END 19 C. Details of Human Annotation Improving Video Generation with Human Feedback We provide additional details regarding the annotation process. First, annotators are provided with detailed scoring guidelines and undergo training sessions to ensure they fully understand the criteria; Tab 8 summarizes the key points for each dimension as outlined in the guidelines. Reference examples are provided to help annotators better grasp the evaluation standards. Each sample is evaluated by three independent annotators. For training and validation sets, annotators provide pairwise preference annotations and pointwise scores for Visual Quality (VQ), Motion Quality (MQ), and Tempotal Alignment (TA). For VideoGen-RewardBench, annotators evaluate the same three aspects along with an additional overall quality, using only pairwise preferences. In cases where the annotators disagree on sample, an additional reviewer is tasked with resolving the discrepancy. The final label is determined on the basis of the reviewers evaluation, ensuring consistency across the dataset. Furthermore, during the annotation process, all annotators are instructed to flag any content deemed unsafe. Videos identified as unsafe are excluded from the dataset, ensuring the safety of the data used for training and evaluation. Table 8. Key points summary outlined in annotation guidelines for each evaluation dimension. Evaluation Dimension Key Points Summary Visual Quality Motion Quality Text Alignment Considering the following dimensions introducted by non-dynamic factors: - Image Reasonableness: The image should be objectively reasonable. - Clarity: The image should be clear and visually sharp. - Detail Richness: The level of intricacy in the generation of details. - Aesthetic Creativity: The generated videos should be aesthetically pleasing. - Safety: The generated video should not contain any disturbing or uncomfortable content. Considering the following dimensions in the dynamic process of the video: - Dynamic Stability: The continuity and stability between frames. - Dynamic Reasonableness: The dynamic movement should align with natural physical laws. - Motion Aesthetic Quality: The dynamic elements should be harmonious and not stiff. - Naturalness of Dynamic Fusion: The edges should be clear during the dynamic process. - Motion Clarity: The motion should be easy to identify. - Dynamic Degree: The movement should be clear, avoiding still scenes. Considering the relevance to the input text prompt description. - Subject Relevance Relevance to the described subject characteristics and subject details. - Dynamic Information Relevance: Relevance to actions and postures as described in the text. - Environmental Relevance: Relevance of the environment to the input text. - Style Relevance: Relevance to the style descriptions, if exists. - Camera Movement Relevance: Relevance to the camera descriptions, if exists. D. Details of Reward Model Evaluation D.1. Evaluation Benchmarks We use two benchmarks to evaluate the performance of our reward model: GenAI-Bench (Jiang et al., 2024b) and VideoGenRewardBench. GenAI-Bench is employed to assess the accuracy of the reward model in evaluating pre-SOTA-era T2V models, while VideoGen-RewardBench is used to evaluate its performance on modern T2V models. In this subsection, we provide detailed description of the two benchmarks, highlighting their key parameters and differences in Fig 8 and Tab. 9. We also visualize the model coverage across the training sets of different baselines and the two evaluation benchmarks, as shown in the Fig 9. GenAI-Bench GenAI-Bench collects data from 6 pre-SOTA-era T2V models and 4 recent open-source T2V models. Human annotations for overall quality are obtained through GenAI-Arena, resulting in benchmark consisting of 10 T2V models, 508 prompts, and 1.9k pairs. As the videos in GenAI-Bench predominantly originate from earlier video generation models, they typically have lower resolutions (most around 320x512) and shorter durations (2s-2.5s). We consider GenAI-Bench as benchmark to assess the performance of reward models on early-generation T2V models. VideoGen-RewardBench VideoGen-Eval (Zeng et al., 2024) has open-sourced dataset containing videos generated by 9 closed-source and 3 open-source models, designed to qualitatively visualize performance differences across models. Due to its high-quality data, broad coverage of the latest advancements in T2V models, and third-party nature, we leverage 20 Improving Video Generation with Human Feedback Figure 8. Video Duration and Resolution in GenAI-Bench and VideoGen-Reward Bench Table 9. Comparison between GenAI-Bench and VideoGen-RewardBench. Eariler Models indicates that pre-Sora-era T2V models, and Modern Models indicates that T2V models after Sora. Benchmark GenAI-Bench VideoGen-RewardBench Prompts and Sampled Videos Human Preference Annotations #Samples #Prompts #Earlier Models #Modern Models #Duration #Annotations #Dimensions 4923 508 420 7 (Open-Source) 3 (Open-Source) 2s - 2.5s 0 3 (Open-Source) 9 (Close-Source) 4s - 6s 1891 26457 4 VideoGen-Eval to create fair benchmark, VideoGen-RewardBench, for evaluating reward models performance on modern T2V models. We manually construct 26.5k video pairs and hire annotators to assess each pairs Visual Quality, Motion Quality, Text Alignment, and Overall Quality, providing preference labels. Ultimately, VideoGen-RewardBench includes 12 T2V models, 420 prompts, and 26.5k pairs. This benchmark represents human preferences for state-of-the-art models, with videos featuring higher resolutions (480x720 - 576x1024), longer durations (4s - 6s), and improved quality. We use VideoGen-RewardBench as the primary benchmark to evaluate reward models performance on modern T2V models. D.2. Comparison Methods Random To eliminate the influence of metric calculations and benchmark distributions on our evaluation results, we introduce special baseline: random scores. Specifically, for each triplet (prompt, video A, video B), we randomly sample rA and rB from standard normal distribution, denoted as rA, rB (0, 1). We then calculate accuracy in the same manner as for the other models. The mathematical expectation of random scores for ties-excluded accuracy is E(acc) = 1 2 , and the mathematical expectation of ties-included accuracy is E(acc) = max( 1 3 , p(c = Ties)). VideoScore VideoScore (He et al., 2024) adopts Mantis-Idefics2-8B (Jiang et al., 2024a) as its base model and trains with point-wise data using MSE loss to model human preference scores. Since VideoScore predicts scores across multiple dimensions, and its dimension definitions differ from those in VideoGen-RewardBench, we compute both the overall accuracy and the dimension-specific (VQ, MQ, TA) accuracy by averaging the scores of five dimensions when conduct evaluation GenAI-Bench and VideoGen-RewardBench, consistent with the evaluation strategy outlined in their paper. The training data for VideoScore predominantly comes from pre-SOTA-era models, which explains its relatively better performance on GenAI-Bench, while accounts for the significant performance drop on VideoGen-RewardBench. LiFT LiFT (Wang et al., 2024b) adopts VILA-1.5-40B (Lin et al., 2024a) as its base model and employs VLM-asa-judge approach. The reward model is trained through instruction tuning with inputs, preference scores along with critic. The model generates video scores and reasons through next-token prediction. LiFT evaluates videos across three dimensions: Video Fidelity, Motion Smoothness, and Semantic Consistency, which are similar to the dimensions defined in VideoGen-RewardBench. We calculate the overall accuracy using the average scores of these three dimensions and compute the dimension-specific accuracy using the corresponding dimensional scores. LiFT predicts discrete scores on 21 Improving Video Generation with Human Feedback Figure 9. The model coverage across the training sets of different baselines and the two evaluation benchmarks. VideoScore, VisionReward, and GenAI-Bench primarily focus on pre-SoRA-era models, while our training set and VideoGen-RewardBench concentrate on state-ofthe-art T2V models. 1-3 scale, which often leads to ties in pairwise comparisons. When calculating accuracy without ties, we randomly convert the predicted tie labels to chosen/rejected with 50% probability, indicating that the model is unable to distinguish the relative quality between the two samples. VisionReward VisionReward (Xu et al., 2024a) adopts CogVLM2-Video-12B(Hong et al., 2024) as the base model and is trained to answer set of judgment questions about the video with binary yes or no response using cross-entropy loss. During inference, VisionReward evaluates 64 checklist items, providing converted into 1/0 scores. The final score is computed as the weighted average of these individual responses. We use the final score to calculate both the overall accuracy and the VQ/MQ/TA accuracy. VisionRewards training data includes models from the pre-SOTA era models (Chen et al., 2024) as well as recent open-source T2V models (Zheng et al., 2024; Yang et al., 2024b). It performs well on GenAI-Bench and demonstrates reasonable capabilities on VideoGen-RewardBench. Our Reward Model We adopts QWen2-VL-2B (Wang et al., 2024a) as the base model and train it with pair-wise data using BTT loss in Eq. 6. Scores are normalized on the validation set and averaged to obtain overall scores for evaluation and optimization. When evaluating on VideoGen-RewardBench, we sample videos at 2 FPS and resolution of 448448, consistent with the training settings. We calculate the overall accuracy by averaging the scores across the three dimensions, 22 Improving Video Generation with Human Feedback and compute dimension-specific accuracies using the respective scores. For GenAI-Bench, we sample videos at 2 FPS and resolution of 256256, as the minimum resolution in GenAI-Bench is 256256. Given the significant disparities in visual quality and motion between the GenAI-Bench videos and our training data, we utilize only the predicted TA scores to calculate the overall score. D.3. Evaluation Metrics Similarly to VisionReward (Xu et al., 2024a), we report two accuracy metrics: ties-included accuracy (Deutsch et al., 2023) and ties-excluded accuracy. For ties-excluded accuracy, we exclude all data labeled as ties and use only data labeled as wins or wins for calculation. Since all competitors predict scores based on pointwise samples, we compute the rewards for each pair, convert the relative reward relationships into binary labels, and calculate classification accuracy. For ties-included accuracy, we adopt the O(n2 log n) algorithm proposed in Algorithm 1 by Deutsch et al. (2023). This method traverses all possible tie thresholds, calculates three-class accuracy for each threshold, and selects the highest accuracy as the final metric. 23 Improving Video Generation with Human Feedback E. Pseudo-code of Flow-DPO and Flow-NRG The Pytorch-style implementation of the Flow-DPO loss (Eq. 10) is shown below: def loss(model, ref_model, x_w, x_l, c, beta): \"\"\" # model: Flow model that takes prompt condition and timestep as inputs and predicts velocity # ref_model: Frozen initialization of the model # x_w: Preferred Image (latents in this work) # x_l: Non-Preferred Image (latents in this work) # c: Conditioning (text in this work) # beta: Regularization Parameter # returns: DPO loss value \"\"\" timestep = torch.rand(len(x_w)) noise = torch.randn_like(x_w) noisy_x_w = (1 - timestep) * x_w + timestep * noise noisy_x_l = (1 - timestep) * x_l + timestep * noise velocity_w_pred = model(noisy_x_w, c, timestep) velocity_l_pred = model(noisy_x_l, c, timestep) velocity_ref_w_pred = ref_model(noisy_x_w, c, timestep) velocity_ref_l_pred = ref_model(noisy_x_l, c, timestep) velocity_w = noise - x_w velocity_l = noise - x_l model_w_err = (velocity_w_pred - velocity_w).norm().pow(2) model_l_err = (velocity_l_pred - velocity_l).norm().pow(2) ref_w_err = (velocity_ref_w_pred - velocity_w).norm().pow(2) ref_l_err = (velocity_ref_l_pred - velocity_l).norm().pow(2) w_diff = model_w_err - ref_w_err l_diff = model_l_err - ref_l_err inside_term = -0.5 * beta * (w_diff - l_diff) loss = -1 * log(sigmoid(inside_term)) return loss The Pytorch-style implementation of the reward guidance (Eq. 15) is shown below: def guidance(model, reward_model, prompt_embeds, latents, timesteps, reward_weight): \"\"\" # model: Flow model that predicts velocity given latents and conditions # reward_model: Model that evaluates the quality of latents based on prompt embeddings and timestep # prompt_embeds: Embeddings of the text prompts # latents: Initial noise # timesteps: Sequence of timesteps # reward_weight: weighting coefficient of multi-dimensional rewards # guidance_scale: scale factor for reward guidance \"\"\" dts = timesteps[:-1] - timesteps[1:] for i, in enumerate(timesteps): v_pred = model(latents, prompt_embeds, t) latents = latents.detach().requires_grad_(True) reward = reward_model(latents, prompt_embeds, t) reward = reward * reward_weight reward_guidance = torch.autograd.grad(reward, latents) v_pred = v_pred - guidance_scale * / (1 - t) * reward_guidance latents = latents - dts[i] * v_pred return latents F. Hyperparameters 24 Improving Video Generation with Human Feedback Algorithm-agnostic hyperparameters for SFT, Flow-RWR, Flow-DPO Training strategy LoRA alpha LoRA dropout LoRA LoRA target-modules Optimizer Learning rate Epochs Batch size LoRA (Hu et al., 2021) 128 0.0 64 proj,k proj,v proj,o proj Adam (Kingma, 2014) 5e-6 1 Flow-DPO β 500 Table 10. Hyperparameters for Alignment Algorithms Training strategy LoRA alpha LoRA dropout LoRA LoRA target-modules Optimizer Learning rate Epochs Batch size θ in Eq. Training strategy Optimizer Learning rate Epochs Batch size Reward Dimension VLM Full training for vision encoder LoRA for language model 128 0.0 64 Linear layers in language model Adam (Kingma, 2014) 2e-6 2 32 5.0 VDM Full training Adam (Kingma, 2014) 5e-6 2 144 3 Table 11. Hyperparameters for reward modeling. 25 G. Extended Experimental Results Improving Video Generation with Human Feedback Figure 10. Human evaluation of Flow-DPO on TA-Hard prompt. 26 H. Prompt Subset of TA-Hard Improving Video Generation with Human Feedback rabbit and turtle racing on track. The rabbit is sprinting ahead, while the turtle is steadily moving along. Spectators are cheering from the sidelines, and finish line is visible in the distance. lion and zebra playing soccer on grassy field. The lion is dribbling the ball, while the zebra is trying to block it. The field is surrounded by trees, and other animals are watching the game. fox and an owl stargazing together on hilltop. The fox is lying on its back, pointing at the stars, while the owl is perched on nearby branch, looking through telescope. The night sky is clear, with countless stars twinkling. dolphin and sea turtle exploring coral reef. The dolphin is swimming gracefully, while the sea turtle is gliding slowly beside it. The coral reef is vibrant with colorful corals and various marine life. dolphin and whale singing together in the ocean. The dolphin is leaping out of the water, while the whale is producing deep, melodic sounds. The ocean is vast and blue, with the sun setting on the horizon. fox and rabbit playing duet on piano in forest clearing. The fox is playing the melody, while the rabbit is accompanying with harmony. The forest is alive with the sounds of nature, and other animals are gathered to listen. squirrel and chipmunk building treehouse in large oak tree. The squirrel is hammering nails, while the chipmunk is holding blueprint. The tree is tall and sturdy, with branches full of leaves. robot with glowing blue eyes and human with cybernetic arm playing basketball in futuristic gym. The robot is dribbling the ball with precision, while the human is preparing to block the shot. The gym is equipped with advanced technology and holographic scoreboards. knight in shining armor and wizard with long, flowing beard practicing archery in medieval courtyard. The knight is aiming at target with longbow, while the wizard is using magic to guide the arrows. The courtyard is surrounded by stone walls and blooming flowers. talking apple with eyes and mouth, and singing banana with legs hosting talent show in vibrant theater. The apple is the judge, giving feedback to contestants, while the banana is the host, entertaining the audience with jokes and songs. The theater is filled with colorful lights and excited spectators. pirate with wooden leg and mermaid with shimmering tail playing duet on grand piano in an underwater cave. The pirate is playing the melody, while the mermaid is accompanying with harmony. The cave is illuminated by bioluminescent sea creatures, creating magical atmosphere. superhero with cape and detective with magnifying glass solving mystery in bustling city. The superhero is flying above the streets, scanning for clues, while the detective is examining evidence on the ground. The city is alive with activity, with skyscrapers towering overhead. chef with tall hat and robot with multiple arms cooking gourmet meal in state-of-the-art kitchen. The chef is chopping vegetables with precision, while the robot is simultaneously stirring, frying, and baking. The kitchen is equipped with the latest culinary technology, creating seamless cooking experience. painter with beret and poet with quill creating art in sunlit studio. The painter is working on vibrant canvas, while the poet is writing verses inspired by the artwork. The studio is filled with natural light and creative energy, with art supplies scattered around. spider with square face and green-furred puppy having playful fight in whimsical garden. The spider is using its web to swing around, while the puppy is playfully nipping at the spiders legs. The garden is filled with oversized flowers and colorful mushrooms. talking teapot with mustache and dancing teacup with legs performing tea ceremony in an enchanted forest. The teapot is pouring tea, while the teacup is twirling and dancing around. The forest is magical, with glowing plants and twinkling lights. robot with television screen for head and toaster with arms and legs having cooking competition in retro kitchen. The robot is displaying recipes on its screen, while the toaster is popping out perfectly toasted bread. The kitchen is styled with vintage appliances and checkered floors. pair of animated scissors with eyes and mouth and roll of tape with tiny arms and legs wrapping presents in festive workshop. The scissors are cutting wrapping paper with precision, while the tape is sealing the packages with smile. The workshop is decorated with holiday lights and ornaments. pair of animated sneakers with eyes and mouth and talking basketball with face playing game of one-on-one on an urban basketball court. The sneakers are dribbling and making quick moves, while the basketball is bouncing and trying to score. The court is surrounded by graffiti-covered walls and cheering spectators. paper airplane with scarf and paper boat with captains hat racing in the rain. The airplane glides through the air while the boat sails through puddles. basketball with mohawk and soccer ball with bandana playing hopscotch in playground. The basketball bounces high while the soccer ball rolls smoothly. mechanical knight with steam-powered joints standing guard at an ancient castle gate. Gears whir softly as its head turns to scan the surroundings, while steam occasionally escapes from its armor joints. wandering alchemist with potion-filled vials clinking on their belt, gathering herbs in an enchanted forest where mushrooms glow and flowers whisper secrets. mysterious plague doctor with clockwork enhancements peeking through their dark robes, mixing herbal remedies in medieval apothecary shop as green smoke swirls from bubbling vials."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}