{
    "paper_title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control",
    "authors": [
        "Zhenxing Mi",
        "Yuxin Wang",
        "Dan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D"
        },
        {
            "title": "Start",
            "content": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control Zhenxing Mi, Yuxin Wang, Dan Xu The Hong Kong University of Science and Technology (HKUST) zmiaa@connect.ust.hk, ywangom@connect.ust.hk, danxu@cse.ust.hk 5 2 0 2 4 2 ] . [ 1 2 2 9 8 1 . 1 1 5 2 : r Figure 1. One4D supports single-image-to-4D generation, sparse-frame-to-4D generation, and full-video reconstruction in single model. It outputs synchronized RGB frames and pointmaps, visualized as 4D point clouds with cameras, and RGB-depth sequences."
        },
        {
            "title": "Abstract",
            "content": "We present One4D, unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from single image, 4D reconstruction from full video, and mixed generation and reconstruction from sparse frames. Our framework adapts powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents step toward general, high-quality geometrybased 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D. 1. Introduction Simulating the dynamics of the physical world has progressed rapidly with video diffusion and flow-matching models [2, 3, 17, 19, 36, 39, 53, 63]. Recent open systems such as Wan [39], HunyuanVideo [17], and Cosmos [23] demonstrate remarkable visual fidelity and strong understanding of real-world dynamics. However, these foundation models operate purely in RGB space while lacking explicit geometry. Augmenting them with accurate geometry generation is key step toward downstream worldsimulation tasks such as spatial reasoning [50]. Concurrently, scalable 3D/4D foundation models have advanced rapidly. Dust3R [43] introduces pointmap representation encoding both geometry and camera information, enabling efficient feedforward 3D reconstruction. Monst3R [56] and VGGT [40], etc., show that pointmaps are effective for static 3D and dynamic 4D reconstruction. Meanwhile, several works extend video or multi-view diffusion models with 6D video representations (RGB+XYZ) for 4D reconstruction [14], 3D world generation [59], 3D generation and reconstruction [33], and 4D generation [5]. However, these methods typically specialize in either reconstruction or generation, or are restricted to static 3D scenes. In this paper, we take step further and propose One4D, unified 4D generation and reconstruction framework that significantly enhances the fidelity of joint RGB-geometry modeling through innovative network designs. We represent each 4D scene as RGB frames and pointmaps due to their flexibility and scalability, where pointmaps are 3-channel 2D (XYZ) videos analogous to RGB videos. Prior work [5, 14, 33, 59] shows that modern video VAEs can effectively encode and decode pointmaps. central challenge in joint RGB-geometry modeling is to fully exploit the strong priors of pretrained video diffusion model to generate distinct modalities. Existing diffusionbased geometry methods [14, 15, 59] typically couple RGB and geometry through channel-wise concatenation, which in our setting causes severe cross-modal interference and rapid degradation under low-resource fine-tuning. To address this, we introduce the Decoupled LoRA Control (DLC), an efficient adaptation design guided by three (i) preserve the base models strong video priors goals: under low-resource finetuning; (ii) decouple RGB and geometry generation to reduce mutual interference; (iii) enable sufficient cross-modal communication for pixel-level consistency. Concretely, DLC attaches two decoupled and modality-specific LoRA adapters to the video backbone, one for RGB and the other for geometry. The adapters share the frozen base parameters but not the base forward computation, forming two fully decoupled computation branches. The RGB branch maintains pretrained video quality, while the geometry branch adapts to the geometry video distribution. To synchronize the two modalities, we add zero-initialized control links [57] between few corresponding layers across branches. These links gradually learn to transmit information for precise pixel-wise alignment. In practice, DLC prevents mode collapse and yields accurate geometry without sacrificing RGB fidelity. To unify 4D generation and reconstruction tasks, we further propose Unified Masked Conditioning (UMC). UMC packs different condition types into single conditioning video that differs in the sparsity of observed frames, filling unobserved frames with zeros. single-image (with text) corresponds to pure generation. set of sparse frames corresponds to mixed generation and reconstruction, and full video corresponds to pure reconstruction. The conditioning video is fed to the diffusion backbone to guide the network to generate missing RGB frames and full pointmaps. With UMC, One4D transitions seamlessly between generation and reconstruction without any architectural changes. We train One4D on curated mixture of synthetic and real 4D datasets, combining accurate synthetic geometry and in-the-wild appearance. The resulting model achieves generalizable, high-quality 4D generation and reconstruction in single framework. Our main contributions are: We introduce One4D, unified 4D framework that bridges 4D generation and 4D reconstruction within single video diffusion model, achieving strong performance across diverse dynamic scenarios. We propose Decoupled LoRA Control (DLC), which uses modality-specific LoRA branches and zero-initialized control links to preserve video priors while enabling accurate, consistent joint RGB-geometry generation. We design Unified Masked Conditioning (UMC), single conditioning interface that supports various 4D generation and reconstruction tasks without model changes. 2. Related Work Video generation models. Video generation models [10, 17, 19, 23, 24, 39, 53, 63] have advanced rapidly with diffusion [11, 26, 28, 30, 31] and flow-matching models [6, 20]. Modern systems typically combine 3D causal VAE [2, 16, 46, 55] with DiTs [26] to model spatiotemporal dynamics. Recent open-weight models such as Wan [39], HunyuanVideo [17], and Cosmos [23] demonstrate strong world modeling abilities, suggesting possibilities for geometry generation. Built upon powerful base textto-video models, image-to-video systems such as Wan [39] and LongCat-Video [37] unify image-to-video, frame interpolation, and video continuation via frame inpainting within single architecture. One4D follows similar spirit to unify the 4D generation and reconstruction tasks, with specific designs for joint RGB and geometry generation. Scalable geometry modeling. DUSt3R [43] introduces paired pointmap representation that encodes both geometry and camera information in 2D map, enabling scalable 3D reconstruction with transformers. Subsequent works [14, 34, 40, 42, 51, 56] extend this representation to multiview and dynamic reconstruction. Our method leverages these advances by adopting pointmaps as the geometry Figure 2. Architecture comparison for joint RGB and geometry modeling. (a) Channel-wise and (b) spatial-wise concatenation feed RGB and XYZ into single diffusion model with shared LoRA branch. (c) Our Decoupled LoRA Control (DLC) employs two modality-specific LoRA branches with zero-initialized control links, achieving decoupled yet controlled RGBXYZ joint generation. denotes concatenation and denotes pixel-wise addition. representation for joint RGBgeometry generation. 3D and 4D generation. Leveraging strong 2D diffusion models [7, 28] for 3D/4D advances rapidly. Several methods repurpose diffusion models for reconstructing depth and normal maps [15, 21], conditioning noisy geometry with clean RGB latents. Other works use diffusion models as multiview generators conditioned on cameras, such as CAT3D [8], Bolt3D [33], and Stable virtual camera [64], and extend this to multi-view videos for 4D generation [47, 49]. Their support for single or multiple views with camera poses as input is also multitasking design. Some other works focus on 3D object generation from images or text [27, 48, 58, 61]. Several methods model geometry and camera poses jointly via pointmaps, raymaps, or depth maps [5, 14, 22, 33, 59]. WVD [59] generates 6D (RGB+XYZ) static scenes from single image using channel-wise concatenation of RGB and pointmaps, while 4DNeX [5] generates dynamic scenes via spatial-wise concatenation. Our method also adopts the RGB+XYZ representation, but differs in two key aspects. (1) We unify dynamic 4D generation and reconstruction within single model. (2) We introduce Decoupled LoRA Control (DLC), which achieves substantially higher quality and geometry accuracy than channel-wise or spatial-wise concatenation for coupling RGB and geometry. 3. Method 3.1. Overview One4D extends flow-matching video generation model [6, 20, 39] for joint RGB and geometry generation. As shown in Figure 4, given input images and text prompt, the model generates synchronized RGB frames Xrgb R3F HW and pointmaps Xxyz R3F HW , where each pointmap pixel stores the 3D coordinates of the corresponding RGB Figure 3. Comparison of architectures for joint RGBgeometry generation. Our Decoupled LoRA Control produces cleaner RGB and sharper, more consistent XYZ and depth than channel-wise and spatial-wise concatenation, while channel-wise concatenation severely degrades both appearance and geometry. pixel. denotes the number of frames and the spatial resolution. At each training step, RGB and pointmaps are encoded by video VAE into latents zrgb, zxyz Rcf hw. We then sample timestep [0, 1] and Gaussian noise ϵt xyz (0, I), , and construct noisy latents using the Rectified Flow formulation [6, 20]: rgb, ϵt rgb = tzrgb + (1 t)ϵt zt xyz = tzxyz + (1 t)ϵt zt rgb, xyz. (1) (2)"
        },
        {
            "title": "The noisy latents and conditioning inputs are fed into",
            "content": "DiTs [26] to predict velocities supervised by: vt rgb = zrgb ϵrgb vt xyz = zxyz ϵxyz, (3) (4) and we train with mean-squared error loss between predicted and ground-truth velocities for both modalities. Based on this structure, we introduce Decoupled LoRA Control (DLC) for stable, modality-specific adaptation with pixel-level consistency between RGB and geometry, and Unified Masked Conditioning (UMC) to handle 4D generation and reconstruction in single model. After generation, camera poses and depthmaps are recovered from the pointmaps via lightweight global optimization [14, 43]. 3.2. Decoupled LoRA Control Previous diffusion-based reconstruction methods, such as Marigold [15] and Geo4D [14], generate depthmaps or pointmaps conditioned on clean RGB inputs. They typically concatenate clean RGB latents with noisy geometry latents channel-wise, which works when RGB is fixed. Figure 4. Overview of the One4D framework. Unified Masked Conditioning (UMC) packs single-image, sparse-frame, and full-video inputs into masked conditioning video. RGB and XYZ videos are encoded into latent spaces via video VAEs, and the conditioning latents are concatenated only with noisy RGB latents. These RGB and XYZ latents are then processed by DiT backbone with Decoupled LoRA Control (DLC). DLC employs modality-specific LoRA branches to decouple computation, and zero-initialized cross-modal control links to learn pixel-wise consistency. The denoised RGB and XYZ latents are finally decoded into RGB frames and pointmaps. However, in the joint generation setting, both RGB and geometry latents are noisy, and such direct coupling leads to severe quality degradation. WVD [59] still adopts channelwise concatenation for joint RGB and XYZ generation, but is trained only on static scenes and requires extreme compute (over 1M steps on 64 A100s). As shown in Figure 3, under moderate compute, we observe that both channelwise and spatial-wise concatenation [5] induce premature and excessive interaction between modalities, degrading RGB quality or preventing high-quality geometry learning. To address these challenges, we propose the Decoupled LoRA Control (DLC), which decouples RGB and XYZ computation to minimize cross-modal interference, while introducing pixel-wise cross-modal communication in gradually learnable manner. Decoupled computation. DLCs first principle is decoupling. We maintain two computation branches for RGB and XYZ tokens, each equipped with its own LoRA adapter, so the model can adapt to each modality without mutual degradation. For submodule with LoRA in DiTs, the modalityspecific computation can be written as: rgb = DiTSubmoduleWithRGBLoRA(zrgb), xyz = DiTSubmoduleWithXYZLoRA(zxyz) (5) (6) parameters between modalities and add two separate LoRA adapters on each DiT submodule, while keeping the forward computation disjoint. rgb = DiTSubmodule(zrgb) + RGBLoRA(zrgb), xyz = DiTSubmodule(zxyz) + XYZLoRA(zxyz) (7) (8) Each DiT submodule is evaluated once per modality, reusing weights but keeping computations decoupled. This drastically reduces memory usage compared to duplicating parameters, making finetuning feasible for large models. The decoupled design of DLC preserves pretrained video priors in RGB branch while allowing XYZ branch to adapt to pointmap generation, achieving high fidelity in both modalities without cross-modal degradation. Control links. DLCs second principle is control. For 4D generation, RGB and geometry outputs must be pixel-wise consistent. Channel-wise concatenation enforces strong coupling but harms video quality, while spatial-wise concatenation relies on non-pixel-aligned attention interactions that are relatively weak, as shown in Figure 3. To obtain strong yet controllable cross-modal consistency, we introduce lightweight control links to connect the two branches. Implementing the decoupled computation by simply duplicating all DiT parameters and attaching distinct LoRAs to each copy is infeasible for large-scale base models (14B parameters in our case). Instead, we share the frozen base"
        },
        {
            "title": "Let",
            "content": "the DiT backbone have layers, and let {Li1 , . . . , Lim }, with 1 i1 < < im denote subset of layers where we insert cross-modal control links. At linked layer l, features of one modality are updated by features from the other modality: 3.4. Post-Optimization ˆz(l) rgb = z(l) xyz = z(l) ˆz(l) rgb + ZCLrgbxyz xyz + ZCLxyzrgb xyz (cid:0)z(l) (cid:0)z(l) rgb (cid:1), (cid:1), (9) where ZCLrgbxyz and ZCLxyzrgb are zero-initialized linear control links. The updated features ˆz(l) xyz then passed to the subsequent DiT layers. Zero initialization [57] keeps the two branches fully independent at the start of training, preserving the pretrained video priors and the links gradually learn pixel-level alignment between RGB and geometry. In practice, we link only small subset of layers. rgb are ˆz(l) Compared to concatenation-based coupling, DLC provides controlled, sparsely inserted, and gradually learned pathway for cross-modal communication. As shown in Figure 3, it achieves stronger pixel-wise consistency while maintaining high RGB fidelity and geometric accuracy. It also avoids the token-doubling issue of spatial-wise concatenation within single attention operation, thereby reducing memory and compute cost. 3.3. Unified Masked Conditioning We introduce Unified Masked Conditioning to express single-image, sparse-frame, and full-video conditioning within single interface. Prior video generation models such as Wan [39] and LongCat-Video [37] unify different video generation tasks via frame inpainting. We extend this idea to unified 4D generation and reconstruction, where RGB and geometry are modeled jointly. Condition construction. We assemble the available image conditions into conditioning video Xc RCF HW , matching the RGB shape, and fill unobserved frames with zeros. Xc is encoded by the video VAE into zc Rcf hw, which is concatenated channel-wise with the RGB latents zrgb. We also build binary mask Mc R1F HW [39] indicating observed vs. unobserved frames, reshape it to latent resolution Mc Rcmf hw, and concatenate it with RGB latents. The DiT input is: zinput = Concat(zrgb, zc, Mc) (10) Given zc and Mc, the model will generate missing RGB frames and the full set of pointmaps. This unified construction handles single-image, sparse-frame, and full-video conditioning without changing the architecture. Controlling geometry. Since all XYZ frames are always generated, we do not feed zc or Mc directly to the geometry branch, avoiding conditioning artifacts in geometry. Instead, conditioning signals reach the geometry branch through DLC control links from the RGB branch, allowing the geometry branch to focus on accurate 3D structure. With the above designs, UMC makes One4D seamlessly switch between 4D generation and reconstruction tasks within one unified model. After 4D generation, we apply simple global optimization to recover camera parameters and depth maps from the generated pointmaps, following MonST3R [56] and Geo4D [14]. Given the generated point maps { ˆXi}N i=1, each ˆXi RHW 3, we recover {Ki, Ri, oi, Di}N i=1, where Ki is the intrinsic matrix, Ri is the world-to-camera rotation matrix, oi is the camera center in the global frame, and Di is the depth map of frame i. For each frame and pixel (u, v), the corresponding 3D point in the global reference frame is parameterized as: Xi uv = Ri(cid:0)Di uv Ki1 (u, v, 1)(cid:1) + oi, (11) where Di uv is the depth value at pixel (u, v) in frame i. The predicted point maps ˆXi is treated as the observations of Xi and they are aligned by the loss: Lp = (cid:80)N . We minimize Lp with respect u,v to {Ki, Ri, oi, Di}N i=1, yielding globally consistent camera parameters and depth maps. We further regularize the camera trajectory by temporally smooth loss [14, 56]: uv ˆXi (cid:13) (cid:13) (cid:13)Xi (cid:13) (cid:13) (cid:13)1 (cid:80) i=1 uv Ls(R, o) = 1 (cid:88) i=1 (cid:16)(cid:13) (cid:13)Ri (cid:13) Ri+1 (cid:13) (cid:13) (cid:13)f + (cid:13) (cid:13)oi+1 oi(cid:13) (cid:13)2 (cid:17) . (12) The final post-optimization objective is weighted combination of the point-map alignment and trajectory smoothness losses: Lall = α1Lp + α2Ls. 4. Experiments 4.1. Implementation Details Datasets. We construct 4D training datasets from both synthetic and real videos. We first collect dynamic synthetic 4D datasets OmniWorld-Game [65], BEDLAM [1], PointOdyssey [62], TarTanAir [44], which provide accurate geometry data. To increase dataset scale and diversity, we further annotate real-world videos in SpatialVID [41] with pseudo geometry using Geo4D [14], covering diverse inthe-wild dynamic scenes. Long videos are clipped into segments of about 81 frames and captioned with Gemini-2.0Flash [9]. In total, we obtain about 17k synthetic and 17k real-world clips, with roughly 2M frames. Given camera parameters, depth maps are lifted to 3D pointmaps using the first frame as the global coordinate frame, and pointmaps are normalized to [1, 1] before video-VAE encoding. Training. One4D is built on Wan2.1-Fun-V1.1-14BInP [35], community finetuned version of Wan2.1-I2V14B [38] for video inpainting. We apply LoRA with rank 64 to all linear layers in the DiT for both RGB and XYZ branches, with 685M parameters. We add DLC control links to five DiT layers, introducing 250.7M parameters. Figure 5. Single-image-to-4D generation comparison between 4DNeX [5] and our One4D. Compared to 4DNeX, One4D produces more dynamic and realistic videos, sharper and cleaner depth, and more complete, coherent 4D point clouds with cameras. Table 1. User study comparing 4DNeX [5] and One4D for 4D generation. Percentages indicate user preference. Our model shows clear overall advantage, especially on geometry-related criteria. Table 2. VBench [13] for video quality. One4D significantly improves motion dynamics and aesthetic quality over 4DNeX, while maintaining comparable image-to-video (I2V) consistency. Method Consistency Dynamic Aesthetic Depthmap 4D Method Dynamic I2V consistency Aesthetic 4DNeX [5] One4D (Ours) 21.0% 78.9% 16.7% 83.3% 17.7% 82.3% 11.7% 88.3% 10.0% 90.0% 4DNeX [5] One4D (Ours) 25.6% 55.7% 98.7% 97.8% 61.9% 63.8% Overall, our model has 935.7M trainable parameters. Training is done on 8 NVIDIA H800 GPUs with batch size 1 per GPU and gradient accumulation of 4, for 5500 steps at learning rate of 1 104. We randomly switch among tasks by varying the number of masked frames. The task sampling ratios are 0.35 for single-image, 0.30 for sparseframe, and 0.35 for full-video input. The maximum number of training frames is 81, at resolution of 352 624. Inference. At inference, we use 50 flow-matching steps with classifier-free guidance scale 6.0. The model jointly generates RGB frames and corresponding pointmaps (XYZ). From the pointmaps, we derive depth maps and estimate camera trajectories via post-optimization. For visualization, pointmaps (XYZ) are interpreted as RGB images. Depth maps are mapped to three-channel images. We use Viser [54] to visualize 4D point clouds together with their camera trajectories, typically subsampling frames with temporal stride to better show motion. 4.2. 4D Generation We compare One4D with the recent 4D generation method 4DNeX [5], which relies on spatial-wise concatenation. Both models take single image and text prompt as input. We evaluate the generated results using VBench [13], user study, and qualitative comparisons. As shown in Table 1, One4D is preferred over 4DNeX across image-to-video consistency, motion dynamics, aesthetics, and geometry-related criteria such as depth quality and overall 4D coherence. VBench scores in Table 2 further show that One4D produces stronger, more natural motion while maintaining comparable image-to-video consistency. These results support the effectiveness of our decoupled design, which learns accurate geometry without sacrificing video quality. Figure 5 qualitatively compares RGB frames, 4D point clouds, and depth maps generated by the two methods. One4D generates finer geometric details, more accurate depth, and richer motion, whereas 4DNeXs spatial-wise concatenation struggles to produce fine-grained geometry, as also illustrated in Figure 3. After lifting to 4D, One4D yields coherent scenes with stable backgrounds and naturally moving foreground objects, while 4DNeX often shows limited dynamics. Additional single-image-to-4D results in Figure 6 demonstrate that One4D produces high-quality geometry and realistic 4D structure across diverse indoor, outdoor, and static, dynamic scenarios. These high-quality RGB videos and fine-grained, pixelFigure 6. Additional single-image-to-4D generation results from One4D. It can generate coherent 4D geometry for various types of scenes. Table 3. Trained as unified generationreconstruction model (G&R), One4D outperforms reconstruction-only (R) pointmapbased methods such as MonST3R and CUT3R, and remains reasonably close to the reconstruction-only Geo4D-ref, demonstrating effective geometry reconstruction within unified architecture. Table 4. Camera trajectory accuracy. The Task column distinguishes reconstruction-only (R) methods from our unified generationreconstruction model (G&R). One4D achieves camera accuracy comparable to strong reconstruction-only baselines, indicating our unified 4D model performs competitive camera estimation. Method Task Marigold [15] Depth-Anything [52] NVDS [45] ChronoDepth [29] DepthCrafter [12] Robust-CVD [18] CasualSAM [60] MonST3R [56] CUT3R [42] Geo4D-ref [14] R R One4D (Ours) G&R Sintel [4] Bonn [25] Abs Rel δ < 1.25 Abs Rel δ < 1.25 Method Task Sintel [4] TUM-dynamics [32] ATE RPE-T RPE-R ATE RPE-T RPE-R 0.532 0.367 0.408 0.687 0.270 0.703 0.387 0.335 0.311 0. 0.273 51.5 55.4 48.3 48.6 69.7 47.8 54.7 58.5 62.0 73.5 70.4 0.091 0.106 0.167 0.100 0.071 - 0.169 0.063 0.070 0.059 0.092 93.1 92.1 76.6 91.1 97.2 - 73.7 96.4 96.7 97. 93.7 Robust-CVD [18] CasualSAM [60] MonST3R [56] CUT3R [42] Geo4D-ref [14] R 0.360 0.141 0.108 0.208 0.185 One4D (Ours) G&R 0. 0.154 0.035 0.042 0.062 0.063 0.057 3.443 0.615 0.732 0.610 0.547 0.818 0.153 0.071 0.063 0.046 0.073 0. 0.026 0.010 0.009 0.014 0.020 0.022 3.528 1.712 1.217 0.446 0.635 1.447 Table 5. Depth accuracy for sparse-frame-to-4D generation on Sintel and Bonn. Using only fraction of frames (Sparsity), One4D remains competitive and degrades gracefully, indicating strong geometry generation from sparse observations. wise aligned geometry validate the design of Decoupled LoRA Control (DLC), which preserves the strong generative priors of the base model while learning accurate geometry and maintaining pixel-wise consistency. 4.3. 4D Reconstruction We evaluate 4D reconstruction in both full-video and sparse-frame settings on several benchmarks to assess our unified generationreconstruction framework. Sintel [4] provides synthetic video with accurate ground-truth depthmaps, about 50 frames per video. Bonn [25] contains real dynamic indoor scenes, about 110 frames per video. TUM-dynamics[32] contains dynamic scenes sampled to 30 frames for each video. Our evaluation setting closely follows MonST3R[56] and Geo4D[14]. We compared depth and camera trajectory derived from our generated pointmaps to reconstruction-only baselines. Marigold [15] and Depth-Anything-V2 [52] are singleimage depth methods. NVDS [45], ChronoDepth [29] and DepthCrafter [12] operate on videos. In addition, RobustCVD [18], CasualSAM [60], MonST3R [56], CUT3R [42], Geo4D [14] jointly reconstruct video depth maps and camera poses. Although Geo4D [14] is used to annotate our real-world training data, we still report its numbers (Geo4DRef) as strong reconstruction-only reference. For depth evaluation, we use Sintel [4] and Bonn [25], align predicted depths to the ground truth, and report the Sparsity 0.50 0.25 0.10 0.05 0.04 0. Full Model Sintel [4] Bonn [25] Abs Rel δ < 1.25 Abs Rel δ < 1.25 0.314 0.443 0.453 0.641 - - 0.273 70.3 67.7 64.0 57.6 - - 70.4 0.094 0.094 0.099 0.151 0.191 0.277 0. 93.5 93.3 92.9 87.2 82.5 71.1 93.7 absolute relative error (Abs Rel) and the percentage of inlier points with δ < 1.25. For camera evaluation, we use Sintel[4] and TUM-dynamics[32], and report Absolute Trajectory Error (ATE), Relative Pose Error in translation (RPE-T), and Relative Pose Error in rotation (RPE-R). Full-video-to-4D. Table 3 reports full-video depth accuracy. Among pointmap-based methods, One4D clearly outperforms MonST3R [56] and CUT3R [42] on Sintel (70.4% vs 58.5 / 62.0 on δ < 1.25), and remains close to the reconstruction-only Geo4D-ref [14], even though those models are trained purely for reconstruction and One4D is trained once for both 4D generation and reconstruction. This indicates that our Decoupled LoRA Control and Unified Masked Conditioning allow single generative model to recover highly accurate geometry. Results for CUT3R are obtained using the official Geo4D evaluation scripts. Results of other baselines are taken from MonST3R [56] and Geo4D [14] papers. Camera accuracy in Table 4 shows that One4D achieves of frames, One4D is almost as accurate as in the full-video setting on both datasets. Even at sparsity 0.10, degradation is modest. Moreover, under extreme sparsity (e.g., 0.05 or 0.03), the model still produces reasonable geometry from only 2 or 3 frames, typically just the first and last frames. This is also partly because these boundary frames already capture sufficient background information. Overall, the sparse-frame evaluation shows that our model can reliably generate unobserved RGB frames and corresponding 4D structure from very sparse observations, complementing the single-image-to-4D generation results and highlighting the strength of our unified generationreconstruction design. 4.4. Additional Ablation Study Beyond our main generation and reconstruction experiments, we further ablate two factors of One4D on Sintel and Bonn, including the classifier-free guidance (CFG) scale and the number of training steps. Results are summarized in Tables 5 and 6. We use CFG = 6 as the default setting in all main experiments. As shown in Table 6, using CFG = 4 or CFG = 5 yields very similar depth accuracy, indicating that One4D is robust to the choice of guidance scale. This robustness simplifies deployment in real-world applications. We also study the effect of training steps by training models with 1K, 3K, and 5.5K optimization steps. Even with only 1k steps, One4D already achieves reasonable accuracy, and at 3k steps, it is close to the full model. Compared to WVD [59], which requires around 1M steps over two weeks on 64 A100 GPUs with channel-wise concatenation, our decoupled design adapts the base video model with much fewer training steps. This efficiency supports our claim that the proposed architecture preserves and effectively leverages pretrained video priors. As the number of training steps increases, performance generally improves, indicating promising potential for further gains when scaling to larger datasets and more computation. 5. Conclusion We presented One4D, unified 4D framework that bridges 4D generation and 4D reconstruction within single video diffusion model. We introduced Decoupled LoRA Control for robust joint RGB and geometry modeling. We proposed Unified Masked Conditioning, simple conditioning scheme that seamlessly handles pure generation, mixed generationreconstruction, and pure reconstruction without modifying the architecture. Trained on curated mixture of synthetic and real 4D data, One4D achieves generalizable, high-quality 4D results and takes step toward geometryaware world simulation with video foundation models. Figure 7. Qualitative full-video 4D reconstruction comparison with CUT3R [42], MonST3R [56], and Geo4D [14]. The proposed One4D recovers sharper object boundaries and more accurate depth, especially on thin structures and challenging geometry. Table 6. Ablation on CFG scale and training steps for depth reconstruction. Our model is robust to CFG choice and attains good accuracy with few training steps, improving with longer training. Setting CFG=4 CFG=5 Step=1000 Step= One4D (Ours) Sintel [4] Bonn [25] Abs Rel δ < 1.25 Abs Rel δ < 1.25 0.257 0.259 0.331 0.284 0.273 71.5 70.9 65.4 68.1 70. 0.092 0.090 0.114 0.097 0.092 94.0 94.1 88.9 91.8 93. competitive ATE and RPE scores within the same range of Geo4D-ref and other reconstruction baselines, confirming that our pointmaps are sufficiently accurate to support robust camera estimation. Qualitative results in Figure 7 further show that One4D can recover fine geometric structures (e.g., bamboo leaves, ropes) and robustly handles challenging scenes such as dense bamboo forests. Overall, the strong depth and camera reconstruction results across Sintel, Bonn, and TUM-dynamics highlight that our designs of DLC and UMC enable One4D to effectively reconstruct 4D geometry and camera trajectories, even though we train it as single unified model for both generation and reconstruction. Sparse-frame-to-4D. In the sparse-frame-to-4D setting, we keep the first frame and last frame of each video and uniformly sample additional frames in between, with the total number of observed frames controlled by sparsity ratio. The model must generate all missing RGB frames and the complete pointmap sequence. Table 5 reports depth accuracy on Sintel and Bonn with different spatial ratios. Remarkably, with only 50% or 25%"
        },
        {
            "title": "References",
            "content": "[1] Michael Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: synthetic dataset of bodies exhibiting detailed lifelike animated motion. In CVPR, 2023. 5 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. 1 [4] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In ECCV, 2012. 7, 8 [5] Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, and Ziwei Liu. 4dnex: arXiv Feed-forward 4d generative modeling made easy. preprint arXiv:2508.13154, 2025. 2, 3, 4, 6 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, [7] Black Forest. Flux. https://github.com/blackforest-labs/flux, 2024. GitHub repository. 3 [8] Ruiqi Gao, Aleksander Hołynski, Philipp Henzler, Arthur Pratul Srinivasan, Brussee, Ricardo Martin-Brualla, Jonathan Barron, and Ben Poole. Cat3d: create anything in 3d with multi-view diffusion models. In NeurIPS, 2024. 3 Introducing gemini 2.0: [9] Demis Hassabis, Koray Kavukcuoglu, and the Gemour new ai ini Team. Online at https : / / model for blog.google/technology/googledeepmind/ googlegeminiaiupdate- december2024/, 2024. 5 the agentic era. [10] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 2 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [12] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025. 7 [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 6 [14] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. In ICCV, 2025. 2, 3, 5, 7, [15] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 2, 3, 7 [16] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2 [17] Weijie Kong, Qi Tian, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 2 [18] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, 2021. 7 [19] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1, [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3 [21] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: SinIn CVPR, gle image to 3d using cross-domain diffusion. 2024. 3 [22] Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, and Shiwei Li. Matrix3d: Large photogrammetry model all-in-one. In CVPR, 2025. 3 [23] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, et al. Cosmos world foundation model platform for physical ai, 2025. 2 Video generation models as world simula- [24] OpenAI. https : / / openai . com / index / video - tors. generation - models - as - world - simulators/, 2024. 2 [25] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In IROS, 2019. 7, [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2, 3 [27] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 3 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3 [29] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. In CVPR, 2025. 7 [30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2020. [31] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 2 [32] Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In IROS, 2012. 7 [33] Stanislaw Szymanowicz, Jason Y. Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan T. Barron, and Philipp Henzler. Bolt3D: Generating 3D Scenes in Seconds. ICCV, 2025. 2, 3 [34] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. In CVPR, 2025. 2 [35] Alibaba PAI Team. Wan2.1-Fun-V1.1-14B-InP. https:// huggingface.co/alibabapai/Wan2.1FunV1.1-14B-InP, 2024. 5 [36] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 1 [37] Meituan LongCat Team. Longcat-video technical report, 2025. 2, 5 [38] Wan-AI Team. Wan2.1-I2V-14B-480P. https : / / huggingface . co / Wan - AI / Wan2 . 1 - I2V - 14B - 480P, 2024. 5 [39] Team Wan, Ang Wang, Baole Ai, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 3, [40] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. 2 [41] Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, et al. Spatialvid: large-scale video dataset with spatial annotations. arXiv preprint arXiv:2509.09676, 2025. 5 [42] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 2, 7, 8 [43] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 2, 3 [44] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In IROS, 2020. 5 [45] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin. Neural video depth stabilizer. In ICCV, 2023. 7 [46] Pingyu Wu, Kai Zhu, Yu Liu, Liming Zhao, Wei Zhai, Yang Improved video vae for latent Cao, and Zheng-Jun Zha. video diffusion model. In CVPR, 2025. 2 [47] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. In CVPR, 2025. 3 [48] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. In NeurIPS, 2024. 3 with multi-frame and multi-view consistency. In ICLR, 2025. 3 [50] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171, 2024. 2 [51] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In CVPR, 2025. [52] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024. 7 [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2 [54] Brent Yi, Chung Min Kim, Justin Kerr, Gina Wu, Rebecca Feng, Anthony Zhang, Jonas Kulhanek, Hongsuk Choi, Yi Ma, Matthew Tancik, et al. Viser: Imperative, web-based 3d visualization in python. arXiv preprint arXiv:2507.22885, 2025. 6 [55] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. 2 [56] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. In ICLR, 2025. 2, 5, 7, 8 [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2, [58] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. TOG, pages 120, 2024. 3 [59] Qihang Zhang, Shuangfei Zhai, Miguel Angel Bautista, Kevin Miao, Alexander Toshev, Joshua Susskind, and Jiatao Gu. World-consistent video diffusion with explicit 3d modeling. In CVPR, 2025. 2, 3, 4, 8 [60] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In ECCV, 2022. 7 [61] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 3 [62] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale In CVPR, synthetic dataset for long-term point tracking. 2023. [49] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation [63] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 1, 2 [64] Jensen Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025. 3 [65] Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, et al. Omniworld: multi-domain and multi-modal dataset for 4d world modeling. arXiv preprint arXiv:2509.12201, 2025."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology (HKUST)"
    ]
}