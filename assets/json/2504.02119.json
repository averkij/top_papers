{
    "paper_title": "Efficient Model Selection for Time Series Forecasting via LLMs",
    "authors": [
        "Wang Wei",
        "Tiankai Yang",
        "Hongjie Chen",
        "Ryan A. Rossi",
        "Yue Zhao",
        "Franck Dernoncourt",
        "Hoda Eldardiry"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 9 1 1 2 0 . 4 0 5 2 : r Preprint. Under review."
        },
        {
            "title": "Efficient Model Selection for Time Series Forecasting via\nLLMs",
            "content": "Wang Wei Department of Computer Science Virginia Tech Blacksburg, VA, USA wangwei718@vt.edu Hongjie Chen Dolby Labs Atlanta, GA, USA hongjie.chen@dolby.com Ryan A. Rossi Adobe Research San Jose, CA, USA ryrossi@adobe.com Tiankai Yang Department of Computer Science University of South California Los Angeles, CA, USA tiankaiy@usc.edu Yue Zhao Department of Computer Science University of South California Los Angeles, CA, USA yzhao010@usc.edu Franck Dernoncourt Adobe Research Seattle, WA, USA dernonco@adobe.com Hoda Eldardiry Department of Computer Science Virginia Tech Blacksburg, VA, USA hdardiry@vt.edu"
        },
        {
            "title": "Abstract",
            "content": "Model selection is critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Metalearning approaches aim to automate this process, but they typically depend on pre-constructed performance matrix, which is costly to build. In this work, we propose to leverage Large Language Models (LLMs) as lightweight alternative for model selection. Our method eliminates the need for explicit performance matrix by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with Llama, GPT, and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting."
        },
        {
            "title": "Introduction",
            "content": "Time series forecasting plays crucial role in wide range of real-world applications, enabling informed decision-making and strategic planning across various domains, including finance (Sezer et al., 2020), healthcare (Bui et al., 2018), software monitoring (Sun et al., 2023), energy (Chou & Tran, 2018), retail (Fildes et al., 2022), and weather prediction (Han et al., 2024). Selecting an appropriate forecasting model is often labor-intensive process requiring domain expertise and extensive computational resources. Traditional time series forecasting methods typically require substantial domain expertise and manual effort in model design, feature engineering, and hyperparameter tuning. This challenge is further intensified by the findings of Abdallah et al. (2022), which indicate that no single learning strategy consistently outperforms others across all forecasting tasks, due to the inherent diversity of time series data. Consequently, traditional methods often fail to deliver high-quality predictions across diverse application domains. straightforward but naıve solution would be evaluating the performance for thousands of models on given dataset to identify the most suitable one. However, such an approach is impractical due to the excessive computational cost and training time required for model evaluation on each new dataset. 1 Preprint. Under review. To address the impracticality of exhaustively evaluating all models for each new dataset, meta-learning has recently gained great popularity in applications demanding model selection such as anomaly detection and classification (Zhao et al., 2021), graph learning (Park et al., 2023), and recommendation (Wang et al., 2022), especially for forecasting (Abdallah et al., 2022), which could quickly infer the best forecasting model after training on the models performances on historical datasets and the time-series meta-features of these datasets. Even though Abdallah et al. (2022) selects the best performing forecasting algorithm and its associated hyper-parameters with 42 median inference time reduction averaged across all datasets compared to the naıve approach, nearly all state-of-the-art meta-learning approaches still require the construction of large performance matrix, consisting of evaluations of hundreds or even thousands of models across vast collection of forecasting datasets. This performance matrix, while crucial for traditional meta-learningbased model selection, is extremely costly to obtain in practice. Each dataset-model pair must be exhaustively evaluated, which demands significant computational resources and time. Furthermore, this matrix is typically used in conjunction with carefully engineered meta-feature vector extracted from each time-series dataset to train meta-learning model that can generalize and infer the best model for new forecasting tasks. LLMs have demonstrated exceptional generalization and reasoning capabilities, positioning them as promising tools for automating model selection in time series forecasting. By leveraging zero-shot prompting techniques, LLMs can generate structured reasoning paths without the need for task-specific exemplars. For instance, Kojima et al. (2023) introduced method where appending the phrase Lets think step by step to prompt enables LLMs to perform complex reasoning tasks effectively. Building upon this, Kumar et al. (2024) proposed the Zero-shot Uncertainty-based Selection (ZEUS) approach, which enhances chain-of-thought (CoT) prompting by utilizing uncertainty estimates to select effective demonstrations without requiring access to model parameters. These advancements suggest that LLMs, through zero-shot and CoT prompting, can be harnessed to streamline model selection processes, reducing the need for exhaustive evaluations and manual interventions. In this work, we propose an alternative paradigm: using LLMs to perform model selection without the need for an explicit performance matrix. Following the benchmark data specified in Abdallah et al. (2022)s work, we investigate the effectiveness of LLMs in model selection for time series forecasting. Extensive experiments on over 320 datasets show that our method outperforms strategies such as directly selecting popular methods and even different meta-learning approaches(Kadioglu et al., 2010) (including simple and optimization-based meta-learners where performance matrix is built and used during training). Summary of Main Contributions. The key contributions of this work are as follows: LLM-Driven Zero-Shot Model Selection for Time-Series Forecasting. To the best of our knowledge, this work is the first to investigate the use of LLMs for selecting the most suitable time series forecasting model via zero-shot prompting. By evaluating multiple LLMs with various prompt designs, we demonstrate that LLM-based selection consistently outperforms both popular forecasting models and meta-learning approaches. Computational Efficiency in Training and Inference. Unlike conventional model selection techniques that require training and evaluation of multiple forecasting models and the costly pre-computed performance matrix required in traditional meta-learning, our approach leverages LLMs to infer the optimal model and hyperparameters instantly. This results in significant reduction in computational overhead, making the method highly scalable and efficient for real-world forecasting applications. Ablation Study on Prompt Design for Model Selection. We conduct an ablation study to analyze the impact of incorporating meta-features and CoT reasoning in prompts across different LLMs. The findings could offer insights into effective prompt design strategies, guiding future improvements in LLM-driven model selection for time-series forecasting. 2 Preprint. Under review."
        },
        {
            "title": "2 Related Work",
            "content": "Model Selection in Time Series Forecasting. Model selection in time series forecasting has evolved through various methodologies, encompassing traditional statistical approaches, meta-learning techniques, and the emerging LLMs. Traditional methods often rely on statistical criteria to choose the most suitable forecasting model. For instance, the average rank method evaluates multiple models across different datasets, selecting the one with the lowest average rank based on performance metrics (Cerqueira et al., 2022). While straightforward, these methods can be computationally intensive and may not generalize well across diverse time series data. To overcome these limitations, Lemke & Gabrys (2010) explored meta-learning strategies that utilize characteristics of time series data to predict the performance of various forecasting models, facilitating more efficient and accurate model selection. Similarly, Prudˆencio & Ludermir (2004) investigated meta-learning techniques to rank and select time series models based on extracted meta-features, demonstrating improved forecasting accuracy. Recently, Abdallah et al. (2022) have also demonstrated that meta-learning can be used to infer the best model given dataset characteristics and model space without needing an exhaustive evaluation of all existing models on new dataset. However, these approaches still require constructing performance matrix that capture the evaluation results of all models across all datasets, which is computationally expensive and time-consuming. LLMs for Time Series Forecasting. The integration of LLMs into time series forecasting has garnered significant attention, with recent studies exploring their potential for model selection and prediction tasks. Jin et al. (2024) introduced Time-LLM, framework that reprograms LLMs for time series forecasting by aligning time series data with natural language inputs. Gruver et al. (2024) demonstrated that LLMs, such as GPT-3 and Llama-2, can perform zero-shot time series forecasting by encoding time series as sequences of numerical digits, framing forecasting as next-token prediction task. Cao et al. (2024) introduced an interpretable prompt-tuning-based generative transformer for time series representation learning. Zhang et al. (2024) provided comprehensive survey on the application of LLMs in time series analysis, highlighting their potential to enhance forecasting performance across various domains. However, these studies differ from our approach as they employ LLMs directly as forecasting models for new datasets, whereas our work focuses on leveraging LLMs for model selection. Specifically, we demonstrate how LLMs can effectively identify the most suitable forecasting model to achieve optimal predictive performance. Prompting. Prompting has emerged as the primary approach for tailoring language models to various downstream applications. Zero-shot prompting enables LLMs to perform tasks without specific examples by appending phrases like Lets think step by step to the prompt, effectively eliciting reasoning process (Kojima et al., 2023). CoT prompting further improves multi-step reasoning by incorporating intermediate reasoning steps into the prompt, leading to better performance on complex tasks (Wei et al., 2023). Surveys on prompt design strategies provide comprehensive overviews of techniques such as manual design and optimization algorithms, emphasizing their impact on LLM performance across diverse tasks (Li, 2023). These developments underscore the critical role of prompt engineering in fully leveraging LLMs for complex reasoning and decision-making tasks."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Overview The model selection task for time series forecasting is formulated as mapping from datasetbased prompts to candidate forecasting models. Let : M, where denotes the set of possible prompts and represents the space of candidate forecasting models. For each dataset di D, the process comprises three components: Prompt Construction: Construct prompt pi from di using one of the predefined prompt templates. 3 Preprint. Under review. Figure 1: An overview of model selection via LLMs. Algorithm 1 Model Selection via LLMs Input: Time series dataset dtest, model space M, prompt template Output: Selected forecasting model Step 1: Prompt Construction Get dataset values Xtest from dtest if Meta-features are included then Include meta-features Ftest of dtest end if if CoT is included then Incorporate reasoning steps into prompt end if Generate prompt using: = Format(Xtest, [Ftest], [CoT]) Step 2: Query LLM for Model Selection Obtain selected model = (a, h, g) by querying the LLM: = S(p) where is the forecasting algorithm, is the hyperparameter set, and is the data representation. Step 3: Forecasting and Evaluation Apply to generate forecasts for dtest Compute performance metrics Return: Selected model LLM-Based Model Selection: The prompt pi is submitted to an LLM to obtain the recommended model mi = S(pi), where mi M. Forecasting and Evaluation: Apply mi to di to produce forecasts and evaluate performance using appropriate metrics. Problem Statement. Given new dataset dtest (i.e., unseen time series forecasting task), select model to employ on that dataset. 3.2 Prompt Construction We designed four distinct prompt structures, each varying in the inclusion of meta-features and CoT reasoning. The detailed structure of our prompts is illustrated in Appendix A.1. Dataset Values Only: Providing raw time series data. 4 Preprint. Under review. Dataset Values and Meta-Features: Combining raw data with pre-computed metafeatures from Abdallah et al. (2022). Details of meta-features are available A.2. Dataset Values with CoT: Including raw data along with step-by-step reasoning instruction in the prompt to guide the LLM. Dataset Values and Meta-Features with CoT: Integrating raw data, meta-features, and CoT reasoning. 3.3 Model Selection The model space is denoted as: = {m1, m2, . . . }. Each model mi is given by the tuple mi = (ai, hi, gi()), where ai is the forecasting algorithm, hi is the hyper-parameter vector associated with ai, and gi() : Rni Rni is the time-series data representation (e.g., raw, exponential smoothing). Unlike traditional meta-learning approaches that operate on predefined, discrete model space, our method allows for an infinite and continuous model space, where hyperparameters hi can take any real-valued configuration. 3.4 Comparison with Meta-Learning Meta-learning methods typically rely on an extensive performance matrix: Rnm where Pi,j represents the performance of model Mj on dataset Di. This matrix is computationally expensive to construct and is essential for training meta-learners. In contrast, our approach eliminates the need for: Explicit performance matrix. Our method does not require historical model-dataset performance mappings. Feature engineering. While meta-learners depend on carefully designed meta-features, our LLM-based selection can operate without them. Fixed model spaces. Our method does not restrict selection to predefined set of models and hyperparameters. Methods hit@1 accuracy hit@5 accuracy hit@10 accuracy hit@50 accuracy Random selection Popular Selection ISAC MLP Ours-Llama3.2 w. data w. data+CoT w. data+meta features w. data+meta features+CoT Ours-GPT4o w. data w. data+CoT w. data+meta features w. data+meta features+CoT Ours-Gemini2.0 flash w. data w. data+CoT w. data+meta features w. data+meta features+CoT 0.31 0 0.82 0.62 0.83 0.62 1.14 1. 0.21 0.10 0.62 0.52 0.21 0.31 0 0.21 1.25 1.33 2.67 1.13 3.84 3.12 4.47 3.43 2.39 1.25 2.39 2.60 0.62 0.93 1.04 1. 3.63 3.77 4.10 4.51 6.65 5.82 7.27 6.44 4.47 4.36 4.88 4.88 3.53 2.91 2.80 3.01 14.75 19.94 11.45 22.25 26.27 26.69 29.60 25. 21.39 21.39 20.56 21.91 20.77 19.94 20.87 17.13 Table 1: hit@k Accuracy (the higher (), the better) comparison of LLMs against the different baselines. denotes meta-learning methods which utilized performance matrix during training. 5 Preprint. Under review."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our LLM-based model selection approach through series of experiments designed to address the following research questions: 1. Does employing LLMs for time-series forecasting model selection improve performance compared to not using model selection or other techniques like meta-learners? 2. How much reduction in inference time do LLM-based methods achieve over the naıve approach, and what is the associated token cost for model selection? 3. To what extent do meta-features and CoT prompting contribute to model selection performance, computational efficiency, and token usage? 4.1 Experiment Settings"
        },
        {
            "title": "4.1.1 Dataset and Metrics",
            "content": "Dataset Source. We use the same dataset as Abdallah et al. (2022), which consists of 321 forecasting datasets spanning various application domains, including finance, IoT, energy, and storage. These datasets include benchmark time series from Kaggle, Adobe real traces, and other open-source repositories. For each dataset, we randomly sample time windows of fixed length (= 16) to form our evaluation samples. Evaluation Metrics. Our evaluation focuses on two primary metrics: hit@k accuracy and average Mean Squared Error (MSE). Hit@k accuracy quantifies whether the selected model ranks among the top models based on ground truth performance, while MSE measures the forecast error magnitude. Formally, hit@k accuracy is defined as: hit@k = (cid:16) 1 N i=1 Mk ranked(Di) (cid:17) (1) where Mranked(Di) denotes the set of models ranked by their performance for given dataset Di, I() is an indicator function that equals 1 if is within the top models and 0 otherwise, and is the total number of test datasets. In addition, we record training and inference time, as well as token usage, to assess the computational efficiency and resource overhead of the approaches. To make it fair to compare, we adopt the same model space as our baselines from Abdallah et al. (2022), which comprises 322 unique models (see Table 5 for the complete list). This model space pairs seven state-of-the-art time-series forecasting algorithms with their corresponding hyperparameters and various data representation methods. In addition, we utilize the precomputed performance matrix from Abdallah et al. (2022) to evaluate our proposed methods. 4.1.2 LLMs and Hardware To evaluate the effectiveness of different LLMs in the forecasting model selection task, we conducted experiments using three competitive models: Llama 3.2-3B-Instruct (MetaAI, 2024), GPT-4o (OpenAI, 2024), and Gemini 2.0 Flash (Google, 2024). In experiments with Llama 3.2-3B-Instruct, we utilized single NVIDIA A100 GPU with 80GB of memory. GPT-4o and Gemini 2.0 Flash are accessed via API. 4.2 Baselines We compare our proposed approach against various baseline methods. They fall into two categories: methods that do not perform explicit model selection and meta-learning-based approaches. No Model Selection. In this category, the same fixed model configuration or an ensemble of all models is applied. We consider the following strategies: 6 Preprint. Under review. Methods Seasonal Naıve DeepAR Deep Factors Random Forest Prophet Gaussian Process VAR ISAC MLP Ours-Llama3.2 Ours-GPT4o Ours-Gemini2.0 flash Mean Square Error 0.0345 0.0382 0.0164 0.0506 0.0217 0.0415 0.0199 0.0398 0.0155 0.0295 0.1661 0.2104 0.0602 0.1260 0.0071 0.0145 0.0351 0.1186 0.0081 0.0297 0.0234 0.0596 0.0169 0.0407 No Model Selection Meta learner LLM based Table 2: Results for one-step ahead forecasting (MSE; the lower () the better). The selected model by LLMs yields second best performance compared to baseline meta-learners and SOTA methods. 1. Random Model. model configuration is randomly selected from the model space for each time-series dataset. 2. Popular Model. The most widely used forecasting model, Prophet (Taylor & Letham, 2017), is selected given its strong community support (e.g., over 19k stars on GitHub). 3. SOTA Model. We consider seven state-of-the-art forecasting models. For each model, we create multiple configurations by adjusting hyperparameters and data representations, resulting in 10 to 72 variants per model, as detailed in Table 5. The variant that achieves the best average performance across all training datasets is selected. Meta-learners. These approaches leverage performance matrix to guide model selection: 4. ISAC (Kadioglu et al., 2010): This clustering-based method groups training datasets based on their extracted meta-features. For new dataset, ISAC identifies the nearest cluster and selects the best-performing model within that cluster. 5. MLP. Given the training datasets and selected time window, the MLP regressor directly maps the meta-features onto model performances by regression (Abdallah et al., 2022). 4.3 Overall Results Superiority of LLM-based Methods in hit@k and MSE. The results presented in Tables 1 and 2 highlight the effectiveness of our LLM-based approach compared to all baseline methods. Ours-LLaMA3.2 consistently outperforms other selection strategies across both hit@k accuracy and mean squared error (MSE). For instance, Ours-LLaMA3.2 achieves 100.27%, 92.83%, 77.32%, and 61.20% higher hit@10 accuracy compared to Random Selection, Popular Selection, ISAC, and MLP, respectively. In terms of forecasting performance, the model selected by Ours-LLaMA3.2 achieves the second lowest MSE among all tested methods, outperforming traditional SOTA forecasting models while achieving performance comparable to the best meta-learning method. Notably, unlike meta-learning approaches that require an extensive precomputed performance matrix for training, our LLM-based method selects models instantly without training. Runtime Analysis. The inference runtime statistics of our methods are presented in Table 6, where our best Llama-based method achieves an inference time of 6.7 seconds for most timeseries datasets. Additionally, as illustrated in Figure 2b, LLM-based methods demonstrate substantial reduction in inference time compared to the naıve approach, which involves evaluating all possible models and selecting the best-performing one. Specifically, our Llama, GPT, and Gemini-based methods achieve median inference time reductions of 14, 18, and 89, respectively, over the naıve approach. 7 Preprint. Under review. (a) Average training and inference time (in seconds). Detailed mean and standard deviation values are provided in Table 6. (b) The inference time reduction of LLM-based methods over the naıve approach. Our Llama, GPT, and Gemini-based methods give median reduction of 14X,18X, and 89X over naıve approach on all the datasets. Figure 2: Comparison of training and inference time across different methods. Methods Input Tokens Output Tokens Ours-Llama3.2 w. data w. data+CoT w. data+meta features w. data+meta features+CoT 661.35 1.16 739.35 1.16 20547.43 125.70 20632.43 125.70 157.14 508.51 519.00 1445.28 116.75 274.24 350.21 931.21 Ours-GPT4o w. data w. data+CoT w. data+meta features 2418.98 1472.00 2711.98 1472.00 22475.06 1490.00 w. data+meta features+CoT 22750.06 1490.00 Ours-Gemini2.0 flash w. data w. data+CoT w. data+meta features 3075.64 2192.18 3075.64 2192.18 26761.32 2267.30 w. data+meta features+CoT 27080.32 2267.30 68.89 7.32 297.18 46.68 67.54 8.37 300.68 46.50 84.33 7.28 340.53 60.29 80.99 4.06 352.31 80. Table 3: Input and output token count for each time series dataset. 4.4 Ablation Studies and Additional Analyses Meta-Features. As shown in the Table [ 1, 6, 3], incorporating meta-features in the prompt improves the performance of Llama and GPT-based methods, while the Gemini-based method appears to be less impacted. This improvement likely stems from the additional information provided by meta-features, which aids in selecting more suitable models. Besides, this performance gain comes at the cost of increased computational overhead inference time rises by at least 25%, and prompt token usage expands by at least 7X. Chain-of-Thought Prompting. Based on the Table [ 1, 6, 3], explicitly incorporating CoT reasoning in the promptguiding the LLM to select the forecasting algorithm, hyperparameters, and data representation step by stepdoes not necessarily enhance model selection performance and sometimes even degrades it while significantly increasing computational costs, leading to at least 2X increase in inference time and 4X rise in output token usage. We suspect that CoT prompting introduces unnecessary complexity, causing the LLM to overanalyze irrelevant aspects of the selection process. Unlike tasks where reasoning clarifies logic, model selection may benefit more from direct pattern recognition. The added reasoning steps could also increase the risk of hallucination, leading to suboptimal choices. 8 Preprint. Under review. Data Representation. Instead of allowing the LLM to select the data representation, we fixed it to either exponential smoothing or raw data. The results in Table 4 indicate that exponential smoothing enhances model selection performance for Llama and GPT-based methods, whereas it negatively impacts Geminis selection performance. Limitations of Different LLMs. Llama3.2 achieves the best model selection performance among the three tested LLMs; however, it produces the most incomplete or irregular outputs as shown In contrast, Gemini2.0 flash conin Figure 3. sistently generates complete and valid outputs while also having the lowest inference time and token usage. However, its performance is it the weakest, under certain prompt settings, even underperforms random selection. GPT4o serves as balanced choice, delivering strong performance that surpasses almost all baselines, with only few invalid outputs where the selected model falls outside the predefined model space. Figure 3: Average Number of Invalid Outputs for LLMs. hit@k accuracy Methods Data Representation LLM Selection Exponential Smoothing Raw hit@1 hit@5 hit@10 hit@50 Ours-Llama3.2 Ours-GPT4o Ours-Gemini2.0 flash Ours-Llama3.2 Ours-GPT4o Ours-Gemini2.0 flash Ours-Llama3.2 Ours-GPT4o Ours-Gemini2.0 flash Ours-Llama3.2 Ours-GPT4o Ours-Gemini2.0 flash 1.14 0.52 0.21 4.47 2.60 0.62 7.27 4.88 3.53 29.60 21.91 20. 1.04 0.62 0 4.98 2.60 0.31 8.41 6.33 1.87 31.15 23.36 19.94 0.31 0.31 0.42 3.32 1.66 2. 4.47 3.01 2.70 14.23 11.84 8.10 Table 4: hit@k accuracy of LLM-based model selection, where the data representation is either chosen by the LLM or defaults to Exponential Smoothing or Raw."
        },
        {
            "title": "5 Conclusion, Limitations and Future Work",
            "content": "In this work, we applied LLMs to the time-series forecasting model selection problem for the first time. Through extensive experiments, we demonstrated that LLMs can effectively address this task without relying on precomputed performance matrix of historical modeldataset pair evaluations. Additionally, this method significantly reduces computational overhead, achieving up to 89X faster inference compared to exhaustive model evaluation. Despite the performance, the underlying mechanisms remain unclear. Furthermore, our current approach has been evaluated solely on univariate datasets. In future work, we aim to expand our testbed to incorporate more diverse set of datasets and models, further exploring the generalizability of LLM-based model selection."
        },
        {
            "title": "Ethics Statement",
            "content": "Our research adheres to COLM Code of Ethics, ensuring that LLM-based model selection for time series forecasting is developed and applied responsibly. We prioritize fairness, 9 Preprint. Under review. transparency, and data privacy, avoiding biases that could impact decision-making across different forecasting applications. By leveraging LLMs for model selection without requiring an extensive historical performance matrix, our approach reduces potential biases introduced by past model rankings. Continuous ethical assessments guide our research to align with societal and regulatory standards. 10 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Mustafa Abdallah, Ryan Rossi, Kanak Mahadik, Sungchul Kim, Handong Zhao, and Saurabh Bagchi. Autoforecast: Automatic time-series forecasting model selection. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, CIKM 22, pp. 514, New York, NY, USA, 2022. Association for ComputISBN 9781450392365. doi: 10.1145/3511808.3557241. URL https: ing Machinery. //doi.org/10.1145/3511808.3557241. Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner A¼rkmen, and Yuyang Wang. Gluonts: Probabilistic and neural time series modeling in python. Journal of Machine Learning Research, 21 (116):16, 2020. URL http://jmlr.org/papers/v21/19-820.html. C. Bui, N. Pham, Anh Vo, A. Tran, A. Nguyen, and Trung Le. Time series forecasting for healthcare diagnosis and prognostics with the focus on cardiovascular diseases. In 6th International Conference on the Development of Biomedical Engineering in Vietnam (BME6), pp. 809818, 06 2018. ISBN 978-981-10-4360-4. doi: 10.1007/978-981-10-4361-1 138. Defu Cao, Furong Jia, Sercan Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-based generative pre-trained transformer for time series forecasting, 2024. URL https://arxiv.org/abs/2310.04948. Vitor Cerqueira, Luis Torgo, and Carlos Soares. Model selection for time series forecasting: Empirical analysis of different estimators, 2022. URL https://arxiv.org/abs/2104. 00584. Jui-Sheng Chou and Duc-Son Tran. Forecasting energy consumption time series using machine learning techniques based on usage patterns of residential householders. Energy, 165:709726, 2018. ISSN 0360-5442. doi: https://doi.org/10.1016/j.energy.2018.09.144. URL https://www.sciencedirect.com/science/article/pii/S0360544218319145. Robert Fildes, Shaohui Ma, and Stephan Kolassa. Retail forecasting: Research and practice. International Journal of Forecasting, 38(4):12831318, 2022. ISSN 0169-2070. doi: https:// doi.org/10.1016/j.ijforecast.2019.06.004. URL https://www.sciencedirect.com/science/ article/pii/S016920701930192X. Special Issue: M5 competition. Google. Gemini 2.0 flash, 2024. generative-ai/docs/gemini-v2#2.0-flash. URL https://cloud.google.com/vertex-ai/ Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters, 2024. URL https://arxiv.org/abs/2310.07820. Tao Han, Song Guo, Zhenghao Chen, Wanghan Xu, and Lei Bai. How far are todays time-series models from real-world weather forecasting applications?, 2024. URL https: //arxiv.org/abs/2406.14399. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models, 2024. URL https://arxiv.org/ abs/2310.01728. Serdar Kadioglu, Yuri Malitsky, Meinolf Sellmann, and Kevin Tierney. Isac instance-specific algorithm configuration. In Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence, pp. 751756, NLD, 2010. IOS Press. ISBN 9781607506058. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205. 11916. Shanu Kumar, Saish Mendke, Karody Lubna Abdul Rahman, Santosh Kurasa, Parag Agrawal, and Sandipan Dandapat. Enhancing zero-shot chain of thought prompting via uncertainty-guided strategy selection, 2024. URL https://arxiv.org/abs/2412.00353. Preprint. Under review. Christiane Lemke and Bogdan Gabrys. Meta-learning for time series forecasting and forecast combination. Neurocomputing, 73(10):20062016, 2010. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2009.09.020. URL https://www.sciencedirect.com/ science/article/pii/S0925231210001074. Subspace Learning / Selected papers from the European Symposium on Time Series Prediction. Richard Lewis and Gregory Reinsel. Prediction of multivariate time series by autoregressive model fitting. Journal of Multivariate Analysis, 16(3):393411, 1985. ISSN 0047-259X. doi: https://doi.org/10.1016/0047-259X(85)90027-2. URL https://www.sciencedirect. com/science/article/pii/0047259X85900272. Yinheng Li. practical survey on zero-shot prompt design for in-context learning. In Ruslan Mitkov and Galia Angelova (eds.), Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, pp. 641647, Varna, Bulgaria, September 2023. INCOMA Ltd., Shoumen, Bulgaria. URL https://aclanthology.org/2023.ranlp-1.69/. Andy Liaw and Matthew Wiener. Classification and regression by randomforest. Forest, 23, 11 2001. MetaAI. Llama 3.2 model card, 2024. model-cards-and-prompt-formats/llama3 2/. URL https://www.llama.com/docs/ Pablo Montero-Manso, George Athanasopoulos, Rob J. Hyndman, and Thiyanga S. Talagala. Fforma: Feature-based forecast model averaging. International Journal of Forecasting, 36(1): 8692, 2020. ISSN 0169-2070. doi: https://doi.org/10.1016/j.ijforecast.2019.02.011. URL https://www.sciencedirect.com/science/article/pii/S0169207019300895. M4 Competition. OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Namyong Park, Ryan Rossi, Nesreen Ahmed, and Christos Faloutsos. Metagl: Evaluationfree selection of graph learning models via meta-learning, 2023. URL https://arxiv.org/ abs/2206.09280. Ricardo B.C. Prudˆencio and Teresa B. Ludermir. Meta-learning approaches to selecting time series models. Neurocomputing, 61:121137, 2004. ISSN 0925-2312. doi: https:// doi.org/10.1016/j.neucom.2004.03.008. URL https://www.sciencedirect.com/science/ article/pii/S0925231204002310. Hybrid Neurocomputing: Selected Papers from the 2nd International Conference on Hybrid Intelligent Systems. David Salinas, Valentin Flunkert, and Jan Gasthaus. Deepar: Probabilistic forecasting with autoregressive recurrent networks, 2019. URL https://arxiv.org/abs/1704.04110. Omer Berat Sezer, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. Financial time series forecasting with deep learning : systematic literature review: 20052019. Applied Soft Computing, 90:106181, 2020. ISSN 1568-4946. doi: https://doi.org/10. 1016/j.asoc.2020.106181. URL https://www.sciencedirect.com/science/article/pii/ S1568494620301216. Skipper Seabold and Josef Perktold. Statsmodels: Econometric and Statistical Modeling with Python. In Stefan van der Walt and Jarrod Millman (eds.), Proceedings of the 9th Python in Science Conference, pp. 92 96, 2010. doi: 10.25080/Majora-92bf1922-011. Yongqian Sun, Daguo Cheng, Tiankai Yang, Yuhe Ji, Shenglin Zhang, Man Zhu, Xiao Xiong, Qiliang Fan, Minghan Liang, Dan Pei, et al. Efficient and robust kpi outlier detection for large-scale datacenters. IEEE Transactions on Computers, 72(10):28582871, 2023. Sean Taylor and Benjamin Letham. Forecasting at scale, 09 2017. Joaquin Vanschoren. Meta-learning: survey, 2018. URL https://arxiv.org/abs/1810. 03548. 12 Preprint. Under review. Chunyang Wang, Yanmin Zhu, Haobing Liu, Tianzi Zang, Jiadi Yu, and Feilong Tang. Deep meta-learning in recommendation systems: survey, 2022. URL https://arxiv.org/ abs/2206.04415. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Weizhong Yan, Hai Qiu, and Ya Xue. Gaussian process for long-term time-series forecasting. In 2009 International Joint Conference on Neural Networks, pp. 34203427, 2009. doi: 10.1109/ IJCNN.2009.5178729. Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. Large language models for time series: survey, 2024. URL https://arxiv.org/abs/2402.01801. Yue Zhao, Ryan A. Rossi, and Leman Akoglu. Automating outlier detection via metalearning, 2021. URL https://arxiv.org/abs/2009.10606. Preprint. Under review."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prompt Structure The prompt structure we used is illustrated as follows. Prompt and Response Structure Prompt: [Role and Objective]... [Model Space]... [CoT Reasoning](optional)... [Input]...Dataset Values...Meta Features(optional)... [Output Format]... [Rules]... Response: {reasoning: ..., result: { forecasting algorithm: ..., hyperparameters: [ {name: ..., value: ...}, {name: ..., value: ...} ], data representation: ... } } We formulate the model selection problem following the framework of Abdallah et al. (2022). A.2 Datasets and Meta-Features Our approach relies on collection of historical time-series forecasting datasets, denoted as = {D1, D2, . . . , DN}, where is the total number of datasets. Each dataset Di comprises sequence of observations in Rni , with ni representing the number of observations in Di. For each dataset Di D, we randomly sample windows. Each time window wt is contiguous segment of observations from Di with length wt that is smaller than the total length of Di. For example, w10 = 16 indicates that the 10th window contains 16 consecutive observations. Meta-Features Tensor. To analyze the impact of meta-features on model selection in our approach, we utilize extracted meta-features for each time-series dataset from Abdallah et al. (2022)s work. Definition 1. Given time-series dataset Di, we define the meta-features tensor Fi = Rd captures the set of meta- {Fi features corresponding to the time window wk of the dataset Di, given by T} RTd, where the meta-features matrix Fi 1, . . . , Fi Fi {ψ(wk(Di)) ψ : Rwk Rd}, (2) where ψ() : Rwk Rd represents the feature extraction module and denotes the number of the meta-features. The extracted meta-features capture the key characteristics of each dataset and are grouped into five categories, as proposed by(Vanschoren, 2018): Simple: General task properties. Statistical: Properties of the underlying dataset distributions. Information-theoretic: Entropy measures. Spectral: Frequency domain properties. Landmarker: Forecasting models attributes on the task. A.3 Performance Matrix Now we introduce the performance matrix: 14 Preprint. Under review. Forecasting Algorithm HyperParameter(s) DeepAR (Salinas et al., 2019) DeepFactor (Salinas et al., 2019) num cells = [10,20,30,40,50] num rnn layers = [1,2,3,4,5] num hidden global = [10,20,30,40,50] num global factors = [1,5,10,15,20] Prophet (Taylor & Letham, 2017) changepoint prior scale = [0.001, 0.01, 0.1, 0.2, 0.5] seasonality prior scale = [0.01, 0.1, 1.0, 5.0, 10.0] Data Representation {Exp smoothing, Raw} {Exp smoothing, Raw} {Exp smoothing, Raw} Seasonal Naive (Montero-Manso et al., 2020) season length = [1,5,7,10,30] {Exp smoothing, Raw} Gaussian Process (Yan et al., 2009) Vector Auto Regression (Lewis & Reinsel, 1985) cardinality = [2,4,6,8,10] max iter jitter = [5,10,15,20,25] cov type={HC0,HC1,HC2,HC3,nonrobust} trend = {n, c, t, ct} Random Forest Regressor (Liaw & Wiener, 2001) estimators = [10,50,100,250,500,1000] max depth = [2,5,10,25,50,None] {Exp smoothing, Raw} {Exp smoothing, Raw} {Exp smoothing, Raw} Total 50 50 10 50 40 72 Table 5: Time-Series Forecasting Model Space. See hyperparameter definitions for various algorithms from GluonTS(Alexandrov et al., 2020) and statsmodels(Skipper Seabold & Josef Perktold, 2010). The number of models (last column) is all possible combinations of hyperparameters and data representations. Definition 2. Given training database and model space M, we define the performance matrix RTnm as = {P1, P2, . . . , PT}, ) Rnm and the element pi,j where Pk = (pi,j performance on the time window wk of the ith training dataset Di. We denote (cid:104) = Mj(wk(Di)) denotes the jth model Mjs (cid:105) pi = pi,1 . . . pi,m as the performance vector of all models in on time window wk of Di. We denote the performance of model on time window using forecasting error metrics such as Mean Squared Error (e.g., MSE) of that model on that window. 15 Preprint. Under review. Methods Training Time Inference Time Naıve ISAC MLP N/A 278.8083 57.9900 705.2908 123.3715 70.9500 1.7801 10.2480 2.7182 1.2745 0.5198 Ours-Llama3.2 w. data w. data+CoT w. data+meta features w. data+meta features+CoT Ours-GPT4o w. data w. data+CoT w. data+meta features w. data+meta features+CoT Ours-Gemini2.0 flash w. data w. data+CoT w. data+meta features w. data+meta features+CoT N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 4.7201 16.9481 11.6174 32.7716 6.7067 17.8676 17.1308 50. 1.4905 0.7234 4.7821 2.0229 2.4630 0.7546 5.2368 1.4140 0.7780 0.0543 2.1587 0.3407 0.9790 0.0597 2.4974 0.4361 Table 6: Average and standard deviation inference and training runtime performance (in seconds) over all datasets."
        }
    ],
    "affiliations": [
        "Adobe Research San Jose, CA, USA",
        "Adobe Research Seattle, WA, USA",
        "Department of Computer Science University of South California Los Angeles, CA, USA",
        "Department of Computer Science Virginia Tech Blacksburg, VA, USA",
        "Dolby Labs Atlanta, GA, USA"
    ]
}