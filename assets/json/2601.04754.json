{
    "paper_title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "authors": [
        "Yen-Jen Chiou",
        "Wei-Tse Cheng",
        "Yuan-Fu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA."
        },
        {
            "title": "Start",
            "content": "PROFUSE: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting Yen-Jen Chiou Wei-Tse Cheng Yuan-Fu Yang National Yang Ming Chiao Tung University remi.ii13@nycu.edu.tw, andy5552555.ii13@nycu.edu.tw, yfyangd@nycu.edu.tw 6 2 0 2 8 ] . [ 1 4 5 7 4 0 . 1 0 6 2 : r Figure 1. Overview of ProFuse. Left: dense matcher supplies cross-view geometric and semantic correspondences. Top: Warped masks are grouped into 3D Context Proposals with shared global feature. Bottom: Triangulated matches initialize compact Gaussian scene, and proposal features are fused without render supervision for coherent open-vocabulary 3D semantics."
        },
        {
            "title": "Abstract",
            "content": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances crossview consistency and intra-mask cohesion within direct registration setup, adding minimal overhead and requiring Instead of relying on no render-supervised fine-tuning. pretrained 3DGS scene, we introduce dense correspondenceguided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries global feature obtained through weighted aggregation of member embeddings, and this feature is fused 1 onto Gaussians during direct registration to maintain perprimitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is 2 faster than SOTA. The code is available at our GitHub page https://github.com/chiou1203/ProFuse. 1. Introduction Open-vocabulary 3D scene understanding aims to understand physical scene using free-form natural language queries, with applications ranging from robotics and autonomous navigation to augmented reality [5, 11, 30, 40, 41, 43]. The task remains challenging, as the system must recover accurate geometry while also assigning meaningful semantic concepts without being restricted to fixed labels. Earlier efforts explored range of 3D representations [10, 12, 15, 23, 25, 33, 40]. Recent work has focused on 3D Gaussian Splatting [14], which represents scene as set of anisotropic Gaussians and enables photo-realistic, real-time rendering. Early work adopts 2D visionlanguage distillation in which images are rendered during training and Gaussian features are optimized to match 2D predictions [9, 27, 34, 42, 44]. This pipeline can propagate open-vocabulary knowledge into 3D, but it also introduces two structural issues. The supervision signal is delivered only after rendering and compositing, leading to mismatches with the original language embedding that described the region. In addition, semantics are acquired and queried through individual views, making reasoning less direct and less stable. These limitations have motivated methods that operate directly in 3D Gaussian space [13, 20, 28, 39]. These approaches assign language features to each Gaussian and answer text query by comparing the query embedding with those perGaussian features in 3D. More recent work has moved toward registrationbased formulation [13]. This approach bypasses rendersupervised semantic training. Language-aligned features are directly registered in Gaussians using their visibility along each viewing ray. The result is compact, queryable 3D semantic field with high efficiency. Despite such progress, the direct registration paradigm is still in its early stages. Our aim is to strengthen the registration framework by injecting semantic consistency into the 3DGS representation without any additional render-supervised training. We propose registration-based framework ProFuse that strengthens semantic coherence in 3D Gaussian Splatting. Our key insight is to enforce two key factors highlighted by previous work [32, 35, 39, 42], namely cross-view consistency and intra-mask cohesion. Prior approaches typically encourage these properties through render-supervised training on 2D feature maps or through explicit feature-learning objectives. The registration pipeline does not impose these constraints. Our approach injects these forms of semantic consistency directly into the registration framework. An overview of the proposed pipeline is shown in Figure 1. We introduce pre-registration stage guided by dense multi-view correspondence [8]. The correspondence signal initializes the 3D Gaussian scene with accurate geometry [17], which allows the representation to cover the scene without relying on iterative densification. The same signal is also used to connect observations of the same object across different viewpoints, consolidating them into consistent, object-level groups that we refer to as 3D Context Proposals. Each 3D Context Proposal encodes an object as it appears across views, rather than as an isolated perframe mask, and provides stable source of semantics that is aligned across viewpoints. During feature registration, each proposal carries global language feature computed from its mask members. We then assign each Gaussian to its corresponding context proposals and associate the global semantics to the Gaussian. Notably, our method does not involve gradient-based fine-tuning or backpropagation of language loss. Through experiments across open-vocabulary 3D perception tasks, we demonstrate effectiveness in 3D object selection, openvocabulary point cloud understanding, and optimizing efficiency. Our contributions are summarized as follows: registration-based semantic augmentation of 3D Gaussian Splatting that introduces cross-view semantic consistency and intra-mask coherence without any rendersupervised training for semantics. pre-registration stage driven by dense multi-view correspondence. The same correspondence signal initializes well-covered 3D Gaussian scene and assembles consistent mask evidence across views into 3D Context Proposals. unified open-vocabulary 3D scene representation that improves object selection, point cloud understanding, and training efficiency on existing benchmarks while maintaining render-free semantic association efficiently. Overall, ProFuse offers compact and training-free route to consistent open-vocabulary 3D scene understanding built directly on correspondence-driven registration. 2. Related Work Neural rendering has progressed from NeRFs to explicit point-based primitives [1, 21, 22]. 3DGS provides fast, spatially local rendering and is now common backbone for open-vocabulary understanding [14, 36]. Rendersupervised distillation methods transfer 2D vision-language signals into 3D by supervising rendered feature maps [9, 12, 15, 25, 27, 29, 34, 35, 37, 44]. Direct 3D retrieval attaches language-aligned descriptors to Gaussians or points for volumetric querying [13, 20, 28, 39]. To stabilize semantics across views, recent works encourage cross-view consistency and semantic cohesion [3, 4, 12, 18, 20, 25, 26, 35, 37, 39, 42]. Finally, dense correspondence provides wide-baseline matches and confidences useful for multiview grouping and correspondence-driven 3DGS initialization [2, 7, 8, 17, 19, 31, 38]. We build on this direction to couple correspondence-guided context association with registration-based semantic field. 2 Figure 2. Pre-registration. For each reference view we select neighbors via view clustering, then apply pre-trained dense matcher to obtain per-pixel warps Wji and confidences αji. Bottom right: Given the warps of pixel pair, we triangulate 3D seed point for Gaussian initialization. Top right: Warped IoU comparison on every referenceneighbor mask pair; masks that pass the selection form edges of bipartite graph. 3. Method We construct semantic 3D Gaussian scene that can be queried with natural language without any rendersupervised semantic training. The pipeline begins with pre-registration stage via dense correspondence. This stage initializes dense Gaussian scene and links segmentation masks across views to form 3D Context Proposals. Each proposal records which masks across views are inferred to refer to the same scene content, giving us cross-view groupings before any semantic fusion. context-guided registration stage then uses these proposals to compute global language feature for each proposal. The features are then assigned to the corresponding Gaussians using visibilitybased weights derived from transmittance and opacity along camera rays. The final output is 3D representation with cross-view consistency and intra-mask cohesion that can be searched directly in 3D by text query. 3.1. Dense Correspondence Pre-registration The pre-registration process begins from set of posed RGB images of scene. Let {Ii}N i=1 denote input views, and let each image Ii have known camera intrinsics and extrinsics. The goal of this stage is to initialize dense set of 3D Gaussians with accurate geometry and initial appearance attributes, and to record cross-view evidence for semantic grouping. As an overview, the full pre-registration workflow is visualized in Figure 2. } using SAM [16], where For each image Ii, we obtain set of non-overlapping region masks {M {0, 1}HW is binary mask for the region in view i. For every mask , we extract language-aligned feature veci RD by cropping the corresponding region in Ii tor and encoding it with CLIP [29]. The result is per-view dictionary Si = {(M ) = 1, . . . , Ki}, where Ki is the number of predicted regions in view i. The sets Si will later serve as semantic evidence. , Dense Feature Matching. To relate content across views, we compute dense correspondences between pairs of images using pretrained dense matching network (see Figure 2) . The network was trained on coarse layer using DINOv2 [24] and fine layer with pyramid convolution. The result is robust dense feature matching. Given two images Ii and Ij, the dense matcher returns C(Ii, Ij) Wji, αji, where Wji R2HW is dense warp field that maps each pixel coordinate (u, v) in Ij to subpixel coordinate in Ii, and αji RHW is confidence map. Intuitively, Wji(u, v) predicts where the content seen at (u, v) in view should appear in view i. The value αji(u, v) measures how reliable that match is. We discard correspondences whose confidence falls below threshold. The result is dense set of pixel-to-pixel matches across views that remains stable under wide viewpoint change. Gaussian Initialization. We use the high-confidence correspondences to seed 3D Gaussian primitives directly in space. For confident match between the pixel (uj, vj) in view and its mapped location (ui, vi) in view i, we backproject both pixels into 3D using known camera poses and triangulate their intersection. The resulting 3D point becomes the initial center of Gaussian (see Figure 2, bottom right). Its initial appearance attributes are taken from the supporting image evidence, and its initial scale and orientation are set to cover small spatial neighborhood around that 3D point. Repeating this over correspondences yields the initial Gaussian set G0 = {gn}, where each gn is Gaussian primitive with position, scale, orientation, opacity, and color. Because these Gaussians are instantiated from dense correspondences rather than grown through iterative densification, G0 already provides broad and near-uniform spatial coverage of the scene. Subsequent geometric refinement adjusts these primitives but does not need to create large number of new Gaussians. 3 6: 7: 8: 9: 10: 11: 12: 13: Algorithm 1 Cross-view mask clustering 1: Inputs: per-view sets {Si} with Si = {(M i )}; dense warp field Wji and certainties αji; visibility mask; thresholds τα, τiou, τbox; size gates smin, vmin. 2: Initialize graph = (V, E) with {(i, k) }, , 3: for all ordered view pairs (i, j) do 4: 5: Γji [αji τα] vis mask for all mask pairs (M ) do , ; Wji) ; Wij) ji Γji) Γji, (cid:102)M (cid:102)M ji W(M Oi,a; j,b IoU(M ij W(M (cid:102)M Oj,b; i,a IoU(M Bi,a; j,b BBoxIoU(M Bj,b; i,a BBoxIoU(M if Oi,a; j,b τiou and Oj,b; i,a τiou and Γij, (cid:102)M ji) ij) , (cid:102)M , (cid:102)M ij Γij) Bi,a; j,b τbox and Bj,b; i,a τbox then Add undirected edge between (i, a) and (j, b) to end if end for 14: 15: 16: end for 17: Extract connected components {Cm} of 18: Filter Cm by Cm smin and views(Cm) vmin 19: {Pm Cm} 20: return from view j. We project Cross-view Context Association. The same correspondence field lets us record which masks from different views refer to the same scene content. Consider two masks from view and into view using the warp field Wji, producing warped support mask in the coordinates of Ii. We then measure how well this warped support overlaps , restricted to pixels with high correspondence confidence αji. If the overlap exceeds threshold, we register link that these two masks are consistent observations of the same underlying scene content. Repeating this procedure over view pairs accumui , lates the link set = {(M ji)}, where each pair in indicates strong cross-view agreement between two masks (see Figure 2, top right). The pre-registration stage produces two artifacts. The first is an initialized Gaussian scene G0 created by triangulating dense correspondences. The second is pool of mask links across views that captures which regions per-view act as the same scene content between viewpoints. Section 3.2 addresses how we cluster masks in into 3D Context Proposals. 3.2. 3D Context Proposals 3D Context Proposals are formed through grouping perview masks that mutually support one another under dense correspondence into stable multi-view units. We realize this by testing pairwise agreements under correspondence warps and linking masks that pass mutual gates; connected components in the resulting graph define the proposals. Cross-view Mask Clustering. Algorithm 1 demonstrates the clustering procedure. Let mask node be = (i, k) with {0, 1}HW . Given candidate pair (i, a) and (j, b) with dense warp Wji from view to and certainty map αji, we gate matches using fixed certainty threshold τα [0, 1] together with renderer-derived visibility mask vis mask. The binary gate is defined as Γji = [ αji τα ] vis mask. (1) The warped support in view is obtained as (cid:102)M ji = W(cid:0)M ; Wji (cid:1), (2) where denotes bilinear sampling at sub-pixel accuracy. The confidence-gated overlap in view is Oi,a; j,b = IoU Γji, (cid:102)M ji Γji (cid:16) (cid:17) . (3) ), box( (cid:102)M We compute coarse bounding-box agreement Bi,a; j,b = IoU(cid:0) box(M ji)(cid:1) and gate links with two thresholds, τiou for mask overlap and τbox for box overlap. Agreement is required in both directions, and an undirected link is accepted only if Oi,a; j,b τiou Bi,a; j,b τbox and Oj,b; i,a τiou, and Bj,b; i,a τbox. (4) graph = (V, E) is then constructed with vertices = {(i, k)}. For every cross-view pair that passes the mutual gates above, we add an undirected edge to E. The connected components of define the raw proposals. Very small components are removed using two criteria: minimal member count smin and minimal distinct-view support vmin. Each proposal Pm is represented only by its membership list (i, k), contributing view set, and compact per-view label maps for efficient lookup. 3.3. Feature Registration The goal of the registration stage is to assign unitnormalized language descriptor to every Gaussian, enabling text queries to be evaluated directly in 3D. This stage operates on the initialized Gaussian set G0, calibrated cameras, the per-view mask dictionary Si = {(M )}, and the proposal set = {Pm} constructed in 3.2. , Figure 3. From context proposal to global feature. Left: masks of the same entity are grouped into 3D Context Proposal. Center: for (cid:1) is computed. Right: pixel p, the renderer returns the top-K Gaussians with contributions {ωi,p,t}K mass-weighted pool of member mask embeddings forms the proposal feature, which is registered to Gaussians via Eq. (8). t=1, from which the mask mass µ(cid:0)M For view and pixel p, the renderer returns the indices and weights of the top-K Gaussians along the ray, denoted {(gi,p,t, ωi,p,t)}K t=1. Their blending contributions are ωi,p,t = Ti,p,t αi,p,t, (cid:89) Ti,p,t = (cid:0)1 αi,p,s (cid:1), s<t (5) where αi,p,t is the effective opacity and Ti,p,t is the transmittance of the preceding Gaussians on the ray. Each proposal Pm contains member masks drawn from multiple views. We compute scalar mass for every mask by integrating renderer contributions over the mask pixels µ(M i ) = (cid:88) (cid:88) pΩ(M ) t=1 ωi,p,t. (6) The proposal descriptor is mass-weighted pool of mask embeddings followed by ℓ2 normalization, fm = (cid:80) (cid:80) (cid:13) (cid:13) (cid:13) (i,k)Pm (i,k)Pm µ(M µ(M ) (cid:13) (cid:13) ) (cid:13)2 . (7) An illustration of this aggregation is provided in Figure 3. pixel-wise proposal map Li(p) is constructed for every training view, assigning each pixel inside mask to the ID of its corresponding proposal in P. Pixels outside all masks receive null label and are ignored. For each Gaussian G0, feature accumulator A[g] RD and scalar weight sum S[g] R0 are initialized to zero. For every pixel with valid proposal = Li(p) and each of its top-K hits, the accumulation step is A[gi,p,t] A[gi,p,t] + ωi,p,t S[gi,p,t] S[gi,p,t] + ωi,p,t. fm, (8) This registration step consumes the proposal feature from Figure 3 and weights it by contributions ωi,p,t. After processing all views, the descriptor for Gaussian is computed as fg = A[g] max(S[g], ε) , ˆfg = fg fg2 , (9) with small ε for numerical stability. The implementation uses batched gatherscatter operations and relies only on renderer outputs. 3.4. Inference Procedure text query is encoded to fq RD and normalized as ˆfq = fq/fq2. Each Gaussian stores registered descriptor from 3.3. Following Dr. Splat [13], Product Quantization (PQ) is used for memory-efficient retrieval. Descriptors are stored as FAISS product-quantized codes and decoded to unit-normalized vectors at query time. sg = Cosine similarity is used to score Gaussians, ˆf ˆfg. FAISS PQ index over { ˆfg} produces shortlist that is re-scored using decoded (full-precision) descriptors. Selection is performed directly in 3D without any renderbased fine-tuning: Gaussian is considered active if sg τact. For visualization in view i, let {(gi,p,t, ωi,p,t)}K t=1 denote the Top-K contributors to pixel p. The activation mask is defined as Mi(p) = 1[Ai(p) γ], where Ai(p) is the sum of contributions over Top-K hits. (10) 4. Experiments 4.1. Implementation Experiments are conducted on the LERF-OVS [15] and ScanNet [6] datasets. All four LERF scenes are used, and 10 scenes are sampled from the ScanNet dataset. SAMbased segmentation and mask embedding are preprocessed on 8 NVIDIA H100 GPUs, while all remaining experiments run on single A100 GPU. 5 Table 1. Evaluation of 3D object selection on LERF-OVS [15] dataset. Scores are averaged per scene and then across scenes. Bold indicates the best performance. mIoU mAcc@0.25 Method waldo kitchen figurines ramen teatime mean waldo kitchen figurines ramen teatime mean LangSplat LEGaussians OpenGaussian Dr. Splat ProFuse (Ours) 9.18 11.78 24.57 29.37 36.91 10.16 17.99 53.01 51.73 56.13 7.92 15.79 24.44 26.32 28.16 11.38 19.27 55.40 55.53 62.78 9.66 16.21 39.36 40.74 46. 9.09 18.18 36.36 50.00 68.18 11.27 23.21 83.93 82.14 85.71 8.93 26.76 39.44 40.85 39.44 20.34 27.12 76.27 79.66 79.66 12.41 23.82 59.00 63.16 68.25 Figure 4. Qualitative comparison of object-level semantic queries on the LERF-OVS [15] dataset. Our method produces more accurate and cleaner object retrieval, showing sharper correspondence between the text query and the selected 3D content. 6 Figure 5. Feature visualizations on the ScanNet [6] dataset using registration-based methods. Colors represent normalized language features transferred to mesh vertices and rendered via fixed RGB projection. ProFuse produces cleaner regions with sharper boundaries and fewer speckles. 4.2. Open-Vocabulary 3D Object Selection We evaluate open-vocabulary 3D object selection on the four LERF scenes using the official text queries and splits. Each method outputs binary activation per frame, while our pipeline performs selection directly in 3D. Let RD be the CLIP text embedding, normalized as ˆq = q/q2. Each Gaussian stores normalized language feature ˆfg from registration. Active Gaussians are defined as Gτ = { ˆfg, ˆq τ }, with method-specific global threshold τ . For view and pixel p, the renderer provides the top-K Gaussians and weights ωi,p,t . The activation is Ai(p) = (cid:88) t=1 ωi,p,t 1[gi,p,t Gτ ] , and the mask is (cid:99)Mi = 1[ Ai γ ] using fixed silhouette threshold γ. small grid search is used to determine the global threshold τ for each method. mean IoU is computed by evaluating intersection-over-union for each queryframe pair and averaging across all queries and frames in scene. The final score is obtained by averaging across the four scenes. Table 1 reports these quantitative results. The metric mAcc@0.25 is also provided, defined as the fraction of queryframe pairs with IoU at least 0.25, using the same τ . Table 2. Open-vocabulary point cloud understanding on ScanNet. Results use mIoU and mAcc for 19/15/10-class settings. Method 19 classes 15 classes mIoU mAcc mIoU mAcc mIoU mAcc 10 classes LangSplat LEGaussians OpenGaussian Dr. Splat ProFuse (Ours) 3.78 3.84 24.73 28.40 30.52 9.11 10.87 41.54 52.77 55.32 5.35 9.01 30.13 32.67 34. 13.20 22.22 48.25 58.53 60.90 8.40 12.82 38.29 36.81 39.74 22.06 28.62 55.19 66.41 69.38 Qualitative results are presented in Figure 4. Our method isolates the queried object with far fewer background activations, yielding cleaner and more semantically precise selections. In contrast, Dr. Splat often exhibit ray-like spillovers into nearby clutter or textured areas. For instance, the Toaster query incorrectly highlights the entire kettle on the left, while the Glass of Water query becomes distracted by specular reflections. 4.3. Open-Vocabulary Point Cloud Understanding The evaluation is conducted on the ScanNet dataset using the label spaces defined in OpenGaussian [39], considering class sets of 19, 15, and 10 categories. Each mesh vertex in the aligned reconstruction is assigned semantic label, and 7 Table 3. Comparison of training requirements and retrieval speed across 3D scene understanding methods. Table 4. Wall-clock comparison of geometry, semantic processing, and indexing time on the LERF dataset. Method Scene Render supervision Feature distill. Query Method Geometry Semantics Total Indexing LERF LangSplat LEGaussians OpenGaussian GOI Dr. Splat ProFuse (Ours) Corr-init 3DGS NeRF SfM3DGS SfM3DGS SfM3DGS SfM3DGS SfM3DGS required required required required required none none 24 4 4 1 12 min 10 min 5 min slow slow slow fast fast fast fast class names are encoded once into language embeddings and reused across all methods. Per-Gaussian language codes are first decoded using FAISS PQ to obtain cosine logits against class embeddings. These logits are transferred to mesh vertices through spatially aware kernel that respects each Gaussians full ellipsoid. Candidate Gaussians are shortlisted by Euclidean proximity (K=64), filtered by an elliptical Mahalanobis gate (σ=3), and weighted by both exp( 1 2 d2) and Gaussian opacity. softmax over class logits yields per-candidate class probabilities, and vertex scores are computed as the weighted sum of all candidates. Because predictions occur directly in 3D, no rendering is involved during evaluation. The same kernel and shortlist configuration is applied to every method so that performance differences reflect the quality of the learned Gaussian features rather than variations in the transfer rule. Ten scenes from ScanNet are sampled for evaluation, and scores are computed with fixed hyperparameters to report average mIoU and mAcc for each class set. Quantitative results for the 19-, 15-, and 10-class settings are provided in Table 2. To contextualize point-level scores, we visualize feature colorings of ScanNet reconstructions and compare them to the pioneer registration-based baseline Dr. Splat [13] in Figure 5. For each scene, we show the reference mesh view and two pseudo-colored point clouds. Colors are obtained by projecting normalized per-Gaussian features to three channels and painting the transferred per-vertex features; views are matched to the reference for consistent framing. Dr.Splat tends to produce darker, patchy fragments and color bleeding near corners, whereas our results exhibit higher region consistency with large surfaces rendered in coherent color swaths. We achieve cleaner boundaries at furniture edges and fixtures with fewer mixed colors at objectwall contacts. 4.4. Training Efficiency The cost of attaching open-vocabulary semantics to reconstructed scene is measured in wall-clock time. As shown in Table 3, render-supervised distillation methods require hours of processing, and existing registration-based approaches [13] still take several minutes. ProFuse achieves the fastest runtime through correspondence-guided initial1 OpenGaussian 0 + 10 30 Dr. Splat ProFuse (Ours) 2 + 15 2m + 20 19 20 20 40 Codebook PQ PQ Table 5. Top-K analysis on ScanNet showing mIoU and feature registration time for registration-based methods. Method Top K=10 Top K=20 Top K= mIoU time mIoU time mIoU time Dr. Splat ProFuse (Ours) 33.82 45 39.74 25 35.57 85 39.74 25 36.81 165 25 39.74 ization, which produces compact Gaussian set without densification, and through lightweight proposal-level feature fusion. These components reduce semantic attachment to about five minutes per scene, making ProFuse 2 faster than the prior SOTA. Table 4 provides runtime breakdown of direct 3D methods. ProFuse reduces scene-specific semantic association to only few minutes because proposal construction is lightweight and registration uses simple contribution accumulation without gradient updates. The compact geometry from correspondence-guided initialization removes densification and further shortens processing time. 4.5. Ablation Study To isolate the effect of correspondence-guided geometry and context proposals, we study the impact of the Top-K Gaussian candidates used during feature registration. Table 5 reports mIoU and registration time on ScanNet under three settings K=10, 20, 40. Without context proposals, registration-based baselines typically require K=40 to achieve saturation, indicating weak concentration of semantic mass along the viewing ray. In contrast, ProFuse reaches its maximum accuracy with K=10. The global proposal features place most of the mass on the leading few Gaussians, while our correspondence-initialized geometry further reduces long-tail ambiguity. As consequence, larger offers no additional benefit, and compact K=10 is sufficient for both accuracy and speed. 5. Conclusion ProFuse enforces cross-view semantic consistency in 3DGS without requiring any render-supervised learning for semantics. Dense correspondences generate 3D Context Proposals, and visibility-weighted fusion yields coherent semantic field. Experiments on LERF and ScanNet confirm accurate open-vocabulary selection and point-level understanding, showing that correspondence-guided geometry provides an efficient path to semantic association in 3DGS."
        },
        {
            "title": "References",
            "content": "[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021. 2 [2] Naijian Cao, Renjie He, Yuchao Dai, and Mingyi He. Loflat: Local feature matching using focused linear attention transformer. arXiv preprint arXiv:2410.22710, 2024. 2 [3] Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, and Qi Tian. Segment any 3d gaussians. In AAAI, 2025. 2 [4] Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, and Douglas Lee. Lifting by gaussians: simple, fast and flexible method for 3d instance segmentation. arXiv preprint arXiv:2502.00173, 2025. 2 [5] Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, and Chengfei Lv. Taoavatar: Real-time lifelike full-body talking avatars for augmented reality via 3d gaussian splatting. In CVPR, 2025. 2 [6] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 5, 7 [7] Johan Edstedt, Ioannis Athanasiadis, Marten Wadenback, Dkm: Dense kernelized feaarXiv preprint and Michael Felsberg. ture matching for geometry estimation. arXiv:2202.00667, 2022. 2 [8] Johan Edstedt, Qiyu Sun, Georg Bokman, Marten Wadenback, and Michael Felsberg. Roma: Robust dense feature matching. arXiv preprint arXiv:2305.15404, 2023. 2 [9] Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, and Qing Li. Semantic gaussians: Open-vocabulary scene understanding with 3d gaussian splatting. arXiv preprint arXiv:2403.15624, 2024. 2 [10] Qingdong He, Jinlong Peng, Zhengkai Jiang, Kai Wu, Xiaozhong Ji, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Mingang Chen, and Yunsheng Wu. Unim-ov3d: Unimodality open-vocabulary 3d scene understanding with finegrained feature representation. In IJCAI, 2024. 2 [11] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram In Burgard. Visual language maps for robot navigation. ICRA, London, UK, 2023. 2 [12] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de Melo, Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba. Conceptfusion: Open-set multimodal 3d mapping. Robotics: Science and Systems (RSS), 2023. 2 [13] Kim Jun-Seong, Kim GeonU, Kim Yu-Ji, Yu-Chiang Frank Wang, Jaesung Choe, and Tae-Hyun Oh. Dr. splat: Directly referring 3d gaussian splatting via direct language embedding registration. In CVPR, 2025. 2, 5, [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 2 [15] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In ICCV, 2023. 2, 5, 6 [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, arXiv preprint and Ross Girshick. arXiv:2304.02643, 2023. 3 Segment anything. [17] Dmytro Kotovenko, Olga Grebenkova, and Bjorn Ommer. Edgs: Eliminating densification for efficient convergence of 3dgs. arXiv preprint arXiv:2504.13204, 2025. 2 [18] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. Panoptic neural fields: semantic object-aware neural scene representation. arXiv preprint arXiv:2205.04334, 2022. [19] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r. In ECCV, 2024. 2 [20] Haijie Li, Yanmin Wu, Jiarui Meng, Qiankun Gao, Zhiyao Zhang, Ronggang Wang, and Jian Zhang. Instancegaussian: Appearance-semantic joint gaussian representation for 3d instance-level perception. In CVPR, 2025. 2 [21] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 2 [22] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. In ACM Trans. Graph., 2022. 2 [23] Phuc D. A. Nguyen, Tuan Duc Ngo, Evangelos Kalogerakis, Chuang Gan, Anh Tran, Cuong Pham, and Khoi Nguyen. Open3dis: Open-vocabulary 3d instance segmentation with 2d mask guidance. In CVPR, 2024. 2 [24] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2024. 3 [25] Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser. Openscene: 3d scene understanding with open vocabularies. In CVPR, 2023. [26] Jens Piekenbrinck, Christian Schmidt, Alexander Hermans, Narunas Vaskevicius, Timm Linder, and Bastian Leibe. Opensplat3d: Open-vocabulary 3d instance segmentation using gaussian splatting. arXiv preprint arXiv:2506.07697, 2025. 2 [27] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In CVPR, 2024. 2 [28] Yansong Qu, Shaohui Dai, Xinyang Li, Jianghang Lin, Liujuan Cao, Shengchuan Zhang, and Rongrong Ji. Goi: Find 3d gaussians of interest with an optimizable open9 [43] Hongjia Zhai, Xiyu Zhang, Boming Zhao, Hai Li, Yijia He, Zhaopeng Cui, Hujun Bao, and Guofeng Zhang. Splatloc: 3d gaussian splatting-based visual localization for augmented reality. arXiv preprint arXiv:2409.14067, 2024. 2 [44] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In CVPR, 2024. vocabulary semantic-space hyperplane. arXiv:2405.17596, 2024. 2 arXiv preprint [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 2, 3 [30] Adam Rashid, Satvik Sharma, Chung Min Kim, Justin Kerr, Lawrence Yunliang Chen, Angjoo Kanazawa, and Ken Goldberg. Language embedded radiance fields for zero-shot taskoriented grasping. In CoRL, 2023. 2 [31] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Superglue: Learning feature and Andrew Rabinovich. matching with graph neural networks. In CVPR, 2020. 2 [32] Hongyu Shen, Junfeng Ni, Yixin Chen, Weishuo Li, Mingtao Pei, and Siyuan Huang. Trace3d: Consistent segmentation lifting via gaussian instance tracing. In ICCV, 2025. 2 [33] William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, and Phillip Isola. Distilled feature fields enable few-shot language-guided manipulation. arXiv preprint arXiv:2308.07931, 2023. [34] Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and Shaofor arXiv preprint Hua Guan. open-vocabulary scene understanding. arXiv:2311.18482, 2023. 2 Language embedded 3d gaussians [35] Wei Sun, Yanzhao Zhou, Jianbin Jiao, and Yuan Li. Cags: Open-vocabulary 3d scene understanding with contextaware gaussian splatting. arXiv preprint arXiv:2504.11893, 2025. 2 [36] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In CVPR, 2024. 2 [37] Ayca Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenIn mask3d: Open-vocabulary 3d instance segmentation. NeurIPS, 2023. [38] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 2 [39] Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, and Jian Zhang. Opengaussian: Towards point-level 3d gaussian-based open vocabulary understanding. In NeurIPS, 2024. 2, 7 [40] Kashu Yamazaki, Taisei Hanyu, Khoa Vo, Thang Pham, Minh Tran, Gianfranco Doretto, Anh Nguyen, and Ngan Le. Open-fusion: Real-time open-vocabulary 3d maparXiv preprint ping and queryable scene representation. arXiv:2310.03923, 2023. 2 [41] Chi Yan, Delin Qu, Dan Xu, Bin Zhao, Zhigang Wang, Dong Wang, and Xuelong Li. Gs-slam: Dense visual slam with 3d gaussian splatting. In CVPR, 2024. 2 [42] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. In ECCV, 2024."
        }
    ],
    "affiliations": [
        "National Yang Ming Chiao Tung University"
    ]
}