{
    "paper_title": "A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning",
    "authors": [
        "Xin Wen",
        "Bingchen Zhao",
        "Yilun Chen",
        "Jiangmiao Pang",
        "Xiaojuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM."
        },
        {
            "title": "Start",
            "content": "A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning Xin Wen1 Bingchen Zhao2 Yilun Chen3 Jiangmiao Pang3 Xiaojuan Qi1* 1The University of Hong Kong 2University of Edinburgh 3Shanghai AI Laboratory {wenxin, xjqi}@eee.hku.hk 5 2 0 2 M 0 1 ] . [ 1 0 6 9 6 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single- )object-centric (NOC) dataa limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, method that induces object-centric representations by introducing semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM . 1. Introduction Pre-trained vision models (PVMs) have become fundamental building blocks in modern computer vision and robotics. While these models have demonstrated remarkable success in traditional vision tasks, their optimal application in robot learning remains an open challenge. Recent works [45, 55] have shown promising results using masked autoencoders (MAE) [29] pre-trained on ego-centric data (e.g., Ego4D [24]). However, two critical questions remain unexplored: 1) Is MAE the optimal pre-training method for robot learning? 2) Is ego-centric data the best choice for *Corresponding author. visuomotor pre-training? As suggested by [19], learning on traditional datasets like ImageNet [20] can obtain better representations. Additionally, scene-centric data (e.g., COCO [41] and Open Images [39]) are also relevant to robot contexts and more information-rich. Web-crawled data (e.g., CC12M [14]) offer another alternative, being easier to collect and more scalable. To systematically answer these questions, we construct comprehensive benchmark that evaluates PVMs across both visuomotor control and perception tasks. For fair comparison, we control the pre-training data scale at 241K images across different data sources and evaluate models on four diverse downstream tasks: two robot manipulation benchmarks (Franka Kitchen [26] and Meta-World [76]), and two segmentation tasks (Pascal VOC [8] and ADE20K [79]). As shown in Fig. 2, our first key finding challenges the role of MAE: DINO [13] and iBOT [80] significantly outperform MAE across all tasks, particularly when trained on objectcentric data. However, we observe critical limitation: these models performance degrades substantially when trained on scene-centric or non-(single-)object-centric (NOC) data. In coherence with the findings of [8], we find that this performance drop strongly correlates with the models diminishing ability to learn object-centric representations from NOC data. This suggests that the challenge lies not in the choice of pre-training method alone, but in maintaining object-centric learning capabilities when training on diverse data sources. Motivated by these insights, we introduce SlotMIM  (Fig. 5)  , method that repurposes and integrates masked image modeling (MIM) and contrastive learning for effective representation learning from NOC datasets. The core idea of SlotMIM is to group patch-level image tokens into objectlevel feature abstractions, referred to as slots, thereby decomposing NOC data into object-centric slots so that objectcentric techniques can be effectively applied. To make patchlevel tokens more semantically aware for subsequent grouping, we enhance MIM with cross-view consistency regularization. Additionally, we introduce semantic bottleneck, which reduces the number of prototypes to encourage the emergence of semantic and objectness at patch-level token representations (see Fig. 4). Building on these semantically 1 Figure 1. An overview of this paper. (a) We conduct comprehensive study evaluating pre-trained vision models (PVMs) on visuomotor control and perception tasks, analyzing how different pretraining (model, data) combinations affect performance. Our analysis reveals that DINO/iBOT excels while MAE underperforms. (b) We investigate the performance drop of DINO/iBOT when trained on non-(single- )object-centric (NOC) data, discovering they struggle to learn objectness from NOC dataa capability that strongly correlates with robot manipulation performance. (c) We introduce SlotMIM, which incorporates explicit objectness guidance during training to effectively learn object-centric representations from NOC data. (d) Through scaled-up pre-training and evaluation across six tasks, we demonstrate that SlotMIM adaptively learns different types of objectness based on the pre-training dataset characteristics, outperforming existing methods. enriched patch tokens, we apply attentive pooling over the learned patch-level features, using prototypes to initialize object representations, thereby grouping patches into objectlevel slots. Contrastive learning [18] is then applied to these slots to promote the discriminativeness of the learned representations. Our scaled experiments  (Fig. 6)  extend the analysis to million-scale datasets, revealing several surprising findings about the relationship between pre-training data and model performance. First, we observe an unexpected inverse scaling trend: while MAEs performance improves with more pre-training data, DINO and iBOT show degraded performance when scaled up. This indicates over-compression of representations: the learning objective reduces properties that are useful for manipulation tasks. Interestingly, we find that SlotMIM trained on ego-centric data learns to discover concepts adaptive to the pre-training data distribution, avoiding over-compression and consistently improving with more data. The resulting modelsevaluated on six diverse tasks including Franka Kitchen, Meta-World, ObjectNav, ImageNav, COCO object detection and instance segmentation, and ADE20K semantic segmentationdemonstrate that SlotMIM consistently outperforms existing methods with better efficiency and scalability. Specifically, when pre-trained with just 241K samples, it already outperforms prior methods that used over 1M samples, including established approaches like MVP and VC-1. More significantly, while other methods plateau or degrade with increased NOC data, SlotMIM continues to improve and surpass previous methods that used 3 more data on both ADE20K and COCO benchmarks. This suggests that NOC data, when properly leveraged, can be more scalable and efficient learning resource than previously thought. In conclusion, our work challenges the prevailing trend of relying solely on scaling up ego-centric data with the MAE model to improve PVM transfer performance. Instead, we advocate for 1) finding method (SlotMIM) that supports robust and scalable pre-training on different types of data, and 2) exploring multiple data types and using the one that best aligns with the target task (e.g., ego-centric data for manipulation tasks, scene-centric data for navigation tasks). This approach not only pushes the boundaries of what is possible with self-supervised learning but also aligns more closely with the practical needs of robot learning applications. 2. Related Work Visual pre-training for robotics. Following the success of vision pre-training, robotics researchers have begun leveraging pre-trained vision models (PVMs) instead of training from scratch. As shown in [53], policies using PVMs can match or exceed the performance of those using ground-truth state information. popular line of work is to train PVMs on ego-centric data (e.g., Ego4D [24]), including R3M [47], MVP [55, 73], VIP [44], and VC-1 [45]. However, recent study [19] shows that PVMs trained on ImageNet can also achieve competitive performance on downstream visuomotor tasks. Our work contributes to this line by expanding the 2 Pre-train Data Source #Image #Obj/Img #Class Type INet-241K COCO+ CC-241K Ego-241K ImageNet COCO CC12M Ego4D 241K 241K 241K 241K 1.7 7.3 1000 Object Scene Web Ego 80 Video Object: Object-centric; Scene: Scene-centric; Web: Web-crawled; Ego: Ego-centric Table 1. Overview of pre-training datasets. We uniformly sample subsets of 241K images from ImageNet, CC12M, and Ego4D. COCO+ is formed by combining train and unlabeled subsets of COCO. Ego4D frames are extracted at 0.2 fps and then sampled to subsets. 1.28M subsets are also considered in later experiments. For scene-centric data, we use the Open Images [39] dataset to scale up. scope of pre-training data to scene-centric and web-crawled data. More importantly, we also investigate methods beyond MAE [29], which has been the predominant choice in many existing works [19, 45, 55, 62]. Other PVM paradigms include training from scratch with proper data augmentation [27], from policy learning [44], from language [36, 43] or 3D [16], etc. Considering evaluation, while many works focus on manipulation tasks [19, 33, 44, 47, 55], we also evaluate navigation tasks as in [45] and include perceptionoriented tasks like segmentation and detection, thus providing more complete picture. Self-supervised visual representation learning. Selfsupervised representation learning aims at learning transferable features from unlabeled data [1, 2, 5, 10, 12, 13, 17, 65]. The field has converged on two main approaches: contrastive learning, which learns representations by comparing positive and negative examples [17, 28, 65], and masked image modeling (MIM), which learns by reconstructing masked image regions [29, 75]. While these methods have proven effective, their evaluation has largely focused on object-centric datasets like ImageNet-1K [20]. Our work broadens this scope by studying self-supervised learning on large-scale non-(single-)object-centric datasets, and primarily evaluating the performance on robot learning tasks. Learning on non-(single-)object centric (NOC) data. Several recent works have tackled self-supervised learning on non-object centric data [3, 22, 30, 31, 6769, 74]. Among them, the majority focuses on learning from scene-centric datasets, and benefiting scene understanding tasks. To solve this problem, the idea is primarily either to build pretext tasks on dense featuremaps [3, 68, 74], object-centric groups [30, 31, 69], or specialized data augmentation techniques [22, 67]. In addition, some works have also explored learning from uncurated datasets [3, 11, 52, 66], which mainly correspond to large-scale web-crawled datasets, and main topic is data deduplication [52]. Our work contributes to this line of research by systematically studying the performance of self-supervised learning on multiple types of NOC data. 3. When are PVMs More Effective for Visuomotor Control and Perception Tasks? 3.1. Pre-Training Setting Datasets. common belief is that ego-centric data are the best for robot learning, primarily due to their contextual similarity to manipulation tasks [36, 44, 45, 47, 55, 73]. As suggested by [19], however, data diversity matters more for learning transferrable PVMs and traditional datasets like ImageNet [20] can be more effective. Our study considers the (single-)object-centric ImageNet [20] and ego-centric Ego4D [24]. Moreover, our study also encompasses scenecentric data (e.g., COCO [41]), which are also close to robot contexts and are more information-rich. We also consider web-crawled data (e.g., CC12M [14]), as they are easier to collect and more friendly to scale up. As shown in Tab. 1, experiments in this section control the data scale to 241K uniformly sampled images from each dataset. This ensures fair comparison and efficient pre-training, allowing us to expand pre-training to more (method, dataset) combinations. Methods. Another widely-accepted belief is that MAE [29] is one of the best pre-training methods for PVMs [45, 55, 62, 73], which remains unchallenged by [19]. We are interested in whether expanding the search space to NOC data could reveal other strong candidates. While existing works have repeatedly demonstrated MAEs superiority over ResNets [44, 47] and robot-oriented ViTs [18, 54], we compare MAE with several other ViT pre-training methods: BEiT [4], SplitMask [22], DINO [13], and iBOT [80]. These methods were selected based on their strong performance on perception tasks like detection and segmentation. Given the significant computational cost of reproducing pre-training (including MAE) on multiple datasets, this selection provides good representative set of methods. Training. We use ViT-B/16 [21] as the backbone. At 241K data scale, all methods are trained for 800 epochs. At 1.28M data scale, we train for 400 epochs. The optimization hyperparameters follow official settings of each method. 3 Figure 2. Performance of PVMs trained with different (model, data) combinations on visuomotor control and perception tasks. (241K scale, best viewed together with Fig. 1a) Our analysis of existing works reveals several key findings: 1) MAE with ego-centric data shows only moderate performance on visuomotor control tasks and performs poorly on ADE20K; 2) DINO and iBOT lead performance across all tasks, with their best models typically trained on object-centric data (except for ADE20K); 3) The top-3 models (DINO, iBOT, and MAE) struggle to learn effective representations for manipulation when trained on scene-centric data. Most notably, 4) SlotMIM (Sec. 4) consistently outperforms prior methods regardless of whether it is pre-trained on object-centric data or not. Benchmark Suite Franka Kitchen [26] Meta-World [76] ObjectNav [6] ImageNav [81] RGB Proprio. Physics Action Goal Learning Continu. Continu. Discrete Class Discrete Image IL IL IL RL Table 2. Overview of robot manipulation and navigation tasks. Bottom: example tasks of each benchmark suite. 3.2. Evaluation Setting We expect generalizable PVM to benefit both visuomotor control tasks (e.g., manipulating household objects and navigating indoor environments) and perceptual understanding tasks (e.g., recognizing objects and scenes, and correctly localizing them). While these tasks may share some common features, they can also require contrasting properties (e.g., grasping vs. navigation). Therefore, we evaluate PVMs on as diverse tasks as possible to understand their properties and how they interact with (pre-training) dataset biases. An overview is shown in Tab. 2. Due to computational constraints, this section focuses on manipulation (control) and segmentation (perception) tasks, with the strongest PVMs selected for navigation (control) and detection (perception) tasks in later sections. Visuomotor control tasks. To make the manipulation tasks better reflect the ability of PVMs, 1) we follow [36] to use trainable attentive pooling (see Fig. 3), as opposed to prior works that employ [CLS] token. This is essential for fair comparison between PVMs as the [CLS] token of some 4 Figure 3. Behavior cloning with attentive probing. An additional token is trained with cross attention to gather information from all patch tokens from the backbone. methods (e.g., MAE) does not receive any training signal and the results are rather arbitrary. Attentive pooling instead performs learnable combination of all output tokens from the encoder, thus better utilizing the potential of models; 2) We also follow [33] to avoid using any proprioceptive input to highlight the effect of PVMs, and 3) run 3 seeds for each experiment to avoid randomness. We then record the best performance of each run following [45], and report average performance across seeds and tasks. We adopt 25 demonstrations per task. The benchmark covers 5 tasks in Franka Kitchen [26] and 8 tasks in Meta-World [76]. Remaining details follows [33]. And all details in navigation tasks follow VC-1 [45]. Perception tasks. Following [8], we report the segmentation Jaccard index of the best attention head on Pascal VOC 2012. ADE20K semantic segmentation experiments follow the setting of MAE [29], which uses UperNet [72] and trains for 160K iterations with batch size 16. For COCO object detection and instance segmentation, we follow the setting in iBOT [80] to train Cascade Mask R-CNN [9] with 1 schedule (12 epochs), and report box and mask AP. (a) Clustering assignment of patch tokens. Each patch is assigned to its nearest-neighbor prototype, with different colors indicating different prototypes. (b) Top-5 segments retrieved by the prototypes (by column). segment consists of patches assigned to the same prototype within an image. Each column shows the top-5 segments with the highest cosine similarity to the prototype corresponding to the column. Figure 4. Comparison of concepts learned by iBOT and SlotMIM. All models are trained on COCO+ for 800 epochs. While iBOT can discover fine-grained patterns, especially when using fewer prototypes (left), these patterns emerge bottom-up and lack semantic meaning. In contrast, SlotMIMs concepts are semantically coherent, making them more effective for instance discrimination pretext tasks (right). 3.3. Main Findings We evaluate models pre-trained on 241K-scale datasets, with complete results shown in Fig. 2. For better understanding, we also visualize processed data in Fig. 1a and Fig. 1b. Below we discuss several key findings. Neither MAE nor ego-centric data are optimal for visuomotor control. As shown in Fig. 2 (complete results) and Fig. 1a (overall results), MAE and ego-centric data achieve only moderate performance in visuomotor control tasks, and perform poorly on ADE20K. This challenges the prevailing belief that MAE combined with ego-centric data is the best pre-training approach for PVMs, suggesting the need to explore alternative methods. DINO/iBOT excel across tasks, especially with objectcentric data. As demonstrated in Fig. 2 and Fig. 1a, DINO and iBOT lead performance on most tasks, particularly when trained on object-centric data. iBOT also achieves strong semantic segmentation results when trained on scene-centric data. However, this performance advantage is sensitive to data distribution and doesnt generalize well across different pre-training datasets. NOC data, particularly scene-centric data, significantly degrades top-performing methods. While DINO/iBOT models achieve leading performance, they suffer substantial degradation on scene-centric data, as does MAE. This observation motivated us to investigate the underlying causes and develop more robust PVM. Objectness predicts DINO/iBOT performance. In Fig. 1b, we demonstrate that DINO/iBOTs ability to learn objectness through attention maps deteriorates alongside performance drops on NOC data. The strong correlation (0.72) between objectness and task success rates aligns with findings from [8]. This suggests that achieving good objectness from NOC data could lead to strong performance, which motivated our design of SlotMIM (introduced in Sec. 4). As shown in Fig. 2, this approach proves effective. 4. Object-centric Learning on NOC Data 4.1. Preliminaries Deep clustering as self-distillation. DINO [13] learns set of prototypes online to cluster image embeddings. Given an input image RHW 3, let fθ and fξ be student and teacher encoders that produce embeddings zθ = fθ(x) and zξ = fξ(x) respectively. The cluster assignments are computed as pθ(x) = softmax(zθC/τ ), where = {cc}C c=1 are the prototypes and τ is temperature parameter. The loss is the cross-entropy between predictions of student and teacher models: LDINO(v1, v2) = (cid:80)C c=1 qξ(v2)c log pθ(v1)c, where v1 and v2 are two augmented views of the same image. Centering and sharpening operations are omitted in the equation for simplicity. Since it resembles knowledge distillation with soft labels produced by itself, DINO is also dubbed as self-distillation. DINO on image patches with MIM. iBOT [80] extends DINO to local image patches using masked image modeling (MIM). Given binary mask {0, 1}N indicating masked patches, the masked input replaces masked patches with mask token m. The iBOT loss predicts cluster assignments of masked patches from unmasked ones: LiBOT(v) = (cid:80) i:Mi=1 LDINO(vi, vi), where vi is the masked patch from the student and vi is the corresponding unmasked patch from the teacher. 5 Figure 5. Overview of SlotMIM. Our framework extends iBOT by: 1) repurposing its within-view patch-level self-distillation for object discovery, 2) introducing cross-view objective for semantic guidance, and 3) performing object-centric contrastive learning on slots (object features grouped from patches with matching cluster assignments). This approach provides explicit objectness supervision without requiring object-centric data, making it applicable to various types of NOC data (see Fig. 1c for comparison and Fig. 1d for results). Slot attention [42] is variant of cross-attention that normalizes attention scores on the query axis instead of the key axis, encouraging queries to attend to different parts of the input. Our approach performs similar attentive pooling on patch embeddings based on their cluster assignments, with prototypes acting as queries and patch embeddings zθ,i as keys. Following convention, we refer to these pooled object features as slots prototypes adapted to image patches. 4.2. SlotMIM Framework High-level intuition. We decompose self-supervised learning on NOC data into two subtasks: 1) learning to group image patches into objects, and 2) learning to discriminate objects as prior work done on object-centric data. The key challenge is unsupervised object discovery, which we find emerges naturally from iBOT when using fewer prototypes. Representation bottleneck for objectness. iBOT uses set of prototype embeddings = {cc}C c=1 to cluster image patches into groups, assigning each patch token soft onehot encoding pθ(xi) of its cluster membership. While iBOT typically uses = 8192 to capture fine patterns, we find much smaller (e.g., 512 for COCO) better serves object discovery by creating an information bottleneck that encourages learning compositional object concepts. As shown in Fig. 4a, iBOTs clusters are very fine-grained (2nd row), but objectness emerges with less prototypes (3rd row). However, these patterns still lack semantic meaning and can fragment single objects. Additionally, matching discovered objects between views remains difficult as their semantics vary despite assigned to the same prototype (Fig. 4b, left). Both issues 6 suggest the need for semantic-level prototypes. Cross-view consistency for semantic learning. The lack of semantic meaning stems from LiBOT being computed between patches within the same view, providing no explicit guidance for learning view-invariant representations. We address this with simple cross-view consistency objective Lcross patch that encourages patches under different transformations to share the same token. We match patches between views using ROIAlign to align overlapping regions. Formally, for two augmented views v1 and v2 with patch embeddings z1 ), the loss is: ξ,j = fξ(v θ,i = fθ(v1 ) and z"
        },
        {
            "title": "Lcross",
            "content": "patch(v1, v2) = 1 (cid:88) (cid:88) (i,j)P c= ξ,i,c log p1 q2 θ,j,c , (1) θ = softmax( z1 ξ = softmax(z2 where p1 ξ Cξ/τt) are cluster assignments, τs and τt are temperature parameters, and contains matched patch pairs. θ Cθ/τs) and Object-level contrastive learning. With aligned object features, we apply contrastive learning at the object level. We only use slots that occupy at least one patch, filtering with the indicator: 1i = such that argmaxc(pθ(v1 )c) = i. Slots with matching tokens form positive pairs. Using MoCostyle approach with slots s1 θ,j), ξ,i = (cid:80) s2 θ,i = hθ((cid:80) ξ,j, the loss is: pθ(v1 qξ(v2 )iz )iz2 Lslot(s1 θ, s2 ξ) ="
        },
        {
            "title": "1\nK",
            "content": "C (cid:88) i=1 log 11 θ,i s2 12 exp(s1 12 11 exp(s1 θ,i s2 ξ,i/τ ) (cid:80)C j=1 ξ,j/τ ) (2) , Figure 6. Results of scaling PVM training data. It considers three factors that influence manipulation success rates: data types, pre-training methods, and data scale. Highlighted lines represent the best-performing data scaling for each method, while faded lines indicate sub-optimal performance. It shows that 1) SlotMIM achieves the best performance, scalability, and data efficiency across evaluation tasks by training on NOC data; 2) On manipulation tasks, most methods (except MAE) face performance drop when scaling up pre-training data. 12 11 where hθ is predictor MLP, = (cid:80) counts positive pairs, and τ = 0.2. The final loss combines these objectives: Lθ,ξ(v1, v2) = λ1Lwithin patch(v1, v2)+ λ2Lslot(s1 patch is identical to LiBOT and λ1 = 0.5, λ2 = 1. In practice, we optimize the symmetrized objective Lθ,ξ(v1, v2) + Lθ,ξ(v2, v1). patch (v1, v2)+λ1Lcross ξ), in which Lwithin θ, s2 4.3. Ablation Study mask Lcross patch Lwithin patch Lslot k-NN ADE Jacc 42.5 8.3 42.3 10.3 39.3 20.7 8.4 42.9 43.9 9.4 47.4 48.6 45.7 47.5 49.1 45.1 44.9 27.7 45.3 46.2 1 2 3 4 5 Table 3. Ablation study on effective modules. We conduct an ablation study to analyze key components of SlotMIM. All models are trained on COCO+ for 800 epochs. As shown in Tab. 3, we evaluate each model variant using three metrics: k-NN accuracy on ImageNet classification, mean IoU on ADE20K semantic segmentation, and Jaccard index on VOC2012. We also report K, the average number of objects/stuff regions discovered per image. The results reveal that Jaccard index positively correlates with representation quality, indicating that better objectness leads to stronger representations. Adding MIM improves downstream segmentation performance (comparing rows 1 and 2). The cross-view consistency loss and slot contrastive loss both contribute significantly to objectness (rows 2, 3, 5). Additionally, the within-view loss acts as an effective regularizer that further enhances the learned representations (rows 4, 5). We provide detailed ablation studies on hyperparameters in the appendix. 7 5. Scaling Up Pre-Training Data To provide an even more comprehensive picture, we extend Fig. 2 by scaling up the pre-training data from 241K to 1.28M and beyond (4M for SlotMIM, 5M for MAE, and 12M for iBOT). The models at 1.28M scale follow the same settings as in Tab. 1. For SlotMIM, we combine ImageNet [20], COCO+ [41], OpenImages [39], Objects365 [63], and LVIS [25] to create 4M-scale scenecentric dataset, which we call DetSoup. We also utilize publicly available checkpoints of MAE trained on 5M-scale ego-centric datasets (MVP [55] and VC-1 [45]), and iBOT trained on 12M-scale ImageNet-21K [58]. The results are presented in Fig. 6. 5.1. Experiments on Manipulation Tasks The MAE regime includes MVP [55] and VC-1 [45], which leverage MAE [29] to pre-train models on massive collection of ego-centric videos [24] and Internet data. VCond [36] further proposes language-driven representation learning from human videos and their associated captions. Fig. 6 examines the relationship between manipulation success rates and pre-training methods, comparing scaling trends across different data types: ego-centric, object-centric, and scene-centric. Notably, increasing dataset size does not always improve performance across benchmarks, as also observed by [19]. What leads to inverse-scaling behaviors on manipulation tasks? In object manipulation tasks, scaling scene-centric and object-centric data to the million level can lead to performance drops for methods like DINO and iBOTwith MAE being the only exception. We hypothesize that selfsupervised representation learning, including MIM, aims to learn invariance by pulling similar visual content together in the embedding space, effectively compressing visual data. However, scaling up data may result in over-compression, losing crucial low-level visual information necessary for visuomotor control tasks (e.g., accurate object grasping). MAE, on the other hand, continues to preserve low-level information due to the nature of its MIM objectives. This also explains why MAE is commonly preferred in existing PVM worksthey typically start with million-scale data, and MAE is one of the few methods that avoid overcompression at such scale. Why SlotMIM does not face over-compression when trained on ego-centric data? SlotMIM builds its training objective on concepts it discovers, with the type of discovered concepts determined by the training data distribution. As shown in Fig. 1d, SlotMIM learns coarse-grained objects from object/scene-centric data, but fine-grained parts from ego-centric data, achieving good balance between invariance and objectness. We believe this occurs because, unlike object/scene-centric data from diverse Internet sources, egocentric images come from consecutive human videos sharing contextual backgrounds or scenarios. This contextual similarity means invariance learning in ego-centric data focuses more on differences within the same video or scenario, particularly in foreground objects. This focus is crucial for robot manipulation learning, which requires effective interaction with these foreground objects. SlotMIM is more data efficient and scalable in leveraging ego-centric data. Compared to general-purpose pre-trained models and state-of-the-art robot learning methods (e.g., MVP [55] and VC-1 [45]), we demonstrate that SlotMIM, pre-trained with just 241K data samples, outperforms prior methods that used over 1M samples. When scaled to 1M ego-centric data, it achieves the highest success rates among all methods in the comparison. 5.2. Experiments on ADE20K and COCO As shown in Fig. 6 (right), compared to previous efforts scaling up with ImageNet-21K (12M images) [58], our SlotMIM models continue to improve and surpass them using 3 less data. This suggests that NOC data can be more scalable learning resource. In Fig. 7, we present an evaluation on COCO object detection and instance segmentation. The superiority of SlotMIM is evident and continues to improve with increased data scale. As supported by the visualizations in Fig. 1d, SlotMIM learns to discover common objects from scenes in alignment with human visual perception, supporting its scaling capability for segmentation/detection tasks. 5.3. Experiments on Navigation Tasks In Tab. 4, we compare SlotMIMs performance on ObjectNav and ImageNav benchmarks with SoTA methods. Details of the navigation tasks can be found in Tab. 2. Due Figure 7. Results on COCO object detection and instance segmentation. SlotMIM shows better data efficiency with both objectcentric and NOC data, and its results continue to improve with more data, surpassing prior models by notable margin. Method Arch Pretrain Data Scale ObjectNav ImageNav MVP [55] ViT-B EgoSoup 4.6M VC-1 [45] ViT-B Ego4D+MNI 5.6M SlotMIM ViT-B Ego4D SlotMIM ViT-B DetSoup 51.2 55.4 1.28M 48.4 62.0 4.0M MVP [55] ViT-L 4.6M VC-1 [45] ViT-L Ego4D+MNI 5.6M"
        },
        {
            "title": "EgoSoup",
            "content": "55.0 60.3 64.7 67.9 65.4 69.8 68.1 70.3 Table 4. Results on ObjectNav and ImageNav. Compared with prior SoTA methods using ViT-B, SlotMIM trained on DetSoup improves by 6.6% and 1.9% on ObjectNav and ImageNav, respectively. Compared with ViT-L models, it still rivals them on ObjectNav and achieves comparable performance on ImageNav. to the extreme GPU demands of these tasks, we evaluated only two SlotMIM models: each trained on the largest-scale scene/ego-centric data (DetSoup and Ego4D) available. SlotMIM trained on Ego4D shows moderate performance on ImageNav but underperforms on ObjectNav, indicating misalignment with downstream tasks. In contrast, when trained on DetSoup, it outperforms MVP and VC-1 by 6.6% and 1.9% on ObjectNav and ImageNav, respectively. This demonstrates both the effectiveness of SlotMIM and suggests navigation are closely related to perception tasks. 6. Conclusion This work revisits the potential of PVMs for robot learning tasks. We construct comprehensive benchmark that evaluates models on robot learning tasks to find out what is the best pretraining method and what is the best data for pretraining. Our findings suggest that while DINO and iBOT lead the benchmark, their performance degrades rapidly when trained on NOC data. Furthermore, this degradation can be partially alleviated by strengthening the ability to learn object-centric representations. Motivated by these findings, we propose SlotMIM to effectively learn object-centric representations from NOC datasets. Through extensive experiments across diverse datasets and downstream tasks, including robotics, we demonstrated the consistent superiority of SlotMIM over 8 existing methods. We hope our promising results open new avenues for adopting PVMs for more robot learning tasks."
        },
        {
            "title": "Acknowledgements",
            "content": "This work has been supported by Hong Kong Research Grant CouncilEarly Career Scheme (Grant No. 27209621), General Research Fund Scheme (Grant No. 17202422), and RGC Matching Fund Scheme (RMGS). Part of the described research work is conducted in the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust."
        },
        {
            "title": "References",
            "content": "[1] Yuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. 3 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In CVPR, pages 1561915629, 2023. 3 [3] Yutong Bai, Xinlei Chen, Alexander Kirillov, Alan Yuille, and Alexander C. Berg. Point-level region contrast for object detection pre-training. In CVPR, pages 1606116070, 2022. 3 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transformers. In ICLR, 2022. 3 [5] Adrien Bardes, Jean Ponce, and Yann LeCun. VICRegL: Self-supervised learning of local visual features. In NeurIPS, pages 87998810, 2022. 3 [6] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. ObjectNav revisited: On evaluation of embodied agents navigating to objects. arXiv:2006.13171, 2020. 4, 13 [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: Robotics transformer for real-world control at scale. In RSS, 2023. [8] Kaylee Burns, Zach Witzel, Jubayer Ibn Hamid, Tianhe Yu, Chelsea Finn, and Karol Hausman. What makes pre-trained visual representations successful for robust manipulation? In CoRL, 2024. 1, 4, 5, 14 [9] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: High quality object detection and instance segmentation. IEEE TPAMI, 43(5):14831498, 2019. 4, 14 [10] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, pages 139156, 2018. 3 [11] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image features on non-curated data. In ICCV, pages 29592968, 2019. 3 [12] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, pages 99129924, 2020. 3, 15 [13] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 96509660, 2021. 1, 3, 5, 13, 14, 15 [14] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text preIn CVPR, training to recognize long-tail visual concepts. pages 35583568, 2021. 1, [15] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. GR-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv:2410.06158, 2024. 14 [16] Shizhe Chen, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. SUGAR: Pre-training 3d visual representations for robotics. In CVPR, 2024. 3 [17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, pages 15971607, 2020. 3 [18] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, pages 96409649, 2021. 2, 3, 13 [19] Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Abhinav Gupta. An unbiased look at datasets for visuo-motor pre-training. In CoRL, 2023. 1, 2, 3, 7 [20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. 1, 3, 7, 17 [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3, [22] Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herve Jegou, and Edouard Grave. Are largescale datasets necessary for self-supervised pre-training? arXiv:2112.10740, 2021. 3 [23] Kuan Fang, Fangchen Liu, Pieter Abbeel, and Sergey Levine. MOKA: Open-world robotic manipulation through markbased visual prompting. In RSS, 2024. 14 [24] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4D: Around the 9 world in 3,000 hours of egocentric video. In CVPR, pages 1899519012, 2022. 1, 2, 3, 7, 14, 15 head, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. 14, [25] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: dataset for large vocabulary instance segmentation. In CVPR, pages 53565364, 2019. 7, 17 [26] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv:1910.11956, 2019. 1, 4 [27] Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su, Huazhe Xu, and Xiaolong Wang. On pre-training for visuo-motor control: Revisiting learning-from-scratch baseline. In ICML, pages 12511 12526, 2023. 3 [28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 97299738, 2020. 3 [29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1600016009, 2022. 1, 3, 4, 7, 16 [30] Olivier J. Henaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, and Joao Carreira. Efficient visual pretraining with contrastive detection. In ICCV, pages 1008610096, 2021. 3 [31] Olivier J. Henaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman, Joao Carreira, and Relja Arandjelovic. Object discovery and representation networks. In ECCV, pages 123143, 2022. [32] Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of GPT-4V in robotic vision-language planning. arXiv:2311.17842, 2023. 14 [33] Yingdong Hu, Renhao Wang, Li Erran Li, and Yang Gao. For pre-trained vision models in motor control, not all policy learning methods are created equal. In ICML, pages 13628 13651, 2023. 3, 4, 13 [34] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. CoPa: General robotic manipulation through spatial constraints of parts with foundation models. arXiv:2403.08248, 2024. 14 [35] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3d value maps for robotic manipulation with language models. In CoRL, pages 540562, 2023. 14 [36] Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-driven representation learning for robotics. In RSS, 2023. 3, 4, 7, 13, 15 [37] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An open-source vision-language-action model. In CoRL, 2024. 14, 15 [38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White- [39] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The Open Images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 128(7):1956 1981, 2020. 1, 3, 7, 14, 17 [40] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: framework for attention-based permutation-invariant neural networks. In ICML, pages 37443753, 2019. 13 [41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, pages 740755, 2014. 1, 3, 7, 14, 17 [42] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In NeurIPS, pages 1152511538, 2020. 6 [43] Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. LIV: Language-image representations and rewards for robotic control. In ICML, pages 2330123320, 2023. 3 [44] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: Towards universal visual reward and representation via value-implicit pre-training. In ICLR, 2023. 2, [45] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? In NeurIPS, pages 655677, 2023. 1, 2, 3, 4, 7, 8, 13, 15 [46] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection with vision transformers. In ECCV, pages 728755, 2022. 14 [47] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3M: universal visual representation for robot manipulation. In CoRL, pages 892909, 2023. 2, 3, 13 [48] Muhammad Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. In NeurIPS, pages 2329623308, 2021. 14 [49] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, TsangWei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, and Brian Ichter. PIVOT: Iterative visual prompting elicits actionable knowledge for VLMs. In ICML, pages 3732137341, 2024. 14 [50] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In RSS, 2024. 14, 15 [51] OpenAI. GPT-4 technical report. arXiv:2303.08774, 2023. 14 [52] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, PoYao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 3, 14, 15, 16 [53] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pretrained vision models for control. In ICML, pages 17359 17371, 2022. 2 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 3, 14 [55] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In CoRL, pages 416426, 2023. 1, 2, 3, 7, [56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. 14 [57] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 largescale 3d environments for embodied AI. In NeurIPS, 2021. 13 [58] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 115(3):211252, 2015. 7, 8, 17 [59] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In ICCV, pages 93399347, 2019. 13 [60] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION400M: Open dataset of CLIP-filtered 400 million image-text pairs. arXiv:2111.02114, 2021. 15 [61] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION5B: An open large-scale dataset for training next generation image-text models. In NeurIPS, pages 2527825294, 2022. [62] Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view masked world In ICML, pages models for visual robotic manipulation. 3061330632, 2023. 3 [63] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In ICCV, pages 84308439, 2019. 7, 17 [64] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-world object manipulation using pre-trained vision-language models. In CoRL, pages 33973417, 2023. 14 [65] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, pages 776794, 2020. 3 [66] Yonglong Tian, Olivier J. Henaff, and Aaron van den Oord. Divide and contrast: Self-supervised learning from uncurated data. In ICCV, pages 1006310074, 2021. 3 [67] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Gool. Revisiting contrastive methods for unsupervised learning of visual representations. In NeurIPS, pages 1623816250, 2021. 3 [68] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In CVPR, pages 30243033, 2021. [69] Xin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, and Xiaojuan Qi. Self-supervised visual representation learning with semantic grouping. In NeurIPS, pages 1642316438, 2022. 3, 14 [70] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In ICLR, 2024. 14 [71] Fei Xia, Amir Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In CVPR, pages 90689079, 2018. 13 [72] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, pages 418434, 2018. 4, 13 [73] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv:2203.06173, 2022. 2, 3 [74] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In CVPR, pages 1668416693, 2021. [75] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: simple In CVPR, pages framework for masked image modeling. 96539663, 2022. 3 11 [76] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In CoRL, 2019. 1, 4 [77] Jia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Chen, Hao Dong, Haoming Song, Dong Wang, Di Hu, Ping Luo, et al. Learning manipulation by predicting interaction. In RSS, 2024. 15 [78] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and LuIn CVPR, pages cas Beyer. Scaling vision transformers. 1210412113, 2022. [79] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, pages 633641, 2017. 1 [80] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image BERT pre-training with online tokenizer. In ICLR, 2022. 1, 3, 4, 5, 13, 14 [81] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, pages 33573364, 2017. 4, 13 [82] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, pages 21652183, 2023. 14 12 Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning"
        },
        {
            "title": "Contents",
            "content": "A. Extended Implementation Details . . . . A.1. Pre-Training Details . . A.2. Evaluation Details . . . A.3. Details for Fig. 1 . . . A.4. Details for Fig. 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13 13 14 14 B. Extended Analysis and Discussion 14 14 . . . . B.1. Why Pre-Training on NOC Data? . B.2. Why Consider Both Control and Perception? 14 B.3. What Determines the Objectness of SlotMIM? 15 15 B.4. Fine-grained Slots for Manipulation Tasks? . 15 . . . . B.5. Limitation and Future Work . . . . . . C. Extended Experiments . . . . C.1. Extended Ablation Study . . . . . C.2. Comparison with DINOv2 . C.3. ImageNet Linear Probing and Fine-tuning . . . . . . C.4. Scaling Up for ImageNet Tasks . . . . . . . . . . . . . 15 15 16 16 16 A. Extended Implementation Details A.1. Pre-Training Details Architecture. We use ViT-B/16 [21] as our backbone. Following common practice in DINO [13], iBOT [80] and MoCo-v3 [18], the projector is 3-layer MLPs with hidden dimension 2048 and output dimension 256, and the predictor is 2-layer MLPs with hidden dimension 4096 and output dimension 256. Augmentation and masking. We use the same augmentation strategy as in iBOT [80] except not using small local crops (multi-crop). Avoiding the use of multi-crop saves significant computational costs in our model, and the modellearned slots work in similar role. The masking strategy follows iBOT [80], with prediction ratio uniformly sampled from range [0.3 0.2, 0.3 + 0.2]. Optimization. Most optimization configurations follow DINO [13] and iBOT [80]. We use AdamW optimizer with cosine schedule for the learning rate and weight decay. The learning rate is linearly ramped up during the first 10 epochs to 1.5 104 scaled with the total batch size: lr = lrbase batch size/256, and then decays following the cosine schedule. The weight decay starts from 0.4 and also decays following the cosine schedule, to 0.04 when training ends. We train for 800 epochs on 241K-scale datasets and 400 epochs on 1.28M-scale datasets, with batch size of 1024 distributed across 8 A100 GPUs. For experiments on 13 4M-scale datasets, we train 200 epochs. Hyperparameters. Follow DINO [13] and iBOT [80], the teacher temperature τt linearly ramps up from 0.04 to 0.07 for the first 30 epochs and remains constant afterwards. The student temperature τs is fixed at 0.1. The number of prototypes is set to 512 for COCO+ and 1024 for other datasets. A.2. Evaluation Details Manipulation tasks. Following the setup of [33], we use shallow 4-layer MLP with hidden sizes [512, 256, 128] and ReLU activations as the policy network for behavior cloning. The 5 Franka Kitchen tasks, 8 Meta-World tasks, and corresponding GT demonstrations are also taken from [33]. Following R3M [47] and VC-1 [45], the policy training involves mini-batches of 128 samples, conducted over 20000 steps with the Adam optimizer and learning rate of 0.001. The model is evaluated every 1000 steps. All tasks and environments use 224 224 RGB images without proprioceptive input and without image augmentations, and each task uses only 25 demonstrations for training, which raises higher requirements on the quality of PVMs. For better measurement of the potential of PVMs, we follow [36] and use attentive pooling (also known as multihead attention pooling [40]), which is suggested as strong and versatile approach [78] as opposed to the commonly used [CLS] token and provides better comparisons between frozen PVMs. Following VC1 [45], we take the best checkpoint for each run on each task. For each environment, we report the average performance over all tasks (3 independent runs each task). Navigation tasks. The settings on navigation tasks strictly follows VC-1 [45], which involves object-goal navigation [6] and image-goal navigation [81]. In both, the agent is initialized at random location in an unknown 3D environment and is tasked to find the goal location specified by an image or object. Both tasks are conducted using Habitat [59] simulator, in which ObjectNav is conducted in the HM3D [57] environment and ImageNav is conducted in the Gibson [71] environment. For ObjectNav, the agent is trained for approximately 400M steps with 512 parallel environments. For ImageNav, the agent is trained for 500M steps with 320 parallel environments. Further details can be found in [45] and omitted here for brevity. Semantic segmentation on ADE20K. We use UperNet [72] implemented in MMSegmentation following iBOT [80]. Specifically, we fine-tune for 160k iterations with stochastic gradient descent, with batch size of 16 and weight decay of 0.0005. The learning rate is 0.01 and decays following the poly schedule with power of 0.9 and min lr of 0.0001. A.4. Details for Fig. Object detection and instance segmentation on COCO. COCO object detection and instance segmentation setting also follows iBOT [80], where the pre-trained model initialized Cascade Mask R-CNN [9]. The image scale is [640, 800] during training and 800 at inference. We finetune all layers end-to-end on COCO [41] train2017 set with the standard 1 schedule and report AP for boxes and masks on the val2017 set. Analytical metrics. Some numeric indicators are considered to help analyze properties of pre-trained models, e.g., object discovery ability (objectness of attention maps) measured by Pascal VOC 2012 object segmentation quality. The Jaccard index measures the overlap between predicted mask and the ground truth mask as J(P, G) = GP GP . Following [8, 48], the attention maps of DINO and iBOT are computed between the [CLS] token and patch tokens in the last layer, and the attention maps of SlotMIM are computed between the prototypes and projected patch tokens. For each object of interest, the attention head/prototype producing the best Jaccard index is selected. Besides, for the ablation studies, we also report k-NN ImageNet classification (k = 20) accuracy following DINO [13]. Additionally, we maintain running mean of the average number of active (assigned to at least one patch) slots in an image during training. A.3. Details for Fig. 1 (a) BC performance regarding dataset and model. Fig. 1a is grouped version of Franka Kitchen and Meta-World results in Fig. 2 in the main paper. For each grid, we report the average performance over all manipulation tasks given PVM pre-trained on specific dataset (by row) using specific model (by column). (b) Correlation between objectness and BC performance. Fig. 1b is joined view between the average manipulation performance (first two subfigures) and VOC object segmentation performance (third subfigure) in Fig. 2 in the main paper. It is presented as scatter plot to show the (Pearsons) correlation between the two metrics. (d) Visualization of attention maps. Fig. 1b and Fig. 1d show examples of the attention maps of DINO and SlotMIM, respectively. All models are pre-trained on 241K-scale datasets for 800 epochs. For DINO, we follow the official implementation [13] to take the attention maps between the [CLS] token and patch tokens in the last layer, and show the best attention head. For SlotMIM, we take the attention maps (prototype assignments) between projected patch tokens and the cat prototypethe prototype with the highest cosine similarity with cat segments. (a) Visualization of attention maps. All attention maps in Fig. 4a are computed in the same manner as SlotMIM in Fig. 1dbetween the prototypes and projected patch tokens. All models are pre-trained on COCO+ (scene-centric) for 800 epochs. For iBOT, we trained two variantsone with 8192 prototypes (dimension of the last layer in the DINOHead in implementation) as default, and one with 512 prototypes. In the visualization, each prototype is assigned random color. (b) Visualization of segmentation consistency. Fig. 4b shows the segments assigned to each prototype following the implementation of [69]. We first obtain all segments (prototype assignments) on COCO val2017 set, pool them to slots, and then retrieve the nearest-neighbor slots for each prototype. Then for each selected prototype, we visualize the segments of the top-5 similar slots. B. Extended Analysis and Discussion B.1. Why Pre-Training on NOC Data? Self-supervised pre-training have benefited numerous downstream tasks, with key advantage in its ability to learn representations from unlabeled data, eliminating the need for human annotations and making it easier to scale up training datasets. Despite this advantage in utilizing diverse types of data, most research has focused on (single-)object-centric datasets like ImageNet for model development, leaving large volumes of non-object-centric (NOC) data, such as Open Images [39], SA-1B [38], LAION [61], and Ego4D [24], underutilized. However, many primary application domains of self-supervised learningsuch as robot learning (manipulation, navigation, etc.), or traditional perception tasks like object detection and image segmentationoften require handling NOC data. This motivates us to explore the potential of NOC data for self-supervised learning, which could bridge the data-domain gap between self-supervised learning and real-world applications, and is rich in information, offering new opportunities for data scaling. B.2. Why Consider Both Control and Perception? The latest development of robot learning has witnessed shift towards mulit-modal generalist models that are capable of handling diverse robot tasks, in which both control and perception are indispensable. This involves integrating PVMs (e.g., CLIP [54], T5 [56], and DINOv2 [52]) as multimodal encoders [7, 15, 37, 50, 70, 82] or explicitly utilizing foundation models (e.g., OWL-ViT [46], SAM [38], and GPT-4V [51]) as tools [23, 32, 34, 35, 49]. It is also shown that improvements in perception can lead to policies that generalize better to unseen environments using less data [64]. In accordance with this trend, recent research in PVM for robot 14 k-NN ADE Jacc k-NN ADE Jacc τt k-NN ADE Jacc Type k-NN ADE Jacc 45.3 49.1 42.2 7.8 256 46.2 49.1 43.9 9.4 512 1024 45.6 48.4 42.8 10.8 0.3 46.2 49.1 43.9 9.4 0.4 45.8 48.6 45.0 8.1 0.5 44.3 48.2 45.7 7.1 0.040.07 46.2 49.1 43.9 9.4 0.07 45.8 48.6 42.1 8.1 0.070.04 45.5 49.1 42.6 7.1 center 46.2 49.1 43.9 9.4 45.1 49.3 40.8 15.2 SH (a) Number of prototypes (b) Mask ratio (0.2) (c) Teacer temp. schedule (d) Patch loss Table 5. Ablation studies on hyperparameters. Default values are marked with cyan background. learning has also started to consider more diverse evaluation protocols [36, 45, 77]. Serving as the visual cortex of modern robot, we believe that good PVM should enhance both perception and control abilities. B.3. What Determines the Objectness of SlotMIM? We consider two quantitative metrics for the objectness of SlotMIM: 1) the Jaccard index between the attention maps and the ground truth object masks in Pascal VOC; and 2) the average number of active slots in an image during training. The former measures the alignment of the attention maps to objects, and the latter roughly measures the granularity of the concepts represented by the attention mapsthe more fine-grained the concepts are, the more parts (slots) each image is segmented into. In the main paper, we have shown that both pre-training dataset and model hyper-parameters can affect the objectness of SlotMIM. Concerning the dataset, object-centric data leads to better alignment to common objects, scene-centric data and web-crawled data lead to slightly worse segmentation quality and similar-level granularity, and ego-centric data leads to very fine-grained segmentation. Concerning the model hyper-parameters, we will show in Appendix C.1 that there are multiple factors can affect the segmentation quality and granularity. B.4. Fine-grained Slots for Manipulation Tasks? In the main paper, we have shown that while scaling up nonego-centric data for SlotMIM can improve segmentation quality and navigation/perception performance, manipulation performance drops due to over-compression. We are curious if learning fine-grained slots/concepts can improve the transferability of SlotMIM pre-trained on non-ego-centric data to manipulation tasks. To this end, we consider pretraining on COCO+ and conduct an ablation study on the number of prototypes and the use of Sinkhorn-Knopp algorithm for patch-level loss. The results are shown in Tab. 6. Increasing the number of prototypes loosens the constraints on the compactness of the slots, and the use of SH encourages more diverse utilization of the prototypes, both leading to more fine-grained slots. The results show that both methods can improve the success rate on manipulation tasks (averaged over all tasks). Explorations in this direction may resolve the inverse-scaling phenomenon on non-ego-centric Model Dataset #Proto. SH Jacc Success (%) SlotMIM COCO+ SlotMIM COCO+ SlotMIM COCO+ SlotMIM COCO+ 256 512 512 1024 7.8 42.2 43.9 9.4 40.8 15.2 42.8 10. 74.3 74.8 76.8 78.4 Table 6. Can fine-grained slots improve manipulation performance? We consider increasing the number of prototypes and using SH for patch-level loss. Both ways enforce the emergence of fine-grained slots and improve success rate on manipulation tasks. data in the main paper, which we leave for future work. B.5. Limitation and Future Work This work explores the interaction between pre-training data and algorithms for robotic manipulation, navigation, and perception-oriented tasks. With these of interest, it requires extremely intensive computation to provide complete comparison of all existing PVMs, scale pre-training data to larger scales, and evaluate more complex robotic tasks (e.g., language-conditioned manipulation and real-world tasks). The results present in the paper is thus result of tradingoff. Will SlotMIM work well for generalist robotic models like Octo [50] and OpenVLA [37]? Will the next-gen PVM for robotics be pre-trained on mixture of SA-1B [38], Ego4D [24], and/or LAION-400M [60], LVD-142M [52]? Will the preceptive module bestly to be supervised by selfsupervision, language, action trajectories, or mixture of them? There are yet much to explore, but still, we believe that the insights and methods proposed in this work can serve as stepping stone for future research in this direction. C. Extended Experiments C.1. Extended Ablation Study In Tab. 5 we present ablations on some numeric design choices. Generally speaking, smaller number of prototypes, higher mask ratio, and the use of centering [13] instead of Sinkhorn-Knopp algorithm [12] encourage the network to discover more holistic concepts/objects, while the opposite discovers more fine-grained ones. Optimal representation is highly related to object discovery quality. 15 Method Dataset Scale Kitchen MW ObjNav ImgNav EgoSoup MVP VC-1 DINOv2 LVD SlotMIM Ego4D SlotMIM DetSoup 4.6M 49.7 Ego4D+MNI 5.6M 58.1 142M 64.0 1.28M 86.0 46.7 4M 70.1 73.3 38.8 84.2 75.1 51.2 55.4 65.8 48.4 62. 64.7 67.9 59.1 65.4 69.8 Table 7. Comparison with DINOv2. SlotMIM achieves comparable or better performance on robot tasks compared to DINOv2, especially on manipulation tasks. (DINOv2 is ViT-B/14, while other models are ViT-B/16) C.2. Comparison with DINOv2 One might argue that the state-of-the-art self-supervised model, DINOv2 [52], already utilizes NOC data with vision transformer backbone. However, its success heavily depends on data curation techniques that leverage the objectcentric ImageNet dataset to select neighboring data from web-crawled data, keeping its data distribution closely tied to object-centric approaches. We also evaluate DINOv2 on the robot learning tasks considered in this paper. As shown in Tab. 7, DINOv2 does not perform as well on these tasks, possibly also due to over-compression of the representations for manipulation tasks. C.3. ImageNet Linear Probing and Fine-tuning Setting. We follow MAE [29] for details on ImageNet evaluations. For linear probing, we insert an extra BatchNorm layer without affine transformation between the features and the linear classifier. We train with batch size 4096, initial learning rate 0.1, and optimize using SGD for 90 epochs. We sweep between [CLS] token and average pooling and report the best results of pre-trained models. For fine-tuning, we train linear classifier on frozen features for 100 epochs using SGD with momentum 0.9, batch size 1024, and initial learning rate 1e-3 with cosine decay. We follow MAE [29] to adopt average pooling. For both settings, accuracy is evaluated on single 224224 crop. We first evaluate models pre-trained on 241K-scale datasets, and show that NOC data can be good learning resources if used properly. The results are present in Fig. 8. Overall, SlotMIM achieves the best performance across classification and segmentation tasks, no matter learning from object-centric data or not. Below, we discuss some other interesting findings. Features learned from NOC data can be linear separatable on ImageNet. From Fig. 8 (left), our models trained on COCO and CC achieve similarly good linear probing performance on ImageNet with best prior ImageNet-trained methods. As clear contrast, all previous methods trained on NOC datasets (COCO, CC, and Ego4D) fall behind the Figure 8. Results on ImageNet tasks. SlotMIM consistently outperforms prior arts whether pre-trained on object-centric data or not. Notably, when trained on COCO+, it transfers better than most ImageNet models despite the domain gap. best ImageNet counterpart. NOC data can be worth more than ImageNet for ImageNet. As shown in Fig. 8 (right), under ImageNet finetuning setting, the top-3 methods (BEiT, SplitMask, and SlotMIM) have the best performance when trained on COCO+ instead of ImageNet. For MAE and DINO, training on CC also transfers better than ImageNet. Note that this is uncommon given the domain gap between NOC pre-training data and OC downstream task, demonstrating that NOC data are information-rich learning resources. C.4. Scaling Up for ImageNet Tasks Figure 9. Scaling on different data sources. We scale up objectcentric, scene-centric, and web-crawled data, and highlight the best (model, data) combinations. Our method learns strong and transferable representations with significant data efficiency and continues to improve with more data. Superior data efficiency allows us to explore larger-scale pre-training data. In Fig. 9, we show that SlotMIM achieves strong performance with remarkable data efficiency. Comparable or better performance with small data scale. As shown in Figure 9, SlotMIM achieves comparable or su16 perior performance to other methods using significantly less data. Our INet-241K model for ImageNet linear probing, and COCO+/INet-241K models for ImageNet fine-tuning outperform or match most models trained on 1.28M ImageNet images across various tasks. This remarkable data efficiency demonstrates our approachs effectiveness in extracting rich, transferable features from limited data. NOC pre-training rivals ImageNet pre-training for ImageNet. Interestingly, we observe that pre-training on NOC datasets like OpenImages-1.28M can lead to performance better than pre-training on ImageNet for the ImageNet classification task (fine-tuning setting). When scaled up to 4M scale, this trend becomes more pronounced. This aligns with the trend in Fig. 8 that NOC data can provide more information-rich features, which can be better-utilized by models like SlotMIM. NOC data also possesses stronger scalability. We extend experiments to 4M scale by combining ImageNet [20], COCO+ [41], OpenImages [39], Objects365 [63], and LVIS [25]. Compared with previous efforts on scaling up with ImageNet-22K [58] (12M images), the performance of SlotMIM models continues to grow and surpasses them with 3 less data. This suggests that NOC data can be more scalable learning resource."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "The University of Hong Kong",
        "University of Edinburgh"
    ]
}