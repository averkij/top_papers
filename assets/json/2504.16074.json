{
    "paper_title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models",
    "authors": [
        "Shi Qiu",
        "Shaoyang Guo",
        "Zhuo-Yang Song",
        "Yunbo Sun",
        "Zeyu Cai",
        "Jiashen Wei",
        "Tianyu Luo",
        "Yixuan Yin",
        "Haoxu Zhang",
        "Yi Hu",
        "Chenyang Wang",
        "Chencheng Tang",
        "Haoling Chang",
        "Qi Liu",
        "Ziheng Zhou",
        "Tianyu Zhang",
        "Jingtian Zhang",
        "Zhangyi Liu",
        "Minghao Li",
        "Yuku Zhang",
        "Boxuan Jing",
        "Xianqi Yin",
        "Yutong Ren",
        "Zizhuo Fu",
        "Weike Wang",
        "Xudong Tian",
        "Anqi Lv",
        "Laifu Man",
        "Jianxiang Li",
        "Feiyu Tao",
        "Qihua Sun",
        "Zhou Liang",
        "Yushu Mu",
        "Zhongxuan Li",
        "Jing-Jun Zhang",
        "Shutao Zhang",
        "Xiaotian Li",
        "Xingqi Xia",
        "Jiawei Lin",
        "Zheyu Shen",
        "Jiahang Chen",
        "Qiuhao Xiong",
        "Binran Wang",
        "Fengyuan Wang",
        "Ziyang Ni",
        "Bohan Zhang",
        "Fan Cui",
        "Changkun Shao",
        "Qing-Hong Cao",
        "Ming-xing Luo",
        "Muhan Zhang",
        "Hua Xing Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 7 0 6 1 . 4 0 5 2 : r PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models Shi Qiu1,*, Shaoyang Guo1,*, Zhuo-Yang Song1,*, Yunbo Sun1,*, Zeyu Cai1,*, Jiashen Wei1,*, Tianyu Luo1,*, Yixuan Yin1, Haoxu Zhang1, Yi Hu2, Chenyang Wang1, Chencheng Tang1, Haoling Chang1, Qi Liu1, Ziheng Zhou1, Tianyu Zhang1, Jingtian Zhang1, Zhangyi Liu1, Minghao Li1, Yuku Zhang1, Boxuan Jing1, Xianqi Yin1, Yutong Ren1, Zizhuo Fu2, Weike Wang1, Xudong Tian1, Anqi Lv1, Laifu Man1, Jianxiang Li1, Feiyu Tao1, Qihua Sun1, Zhou Liang1, Yushu Mu1, Zhongxuan Li1, Jing-Jun Zhang1, Shutao Zhang1, Xiaotian Li1, Xingqi Xia1, Jiawei Lin1, Zheyu Shen1, Jiahang Chen1, Qiuhao Xiong1, Binran Wang1, Fengyuan Wang1, Ziyang Ni1, Bohan Zhang5, Fan Cui4, Changkun Shao1, Qing-Hong Cao1, Ming-xing Luo3, Muhan Zhang2, and Hua Xing Zhu 1School of Physics, Peking University 2Institute for Artificial Intelligence, Peking University 3Beijing Computational Science Research Center 4School of Integrated Circuits, Peking University 5Yuanpei College, Peking University"
        },
        {
            "title": "Abstract",
            "content": "We introduce PHYBench, novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on realworld physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/ ."
        },
        {
            "title": "Introduction",
            "content": "Benchmarks dont idolize or diminish models; they guide humanity and AI together toward AGI. Recent advances in reasoning models have significantly enhanced the complex reasoning capabilities of large language models (LLMs) [21, 8, 25, 7, 29]. Evaluation frameworks such as MathArena [2] have demonstrated that frontier LLMs are already capable of comprehending, modeling, and answering problems at Olympiad levels of difficulty. Despite these significant advances, existing benchmarks are still severely lagging behind in accurately assessing the nuanced ability of models to perceive and reason about the physical world. In particular, comprehensive and rigorous evaluation benchmark is still falling behind due to the following three limitations: Equal Contribution. Preprint. Under review. Figure 1: An example problem from PHYBench. We employ two main metrics to evaluate model performance: Expression Edit Distance (EED) Score and accuracy. We show the scores for three different responses, with Model Answer 1 and Model Answer 2 generated by DeepSeek-R1 and GPT-4o respectively. Oversimplified Reasoning Tasks. While numerous high-quality reasoning benchmarks have significantly advanced the evaluation and improvement of LLMs reasoning capabilities [42, 11, 14, 24, 16, 5], the rapid progress of modern models now demands benchmarks of greater complexity and discriminative power. As contemporary models achieve increasingly high performance, sometimes even rivaling human experts [16, 5], existing benchmarks are no longer sufficient. To ensure robust evaluation, it is critical to develop more challenging benchmarks that can better differentiate models based on their genuine reasoning abilities and depth of understanding [24, 40]. Excessively Abstract Problems. While existing benchmarks have established sufficient reasoning difficulty through abstract mathematical or highly specialized domains [9, 14], they often lack grounding in physical reality and real-world applications. Physical reasoning tasks that model concrete scenarios and everyday phenomena provide complementary value by assessing reasoning capabilities in physical contexts. By anchoring problems in tangible physical situations, our benchmark evaluates how well models understand and reason about the actual world [6, 37]. This physical grounding makes the benchmark particularly relevant for developing AI systems that need to operate effectively in real-world environments. Lack of Precise Evaluation Metric. Current automated evaluation methods remain inadequate for capturing the nuanced aspects of complex reasoning. While manual assessment offers deeper insights, its high cost and scalability limitations hinder widespread adoption in practical settings [24]. Existing automated approaches predominantly rely on multiple-choice formats, which significantly constrain the assessment of models true reasoning capabilities [10, 15]. Recent advancements, such as pattern-based recognition and outcome verification, provide partial solutions by evaluating reasoning processes more directly. However, these methods often employ binary (correct/incorrect) scoring, which fails to distinguish between degrees of reasoning quality or partial understanding [2, 42]. Moreover, these methods frequently rely on strict output forms, such as specific numerical values or exact textual matches, introducing bias and uncertainty [14]. These limitations hinder granular measurement of model capabilities and impose strict constraints on benchmark design. When problems are narrowly concentrated at high difficulty levels, models may exhibit near-zero performance, reducing discriminative power and sample efficiency. Binary metrics also obscure incremental improvements, as models may appear to make abrupt leaps from low to high performance upon acquiring specific skills, rather than demonstrating steady 2 progression [26]. more refined evaluation framework is needed to capture partial reasoning competence and enable smoother assessment of model development. To address these limitations, we introduce PHYBench, comprehensive, human-curated benchmark designed to rigorously evaluate models reasoning capabilities within realistic physical scenarios. PHYBench covers diverse domains within physics, using clearly defined physical expressions as answers. This ensures an accurate evaluation of models physical perception and robust reasoning capabilities. Based on the curated dataset, we further proposed the Expression Edit Distance (EED) Score, an automatic, fine-grained evaluation metric leveraging tree expression and edit distance, demonstrating how close the expressions given by LLMs are to the ground truth. In addition to evaluating language models, we established human baseline by recruiting undergraduate physics students from Peking University to solve the same problems. Our results reveal substantial performance gap between machine and human capabilities. Even the most advanced LLM we tested, Gemini 2.5 Pro, achieved only 36.9% accuracy, significantly lower than the human baseline of 61.9%, which is detailed shown in 4. Our key contributions are summarized as follows: We propose PHYBench, the first large-scale, human-curated benchmark specifically designed for detailed evaluations of models complex reasoning capabilities within physical contexts. PHYBench is executed through set of rigorous curation processes ensuring high dataset quality and eliminating any risk of contamination or data leakage. PHYBench provides comprehensive and realistic evaluation, significantly advancing and redefining the standard for assessing reasoning capabilities of Large Language Models. We introduce EED Score, novel, automated evaluation metric designed to assess the semantic similarity of general mathematical expressions based on substructure matching. This metric yields continuous score that reflects the degree of problem-solving achieved by the model, even when exact solutions are unattainable."
        },
        {
            "title": "2 Related Work",
            "content": "Large Language Models and Slow-thinking Models. Recent years have seen the emergence of complex reasoning capabilities in large language models (LLMs) [17, 18, 12, 34, 25, 39, 1, 7, 6]. Notably, the release of o1 [21] has demostrated that post-training scaling laws further elicit LLM reasoning, particularly in mathematics and programming, which further ignites our expectations for LLMs to serve as general-purpose reasoning model [8, 31, 35, 41, 38, 36]. As model capabilities grow, increasingly difficult benchmarks have emerged, including Olympiad-level math problem sets [14, 28, 24]. However, they fail to measure critical facet of intelligence: physical reasoning the ability to align models with real-world dynamics through concepts such as acceleration, forces, and energy, etc. To comprehensively evaluate the physical reasoning capabilities of LLMs, we introduce PHYBench, benchmark of high-quality physics problems of various difficulty levels. Reasoning Benchmarks. Despite the rapid development in complex reasoning, current benchmarks on reasoning abilities remain inadequate. Previous benchmarks, such as GSM-8K [5], Math-500 [16] and MMLU [5], primarily assessed the extent of models knowledge base and involved reasoning tasks limited to basic knowledge applications, simple arithmetic, or spatial relationship inference. Consequently, mainstream models tend to score very high on these benchmarks, making the tests ineffective at accurately measuring reasoning capabilities [8, 21, 25, 29]. Furthermore, the marginal effects and overfitting associated with competing for scores on such benchmarks will become increasingly severe. Recently, several benchmarks have specifically focused on assessing the reasoning skills of models in scientific thinking, understanding, and problem-solving by leveraging Olympiad or college-level problems [14, 9, 28]. However, they often rely on topics that are overly abstract and detached from real-world scenarios in order to prevent models from memorizing patterns or answers through training, which are not enough effective in guiding the meaningful development of reasoning models [27]. few small-scale datasets [24] utilize rigorous competition-style questions with detailed scoring criteria, but their limited data size lacks comprehensiveness and breadth for evaluation. To address these shortcomings, PHYBench provides comprehensive evaluation framework that includes 500 high-quality examples. PHYBench features realistic physical scenarios and covers various complexity levels, allowing for an accurate and insightful assessment of models complex reasoning abilities within physical context. 3 Table 1: The comparison between PHYBench with other reasoning benchmarks. For Difficulty Level, COMP:Olympiad-level Competition; COL:College Level; GS: Grade School; ES: Elementary School; HS: High School. For Scoring Type, ACC:Accuracy; EED: EED Score (3.3). Dataset Evaluation Metric Scoring Type Data Scale Difficulty Level Answer Type GSM8K [5] OlympiadBench [14] Olympiad-Math [28] USAMO [24] PHYBench (Ours) 8.5K 8K 200 6 500 GS COMP COMP COMP COMP,COL,HS Number Restricted LATEX Expressions Restricted LATEX Expressions Step-Wise Solution LATEX Expressions Pass@k ACC ACC Human Evaluation ACC&EED Score Binary Binary Binary Detailed Detailed Evaluation Metrics for Complex Reasoning Tasks. Traditional evaluation metrics utilizing highquality questions typically rely on multiple-choice questions or simple numerical answers, as in SuperGPQA [10] and MMLU [5]. These approaches fail to reflect the models true reasoning abilities, because multiple-choice formats can be cracked by elimination or surface-level cues without requiring detailed, step-by-step inference. Recent metrics have attempted to introduce fine-tuned LLM evaluation process, which improves refinement but does not guarantee sufficient confidence or robustness. Some recent benchmarks, such as OlympiadBench [14] assess answer consistency by restricting the form to simple numerical expressions, but this also greatly limits the selection of questions. PHYBench provides EED Score, an evaluation metric specifically designed for symbolic expressions containing multiple physical quantities, which is compatible with most physical problems. EED Score is based on the expression tree structure in Sympy, and integrated with an extended tree editing distance algorithm that further enables the evaluation to accommodate partially correct expressions beyond solely binary scoring. This clearly and in detail demonstrates the similarities and subtle differences between the generated answer and ground truth, allowing for an accurate assessment of the models genuine reasoning ability."
        },
        {
            "title": "3 The PHYBench Benchmark",
            "content": "3.1 Overview In this section, we present PHYBench, comprehensive, high-quality benchmark for measuring LLMs ability for complex reasoning within physical context. As shown in Table 1, PHYBench contains 500 carefully selected questions across diverse domains including mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics. The questions span difficulty levels from high school physics problems to undergraduate exercises and Physics Olympiad challenges. An example question is shown in Figure 1. Each question is built around specific physical scenario, requiring the model to derive symbolic expression for key physical quantity based on given conditions. All questions have definitive answers (allowing all equivalent forms, see 3.3) and can be solved through physics principles without external knowledge. The challenge lies in the models ability to construct spatial and interaction relationships from textual descriptions, selectively apply multiple physics laws and theorems, and robustly perform complex calculations on the evolution and interactions of dynamic systems. Furthermore, most problems feature long-chain reasoning. Models must discard irrelevant physical interactions and eliminate non-physical algebraic solutions across multiple steps to prevent an explosion in computational complexity. Unlike previous reasoning benchmarks that emphasize intensive reasoning or exhaustive search spaces, PHYBench focuses on realistic physical scenarios that evaluate models step-by-step physics perception and reasoning abilities. The questions are naturally accessible to human experts (with only 10% of human experts scoring below 30 on the EED metric), allowing us to clearly distinguish between models genuine physical understanding and mere computational prowess. We employ generalized expression similarity scoring function, the EED Score metric, to provide fair and consistent evaluation across the entire benchmark. 3.2 Dataset Curation Our benchmark questions are adapted from physics exercises for humans, with difficulty levels ranging from high school to college physics competitions. We engaged 178 students from the School of Physics at Peking University to contribute and refine questions. We illustrate the process of data 4 Figure 2: Pipeline of PHYBench data curation. curation in Figure 2. Our data source consist of both non-public and publicly available problems, none of which can be easily found through direct internet searches or standard reference materials. During adaptation, each problem is treated as physical scenario. Compilers identify core physical quantity as the target variable and formulate question requiring solvers to derive its symbolic expression from given conditions. For example, for the mechanics problem shown in Fig1, the solver must analyze the balls acceleration and derive the expression for the top strings tension: = 2mg + 4mv2 0/l. We impose three key requirements: Text-based formulation: All problems must be solvable purely through textual descriptions, without relying on multimodal inputs. Strict symbolic answers: Solutions must be single, unambiguous symbolic expressions (e.g., 0/l). We allow different equivalent forms of the symbolic expressions (e.g., factored or 0/l) or floating-point approximations 2mg + 4mv2 rearranged). We do not accept equations (e.g., /m 2g = v2 in answers. Unambiguous phrasing: The statements must be rigorously precise to avoid ambiguity. Then, each question undergoes multiple rounds of review, filtering, and refinement. First, these questions are deposited into the Question Bank, pool hosted on an internal platform. Then, the questions are assigned to reviewers for evaluation. The reviewers check whether the questions comply with the standards mentioned above. If not, they make modifications, and when necessary, return the questions to the contributors for further editing. During the review process, our internal platform displays several LLM-generated solutions, which reviewers can reference to determine whether the question is clearly and comprehensively phrased, revising the description until it no longer causes ambiguity. Once the reviewer approves the question and provides comments, it enters the Reviewers Library. After entering the Reviewers Library, the questions undergo multiple rounds of model testing. Based on the model answers, we determine whether the question can elicit responses from the model that meet the specified standards. We also employ GPT-4o to check the grammar and formatting of the questions. Based on these results, we further refine questions iteratively until the LLMs generate outputs that meet the format requirements for all questions in the benchmark. Finally, we invited 109 human experts to solve the questions. They further evaluate whether the questions are appropriately phrased. Based on feedback from the human experts, we further refine the questions. 5 3.3 Evaluation Metric In this section, we introduce the pipeline and details of the Expression Edit Distance Score (EED Score), our automated, model-free metric designed to evaluate the correctness of AI-generated solutions. The EED Score assesses the similarity between two symbolic expressions by computing the expression tree edit distance. This distance represents the minimum number of node-level edits (insertions, deletions, and updates) needed to transform one tree-structured expression into another. For clarity, we use \"gt\" to abbreviate \"ground truth\" and \"gen\" for \"model-generated answer\" throughout the following discussion. The calculation of the EED Score involves several steps. First, we convert both gt and gen expressions from LATEX into forms compatible with SymPy. We then verify their equivalence using the simplify() function, which transforms each expression into simplified and consistently structured format. Thus, before proceeding further with calculating the EED Score, we can use simple equivalence criterion for basic binary scoring, defined as: scoreBin = (cid:26)100, 0, if simplify(gt) simplify(gen) otherwise (1) Subsequently, we convert simplified SymPy expressions into tree structures and calculate their minimal edit distances using the extended Zhang-Shasha algorithm [3]. The final EED Score is determined by piecewise linear function based on the relative edit distance (the ratio of edit distance to the total number of nodes in the ground truth tree): = Distance(Tgt, Tgen) Size(Tgt) , score = 100 60 100r 0 if = 0(exactly match) (0 < < 0.6) > 0.6 (2) Additionally, our algorithm extends the basic three tree-editing operations by including subtree insertions and deletions. For larger subtrees (greater than five nodes), these operations are assigned reduced cost coefficient of 60%, significantly lower than performing edits node-by-node. The scoring function 60 100r is set to ensure zero scores for completely incorrect outputs and to award baseline of 60 points for nearly correct model-generated answers. The primary motivation for adopting the EED Score instead of traditional binary scoring is to better capture the degree of correctness in model solutions. Even if generated answer differs slightly, such as small miscalculation or coefficient error, it indicates partial understanding rather than complete misunderstanding of the underlying physics. More detailed explanations and illustrative examples regarding our evaluation metric are provided in Appendix A.1."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate set of representative LLMs on the PHYBench benchmark and analyze their results. 4.1 Experiment Setup To comprehensively evaluate the capabilities of current language models, we test diverse set of models, including several state-of-the-art systems as well as other widely adopted or representative models. Baseline Models. To comprehensively evaluate the capabilities of current LLMs, we test diverse set of models, including state-of-the-art models as well as other widely adopted or representative models. For API-based evaluations, we include GPT-4o[19], o1[20], Gemini 2.0 Flash Thinking[30], DeepSeek-V3[7], DeepSeek-R1[8], Qwen2.5-max[25], o3-mini[23], Grok 3 Beta[13], Claude 3.7 Sonnet[4], Claude 3.7 Sonnet Thinking[4], Gemini 2.5 pro[30], o4-mini[22], and o3[22]. The remaining models (DeepSeek-R1-Distill-Qwen-32B[8] and QwQ-32B[33]) are evaluated locally. 6 Evaluation Details. We employ two evaluation metrics, accuracy and EED Score, as detailed in Section 3.3. For API-based evaluations, we use the default hyperparameters provided by each service. The remaining models (DeepSeek-R1-Distill-Qwen-32B and QwQ-32B) are evaluated locally. For the locally run models, we set the inference parameters to temperature to 0.6, top_p to 0.95, and max_tokens to 32,768. Each model generates single reasoning trace under the specified configurations. The detailed prompt are shown in Appendix C. 4.2 Human Baseline We recruited 81 students from the School of Physics at Peking University, each of whom solved eight problems drawn from the PHYBench dataset. In total, we obtained 559 valid answer sheets corresponding to problems within the scope of the publicly released PHYBench dataset. Human performance averaged an accuracy of 61.92.1% and an EED Score of 70.41.8, where uncertainties denote the bootstrapped standard error from 10,000 resamples. At the 99% confidence level, experts significantly outperformed all evaluated LLMs on both metrics. Moreover, the upper quartile of the human score distributions reached 71.4% for accuracy and 80.4 for the EED Score. 4.3 Main Results Figure 3: Model performance on PHYBench. We show the performance of both reasoning models and general language models on PHYBench. For each model, we report two metrics, including accuracy and EED Score. Both metrics are averaged on all samples in PHYBench. We assessed several models on the PHYBench dataset, and their accuracy and EED Score are visualized in Figure 3. Our results indicate that even state-of-the-art reasoning models display limited proficiency in physics reasoning tasks. The highest-performing model, Gemini 2.5 pro, attains an accuracy of 36.9% and an EED Score of 49.5, which remains significantly below the human baseline. This disparity underscores the considerable complexity of our benchmark. Notably, while reasoning-specific models generally outperform general-purpose language models, recent general models, such as DeepSeek-V3 GPT-4.1 and Claude 3.7 Sonnet, exhibit competitive results with accuracies of 13.6%, 12.9% and 13.2% respectively. Besides, 32B models, including DeepSeek-Distill-32B and QwQ-32B, demonstrate substantially weaker performance, achieving accuracy of 2.6% and 1.2% and EED Scores of 4.5 and 3.2 respectively, which stands in stark contrast to their strong performance on other benchmarks [8, 32]. Their limited performance on PHYBench 7 Table 2: Average raw scores by model across domains. ( Abbreviations: ALL = All questions; Mech. = Mechanics; Elec. = Electricity; Thmo. = Thermodynamics; Opt. = Optics; Mod. = Modern; Adv. = Advanced.)"
        },
        {
            "title": "Model",
            "content": "ALL Mech. Elec. Thmo. Opt. Mod. Adv."
        },
        {
            "title": "49.46\nGemini 2.5 pro\n46.37\no3 (high)\n37.86\nDeepSeek-R1\n37.26\no3-mini (high)\no1\n27.44\n27.14\nClaude 3.7 Sonnet Thinking\n23.74\nGPT-4.1\n24.23\nDeepSeek-V3\nClaude 3.7 Sonnet\n23.80\n15.39\nGPT-4o\n13.94\nQwen2.5-max\n4.50\nQwQ-32B\nDeepSeek-R1-Distill-Qwen-32B 3.16",
            "content": "50.47 51.02 48.22 46.65 38.72 35.10 38.20 36.52 31.58 21.44 26.96 26.59 22.67 24.77 23.50 27.38 21.46 27.22 14.60 16.25 13.99 14.84 4.55 6.60 1.91 4.17 49.45 44.66 44.74 50.01 47.45 41.66 39.85 48.27 41.74 38.01 33.61 46.33 35.35 42.35 32.34 39.93 32.14 24.00 22.14 25.79 27.50 32.05 25.10 24.64 19.21 28.55 24.23 31.42 20.29 28.95 19.93 20.37 22.56 26.19 19.58 31.16 11.57 23.32 14.88 12.42 9.51 21.33 11.11 11.24 3.10 3.85 3.10 2.80 0.00 3.70 0.40 2.26 may be attributed to the long-horizon nature of PHYBench tasks or physical perception challenge, which go beyond the scope of conventional fast QA settings. We employ two evaluation metrics, accuracy and EED Score, on our dataset. Although the two metrics yield nearly identical rankings among models, the EED Score provides broader numerical distribution and smaller statistical uncertainties. Our bootstrap analysis (see Appendix B) reveals that the EED metric achieves an average sample efficiency enhancement of 304% with standard deviation of 80% (details in Appendix B). In other words, evaluating on 500 problems with the EED metric provides discriminatory power equivalent to on approximately 1500 problems with the accuracy metric. This improvement allows for more reliable differentiation between model performances. 4.4 Model Performance in Different Domains To better visualize the relative strengths of each model across domains, we define two metrics: the absolute advantage and the relative advantage. Let si,d be the mean score of model on domain d, and let sd = 1 (cid:88) j= sj,d be the mean across all models in that domain. We then introduce Ai,d = si,d sd, Ri,d = Ai,d sd , (absolute advantage) (relative advantage) (3) (4) Notably, the relative advantage is computed after normalization with respect to the domain average, which mitigates the disproportionate impact of individual problems with exceptionally low average scores. We further analyzed the distribution of model scores across different domains of physics, presenting the results of some typical models in Table 2. Among our detailed domains, mechanics emphasizes spatial reasoning and understanding of dynamics. Electromagnetism assesses models ability to comprehend field distributions and spatial segmentation. Thermodynamics tests the models proficiency in understanding multi-physics interactions and complex processes. Optics also stresses spatial reasoning skills. Modern and advanced physics questions require deep understanding and application of intricate physical concepts. Our analysis reveals that traditional models (e.g., GPT-4o) perform relatively poorly in mechanics, likely due to their limited training on data involving three-dimensional geometry and dynamics. 8 Reasoning-specialized models like o1 and QwQ-32B perform relatively well in mechanics. Thermodynamics problems typically involve multi-step reasoning and complex process analysis, where we observe clear performance gap between reasoning and non-reasoning models. The results in modern and advanced physics also suggest differences in the depth of knowledge across LLMs. GPT-4.1, general-purpose model, demonstrates notable advantage over GPT-4o, o1, and DeepSeek V3, underscoring its superior ability in scientific problem-solving tasks. (a) Model performance in different domains. (b) Model performance on different difficulties. Figure 4: Model Advantage on Different Problems The relative advantage metric highlights each models strengths in given domain compared to its peers. The radar plot in Figure 4a shows the relative advantage of some typical models, providing clear visualization of the distribution of model strengths discussed above. Figure 4b shows the distribution of model advantage across varying levels of problem difficulty, characterized by the average score of all models sd on the problem. Notably, the Gemini model exhibits significant advantage on difficult problems (sd 30), suggesting its superior ability to tackle challenging problems. In contrast, on easier questions (those with higher average scores), Gemini shows little to no advantage over o3-mini (high) and DeepSeek R1, suggesting that todays state-of-the-art inference models perform essentially similarly in less challenging situations."
        },
        {
            "title": "5 Error Analysis",
            "content": "This section provides detailed analysis of the errors found during our assessment. We categorize the capabilities assessed by the PHYBench benchmark into two key dimensions: Physical Perception (PP) and Robust Reasoning (RR). To illustrate these categories, we analyze the reasoning processes of large language models (LLMs), specifically DeepSeek-R1. As demonstrated in the following example, the reasoning of DeepSeek-R1 unfolds in two distinct phases, corresponding exactly to of PP and RR. Errors associated with these phases are consistently observed across various models, highlighting the representativeness of these two categories. Additionally, the proposed EED metric effectively quantifies model performance in both PP and RR aspects. 9 Example Reasoning Process Physical Perception (PP): First, need to understand the entire systems initial state and . . . should draw sketch. . . . the tension is continuous, but still have to analyse each balls forces one by one. . . . the strings havent had time to swing yet. The top balls sudden horizontal motion requires centripetal force . . . Robust Reasoning (RR): From equation (3): T3 mg = a1r, so T3 = mg + a1r. Substitute into equation (2): T2 (mg + a1r) mg = a1r, which becomes T2 mg a1r mg = a1r, Substitute the expression for T2: T1 = (2mg + 2m a1r) + mg + a1r = 3mg + 3m a1r 5.1 Understanding Model Reasoning Processes When examining solution traces across multiple models, we observe that reasoning chains exhibit characteristic structures that align precisely with our two identified challenge categories: Physical Perception: During this phase, models engage in intensive semantic reasoning, expending significant cognitive effort to identify relevant physical objects, variables, and dynamics. Models make qualitative judgments about which physical effects are significant and which can be safely ignored. PP manifests as critical decision nodes in the reasoning chain. Robust Reasoning: In this phase, models produce numerous lines of equations and perform symbolic reasoning. This process forms the connecting chains between perception nodes. RR involves consistent mathematical derivation, equation solving, and proper application of established conditions. From structural perspective, PP represents critical decision nodes while RR forms the connecting links in the reasoning chain. Errors at PP nodes can lead to fundamental misunderstandings of the physical scenario, resulting in incorrect answers. They may also introduce unnecessary physical effects, complicating subsequent symbolic reasoning. Meanwhile, RR errors involve inconsistencies in deriving expressions, solving equations, or applying conditions, which accumulate and cause the final expression to increasingly diverge from the correct answer. Figure 5: Examplar questions and errors. The errors are from the solution generated by DeepSeek-R1. Here we demonstrate the main parameters and physical process. For whole question text, please refer to Appendix D. 10 5.2 Physical Perception The first typical challenge arises from insufficient understanding of physical processes and inadequate modeling skills. As illustrated in Fig. 5, Example Problem 1 presents classical mechanics scenario involving three balls connected by an inextensible string. The erroneous solution from the LLM results from misunderstanding of the kinematics relationships among these balls, particularly perceiving the angular velocity of the middle string to be zero incorrectly. Even if the symbolic derivation is right, the model results in wrong answer. The PP challenge in this problem is easy for average college students, but even cutting-edge models like Gemini-2.5-Pro, o3 and DeepSeek-R1 failed to handle this kinematics. Our experiments further reveal that 32B models perform especially poorly on PP phases, often failing even on elementary problems. Such failures highlight not only fundamental limitation in the models perception capacity but also semantic reasoning. The PHYBench benchmark is deliberately designed to distinguish models with true physical understanding from those that rely merely on memorization or pattern recognition. 5.3 Robust Reasoning Another common error involves maintaining consistency across lengthy and intricate reasoning processes, as well as difficulties in accurately solving the resulting equations. For instance, in Fig. 5, Example Problem 2 presents scenario where mirror, moving at relativistic speed, is recoiled by high-energy photon. Although the LLM correctly interpreted the physical setup and identified the appropriate equations, it ultimately failed to derive the correct solution after an extended symbolic derivation. This reflects typical lack of robustness in mathematical reasoning. Physics problems often require extensive symbolic manipulation. Due to space limitations, the two illustrative problems shown are relatively short; however, as noted earlier, the average length of full solution in PHYBench is approximately 3,000 characters, and human solvers typically employ dozens of intermediate expressions before arriving at the final answer. Moreover, LLMswhen unaided by external mathematical toolstend to generate significantly more intermediate steps than human reference solutions, bringing more risks of making mistakes. This observation suggests that physics problems effectively represent long-range reasoning tasks constrained by diverse but definite rules. Our experimental results indicate that such long-range symbolic reasoning remains significant challenge for current models. 5.4 Capturing Errors with the EED metric The EED Score effectively captures errors in both PP and RR phases. PP errors typically result in missing or extraneous terms in the intermediate processminor errors lead to coefficient mistakes, while major ones can add or omit entire terms, significantly reducing the EEDS. RR process errors cause transitional changes in expressions that EEDS captures smoothly. The analysis of these problems within the PHYBench framework underscores its effectiveness in evaluating both physical perception and logical reasoning abilities of AI models. By providing realistic and complex physical scenarios, PHYBench challenges models to demonstrate their true understanding and reasoning proficiency. This benchmark not only advances the standard for assessing AI reasoning capabilities but also guides the development of more sophisticated models capable of comprehending and interacting with the physical world. The analysis of these problems within the PHYBench framework reveals the benchmarks core focus. By presenting realistic and complex physical scenarios, PHYBench assesses reasoning skills that go beyond traditional physics derivation. These two capabilitiesPP and RRserve as general indicators of an AI models ability to perform diverse rule-based reasoning, which is central not only to physics, but also to broader scientific and engineering tasks. They reflect the models competence in perceiving abstract structure from real-world information and in executing long-range jobs. This benchmark not only sets new standard for assessing reasoning capabilities in AI systems, but also provides guidance for the development of more advanced models capable of meaningful interaction with the physical world. We argue that for AI to operate effectively in real-world environments, it must go beyond imitation-based learning from videos or text and beyond the accumulation of expert-level domain knowledge. Equally essential is the models ability to autonomously construct internal representations and derive reliable physical relationships through principled reasoning."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces PHYBench, comprehensive benchmark featuring 500 meticulously curated problems aimed at assessing reasoning capabilities within physical contexts. PHYBench covers multiple physics domains and difficulty levels, focusing on step-by-step physics perception and robust reasoning. PHYBench offers distinct metric for evaluating the genuine reasoning capabilities of models, mirroring real-world scientific and engineering challenges. The proposed EED score further allows for nuanced assessment of symbolic expressions, identifying subtle distinctions in reasoning processes. Our experimental results demonstrate that even state-of-the-art LLMs achieve results significantly lower than those of human experts. This considerable gap underscores fundamental limitations in current models abilities, specifically in their physical perception and robust reasoning, thereby highlighting the challenges and opportunities that lie ahead in this domain."
        },
        {
            "title": "7 Contributions and Acknowledgements",
            "content": "PHYBench was constructed with strong support from the School of Physics at Peking University, Ministry of Education Physics 101 Plan, and National Science Foundation of China under contract No. 12425505, 12235001, U2230402. In total, more than hundred students in the School have participated in this project and made valuable contributions. The PHYBench project aspires to lead the development of LLM by using high-quality physics benchmarks and data-driven to reveal the nature of AIs understanding and reasoning in the physical world and in the face of complex problems. Our team members contribute to the development of PHYBench from the following perspectives: Research Pipeline Construction Data Annotation Data Quality Inspection Model Evaluation Result Analysis Paper Writing Core Contributors Shi Qiu Shaoyang Guo Zhuo-Yang Song Yunbo Sun Zeyu Cai Jiashen Wei Tianyu Luo Contributors Xianqi Yin Yutong Ren Zizhuo Fu Weike Wang Xudong Tian Laifu Man Jianxiang Li Feiyu Tao Xiaotian Li Xianqi Xia Yixuan Yin Haoxu Zhang Yi Hu Chenyang Wang Chencheng Tang Haoling Chang Qi Liu Jiawei Lin Zheyu Shen Jiahang Chen Qiuhao Xiong Binran Wang Fengyuan Wang Ziyang Ni Bohan Zhang Fan Cui Changkun Shao 12 Ziheng Zhou Tianyu Zhang Jingtian Zhang Zhangyi Liu Minghao Li Yuku Zhang Boxuan Jing Bozu Zhang Lixiang Tang Zekai Zhao Heyun Zou Zan Lou Yizhe Tian Chenxu Yu Wenshuai Liu Dihang Sun Hanyu Cao Yuchen Lu Haoyu Mo Shuran Yang Qianyi Wang Zhiyuan Zhou Yuxin He Anqi Lv Yifan Shi Zijian Wang Jinyu Zhou Zhiji Feng Xinlin Zhu Yixin Liu Zihan Tang Boqian Yao Jiawei Chen Tianxing Huang Boxun Yu Zihao Xu Rundong Liu Xuqi Jiang Haoxiang Li Wei Yan Aoqin Liang Zirui Peng Tianxiao Li Jiarui Tang Yuyang Weng Chen Huang Yiwei Deng Qihang Li Yuntian Xie Chengkai Sheng Xianhong Zeng Yizhe Zheng Bowen Yu Chengzhou Wu Mengyao Zhang Houcheng Li Peilin Li Yuyang Zhao Bingru He Zongyue Hou Jiajun Yan Lingrui Zhang Jianyuan Luo"
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report, 2023. URL https://arxiv.org/abs/2309. 16609. [2] Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https://matharena.ai/. [3] David T. Barnard, Gwen Clarke, and Nicholas Duncan. Tree-to-tree correction for document trees: Technical report 95-372. Technical report, Dept. of Computing and Information Science, Queens University, Kingston, ON, Canada, 1995. [4] claude. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/ claude-3-7-sonnet, 2025. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/ abs/2110.14168. [6] DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism, 2024. URL https://arxiv.org/abs/2401.02954. [7] DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412. 19437. [8] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [9] Long Phan et al. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. [10] Team et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. URL https://arxiv.org/abs/2502.14739. [11] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. 13 [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [13] grok. Grok 3 beta the age of reasoning agents. https://x.ai/news/grok-3, 2025. [14] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https: //arxiv.org/abs/2009.03300. [16] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. [17] OpenAI. Introducing chatgpt, 2022. https://openai.com/blog/chatgpt. [18] OpenAI. Gpt-4 technical report, 2023. [19] OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [20] OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [21] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. [22] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, 2025. [23] OpenAI. Openai o3-mini: Pushing the frontier of cost-effective reasoning. https://openai. com/index/openai-o3-mini/, 2025. [24] Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovic, Nikola Jovanovic, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad, 2025. URL https://arxiv.org/abs/2503.21934. [25] Qwen Team. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [26] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage?, 2023. URL https://arxiv.org/abs/2304.15004. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. [28] Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models, 2025. URL https://arxiv.org/abs/2503.21380. [29] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. [30] Gemini Team. Gemini: family of highly capable multimodal models, 2024. URL https: //arxiv.org/abs/2312.11805. [31] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv. org/abs/2501.12599. [32] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. 14 [33] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. [35] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel M. Ni, Linyi Yang, Ying Wen, and Weinan Zhang. Openr: An open source framework for advanced reasoning with large language models, 2024. URL https://arxiv.org/abs/2410.09671. [36] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei LÃ¼, Rui Hu, et al. Skywork: more open bilingual foundation model, 2023. URL https://arxiv.org/abs/2310.19341. [37] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. URL https://arxiv.org/abs/2501.09686. [38] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems?, 2025. URL https://arxiv.org/abs/2501.11284. [39] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. [40] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. [41] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions, 2024. URL https://arxiv.org/abs/2411.14405. [42] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: cross-system benchmark for formal olympiad-level mathematics, 2022. URL https://arxiv.org/abs/2109.00110. Appendix / supplemental material of PHYBench"
        },
        {
            "title": "A Evaluation Metric",
            "content": "A.1 Tree Editing Distance Algorithm In this section, we present detailed illustration of our EED scoring metrics operational pipeline and fundamental principles. component from the input string-formatted LATEX The pipeline initiates by extracting the final expression. Subsequently, series of preprocessing procedures (e.g., removing formatting commands and complete begin...end environments) are applied, normalizing non-standard LATEX expressions to parser-compatible form. Next, we utilize Python library called latex2sympy_extended to translate the normalized LATEX into symbolic expression compatible with SymPy. For computational efficiency during simplification, we assume all symbolic variables to be positive. The simplify() function is then applied individually to both the ground truth (gt) and generated (gen) expressions. 15 solution is considered fully correct if the simplified gt and gen expression are equivalent, which is checked through equals method. However, unlike conventional benchmarks that employ binary scoring based on final results, our EED scoring propose partial credit mechanism to better reflect solution correctness in symbolic mathematics. For detailed illustration, consider an electromagnetic problem where the ground truth is: (cid:115) = n2 2 n2 1 + 1 2 4mQ ÏÏµ0a3q (5) Two incorrect generated answers may demonstrate fundamentally different understanding levels: Coefficient error: = (cid:113) n2 2 n2 1 Structural error: = ÏQq n1n2a + 1 2 2mQ ÏÏµ0a3q The former preserves the solutions physical essence with minor computational errors, while the latter indicates fundamental misunderstanding. To quantify this distinction, we implement an extended tree editing distance metric for similarity assessment. In SymPys expression tree representation, fundamental mathematical components (constants, variables, operators, functions) constitute tree structure. Following the conversion of SymPy expressions into trees, we calculate the minimum editing distance between gt and gen trees through sequence of basic node operations (insertions, deletions, and updates) with specific cost. This edit distance metric effectively quantifies structural dissimilarity between expressions. The implementation leverages the dynamic programming-based Zhang-Shasha algorithm, which exhibits time complexity of O(n1n2d1d2) and space complexity of O(n1n2) where n12, d12 denote the node count and maximum depth of respective trees. For our specific expression tree editing problem, these computational requirements remain entirely acceptable compared to the time cost of simplify() method. Figure 6: An example of expression tree editing from 2L sin Ït to 2L sin 2Ït. Where numbers, variables, functions and fundamental binary operations are regarded as tree nodes. An typical editing contains some insertion,deletion and updates. This figure shows the case of inserting coefficient 2 The score is then determined by the \"relative editing distance\", i.e., the ratio of the editing distance to the tree size (the number of tree nodes). If any error occurs during the aforementioned formatting, conversion, or computation procedures, the returned score will be set to zero due to the models incorrect input format - phenomenon particularly prevalent among those 32B distilled models. = Distance(Tgt, Tgen) Size(Tgt) , score = 100 60 100r 0 if = 0(exactly match) (0 < < 0.6) > 0.6 (6) Additionally, in real scenarios,a physical result can be factorized into sum or product of several terms or factors with different physical meanings. For instance,a standard formulation for electric potential typically comprises three principal components: an external field term, charge distribution term, and an electric dipole moment term, each representing distinct physical contributions to the overall potential field. 16 Figure 7: An Example of removing subtree \"cluster\"(blue boxed subtree) corresponding to an electric dipole moment contribution.. We introduce cluster editing discount to reduce cost of such an operation since it corresponds to whole physical components. (r) = E0r cos Î¸ + 4ÏÏµ0r + cos Î¸ 8ÏÏµr2 (7) We then introduce cluster editing discount to quantify the correctness of physical components. If AIs answer ignores some components but writes down other components correctly,its score is expected to be higher for its correct calculation on some discrete parts of the overall contribution. Consequently, the \"clustered mistakes\",which often relate to whole component, should have discount on its total insertion or deletion cost. For this reason, our tree editing algorithm is extended with two additional operations:inserting and removing subtree. We set the cost function of inserting or removing subtree with size to be: Cost(InsertTree(T ), DeleteTree(T )) = min(x, 0.6(x 5) + 5) (8) The formula degenerates back to original cost for 5, reducing the computational expense of term deletion and insertion operations while ensuring the corresponding score remains zero when the entire formula is either deleted or inserted. This mechanism can be implemented through an extended Zhang-Shasha algorithm, preserving identical time and space complexity characteristics as the original algorithm. A.2 Qualitative Interpretations for Advantages of EED Score Traditional binary scoring based on solely correctness fails to adequately reflect models capability when tasks are either too difficult or too easy, as the scores tend to cluster at either extremely high or extremely low levels in such cases,while our EED scoring addresses this problem. Such phenomenon can be qualitatively illustrated through simple model. Considering quantifying models physical ability and problem difficulty using real-valued parameter and respectively. The corresponding score = (a d) is then determined by function of their difference. In the case of binary scoring, the system operates under an all-or-nothing principle: the model receives full credit only when its ability strictly exceeds the problems difficulty threshold (i.e., > d). Otherwise, it scores zero. This scoring function can be mathematically represented using the Heaviside step function: fBIN(x) = Î¸(x) = (cid:26)1 0 ifx 0 otherwise (9) 17 For our EED scoring, even if the model answer is incorrect, but partially correct answer still can get non-zero score,which can be approximately described as linear function. (cid:26) fEED(x) = max (0, 0.6 + 0.01x) ifx 0 otherwise (10) In benchmark setting, problem difficulty typically follows Gaussian distribution with specific center and variance parameters. The relationship between model score and its ability can be expressed as the convolution of the scoring function and the difficulty distribution function within fundamental calculation. Furthermore, benchmarks capacity to differentiate model abilities, referred to as \"discrimination\", can be characterized by the derivative of the score-ability function. The numerical results are demonstrated below. S(a) = fscore Ndiff(Âµ, Ï2), Dis = dS(a) da (11) Consequently, when model ability falls significantly below average difficulty (i.e., in the low-score region), the binary scoring yields exponentially diminishing expected scores due to extremely low correct rates. However, implementing partial correctness evaluation mechanism based on our EED scoring significantly enhances both discrimination value and linearity in this region, offering higher information capacity. Figure 8: An illustration of the superiority of EED scoring in comparison to Binary scoring for its better linearity at low-score region. The qualitative analysis above elucidates the rationale behind the EED scoring metrics capacity for precise model capability assessment by enabling structural dissimilarity quantification - property empirically validated through statistical analysis in Appendix B."
        },
        {
            "title": "B Statistical Analysis",
            "content": "We employed bootstrap analysis with 1000 resamples to evaluate the statistical uncertainty of our main results under the two metrics. The results are show in Table 3. While the ranking of models remains consistent across both metrics, the EED Scores demonstrate higher absolute values and smaller relative uncertainties compared to the accuracy-based metric. The relative uncertainty is proportional to the square root of sample size, allowing us to quantify the sample efficiency of the EED metric relative to the accuracy metric using the following formula: Sample Efficiency = (cid:0) CVACC CVEED (cid:1)2 . (12) Table 3: Performance of models on EED and accuracy metrics. (Notation: SEED = EED Score; ÏEED = EED Std Dev; CVEED = ÏEED/SEED 100% ; ACC = Accuracy; ÏACC = Accuracy Std Dev; CVACC = ÏACC/SACC 100%; Efficiency = (CVACC/CVEED)2 ).) Model SEED ACC ÏEED ÏACC CVEED (%) CVACC (%) Efficiency Gemini 2.5 pro o3 (high) o4-mini (high) DeepSeek-R1 o3-mini (high) o4-mini o3-mini Grok 3 Beta Gemini 2.0 Flash Thinking o1 Claude 3.7 Sonnet Thinking GPT-4.1 DeepSeek-V3 o3-mini (low) Claude 3.7 Sonnet GPT-4o Qwen2.5-max QwQ-32B DeepSeek-R1-Distill-Qwen-32B 49.40 36.65 46.30 34.58 41.95 29.33 37.78 24.88 37.22 24.92 36.44 24.77 33.21 21.13 31.94 21.09 30.25 17.93 27.46 10.72 27.12 15.25 23.71 13.18 24.17 13.45 25.34 8.13 23.73 12.78 6.89 15.35 6.03 13.92 1.58 4.54 0.70 3.19 1.71 1.72 1.68 1.59 1.57 1.66 1.59 1.56 1.48 2.03 1.44 1.44 1.39 1.85 1.35 1.11 1.04 0.94 0.71 1.97 1.91 1.83 1.71 1.69 1.72 1.65 1.59 1.51 1.27 1.43 1.41 1.38 1.13 1.34 1.04 0.96 0.51 0. 3.47 3.71 4.01 4.20 4.21 4.54 4.79 4.90 4.88 7.40 5.30 6.07 5.75 7.29 5.71 7.26 7.44 20.77 22.30 5.38 5.53 6.25 6.87 6.77 6.95 7.79 7.53 8.40 11.86 9.40 10.68 10.27 13.88 10.46 15.12 15.83 32.26 49.56 240.79% 221.48% 242.84% 267.24% 258.06% 233.88% 264.18% 236.67% 296.31% 257.09% 314.68% 309.90% 318.79% 362.12% 335.79% 434.02% 452.20% 241.21% 493.72% Table 4: Pairwise Advantage Confidence. (Notations: Each block is confidence level of the claim that the row model outperforms the column model in PHYBench. The OpenAI o-series are with reasoning effort=high Model Model Gemini-2.5-pro o4-mini DeepSeek-R1 o3-mini GPT-4.1 DeepSeek-V3 GPT-4o Gemini-2.5-pro o3 (high) o4-mini (high) DeepSeek-R1 o3-mini (high) GPT-4.1 DeepSeek-V3 GPT-4o 50% 10% 0% 0% 0% 0% 0% 0% 90% 100% 50% 96% 4% 50% 4% 0% 2% 0% 0% 0% 0% 0% 0% 0% 100% 100% 96% 50% 40% 0% 0% 0% 100% 100% 100% 100% 100% 98% 100% 60% 100% 50% 50% 0% 59% 0% 0% 0% 100% 100% 100% 100% 100% 41% 50% 0% 100% 100% 100% 100% 100% 100% 100% 50% As shown in Table 3, our analysis reveals that the EED metric yields an average sample efficiency enhancement of 204% (Ï = 80%). This indicates that our benchmark under the EED metric with 500 problems provides evaluation strength equivalent to that under the accuracy metric with approximately 1000 problems, representing substantial improvement in evaluation efficiency. To establish the statistical significance of performance differences between models, we calculated pairwise advantage confidence levels. Using the scores and their associated uncertainties, we determined our confidence in asserting that one model outperforms another on PHYBench. The confidence level is calculated using Gaussian estimation: CLsi>sj = Î¦( (cid:113) Ësi Ësj Ï2 Ësi + Ï2 Ësj ). (13) Notably, Gemini-2.5-Pro demonstrates superior performance with high confidence over most models, showing 99% confidence of outperforming all other models except o3 (90%). Table 4 also reveals clear performance tiers among the evaluated models, with statistically significant separations between the top performers (Gemini-2.5-Pro, o3 and o4-mini), mid-tier models (DeepSeek-R1, o3-mini), non-reasoning models (GPT-4.1, DeepSeek-V3) and legacy non-reasoning models (GPT-4o)."
        },
        {
            "title": "C Prompt Template",
            "content": "All models are queried with the following unified prompt template: You are physics expert. Please read the following question and provide step-by-step solution. Put your final answer, which must be readable LaTeX formula, in boxed{} environment. Question: {problem from PHYBench} Answer: The final answer is then automatically extracted from within the boxed{} environment. We ignore any extra output outside the box, retain only the inner LaTeX expression, and tolerate additional text or commands inside the box as long as exactly one expression appears."
        },
        {
            "title": "D Example Questions",
            "content": "D.1 Full Question Text for Given Errors in Fig5 Example Problem 1: Three small balls are connected in series with three light strings to form line, and the end of one of the strings is hung from the ceiling. The strings are non-extensible, with length of l, and the mass of each small ball is m. Initially, the system is stationary and vertical. hammer strikes one of the small balls in horizontal direction, causing the ball to acquire an instantaneous velocity of v0. Determine the instantaneous tension in the middle string when the topmost ball is struck. (The gravitational acceleration is g.) Example Problem 2: Consider an ideal mirror moving at relativistic velocity, with mass and area S. (The direction of photon incidence is the same as the direction of the mirrors motion.) Now consider the case where the mirror is moving with an initial velocity Î²0c. In this situation, the mirror is unconstrained by external forces, and photons are incident on it with constant power for certain period of time, with energy E. Assuming the mirrors velocity after irradiation is Î²1c, find the expression for Î²1. D.2 Demonstration of Selected Problems We demonstrate 5 additional problems with their answers. For more detailed information, please refer to the PHYBench website. Selected Problem 1 smooth bowl with radius of is fixed, and the plane at the mouth of the bowl is horizontal. smooth, homogeneous, thin rod AB with length = 4 . is located outside the bowl, while end presses against point inside the bowl. The rod achieves static equilibrium in plane passing through the center of the sphere O. Points and on the rod are nearly coincident with the point of contact at the rim of the bowl, but is slightly lower-left, and is slightly upper-right. Let the angle between the rod and the horizontal plane be Î¸.The rod is suddenly cut at point D. Note that after being cut, point will gently rest on the inner surface of the bowl. Find the angular acceleration Î² = Î¸ of the rod at this instant. Answer: 3R 3 Î² = 2R Selected Problem 2 Consider child with mass sitting on swing, the child can be regarded as point mass with the mass concentrated at the seat plank. Ignore the mass of the other parts of the system. The distance from the swing seat plank to the pivot is l. At this time, consider the frictional torque Mf = (where is constant) at the swings suspension point. There is someone behind who applies an impulsive torque J0 to the swing every time it reaches the furthest back position. Find the difference in speed rates of the child after passing the lowest point twice successively when the motion reaches steady state (with gravitational acceleration and assuming the swing angle is relatively small). Answer: (cid:115) = gl (cid:18) 2 8aml2 + (cid:19) (cid:115) ( mgl J0 8aml2 + 3a mgl (cid:115) 2 J0 8aml2 mgl ) Selected Problem 3 Consider an infinite-length black body with inner and outer cylinders, which are in contact with heat sources at temperatures T1 and T2, respectively; assume that the temperature of the heat sources remains constant. Let the inner cylinder have radius r, the outer cylinder have radius R, and the distance between the axes of the inner and outer cylinders be b, with < < and + < R. Find the power p(Î¸) absorbed per unit area from the heat source at angle Î¸ on the surface of the outer cylinder (i.e., the power density at Î¸), where Î¸ is the angle between the line connecting point on the surface of the outer cylinder and the center of the outer cylinder, and the line connecting the centers of the inner and outer cylinders. The Stefan-Boltzmann constant is denoted as Ï. Answer: p(Î¸) = (ÏT 4 2 ÏT 4 1 ) r(R cos Î¸) R2 + b2 2Rb cos Î¸ Selected Problem 4 square loop with side length and mass is made from resistive material, with total resistance of R. At = 0, the loop is located at = 0 and moves with velocity v0 Ëx. The loop lies in the x-y plane. There is magnetic field = B0 Ëz, where B0 > 0 is constant. In this problem, we ignore the effects of gravity. What is the velocity v(t) of the square loop at time t? Write the expression for v(t) in terms of using the parameters B0, v0, a, m, and R. Answer: (cid:16) x0 (cid:17) v(t) = v0e 1 mR (cid:18) (cid:19)2 a2B0 x0 Selected Problem 5 For the electromagnetic cannon model, its structure consists of two parallel rails spaced apart, with one end connected to power supply for energy, and the other end connected to metal rod that can slide freely on the rails to form circuit. In the situation where the circuit length is much larger than the spacing (but ignoring the delay in circuit signal propagation caused by the length), it can be assumed that the self-inductance coefficient of the circuit is linearly related to x, i.e., = Ax + B. and are two constants. The current flowing through the metal rod is I, and the permeability of vacuum is Âµ0. In fact, for different electromagnetic cannon configurations, the value of the Ampere force on the metal rod is actually different. Assume the rail is thin-walled cylinder with radius . Under direct current conditions, it can be assumed that the current is uniformly distributed over the surface of the cylinder. Make an appropriate approximation and calculate the specific expression of the Ampere force on the metal rod. Answer: Âµ0I 2 2Ï ln r"
        }
    ],
    "affiliations": [
        "Beijing Computational Science Research Center",
        "Institute for Artificial Intelligence, Peking University",
        "School of Integrated Circuits, Peking University",
        "School of Physics, Peking University",
        "Yuanpei College, Peking University"
    ]
}