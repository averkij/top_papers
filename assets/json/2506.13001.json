{
    "paper_title": "Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV",
    "authors": [
        "Christian Zhou-Zheng",
        "Philippe Pasquier"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing work in automatic music generation has primarily focused on end-to-end systems that produce complete compositions or continuations. However, because musical composition is typically an iterative process, such systems make it difficult to engage in the back-and-forth between human and machine that is essential to computer-assisted creativity. In this study, we address the task of personalizable, multi-track, long-context, and controllable symbolic music infilling to enhance the process of computer-assisted composition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear architecture, to enable efficient and coherent musical cocreation on edge devices. We also demonstrate that MIDI-RWKV admits an effective method of finetuning its initial state for personalization in the very-low-sample regime. We evaluate MIDI-RWKV and its state tuning on several quantitative and qualitative metrics, and release model weights and code at https://github.com/christianazinn/MIDI-RWKV."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 1 0 0 3 1 . 6 0 5 2 : r Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV Christian Zhou-Zheng Metacreation Lab, Simon Fraser University christianzhouzheng@gmail.com Philippe Pasquier Metacreation Lab, Simon Fraser University pasquier@sfu.ca"
        },
        {
            "title": "Abstract",
            "content": "Existing work in automatic music generation has primarily focused on end-to-end systems that produce complete compositions or continuations. However, because musical composition is typically an iterative process, such systems make it difficult to engage in the back-and-forth between human and machine that is essential to computer-assisted creativity. In this study, we address the task of personalizable, multi-track, long-context, and controllable symbolic music infilling to enhance the process of computer-assisted composition. We present MIDI-RWKV, novel model based on the RWKV-7 linear architecture, to enable efficient and coherent musical cocreation on edge devices. We also demonstrate that MIDI-RWKV admits an effective method of finetuning its initial state for personalization in the very-low-sample regime. We evaluate MIDI-RWKV and its state tuning on several quantitative and qualitative metrics, and release model weights and code at https://github.com/christianazinn/MIDI-RWKV."
        },
        {
            "title": "Introduction",
            "content": "Music is millennia-old art form that provides means of artistic and personal expression across cultures. Its composition often requires substantial human effort, prompting recent work on foundation models for music generation [1]. Many of these works formulate music generation as sequence modeling task and leverage the autoregressive Transformer [2] to achieve remarkable performance. However, critical gap remains between technical capabilities and practical compositional needs. Despite the advancements made in music generation, most systems remain difficult for composers to actively use in their creative workflow. Autoregressive symbolic music models [36] allow manipulation of their outputs in digital audio workstations (DAWs), but are unable to regenerate parts of compositions. Symbolic music infilling models [713] allow selective regeneration of musical segments, but have limited context windows that prevent the capture of long-range dependencies that are common in music, and struggle with controllability and personalizability [14]. Feedback from the 2020 AI Song Writing Contest [15] agrees that many existing systems are insufficiently steerable or context-aware for composers to use smoothly. In this work, we propose MIDI-RWKV, symbolic music infilling model designed for the above specifications of long-context awareness, selective regeneration, controllability, and personalizability. We employ the RWKV-7 architecture [16] to capture significantly longer-range dependencies than past work. We adapt the Bar-Fill representation of Ens and Pasquier [7] to selectively regenerate sections at longer context lengths, and incorporate numerical attribute controls to condition generation on select musical attributes. We train on the diverse GigaMIDI dataset [17] to create well-rounded foundation model, and demonstrate that it admits novel finetuning/personalization method that surpasses existing methods in the small-data regime in which we expect most composers to be. We evaluate our model and finetuning method on objective metrics and subjective listening test. The main contributions of our work are as follows: Preprint. Under review. Table 1: comparison of recent existing multi-track symbolic music infilling systems. Context lengths differ in units reported and tokenization used: superscript 1 uses REMI [24], 2,3 are custom. Single section infilling refers to the top prompt format in Figure 1. System Number of Tracks Fixed Schema MIDI-RWKV MIDI-Mistral [13] MIDI-GPT [8] MMM [7] CA [9, 10] MusIAC [11] Any Any Any Any Any 3 no no no no no yes Maximum Context Length 1 8192 tokens1 2048 tokens1 2048 tokens1 3300 tokens2 16 bars3 Attribute Controls Single Section Infilling yes no yes density only yes yes yes yes no no no no We introduce MIDI-RWKV, symbolic music infilling model based on the RWKV-7 architecture that enables more context-aware, controllable computer-assisted music generation. We demonstrate that our novel state tuning approach for MIDI-RWKV outperforms both the base model and LoRA fine-tuning in downstream task in the low-sample regime. We evaluate our model on quantitative and qualitative metrics to show that our model outperforms existing approaches across the board on long-context infilling objectives."
        },
        {
            "title": "2 Related work",
            "content": "Symbolic music infilling. Musical infilling, or inpainting in reference to the corresponding imagedomain task [18], refers to the general objective of reconstructing musical information from surrounding context. The objective of autoregressive music generation/continuation, as in von Rütte et al. [3], Yu et al. [4], Lu et al. [5], and Huang et al. [6], can be viewed as special case of infilling without future context. Early approaches to infilling leveraged Markov chain Monte Carlo methods [19], variational autoencoders [20], convolutional [21] and recurrent [22] neural networks, and Transformers [23] to infill single tracks with light conditioning. Longer-context, multi-track, and more controllable models have by and large used Transformers. Table 1 contains comparison of these models. MMM [7] infills across an arbitrary number of tracks, subject to bar count limits, and works on all MIDI instruments. Its successor MIDI-GPT [8] adds effective control tokens on attributes including density and polyphony. Composers Assistant [9, 10] also incorporates user controls and offers an integration with the REAPER DAW. MusIAC [11] also incorporates user controls, but is limited to three tracks, sixteen measures, and four time signatures. [12] infills an arbitrary number of tracks subject to user controls with novel representation scheme. MIDI-Mistral [13] has an unlimited context size in terms of bars, but is severely limited in practical output length and only infills one contiguous section of masked track-measures at time, whereas other mentioned work can infill arbitrary patterns of masking in their limited context windows. Controllability. Useful generative music models must be effectively controllable by the user [25]. Of the above infilling models, MMM provides controls on instrument choice and note density and polyphony, MIDI-GPT conditions on those as well as style and note duration, Composers Assistant offers polyphony controls in version 1 and eight controls including note density in version 2, and MusIAC offers five controls including note density. MIDI-Mistral does not offer attribute controls. Non-infilling models such as FIGARO [3], Museformer [4], MuseCoco [5], and the Music Transformer [6] also condition on various numeric attribute controls, and Bhandari et al. [26] condition directly on text input in similar (though non-contrastive) vein to CLIP [27] for images. Personalization. Artists desire generative systems that can mimic their own style, rather than making bland, generic art [10, 28]. The most popular solution in the literature has been the development of methods to effectively leverage small inspiration sets, in light of the often limited examples available from an individual artist; Vigliensoni et al. [29] aptly call this the small data approach and discuss its merits. Studies of small data in the visual domain include Sobhan et al. [30], Abuzuraiq and Pasquier [31], and Abuzuraiq and Pasquier [32]. 2 In the music domain, Bryan-Kinns et al. [28] highlight that practicing musicians tend toward the small data approach and offer suggestions for future model design. Vigliensoni et al. [33] present system for live drum rhythm generation from small data. Sarmento et al. [34] explore adaptation of GuitarPro Transformer model to four individual guitar players. Malandro [10] indicates the Composers Assistant 2 infilling model admits low-data finetuning for individual users. Musical personalization is part of the general objective of style adaptation. Mossmyr et al. [35] and Casini and Sturm [36] adapt Irish folk music models to generate Swedish folk music. Loth et al. [37] finetune GuitarPro Transformer model to the genre of progressive metal. Style transfer, which transfers the style of one piece of music to another, can be seen as special case of style adaptation. Studies of style transfer models on individual compositions include Brunner et al. [38], Cífka et al. [39], Cífka et al. [40], and Hu et al. [41]. Linear architectures. Traditional softmax attention [2] incurs O(n2) time and space cost in the sequence length, which becomes significant as sequence length grows. Consequently, there has been considerable recent interest into the design of recurrent neural network architectures with custom update rules that afford O(n) time and space while still admitting parallelizability of training. Examples include state space models [42, 43] and linear attention variants [4446], many of which have rivaled or surpassed Transformer performance at equivalent model size and compute budget. The most recent variations largely integrate the delta rule [47] into their state updates to train an expressive hidden state at test time [48, 49], including the RWKV-7 architecture [16] we use. Initial state tuning. The convention when modeling with RNNs is to set the initial hidden state to zero vector. However, it has been known since the 1990s that this can be suboptimal in certain settings, and that training the initial state as variable can improve performance [50, 51]. We refer to this as state tuning. To our knowledge, there have not been many studies on state tuning, but existing research [52, 53] demonstrates intuitive changes in model dynamics; for instance, Mohajerin and Waslander [54] show that zero initial state corresponds to steady state, hence state tuning improves modeling of transient behavior. State tuning therefore presents promise for modern RNNs with expressive hidden states, such as those aforementioned, as their hidden states encode more information more expressively [16]. We explore this premise using the RWKV-PEFT library [55], which pioneers state tuning for RWKV."
        },
        {
            "title": "3 MIDI-RWKV",
            "content": "We aim to create system designed for long-context awareness, selective regeneration, controllability, and personalizability. We address regeneration and controllability by formulating symbolic music infilling as conditional sequence generation task, where the model must generate sequence of musical tokens given surrounding context and control tokens [713]. To address long-context awareness, we deviate from prior works by modifying the prompt format to admit longer effective context lengths, which we detail in Section 3.1. We address personalization with novel finetuning method specific to modern linear architectures, which we detail in Section 3.3. 3.1 Architecture Data representation. We use the MidiTok library [56] to encode multi-track, multi-instrument MIDI files with the REMI encoding [24], as is standard for infilling tasks [7, 8, 12, 13]. REMI represents musical notes with tokens for tempo, pitch, velocity, duration, time signature, bar breaks, and position within bar. Since one note takes several tokens in this scheme, we use byte-pair encoding [57] to reduce the length of tokenized sequences and augment the vocabulary from 663 to 16000 tokens, motivated by subword tokenization in language modeling [58]. We adopt the Bar-Fill representation [7, 8, 13], as shown in Figure 2. Bars to be infilled are masked by one Infill_Bar token per bar. track is sequence of bars delimited by Track_Start and Track_End tokens, containing one of 128 Program tokens that specifies the instrument used. processed score consists of concatenation of all tracks, followed by the infilled section enclosed by the FillBar_Start and FillBar_End tokens, if applicable. Our format, which we deem single-section infilling, differs slightly from that of prior works [710]. See Figure 1 for visualization. We elect to infill only one section per run to allow longer context Figure 1: Example prompts to MIDI-RWKV (above) and other multi-track infilling systems [710] (below). Measures to infill are outlined in red, the full context window in green. These examples are smaller than those the model sees during training and evaluation for space reasons. Full score Track Bar Track_Start Program= <TRACK> Track_End Track_Start <TRACK> Track_End ... FillBar_Start <BARS> FillBar_End <CONTROL> Bar_None <BAR> Bar_None <BAR> Infill_Bar Infill_Bar Bar_None ... Position=0 Pitch= Duration=4 Velocity=100 Position=12 Pitch=58 Duration=4 Velocity= ... Figure 2: Our adapted Bar-Fill representation. The gray <BAR>, <TRACK> and <CONTROL> placeholders correspond to token sequences of complete bars, complete tracks, and attribute controls, respectively. The gray <BARS> placeholder corresponds to the token sequences of the two bars previously located at the Infill_Bar positions. lengths, as this format allows the model to concentrate its attention (literal in Transformer-based model, figurative here) on the single infilling section instead of splitting it across multiple. It should be noted that infilling in the bottom format in Figure 1 can be achieved with MIDI-RWKV by using multiple successive calls to the model. However, each call would have only one masked section, and would thus leak unmasked content in the other infilling locations to the model during inference. Because giving the model masks in other locations is information in and of itself, this may not achieve the same results as other models that mask all sections at once. Attribute controls. Given measurable musical attribute for which we can compute categorical or ordinal value from musical excerpt x, we can teach model the conditional relationship between the relevant musical material and an attribute control token encoding the value of [8]. We use three attributes, each computed on per-bar basis using MidiTok [56]: note density, note duration, and polyphony. Note density represents the number of notes in bar and ranges from 1 to 18, with an extra bin for 18+. Note duration represents which note types (whole, half, quarter, eighth, sixteenth) are present in bar, using binary token for each note type. Polyphony represents the minimum and maximum number of notes played simultaneously at any note onset time in the bar. Model. Since the Bar-Fill representation represents the MIDI data as sequence of tokens, we can use an autoregressive model to generate it. We elect to use an RWKV-7 [16] linear model instead of traditional Transformer decoder [2], as it shows comparable or better performance on most sequence modeling tasks and admits more efficient training and inference than even FlashAttention-based Transformers [59, 60]. RWKV-7, as an RNN, also admits state tuning method as described in Section 2. We optimize the model using the categorical cross-entropy loss over our BPE vocabulary. 3.2 Experimental procedures Training data preprocessing. We tokenize our dataset as previously, and discard files with less than 8 bars or 100 notes. The remaining files are shuffled and reprocesssed for each epoch. Since this adds random component to data loading, we perform all experiments with fixed seed of 42. To create each example, we transpose the musical pitches on all tracks except drums by random number of octaves in the range [6, 6], as in [13]. The tracks are also randomly reordered so that the model learns different conditional orderings; for instance, the POP909 dataset [61] has the tracks in melody-submelody-accompaniment order, which is not universal. From randomly selected track of measures, we select random section to infill of length = max(choice([1, 2, 4, 8]), unif orm(0.1, 0.4) L), where choice and unif orm are as in the Python random module. We ensure that this section contains no empty measures; we found in preliminary experiments that the model learned from long, sparse tracks (e.g. chorus-only accompaniment) that the Bar_None bar separation token follows itself most frequently, resulting in mode collapse, so we avoid this in the infilling section during training. We still allow the model to see consecutive Bar_None tokens in the context to learn the dynamics of silence. Next, we select bars on each side of the masked section across all tracks, such that is the greatest number of context bars available without exceeding the training sequence length; i.e. the selected bars encode to representation (see Section 3.1) below the training sequence length, but selecting + 1 bars on each side would encode to representation in excess of the training sequence length. This results in total window size of + 2C bars across tracks. We then extract the contents of the infilling bars and replace them with Infill_Bar tokens to indicate to the model where the infilling content will go. We then compute the aforementioned attributes for each of the infilling bars and inject the attribute control tokens after the bar separation token starting each bar. Notably, this means all attribute controls are always inserted for each bar. The block of attribute-controlled infilling bars is prefixed with FillBar_Start token and suffixed with FillBar_End token. The tracks are then concatenated together in random order, followed by the infilled section, as in Figure 2. This is the training input to the model. Inference. During inference, we receive as user input score, track ID , context length C, list of attribute controls for each bar, and range of bars to infill. We mask the specified bars on track and extract bars of context across all tracks before and after the infilling section. The tracks are then concatenated, and FillBar_Start token and the attribute controls for the first bar are appended, to signal to the model to begin infilling. We sample until we have generated bars, at which point we stop and replace the bars in the original score. The attribute control tokens for each bar are injected into the sequence after the previous bar is generated to condition the subsequent bar. These attribute controls can be provided by the user, computed from the original content, or mixture of both. Because RWKV-7 is an RNN at inference time and thus has constant hidden state size, the window size + 2C is unlimited in inference, unlike training. Because this is not possible with the standard HuggingFace Transformers [62] sampling loop, we write our own custom sampling loop that is equivalent but also injects bar attribute controls into the sequence when bar separation token is encountered. Our sampling loop allows for temperature, repetition penalty, top-k [63], and top-p (nucleus) [64] sampling, and can be extended to more. We also reject the sampling of two Bar_None tokens in succession, i.e. an empty measure, because this is rarely desirablethe user can make an empty measure themselves by simply doing nothing. 5 3.3 Model personalization with state tuning Since RWKV models are fundamentally modified RNNs, they maintain state vector that encodes information from previously processed tokens, unlike autoregressive Transformers. This presents unique opportunity for personalization. We leverage initial state tuning to condition MIDI-RWKV on particular style without extensive retraining. Specifically, given an inspiration set of examples from composer, we freeze the models parameters and optimize only the initial state vectors {h0,i}L i=1 for each of the layers using the cross-entropy loss on samples derived from the exemplars via the process of the previous section. State tuning requires significantly fewer parameters (only Ld parameters, where is the hidden dimension) compared to full fine-tuning or LoRA-based approaches [65], but does not allow for variability in trainable parameter count as LoRA does. Like LoRA adapters, multiple tuned state vectors can be used with the same model and shared at less expense than full finetune, but unlike LoRA adapters, multiple cannot be used in conjunction. Initial state tuning is effective due to the biasing the trajectory of the model through its representation space [54], directing the hidden state evolution to operate within separate submanifold of the space that better corresponds to the finetuning data. In this sense, it does not teach the model anything, but rather extracts information the model has already learned. State tuning is therefore most effective for adapting foundation models trained on diverse datasets. Symbolic music, where fundamental music structures stay mostly constant across composers and genres but individual techniques vary widely, presents an example of space where this is effective: the model learns the shared fundamental structure, while state tuning isolates stylistic variations particular to individual composers."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental design All training was performed on 1RTX 4090 with sequence length of 2048. All statistical tests are Wilcoxon signed rank tests [66] with Holm-Bonferroni correction [67] unless otherwise stated. Base model. We train our base model using the RWKV-LM library [68] on the train set of the GigaMIDI dataset [17], comprising 1.05 million MIDI files. The model follows the \"deep and narrow\" paradigm suggested by Malandro [9] and Tay et al. [69], under the premise that architecture parameters for RWKV-7 follow similar dynamics to those of Transformers, which is supported by Peng et al. [16]. We use 12 RWKV-7 layers with head size 64, using hidden dimension of 384 and feedforward dimension of 1,344, for roughly 38.0 million parameters in total. The weights are initialized as in Peng et al. [16]. We train for 24 epochs with the Adam optimizer and learning rate of 1e-4, weight decay of 0.1, no dropout, and batch size of 16, which takes 32 hours. Finetuning experiments. We perform finetuning using the RWKV-PEFT library [55] on 99 randomly selected songs from the POP909 dataset [61], leaving the remaining 810 for testing. We train and evaluate only on the melody track (track 0) of each song, instead of training on all tracks, to more easily discern differences between finetuning strategies (being more specialized task) and to facilitate subjective listening test (melody being the easiest track to isolate aurally). We select such small training set to be representative of the small corpuses of composers who will likely be using our system. The scripts for both LoRA and state tuning are written in Triton [70], allowing users on GPU-less edge devices to train in tractable time using the experimental CPU backend for Triton. We state tune with high learning rate of 5e-2 and no dropout for 16 epochs, which trains 294 thousand parameters in about 4 minutes. We perform this experiment three times to establish that such high learning rate still results in stable training dynamics  (Table 3)  , using different 99/810 train/test split each time. We use the model from the first run for comparisons with other experiments. We also train baselines using LoRA [65] for the same amount of time using lower learning rate of 5e-4 and no dropout. We choose time as the unit of measure due to its importance in practical applications [28, 29]. We train on the same train/test split as the first state tuned model. We train two adapters: one with = α = 32, training 2.7 million parameters in about 6 minutes, and one with = α = 4, training 331 thousand parameters in about 4.5 minutes. The latter is used for parameter parity with state tuning, and the former is more demonstrative of LoRAs theoretical performance. Because RWKV models LoRA dynamics are known to be stable [55], we train only one of each. 6 Inference. We use the rwkv.cpp library [71] for inference. We perform sampling on all generations with temperature=1.0, repetition penalty=1.2, top-k=20, and top-p=0.95, as suggested by Rizzotti [13]; we find that these are also optimal for MIDI-RWKV in Appendix A. To evaluate long-context performance, we use = 4N in the notation of Section 3.2, i.e. we take context on either side of the infilling region of length 4 times that of the infilling region in bars. We evaluate on = 2, = 8, = 4, = 16, and = 8, = 32 objectives, labeled in tables as \"N -bar ModelName\". 4.2 Objective evaluation We evaluate on four standard objectives using the MIDI-Metrics library [13]. More details on the metrics may be found in Appendix B. Content preservation (CP) [72], the average cosine similarity between moving averages of pitch chroma vectors of corresponding bars of the original and infilled content, which measures style preservation of the original song. Higher is better. Groove similarity (GS) [73], the average ratio of onset positions that match between corresponding bars of the original and infilled content, which measures preservation of rhythm. Higher is better. Pitch class histogram entropy difference (PCHE) [73], the difference between the information-theoretic entropy of the pitch frequency vectors of corresponding bars of the original and infilled content, which measures tonality preservation. Lower is better. F1 score (F1), the harmonic mean of precision and recall, measuring how well infilled notes match the original content. Higher is better. To our knowledge, MIDI-RWKV is the first controllable multi-track infilling model with focus on long-context tasks, so there are few comparable systems. Per Table 1, MIDI-Mistral [13] and Composers Assistant [9, 10] are closest in capabilities. We exclude Composers Assistant from our comparisons due to its different infilling objective, which would confound interpretation of results. This follows the precedent set by Malandro [10], which avoided comparisons with MMM for methodological differences. We therefore choose MIDI-Mistral as baseline comparison. Base model comparison. The MIDI-RWKV model was tested against MIDI-Mistral on subset of 5000 songs of the GigaMIDI test set. Results are reported in Table 2. All comparisons within the same section and column are statistically significant (p < 0.05). Because MIDI-Mistral does not use BPE, it struggles to perform at long contexts, and fails completely on the = 8, = 32 task since several examples are of length greater than its maximum of 8192. We therefore do not report its performance on that task. It should be noted that even with BPE, several examples in the = 8, = 32 task were over 8192 tokens in length, and few were below the training sequence length of 2048. This demonstrates the remarkable context length extrapolation abilities of the RWKV-7 architecture. The results of Table 2 indicate that MIDI-RWKV holds long-context coherence significantly better than MIDI-Mistral across all input lengths, which may be explained by MIDI-RWKV being better able to capture long-range dependencies in the musical structure. Comparing across tasks, MIDIRWKV tends to perform better on shorter tasks, but this is also due to longer tasks permitting more freedom and hence deviation from the ground truth; this is reflected in the downwards trend of style preservation (content preservation) and reconstruction (F1 score). However, the model appears to be mostly consistent in rhythm (groove similarity) and tonality (PCHE) across task lengths, which may be due to these properties being inferrable from the contemporaneous context, e.g. harmony. Finetuning comparison. Table 3 indicates that state tuning dynamics are stable even at high learning rate of 5e-2, as the results for three training runs on different splits are not statistically significantly different from each other after Wilcoxon rank sum test with Holm-Bonferroni correction (p > 0.05). Table 4 demonstrates that state tuning on small subset of the POP909 dataset, as in Section 4.1, generally performs objectively better than the base model and LoRA finetuning. The model performs considerably worse on content preservation on POP909 than on GigaMIDI. This may be due to the greater difficulty in specializing for style than for rhythm or tonality. Table 4 indicates very few statistically significant differences in F1 score, which may be explained by 7 Table 2: Objective evaluation of MIDI-RWKV and MIDI-Mistral on the GigaMIDI test set. CP GS PCHE F1 2-bar MIDI-RWKV 0.526 0.272 0.227 0.216 2-bar MIDI-Mistral 0.925 0.062 0.905 0.079 0.518 0.499 0.876 0.688 0.125 0.176 0.026 0. 4-bar MIDI-RWKV 0.468 0.239 0.011 0.034 4-bar MIDI-Mistral 0.903 0.072 0.862 0.095 0.487 0.409 0.800 0.669 0.109 0.231 0.011 0.034 8-bar MIDI-RWKV 0.419 0.225 0.928 0. 0.466 0.423 0.093 0.146 Table 3: Objective evaluation of three state tuning runs on POP909 test set. CP GS PCHE F1 2-bar v1 2-bar v2 2-bar v3 4-bar v1 4-bar v2 4-bar v3 8-bar v1 8-bar v2 8-bar v3 0.351 0.269 0.332 0.250 0.337 0.245 0.345 0.152 0.340 0.184 0.332 0. 0.352 0.168 0.350 0.154 0.348 0.138 0.952 0.037 0.952 0.036 0.947 0.043 0.928 0.043 0.924 0.036 0.929 0.037 0.908 0.065 0.910 0.057 0.909 0.043 0.439 0.407 0.447 0.377 0.443 0.371 0.355 0.312 0.352 0.349 0.346 0. 0.244 0.339 0.246 0.172 0.249 0.230 0.073 0.190 0.074 0.118 0.066 0.108 0.070 0.105 0.073 0.126 0.066 0.122 0.054 0.107 0.053 0.078 0.056 0.096 reconstruction of the ground truth simply not being very good metric for medium as diverse as music. The state tuned model performs significantly better than the comparisons on the other three metrics, though the LoRA finetunes admit minor improvements over the base model as well. Despite its performance benefits, state tuning should not be considered replacement for LoRA. Because LoRA does not affect the initial state and state tuning does not affect the model weights, the methods can be used in conjunction. LoRAs are also generally more flexible, with more tunable hyperparameters, variable parameter counts, and the ability to use multiple at once. Future work should explore the possibilities of using the methods together. Attribute control effectiveness. We also evaluate adherence to the attribute controls of note density, note duration, and polyphony bounds in Figure 3. The categorical control tokens are observed to be broadly effective, and the note density is on average around 1 note per bar of the desired amount. Table 4: Objective evaluation of finetuning experiments on POP909 test set. Different superscripts in the same column and section indicate statistically significant (p < 0.05) differences. 2-bar base 2-bar LoRA 4 2-bar LoRA 32 2-bar state 4-bar base 4-bar LoRA 4 4-bar LoRA 32 4-bar state 8-bar base 8-bar LoRA 4 8-bar LoRA 32 8-bar state CP 0.316 0.182a 0.331 0.205b 0.341 0.201c 0.351 0.269d 0.293 0.159a 0.338 0.152b 0.330 0.152c 0.345 0.152d 0.287 0.116a 0.302 0.115b 0.310 0.108c 0.352 0.168d GS 0.947 0.039a 0.949 0.040a 0.944 0.043b 0.952 0.037c 0.925 0.052a 0.922 0.058b 0.918 0.060c 0.928 0.043d 0.884 0.062a 0.885 0.057a 0.883 0.058a 0.908 0.065b PCHE 0.497 0.415a 0.460 0.399b 0.473 0.400c 0.439 0.407d 0.433 0.348a 0.368 0.299b 0.358 0.286c 0.355 0.312c 0.314 0.276a 0.275 0.258b 0.261 0.200c 0.244 0.339d F1 0.063 0.146a 0.074 0.103b 0.070 0.129b 0.073 0.190b 0.072 0.107a 0.063 0.098b 0.065 0.102b 0.070 0.105a 0.054 0.072a 0.056 0.069a 0.053 0.08a 0.054 0.107a 8 Figure 3: Evaluation of attribute control effectiveness. Left: Average absolute difference between real and intended note density. Right: Success rate of categorical control tokens. Table 5: Subjective evaluation of MIDI-RWKV and finetunes on POP909 test set. Original music Base model LoRA = α = 4 State tuned First place count Second place count Average rank 54 40 2.057 27 27 2.779 23 29 2. 36 44 2.357 However, one must note that attribute control can go against consistency with context (e.g. rapidly swapping between high and low note densities); in this sense, 100% success rate may not be desirable, as the model must weigh adherence to the prompt against musical coherence. Attribute control evaluations on POP909 between the models of Table 4 are available in Appendix C. 4.3 Subjective evaluation We also use subjective listening test because the ground truth may not reflect the only reasonable way to infill track section. We did not compare against MIDI-Mistral on account of the objective differences in Table 2, so we ran small listening test with 28 participants on the MIDI-RWKV outputs only. For each of five randomly selected examples from set of ten, participants ranked four anonymized clips (original, base model, LoRA = α = 4, and state-tuned) in order of preference. All clips were 8-bar infills with 32 bars of context from POP909, truncated to 4 bars around the infill region for rendering with the symusic library [74]. Results and p-values are in Tables 5 and 6. The respondents indicated clear preference for the original music, confirming that human-composed music still outperforms generative approaches (p < 0.05 for all). The state-tuned model demonstrates significant improvements over both the base model and LoRA fine-tuning (p < 0.05), including the most second place rankings of any model. Notably, there was no significant difference between the base model and LoRA fine-tuning (p > 0.05). The most common qualitative feedback received regarded minor rhythmic inconsistencies (twelve respondents) and boring or uninteresting melodies (seven respondents). Curiously, two respondents described one clip of original music as being relatively boring compared to the generated content. Table 6: p-values for Table 5 after Wilcoxon signed rank test with Holm-Bonferroni correction. Original Base model LoRA = α = 4 State tuned Original Base model LoRA = α = 4 State tuned - 5.60 105 1.22 105 0.0486 - - 0.896 0. 9 - - - 0.034 - - - -"
        },
        {
            "title": "5 Conclusion",
            "content": "We presented MIDI-RWKV, novel framework for multi-track, long-context, controllable symbolic music infilling based on linear attention. Our approach addresses several limitations of existing music generation systems by enabling efficient processing of arbitrarily long contexts, selective regeneration of musical segments, fine-grained control over musical attributes, and effective personalization through state tuning. Objective and subjective experimental results demonstrate that MIDI-RWKV significantly outperforms the comparable MIDI-Mistral model on long-context tasks, and that our proposed state tuning technique provides superior personalization compared to LoRA-based approaches in the low-sample regime typical of individual composers. Limitations. While MIDI-RWKV aims to be style-agnostic and diverse in its outputs, it inherits from the biases of the GigaMIDI dataset [17] on which it was trained. Some musical styles and instruments are therefore overrepresented and others are underrepresented. Our prompt format also restricts infilling to contiguous sections, unlike prior work that allows arbitrary masking patterns, hence it may require more inference compute to infill the same masking pattern. Our prompt format also requires symmetric context, which may not reflect compositional needs. Despite its impressive context length extrapolation capabilities, the models relatively low training context length of 2048 limits its usability in extremely-long-context situations of full songs with ten or more tracks. Additionally, we train on few attribute controls, which may lack the granularity that professional composers require. The model also requires all attribute controls to be externally provided for each bar, removing degree of freedom from the generation process. While state tuning proved effective in Section 4, it may perform poorly on highly experimental or unusual compositional styles that deviate significantly from the distribution of the pretraining data. We do not explore full parameter finetuning, nor do we compare state tuned outputs to those of any Transformer model due to lack of reasonable comparisons (Section 4.2). Our finetuning experiments were also limited to single low-sample dataset and the sole task of melody infilling. The system cannot yet compose in real time due to inference latency and lack of ability to \"stream\" tokens, limiting its application in agentic and live contexts. This is in part due to the relative immaturity of the RWKV ecosystem compared to that of Transformers [62], which also makes integration with mainstream systems and DAWs challenging. Future work will attempt to integrate MIDI-RWKV into DAWs, including Calliope [75]. Music is fundamentally creative and subjective, making objective metrics that compare to the ground truth incomplete measures of quality. \"good\" infilling might diverge significantly from the original while maintaining musical coherence. For instance, an ABAB section can be reasonably infilled to an ABCB section. Our subjective evaluation is also limited: we did not give listeners the full 32 bars of context provided to the model, nor did we compare with the rank 32 LoRA, as adding either would make the test prohibitively long. We hope future work can address some of these many limitations. Ethical statement. The subjective listening test of Section 4.3 was performed under Ethical Approval #30000063 from the Simon Fraser University Office for Research Ethics. There are outstanding ethical and legal questions concerning the generation of music that exactly matches copyrighted work, which extend far beyond the scope of this paper. State tuning also raises the risk of users finetuning the model to impersonate the styles of others, including potential copyrighted work."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We acknowledge the support of the National Science and Engineering Research Council of Canada (NSERC) and the Metacreation Lab at Simon Fraser University. We thank Martin Malandro and Davide Rizzotti for their correspondence, as well as Jiale Kang and the RWKV community for their technical input and discussion. We also thank the anonymous respondents to our listening survey."
        },
        {
            "title": "References",
            "content": "[1] Yinghao Ma, Anders Øland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elona Shatri, Fabio 10 Morreale, Ge Zhang, György Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, and Ziyu Wang. Foundation models for music: survey, 2024. URL https://arxiv.org/abs/2408.14340. [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, page 60006010, Red Hook, NY, USA, 2017. [3] Dimitri von Rütte, Luca Biggio, Yannic Kilcher, and Thomas Hofmann. FIGARO: Controllable music generation using learned and expert features. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=NyR8OZFHw6i. [4] Botao Yu, Peiling Lui, Rui Wang, Wei Hu, Xu Tan, Wei Ye, Shikun Zhang, Tao Qin, and Tie-Yan Liu. Museformer: transformer with fineand coarse-grained attention for music generation. In Proceedings of the 36th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2022. [5] Peiling Lu, Xin Xu, Chen Wen Kang, Botao Yu, Chengyi Xing, Xuejiao Tan, and Jiang Bian. Musecoco: Generating symbolic music from text, 2023. URL https://arxiv.org/abs/ 2306.00110. [6] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam M. Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer: Generating music with long-term structure. In International Conference on Learning Representations, 2018. [7] Jeff Ens and Philippe Pasquier. MMM: Exploring conditional multi-track music generation with the transformer, 2020. URL https://arxiv.org/abs/2008.06048. [8] Philippe Pasquier, Jeff Ens, Nathan Fradet, Paul Triana, Davide Rizzotti, Jean-Baptiste Rolland, and Maryam Safi. MIDI-GPT: controllable generative model for computer-assisted multitrack music composition. Proceedings of the AAAI Conference on Artificial Intelligence, 39(2): 14741482, 2025. doi: 10.1609/aaai.v39i2.32138. [9] Martin Malandro. Composers Assistant: An Interactive Transformer for Multi-Track MIDI Infilling. In Proc. 24th Int. Society for Music Information Retrieval Conf., pages 327334, Milan, Italy, 2023. [10] Martin Malandro. Composers Assistant 2: Interactive Multi-Track MIDI Infilling with FineGrained User Control. In Proc. 25th Int. Society for Music Information Retrieval Conf., pages 438445, San Francisco, CA, USA, 2024. [11] Rui Guo, Ivor Simpson, Chris Kiefer, Thor Magnusson, and Dorien Herremans. Musiac: An extensible generative framework for music infilling applications with multi-level control. In Artificial Intelligence in Music, Sound, Art and Design: 11th International Conference, page 341356, Berlin, Heidelberg, 2022. doi: 10.1007/978-3-031-03789-4_22. [12] Rui Guo and Dorien Herremans. An exploration of controllability in symbolic music infilling. IEEE Access, 13:5487354891, 2025. doi: 10.1109/ACCESS.2025.3554648. [13] Davide Rizzotti. Midi-mistral: Controllable transformer-based midi generation for bar and track infilling. Masters thesis, Politecnico di Milano, Milan, Italy, 2024-2025. Advisor: Prof. Fabio Antonacci; Co-advisors: Prof. Philippe Pasquier and Dr. Riccardo Giampiccolo. [14] Renaud Bougueng Tchemeube, Jeffrey Ens, Cale Plut, Philippe Pasquier, Maryam Safi, Yvan Grabit, and Jean-Baptiste Rolland. Evaluating human-ai interaction via usability, user experience and acceptance measures for MMM-C: creative AI system for music composition. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, 2023. doi: 10.24963/ijcai.2023/640. 11 [15] Cheng-Zhi Anna Huang, Hendrik Vincent Koops, Ed Newton-Rex, Monica Dinculescu, and Carrie J. Cai. AI song contest: Human-AI co-creation in songwriting. In Proceedings of the 21st International Society for Music Information Retrieval Conference, pages 708716, Montréal, Canada, 2020. [16] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi Wu, Daniel Wuttke, and Christian Zhou-Zheng. RWKV-7 \"Goose\" with expressive dynamic state evolution, 2025. URL https://arxiv.org/abs/2503.14456. [17] Keon Ju Maverick Lee, Jeff Ens, Sara Adkins, Pedro Sarmento, Mathieu Barthet, and Philippe Pasquier. The GigaMIDI dataset with features for expressive music performance detection. Transactions of the International Society for Music Information Retrieval, 8(1), 2025. ISSN 2514-3298. doi: 10.5334/tismir.203. [18] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, page 417424, USA, 2000. doi: 10.1145/344779.344972. [19] Gaëtan Hadjeres, François Pachet, and Frank Nielsen. DeepBach: steerable model for Bach chorales generation. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 13621371, Sydney, Australia, 2017. [20] Ashis Pati, Alexander Lerch, and Gaëtan Hadjeres. Learning to traverse latent spaces for musical score inpaintning. In 20th International Society for Music Information Retrieval Conference, Delft, The Netherlands, 2019. [21] Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron C. Courville, and Douglas Eck. Counterpoint by convolution. In Proceedings of the 18th ISMIR Conference, pages 211218, 2018. doi: 10.5281/zenodo.1416370. [22] Adam Roberts, Jesse Engel, Yotam Mann, Jon Gillick, Claire Kayacik, Signe Nørly, Monica Dinculescu, Carey Radebaugh, Curtis Hawthorne, and Douglas Eck. Magenta studio: Augmenting creativity with deep learning in Ableton Live. In Proceedings of the International Workshop on Musical Metacreation, 2019. [23] Jia-Lien Hsu and Shuh-Jiun Chang. Generating music transition by using transformer-based model. Electronics, 10(18), 2021. ISSN 2079-9292. doi: 10.3390/electronics10182276. [24] Yu-Siang Huang and Yi-Hsuan Yang. Pop music transformer: Beat-based modeling and generation of expressive pop piano compositions. In Proceedings of the 28th ACM International Conference on Multimedia, page 11801188, New York, NY, USA, 2020. doi: 10.1145/3394171. 3413671. [25] Michele Newman, Lidia Morris, and Jin Ha Lee. Human-ai music creation: Understanding the perceptions and experiences of music creators for ethical and productive collaboration. In Proceedings of the 24th International Society for Music Information Retrieval Conference, pages 8088, 2023. doi: 10.5281/zenodo.10265227. [26] Keshav Bhandari, Abhinaba Roy, Kyra Wang, Geeta Puri, Simon Colton, and Dorien Herremans. text2midi: Generating symbolic music from captions. In Proceedings of the 39th AAAI Conference on Artificial Intelligence (AAAI 2025), 2025. [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, volume 139, pages 87488763. PMLR, 2021. URL https://arxiv.org/abs/2103.00020. [28] Nick Bryan-Kinns, Anna Wszeborowska, Olga Sutskova, Elizabeth Wilson, Phoenix Perry, Rebecca Fiebrink, Gabriel Vigliensoni, Rikard Lindell, Andrei Coronel, and Nuno N. Correia. Leveraging small datasets for ethical and responsible ai music making. In Proceedings of AudioMostly 2025, Coimbra, Portugal, 2025. 12 [29] Gabriel Vigliensoni, Phoenix Perry, and Rebecca Fiebrink. small-data mindset for generative ai creative work. In Proceedings of the Generative AI and HCI Workshop - Conference on Human Factors in Computing Systems Workshop, New Orleans, LA, USA, 2022. ACM. doi: 10.5281/zenodo.7086327. [30] Arshia Sobhan, Ahmed M. Abuzuraiq, and Philippe Pasquier. Autolume 2.0: gan-based In Proceedings of the 38th no-coding small data and model crafting visual synthesizer. Conference on Neural Information Processing Systems, Vancouver, Canada, 2024. URL https://creativity-ai.github.io/assets/papers/49.pdf. Workshop Paper. [31] Ahmed M. Abuzuraiq and Philippe Pasquier. Seizing the means of production: Exploring the landscape of crafting, adapting and navigating generative ai models in the visual arts. In Proceedings of the 3rd Generative AI and HCI Workshop at CHI 2024, Honolulu, Hawaii, USA, 2024. ACM. URL https://arxiv.org/abs/2404.17688. Workshop Paper. [32] Ahmed M. Abuzuraiq and Philippe Pasquier. Towards personalizing generative ai with small data for co-creation in the visual arts. In Joint Proceedings of the ACM IUI Workshops 2024, volume 3660, pages 114, Greenville, South Carolina, USA, 2024. [33] Gabriel Vigliensoni, Louis McCallum, Esteban Maestre, and Rebecca Fiebrink. R-vae: Live latent space drum rhythm generation from minimal-size datasets. Journal of Creative Music Systems, 1(1), 2022. ISSN 2399-7656. doi: 10.5920/jcms.902. [34] Pedro Sarmento, Adarsh Kumar, Dekun Xie, CJ Carr, Zack Zukowski, and Mathieu Barthet. ShredGP: Guitarist style-conditioned tablature generation with transformers. In Proceedings of the 16th International Symposium on Computer Music Multidisciplinary Research, pages 110121, Tokyo, Japan, 2023. [35] Simon Mossmyr, Eric Hallström, Bob L. Sturm, Victor Hansjons Vegeborn, and Jonas Wedin. From jigs and reels to schottisar och polskor: Generating scandinavian-like folk music with deep recurrent networks, 2019. [36] Luca Casini and Bob L. T. Sturm. Tradformer: transformer model of traditional music transcriptions. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 49154920, 2022. doi: 10.24963/ijcai.2022/681. [37] Jackson Loth, Pedro Sarmento, CJ Carr, Zack Zukowski, and Mathieu Barthet. ProgGP: From GuitarPro tablature neural generation to progressive metal production. In Proceedings of the 16th International Symposium on Computer Music Multidisciplinary Research, pages 110121, Tokyo, Japan, 2023. [38] Gino Brunner, Yuyi Wang, Roger Wattenhofer, and Sumu Zhao. Symbolic music genre transfer with cyclegan. 2018 IEEE 30th International Conference on Tools with Artificial Intelligence, pages 786793, 2018. [39] Ondˇrej Cífka, Umut Simsekli, and Gaël Richard. Groove2groove: One-shot music style transfer with supervision from synthetic data. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. doi: 10.1109/TASLP.2020.3019642. [40] Ondˇrej Cífka, Umut Simsekli, and Gaël Richard. Supervised symbolic music style translation using synthetic data. In 20th International Society for Music Information Retrieval Conference, Delft, The Netherlands, 2019. doi: 10.5281/zenodo.3527878. [41] Zhejing Hu, Yan Liu, Gong Chen, and Yongxu Liu. Can machines generate personalized music? hybrid favorite-aware method for user preference music transfer. IEEE Transactions on Multimedia, 25:22962308, 2022. [42] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. URL https://arxiv.org/abs/2312.00752. [43] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning, 2024. 13 [44] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1404814077, Singapore, 2023. doi: 10.18653/v1/2023.findings-emnlp.936. [45] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, 2020. [46] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time, 2024. URL https://arxiv.org/abs/2501.00663. [47] Bernard Widrow and Marcian Hoff. Adaptive switching circuits. IRE WESCON Convention Record, 4(1):96104, 1960. [48] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Proc. Int. Conf. on Machine Learning, Virtual only, 2021. [49] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=y8Rm4VNRPH. [50] Felix Gers, Nicol Schraudolph, and Jürgen Schmidhuber. Learning precise timing with lstm recurrent networks. Journal of Machine Learning Research, 3:115143, 2002. doi: 10.1162/ 153244303768966139. [51] Mikel L. Forcada and Rafael C. Carrasco. Learning the initial state of second-order recurrent neural network during regular-language inference. Neural Computation, 7(5):923930, 1995. doi: 10.1162/neco.1995.7.5.923. [52] Silviu Pitis. Non-zero initial states for recurrent neural networks. R2RT Blog, 2016. URL https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks. [53] Sam Wenke and Jim Fleming. Contextual recurrent neural networks, 2019. URL https: //arxiv.org/abs/1902.03455. [54] Nima Mohajerin and Steven L. Waslander. State initialization for recurrent neural network modeling of time-series data. In 2017 International Joint Conference on Neural Networks, pages 23302337, 2017. doi: 10.1109/IJCNN.2017.7966138. [55] Jiale Kang. Disha: Dimension-sharding adaptation of large language models with fast convergence and fast computation, 2025. URL https://arxiv.org/abs/2409.15371. [56] Nathan Fradet, Jean-Pierre Briot, Fabien Chhel, Amal El Fallah Seghrouchni, and NicoIn Extended Ablas Gutowski. MidiTok: python package for MIDI file tokenization. stracts for the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference, 2021. URL https://archives.ismir.net/ismir2021/ latebreaking/000005.pdf. [57] Philip Gage. new algorithm for data compression. The Users Journal, 12(2):2338, 1994. [58] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 17151725, 2016. [59] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 14 [60] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [61] Ziyu Wang, K. Chen, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi Dai, Xianbin Gu, and Gus G. Xia. Pop909: pop-song dataset for music arrangement generation. In International Society for Music Information Retrieval Conference, 2020. [62] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing, 2020. URL https://arxiv.org/abs/1910. 03771. [63] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 889898, 2018. doi: 10.18653/v1/P18-1082. [64] Ari Holtzman, Jan Buys, Du Li, Daniel Fried, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rygGQyrFvH. [65] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9. [66] Frank Wilcoxon. Individual Comparisons by Ranking Methods, pages 196202. Springer New York, New York, NY, 1992. doi: 10.1007/978-1-4612-4380-9_16. [67] Sture Holm. simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6(2):6570, 1979. ISSN 03036898, 14679469. URL http://www.jstor.org/ stable/4615733. [68] Bo Peng. Rwkv-lm. https://github.com/BlinkDL/RWKV-LM, 2025. [69] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from pretraining and finetuning transformers. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=f2OYVDyfIB. [70] Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, page 1019, New York, NY, USA, 2019. doi: 10.1145/3315508.3329973. [71] RWKV. rwkv.cpp. https://github.com/RWKV/rwkv.cpp, 2025. [72] Wei Tsung Lu and Li Su. Transferring the style of homophonic music using recurrent neural networks and autoregressive model. In Proceedings of the 19th International Society for Music Information Retrieval Conference, pages 740746, 2018. doi: 10.5281/zenodo.1492523. [73] Shih-Lun Wu and Yi-Hsuan Yang. The jazz transformer on the front line: Exploring the shortcomings of ai-composed music through quantitative measures. In Proceedings of the 21st ISMIR Conference, pages 142149, 2020. doi: 10.5281/zenodo.4245390. [74] Yikai Liao and Zhongqi Luo. symusic: swift and unified toolkit for symbolic music processing. In Extended Abstracts for the Late-Breaking Demo Session of the 25th International Society for Music Information Retrieval Conference, 2024. URL https://ismir2024program.ismir. net/lbd_426.html#lbd. [75] Renaud Bougueng Tchemeube, Jeffrey Ens, and Philippe Pasquier. Calliope: An online generative music system for symbolic multitrack composition. In International Conference on Innovative Computing and Cloud Computing, 2025. 15 Table 7: Objective evaluations under different sampling parameters. CP GS PCHE F1 Default Temperature 0.8 Temperature 1.2 Rep. penalty 1.0 Rep. penalty 1.4 Top-p 0.9 Top-p 0.98 Top-k 15 Top-k 30 0.419 0.225 0.438 0.226 0.358 0.188 0.382 0.228 0.397 0.202 0.416 0.224 0.395 0.209 0.419 0.229 0.391 0.245 0.928 0.077 0.915 0.072 0.895 0.074 0.924 0.061 0.895 0.088 0.919 0.070 0.901 0.084 0.896 0.079 0.910 0.078 0.466 0.423 0.490 0.462 0.501 0.553 0.586 0.554 0.423 0.375 0.476 0.486 0.503 0.449 0.547 0.587 0.449 0.447 0.093 0.146 0.115 0.226 0.078 0.096 0.125 0.248 0.075 0.092 0.066 0.079 0.063 0.080 0.088 0.158 0.075 0."
        },
        {
            "title": "A Sampling parameter ablations",
            "content": "We compare objective metrics on different sampling parameters in Table 7. The default parameters, as suggested by [13], are temperature=1.0, repetition penalty=1.2, top-k=20, and top-p=0.95. One parameter was varied at time to other common values to determine their impact on generation, and 100 examples were generated with each. We observed that, while the default choice of sampling parameters does not achieve the best score in all categories, it performs admirably well in each. There are also subjective reasons for the choice of default sampling parameters: we found that temperature below 1.0 caused the model to repeat the same output several times, which will inevitably bring frustration from the user but is not captured in the objective metrics. Generations with repetition penalty above 1.2 also tended to lose structural coherence, likely because the high penalty discouraged repetitions that are natural in music; this is reflected in the groove similarity scores."
        },
        {
            "title": "B Objective metrics",
            "content": "Content preservation. Introduced by [72], the content preservation is calculated as follows. Each bar of the infilled content and the corresponding original content is split into time steps, typically = 16. Each time step is associated with pitch chroma vector ct representing the probability distribution over the pitch classes (C, C#, D, . . . ) at that time. The moving average of the chroma vectors is then calculated using frame size of /2 time steps, resulting in averaged chroma vectors ao,t (where is the number of bars in the infilling region) for the original content and another averaged chroma vectors ai,t for the infilled content. The content preservation is then the average cosine similarity between contemporaneous chroma vectors across all timesteps, CP ({ ao,t}T t=1, { ai,t}T t=1) = 1 N (cid:88) t=1 ao,t ai,t ao,tai,t . (1) Introduced as grooving pattern similarity by [73], the groove similarity is Groove similarity. calculated as follows. Let the grooving pattern be binary vector representing the positions in bar at which there is at least one note onset. The groove similarity between the grooving patterns go of the original content and gi of the infilled content is then the average number of onset positions (or absence of such) that match, GS(go, gi) = 1 1 dim go dim go1 (cid:88) j=1 XOR(go,j, gi,j). (2) Pitch class histogram entropy difference. Introduced by [73], the pitch class histogram entropy is calculated as follows. The pitch chroma vector is calculated as in the calculation of content preservation, but is done across an entire bar instead of by time subdivisions. Its entropy is given by 11 (cid:88) H(c) = ci log ci. (3) i=0 The pitch class histogram entropy difference is the difference between the pitch class histogram entropies of bar of the original content and bar of the infilled content. 16 F1 score. common measure in classification tasks, the F1 score is the harmonic mean of precision and recall, where precision is the ratio of correctly predicted notes to all predicted notes and recall is the ratio of correctly predicted notes to all ground truth notes. Formally, for the original content with note set No and the infilled content with note set Ni, we have 1(No, Ni) = 2 precision recall precision + recall = 2No Ni No + Ni . (4) POP909 finetuned attribute control evaluations In this section we provide attribute control evaluations on the base model, LoRA-trained models with = α = 4 and = α = 32, and state tuned representative model. The left subfigure of each is the average absolute difference between real and intended note density; the right is the success rate of categorical control tokens. Across Figures 4, 5, and 6, the state tuned model typically outperforms the base model slightly, which in turn outperforms the LoRA finetuned models. Figure 4: Evaluation of attribute control effectiveness on the 2-bar = 2, = 8 objective. Figure 5: Evaluation of attribute control effectiveness on the 4-bar = 4, = 16 objective. Figure 6: Evaluation of attribute control effectiveness on the 8-bar = 8, = 32 objective."
        }
    ],
    "affiliations": [
        "Metacreation Lab, Simon Fraser University"
    ]
}