{
    "paper_title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning",
    "authors": [
        "Yibin Wang",
        "Zhimin Li",
        "Yuhang Zang",
        "Chunyu Wang",
        "Qinglin Lu",
        "Cheng Jin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 8 1 3 3 0 . 5 0 5 2 : r Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning Yibin Wang1,2, Zhimin Li4, Yuhang Zang3, Chunyu Wang4, Qinglin Lu4, Cheng Jin1, Jiaqi Wang2,3 1Fudan University, 2Shanghai Innovation Institute 3Shanghai AI Lab, 4Hunyuan, Tencent https://codegoat24.github.io/UnifiedReward/think Figure 1: Overview of Comparison Results. (a) Our method enables multi-dimensional long CoT reasoning to improve reward signal accuracy. (b) Extensive quantitative results demonstrate our superiority in both vision understanding and generation reward tasks."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UNIFIEDREWARD-THINK, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement finetuning approach to elicit and incentivize the models latent complex reasoning Equal contribution. Corresponding authors. Preprint. Under review. ability: (1) We first use small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the models cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the models prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the models reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments confirm that incorporating long CoT reasoning significantly enhances the accuracy of reward signals. Notably, after mastering CoT reasoning, the model exhibits implicit reasoning capabilities, allowing it to surpass existing baselines even without explicit reasoning traces."
        },
        {
            "title": "Introduction",
            "content": "In recent years, multimodal reward models (RMs) Wang et al. [2024, 2025], Zang et al. [2025], Xiong et al. [2024], He et al. [2024], Xu et al. [2024], Liu et al. [2025a], Li et al. [2025] have excelled at aligning vision model outputs with human preferences, providing crucial reward signals to guide model training Wang et al. [2024, 2025], Liu et al. [2025a], Ouyang et al. [2022], Rafailov et al. [2023], Schulman et al. [2017] and inference Gulcehre et al. [2023], Snell et al. [2024]. Traditional reward models are typically trained on large-scale human-annotated preference data through supervised finetuning (SFT). At test time, most methods He et al. [2024], Liu et al. [2025a], Xu et al. [2024] directly assign scores or provide pairwise ranking for vision model outputs, relying on the knowledge and intuitions acquired from the training data. While effective, these methods tend to lack interpretability, which makes it difficult for users to understand the underlying reasoning process behind the assigned scores or rankings. To this end, recent studies Wang et al. [2025], Xiong et al. [2024], Wang et al. [2024], Li et al. [2025] leverage the generative capabilities of Visual-Language Models (VLMs), enabling RMs to provide concise justifications alongside the assigned reward signals. Despite their success, their reasoning processes often lack rigorous logical structure and the capacity for multidimensional, deep analysis, which may result in inaccurate reward signals in complex scenarios or misguided conclusions arising from flawed reasoning processes. In light of these issues, we posit two key hypotheses: (1) Incorporating explicit long Chains-ofThought (CoT) into the reward reasoning process is essential for significantly enhancing RMs reliability and robustness; (2) Once the model internalizes this ability, the accuracy of its directly provided reward signals, even without CoT reasoning traces, can also be improved by leveraging its implicit logical thinking capabilities. However, equipping RMs with CoT reasoning using traditional training schemes like SFT poses highly challenge due to the scarcity of large-scale CoT reward data, as manual annotation requires substantial human resources and time. In this work, we argue that this challenge can be effectively addressed, as VLMs inherently possess prior knowledge of complex reasoning; what is needed is an effective strategy to elicit and incentivize this capability Guo et al. [2025]. Therefore, this paper proposes UNIFIEDREWARD-THINK, the first unified multimodal CoT-based reward model, capable of performing multi-dimensional, step-by-step long-chain reasoning across both visual understanding and generation reward tasks. The core idea is to activate the models latent long-chain reasoning capabilities through limited CoT reward data and to progressively reinforce and refine this capability through exploration-driven reinforcement fine-tuning that optimizes for accurate and robust reasoning patterns. Specifically, our training pipeline, as shown in Fig. 2, consists of three stages: (1) Cold Start. We first distill the reasoning process of GPT-4o Hurst et al. [2024] using fewer image generation preference data, which serves to cold-start the model training, enabling it to learn the structure and format of long CoT reasoning. (2) Rejection Sampling. Next, we prepare large-scale unified multimodal preference data to incentivize the models CoT reasoning outputs across various visual reward tasks. Correct reasoning trajectories are retained and utilized through rejection sampling to strengthen the distribution of correct reasoning patterns. (3) Group Relative Policy Optimization (GRPO). Finally, incorrectly reasoned samples are leveraged for GRPO-based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize toward desirable outcomes defined by our verified rewards (i.e., format reward and accuracy reward). Unlike SFT, which merely imitates predefined answers, GRPO promotes trial-and-error 2 Figure 2: Method Overview. The training pipeline consists of three key stages: (1) Cold Start: We first distill GPT-4os reasoning process on small amount of image generation preference data to initialize the models CoT reasoning format; (2) Rejection Sampling: Then, we leverage the models generalization capabilities on large-scale unified multimodal preference data to elicit its CoT reasoning process across various vision tasks, using correctly predicted samples for rejection sampling to refine the model; (3) GRPO: Finally, incorrectly predicted samples are utilized for GRPO-based reinforcement fine-tuning to further enhance the models step-by-step reasoning capabilities. learning by evaluating and refining the models reasoning outputs based on verified rewards, thus encouraging deeper reasoning discovery rather than passive memorization. Extensive experiments demonstrate that incorporating long CoT reasoning significantly improves the accuracy and reliability of reward signals. Remarkably, experimental results also prove that once the model internalizes the CoT reasoning ability, it also exhibits strong implicit reasoning capabilities: even when providing direct reward outputs without explicit reasoning traces, it consistently outperforms existing baselines across all vision reward tasks. In summary, our contributions are threefold: (1) We propose the first unified multimodal CoT-based reward model, UNIFIEDREWARD-THINK, capable of multi-dimensional, step-by-step long-chain reasoning across both visual understanding and generation tasks; (2) We demonstrate that explicit long CoT reasoning substantially enhances reward model reliability, and once mastered, also strengthens implicit reasoning, enabling more accurate reward signals even without explicit reasoning traces; (3) Extensive experiments validate the superiority of our method compared with existing baselines across all vision reward tasks. We hope our work can unlock reward models reasoning potential to enhance interpretability, generalization, and alignment, enabling more trustworthy and human-aligned reward signals for multimodal generation and understanding."
        },
        {
            "title": "2.1 Multimodal Reward Models",
            "content": "Multimodal reward models have become increasingly important for aligning vision understanding and generation models with human preferences. dominant approach is to fine-tune visual-language 3 models (VLMs) Li et al. [2024a], Bai et al. [2025], exploiting their powerful multimodal alignment capabilities to learn human judgment-based reward functions. Earlier studies have explored reward models on visual generation Wang et al. [2024], Liu et al. [2025a], He et al. [2024], Xu et al. [2024] and understanding Zang et al. [2025], Xiong et al. [2024] tasks. For instance, Wang et al. [2024] collects human feedback and constructs human-rated video datasets to train the reward model, LiFTCritic, which measures how well the generated videos align with human expectations. Zang et al. [2025] develops an effective pipeline for constructing multimodal preference datasets and leverages existing high-quality data to train the reward model, IXC-2.5-Reward, enabling accurate evaluation of visual understanding outputs. However, these reward models are task-specific, limiting their adaptability across diverse visual understanding and generation tasks. To address this limitation, Wang et al. [2025] introduces UnifiedReward, unified reward model capable of assessing both image and video generation and understanding tasks, demonstrating that joint learning across diverse visual tasks can yield substantial mutual benefits. Despite their effectiveness, these reward models are largely limited to providing direct responses Liu et al. [2025a], Xu et al. [2024], He et al. [2024], Xu et al. [2023] or engaging in shallow reasoning with limited depth Wang et al. [2024, 2025], Xiong et al. [2024], often resulting in inaccurate or unreliable reward signals in complex scenarios or misguided conclusions arising from flawed reasoning processes. To this end, we propose UNIFIEDREWARD-THINK, the first unified multimodal long CoT-based reward model, enabling multi-dimensional long-chain reasoning for both visual understanding and generation tasks. The core idea is to use reinforcement learning to activate and enhance VLMs latent reasoning capabilities, which will be discussed in the following section."
        },
        {
            "title": "2.2 Reinforcement Learning",
            "content": "Recently, reinforcement learning (RL) techniques have been extensively used to enhance the reasoning capabilities of Large-Language Models (LLMs), enabling them to effectively solve complex problems Jaech et al. [2024], Luong et al. [2024], Shao et al. [2024], Yang et al. [2024], Ying et al. [2024], Hui et al. [2024], Jiao et al. [2024], Zhang et al. [2024a,c]. significant advancement in this field is Deepseek-R1-Zero Guo et al. [2025], which introduced novel approach for developing robust reasoning capabilities using Group Relative Policy Optimization (GRPO). By leveraging rule-based rewards, it enhances reasoning without the need for supervised fine-tuning (SFT). For VLMs, RL has mainly been applied to tasks such as mitigating hallucinations Sun et al. [2023], Yu et al. [2024a], aligning models with human preferences Yu et al. [2024b], Zhou et al. [2024], improving reasoning and visual perception Liu et al. [2025b], and visual quality assessment Li et al. [2025]. However, the application of RL in multimodal reward models is still under exploration. To address this gap, our work introduces reinforcement fine-tuning strategy that leverages verifiable rewards with GRPO-based RL to integrate long CoT reasoning, enhancing the accuracy of reward signals by enabling multi-dimensional and step-by-step reasoning processes."
        },
        {
            "title": "3.1 Overview",
            "content": "This work aims to incorporate long Chain-of-Thought (CoT) reasoning into the reward models decision-making process to enhance the reliability and robustness of reward signals. However, achieving this with traditional training methods like Supervised Fine-Tuning (SFT) remains highly challenging due to the scarcity of large-scale CoT-based reward data. This work posits that VisualLanguage Models (VLMs) inherently possess prior knowledge of complex reasoning; the key challenge lies in devising an effective strategy to elicit and incentivize this capability. Therefore, we take the first step to propose the unified multimodal CoT-based reward model, UNIFIEDREWARDTHINK, adopting exploration-driven reinforcement fine-tuning to activate and refine the VLMs multi-dimensional and step-by-step long chain reasoning across various vision reward tasks. Specifically, as shown in Fig. 2, our pipeline includes three key stages: (1) Cold Start: use small amount of distilled CoT reward data to initialize the reward model with the format and structure of multi-step reasoning (Sec. 3.2); (2) Rejection Sampling: Utilize large-scale unified preference data to elicit the models generalizable CoT reasoning across diverse vision tasks; correctly reasoned samples are retained for rejection sampling to reinforce accurate reasoning patterns (Sec. 3.3); (3) Group 4 Figure 3: Qualitative Results of Video Generation CoT Reward Reasoning. Given pair of videos and the corresponding caption, our model performs quality assessment across semantic consistency, temporal coherence, and authenticity through CoT reasoning. Relative Policy Optimization (GRPO): Leverage incorrectly reasoned samples for GRPO-based reinforcement fine-tuning to further improve the models CoT reasoning capabilities (Sec. 3.4)."
        },
        {
            "title": "3.2 Cold Start: Learning CoT Reward Format",
            "content": "In this work, we hypothesize that VLMs inherently possess the potential for complex, long-chain reasoning. However, due to the absence of exposure to reward modeling during pre-training, they often lack suitable format to articulate such reasoning in this context, which leads to inconsistent or shallow outputs. To address this, we initiate the training with cold start phase, where small amount of high-quality CoT reward data distilled from GPT-4o Hurst et al. [2024] is used to explicitly teach the model the desired reasoning format and structure. Specifically, we begin by preparing small set of image generation preference data, each consisting of an image pair and prompt (i.e., instruction and image caption). These samples are then fed into GPT-4o to generate the detailed long-chain reasoning process and the corresponding final answer. Among the generated samples, we retain only those where the reasoning trajectory leads to correct answers, denoted as (x, y) where is the original input, and = {y1, y2, . . . , yT } is the distilled output, which are subsequently used to cold-start the training. The objective function is defined as: Lcold_start(θ) = (cid:88) i=1 log (cid:0)yi x, y<i; θ(cid:1) , (1) where θ represents the parameters of the reward model. This stage serves to initialize the models ability to follow structured CoT reasoning format."
        },
        {
            "title": "3.3 Rejection Sampling: Unified CoT Reward Generalization Fine-Tuning",
            "content": "After learning the CoT reasoning format through cold-start training, we further elicit and expand the models reasoning capabilities to vision understanding and generation tasks. This is inspired by prior study Wang et al. [2025], which highlights the benefits of multi-task joint training: reward reasoning abilities learned in one task can generalize well and effectively enhance performance in other vision tasks. Therefore, we apply rejection sampling to reinforce the learning of correct reasoning patterns to improve both reliability and accuracy in vision understanding and generation tasks. Specifically, we first prompt the model to perform generalization CoT reasoning on large-scale unified preference datasets, leveraging its prior knowledge across various tasks (e.g., evaluating video temporal consistency and vision-question-answer accuracy). In vision generation tasks, each input sample consists of textual prompt (instruction and caption) and corresponding image/video pair, whereas in vision understanding tasks, it comprises textual prompt (instruction and query) along with an image/video. Then, we retain only the samples with correct final answers and apply rejection sampling, filtering out incorrect or low-quality reasoning. The training objective for this stage is similar to Eqn. 1, but it utilizes the filtered reasoning data of diverse vision tasks (x, y) obtained through rejection sampling. 5 Figure 4: Qualitative Cases of Image and Video Understanding CoT Reward Reasoning. Given an image or video, query, and pair of candidate answers, our model performs quality assessment across semantic accuracy, factual correctness, and clarity through CoT reasoning. This process concentrates the training distribution around accurate reasoning patterns and enhances the models generalization ability across diverse visual domains."
        },
        {
            "title": "3.4 GRPO: Unified CoT Reward Reinforcement Fine-Tuning",
            "content": "In the rejection sampling stage, subset of challenging data featuring intricate reasoning patterns is filtered, which the model has yet to fully comprehend and master. To ensure the model fully learns the underlying knowledge in the training dataset and further enhances its reasoning ability, we introduce GRPO-based reinforcement fine-tuning Guo et al. [2025]. Specifically, in GRPO, the policy model πθ generates multiple candidate responses for given input, which are evaluated using predefined verifiable reward functions, providing corresponding reward signals. These signals guide policy updates, encouraging alignment with high-quality reasoning while constraining deviations from the reference model πref. This approach enables the model to explore diverse reasoning processes, guiding it toward the correct reasoning trajectory and improving its ability to handle complex scenarios. In the following, we will first introduce the design of verifiable rewards in our work, followed by detailed description of the GRPO training process."
        },
        {
            "title": "3.4.1 Verifiable Reward",
            "content": "In GRPO, verifiable rewards are essential for guiding the models learning by offering rule-based feedback. In this work, we employ two types of verifiable rewards i.e., format reward and accuracy reward, to ensure both the quality and accuracy of the models responses: 6 Format Reward ensures that the generated response follows specific reasoning structure, which is critical for maintaining clarity and consistency in the reasoning process. In this case, the response is expected to contain two key tags: think and answer. These tags are used to delineate the models reasoning process and the final answer, respectively. If both tags are present and properly formatted in the response, the reward rf mt is set to 1. Otherwise, the reward rf mt is 0. This mechanism helps reinforce the importance of structuring the response. Accuracy Reward evaluates whether the final answer output (i.e., Image/Video/Answer is better), enclosed within the answer tag, exactly matches the ground truth. This reward serves as reliable signal to ensure that the model produces correct answers. If matches the ground truth precisely, the reward is set to 1; otherwise, it is 0. Formally, the accuracy reward is defined as: racc = (cid:26)1 if = ground truth otherwise. This is crucial for reinforcing accurate reasoning and encouraging the model to generate correct responses. Finally, the overall verifiable reward is formulated as: = rf mt + racc. By incorporating both the format and accuracy rewards into the GRPO training, we provide the model with explicit feedback that encourages it to generate responses that are both well-structured and factually accurate."
        },
        {
            "title": "3.4.2 Reinforcement Fine-Tuning",
            "content": "Given an input x, GRPO first samples distinct responses {o(1), o(2), . . . , o(N )} from the previous policy model πθold . Each response is evaluated using our verifiable rewards, i.e., format and accuracy rewards, resulting in corresponding reward scores {r(1), r(2), . . . , r(N )}. Then, GRPO normalizes these scores and quantifies the relative quality of each response by computing the advantage of each response using the standardized reward: ˆA(i) = r(i) mean({r(1), . . . , r(N )}) std({r(1), . . . , r(N )}) , where ˆA(i) quantifies the relative quality of the i-th response in comparison to other candidates within the same sampled group. Then, GRPO estimates magnitude of the policy update by computing the likelihood ratio of each response under the new policy πθnew relative to the old policy πθold, defined as: ratio(i) = πθnew(o(i) x) πθold(o(i) x) . To stabilize training and avoid overly aggressive updates, the ratio is clipped to bounded interval [1 δ, 1 + δ]. Moreover, to ensure that the updated policy does not diverge significantly from fixed reference model πref, KL divergence penalty term is introduced with weighting factor β. The final optimization objective is defined as: (cid:16) Lgrpo(θ) = (cid:17) ratio(i), clip(ratio(i), 1 δ, 1 + δ) ˆA(i) β DKL(πθnew πref) min (cid:104) xX ,o(i)πθold (cid:105) , where denotes the set of training sample input and DKL( ) denotes the KL divergence. By integrating normalized reward advantages, clipped importance sampling, and reference-model regularization, GRPO enables stable and effective policy optimization, driving our model toward generating higher-quality, verifiably correct CoT reasoning paths."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. For Image Generation, we utilize HPD (25.6K) Christodoulou and Kuhlmann-Jørgensen [2024], OIP (7.4K)3, EvalMuse (3K) Han et al. [2024], all preprocessed by Wang et al. [2025]as 3https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized 7 Table 1: Image Understanding Assessment Comparison. We evaluate baselines across different understanding aspects on VLRewardBench. Models General Hallu. Reason. Overall Accuracy Macro Accuracy Gemini-1.5-Pro GPT-4o LLaVA-Critic UnifiedReward Ours (w/o CoT) Ours 50.8 49.1 47.4 76.5 77.9 78.1 72.5 67.6 38.5 58.1 70.5 72.7 64.2 70.5 53.8 65.1 65.4 66. 67.2 65.8 46.9 67.5 73.1 73.8 62.5 62.4 46.6 66.6 71.3 72.3 Table 2: Image and Video Generation Assessment Comparison. Evaluation is conducted on GenAI-Bench (Image and Video) and VideoGen-Reward. tau indicates that accuracy is calculated with ties, and diff excludes tied pairs when calculating accuracy. Method PickScore HPSv2 ImageReward VisionReward UnifiedReward Ours (w/o CoT) Ours Image Generation GenAI-Bench tau 53.2 51.6 47.8 46.8 54. 54.1 - diff 67.2 68.4 65.0 66.4 70.9 71.9 72.5 Video Generation Method GenAI-Bench VideoScore LiFT VisionReward VideoReward UnifiedReward Ours (w/o CoT) Ours tau 46.2 41.2 52.1 50.2 60.7 57.8 - diff 70.6 60.1 73.1 73.3 77.2 81.6 82.3 VideoGen-Reward diff tau 42.1 40.6 57.4 60.1 66.6 64.9 - 49.9 58.3 68.2 73.9 79.3 79.9 80.5 well as OpenAI-4o_t2i_human_preference (6.7K) collected by Rapidata4. For Video Generation, we employ VideoDPO (10K) Liu et al. [2024] and Text2Video-Human Preferences (5.7K), also collected by Rapidata. For Image Understanding, we sample 30K data from LLaVA-Critic-113K Xiong et al. [2024]. For Video Understanding, we adopt ShareGPTVideo-DPO (17K) Zhang et al. [2024b]. In the cold-start stage, we distill 5K image generation CoT reward reasoning samples from GPT-4o. The input data are randomly drawn from four image generation datasets, with the remaining data reserved for the subsequent training stages. Reward Model. We adopt UnifiedReward Wang et al. [2025] as our base model, which is capable of assessing both image/video generation and understanding. We leverage its strong performance and extensive prior knowledge in visual perception and generation, and further activate its latent capacity for long-chain CoT reasoning. Evaluations. We evaluate image and video understanding reward assessment on VLRewardBench Li et al. [2024b] and ShareGPTVideo Zhang et al. [2024b], using 5K test samples, respectively. For generation evaluation, we adopt GenAIBench Jiang et al. [2024], which covers both image and video reward benchmarks. Additionally, we utilize VideoGen-RewardBench Liu et al. [2025a] to further assess video generation."
        },
        {
            "title": "4.2 Comparison Results",
            "content": "The quantitative results, presented in Tabs. 1, 2, and Fig. 1, demonstrate the superiority of our model across visual generation and understanding tasks. In image and video generation tasks, our model does not account for tie scenarios during evaluation, since such cases were not included in the training data. Nevertheless, it consistently outperforms existing methods across all other evaluations. Notably, compared to the base model UnifiedReward, incorporating multi-dimensional and multi-step reasoning yields substantial performance gains across all tasks. In particular, our model achieves significant improvements in image understanding reward tasks. This is intuitive, as CoT reasoning in vision tasks fundamentally centers on deeper understanding of the visual content. Additionally, we explore whether our model, after learning long CoT reasoning, can improve the accuracy of direct reward signals, i.e., without explicit CoT reasoning. Experimental results show that the model still outperforms existing methods by leveraging implicit logical reasoning, with only slight drop in performance compared to explicit CoT reasoning. These results strongly validate the effectiveness and superiority of our approach. Our qualitative results are provided in Figs. 1, 3, and 4. 4https://huggingface.co/datasets/Rapidata 8 Table 3: Ablation Results of Image Understanding Assessment. We conduct ablation experiments under different settings and evaluate them across multiple aspects on VLRewardBench. Models General Hallu. Reason. Overall Accuracy Macro Accuracy UnifiedReward +GRPO (w/o CoT) UnifiedReward +cold start +rejection sampling +GRPO(Ours) 76.5 77.8 76.5 76.0 77.6 78.1 58.1 59.1 58.1 56.7 64.9 72.7 65.1 65.5 65.1 65.2 65.4 66. 67.5 69.0 67.5 66.9 72.1 73.8 66.6 67.4 66.6 66.0 69.3 72.3 Table 4: Ablation Results of Image and Video Generation Assessment Comparison. We conduct ablation experiments under different settings and evaluate them on GenAI-Bench (Image and Video) and VideoGen-Reward. tau indicates that accuracy is calculated with ties, and diff excludes tied pairs when calculating accuracy. Method Image Generation GenAI-Bench diff tau Method Video Generation GenAI-Bench diff tau VideoGen-Reward tau diff UnifiedReward +GRPO (w/o CoT) UnifiedReward +cold start +rejection sampling +GRPO(Ours) 54.8 54.1 54.8 - - - 70.9 71.3 70.9 68.6 72.0 72.5 UnifiedReward +GRPO (w/o CoT) UnifiedReward +cold start +rejection sampling +GRPO(Ours) 60.7 56.7 60.7 - - - 77.2 78.4 77.2 75.3 78.9 82.3 66.6 64.8 66.6 - - - 79.3 79.5 79.3 76.8 79.7 80."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "Ablation of Each Training Stage. We conduct ablation studies to assess the effectiveness of each training stage. As shown in Tabs. 3 and 4, after the cold start phase, though the model learns the CoT reasoning format, it still struggles with accurate reward prediction. Notably, introducing rejection sampling leads to clear improvements by retaining correctly reasoned samples for supervised fine-tuning, thereby reinforcing desirable reasoning patterns. Further, the GRPO stage yields the most substantial gains, as it focuses on previously mispredicted cases, allowing the model to explore multiple reasoning paths and converge on more accurate solutions. These results highlight the complementary roles of each stage and demonstrate how our staged training strategy progressively enhances CoT-based reward modeling. Ablation of GRPO without CoT Reasoning. To further validate the necessity of learning the CoT reasoning process, we evaluate GRPO variant that removes CoT and directly optimizes reward predictions based on final answers. As shown in Tabs. 3 and 4, although this approach yields slight improvements over the baseline, the gains are significantly limited. This suggests that learning from final answers alone fails to teach the model the underlying reasoning process. In contrast, our CoT-based GRPO guides the model to explore multiple reasoning trajectories and gradually converge toward the correct path, leading to deeper understanding and more robust generalization. These results show that the effectiveness of our GRPO-based reinforcement fine-tuning stems from explicitly modeling the reasoning process, rather than simply reinforcing the final answer."
        },
        {
            "title": "5 Conclusion",
            "content": "This work proposes UNIFIEDREWARD-THINK, the first unified multimodal CoT reward model capable of multi-dimensional, step-by-step reliable reward reasoning for both visual understanding and generation tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the models latent complex reasoning ability, including three key stages: cold start, rejection sampling, and GRPO-based reinforcement fine-tuning. Extensive experiments demonstrate that CoT reasoning not only improves the accuracy and robustness of reward signals but also equips the model with strong implicit reasoning capabilities, enabling superior performance even without explicit CoT outputs."
        },
        {
            "title": "References",
            "content": "S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. D. Christodoulou and M. Kuhlmann-Jørgensen. Finding the subjective truth: Collecting 2 million votes for comprehensive gen-ai model evaluation. arXiv preprint arXiv:2409.11904, 2024. C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. S. Han, H. Fan, J. Fu, L. Li, T. Li, J. Cui, Y. Wang, Y. Tai, J. Sun, C. Guo, et al. Evalmuse-40k: reliable and fine-grained benchmark with comprehensive human annotations for text-to-image generation model evaluation. arXiv preprint arXiv:2412.18150, 2024. X. He, D. Jiang, G. Zhang, M. Ku, A. Soni, S. Siu, H. Chen, A. Chandra, Z. Jiang, A. Arulraj, K. Wang, Q. D. Do, Y. Ni, B. Lyu, Y. Narsupalli, R. Fan, Z. Lyu, Y. Lin, and W. Chen. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. D. Jiang, M. Ku, T. Li, Y. Ni, S. Sun, R. Fan, and W. Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024. F. Jiao, G. Guo, X. Zhang, N. F. Chen, S. Joty, and F. Wei. Preference optimization for reasoning with pseudo feedback. arXiv preprint arXiv:2411.16345, 2024. Y. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NeurIPS, 36:3665236663, 2023. B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. L. Li, Y. Wei, Z. Xie, X. Yang, Y. Song, P. Wang, C. An, T. Liu, S. Li, B. Y. Lin, et al. Vlrewardbench: challenging benchmark for vision-language generative reward models. arXiv preprint arXiv:2411.17451, 2024b. W. Li, X. Zhang, S. Zhao, Y. Zhang, J. Li, L. Zhang, and J. Zhang. Q-insight: Understanding image quality via visual reinforcement learning. arXiv preprint arXiv:2503.22679, 2025. J. Liu, G. Liu, J. Liang, Z. Yuan, X. Liu, M. Zheng, X. Wu, Q. Wang, W. Qin, M. Xia, X. Wang, X. Liu, F. Yang, P. Wan, D. Zhang, K. Gai, Y. Yang, and W. Ouyang. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025a. R. Liu, H. Wu, Z. Ziqiang, C. Wei, Y. He, R. Pi, and Q. Chen. Videodpo: Omni-preference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025b. T. Q. Luong, X. Zhang, Z. Jie, P. Sun, X. Jin, and H. Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024. 10 L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35: 2773027744, 2022. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36:5372853741, 2023. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Y. Wang, Z. Tan, J. Wang, X. Yang, C. Jin, and H. Li. Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024. Y. Wang, Y. Zang, H. Li, C. Jin, and W. Jiaqi. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. T. Xiong, X. Wang, D. Guo, Q. Ye, H. Fan, Q. Gu, H. Huang, and C. Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS, 36:1590315935, 2023. J. Xu, Y. Huang, J. Cheng, Y. Yang, J. Xu, Y. Wang, W. Duan, S. Yang, Q. Jin, S. Li, J. Teng, Z. Yang, W. Zheng, X. Liu, M. Ding, X. Zhang, X. Gu, S. Huang, M. Huang, J. Tang, and Y. Dong. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. H. Ying, S. Zhang, L. Li, Z. Zhou, Y. Shao, Z. Fei, Y. Ma, J. Hong, K. Liu, Z. Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. T. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu, H.-T. Zheng, M. Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, pages 1380713816, 2024a. T. Yu, H. Zhang, Y. Yao, Y. Dang, D. Chen, X. Lu, G. Cui, T. He, Z. Liu, T.-S. Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024b. Y. Yue, Z. Chen, R. Lu, A. Zhao, Z. Wang, S. Song, and G. Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Y. Zang, X. Dong, P. Zhang, Y. Cao, Z. Liu, S. Ding, S. Wu, Y. Ma, H. Duan, W. Zhang, K. Chen, D. Lin, and J. Wang. Internlm-xcomposer2.5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. K. Zhang, G. Li, Y. Dong, J. Xu, J. Zhang, J. Su, Y. Liu, and Z. Jin. Codedpo: Aligning code models with self generated and verified source code. arXiv preprint arXiv:2410.05605, 2024a. 11 R. Zhang, L. Gui, Z. Sun, Y. Feng, K. Xu, Y. Zhang, D. Fu, C. Li, A. Hauptmann, Y. Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024b. Y. Zhang, S. Wu, Y. Yang, J. Shu, J. Xiao, C. Kong, and J. Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024c. Y. Zhou, C. Cui, R. Rafailov, C. Finn, and H. Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024."
        },
        {
            "title": "A Further Methodological Insights",
            "content": "Cold Start only with Image Generation Preference Data: Why It Works. Our experiments demonstrate that using small amount of high-quality image generation CoT reward reasoning data, instead of distilling data for every task, is sufficient for the model to learn the CoT reasoning format and structure across all visual reward tasks. The underlying reason lies in the fact that video tasks can be seen as multi-image understanding problems. Video frames, like images, also require recognizing objects, spatial relationships, and context. Once the model masters CoT reasoning for images, it can naturally extend this reasoning to videos by leveraging its inherent prior knowledge of temporal dynamics and sequential visual understanding. Therefore, by learning CoT reasoning from images, the model can seamlessly generalize to both static and dynamic visual tasks, eliminating the need for separate distillation for each task. Rejection Sampling for Unified Reward Generalization Fine-Tuning: Why We Need It. After the cold start, the model has already internalized the format and structure of CoT reasoning. With prior knowledge across tasks, it can generate accurate CoT-based reward analyses for many simple scenarios. However, directly applying GRPO to the entire training set would be inefficient and computationally costly. Besides, GRPO offers limited gains on samples that the model has already mastered. Therefore, we first apply rejection sampling to filter out cases that the model already performs well on. This not only reduces training cost but also reinforces the distribution of correct reasoning patterns by prioritizing high-confidence outputs. More challenging or ambiguous samples are passed to the GRPO stage, where the model explores diverse reasoning trajectories and gradually learns to prefer more accurate solutions. Why We Trust the CoT Reasoning When the Final Answer is Correct during GRPO. In MLLMs, common failure mode during CoT reasoning is the disconnect between the reasoning steps and the final answer: models may produce plausible reasoning chains but ultimately rely on shortcuts or intuition to generate the conclusion. To mitigate this, we explicitly structure the CoT process by having the model score each image across multiple dimensions and then aggregate these scores to derive the final decision. This enforces step-by-step alignment between intermediate reasoning and outcome, ensuring that correct final answer emerges from coherent and interpretable reasoning process. As result, in GRPO training, verifying the correctness of the final answer implicitly validates the reasoning trajectory, offering principled yet efficient way to supervise complex CoT generation."
        },
        {
            "title": "B More Experimental Details",
            "content": "B.1 Base Model. This work adopts UnifiedReward Wang et al. [2025] as our base architecture, which unifies image/video generation and understanding reward tasks, demonstrating the mutual benefits of multi-task learning and achieving strong performance across various vision reward benchmarks. However, its reasoning is limited to direct responses or shallow thinking, lacking the capability for long and structured CoT reasoning, which may lead to lower accuracy and weaker interpretability in complex scenarios. Inspired by this work, we build upon UnifiedReward and further activate its latent long CoT reasoning capabilities across different vision reward tasks, aiming to enhance the accuracy and robustness of the reward signals. B.2 Training Details. For both the cold-start and rejection sampling stages, training is performed with batch size of 1, 16 gradient accumulation steps, learning rate of 2.5 106, and warm-up ratio of 0.3, using 8 NVIDIA H100 (80GB) GPUs. For GRPO, training is conducted with batch size of 1, single gradient accumulation step, learning rate of 1 106, and KL penalty coefficient of β = 0.04. The number of generated responses is set to 8, using 64 NVIDIA H20 (97GB) GPUs. B.3 Reward Model Baselines. We compare our method against series of strong reward model baselines across both image and video domains, covering generation and understanding tasks. 13 PickScore Kirstain et al. [2023] is text-to-image preference model that integrates CLIP-based vision-language features with reward modeling strategy inspired by InstructGPT. It is trained on the Pick-a-Pic dataset to align image generation outputs with human preferences. HPSv2 Christodoulou and Kuhlmann-Jørgensen [2024] builds upon CLIP and is fine-tuned using the HPD_v2 Christodoulou and Kuhlmann-Jørgensen [2024] to predict human preferences over generated images. It demonstrates strong performance in pairwise ranking tasks and serves as representative image generation reward baseline. ImageReward Xu et al. [2023] is trained on large-scale preference dataset containing 137k human expert comparisons through both rating and ranking. It is specifically designed to capture subtle aspects of human preferences in text-to-image generation quality. LLaVA-Critic Xiong et al. [2024] extends large language models for evaluating image understanding through critic-style framework. It is trained on high-quality instruction-following data covering diverse criteria such as accuracy, relevance, and hallucination, supporting both pointwise and pairwise evaluation. VisionReward Xu et al. [2024] introduces fine-grained, multi-dimensional evaluation framework for both image and video domains. It trains separate reward models tailored to human preferences collected through carefully curated datasets, offering strong baselines for visual content assessment. VideoScore He et al. [2024] focuses on assessing video generation quality. It is trained on the VideoFeedback dataset comprising human-annotated scores over 37.6K videos, each evaluated across multiple aspects including fidelity, consistency, and alignment. LiFT-Critic Wang et al. [2024] is reward model developed under the LiFT framework, which aligns text-to-video models using human feedback. Trained on LiFT-HRAa dataset containing over 10K human-labeled samples with both scores and rationalesit captures detailed human evaluation signals across multiple dimensions. VideoReward Liu et al. [2025a] offers multi-dimensional assessment for video generation tasks. It is trained on large-scale 182K dataset of human-labeled comparisons collected from outputs of 12 video generation models, providing strong performance on complex video benchmarks. UnifiedReward Wang et al. [2025] serves as our base architecture. It leverages multi-task learning across diverse image and video generation and understanding datasets. By unifying multimodal reward tasks into single framework, UnifiedReward demonstrates mutual enhancement effects and establishes solid baseline for holistic visual reward modeling. Our UnifiedReward-Think extends UnifiedReward by integrating explicit long CoT reasoning across both visual understanding and generation tasks. Through three-stage training pipelineincluding cold start to learning CoT reward format, rejection sampling for unified CoT reward generalization fine-tuning, and GRPO for unified CoT reward reinforcement fine-tuning, the model achieves stronger accuracy and interpretability in reward assessment. It also generalizes well without explicit reasoning, leveraging implicit CoT capabilities for robust performance. B.4 Evaluation Benchmarks VLRewardBench Li et al. [2024b] serves as diverse benchmark for evaluating image understanding capabilities, featuring 1,250 carefully curated samples across general vision-language queries, hallucination detection, and complex reasoning. To ensure robust evaluation, response orders are randomly shuffled during testing. ShareGPTVideo Zhang et al. [2024b] provides large-scale video-caption pairs and human preference data, covering various aspects of video understanding such as temporal reasoning, spatial relations, and factual grounding. We 3K for evaluation in our reward modeling experiments. GenAI-Bench Jiang et al. [2024] is multimodal generation benchmark designed to assess how well models align with human preferences across image and video generation tasks. We adopt its image and video generation subsets for evaluating generative reward performance. VideoGen-RewardBench Liu et al. [2025a] offers large-scale benchmark tailored for evaluating video reward models, consisting of 26.5k video pairs labeled by humans. Each pair is ranked 14 according to multiple criteria, and we use the Overall Quality scores to benchmark the performance of the model."
        },
        {
            "title": "C Prompting Templates and More Qualitative Cases",
            "content": "We provide more qualitative cases across diverse vision tasks with prompting templates in Figs. 5, 6, 7 and 8."
        },
        {
            "title": "D Limitations and Future Works",
            "content": "While our method introduces long-form CoT reasoning to improve reward modeling, this inevitably increases inference time during reasoning. However, we show that once the model has mastered CoT reasoning, it can leverage implicit reasoning to enhance answer accuracy even without explicitly generating CoT traces. This suggests strong internalization of the reasoning process. In future work, we aim to further optimize efficiency by exploring shorter or more efficient CoT formats without compromising reasoning quality. Besides, although our reinforcement fine-tuning strategy successfully activates the models latent long CoT reasoning ability using only small amount of high-quality data, the prior study Yue et al. [2025] has shown that reinforcement learning cannot fundamentally extend models capability: it can only amplify the potential already acquired during supervised fine-tuning (SFT). Therefore, to further push the boundaries of CoT-based reward reasoning, scaling up high-quality CoT supervision still remains promising direction."
        },
        {
            "title": "E Societal Impacts",
            "content": "Our work introduces unified multimodal CoT reward model capable of high-quality, interpretable assessment across diverse multimodal tasks. This advancement can significantly enhance the alignment of generative models with human preferences in real-world applications such as AI-assisted content creation, and education. By improving both the accuracy and interpretability of reward signals, our method contributes more transparent and controllable AI behaviors, potentially increasing public trust in generative technologies. However, as reward models become more capable and general, they may also be misused to reinforce harmful biases in generation models, especially if the training data or preference annotations reflect subjective or skewed human values. We encourage future work to further examine the ethical implications of large-scale reward modeling and to include fairness-aware training strategies."
        },
        {
            "title": "F Ethical Statement",
            "content": "In this work, we affirm our commitment to ethical research practices and responsible innovation. To the best of our knowledge, this study does not involve any data, methodologies, or applications that raise ethical concerns. All experiments and analyses were conducted in compliance with established ethical guidelines, ensuring the integrity and transparency of our research process. 15 Figure 5: More Qualitative Results of Image Generation CoT Reward Reasoning. Given pair of images and the corresponding caption, our model performs quality assessment across semantic consistency, aesthetics, and authenticity through CoT reasoning. 16 Figure 6: More Qualitative Results of Video Generation CoT Reward Reasoning. Given pair of videos and the corresponding caption, our model performs quality assessment across semantic consistency, temporal coherence, and authenticity through CoT reasoning. Figure 7: More Qualitative Cases of Image Understanding CoT Reward Reasoning. Given an image, query, and pair of candidate answers, our model performs quality assessment across semantic accuracy, factual correctness, and clarity through CoT reasoning. 18 Figure 8: More Qualitative Cases of Video Understanding CoT Reward Reasoning. Given video, query, and pair of candidate answers, our model performs quality assessment across semantic accuracy, factual correctness, and clarity through CoT reasoning."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Hunyuan, Tencent",
        "Shanghai AI Lab",
        "Shanghai Innovation Institute"
    ]
}