{
    "paper_title": "Model Merging with Functional Dual Anchors",
    "authors": [
        "Kexuan Shi",
        "Yandong Wen",
        "Weiyang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of a shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), a framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce a principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 3 2 2 1 2 . 0 1 5 2 : r a"
        },
        {
            "title": "MODEL MERGING WITH FUNCTIONAL DUAL ANCHORS",
            "content": "Kexuan Shi1 Yandong Wen2 Weiyang Liu1 1The Chinese University of Hong Kong 2Westlake University SphereLab.ai/fda"
        },
        {
            "title": "ABSTRACT",
            "content": "Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging."
        },
        {
            "title": "INTRODUCTION",
            "content": "Model merging has emerged as promising post-training strategy for integrating knowledge from multiple finetuned checkpoints of foundation models. The core idea is to combine diverse domain knowledge from multiple homologous downstream models into single unified one (Matena & Raffel, 2022; Jin et al., 2022). Compared to multi-task learning (Ruder, 2017) and continual learning (Wang et al., 2024), model merging is appealing because it consolidates knowledge directly through the parameters of downstream models finetuned from the same pretrained backbone. Figure 1: Illustration of our input-space model merging framework using FDAs. On the left, we compare multi-task joint training, task arithmetic and FDA. Inspired by joint training, FDA models the knowledge in the input space. θA = (XA, θ0) denotes the model finetuned by the task data XA from the initial model θ0 with some loss function. However, model merging still faces fundamental challenges due to conflicts arising from diverse task-specific knowledge. Since this knowledge is encoded in the parameters of downstream models, such conflicts inevitably manifest as parameter conflicts. The prevailing paradigm for addressing them is to scale the task vectors (Ilharco et al., 2022) (i.e., parameter offsets between these downstream checkpoints and the pretrained model), and then add them back to the pretrained parameters. Within this paradigm, prior works interpret parameter conflicts as task vector conflicts (Yadav et al., 2023; Yu et al., 2024) and propose various adjustment strategies. These methods either exploit intrinsic properties of the parameter space (e.g., magnitude (Yadav et al., 2023; Yu et al., 2024), similarity (Du et al., 2024), orthogonality (Xiong et al., 2024), or subspace structure (Wei et al., 2025b; Gargiulo et al., 2025; Cheng et al., 2025)) or leverage task-specific data to guide adjustments (e.g., entropy measures (Yang et al., 2023; 2025) or representation distributions (Jin et al., 2022; Wei et al., 2025a; Xu et al., 2025)). unifying characteristic of both approaches is their emphasis on modeling the parameter space, either through structural priors or through data-driven priors. In contrast to existing approaches, we focus on modeling the input-representation space to mitigate task-specific knowledge conflicts. Rather than directly manipulating parameter offsets, we propose generating synthetic inputs, termed functional dual anchors (FDAs), that can effectively simulate the role of task vectors. An illustration of this idea is provided in Figure 1. Conceptually, this is akin to projecting task-specific knowledge into the input-representation space by constructing inputs that reproduce the downstream models functional shift relative to the pretrained model. Specifically, for each downstream checkpoint, we construct set of inputs whose induced gradients on the pretrained parameters align with the corresponding task vector. In this way, FDAs effectively act as the dual of task vectors. While task vectors encode task-specific knowledge in the parameter space, FDAs capture the analogous knowledge in the input space through their induced gradients. This"
        },
        {
            "title": "Technical Report",
            "content": "perspective introduces new way of thinking about knowledge consolidation. Instead of constraining adjustments to the parameter space, we shift the merging process into the input space, where representations can naturally capture task-specific variations. The key intuition is to bridge the gap between joint multi-task training, where knowledge integration inherently happens in the input space, and model merging, where it is typically confined to the parameter space. By obtaining FDAs for different task vectors, our approach can emulate the effect of joint multi-task training. To gain an intuitive understanding of FDAs, we compare their optimization trajectories with those of task arithmetic in Figure 2. We treat the obtained FDAs as finetuning data and optimize the model parameters accordingly. As shown in the figure, optimizing with FDAs moves the model closer to the local minima of the loss landscape (computed over eight downstream datasets). While task vectors provide useful guidance from the pretrained model, they quickly drift away from the loss basin, whereas FDAs consistently guide optimization toward more favorable regions. Moreover, by capturing functional shifts in the input space, FDAs offer greater robustness for model merging. Unlike task vectors, which are sensitive to initialization and can drift under different starting points, FDAs exhibit robustness to such variations, facilitating more reliable model merging. Figure 2: Comparison between task arithmetic and FDAs on the loss landscape of the pretrained across all 8 downstream datasets. FDAs can effectively follow the loss landscape and guide the model toward better local minima. Another motivation behind FDAs is that modeling the input space is generally easier than modeling the parameter space, as the input space tends to be more structured. The effectiveness of modeling the input space for knowledge transfer is has been extensively explored and empirically validated in the context of dataset distillation (Wang et al., 2018b; Cazenavette et al., 2022), iterative teaching (Liu et al., 2017a; Qiu et al., 2023), dataset condensation (Zhao et al., 2021; Zhao & Bilen, 2023) and continual learning (Shin et al., 2017; Yu et al., 2023). Building on these insights, FDAs provide an alternative perspective on model merging by extending input-space modeling to this setting. Our major contributions are listed as follows: Instead of modeling the parameter space, we propose novel model merging framework that leverages functional dual anchors to model the input-representation space for knowledge encoding. Building on theoretical insights from linear model, we introduce principled initialization scheme for FDAs, which leads to substantial performance improvements. While FDAs can be used independently and yield significant gains, they are also complementary to standard parameter-centric model merging methods, such as TA (Ilharco et al., 2022), TSV (Gargiulo et al., 2025), and WUDI (Cheng et al., 2025). Our empirical results show that incorporating FDAs consistently improves the performance of these parameter-centric approaches."
        },
        {
            "title": "2 A MODEL MERGING FRAMEWORK WITH FUNCTIONAL DUAL ANCHORS",
            "content": "Our model merging framework consists of two stages: (1) FDA construction, and (2) parameter optimization using FDAs. Finally, we discuss the practical implementation of this framework for large-scale foundation models and present the complete procedure in Algorithm 1."
        },
        {
            "title": "2.1 PRELIMINARIES AND BACKGROUND",
            "content": "Before introducing our framework, we briefly recap the formulation of model merging. Consider foundation model φ with pretrained parameters θ0 Rp and collection of downstream finetuned checkpoints with parameters {θi}m i=1. The goal of model merging is to derive merged parameter ˆθ from θ0 and {θi}m i=1 that consolidates knowledge across tasks and achieves multi-task capability without requiring retraining on the original task data. The prevailing approach to model merging is to first compute the task vectors (Ilharco et al., 2022) {τi = θi θ0}m i=1, apply adjustments (Yadav et al., 2023; Yu et al., 2024; Wei et al., 2025b) to {τi}m i=1, and then add the adjusted task vectors back to the pretrained parameter θ0. The merged parameter ˆθ is given by ˆθ = θ0 + (cid:80)m i=1 ϕi(τi) where ϕi : Rp Rp is introduced to denote possible adjustments of the task vectors {τ }m i=1. In task arithmetic (TA) (Ilharco et al., 2022), {ϕi}m i=1 are linear transformations with uniform scaling factor between 0 and 1. {ϕi}m i=1 can also take other forms, e.g., the magnitude of parameter values (Yadav et al., 2023) or the subspace spanned by {τi}m i=1 (Xiong et al., 2024). Recently, several works"
        },
        {
            "title": "Technical Report",
            "content": "incorporate task-specific entropy measures (Yang et al., 2023; 2025) or representation distribution (Wei et al., 2025a; Xu et al., 2025) to determine ϕi through iterative optimization. For notational convenience, we use φ(θ0) to denote the model φ(θ = θ0). Instead of leveraging knowledge in the parameter space, we propose to project the knowledge encoded in checkpoints into the input-representation space. Concretely, we construct set of synthetic inputs (i.e., FDAs) whose induced gradients on the pretrained model align with task vectors."
        },
        {
            "title": "2.2 FDA CONSTRUCTION: KNOWLEDGE PROJECTION VIA GRADIENT MATCHING",
            "content": "We aim to construct set of inputs whose induced gradients on the pretrained model align with the task vector. These gradients can be refined by comparing representation discrepancies between the downstream checkpoints {φ(θi)}m i=1 and the pretrained model φ(θ0) on the constructed inputs. Formally, assuming the model φ operates in d-dimensional input space, we consider set of j=1 Rd for the downstream checkpoint φ(θi). We refer to these points as input points {xij}n anchors, as they link φ(θ0) and φ(θi). When these anchors ideally satisfy the following objective, they constitute set of Functional Dual Anchors (FDAs) for φ(θ0) and φ(θi) (i.e., τi): (cid:88) (cid:18) (cid:19) min xi1,...,xin cos dist θ (cid:12) (cid:12) Dist(cid:0)φ(θ, xij), φ(θi, xij)(cid:1) (cid:12) (cid:12)θ=θ0 j=1 , τi , (1) where cos dist(A, B) = 1 vec(A)vec(B) , vec denotes the operation that vectorizes matrix into AF BF vector in row-major order, and Dist() denotes differentiable distance function measuring the representation discrepancy between φ(θ0) and φ(θi). We primarily use cosine distance (cos dist), as semantic information is often encoded in direction rather than magnitude (Liu et al., 2017b; 2018). We also evaluate ℓ1 and ℓ2 distances in Section 5.3. Importantly, the set {xij}n j=1 induces gradients from representation discrepancies that align with the task vector τi in the input-representation space, and thereby serves as the FDAs of τi. Correspondingly, we construct separate set of FDAs {xij}n j=1 for each downstream checkpoint φ(θi), i.e., for each task vector τi. Gradient-based construction for FDAs. Due to the non-convex nature of Eq. 1, we solve it with gradient descent. We perform gradient-based search in the data space , where the loss landscape is shaped by fixed model parameters. We refer the process of the gradient-based search in the data space as the construction process of FDAs. This process can be formalized as: (cid:12) (cid:12) ij)(cid:1) (cid:12) (cid:12)θ=θ0 (cid:16) Xt cos dist Xt+1 = Xt + η Dist(cid:0)φ(θ, xt ij), φ(θi, xt (cid:17)(cid:27) , (cid:88) , τi θ (2) (cid:26) j=1 i1, . . . , xt in] Rdn denotes the candidate FDAs at t-th iteration; denotes the where Xt = [xt gradient-based optimizer and η denotes the update step. While the above gradient-based optimization offers practical solution in high-dimensional space, it may suffer from slow convergence or limited generalization due to non-convexity. To mitigate these issues, carefully designed initialization X0 is essential (Glorot & Bengio, 2010; He et al., 2015). We therefore focus on improving initialization to address these optimization challenges. To illustrate how the choice of initialization influences the resulting solution, we begin with an analysis based on simplified linear model. 2 W0x Wix2 Linear model analysis for initialization. We consider linear encoder φ, i.e., = with Rdd, Rd. The pretrained parameters and the downstream parameters on the i-th task are denoted by W0 and Wi, respectively. Assuming that Dist(W0x, Wix) = 1 2, we analyze the optimization dynamics of single anchor xt (with the task index omitted for clarity): xt+1 = xt + ηβt W xt, = 0, . . . , 1, (3) where = Wi W0 and βt = 1/(W W xt2 xt2). The derivation of Eq. 3 is provided in Appendix A. We assume that is full-rank matrix and the eigenvalue magnitudes follow long-tailed distribution. These assumptions are mild, as empirical evidence shows that parameter updates often follow an approximately low-rank structure (Gur-Ari et al., 2018; Hu et al., 2022; Zhang et al., 2025). Therefore, there exists spectral decomposition that W = ΛU , = [u1, . . . , ud] Rdd, Λ = diag(λ1, . . . , λd), with eigenvalues λ1 > > λd > 0 following long-tailed distribution. By construction, {ui}d i=1 form complete basis of the ddimensional space and remain fixed throughout optimization. Thus, we analyze the optimization trajectory by projecting xt onto this basis and tracking the dynamics of its coefficients, as formalized in the following proposition. The proof is provided in Appendix B."
        },
        {
            "title": "Technical Report",
            "content": "Proposition 2.1. Under the above setting, for any iteration t, xt can be expressed as the linear combination of {ui}d associated with basis vector ui is given by (cid:81)t = ci ci 0 (cid:1), where γj = ηβj > 0 and βj = 1/(W W xj2 xj2). i=1. Specifically, the coefficient ci (cid:0)1 γjλi j=1 Remark 2.1. For finite number of iterations , when 1 γjλi deviates significantly from 1, then ci is dominated by 1 γjλi due to the exponential growth or decay of the product term, and the effect of initialization is negligible. Conversely, if 1 γjλi is close enough to 1 that no exponential growth or decay arises within iterations, then ci remains primarily determined by ci 0. This latter case typically occurs when λi is close to zero. The initialization strategy. The above analysis suggests that the optimization has almost no effect on components ui corresponding to near-zero eigenvalues. This motivates an investigation into how initial values of these components affect the convergence of the cosine similarity. Following the above decomposition, we express = QΛU , where is an orthogonal matrix and Λ2 = Λ. For the j-th row, we can write Wj,: = (cid:80)d , αji = (QΛ)j,i. Here, we consider that i=1 αjiu does not amplify the low-energy directions of Λ and Λ and assume that the eigenvalues of Λ beyond the k-th index are near-zero, i.e., αj,i>k 0. From Proposition 2.1, that means that ci>k ci>k . We denote the j-th row of gradients induced by xt as j,:. Under the above assumptions, the cosine similarity between Wj,: and (cid:80)d (cid:113)(cid:80)k 0 i=1 αjiu i=1 α2 ji (cid:113)(cid:80)d , (cid:80)d (cid:113)(cid:80)d tu 2 i=1 ci i=1 ci (cid:113)(cid:80)d i=1 α2 ji (cid:113)(cid:80)k i=1 αji i=1 α2 ji 2 +(cid:80)d i=k+1 ci 0 2 , (4) j,: can be approximated as: (cid:80)k (cid:113)(cid:80)k i=1 αjici i=1 ci , γj = L(φ) i=k+1 ci 0 2 +(cid:80)d < 2 tu i=1 ci = γj j,: = γjx (cid:80)d where (W0xt)j,: this expression, the fixed energy in the tail components (cid:80)d hinders the increase of the cosine similarity at step t, which in turn slows down the convergence of the optimization. Moreover, in the idealized case where the first coefficients are perfectly aligned, the upper bound is given in Eq. 4. Thus, larger initial tail energy leads to lower optimal cosine similarity, whereas smaller tail eigenvalue energy enables faster convergence and results in higher optimal cosine similarity. Given the analysis above, we summarize an initialization principle for FDAs as follows: ; denotes the finetuning loss. From i=k+1 ci 2 Principle 2.2 (Initialization Principle). An effective initialization strategy should limit the energy of the initialization point within the tail subspace spanned by the task vector. Following the insight from the simplified linear model analysis, we propose two simple yet effective initialization strategies that can control the tail energy. Initialization strategy I: Linear Weight Sampling. We propose to sample the row vectors of the weight matrix as anchor initializations, since they typically also follow long-tailed spectrum and their total energy is similar to that of the overall , thereby avoiding excessive tail energy. Specifically, we initialize an anchor xij Rd by sampling row of weight matrix Wi Rqd of φ(θi). The process is formalized as xij = (Wi)lj ,:, lj {1, . . . , q}. Initialization strategy II: Scaled Gaussian Sampling. We first draw samples from standard normal distribution and then scale them using coefficient σ. Sampling from Gaussian ensures that the initialization spans the entire Rd, avoiding zero coefficients in the decomposition along ui. By controlling σ, we directly constrain the energy of the whole vector, which in turn limits the energy allocated to the tail subspace. The process is formalized as xij = σ xij, xij (0, Id)."
        },
        {
            "title": "2.3 PARAMETER OPTIMIZATION: LEVERAGING FDAS FOR MULTI-TASK MODEL MERGING",
            "content": "We leverage the knowledge encoded in FDAs by conducting the dual process of Eq. 1. We first initialize the merged model with the pretrained checkpoint, and then align the output of the model with the downstream checkpoints at all the FDAs. Assume that we have obtained groups of FDAs {xij}n j=1, one for each τi. We then optimize the model parameters with the following objective: min θ0 (cid:88) (cid:88) (cid:16) Dist i= j=1 φ(θ0, xij), φ(θi, xij) (cid:17) , (5) which is the standard adaptation from the pretrained model (i.e., the first usage of FDAs). The default Dist in Eq. 5 can be consistent with that in Eq. 1, and our ablation studies in Section 5."
        },
        {
            "title": "Technical Report",
            "content": "Algorithm 1: Model Merging with Functional Dual Anchors Input: Model architecture φ, pretrained parameters θ0, downstream parameters {θi}m i=1 Output: Merged parameter ˆθ, FDAs {x(l) ij }n for = 1 to do j=1, 1 m, 1 /* --- Stage I: FDA Construction --- for = 1 to do */ Initialization & Optimization: Initialize {x(l) Gaussian sampling as starting points and then solve the following objective with gradient descent: ij }n j=1 using linear weight sampling or scalable Dist(cid:0)φ(l)(θ(l), x(l) ij ), φ(θ(l) (cid:18) cos dist ij }n {x(l) , x(l) (cid:88) θ(l) (cid:12) (cid:12) ij )(cid:1) (cid:12) (cid:12)θ(l)=θ (l) 0 j=1 = arg min (l) i1 ,...,x Store the optimized anchors {x(l) (l) in ij }n j=1. j=1 end /* --- Stage II: Parameter Optimization using FDAs --- Aggregate anchors across tasks {x(l) Acquire the merged parameter by solving: ij }. ˆθ(l) = arg min (cid:88) (cid:88) θ(l) i=1 j= Dist(cid:16)φ(l)(θ(l), x(l) ij ), φ(l)(θ(l) , x(l) ij )(cid:17), from θ(l) 0 . end return ˆθ, {x(l) ij }n j=1 for 1 m, 1 L. , τ (l) (cid:19) . */ show that adaptation by FDAs remains robust to different choices of Dist. Please note that in the early optimization stage of Eq. 5, the guidance provided by FDAs approximates to the sum of task vectors. As the optimization proceeds, the guidance provided by FDAs adapts dynamically to the loss landscape of θt, while task vectors only prescribe fixed linear path starting from θ0. Refinement for the merged model. In particular, we propose the second usage of FDAs by employing them to refine the task vectors obtained from such methods. Given task vector based merged model φ (θ + (cid:80)m i=1 ϕi(τi)), we can refine {ϕi(τi)}m i=1 by minimizing the following objective: (cid:88) (cid:1), φ(θi, xij) (cid:17) . ϕi(τi), xij (6) min {ϕ(τi)}m i=1 (cid:88) (cid:88) i=1 j=1 (cid:16) Dist φ(cid:0)θ + i= : Rp Rp denotes possible adjustments of the task vector τi. As previously introduced, ϕi To demonstrate FDAs potential on complementing parameter-centric model merging, we evaluate FDAs on three representative data-free approaches, including TA (Ilharco et al., 2022), TSV (Gargiulo et al., 2025) and WUDI (Cheng et al., 2025). TSV derives ϕi(τi) by performing Singular Value Decomposition (SVD) and retaining the top components, while WUDI constructs them by reducing the discrepancy between (cid:80)m i=1 ϕi(τi) and {τi}m i=1."
        },
        {
            "title": "2.4 PRACTICAL IMPLEMENTATION",
            "content": "We discuss the practical implementation for Transformer-based foundation models in natural language (Vaswani et al., 2017; Liu et al., 2019), vision (Dosovitskiy et al., 2020; Caron et al., 2021). 0 }L }L l=1, {θ(l) l=1. For each layer l, we construct FDAs for τ (l) Layer-wise Construction and Adaptation. The construction process (Eq. 2) involves secondInstead, we adopt order gradients, which is challenging for the whole foundation models. layer-wise strategy by partitioning the architecture φ, parameters θ0, θi into parts: {φ(l)}L l=1, {θ(l) and perform adaptation accordingly. Please note that this strategy only requires replacing the entire model in the objectives (Eq. 1, 5, 6) with the corresponding layer-wise components. In our settings, one Resblock is deemed one layer. The overall procedure is summarized as pseudocode in Algorithm 1. j=1 Rd for the i-th task, where Shape of FDAs. Generally, we construct anchors {xij}n is the representation dimensionality. For Transformer-based models, the representation space is of the size token num embedding dim, as they operate at the token level. As embedding dim is fixed, the shape of FDAs is determined by and token num. For vision tasks, we follow the = θ(l) θ(l)"
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Evolution of Normalized singular values of FDAs in the FDA construction. We visualize the results of FDAs from the 12-th layer of the ViT-B/32 checkpoint on MNIST. σ = 101 denotes FDAs initialized by sampling from (0, Id) and scaling by 101; Weight denotes FDAs initialized from linear weight. FDAs of different initialization schemes tend to evolve into long-tailed structures. default token num. For natural language tasks, we set fixed token num. Increasing enlarges the solution space but at the cost of higher computational overhead. We discuss the effect of these hyperparameters in Section 5.2 and also list the detailed settings for experiments in Appendix C.3. The scale coefficient σ. smaller scaling factor σ reduces the tail energy of anchors. However, if σ is too small, the head energy is also suppressed, requiring more iterations. discussion on determining σ in practice is in Appendix C.5. For our experiments, we use σ = 0.01. The effect of σ is given in Figure 4 and Table 4."
        },
        {
            "title": "3 TOWARDS UNDERSTANDING FDA-ENCODED KNOWLEDGE",
            "content": "In this section, we investigate the knowledge encoded by FDAs. We analyze their energy distribution and loss during construction, and compare them with real data in both input-representation and parameter spaces. For analysis, FDAs are constructed from ViT-B/32 (Ilharco et al., 2022) and unfolded into [n token num, embedding dim] matrices. Details are in Appendix D. Observation 1: FDAs evolve into long-tailed spectrum structure during optimization. We perform SVD on the unfolded FDA matrices and normalize singular values by the largest one. From the example in Figure 3, the normalized tail singular values decays rapidly in construction. This implies that optimization guides FDAs to allocate less energy to the tail, therefore exhibiting long-tailed structure. The larger tail energy (σ = 10) results in slower allocation. Furthermore, loss curves (cos dist) of different initializations in Figure 4 are consistent with our analysis: the convergence speed first rises and then falls as σ decreases from 101 to 104. Notably, initializing FDAs with weights achieves the fastest convergence. Figure 4: Average loss curves. Figure 5: Evolution of subspace similarity of FDAs in the FDA construction. We visualize the results of FDAs from the 12-th layer of the ViT-B/32 checkpoint. σ = 101 denotes the FDAs initialized by sampling from (0, Id) and scaling by 101; Weight denotes the FDAs initialized from linear weight. FDAs of different initialization schemes tend to align the subspace spanned by real data. Figure 6: Evolution of projection energy ratio on pretrained model and merged model (TA). We visualize the results of FDAs from the 12-th layer of the ViT-B/32 checkpoints. σ = 101 denotes the FDAs initialized by sampling from (0, Id) and scaling by 101; Weight denotes the FDAs initialized from linear weight. The dashed line indicates the projection energy ratio of task vector induced by real data. Observation 2: The high-energy subspace of FDAs gradually aligns with that of real data. We adopt the features of real task-specific data as reference and also unfolded them. As both real data"
        },
        {
            "title": "Technical Report",
            "content": "(Pope et al., 2021) and FDAs exhibit long-tailed distributions, we measure subspace similarity of top 20% singular vectors via Projection Matrix (Fernando et al., 2013). From Figure 5, the similarity gradually increases as the optimization proceeds, which is consistent across all datasets. An analysis on whether optimization brings FDAs closer to the manifold of real features is in Appendix D. Observation 3: FDAs-induced adaptation increasingly aligns with that induced by real data. We analyze FDAs by re-projecting them into the parameter space, i.e., the adaptation they induce. We repeat the finetuning procedure in Ilharco et al. (2022) to sample parameter update vectors by real data and project the FDAs-induced adaptation onto their non-negative cone. As shown in Figure 6, the projection energy onto the sampled cone steadily increases during optimization, both for the pretrained model and merged model by TA. This indicates that the adaptation induced by FDAs is partially consistent with that of real data. Further, following Yang et al. (2024), we measure the representation discrepancies on real data and observe that FDAs also effectively mitigate representation bias as shown in Figure 7. Figure 7: Effects of FDAs on the representations."
        },
        {
            "title": "4 MAIN EXPERIMENTS AND RESULTS",
            "content": "In this section, we first introduce the experimental settings for FDAs and then the main results. Due to page limits, the remaining setups and results are presented in the Appendix C."
        },
        {
            "title": "4.1 EXPERIMENTAL SETTINGS",
            "content": "Downstream Models for Merging. The foundation models in vision and natural language are both considered. For vision tasks, we follow prior works (Ilharco et al., 2022; Yadav et al., 2023) and use eight publicly available domain-specific checkpoints of CLIP Vision Encoder (Radford et al., 2021). All three backbones, ViT-B/32, ViT-B/16, and ViT-L/14, are considered. For natural language tasks, following previous works (Yu et al., 2024; Xu et al., 2025), we adopt the downstream checkpoints on GLUE benchmark (Wang et al., 2018a) of RoBERTa-Base and RoBERTa-Large (Liu et al., 2019). We further extend FDAs to auto-regressive models, WizardMath-13B (Luo et al., 2023) and LLaMA-2-13B-code-Alpaca1, which are based on LLaMA-2-13B (Touvron et al., 2023), to validate the effectiveness on large models. Settings for FDAs. The layer-wise strategy of FDAs are adopted for the above foundation models. For construction, we use Gaussian and weight initialization. σ for Gaussian initialization is fixed at 0.01. For adaptation, two usages (Eq. 5, 6) of FDAs are both considered. For Eq. 6, we consider TA (Ilharco et al., 2022), TSV (Gargiulo et al., 2025), and WUDI (Cheng et al., 2025), where TA serves as classical baseline, while TSV and WUDI represent recent state-of-the-art approaches. For autoregressive models, FDAs are constructed and adapted only in each Resblocks feed-forward layer. Baseline Methods. In addition to the mentioned data-free merging methods, we include baselines that use task-specific data to guide adjustments: RegMean (Jin et al., 2022), Fisher merging (Matena & Raffel, 2022), AdaMerging (Yadav et al., 2023), and ProDistill (Xu et al., 2025)."
        },
        {
            "title": "4.2 EXPERIMENTS ON VISION AND LANGUAGE MODELS",
            "content": "Table 1, 2 and 3 show the results on the ViT-B/16, RoBERTa-Large and auto-aggressive models, respectively. We leave other results in Appendix C. We made the following observations: FDAs can effectively leverage existing task-specific knowledge for multi-task model merging. Specifically, comparing to the dual framework TA, our framework bring significant improvement: the multi-task performance of pretrained model adapted by FDAs achieves 87.26, compared with 73.94 of TA, representing an improvement of nearly 18%; meanwhile, the average GLUE score achieves 15.4% improvement. Moreover, FDAs also surpass many post-hoc enhancements of vanilla task vectors (Daheim et al., 2023; Yadav et al., 2023; Du et al., 2024; Xiong et al., 2024), while approaching the performance of current state-of-the-art methods. 1https://huggingface.co/layoric/llama-2-13b-code-alpaca"
        },
        {
            "title": "Technical Report",
            "content": "Method Pretrained Individual RegMean Fisher merging AdaMerging Representation Surgery ProDistill TA TSV WUDI FDA (Pretrained, Gauss) FDA (Pretrained, Weight) FDA (TA, Gauss) FDA (TA, Weight) FDA (TSV, Gauss) FDA (TSV, Weight) FDA (WUDI, Gauss) FDA (WUDI, Weight) SUN397 63.80 78.56 70.84 66.78 64.30 73.60 72.82 62.07 72.83 75.40 72.54 73.60 73.72 74.53 74.79 74.93 76.21 76.15 Cars 64.60 87.08 75.18 70.49 74.37 81.50 81.94 66.14 80.20 81.71 80.62 80.48 81.42 81.25 82.65 81.92 82.84 82.75 RESISC45 65.70 96.92 83.13 72.17 74.63 90.40 91.94 74.00 88.97 90.14 87.75 88.00 88.63 88.37 89.75 89.79 91.03 91.21 EuroSAT 54.50 99.78 94.44 80.19 94.89 98.50 99.52 76.48 97.22 98.52 98.44 98.26 98.37 98.37 98.37 98.33 98.93 98.89 SVHN GTSRB MNIST DTD 45.10 52.00 82.07 97.86 60.74 90.80 48.46 88.33 69.63 91.19 77.00 93.20 97.11 70.74 52.50 88.02 72.66 93.93 95.30 73.78 70.11 94.31 70.64 94.35 71.54 94.61 71.65 94.55 73.67 94.25 73.78 94.10 74.52 94.58 74.41 94. 43.30 99.17 82.43 68.14 94.94 97.40 97.65 73.79 93.94 96.55 93.43 93.41 94.44 94.28 94.47 93.78 96.32 96.24 51.70 99.76 98.66 96.60 97.95 98.90 99.60 98.52 99.27 99.44 99.38 99.31 99.39 99.34 99.40 99.36 99.40 99.39 Avg 55.00 92.65 82.03 73.89 82.74 88.80 88.92 73.94 87.38 88.85 87.07 87.26 87.77 87.79 88.42 88.25 89.23 89.19 - - - - - - - - - - +32.07 +32.26 +13.83 +13.85 +1.04 +0.87 +0.38 +0.34 Table 1: Performance of merging ViT-B-16 models across eight downstream vision tasks. The second section (from RegMean to ProDistill) include methods that use task-specific data, and the third section is data-free methods. FDA (init model, FDA init) denotes the choice of the initial model and the initialization strategies for FDAs, respectively. denotes the performance improvement compared to the initial model. Method Pretrained Individual RegMean Fisher merging AdaMerging ProDistill TA TSV WUDI FDAs (Pretrained, Gauss) FDAs (Pretrained, Weight) FDAs (TA, Gauss) FDAs (TA, Weight) FDAs (TSV, Gauss) FDAs (TSV, Weight) FDAs (WUDI, Gauss) FDAs (WUDI, Weight) CoLA 0.1679 0.6335 0.3449 0.2700 0.1027 0.4833 0.1635 0.4791 0.4201 0.3198 0.3883 0.4043 0.4511 0.5036 0.5021 0.4841 0.4848 SST-2 MRPC 0.7480 0.4897 0.9224 0.9001 0.5949 0.8922 0.7517 0.7856 0.7480 0.9335 0.8655 0.9427 0.7480 0.8716 0.7459 0.9323 0.7487 0.9232 0.7790 0.8463 0.7858 0.8911 0.9461 0.7692 0.7578 0.9404 0.7521 0.9438 0.7490 0.9427 0.7647 0.9404 0.7573 0.9392 STS-B -0.0471 0.9418 0.3509 0.2624 0.7432 0.7310 0.6603 0.6660 0.7345 0.6828 0.7230 0.7897 0.7926 0.7975 0.7418 0.7645 0.7546 QQP MNLI 0.3545 0.3159 0.8267 0.9055 0.5894 0.8045 0.4385 0.3159 0.7506 0.3159 0.8122 0.8269 0.6101 0.3159 0.6750 0.3300 0.6430 0.5393 0.7423 0.5605 0.5791 0.7410 0.7190 0.6916 0.7411 0.6518 0.7075 0.4128 0.7292 0.5062 0.7004 0.6778 0.7072 0.6979 QNLI 0.5054 0.9507 0.6132 0.5367 0.8578 0.8825 0.8716 0.7761 0.5746 0.6021 0.6207 0.7487 0.6965 0.8477 0.8146 0.5911 0.5656 RTE 0.4693 0.9222 0.6534 0.6426 0.6245 0.7545 0.7366 0.6751 0.5740 0.7726 0.7329 0.7076 0.7148 0.7365 0.7365 0.6643 0. Avg 0.3754 0.8754 0.6054 0.5004 0.6345 0.7873 0.5918 0.6599 0.6447 0.6632 0.6827 0.7220 0.7183 0.7127 0.7153 0.6984 0.6964 - - - - - - - - - +0.2878 +0.3073 +0.1302 +0.1265 +0.0528 +0.0554 +0.0537 +0.0517 Table 2: Performance of merging RoBERTa-Large models across eight NLU tasks. The second section (from RegMean to ProDistill) include methods that use task-specific data, and the third section is data-free methods. FDA (init model, FDA init) denotes the choice of the initial model and the initialization strategies for FDAs, respectively. denotes the performance improvement compared to the initial model."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH MBPP HEval Flexible knowledge modeling. FDA establishes that projecting task-specific knowledge into the input-representation space uncovers richer taskspecific information, enabling more effective model merging. Specifically, although FDAs and data-free parameter-centric methods leverage the same task-specific knowledge, FDAs still improve the merged models by these methods. The average improvement via FDAs on TA, TSV, and WUDI is nearly 5.10% on ViT-B/16, and about 13% on RoBERTa-Large. For the auto-regressive model, as we only adapt for feed-forward network, FDA still achieves 10% improvement on TA. Table 3: Performance of merging LLama2-13b-Alpaca and WizardMath-13B on Code and Math tasks. denotes the weight initialization; denotes the Gaussian initialization. Individual TA FDAs (TA, W) FDAs (TA, G) 0.634 0.560 0.602 0.600 0.282 0.082 0.098 0. 0.226 0.085 0.079 0.098 0.147 0.111 0.124 0.126 0.322 0.209 0.226 0."
        },
        {
            "title": "5 ABLATION AND EXPLORATIVE STUDY",
            "content": "We investigate the impact of different construction choices on the quality of the FDAs. The quality is defined by the average multi-task performance of models from Eq. 5, with higher performance indicating better-quality FDAs. Experimental details are provided in Appendix E."
        },
        {
            "title": "5.1 COMPARISON OF INITIALIZATION SCHEMES",
            "content": "We evaluate effects of initialization schemes on previous eight ViT-B/32 checkpoints. For Gaussian initialization, we consider: σ = 101, 100, 102, 104. As shown in Table 4, initialization signifi-"
        },
        {
            "title": "Technical Report",
            "content": "Figure 8: Multi-task performance of FDAs with different shape of FDAs on ViT-B/32 and RoBERTa-Base. cantly affects the quality of FDAs. As σ decreases from 101, the performance increases and then decreases, consistent with our analysis. FDAs by weights perform best, aligning with their lowest optimization loss (Figure 4). Despite wide range of settings, FDAs consistently outperform TA."
        },
        {
            "title": "5.2 THE SHAPE OF FDAS",
            "content": "We study the impact of the number of anchors (anchor num) and tokens (token num) on the quality of FDAs. We vary anchor num over {32, 64, 128, 256} and token num over {25, 50, 75, 100} for ViT-B/32 and {1, 5, 10, 20} for RoBERTa-Base, evaluating FDAs across their respective datasets. Performances at different adaptation epochs are also reported. From Figure 8, larger FDAs generally lead to better quality, as reflected in the multitask performance. This is reasonable as larger optimization space makes it easier to reach lower loss. However, for RoBERTa-Base, the average performance decreases when token num increases from 5 to 20. Further related analysis is in Appendix E. Init. Scheme σ = 101 σ = 100 σ = 102 σ = 104 Weight ViTB/32 77.42 81.78 83.03 71.15 83.75 Table 4: Multi-task performance of FDAs with different initialization schemes."
        },
        {
            "title": "5.3 THE EFFECT OF DISTANCE FUNCTIONS",
            "content": "Distance function Dist influences both the construction (Eq. 1) and the adaptation (Eq. 5, 6). We evaluate three metrics, cosine, ℓ1, and ℓ2, for their impacts on FDAs. From Figure 9, Dist matters more during construction than adaptation. Overall, cosine distance constructs the highest-quality FDAs, ℓ1 performs the worst, and our method consistently outperforms TA across all metrics. Figure 9: Multi-task performance of FDAs with different Dist functions on ViT-B/32 and RoBERTa-Base."
        },
        {
            "title": "5.4 OPTIMIZATION STEPS IN FDA CONSTRUCTION",
            "content": "We observe the effect of the number of optimization steps on FDAs. From Figure 10, more steps consistently improve their quality. For ViT-B/32, high-quality FDAs can be obtained from random noise in as few as 40 steps, indicating that our optimization is efficient."
        },
        {
            "title": "6 RELATED WORK AND CONCLUDING REMARKS",
            "content": "Figure 10: Multi-task performance of FDAs with different optimization steps. Related work. Recently, the prevailing paradigm in model merging is the scaled addition of task vectors (Ilharco et al., 2022). This paradigm offers perspective that knowledge could be transferred through parameters. Motivated by this insight, diverse parameter-centric methods for model merging have emerged. One line of works exploit the structural priors in the parameter space and adjust the task vectors (Yadav et al., 2023; Yu et al., 2024; Davari & Belilovsky, 2024; Zheng & Wang, 2024; Wei et al., 2025b; Xiong et al., 2024; Gargiulo et al., 2025; Cheng et al., 2025). In parallel, another line of works tries to introduce the data-driven priors to guide the adjustments (Matena & Raffel, 2022; Jin et al., 2022; Yang et al., 2023; 2024; Wei et al., 2025a; Xu et al., 2025; Yang et al., 2025). The unifying characteristic of both approaches is their emphasis on modeling the parameter space. Instead of modeling in the parameter space, FDAs encode the task-specific knowledge in the input-representation space, which provides an alternative perspective on model merging. Concluding remarks. This paper introduces novel input-space-centric model merging framework. The obtained synthetic data (FDAs) models the task-specific knowledge in the parameters through their induced gradient. FDAs can be used independently or alongside existing parameter-centric methods. Experiments demonstrate the effectiveness of FDAs in model merging."
        },
        {
            "title": "REFERENCES",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 17 Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. 2009. 17 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 5 George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In CVPR, 2022. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and cross-lingual focused evaluation. In International Workshop on Semantic Evaluation, 2017. 17 Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 17 Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: BenchIEEE Geoscience and Remote Sensing Magazine, 5(4):836, 2017. mark and state of the art. 17 Runxi Cheng, Feng Xiong, Yongxian Wei, Wanyun Zhu, and Chun Yuan. Whoever started the interference should end it: Guiding data-free model merging via task vectors. arXiv preprint arXiv:2503.08099, 2025. 1, 2, 5, 7, 9, 17 Mircea Cimpoi, Subhransu Maji, Ioannis Kokkinos, Svetlana Lazebnik, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 36063613, 2014. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning. What does bert look at? an analysis of berts attention. arXiv preprint arXiv:1906.04341, 2019. 20 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 17 Nico Daheim, Thomas Mollenhoff, Edoardo Maria Ponti, Iryna Gurevych, and Mohammad Emtiyaz Khan. Model merging by uncertainty-based gradient matching. arXiv preprint arXiv:2310.12808, 2023. 7 MohammadReza Davari and Eugene Belilovsky. Model breadcrumbs: Scaling multi-task model merging with sparse masks. In ECCV, 2024. William B. Dolan and Chris Brockett. Automatically constructing corpus of sentential paraphrases. In Proceedings of the International Workshop on Paraphrasing, 2005. 17 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 5, 20 Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Goh, Ho-Kin Tang, Daojing He, et al. Parameter competition balancing for model merging. In NeurIPS, 2024. 1, 7 Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual domain adaptation using subspace alignment. In ICCV, 2013."
        },
        {
            "title": "Technical Report",
            "content": "Antonio Andrea Gargiulo, Donato Crisostomi, Maria Sofia Bucarelli, Simone Scardapane, Fabrizio Silvestri, and Emanuele Rodola. Task singular vectors: Reducing task interference in model merging. In CVPR, 2025. 1, 2, 5, 7, 9, 17 Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, 2007. 17 Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010. 3 Guy Gur-Ari, Daniel Roberts, and Ethan Dyer. Gradient descent happens in tiny subspace. arXiv preprint arXiv:1812.04754, 2018. Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan In Proceedings of the Szpektor. The second pascal recognising textual entailment challenge. Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, pp. 785 794, 2006. 17 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. 3 Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. arXiv preprint arXiv:1709.00029, 2019. 17 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 3 Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. 1, 2, 5, 6, 7, 9, 17,"
        },
        {
            "title": "Shankar",
            "content": "Iyer,"
        },
        {
            "title": "Nikhil",
            "content": "dataset First-Quora-Dataset-Release-Question-Pairs, 2017. 17 release: Dandekar, Question and pairs. Kornel Csernai. quora https://data.quora.com/"
        },
        {
            "title": "First",
            "content": "Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849, 2022. 1, 7, 9, 17 Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting large-scale dataset of finegrained cars. arXiv preprint arXiv:1312.2234, 2013. 17 Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda Smith, James Rehg, and Le Song. Iterative machine teaching. In ICML, 2017a. 2 Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep hyperspherical learning. In NeurIPS, 2017b. 3 Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James Rehg, and Le Song. Decoupled networks. In CVPR, 2018. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 5, 7 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017."
        },
        {
            "title": "Technical Report",
            "content": "Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. 7, 17 Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. In NeurIPS, 2022. 1, 7, 9, 17 Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Ng, et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011. 17 Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894, 2021. Zeju Qiu, Weiyang Liu, Tim Xiao, Zhen Liu, Umang Bhatt, Yucen Luo, Adrian Weller, and Bernhard Scholkopf. Iterative teaching by data hallucination. In AISTATS, 2023. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 7, 17 Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? In NeurIPS, 2021. 20 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In EMNLP, 2016. 17 Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017. 1 Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NeurIPS, 2017. 2 Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In EMNLP, 2013. Johannes Stallkamp, Marc Schlipsing, Jens Salmen, and Christian Igel. The german traffic sign recognition benchmark: multi-class classification competition. International Journal of Computer Vision, 94(3):137150, 2011. 17 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 7 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 5 Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018a. 7, 17 Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. comprehensive survey of continual learning: Theory, method and application. IEEE transactions on pattern analysis and machine intelligence, 46(8):53625383, 2024. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018b. 2 Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018. 17 Qi Wei, Shuo He, Enneng Yang, Tingcong Liu, Haobo Wang, Lei Feng, and Bo An. Representation surgery in model merging with probabilistic modeling. In ICML, 2025a. 1, 3,"
        },
        {
            "title": "Technical Report",
            "content": "Yongxian Wei, Anke Tang, Li Shen, Zixuan Hu, Chun Yuan, and Xiaochun Cao. Modeling multitask model merging as adaptive projective gradient descent. arXiv preprint arXiv:2501.01230, 2025b. 1, 2, 9 Adina Williams, Nikita Nangia, and Samuel Bowman. broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017. 17 Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring large collection of scene categories. International Journal of Computer Vision, 119 (1):322, 2016. 17 Feng Xiong, Runxi Cheng, Wang Chen, Zhanqiu Zhang, Yiwen Guo, Chun Yuan, and Ruifeng arXiv preprint Xu. Multi-task model merging via adaptive weight disentanglement. arXiv:2411.18729, 2024. 1, 2, 7, Jing Xu, Jiazheng Li, and Jingzhao Zhang. Scalable model merging with progressive layer-wise distillation. arXiv preprint arXiv:2502.12706, 2025. 1, 3, 7, 9, 17 Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36:70937115, 2023. 1, 2, 7, 9, 17 Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning. arXiv preprint arXiv:2310.02575, 2023. 1, 3, 9, 17 Enneng Yang, Li Shen, Zhenyi Wang, Guibing Guo, Xiaojun Chen, Xingwei Wang, and Dacheng Tao. Representation surgery for multi-task model merging. arXiv preprint arXiv:2402.02705, 2024. 7, 9 Zongzhen Yang, Binhang Qi, Hailong Sun, Wenrui Long, Ruobing Zhao, and Xiang Gao. Cabs: Conflict-aware and balanced sparsification for enhancing model merging. arXiv preprint arXiv:2503.01874, 2025. 1, 3, Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In ICML, 2024. 1, 2, 7, 9, 17 Longhui Yu, Tianyang Hu, Lanqing Hong, Zhen Liu, Adrian Weller, and Weiyang Liu. Continual learning by modeling intra-class variation. Transactions on Machine Learning Research, 2023. 2 Yuanhe Zhang, Fanghui Liu, and Yudong Chen. Lora-one: One-step full gradient could suffice for fine-tuning large language models, provably and efficiently. arXiv preprint arXiv:2502.01235, 2025. 3 Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In WACV, 2023. 2 Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In ICLR, 2021. 2 Shenghe Zheng and Hongzhi Wang. Free-merging: Fourier transform for efficient model merging. arXiv preprint arXiv:2411.16815, 2024. 9 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 16 17 17 17 17 19 19"
        },
        {
            "title": "Table of Contents",
            "content": "A Derivation for equation 3 Proof for Proposition 2."
        },
        {
            "title": "Appendix",
            "content": "C.1 Details of downstream models for Merging . . . C.2 Details of Baseline methods . . . C.3 Details of FDAs . . . . . . . C.4 Remaining Results. . C.5 practical method for choosing the scaling coefficient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Results and Disucssions for Section 3 More Results and Discussions for Section"
        },
        {
            "title": "Technical Report",
            "content": "A DERIVATION FOR EQUATION 3 We first recall the settings. Given linear encoder φ, i.e., = with Rdd, Rd, the corresponding pretrained parameter and the downstream parameter on the i-th task are denoted by W0 and Wi, respectively. Assuming that Dist(W0x, Wix) = 1 2, we derive the iteration formula via gradient descent. 2 W0x Wix2 Proof. The objective function can be written as follows: (cid:18) min cos dist (cid:13)W Wix(cid:13) (cid:13) 2 (cid:13) 2 1 2 (cid:12) (cid:12) (cid:12)W =W0 (cid:19) , Wi W0 . (7) Let η > 0 be the step size and {0, 1, 2, . . . } the iteration index. The gradient-descent update is xt+1 = xt η xt cos dist (cid:18) 1 2 (cid:13) (cid:13)W xt Wixt (cid:13) 2 (cid:13) 2 (cid:12) (cid:12) (cid:12)W =W0 (cid:19) , Wi W0 . (8) Since cos dist(A, B) = 1 A,BF AF BF with A, BF := tr(AB), we rewrite equation 8 as xt+1 = xt + η t, := xt (cid:13) (cid:13) (cid:13) (cid:13) 1 2 (cid:13) (cid:13)W xt Wixt (cid:13) 2 (cid:13) 2 Step 1: Computing the -gradient. For the j-th row Wj,:, (cid:28) 1 2 (cid:13) (cid:13)W xt Wixt (cid:13) 2 (cid:13) 2 (cid:12) (cid:12) (cid:12)W =W0 (cid:13) (cid:13) (cid:13) (cid:13)F (cid:12) (cid:12) (cid:12)W =W0 , Wi W0 (cid:29) . (9) Wi W0F Wj,: 1 2 (cid:13) (cid:13)W xt Wixt (cid:13) 2 (cid:13) 2 (cid:12) (cid:12) (cid:12)W =W0 = (cid:0)W0xt Wixt (cid:1) . (10)"
        },
        {
            "title": "Stacking rows gives",
            "content": "W 1 2 (cid:13) (cid:13)W xt Wixt (cid:13) 2 (cid:13) 2 (cid:12) (cid:12) (cid:12)W =W0 = (W0 Wi) xtx = xtx , := Wi W0. (11) Step 2: Plugging equation 11 into t. The numerator in equation 9 becomes (cid:10)W xtx , (cid:11)"
        },
        {
            "title": "The denominator equals",
            "content": "F = tr(cid:0)xtx W (cid:1) = W xt = xt2 2. (12) (cid:13) (cid:13)W xtx (cid:13) (cid:13)F F = xt2 xt2 F ."
        },
        {
            "title": "Hence the scalar inside the gradient is",
            "content": "Therefore, xt2 xt2 F . (cid:18) = xt xt2 xt2 F (cid:19) . Step 3: Evaluating (assume xt = 0 and xt = 0). Using uAu2 = AA Au2 uu2 = u2 , = 1 F (cid:20) W xt xt2 xt2 xt2 xt3 2 (cid:21) xt . Step 4: Iteration in affine form. Write = σt xt + βt W xt, 15 (13) (14) (15) and (16) (17)"
        },
        {
            "title": "Technical Report",
            "content": "where σt = xt2 F xt3 2 > 0, βt = 1 F xt2 xt < 0."
        },
        {
            "title": "Hence",
            "content": "xt+1 = xt + ηt = (1 + ησt) xt + ηβt W xt. (18) Note that ησt generally is small positive number. Thus, for better analysis, we approximate the iteration as xt+1 = xt + ηt = xt + ηβt W xt. (19) PROOF FOR PROPOSITION 2.1 Proof of Proposition 2.1. We start from the iteration in equation 3: xt+1 = xt + ηβt W xt, = Wi W0, = 0, . . . , 1. And we have that: W = ΛU , where = [u1, . . . , ud] Rdd is orthogonal and Λ = diag(λ1, . . . , λd) with λ1 > > λd > 0. Step 1: Project xt onto the eigenbasis. Let xt = (cid:88) i= ci ui, where ci = xt. Then W xt = W (cid:88) i=1 ci ui = (cid:88) i=1 ci λi ui. Step 2: Plugging the projection into equation 3. xt+1 = xt + ηβt (cid:88) i=1 ci tλiui = (cid:88) i= ci tui + (cid:88) i=1 (ηβtλici t)ui = (cid:88) i=1 ci t(1 + ηβtλi)ui. Define γt = ηβt > 0, then ci t+1 = ci t(1 γtλi), = 0, . . . , 1. Step 3: Recursion. By recursion, we can get the formula in proposition 2.1: t1 (cid:89) = ci ci 0 (1 γjλi), = 1, . . . , d. j="
        },
        {
            "title": "C EXPERIMENT DETAILS",
            "content": "C.1 DETAILS OF DOWNSTREAM MODELS FOR MERGING For vision task, we follow the setup in previous works (Ilharco et al., 2022; Yadav et al., 2023). Specifically, we adopt eight public, domain-specific foundation models from Ilharco et al. (2022), which obtained by finetuning the pretrained Vision Encoder of CLIP (Radford et al., 2021) on the following datasets: SUN397 (Xiao et al., 2016), Cars (Krause et al., 2013), RESISC45 (Cheng et al., 2017), EuroSAT (Helber et al., 2019), SVHN (Netzer et al., 2011), GTSRB (Stallkamp et al., 2011), MNIST (LeCun et al., 1998) and DTD (Cimpoi et al., 2014). All sizes of these models, i.e., ViT-B/32, ViT-B/16 and ViT-L/14, are taken into consideration. For natural language processing task, we also follow previous works (Yu et al., 2024; Xu et al., 2025). Specifically, we consider the downstream models of eight datasets from the GLUE benchmark (Wang et al., 2018a), including CoLA (Warstadt et al., 2018), SST-2 (Socher et al., 2013), MRPC (Dolan & Brockett, 2005), STS-B (Cer et al., 2017), QQP (Iyer et al., 2017), MNLI (Williams et al., 2017), QNLI (Wang et al., 2018a; Rajpurkar et al., 2016), RTE (Wang et al., 2018a; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). To obtain the downstream models, we follow the finetuning procedure of Yu et al. (2024) on publicly available pretrained RoBERTa-Base and RoBERTa-Large. For auto-regressive models, we follow the practice in Yu et al. (2024) and consider two expert models: the Math expert model WizardMath-13B (Luo et al., 2023) and the Code expert model LLaMA-2-13B-Code-Alpaca. We use four datasets for evaluation, including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2020), HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). For evaluation, we adopt the evaluation codes of Xu et al. (2025). C.2 DETAILS OF BASELINE METHODS For data-based baselines (i.e., RegMean (Jin et al., 2022), Fisher merging (Matena & Raffel, 2022), AdaMerging (Yang et al., 2023), and ProDistill (Xu et al., 2025)), we follow the released implementations. For ViT, we adopt the results reported in Xu et al. (2025) as they rely on the same public checkpoints. For LLM, we simply set the coefficient of TA method as 0.5, which is adopted in Xu et al. (2025). For data-free baseline methods (i.e., TA (Ilharco et al., 2022), TSV (Gargiulo et al., 2025), and WUDI (Cheng et al., 2025)), we use their publicly available open-source code. We try our best to ensure fair and strong baselines. C.3 DETAILS OF FDAS All FDAs in our experiments follows the layer-wise manner. We keep the settings of the construction and adaptation consistent across layers. Both Gaussian and parameter initializations are considered. For Gaussian initialization, we set σ = 0.01. Both initializations share the same settings. FDA Construction. For ViT and RoBERTa, we first set the number of anchors as 64 for each task. Then, the token num of FDAs for ViT follows the default settings: 50 for ViT-B/32, 197 for ViT-B/16, and 257 for ViT-L/14. For RoBERTa-Base and RoBERTa-Large, we fix the token num as 5. To optimize these anchors, we adopt the AdamW optimizer (Loshchilov & Hutter, 2017), iterating for 1200 steps with learning rate of 1e 2. For WizardMATH and llama-2-13b-alpaca, we construct FDAs for feed-forword networks. Thus, we only set the number of anchors as 8192. We also adopt AdamW optimizer, iterating for 200 steps with learning rate of 1e 2. All the above optimizations are performed with full batch size. Parameter Optimization. We adopt the Adam optimizer to optimize parameters. There are three hyperparameters: learning rate, batch size and optimization epochs. For FDAs from different initialization schemes, we adopt the same settings. When the initial model is initialized by pretrained parameter, we adopt Eq. 5. We use batch size of 128 for all models. For all ViT models, we set the learning rate to 1e-5 and train for 100 epochs. For all RoBERTa, we use learning rate of 5e-5, training for 100 epochs on the base model and 50 epochs on the large model. When the initial model is initialized by task-vector-based merging method, we adopt Eq. 6. We follow previous works (Yang et al., 2023; Xu et al., 2025) and use large learning rate 1e 2. Generally, for ViT and"
        },
        {
            "title": "Technical Report",
            "content": "Method SUN397 Individual RegMean Fisher merging AdaMerging Representation Surgery ProDistill TA TSV-M WUDI FDAs (Pretrained, Gauss) FDAs (Pretrained, Weight) FDAs (TA, Gauss) FDAs (TA, Weight) FDA (TSV, Gauss) FDA (TSV, Weight) FDA (WUDI, Gauss) FDA (WUDI, Weight) 75.34 67.47 63.95 63.69 71.20 68.90 55.16 69.08 70.92 67.46 68.12 69.48 69.61 71.17 71.23 72.71 72.82 Cars 77.73 66.63 63.84 65.74 72.00 71.21 54.98 70.92 71.38 69.05 70.46 71.43 71.83 73.25 73.71 73.71 73. RESISC45 EuroSAT SVHN GTSRB MNIST 95.98 81.75 66.86 77.65 92.30 89.89 66.68 85.67 85.68 81.87 83.94 83.79 85.27 86.46 86.76 86.97 87.02 99.89 93.33 83.48 91.00 99.00 99.37 78.89 95.15 96.33 96.89 97.07 96.89 97.00 91.19 97.19 96.67 96. 97.46 86.68 79.54 82.48 92.20 96.13 80.21 92.02 94.27 94.02 94.08 94.43 94.33 94.25 94.11 94.20 94.13 98.73 79.92 60.11 93.12 97.90 95.29 69.68 91.93 94.51 89.58 90.03 91.20 91.59 92.03 91.79 93.99 93.76 99.69 97.30 91.27 98.27 99.00 99.46 97.34 99.25 99.47 99.28 99.33 99.36 99.39 99.39 99.44 99.42 99.40 DTD 79.36 60.16 49.36 62.29 76.10 68.03 50.37 69.20 69.47 66.12 66.97 68.67 69.10 70.64 70.74 70.32 70.59 Avg 90.52 79.15 69.80 79.28 87.50 86.04 69.16 84.15 85.26 83.03 83.75 84.41 84.76 85.55 85.62 86.00 86.04 Table 5: Performance of merging ViT-B/32 models across eight downstream vision tasks. FDA (init model, FDA init) denotes the choice of the initial model and the initialization strategies for FDAs, respectively. Pretrained denotes the initial model is the pretrained model."
        },
        {
            "title": "Method",
            "content": "SUN397 Individual RegMean Fisher merging AdaMerging Representation Surgery ProDistill TA TSV WUDI FDAs (Pretrained, Gauss) FDAs (Pretrained, Weight) FDAs (TA, Gauss) FDAs (TA, Weight) FDAs (TSV, Gauss) FDAs (TSV, Weight) FDAs (WUDI, Gauss) FDAs (WUDI, Weight) 82.32 74.04 71.28 75.96 80.30 77.73 74.16 79.00 81.15 77.59 77.91 78.96 78.92 79.84 79.69 81.09 81."
        },
        {
            "title": "Cars",
            "content": "92.35 87.22 85.18 89.42 90.80 90.04 82.09 89.99 90.95 90.05 90.14 90.41 90.35 90.66 90.60 91.16 91.27 RESISC"
        },
        {
            "title": "GTSRB MNIST",
            "content": "97.38 88.52 81.59 90.08 94.30 94.43 86.67 93.95 94.00 92.75 92.84 93.13 93.19 93.95 93.68 94.41 94.48 99.78 98.15 89.67 96.59 98.20 99.48 94.07 99.15 99.33 99.04 99.04 99.07 99.11 99.19 99.19 99.26 99.26 98.11 92.89 81.51 91.78 94.10 97.71 87.91 95.34 96.21 95.42 95.44 95.63 95.53 95.81 95.51 96.21 96.11 99.24 90.22 83.39 97.52 98.70 98.26 86.77 96.16 98.04 96.78 96.56 97.15 96.83 97.35 97.05 97.86 97.72 99.69 99.27 96.31 98.91 99.20 99.63 98.94 99.51 99.63 99.56 99.59 99.58 99.61 99.61 99.60 99.68 99."
        },
        {
            "title": "DTD",
            "content": "84.15 69.84 65.48 77.61 82.50 78.24 65.69 79.10 80.64 76.76 76.86 77.23 77.13 79.04 78.40 80.37 80."
        },
        {
            "title": "Avg",
            "content": "94.13 87.52 81.80 89.73 92.30 91.94 84.54 91.52 92.49 90.99 91.05 91.40 91.33 91.93 91.71 92.51 92.45 Table 6: Performance of merging ViT-L/14 models across eight downstream vision tasks. FDA (init model, FDA init) denotes the choice of the initial model and the initialization strategies for FDAs, respectively. Pretrained denotes the initial model is the pretrained model."
        },
        {
            "title": "Method",
            "content": "Individual RegMean Fisher merging AdaMerging ProDistill TA TSV WUDI FDAs (Pretrained, Gauss) FDAs (Pretrained, Weight) FDAs (TA, Gauss) FDAs (TA, Weight) FDAs (TSV, Gauss) FDAs (TSV, Weight) FDAs (WUDI, Gauss) FDAs (WUDI, Weight)"
        },
        {
            "title": "CoLA",
            "content": "0.626 0.2078 0.123 0.0864 0.4968 0.0257 0.0722 0.1459 0.2178 0.2229 0.2304 0.2119 0.1923 0.2604 0.2231 0.2872 SST-2 0.9427 0.9266 0.8188 0.8968 0.9209 0.9048 0.9014 0.922 0.9232 0.9209 0.9174 0.9083 0.8865 0.8979 0.9278 0."
        },
        {
            "title": "MRPC",
            "content": "0.8946 0.8215 0.7598 0.795 0.8340 0.7916 0.806 0.7925 0.8144 0.8057 0.8124 0.8215 0.7962 0.8200 0.7838 0.8108 STS-B 0.9070 0.5350 -0.1194 0.398 0.6623 0.2873 0.3081 0.3832 0.4256 0.2291 0.6029 0.5445 0.4612 0.2487 0.3999 0."
        },
        {
            "title": "QQP",
            "content": "0.8986 0.8141 0.7319 0.7936 0.8044 0.8169 0.8365 0.8393 0.8019 0.8117 0.7763 0.7929 0.8064 0.8231 0.8218 0."
        },
        {
            "title": "MNLI",
            "content": "0.8721 0.7551 0.6056 0.7579 0.7987 0.7437 0.8031 0.7917 0.7065 0.6871 0.7679 0.75 0.7796 0.7930 0.8015 0."
        },
        {
            "title": "QNLI",
            "content": "0.9257 0.8541 0.507 0.7128 0.8918 0.7216 0.7893 0.7972 0.7928 0.8294 0.7366 0.7424 0.7695 0.8052 0.7926 0."
        },
        {
            "title": "RTE",
            "content": "0.7581 0.7256 0.4874 0.7076 0.7148 0.7220 0.7401 0.7292 0.7365 0.7329 0.6968 0.7112 0.6895 0.7256 0.7292 0."
        },
        {
            "title": "Avg",
            "content": "0.8531 0.7050 0.4893 0.6435 0.7655 0.6267 0.6571 0.6751 0.6773 0.6550 0.6926 0.6853 0.6727 0.6717 0.6850 0.6913 Table 7: Performance of merging RoBERTa-Base models across eight NLU tasks. RoBERTa, we use batch size of 128, also training for 100 epochs (15 epochs for RoBERTa-Large). For the initial ViT model by WUDI, we set the batch size as 512 and train for 25 epochs. For the auto-regressive model, we use batch size of 8192, training for 50 epochs."
        },
        {
            "title": "Technical Report",
            "content": "C.4 REMAINING RESULTS. We present the experimental results on ViT-B/32, ViT-B/L-14 and RoBERTa-Base on Table 5, 6 and 7, respectively. FDAs bring slight improvements on the WUDI-initialized merged model. Please note that WUDI-initialzed model has already achieves 98.3% of the performance of eight individual models. That means that this initialization is already situated in well-optimized local minima. Generally, the remaining results are consistent with the observations in our main paper. C.5 PRACTICAL METHOD FOR CHOOSING THE SCALING COEFFICIENT As discussed in the main paper, the scaling coefficient σ is crucial for the convergence. We provide practical heuristic to choose σ. Specifically, we fix one layer with the same initial FDAs and evaluate set of candidate σ values by comparing their alignment after fixed number of iterations, selecting the σ that yields the best alignment as the scaling coefficient."
        },
        {
            "title": "Technical Report",
            "content": "D MORE RESULTS AND DISUCSSIONS FOR SECTION 3 In this section, we describe the details in the investigation for the knowledge in FDAs, and then present further examples. ij }64 Details of Observation 1. We first follow the same construction procedure in Section C.3 and obtain sets of FDAs {x(l) j=1, xij R50768, = 1, . . . , 8; = 1, . . . , 12. Then we unfold each set into the matrix (l) R3200768, treating each token embedding as representation unit (Clark et al., 2019; Dosovitskiy et al., 2020; Raghu et al., 2021). We conduct the singular value decomposition for (l) i,768. The normalized singular values λ(l) are computed as λ(l) and obtain the sorted singular values: λ(l) ij /λ(l) i,1. We visualize more results in Figure 11. i,1, . . . , λ(l) ij = λ(l) ij Figure 11: Evolution of Normalized singular values of FDAs in the FDA construction. σ = 101 denotes FDAs initialized by sampling from (0, Id) and scaling by 101; Weight denotes that of FDAs initialized from linear weight. FDAs of different initialization schemes tend to evolve into long-tailed structures. Details of Observation 2. For the features of real task-specific data in the l-th layer, we attach hooks at the corresponding layers of the downstream checkpoints to extract features. 64 real examples are randomly sampled from validation dataset for each task. We unfold them into the matrices denoted as ˆX , = 1, . . . , 8; = 1, . . . , 12. Then we perform the projection matrix method to analyze the similarity of subspaces spanned by the top 20% singular vectors. Assume that (l) are spanned by their top 20% singular vectors, the similarity is measured by: and ˆU (l) Sim = ˆP tr(P ) 768 20% ,"
        },
        {
            "title": "Technical Report",
            "content": "Figure 12: Evolution of subspace similarity of FDAs in the FDA construction. σ = 101 denotes the FDAs initialized by sampling from (0, Id) and scaling by 101; Weight denotes the FDAs initialized from linear weight. FDAs of different initialization schemes tend to align the subspace spanned by real data. Figure 13: The low-dimensional visualization via t-SNE of FDAs and real data. , ˆP where ˆ ˆ (l) (l) ). We present more results in Figure 12. )( ( are the projection matrices computed by (l) = (l) (U (l) ), ˆP (l) = Moreover, we adopt the t-SNE visualization to observe whether the optimization process drives FDAs closer to the manifold of real features. As shown in Figure 13, there is no clear evidence that optimization process makes the initial anchors closer to the manifold of real data. Details of Observation 3. To acquire the parameter update vectors, we follow the finetuning procedure in Ilharco et al. (2022) and sample the parameter update vectors from consecutive batches. The fine-tuning procedure is performed starting from both the pretrained model and the merged model obtained by TA. This yields two sets of updated task vectors: one initialized from the pretrained model and the other from the merged model. Instead of completing full fine-tuning, we sample 512 vectors per task and then stop. These sampled vectors are used to span the corresponding cones. We denote the sampled vectors for i-th task as wi,1, . . . , wi,512 Rp. We use the τ Rp to denote the adaptation direction induced by FDAs. Then, we solve the following non-negative least square problem to obtain the projection energy ratio: Ratioi = [wi,1, . . . , wi,512] αiF τiF , where αi = arg min αij 0 j=1,...,512 τi [wi,1, . . . , wi,512] αi2 ."
        },
        {
            "title": "Technical Report",
            "content": "Figure 14: Evolution of projection energy ratio on pretrained model and merged model (TA). σ = 101 denotes the FDAs initialized by sampling from (0, Id) and scaling by 101; Weight denotes the FDAs initialized from linear weight. Figure 15: The Effect of FDAs in the representation. In general, the adaptation with FDAs substantially mitigates the representation bias observed in both pretrained and merged models. Note that solving the above optimization problem, even for single layer, is nearly intractable. Therefore, we compute it separately for the attention block and the MLP layer, and then report the averaged energy. We present more visualization of projection energy in Figure 14 and the effects on the representation in Figure 15. Although the improvement in shallow-layer projection energy is not significant during the optimization, FDAs still effectively alleviate the overall representation bias. In the paper, the task vectors obtained from real data are also projected onto these cones. We take the projection energy ratio as the reference. Specifically, for the pretrained model, we directly use the task vectors corresponding to the publicly available fine-tuned checkpoints; for the merged model, we fine-tune for one epoch to obtain checkpoints, from which the task vectors are generated."
        },
        {
            "title": "Technical Report",
            "content": "E MORE RESULTS AND DISCUSSIONS FOR SECTION 5 In this section, we first present more experimental details about ablation study. Then, we further analyze the effect of token num in RoBERTa-Base. Experimental Details. To observe the effect of different initialization schemes, we follow the same construction and adaptation settings as in the experiments on ViT-B/32, while only varying the initialization scheme. For the shape of FDAs, we also follow the same construction settings and keep the batch size and learning rates in the adaptation, while varying the shape of FDAs and the adaptation steps. For the effect of distance function, we only varies the distance functions both in construction and adaptation process. For optimization steps, we only vary the optimization steps in the construction phase. token num 5 10 20 5 10 CoLA 0.2178 0.2685 0.2421 0.2020 0.2432 0.2468 SST-2 0.9232 0.9197 0.9243 0.9243 0.9255 0. MRPC 0.8144 0.8043 0.8082 0.8046 0.8004 0.7981 STSB 0.4256 0.1604 0.0904 QQP 0.8019 0.8214 0.8226 0.8030 0.8206 0.8218 MNLI 0.7065 0.7256 0.7410 0.7137 0.7289 0. QNLI 0.7928 0.8215 0.8294 0.8078 0.8252 0.8270 RTE 0.7365 0.7365 0.7148 0.7112 0.7184 0. Avg 0.6773 0.6572 0.6466 0.7100 0.7232 0.7251 Table 8: Performance on each dataset of RoBERTa-Base with different token num. The upper part includes STSB, while the lower part excludes STSB (STSB is denoted as ). Further analysis on token num. As shown in the Figure 8, we observe that the average performance decreases when the token num increases from 5 to 20 for RoBERTa-Base models, which appears to contradict the trends observed in ViT-B/32. We further analyze the performance of each dataset. As shown in Table 8, we find that the performance drop is mainly attributed to STSB. Therefore, we conduct merging without STSB and observe that the performance consistently increases with larger token num, which is consistent with the trend in ViT-B/32. We hypothesize that FDAs yield better performance when their shape is closer to that of the real data space."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong",
        "Westlake University"
    ]
}