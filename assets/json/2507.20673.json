{
    "paper_title": "Geometric-Mean Policy Optimization",
    "authors": [
        "Yuzhong Zhao",
        "Yue Liu",
        "Junpeng Liu",
        "Jingye Chen",
        "Xun Wu",
        "Yaru Hao",
        "Tengchao Lv",
        "Shaohan Huang",
        "Lei Cui",
        "Qixiang Ye",
        "Fang Wan",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 3 7 6 0 2 . 7 0 5 2 : r Geometric-Mean Policy Optimization Yuzhong Zhao1 Yue Liu1 Junpeng Liu2 Jingye Chen3 Xun Wu4 Yaru Hao4 Tengchao Lv4 Shaohan Huang4 Lei Cui4 Qixiang Ye1 Fang Wan1 Furu Wei4 1UCAS 2CUHK 3HKUST 4Microsoft Research https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO. Figure 1: Comparison between vanilla Group Relative Policy Optimization (GRPO) and our proposed GMPO. GRPO optimizes the arithmetic mean of token-level rewards, while GMPO uses the geometric mean (left). For GRPO, extreme importance sampling ratios emerge frequently during training, which implies unstable policy updates. In contrast, GMPO results in more stable importance sampling ratios with less extreme values and narrower range (right)."
        },
        {
            "title": "Introduction",
            "content": "As test-time scaling becomes key research focus in the large language model community, recent post-training methods have increasingly aimed to extend chain-of-thought (CoT) generation, enhancEqual contribution. Work done during internship at Microsoft Research. ing reasoning capabilities of large language models. Recent advances like Group Relative Policy Optimization (GRPO) [16], utilize multiple sampled responses per input prompt to calculate relative rewards and advantages ( ˆA in Figure 1 left) within each group, resulting in notable improvements in reasoning performance. By maximizing the arithmetic mean of token-level rewards, GRPO achieve strong results on complex tasks like math, code generation, and multimodal reasoning. ˆA, During GRPO training, the importance-weighted reward for each token is given by πθ(otq,o<t) πθold (otq,o<t) where πθ(otq,o<t) πθold (otq,o<t) is the importance sampling ratio. This ratio plays key role in PPO [15] and GRPO [16], ensuring that policy updates are approximately based on data from the current policy πθ. If the ratio deviates too far from 1, it indicates excessive policy fluctuations, which can make policy updates overly aggressive and cause instability. Maintaining the ratio within controlled range ensures more stable and reliable training. As shown in Figure 1 (top left), the GRPO training objective involves the arithmetic mean of tokenlevel rewards, which is sensitive to tokens with outlier importance-weighted rewards. As shown in Figure 1 (right), as training progresses, the range of the importance sampling ratio of GRPO expands. Such ratios lead to unstable policy updates and degrade the model performance. To stabilize the training process, GRPO uses clipping range of (1 ϵ, 1 + ϵ) to limit large deviations of importance sampling ratios. However, as illustrated in DAPO [21], this constraint causes limited exploration and early deterministic policy, which can hinder the scaling process. To alleviate the instability while enhancing exploration capabilities of GRPO, we propose GeometricMean Policy Optimization (GMPO) as shown in Figure 1 (left bottom). Compared to GRPO, the training objective of GMPO involves the geometric mean of token-level rewards. Unlike the arithmetic mean used in GRPO, the geometric mean is inherently less sensitive to outliers, making policy updates more stable and reliable. As shown in Figure 1 (right), as training progresses, the range of GMPOs importance sampling ratio stabilizes, with fewer extreme values than GRPO. With this modification, we can maintain stable policy optimization while using larger clipping range (ϵ1, ϵ2) for greater policy exploration. To further emphasize the advantages of GMPO, we provide detailed theoretical and experimental analyses to justify its training objective. First, we show that GMPO objective results in narrower value range compared with the GRPOs objective, which suggests reduced training variance and is associated with improved stability of policy updates. Second, we compared GRPO and GMPO from the perspective of gradient, where GMPO provides holistic view for policy updates and is robust to extreme values. Third, we observe that as training progresses, GMPO maintains smaller KL divergence from the pre-RL model and exhibits higher token entropy than GRPO, indicating improved stability and greater policy exploration. Extensive experiments on both language and multimodal reasoning tasks highlight the superiority of GMPO over GRPO. Specifically, on five mathematical reasoning benchmarks of varying difficultyAIME24, AMC, MATH500, Minerva (MIN) [10], and OlympiadBench (OLY) [9]GMPO improves the average Pass@1 accuracy by 4.1% (63.4% vs. 59.3%) with DeepSeek-R1-Distill-Qwen7B compared to GRPO [16]. On Geometry3K multimodal reasoning benchmark, GMPO increases the average Pass@1 accuracy by 1.4% (54.7% vs. 53.3%) with Qwen2.5-VL-Instruct-7B. The contributions of this study are summarized as follows: We propose Geometric-Mean Policy Optimization (GMPO), which stabilizes the GRPO algorithm by maximizing the geometric mean of token-level rewards. We present detailed theoretical and experimental analyses to prove that GMPO is more stable from the gradient perspective, maintains smaller KL divergence from the pre-RL model, and exhibits higher token entropy than GRPO. On five mathematical reasoning benchmarks, GMPO outperforms GRPO by 4.1% in average Pass@1 accuracy with the 7B model. Similarly, on the Geometry3K multimodal reasoning benchmark, GMPO surpasses GRPO by 1.4% in Pass@1 accuracy."
        },
        {
            "title": "2 Background",
            "content": "2.1 Related Works Recently, reinforcement learning (RL) has become pivotal approach for post-training of large language models (LLMs). RL, drawing from verifiable rewards, has been shown to enhance largescale reasoning, as exemplified by DeepSeek-R1. Variants of Proximal Policy Optimization (PPO) [15] have proliferated, including GRPO [16, 4], SRPO [25], DAPO [21], Dr.GRPO [11], OPO [6], EMPO [24], AAPO [19], and BNPO [18], each designed to augment efficiency and performance. GRPO stands out as PPO variant by eliminating the need for value models that are difficult to train and resource-intensive, while still achieving strong performance across reasoning benchmarks, including mathematics, coding, and question answering. SRPO utilizes history resampling to retain critical problems for future training stages, while DAPO employs dynamic sampling to sift through extreme samples, ensuring effective training outcomes. Dr.GRPO addresses the length bias through an improved methodology, and OPO integrates practically feasible formulation of the optimal reward baseline that minimizes gradient varianceand. EMPO integrates semantic entropy into its optimization objectives, thereby assessing uncertainty and incorporating it into advantage calculations. AAPO proposes an advantage estimation method that incorporates advantage momentum. BNPO adaptively normalizes rewards using Beta distribution with dynamically updated parameters. Beyond the aforementioned PPO variants, other advancements in RL for LLMs focus on different facets. For example, PRIME [3] introduces scalable reinforcement learning framework to improve reasoning capabilities. Data-centric approaches are also crucial, with Open-Reasoner-Zero [8] emphasizing robust data curation by assembling 129k training samples from diverse sources, coupled with strict quality control and curriculum learning. Complementing this, Eurus [22] introduces novel, large-scale, high-quality alignment dataset alongside new reward modeling objective. GPG [2] simplifies training by eliminating surrogate losses, critics, and KL constraints, offering more direct approach to policy optimization. 80/20 rule [17] improves LLM reasoning by optimizing high-entropy minority tokens. Despite the progress made, the stability of LLM reinforcement learning is seldom addressed, yet it is key to developing more reliable and scalable RL systems. 2.2 Preliminary The Group Relative Policy Optimization algorithm is initially proposed in DeepSeek-math [16]. The core idea is to estimate the baseline through relative reward within group of rollouts, which reduces the computational cost of the critic model and improves the training stability. Specifically, for each question from the training set Q, GRPO samples group of rollouts {o1, o2, , oG} from the old policy πθold and calculates the corresponding rewards {r1, r2, , rG}. Then the policy model πθ is optimized through maximizing the following objective: JGRPO(πθ) = qQ,{oi}G i=1πθold (q) 1 (cid:88) i= 1 oi oi (cid:88) t=1 (cid:26) min (cid:20) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) ˆAi, clip (cid:16) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) , 1ϵ, 1+ϵ (cid:17) ˆAi βDKL [πθ πref ] (cid:21)(cid:27) , (1) where ˆAi = ri mean({r1, r2, rG}) std({r1, r2, rG}) , πθ(oi,tq, oi,<t) represents the generation probability of the t-th token in the i-th rollout based on the question and the policy πθ. ˆAi is the advantage of the i-th rollout and is calculated by normailizing the rewards that belong to the same group according to GRPO. Following Dr. GRPO [11], we ignore the KL divergence term DKL [πθ πref ] for simplicity and memory saving. The training objective of GRPO is equivalent to the arithmetic mean of token-level rewards (We ignore the clip range term for clarity), which can be formatted as: JGRPO(πθ) = qQ,{oi}G i=1πθold (q) 1 (cid:88) i=1 1 oi oi (cid:88) t= πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) ˆAi . (2) In practice, the rollouts are sampled from the old policy πθold . To ensure that policy updates are approximately based on data sampled from the current policy πθ, the advantage/normalized reward 3 of each token ˆAi is weighted by the importance sampling ratio πθ(otq,o<t) importance sampling ratios often lead to outliers, and vice versa. πθold (otq,o<t) . Therefore, extreme"
        },
        {
            "title": "3 Geometric-Mean Policy Optimization",
            "content": "As shown in Figure 1(right), during GRPO training, we observe tokens with extreme importance sampling ratios, which implies unreliable model updates. The main cause of this phenomenon is that GRPOs training objective is sensitive to tokens with outlier importance-weighted reward values, leading to aggressive policy updates and, consequently, extreme importance sampling ratios. To solve that, we propose Geometric-Mean Policy Optimization (GMPO), stabilized variant of GRPO [16]. Instead of optimizing the arithmetic mean of token-level rewards as shown in Equation 2, GMPO maximizes the geometric mean of them: JGMPO(πθ) = qQ,{oi}G i=1πθold 1 (cid:88) oi (cid:89) i=1 t=1 (q) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) 1 oi ˆAi sgn( ˆAi) , (3) where sgn( ˆAi) returns 1 if ˆAi is positive, and -1 otherwise. JGMPO(πθ) has narrower value range than JGRPO(πθ), which can be derived as: JGMPO(πθ) = qQ,{oi}G i=1πθold (q) qQ,{oi}G i=1πθold (q) = JGRPO(πθ). (cid:88) oi (cid:89) i=1 t=1 1 πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) 1 oi ˆAi 1 (cid:88) i=1 1 oi oi (cid:88) t=1 πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) ˆAi This narrower range suggests that the training process of GMPO experiences lower variance in the optimization objective, which can be viewed as an evidence of more stable policy updates. Compared to JGRPO(πθ), JGMPO(πθ) is less sensitive to outliers because the geometric mean is inherently more robust to outliers than the arithmetic mean. As result, JGMPO(πθ) provides more reliable policy updates and has stable range of importance sampling ratio as shown in Figure 1(right). By expanding Equation 3 and incorporating the clipping thresholds from PPO [15] at the token level, we can derive the complete objective function of GMPO as follows: JGMPO(πθ) = qQ,{oi}G i=1πθold (q) oi (cid:89) (cid:88) i=1 t=1 1 (cid:18) min (cid:20)(cid:16) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) (cid:17)sgn( ˆAi) , clip (cid:16)(cid:16) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) (cid:17)sgn( ˆAi) , ϵ1, ϵ2 (cid:17)(cid:21)(cid:19)sgn( ˆAi) 1 oi ˆAi, (4) where ˆAi = ri mean({r1, r2, rG}) std({r1, r2, rG}) . The function sgn( ˆAi) ensures that large importance sampling ratios are clipped when ˆAi is positive, and small ratios are clipped when ˆAi is negative, thus preventing rapid policy changes. (ϵ1, ϵ2) represent the lower and upper clipping thresholds for the importance sampling ratio. The pseudo code of GMPO is provided in Algorithm 1. To ensure numerical stability, the product and clipping operations in Equation 4 are performed in the log space. To better understand why GMPO is more stable than GRPO, we show that GMPO is more robust to tokens with extreme importance sampling ratios from gradient perspective. Specifically, given question and rollout oi, the gradients of JGRPO(πθ) (Equation 1) and JGMPO(πθ) (Equation 4) 4 Algorithm 1 GMPO Training objective def gmpo_loss(new_probs, old_probs, mask, advantage, epsilon=0.4): \"\"\" new_probs [L, 1]: Token probabilities from the current model old_probs [L, 1]: Token probabilities from the old model mask [L, 1]: Indicates valid (non-padded) tokens advantage [1]: Advantage or normalized reward for the sequence epsilon [1]: Controls the clipping range \"\"\" # Clipping at token-level & Clipping wider new_log_probs, old_log_probs = torch.log(new_probs), torch.log(old_probs) sgn_A = 1 if advantage > 0 else -1 sgn_A_log_probs_diff = sgn_A * (new_log_probs - old_log_probs) sgn_A_log_probs_diff2 = torch.clamp(sgn_A_log_probs_diff, -epsilon, epsilon) sgn_A_log_probs_diff_min = torch.min(sgn_A_log_probs_diff, sgn_A_log_probs_diff2) log_probs_diff_min = sgn_A * sgn_A_log_probs_diff_min # Geometric-Mean Policy Optimization importance_sampling_ratio = torch.exp(log_probs_diff_min[mask].sum()/mask.sum()) loss = -advantage * importance_sampling_ratio return loss with respect to the model parameter θ are as follows2: JGRPO(πθ) θ JGMPO(πθ) θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)q,oi (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)q,oi = = ˆAi oi ˆAi oi oi (cid:88) t=1 oi (cid:88) oi (cid:89) t=1 t=1 πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) 1 πθ(oi,tq, oi,<t) πθ(oi,tq, oi,<t) θ , (5) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) 1 oi 1 πθ(oi,tq, oi,<t) πθ(oi,tq, oi,<t) θ . (6) The term πθ(oi,tq,oi,<t) of the two objectives are all weighted sums of the tokens gradient θ measures how much the generated token oi,t influences θ. The gradients (cid:110) πθ(oi,tq,oi,<t) θ (cid:111) t=1,2,...,oi yet with different weights. For JGRPO(πθ), the weight of the token oi,t includes its individual importance sampling ratio πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) . An extreme importance sampling ratio will cause the token gradient to be too large or small, resulting in aggressive policy updates. For JGMPO(πθ), the (cid:105) 1 oi weight of the token oi,t includes the geometric mean of all the ratios which provides holistic view for policy updates and is robust to extreme values. πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) (cid:104)(cid:81)oi t=1 , Beyond the proposed training objective, we demonstrate the effectiveness of the following key designs in GMPO: (cid:16) πθ(oiq) πθold (oiq) , 1 ϵ, 1 + ϵ . In the proposed GMPO, the term (cid:81)oi t=1 (i) Clipping at token-level. Unlike the vanilla GRPO in DeepSeek-math [16], DeepSeekˆAi and clip outliers in sequence-level, i.e., R1 [4] maximizes the sequence-level reward: πθ(oiq) πθold (oiq) (cid:17) πθ(oi,tq,oi,<t) clip πθold (oi,tq,oi,<t) in Equation 4 is equal to πθ(oiq) πθold (oiq) in DeepSeek-R1. Instead of clipping the importance sampling ratio at sequencelevel following DeepSeek-R1, we find it better choice to clip it at token-level as shown in Equation 4. The reasons are as follows: (1) Clipping at the token level is more stable than at the sequence level. As shown in Figure 2, the sequence-level clip (GMPO-seqclip-(e0.4, e0.4)) has larger importance sampling range than the token-level clip (GMPO (e0.4, e0.4)), which makes it more prone to create extreme gradients during optimization. (2) Sequence-level clipping is too aggressive compared to token-level clipping. Once triggered, it sets the gradients of all tokens in the sequence to zero, potentially discarding valuable gradient signals from informative parts of rollouts. 2The influence of the clipping thresholds is omitted in the gradient calculation for clarity. 5 Figure 2: The range of importance sampling ratio with respect to different clip range and training steps. wider range indicates less stable policy updates. Compared to GRPO with clip range of (0.8, 1.2), GMPO demonstrates greater stability, even with larger clip range of (e0.4, e0.4). All curves are smoothed for clarity. (ii) Clipping wider. As illustrated in DAPO [21], the clipping operation can limit exploration and cause early deterministic policy, which hinders the scaling process. To encourage exploration without sacrificing stability, DAPO uses clip-higher strategy, which slightly expands the clipping range from (0.8, 1.2) to (0.8, 1.28). As shown in Figure 1, we visualize the maximum and minimum importance sampling ratios at each training step for both GRPO and GMPO. The key observations are: (1) As training progresses, the range of the importance sampling ratio widens, indicating more aggressive policy updates and increased instability. (2) Compared to GRPO, GMPO maintains more stable importance sampling ratio range, suggesting more stable updates. (3) For GMPO, enlarging the clipping range from (e0.2, e0.2) to (, +) increases instability in policy updates. Based on these findings, we balance training stability with exploration by setting clipping thresholds (ϵ1, ϵ2) in Equation 4 to (e0.4, e0.4). This range is significantly larger than both GRPO and DAPO, encouraging greater exploration and improving performance."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Implementation Details Model. We evaluate the algorithms performance on both language-only and multimodal reasoning tasks. For the language-only task, following Dr.GRPO [11], we use Qwen2.5-Math-1.5B [20], Qwen2.5-Math-7B, and DeepSeek-R1-Distill-Qwen-7B [5] as our base models to assess performance on mathematical tasks. For the multimodal task, we use Qwen2.5-VL-Instruct-7B [1] as the base model to train GRPO and GMPO, and evaluate their performance on geometry reasoning tasks. Training. For language-only task, following the same setting of Dr.GRPO [11], we use MATH [7] Level 3-Level 5 as the training dataset, which includes 8523 mathematical samples. For each question, we generate 8 rollouts and limit the maximum response length of the model to 3,000 tokens. During each round of RL training, the old policy πθold generates 1024 rollouts, and the current policy πθ is updated 8 times using batch size of 128. For the multimodal task, we follow EasyR1 [26] and use Geometry3K [12] as the training dataset, with all other settings consistent with EasyR1. All models are trained on server with 8A800 GPUs. Mathematical problems have verifiable rewards, the rewards are set to 1 for correct responses and 0 for incorrect ones. The proposed method is primarily compared with Dr.GRPO and GRPO, sharing the same experimental setup in Table 1. Evaluation. We evaluate our method on five mathematical reasoning benchmarks of varying difficulty following Dr.GRPO [11] and one multimodal reasoning benchmark following EasyR1 [26]: AIME24, which consists of 30 high-school level olympiad problems from the American Invitational Mathematics Examination 2024; AMC, containing 83 intermediate-difficulty multiple-choice problems; MATH500, subset of 500 problems from the original MATH dataset covering algebra, Table 1: Comparison of Pass@1 performance across various mathematical reasoning benchmarks. Model AIME24 AMC MATH500 Minerva Oly. Avg. Qwen2.5-Math-1.5B [14] Qwen2.5-Math-1.5B-Instruct [14] GRPO-1.5B [16] Oat-Zero-1.5B [11] GMPO-1.5B (Ours) Qwen2.5-Math-7B [14] SimpleRL-Zero-7B [23] PRIME-Zero-7B [3] OpenReasoner-Zero-7B @ 3k [8] OpenReasoner-Zero-7B @ 8k [8] Eurus-7B [22] GPG-7B [2] GRPO-7B [16] Oat-Zero-7B [11] GMPO-7B (Ours) GRPO-7B [16] (R1-Distill) Oat-Zero-7B [11] (R1-Distill) GMPO-7B (R1-Distill, Ours) 16.7 10.0 23.3 20.0 20.0 16.7 26.7 16.7 13.3 13.3 16.7 33.3 40.0 43.3 43.3 43.3 50.0 46.6 43.4 48.2 49.4 53.0 53.0 38.6 60.2 62.7 47.0 54.2 62.7 65.0 59.0 62.7 61.4 67.5 74.7 78. 61.8 74.2 75.2 74.2 77.6 50.6 78.2 83.8 79.2 82.4 83.8 80.0 83.4 80.0 82.0 89.0 89.6 91.4 15.1 26.5 25.7 25.7 30.1 9.9 27.6 36.0 31.6 31.6 36.0 34.2 32.4 30.1 33.5 39.7 37.5 37. 28.4 40.2 39.0 37.6 38.7 16.6 40.3 40.9 44.0 47.9 40.9 42.4 41.3 41.0 43.6 56.7 55.7 62.5 33.1 39.8 42.5 42.1 43.9 26.5 46.6 48.0 43.0 45.9 48.0 51.0 51.2 51.4 52.7 59.3 61.5 63. Table 2: Comparison of Pass@1 performance on Geometry3K multimodal reasoning benchmark. Model Qwen2.5-VL-Instruct-7B [1] GRPO [16] GMPO (Ours) Pass@1 38.1 53. 54.7 geometry, and number theory; Minerva (MIN) [10], featuring 272 graduate-level problems requiring multi-step reasoning; and OlympiadBench (OLY) [9], collection of 675 high-difficulty olympiad problems. These benchmarks collectively cover broad spectrum of problem types and difficulty levels. Geometry3K [12] is visual question answering dataset that consists of set of 601 questions focused on geometric problem-solving. We primarily use the Pass@1 metric for comparative analysis. This metric evaluates whether single generated response to problem meets the required criteria. For language tasks, we set the temperature to 0.0 and generate one answer per question, following Dr.GRPO [11]. For the multimodal task, we set the temperature to 0.5 and generate 16 answers for each question. 4.2 Performance Table 1 and Table 2 present comprehensive evaluation of our GMPO approach against established reasoning methods across multiple benchmarks. Our method demonstrates consistent and substantial improvements over strong baseline systems. Language-only task. GMPO demonstrates consistent improvements across different base models. With Qwen2.5-Math-1.5B, it achieves 43.9% average performance, outperforming GRPO by 1.4% and Dr.GRPO by 1.8%. Similar gains are observed with Qwen2.5-Math-7B (+1.5% vs GRPO, +1.3% vs Dr.GRPO) and DeepSeek-R1-Distill-Qwen-7B (+4.1% vs GRPO, +1.9% vs Dr.GRPO). Multimodal task. Using Qwen2.5-VL-Instruct-7B as the base model, GMPO surpasses GRPO by 1.4% on Geometry3K, highlighting its potential for broader application in multimodal reasoning tasks. 4.3 Ablation Studies Table 3 shows the ablation study of key changes made in evolving the GRPO training objective into that of GMPO. The effect of the clipping thresholds is presented in Table 4, and training statistics are shown in Figure 3. 7 Table 3: Comparison of training objectives and their performance under same training settings. (cid:16) πθ(otq,o<t) (cid:21)(cid:27) (cid:26) (cid:20) (otq,o<t) , 1 ϵ, 1 + ϵ πθold (cid:17) ˆA (cid:80)o t=1 min πθ(otq,o<t) πθold (otq,o<t) (cid:21) 1 ˆA, clip πθ(otq,o<t) πθold (otq,o<t) ˆA (cid:81)o t=1 πθ(otq,o<t) πθold (otq,o<t) ˆA, clip (cid:16) (cid:81)o t= πθ(otq,o<t) πθold (otq,o<t) , ϵ1, ϵ2 (cid:21) 1 (cid:17) ˆA (cid:18) (cid:18) min min (cid:20)(cid:16) πθ(otq,o<t) (cid:17)sgn( ˆA) πθold (otq,o<t) , clip (cid:16)(cid:16) πθ(otq,o<t) (otq,o<t) πθold (cid:17)sgn( ˆA) , ϵ1, ϵ (cid:20)(cid:16) πθ(otq,o<t) (cid:17)sgn( ˆA) πθold (otq,o<t) , clip (cid:16)(cid:16) πθ(otq,o<t) (otq,o<t) πθold (cid:17)sgn( ˆA) , ϵ1, ϵ2 (cid:17)(cid:21)(cid:19)sgn( ˆA) (cid:17)(cid:21)(cid:19)sgn( ˆA) ˆA 1 oi ˆA 1 : 1 (cid:20) 2 : (cid:81)o t=1 (cid:20) 3 : min 4 : 5 : (cid:81)o t=1 (cid:81)o t=1 Training objectives AIME24 AMC MATH500 Minerva Oly. 0 (Pre-RL model) 1 (GRPO) 2 (w/o clip) 3 (w seqclip) 4 (w/o norm) 5 (GMPO) 16.7 40.0 40.0 46.6 36.6 43.3 38.6 59.0 63.9 57.8 67.4 61.4 50.6 83.4 80.6 80.2 82.0 82.0 9.9 32.4 33.5 34.2 29.8 33.5 16.6 41.3 43.7 44.3 44.1 43. Avg. 26.5 51.2 52.3 52.6 52.0 52.7 Table 4: Influence of the clipping thresholds on model performance. Clipping thresholds (ϵ1, ϵ2) AIME24 AMC MATH500 Minerva Oly. Avg. 1 2 3 4 (e0.2, e0.2) (e0.4, e0.4) (e0.8, e0.8) (, +) 36.6 43.3 40.0 40.0 60.2 61.4 60.2 63.9 84.2 82.0 82.2 80.6 35.7 33.5 33.5 33.5 45.0 43.6 44.7 43.7 52.4 52.7 52.1 52. Geometric mean vs. Arithmetic mean. The performance of GRPO and GMPO is reported in lines 1 and 5 of Table 3, respectively. GRPO achieves an average performance of 51.2% by optimizing the arithmetic mean of token-level rewards. In contrast, GMPO improves this to 52.7%, outperforming GRPO by 1.5%, by optimizing the geometric mean instead. As shown in Figure 2, GMPO also exhibits more stable training. These results highlight the advantage of using geometric-mean rewards over arithmetic-mean rewards. Additionally, in row 4 of Table 3, we test removing the normalization term 1 from the training objective, similar to Dr. GRPO [11]. This results in 0.7% drop in average performance (52.0% vs. 52.7%), suggesting that the normalization term is crucial for maintaining optimal performance. Clipping strategy. The performance of GMPO without clip, with token-level clip, and with sequencelevel clip are shown in lines 2, 3, and 5, respectively. The corresponding ranges of importance sampling ratios are displayed in Figure 2, labeled as GMPO (e0.4, e0.4), GMPO-seqclip-(e0.4, e0.4), and GRPO(0.8, 1.2). Clipping at the sequence level achieves similar performance to token-level clipping but has larger range of importance sampling ratios. Therefore, we use the token-level clipping strategy. Removing the clip range term (GMPO (, +)) leads to excessive fluctuations in the importance sampling ratio during training, which affects stability and results in 0.4% decrease in average performance (52.3% vs. 52.7%). Influence of the clipping thresholds. To find the optimal clipping thresholds for GMPO, we train the model under different clipp thresholds, as shown in Table 4 and Figure 2. larger clipping range encourages exploration but introduces instability to optimization, which ultimately affects performance. To balance stability and performance, we set (ϵ1, ϵ2) in Equation 4 to (e0.4, e0.4), which has stable range of importance sampling ratio and achieves the best performance. 8 Figure 3: Analysis of entropy, KL divergence and reward over training steps. (ab) GMPO maintains higher entropy than GRPO, whether trained on MATH Level 3Level 5 or DeepScaleR dataset. (c) GMPO maintains smaller KL divergence from the pre-RL model than GRPO, suggesting more stable learning and reduced overfitting risk. (d-f) GMPO achieves competitive rewards with GRPO on simpler dataset like MATH Level 3Level 5, and outperforms GRPO in training rewards on more challenging datasets, such as DeepScaleR [13] and Geometry3K [12]. Exploration capability. As shown in Figure 3 (a-b), we visualize the mean token entropy of GMPO and GRPO when training the policy model on MATH Level 3-Level 5 and the more challenging mathematical dataset DeepScaleR [13]. GRPOs entropy drops rapidly during training, limiting the models ability to explore. Although applying clipping wider operation to GRPO increases the entropy initially, it still declines quickly over time. This indicates that GRPOs unstable updates and extreme importance sampling ratios cause the policy model to become overly deterministic, hindering both exploration and the scaling process. In contrast, GMPO maintains stable and moderate entropy, enabling more consistent exploration throughout training. As shown in Figure 3 (d), with greater exploration, GMPO consistently achieves higher reward than GRPO during training. Training stability. As shown in Figure 3 (c), we visualize the KL divergence between the current model πθ and the reference model πref . πref is initialized as the base model before RL training. As training progresses, GMPO maintains low KL divergence from the reference model, indicating 9 In contrast, GRPO exhibits large KL greater training stability and lower risk of overfitting. divergence on certain samples, suggesting unstable learning and greater tendency to drift away from the reference model. Training rewards. As shown in Figure 3 (d-f), GMPOs training rewards on MATH Level 3 to Level 5 are comparable to GRPO. However, we find that this dataset is too simple for 7B model, with the final rewards approaching 0.9. To assess GMPOs performance on more challenging datasets, we visualized its training rewards on DeepScaleR [13] and Geometry3K [12], as shown in (d) and (f). GMPO consistently achieved higher and more stable training rewards than GRPO, demonstrating its superior performance."
        },
        {
            "title": "5 Conclusion",
            "content": "We present Geometric-Mean Policy Optimization (GMPO), stabilized variant of GRPO. By optimizing the geometric mean of token-level rewards and enlarging the clipping range of importance sampling ratio, GMPO not only alleviates the instability in policy updates but also enhances exploration capabilities as evidenced by narrower objective value range, more stable gradients, and consistently lower KL divergence with higher token entropy throughout training. Extensive experiments on language-only and multimodal reasoning benchmarks demonstrate that GMPO outperforms GRPO in terms of both stability and reasoning capacity. This work sets the stage for future research on developing more reliable and scalable RL systems."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. [3] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [6] Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, and Furu Wei. On-policy rl with optimal reward baseline. arXiv preprint arXiv:2505.23585, 2025. [7] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [8] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [9] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. Advances in Neural Information Processing Systems, 37:1920919253, 2024. [10] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. [11] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [12] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Intergps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [13] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. [14] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [16] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [17] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. 11 [18] Changyi Xiao, Mengdi Zhang, and Yixin Cao. Bnpo: Beta normalization policy optimization. arXiv preprint arXiv:2506.02864, 2025. [19] Jian Xiong, Jingbo Zhou, Jingyong Ye, and Dejing Dou. Aapo: Enhance the reasoning capabilities of llms with advantage momentum. arXiv preprint arXiv:2505.14264, 2025. [20] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [21] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [22] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. [23] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [24] Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025. [25] Xiaojiang Zhang, Jinghui Wang, Zifei Cheng, Wenhao Zhuang, Zheng Lin, Minglei Zhang, Shaojie Wang, Yinghan Cui, Chao Wang, Junyi Peng, et al. Srpo: cross-domain implementation of large-scale reinforcement learning on llm. arXiv preprint arXiv:2504.14286, 2025. [26] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/ EasyR1, 2025."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "Microsoft Research",
        "UCAS"
    ]
}