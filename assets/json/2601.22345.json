{
    "paper_title": "Failing to Explore: Language Models on Interactive Tasks",
    "authors": [
        "Mahdi JafariRaviz",
        "Keivan Rezaei",
        "Arshia Soltani Moakhar",
        "Zahra Sodagar",
        "Yize Cheng",
        "Soheil Feizi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration."
        },
        {
            "title": "Start",
            "content": "Failing to Explore: Language Models on Interactive Tasks Mahdi JafariRaviz * Keivan Rezaei * Arshia Soltani Moakhar * Zahra Sodagar Yize Cheng Soheil Feizi Department of Computer Science, University of Maryland {mahdij, krezaei, asoltan3, zsodagar, yzcheng, sfeizi}@umd.edu 6 2 0 2 J 9 2 ] . [ 1 5 4 3 2 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We evaluate language models on their ability to explore interactive environments under limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple exploreexploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two interventions: splitting fixed budget into parallel executions, which surprisingly improves performance despite no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration 1. 1. Introduction Agentic AI is an increasingly prominent direction, where language models (LMs) act as agents that go beyond single static response by iteratively taking actionse.g., calling tools and APIs (Shen, 2024), invoking other LMs (Tran et al., 2025), processing feedback, and choosing follow-up steps. This paradigm is increasingly used in practical settings such as tool-using LMs (Schick et al., 2023; Yao et al., 2023), web-based agents in realistic web environments (Yao et al., 2022; Zhou et al., 2024a; Ning et al., 2025), software engineering agents (Jimenez et al., 2024; Liu et al., 2025a), and grounded robotic decision making (Ahn et al., 2022; Li et al., 2025). In these settings, agents operate in environments with partial information and costly interactions (e.g., physical robot action, an experiment, or paid API call). Achieving strong performance therefore requires effective use of limited interaction budget, which raises *Equal contribution . Correspondence to: Mahdi JafariRaviz <mahdij@umd.edu>. 1Code is at github.com/mahdi-jfri/explore-exploit-bench. 1 critical need to evaluate whether LMs can act well in such environments under resource constraints. Most current evaluations of LMs focus on static performance (Rein et al., 2023; Romanou et al., 2024; Salazar et al., 2025; Lin et al., 2025). In this work, we focus on discovery: the model interacts with black-box environment that contains hidden solutions, and must use limited budget of queries to an oracle to iteratively learn about the environment and identify high-quality solutions. More specifically, we study how well LMs can explore such environments i.e., how effectively they probe to discover better solutions, rather than committing early to sub-optimal ones. We provide suite of three parametric tasks, HillSearch, TreeSearch, and MaxSatSearch, each specifying family of problem instances that can be instantiated by choosing interpretable parameters; importantly, these parameters explicitly control the exploration required to discover high-quality solutions, making the explorative difficulty tunable. These include discovering the maximum of hidden continuous function (HillSearch), discovering high-reward nodes in tree with hidden node values (TreeSearch) and discovering highsatisfaction variable assignment under hidden clause structure (MaxSatSearch) (See Figure 1 for an illustration). To require meaningful exploration, we explicitly incorporate traps: sub-optimal solutions that are easy to find early in the environment, but can distract the model from continued exploration and lead to premature commitment. All tasks are easy to describe in short natural language, resembling familiar problems in LMs training data. The model is given total budget of interactions (i.e., rounds of querying the oracle) to gather information and discover solutions. We evaluate suite of recent LMs (including reasoning models) on instances of our task suite, and compare their performance to simple exploreexploit heuristic baselines. These baselines are intentionally lightweight (a few lines of code) and easy to describe, and they serve as strong sanity check that additional interaction budget indeed translates into improved discovery. Our evaluation reveals consistent underperformance of LMs compared to these simple Failing to Explore: Language Models on Interactive Tasks Figure 1. Overview of our environments. Across all tasks, the model interacts with partially observed environment under limited budget and must balance exploration with identifying optimal solutions while avoiding sub-optimal decisions. Left (HillSearch): there is hidden function (x). At each interaction, the model observes the value of the function at chosen point. The model should avoid identifying suboptimal solutions and explore sufficiently to find optimal points. Middle (TreeSearch): there is tree in which each node has hidden reward. At each interaction, the model can query the reward of node that has already been explored or is adjacent to explored nodes. The model should not get trapped by early high rewards from trap gateways and must explore enough to eventually find optimal leaves of good gateways. Right (MaxSatSearch): there are hidden clauses, among which one important clause exists and is repeated wgold times. At each iteration, the model queries the number of satisfied clauses for given assignment of the variables. While the model may increase the number of satisfied clauses by modifying variable assignments, achieving high reward requires satisfying the important clause, necessitating sufficient exploration of assignments for variables involved. baselines. This gap signals systematic under-exploration: models often commit early to trap solutions and fail to use additional interaction budget effectively. In particular, we observe that performance scales weakly with , despite the fact that the environments contain better solutions that can be discovered with further exploration. To improve performance in budgeted interactive tasks, we propose two interventions. (1) Parallel budget allocation: we split the total budget into independent threads, run the model separately in each, and select the best solution found across threads. Although we prove that, on our tasks, parallelizing fixed budget into threads cannot improve upon an optimal single-thread strategy, we nonetheless observe consistent gains for LMs, suggesting their single-thread behavior is far from optimal. We further provide theory clarifying when such gains can arise. (2) Periodic summarization: we periodically summarize the interaction history every fixed number of interactions, remove the full context, and continue from the summary. This guides the model to retain the key takeaways while reducing context-related failures, improving discovery and increasing performance. Contributions. Our contributions are as follows: We go beyond static evaluation by introducing suite of parametric, controllable tasks that make LM exploration measurable and difficulty-tunable across continuous and discrete/combinatorial environments. We find that LMsincluding frontier GPT-5 modelsunderperform simple exploreexploit methods, committing prematurely to sub-optimal solutions and scaling weakly with increased interaction budget. We study two lightweight interventionsparallelizing fixed budget into independent threads and periodically summarizing the interaction historythat consistently improve explorative performance. 2. Related Work LMs as combinatorial optimizers and algorithmic simulators Recent studies demonstrate that LMs can achieve strong performance at solving combinatorial problems (Sun et al., 2025; Ros et al., 2025; Jiang et al., 2025a; Yang et al., 2025b), performing in-context algorithmic simulation (Zhou et al., 2022; Lyu et al., 2024), and executing complex optimization tasks (Jiang et al., 2025b). Collectively, these capabilities suggest that LMs are well-equipped for the requirements of our study, providing strong foundation for their application in our setting. Summarization and parallelization for LM exploration Both parallelization and summarization have been widely studied for LMs. Parallel sampling is commonly used to encourage exploration (e.g., pass@k). However, prior work on parallel inference typically increases the total compute or budget rather than splitting fixed budget across parallel branches (Wang et al., 2023; Chow et al., 2025). In contrast, our work allocates fixed budget across branches. Other work trains models specifically for parallel reasoning (Wen et al., 2025; Zheng et al., 2025), which is outside the scope of this paper. Summarization is widely used to compress hisFailing to Explore: Language Models on Interactive Tasks tory to fit within the context window (Fei et al., 2023; Wang et al., 2024; Wu et al., 2025), but its impact on exploration behavior is rarely measured. LMs on interactive benchmarks Closest to our setup are single-player interactive environments and navigationstyle tasks. Single-player games and puzzles often require little exploration beyond deterministic reasoning or planning (Gong et al., 2023; Long et al., 2025). Maze and navigation benchmarks study sequential decision-making under partial observability (Abdulhai et al., 2023; ChevalierBoisvert et al., 2019; Bianchi et al., 2024; Wu et al., 2024; Xi et al., 2024; Einarsson, 2025), but typically emphasize goal reaching, memory, or control rather than exploreexploit tradeoffs. Some other benchmarks (e.g., multi-turn puzzles, feedback-based learning, graph navigation, bandits, or black-box optimization) (Badola et al., 2025; Cheng et al., 2023; Wu et al., 2024) are closer to our setup. However, they are primarily designed to evaluate multi-turn conversational behavior, in-context learning, or agentic capabilities, rather than exploration itself. Moreover, they do not provide controlled, parameterized family of tasks that enables isolating and interpreting how LLMs explore. Premature commitment and limited self-correction LMs often latch onto an initial idea and struggle to revise it. Wen et al. (2025) show that seeding rollout with the first few tokens of an incorrect trajectory can significantly reduce accuracy. Relatedly, Huang et al. (2024) describe this as limited self-correction, and Zhang et al. (2023) find that models often preserve consistency with earlier errors over factual truth. 3. Exploring Interactive Environments In this work, we consider model that interacts with an environment induced by an underlying problem instance, containing set of solutions that are initially unrevealed (hidden) and only become partially revealed through interaction. The model begins with prior knowledge of the environments structure and the prescribed interaction protocol. The environment is inherently explorative, requiring the model to probe the problem instance to find high-quality solutions. To simulate real-world interactive constraints, we impose fixed interaction budget . Formally, the model interacts with an oracle over discrete rounds, generating sequence of queries q1, q2, . . . , qN . At each round i, the oracle returns feedback determined by the current query and the static underlying problem instance. Although the environment is staticin the sense that the oracles feedback is deterministic function of the query and fixed state the interaction is sequentially constrained: the set of admissible queries at round may depend on the previously issued queries q<i. The overall performance is measured by reward function that quantifies the quality of the best solution discovered by the end of rounds, reflecting the models ability to explore effectively and avoid converging on suboptimal solutions. As shown in Figure 1, we introduce suite of three tasks: HillSearch, which features continuous environment; TreeSearch, which features discrete, graph-structured environment; and MaxSatSearch, which features discrete combinatorial environment. We evaluate two aspects of models behavior in these settings: (i) their absolute performance compared to mostly simple baseline algorithms that implement variants of exploreexploit strategies; and (ii) how increasing the search budget helps models achieve better solutions, and how this improvement compares to the growth observed for baseline algorithms. Acceptable performance is characterized by satisfying (i) through achieving comparable performance, and (ii) by exhibiting similar growth rate as budget increases. 4. Tasks In this section, we formally define our task suite. For each task, we specify the environment induced by the underlying problem instance; the models interaction protocol with the oracle; the queries and the feedback revealed at each round; and the final reward under fixed interaction budget . We also describe the solution space and the parameterization of each problem instance, including how instances are generated. Note that the task description is provided to the model before the interaction begins. We also explicitly instruct the model to maximize the task reward (as defined for each environment) under the interaction budget . For each task, we also provide simple exploreexploit algorithm as baseline for interacting with the environment. 4.1. HillSearch In this task, the problem instance is hidden smooth funcR constructed as sum of Gaussian tion : [0, 10] hills. At each interaction round, the model queries point [0, 10], and the oracle reveals the value (x). The final reward is the maximum value observed over rounds. We construct to contain many moderate peaks (decoys) and single very high but narrow peak (needle). See Figure 1 (Left) for an illustration of an interaction on this instance. This setup can cause model to focus its budget around an early local maximum, rather than exploring the domain sufficiently to locate the needle. The task becomes harder as the needle becomes narrower (e.g., decreasing its Gaussian width / standard deviation), since hitting it requires more effective exploration. We refer to Appendix B.1 for more details on the task setup. 3 Failing to Explore: Language Models on Interactive Tasks simple exploreexploit baseline We first query αN points drawn using stratified random sampling. Let ˆx denote the best point found so far. We then use the remaining budget to query points drawn uniformly from small window (of size β) around ˆx and whenever better point is found, ˆx is updated accordingly (i.e., we locally refine around the current best point). We use α = 0.8 and β = 0.5. 4.2. TreeSearch In this task, the problem instance is rooted tree where each node has hidden reward, and the oracle reveals rewards upon query. At each interaction round, the model queries the reward of node (the roots reward is 0 and is revealed at the beginning), and gradually increases its knowledge of the tree under the constraint that explored nodes must always form connected component. The final reward is the best (maximum) reward observed over the interaction episode. We construct tree instances by assembling set of disjoint chains. Some chains, which we call traps, yield relatively high reward early on providing short-term positive momentum by increasing for only small number of nodes but then improve only sparsely and do not grow substantially along the remainder of the chain. In contrast, good chains start with low reward but increase steadily, reaching high (optimal) rewards near the leaves. Throughout, we design rewards to vary smoothly along edges: the reward difference between any two adjacent nodes is at most 4. More specifically, the root has two types of children: trap gateways and good gateways. From each gateway node, we attach fanout disjoint simple paths (i.e., independent chains). Trap gateways expand into chains that are intentionally longer than those of good gateways (which are shorter). As result, once model enters trap chain, it tends to spend its budget traversing these nodes, leaving less budget to explore good branches and reach higher-reward nodes. Conversely, good gateways start low but improve along their chains, so reaching high-reward leaves requires sustained exploration. See Figure 1 (Middle) for an illustration. Instances are harder when traps are more common, as this increases the search cost to locate good chain. Difficulty also increases when traps are more deceptivefor example, offering higher initial rewards or sustaining increases for more stepsas this delays the evidence that their long-term growth is limited. See Appendix B.1 for further details. simple exploreexploit baseline As simple heuristic, we randomly select node to explore, preferring nodes whose parent achieved higher reward. We implement this preference with softmax-style sampling rule: higher parent rewards make node more likely to be chosen. Node probabilities are defined via softmax over parent rewards with temperature τ . τ controls how strongly we favor highreward parents. We use τ = 4 in all experiments. 4 4.3. MaxSatSearch In this task, the problem instance comes from the Boolean Satisfiability (SAT) family of problems over variables with hidden clauses. At each interaction round, the model proposes full assignment = (x1, x2, . . . , xn) where , and the oracle reveals the number xi } of satisfied clauses (out of m) under x. The final reward is the maximum number of satisfied clauses over the interactions (i.e., over the proposed assignments). true, false { The hidden formula includes clauses, where each clause is an AND of literals (variables or their negations). We generate clauses so that achieving high score requires exploration. More specifically, we include gold clause containing kgold literals, and repeat this gold clause wgold times. The remaining wgold clauses each contain kother variables and only have variables that do not appear in the gold clause. We guarantee that there exists an assignment that satisfies all clauses. Crucially, obtaining high score requires satisfying the gold clause, which amounts to finding the correct assignment to its kgold variables; since these variables do not appear in any other clause, the feedback from non-gold clauses provides no information about them, and the model must effectively explore by trying assignments until it hits one that satisfies the gold clause. See Figure 1 (Right) for an illustration of the task and Appendix B.1 for more details on it. Difficulty increases with larger kgold: uniformly random assignment satisfies the gold clause with probability 2kgold, making accidental discovery increasingly rare as kgold grows. Difficulty also typically increases with larger kother, which introduces more hidden constraints that must be satisfied simultaneously. simple exploreexploit baseline We first query αN random assignments, and then in the remaining rounds, we take the best assignment found so far, flip the value of one randomly chosen variable, and query the resulting assignment. This gradually evolves the assignment toward better ones. We use α = 0.5 in our experiments. Details on baselines We refer to Appendix for baseline pseudo-code and hyperparameter choices. 5. Experiments In this section, we instantiate tasks from our suite and evaluate both LLMs and LRMs under fixed interaction budget . We compare model performance against the simple exploreexploit baselines (discussed in 4) and study how reward scales with (5.1). We then evaluate two lightweight interventionsparallel threads (5.2) and periodic summarization (5.3)and finally test robustness to task difficulty variations (5.4). We defer full experimental details (task instances, models list, and interaction protocol) to the end of this section (5.5). Failing to Explore: Language Models on Interactive Tasks Table 1. Reward of language models on our tasks: they achieve sub-optimal reward across all tasks, compared to simple explore-exploit baselines. Results are reported on an instance of each task with various interaction budgets. We report the model reward and compare it to the baseline. Relative performance to the baseline is shown in (x%). The last three models are evaluated in reasoning mode. Model = 36 = 48 = 36 HillSearch TreeSearch = 48 = 60 = 36 = 48 MaxSatSearch Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gptQwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium 0.30 (32%) 0.20 (21%) 0.71 (75%) 0.36 (38%) 0.41 (43%) 0.85 (90%) 0.85 (90%) 0.30 (32%) 0.48 (51%) 0.61 (64%) 0.33 (34%) 0.27 (28%) 0.59 (61%) 0.41 (43%) 0.31 (32%) 0.77 (79%) 0.72 (75%) 0.29 (30%) 0.48 (49%) 0.58 (59%) 0.71 (75%) 0.75 (80%) 0.64 (68%) 0.64 (68%) 0.69 (74%) 0.67 (71%) 0.58 (62%) 0.72 (77%) 0.65 (69%) 0.53 (56%) 0.71 (74%) 0.91 (95%) 0.68 (71%) 0.63 (66%) 0.69 (72%) 0.64 (67%) 0.63 (65%) 0.70 (73%) 0.68 (70%) 0.54 (56%) 0.72 (74%) 0.95 (98%) 0.68 (70%) 0.76 (78%) 0.79 (82%) 0.73 (75%) 0.77 (79%) 0.85 (88%) 0.82 (84%) 0.68 (70%) 0.38 (50%) 0.41 (53%) 0.52 (68%) 0.54 (70%) 0.56 (73%) 0.61 (79%) 0.68 (88%) 0.48 (62%) 0.54 (70%) 0.59 (77%) 0.45 (54%) 0.39 (47%) 0.48 (57%) 0.62 (75%) 0.62 (74%) 0.67 (80%) 0.78 (93%) 0.54 (64%) 0.54 (64%) 0.66 (79%) explore-exploit baseline 0.94 0. 0.94 0.96 0.97 0.77 0.84 5.1. Evaluating Models on Tasks For each task, we run the models and the baselines with different interaction budgets (N ). We normalize rewards to the [0, 1] range by dividing the achieved rewards by the maximum possible reward of the instance. Table 1 shows evaluation results on an instance of each task (see 5.5 for details). As seen there, models are consistently and significantly outperformed by simple baselines that implement simple exploreexploit strategies, across all tasks and budgets. These results indicate that even when capable of reasoning, the models often fail to explore effectively and consequently converge to sub-optimal solutions within the environment. We refer to Appendix E.3 and Appendix E.4 for experiments on 50 instances of each task, which confirm the same overall trends. Next, we interpret models behavior (Qwen2.5-7B-Instruct) in our tasks by analyzing episodes where it achieves sub-optimal solutions. HillSearch: Models spend their budget around local maxima In HillSearch, models often commit early to local maximum and then spend most of their remaining budget querying nearby points. Figure 2(a) visualizes the points queried by the model in an episode where the global maximum is not found. TreeSearch: Models commit to depth-first traversal In TreeSearch, models often explore multiple branches but quickly commit to one once chosen. Figure 2(b) shows an episode where, after entering branch, the model continues descending to leaf regardless of branch quality. As result, if it enters trap gateway, it spends most of its remaining budget on that path and ends at low-reward leaf. MaxSatSearch: Models explore locally around their initial guess For MaxSatSearch, we observe that the model attempts to increase the number of satisfied clauses by modifying only small number of variables at each step. However, because our setup repeats specific gold clause, failing to satisfy it prevents the model from achieving high rewards. While non-local change (e.g., random re-initialization) may satisfy the gold clause, local changes provide limited signal on how to do so. Figure 2(c) shows that the Hamming distance (defined as the number of variables with differing assignments) between consecutive interactions remains low throughout an episode. This confirms (a) HillSearch (b) TreeSearch (c) MaxSatSearch Figure 2. Visualization of Qwen2.5-7B-Instruct interactions during failed episode in our environments (N = 48). Left (HillSearch): Queries (crosses) on the hidden function. Early steps (16) explore the space, while later steps (748) cluster near local maxima. Middle (TreeSearch): Darker nodes denote higher reward. The model descends trap gateway branch. Right (MaxSatSearch): Minimum Hamming distance to the last three queries and reward per step. The model makes only small variations with limited gains, whereas the baseline shifts from broad exploration to local refinement. See Appendix E.1 for additional visualizations. 5 Failing to Explore: Language Models on Interactive Tasks (a) HillSearch (b) TreeSearch (c) MaxSatSearch Figure 3. Scaling behavior of models and explore-exploit baseline rewards as function of the interaction budget. Despite their simplicity, the baselines exhibit stronger reward growth as the budget increases. In contrast, the models show limited improvement with additional budget, indicating inefficient use of interactions and tendency to prematurely exploit sub-optimal solutions. This gap suggests persistent under-exploration and poor budget utilization by the models. that the model is restricted to local neighborhoods, yielding only incremental gains and missing the global optimum. How do models leverage the interaction budget? To systematically understand how performance in our task setup grows with the interaction budget, we evaluate Qwen-2.5-7B-Instruct and gpt-5-mini across tasks with different values of interaction budget (N ). Figure 3 shows the performance of the models and baselines as function of . As seen there, model performance does not scale in the same way as the baselines; in particular, its growth rate is substantially smaller. This suggests that additional budget is often not used effectively to achieve better results: even with more interactions, the model struggles to improve its solutions, leading to an increasing gap from simple exploreexploit baselines as grows. 5.2. Intervention: Parallel Threads As seen in Figure 3, simply allowing the model to interact longer with the environment does not substantially improve performance. We hypothesize that splitting the interaction budget into several independent threads, and then merging their outcomesi.e., taking the best solution found across threads 2 may yield improved performance. More specifically, we consider parallel threads with no interaction between them, each equipped with an interaction budget of N/p on the same problem instance. In what follows, we prove that splitting the budget into independent threads and selecting the best outcome among them is valid interaction strategy (parallel method), and that it provides no theoretical advantage over single interaction trace of length . Therefore, this method can be compared fairly against other methods, including the standard single-thread interaction with budget . 2This matches best-of-N selection in the RLHF literature: sample candidates and return the highest-scoring one under proxy reward model (Stiennon et al., 2022; Liu et al., 2024). 6 HillSearch: Queries of parallel threads can be asked in single-thread interaction Consider parallel threads, and each querying N/p points. Let [p] := := [0, 10] denote the query domain. For each thread denote the set of queried points, with [p], let Xi N/p. Thread achieves final reward maxxXi (x). Xi We merge the solutions by taking the best one, achieving parallel = maxi[p] maxxXi (x). 1, 2, . . . , } { Now consider single-thread interaction with budget that queries the union of points asked by the parallel threads, = (cid:91) i[p] Xi, so that N. This single-thread episode achieves reward at least parallel. Thus, semaxxP (x), and maxxP (x) lecting the best outcome among independent threads with total budget is upper-bounded by what single-thread interaction with budget could achieve. This finishes the proof. See Appendix for the same proof on other tasks. } { 2, 3, Evaluating parallel threads We consider parallelization with threads on the same task instances evaluated in 5.1, and report results and interpretations across our task suite. As shown in Table 2 (N = 48), across all models, parallelization improves over single-thread interaction (p = 1) on all tasks: for HillSearch, multiple threads increase the chance that at least one samples near the global maximum and uses the remaining budget to refine; for TreeSearch, parallel runs make it more likely that thread enters good gateway and commits to exploring it, reaching high-reward nodes; and for MaxSatSearch, multiple independent initial assignments increase the chance that one thread satisfies the gold clause by chance, after which local refinement yields higher reward. We refer to Appendix E.2 for evaluation on the same problem instances with = 36, provide complementary experiments on additional instances of each task in Appendix E.3, and discuss standard errors in Appendix E.6. Failing to Explore: Language Models on Interactive Tasks Table 2. Comparing parallel execution with various values of under the same total budget (N = 48) across all tasks. Results are reported on single instance of each task. Although we theoretically show that parallelism should not yield gains over single-thread execution (p = 1), we observe improved performance across all models and tasks. We use (x%) to denote the relative performance of parallel execution compared to single-thread execution when positive, and (x%) when negative. Model = 1 = = 3 = 4 = 1 = 2 = 3 = = 1 HillSearch TreeSearch Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.33 0.27 0.59 0.41 0.31 0.77 0.72 0.29 0.48 0.58 0.52 (59%) 0.52 (95%) 0.71 (21%) 0.58 (40%) 0.47 (51%) 0.82 (7%) 0.83 (15%) 0.35 (20%) 0.75 (56%) 0.84 (45%) 0.74 (125%) 0.45 (70%) 0.57 (3%) 0.57 (38%) 0.55 (77%) 0.87 (13%) 0.77 (7%) 0.38 (32%) 0.88 (85%) 0.86 (49%) 0.97 0.66 (102%) 0.30 (12%) 0.72 (23%) 0.55 (34%) 0.74 (138%) 0.91 (18%) 0.75 (4%) 0.30 (3%) 0.96 (101%) 0.79 (38%) 0.71 0.91 0.68 0.63 0.69 0.64 0.63 0.70 0.68 0.54 0.83 (16%) 0.85 (7%) 0.76 (13%) 0.75 (19%) 0.83 (20%) 0.84 (31%) 0.74 (18%) 0.85 (22%) 0.80 (18%) 0.58 (8%) 0.93 (31%) 0.84 (7%) 0.88 (30%) 0.80 (26%) 0.91 (31%) 0.90 (40%) 0.78 (25%) 0.94 (35%) 0.89 (31%) 0.67 (24%) 0.96 0.96 (35%) 0.91 (0%) 0.93 (36%) 0.85 (35%) 0.96 (39%) 0.91 (41%) 0.94 (50%) 0.94 (35%) 0.95 (40%) 0.66 (22%) 0.45 0.39 0.48 0.62 0.62 0.67 0.78 0.54 0.54 0.66 MaxSatSearch = 3 = 2 0.53 (18%) 0.58 (48%) 0.70 (45%) 0.71 (14%) 0.66 (6%) 0.66 (1%) 0.82 (5%) 0.70 (30%) 0.59 (9%) 0.65 (2%) 0.58 (28%) 0.66 (69%) 0.69 (45%) 0.70 (13%) 0.60 (3%) 0.82 (23%) 0.74 (6%) 0.69 (30%) 0.61 (15%) 0.73 (9%) 0.84 = 4 0.63 (39%) 0.57 (47%) 0.72 (50%) 0.68 (9%) 0.72 (16%) 0.67 (1%) 0.76 (3%) 0.64 (20%) 0.65 (22%) 0.71 (8%) Theoretical analysis To understand why parallelization is beneficial, we analyze success probability rather than reward. Let q(x) denote the probability of success with budget [0, 1] (treated as continuous). Success can be defined as achieving reward above threshold (e.g., fraction of maximum possible reward). Parallelizing budget into > 1 threads is beneficial if Table 3. Summary method results (N = 48) . We report reward when using summary method with parameter s. Qwen2.5-7B-Instruct corresponds to no summarization (standard interaction). Parentheses report relative change compared to standard interaction. We use (x%) to denote the relative improvement. Summarization helps over no summary. Method HillSearch TreeSearch MaxSatSearch Qwen2.5-7B-Instruct (cid:0)1 1 q(x/p)(cid:1)p > q(x). (1) summary = 2 = 3 = 4 = 6 We model success with sublinear (concave) power law, explore-exploit baseline 0.97 0.33 0.45 (36%) 0.43 (29%) 0.52 (57%) 0.62 (87%) 0.71 0.78 (9%) 0.82 (16%) 0.82 (16%) 0.80 (13%) 0. 0.45 0.55 (23%) 0.57 (26%) 0.66 (46%) 0.60 (34%) 0.84 q(x) = cxα, 0 < 1, 0 < α < 1. (2) Empirically, we often observe q(x) success regime (e.g., small effective and/or small c). 1, indicating lowTheorem 5.1 (Parallelization under sublinear power law). Let q(x) = cxα with 0 < 1 and 0 < α < 1. For any integer > 1, there exists vp (0, 1] such that (1) holds for all < vp and fails for all vp. Theorem 5.1 shows that parallelization helps in the lowsuccess regime < vp, matching our empirical results where q(x) is small. We refer to Appendix for the proof and further discussion for = 2. 5.3. Intervention: Summary We explore another single-thread approach (in contrast to the parallel method) to improve over the standard execution. We hypothesize that the long context accumulated during interaction may mislead the model into committing to lowquality solutions and under-exploring the solution space. To test this, we introduce summary method: over an episode with budget , we provide the model with summaries. After every N/s interactions, we remove the earlier interaction content and continue the episode by conditioning the model on summary rather than the full history. More specifically, the summary is generated from the interaction history (the models past queries and obIt includes served feedback) and the task description. reflective questions (e.g., whether it has explored all parts of the search space) and highlights key takeaways from the interactions so far (e.g., the current best solutions). The summary guides future interactions by tracking progress and blind spots, but adds no new informationit only organizes what was revealed. Next, we describe how we generate task-specific summaries and evaluate Qwen-2.5-7B-Instruct equipped with summaries. Summarizing interaction history We summarize episodes via short mission hand-off that replaces the full interaction transcript every N/s steps. For HillSearch, the hand-off lists all queried points (x, (x)) sorted by x, reports remaining budget, explicitly highlights unexplored intervals (gaps) in the domain, and prompts brief reflection on whether the trajectory is stuck near local maximum. For TreeSearch, it summarizes the explored connected subgraph by including the full query history (chronological), the current known node with the highest reward, and frontier of actionable next nodes (unknown nodes adjacent to explored ones); we group frontier nodes by height (distance from the root). For MaxSatSearch, we summarize progress across assignment queries. We provide Failing to Explore: Language Models on Interactive Tasks (a) HillSearch (b) TreeSearch (c) MaxSatSearch Figure 4. Task difficulty variations for HillSearch, TreeSearch, and MaxSatSearch. We generate multiple task instances by varying key difficulty-controlling parametersthe peak width for HillSearch, the ratio of good to trap gateways for TreeSearch, and the size of the gold clause for MaxSatSearchwith harder settings yielding lower baseline performance. Across all tasks and difficulty levels, the parallel and the summary methods consistently improves over single-thread execution of Qwen2.5-7B-Instruct, narrowing the gap to exploreexploit baselines. = 36 is the budget for these episodes. an ordered history of past (score, assignment) pairs and explicitly restate the highest-scoring query. Additionally, we append coverage summary that quantifies how often each variable is set to 0 or 1, highlighting those with the most imbalanced distributions. We refer to Appendix for full details. dle (peak = 20) and 7 decoys (peak 5). Second, an instance of TreeSearch with 772 nodes, containing 3 trap gateways and 3 good gateways. Third, an instance of MaxSatSearch with 15 variables and 120 hidden clauses with wgold = 80, kgold = 4, and kother = 2. See Appendix B.2 for full parameters used in their instantiation. Providing summaries boosts performance Table 3 reports results of the summary method for , } on the same instances evaluated in 5.1. As seen there, summaries can improve performance compared to standard protocol by encouraging broader exploration and reducing premature commitment to suboptimal solutions. We present standard errors for these results in Appendix E.6. 2, 3, 4, { 5.4. Robustness to Task Difficulty Variations We consider multiple task variants by generating instances from our controllable task suite, as discussed in 4. For all tasks, we construct range of difficulty levels, calibrated by baseline performance on the corresponding instances (harder settings yield lower baseline rewards). Difficulty is varied by tuning task-specific influential parameters: for HillSearch, we adjust the peak width; for TreeSearch, we vary the ratio of good branches to trap branches; and for MaxSatSearch, we increase the number of variables in the gold clause. As seen in Figure 4, the parallel and summary methods consistently outperform standard execution across all tasks and difficulty levels, highlighting their robustness and effectiveness across wide range of environments. See Appendix E.5 for full results and detailed task configurations. Models We evaluate Qwen2.5-7B-Instruct (Qwen et al., 2025), Qwen3-4B-Instruct, and Qwen3-8B (Yang et al., 2025a) in non-reasoning mode, as well as Qwen3-8B in reasoning mode with the reasoning length capped at 2048 tokens (denoted by Qwen3-8B-medium). We also evaluate gemini-2.5-flash-lite (Comanici et al., 2025). Finally, we evaluate gpt-5-nano, gpt-5-mini, and gpt-5 (with reasoning effort set to minimal), and consider medium-reasoning variants for the small models, denoted by gpt-5-nano-medium and gpt-5-mini-medium. Overall, we cover 48B open models and proprietary models, with and without reasoning. See Appendix D.1 for generation parameters and Appendix D.2 for reasoning specifications. Interacting with the environment To evaluate model under an interaction budget , we provide the task description and budget, and then run an -round episode. At each round, the model outputs query and the oracle returns the corresponding feedback. The full interaction history is kept in the model context throughout the episode. We refer to Appendix for prompt and protocol details, and Appendix D.3 for output formatting and error handling. For robustness, we evaluate each modelinstance pair with at least 40 runs (see Appendix D.4) and report the mean reward across runs. 5.5. Experimental Setup"
        },
        {
            "title": "Conclusion",
            "content": "In this section, we describe task instances used in 5.1, 5.2, and 5.3; the evaluated models and interaction protocol. Task Instances We consider three instances, one per task. First, an instance of HillSearch with 8 hills: one neeIn this work, we propose an evaluation framework for LMs in interactive domains, focusing on their ability to interact with an environment, explore the space, and obtain good solutions within limited budget. We instantiate this framework with simple, controllable tasks, and observe sub8 Failing to Explore: Language Models on Interactive Tasks optimal LM performance, which we attribute to early commitment to sub-optimal solutions. We study two lightweight paradigms that improve performance in practice: parallel runs and periodic summarization to track progress and blind spots. Overall, improving agentic discovery may require more than longer context, and instead benefit from mechanisms that encourage sustained exploration and revision."
        },
        {
            "title": "Acknowledgment",
            "content": "This project was supported in part by grant from an NSF CAREER AWARD 1942230, the ONR PECASE grant N00014-25-1-2378, AROs Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), MURI grant 14262683, DARPA AIQ grant HR00112590066 and an award from meta 314593-00001."
        },
        {
            "title": "Impact Statement",
            "content": "This paper introduces suite of three controllable tasks that define interactive environments for measuring how well language models exploregiven partial prior knowledgeand find high-quality solutions under fixed interaction budget. By revealing systematic under-exploration, and showing that lightweight interventions such as parallel execution and periodic summarization can improve performance, our results help developers diagnose and reduce premature commitment in interactive agentic systems. We anticipate positive impacts on safer and more reliable deployment of agentic models, including better search behavior and improved detection of failure modes. To the best of our knowledge, this work does not have any negative societal impact."
        },
        {
            "title": "References",
            "content": "Badola, K., Simon, J., Hosseini, A., Carthy, S. M. M., Munkhdalai, T., Goyal, A., Koˇcisky, T., Upadhyay, S., Fatemi, B., and Kazemi, M. Multi-turn puzzles: Evaluating interactive reasoning and strategic dialogue in llms, 2025. URL https://arxiv.org/abs/ 2508.10142. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., and Hoefler, T. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1768217690, March doi: 10.1609/aaai.v38i16. 2024. URL http://dx.doi.org/10.1609/ 29720. aaai.v38i16.29720. ISSN 2159-5399. Bianchi, F., Chia, P. J., Yuksekgonul, M., Tagliabue, J., Jurafsky, D., and Zou, J. How well can llms negotiate? negotiationarena platform and analysis, 2024. URL https://arxiv.org/abs/2402.05863. Cheng, C.-A., Kolobov, A., Misra, D., Nie, A., and Swaminathan, A. Llf-bench: Benchmark for interactive learning from language feedback, 2023. URL https://arxiv. org/abs/2312.06853. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. Babyai: platform to study the sample efficiency of grounded language learning, 2019. URL https://arxiv.org/ abs/1810.08272. Chi, W., Chen, V., Angelopoulos, A. N., Chiang, W.-L., Mittal, A., Jain, N., Zhang, T., Stoica, I., Donahue, C., and Talwalkar, A. Copilot arena: platform for code llm evaluation in the wild, 2025. URL https://arxiv. org/abs/2502.09328. Abdulhai, M., White, I., Snell, C., Sun, C., Hong, J., Zhai, Y., Xu, K., and Levine, S. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023. URL https://arxiv.org/abs/2311. 18232. Chow, Y., Tennenholtz, G., Gur, I., Zhuang, V., Dai, B., Thiagarajan, S., Boutilier, C., Agarwal, R., Kumar, A., and Faust, A. Inference-aware fine-tuning for best-of-n sampling in large language models, 2025. URL https: //arxiv.org/abs/2412.15287. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. URL https://arxiv. org/abs/2204.01691. Anupam, S., Brown, D., Li, S., Wong, E., Hassani, H., and Bastani, O. Browserarena: Evaluating llm agents on real-world web navigation tasks, 2025. URL https: //arxiv.org/abs/2510.02418. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., and et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https: //arxiv.org/abs/2507.06261. Costarelli, A., Allen, M., Hauksson, R., Sodunke, G., Hariharan, S., Cheng, C., Li, W., Clymer, J., and Yadav, A. Gamebench: Evaluating strategic reasoning abilities of llm agents, 2024. URL https://arxiv.org/abs/ 2406.06613. 9 Failing to Explore: Language Models on Interactive Tasks de Oliveira, B. L. M., Martins, L. G. B., Brandao, B., and Melo, L. C. Infoquest: Evaluating multi-turn dialogue agents for open-ended conversations with hidden context, 2025. URL https://arxiv.org/abs/2502. 12257. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. SWE-bench: Can language models resolve real-world GitHub issues? In International Conference on Learning Representations (ICLR), 2024. URL https://arxiv.org/abs/2310.06770. Duan, J., Zhang, R., Diffenderfer, J., Kailkhura, B., Sun, L., Stengel-Eskin, E., Bansal, M., Chen, T., and Xu, K. Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations, 2024. URL https://arxiv.org/abs/2402.12348. Jin, B., Xie, C., Zhang, J., Roy, K. K., Zhang, Y., Li, Z., Li, R., Tang, X., Wang, S., Meng, Y., and Han, J. Graph chain-of-thought: Augmenting large language models by reasoning on graphs, 2024. URL https://arxiv. org/abs/2404.07103. Einarsson, H. Mazeeval: benchmark for testing sequential decision-making in language models, 2025. URL https://arxiv.org/abs/2507.20395. Fei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng, L., and Han, W. Extending context window of large language models via semantic compression, 2023. URL https: //arxiv.org/abs/2312.09571. Gong, R., Huang, Q., Ma, X., Vo, H., Durante, Z., Noda, Y., Zheng, Z., Zhu, S.-C., Terzopoulos, D., Fei-Fei, L., and Gao, J. Mindagent: Emergent gaming interaction, 2023. URL https://arxiv.org/abs/2309.09971. Guertler, L., Cheng, B., Yu, S., Liu, B., Choshen, L., and Tan, C. Textarena, 2025. URL https://arxiv. org/abs/2504.11442. Hajiaghayi, M., Rezaei, K., and Shin, S. Delegating to multiple agents, 2023. URL https://arxiv.org/ abs/2305.03203. Hajiaghayi, M., Mahdavi, M., Rezaei, K., and Shin, S. Regret analysis of repeated delegated choice, 2024. URL https://arxiv.org/abs/2310.04884. Hoover, A. K., Togelius, J., Lee, S., and de Mesentier Silva, F. The many ai challenges of hearthstone, 2019. URL https://arxiv.org/abs/1907.06562. Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language models cannot self-correct reasoning yet, 2024. URL https: //arxiv.org/abs/2310.01798. Jiang, X., Wu, Y., Li, M., Cao, Z., and Zhang, Y. Large language models as end-to-end combinatorial optimization solvers, 2025a. URL https://arxiv.org/abs/ 2509.16865. Kleinberg, J. and Kleinberg, R. Delegated search approximates efficient search, 2018. URL https://arxiv. org/abs/1806.06933. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention, 2023. URL https:// arxiv.org/abs/2309.06180. Li, P., An, Z., Abrar, S., and Zhou, L. Large language models for multi-robot systems: survey, 2025. URL https://arxiv.org/abs/2502.03814. Lin, B. Y., Bras, R. L., Richardson, K., Sabharwal, A., Poovendran, R., Clark, P., and Choi, Y. Zebralogic: On the scaling limits of llms for logical reasoning, 2025. URL https://arxiv.org/abs/2502.01100. Liu, J., Wang, K., Chen, Y., Peng, X., Chen, Z., Zhang, L., and Lou, Y. Large language model-based agents for software engineering: survey, 2025a. URL https: //arxiv.org/abs/2409.02977. Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., and Liu, J. Statistical rejection sampling improves preference optimization, 2024. URL https://arxiv. org/abs/2309.06657. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench: Evaluating llms as agents, 2025b. URL https://arxiv.org/abs/2308.03688. Long, Y., Jiang, Y., Liu, H., Zhao, Y., Sun, J., Shen, Y., Zhao, C., Cohan, A., and Shasha, D. Puzzleplex: Benchmarking foundation models on reasoning and planning with puzzles, 2025. URL https://arxiv.org/abs/2510. 06475. Jiang, X., Wu, Y., Li, M., Cao, Z., and Zhang, Y. Large language models as end-to-end combinatorial optimization solvers, 2025b. URL https://arxiv.org/abs/ 2509.16865. Lyu, C., Yan, L., Xing, R., Li, W., Samih, Y., Ji, T., and Wang, L. Large language models as code executors: An exploratory study, 2024. URL https://arxiv.org/ abs/2410.06667. 10 Failing to Explore: Language Models on Interactive Tasks L`u, X. H., Kasner, Z., and Reddy, S. Weblinx: Real-world website navigation with multi-turn dialogue, 2024. URL https://arxiv.org/abs/2402.05930. Ma, C., Zhang, J., Zhu, Z., Yang, C., Yang, Y., Jin, Y., Lan, Z., Kong, L., and He, J. Agentboard: An analytical evaluation board of multi-turn llm agents, 2024. URL https://arxiv.org/abs/2401.13178. Mozannar, H., Chen, V., Alsobay, M., Das, S., Zhao, S., Wei, D., Nagireddy, M., Sattigeri, P., Talwalkar, A., and Sontag, D. The realhumaneval: Evaluating large language models abilities to support programmers, 2024. URL https://arxiv.org/abs/2404.02806. Ning, L., Liang, Z., Jiang, Z., Qu, H., Ding, Y., Fan, W., yong Wei, X., Lin, S., Liu, H., Yu, P. S., and Li, Q. survey of webagents: Towards next-generation ai agents for web automation with large foundation models, 2025. URL https://arxiv.org/abs/2503.23350. Pan, J., Shar, R., Pfau, J., Talwalkar, A., He, H., and Chen, V. When benchmarks talk: Re-evaluating code llms with interactive feedback, 2025. URL https://arxiv. org/abs/2502.18413. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Romanou, A., Foroutan, N., Sotnikova, A., Chen, Z., Nelaturu, S. H., Singh, S., Maheshwary, R., Altomare, M., Haggag, M. A., A, S., Amayuelas, A., Amirudin, A. H., Aryabumi, V., Boiko, D., Chang, M., Chim, J., Cohen, G., Dalmia, A. K., Diress, A., Duwal, S., Dzenhaliou, D., Florez, D. F. E., Farestam, F., Imperial, J. M., Islam, S. B., Isotalo, P., Jabbarishiviari, M., Karlsson, B. F., Khalilov, E., Klamm, C., Koto, F., Krzeminski, D., de Melo, G. A., Montariol, S., Nan, Y., Niklaus, J., Novikova, J., Ceron, J. S. O., Paul, D., Ploeger, E., Purbey, J., Rajwal, S., Ravi, S. S., Rydell, S., Santhosh, R., Sharma, D., Skenduli, M. P., Moakhar, A. S., Moakhar, B. S., Tamir, R., Tarun, A. K., Wasi, A. T., Weerasinghe, T. O., Yilmaz, S., Zhang, M., Schlag, I., Fadaee, M., Hooker, S., and Bosselut, A. Include: Evaluating multilingual language understanding with regional knowledge, 2024. URL https://arxiv.org/abs/2411.19799. Ros, F. D., Soprano, M., Gaspero, L. D., and Roitero, K. Large language models for combinatorial optimization: systematic review, 2025. URL https://arxiv. org/abs/2507.03637. Salazar, I., Burda, M. F., Islam, S. B., Moakhar, A. S., Singh, S., Farestam, F., Romanou, A., Boiko, D., Khullar, D., Zhang, M., Krzeminski, D., Novikova, J., Shimabucoro, L., Imperial, J. M., Maheshwary, R., Duwal, S., Amayuelas, A., Rajwal, S., Purbey, J., Ruby, A., Popoviˇc, N., Suppa, M., Wasi, A. T., Kadiyala, R. M. R., Tsymboi, O., Kostritsya, M., Moakhar, B. S., da Costa Merlin, G., Coletti, O. F., Shiviari, M. J., farahani fard, M., Fernandez, S., Grandury, M., Abulkhanov, D., Sharma, D., Mitri, A. G. D., Marchezi, L. B., Heydari, S., Obando-Ceron, J., Kohut, N., Ermis, B., Elliott, D., Ferrante, E., Hooker, S., and Fadaee, M. Kaleidoscope: In-language exams for massively multilingual vision evaluation, 2025. URL https://arxiv.org/abs/2504.07072. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.04761. Schlangen, D., Hakimov, S., Jordan, J., and Sadler, P. third paradigm for llm evaluation: Dialogue game-based evaluation using clembench, 2025. URL https:// arxiv.org/abs/2507.08491. Shen, Z. Llm with tools: survey, 2024. URL https: //arxiv.org/abs/2409.18807. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. Learning to summarize from human feedback, 2022. URL https://arxiv.org/abs/2009.01325. Sun, W., Feng, S., Li, S., and Yang, Y. Co-bench: Benchmarking language model agents in algorithm search for combinatorial optimization, 2025. URL https: //arxiv.org/abs/2504.04310. Tran, K.-T., Dao, D., Nguyen, M.-D., Pham, Q.-V., OSullivan, B., and Nguyen, H. D. Multi-agent collaboration mechanisms: survey of llms, 2025. URL https://arxiv.org/abs/2501.06322. Wang, P., Zhang, L., Liu, F., Shi, L., Li, M., Shen, B., and Fu, A. Codeif-bench: Evaluating instruction-following capabilities of large language models in interactive code generation, 2025. URL https://arxiv.org/abs/ 2503.22688. Failing to Explore: Language Models on Interactive Tasks Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203. 11171. Wang, X., Chen, Z., Xie, Z., Xu, T., He, Y., and Chen, E. Incontext former: Lightning-fast compressing context for large language model, 2024. URL https://arxiv. org/abs/2406.13618. Wen, H., Su, Y., Zhang, F., Liu, Y., Liu, Y., Zhang, Y.-Q., and Li, Y. Parathinker: Native parallel thinking as new paradigm to scale llm test-time compute, 2025. URL https://arxiv.org/abs/2509.04475. Wu, X., Li, K., Zhao, Y., Zhang, L., Ou, L., Yin, H., Zhang, Z., Yu, X., Zhang, D., Jiang, Y., Xie, P., Huang, F., Cheng, M., Wang, S., Cheng, H., and Zhou, J. Resum: Unlocking long-horizon search intelligence via context summarization, 2025. URL https://arxiv.org/abs/2509. 13313. Wu, Y., Tang, X., Mitchell, T. M., and Li, Y. Smartplay: benchmark for llms as intelligent agents, 2024. URL https://arxiv.org/abs/2310.01557. Xi, Z., Ding, Y., Chen, W., Hong, B., Guo, H., Wang, J., Yang, D., Liao, C., Guo, X., He, W., Gao, S., Chen, L., Zheng, R., Zou, Y., Gui, T., Zhang, Q., Qiu, X., Huang, X., Wu, Z., and Jiang, Y.-G. Agentgym: Evolving large language model-based agents across diverse environments, 2024. URL https://arxiv.org/abs/ 2406.04151. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025a. URL https: //arxiv.org/abs/2505.09388. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers, 2024. URL https://arxiv.org/abs/2309.03409. Yang, J., Prabhakar, A., Narasimhan, K., and Yao, S. Intercode: Standardizing and benchmarking interactive coding with execution feedback, 2023. URL https: //arxiv.org/abs/2306.14898. 12 Yang, X., Zhang, L., Qian, H., Song, L., and Bian, J. Heuragenix: Leveraging llms for solving complex combinatorial optimization challenges, 2025b. URL https: //arxiv.org/abs/2506.15196. Yang, X., Zhang, L., Qian, H., Song, L., and Bian, J. Heuragenix: Leveraging llms for solving complex combinatorial optimization challenges, 2025c. URL https: //arxiv.org/abs/2506.15196. Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206, 2022. URL https://arxiv. org/abs/2207.01206. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv. org/abs/2210.03629. Yu, Q., Zheng, Z., Chen, D., Niu, S., Tang, B., Xiong, F., and Li, Z. Guessarena: Guess who am? self-adaptive framework for evaluating llms in domain-specific knowledge and reasoning, 2025. URL https://arxiv. org/abs/2505.22661. Zhang, M., Press, O., Merrill, W., Liu, A., and Smith, N. A. How language model hallucinations can snowball, 2023. URL https://arxiv.org/abs/2305.13534. Zheng, T., Zhang, H., Yu, W., Wang, X., Dai, R., Liu, R., Bao, H., Huang, C., Huang, H., and Yu, D. Parallel-r1: Towards parallel thinking via reinforcement learning, 2025. URL https://arxiv.org/abs/2509.07980. Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. Teaching algorithmic reasoning via in-context learning, 2022. URL https: //arxiv.org/abs/2211.09066. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., Alon, U., and Neubig, G. Webarena: realistic web environment for building autonomous agents. In International Conference on Learning Representations (ICLR), 2024a. URL https://arxiv.org/abs/2307.13854. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., Alon, U., and Neubig, G. Webarena: realistic web environment for building autonomous agents, 2024b. URL https: //arxiv.org/abs/2307.13854. Failing to Explore: Language Models on Interactive Tasks"
        },
        {
            "title": "Appendix Outline",
            "content": "This appendix provides additional details needed to reproduce and contextualize our evaluation. We first include an extended review of related work (Appendix A), situating our interactive benchmark relative to prior work on LLM-based optimization, delegated search, and interactive evaluation. We then describe how task instances are procedurally generated for each task (Appendix B.1) and report the exact evaluated instances used in the main text (Appendix B.2). We then document how tasks are explained to models and how interactions are structured (Appendix C). We next provide evaluation setup details, including decoding parameters, reasoning budgets, structured output enforcement, error handling, and the number of runs (Appendix D). We then present complementary results, including interpretability analyses, budget sweeps, additional instances, evaluation on 50 randomly generated instances, and difficulty variation studies with standard errors (Appendix E). We then provide the prompts used to generate summaries (Appendix F). We next give baseline details, including pseudocode and hyperparameter sensitivity analyses (Appendix G). We finally present theoretical results regarding parallelization (Appendix H). A. Extended Related Work In this appendix, we provide an extended review of related work most relevant to our setting and contributions. A.1. LLMs as Combinatorial Optimizers and Algorithmic Simulators Recent literature has shown the surprising efficacy of LLMs in functioning as autonomous combinatorial optimizers and algorithmic simulators. For example, Yang et al. (2024) introduced OPRO, demonstrating that LLMs can iteratively generate and refine solutions for NP-hard problems like the Traveling Salesperson Problem (TSP). Similarly, HeurAgenix (Yang et al., 2025c) demonstrates that LLMs can function as hyper-heuristics, selecting and evolving heuristic strategies for complex scheduling tasks without requiring gradient updates. Jiang et al. (2025b) further proposed framework where LLMs operate as end-to-end solvers, mapping natural language constraints directly to feasible solutions. On the algorithmic simulation front, LLMs have long been shown to be able to simulate algorithms via algorithmic prompting (Zhou et al., 2022), and some recent work on reasoning, such as Graph-of-thought (Besta et al., 2024), illustrates that modeling LLM reasoning as graph traversals allows for dynamic backtracking and aggregation of thought paths, mimicking complex search algorithms to solve elaborate problems like sorting and set operations. Similarly, Jin et al. (2024) developed Graph Chain-of-Thought (Graph-CoT), which prompts models to interact with the graph and perform iterative, step-by-step graph traversal. In our work, we leverage such capabilities of LLMs and design interactive task suites to evaluate the explorative searching capability of LLMs. A.2. Delegated Search and Constrained Interaction key motivation for our interactive task suite is delegation, where principal relies on an agent to explore an unknown solution space and propose candidates under limited feedback and interaction constraints. Prior work has formalized delegated search and shown that simple acceptance mechanisms can obtain meaningful approximation guarantees relative to the principals optimal choice (Kleinberg & Kleinberg, 2018). More recent work studies richer delegation settings, including multi-agent delegation (Hajiaghayi et al., 2023) and repeated delegation with online adaptation and regret guarantees (Hajiaghayi et al., 2024). These results highlight delegation as natural application domain where strategic exploration under constrained interaction protocol is essential. A.3. Task-Oriented vs. Exploratory Interactive LLM Benchmarks While growing body of work evaluates LLMs in interactive settings, most existing benchmarks are task-oriented, that is they primarily measure task completion, code generation quality, or strategic competition. They rarely isolate an agents ability to systematically explore an unknown environment under strict query budget. One category of benchmarks utilizes real-world task execution as the interactive environment, notably web navigation and tool-use settings such as shopping or web-based workflows (Ma et al., 2024; Xi et al., 2024; Anupam et al., 2025; L`u et al., 2024; Zhou et al., 2024b). While highly realistic, performance in these settings is typically dominated by planning, instruction-following, and UI manipulation skills rather than pure exploratory capability. second category focuses on interactive coding, where the agent receives execution feedback, unit-test results, or evaluator Failing to Explore: Language Models on Interactive Tasks responses (Pan et al., 2025; Chi et al., 2025; Mozannar et al., 2024; Wang et al., 2025; Yang et al., 2023). In these environments, the interaction serves largely as correctness oracle for well-specified objective; the challenge lies in synthesis and debugging rather than navigating an unknown solution space. third category spans game-based evaluations and dialogue protocols, including two-player and hostplayer formats (Schlangen et al., 2025; Yu et al., 2025; Guertler et al., 2025; Duan et al., 2024; Costarelli et al., 2024; Ma et al., 2024; Hoover et al., 2019; Long et al., 2025; Liu et al., 2025b; de Oliveira et al., 2025). These settings effectively probe strategic reasoning and social dynamics, but the signal for exploration quality is often confounded by opponent behavior and game-theoretic considerations. Closest to our setup are single-player interactive environments and navigation tasks. However, many puzzles in this domain require little exploration beyond deterministic reasoning (Gong et al., 2023; Long et al., 2025). Similarly, while maze and navigation benchmarks involve sequential decision-making under partial observability (Abdulhai et al., 2023; Chevalier-Boisvert et al., 2019; Bianchi et al., 2024; Wu et al., 2024; Xi et al., 2024; Einarsson, 2025), the objective is typically goal-state navigation. The design of these benchmarks tends to emphasize memory, control, or policy learning rather than isolating exploreexploit tradeoffs. Finally, while several benchmarks incorporate partial observability and interactive learningsuch as multi-turn puzzles or black-box optimization with natural language feedback (Badola et al., 2025; Cheng et al., 2023; Wu et al., 2024)they often lack controlled, parameterized family of environments specifically constructed to disentangle exploration strategies. In contrast, our task suite is explicitly designed to measure exploration quality under limited budget across diverse problem geometries. By utilizing simple, parameterized environments with controllable difficulty, we are able to isolate specific failuressuch as premature commitment, under-exploration, and inefficient budget utilizationthat are often obscured in more complex, entangled task environments. B. Task Instances Details In this section, we first describe how we generate task instances, as detailed in Appendix B.1. We then provide details on the specific instances used in the main-text tables in Appendix B.2. B.1. Details on Creating Instances In this section, we will describe in greater detail how we generate instances for each of our tasks. HillSearch: We construct functions for measuring exploration by defining : [0, 10] Each hill is set by center c, width > 0, and height > 0, and is given by (cid:19) (cid:18) g(x) = exp . (x c)2 as sum of Gaussian hills. Fix an integer level and let = 10/2k. Define the coarse grid points xm = 10m/2k for interior point , we place one decoy hill with center 1, 2, . . . , 2k 1 { } 0, 1, . . . , 2k . For each } { All decoy hills have width wm = αdecoyk, and their heights are sampled uniformly from 1, 2, 3, 4, 5. cm = xm + εm, εm Uniform [ jdecoyk, jdecoyk] . We then place one needle hill on finer grid with level so that the base point = 10m/2k 1, 3, . . . , 2k and spacing = 10/2k is not on the coarse grid. The needle center is . We choose an odd index { } cneedle = + εneedle, εneedle Uniform [ jneedlek, jneedlek] . The needle has fixed height hneedle = 20 and width wneedle = αneedlek. In total, we have 2k hills. Let (ci, wi, hi) denote the center, width, and height of hill i. We define (x) = 2k (cid:88) i=1 hi exp (cid:18) (x (cid:19) , ci)2 wi [0, 10]. 14 Failing to Explore: Language Models on Interactive Tasks Each instance is generated by the parameters k, k, jdecoy, jneedle, αdecoy, and αneedle. TreeSearch: As described earlier, we construct rooted trees where the root has rtrap children, called trap gateways, and rgood children, called good gateways. From each gateway node, we attach (called fanout in the main text) disjoint simple paths (independent chains). Each trap gateway has chains of length dtrap 1, and each good gateway has chains 1. After constructing the tree, we randomly permute node IDs to remove any link between identifiers and of length dgood structural roles. We assign values by setting the root value to 0 and adding edge increments along each root-to-node path. Trap gateways. Entering trap gateway adds one-time increment of 2. For the next 6 steps along any of its chains, values increase by 1 per step, creating short-term positive momentum. After that, increases are sparse: most steps add 0, with +1 added once every 4 steps. Good gateways. Entering good gateway adds one-time increment of 1. After that, each additional step along its chains adds 4, giving steady growth. Each instance is defined by the parameters rtrap, rgood, b, dtrap, and dgood. 1, . . . , MaxSatSearch: To generate formula with variables and clauses, we first sample an assignment n, where each is drawn uniformly from true and false. We then form gold clause as conjunction over kgold variables and include this same clause wgold times. For the remaining wgold clauses, we exclude the kgold variables used in the gold clause. Each remaining clause is constructed by conjoining kother variables selected uniformly at random. We then negate literals as needed so that the assignment satisfies every clause. 1, . . . , Each instance is generated by the parameters n, m, kgold, kother, and wgold. B.2. Details on Evaluated Instances HillSearch The instance used in 5.1 has decoy hills centered at [1.33, 2.77, 4.01, 5.31, 6.45, 7.82, 8.95], with widths [0.1, 0.2, 0.1, 0.1, 0.1, 0.2, 0.1], and heights [1, 5, 2, 1, 2, 3, 4]. The needle hill is centered at 1.3 with height 20 and width 0.01. Figure 1 (left) shows this function. TreeSearch The instance used in 5.1 is generated as described in Appendix B.1 with parameters rtrap = rgood = 3, = 5, dtrap = 40, and dgood = 12. This produces tree with 772 nodes and maximum value of 45. MaxSatSearch The instance used in 5.1 is generated as described in Appendix B.1 with parameters = 15, = 120, kgold = 4, kother = 2, and wgold = 80. C. Explaining Tasks to Models Each conversation begins with system prompt that defines the task and may include some instance details. user prompt then provides all remaining instance details and asks the model for its first query. The conversation templates are shown in Figures 5, 6, and 7. For the TreeSearch task, we require that each queried node be connected to previously queried node. We find that models often struggle to identify all nodes that satisfy this rule. Therefore, after answering each model query, we also return randomly shuffled list of nodes that have just become available to query. If no new node becomes available, we instead return the full set of available nodes. In this case, we sort nodes by when they became available (most recent first) and break ties at random. 15 Failing to Explore: Language Models on Interactive Tasks Prompt Template for HillSearch System Prompt You are playing game to find the global maximum value of hidden continuous function f(x). Note that the function may have many local maximums which are not globally maximal. You have limited number of queries. The function is defined on the interval [0, 10]. Rules: - In each turn, you can query the value of the function f(x) for any real number in the interval [0, 10]. - The number of remaining queries will be provided in each turn. - Your goal is to find the global maximum value of f(x). How to play: - Use the tool with JSON payload like this: reason: your strategy for querying x, query: x: v, where is any real number between 0 and 10 that you want to query. The tool call MUST ONLY contain this JSON. No other text is allowed. - After each query, you will be told the value of f(x) for the you queried. User You have queries. Your goal is to find the maximum value. Assistant {reason: ..., query: x: x} User Value of f(x) is y. Remaining queries: {q 1}. Figure 5. The prompt structure used for the HillSearch task. Blue text indicates dynamic variables specific to the problem instance or generated queries. Prompt Template for TreeSearch System Prompt You are playing graph game to find the maximum hidden node value. You have limited queries. Rules: - You are given the graph structure and the value of starting node. - The order of nodes in adjacency lists is randomized. The list of valid nodes to query is ordered such that neighbors of more recently queried nodes appear first, with ties broken randomly. - Querying node reveals its value. - Do not query the same node twice. - You can only query node if its neighbor of an already known node. - Values of connected nodes differ by at most 4. How to play: - Use the tool with JSON payload like this: {reason: your reasoning here, query: {node: i}}, where is the index of the node you want to query. The tool call MUST ONLY contain this JSON. No other text is allowed. - After each query, you will be told the value of the queried node. User = Graph (by adjacency list): The order of nodes in the following adjacency list and the order of neighbors for each node are both randomized. v1: v2, v3, v4: v5, Start node: v6. Value: 0. You have queries remaining. Your goal is to find the maximum value. Valid nodes to query next: v7 v8 v9 . Assistant {reason: ..., query: {node: v}} User VALUE of is x. Remaining queries: {q 1}. This query has made new neighbors available to query: v10 v11 v12 . You can still choose to query from previously available neighbors. Figure 6. The prompt structure used for the TreeSearch task. Blue text indicates dynamic variables specific to the problem instance or generated queries. 16 Failing to Explore: Language Models on Interactive Tasks Prompt Template for MaxSatSearch System Prompt You are playing game to find Boolean assignment that maximizes the weighted sum of satisfied constraints of hidden boolean formula. IMPORTANT: The formula is NOT in Conjunctive Normal Form (CNF). Rules: - There are Boolean variables x0..x{b 1}. - There are weighted clauses. - Each clause is formed by 1 to literals (variables or their negations) combined with logical AND. - clause is satisfied if and only if ALL its literals are True (unlike standard CNF which uses OR). - In each turn, you may query ANY full assignment (a list of 0/1 values of length b). - You will receive the sum of weights of satisfied clauses for that assignment. - Your goal is to find the maximum weighted sum within the query budget. How to play: - Use the tool with JSON payload like this: reason:your strategy,query:x0:0, x1:1, x2:0, .... - The tool call MUST ONLY contain this JSON. No other text is allowed. User You have queries. Maximize the number of satisfied clauses (m=c). Assistant {reason:...,query:x0:v1,x1:v2,...,x{b 1}:vb} User Satisfied clauses: c1. Remaining queries: {q 1}. Figure 7. The prompt structure used for the MaxSatSearch task. Blue text indicates dynamic variables specific to the problem instance or generated queries. 17 Failing to Explore: Language Models on Interactive Tasks D. Evaluation Setup Details In this section, we provide additional evaluation details used throughout the paper. In Appendix D.1, we describe the decoding parameters used for token generation. In Appendix D.2, we provide details on how we evaluate reasoning-capable models. In Appendix D.3, we specify the required model output format and our error-handling protocol. In Appendix D.4, we report the number of runs per task instance and model used to obtain robust, statistically meaningful results. D.1. Generation parameters Unless otherwise specified, we decode queries with temperature = 0.7 and nucleus sampling top-p = 0.95 for all models that expose these controls (both local inference and API models). For models/APIs that do not support parameter (e.g., gpt-5 familys temperature), we use the provider default and keep the remaining decoding parameters identical when possible. All other decoding settings (e.g., max output tokens, top-k, repetition penalties) are left at their framework/provider defaults. D.2. Thinking Budget for Reasoning With the exception of Qwen2.5-7B-Instruct and Qwen3-4B-Instruct, all evaluated models support reasoning capabilities. For the gpt-5 family, we set the reasoning effort parameter to minimal to prevent them from reasoning. We also evaluate gpt-5-nano and gpt-5-mini with reasoning effort set to medium, denoting these variations as gpt-5-nano-medium and gpt-5-mini-medium, respectively. For Qwen3-8B, we examine one variation with reasoning disabled and another, denoted as Qwen3-8B-medium, with reasoning limited to 2048 tokens per message. Following the official Qwen documentation (Yang et al., 2025a) for the latter, we terminate the models reasoning generation after 2048 tokens and append Considering the limited time by the user, have to give the solution based on the thinking directly now to the reasoning output. Finally, for gemini-2.5-flash-lite, we utilize the default reasoning effort setting of none. D.3. Model Output We use structured outputs for all models. Supported by vLLM (Kwon et al., 2023) for local inference and by the OpenAI and Gemini APIs, this method enforces the JSON structure defined in our prompts (see Figure 5). We include reason or think field in the response schema. This enables models to execute chain of thought before generating query, even if their internal reasoning features are disabled or unavailable. Despite format enforcement, models may still rarely make semantic errors. For example, in the TreeSearch task, model might request node lacking previously queried neighbor. In such cases, we first retry the generation silently, allowing up to 20 retries per episode. If errors persist, we provide an informative error message for up to 5 additional attempts. We find silent retries more effective than feedback, as models often persist in the invalid query when explicitly shown their errors. Invalid queries do not count toward the budget; however, the episode ends if the error count exceeds these thresholds. D.4. Number of runs To ensure statistical robustness and account for variance, we evaluate all models and baselines using multiple independent runs. Table 4 details the specific sample sizes used for the main results reported in Table 1. For the complementary analyses, we adopted the following sample sizes: For the scaling analysis (Figure 3), we performed 100 runs for Qwen2.5-7B-Instruct and 50 runs for gpt-5-mini for each instance-budget pair, while the explore-exploit baseline was evaluated over 300 runs for HillSearch, 200 for TreeSearch, and 100 for MaxSatSearch. Similarly, for both the summary method evaluation  (Table 3)  and the difficulty variation experiments (Figure 4), we conducted 200 baseline runs and 50 Qwen2.5-7B-Instruct runs per setting. Regarding the parallel method (5.2), we simulated performance combinatorially using the available pool of independent runs. We evaluated all (cid:0)r (cid:1) subsets of size p, defined the subset reward as the maximum reward achieved by any run within that subset, and reported the average reward in Table 2. This same combinatorial pooling strategy was applied to the difficulty variation experiments. 18 Failing to Explore: Language Models on Interactive Tasks Table 4. Number of times we run each model on each instance and budget. Model Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gptQwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium HillSearch MaxSatSearch TreeSearch = 36 = 48 = 36 = 48 = 60 = 36 = 48 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 100 100 100 100 100 100 50 50 50 50 200 100 100 100 100 100 100 50 50 50 200 100 100 100 100 100 100 50 50 50 50 200 60 60 60 60 60 60 40 40 40 60 60 60 60 60 60 40 40 40 40 200 200 explore-exploit baseline 500 E. Complementary Results In this section, we present additional experimental results that support and extend the findings discussed in the main text. We begin by analyzing the specific exploration trajectories of the models to provide further interpretability regarding their greedy behavior (Appendix E.1). Subsequently, we evaluate performance across different interaction budgets (Appendix E.2) and validate the robustness of our conclusions using set of additional problem instances (Appendix E.3). We also verify the consistency of our results by scaling our evaluation to larger, diverse dataset consisting of 50 randomly generated instances for each task (Appendix E.4). Finally, we present the comprehensive results of our robustness analysis across varying levels of task difficulty (Appendix E.5). In this section, we present additional experimental results that support and extend the findings discussed in the main text. We begin by analyzing the specific exploration trajectories of the models to provide further interpretability regarding their greedy behavior (E.1). Subsequently, we evaluate performance across different interaction budgets (E.2) and validate the robustness of our conclusions using set of additional problem instances (E.3). E.1. Interpreting Models Exploration Patterns We analyze how the model allocates its budget in Figure 8 for the instance described in 5.1. These results complement Figure 2 and illustrate the greedy behavior of the model. With HillSearch, the model spends small fraction of its budget on exploration before switching to consistent exploitation. With TreeSearch, the model persists along the branch selected during the initial queries. We also include more episodes similar to Figure 2 in Figure 9, 10, and 11. E.2. Evaluation With Different Budgets We extend the evaluation from 5.2 to different budgets. Table 5 presents results for all tasks with = 36, and Table 6 presents results for the TreeSearch task with = 60. These configurations cover the complete set of budgets listed in Table 1. We use the same number of episodes that can be found in Table 4. E.3. Additional Instances We validate the findings from 5.1 by repeating the experiments on additional problem instances. We use the same number of episodes that can be found in Table 4. HillSearch We evaluate second instance containing decoy hills centered at [1.28, 2.65, 3.83, 5.11, 6.44, 7.6, 8.77], with widths [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] and heights [1, 2, 4, 1, 2, 3, 4]. The needle hill is centered at 6.2 with height 20 and width 0.01. Table 7 presents the results for this instance. TreeSearch We generate two additional instances following the procedure described in Appendix B.1, using the following parameters: 19 Failing to Explore: Language Models on Interactive Tasks (a) HillSearch (b) TreeSearch Figure 8. Query patterns of Qwen2.5-7B-Instruct on HillSearch and TreeSearch. Left (HillSearch, 36 queries): We report the average distance to the closest hill and the average reward at each step across episodes. The model starts exploiting in fewer than 10 steps and results in suboptimal final reward, as shown in Table 1. Right (TreeSearch, 48 queries): We report the average distance to the previously queried node and the average reward at each query. The distance remains almost constant at 1 until the model reaches the end of the trap gateway branches at height 42. The reward rises sharply during the first 13 queries (good gateway branches) and drops after these branches end, though it continues to increase slowly along the trap branch. Together with Figure 2, this indicates that the model exploits early and reaches the end of any branch it enters. Table 5. Comparing parallel execution with various values of under total budget of = 36 for all tasks, on the same instance as Table 2. Model = 1 = 2 = 3 = = 1 = 2 = 3 = 4 = 1 HillSearch TreeSearch Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gemini-2.5-flash gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.34 0.60 0.52 0.83 0.49 0.69 0.41 0.74 0.58 0.33 0. 0.35 (3%) 0.77 (29%) 0.94 (81%) 0.96 (16%) 0.67 (37%) 0.88 (28%) 0.67 (65%) 0.86 (16%) 0.68 (16%) 0.31 (5%) 0.96 (68%) 0.61 (78%) 0.86 (44%) 0.97 (87%) 0.98 (18%) 0.71 (46%) 0.84 (23%) 0.48 (18%) 0.90 (22%) 0.53 (10%) 0.23 (31%) 0.94 (64%) 0.88 0.67 (95%) 0.89 (48%) 0.94 (80%) 0.98 (18%) 0.85 (74%) 0.97 (42%) 0.64 (59%) 0.90 (22%) 0.94 (61%) 0.61 (87%) 0.98 (71%) 0.71 0.75 0.64 0.64 0.69 0.67 0.58 0.72 0.64 0.65 0.53 0.88 (25%) 0.79 (5%) 0.77 (21%) 0.77 (20%) 0.85 (23%) 0.81 (21%) 0.73 (25%) 0.87 (21%) 0.85 (33%) 0.80 (23%) 0.61 (14%) 0.85 (21%) 0.91 (21%) 0.90 (40%) 0.80 (26%) 0.85 (23%) 0.90 (35%) 0.93 (59%) 0.93 (29%) 0.82 (28%) 0.89 (36%) 0.72 (36%) 0.94 0.71 (0%) 0.67 (11%) 0.65 (2%) 0.68 (6%) 0.70 (1%) 0.68 (2%) 0.65 (11%) 0.70 (3%) 0.71 (10%) 0.69 (5%) 0.60 (13%) 0.38 0.41 0.52 0.54 0.56 0.61 0.68 0. 0.54 0.59 MaxSatSearch = 3 = 2 0.48 (25%) 0.60 (45%) 0.63 (22%) 0.62 (15%) 0.61 (8%) 0.65 (7%) 0.57 (16%) 0.55 (43%) 0.60 (46%) 0.64 (23%) 0.64 (19%) 0.54 (3%) 0.65 (8%) 0.70 (3%) = 4 0.50 (31%) 0.71 (72%) 0.67 (28%) 0.67 (23%) 0.68 (22%) 0.77 (26%) 0.63 (8%) 0.57 (21%) 0.52 (10%) 0.58 (21%) 0.70 (30%) 0.69 (16%) 0.63 (17%) 0.55 (8%) 0.45 (16%) 0.56 (6%) 0.77 1. rtrap = rgood = 2, = 3, dtrap = 40, and dgood = 14. This yields tree with 323 nodes and maximum value of 53. Results are shown in Table 8. 2. rtrap = rgood = 4, = 4, dtrap = 40, and dgood = 16. This yields tree with 889 nodes and maximum value of 61. Results are shown in Table 9. MaxSatSearch We evaluate three further instances generated via the method in Appendix B.1: 1. = 15, = 135, kgold = 4, kother = 2, and wgold = 90. See Table 10 for results. 2. = 15, = 150, kgold = 4, kother = 2, and wgold = 100. See Table 11 for results. 3. = 15, = 165, kgold = 4, kother = 2, and wgold = 110. See Table 12 for results. E.4. Evaluating on 50 More Instances We extend our analysis from specific cases to larger dataset to check the consistency of our results. We evaluate Qwen2.5-7B-Instruct and present the outcomes in Table 13. The model generally underperforms compared to the explore-exploit baselines, although summarization and parallelization improve performance. Failing to Explore: Language Models on Interactive Tasks Figure 9. Visualization of 18 different episodes of Qwen2.5-7B-Instruct on HillSearch with budget = 48. Queries are shown as crosses on the hidden function. Early steps (16) explore the space, while later steps (748) cluster near local maxima. 21 Failing to Explore: Language Models on Interactive Tasks Figure 10. Visualization of 18 different episodes of Qwen2.5-7B-Instruct on TreeSearch with budget = 48. Crosses indicate queried nodes. Darker node color indicate higher reward. Other unqueried nodes are omitted. In almost all episodes, the model greedily descends any branch it enters. Failing to Explore: Language Models on Interactive Tasks Figure 11. Visualization of 18 different episodes of Qwen2.5-7B-Instruct and the explore-exploit baseline on MaxSatSearch with budget = 48. The LM often makes local changes with single flip. In some episodes, the model refuses to make further queries when it finds large reward, and keeps querying the same assignment. The baseline often finds good assignment during the first half of its queries and applies local search for the rest of its queries, slightly increasing its reward. 23 Failing to Explore: Language Models on Interactive Tasks Table 6. Comparing parallel execution with various values of under the same total budget (N = 60) for task TreeSearch, on the same instance as Table 2. TreeSearch Model = 1 = 2 = 3 = 4 Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gptQwen3-8B-medium gemini-2.5-flash gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.72 0.95 0.68 0.76 0.79 0.73 0.77 0.85 0.68 0.82 0.68 0.77 (7%) 0.86 (9%) 0.87 (27%) 0.81 (6%) 0.85 (7%) 0.85 (17%) 0.76 (1%) 0.91 (7%) 0.85 (24%) 0.85 (4%) 0.68 (0%) 0.88 (23%) 0.92 (3%) 0.92 (35%) 0.77 (1%) 0.88 (11%) 0.95 (30%) 0.79 (2%) 0.89 (4%) 0.91 (33%) 0.86 (4%) 0.86 (27%) 0.97 0.95 (31%) 0.97 (2%) 0.94 (37%) 0.89 (17%) 0.95 (20%) 0.96 (31%) 0.95 (23%) 0.97 (14%) 0.98 (43%) 0.94 (15%) 0.86 (27%) Table 7. Task HillSearch. Additional Instance 1. Results across . = 1 = 2 = 3 = 4 = 1 = = 3 = 4 = 36 = 48 Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.34 0.60 0.52 0.83 0.49 0.69 0.41 0.74 0.33 0.57 0.35 (3%) 0.77 (29%) 0.94 (81%) 0.96 (16%) 0.67 (37%) 0.88 (28%) 0.67 (65%) 0.86 (16%) 0.31 (5%) 0.96 (68%) 0.61 (78%) 0.86 (44%) 0.97 (87%) 0.98 (18%) 0.71 (46%) 0.84 (23%) 0.48 (18%) 0.90 (22%) 0.23 (31%) 0.94 (64%) 0.88 0.67 (95%) 0.89 (48%) 0.94 (80%) 0.98 (18%) 0.85 (74%) 0.97 (42%) 0.64 (59%) 0.90 (22%) 0.61 (87%) 0.98 (71%) 0.30 0.59 0.58 0.85 0.49 0.56 0.63 0.76 0.34 0. 0.46 (57%) 0.83 (40%) 0.71 (24%) 0.93 (10%) 0.59 (20%) 0.80 (42%) 0.69 (9%) 0.86 (13%) 0.48 (43%) 0.88 (46%) 0.61 (107%) 0.90 (51%) 0.95 (65%) 0.99 (17%) 0.77 (58%) 0.97 (73%) 0.78 (23%) 0.89 (17%) 0.57 (69%) 0.96 (59%) 0.89 0.65 (121%) 0.88 (48%) 0.96 (66%) 0.99 (17%) 0.63 (29%) 0.83 (47%) 0.54 (14%) 0.93 (22%) 0.30 (13%) 0.94 (55%) For HillSearch, we generate 50 instances using the procedure in Appendix B.1. We sample parameters uniformly at random, with . We fix the remaining parameters: αdecoy = 0.01, αneedle = 0.008, } jdecoy = 0.1, and jneedle = 0.2. + 1, + 2 and 2, 3, 4 { { } For TreeSearch, we generate 50 instances uniformly at random following Appendix B.1. We use the parameters rtrap [20, 40], and dgood [1, 4], rgood [3, 5], dtrap [10, 16]. [1, 4], For MaxSatSearch, we generate 50 instances as defined in Appendix B.1. We sample parameters uniformly: kgold [60, 160]. We set = wgold + mother, where mother [2, 6], and wgold [3, 4], kother [30, 80]. [12, 24], For all tasks, we run the baseline for 200 episodes per instance. We run the model for 50 episodes per instance for each setting of and p. E.5. Full Results of Difficulty Variation We generate task instances with varying difficulty levels based on the parameters defined in Appendix B.1. For HillSearch, we vary from 2 to 8, with = αneedle = 0.008, jdecoy = 0.1, and jneedle = 0.2. 1. We set the remaining parameters as follows: αdecoy = 0.01, For TreeSearch, we vary rgood from 1 to 7. We fix the other parameters at rtrap = 4, = 3, dtrap = 40, and dgood = 12. For MaxSatSearch, we vary kgold from 1 to 7. We fix the number of non-gold variables at 11 (by setting = 11 + kgold) and the number of non-gold clauses at 50 (by setting = 50 + wgold, where wgold = 130 + 10kgold). We also fix kother = 2. Figure 12 presents the complete evaluation results with budget = 36, covering both intervention methods: parallel 24 Failing to Explore: Language Models on Interactive Tasks Table 8. Task TreeSearch. Additional Instance 1. Results across . = 1 = 2 = 3 = 4 = 1 = = 3 = 4 = 1 = 2 = 3 = = 36 = 48 = 60 Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.65 0.71 0.68 0.62 0.70 0.60 0.67 0.58 0.65 0.51 0.87 (34%) 0.85 (20%) 0.84 (23%) 0.80 (29%) 0.80 (15%) 0.80 (32%) 0.66 (2%) 0.81 (38%) 0.89 (37%) 0.68 (33%) 0.76 (17%) 0.76 (8%) 0.78 (13%) 0.72 (15%) 0.77 (10%) 0.78 (29%) 0.64 (4%) 0.78 (33%) 0.75 (16%) 0.68 (32%) 0.61 (6%) 0.59 (16%) 0.59 (14%) 0.56 (9%) 0.59 (15%) 0.60 (1%) 0.54 (19%) 0.60 (2%) 0.57 (13%) 0.50 (3%) 0.93 0.65 0.81 0.71 0.65 0.66 0.71 0.60 0.67 0.63 0.51 0.83 (28%) 0.81 (0%) 0.81 (14%) 0.73 (12%) 0.84 (27%) 0.74 (4%) 0.53 (11%) 0.74 (11%) 0.78 (24%) 0.71 (38%) 0.92 (42%) 0.91 (13%) 0.91 (28%) 0.90 (38%) 0.89 (35%) 0.89 (25%) 0.83 (39%) 0.81 (21%) 0.89 (42%) 0.75 (47%) 0.96 0.79 (22%) 0.83 (3%) 0.79 (11%) 0.76 (17%) 0.82 (24%) 0.80 (13%) 0.78 (30%) 0.80 (20%) 0.80 (28%) 0.67 (31%) 0.69 0.85 0.72 0.69 0.73 0.79 0.64 0.70 0.78 0.69 0.85 (22%) 0.79 (8%) 0.80 (11%) 0.88 (28%) 0.87 (19%) 0.81 (4%) 0.77 (20%) 0.80 (15%) 0.71 (9%) 0.58 (16%) 0.96 (38%) 0.89 (5%) 0.84 (17%) 0.88 (28%) 0.90 (23%) 0.90 (15%) 0.88 (37%) 0.92 (31%) 0.90 (16%) 0.71 (3%) 0.98 0.96 (39%) 0.91 (7%) 0.97 (36%) 0.93 (35%) 0.97 (33%) 0.93 (19%) 0.90 (40%) 0.93 (33%) 0.93 (19%) 0.74 (7%) Table 9. Task TreeSearch. Additional Instance 2. Results across . = 1 = = 3 = 4 = 1 = 2 = 3 = = 1 = 2 = 3 = 4 = 36 = = 60 Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.62 0.82 0.74 0.57 0.67 0.66 0.61 0.52 0.56 0. 0.83 (34%) 0.80 (3%) 0.74 (0%) 0.82 (43%) 0.81 (22%) 0.79 (19%) 0.75 (23%) 0.77 (48%) 0.79 (40%) 0.55 (14%) 0.67 (7%) 0.63 (23%) 0.69 (7%) 0.65 (14%) 0.68 (3%) 0.69 (4%) 0.53 (13%) 0.70 (35%) 0.67 (20%) 0.46 (5%) 0.48 (22%) 0.53 (36%) 0.52 (30%) 0.49 (14%) 0.52 (22%) 0.51 (23%) 0.51 (16%) 0.50 (2%) 0.51 (9%) 0.44 (9%) 0.67 0.89 0.66 0.63 0.63 0.65 0.57 0.62 0.62 0.41 0.85 (26%) 0.81 (8%) 0.85 (30%) 0.80 (26%) 0.76 (20%) 0.87 (34%) 0.67 (19%) 0.80 (29%) 0.86 (40%) 0.65 (60%) 0.92 (37%) 0.87 (2%) 0.86 (31%) 0.83 (32%) 0.84 (33%) 0.86 (33%) 0.74 (31%) 0.89 (43%) 0.91 (47%) 0.78 (91%) 0.69 (2%) 0.72 (19%) 0.71 (8%) 0.67 (6%) 0.69 (9%) 0.68 (5%) 0.64 (13%) 0.69 (11%) 0.69 (13%) 0.64 (57%) 0.89 0.98 0.65 0.93 0.73 0.67 0.78 0.75 0.74 0.74 0.71 0. 0.84 (29%) 0.85 (8%) 0.74 (1%) 0.86 (27%) 0.79 (1%) 0.82 (11%) 0.71 (4%) 0.83 (12%) 0.77 (9%) 0.67 (12%) 0.93 (42%) 0.91 (2%) 0.89 (21%) 0.82 (22%) 0.84 (8%) 0.84 (12%) 0.89 (20%) 0.94 (27%) 0.88 (25%) 0.62 (19%) 0.99 0.84 (29%) 0.88 (5%) 0.90 (23%) 0.83 (24%) 0.84 (8%) 0.86 (15%) 0.84 (13%) 0.89 (21%) 0.87 (23%) 0.85 (12%) (p 2, 3, 4 } { ) and summary (s 2, 3, 4, 6 ). } { E.6. Standard Errors Table 14 reports the standard errors for the results in Table 2, calculated using bootstrap resampling. The non-overlapping intervals indicate clear separation in performance between the parallel method and the single-thread baseline. Similarly, Table 15 details the standard errors for the summary method experiments  (Table 3)  , also derived via bootstrap resampling. F. Details on Providing Models with Summary We include our prompts for summary generation in Figures 13, 14, and 15. These prompts rely on the query history and previously revealed context, providing no new information to the model. 25 Failing to Explore: Language Models on Interactive Tasks Table 10. Task MaxSatSearch. Additional Instance 1. Results across . = = 2 = 3 = 4 = 1 = 2 = = 4 = 36 = 48 Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.42 0.42 0.60 0.52 0.56 0.61 0.83 0.48 0.47 0.64 0.58 (40%) 0.50 (19%) 0.61 (2%) 0.66 (28%) 0.62 (10%) 0.72 (17%) 0.75 (9%) 0.66 (38%) 0.59 (26%) 0.59 (8%) 0.55 (33%) 0.58 (37%) 0.72 (21%) 0.63 (21%) 0.63 (12%) 0.65 (7%) 0.69 (17%) 0.71 (49%) 0.56 (19%) 0.55 (14%) 0.74 0.56 (35%) 0.74 (75%) 0.73 (22%) 0.65 (26%) 0.60 (7%) 0.65 (5%) 0.64 (23%) 0.64 (34%) 0.65 (39%) 0.63 (3%) 0.45 0.36 0.53 0.58 0.57 0.72 0.76 0.53 0.52 0.70 0.60 (33%) 0.53 (47%) 0.73 (37%) 0.66 (14%) 0.69 (22%) 0.73 (1%) 0.90 (18%) 0.62 (16%) 0.58 (11%) 0.81 (16%) 0.62 (37%) 0.69 (91%) 0.69 (30%) 0.65 (11%) 0.68 (19%) 0.76 (6%) 0.82 (7%) 0.77 (45%) 0.65 (25%) 0.80 (15%) 0.83 0.61 (37%) 0.68 (88%) 0.68 (28%) 0.70 (20%) 0.70 (23%) 0.74 (2%) 0.66 (14%) 0.71 (33%) 0.61 (17%) 0.75 (8%) Table 11. Task MaxSatSearch. Additional Instance 2. Results across . = 1 = 2 = 3 = 4 = = 2 = 3 = 4 = 36 = 48 Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gptQwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.44 0.33 0.56 0.57 0.53 0.64 0.65 0.53 0.49 0.69 0.47 (7%) 0.53 (61%) 0.64 (13%) 0.59 (3%) 0.61 (14%) 0.64 (0%) 0.73 (12%) 0.57 (8%) 0.48 (3%) 0.71 (3%) 0.56 (27%) 0.57 (72%) 0.55 (3%) 0.67 (17%) 0.61 (16%) 0.79 (23%) 0.72 (12%) 0.56 (6%) 0.36 (28%) 0.65 (6%) 0.73 0.57 (31%) 0.64 (94%) 0.67 (18%) 0.66 (17%) 0.60 (14%) 0.73 (15%) 0.72 (11%) 0.64 (23%) 0.65 (31%) 0.59 (14%) 0.50 0.43 0.42 0.54 0.51 0.72 0. 0.48 0.59 0.71 0.58 (17%) 0.58 (34%) 0.68 (61%) 0.66 (21%) 0.59 (16%) 0.75 (3%) 0.77 (6%) 0.76 (58%) 0.56 (5%) 0.58 (19%) 0.67 (35%) 0.63 (45%) 0.71 (69%) 0.75 (38%) 0.64 (27%) 0.80 (10%) 0.83 (14%) 0.68 (41%) 0.56 (4%) 0.74 (4%) 0. 0.66 (33%) 0.69 (58%) 0.70 (68%) 0.67 (24%) 0.72 (42%) 0.71 (2%) 0.70 (4%) 0.66 (36%) 0.47 (21%) 0.59 (17%) Table 12. Task MaxSatSearch. Additional Instance 3. Results across . = 1 = 2 = = 4 = 1 = 2 = 3 = 4 = = 48 Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit 0.48 0.30 0.45 0.60 0.50 0.67 0.74 0.51 0.50 0. 0.51 (6%) 0.55 (80%) 0.67 (48%) 0.62 (4%) 0.55 (11%) 0.68 (0%) 0.74 (0%) 0.58 (13%) 0.61 (23%) 0.62 (0%) 0.61 (26%) 0.56 (84%) 0.68 (50%) 0.56 (6%) 0.62 (24%) 0.64 (4%) 0.64 (13%) 0.59 (15%) 0.60 (21%) 0.53 (16%) 0.76 0.61 (25%) 0.68 (123%) 0.70 (55%) 0.62 (5%) 0.59 (18%) 0.70 (4%) 0.51 (32%) 0.68 (33%) 0.59 (20%) 0.63 (1%) 0.49 0.35 0.52 0.59 0.53 0.63 0.71 0.56 0.53 0.70 0.54 (9%) 0.57 (61%) 0.70 (36%) 0.57 (3%) 0.69 (29%) 0.65 (3%) 0.70 (2%) 0.67 (19%) 0.51 (4%) 0.66 (6%) 0.63 (28%) 0.60 (71%) 0.71 (38%) 0.68 (16%) 0.66 (25%) 0.76 (19%) 0.77 (8%) 0.68 (21%) 0.50 (5%) 0.68 (2%) 0.81 0.61 (24%) 0.64 (83%) 0.67 (30%) 0.62 (4%) 0.69 (30%) 0.72 (14%) 0.63 (11%) 0.52 (7%) 0.59 (13%) 0.68 (2%) 26 Failing to Explore: Language Models on Interactive Tasks Table 13. Performance comparison on the general dataset with budget = 48. We report the average reward using summarization (s) and parallelization (p) strategies. Relative improvements over the base model are shown in parentheses."
        },
        {
            "title": "MaxSatSearch",
            "content": "Qwen2.5-7B-Instruct summary parallel = 2 = 3 = 4 = 2 = 3 = 4 0. 0.30 (26%) 0.32 (38%) 0.36 (55%) 0.30 (29%) 0.37 (57%) 0.37 (59%) explore-exploit baseline 0.75 0.66 0.76 (15%) 0.78 (18%) 0.81 (22%) 0.78 (17%) 0.86 (30%) 0.84 (27%) 0.92 0.42 0.50 (20%) 0.54 (29%) 0.57 (35%) 0.54 (28%) 0.57 (36%) 0.59 (41%) 0. (a) HillSearch (b) TreeSearch (c) MaxSatSearch Figure 12. Task difficulty variations for HillSearch (a), TreeSearch (b), and MaxSatSearch (c). We generate multiple task instances by varying key difficulty-controlling parametersthe peak width for HillSearch, the ratio of good to trap gateways for TreeSearch, and the size of the gold clause for MaxSatSearchwith harder settings yielding lower baseline performance. Across all tasks and difficulty levels, the parallel method (same total budget, split across independent threads) and the summary method consistently improve over single-thread execution of Qwen2.5-7B-Instruct, narrowing the gap to simple exploreexploit baselines. Budget = 36 is considered for these episodes. Table 14. Results of Table 2 along with their standard errors in parentheses. Text color denotes performance relative to the single-thread execution (p = 1): blue indicates an improvement with non-overlapping error intervals; black indicates an improvement where intervals overlap; and red indicates performance below the single-thread execution. Bold values represent the best result for each model across values. Model Qwen2.5-7B-Instruct Qwen3-4B-Instruct Qwen3-8B gemini-2.5-flash-lite gpt-5-nano gpt-5-mini gpt-5 Qwen3-8B-medium gpt-5-nano-medium gpt-5-mini-medium explore-exploit = 1 0.33 (5%) 0.27 (4%) 0.59 (6%) 0.41 (5%) 0.31 (5%) 0.77 (5%) 0.72 (5%) 0.29 (4%) 0.48 (6%) 0.58 (6%) HillSearch = 2 = 3 = 4 = 1 TreeSearch = 2 = = 4 = 1 MaxSatSearch = 3 = 2 0.52 (7%) 0.52 (7%) 0.71 (6%) 0.58 (7%) 0.47 (7%) 0.82 (5%) 0.83 (5%) 0.35 (6%) 0.75 (6%) 0.84 (5%) 0.74 (7%) 0.45 (8%) 0.57 (9%) 0.57 (8%) 0.55 (9%) 0.87 (5%) 0.77 (7%) 0.38 (7%) 0.88 (5%) 0.86 (5%) 0.66 (9%) 0.30 (8%) 0.72 (7%) 0.55 (10%) 0.74 (8%) 0.91 (5%) 0.75 (8%) 0.30 (7%) 0.96 (3%) 0.79 (8%) 0.71 (3%) 0.91 (2%) 0.68 (3%) 0.63 (3%) 0.69 (3%) 0.64 (3%) 0.63 (5%) 0.70 (5%) 0.68 (3%) 0.54 (4%) 0.83 (4%) 0.85 (3%) 0.76 (4%) 0.75 (4%) 0.83 (4%) 0.84 (3%) 0.74 (6%) 0.85 (5%) 0.80 (4%) 0.58 (7%) 0.93 (2%) 0.84 (4%) 0.88 (3%) 0.80 (5%) 0.91 (3%) 0.90 (3%) 0.78 (7%) 0.94 (3%) 0.89 (3%) 0.67 (8%) 0.96 (2%) 0.91 (3%) 0.93 (3%) 0.85 (4%) 0.96 (2%) 0.91 (3%) 0.94 (4%) 0.94 (4%) 0.95 (2%) 0.66 (9%) 0.45 (4%) 0.39 (4%) 0.48 (4%) 0.62 (4%) 0.62 (4%) 0.67 (4%) 0.78 (5%) 0.54 (5%) 0.54 (5%) 0.66 (5%) 0.53 (5%) 0.58 (5%) 0.70 (4%) 0.71 (5%) 0.66 (4%) 0.66 (5%) 0.82 (6%) 0.70 (6%) 0.59 (6%) 0.65 (7%) 0.58 (6%) 0.66 (6%) 0.69 (4%) 0.70 (5%) 0.60 (5%) 0.82 (4%) 0.74 (7%) 0.69 (7%) 0.61 (6%) 0.73 (7%) 0.97 (0.2%) 0.96 (1%) 0.84 (2%) = 4 0.63 (6%) 0.57 (6%) 0.72 (4%) 0.68 (6%) 0.72 (4%) 0.67 (6%) 0.76 (7%) 0.64 (8%) 0.65 (7%) 0.71 (7%) 27 Failing to Explore: Language Models on Interactive Tasks Table 15. Results of Table 3 along with their standard errors in parentheses. Text color denotes performance relative to no summary execution: blue indicates an improvement with non-overlapping error intervals; black indicates an improvement where intervals overlap; and red indicates performance below the no summary execution."
        },
        {
            "title": "MaxSatSearch",
            "content": "Qwen2.5-7B-Instruct summary = 2 = 3 = 4 = 6 explore-exploit baseline 0.33 (5%) 0.45 (6%) 0.43 (5%) 0.52 (5%) 0.62 (5%) 0.97 (0.2%) 0.71 (3%) 0.78 (4%) 0.82 (4%) 0.82 (4%) 0.80 (3%) 0.96 (1%) 0.45 (4%) 0.55 (4%) 0.57 (4%) 0.66 (4%) 0.60 (4%) 0.84 (2%) Summarization prompt for HillSearch task ### MISSION HAND-OFF ### previous agent spent queries exploring the domain [0, 10] but was terminated. You are NEW agent brought in to take over. You must find the global maximum. **Data Collected by Previous Agent (Sorted by x):** x=x1 f(x)=y1 x=x2 f(x)=y2 **Unexplored Gaps:** * Interval [l1 , r1 ] (Gap size: d1 ). * Interval [l2 , r2 ] (Gap size: d2 ). **Instructions for the New Agent:** 1. Review the history. Did the previous agent get stuck in local maximum? 2. Formulate fresh plan to utilize your remaining budget. You have queries remaining. Review the Hand-off data above and output your next query. Figure 13. The prompt used to summarize model interactions in the HillSearch task. As the model can deduce this information from the query history and previously revealed context, this prompt provides nothing new. Blue text indicates dynamic variables specific to the problem instance or generated queries. 28 Failing to Explore: Language Models on Interactive Tasks Summarization prompt for TreeSearch task ### MISSION HAND-OFF ### previous agent spent queries exploring the graph but was terminated. You are NEW agent brought in to take over. You must find the maximum value. **Query History (in order):** * Node v1 value x1 * Node v2 value x2 **Current Best Found:** Node v3 with Value x3. **Next Nodes to Query (grouped by height):** Each listed node is actionable (unknown, with known neighbor). Node order is shuffled within each height. * height 1: [v4, v5, ] * height 2: [v6, v7, ] **Instructions for the New Agent:** Pick node to query next. Do not blindly continue the last path; consider switching to different height/frontier. You have queries remaining. Output your next query. Figure 14. The prompt used to summarize model interactions in the TreeSearch task. As the model can deduce this information from the query history and previously revealed context, this prompt provides nothing new. Blue text indicates dynamic variables specific to the problem instance or generated queries. Summarization prompt for MaxSatSearch task ### MISSION HAND-OFF ### previous agent spent queries exploring the MAX-SAT black-box but was terminated. You are NEW agent brought in to take over. **Progress:** tried assignments. Best score so far: **s**. **Query History (in order):** * step 1: score=s1 assignment=bitstring1 * step 2: score=s2 assignment=bitstring2 **Best Assignment Found (inner x0..xN-1 bitstring):** bitstring **Coverage Summary:** Counts below summarize how often each variable took value 0 or 1 across the queried assignments. - Variables with fewest 0s observed: xa1 (was 0 in ca1 /t), xa2 (was 0 in ca2 /t), - Variables with fewest 1s observed: xb1 (was 1 in cb1 /t), xb2 (was 1 in cb2 /t), You have queries remaining. Figure 15. The prompt used to summarize model interactions in the MaxSatSearch task. As the model can deduce this information from the query history and previously revealed context, this prompt provides nothing new. Blue text indicates dynamic variables specific to the problem instance or generated queries. 29 Algorithm 1 explore-exploit Baseline for HillSearch Failing to Explore: Language Models on Interactive Tasks = { (xi, yi) i=1 } = [L, R] where = 0, = 10, Exploration fraction α = 0.8, Window fraction β = 0. αN # Current query count # Exploration: Stratified Sampling L)/Nexplore 1: Input: Query budget , Query history 2: Output: Next query point xnext 3: Parameters: Domain 4: 5: Nexplore 6: if < Nexplore then 7: 8: = (R 9: 10: Sample xnext 11: 12: else 13: 14: 15: 16: 17: end if 18: return xnext # Exploitation: Local Search argmaxx{ β L) (R Uniform(x Sample xnext + + Uniform(l, r) (x, y) H} # Best observed point w/2, + w/2) G. Baseline Details In this section, we provide implementation details for the exploreexploit baselines and study the sensitivity of their performance to parameter choices. G.1. Pseudocodes We present pseudocode for the explore-exploit baselines to ensure reproducibility. These algorithms share the same interface as the language models: they receive the query budget , the task structure, and the query history as input, and output the next query. HillSearch. Algorithm 1 details the query selection strategy for HillSearch. The method partitions the budget into exploration and exploitation phases. During exploration, it uses stratified sampling strategy. During exploitation, it defines local window around the best point observed so far and samples uniformly within that region. TreeSearch. Algorithm 2 outlines the node selection process for the TreeSearch baseline. This method identifies frontier nodes (unvisited nodes with visited parents) and assigns score to each candidate equal to the observed value of its parent. The algorithm then selects the next query via softmax sampling over these scores, controlled by temperature parameter τ . MaxSatSearch. Algorithm 3 describes the variable assignment procedure for MaxSatSearch. In the exploration phase, the algorithm generates variable assignments uniformly at random. In the exploitation phase, it identifies the highest-value assignment in the history and flips single random bit to generate local neighbor as the next query. G.2. Parameters In this section, we evaluate the sensitivity of the explore-exploit baselines to different parameter settings across all tasks. We conduct 500 evaluation episodes for the baselines using the same problem instances described in 5.1. These results are summarized in Figure 16. HillSearch. We vary the exploration fraction α , which represents the proportion of queries allocated to stratified random sampling. Throughout the paper, we set α = 0.8. Figure 16 (a) illustrates the reward as function of α given budget of = 48. The results indicate that settings with 0.5 0.1 1 outperform the LMs, and any α 0, 0.1, . . . , 1 { α } 30 Algorithm 2 explore-exploit Baseline for TreeSearch Failing to Explore: Language Models on Interactive Tasks be the set of nodes queried in denote the observed values 1: Input: Query budget , Tree = (V, E) rooted at r, Query history 2: Hyperparameter: Temperature τ 3: Output: Next node to query unext 4: # Identify queried state 5: Let Vobs 6: Let : Vobs 7: # Identify and score candidates 8: Initialize candidate set 9: for each 10: 11: 12: 13: end for 14: # Weighted sampling 15: Define probability (u) for each (u) 16: 17: Sample unext 18: return unext # Score candidate based on parent value Su Vobs such that its parent p(u) exp(Su/τ ) { R(p(u)) Vobs do } : Algorithm 3 explore-exploit Baseline for MaxSatSearch { α = # Current query count # Exploration Phase: Random Sampling Uniform( Sample xnext 1: Input: Query budget , Number of variables n, Exploration fraction α 2: Input: Query history (xi, yi) i=1 } 3: Output: Next query assignment xnext 4: 5: Nexplore 6: if < Nexplore then 7: 8: 9: else 10: 11: 12: 13: 14: end if 15: return xnext # Exploitation Phase: Local Search Sample variable index Generate xnext by flipping the k-th bit of argmax(x,y)H # Best found assignment Uniform(0, 0, 1 } n) 1) { yields results superior to the Qwen2.5-7B-Instruct model. This demonstrates that our baseline, despite its simplicity, achieves consistent performance across parameters and can be tuned to maximize results. TreeSearch. We examine the effect of the soft-max temperature τ , which controls the next-node selection. We vary τ , with τ = 4 used as the default in the paper. Figure 16 (b) plots the reward against τ for budget of = 48. We observe that all variations, including the greedy strategy (τ = 0), perform comparably to Qwen2.5-7B-Instruct, while values in the range 2 τ 6 surpass the performance of the best LM. 0, 0.5, . . . , { } MaxSatSearch. We vary the fraction α , representing the proportion of queries where we randomly } sample an assignment. We utilized α = 0.5 for the main experiments. Figure 16 (c) displays the reward for each α with budget of = 48. All variations achieve better performance than Qwen2.5-7B-Instruct, while settings where α 0.3 outperform all evaluated LMs. 0, 0.1, . . . , 1 { 31 Failing to Explore: Language Models on Interactive Tasks (a) HillSearch (b) TreeSearch (c) MaxSatSearch Figure 16. Performance of the explore-exploit baselines with query budget of = 48 across all tasks under varying parameter settings. The baselines consistently outperform the language models across wide range of parameter values. H. Theoretical Results We will first restate Theorem 5.1 and prove it. Then for tasks MaxSatSearch and TreeSearch, we provide theoretical proof that parallelization has no gain over an optimal single-thread strategy. Theorem 5.1. Let q(x) = cxα with 0 < holds for all < vp and fails for all 1 and 0 < α < 1. For any integer > 1, there exists vp (0, 1] such that (1) vp. Proof of Theorem 5.1. Substituting q(x) = cxα, we observe that q(x) = pαq(x/p). By defining := q(x/p), the condition in (1) can be rewritten in terms of the single sub-trace probability y: ( 1 y)p > pαy. We define the function h(y) as the difference between the parallel success probability (LHS) and the single success probability (RHS): h(y) = (1 y)p pαy. We analyze the shape of h(y) to determine where it is positive. First, we evaluate the function at the origin as h(0) = pα. Evaluating the 1 derivative at = 0: 0 = 0. Next, we examine the first derivative with respect to y: h(y) = p(1 y)p1 (1)p Since α < 1 and > 1, we have > pα. Consequently, h(0) > 0. This implies that for small > 0, h(y) is positive, and parallelization is beneficial. Finally, we examine the second derivative to determine concavity: h(0) = pα. h(y) = p(p 1)(1 y)p2. 2, h(y) < 0 for all Since [0, 1). This indicates that h(y) is strictly concave. Because h(y) starts at 0 with positive slope (h(0) > 0) and is strictly concave, the derivative h(y) is strictly decreasing. The function h(y) will rise to peak and then decrease. It follows that there exists unique crossing point and h(y) > 0 such that h(y) > 0 for < 0 for p. Mapping back to the budget via = c(x/p)α, we define the threshold budget: vp = (cid:19)1/α . (cid:18) c Thus, parallelization is beneficial for < vp and not beneficial for For = 2, solving h(y 2 = 0, we get 2) = 1 2αy 2)2 2 = 2 (1 vp. 2α. Then v2 = 2 (cid:18) 2 (cid:19)1/α 2α . Using the Taylor expansion for α 1, we get v2 4 ln 2 1α . 32 Failing to Explore: Language Models on Interactive Tasks TreeSearch: Parallel exploration is subset of single-thread exploration We demonstrate that any set of nodes explored by parallel threads is valid search space for single thread with an equivalent total budget. Let there be parallel threads, where each thread has budget of N/p. Let forms connected component that includes the root node. The parallel method returns the best node found across all threads: be the set of nodes explored by thread i. By definition, each parallel = max i[p] max vSi r(v). Consider single-thread agent with total budget . Let this agent explore the union of the sets visited by the parallel = (cid:83) threads, defined as is is also connected and rooted. Therefore, valid trajectory exists for connected and shares the same root, their union single thread to visit every node in i. The total size of this set satisfies within budget , and thus: . Because every subset i[p] (cid:80) S Thus, the parallel strategy does not achieve better reward than the optimal single-thread strategy. max vS r(v) max i[p] max vSi r(v) = parallel. MaxSatSearch: Queries of parallel threads can be asked in single-thread interaction Consider parallel threads, each querying N/p assignments. Let denote the set of queried assignments, with solutions by taking the best across threads, achieving N/p. Thread achieves final reward maxxSi r(x). We merge the denote the set of all possible truth assignments. For each thread [p], let Si Si parallel = max i[p] max xSi r(x). Now consider single-thread interaction with budget that queries the union of all assignments queried by the parallel threads, = (cid:91) i[p] Si, so that N. This single-thread episode achieves reward at least maxxS r(x), and therefore max xS r(x) parallel. Thus, selecting the best outcome among independent threads with total budget is upper-bounded by what single-thread interaction with budget could achieve by querying the same set of assignments. This finishes the proof."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Maryland",
        "University of Maryland"
    ]
}