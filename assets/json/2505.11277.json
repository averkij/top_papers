{
    "paper_title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs",
    "authors": [
        "Yaorui Shi",
        "Shihan Li",
        "Chang Wu",
        "Zhiyuan Liu",
        "Junfeng Fang",
        "Hengxing Cai",
        "An Zhang",
        "Xiang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 7 7 2 1 1 . 5 0 5 2 : r Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs Yaorui Shi1*, Shihan Li1*, Chang Wu1, Zhiyuan Liu2, Junfeng Fang2, Hengxing Cai3, An Zhang1, Xiang Wang1, 1 University of Science and Technology of China 2 National University of Singapore 3 DP Technology {yaoruishi, sihang0520, xiangwang1223}@gmail.com, caihengxing@dp.tech Equal contribution. Corresponding author."
        },
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, reinforcement learning posttraining framework that adopts new search-and-refine-during-think paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively. Code is available at https://github.com/syr-cn/AutoRefine."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have shown impressive abilities in language understanding, planning, and problem solving [13]. Recent advances demonstrate that reinforcement learning (RL) [4] further enhances LLMs reasoning capabilities [5, 6], especially in complex tasks such as mathematics and coding [7, 8]. However, the knowledge encoded in LLMs is inherently constrained by their training corpora, limiting their reasoning performance on tasks requiring up-to-date information [9, 10]. common strategy to address this limitation is retrieval-augmented generation (RAG), which equips LLMs with retrieval tools to access external knowledge bases during question answering [1113]. Widely-adopted RAG pipelines typically rely on supervised fine-tuning (SFT) to train LLMs to issue search queries and generate responses based on retrieved documents [1416]. However, SFT often fails to generalize retrieval behaviors beyond the training distribution, leading to limited search capabilities in out-of-distribution scenarios [17]. To address this, recent studies draw inspiration from RL-based post-training [5] and explore RL for retrieval-augmented reasoning [1823]. Scrutinizing existing studies on retrieval-augmented reasoning, we summarize common search-during-think paradigm: given prompts with special search tokens (e.g., <search> ... </search>), the LLM is Preprint. Under review. Figure 1: Comparison between previous retrieval-augmented reasoning methods and AutoRefine. (a) Previous methods follow search-during-think paradigm, which may lead to incorrect answers due to irrelevant or noisy information. AutoRefine introduces search-and-refine-during-think paradigm to encourage identifying crucial information before generating the answer. (b) Previous methods optimize the model with solely outcome-based rewards, while AutoRefine introduces additional retrieval-specific rewards to explicitly guide document retrieval and utilization. trained via RL to autonomously invoke retrieval tools, retrieve some documents from external knowledge bases, and generate answers within <answer> ... </answer> using the retrieved information. Despite their promising results, we identify two core limitations inherent in the current retrievalaugmented reasoning paradigm: Lack refinement of retrieved documents. When facing out-of-scope questions, LLMs often require pieces of precise factual information (e.g., names of historical figures, dates of events). However, the current search-during-think paradigm typically uses retrieval tools to return full documents based on input queries, many of which are noisy or only weakly relevant. As illustrated in Figure 1(a), previous methods directly generate final answers from the retrieved content without first distilling key information, thus limiting the models ability to identify missing knowledge, reason over incomplete evidence, or iteratively refine its retrievals. Underexplored retrieval-specific rewards. While prior work on RL post-training highlights the importance of reward design [2426], most retrieval-augmented reasoning methods rely solely on an outcome-based reward typically assessing the correctness of the final answer. As shown in Figure 1(b), this coarse supervision underexplores retrieval-specific rewards and offers little direct guidance for improving the retrieval process itself. As result, it could be difficult for the LLM to learn how to retrieve more relevant or informative documents. To address these limitations, we propose AutoRefine, simple yet effective RL post-training framework that enhances the LLMs autonomous retrieval-augmented reasoning capability. At its core, AutoRefine adopts search-and-refine-during-think paradigm, guided by combination of answer and retrieval rewards. First, unlike prior search-during-think approaches that overlook refinement, we introduce an explicit knowledge refinement step into the reasoning loop using <search> ... </search>[documents]<refine> ... </refine> template (2.1). This search-and-refine-during-think training template explicitly encourages the model to refine retrieved documents identifying informative pieces while discarding irrelevant ones before generating final answers. Second, under this template, we apply Group Relative Policy Optimization (GRPO) [5] to train the model with both outcome-based and retrieval-specific rewards, rather than outcome-based rewards alone (2.2). Specifically, during training, we first sample several trajectories from the model, each consisting of sequence of <think>, <search>, <refine>, and <answer> steps. Retrieval rewards are computed 2 Figure 2: The training scheme of AutoRefine. (right) An actor model generates diverse reasoning trajectories for given question, including think, search, refine, and answer. (left) These trajectories are optimized using the GRPO [7] algorithm described in Equation (4), where the loss on retrieved documents is masked out. We take = 4 in this example. based on the quality of content distilled within the <refine> blocks, while answer rewards reflect the correctness of the final output in the <answer> block. This joint reward design explicitly guides the model to extract, organize, and utilize fine-grained knowledge throughout reasoning. To empirically assess AutoRefine, we conduct experiments on both single-hop [2729] and multihop [3033] question answering (QA) benchmarks. AutoRefine surpasses leading methods [5, 18, 21, 19] by 6.9% higher average accuracy, and shows especially high performances in multi-hop scenarios (cf. Table 1). It demonstrates strong ability in identifying and addressing knowledge gaps via multi-turn, high-quality search queries. The knowledge refinement steps also effectively extract crucial information from noisy retrieved documents, directly contributing to improved answer quality. Additional experiments confirm the contribution of both the retrieval-specific reward and the refinement module, and AutoRefine holds robust performance across different retrieval depths."
        },
        {
            "title": "2 Method",
            "content": "Due to limited space, we refer to Appendix for reasoning in LLMs and retrieval augmented generation. In this section, we introduce AutoRefine, simple yet effective RL framework that enhances the LLMs autonomous retrieval-augmented reasoning capability. We first outline the overall task formulation and trajectory generation steps, highlighting the novel search-and-refineduring-think paradigm (2.1). Next, we detail our reward modeling with both answer-based and retrieval-specific rewards to encourage fine-grained knowledge refinement (2.2). 2.1 Trajectory Generation with Searching and Refinement Task Formulation. Given dataset = {(q, a)} containing questionanswer pairs and an external search engine E, the task of retrieval-augmented reasoning requires the LLM to generate reasoning trajectories by iteratively interacting with the knowledge source E. Formally, for each question q, we generate reasoning trajectory: = (τ1, τ2, . . . , τT ), where the t-th intermediate reasoning step τt = (st, ct) consists of an action st {<think>, <search>, <documents>, <refine>, <answer>} and its associated content ct. The model is expected to repeatedly retrieve and refine knowledge from until reaching final answer oans that correctly addresses the question q. Rollout Generation. The actor LLM πθ generates trajectories by performing multiple rounds of interactions with the search engine E. The trajectories contain multiple internal reasoning cycles, as illustrated in Figure 2. Each cycle consists of sequence of structured operations: <think>...</think> for overall planning of consequent search actions, <search>...</search> for querying the external search engine, <document>...</document> for incorporating the retrieved documents, and 3 You are helpful assistant who is good at answering questions with multi-turn search engine calling. To answer questions, you must first reason through the available information using <think> and </think>. If you identify missing knowledge, you may issue search request using <search> query </search> at any time. The retrieval system will provide you with the three most relevant documents enclosed in <documents> and </documents>. After each search, you need to summarize and refine the existing documents in <refine> and </refine>. You may send multiple search requests if needed. Once you have sufficient information, provide concise final answer using <answer> and </answer>. <user> Question: {QUESTION} </user> Figure 3: Prompt template for rollout generation. <refine>...</refine> for distilling relevant information from the retrieved content. Following the reasoning phase, the model generates the final response within the <answer>...</answer> block based on the refined knowledge. Notably, the number of internal cycles is not manually pre-defined but autonomously determined by the actor LLM, adapting dynamically to the difficulty of the question. These tokens are defined and explained to the model via system instructions (cf. Figure 3). Stopping Criteria. The generation terminates when an answer action is produced, i.e., sT = <answer>. The content cT of the terminal state τT is extracted as the trajectorys final answer oans. 2.2 Reward Modeling with Retrieval-Aware Signals We use simple rule-based rewards to encourage free exploration during the RL process. The reward in AutoRefine consists of two complementary components: (1) the Outcome-Based Reward, which directly assesses the correctness of the answer generated by the model, and (2) the Retrieval-Specific Reward, which encourages the model to accurately identify and extract answer-relevant information from the retrieved documents. Outcome-Based Reward. The outcome-based reward (aka. the answer reward) RAns [0, 1] compares the models final answer within the <answer></answer> block to the ground-truth answer, measuring its correctness. Formally, we treat the predicted and ground-truth answers as sets of words, and use F1-score between these two sets as the reward: RAns = F1(oans, a) = 2oans oans + , (1) where oans is the predicted answer (e.g., Pierre-Auguste Renoir in yellow box of Figure 1(a), and is the ground truth answer from the (q, a) pair (e.g., the gray box in Figure 1(b)). Retrieval-Specific Reward. We further introduce an additional reward, the retrieval reward RRet {0, 1}, to explicitly encourage the extraction and utilization of relevant information from noisy retrieved documents. The retrieval reward is measured based on the quality of refined documents within the <refine></refine> blocks. Specifically, we collect all knowledge refinement steps (i.e., content within the <refine>...</refine> blocks) across the trajectory and concatenate them into single text sequence: RRet = I(a orefine = a), (2) where I() is the indicator function, orefine = (cid:83){ ct (st, ct) st = <refine> } is the concatenation of all the knowledge refinement steps (e.g., The documents concludes...The Umbrellas. in blue box of Figure 1). This reward is activated when all components of the ground-truth answer are present in the refined knowledge, reinforcing faithful and targeted information extraction. Integrating Outcome and Retrieval Rewards. The overall reward function in AutoRefine is designed to encourage both accurate final answers and meaningful intermediate knowledge extraction. Specifically, the model receives full reward of 1 if it generates the correct answer. If the final answer is incorrect but some relevant information has been extracted during the refinement step, partial reward of 0.1 is assigned. No reward is granted if neither correct answers nor relevant information are produced. The overall reward ROverall can be formally written as: ROverall = RAns, 0.1, 0. if RAns > 0 if RAns = 0 and RRet > 0 if RAns = RRet = 0 (3) 4 Training Objective. We apply Group Relative Policy Optimization (GRPO) [7] as the policy optimization algorithm for RL. The overview of the GRPO training scheme is shown in the top-left corner of Figure 2. Formally, given an actor model πθ and reference model πref, group of rollouts {oi}G i=1 is sampled as described in 2.1. We optimize the actor model πθ by maximizing: (cid:88) oi (cid:88) (cid:32) (cid:34) argmax θ JGRPO(θ) =E (q,a)D,{oi}G i=1πθold (q)"
        },
        {
            "title": "1\nG",
            "content": "i=1 1 oi min t=1 (cid:19) ˆAi,t (cid:33) πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) (cid:35) ˆAi,t, βDKL [πθ πref] clip (cid:18) πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) , 1 ϵ, 1 + ϵ (4) where ˆAi,t = [ri,t mean(rt)]/std(rt) is the normalized token-level advantage for the i-th rollout in the group, is the group size, ϵ is the clipping ratio, and β is the coefficient for the estimated KL divergence. As shown in the bottom-left part of Figure 2, we mask out the retrieved documents during the loss computation."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we aim to answer the following Research Questions (RQs): RQ1: How effectively does AutoRefines \"search-and-refine-during-think\" paradigm enhance performance in retrieval-augmented question answering? RQ2: Can AutoRefine effectively resolve information gaps through retrieval, especially when facing complex multi-hop problems? RQ3: Can knowledge refinement steps distill critical information from retrieved documents? RQ4: Can AutoRefine achieve robust performance improvements under different retriever settings? 3.1 Experiment Setup Datasets. We evaluate performance using seven diverse QA benchmarks, including three single-hop QA datasets: Natural Questions (NQ) [27], TriviaQA [29], PopQA [28], and four datasets that require multi-hop searching: HotpotQA [30], 2WikiMultihopQA (2Wiki) [31], Musique [32], Bamboogle [33] for evaluation. Exact match accuracy serves as the evaluation metric for all downstream datasets. Following the setting of prior works [19], we train AutoRefine using combined training set from NQ and HotpotQA. Baselines. In our experiments, we compare AutoRefine against three kinds of methods: (1) generation without retrieval (w/o Retrieval), including direct generation with LLM, supervised fine-tuning (SFT), and R1-like training (R1) [5] without Retrieval; (2) methods with single-hop retrieval (w/ Single-Hop Retrieval), including direct retrieval with the input question (Naive RAG); (3) training with retrieval (w/ Multi-Hop Retrieval) including: agentic search method Search-o1 [18], IRCoT [34], retrieval-augmented reasoning model Search-R1 [19] and ReSearch [21]. Implementation Details. To simulate real-world search scenario, we remove original context documents from the QA datasets [3032] and instead use the December 2018 Wikipedia dump [35] as the external knowledge source, with E5-base-v2 [36] as the retrieval engine. By default, the search engine retrieves the top three most relevant documents on each query. For RL-based baselines, we run experiments using both Qwen2.5-3B-Base and -Instruct models. For SFT and direct generation baselines, we use the instruct variant to better align with instruction-following tasks. Most baseline results are taken from Search-R1 [19], which has experimental settings consistent with ours. We reproduce ReSearch using the authors publicly available code. Additional implementation details can be found in Appendix B, and further experimental results are provided in Appendix C. 3.2 Overall Performance (RQ1) Table 1 presents the overall performance comparison between AutoRefine and the baseline methods. The Avg. column stands for the average accuracy. As shown in the results, AutoRefine significantly Table 1: (RQ1) Accuracy comparison of AutoRefine versus baseline methods with Qwen2.5-3B [3] across various QA benchmarks. Bold denotes best results, and underline denotes second best results. Methods w/o Retreival Direct Generation SFT R1-Instruct [5] R1-Base [5] w/ Single-Hop Retrieval Naive RAG [37] w/ Multi-Hop Retrieval Single-Hop QA Multi-Hop QA NQ TriviaQA PopQA HotpotQA 2Wiki Musique Bamboogle Avg. 0.106 0.249 0.210 0.226 0.288 0.292 0.449 0.455 0.108 0.104 0.171 0.173 0.149 0.186 0.208 0.201 0.244 0.248 0.275 0.268 0.020 0.044 0.060 0. 0.024 0.112 0.192 0.224 0.134 0.176 0.224 0.229 0.348 0.544 0.387 0. 0.226 0.047 0.080 0.270 Search-o1 [18] IRCoT [34] ReSearch-Instruct [21] ReSearch-Base [21] Search-R1-Instruct [19] Search-R1-Base [19] AutoRefine-Instruct AutoRefine-Base 0.238 0.111 0.365 0.427 0.397 0.421 0.436 0. 0.472 0.312 0.571 0.597 0.565 0.583 0.597 0.620 0.262 0.200 0.395 0.430 0.391 0.413 0.447 0.450 0.221 0.164 0.351 0.305 0.331 0.297 0.404 0.405 0.218 0.171 0.272 0.272 0.310 0.274 0.380 0.393 0.054 0.067 0.095 0.074 0.124 0.066 0.169 0.157 0.320 0.240 0.266 0.128 0.232 0.128 0.336 0. 0.255 0.181 0.331 0.319 0.336 0.312 0.396 0.405 outperforms baseline models across the seven benchmarks. It achieves 0.069 accuracy gain on the base variant and 0.060 improvement on the instruct one compared to the strongest baseline. Besides the overall performance, we observe that the performance gains achieved by AutoRefine are more obvious on the multi-hop QA benchmarks. For example, AutoRefine improves the performance on 2Wiki by 0.083 and Musique by 0.045, which implies 21% and 26.7% relative increase, respectively. According to further analysis in 3.3.1 and 3.3.2, we attribute AutoRefines extraordinary performance on multi-hop benchmarks to its ability to perform high-quality searching and efficient utilization of retrieved documents. Obs 1: AutoRefine significantly improves QA accuracies, especially on multi-hop benchmarks. 3.3 Analytical Results 3.3.1 Search Behaviors (RQ2) crucial capability of retrieval-augmented reasoning models is identifying and addressing knowledge gaps via retrieval. To evaluate this, we analyze the search frequency and search quality of AutoRefine in four scenarios: training samples, all seven downstream benchmarks, single-hop QA benchmarks, and multi-hop QA benchmarks. These analyses are depicted in Figure 4. Search Frequency reflects the models capability to recognize knowledge gaps and perform searches accordingly. This search behavior is measured by the average number of search calls model makes per rollout. We analyze the search frequency of both -Base and -Instruct variants of AutoRefine in Figure 4(a). As the figure shows, both variants have evolved multi-turn searching abilities after enough training steps. The average number of search calls converges to around 1.5 for AutoRefine-Instruct, and higher than 2 for AutoRefine-Base. Despite the high overall search frequency on seven benchmarks, AutoRefine demonstrates distinct search behaviors for single-hop and multi-hop questions. On the three single-hop benchmarks, both variants begin with fewer than 1.3 searches per rollout, and gradually adjust to 2.0 and 1.2. In contrast, the models begin with much higher search frequencies when facing multi-hop questions, which rapidly go up to 2.0 2.5. This phenomenon exhibits AutoRefines ability to dynamically adjust the number of search calls according to the complexity of downstream tasks, with more frequent searching on multi-hop questions and less on single-hop ones. Obs 2.1: AutoRefine learns to perform multi-turn searching and can adaptively issue search queries depending on task complexity. 6 Figure 4: (RQ2) Visualization of the search behaviors. (a) AutoRefines average number of search calls per rollout. For both variants, AutoRefine learns to adaptively issue more search queries for multi-hop questions and fewer for single-hop ones. (b) Comparison of search success rates between retrieval-augmented reasoning methods. While all methods draft more efficient search queries than naive retrieval, AutoRefine achieves more significant performance gains. Search Quality evaluates whether the model generates effective search queries that can return informative documents. Knowledge-intensive questions often demand precise factual information to ask, e.g., names of historical figures or dates of events. In such cases, search calls can only be considered successful if the retrieved documents directly contain the answer. Hence, we estimate the search quality by counting the proportion of successful searches where retrieved documents contain the ground truth answer a. We also include naive retrieval, which directly uses the input question to conduct one-turn searches, as reference. We compare the search quality of AutoRefine against Search-R1 and ReSearch trained from Qwen2.53B-Base in Figure 4(b). All reasoning-based methods learns to draft efficient queries that have higher searching quality than naive retrieval. In single-hop scenarios, all three methods converge to high success rate of about 70% after 100 steps. On multi-hop benchmarks, the search qualities of baseline methods also successfully converge to 30% 40%, much higher than that of baseline methods. In contrast, the search quality of AutoRefine continuously goes up to higher than 50%, which surpasses baseline methods by large margin of 10% 15%. Obs 2.2: AutoRefine drafts efficient queries that retrieve documents relevant to the answer. 3.3.2 Effectiveness of Knowledge Refinement (RQ3) To investigate the effectiveness of knowledge refinement, we specifically analyze whether the refinement steps successfully distill critical information from retrieved documents. We start by comparing the success rate of different actions: <search>, <refine>, and <answer>. For success rates of search and refine actions, we continuously use the quality measurement defined in 3.3.1, i.e., the proportion of documents/refinements that contain the ground truth answer. For the success rate of the answer action, we report the exact match accuracy as mentioned in 3.1. The results are shown in Figure 5. After enough training steps, the success rate of <refine> actions tends to align with that of the <search> action (Figure 5(a)). This suggests the model gradually learns to keep crucial evidence as long as the search returns correct documents. Figure 5(b) also provides the length of each component. The token count of refinement steps is about 100 200 tokens, which is much fewer than the documents ( 600 tokens). Comparing Figure 5(a) and (b), we find that 7 Figure 5: (RQ3) Comparison of search, refine, and answer actions over (a) success rates and (b) average token counts. Knowledge refinement keeps crucial information from retrieved documents while reducing context length. (RQ4) Comparison of Figure 6: downstream accuracies under different retrieval depths. AutoRefine exhibits robust gains for 1 7. the knowledge refinement steps of AutoRefine greatly reduce the context length, while successfully preserving the information that is relevant to the answer. Obs 3: Knowledge refinement steps efficiently distill critical evidence from retrieved documents while filtering out irrelevant content. 3.3.3 Impact of Retrieval Depths (RQ4) Different retriever settings may also influence retrieval-augmented generation models, and one important aspect is the retrieval depth. While more documents per search could potentially provide richer external knowledge, it also includes more noise in documents. To explore the models robustness across different retrieval depths, we vary the number of documents (k) returned by the retrieval engine at evaluation time, from 1 to 7, while training fixed to = 3. The comparison of inference accuracy is shown in Figure 6. All three methods achieve robust performance across different levels. Compared to the baseline methods, AutoRefine steadily boosts the average accuracy by 0.04 0.1, demonstrating its strong document denoising ability. The accuracy increments caused by AutoRefine are particularly obvious when 3, which is likely caused by its strong ability to discover useful information under increasingly noisy conditions. Peak performance gain is 0.09 observed at = 5, where balanced trade-off between information richness and noise is reached. Obs 4: AutoRefine exhibits consistent improvements across varying retrieval depths. 3.4 Ablation Studies We conduct ablation studies over the key components in AutoRefine. Specifically, we consider three configurations: (1) the full AutoRefine model, (2) AutoRefine without the retrieval-specific reward RRet (w/o Retrieval Reward), and (3) AutoRefine without both RRet and the knowledge refinement step (w/o Retrieval Reward & Refinement). Impact on Answer Accuracy. Table 2 presents the answer accuracy on downstream benchmarks for each configuration. The results demonstrate that both the retrieval-specific reward and the knowledge refinement step are essential for achieving strong performance. The full AutoRefine model consistently achieves the highest average accuracy across both the base and instruct variants. Impact on Search and Refinement Abilities. We further analyze how each component affects AutoRefines search and refinement capabilities. Figure 7 shows comparative analysis using Qwen2.5-3B-Base. As illustrated in Figure 7(a), the retrieval-specific reward effectively promotes multi-turn search behavior. Additionally, it significantly boosts knowledge refinement quality, yielding approximately 20% improvement in refinement success rate (Figure 7(c)). The inclusion of the knowledge refinement step also enhances both the frequency and quality of retrieval, as shown in Figures 7(a) and (b). 8 Table 2: Ablation study over key components in AutoRefine. Multi-Hop QA Single-Hop QA Model Variants AutoRefine-Base w/o Retrieval Reward w/o Retrieval Reward & Refinement AutoRefine-Instruct w/o Retrieval Reward w/o Retrieval Reward & Refinement NQ TriviaQA PopQA HotpotQA 2wiki Musique Bamboogle Avg. 0.467 0.423 0.422 0.436 0.418 0.406 0.620 0.583 0.585 0.597 0.587 0.580 0.450 0.424 0.419 0.447 0.429 0. 0.405 0.368 0.294 0.404 0.355 0.319 0.393 0.351 0.257 0.380 0.335 0.312 0.157 0.139 0.062 0.169 0.124 0. 0.344 0.344 0.144 0.336 0.272 0.210 0.405 0.376 0.312 0.396 0.360 0.333 Figure 7: Effectiveness of key components over the search behaviors and the refinement quality."
        },
        {
            "title": "4 Limitations",
            "content": "Despite the promising performance of AutoRefine in retrieval-augmented reasoning tasks, several limitations remain for further investigation. Model Scale. Due to constraints in training time and computational resources, all experiments in this paper use 3B-parameter language models. It remains unclear how well AutoRefines framework scales with larger backbones. Evaluation Metrics. This work evaluates model performance solely on exact match accuracy, which may overlook semantically correct responses with minor textual variations. This limits the evaluation of nuanced or open-ended responses. Static Retrieval Corpus. The retrieval component uses fixed Wikipedia snapshot, lacking current or time-sensitive information. This setting limits the systems applicability to real-world use cases where users expect information from live search engines."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "This work proposes AutoRefine, an RL post-training framework designed to improve the retrievalaugmented reasoning capabilities of LLMs. AutoRefine adopts novel search-and-refine-duringthink paradigm that explicitly encourages the model to identify and distill relevant information from noisy retrieved content. By jointly optimizing for both outcome-level and retrieval-specific rewards, AutoRefine effectively guides LLMs to extract, assess, and integrate external knowledge. Comprehensive evaluations show that AutoRefine consistently surpasses existing methods, achieving up to 6.9% average improved accuracy on seven QA benchmarks. These results underscore its potential to enhance the accuracy and reliability of retrieval-augmented LLMs. Future work will focus on addressing the limitations identified above. First, we plan to extend AutoRefine to larger-scale language models, such as 7B or 13B variants, to assess how the framework scales with model capacity. This may reveal new behaviors in refinement and reasoning that are not apparent at smaller scales. Second, we aim to adopt more flexible and semantically aware evaluation metrics such as LLM-as-a-Judge evaluation to more effectively measure answer quality in complex question answering tasks, where traditional metrics may fall short. Finally, we intend to adapt AutoRefine to dynamic retrieval settings, including live web search and continuously evolving document corpora. This will allow us to assess the frameworks performance in time-sensitive scenarios. By addressing these directions, we aim to further improve the scalability, flexibility, and time-sensitivity of AutoRefine, thus broadening its practicality in more realistic applications."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [3] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [4] Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. [5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. [6] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. doi: 10.48550/ARXIV.2412.16720. [7] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [8] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [9] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical 10 Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 79697992. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023. EMNLP-MAIN.495. [10] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 92489274. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP. 620. [11] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Yun-Nung Vivian Chen, Margot Mieskes, and Siva Reddy, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4146. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-TUTORIALS.6. [12] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. CoRR, abs/2312.10997, 2023. doi: 10.48550/ARXIV.2312.10997. [13] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Trans. Assoc. Comput. Linguistics, 11:13161331, 2023. doi: 10.1162/TACL_A_00605. [14] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [15] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models. In Kevin Duh, Helena Gómez-Adorno, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 83718384. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.NAACL-LONG.463. [16] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884, 2024. [17] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. CoRR, abs/2501.17161, 2025. doi: 10.48550/ARXIV.2501. 17161. [18] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [19] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [20] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [21] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng Chen, Haofen Wang, Jeff Pan, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. 11 [22] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. [23] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, and Yan Zhang. Zerosearch: Incentivize the search capability of llms without searching, 2025. [24] Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learnarXiv preprint ing approach for concise mathematical reasoning in language models. arXiv:2504.09696, 2025. [25] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. [26] Youssef Mroueh. Reinforcement learning with verifiable rewards: Grpos effective loss, dynamics, and success amplification. arXiv preprint arXiv:2503.06639, 2025. [27] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [28] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, 2023. [29] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017. [30] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. [31] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, 2020. [32] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [33] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711, 2023. [34] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1001410037. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.557. [35] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781, 2020. [36] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. 12 [37] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [38] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [39] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [40] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [42] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37:6473564772, 2024. [43] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. [44] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [45] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning. arXiv preprint arXiv:2410.01707, 2024. [46] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195, 2024. [47] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [48] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [49] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [50] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [51] Zilong Wang, Zifeng Wang, Long Le, Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, Chen-Yu Lee, and Tomas Pfister. Speculative RAG: Enhancing retrieval augmented generation through drafting. In The Thirteenth International Conference on Learning Representations, 2025. [52] Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, and Jie Zhou. Unsupervised information refinement training of large language models for retrieval-augmented In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of generation. the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 133145. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.9. 13 [53] Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, and Chenyan Xiong. RAG-DDR: Optimizing retrievalIn The Thirteenth International augmented generation using differentiable data rewards. Conference on Learning Representations, 2025. [54] Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin Mao. Improving retrieval-augmented generation through multi-agent reinforcement learning. arXiv preprint arXiv:2501.15228, 2025. [55] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. [56] Pengcheng Jiang. Deepretrieval: Powerful query generation for information retrieval with reinforcement learning. arXiv preprint arXiv:2503.00223, 2025. [57] Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Linpeng Tang, Wentao Zhang, et al. Rare: Retrieval-augmented reasoning modeling. arXiv preprint arXiv:2503.23513, 2025. [58] Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Julia Hockenmaier, and Tong Zhang. Rag-rl: Advancing retrieval-augmented generation via rl and curriculum learning. arXiv preprint arXiv:2503.12759, 2025. [59] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research. arXiv preprint arXiv:2502.04644, 2025. [60] Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. Cycleresearcher: Improving automated research via automated review. arXiv preprint arXiv:2411.00816, 2024. [61] Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, and Le Sun. Deepsolution: Boosting complex engineering solution design via tree-based exploration and bi-point thinking. arXiv preprint arXiv:2502.20730, 2025. [62] Ruibin Xiong, Yimeng Chen, Dmitrii Khizbullin, and Jürgen Schmidhuber. Beyond outlining: Heterogeneous recursive planning for adaptive long-form writing with language models. arXiv preprint arXiv:2503.08275, 2025. [63] Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, and Kang Liu. Reinforced internal-external knowledge synergistic reasoning for efficient adaptive search agent, 2025. [64] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024."
        },
        {
            "title": "A Related Work",
            "content": "This section reviews prior research on reasoning in LLMs and retrieval-augmented generation, two areas central to our approach. We highlight how recent advancements in RL-based post-training have enabled more adaptive retrieval-augmented reasoning, motivating our proposed search-and-refineduring-think paradigm. Reasoning in Large Language Models. The reasoning capabilities of large language models (LLMs) have advanced significantly in recent years [38, 39, 6, 40]. Early work introduces explicit chain-of-thought prompting [41] and test-time scaling methods such as monte carlo tree search [4246] to guide intermediate reasoning steps. Follow-up methods leveraged reinforcement learning (RL), particularly reinforcement learning from human feedback (RLHF) [47], to align outputs with human preferences via Proximal Policy Optimization (PPO) [48]. Due to RLHFs high resource demands, more recent developments [49, 50] such as Group Relative Policy Optimization (GRPO) [7, 5] optimize models using outcome-based rewards to reduce dependency on human annotations. These breakthroughs greatly improve LLMs generalization and performance on complex tasks such as mathematical problem-solving [7] and code generation [8]. Retrieval Augmented Generation. Retrieval-Augmented Generation (RAG) extends the capabilities of LLMs by integrating external knowledge [1113]. critical challenge within RAG systems is determining when and how to perform retrieval actions [10, 34]. Prior works have leveraged supervised fine-tuning (SFT) methods to train LLMs in generating appropriate retrieval queries [14, 51, 16, 15, 9, 52]. However, these SFT-based approaches struggle to generalize in out-ofdistribution retrieval scenarios [53, 54]. Recently, RL-based methods have enabled adaptive retrieval and context-aware query generation [5558], which can be termed as retrieval-augmented reasoning that facilitates deep research applications [5962]. Current methods follow the search-during-think paradigm, where the model learns multi-turn searching and reasoning with outcome-based reward [19, 21, 20, 22, 23, 63]. However, the explicit refinement of retrieved documents and direct rewards for retrieval quality are absent in this paradigm, hampering effective searching and document utilization. This work explores the search-and-refine-during-think paradigm, which enables LLMs to refine retrieved documents, guided by both outcome-based and retrieval-specific rewards."
        },
        {
            "title": "B More Implementation Details",
            "content": "B.1 Training Details AutoRefine is trained on 8 NVIDIA A100-80GB GPUs with full-parameter fine-tuning. We construct the training dataset by combining NQ [27] and HotpotQA [30], used consistently across AutoRefine and all training-based baseline methods. For distributed training, we adopt Fully Sharded Data Parallelism (FSDP), using BFloat16 precision throughout both training and evaluation. Table 3 summarizes the key hyperparameters used in our experiments. The actor model is optimized using learning rate of 1.0106 without warmup. Both the base and instruct variants of AutoRefine are trained for 200 steps using the VeRL framework [64], with random data shuffling. For efficient rollout generation, we use vLLM1 at GPU memory utilization rate of 0.6. Sampling is performed with temperature of 1.0, and maximum of 5 search calls per rollout is allowed. We generate 5 rollouts per data point, each with up to 5 search queries. Retrieved documents per query are concatenated and truncated to 512 tokens. Token length statistics shown in Figure 5(b) are computed using tiktoken2. For direct-inference and SFT baselines, we use Qwen2.5-3B-Instruct [3] as the backbone LLM. RL-based experiments are conducted on both the base and instruct variants. 1https://github.com/vllm-project/vllm 2https://github.com/openai/tiktoken Table 3: Primary hyperparameters used by AutoRefine. Hyper-parameter Value Training Batch Size Micro Training Batch Size Validation Batch Size Total Training Steps Actor Model Learning Rate Max Response Length Max Search Actions KL Coefficient β Clip Ratio ϵ Group Size 256 64 256 250 1 106 2048 5 0.001 0.2 5 B.2 Dataset Statistics All datasets are sourced from the FlashRAG Datasets collection3. Table 4 presents detailed statistics of the datasets used. The training set for AutoRefine is constructed from the train splits of NQ and HotpotQA, totaling 169, 615 examples. For evaluation, we combine the test or dev splits from seven datasets. Specifically, for benchmarks with test split (NQ, TriviaQA, PopQA, and Bamboogle), the test split is used; for those without test split (HotpotQA, 2Wiki, and Musique), we use the dev split instead. This results in an evaluation set comprising 51, 713 examples. Table 4: Statistics of the seven datasets used in this paper. NQ TriviaQA PopQA HotpotQA 2Wiki Musique Bamboogle Train Dev Test 79168 8757 3610 78785 8837 11313 - - 14267 90447 7405 - 15,000 12576 - 19,938 2417 - - -"
        },
        {
            "title": "C More Experimenal Results",
            "content": "C.1 Training Dynamics For more comprehensive understanding of AutoRefine, we visualize its training dynamics, including the training rewards, validation accuracies, and response length per sample. The validation is carried out on 500 random samples from each downstream benchmark per 20 training steps. The results are reported in Figure 8. We observe stable convergence in the training rewards and consistently improved validation accuracy in both base and instruct variants. Figure 8: Training dynamics of AutoRefine-Base and -Instruct. Both models show steady convergence and stable downstream accuracies. 3https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets 16 C.2 Case Studies Table 5 presents the answer generated by AutoRefine and Search-R1 on two randomly selected samples from 2Wiki and bamboogle. The comparison is made between AutoRefine, SearchR1 [19], and ReSearch [21]."
        },
        {
            "title": "D Broader Impacts",
            "content": "This work on AutoRefine has several potential broader impacts. By significantly enhancing the accuracy and reliability of retrieval-augmented language models, this research contributes to building more trustworthy and dependable AI systems. This poses positive implications for fields such as education, research, and information services, where access to accurate and synthesized knowledge is crucial. Furthermore, the \"search-and-refine-during-think\" paradigm introduced by AutoRefine explicitly encourages the model to engage in more deliberate process of information extraction and evaluation. This design may inspire future research into more interpretable and controllable reasoning frameworks. Understanding how LLMs can be guided to selectively utilize external knowledge is step towards demystifying their decision-making processes. 17 Table 5: Case Studies. The predictions are colored Red if the model answers incorrectly and Green if it answers correctly. The core evidences are colored Blue. Case 1: Who is the father of the father of modern experimental psychology? Ground Truth: Maximilian Wundt ReSearch: <think> To answer the question about who is the father of the father of modern experimental psychology, need to trace back through history and identify the person who founded modern experimental psychology, and then identify the father of this individual. </think> <search> Who is the father of the father of modern experimental psychology </search> <documents> [Doc 1: Wilhelm Wundt] Wilhelm Wundt Wilhelm Maximilian Wundt ... was the first person ever to call himself psychologist. He is widely regarded as the father of experimental psychology...</documents> <answer> Wilhelm Wundt </answer>. Search-R1: <think> have entrained myself with prepared oniosis in sync with tractable graftally towards emotionally lag useards glykerized ocal gambling huffely bite-support. Further exposure hides sectional stemming non-quently. </think> <search> Who is the father of the father of modern experimental psychology </search><documents>[Doc 1: Wilhelm Wundt] Wilhelm Wundt Wilhelm Maximilian Wundt ... was the first person ever to call himself psychologist. He is widely regarded as the father of experimental psychology...</documents> <answer> Wilhelm Wundt </answer>. AutoRefine: <think> To answer the question \"who is the father of the father of modern experimental psychology?\", would first research father of modern experimental psychology. </think> <search> father of the father of modern experimental psychology </search><documents>[Doc 1: Wilhelm Wundt] Wilhelm Wundt Wilhelm Maximilian Wundt ... was the first person ever to call himself psychologist. He is widely regarded as the father of experimental psychology...</documents> <refine> From the available documents, extract key information. Wilhelm Wundt is regarded as the father of experimental psychology.</refine> <think> Now, will research who Wundts father was by understanding psychology back then.</think> <search> Who was Wilhelm Wundt father </search><documents> [Doc 2: Wilhelm Wundt] ... Wundt was born at Neckarau, Baden (now part of Mannheim) on the 16 of August 1832, the fourth child to parents Maximilian Wundt </documents> <refine> From the available documents, extract key information. Wilhelm Wundts father was Maximilian Wundt.</refine> <answer> Maximilian Wundt </answer>. Case 2: What is the date of death of Alexander Of Masovias father? Ground Truth: 21 January 1426 ReSearch: <think> To answer this question, need the date of death of Alexander Of Masovias father. </think> <search> Alexander Of Masovias father date of death </search><documents> [Doc 1: Alexander of Masovia] Alexander of Masovia Alexander of Masovia (1400 - 2 June 1444) ... was the second son of Siemowit IV, Duke of Masovia...</documents> <answer> 1400 </answer>. Search-R1: <think>Im overknowledgeable in whatever guide possess it but must forth motion where direction need to going quests.</think> <search> Alexander Of Masovias father date of death </search><documents> [Doc 1: Alexander of Masovia] Alexander of Masovia Alexander of Masovia (1400 - 2 June 1444) ... was the second son of Siemowit IV, Duke of Masovia ...</documents> <answer> 1400 </answer>. AutoRefine <think> need to first determine the date of death of Alexander of Masovias father. </think> <search> date of death of Alexander Of Masovias father </search><documents> [Doc 1: Alexander of Masovia] Alexander of Masovia Alexander of Masovia (1400 - 2 June 1444) ... was the second son of Siemowit IV, Duke of Masovia ...</documents> <refine> From the provided documents, deduce that Alexander of Masowias father was Siemowit IV, Duke of Masovia.</refine> <think> To identify the date of death of Siemowit IV, Duke of Masowia who was the father of Alexander of Masowia. </think> <search> date of death Siemowit IV Duke of Masovia </search><documents> [Doc 1: Siemowit IV, Duke of Masovia] ... Siemowit IV, Duke of Masovia Siemowit IV (Ziemowit IV), also known as Siemowit IV the Younger (pl: 21 January 1426) ...</documents> <refine> From the provided documents, deduce that Siemowit IV, Duke of Masovia died on 21 January 1426. </refine> <answer> 21 January 1426 </answer>."
        }
    ],
    "affiliations": [
        "DP Technology",
        "National University of Singapore",
        "University of Science and Technology of China"
    ]
}