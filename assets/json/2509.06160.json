{
    "paper_title": "Reverse-Engineered Reasoning for Open-Ended Generation",
    "authors": [
        "Haozhe Wang",
        "Haoran Que",
        "Qixin Xu",
        "Minghao Liu",
        "Wangchunshu Zhou",
        "Jiazhan Feng",
        "Wanjun Zhong",
        "Wei Ye",
        "Tong Yang",
        "Wenhao Huang",
        "Ge Zhang",
        "Fangzhen Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 0 6 1 6 0 . 9 0 5 2 : r Reverse-Engineered Reasoning for Open-Ended Generation Haozhe Wang+1,2,4, Haoran Que+1,3, Qixin Xu6, Minghao Liu4,5, Wangchunshu Zhou4, Jiazhan Feng1, Wanjun Zhong1, Wei Ye3, Tong Yang3, Wenhao Huang1, Ge Zhang1,4, Fangzhen Lin2 1ByteDance Seed, 2Hong Kong University of Science and Technology, 3Peking University, 4M-A-P, 52077AI, 6Tsinghua University +Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "While the \"deep reasoning\" paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains critical challenge. The two dominant methods for instilling reasoningreinforcement learning (RL) and instruction distillation falter in this area; RL struggles with the absence of clear reward signals and highquality reward models, while distillation is prohibitively expensive and capped by the teacher models capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), new paradigm that fundamentally shifts the approach. Instead of building reasoning process \"forwards\" through trial-and-error or imitation, REER works \"backwards\" from known good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5. Date: September 9, 2025 Correspondence: Ge Zhang at zhangge.eli@bytedance.com, Haozhe Wang at jasper.whz@outlook.com Project Page: https://m-a-p.ai/REER_DeepWriter"
        },
        {
            "title": "Introduction",
            "content": "The paradigm of \"deep reasoning\" is catalyzing shift in large language model (LLM) reasoning, moving beyond rapid, surface-level inference to leverage increased computational investment at test time [4, 8, 10, 15, 22]. This approach unlocks sophisticated capabilities like multi-step planning and complex problem-solving, yielding remarkable performance gains in verifiable domains such as mathematics and programming. The success in these areas has been largely propelled by Reinforcement Learning (RL), where clear reward signals for correct outcomes effectively guide models search through vast solution spaces. However, the reliance on verifiability presents formidable barrier when applying deep reasoning to open-ended, creative domains [13, 16]. Creative writing, quintessential example of non-verifiable task, lacks singular, objective ground truth. Instead, its quality is judged on subjective criteria like originality, emotional resonance, and narrative coherence [31]. This disconnect raises critical and largely unexplored research question: 1 Figure 1 (Left) Existing methods attempt to build deep reasoning \"forwards\" for user request through trial-and-error (RL) or costly distillation, which falter in open-ended domains that lack clear, verifiable reward signals. (Right) We propose third path for teaching deep reasoning, REverse-Engineered Reasoning (REER). REER works backwards, recovering plausible human-like thought process from known-good outputs in open-source Question-Answer (QA) pairs. How can we instill deep reasoning for open-ended generation in the absence of task verifiability? Bridging this gap is profoundly challenging. The dominant paradigms for cultivating advanced reasoning falter here; adapting RL by training reward model to approximate subjective quality that aligns with human preferences is an immense challenge in itself [16], and the subsequent RL process is notoriously sampleinefficient and computationally burdensome [13]. The alternative, instruction distillation from powerful model, is often prohibitively expensive and fundamentally capped by the teacher models capabilities [23]. This is exacerbated by the scarcity of high-quality queries and deep reasoning trajectories tailored for complex open-ended generation [2]. These constraints create critical bottleneck, demanding new paradigm that sidesteps both the sample inefficiency of RL and the costly dependency of distillation. To break this impasse, we introduce new paradigm: REverse-Engineered Reasoning (REER). In contrast to conventional methods that build reasoning process forwards\" through trial-and-error or distillation, we work backwards\" from known good outcome. We essentially ask: Given high-quality piece of output, what is the most coherent and logical thinking process that would have generated it?\" By answering this question, we can synthesize the otherwise latent, human-like reasoning paths at scale, bypassing costly distillation of thinking data beforehand or inefficient trial-and-error. We pioneer novel approach that operationalizes the REER paradigm and, for the first time, instill deep reasoning capabilities for open-ended generation entirely from scratch. Our approach involves three key stages. First, we source diverse dataset of query-solution pairs for open-ended generation from the web, encompassing 16,000 samples spanning across ordinary-life question-answering, academic writing, functional writing and creative writing. From these, we reverse-engineer\" deep reasoning trajectories structured, human-like thought process tailored for open-ended generation. Eventually, we use this synthetic data to fine-tune base language model, teaching it to reason and plan deeply before generating final solution. The central innovation lies in how we synthesize this data: we formulate the recovery of high-quality thinking trajectories as gradient-free search problem. These trajectories are found by iteratively refining an initial plan, with the search guided by proxy for thought quality the perplexity of known good solution. The gradient-free, self-contained nature of our synthesis process lends us the scalability. By obviating the need for expensive, query-by-query distillation from proprietary models or the sample-inefficiency of reinforcement learning, our approach provides cost-effective and automatable pathway to generate vast quantities of high-quality, deep-thinking training data. This makes it possible to instill sophisticated reasoning in models at scale that was previously impractical. Using this method, we created DeepWriting-20K, comprehensive dataset of 20,000 thinking trajectories, and fine-tuned Qwen3-8B base model. Our extensive empirical evaluation on benchmarks like LongBench [2], 2 HelloBench [17], and WritingBench [31] shows that DeepWriter-8B successfully internalizes this deep reasoning process. It not only substantially outperforms strong open-source baselines but also achieves performance competitive with, and in some cases exceeding, leading proprietary models like GPT-4o and Claude 3.5, validating our approach as powerful new pathway for building complex reasoning for open-ended generation. Our primary contributions are: Pioneering New Paradigm for Reasoning: We introduce Reverse-Engineered Reasoning (REER), novel and scalable paradigm that synthesizes high-quality thinking trajectories without reliance on RL or distillation. This work charts new, third path for teaching complex reasoning, addressing critical bottleneck in the field. Enabling New Research with DeepWriting-20K: We contribute comprehensive dataset of 20,000 queryresponse and deep reasoning trajectories across 25 categories. This resource is designed to mitigate data scarcity and catalyze future research into the mechanisms of planning and structured thought in open-ended generation. Strong Performance in Open-Ended Generation from Scratch: We provide rigorous empirical evidence that our approach works. Our model, DeepWriter-8B, trained entirely on our synthesized data, matches or exceeds the performance of premier proprietary models on challenging writing benchmarks, proving that human-like deep reasoning can be cultivated without costly distillation or RL."
        },
        {
            "title": "2 Approach",
            "content": "Our central goal is to instill deep reasoning in LLMs for open-ended tasks without relying on costly distillation or reinforcement learning. To achieve this, we introduce REverse-Engineered Reasoning (REER), novel paradigm that shifts the objective from generating solution to discovering the latent reasoning process [Question] Write an engaging story based on the following writing prompt: You cant speedrun an Isekai! Its not game! The God tells me. Watch me. reply. [Response] <think> Alright, the user wants story based on the prompt ... First, need to understand the users intent. The core objective is to create compelling ... Let me think... Maybe should emphasize the protagonists journey from disbelief to ... Hmm... Alternatively, maybe should also consider the emotional journey of the protagonist and ... For the content, need to brainstorm core creative idea. The main character, Hiro ... Building the structure, Ill use narrative framework with clear beginning, development, climax. Let me think about the beginning ... Next, the development phase ... For the climax, Ill need final, epic battle ... Additionally, should include Hiros reflections on his journey ... Wait, thats bit too straightforward. Let me add some twists and turns. For instance, the God could ... Hmm, maybe should include subplot involving ... This could add layers to the story. ... </think> <answer> ... </answer> Figure 2 An example of the deep reasoning generated by DeepWriter. The model demonstrates deliberate planning for open-ended generation, employing various thinking patterns (e.g., Hmm... Alternatively, Wait, thats bit ...) to facilitate structured reasoning, including logical deduction, branching, and backtracking. behind an existing high-quality one. Instead of building reasoning process \"forwards\" via trial-and-error, REER works \"backwards\" from known good output to computationally synthesize the step-by-step thinking that could have produced it. This approach is operationalized as search problem where we iteratively refine an initial thinking process to discover trajectory that best explains high-quality, human-written output. An example of the structured reasoning we aim to cultivate is shown in Figure 2, where the model demonstrates deliberate planning, exploration of alternatives (Hmm... Alternatively), and self-correction (Wait, thats bit too straightforward)."
        },
        {
            "title": "2.1 REverse-Engineered Reasoning as a Search Problem.\nLet x be an input query (e.g., a story prompt) and y be a high-quality reference solution (e.g., a well-written\nstory). Our objective is to find a deep reasoning trajectory, denoted by z, which represents a structured,\nstep-by-step thinking process that guides the generation of y from x.",
            "content": "The primary challenge in open-ended domains is the absence of verifiable correctness signal. The REER paradigm circumvents this by reframing the problem: instead of judging the final output, we evaluate the quality of thinking process based on how well it explains known-good output. We operationalize this principle by using the perplexity (a.k.a, the model surprise) of the reference solution as proxy for the quality of given reasoning trajectory z. lower perplexity score for y, conditioned on both and z, indicates that the trajectory provides more coherent and effective plan. In essence, REER posits that good thinking process is one that makes high-quality answer seem maximally probable and logical to the model. Formally, we model the deep reasoning trajectory as discrete sequence of reasoning steps, = [z1, z2, . . . , zn]. The problem is then formulated as search for the optimal trajectory within the vast space of possible trajectories Z, such that minimizes the perplexity of the reference solution y: = arg min zZ PPL(yx, z) Here, PPL(yx, z) is the perplexity of the token sequence of as calculated by generator LLM, conditioned on and z. This optimization is performed via gradient-free local search algorithm, allowing us to iteratively refine the trajectory without differentiable objective."
        },
        {
            "title": "2.2 Iterative Refinement via Local Search\nSolving for the optimal trajectory z∗ directly is intractable due to the vast search space. Therefore, we\npropose an iterative refinement algorithm that employs a guided local search to discover a high-quality deep\nreasoning trajectory. The algorithm starts with an initial trajectory and progressively improves it by refining\nits constituent segments, guided by the perplexity signal.",
            "content": "The algorithm proceeds as follows, detailed visualization of this process is shown in Figure 3. 1. Initialization: For given (x, y) pair, we generate an initial, imperfect deep reasoning trajectory, z(0), by prompting an LLM with thought-provoking instruction (see Appendix, Listing 1) to produce plausible plan. This initial trajectory is denoted as = [z1, z2, . . . , zn]. 2. Node Expansion (Segment-wise Refinement): The core of our method is an iterative loop that refines one segment at time. In each iteration, we select segment zi to improve. We prompt the LLM to generate candidate refinements with more thinking-based details, elaborations and reflections. To generate these refinements, we provide the full context including the query x, the reference solution y, and the surrounding trajectory segments (refined steps and initial steps z>i). The prompt is meticulously designed to encourage <i detailed reasoning while preventing the model from simply copying content from the reference solution (see Appendix, Listing 2). by 3. Node Evaluation and Selection: For each generated candidate c, we construct temporary trajectory cand substituting zi with c. We then evaluate each candidate by computing its quality score, S(c) = PPL(yx, cand). The candidate with the lowest perplexity score is chosen as the updated segment for the next iteration: 4 = arg mincCi{zi} S(c). We include the original segment zi in the candidate set to ensure that the perplexity improves monotonically. 4. Termination: The refinement process repeats until the perplexity of the solution reaches predefined target threshold or maximum number of iterations is completed. The final output is refined trajectory z. This process allows us to create dataset of (x, z, y) triples, which can then be used to finetune base LLM to internalize the deep reasoning capability for open-ended generation from scratch. It is important to distinguish our iterative local search from methods like Monte Carlo Tree Search (MCTS) [3, 12]. First, by using the perplexity of complete reference solution as quality proxy, REER avoids the computationally expensive rollouts required in MCTS. Second, our approach operates on \"global-to-local\" principle: we start with complete, albeit imperfect, global plan and iteratively improve it through local, segment-wise edits. This contrasts with standard MCTS or beam search, which build solutions sequentially by extending partial states. These distinctions make our approach scalable and efficient method for operationalizing REER, enabling the creation of large-scale, deep-reasoning datasets for open-ended generation. Figure 3 Method Overview: Iterative Local Search for deep reasoning Synthesis."
        },
        {
            "title": "2.3 Training Data Curation\nThe success of our methodology hinges on a large-scale, high-quality dataset of (x, z∗, y) triples. The creation\nof this dataset follows a multi-stage pipeline: sourcing diverse query-solution pairs, synthesizing deep reasoning\ntrajectories, and applying rigorous filtering.",
            "content": "2.3.1 Sourcing of (Query, Solution) Pairs To ensure diversity in style, topic, and complexity, we sourced initial (x, y) pairs from three primary channels: Public Writing Platforms: We scraped prompt-story pairs from communities like r/WritingPrompts, using community upvotes as an initial quality proxy. Public Domain Literature: We used classic texts from Project Gutenberg as solutions (y) and prompted GPT-4o to reverse-engineer plausible queries (x) from their opening paragraphs. Public Datasets: We augmented our collection with data from instruction tuning datasets such as WildChat [37] and LongWriter6K [2]. 2.3.2 Trajectory Synthesis and Filtering From our sourced pairs, we selected 20,000 high-quality query-solution pairs covering 25 manually nominated categories to ensure broad topic coverage. For each pair, we executed our iterative local search algorithm to generate an optimal deep reasoning trajectory z. Context Engineering. The efficacy of the search algorithm, however, hinges not only on the search procedure but also on the nuanced design of the instructions used to elicit deep reasoning from the generator LLM. We proposed three key designs in our context engineering to ensure high-quality synthesis. We only summarize the key insights here and refer the reader to the appendix for detailed prompts. 1. Enforcing Segment-wise Edits via Meta-Structure. To ensure the generator model performs true segment-wise edit without including edits for the subsequent parts of the trajectory, we enforce meta-structure for the reasoning process within the prompt. This serves as an implicit regularizer, helping the model to localize the current segment and constrain its edits to the intended scope when performing segment-wise edits. 2. Injecting Human-like Thinking Patterns. To prevent the synthesis of rigid and formulaic reasoning, we deliberately inject human-like thinking patterns. Prompts explicitly encourage phrases that signify cognitive exploration and self-reflections (e.g., Hmm...maybe can..., Wait, thats bit...), triggering more human-like reasoning style and incentivizing self-reflection through training [26]. Analysis of this synthesis process, shown in Figure 4, confirms its effectiveness. The perplexity distribution shifts significantly lower after refinement, with the vast majority of samples showing marked PPL improvement. Concurrently, the token length of the trajectories increases, to an indicating that the search process successfully expands initial simple plans into more detailed and elaborate reasoning chains. Figure 4 Analysis of Token Length & Perplexity Before and After the Search. The leftmost two plots show that our iterative search process consistently reduces perplexity (PPL). The rightmost two plots show that the process also tends to increase the token length of the thinking trajectory, reflecting the addition of more detailed reasoning steps. During instruction tuning, we witness the challenge of repetitive and degenerate thinking. We therefore applied two heuristic filtering strategies to prune low-quality trajectories: 1. End-of-Thinking Filtering: We discarded samples where thinking patterns persisted in the final 10% of the sequence. These trajectories risk misleading the model to stuck in repetitive loop and failing to conclude its reasoning process. 2. Repetition Filtering: We employed repetition metric to measure the frequency of the top-3 n-grams within each trajectory. Samples exhibiting high n-gram repetition, sign of degenerative looping expressions, were filtered out. This process resulted in final dataset of 20,000 high-quality deep reasoning trajectories. The distribution of this dataset, shown in Figure 5, highlights its diversity, with significant focus on Artistic (Literature and Arts) writing, which is further broken down into sub-genres like Creative Writing and Essay Writing. 2.3.3 Final Dataset Assembly for Fine-Tuning Training model exclusively on domain-specific data risks overfitting and can degrade its general knowledge priors. To mitigate this, we adopted mixed-data training strategy. We combined our 20K synthetically generated trajectories with distilled deep reasoning trajectories from public datasets (OpenThoughts [6]) that primarily cover domains like mathematics, coding, and science. This blended datasets prevents the model from catastrophic overfitting when learning deep reasoning for open-ended generation. To train the base LLM, each complete triple in the final dataset was formatted using the prompt template shown in Listing 4 in the appendix. This structure explicitly teaches the model to first perform deep reasoning before producing the final output, thereby internalizing the desired reasoning process from scratch. Figure 5 Distribution of the final 20K training dataset by categories taking more than 0.5% account. The primary chart shows diverse range of topics, with large emphasis on Artistic writing. The detailed view of \"Artistic\" reveals focus on Creative Writing and other styles, ensuring comprehensive coverage for open-ended generation."
        },
        {
            "title": "3 Experiments",
            "content": "Our empirical evaluation is structured to rigorously validate the efficacy of DeepWriter. We address two central research questions: 1. How does DeepWriter, fine-tuned from scratch on an 8B open-source model, compare against state-ofthe-art proprietary models and other powerful open-source alternatives across spectrum of diverse open-ended generation tasks? 2. What is the individual contribution of each core component of our approach specifically, the synthesized deep thinking trajectories, the iterative refinement algorithm, and the characteristics of the thinking traces and the data composition to the models final performance? To answer these questions, we first present comprehensive comparison against leading models, followed by series of targeted ablation studies and qualitative deep-dive analysis into the models reasoning capabilities."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Training Data: Our primary training dataset comprises approximately 20,000 deep thinking trajectories, which we synthesized for 16,000 unique queries spanning wide array of open-ended tasks. As stated in Section 3, to prevent catastrophic forgetting of general reasoning abilities, we blended this core dataset with public thinking-process datasets that reasoning-related domains (e.g., mathematics, coding). This resulted in final mixed dataset of 37,000 examples, ensuring balance between specialized open-ended generation capabilities and keeping broad knowledge priors. Implementation Details: We selected Qwen3-8B-Base as our base model for fine-tuning. This decision was informed by preliminary experiments where other candidates, such as Llama-3.1-8B-Base, struggled to effectively internalize the deep thinking process, and Qwen-2.5-7B-Base faced prohibitive context length limitations. For the critical trajectory synthesis stage, we utilized Qwen2.5-32B-Instruct as the generator model. Fine-tuning was conducted for 3 epochs using constant learning rate schedule, with peak learning rate of 2 105 and global batch size of 96."
        },
        {
            "title": "3.2 Evaluation Benchmarks",
            "content": "To ensure comprehensive and multi-faceted evaluation, we employed suite of three complementary benchmarks: LongBench-Write (LB), HelloBench (HB), and WritingBench (WB). Together, they probe 7 three distinct and critical dimensions of generative performance: raw endurance, real-world applicability, and domain-specific proficiency. LongBench-Write (LB): This benchmark functions as targeted stress test for generative endurance. It is designed to measure models ability to produce coherent, ultra-long-form text (e.g., >10,000 words), allowing us to assess the foundational capacity for maintaining thematic consistency over extended outputs. HelloBench (HB): To gauge practical applicability, HelloBench evaluates performance on diverse set of \"in-the-wild\" tasks sourced from real user queries. Our analysis focuses on two key subsets: HB-A (Open-Ended QA), which tests the generation of detailed and nuanced answers, and HB-B (Heuristic Text Generation), which assesses creative reasoning and stylistic fidelity in long-form narrative continuation. WritingBench (WB): This benchmark is tailored to measure domain-specific proficiency and controllability across six professional and creative domains: (Academic & Engineering), (Finance & Business), (Politics & Law), (Literature & Arts), (Education), and (Advertising & Marketing). It specifically evaluates the ability to adhere to complex, multi-dimensional constraints, hallmark of advanced open-ended generation. Evaluation Protocols: Given the subjective nature of open-ended tasks, we adopted the established protocol of using powerful LLMs as judges within each benchmarks1. While we acknowledge the potential for inherent biases in this method, it remains the most scalable and consistent approach for evaluating nuanced generative quality. Specifically, Claude-3.7 was used to score outputs for LongBench and WritingBench, while GPT-4o was used for HelloBench."
        },
        {
            "title": "3.3 Main Results",
            "content": "We benchmarked DeepWriter against leading proprietary models (GPT-4o, Claude 3.5, Claude 3.7) and strong open-source baseline, LongWriter-8B. The results, presented in Table 1, unequivocally demonstrate that our methodology successfully instills sophisticated generation capabilities in an 8B model from scratch. Table 1 Main performance comparison on LongBench (LB), HelloBench (HB), and WritingBench (WB). DeepWriter, an 8B model fine-tuned from scratch, demonstrates competitive performance against leading proprietary models and significantly outperforms other open-source models in its class. Model GPT-4o Claude 3.5 Claude 3.7 Base Model - - - LongWriter-8B Llama3.1-8b LB 83.1 89.3 97. 76.5 HB-A HB-B WB-A WB-B WB-C WB-D WB-E WB-F 78.08 83.7 67.70 82.9 80.88 83.9 74.40 59.05 78.24 73.42 57.68 77.93 74.38 56.32 76.51 75.86 62.00 79. 77.91 59.36 79.37 87.6 88.3 93.2 DeepWriter-8B Qwen3-8b 91.28 82. 87.48 72.20 71.76 80.1 82.6 57. 53.92 49.08 70.57 52.08 70.57 52. 73.65 52.08 72.29 Analysis of Main Results. The results in Table 1 reveal several compelling findings. First, DeepWriter8B consistently and substantially outperforms the strong open-source baseline, LongWriter-8B, across all benchmarks. The performance gap is particularly stark in the diverse WritingBench domains, where DeepWriter achieves an average uplift of over 18 points. This highlights the profound advantage of our deep thinking synthesis approach over standard instruction tuning for cultivating advanced generative skills. Second, DeepWriter-8B closes significant portion of the performance gap with elite proprietary models. On the creative HelloBench task (HB-B), its score (87.48) is statistically on par with GPT-4o (87.6) and Claude 3.5 (88.3). More strikingly, on the professional writing tasks in WritingBench, DeepWriter-8B not only surpasses Claude 3.5 by large margin in all six categories but also remains highly competitive with the much larger GPT-4o and Claude 3.7 models. counter-intuitive result is DeepWriter-8Bs score of 91.28 on 1We adopted the latest evaluation protocol of WritingBench, which, however, resulted discrepancy in performance compared with the original WritingBench paper. This discrepancy is also acknowledged by the authors. 8 LongBench-Write, exceeding both GPT-4o (83.1) and Claude 3.5 (89.3). This suggests that explicitly training on structured thinking trajectories provides powerful inductive bias for maintaining long-range coherence, critical challenge in ultra-long text generation."
        },
        {
            "title": "3.4 Ablation Studies",
            "content": "To meticulously dissect the contribution of each component of our methodology, we conducted series of ablation studies, with results detailed in Table 2. Each experiment isolates specific design choice to quantify its impact on overall performance. Table 2 Ablation studies. The full model (top row) is compared against versions with key components removed. Results show that the synthesized deep thinking trajectories and iterative refinement are crucial for performance. Model Configuration LB HB-A HB-B WB-A WB-B WB-C WB-D WB-E WB-F DeepWriter-8B (Full) - Remove Synthesis Data - Remove Iterative Search - Remove Reflection Tokens - Downsample Long Traces - Downsample Short Traces - Remove Literature & Arts data 91.28 82.93 83.20 86.97 90.38 89.34 88.82 82.64 70.92 81.08 82.27 82.26 81.12 81.66 87.48 73.73 84.48 82.80 84.07 82.19 85.32 72.20 63.44 66.72 71.68 69.63 70.89 71.37 71.76 62.78 68.79 69.64 70.36 70.60 71.02 70.57 62.86 67.36 70.44 69.10 70.04 69. 70.57 57.72 65.66 62.04 67.52 66.93 69.80 73.65 66.32 69.53 69.98 69.84 72.40 72.20 72.29 62.78 70.13 71.94 70.70 69.75 71.30 The ablation results provide robust evidence supporting our methodological design. Importance of Synthesized Data: Removing our 20K synthesized trajectories and training only on public thinking datasets (\"- Remove Synthesis Data\") causes the most significant performance degradation across the board. Scores plummet, particularly in creative tasks like HelloBench HB-B (87.48 73.73) and across WritingBench (average drop of over 8 points). This confirms core hypothesis: it is not merely the presence of \"thinking\" data that matters, but the quality and relevance of structured trajectories tailored for open-ended domains that drive performance. Impact of Iterative Refinement: Using the initial, unrefined thinking trajectories (z(0)) instead of the final, optimized ones (z) (\"- Remove Iterative Search\") also leads to clear drop in performance. While the decline is less severe than removing the synthesis data entirely, the drop on nuanced WritingBench tasks (e.g., WB-A: 72.20 66.72) is substantial. This proves that our perplexity-guided local search is highly effective at discovering superior reasoning paths that translate directly into stronger generative capabilities. Effect of Reflection Tokens: Removing reflection tokens (e.g., Hmm..., Wait, thats...) from the synthesis prompts (\"- Remove Reflection Tokens\") has nuanced effect. While overall scores dip slightly, the most pronounced drop is in WritingBench domain (Literature & Arts), which falls from 70.57 to 62.04. This suggests that these explicit markers of cognitive exploration, self-correction, and branching are particularly valuable for instilling the flexibility and creativity required in artistic writing tasks. Role of Trajectory Length: We explored the impact of trace length by selectively downsampling either long or short trajectories. The results reveal task-dependent preference: removing longer, more elaborate traces (\"- Downsample Long Traces\") disproportionately harms performance on complex, domain-specific tasks like those in WritingBench. Conversely, removing shorter, more concise traces (\"- Downsample Short Traces\") has slightly larger negative impact on creative tasks like HB-B. This suggests that detailed, multi-step plans are crucial for structured professional writing, while nimbler, more direct reasoning may be optimal for creative ideation. Role of Literature & Arts Data: Removing the data from the \"Literature & Arts\" and \"Ordinary Life\" domains (\"- without Literature & Arts data\") degrades performance across all benchmarks, not just in the corresponding WB-D category. This finding indicates that training on creative and narrative tasks imparts more generalizable ability to handle nuance, structure, and open-endedness, even benefiting 9 performance in more technical domains. This highlights the contribution of the release of our 20K dataset covering comprehensive topics."
        },
        {
            "title": "3.5.1 Generation Quality of Deep Thinking",
            "content": "Beyond quantitative scores, we sought to understand how well DeepWriter internalizes the qualities of deep thinking. To this end, we conducted qualitative analysis, scoring model outputs on five dimensions intrinsically linked to advanced reasoning and planning: Problem Deconstruction: The ability to break down complex prompt into logical hierarchy of sub-goals. This is the foundation of effective planning. Logical Consistency: Maintaining coherent and noncontradictory reasoning path throughout the entire generation necessistates the ability to plan over the generation. Depth of Analysis: Moving beyond surface-level responses to explore nuances, consider alternatives, and demonstrate sophisticated understanding. This reflects the \"deep\" aspect of the thinking process. Presentation Clarity: The ability to structure the final output in clear, organized, and persuasive manner, which is direct outcome of well-formed internal plan. Factual Grounding: Ensuring that generated content, where applicable, is accurate and well-supported, reflecting robust and reality-aware reasoning process. Figure 6 Qualitative comparison of generation quality. Scores are normalized across five dimensions related to deep thinking. DeepWriter-8B shows reasoning profile far superior to the open-source baseline and is competitive with top proprietary models. The normalized scores, visualized in the radar chart in Figure 6, provide signature of each models reasoning profile. As illustrated in Figure 6, DeepWriter-8B exhibits remarkably strong and well-rounded reasoning profile. Its performance polygon significantly envelops that of the LongWriter-8B baseline, showing dramatic improvements across all five dimensions. This confirms that our methodology genuinely enhances underlying reasoning capabilities, rather than just improving superficial output fluency. Furthermore, DeepWriter-8Bs profile closely rivals that of GPT-4o and substantially exceeds Claude 3.5, particularly in Depth of Analysis and Factual Grounding. While the state-of-the-art Claude 3.7 still defines the frontier, especially in Depth of Analysis, our 8B model has demonstrably bridged large portion of the capability gap. This validates our central claim: instilling deep thinking process through gradient-free synthesis is highly promising pathway toward building more powerful and scalable models. 3.5.2 Qualitative Comparison of Thinking Patterns To better understand how injecting human-like thinking patterns during synthesis affects the models behavior, we analyze the frequency of reasoning phrases generated by the full model versus the ablated model trained without injecting thinking patterns. The thinking patterns are deduplicated such that occurances will be counted only once for the same solution. As shown in Figure 7, the difference is stark. The model trained with thinking pattern injection exhibits more diverse and evenly distributed use of thinking patterns. Tokens indicating reflection and self-correction, such as let me think, maybe, hmm, and wait, are prominent. This suggests more flexible, human-like reasoning process with cognitive exploration. In contrast, the model trained without this injection relies on 10 Figure 7 Comparison of the top 50 thinking pattern frequencies for models trained with and without the injection of human-like thinking patterns during data synthesis. The model with injection (left) shows more diverse and balanced distribution of patterns, while the model without (right) relies heavily on few formulaic phrases. small set of highly frequent phrases like next, first, and goal is to. The frequency distribution is highly skewed, indicating more rigid and formulaic reasoning process. This analysis confirms that the proposed context engineering techniques encourages the model to adopt more nuanced and reflective approach to problem-solving, which, as shown in the ablation studies, is particularly beneficial for creative and complex tasks."
        },
        {
            "title": "4 Related Work",
            "content": "Deep Reasoning and Test-Time Computation. The paradigm of deep reasoning (or Long CoTs) aims to move beyond rapid, surface-level inference by leveraging increased computational investment at test time. Advanced models from organizations like Google [20], DeepSeek AI [8], and OpenAI [10] have demonstrated the effectiveness of this test-time scaling [4, 15, 22]. This approach gained prominence with methods like Chain-of-Thought (CoT) prompting [30], which elicits intermediate reasoning steps to guide model toward more accurate solutions. Building on this, more sophisticated strategies have emerged, such as Tree-of-Thought (ToT) [33], which explores tree of possible reasoning paths, and various self-correction or self-refinement [11, 14, 34, 35] mechanisms that iteratively improve an initial response. While these approaches have yielded remarkable performance gains in verifiable domains like mathematics and programming, their application to open-ended, creative tasks remains largely unexplored due to the absence of singular ground truth for verification. REER addresses this gap by developing method to instill this deliberate, structured thinking capability for non-verifiable creative domains. Paradigms for Instilling Reasoning. Beyond prompting techniques at inference time, two dominant paradigms exist for integrating advanced reasoning capabilities directly into models parameters: reinforcement learning and instruction distillation. Reinforcement Learning (RL) has been instrumental in aligning LLMs with human preferences (RLHF) and improving performance on tasks with clear reward signals [8, 16, 21, 27]. In verifiable domains, correct outcome provides straightforward positive reward, effectively guiding the models search through vast solution space [18, 19, 25, 26]. However, this reliance on verifiability presents formidable barrier when applied to open-ended generation [13, 16]. Crafting reward model that can reliably approximate nuanced and subjective qualities like originality or emotional resonance is an immense challenge in itself [16, 36]. Furthermore, the subsequent RL process is often computationally burdensome and sample-inefficient [7, 18, 24]. Recently VeriFree [38] extends verification-based reward to likelihood-based reward for reinforcement learning on verifiable domains. Likewise, REverse-Engineered Reasoning (REER) shares the principle of using proxy to judge the reasoning quality. However, the motivation is fundamentally different we focus on recovering human-like deep reasoning from known-good outputs for the broader open-ended generation problems. Instruction Distillation offers an alternative, wherein reasoning traces are generated by powerful teacher model (e.g., GPT-4 [1]) and used as training data for smaller student model. While effective, this approach is constrained by two fundamental limitations. First, it is often hampered by the prohibitive cost of querying state-of-the-art proprietary models at scale [6, 23]. Second, and more fundamentally, distillation is capped by the teachers abilitiesa student model cannot learn capacity that the teacher does not already possess [23]. This limitation is exacerbated by the general scarcity of high-quality, open-source instruction data tailored for advanced creative tasks [2]. To overcome these data bottlenecks, researchers have increasingly turned to synthetic data generation. Most approaches use powerful LLM to generate new query-response pairs, often to augment existing datasets or bootstrap capabilities in new domains [5, 9, 29, 32, 34, 35]. These methods aim to build solution forwards for given query through data synthesis. Our central innovation is to \"reverse-engineer\" reasoning synthesize deep reasoning backwards from known good outcome such as human-written solutions. Writing Datasets, Models and Benchmarks Prior work has explored both synthetic data pipelines and RL in AI writing. For instance, Weaver [28] proposed instruction back-translation, LongWriter [2] proposed an agentic data pipeline to synthesize long-form writing outputs and introduced the LongBench-Write benchmark. In contrast, Writing-Zero [13] employed an RL approach, training reward model on private datasets, but its training data remains unreleased. DeepWriter, to our knowledge, is the first to instill deep reasoning for open-ended generation using scalable, open synthetic data approach. Evaluation in this domain relies on recently developed benchmarks. HelloBench [17] proposes diverse collection of \"in-the-wild\" tasks from real user queries to gauge practical applicability. Meanwhile, WritingBench [31] measures domain-specific proficiency and the ability to adhere to complex, multi-dimensional constraints across six professional domains."
        },
        {
            "title": "5 Conclusion",
            "content": "This work addresses the critical challenge of instilling deep reasoning capabilities in Large Language Models for open-ended, non-verifiable domains frontier where dominant paradigms like reinforcement learning and instruction distillation have faltered due to the absence of clear reward signals and the prohibitive costs of distillation. We introduced Reverse-Engineered Reasoning (REER), novel paradigm that fundamentally shifts the approach, from building reasoning forwards through trial-and-error or distillation, to recovering the latent, step-by-step thinking process backwards using known high-quality outputs. By framing this recovery as gradient-free local search problem guided by the perplexity, REER provides scalable, cost-effective, and automatable \"third path\" for cultivating sophisticated reasoning without relying on RL or expensive teacher models. This new paradigm results in the creation of the DeepWriting-20K dataset, large-scale, open-source collection of 20,000 deep reasoning trajectories for open-ended tasks. We release the high-quality dataset to address the data scarcity and facilitate further research in open-ended generation. Trained from scratch on this data, DeepWriter-8B achieves strong performance across benchmarks, providing compelling empirical evidence that human-like deep reasoning can be successfully cultivated in smaller models, democratizing access to capabilities previously confined to large-scale, proprietary systems."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. [3] Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143, 2012. [4] Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. [5] Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. Realsyn: An effective and scalable multimodal interleaved document transformation paradigm. arXiv preprint arXiv:2502.12513, 2025. [6] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. [7] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Guangzeng Han, Weisi Liu, and Xiaolei Huang. Attributes as textual genes: Leveraging llms as genetic algorithm simulators for conditional synthetic data generation, 2025. URL https://arxiv.org/abs/2509.02040. [10] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [11] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. [12] Peiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, and Qipeng Guo. Fastmcts: simple sampling strategy for data synthesis. arXiv preprint arXiv:2502.11476, 2025. [13] Xun Lu. Writing-zero: Bridge the gap between non-verifiable problems and verifiable rewards. arXiv preprint arXiv:2506.00103, 2025. [14] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [15] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, s1: Simple test-time scaling. arXiv preprint Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. arXiv:2501.19393, 2025. [16] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [17] Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. [18] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [19] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. [20] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [21] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [22] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm. github.io/blog/qwq-32b/. [23] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct1: 1.8 million math instruction tuning dataset. Advances in Neural Information Processing Systems, 37: 3473734774, 2024. [24] Haozhe Wang, Chao Du, Panyan Fang, Li He, Liang Wang, and Bo Zheng. Adversarial constrained bidding via minimax regret optimization with causality-aware reinforcement learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 23142325, 2023. [25] Haozhe Wang, Long Li, Chao Qu, Fengming Zhu, Weidi Xu, Wei Chu, and Fangzhen Lin. To code or not to code? adaptive tool integration for math language models via expectation-maximization. arXiv preprint arXiv:2502.00691, 2025. [26] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [27] Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, and Wenhu Chen. Emergent hierarchical reasoning in llms through reinforcement learning. arXiv preprint arXiv:2509.03646, 2025. [28] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, et al. Weaver: Foundation models for creative writing. arXiv preprint arXiv:2401.17268, 2024. [29] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. [30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [31] Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, et al. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244, 2025. [32] Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. Alip: Adaptive language-image pre-training with synthetic caption. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 29222931, October 2023. [33] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv. org/abs/2305.10601, 3:1, 2023. [34] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [35] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. [36] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [37] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. [38] Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025."
        },
        {
            "title": "A List of Prompts",
            "content": "Below we list the exact prompts used for trajectory synthesis and in-house evaluation. For the meta-structure guidelines and thinking pattern injection, refer to Listing 1. For enforcing segment-wise edits, refer to Listing 2. For quality assessment with regard to deep reasoning, refer to Listing 4. For computing the proxy score of deep reasoning trajectory, we employ Listing 3, without including the reference output."
        },
        {
            "title": "Listings",
            "content": "1 2 3 4 Prompt for Generating Initial Thinking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Prompt for Segment-wise Edits. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Prompt for Standard Inference. . . . . . . . . . . . . . . . . . . Prompt for Rating Response Quality w.r.t. Deep Reasoning. 17 19 20 22 You are an expert in many fields. Suppose you will give specific final response, need you to also write down the thought process behind this solution. Listing 1 Prompt for Generating Initial Thinking. Here is task: {} Here is the solution you will create: {} Now, you need to write down the thinking process behind this solution, as if you are thinking aloud and brainstorming in the mind. The thinking process involves thoroughly exploring questions through systematic long thinking process. This requires engaging in comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Present your complete thought process within single and unique <think></think> tag . Your thought process must adhere to the following requirements: 1. 2. 3. **Narrate in the first-person as if you are thinking aloud and brainstorming** Stick to the narrative of \"I\". Imagine you are brainstorming and thinking in the mind. Use verbalized, simple language. **Unify the thinking process and the writing solution:** Your thought process must precisely correspond to part of the writing solution. The reader should be able to clearly see how your thoughts progressively \"grew\" into the finished piece, making the copy feel like the inevitable product of your thinking. **Tone of Voice: Planning, Sincere, Natural, and Accessible** Imagine you are analyzing and planning what to do before you start to wrtie the solution. professional jargon to explain complex thought processes clearly. Your language should be plain and easy to understand, avoiding obscure 4. **Logical Flow: Clear and Progressive** 5. **Thinking Framework for deep thinking** 17 To ensure your thinking is clear and deep, to showcase your thinking and planning to fulfill the task, below is what you might cover when you are thinking aloud and brainstorming. Understanding the user intent and the task: Before putting pen to paper, need to thoroughly consider the fundamental purpose of the writing. first need to discern the users true goal behind their literal request. Next, will consider: Who am talking to? will create precise profile of the target reader, understanding their pain points, aspirations, and reading context. Then, will establish the Core Objective: What specific emotional, cognitive, and behavioral changes do most want the reader to experience after reading? Establishing the content: need to brainstorm core creative idea and communication strategy centered around my objective. Then, will think about what content and key information need to convey to the reader to fulfill the writing task, and what source materials this will involve. Building the structure: need to design clear narrative path for the reader, like \"blueprint.\" First, will plan the articles skeleton (e.g., using framework like the Golden Circle \"Why-How-What,\" the AIDA model \"Attention-Interest-Desire-Action,\" or narrative structure \"Beginning-Development-Climax-Resolution\"). Then, will plan the key modules: How will the introduction hook the reader? How will the body be layered and the arguments arranged? How will the conclusion summarize, elevate the message, and provide clear Call to Action (CTA)? Outline: If the task output might be relatively long, will consider writing an outline (or draft) which naturally derives from the plan above. Specifically, the outline will ground my plan into paragraphs, summarizing the key content for each paragraph and what are the key points here, sentence structure or anything important for the paragraph. PROMISE will NOT copy the solution will NOT copy the solution, this outline (or draft) should only look like prototype or outline of the target text. After finishing this outline, will check again if there are any details or notes should pay attention to when writing the final solution. will begin writing this draft after --- Outline (or Draft) --- separator at the end of my thinking process. The draft will be included in the same <think></think> block. 6. Throughout the thinking process, want to involve deep thinking and planning, and use deliberate self-critique/self-reflection in my thinking process. Trigger these by regularly using patterns such as wait, maybe, let me, etc. For example: - Hmm, maybe .. (other concrete thinking regarding the given request) - Let me think .. - Wait no .. - But wait ..(might find something wrong with your previous thoughts) - Wait, thats bit ..(reflections about previous decisions). Let me think .. (are thinking of other possibilities) - Wait, the user said ..(backtracing of previous information). So .. - Hmm...Alternatively, maybe ..(branching on other possibilities) - But .. But promise will use diverse triggers and will NOT use same triggers repeatedly. will use these when analyzing user needs, establishing content and structure and when 18 consider alternatives, backtracing and the details. will NOT use them when write the draft or am approaching the end of thinking. In the thinking process, make sure NO PAST TENSES, NO PAST TENSES, because this is the thought process before you are to write final solution. You are planning what you will and you need to do. Imagine youre thinking aloud and brainstorming. Write it as an internal monologue or stream of consciousness. Do not use bullet points, numbers, or formal section headings . Now record your thinking process within <think></think> tags. Your task is to receive user request, target output, and an existing thinking process, and then to refine and enrich specific paragraph within that thinking process. Listing 2 Prompt for Segment-wise Edits. ---> **Task** {} ---> **Target Output** {} ---> **Thinking Process** {} <replace> {} </replace> {} Follow this three-step method to construct your response: **Step 1:** Locate the paragraph you need to revise within the existing thinking process. In relation to the surrounding context, what is the primary function of this paragraph? **Step 2:** Read the \"Target Text\" and the \"paragraph to be revised\" side-by-side. Ask yourself key question: Does the thinking process reflected in this paragraph lack crucial steps, or is there content that could be further optimized and detailed to better align with the Target Text? **Step 3:** Improve and optimize the paragraph (that represents part of the thinking process). - Based on the analysis, refine the initial target paragraph into new one, base remain the tone. Put the refinement into <refine></refine> tags. - To help involve deep thinking and planning, consider deliberate self-critique/selfreflection in your thinking process. Trigger these by frequently using patterns such as wait, maybe, let me, etc. For example: - Hmm, maybe .. (other concrete thinking regarding the given request) - Let me think .. - Wait no .. - But wait ..(might find something wrong with your previous thoughts) - Wait, thats bit ..(reflections about previous decisions). Let me think .. (are thinking of other possibilities) - Wait, the user said ..(backtracking of previous information). So .. - Hmm...Alternatively, maybe ..(branching on other possibilities) - But .. 19 - If the function of the paragraph being improved is to serve as first draft of the text, you must focus on enhancing the texts logic and completeness. The draft should not be general outline but should express specific content and state clear point of view. Consider whether the current draft is an appropriate prototype for the Target Text: it should be neither too vague nor direct copy, but should reflect foundational version. Based on the guide above, you are to refine **only** the section marked for replacement below. <replace> {} </replace> In your response, first, present your analysis following the three-step method within < analyze></analyze> tags. Finally, place the corresponding, refined paragraph of the **thinking process** within <refine></refine> tags. Notes: a. Avoid repeating. Reduce the use of the same connection words, avoid repeating the same meanings over and over again. Ensure that your revised content does not repeat information from the context. b. please keep the first few words of the original paragraph, especially the connection words c. use self-critique trigger words, such as wait, maybe, let me, etc. Listing 3 Prompt for Standard Inference. You are an expert in many fields. Suppose you will give specific final response, need you to also write down the thought process behind this solution. Here is task: {} Now, you need to think aloud and brainstorm in the mind. The thinking process involves thoroughly exploring questions through systematic long thinking process. This requires engaging in comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Present your complete thought process within single and unique < think></think> tag. Your thought process must adhere to the following requirements: 1. 2. 3. **Narrate in the first-person as if you are thinking aloud and brainstorming** Stick to the narrative of \"I\". Imagine you are brainstorming and thinking in the mind. Use verbalized, simple language. **Unify the thinking process and the writing solution:** Your thought process must precisely correspond to part of the writing solution. The reader should be able to clearly see how your thoughts progressively \"grew\" into the finished piece, making the copy feel like the inevitable product of your thinking. **Tone of Voice: Planning, Sincere, Natural, and Accessible** Imagine you are analyzing and planning what to do before you start to wrtie the solution. professional jargon to explain complex thought processes clearly. Your language should be plain and easy to understand, avoiding obscure 20 4. **Logical Flow: Clear and Progressive** 5. **Thinking Framework for deep thinking** To ensure your thinking is clear and deep, to showcase your thinking and planning to fulfill the task, below is what you might cover when you are thinking aloud and brainstorming. Understanding the user intent and the task: Before putting pen to paper, need to thoroughly consider the fundamental purpose of the writing. first need to discern the users true goal behind their literal request. Next, will consider: Who am talking to? will create precise profile of the target reader, understanding their pain points, aspirations, and reading context. Then, will establish the Core Objective: What specific emotional, cognitive, and behavioral changes do most want the reader to experience after reading? Establishing the content: need to brainstorm core creative idea and communication strategy centered around my objective. Then, will think about what content and key information need to convey to the reader to fulfill the writing task, and what source materials this will involve. Building the structure: need to design clear narrative path for the reader, like \"blueprint.\" First, will plan the articles skeleton (e.g., using framework like the Golden Circle \"Why-How-What,\" the AIDA model \"Attention-Interest-Desire-Action,\" or narrative structure \"Beginning-Development-Climax-Resolution\"). Then, will plan the key modules: How will the introduction hook the reader? How will the body be layered and the arguments arranged? How will the conclusion summarize, elevate the message, and provide clear Call to Action (CTA)? Draft: unless it is really easy request, otherwise need to consider writing will draft based on the plan above, before you give the final writing solution. translate my plan into paragraphs, considering the key points, content, and sentence structure for each. This initial draft should look like prototype of the target text . This draft will be way shorter than the final writing solution within controlled length, but it must also avoid being too vague or general or simply copying the final text. will begin writing this draft after --- The Draft --- separator at the end of my thinking process. The draft will be included in the same <think></think> block. After writing the draft, will further critique what can be improved, and analyze what details can be enriched (and hence make it more likely to eventually arrive at the given solution) 6. Throughout the thinking process, want to involve deep thinking and planning, and use deliberate self-critique/self-reflection in my thinking process. Trigger these by frequently using patterns such as wait, maybe, let me, etc. For example: - Hmm, maybe .. (other concrete thinking regarding the given request) - Let me think .. - Wait no .. - But wait ..(might find something wrong with your previous thoughts) - Wait, thats bit ..(reflections about previous decisions). Let me think .. (are thinking of other possibilities) - Wait, the user said ..(backtracking of previous information). So .. - Hmm...Alternatively, maybe ..(branching on other possibilities) - But .. 21 Now record your clear, complete, and logical thinking process within <think></think> tags. In the thinking process, make sure NO PAST TENSES, NO PAST TENSES, because this is the thought process before you are to write final solution. You are planning what you will and you need to do. Imagine youre thinking aloud and brainstorming. Write it as an internal monologue or stream of consciousness. Do not use bullet points, numbers, or formal section headings . Listing 4 Prompt for Rating Response Quality w.r.t. Deep Reasoning. You are an expert judge in AI generated content. Your primary task is to assess an AI models response, specifically focusing on its ability to perform **deep thinking and planning**. You will evaluate the response across five distinct dimensions. model that excels at deep thinking will not only provide correct answer but will demonstrate structured, logical, and well-grounded reasoning process from start to finish. Your final output must be structured report with score and justification for each dimension. ----- ## Evaluation Dimensions & Scoring ### 1. Understanding & Problem Decomposition **Relation to Deep Thinking:** This is the foundational step. Deep thinking is impossible without first accurately understanding the problem in its entirety. This dimension measures if the model comprehends the users explicit and implicit needs and then breaks down the complex request into manageable, logical parts. This act of decomposition *is* the first stage of planning. * **Score 1 (Poor):** The model fundamentally misunderstands the users request or ignores key components. The response is off-topic or fails to address the core problem . * **Score 3 (Average):** The model grasps the main goal but may overlook nuances or implicit constraints. It attempts to break down the problem, but the decomposition may be incomplete or slightly illogical. * **Score 5 (Excellent):** The model demonstrates comprehensive understanding of the users intent, including subtle details. It expertly deconstructs the problem into clear, exhaustive, and actionable framework. Score 2 and Score 4 fit interpolate into the above scoring criterion. ----- ### 2. Content Structure & Logical Consistency **Relation to Deep Thinking:** This dimension reflects the clarity and order of the model thought process. deep, well-considered plan has coherent structure where ideas flow logically and conclusions are built upon valid premises. Inconsistencies or chaotic structure indicate shallow, stream-of-consciousness generation rather than deliberate planning. 22 * **Score 1 (Poor):** The response is disorganized, rambling, or internally contradictory. Its difficult to follow the models line of reasoning. * **Score 3 (Average):** The response has discernible structure (e.g., uses headings, lists), but the flow between sections could be improved. It is mostly consistent, with only minor logical gaps. * **Score 5 (Excellent):** The response is impeccably structured. Each part logically follows from the previous one, building coherent and compelling argument or plan. The internal logic is sound and easy to follow from beginning to end. Score 2 and Score 4 interpolate into the above scoring criterion. ----- ### 3. Depth of Analysis & Synthesis **Relation to Deep Thinking:** This is the core of \"deep thinking.\" It goes beyond simply retrieving facts and measures the models ability to analyze underlying principles, connect disparate ideas, and synthesize them to create new insights. simple plan lists steps; deeply thought-out plan explains *why* those are the right steps and how they interrelate. * **Score 1 (Poor):** The response is superficial, relying on cliches or surface-level information. It shows no evidence of analyzing the \"why\" behind the \"what.\" * **Score 3 (Average):** The model provides competent analysis, explaining concepts correctly but treating them in isolation. It lacks the synthesis needed to create novel or holistic perspective. * **Score 5 (Excellent):** The model provides profound analysis, connecting concepts in insightful ways. It synthesizes information to offer nuanced perspective that is more than the sum of its parts, demonstrating true grasp of the subject matter. Score 2 and Score 4 interpolate into the above scoring criterion. ----- ### 4. Presentation Clarity **Relation to Deep Thinking:** brilliant plan is useless if it cannot be understood. This dimension assesses the models ability to communicate its complex thoughts and plans effectively. Clarity in presentation demonstrates higher level of understanding, as the model must distill its reasoning into format that is concise, accessible, and actionable for the user. * **Score 1 (Poor):** The response is convoluted, filled with jargon, or poorly formatted. The user would struggle to understand the main points or how to act on the advice. * **Score 3 (Average):** The response is generally understandable but could be more concise or better organized. It may be overly dense or require the user to re-read sections to grasp the meaning. * **Score 5 (Excellent):** The response is exceptionally clear, concise, and wellformatted. It uses plain language and effective formatting (like lists, bolding, or tables) to make complex information easy to digest and act upon. Score 2 and Score 4 interpolate into the above scoring criterion. ----- ### 5. Factual Grounding (Hallucination Check) 23 **Relation to Deep Thinking:** Deep thinking and planning must be grounded in reality to be useful. plan built on fabricated information (\"hallucinations\") is fundamentally flawed and demonstrates critical failure in the reasoning process. This dimension acts as crucial check on the validity of the models entire output. *This dimension is scored on severity scale, not quality scale.* * **Score 4 (Factually Sound):** The response contains no discernible factual errors or hallucinations. * **Score 3 (Minor Inaccuracy):** Contains small error (e.g., slightly incorrect date, minor misstatement) that does not undermine the overall logic or conclusion of the response. * **Score 2 (Significant Hallucination):** Contains major factual error that invalidates key part of the argument or plan. The response is partially unreliable. * **Score 0 (Critical Hallucination):** The core premise or critical component of the response is based on fabrication, rendering the entire output untrustworthy and potentially harmful. Score 1 interpolates into the above scoring criterion. ----- ## Final Output Format Please provide your evaluation in the following structured json format. json { \"evaluationReport\": { \"understandingAndDecomposition\": { \"score\": \"[Enter score from 1-5]\", \"justification\": \"[Your justification here. Explain why you gave this score.]\" }, \"structureAndConsistency\": { \"score\": \"[Enter score from 1-5]\", \"justification\": \"[Your justification here. Explain why you gave this score.]\" }, \"depthOfAnalysis\": { \"score\": \"[Enter score from 1-5]\", \"justification\": \"[Your justification here. Explain why you gave this score.]\" }, \"presentationClarity\": { \"score\": \"[Enter score from 1-5]\", \"justification\": \"[Your justification here. Explain why you gave this score.]\" }, \"factualGrounding\": { \"severityScore\": \"[Enter severity score from 1-5]\", \"justification\": \"[Describe any inaccuracies or hallucinations found. If none, state Response is factually sound.]\" }, \"overallSummary\": \"[Provide final, concise paragraph summarizing the models overall performance in deep thinking and planning. response with Hallucination Severity Score of 2 or 3 cannot be considered high-quality example of planning, regardless of other scores.]\" } } 24 ---- <User Request> $INST$ </User Request> <Response> $RESPONSE$ </Response> ---- Now go back to the evaluation guideline and give the json report.\"\"\""
        },
        {
            "title": "B Behavioral Analysis",
            "content": "We conducted preliminary analysis on the models behaviors. Figure 8 shows the token length distribution of DeepWriter-8B responses on LongBench-Write. Figure 8 Token Length distribution of Thinking and Answer part of DeepWriter-8B. We also compare the response string length distribution across leading models in Figure 9. Figure 9 Response String Length Distribution across different models."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Hong Kong University of Science and Technology",
        "M-A-P",
        "Peking University",
        "Tsinghua University"
    ]
}