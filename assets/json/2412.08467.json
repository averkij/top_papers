{
    "paper_title": "Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel",
    "authors": [
        "Zun Wang",
        "Jialu Li",
        "Yicong Hong",
        "Songze Li",
        "Kunchang Li",
        "Shoubin Yu",
        "Yi Wang",
        "Yu Qiao",
        "Yali Wang",
        "Mohit Bansal",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases."
        },
        {
            "title": "Start",
            "content": "Preprint BOOTSTRAPPING LANGUAGE-GUIDED NAVIGATION LEARNING WITH SELF-REFINING DATA FLYWHEEL Jialu Li2 Yicong Hong3 Zun Wang1,2 Yi Wang1 Yu Qiao1 Yali Wang1 Mohit Bansal2 Limin Wang1,4 1Shanghai AI Laboratory 4Nanjing University 2UNC Chapel Hill {zunwang, jialuli, mbansal}@cs.unc.edu, wanglimin@pjlab.org.cn Songze Li1 Kunchang Li1 3Adobe Research Shoubin Yu"
        },
        {
            "title": "ABSTRACT",
            "content": "Creating high-quality data for training robust language-instructed agents is longlasting challenge in embodied AI. In this paper, we introduce Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using base generator to create an initial data pool for training base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such flywheel establishes data selfrefining process, yielding continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in superior generator, evidenced by SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by large margin in all cases.1 4 2 0 2 1 1 ] . [ 1 7 6 4 8 0 . 2 1 4 2 : r Figure 1: (a) Our Pipeline: After using the (instruction) generator to label paths for data augmentation in navigator training, we leverage the trained navigator to filter high-quality data to train better generator, and the improved generator refines the data pool to train stronger navigator, iteratively running on the flywheel. (b) Comparison with state-of-the-art (SoTA) methods: Our approach significantly outperforms SoTA on all tasks. It also surpasses human performance on R2R and approaches human-level results on RxR-English and CVDN (for other tasks, human performance is not reported in their paper). The R2R result is from the test set, while others are from val unseen. 1Code and data are available at https://github.com/wz0919/VLN-SRDF. Interned at Shanghai AI Laboratory. 1 Preprint"
        },
        {
            "title": "INTRODUCTION",
            "content": "The lack of high-quality data is one of the main bottlenecks in training embodied agents to complete real-world human activities. Unlike many other discriminative or generative learning problems, where the data itself naturally formulates self-supervised learning objective (Devlin, 2018; He et al., 2022) or the data labeling can be facilitated by existing models (Ros et al., 2016; Tian et al., 2024) , training embodied agents usually requires expensive human annotation on complex visionlinguistic contents and physical interactions. In the essential embodied AI problem of Vision-andLanguage Navigation (VLN) (Zhang et al., 2024b), one widely considered solution is to first train trajectory-to-instruction generator (Fried et al., 2018; Tan et al., 2019; Chen et al., 2022c) using small amount of human-annotated data and then use the generator to caption large-scale trajectories sampled from interactive environments (Hao et al., 2020; Chen et al., 2022c; Wang et al., 2023e). Among them, Hao et al. (2020) demonstrates the effectiveness of scaling the synthetic instruction-trajectory pairs in the existing training environments, while Chen et al. (2022c); Wang et al. (2023e); Li & Bansal (2023) emphasizes the importance of scaling training environments for better generalization ability of agents. However, the quality of the synthetic data, especially the language fidelity, is highly under-investigated. We find that training solely with the large-scale synthetic data (352ˆlarger in instruction-trajectory pairs, and 13ˆmore diverse in environments) yields worse results compared to training with small human-annotated dataset (see Table 1), which indicates the need for high-quality instructions beside simply scaling the data and environments. Though many methods have been proposed for improving data quality in multi-modal understanding and generation tasks (Fang et al., 2023; Li et al., 2024a; Betker et al.; Fang et al., 2024; Li et al., 2024b; Nguyen et al., 2024b), improving synthetic instruction-trajectory pairs for languageguided navigation has several distinct challenges. First, the most straightforward approach is to build strong instruction generator, but the limited amount of high-quality training data (e.g., R2R (Anderson et al., 2018b) train split has only 14K human-labeled data) makes it challenging to train robust generator capable of producing high-fidelity instructions for diverse trajectories. Additionally, manually correcting instructions by humans is resource-intensive and costly. Moreover, the alignment of instruction-trajectory pairs in VLN is hard to evaluate, as the instructions not only contain semantic information (e.g., Walk past the table) but also have rich directional information (e.g., Turn left) to match with the corresponding trajectory. Besides, the visual elements mentioned in the instructions are also temporally aligned to panoramas in multiple scenes. As result, traditional metrics like CLIP score (Radford et al., 2021) struggle to evaluate such multi-scene directional and semantic alignment, as they only capture single-scene semantic relationships. Table 1: Performance (on R2R validation unseen split) on different datasets solely. Directly training with R2R yields the best SPL compared to training with other augmentation datasets. #Env. #data 61 14K 1.0M 60 4.9M 800 Training Data R2R Prevalent ScaleVLN SPLÒ 55.9 54.8 50. SRÒ 65.9 67.1 63.9 In this work, we propose Self-Refining Data Flywheel, SRDF, that automatically evaluates and improves the quality of the generated instructions at scale, through an iterative collaboration between the navigator and the instruction generator. As shown in Figure 1 (a), our first step is similar to ScaleVLNs process (Wang et al., 2023e), which trains an instruction generator using the original human-labeled data, then generates instructions for unlabeled paths sampled from 800 HM3D training environments and trains strong navigator using the generated data. Then, for evaluating the generated instructions, we propose to use the trained navigators path-fidelity score (nDTW (Ilharco et al., 2019) and SPL (Anderson et al., 2018a), measuring how closely the navigators followed path matches the original trajectory) as the similarity score. Since the trained navigator is strong enough (achieves human-level performance in Wang et al. (2023e)) and has already learned to connect visual landmarks and directional cues to actions effectively, its fidelity in following the instructions can naturally reflect how well the instruction-trajectory pairs are aligned. It also avoids manually setting vague thresholds when using CLIP-score-like metrics for filtering. In our case, SPL=1 yields perfect trajectory match. After using the navigator as filter to obtain high-quality subset of the generated data, this subset of data will be used to train better instruction generator in the next iteration. The improved instruction generator re-generates instructions for bad samples to produce better datasets, which is used to train stronger navigator. The process iterates, with the navigator improving the instruction generator via data filtering, and the instruction generator improving the 2 Preprint navigator via data refining, ultimately producing both highly capable navigator and instruction generator, along with substantially high-quality synthetic VLN dataset. We build our flywheel upon the R2R dataset (Anderson et al., 2018b) and provide detailed analysis. Empirically, we show that our navigator and instruction generator can iteratively improve each other with our SRDF. We also demonstrate the scalability of our method: the instruction generator consistently improves with additional environments and data, and the navigator benefits more from increased instruction diversity when trained with our high-quality instructions compared to with low-quality datasets. On the R2R dataset, our approach surpasses previous state-of-the-art results by wide margin in both instruction following and generation, and notably, for the first time, we significantly surpass human performance (76% SPL) in instruction following, demonstrating the effectiveness of our SRDF to improve data quality. Furthermore, we evaluate the transferability of our pre-trained navigator across various downstream navigation tasks, including VLN with dialogue-based instructions (CVDN (Thomason et al., 2020)), long-term VLN (RxR-English (Ku et al., 2020), R4R (Zhu et al., 2020)), high-level VLN (SOON (Zhu et al., 2021), REVERIE (Qi et al., 2020)), and even VLN in continuous environments (R2R-CE (Krantz et al., 2020)). As shown in Figure 1 (b), we achieve state-of-the-art performance on all the tasks, while approaching human performance for RxR-English and CVDN, underscoring the superior quality of our generated data and the robust transferability of our pre-trained navigator."
        },
        {
            "title": "2 RELATED WORK\nVision-and-Language Navigation. VLN requires an agent to navigate in unseen environments\nbased on natural language instructions. Numerous datasets have been proposed for this task (An-\nderson et al., 2018b; Ku et al., 2020; Qi et al., 2020; Shridhar et al., 2020; Thomason et al., 2020;\nPadmakumar et al., 2022; Nguyen & Daumé III, 2019; Chen et al., 2019; Kim et al., 2021), span-\nning both indoor and outdoor environments with varied levels of instruction detail. The limited\navailability of human-annotated data for training generalizable VLN agents to achieve near-human\nperformance is a key challenge due to the high cost of collecting instruction-trajectory pairs. To\naddress this, various data augmentation approaches have been explored. Some focus on scaling\nenvironments by editing existing ones (Li et al., 2022; Liu et al., 2021b) or generating new ones\nwith text-to-image models (Li & Bansal, 2023). Others scale instruction data by training instruction\ngenerators to generate instructions for unannotated paths (Hao et al., 2020; Zhang & Kordjamshidi,\n2023; Zhang et al., 2024a), or by leveraging large sets of rendered environments from simulators\n(e.g., HM3D (Ramakrishnan et al., 2021), Gibson (Xia et al., 2018)) (Wang et al., 2023e; Kamath\net al., 2022; Chen et al., 2022c). While data scaling has been effective for VLN, the quality of\nthe data, particularly the alignment between instructions and trajectories, remains under-explored.\nIn this paper, we investigate the impact of data quality on VLN and propose a data flywheel that\niteratively refines itself through collaboration between the navigator and the generator.",
            "content": "High-Quality Multimodal Dataset Curation. Many multimodal studies show that improving data quality can significantly enhance model performance, either through advanced dataset filtering (Fang et al., 2023; Li et al., 2024a; Gadre et al., 2024; Sun et al., 2023) or refining captions with strong models (Betker et al.; Fang et al., 2024; Li et al., 2024b; Nguyen et al., 2024b; Wang et al., 2024c). Recently, Segment Anything (SAM) (Kirillov et al., 2023) demonstrated how data and models can improve each other through data flywheel with human-in-the-loop process, evolving from model-assisted to fully automated annotation. Our data flywheel similarly integrates filtering and re-captioning data via navigator verification and generator refinement, operating in data-model loop like SAM, but without any human intervention. Self-Improving Language Models. Studies show that Large Language Models (LLMs) can improve themselves by training on their own generated outputs across tasks like programming (Haluptzok et al., 2022), summarization (Patil et al., 2024), question-answering (Lee et al., 2024; Yu et al., 2024), reasoning (Prasad et al., 2024), and others (Li et al., 2023a; Madaan et al., 2024; Zhou et al., 2024c), where the quality of the self-generated data is ensured via human (Ouyang et al., 2022; Bai et al., 2022a), off-the-shelf verifiers/reward models (Ni et al., 2023; Wang et al., 2019; Dou et al., 2024; Bai et al., 2022b; Lee et al., 2023; Nguyen et al., 2024a) or models self feedback (Yuan et al., 2024; Wu et al., 2024; Wang et al., 2024d). In our pipeline, the instruction generator iteratively self-improves using its own generated data. Unlike prior work, our approach establishes fully automated, multi-round, two-model mutual improvement process, enabling the navigator, the generator, and the dataset to evolve concurrently through continuous model-driven feedback. 3 Preprint Figure 2: (a) Our (instruction) generator refines low-quality data filtered in the previous round via sampling to train the next-round navigator, and greedily generates high-confidence instructions as candidates to train next-round generator. Then the navigator filters the data for further use. (b) In filtering, the navigator re-runs instructions to compute the path-path similarity score (nDTW & SPL). High-fidelity data is kept for training while low-quality data will be refined."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 BACKGROUND VLN data typically consists of instruction-trajectory pairs, where trajectory represents path in 3D environment, and the corresponding instruction guides the agent to follow it. This data can be used for training either instruction-to-action navigators or trajectory-to-instruction generators. Since manually annotating trajectories is costly, common approach is to first train generator on limited human-annotated data and then use it to generate instructions for paths in unlabeled environments, which are subsequently used to augment navigator training. While this method helps increase the data amount, it poses challenges regarding the quality and fidelity of the generated instructions. These challenges imply two essential problems how to generate better data and evaluate the data quality, which we aim to address in the following sections. 3.2 SRDF: SELF-REFINING DATA FLYWHEEL In this section, we introduce the Self-Refining Data Flywheel (SRDF) to tackle the challenge of evaluating and improving VLN data to bootstrap its learning. Broadly, our system comprises navigator, , and an instruction generator, G, shown in Figure 1 (a). We use to assist by optimizing using the data filtered by , while enhances by refining the low-quality data. This iterative process is repeated multiple times, consistently enhancing both and performance. Main Components. Our SRDF requires the following resources at the beginning: (1) seed data DSeed, typically human-annotated, for training base and (2) unlabeled trajectory pool, DT raj, usually collected from large-scale environment datasets for generating new training data. Training Base Instruction Generator. Most previous works (Fried et al., 2018; Tan et al., 2019; Dou & Peng, 2022) train instruction generators from scratch, which limits their ability to generalize text effectively. Some recent approaches leverage pretrained vision-and-language models but neglect critical directional information during trajectory encoding (Li & Bansal, 2023; Kong et al., 2024), leading to instructions with inconsistent or incorrect directional cues. We hypothesize that an effective instruction generator should be capable of understanding both multi-image inputs and interleaved image-text inputs. Multi-image understanding is crucial for accurately encoding multi-panorama trajectories and interleaved image-text comprehension helps encode directional images within the raw text space, enabling simple yet effective visual prompting of trajectories. To achieve this, we utilize Mantis (Jiang et al., 2024), an interleaved multi-image mul4 Preprint timodal large language model (MLLM), which includes SigLIP vision encoder (Zhai et al., 2023), multimodal projector, and LLaMA-3 language model backbone (Dubey et al., 2024), pretrained on curated corpus of interleaved image-text pairs. We use our designed template (see Appendix 3, 13 for details) to convert the instruction-trajectory pairs from DSeed into supervised fine-tuning (SFT) data, and then LoRA-fine-tune the language backbone using this converted data. This results in robust instruction generator G1, which serves as the foundation for our data generation process. Generating Base Training Data. Once the base instruction generator, G1, is trained, it is used to generate two types of data from the unlabeled trajectories, DT raj: one for training the navigator and another for improving the round-2 instruction generator. For data training the instruction generator, we use greedy decoding to generate the most reliable instruction for each trajectory to form DG 2 . For data used to train the navigator, we prioritize diversity by generating multiple instructions via random sampling to form DN 1 . The data generation step aims to provide sufficient training data for both improving the navigator and the instruction generator in subsequent iterations. Training Base Navigator. We employ the DUET model (Chen et al., 2022b) as our navigator. The model is pre-trained using DN 1 and subsequently fine-tuned on DSeed. This results in highly capable navigator N1, which will be used to evaluate the quality of the generated data. Evaluating and Filtering the Generated Data. We propose using the trained navigator N1 to self-evaluate the generated data DN 1 . Intuitively, if the navigator successfully follows the generated instruction and navigates along the original path, it suggests that the instruction is well-aligned with the trajectory and the strong performance of this navigator (Could achieve near-human performance in previous work (Wang et al., 2023e)) further ensures its reliability. Such self-consistency transforms the challenging task of computing instruction-trajectory similarity into the simpler problem of computing trajectory-trajectory similarity. We run the navigator on the generated instructions and compute trajectory-trajectory similarity scores. We filter high-quality data DG 2 for the round-2 instruction generator training by selecting instances from DG 2 with SPL=1 (indicating the navigator perfectly follows the path). For DN 1 (navigator training data), we filter with nDTWě0.9 (indicating very close alignment between trajectories) to get DN ă2, to use in the round-2 training, as shown in Figure 2. This filtered-out highquality data will be kept in the follow-up round, and we only re-generate low-quality data LDN 2 filtered by nDTWă0.9. This filtering step ensures that only reliable data is used for subsequent iterations of training, reducing data noise and increasing alignment quality. Iterative Self-Refining. The core of SRDF is its iterative loop between the instruction generator and the navigator, where each model contributes to improving the other by providing data feedback. Specifically, at each iteration t, we first train the generator Gt using the filtered high-quality data DG . Then we use Gt to generate new navigation-training data, DN , for previously bad samples, , and new generator training data, DG LDN t`1, for DT raj, following the same details in generating base training data. Then we Combine DN with previously filtered navigation data, DN ăt, to form the complete dataset, DN , for training the navigator. Note that the whole data size wont change as we are only refining base samples. After training the navigator Nt using DN , we use Nt to from DN filter high-quality data subsets, resulting in DG . Finally, we combine the newly filtered navigation data, DN ăt`1 for the nextround training. This looping mechanism ensures that the data quality continually improves through self-refining process. Each iteration benefits from the enhanced quality of both the instruction generator and the navigator, ultimately yielding high-quality dataset and highly capable models. To summarize, the pseudocodes of the SRDF are detailed in Appendix Alg. 1. t`1 filtered from DG , to form DN t`1 and DN 3.3 FINAL DATASET We build our flywheel upon the R2R dataset (Anderson et al., 2018b) as DSeed, containing 14,039 human-annotated training data, along with the 178,270 and 2,890,267 unlabelled trajectories from MP3D (Chang et al., 2017) and HM3D (Wang et al., 2023e) environments, respectively, as DT raj. We run the flywheel for three rounds to create the final dataset, named SRDF-20M. This dataset consists of 19.5 million pre-training examples DN 3 , with 6 instructions generated for each HM3D path 5 Preprint Table 2: Statistics of training data on different VLN datasets. Dataset R2R (Anderson et al., 2018b) RxR-en (Ku et al., 2020) REVERIE (Qi et al., 2020) CVDN (Thomason et al., 2020) SOON (Zhu et al., 2021) R4R (Zhu et al., 2020) Prevalent (Hao et al., 2020) Marky (Wang et al., 2022b) AutoVLN (Chen et al., 2022c) ScaleVLN (Wang et al., 2023e) SRDF-20M (Ours) Instruction #Env. Manually Labelled Generated 61 60 60 57 34 59 60 60 900 1289 #Instr. 14,039 26,464 10,466 4,742 2,780 233,532 1,069,620 333,777 217,703 4,941,710 20,417,874 #Vocab. 3,063 7,249 1,140 2,068 735 3,004 993 2,231 1,696 172 10,363 Instr. Length 26.33 102.13 18.64 53.21 44.09 52.25 24.23 99.45 20.52 21.61 24.05 (via top-3 sampling) and 12 instructions (6 top-3 and 6 top-5 sampling) for each MP3D path, and 0.9M greedy-decoded filtered data DG 4 (which will be used to fine-tune the pretrained navigator and the third-round generator here). Overall, we generated 20M instructions across 860 environments for navigator training. Comparison to Previous VLN Datasets. Table 2 presents detailed statistics of our dataset and previous VLN datasets. common issue in existing augmentation datasets is the lack of vocabulary diversity. For instance, prior datasets like Prevalent (Hao et al., 2020), despite being significantly larger than R2R, possess only 1/3 of R2Rs vocabulary size, even with 76 times more instructions. This issue is even more pronounced in ScaleVLN (Wang et al., 2023e), which suffers from highly limited vocabulary. In contrast, our dataset contains over 10,000 unique words and 20 million instructions three times the vocabulary diversity and 1,454 times the instruction count compared to the original R2R dataset surpassing all previous human-labeled and synthetic VLN datasets in both vocabulary richness and instruction quantity. Importantly, this substantial increase in size and diversity is achieved without compromising quality, due to the robustness of our instruction generator and the high-precision filtering process guided by our strong navigator. We also provide some visualization examples of our generated instructions in Appendix Figure 4 and 5."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets and Evaluation Metrics. We establish our data flywheel and perform ablation studies primarily on the R2R dataset, while also assessing the transferability of our pre-trained navigator on range of downstream tasks. These include fine-grained VLN (R2R, RxR-English), VLN with dialog history (CVDN), high-level VLN (REVERIE, SOON), long-term VLN (R4R), and VLN in continuous environments (R2R-CE). Each dataset is split into training, val_seen, and val_unseen sets, with R2R, CVDN, REVERIE, SOON, and R2R-CE also containing test splits. The statistics for the training splits are summarized in Table 2 (manually-labeled datasets), and further details can be found in the Appendix. We evaluate our navigator using the standard path-fidelity metrics including Success Rate (SR), Success Rate Weighted by Path Length (SPL) (Anderson et al., 2018a), Goal Progress (GP) (Thomason et al., 2020), Navigation Error (NE), normalized Dynamic Time Warping (nDTW) (Ilharco et al., 2019) and Success Rate Weighted by Dynamic Time Warping (sDTW) (Ilharco et al., 2019). We leave the details of these metrics to Appendix. Although REVERIE and SOON include object grounding tasks, we focus on evaluating navigation performance, as our generated dataset is specifically designed to enhance navigation tasks. We use SPL as the primary metric for R2R, REVERIE, SOON, and R2R-CE, nDTW for RxR-English, sDTW for R4R, and GP for CVDN. 6 Preprint Table 3: Navigator and instruction generator results in different rounds in R2R val unseen split. Method Baseline Ours (round 1) Ours (round 2) Ours (round 3) Instruction Following NEÓ OSRÒ 85.5 2.37 87.1 1.95 88.5 1.81 89.6 1.76 SRÒ 78.6 82.4 83.6 84. SPLÒ 69.9 75.9 77.3 77.6 SPICEÒ 21.8 23.7 25.2 25.7 Instruction Generation SPICE-DÒ Bleu-1Ò Bleu-4Ò CIDErÒ MeteorÒ RougeÒ 28.0 28.4 29.9 30.4 72.5 71.4 73.7 74. 27.7 29.5 31.0 30.8 42.2 46.5 50.7 49.7 23.6 23.1 24.2 24.5 49.0 50.2 51.3 51.3 For evaluating our instruction generator, we assess the linguistic quality of the generated instructions using standard text similarity metrics such as BLEU (Papineni et al., 2002), Meteor (Banerjee & Lavie, 2005), and Rouge (Lin, 2004), along with commonly used image captioning metrics, including SPICE (Anderson et al., 2016) and CIDEr (Vedantam et al., 2015). Additionally, we consider SPICE-D (Zeng et al., 2023), directional-aware variant of SPICE tailored for VLN instruction evaluation. We adopt SPICE as our primary metric. Implementation. We utilize InternViT (Chen et al., 2024), powerful ViT with 6B parameters, as the visual encoder in our instruction-following experiments unless otherwise specified. For continuous environments, we employ CLIP-B/16 (Radford et al., 2021) as the visual backbone. In our data flywheel, we pre-train the DUET navigator from scratch for 45,000 iterations using batch size of 1024 and learning rate of 5 ˆ 105 on 8 NVIDIA Tesla A100 GPUs. Multiple checkpoints are fine-tuned to select the best pre-training model. The selected model is then finetuned for 6K iterations with batch size of 16 on single GPU using only the R2R dataset. For the instruction generator, we fine-tune Mantis-8B-SigLIP (Jiang et al., 2024) using LoRA, applying it to the query and value layers in each transformer block. Initially, both the navigator and instruction generator are trained from random weights, while subsequent rounds use previous-round weights. After final-round navigator pre-training, we fine-tune the navigator for various downstream tasks, using PanoGen (Li & Bansal, 2023) for MP3D-level environment augmentation. For augmentation, we use round-3 filtered data DG 4 for R2R, ScaleVLN(REVERIE) (Wang et al., 2023e) for REVERIE, and marky-English (Wang et al., 2022b) for RxR-English, and no augmentation for other tasks. For R2R-CE, we fine-tune our pre-trained navigator on ETPNav (An et al., 2023b) with waypoint predictor (Hong et al., 2022; An et al., 2022). Modules not pre-trained, such as the object grounding module in REVERIE and the depth-image embedding module in ETPNav, are randomly initialized and trained from scratch. We also use DG 4 to fine-tune the 3rd-round generator to build the final instruction generator. This process can also be viewed as the fourth round of generator training. 4.2 FLYWHEEL RUNNING RESULTS Table 3 presents the results for both the instruction generator and navigator across all rounds. We follow ScaleVLN (Wang et al., 2023e) to train the navigator with ScaleVLN-HM3D and Prevalent data for augmentation, while we use InternViT (Chen et al., 2024) features for fair comparison. The instruction generator for ScaleVLN is EnvDrop (Tan et al., 2019). In round 1, our new instruction generator, fine-tuned on R2R with LoRA using the pre-trained Mantis, significantly surpasses EnvDrop. This results in substantial SR boost for the navigator from 78.6% to 82.4%. Navigator and Instruction Generator Improve Each Other. At each round, we use the navigator to filter high-quality data DG to re-train the instruction generator, and use the improved instruction generator to refine low-quality instructions LDN to re-train the navigator. Despite the strong performance of the round 1 baseline, the generator is further improved by incorporating navigatorfiltered data in round 2. The high-quality data filtered by the navigator leads to +1.5 SPICE and +4.2 CIDEr. This trend continues in round 3, where SPICE reaches 25.7, while other metrics remain stable, demonstrating the crucial role of the navigator in enhancing the instruction generator via data filtering. For the navigator, the data-refining process leads to continuous improvements in navigation performance with +1.2% SR in round 2, and +0.8% SR in round 3, underscoring the importance of the generator data refinement in enhancing the navigator, as well as the effectiveness of iterative navigator-generator collaboration to build an effective self-refining flywheel. Preprint Table 5: Results of instruction diversity (#instr. per path) in navigator training in val unseen split. Aug Data Prev #Instr=1 Ours #Instr=1 Prev #Instr=3 Ours #Instr=3 Prev #Instr=6 Ours #Instr=6 Ours #Instr=12 NEÓ 3.21 2.97 3.12 2.81 3.07 2.55 2.59 SRÒ 71.86 73.86 72.67 75.21 72.84 76.93 77.05 SPLÒ 61.04 63.58 62.53 64.56 63.12 66.89 66. Table 6: Results of different additional augmentation data in instruction generator training in val unseen split. Additional Data - ScaleVLN Data Prevalent Data 100-HM3D-Env Ours 200-HM3D-Env Ours 400-HM3D-Env Ours 800-HM3D-Env Ours 800-HM3D-Env Ours (Sample) SPICEÒ Bleu-4Ò CIDErÒ RougeÒ 23.7 23.5 23.6 23.9 24.2 24.6 25.2 24.8 29.5 29.0 29.3 29.8 30.1 30.3 31.0 30.3 46.5 46.1 46.7 47.8 49.1 48.9 50.7 48. 50.2 49.8 50.1 50.0 50.3 50.7 51.3 51.0 4.3 ANALYSIS Comparison of Different Scoring Functions. We analyzed classical filtering methods will likely fail to capture complex path-instruction similarity in the previous discussion. In Table 4, we further verify the importance of our navigator-filtering compared to other filtering baselines. Using our round 1 instruction generator, we produce instructions for 783 trajectories from the validation unseen split, rank them based on various scoring functions, and filter the top 400 to assess their similarity to GT instructions. No filter refers to the average score of all 783 instructions, and intuitively, more similar instructions should yield higher NLP metric scores. We compare our navigators nDTW-score with CLIP-Sim (Radford et al., 2021) and Mantis score (Jiang et al., 2024). CLIP-Sim is computed by averaging image-instruction similarities across all observations in the trajectory, while Mantis-score is produced by inputting the path as interleaved image-text pairs and asking Mantis to provide similarity score. Results show that the Mantis score fails to improve over the baseline, likely due to the trajectories is too complex to understand for the MLLM. CLIP-Sim provides slight SPICE improvement, possibly because it can capture some landmark-level similarities between the trajectory images and the instructions, but does not improve SPICE-D as it lacks directional understanding. In contrast, our Navigator-nDTW similarity filtering method successfully identifies high-quality instructions, leading to substantial improvement, demonstrating our navigator-nDTW captures path-instruction similarities much better than others. Table 4: Effects of Scoring Functions. Effect of Instruction Diversity in Navigator Training We assess the impact of instruction diversity by training the navigator with different numbers of instructions per path (#instr 1, 3, 6, 12) on the MP3D environments, as shown in Table 5. We use CLIP-B/16 as the visual feature to establish well-known baseline (Li & Bansal, 2023; Wang et al., 2023e) with the Prevalent dataset, while Our uses instructions generated by our round 2 instruction generator. Compared to Prevalent, our instructions consistently achieve stronger downstream results at each #instr level, emphasizing the importance of instruction quality. Our navigator also benefits significantly when increasing #instr from 1 to 3 and 3 to 6, while Prevalents performance saturates after 3, demonstrating that scaling instruction diversity will be more effective when instruction quality is higher. Increasing #instr to 12 yields similar results to 6, suggesting that #instr 6 is an optimal balance for training. SPICEÒ SPICE-DÒ CIDErÒ 28.4 28.7 28.2 30.6 Filter 23.7 No Filter 24.4 CLIP-Sim Mantis-Score 23.6 Navigator-nDTW 25.4 46.5 45.8 48.3 53.9 Effect of Additional Data in Instruction Generator Training. In Table 6, we examine the importance of high-quality data in instruction generator training, and the potential scalability of our pipelines by evaluating the influence of environment numbers. The round-1 generator without supplementary data serves as the baseline. Adding the ScaleVLN dataset does not improve performance, likely due to its low diversity, which limits generalization in text generation tasks. When training with the Prevalent dataset, which has greater diversity, performance gains remain minimal, possibly because of the data noise, as it also shows low quality in Table 1. In contrast, adding data from ours with increased environments (we split DG 2 by environments) consistently Preprint Table 7: Comparison of single-run performance on R2R and R2R-CE datasets. Best results are marked in bold blue and second best in bold. Methods Validation-Unseen Test-Unseen Validation-Unseen Test-Unseen NEÓ OSRÒ SRÒ SPLÒ NEÓ OSRÒ SRÒ SPLÒ NEÓ SRÒ SPLÒ NEÓ SRÒ SPLÒ Room-to-Room Dataset Room-to-Room-CE Dataset Human Seq2Seq (Anderson et al., 2018b) Speaker Follower (Fried et al., 2018) RCM (Wang et al., 2019) EnvDrop (Tan et al., 2019) PREVALENT (Hao et al., 2020) NvEM (An et al., 2021) AirBert (Guhur et al., 2021) VLNœBERT (Hong et al., 2021) HAMT (Chen et al., 2021) HOP (Qiao et al., 2022) HOP+ (Qiao et al., 2023a) DUET (Chen et al., 2022b) Lily (Lin et al., 2023) DreamWalker (Wang et al., 2023a) BEVBert (An et al., 2023a) ScaleVLN (Wang et al., 2023e) GridMM (Wang et al., 2023d) ETPNav (An et al., 2023b) DualAction (Zhang & Kordjamshidi, 2024) HNR (Wang et al., 2024e) NaviLLM (Zheng et al., 2024) NavGPT-2 (Zhou et al., 2024a) VER (Liu et al., 2024) MAGIC-L (Wang et al., 2024b) GOAT (Wang et al., 2024a) SRDF (Ours) - 7.81 6.62 6.09 5.22 4.71 4.27 4.10 3.93 2.29 3.80 3.49 3.31 2.90 - 2.81 2.09 2.83 - - - 3.51 2.84 2.80 2.22 2.40 1. - 28 45 50 - - - - - - - - 81 - - 84 88 - - - - - 84 - 86 85 90 - 21 36 43 52 58 60 62 63 66 64 67 72 74 - 75 81 75 - - - 67 74 76 79 78 86 - - - - 48 53 55 56 57 61 57 61 60 62 - 64 70 64 - - - 59 61 65 70 68 79 1.61 7.85 6.62 6.12 5.23 5.30 4.37 4.13 4.09 3.93 3.83 3.71 3.65 3.44 - 3.13 2.27 3.13 - - - 3.71 3.33 2.74 2.75 3.04 1.82 90 27 - 50 59 61 - - 70 72 - - 76 - - 81 86 - - - - - 80 - 82 80 89 86 20 35 43 51 54 58 62 63 65 64 66 69 72 - 73 80 73 - - - 68 72 76 77 75 76 - 28 38 47 51 54 57 57 60 59 60 59 60 - 62 70 62 - - - 60 60 66 69 65 78 - - - - - - - - - - - - - - 5.53 4.57 4.80 5.11 4.71 - 4.42 - - - - - 4.12 - - - - - - - - - - - - - - 49 59 55 49 57 58 61 - - - - - 65 - - - - - - - - - - - - - - 44 50 51 41 49 49 51 - - - - - 57 - - - - - - - - - - - - - - 5.48 4.70 5.11 5.64 5.12 - 4.81 - - - - - 4.35 - - - - - - - - - - - - - - 49 59 55 46 55 56 58 - - - - - - - - - - - - - - - - - - - 44 50 50 39 48 48 50 - - - - - 56 enhances performance. This improvement is likely due to both the diversity and quality of our data, which are carefully maintained throughout the process, boosting the SPICE from 23.7 to 25.2. We also experimented with training using sampled versus greedy-decoded instructions. Sampled instructions resulted in slightly lower performance, suggesting they may introduce noise, whereas greedy-decoded instructions, produced with higher confidence, are more reliable. These results show the strong extensibility of our pipeline, and the critical role of high-quality data, both in instruction generator training. 4.4 COMPARISION WITH STATE OF THE ARTS R2R and R2R-CE. Table 7 compares agent performance on the R2R and R2R-CE datasets. The DUET navigator, trained on our high-quality datasets, improves SoTA SPL (ScaleVLN (Wang et al., 2023e)) by 8% on the R2R test set, demonstrating our datas strong instruction-trajectory alignment that allows effective decision-making learning. Additionally, The gap between oracle success and success is reduced to 4%, compared to the previous best of 6%, highlighting that our data provides stronger clues for the agent to learn when to stop. Notably, our data-centric approach yields greater improvements compared to most model design modifications, highlighting the importance of building high-quality data to boost model performance. For R2R-CE, despite ETPNav using in-domain pre-training with Habitat-rendered RGBD images (Savva et al., 2019), our model, using no-rendered images from MP3D and ScaleVLNs HM3D without depth-image pre-training, achieves an absolute gain of +8% in SR and SPL, demonstrating the strong transferability of our pre-trained navigator. REVERIE and SOON. For high-level navigation tasks, as shown in Table 8, our method achieves notable improvements on the val unseen split for REVERIE and SOON, with +3.5% SPL over ScaleVLN and +10.0% SPL over AutoVLN (Chen et al., 2022c), respectively. These gains are especially impressive given that both AutoVLN and ScaleVLN (Wang et al., 2023e) used in-domain pre-training data, while ours comes from the out-of-domain R2R-style dataset. This demonstrates that our high-quality data not only improves fine-grained instruction-following but also enhances 9 Preprint Table 8: Comparison with previous methods on various downstream tasks. : indicates the RxR-en results are reproduced using their officially released checkpoints. means pre-exploration methods. Best results are marked in bold blue and second best in bold. Methods RxR-english Val unseen R4R Val unseen Val Test Val unseen Test unseen Val unseen Test unseen SRÒ nDTWÒ SRÒ sDTWÒ GPÒ GPÒ SRÒ SPLÒ SRÒ SPLÒ SRÒ SPLÒ SRÒ SPLÒ CVDN REVERIE SOON HAMT (Chen et al., 2021) MARVAL (Kamath et al., 2022) DUET (Chen et al., 2022b) AutoVLN (Chen et al., 2022c) RREx-Bot (Sigurdsson et al., 2023) BEVBert (An et al., 2023a) KERM (Li et al., 2023b) ScaleVLN (Wang et al., 2023e) PanoGen (Li & Bansal, 2023) BSG (Liu et al., 2023) MiC (Qiao et al., 2023b) NaviLLM (Zheng et al., 2024) VER (Liu et al., 2024) PRET (Lu et al., 2024) MAGIC-L (Wang et al., 2024b) VLN-Copilot (Qiao et al., 2024) GOAT (Wang et al., 2024a) SRDF (Ours) 56.4 64.7 - - - 66.7 - - - - - - - 71.0 72.9 - 68.2 78. 63.0 70.4 - - - 69.6 - - - - - - - 70.9 68.1 - 66.8 74.4 44.6 - - - - - - - 47.8 47.0 - - 47.0 - - - - 64.4 31.8 - - - - - - - - 34.0 - - 33.0 - - - - 44.6 5.13 5.58 33.0 30.2 30.4 26.7 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 47.0 33.7 52.5 36.1 36.3 22.6 33.4 21.4 55.9 40.9 55.2 38.9 41.0 30.7 40.4 27.9 61.0 58.8 65.9 62.0 49.2 48.6 47.5 47.1 51.8 36.4 52.8 36.4 50.4 35.4 52.4 39.2 38.1 23.2 - - 6.12 6.97 57.0 41.9 56.1 39.5 5.93 7.17 - - - - - - - - 52.1 35.6 56.5 38.7 57.0 43.6 55.7 42.0 - - - - - - - - 6.16 7.90 44.6 36.6 43.5 34.5 38.3 29.2 35.0 26. 56.0 39.7 56.8 38.8 - - - - - - - - - - - - - - - - 57.4 43.6 57.8 42.3 53.4 36.7 57.7 40.5 40.4 28.1 40.5 25.2 7.67 8.19 60.4 45.4 61.4 47.7 50.3 41.7 46.6 37.9 - - - - - - - - - - - - - - - - - - - - Table 9: Performance of different instruction generators on R2R. : means reproduced results. Best results are marked in bold blue, second best in bold, and third best in underlined. Methods R2R Validation Unseen SPICEÒ SPICE-DÒ Bleu-1Ò Bleu-4Ò CIDErÒ MeteorÒ RougeÒ BT-speaker (Fried et al., 2018) LandmarkSelect (Agarwal et al., 2019) EnvDrop (Tan et al., 2019) CCC (Wang et al., 2022a) FOAM: (Dou & Peng, 2022) KEFA (Zeng et al., 2023) LANA (Wang et al., 2023b) LANA+ (Wang et al., 2023c) C-Instructor (Kong et al., 2024) BEVInsructor (Fan et al., 2024) SRDF (Ours, round 2) SRDF (Ours, round 3) SRDF (Ours, round 3 fine-tuned w/ DG 4 ) 18.9 19.7 21.8 21.4 21.7 23.4 22.6 22.8 21.2 20.8 25.2 25.7 26.2 25.1 - 28.0 27.8 28.1 29.3 - - - - 29.9 30.4 30. 68.2 54.8 72.3 70.8 72.5 73.8 73.6 73.2 71.3 69.9 73.7 74.5 75.3 26.3 15.9 27.1 27.2 27.3 28.3 28.9 29.5 26.6 26.4 31.0 30.8 31.1 37.9 13.2 41.7 46.1 42.4 42.7 45.7 46.0 44.7 44.9 50.7 49.7 49.2 21.7 23.1 23.6 23.1 23.4 24.4 23.7 24.1 23.9 23.0 24.2 24.5 25.0 48.0 35.7 49.0 47.7 49.2 50.3 49.8 49.6 47.3 46.7 51.3 51.3 51.4 goal-finding, likely due to diverse stopping guidance. Surprisingly, our model achieves results comparable to the pre-exploration agent RREX-Bot (Sigurdsson et al., 2023) on REVERIE (-0.6% SR), and even surpasses it on SOON (+1.4% SR), showing our models robust goal-finding ability. CVDN. Our method also improves the previous best on the val unseen set of CVDN by large margin (+1.51 meters) in Table 8, showing that our model can be generalized to different instruction styles, likely due to learning strong landmark alignment ability, which can be shared across tasks. RxR-English and R4R. For long-horizon (and fine-grained) VLN tasks including RxR-en and R4R, our navigator also shows strong results on the val unseen split, surpassing previous SoTAs by large margin shown in Table 8. Its worth noting that our results on R4R provide very strong improvement of +16.6% SR. This shows our highly aligned data facilitates learning step-by-step instruction following even with very long trajectories. R2R Instruction Generation. We also compare our instruction generator with previous SoTAs for path-to-instruction generation task on R2R val unseen split in Table 9. Thanks to our navigatorfiltered high-quality data, our round 2 generator has already beat the previous SoTA significantly, 10 Preprint with + 1.8 SPICE and + 4.6 CIDEr. The stronger data generated in the third round results in an additional +0.5 SPICE improvement while keeping other scores comparable or better. This substantial strong performance can even be further enhanced by incorporating DG 4 for fine-tuning, leading to +0.5 improvement in SPICE and better results across five additional metrics, indicating the importance of high-quality data in instruction generator training."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce fully automatic self-refining data flywheel to construct substantially high-quality VLN dataset for augmentation. We propose to iteratively refine the data with the navigator and instruction generator working in tandemthe navigator filtering high-quality data to train better instruction generator and the instruction generator regenerating better instructions to train better navigator, ultimately producing both strong navigator, instruction generator and highquality VLN dataset. We thoroughly analyzed the impact of each component in the flywheel, demonstrating that our approach significantly surpasses state-of-the-art methods across multiple VLN benchmarks, covering various instruction styles (R2R, CVDN, REVERIE, SOON), trajectory lengths (R4R, RxR-English), and control spaces (R2R-CE), as well as instruction generation task on R2R. Our self-refining flywheel provides novel, scalable solution to the data bottleneck in VLN, highlighting the crucial role of instruction quality and alignment in training embodied agents. This method has the potential to drive future advancements in embodied navigation models and paves the way for exploring more sophisticated tasks that rely on high-quality, dynamic, and scalable data."
        },
        {
            "title": "6 ACKNOWLEGEMENT",
            "content": "This work is partially supported by the National Key R&D Program of China (NO. 2022ZD0160100). We warmly thank Hao Tan, Archiki Prasad, Zhaoyang Wang, and Yiyang Zhou for their helpful suggestions and feedback on the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Sanyam Agarwal, Devi Parikh, Dhruv Batra, Peter Anderson, and Stefan Lee. Visual landmark selection for generating grounded and interpretable navigation instructions. In CVPR Workshop, volume 3, pp. 7, 2019. Dong An, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, and Tieniu Tan. Neighbor-view enhanced model for vision and language navigation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 51015109, 2021. Dong An, Zun Wang, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang, and Jing Shao. 1st place solutions for rxr-habitat vision-and-language navigation competition (cvpr 2022). arXiv preprint arXiv:2206.11610, 2022. Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, and Jing Shao. Bevbert: Multimodal map pre-training for language-guided navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 27372748, 2023a. Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, and Liang Wang. Etpnav: Evolving topological planning for vision-language navigation in continuous environments. arXiv preprint arXiv:2304.03047, 2023b. Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propoIn Computer VisionECCV 2016: 14th European Confersitional image caption evaluation. ence, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part 14, pp. 382398. Springer, 2016. Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018a. 11 Preprint Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 36743683, 2018b. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Perez, Jamie Kerr, Jared Mueller, Jeff Ladish, Landau, Kamal Ndousse, Kamile Lukovsiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova Dassarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Sam Bowman, Zac Hatfield-Dodds, Benjamin Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom B. Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback. ArXiv, abs/2212.08073, 2022b. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng Wang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee, YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and Aditya Ramesh. Improving image generation with better captions. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In 2017 International Conference on 3D Vision (3DV), pp. 667676. IEEE, 2017. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1253812547, 2019. Jinyu Chen, Chen Gao, Erli Meng, Qiong Zhang, and Si Liu. Reinforced structured state-evolution for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1545015459, 2022a. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. Advances in Neural Information Processing Systems, 34:58345847, 2021. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Think In Proglobal, act local: Dual-scale graph transformer for vision-and-language navigation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16537 16547, 2022b. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Learning from unlabeled 3d environments for vision-and-language navigation. In European Conference on Computer Vision, pp. 638655. Springer, 2022c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF Conference on Computer for generic visual-linguistic tasks. Vision and Pattern Recognition, pp. 2418524198, 2024. 12 Preprint Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Zi-Yi Dou and Nanyun Peng. Foam: follower-aware speaker model for vision-and-language navigation. arXiv preprint arXiv:2206.04294, 2022. Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, and Nanyun Peng. Reflection-reinforced self-training for language agents. arXiv preprint arXiv:2406.01495, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Sheng Fan, Rui Liu, Wenguan Wang, and Yi Yang. Navigation instruction generation with bev perception and large language models. arXiv preprint arXiv:2407.15087, 2024. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. Vila2: Vila augmented vila. arXiv preprint arXiv:2407.17453, 2024. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In Advances in Neural Information Processing Systems, pp. 33143325, 2018. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024. Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, and Cordelia Schmid. Airbert: In-domain pretraining for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16341643, 2021. Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. arXiv preprint arXiv:2207.14502, 2022. Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning In Proceedings of the generic agent for vision-and-language navigation via pre-training. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1313713146, 2020. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Keji He, Yan Huang, Qi Wu, Jianhua Yang, Dong An, Shuanglin Sima, and Liang Wang. Landmarkrxr: Solving vision-and-language navigation with fine-grained alignment supervision. In Conference on Neural Information Processing Systems, volume 34, pp. 652663, 2021. Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity relationship graph for agent navigation. Advances in Neural Information Processing Systems, 33, 2020. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. recurrent vision-and-language bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16431653, June 2021. Yicong Hong, Zun Wang, Qi Wu, and Stephen Gould. Bridging the gap between learning in disIn Proceedings of the crete and continuous environments for vision-and-language navigation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1543915449, 2022. 13 Preprint Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evalarXiv preprint uation for instruction conditioned navigation using dynamic time warping. arXiv:1907.05446, 2019. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv2405.01483, 2024. Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh, Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, and Zarana Parekh. new path: Scaling vision-and-language navigation with synthetic instructions and imitation learning. arXiv preprint arXiv:2210.03112, 2022. Hyounghun Kim, Jialu Li, and Mohit Bansal. NDH-full: Learning and evaluating navigational agents on full-length dialogue. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 64326442, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.518. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Xianghao Kong, Jinyu Chen, Wenguan Wang, Hang Su, Xiaolin Hu, Yi Yang, and Si Liu. Controllable navigation instruction generation with chain of thought prompting. arXiv preprint arXiv:2407.07433, 2024. Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVIII 16, pp. 104120. Springer, 2020. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 43924412, 2020. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In International Conference on Machine Learning, 2023. Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng Shen, Gopala Anumanchipali, Michael Mahoney, Kurt Keutzer, and Amir Gholami. Llm2llm: Boosting llms with novel iterative data enhancement. arXiv preprint arXiv:2403.15042, 2024. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024a. Jialu Li and Mohit Bansal. Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation. arXiv preprint arXiv:2305.19195, 2023. Jialu Li, Hao Tan, and Mohit Bansal. Envedit: Environment editing for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1540715417, 2022. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023a. Xiangyang Li, Zihan Wang, Jiahao Yang, Yaowei Wang, and Shuqiang Jiang. Kerm: Knowledge enhanced reasoning for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 25832592, 2023b. 14 Preprint Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024b. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Kunyang Lin, Peihao Chen, Diwei Huang, Thomas Li, Mingkui Tan, and Chuang Gan. Learning vision-and-language navigation from youtube videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 83178326, 2023. Chong Liu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang, Zongyuan Ge, and Yi-Dong Shen. Visionlanguage navigation with random environmental mixup. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16441654, 2021a. Chong Liu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang, Zongyuan Ge, and Yi-Dong Shen. Visionlanguage navigation with random environmental mixup. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16441654, 2021b. Rui Liu, Xiaohan Wang, Wenguan Wang, and Yi Yang. Birds-eye-view scene graph for visionlanguage navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1096810980, 2023. Rui Liu, Wenguan Wang, and Yi Yang. Volumetric environment representation for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1631716328, 2024. Renjie Lu, Jingke Meng, and Wei-Shi Zheng. Pret: Planning with directed fidelity trajectory for vision and language navigation. arXiv preprint arXiv:2407.11487, 2024. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Laser: Learning to adaptively select reward models with multi-armed bandits. arXiv preprint arXiv:2410.01735, 2024a. Khanh Nguyen and Hal Daumé III. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. arXiv preprint arXiv:1909.01871, 2019. Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. Advances in Neural Information Processing Systems, 36, 2024b. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria In International Lin. Lever: Learning to verify language-to-code generation with execution. Conference on Machine Learning, pp. 2610626128. PMLR, 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven In Proceedings of the AAAI Conference on Artificial Intelligence, embodied agents that chat. volume 36, pp. 20172025, 2022. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. 15 Preprint Vaidehi Patil, Leonardo Ribeiro, Mengwen Liu, Mohit Bansal, and Markus Dreyer. Refinesumm: In Proceedings of the Self-refining mllm for generating multimodal summarization dataset. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1377313786, 2024. Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference optimization. arXiv preprint arXiv:2411.04109, 2024. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 99829991, 2020. Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, and Qi Wu. Hop: History-andorder aware pre-training for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1541815427, 2022. Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, and Qi Wu. Hop+: Historyenhanced and order-aware pre-training for vision-and-language navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023a. Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, and Qi Wu. March in chat: Interactive prompting for remote embodied referring expression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1575815767, 2023b. Yanyuan Qiao, Qianyi Liu, Jiajun Liu, Jing Liu, and Qi Wu. Llm as copilot for coarse-grained visionand-language navigation. In European Conference on Computer Vision, pp. 459476, 2024. Wenda Qin, Teruhisa Misu, and Derry Wijaya. Explore the potential performance of vision-andlanguage navigation model: snapshot ensemble method. arXiv preprint arXiv:2111.14267, 2021. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PMLR, 2021. Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio Lopez. The synthia dataset: large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3234 3243, 2016. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 93399347, 2019. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1074010749, 2020. Gunnar Sigurdsson, Jesse Thomason, Gaurav Sukhatme, and Robinson Piramuthu. Rrex-bot: Remote referring expressions with bag of tricks. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 52035210. IEEE, 2023. 16 Preprint Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning text-to-image generation with image understanding feedback. In Synthetic Data for Computer Vision Workshop@ CVPR 2024, 2023. Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language formers. Processing, 2019. Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In Proceedings of NAACL-HLT, pp. 26102621, 2019. Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning, pp. 394406, 2020. Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-to-image models make strong visual representation learners. Advances in Neural Information Processing Systems, 36, 2024. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575, 2015. Hanqing Wang, Wenguan Wang, Wei Liang, Caiming Xiong, and Jianbing Shen. Structured scene memory for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84558464, 2021. Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool, and Wenguan Wang. Counterfactual cycle-consistent learning for instruction following and generation in vision-language navigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1547115481, 2022a. Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang. Dreamwalker: Mental planning for continuous vision-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1087310883, 2023a. Liuyi Wang, Zongtao He, Ronghao Dang, Mengjiao Shen, Chengju Liu, and Qijun Chen. Visionand-language navigation via causal learning. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024a. Liuyi Wang, Zongtao He, Mengjiao Shen, Jingwei Yang, Chengju Liu, and Qijun Chen. Magic: Meta-ability guided interactive chain-of-distillation for effective-and-efficient vision-andlanguage navigation. arXiv preprint arXiv:2406.17960, 2024b. Su Wang, Ceslee Montgomery, Jordi Orbay, Vighnesh Birodkar, Aleksandra Faust, Izzeddin Gur, Natasha Jaques, Austin Waters, Jason Baldridge, and Peter Anderson. Less is more: Generating grounded navigation instructions from landmarks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1542815438, 2022b. Xiaohan Wang, Wenguan Wang, Jiayi Shao, and Yi Yang. Lana: language-capable navigator for instruction following and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1904819058, 2023b. Xiaohan Wang, Wenguan Wang, Jiayi Shao, and Yi Yang. Learning to follow and generate instructions for language-capable navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023c. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 66296638, 2019. 17 Preprint Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024c. Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao. Cream: Consistency regularized self-rewarding language models. arXiv preprint arXiv:2410.12735, 2024d. Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang. Gridmm: Grid memory map for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1562515636, 2023d. Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, and Shuqiang Jiang. Lookahead exploration with neural radiance representation for continuous vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1375313762, 2024e. Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, In Proceedings of and Yu Qiao. Scaling data generation in vision-and-language navigation. the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1200912020, October 2023e. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. Fei Xia, Amir Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson In Proceedings of the IEEE Conference on env: Real-world perception for embodied agents. Computer Vision and Pattern Recognition, pp. 90689079, 2018. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. Advances in Neural Information Processing Systems, 36, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. Haitian Zeng, Xiaohan Wang, Wenguan Wang, and Yi Yang. Kefa: knowledge enhanced and finegrained aligned speaker for navigation instruction generation. arXiv preprint arXiv:2307.13368, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language In Proceedings of the IEEE/CVF International Conference on Computer image pre-training. Vision, pp. 1197511986, 2023. Yue Zhang and Parisa Kordjamshidi. Explicit object relation alignment for vision and language In Proceedings of the 60th Annual Meeting of the Association for Computational navigation. Linguistics: Student Research Workshop, pp. 322331, 2022a. Yue Zhang and Parisa Kordjamshidi. Lovis: Learning orientation and visual signals for vision and In Proceedings of the 29th International Conference on Computational language navigation. Linguistics, pp. 57455754, 2022b. Yue Zhang and Parisa Kordjamshidi. Vln-trans: Translator for the vision and language navigation agent. arXiv preprint arXiv:2302.09230, 2023. Yue Zhang and Parisa Kordjamshidi. Narrowing the gap between vision and action in navigation. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 856865, 2024. 18 Preprint Yue Zhang, Quan Guo, and Parisa Kordjamshidi. Navhint: Vision and language navigation agent with hint generator. arXiv preprint arXiv:2402.02559, 2024a. Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, and Parisa Kordjamshidi. Vision-and-language navigation today and tomorrow: survey in the era of foundation models. arXiv preprint arXiv:2407.07035, 2024b. Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong Yang, Haibing Ren, Huaxia Xia, and Si Liu. Target-driven structured transformer planner for vision-language navigation. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 41944203, 2022. Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. Towards learning generalist model for embodied navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1362413634, 2024. Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pp. 260278, 2024a. Gengze Zhou, Yicong Hong, Zun Wang, Chongyang Zhao, Mohit Bansal, and Qi Wu. Same: Learning generic language-guided visual navigation with state-adaptive mixture of experts. arXiv preprint arXiv:2412.05552, 2024b. Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024c. Fengda Zhu, Xiwen Liang, Yi Zhu, Qizhi Yu, Xiaojun Chang, and Xiaodan Liang. Soon: Scenario oriented object navigation with graph-based exploration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1268912699, 2021. Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, and Fei Sha. Babywalk: Going farther in vision-and-language navigation by taking baby steps. arXiv preprint arXiv:2005.04625, 2020. 19 Preprint"
        },
        {
            "title": "A APPENDIX",
            "content": "We first present additional implementation details of our experiments in Section A.1, including specifics of the generator, navigator model, and training procedures. Section A.2 illustrates the SRDF pipeline with pseudo code, while Section A.4 provides further details and experiments regarding our trajectory-encoding template design. Details of datasets and evaluation metrics are provided in Section A.5, with more comprehensive downstream results shown in Section A.6. Finally, Section A.7 presents detailed examples of our generated instructions in comparison with other baselines. A.1 ADDITIONAL IMPLEMENTATION DETAILS Navigator. We use DUET model (Chen et al., 2022b) as our navigator, which integrates global and local information through dual-scale graph transformer architecture. This architecture processes both high-level scene representations as well as detailed local features simultaneously, enhancing the models capability to interpret language instructions in complex visual contexts. By constructing topological map in the meanwhile, DUET extends the navigational action space from the current viewpoint to all navigable directions encountered, thus improving planning and error correction. The model employs attention mechanisms to balance global scene contexts with local observations, significantly improving navigation accuracy towards targets based on natural language commands. Generator. We use Mantis (Jiang et al., 2024) as our base model for instruction generation. Mantis comprises SigLIP vision encoder (Zhai et al., 2023), multimodal projector, and LLaMA-38B-Instruct language model backbone (Dubey et al., 2024). It is first pre-trained on multimodal projector data using LLaVa pre-training, followed by fine-tuning on the multi-image interleaved Mantis-Instruct dataset.For our task, we initialize the instruction-tuned model, Mantis-8B-siglipllama3, and apply updates only to the added LoRA layers in the language backbone. Training Details. The navigator is trained using similar objectives as in (Wang et al., 2023e), including Masked Language Modeling and Single Action Prediction. We initialize our model with LXMERT (Tan & Bansal, 2019) for the first and second rounds, and with our round-2 pre-trained model for the third round. During generator training, all parameters except the injected LoRA layers are frozen, and only the LoRA layers are fine-tuned. For navigator training, we initialize the instruction generator with Mantis in the first and second rounds and use the round-2 trained generator directly for round 3. Additionally, for downstream task supervisions, we encourage the model to go back to the viewpoint on the GT path yielding best nDTW to the current progress for R4R and RxREnglish as their trajectories are not shortest-path, while we use shortest-path supervision as teacher action for other tasks. A.2 PSEUDO CODE OF SRDF Alg. 1 provides detailed pseudo code for our Self-Refining Data Flywheel (SRDF), illustrating the process described in Section 3.2. Specifically, the pipeline starts with training an initial instruction generator using seed data. The generator creates training data for the navigator, which is then used to train base navigator. The navigator, in turn, filters high-quality data to further improve both itself and the generator in subsequent rounds. This iterative process continues for iterations, refining data quality and improving model robustness with each cycle. A.3 SFT DATA TEMPLATE We use the template shown in Figure 3 to construct our SFT data for fine-tuning the instruction generator, which encodes trajectory into an interleave image-action sequence. At each viewpoint, we include key views, such as the view when arriving at the viewpoint and the view when leaving it. For each view, we also append the corresponding action in raw text after the image tokens, creating multi-image interleaved format to effectively encode the trajectories. The output is the corresponding instruction of the input trajectory. 20 Preprint [Model Input] This is navigation trajectory consists of several image sequences: (Viewpoint 1: Image:<image>, Action: left (135.71 degree) and up (0.00 degree)), (Viewpoint 1: Image:<image>, Action: forward), (Viewpoint 2: Image:<image>, Action: left (33.14 degree) and up (0.00 degree)), ... (Viewpoint 5: Image:<image>, Action: forward), (Viewpoint 6: Image:<image>, Action: stop). Could you give me its corresponding navigation instruction in details? [Model Output] Start by exiting the bedroom into the hallway. Turn right and proceed down the hall until you reach the first door on your left. Enter this room, which is the library. Once inside, walk towards the large bookshelf directly across from the doorway. Stop there and face towards the window on the right side of the room. Figure 3: Interleaved trajectory prompt and expected response for training our instruction generator. The LLMs response is the corresponding instruction of this path. A.4 ADDTIONAL EXPERIMENTS Fourth-Round Generator-Training. Our method demonstrates the potential for continued performance improvement with additional refinement rounds. To illustrate this, we conducted fourthround generator training experiment (which is equivalent to the round 3 w/ DG 4 results in Table 9) Following similar procedure, we generated DG 4 , trained the generator using this data, and observed consistent performance gains over the third round. Results in Table 10 show new stateof-the-art (SoTA) SPICE score of 26.2, marking an improvement of +2.8 over the previous SoTA (as shown in Table 9). These results highlight the scalability and effectiveness of our approach, indicating that with sufficient computational resources and time, further rounds of improvement can be sustained. Effect of Different Scoring Metrics in Generator Training. We conducted additional experiments to demonstrate that classical methods for language model self-improvement face limitations in the VLN context without reliable feedback from the navigator. Specifically, we evaluated two approaches: (1) self-score, self-rewarding method where the language model scores its own outputs, and (2) CLIP-score, which uses an external tool (CLIP) to provide similarity scores. In these experAlgorithm 1 Pipeline of Self-Refining Data Flywheel (SRDF) Require: Seed data DSeed (Human-annotated), Unlabelled trajectories DT raj, Total iterations . 2 from DG 2 for DT raj. 1 via greedy decoding 1 and gen-training data DG 1 is generated via random sampling while DG 1: Train base instruction generator G1 with DSeed. 2: Use G1 to generate nav-training data DN 3: /* DN 4: Train base navigator N1 with DN 1 . 5: Use N1 to filter high-quality subsets DG 6: for each iteration p1 ă ď do 7: 8: 9: 10: 11: 12: 13: 14: 15: end for ăt to form DN /* Note: Seed data DSeed is used in training stages of both Gt and Nt but omitted for simplicity Train generator Gt with DG . Use Gt to generate nav-training data DN for LDN /* DN is generated via random sampling while DG and DN Combine DN ăt to form DN . Train navigator Nt with DN . Use Nt to filter high-quality subsets DG and DN Combine DN and gen-training data DG t`1 via greedy decoding t`1 and DN t`1 from DG t`1 for DT raj. from DN . ă2 from DN 1 . 2 and DN ăt`1. Preprint iments, conducted during round 1, instructions were scored and the top 300K instructions filtered using either (1) or (2) were used to train the instruction generator in round 2. The results in Table 11 showed that neither self-scores nor CLIP scores yielded significant improvement over the round 1 baseline. In contrast, our navigator-filtering method using nDTW demonstrated substantial gains, highlighting the challenges of providing effective feedback and emphasizing the effectiveness of our approach. Table 10: Generator results in the additional fourth round. Method Baseline Ours (round 1) Ours (round 2) Ours (round 3) Ours (round 4) SPICE Bleu-1 Bleu-4 CIDEr Meteor Rouge 21.8 23.7 25.2 25.7 26. 72.5 71.4 73.7 74.5 75.3 27.7 29.5 31.0 30.8 31.1 42.2 46.5 50.7 49.7 49.2 23.6 23.1 24.2 24.5 25.0 49.0 50.2 51.3 51.3 51.4 Table 11: Second-round generator performance with different scorers. scorer round 23.7 round 1 - 23.6 round 2 Self-score round 2 CLIP-score 23.9 round 2 navigator-nDTW 25.2 SPICE Bleu-1 Bleu-4 CIDEr Meteor Rouge 71.4 71.3 70.6 73.7 29.5 29.4 30.0 31.0 46.5 46.4 48.6 50. 23.1 23.5 23.1 24.2 50.2 50.3 50.4 51.3 Table 12: Two-round generator performance with mPLUG-Owl. Method Baseline mPLUG-owl (round 1) mPLUG-owl (round 2) SPICE Bleu-1 Bleu-4 CIDEr Meteor Rouge 21.8 22.7 24. 72.5 70.3 72.2 27.7 28.0 29.1 42.2 44.4 45.2 23.6 23.0 23.7 49.0 49.1 50.0 Effect of Different MLLM To demonstrate that our model-boosting process is not reliant on Mantis (Jiang et al., 2024), we conducted additional experiments with weaker multimodal large language model (MLLM), mPLUG-Owl-7B (Ye et al., 2023). Using the same methodology, we applied the flywheel process and completed the first two rounds of generator training. In Table 12, we observed significant improvement in the round 2 generators performance when trained on its data filtered by the navigator, highlighting the navigators critical role in enhancing the generator. Given that the reciprocal improvement of the navigator by the generator has been validated in prior work using the Speaker-Follower framework, these results strongly support our assertion that the model-boosting process is robust and not tied to specific MLLM. Effect of Different Encoding Formats. In our previous discussion, we hypothesized that the interleaved image-text understanding ability is crucial for training instructor generator based on pre-trained MLLMs. Some prior works (Li & Bansal, 2023; Kong et al., 2024) use only image information to build an image sequence for fine-tuning the VLM, but we argue that this approach loses important directional clues. Additionally, if action information is added without an interleaved format (i.e., an action sequence followed by an image sequence), the model may struggle to reason effectively between the two sequences. We verify this hypothesis in Table 13, using two baselines: (1) an image-only sequence (Figure 3 without action descriptions) and (2) an action sequence followed by an image sequence (Figure 3 with actions listed after the image sequence). Our results show that using only an image sequence leads to much lower performance, primarily because the model finds it difficult to infer actions between key frames. For the image-sequence + action-sequence format, it still underperforms compared to our interleaved image-text sequence template, likely due to challenges in reasoning across 22 Preprint Table 13: Effect of different trajectory-encoding templates. Templates R2R Validation Unseen SPICEÒ SPICE-DÒ Bleu-1Ò Bleu-4Ò CIDErÒ MeteorÒ RougeÒ Image Seq. Image Seq. + Action Seq. Interleave Image-Action Seq. 21.8 22.9 23.7 24.4 27.1 28.4 68.7 70.4 71.4 25.7 29.1 29. 46.5 46.5 46.5 21.9 22.9 23.1 48.3 50.2 50.2 separate sequences. In contrast, the Interleaved Image-Action Sequence performs best, demonstrating its effectiveness in trajectory encoding, which is used in our experiments. A.5 DETAILS OF DATASETS AND EVALUATION METRICS Datasets. We conduct our downstream experiments on 7 datasets listed below. R2R: Consists of 22k human-annotated navigational instructions, each describing trajectory that traverses multiple rooms in MP3D. On average, an instruction contains 32 words, and each ground-truth path is formed by seven nodes with total length of 10 meters. REVERIE: Inherits the trajectories in R2R but provides high-level instructions that describe target object. The task for an agent is first to find the object and then localize it in the observation. SOON: Provides instructions describing target rooms and objects. The average length of instructions is 47 words. SOON does not provide object bounding boxes and requires the agent to predict object center locations in the panorama. We use an automatic object detector to obtain candidate object boxes. The length of expert paths ranges from 2 to 21 steps, with an average of 9.5 steps. CVDN: Provides dialogues between navigator who tries to find target by asking for guidance and an oracle with privileged view of the best next step. The agent must find the way by interpreting the dialogue history. R2R-CE: Transfers the discrete trajectories in R2R to continuous 3D scans rendered by the Habitat simulator, where an agent can freely travel in the open space and interact with obstacles. The dataset contains 16k instruction-trajectory pairs after removing non-transferable paths. RxR: An extension of R2R that addresses shortest path biases and includes more object references. We use the English segment of RxR, which consists of 42,002 instructions, averaging 108 words per instruction. R4R: Created by concatenating adjacent paths in the Room-to-Room dataset. The ground-truth path is not the shortest path, encouraging the agent to follow the instructions to reach the target rather than exploit environment bias to navigate the shortest route. Detailed Evaluation Metrics. (1) Success Rate (SR), which measures whether the agent stops within 3 meters of the target; (2) Success Rate Weighted by Path Length (SPL), which penalizes inefficient, longer paths; (3) Goal Progress (GP), which calculates the agents progress toward the target; (4) Navigation Error (NE), which is the average distance between the agents final position and the target in meters; (5) normalized Dynamic Time Warping (nDTW), which measures stepwise alignment between the ground truth and the agent-predicted path; (6) Success Rate Weighted by Dynamic Time Warping (sDTW); (7) Coverage weighted by Length Score (CLS); (8) Remote Grounding Success (RGS), the proportion of successfully executed instructions; and (9) RGS Penalized by Path Length (RGSPL). Metrics (7) to (9) are used only in the detailed results provided in the appendix, with (8) and (9) specifically evaluating object grounding in REVERIE and SOON. A.6 DETAILED RESULTS In this section, we present detailed results for our downstream navigators, across multiple datasets: R2R  (Table 18)  , R4R  (Table 14)  , RxR-English  (Table 15)  , REVERIE  (Table 17)  , and SOON datasets  (Table 16)  . Specifically, on the R2R dataset, our model not only demonstrates improved generalizability to unseen environments but also achieves higher success rates and SPL in seen environments, surpassing previous state-of-the-art (SoTA) approaches by over 3% in SR and 4% in SPL. Our method achieved more than 10% improvement in both SR and sDTW on the R4R dataset. Besides, on the RxR English dataset, our approach significantly enhances SPL and sDTW in addition to SR and nDTW, elevating the state-of-the-art SPL to 69.2% and sDTW to 66.3%. Lastly, on REVERIE 23 Preprint and SOON datasets, our navigator not only enhances the agents navigation performance significantly but also substantially improves their grounding capabilities, improving RGSPL by 0.4% on REVERIE test set, and 1.7% on SOON test set compared with previous SoTA approaches. Notably, our navigator relies solely on pretraining with Masked Language Modeling (MLM), and Single Action Prediction (SAP) objectives on R2R datasets and the augmentation dataset collected with our data flywheel. This is in contrast to other approaches that additionally employ an Object Grounding (OG) objective. The superior performance indicates the strong instruction-trajectory alignment in our high-quality data, which is crucial for effectively learning object grounding from scratch during fine-tuning. Table 14: Comparison of single-run performance on R4R dataset. Methods Validation Unseen NEÓ SRÒ CLSÒ nDTWÒ sDTWÒ 6.09 44.6 57.7 HAMT (Chen et al., 2021) 6.07 45.0 45.0 LOVIS (Zhang & Kordjamshidi, 2022b) VLN-Trans (Zhang & Kordjamshidi, 2023) 5.87 46.0 45.0 PanoGen (Li & Bansal, 2023) BSG (Liu et al., 2023) NavHint (Zhang et al., 2024a) VER (Liu et al., 2024) SRDF (Ours) 6.02 47.8 6.12 47.0 59.0 6.04 46.0 45.0 6.10 47.0 61.0 4.21 64.4 61.0 - 50.3 43.0 - - 53.0 - 54.0 56. 31.8 23.0 25.0 - 34.0 25.0 33.0 44.6 Table 15: Comparison of single-run performance on RxR English dataset. : indicates the results are reproduced using their officially released checkpoints. Methods Landmark-RxR (He et al., 2021) HAMT: (Chen et al., 2021) MARVAL (Kamath et al., 2022) PRET: (Lu et al., 2024) MAGIC-L (Wang et al., 2024b) BEVBert: (An et al., 2023a) GOAT (Wang et al., 2024a) SRDF (Ours) NEÓ - - 3.31 2.68 - - - 1. SRÒ 64.1 59.4 74.0 77.1 81.3 - 74.1 82.9 Validation Seen SPLÒ sDTWÒ nDTWÒ NEÓ - - - 71.8 77.5 - 68.1 77.7 53.1 50.9 66.7 67.1 69.2 - 61.4 75.6 63.4 65.3 77.5 77.5 76.6 - 71.0 83.4 - - 4.47 3.36 - 4.2 - 2.61 SRÒ 40.1 56.5 64.7 71.0 72.9 66.7 68.2 78. Validation Unseen sDTWÒ SPLÒ nDTWÒ - - - 63.6 65.4 61.1 61.7 69.2 27.5 48.3 57.1 63.5 58.7 57.0 56.6 66.3 40.3 63.1 70.5 70.9 68.1 68.6 67.1 74. Table 16: Comparison of single-run performance on SOON dataset. Methods Navigation SRÒ OSRÒ SPLÒ Grounding RGSPLÒ Navigation SRÒ OSRÒ SPLÒ Grounding RGSPLÒ Validation Unseen Test Unseen DUET (Chen et al., 2022b) AutoVLN (Chen et al., 2022c) KERM (Li et al., 2023b) NaviLLM (Zheng et al., 2024) GOAT (Wang et al., 2024a) SRDF (Ours) 50.9 53.2 51.6 - 54.7 59.6 36.3 41.0 38.1 38.3 40.4 50.3 22.6 30.7 23.2 29.2 28.1 41.7 3.8 4.1 4.0 - 5.9 5.1 43.0 48.7 - - 50.6 51. 33.4 40.4 - 35.0 40.5 46.6 21.4 27.8 - 26.3 25.2 37.9 4.2 5.1 - - 6.1 8.4 24 Preprint Table 17: Comparison of single-run performance on REVERIE datasets. Methods Validation Unseen Test Unseen Navigation SRÒ Grounding SPLÒ RGSÒ RGSPLÒ OSRÒ Navigation SRÒ OSRÒ Grounding SPLÒ RGSÒ RGSPLÒ HAMT (Chen et al., 2021) DUET (Chen et al., 2022b) BEVBert (An et al., 2023a) AutoVLN (Chen et al., 2022c) KERM (Li et al., 2023b) BSG (Liu et al., 2023) ScaleVLN (Wang et al., 2023e) MiC (Qiao et al., 2023b) NaviLLM (Zheng et al., 2024) VER (Liu et al., 2024) GOAT (Wang et al., 2024a) VLN-Colipot (Qiao et al., 2024) SRDF (Ours) 36.8 51.1 56.4 62.1 55.2 58.1 63.9 62.4 53.7 61.1 - 62.6 72.2 33.0 47.0 51.8 55.9 50.4 52.1 57.0 57.0 44.6 56.0 53.4 57.4 60. 30.2 33.7 36.4 40.9 35.4 35.6 41.8 43.6 36.6 39.7 36.7 43.6 45.4 18.9 32.2 34.7 36.6 34.5 35.4 - 37.5 - 33.7 38.4 38.9 37.3 17.3 23.0 24.4 26.8 24.5 24.2 - 28.7 - 23.7 26.1 29.8 27.8 33.4 56.9 57.3 - 57.6 62.8 62.7 62.4 56.2 62.2 - 63.3 66.2 30.4 52.5 52.8 - 52.4 56.5 56.1 55.7 43.5 56.8 57.7 57.8 61.4 26.7 36.1 36.4 - 39.2 38.7 39.5 42.0 34.4 38.8 40.5 42.3 47. 14.9 31.9 32.1 - 32.4 33.2 - 35.3 - 33.9 38.3 36.6 35.6 13.1 22.1 22.1 - 23.6 22.3 - 26.2 - 23.2 26.7 26.6 27.1 Table 18: Comparison of single-run performance on R2R dataset. Methods Validation Seen Validation Unseen NEÓ OSRÒ SRÒ SPLÒ NEÓ OSRÒ SRÒ SPLÒ NEÓ OSRÒ SRÒ SPLÒ Test Unseen - Human 6.01 Seq2Seq (Anderson et al., 2018b) 3.36 Speaker Follower (Fried et al., 2018) 3.53 RCM (Wang et al., 2019) 3.99 EnvDrop (Tan et al., 2019) 3.67 PREVALENT (Hao et al., 2020) 3.47 EntityGraph (Hong et al., 2020) 3.44 NvEM (An et al., 2021) 3.10 SSM (Wang et al., 2021) 2.68 AirBert (Guhur et al., 2021) 2.90 VLNœBERT (Hong et al., 2021) 2.51 HAMT (Chen et al., 2021) 2.48 EnvMix (Liu et al., 2021a) - SnapEnsemble (Qin et al., 2021) - EXOR (Zhang & Kordjamshidi, 2022a) 3.56 SEvol (Chen et al., 2022a) 2.99 MARVAL (Kamath et al., 2022) 2.40 LOVIS (Zhang & Kordjamshidi, 2022b) 2.33 HOP+ (Qiao et al., 2023a) 2.34 TD-STP (Zhao et al., 2022) 2.28 DUET (Chen et al., 2022b) 2.12 ScaleVLN (Wang et al., 2023e) BEVBert (An et al., 2023a) 2.17 VLN-Trans (Zhang & Kordjamshidi, 2023) 2.45 Lily (Lin et al., 2023) NaviLLM (Zheng et al., 2024) NavHint (Zhang et al., 2024a) NavGPT-2 (Zhou et al., 2024a) SAME (Zhou et al., 2024b) VER (Liu et al., 2024) MAGIC-L (Wang et al., 2024b) GOAT (Wang et al., 2024a) SRDF (Ours) - - - 2.84 - - 1.73 1.79 1.54 - 53 74 75 - - - - 80 - - - - - - - - - - 83 86 87 88 - - - - 83 - - 89 89 91 - 7.81 6.62 6.09 5.22 4.71 4.73 4.27 4.32 4.10 3.93 2.29 3.89 3.63 - 3.99 4.06 3.71 3.49 3.22 3.31 2.09 2.81 3.34 2.90 3.51 3.23 2.84 2.73 2.80 2.22 2.40 1.62 - 28 45 50 - - - - 73 - - - - - - - - - - 76 81 88 84 - - - - 84 - - 86 85 - 21 36 43 52 58 57 60 62 62 63 66 64 67 52 62 65 65 67 70 72 81 75 69 74 67 69 74 76 76 79 78 86 - - - - 48 53 53 55 45 56 57 61 58 60 49 57 61 59 61 63 60 70 64 63 62 59 65 61 66 65 70 68 79 1.61 7.85 6.62 6.12 5.23 5.30 4.75 4.37 4.57 4.13 4.09 3.93 3.87 3.82 - 4.13 4.18 4.07 3.71 3.73 3.65 2.27 3.13 3.94 3.44 3.71 4.00 3.33 3.03 2.74 2.75 3.04 1.82 90 27 - 50 59 61 61 66 70 - 70 72 72 - - - 67 - - 72 76 86 81 - - - - 80 - - 82 80 89 86 20 35 43 51 54 55 58 61 62 63 65 65 65 49 62 62 63 66 67 69 80 73 66 72 68 65 72 74 76 77 75 85 76 - 28 38 47 51 52 54 46 57 57 60 59 60 46 57 58 58 60 61 59 70 62 60 60 60 60 60 64 66 69 65 - - - - 59 65 65 65 62 70 68 72 72 - 58 63 69 72 73 73 73 75 74 72 - - - 63 - - 79 79 83 - 39 66 67 62 69 67 69 71 75 72 76 75 - 60 67 73 77 78 77 79 81 81 77 - - - 74 - - 84 84 87 25 Preprint A.7 QUALITATIVE CASE STUDY OF GENERATED INSTRUCTIONS In Figure 4 and 5, we visualized some examples of our generated instructions, and compare them with Prevalent (Hao et al., 2020) and ScaleVLN (Wang et al., 2023e) baselines. All the example trajectories in Figure 4, and Figure 5 (a), (b) are collected using recovered environment images from ScaleVLN (Wang et al., 2023e), while Figure 5 (c), (d) are from MP3D environments. Rare-Room/Landmark Recognition Ability. Figure 4 demonstrates the strong image-text understanding capability of our instruction generator. Specifically, our generator can recognize rare objects, such as dentist chair/room or grandfather clock, thanks to our interleaved image-action trajectory-encoding design. This design preserves the original abilities of the pretrained MLLM while effectively encoding trajectories to generate instructions with rich and accurate landmarks. In contrast, the baseline instruction generator fails to capture these rare concepts due to its from-scratch training paradigm. Instead, it only generates some general landmarks with weak clues. Detailed Object-Describing Ability. Figures 4 (c) and (d) illustrate that our instruction generator can describe key objects along the path with greater detail. For instance, while the baseline mentions the bar and the painting successfully, our generator provides more specifics, such as brown leather bar stool and large painting on the wall. Such detailed descriptions are crucial for helping the navigator learn richer visual cues. Additionally, in Figure 4 (d), our generator performs slight spatial reasoning between objects, resulting in more precise stopping guidance the blue and white throw pillows on the right side of the couch. Generalization to Outdoor Environments. In Figure 5 (a), (b), we demonstrate the ability of our generator to produce some useful instructions for outdoor environments, even though the model is training using instructions from indoor environments. For instance, in (a), our generated instruction identifies the glass doors leading outside, which is more distinct the ScaleVLNs table still general landmark without strong viewpoint-specific clue. In (b), the generator successfully identifies outdoor landmarks including the car and the buches, while the baseline only knows walkway. OCR Ability. Surprisingly, our instruction generator demonstrates interesting OCR capabilities, as shown in Figures 5 (c) and (d). In example (c), the generator successfully identifies the words cape and plug on the wall, while in example (d), it even identifies full sentenceLets start to redefine how work gets done. This OCR ability is likely inherited from the pre-trained MLLM, and our fine-tuning approach effectively retains this capability, resulting in highly detailed and accurate guidance in the generated instructions. Idiomatic Expressions. Our generator sometimes uses idiomatic expressions in its instructions. An example is shown in Figure 5 (c), Sample 2, where the generator says, go past the desk then stop at the end of the rope. The phrase at the end of the rope usually means that someone has reached the limit of their patience or endurance. In this context, however, it refers to reaching the farthest point that the navigator can proceedlikely the wall. This ability adds diversity to the instructing style, making the generated instructions more varied and engaging. Preprint Figure 4: Visualization of generated instructions. 27 Preprint Figure 5: Visualization of generated instructions."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "UNC Chapel Hill"
    ]
}