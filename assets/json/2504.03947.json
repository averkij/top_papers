{
    "paper_title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking",
    "authors": [
        "Chris Samarinas",
        "Hamed Zamani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 4 9 3 0 . 4 0 5 2 : r Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking Chris Samarinas University of Massachusetts Amherst Amherst, MA, United States csamarinas@cs.umass.edu Hamed Zamani University of Massachusetts Amherst Amherst, MA, United States zamani@cs.umass.edu ABSTRACT We present novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as reinforcement learning problem and incentivizing explicit reasoning capabilities, we train compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more eÔ¨Äective reasoning with smaller language models. The self-supervised nature of our method oÔ¨Äers scalable and interpretable solution for modern information retrieval systems."
        },
        {
            "title": "1 INTRODUCTION\nSearch engines and retrieval-augmented generation systems increas-\ningly face queries that require complex reasoning and multi-step\nsynthesis and analysis. They demand a deeper understanding of\nthe query and the documents to identify the connections between\nthem. For example, Ô¨Ånding documentation for a coding error re-\nquires understanding program logic and syntax, and identifying\neconomic case studies that share underlying theoretical principles\ndemands sophisticated domain knowledge and analytical reason-\ning [29]. Traditional approaches to training ranking models for\nsuch complex tasks often rely on expensive human annotations\nto provide relevance judgments and explanations. In contrast, we\npresent a framework that automatically generates its own training\nsignal by leveraging existing question-answer pairs on the Web.",
            "content": "Although neural ranking models have made signiÔ¨Åcant progress in recent years [7, 13, 21, 22, 27] and led to substantial performance gains on standard benchmarks such as MS MARCO [19] and the TREC Deep Learning (DL) Track [3, 4], we observe that they often struggle with reasoning-intensive queries that demand deeper understanding and explicit justiÔ¨Åcation of relevance decisions. For instance, state-of-the-art dense retrievers that achieve strong performance on TREC DL show signiÔ¨Åcant degradation on reasoning-intensive queries, with the best models achieving only about 18% nDCG@10 on the BRIGHT benchmark [29]a recent benchmark designed for reasoning-intensive ranking tasks. We argue that ranking models must engage in deliberate reasoning to bridge the gap between query intent and document content. Recent work has suggested that large language models with tens of billions of parameters can eÔ¨Äectively serve as zero-shot rerankers [23, 24, 30], demonstrating strong reasoning capabilities across diverse domains. However, deploying these models at scale remains challenging due to their computational requirements and latency constraints. While smaller models oÔ¨Äer practical advantages, they typically lack the sophisticated reasoning abilities of their larger counterparts. Recent LLMs such as DeepSeek R1 [5] have demonstrated that encouraging models to learn explicit reasoning strategies and leveraging inference-time compute for stepby-step analysis can signiÔ¨Åcantly improve performance on complex tasks. While this has been demonstrated for language modeling and generation tasks, exploring these principles in retrieval remains understudied. Our work shows that by decomposing document relevance assessment into explicit reasoning steps and optimizing for high-quality explanations, we can achieve strong performance even with relatively compact models. In more detail, our work introduces framework for distilling and reÔ¨Åning reasoning capabilities in small language models for reasoning-intensive ranking. Our approach does not require any manually labeled data for training; instead, we perform diverse data scraping approach from the Web for collecting reasoning intensive questions and pseudo-labeling approach using teacher LLM (with 70B parameters), resulting in dataset with 20K examples. We then introduce knowledge distillation approach that helps compact student LLM (with 3B parameters) to mimic the reasoning and labeling capability of the teacher. Subsequently, we introduce reinforcement learning approach that reÔ¨Ånes these reasoning capabilities by rewarding high-quality explanations and accurate relevance predictions. Through this approach, we demonstrate that our eÔ¨Écient 3B parameter model achieves performance comparable to 70B+ parameter models on reasoning-intensive ranking tasks. Most notably, our model ranks third on the BRIGHT benchmark leaderboard and is the Ô¨Årst eÔ¨Äective ranking model under 8B parameters, with the only models achieving better performance being 70B zero-shot ranker using GPT-4 for query reformulation and JudgeRank, an ensemble of three LLMs (8B, 70B, and 405B parameters). Our 3B parameter model outperforms all other baseline methods on the BRIGHT benchmark, including recent approaches like Reason-toRank [12] which uses an 8B parameter model, while using almost three times fewer parameters and avoiding complex query rewriting or multi-step prompting strategies. We release our code and data for improved reproducibility.1 1https://github.com/algoprog/InteRank SIGIR25, July 2025, Padova, Italy"
        },
        {
            "title": "2.1 Model Architecture\nWe adopt a two-stage ranking architecture that is common in mod-\nern search systems: an eÔ¨Écient Ô¨Årst-stage retrieval followed by\nmore expressive reranking model capable of reasoning.",
            "content": "First-stage Retrieval: lightweight sparse or dense retrieval model is used to retrieve potentially relevant documents from the corpus. To better understand the impact of retrieval quality on the Ô¨Ånal ranking performance, we experiment with various retrievers, including BM25 [26] and dense embedding models (see section 3). We retrieve the top 100 documents for re-ranking. While optimizing the Ô¨Årst-stage retriever is important, in this work we focus on improving the second-stage re-ranking component. Second-stage Re-ranking: Various learning-to-rank models, from traditional feature-based [2, 17] to transformer-based crossencoder models [8, 21, 22], have been used for reranking. We aim at training reranking model for eÔ¨Äective reasoning-intensive tasks. To do so, we train language model that takes query-document pair at time and generates some reasoning to analyze and describe whether and how the provided document is relevant. These reasoning steps are then followed by discrete relevance label as the Ô¨Ånal generation token. This relevance label is either 0 (i.e., nonrelevant), 1 (partially relevant), or 2 (highly relevant). This stage is crucial for complex reasoning tasks, as it allows deeper analysis of document content in relation to the query intent. Since the scores produced by our reranker are discrete, many documents are assigned the same relevance score, and we cannot distinguish them in ranking. Therefore, we employ hybrid scoring strategy that combines the generated discrete reranking score with the retrieval score produced in the Ô¨Årst-stage retrieval. In fact, score(ùëû, ùëë) = retrieval score(ùëû, ùëë) + ùõº reranking score(ùëû, ùëë) where ùëû and ùëë denote query and document and ùõº R+ is hyperparameter controlling the impact of re-ranking score. ùõº is expected to be relatively high number 1. (ùõº = 100 in our experiments)."
        },
        {
            "title": "2.2 Model Optimization\nOur training process combines knowledge distillation [10] with\nreinforcement learning (RL). Following recent work showing the\nbeneÔ¨Åts of incentivizing explicit reasoning capabilities through RL\n[5], we structure our approach to encourage the development of\neÔ¨Äective reasoning patterns while maintaining computational eÔ¨É-\nciency. The process consists of three phases:",
            "content": "1. Synthetic Data Generation: High-quality training data is crucial for developing models that can handle diverse reasoning patterns. However, obtaining human annotations for reasoningintensive ranking is expensive and time-consuming. We address this challenge through an automated data generation process that leverages existing question-answer pairs from social websites like StackExchange. Our data generation pipeline, summarized in Algorithm 1, starts with seed set of query-answer pairs C. In our experiments, we sampled 20K pairs round robin from 186 diÔ¨Äerent communities on StackExchange. For each answer, we extract linked documents (hyperlinks) that potentially contain supporting evidence, establishing an initial set of query-document pairs. To increase diversity and to source potential negative documents, we use teacher LLM to generate related queries and retrieve additional documents through web search using the Brave Search API. The teacher model is then instructed to generate an explanation and discrete relevance label for each query-document pair, creating distillation dataset. This approach naturally captures diverse reasoning patterns since the teacher model must explain how different types of evidence support or fail to support answers across technical domains - from code analysis to scientiÔ¨Åc explanations. The explanations demonstrate diÔ¨Äerent forms of reasoning like logical deduction, causal analysis, and domain-speciÔ¨Åc technical reasoning. The next two phases are summarized in Algorithm 2. 2. Knowledge Distillation: We Ô¨Årst transfer knowledge from large zero-shot teacher model to more compact student model through supervised Ô¨Åne-tuning. We use Llama 3.2 3B [9] as our base student model and as mentioned earlier, Llama 3.3 70B is used as our teacher model. The objective is to maximize the log likelihood of teacher-generated outputs: ùúÉ 1 = arg max ùúÉ E(ùëû,ùëë,ùëí,ùëô )ùê∑synth [log ùëùùúÉ (ùëí, ùëô ùëû,ùëë)] (1) where ùëí and ùëô denote an explanation and discrete relevance label. ùúÉ 1 is the trained student model parameters after knowledge distillation. This phase helps the student model learn some initial reasoning patterns. 3. Reinforcement Learning: While distillation helps transfer basic reasoning patterns from the teacher, it is limited to imitating single explanation path per example. In practice, there may be multiple valid ways to reason about document relevance. The reinforcement learning (RL) phase enables exploration of diverse reasoning strategies through sampling, with the reward model providing feedback to identify the most eÔ¨Äective explanations. For each query-document pair, we sample ùëò = 8 outputs from the model being trained (i.e., starting from the student model from Step 2) and evaluate them using reward model. We observed that the reward values can have very high variance, and they heavily depend on query complexity and domain. For this reason, we use relative reward values after max-min normalization for each set of outputs ÀÜùë¶ for given query-document input (ùëû, ùëë): (ùëû, ùëë, ÀÜùë¶) = (ùëû,ùëë, ÀÜùë¶) min(R) max(R) min(R) (2) where min(R) and max(R) are the minimum and maximum reward values for the given query-document input pair. High-quality Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking SIGIR25, July 2025, Padova, Italy Algorithm 1 Synthetic Data Generation for Ranking Input: Teacher LLM T, query-answer pairs = { (ùëûùëñ , ùëéùëñ ) }ùëÅ ùëñ=1 Output: Synthetic dataset ùê∑synth long-form questions requiring multi-step reasoning, while positive documents provide critical concepts, theories, or techniques needed to address the queries rather than direct answers. Extract linked documents ùê∑linked from ùëé for each ùëë ùê∑linked do (ùëí, ùëô ) (ùëû, ùëë ) ùê∑synth ùê∑synth { (ùëû, ùëë, ùëí, ùëô ) } 1: Initialize ùê∑synth 2: for each (ùëû, ùëé) do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: return ùê∑synth end for ùëÑgen (ùëû, ùëé, ùê∑linked ) Sample random ùëû ùëÑgen ùê∑web WebSearch(ùëû ) Sample random ùëë ùê∑web (ùëí, ùëô ) (ùëû, ùëë ) ùê∑synth ùê∑synth { (ùëû, ùëë, ùëí, ùëô ) } Generate explanation and label Generate related queries Get top-10 results Algorithm 2 LLM alignment for ranking Input: Student LLM MùúÉ , reward model R, synthetic dataset ùê∑synth Output: Trained model parameters ùúÉùëá +1 1: ùúÉ 1 arg maxùúÉ 2: for ùë° = 1 to ùëá do 3: 4: for each (ùëû, ùëë, ùëô ) in training data do E(ùëû,ùëë,ùëí,ùëô )ùê∑synth [log ùëùùúÉ (ùëí, ùëô ùëû, ùëë ) ] ùëò = 8 samples Sample ùëåùëû,ùëë = { ÀÜùë¶ ùëó }ùëò Compute rewards (ùëû, ùëë, ÀÜùë¶ ùëó ) for all ÀÜùë¶ ùëó Normalize rewards: = ùëó=1 ùëÄùúÉùë° (ùëû, ùëë ) Rmin(R) max(R) min(R) end for ùê∑ùë° = { (ùëû, ùëë, ÀÜùë¶ ùëó , (ùëû, ùëë, ÀÜùë¶ ùëó ) ) : (ùëû, ùëë, ÀÜùë¶ ùëó ) ùúè } ùúÉ ùë° +1 arg maxùúÉ [ ùëö (ùëû,ùëë, ÀÜùë¶,R )ùê∑ùë° log ùëùùúÉ ( ÀÜùë¶ ùëû, ùëë ) ] ùëö = 3 5: 6: 7: 8: 9: 10: end for 11: return ùúÉùëá +1 output samples ÀÜùë¶ ùëó are then selected using threshold ùúè: ùê∑ùë° = {(ùëû, ùëë, ÀÜùë¶ ùëó , R) : (ùëû, ùëë, ÀÜùë¶ ùëó ) ùúè } The model parameters are updated using scaled rewards to further emphasize higher-reward outputs: (3) ùúÉùë°+1 = arg max ùúÉ (ùëû,ùëë, ÀÜùë¶,R )ùê∑ùë° [(R (ùëû, ùëë, ÀÜùë¶))ùëö log ùëùùúÉ ( ÀÜùë¶ùëû, ùëë)] (4)"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "Evaluation Data. Our evaluation uses the BRIGHT benchmark [29], which spans diverse domains requiring complex reasoning capabilities. BRIGHT includes seven datasets from StackExchange communities (Biology, Earth Science, Economics, Psychology, Robotics, Stack OverÔ¨Çow, and Sustainable Living), each containing 100-200 expert-validated query-document pairs where relevance is determined by citations in accepted answers. The remaining 5 datasets focus on coding and mathematical reasoning: Pony (syntax documentation pairs), LeetCode (algorithmic problems), TheoremQA (theorem-based questions), AoPS (competition math problems), and Theorem Retrieval (problems paired with ProofWiki statements). In total, BRIGHT contains 1,384 queries with 6.37 positive documents per query on average. The queries are typically Experimental Setup. The base LLM for InteRank is Llama 3.2 3B, while Llama 3.3 70B is our teacher model [9]. We use QLoRA [6] for parameter-eÔ¨Écient Ô¨Åne-tuning, with 4-bit quantization of the base model and trainable rank-64 adapters. Due to resource constraints, we limit the context length to 4K tokens. Training is performed on single A100 GPU with an eÔ¨Äective batch size of 16 (batch size 1 with 16 gradient accumulation steps) using the AdamW optimizer with learning rate 2e-4. For the sampling of outputs in the RL stage, we use temperature 1.0 for nucleus sampling, reward threshold ùúè = 0.85, and reward scaling power ùëö = 3. We perform two epochs of RL training. For the reward model, we use pretrained Llama 3.1 8B model [16] that has demonstrated strong performance on RewardBench [14]. We found that this model has very high agreement with larger open-weight and commercial LLMs in relative comparison of explanation outputs, making it suitable for our training process. Baselines. We compare against diverse set of baseline models: (1) Traditional sparse retrieval using BM25 [26]; (2) Dense retrievers of varying sizes, from MSMARCO-trained models like TAS-B (66M) [11] to recent models like BGE (0.3B) [33], Instruction-tuned models Inst-L/XL [28], GTE-large (0.4B) [15], E5 (7B) [31], GritLM (7B) [18], and Qwen1.5 (7B) [1]; (3) Cross-encoder rerankers including MiniLM Ô¨Åne-tuned on MSMARCO [25] and ModernBERTlarge Ô¨Åne-tuned on our synthetic examples [32]; and (4) Zero-shot LLM rerankers using Llama 3.2 (3B) and Llama 3.3 (70B). These baselines represent the spectrum of current approaches, from lightweight traditional methods to LLMs."
        },
        {
            "title": "3.1 Experimental Results\nOur experimental results, shown in Table 1, reveal several key Ô¨Ånd-\nings about reasoning-intensive ranking:",
            "content": "1. Traditional dense retrievers with small number of parameters or training data fail in reasoning-intensive domains. Smaller dense retrievers trained on MSMARCO like TAS-B (66M) perform poorly with only 8.53% average nDCG@10, highlighting their limitations beyond simple semantic matching. This is particularly evident in reasoning-intensive domains like theorem-based tasks (1.51% on TheoT) and complex StackExchange queries (2.77% on Biology). In contrast, larger dense retrievers trained on more diverse data with 100M+ training examples, show signiÔ¨Åcant improvements; GTE-large (400M) achieves 18.0% and Qwen1.5 (7B) reaches 22.4% average nDCG@10, demonstrating the importance of model scale and training data for complex retrieval tasks. 2. Explanations are crucial for eÔ¨Äective ranking. As shown in Table 1, our ablation studies reveal that removing the explanation component (rows marked w/o expl.) causes accuracy to drop signiÔ¨Åcantly from 21.5% to 14.4% nDCG@10 on average. Traditional BERT re-ranking models that rely purely on semantic matching also show surprisingly poor performance, with ModernBERTL achieving only 4.57% average nDCG@10. This shows that the process of generating explanations helps develop better reasoning SIGIR25, July 2025, Padova, Italy"
        },
        {
            "title": "Chris Samarinas and Hamed Zamani",
            "content": "Table 1: Performance (nDCG@10) of diÔ¨Äerent retriever and reranker combinations on the BRIGHT benchmark. Our 3B parameter model InteRank, matches or exceeds the performance of the 70B teacher LLM, with explanations being crucial for eÔ¨Äectiveness (see \"w/o expl.\" ablations). Adding domain-speciÔ¨Åc relevance deÔ¨Ånitions (marked with \"+ instruct\") further improves performance. The symbol * indicates statistical signiÔ¨Åcance (paired t-test, < 0.05) compared to all baseline models. Retriever Re-ranker Bio. Earth. StackExchange Psy. Econ. Rob. Stack. Sus. Leet. Pony AoPS Coding Theorem-based TheoQ. TheoT. Avg. BM25 [26] BM25 BM25 BM TAS-B (66M) [11] BGE (0.3B) [33] Inst-L (0.3B) [28] GTE-L (0.4B) [15] GTE-L (0.4B) GTE-L (0.4B) GTE-L (0.4B) GTE-L (0.4B) GTE-L (0.4B) GTE-L (0.4B) GTE-L (0.4B) E5 (7B) [31] Inst-XL (1.5B) [28] GritLM (7B) [18] Qwen1.5 (7B) [1] Qwen1.5 (7B) Qwen1.5 (7B) Qwen1.5 (7B) Qwen1.5 (7B) Qwen1.5 (7B) Qwen1.5 (7B) - InteRank (3B) InteRank w/o expl. (3B) InteRank + instruct (3B) - - - - MiniLM-MARCO (33M) [25] ModernBERT-L (0.4B) [32] Llama3.2 (3B) [9] Llama3.3 (70B) [9] InteRank (3B) InteRank w/o expl. (3B) InteRank + instruct (3B) - - - - MiniLM-MARCO (33M) [25] ModernBERT-L (0.4B) [32] Llama3.2 (3B) [9] InteRank (3B) InteRank w/o expl. (3B) InteRank + instruct (3B) 19.2 34.3 15.1 36. 2.7 12.0 15.6 21.0 11.6 10.8 18.3 29.6 35.2 20.3 37.0 18.8 21.9 25.0 30.1 9.72 11.8 27.6 48.5 21.3 51.2* 27.1 44.2 20.2 45.0 Sparse retrieval model 14.9 15.8 12.1 16.3 12.5 18.9 10.2 19. 13.5 15.5 11.2 15.3 16.5 20.1 13.3 20.3 Dense retrieval models with < 1B parameters 10.2 24.2 21.5 31.1 5.7 7.1 22.5 37.2 45.7 19.7 46.5 6.4 16.6 16.0 20.5 5.2 7.2 11.1 23.1 24.1 14.4 24.8 5.6 17.4 21.9 24.3 6.4 7.4 17.5 30.7 27.4 16.1 28. 7.5 12.2 11.5 12.6 2.7 4.1 7.1 13.6 16.1 13.1 15.8 8.0 9.5 11.2 15.9 4.0 5.8 11.1 22.8 21.8 11.4 22.1 Dense retrieval models with > 1B parameters 26.0 34.4 32.8 38.3 6.21 8.1 30.3 50.6 25.6 51.4* 15.5 22.8 19.0 17.7 6.60 8.2 14.6 21.7 15.2 22.4* 15.8 27.4 19.9 23.7 6.72 8.4 19.5 30.3 16.8 31.9* 16.4 17.4 17.3 13.3 3.59 5.1 9.7 17.6 13.8 17.3 9.8 19.1 11.6 22.4 5.12 6.8 17.6 26.3 16.2 26.6* 15.2 21.6 12.8 23.7 4.1 13.3 13.2 15.3 5.5 7.0 12.6 20.5 20.8 13.8 23.0 18.5 18.8 18.0 14.6 6.25 8.0 11.9 20.2 15.1 22.4* 24.4 23.4 19.9 26. 24.7 26.7 20.0 28.3 4.0 5.8 18.6 23.7 22.0 19.6 25.3 28.7 27.5 29.8 25.5 5.11 6.8 25.4 21.3 22.4 24.5* 7.9 10.3 6.1 9.0 14.6 5.6 1.3 7.3 2.1 6.8 7.0 18.6 11.7 10.2 10.2 4.8 5.0 22.0 8.7 6.10 7.8 14.6 26.7 11.2 23.1 6.0 6.1 4.8 6. 8.7 6.0 8.1 8.3 0.0 6.4 4.0 7.0 8.7 9.1 9.5 7.1 8.5 8.8 14.5 5.90 7.4 12.8 12.4 10.1 13.5* 13.0 10.3 10.1 9.1 7.9 13.0 20.9 20.3 3.6 4.6 18.0 23.9 17.4 15.4 15.4 26.1 15.6 25.1 27.7 4.04 5.6 25.6 21.7 16.2 19.3 6.9 6.7 5.2 6. 1.5 6.9 9.1 11.6 1.2 3.8 15.7 23.4 7.5 9.4 7.0 26.8 5.9 21.1 32.8 3.26 4.8 26.1 27.4 10.1 25.5 14.8 18.9 11.8 19.5 8.5 13.6 14.2 18.0 4.3 6.4 13.6 22.8 21.5 14.4 22.1 17.9 18.7 20.9 22.4 5.72 7.4 19.6 27.1 16.2 27.4* capabilities compared to approaches that only predict relevance scores directly. 3. Distillation results in small student models with teacher performance. Our results also demonstrate that our approach successfully distills complex reasoning capabilities into compact 3B parameter model, achieving performance comparable to models over 20 times larger (see Llama 3.3 70B in Table 1). When combined with the Qwen1.5 retriever and domain-speciÔ¨Åc relevance deÔ¨Ånitions in the rankers prompt (rows marked with + instruct in Table 1), InteRank achieves state-of-the-art performance with an average of 27.4% across all domains reaching the third spot in BRIGHT leaderboard, just below JudgeRank [20], an ensemble of 3 zero-shot LLMs (8B, 70B, and 405B parameters) and baseline using Llama 70B with query-rewriting with GPT-4. Our 3B parameter model outperforms all other baseline methods on the BRIGHT benchmark, including recent approaches like Reason-to-Rank [12] (nDCG@5 26.2 vs 19.6) which uses an 8B parameter model. 4. RL improves reasoning for ranking. The iterative RL process shows domain-dependent eÔ¨Äects, as detailed in Table 2. While the Ô¨Årst iteration leads to broad improvements (+1.1% nDCG@10 on average), the second iteration reveals an interesting pattern - performance continues to improve in reasoning-intensive domains like mathematics and coding while declining in domains with simpler reasoning requirements. This suggests that additional RL iterations help reÔ¨Åne complex reasoning capabilities but may lead to over-Ô¨Åtting in domains where simpler strategies suÔ¨Éce. Table 2 presents detailed results examining the impact of diÔ¨Äerent training stages. The supervised Ô¨Åne-tuning (SFT) stage establishes strong initial performance, particularly in domains like Biology and Earth Science. The Ô¨Årst RL iteration shows the largest gains in theoretical domains (TheoQ), coding tasks (Pony, Leetcode), and earth science. The second iteration further improves performance speciÔ¨Åcally in reasoning-intensive tasks (Leetcode, Pony, TheoQ, TheoT) while showing decline in simpler domains, highlighting the trade-oÔ¨Ä between specialized reasoning capabilities and general performance. Table 2: Performance (nDCG@10) of the reranker in various training stages with GTE-large as Ô¨Årst-stage retriever. Domain Bio. Earth. Econ. Psy. Rob. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. Average SFT 39.4 42.4 23.2 27.1 14.1 21.8 20.6 19.9 7.4 6.9 12.8 8.8 20. RL, t=1 35.2 45.7 24.1 27.4 16.1 21.8 20.8 22.0 11.7 8.7 17.4 7.5 21.5 RL, t=2 30.0 38.4 22.7 25.9 13.6 18.2 16.7 25.3 15.6 8.6 20.2 9.8 20.4 ùõø(t=1 vs SFT) -4.2 +3.3 +0.9 +0.3 +2.0 0.0 +0.2 +2.1 +4.3 +1.8 +4.6 -1.3 +1.1 ùõø(t=2 vs t=1) -5.2 -7.3 -1.4 -1.5 -2.5 -3.6 -4.1 +3.3 +3.9 -0.1 +2.8 +2.3 -1."
        },
        {
            "title": "4 CONCLUSIONS\nThis paper presents a novel approach for training compact lan-\nguage models to perform reasoning-intensive document ranking.\nOur methodology combines knowledge distillation from a large\nteacher model with reinforcement learning optimization to create\neÔ¨Écient yet powerful ranking models that can explain their de-\ncisions. Through extensive experimentation we demonstrate that\na 3B parameter LLM achieves performance comparable to mod-\nels over 20 times larger, reaching state-of-the-art results across\ndiverse domains. Dedicating inference-time compute to generate\nexplanations, rather than directly predicting relevance scores, en-\nables more eÔ¨Äective reasoning with smaller language models.",
            "content": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking SIGIR25, July 2025, Padova, Italy [20] Tong Niu, ShaÔ¨Åq Joty, Ye Liu, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024. JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking. (2024). arXiv:2411.00142 [cs.CL] https://arxiv.org/abs/2411.00142 [21] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019). [22] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. arXiv preprint arXiv:1910.14424 (2019). [23] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna: Zero-shot listwise document reranking with open-source large language models. arXiv preprint arXiv:2309.15088 (2023). [24] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankZephyr: EÔ¨Äective and Robust Zero-Shot Listwise Reranking is Breeze! arXiv preprint arXiv:2312.02724 (2023). [25] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 39823992. https://doi.org/10.18653/v1/D19-1410 [26] Stephen Robertson, S. Walker, S. and M. Gatford. 1995. Text REtrieval Conference trieval conference https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/ Jones, M. M. Hancock-Beaulieu, the Third re109126. Okapi at TREC-3. (TREC-3) ed.). Gaithersburg, MD: NIST,"
        },
        {
            "title": "In Overview of\nthe",
            "content": "(overview of third text (trec3) [27] Chris Samarinas, Wynne Hsu, and Mong Li Lee. 2021. Improving Evidence Retrieval for Automated Explainable Fact-Checking. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations, Avi Sil and Xi Victoria Lin (Eds.). Association for Computational Linguistics, Online, 8491. https://doi.org/10.18653/v1/2021.naacl-demos.10 [28] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One Embedder, Any Task: Instruction-Finetuned Text Embeddings. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 11021121. https://doi.org/10.18653/v1/2023.Ô¨Åndings-acl. [29] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas MuennighoÔ¨Ä, Hanyu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, et al. 2024. BRIGHT: Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. arXiv preprint arXiv:2407.12883 (2024). [30] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Is ChatGPT good at search? arXiv preprint Chen, Dawei Yin, and Zhaochun Ren. 2023. Investigating large language models as re-ranking agents. arXiv:2304.09542 (2023). [31] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022). [32] Benjamin Warner, Antoine ChaÔ¨Én, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, GriÔ¨Én Adams, Jeremy Howard, and Iacopo Poli. 2024. Smarter, Better, Faster, Longer: Modern Bidirectional Encoder for Fast, Memory EÔ¨Écient, and Long Context Finetuning and Inference. arXiv:2412.13663 [cs.CL] https://arxiv.org/abs/2412.13663 [33] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas MuennighoÔ¨Ä, Defu Lian, and Jian-Yun Nie. 2024. C-Pack: Packed Resources For General Chinese Embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR 24). Association for Computing Machinery, New York, NY, USA, 641649. https://doi.org/10.1145/3626772. ACKNOWLEDGMENTS This work was supported in part by the Center for Intelligent Information Retrieval (CIIR), in part by the OÔ¨Éce of Naval Research contract number N000142212688, and in part by NSF grant #2143434. Any opinions, Ô¨Åndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reÔ¨Çect those of the sponsors. REFERENCES [1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, and et al. 2023. Qwen Technical Report. arXiv:2309.16609 [cs.CL] https://arxiv.org/abs/2309.16609 [2] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81. [3] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen Voorhees. 2020. Overview of the TREC 2019 deep learning track. arXiv preprint arXiv:2003.07820 (2020). [4] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen Voorhees, and Ian SoboroÔ¨Ä. 2021. TREC deep learning track: Reusable test collections in the large data regime. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 23692375. [5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, and Bingxuan Wang et al. 2025. DeepSeekR1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501.12948 [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: EÔ¨Écient Ô¨Ånetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024). [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805 (2018). [8] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Rethink training of BERT rerankers in multi-stage retrieval pipeline. In Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021. Springer, 280286. [9] Aaron GrattaÔ¨Åori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, and Ahmad Al-Dahle et al. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [10] GeoÔ¨Ärey Hinton, Oriol Vinyals, and JeÔ¨Ä Dean. 2015. Distilling the Knowledge in Neural Network. arXiv:1503.02531 [stat.ML] https://arxiv.org/abs/1503.02531 [11] Sebastian Hofst√§tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. EÔ¨Éciently Teaching an EÔ¨Äective Dense Retriever with Balanced Topic Aware Sampling. In Proc. of SIGIR. [12] Yuelyu Ji, Zhuochun Li, Rui Meng, and Daqing He. 2024. ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation. (2024). arXiv:2410.05168 [cs.CL] https://arxiv.org/abs/2410.05168 [13] Julian Killingback, Hansi Zeng, and Hamed Zamani. 2025. Hypencoder: arXiv:2502.05364 [cs.IR] Hypernetworks for Information Retrieval. https://arxiv.org/abs/2502.05364 [14] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, RewardBench: EvaluatNoah A. Smith, and Hannaneh Hajishirzi. 2024. ing Reward Models for Language Modeling. arXiv:2403.13787 [cs.LG] https://arxiv.org/abs/2403.13787 [15] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281 (2023). [16] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024. Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs. arXiv preprint arXiv:2410.18451 (2024). Learning to rank for information retrieval. In Pro- [17] Tie-Yan Liu. 2010. the 33rd International ACM SIGIR Conference on Research ceedings of (SIGIR and Development 10). Association for Computing Machinery, New York, NY, USA, 904. https://doi.org/10.1145/1835449.1835676 in Information Retrieval (Geneva, Switzerland) [18] Niklas MuennighoÔ¨Ä, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative representational instruction tuning. arXiv preprint arXiv:2402.09906 (2024). [19] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: Human Generated MAchine Reading COmprehension Dataset. CoRR abs/1611.09268 (2016). arXiv:1611.09268 http://arxiv.org/abs/1611."
        }
    ],
    "affiliations": [
        "University of Massachusetts Amherst"
    ]
}