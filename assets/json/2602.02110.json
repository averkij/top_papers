{
    "paper_title": "An Empirical Study of World Model Quantization",
    "authors": [
        "Zhongqian Fu",
        "Tianyi Zhao",
        "Kai Han",
        "Hang Zhou",
        "Xinghao Chen",
        "Yunhe Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 0 1 1 2 0 . 2 0 6 2 : r a"
        },
        {
            "title": "An Empirical Study of World Model Quantization",
            "content": "Zhongqian Fu, Tianyi Zhao, Kai Han, Hang Zhou, Xinghao Chen, and Yunhe Wang Huawei Noahs Ark Lab {fuzhongqian, zhaotianyi13, kai.han}@huawei.com Abstract. World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present systematic empirical study of world model quantization using DINO-WM as representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https: //github.com/huawei-noah/noah-research/tree/master/QuantWM."
        },
        {
            "title": "Introduction",
            "content": "In the field of artificial intelligence, World Model is an internal, predictive representation of an environment that allows an AI system to understand and simulate how that environment works [13, 31]. World models play an increasingly important role in embodied decision-making by enabling agents to predict and reason about future environment dynamics. These models are commonly employed for planning from visual observations through iterative latent rollouts, and have demonstrated strong performance across wide range of control and manipulation Equal contribution Project lead ii Zhongqian Fu et al. Fig. 1: Activation outliers and scale imbalance in the encoder and predictor of DINOWM before and after smoothing [28], illustrating numerical challenges for low-bit quantization in planning-based world models. tasks. In this work, we focus on world models for visual planning and adopt DINO-WM [30] as representative instantiation. It is worth noting that our approach can also be transferred to other world model architectures. Despite their strong performance, planning paradigms based on world models generally rely on repeated model inference across multiple planning iterations. At each planning step, the world model is invoked multiple times to evaluate candidate trajectories, causing both computational cost and memory footprint to scale with the planning horizon. This repeated inference poses significant efficiency challenge in practical deployment scenarios, such as robotics [27] and large-scale simulation [1]. Although reduced-precision inference such as FP16 is commonly adopted to alleviate computational and memory overhead, it remains insufficient for long-horizon planning where inference is executed tens of times per episode. This limitation motivates the exploration of more aggressive model compression techniques. Among them, post-training quantization (PTQ) is particularly attractive due to its simplicity and the fact that it does not require retraining the model. However, the behavior of PTQ has been predominantly studied for discriminative vision tasks [19] or autoregressive language modeling [28], but poorly understood in iterative planning on world models, where recurrent latent rollouts present unique challenges. Unlike single-pass inference tasks, planning with world models involves temporal accumulation of errors, where small numerical perturbations introduced by quantization may compound across planning iterations and significantly affect long-horizon performance. Moreover, different PTQ methods, ranging from simple rounding-based approaches to calibrationdriven and activation-aware techniques, exhibit substantially different robustness characteristics, especially under low-bit configurations. As result, it remains unclear which quantization strategies are suitable for world model-based planning An Empirical Study of World Model Quantization iii and how aggressively quantization can be applied without compromising planning reliability. key challenge in quantizing world models lies in their activation and representation characteristics. As illustrated in Figure 1, the encoder and predictor in DINO-WM exhibit pronounced activation outliers and highly non-uniform scale distributions. Such behavior amplifies the impact of quantization noise and makes numerical errors more likely to propagate across planning iterations. While recent techniques such as activation smoothing can partially mitigate these effects, how quantization-induced distortions interact with long-horizon rollout dynamics remains unclear. In this work, we conduct systematic post-training quantization study on world models for planning to characterize the trade-offs between quantization efficiency and planning performance, using DINO-WM as representative case study. We evaluate diverse set of representative PTQ methods, including RTN [12], OMSE [3], AWQ [17], SmoothQuant [28], and OmniQuant [22], under both weight-only and joint weight-activation quantization settings. Our experiments cover wide range of bit-widths and quantization granularities, and are evaluated on two embodied planning environments with planning horizons of up to 50 iterations. Specifically, this study seeks to answer the following questions: How does aggressive low-bit quantization affect the performance and stability of world model in long-horizon planning, beyond standard accuracy and bit-width trade-offs? How do different PTQ strategies and granularities (e.g., per-channel vs. pergroup weights, per-tensor vs. per-token activations) influence rollout dynamics and planning robustness? How sensitive are the encoder and predictor components of world model to quantization noise, and which module constitutes the primary bottleneck for low-bit deployment? Through extensive empirical evaluation, our work provides practical insights into quantizing world models. We hope these findings can serve as useful reference for deploying DINO-WM and similar world models under strict computational constraints."
        },
        {
            "title": "2.1 World Models",
            "content": "World models aim to learn compact and predictive representations of environments to enable efficient planning and decision-making in reinforcement learning (RL) [8 10,14]. These models typically combine latent state estimation, reward prediction, and policy learning within unified framework. Early works like World Models [8] introduced combination of variational autoencoders (VAEs) and recurrent neural networks (RNNs) to model environment dynamics, while later approaches such as PlaNet [10] and Dreamer [9] replaced RNNs with deep latent variable iv Zhongqian Fu et al. models for improved scalability and performance. Recent advancements focus on integrating transformers [25] to capture long-term dependencies [11] and incorporating multi-modal observations [5]. notable recent development is DINO-WM [30], which leverages pre-trained visual features from self-supervised models (e.g., DINO [2, 21]) to construct world models capable of zero-shot planning in unseen environments. By decoupling visual feature extraction from environment modeling, DINO-WM avoids the need for task-specific reward functions and instead relies on the semantic richness of pre-trained features to guide planning. This approach demonstrates strong generalization across diverse tasks and environments, particularly in scenarios where reward engineering is challenging. However, relying on pre-trained features introduces fixed bottleneck, and the computational cost of maintaining high-resolution latent representations remains major challenge for real-time deployment. Furthermore, due to the complexity of modeling long-term dynamics, scaling world models to high-fidelity visual environments still requires substantial computational resources [14]."
        },
        {
            "title": "2.2 Model Quantization",
            "content": "Model quantization is model compression technique that improves inference efficiency by converting full-precision parameters into low-precision integer representations, leading to direct computational accelaration and memory saving [6, 7, 20]. The quantization process is mathematically formalized as: Q(x) = clip( + z, 0, 2b 1), where denotes the full-precision parameter, indicates the number of bits to be quantized, denotes the round-to-nearest operation, and the clip function is used to restrict the rounded value to the specified range (0, 2b 1). Notably, is the scaling factor, and is the zero point, both determined by the boundary value of x. (1) Among various quantization methods, PTQ has emerged as dominant strategy for compressing deep learning models due to its compatibility with pre-trained systems and minimal reliance on retraining [6]. PTQ leverages small calibration dataset to optimize quantization parameters, minimizing the accuracy drop caused by low-precision inference. PTQ has been effectively applied to various neural networks, including CNNs [3, 12, 15, 26], Vision Transformers [18, 19, 29], and Language Transformers [16, 17, 22, 23, 28]. Despite its demonstrated success, the application of quantization to world models remains largely unexplored, presenting significant open challenge within the research community. For instance, quantization errors in latent state transitions or reward prediction modules can accumulate over time, degrading planning performance. To bridge this gap, our research delves into the unique challenges of world modeling and provides systematic empirical study of world model quantization. We hope this research will inspire future studies on quantization strategies that explicitly consider long-horizon planning dynamics. An Empirical Study of World Model Quantization Table 1: 3 to 8-bits weight-only PTQ results on the Wall dataset. #W denotes the weight quantization bit-width, #A denotes the activation quantization bit-width, and #G denotes the group size. Noted that the AQG represents the Activation Quantization Granularity. Dataset Method #W #A #G AQG Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters 40 iters 50 iters Wall RTN OMSE AWQ RTN OMSE AWQ FP32 RTN OMSE AWQ 32 8 8 8 OmniQuant 8 4 4 4 OmniQuant 4 3 3 3 OmniQuant 3 8 8 8 OmniQuant 8 4 4 4 OmniQuant 4 3 3 3 OmniQuant 3 RTN OMSE AWQ RTN OMSE AWQ RTN OMSE AWQ 0.94 0.48 32 per-tensor 0.52 0.92 32 per-tensor 0.54 0.94 32 0.90 per-tensor 0.44 32 per-tensor 0.56 0.94 32 0.18 per-tensor 0.00 32 0.58 per-tensor 0.04 32 per-tensor 0.04 0.50 32 per-tensor 0.10 0.70 32 0.00 per-tensor 0.00 32 0.00 per-tensor 0.00 32 per-tensor 0.00 0.00 32 32 per-tensor 0.00 0.02 32 128 per-tensor 0.52 0.90 32 128 per-tensor 0.52 0.90 32 128 per-tensor 0.50 0.92 32 128 per-tensor 0.46 0.92 0.76 32 128 per-tensor 0.18 0.54 32 128 per-tensor 0.08 0.72 32 128 per-tensor 0.18 32 128 per-tensor 0.20 0.92 0.00 32 128 per-tensor 0.00 0.00 32 128 per-tensor 0.00 32 128 per-tensor 0.00 0.00 32 128 per-tensor 0.00 0.02 0.94 0.94 0.94 0.94 0.94 0.22 0.68 0.62 0.74 0.00 0.00 0.00 0.02 0.94 0.94 0.96 0.96 0.78 0.54 0.74 0.94 0.00 0.00 0.00 0.02 0.94 0.94 0.94 0.94 0.94 0.24 0.76 0.66 0.78 0.00 0.06 0.00 0.06 0.94 0.94 0.96 0.96 0.82 0.60 0.78 0.94 0.00 0.00 0.00 0. 0.94 0.94 0.94 0.96 0.96 0.30 0.80 0.70 0.80 0.00 0.10 0.02 0.10 0.94 0.94 0.96 0.96 0.82 0.62 0.78 0.94 0.00 0.00 0.00 0.02 0.94 0.96 0.94 0.96 0.96 0.40 0.82 0.72 0.82 0.00 0.14 0.02 0.14 0.94 0.94 0.96 0.96 0.84 0.62 0.82 0.94 0.00 0.00 0.00 0.04 0.94 0.96 0.94 0.96 0.96 0.42 0.82 0.72 0.82 0.00 0.14 0.02 0.14 0.94 0.94 0.96 0.96 0.86 0.62 0.82 0.94 0.00 0.00 0.00 0."
        },
        {
            "title": "3.1 Experiment Settings",
            "content": "Base Model. We conduct all experiments on DINO-WM [30], transformerbased world model designed for planning from visual observations. Following the original work, we use the publicly released model checkpoint and do not distinguish between different model scales. All quantization methods are applied to the same pretrained model to ensure fair comparison. Evaluation Environments. We evaluate quantized world models on two representative visual planning environments: Wall and PushT, following the evaluation protocol of DINO-WM [4, 24, 30]. These environments are drawn from standard embodied control benchmarks and exhibit different levels of dynamics complexity, covering both navigation and fine-grained manipulation tasks. In both environments, the task is to reach randomly sampled goal state specified by target visual observation, starting from arbitrary initial states. Planning is performed by rolling out candidate action sequences using the world model and selecting vi Zhongqian Fu et al. actions that minimize the discrepancy between predicted future observations and the target observation. Observations in all environments consist of RGB images. We refer readers to the DINO-WM paper for full description of the environments and task specifications. We report planning results over planning horizons ranging from 0 to 50 iterations, covering both short-horizon and long-horizon planning behaviors. Quantization Methods. We evaluate several representative PTQ methods, covering both naive and calibration-based approaches that are widely adopted. Specifically, we consider the following methods: RTN [12]: uniform round-to-nearest quantization method that directly quantizes weights using affine quantization parameters. OMSE [3]: calibration-based quantization method that determines quantization parameters by minimizing the mean squared error between floating-point and quantized layer outputs. AWQ [17]: an activation-aware weight quantization method that identifies and preserves salient weight channels based on activation statistics, improving robustness under low-bit settings. SmoothQuant [28]: joint weight-activation quantization method that mitigates activation outliers by smoothing the scale distribution between weights and activations. OmniQuant [22]: calibration-driven quantization framework that jointly optimizes quantization parameters across layers, supporting both weight-only and weight-activation quantization. Quantization Configuration. We evaluate both weight-only and joint weightactivation quantization settings to comprehensively assess the impact of numerical precision on planning performance. For weight-only quantization, we consider 3-bit, 4-bit, and 8-bit weight precision. For joint weight-activation quantization, we evaluate the configurations including W8A8, W6A6, W4A8, and W4A4. Quantization parameters are determined using dedicated calibration dataset, which is strictly separated from the evaluation data. Calibration trajectories are collected by interacting with the environment using random seeds different from those used for evaluation, ensuring that no evaluation information is leaked during quantization. For calibration, we sample short planning trajectories with fixed planning horizon of 2 iterations. The resulting observations and intermediate model activations are used to estimate quantization parameters for both weights and activations. We further investigate the impact of quantization granularity by comparing per-channel and per-group quantization for weights, and per-tensor and per-token quantization for activations. For per-group quantization, we use fixed group size of 128 across all experiments. Unless otherwise stated, symmetric quantization is used for weights, while asymmetric quantization is used for activations. All quantized models are evaluated using the same inference codebase to ensure that any observed performance differences are solely attributed to the quantization effects. An Empirical Study of World Model Quantization vii Table 2: 4 to 8-bits weight-only PTQ results on the PushT dataset. Dataset Method #W #A #G AQG Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters PushT FP32 RTN OMSE AWQ OmniQuant RTN OMSE AWQ OmniQuant RTN OMSE AWQ OmniQuant RTN OMSE AWQ OmniQuant 32 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 4 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 128 128 128 128 128 128 128 128 per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor 0.92 0.88 0.90 0.64 0.82 0.04 0.00 0.00 0.00 0.88 0.86 0.58 0.86 0.16 0.06 0.32 0.24 0.94 0.90 0.90 0.70 0.88 0.08 0.06 0.00 0.00 0.90 0.86 0.72 0.88 0.38 0.12 0.50 0. 0.94 0.90 0.90 0.70 0.88 0.08 0.08 0.00 0.00 0.90 0.86 0.72 0.88 0.40 0.12 0.50 0.36 0.94 0.90 0.90 0.70 0.88 0.10 0.08 0.00 0.00 0.90 0.86 0.72 0.88 0.44 0.12 0.50 0.40 0.94 0.90 0.90 0.72 0.88 0.10 0.08 0.00 0.00 0.90 0.86 0.72 0.88 0.44 0.12 0.50 0."
        },
        {
            "title": "3.2 Post-Training Quantization Experiments",
            "content": "We present detailed analysis of PTQ effects on world model planning performance. In addition to success rate, we also investigated how quantization precision, quantization granularity, and different modules interact with long-horizon rollout dynamics on the Wall and PushT environments. Per-Channel vs. Per-Group Weight Quantization. Tables 1 and 2 report weightonly quantization results under per-channel and per-group settings. At 8-bit precision, all evaluated methods achieve performance comparable to FP32 across planning horizons, indicating that moderate weight quantization introduces negligible distortion to the learned dynamics. When weight precision is reduced to 4-bit, performance degradation is already evident at short planning horizons, particularly at 0-5 iterations, under both per-channel and per-group quantization. On Wall, increasing the number of planning iterations leads to substantial recovery primarily under per-group quantization, indicating that the planner can partially compensate for quantization-induced transition bias when grouping is applied. For example, under 4-bit weight quantization with OmniQuant and per-group grouping (#G = 128), the success rate improves from 0.20 at 0 iterations to 0.94 at 50 iterations, closely matching the FP32 baseline. PushT exhibits limited recoverability under per-channel 4-bit quantization, while per-group quantization (#G = 128) enables only partial recovery. However, under extreme precision reduction (3-bit), performance collapses across both datasets regardless of the quantization method or grouping strategy. In this regime, success rates remain near zero even as planning iterations increase, indicating that quantization noise overwhelms the learned transition dynamics. As result, the stabilizing effect of group-wise quantization largely disappears, suggesting that grouping is only viii Zhongqian Fu et al. Fig. 2: Open-loop rollouts of the world model under different quantization methods and bit-widths on WALL and Push-T. Given the first frame and fixed action sequence, the model predicts future observations, which are reconstructed by the decoder. Results are shown for various quantization settings to illustrate their impact on long-horizon prediction quality. The top row shows the full-precision trajectories. effective when the underlying latent structure is sufficiently preserved under quantization. Per-Tensor vs. Per-Token Activation Quantization. Tables 3, 4, 5, and 6 investigate the impact of activation quantization granularity under both per-channel and per-group weight quantization. Across both environments, per-token activation quantization does not consistently outperform per-tensor quantization, despite its more fine-grained scaling. At moderate bit-widths (e.g., W8A8 and W6A6), pertoken activation quantization occasionally matches or slightly exceeds per-tensor performance, particularly on Wall. However, these gains become inconsistent under lower precision and longer planning horizons. In several low-bit settings, per-token quantization introduces additional variability, leading to reduced sucAn Empirical Study of World Model Quantization ix Table 3: 4 to 8-bits weight-activation PTQ results on the Wall dataset. #W denotes the weight quantization bit-width, #A denotes the activation quantization bit-width, and #G denotes the group size. Noted that the AQG represents the Activation Quantization Granularity. Dataset Method #W #A #G AQG Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters 40 iters 50 iters Wall FP32 RTN OMSE 32 8 8 SmoothQuant 8 8 OmniQuant 6 RTN 6 OMSE SmoothQuant 6 6 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 8 RTN 8 OMSE SmoothQuant 8 8 OmniQuant 6 RTN 6 OMSE SmoothQuant 6 6 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 32 8 8 8 8 6 6 6 6 8 8 8 8 4 4 4 4 8 8 8 8 6 6 6 6 8 8 8 8 4 4 4 4 0.48 0.94 per-tensor 0.48 0.94 per-tensor 0.44 0.94 per-tensor 0.46 0.94 per-tensor 0.50 0.92 0.78 per-tensor 0.24 per-tensor 0.26 0.84 per-tensor 0.32 0.90 per-tensor 0.20 0.90 0.12 per-tensor 0.00 per-tensor 0.00 0.30 per-tensor 0.18 0.68 per-tensor 0.14 0.70 per-tensor 0.00 0.00 per-tensor 0.00 0.06 0.02 per-tensor 0.00 0.00 per-tensor 0.00 per-token 0.44 0.92 per-token 0.48 0.96 per-token 0.52 0.92 per-token 0.52 0.94 per-token 0.30 0.82 per-token 0.28 0.78 per-token 0.32 0.80 0.76 per-token 0.26 0.08 per-token 0.00 per-token 0.00 0.48 per-token 0.14 0.74 0.68 per-token 0.10 0.00 per-token 0.00 0.00 per-token 0.00 per-token 0.00 0.02 0.00 per-token 0.00 0.94 0.96 0.96 0.94 0.94 0.80 0.88 0.90 0.92 0.20 0.42 0.72 0.76 0.04 0.06 0.02 0.00 0.96 0.96 0.94 0.94 0.88 0.84 0.82 0.78 0.10 0.60 0.78 0.78 0.00 0.00 0.04 0.02 0.94 0.96 0.96 0.98 0.96 0.84 0.88 0.90 0.92 0.28 0.60 0.82 0.82 0.08 0.06 0.08 0.02 0.96 0.96 0.94 0.94 0.92 0.84 0.88 0.80 0.20 0.70 0.82 0.80 0.04 0.00 0.04 0.06 0.94 0.96 0.98 0.98 0.96 0.86 0.90 0.90 0.94 0.30 0.64 0.84 0.82 0.08 0.06 0.10 0.04 0.96 0.96 0.94 0.94 0.92 0.84 0.88 0.82 0.28 0.74 0.84 0.84 0.06 0.00 0.04 0.08 0.94 0.96 0.98 0.98 0.96 0.86 0.90 0.90 0.94 0.36 0.76 0.84 0.82 0.08 0.06 0.12 0.04 0.96 0.96 0.94 0.96 0.92 0.84 0.88 0.84 0.32 0.74 0.86 0.84 0.06 0.00 0.04 0. 0.94 0.96 0.98 0.98 0.96 0.88 0.90 0.90 0.94 0.38 0.84 0.84 0.88 0.08 0.06 0.12 0.06 0.96 0.96 0.94 0.96 0.92 0.84 0.88 0.86 0.34 0.76 0.88 0.86 0.06 0.00 0.04 0.08 cess rates despite comparable short-horizon performance. These results suggest that, in iterative planning settings, maintaining globally consistent activation scaling is often more critical than maximizing instantaneous representational flexibility. Fine-grained activation scaling alone is insufficient to guarantee stable long-horizon rollouts when quantization noise accumulates over time. Quantization Sensitivity of Encoder and Predictor. Tables 7 and 8 isolate the effects of quantization on the encoder and predictor modules. clear asymmetry emerges across both environments. Quantizing the encoder leads to rapid performance degradation, especially under low-bit configurations. Errors introduced at the representation level propagate through all subsequent rollout steps, resulting Zhongqian Fu et al. Table 4: 4 to 8-bits weight-activation PTQ results on the Wall dataset (#G=128). Dataset Method #W #A #G AQG Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters 40 iters 50 iters Wall FP32 RTN OMSE 32 8 8 SmoothQuant 8 8 OmniQuant 6 RTN 6 OMSE SmoothQuant 6 6 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 8 RTN 8 OMSE SmoothQuant 8 8 OmniQuant 6 RTN 6 OMSE SmoothQuant 6 6 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 4 RTN 4 OMSE SmoothQuant 4 4 OmniQuant 0.48 32 0.94 8 128 per-tensor 0.42 0.94 8 128 per-tensor 0.60 0.94 8 128 per-tensor 0.48 0.92 8 128 per-tensor 0.42 0.94 0.82 6 128 per-tensor 0.20 6 128 per-tensor 0.26 0.80 6 128 per-tensor 0.38 0.92 6 128 per-tensor 0.26 0.88 8 128 per-tensor 0.24 0.64 8 128 per-tensor 0.10 0.50 8 128 per-tensor 0.22 0.84 0.80 8 128 per-tensor 0.12 4 128 per-tensor 0.00 0.02 4 128 per-tensor 0.00 0.02 4 128 per-tensor 0.00 0.02 4 128 per-tensor 0.00 0.02 8 128 per-token 0.42 0.94 8 128 per-token 0.60 0.94 8 128 per-token 0.48 0.92 8 128 per-token 0.52 0.94 0.82 6 128 per-token 0.20 6 128 per-token 0.26 0.80 6 128 per-token 0.38 0.92 0.80 6 128 per-token 0.36 0.64 8 128 per-token 0.24 8 128 per-token 0.10 0.50 8 128 per-token 0.22 0.84 8 128 per-token 0.26 0.84 4 128 per-token 0.00 0.02 4 128 per-token 0.00 0.02 4 128 per-token 0.00 0.02 0.00 4 128 per-token 0.00 0.94 0.94 0.96 0.92 0.94 0.82 0.82 0.94 0.94 0.66 0.54 0.84 0.84 0.06 0.04 0.10 0.06 0.94 0.96 0.94 0.94 0.82 0.82 0.94 0.82 0.66 0.54 0.84 0.88 0.06 0.04 0.10 0.00 0.94 0.96 0.96 0.94 0.96 0.84 0.84 0.94 0.96 0.66 0.60 0.92 0.88 0.08 0.06 0.14 0.08 0.96 0.96 0.96 0.94 0.84 0.84 0.94 0.86 0.66 0.60 0.92 0.88 0.08 0.06 0.14 0. 0.94 0.98 0.98 0.96 0.96 0.84 0.84 0.94 0.96 0.66 0.64 0.94 0.94 0.12 0.10 0.14 0.10 0.98 0.98 0.96 0.94 0.84 0.84 0.94 0.86 0.66 0.64 0.94 0.92 0.12 0.10 0.14 0.08 0.94 0.98 0.98 0.96 0.98 0.86 0.86 0.94 0.96 0.68 0.64 0.96 0.94 0.12 0.10 0.14 0.10 0.98 0.98 0.96 0.94 0.84 0.86 0.94 0.92 0.66 0.64 0.94 0.92 0.12 0.10 0.14 0.12 0.94 0.98 0.98 0.96 0.98 0.86 0.86 0.94 0.96 0.68 0.64 0.96 0.94 0.12 0.10 0.14 0.12 0.98 0.98 0.96 0.94 0.84 0.86 0.94 0.92 0.68 0.64 0.96 0.92 0.12 0.10 0.14 0.12 in persistent performance loss that cannot be corrected by additional planning iterations. In contrast, the predictor exhibits greater robustness to quantization. Moderate predictor quantization primarily affects temporal consistency rather than representation quality, and its impact can often be partially mitigated by increasing the planning horizon. This behavior is particularly evident on Wall, where success rates recover as planning iterations increase when only the predictor is quantized. These observations indicate that representation quality constitutes the dominant bottleneck in low-bit world model deployment, while moderate transition noise introduced by predictor quantization is comparatively more tolerable in planning-based evaluation. Qualitative Rollouts and Planning Loss Behavior. Figure 2 provides qualitative evidence supporting the quantitative trends. On Wall, encoder quantization leads to visible degradation in reconstructed observations starting from the An Empirical Study of World Model Quantization xi Table 5: 4 to 8-bits weight-activation PTQ results on the PushT dataset. Dataset Method #W #A #G AQG Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters PushT FP32 RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant 32 8 8 8 8 6 6 6 6 4 4 4 4 8 8 8 8 6 6 6 6 4 4 4 4 32 8 8 8 8 6 6 6 6 8 8 8 8 8 8 8 8 6 6 6 6 8 8 8 8 per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-token per-token per-token per-token per-token per-token per-token per-token per-token per-token per-token per-token 0.92 0.50 0.54 0.58 0.58 0.14 0.22 0.20 0.16 0.04 0.00 0.02 0.02 0.78 0.76 0.80 0.74 0.22 0.50 0.40 0.40 0.04 0.02 0.00 0.00 0.94 0.66 0.68 0.76 0.72 0.24 0.36 0.34 0.26 0.10 0.04 0.06 0.02 0.88 0.82 0.86 0.78 0.46 0.62 0.46 0.58 0.10 0.10 0.04 0.00 0.94 0.66 0.68 0.78 0.72 0.26 0.40 0.36 0.26 0.10 0.06 0.06 0.02 0.88 0.84 0.88 0.78 0.48 0.64 0.56 0.58 0.10 0.10 0.04 0.00 0.94 0.66 0.68 0.78 0.72 0.28 0.40 0.36 0.30 0.10 0.06 0.06 0.02 0.88 0.84 0.88 0.78 0.54 0.66 0.58 0.58 0.10 0.10 0.04 0.00 0.94 0.66 0.68 0.78 0.72 0.30 0.40 0.36 0.30 0.10 0.06 0.06 0.02 0.88 0.84 0.88 0.78 0.54 0.66 0.58 0.60 0.10 0.10 0.04 0.00 Table 6: 4 to 8-bits weight-activation PTQ results on the PushT dataset (#G=128). Dataset Method #W #A #G AQG Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters PushT FP32 RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant RTN OMSE SmoothQuant OmniQuant 32 8 8 8 8 6 6 6 6 4 4 4 4 8 8 8 8 6 6 6 6 4 4 4 4 32 8 8 8 8 6 6 6 6 8 8 8 8 8 8 8 8 6 6 6 6 8 8 8 8 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-tensor per-token per-token per-token per-token per-token per-token per-token per-token per-token per-token per-token per-token 0.92 0.68 0.46 0.62 0.66 0.14 0.12 0.34 0.38 0.14 0.08 0.14 0.12 0.78 0.74 0.78 0.70 0.32 0.38 0.26 0.26 0.32 0.12 0.32 0.18 0.94 0.72 0.58 0.78 0.76 0.36 0.28 0.50 0.44 0.24 0.18 0.18 0.18 0.86 0.82 0.84 0.78 0.46 0.44 0.52 0.38 0.28 0.32 0.30 0. 0.94 0.72 0.60 0.80 0.76 0.38 0.34 0.52 0.48 0.28 0.22 0.20 0.18 0.86 0.84 0.84 0.78 0.48 0.46 0.52 0.40 0.30 0.18 0.34 0.28 0.94 0.72 0.60 0.80 0.76 0.38 0.36 0.54 0.48 0.28 0.26 0.20 0.20 0.86 0.84 0.84 0.78 0.48 0.48 0.52 0.40 0.30 0.20 0.34 0.30 0.94 0.74 0.60 0.80 0.76 0.38 0.36 0.54 0.48 0.34 0.26 0.20 0.22 0.86 0.84 0.84 0.78 0.48 0.48 0.54 0.40 0.30 0.20 0.36 0.30 xii Zhongqian Fu et al. Table 7: Encoder and Predictor quantization bit-width ablation on the Wall dataset. Dataset Method #W_Enc #A_Enc #W_Pred #A_Pred Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters 40 iters 50 iters Wall FP32 RTN RTN RTN RTN RTN RTN RTN RTN RTN RTN RTN SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant 32 8 6 6 4 4 4 8 8 8 8 8 8 6 6 4 4 4 8 8 8 8 32 8 8 6 8 6 4 8 8 8 8 8 8 8 6 8 6 4 8 8 8 8 8 32 8 8 8 8 8 8 6 6 4 4 4 8 8 8 8 8 8 6 6 4 4 4 32 8 8 8 8 8 8 8 6 8 6 4 8 8 8 8 8 8 8 6 8 6 4 0.48 0.48 0.42 0.18 0.02 0.00 0.00 0.48 0.44 0.34 0.26 0.08 0.46 0.48 0.26 0.18 0.14 0.00 0.52 0.46 0.40 0.42 0.40 0.94 0.94 0.94 0.74 0.14 0.08 0.00 0.96 0.94 0.90 0.90 0.56 0.94 0.94 0.76 0.74 0.64 0.00 0.92 0.94 0.94 0.92 0.70 0.94 0.96 0.94 0.78 0.38 0.22 0.00 0.96 0.96 0.92 0.94 0.58 0.94 0.94 0.80 0.78 0.76 0.00 0.94 0.96 0.94 0.94 0. 0.94 0.96 0.94 0.80 0.50 0.38 0.02 0.96 0.96 0.94 0.96 0.58 0.98 0.94 0.84 0.78 0.78 0.00 0.94 0.96 0.94 0.94 0.72 0.94 0.96 0.94 0.80 0.54 0.44 0.02 0.98 0.96 0.94 0.96 0.60 0.98 0.94 0.84 0.80 0.80 0.00 0.94 0.96 0.94 0.94 0.74 0.94 0.96 0.94 0.80 0.56 0.54 0.02 0.98 0.96 0.94 0.96 0.60 0.98 0.94 0.86 0.80 0.80 0.00 0.94 0.96 0.94 0.94 0.74 0.94 0.96 0.94 0.80 0.64 0.56 0.02 0.98 0.96 0.94 0.96 0.60 0.98 0.94 0.88 0.82 0.80 0.00 0.96 0.96 0.96 0.94 0.74 Table 8: Encoder and Predictor quantization bit-width ablation on the PushT dataset. Dataset Method #W_Enc #A_Enc #W_Pred #A_Pred PushT FP32 RTN RTN RTN RTN RTN RTN RTN RTN RTN RTN RTN SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant SmoothQuant 32 8 6 6 4 4 4 8 8 8 8 8 8 6 6 4 4 4 8 8 8 8 8 32 8 8 6 8 6 4 8 8 8 8 8 8 8 6 8 6 4 8 8 8 8 8 32 8 8 8 8 8 8 6 6 4 4 4 8 8 8 8 8 8 6 6 4 4 4 32 8 8 8 8 8 8 8 6 8 6 4 8 8 8 8 8 8 8 6 8 6 Success Rate 0 iters 5 iters 10 iters 20 iters 30 iters 0.92 0.50 0.38 0.04 0.02 0.00 0.02 0.46 0.46 0.40 0.48 0.00 0.58 0.60 0.14 0.00 0.00 0.00 0.58 0.54 0.40 0.34 0.00 0.94 0.66 0.54 0.12 0.02 0.02 0.02 0.70 0.66 0.44 0.50 0.00 0.76 0.68 0.22 0.00 0.00 0.00 0.76 0.74 0.70 0.60 0.00 0.94 0.66 0.56 0.12 0.02 0.02 0.02 0.72 0.66 0.44 0.54 0.02 0.78 0.70 0.24 0.00 0.00 0.00 0.76 0.76 0.70 0.64 0.00 0.94 0.66 0.56 0.14 0.02 0.02 0.02 0.72 0.66 0.44 0.54 0.02 0.78 0.74 0.24 0.00 0.00 0.00 0.78 0.78 0.70 0.66 0. 0.94 0.66 0.56 0.14 0.02 0.04 0.02 0.72 0.66 0.44 0.54 0.02 0.78 0.74 0.24 0.00 0.00 0.00 0.78 0.78 0.70 0.66 0.00 An Empirical Study of World Model Quantization xiii (a) RTN (b) OMSE (c) AWQ (d) SmoothQuant (e) OmniQuant Fig. 3: Comparison of the mean squared error (MSE) distance between current and target latent states across varying iterations under different quantization methods and configurations. initial frame, consistent with representation-level distortion. On PushT, reconstructed observations often remain visually plausible even when success rates drop sharply, indicating that task failures are not primarily caused by immediate visual representation collapse. Figure 3 further shows that under aggressive quantization settings, the planning loss often fails to decrease and can even increase over optimization iterations. This behavior suggests reduced alignment between the planning objective and task success under quantization, making optimization increasingly difficult as precision is reduced."
        },
        {
            "title": "4 Key Insights of World Model Quantization",
            "content": "Insight I: Group-wise Weight Quantization Improves Stability at 4-bit, but Fails under Extreme Precision Reduction. Applying group-wise weight quantization with moderate group size (e.g., #G = 128) consistently improves 4-bit performance on Wall and yields moderate gains on PushT, particularly in mid-to-long planning horizons. This suggests that grouping partially alleviates scale mismatch within weight subsets, leading to more stable rollout dynamics. However, when weight precision is further reduced (e.g., #W = 3 on Wall), quantization noise dominates the learned transition dynamics. In this regime, grouping no longer provides meaningful benefits, indicating failure to preserve useful latent structure under extreme low-bit constraints. Group-wise quantization primarily acts as stabilizer that mitigates scale mismatch in moderately quantized regimes, but it does not fundamentally improve robustness once quantization noise overwhelms the learned dynamics. Insight II: Per-token Activation Quantization Provides Limited and Inconsistent Gains over Per-tensor. Across both Wall and PushT, per-token activation quantization does not consistently outperform per-tensor quantization in world model planning. While more fine-grained scaling can slightly improve performance at moderate bit-widths, these gains are highly sensitive to rollout length and task difficulty. In low-bit configurations, per-token quantization can even introduce additional instability, suggesting that increased activation flexibility does not directly translate to improved long-horizon predictions. For world models quantization, activation granularity alone is insufficient to guarantee xiv Zhongqian Fu et al. improved planning performance, maintaining stable temporal dynamics appears more critical than maximizing instantaneous representational expressiveness. Insight III: World Models Exhibit Higher Sensitivity to Representation Quantization. clear asymmetry emerges between the encoder and predictor modules with respect to quantization sensitivity. Quantization applied to the encoder introduces irreversible distortions in the latent representations, which propagate through all subsequent rollout steps and cannot be corrected by additional planning iterations. In contrast, predictor quantization primarily affects the temporal consistency of rollouts. Its impact is often partially mitigated by longer planning horizons, as the planner can compensate for moderate transition noise through repeated optimization. Visual representation is the primary bottleneck in low-bit world models, making encoder precision substantially more critical than predictor precision. Insight IV: PushT Suffers from Planning-level Geometric Misalignment, While Wall Exposes Representation-level Collapse. On PushT, quantization primarily disrupts planning effectiveness rather than visual reconstruction. Across nearly all low-bit settings, success rates drop sharply and cannot be recovered through additional planning iterations. However, qualitative rollouts indicate that reconstructed observations often remain visually plausible as shown in Figure 2, suggesting that latent representations are not collapsed. Instead, small but systematic quantization-induced distortions bias the latent dynamics, leading to geometric misalignment between the planned trajectories and the narrow success conditions of the task. In contrast, Wall is more sensitive at the representation level. Encoder quantization leads to severe and irreversible degradation in reconstructed observations starting from the initial frame as shown in Figure 2. Once corrupted, these representation errors persist throughout the rollout and cannot be corrected by planning, even when quantitative success metrics remain relatively high under moderate quantization. Together, these results indicate that low-bit quantization manifests distinct failure modes across tasks, affecting planning effectiveness on PushT and representation fidelity on Wall. Insight V: Aggressive Low-bit Quantization Disrupts the Planning Objective and Long-horizon Rollout Dynamics. Under aggressive quantization settings (e.g., Wall with W3 or W4A4, and PushT with W4A8), we consistently observe that the planning loss fails to decrease and even increases over optimization iterations as shown in Figure 3. This behavior suggests that quantization degrades not only predictive accuracy, but also the structure of the rollout dynamics, resulting in planning objective that is poorly aligned with true task success. Consequently, additional planning iterations yield diminishing or even negative returns. Rather than merely slowing convergence, severe precision constraints induce mismatch between the learned world model and the planner, rendering optimization-based planning ineffective. An Empirical Study of World Model Quantization xv"
        },
        {
            "title": "5 Conclusion",
            "content": "We presented systematic empirical study of PTQ for world models in planningbased settings, using DINO-WM as representative example. Through extensive experiments across multiple quantization methods, quantization granularity, bitwidths, and planning horizons, we showed that quantization effects in world models extend beyond standard accuracy degradation and are tightly coupled with latent rollout dynamics and planning objectives. Our results highlight strong asymmetries in quantization sensitivity across model components, taskdependent failure modes, and the limits of aggressive low-bit precision in longhorizon planning. We hope these findings provide practical guidance for deploying quantized world models under strict computational constraints and motivate future work on quantization strategies that explicitly account for long-horizon planning dynamics."
        },
        {
            "title": "References",
            "content": "1. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., et al.: Video generation models as world simulators. OpenAI Blog 1(8), 1 (2024) ii 2. Caron, M., Touvron, H., Misra, I., JÃ©gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers (2021), https://arxiv. org/abs/2104.14294 iv 3. Choukroun, Y., Kravchik, E., Yang, F., Kisilev, P.: Low-bit quantization of neural networks for efficient inference. In: 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). pp. 30093018. IEEE (2019) iii, iv, vi 4. Fu, J., Kumar, A., Nachum, O., Tucker, G., Levine, S.: D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219 (2020) 5. Ge, Z., Huang, H., Zhou, M., Li, J., Wang, G., Tang, S., Zhuang, Y.: Worldgpt: Empowering llm as multimodal world model (2024), https://arxiv.org/abs/2404. 18202 iv 6. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.W., Keutzer, K.: survey of quantization methods for efficient neural network inference (2021), https://arxiv. org/abs/2103.13630 iv 7. Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P.: Deep learning with limited numerical precision (2015), https://arxiv.org/abs/1502.02551 iv 8. Ha, D., Schmidhuber, J.: Recurrent world models facilitate policy evolution (2018), https://arxiv.org/abs/1809.01999 iii 9. Hafner, D., Lillicrap, T., Ba, J., Norouzi, M.: Dream to control: Learning behaviors by latent imagination (2020), https://arxiv.org/abs/1912.01603 iii 10. Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., Davidson, J.: Learning latent dynamics for planning from pixels (2019), https://arxiv.org/ abs/1811.04551 iii 11. Hafner, D., Pasukonis, J., Ba, J., Lillicrap, T.: Mastering diverse domains through world models (2024), https://arxiv.org/abs/2301.04104 iv 12. Krishnamoorthi, R.: Quantizing deep convolutional networks for efficient inference: whitepaper. arXiv preprint arXiv:1806.08342 (2018) iii, iv, vi xvi Zhongqian Fu et al. 13. LeCun, Y.: path towards autonomous machine intelligence version 0.9. 2, 202206-27. Open Review 62(1), 162 (2022) 14. Li, X., He, X., Zhang, L., Wu, M., Li, X., Liu, Y.: comprehensive survey on world models for embodied ai (2025), https://arxiv.org/abs/2510.16732 iii, iv 15. Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., Gu, S.: Brecq: Pushing the limit of post-training quantization by block reconstruction (2021), https://arxiv.org/abs/2102.05426 iv 16. Lin, H., Xu, H., Wu, Y., Cui, J., Zhang, Y., Mou, L., Song, L., Sun, Z., Wei, Y.: Duquant: Distributing outliers via dual transformation makes stronger quantized llms (2024), https://arxiv.org/abs/2406.01721 iv 17. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.M., Wang, W.C., Xiao, G., Dang, X., Gan, C., Han, S.: Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems 6, 87100 (2024) iii, iv, vi 18. Liu, Y., Yang, H., Dong, Z., Keutzer, K., Du, L., Zhang, S.: Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers (2023), https://arxiv.org/abs/2211.16056 iv 19. Liu, Z., Wang, Y., Han, K., Zhang, W., Ma, S., Gao, W.: Post-training quantization for vision transformer. Advances in Neural Information Processing Systems 34, 2809228103 (2021) ii, iv 20. Nagel, M., Fournarakis, M., Amjad, R.A., Bondarenko, Y., van Baalen, M., Blankevoort, T.: white paper on neural network quantization (2021), https: //arxiv.org/abs/2106.08295 iv 21. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.Y., Li, S.W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without supervision (2024), https://arxiv.org/ abs/2304.07193 iv 22. Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., Luo, P.: Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137 (2023) iii, iv, vi 23. Sun, Y., Liu, R., Bai, H., Bao, H., Zhao, K., Li, Y., Hu, J., Yu, X., Hou, L., Yuan, C., Jiang, X., Liu, W., Yao, J.: Flatquant: Flatness matters for llm quantization (2025), https://arxiv.org/abs/2410.09426 iv 24. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D.d.L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al.: Deepmind control suite. arXiv preprint arXiv:1801.00690 (2018) 25. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (2023), https://arxiv.org/abs/1706. 03762 iv 26. Wei, X., Gong, R., Li, Y., Liu, X., Yu, F.: Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization (2023), https://arxiv.org/abs/ 2203.05740 iv 27. Wu, P., Escontrela, A., Hafner, D., Abbeel, P., Goldberg, K.: Daydreamer: World models for physical robot learning. In: Conference on robot learning. pp. 22262240. PMLR (2023) ii 28. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., Han, S.: Smoothquant: Accurate and efficient post-training quantization for large language models. In: International conference on machine learning. pp. 3808738099. PMLR (2023) ii, iii, iv, vi An Empirical Study of World Model Quantization xvii 29. Yuan, Z., Xue, C., Chen, Y., Wu, Q., Sun, G.: Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization (2024), https://arxiv. org/abs/2111.12293 iv 30. Zhou, G., Pan, H., LeCun, Y., Pinto, L.: Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983 (2024) ii, iv, 31. Zhu, Z., Wang, X., Zhao, W., Min, C., Li, B., Deng, N., Dou, M., Wang, Y., Shi, B., Wang, K., et al.: Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520 (2024) i"
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab"
    ]
}