{
    "paper_title": "Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning",
    "authors": [
        "Yuanda Xu",
        "Hejian Sang",
        "Zhengze Zhou",
        "Ran He",
        "Zhipeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 0 2 4 1 2 . 2 0 6 2 : r Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning Yuanda Xu* Hejian Sang* Zhengze Zhou* Ran He* Zhipeng Wang LinkedIn Corporation, CA, USA *Co-first authors with equal contribution. ericxu@linkedin.com zhipwang@linkedin.com"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from well-documented pathology: while improving Pass@1 through sharpened sampling, they simultaneously narrow the models reasoning boundary and reduce generation diversity. We identify root cause that existing methods overlook: the uniform penalization of errors. Current approacheswhether data-filtering methods that select prompts by difficulty, or advantage normalization schemestreat all incorrect rollouts within group identically. We show that this uniformity allows overconfident errorsincorrect reasoning paths that the RL process has spuriously reinforcedto persist and monopolize probability mass, suppressing valid exploratory trajectories. We propose the Asymmetric Confidence-aware Error Penalty (ACE), which introduces per-rollout confidence shift metric ci = log(πθ(yix)/πref (yix)) to dynamically modulate negative advantages. Theoretically, we show that ACEs gradient can be decomposed into the gradient of selective regularizer restricted to overconfident errors, plus well-characterized residual that partially moderates the regularizers strength (Theorem 1). Experiments fine-tune Qwen2.5-Math-7B [Qwen Team et al., 2024], Qwen3-8B-Base [Qwen Team et al., 2025], and Llama3.1-8B-Instruct [Grattafiori et al., 2024] on the DAPO-Math-17K dataset [Yu et al., 2025] using GRPO and DAPO with VERL [Volcano Engine, 2024], evaluating on MATH-500 [Hendrycks et al., 2021] and AIME 2025. ACE composes with both GRPO and DAPO, consistently improving the full Pass@k spectrum across all three model families and benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) [DeepSeek-AI et al., 2025, OpenAI et al., 2024] has emerged as primary method for post-training Large Language Models (LLMs) on reasoning tasks. By using binary correctness signals from deterministic verifiers, algorithms such as PPO [Schulman et al., 2017], GRPO [Shao et al., 2024], and REINFORCE [Williams, 1992] iteratively refine the models Chain-of-Thought (CoT) generation [Wei et al., 2022]. Despite its successes, growing body of evidence reveals fundamental tension in RLVR training. While RLVR models excel at Pass@1, they consistently underperform their own base models at Pass@k for large [Chen et al., 2021, Yue et al., 2025, Brown et al., 2024], indicating narrowing of the reasoning boundary rather than an expansion. This phenomenon has been attributed to diversity collapse: the training process concentrates probability mass on small number of successful reasoning paths, suppressing the broader solution space. Preprint. prominent strategy addresses this: Difficulty-based curriculum learning filters prompts to maximize gradient signal. However, such methods operate at macro levelselecting which problems to train onignoring critical micro-level distinction: not all errors are equal. Within incorrect rollouts, we identify distinct regimes: exploratory errors (benign stochastic deviations), self-correcting errors (paths the model is already abandoning), and overconfident errors (spuriously reinforced paths acting as value traps). Standard RLVR penalizes these uniformly. While the global KL penalty βDKL(πθπref ) offers some correction, it is symmetric and indiscriminate, suppressing beneficial exploration alongside harmful overconfidence. Our contribution. We propose to break this dilemma by introducing asymmetric correction at the level of individual rollouts. Our method, ACE (Asymmetric Confidence-aware Error penalty), dynamically amplifies the penalty for overconfident errors using per-rollout confidence shift metric, while leaving exploratory and self-correcting errors largely untouched. Concretely, our contributions are: 1. new analytical dimension. We formalize log(πθ(yix)/πref (yix)) as per-rollout diagnostic that difficulty, and show empirically that overconfident errors accumulate during training (3). 2. Theoretical foundations. We show that ACEs gradient admits decomposition into selective regularizer targeting the overconfident portion of the policy, plus residual term that partially moderates the regularizers correction strength (4.4). error confidence is orthogonal shift = to prompt-level ci 3. Empirical validation. ACE consistently improves the full Pass@k spectrum on MATH-500 and AIME 2025, across three model families (Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama3.1-8B-Instruct) and two base algorithms (GRPO and DAPO), with particularly strong gains at large k, confirming that it preserves and expands the reasoning boundary (5). Figure 1 highlights the core idea: ACE reshapes the negative penalty as smooth, confidencedependent curve, amplifying penalties for overconfident errors while keeping exploratory and selfcorrecting errors close to the base level."
        },
        {
            "title": "2 Related Work",
            "content": "Curriculum and advantage shaping. Curriculum methods [Zeng et al., 2025, Parashar et al., 2025, Zhang et al., 2025] select prompts by difficulty, operating at the prompt level. Advantage shaping methods [Tang et al., 2025, Wen et al., 2025] balance correct vs. incorrect samples at the group level. ACE operates at the rollout level, modulating penalties within incorrect samples based on per-rollout confidence shift. KL regularization in RLHF/RLVR. KL divergence penalties are standard in RLHF pipelines to prevent reward hacking and mode collapse [Ouyang et al., 2022, Stiennon et al., 2020]. The typical formulation adds global term βDKL(πθπref ) that symmetrically penalizes all deviations from the reference. DPO [Rafailov et al., 2023a] implicitly constrains the KL divergence through its closed-form reward parameterization. However, all these methods apply KL penalties uniformly across correct and incorrect outputs alike, suppressing beneficial exploration alongside harmful overconfidence. ACE introduces an asymmetric and selective KL-like penalty that targets only overconfident errors while leaving correct outputs and self-correcting errors untouched. Entropy regularization and clipping strategies. Entropy bonuses have long history in RL for encouraging exploration [Williams, 1992, Schulman et al., 2017]. In the LLM context, DAPO [Yu et al., 2025] combats entropy collapse through its Clip-Higher strategy, which decouples the upper and lower clipping thresholds of the importance sampling ratio to give low-probability exploration tokens more room for probability increase. While such clipping-based strategies promote diversity globally, they operate at the token level and cannot distinguish between beneficial diversity on correct reasoning paths and harmful persistence of incorrect ones. ACE provides complementary, more targeted mechanism: rather than modifying the clipping bounds, it modulates the penalty magnitude per rollout based on confidence shift, achieving diversity preservation as consequence of selectively suppressing overconfident errors (see 5.4). 2 Figure 1: ACE Method Overview. Top: Incorrect rollouts fall into three regimes based on the confidence shift ci = log(πθ(yix)/πref (yix)). Bottom-left: Standard GRPO assigns uniform penalty ˆA to all errors regardless of regime. Bottom-right: ACE modulates the penalty via Softplus(ci), strongly penalizing overconfident errors while leaving self-correcting errors nearly untouched. Reward shaping. Potential-based reward shaping [Ng et al., 1999, Wiewiora et al., 2003, Devlin and Kudenko, 2012] transforms the reward function to accelerate learning while preserving the optimal policy. ACE can be viewed through the reward shaping lens: the confidence-dependent term α Softplus(ci) acts as an auxiliary reward signal derived from the policyreference divergence. Unlike classical potential-based shaping, ACEs shaping signal is asymmetric (applied only to negative advantages) and adaptive (it evolves with the policy). Process reward models [Lightman et al., 2023] offer another form of reward enrichment at the step level; ACE is complementary, operating at the trajectory level with zero additional annotation cost. Diversity loss in RLVR. Yue et al. [Yue et al., 2025] show RLVR narrows reasoning boundaries. Negative sample reinforcement [Zhu et al., 2025] demonstrates the importance of learning from incorrect rollouts but does not differentiate among error types. We identify overconfident errors as key mechanism driving diversity collapse and propose confidence-based differential penalization to address this."
        },
        {
            "title": "3 Preliminaries",
            "content": "Setting. We consider policy πθ parameterized by θ, initialized from reference model πref . Given prompt D, the model generates rollouts {y1, . . . , yG}. Each rollout receives reward ri from reward function or verifier. We define: Empirical mean reward: ˆµx = 1 (cid:80)G i=1 ri Empirical reward standard deviation: ˆσx = (cid:113) 1 (cid:80)G i=1(ri ˆµx)2 Above-average rollout set: +(x) = {yi : ri > ˆµx} 3 Below-average rollout set: (x) = {yi : ri ˆµx} GRPO objective. rollout yi is computed via group normalization: In Group Relative Policy Optimization [Shao et al., 2024], the advantage for ˆAi = ri ˆµx ˆσx + ϵ (1) where ϵ is small constant for numerical stability. The clipped surrogate objective is: LGRPO(θ) = ExD (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:16) min ρi ˆAi, clip(ρi, 1ϵc, 1+ϵc) ˆAi (cid:35) (cid:17) + βDKL(πθπref ) (2) where ρi = πθ(yix)/πold(yix) is the importance sampling ratio and ϵc is the clipping threshold. Observation: uniform penalty within groups. For rollouts with identical rewards ri = rj, the advantages are also identical: ˆAi = ˆAj. In the special case of binary rewards, all incorrect rollouts (ri = 0) share the same advantage: ˆA = ˆµx ˆσx + ϵ (3) More generally, rollouts with the same reward receive identical advantage values regardless of their qualitative differences. The only per-rollout modulation comes from the importance ratio ρi, which is bounded by clipping and provides limited differentiation. ci shift confidence overconfident the per-rollout errors. Define = Motivation: log(πθ(yix)/πref (yix)): positive values indicate the policy has become more confident than the reference on rollout yi, while negative values indicate the opposite. Training Qwen2.5-Math7B with standard GRPO on DAPO-Math-17K [Yu et al., 2025], we observe that the distribution of ci among incorrect rollouts develops heavy right tail as training progressesa substantial fraction of errors become significantly more probable under the trained policy than under the reference, even though they remain incorrect. This is consistent with analyses of implicit reward distributions in preference optimization [Rafailov et al., 2023b, Meng et al., 2024]. These overconfident errors consume probability mass that would otherwise support diverse reasoning paths, contributing to the diversity collapse documented by Yue et al. [2025]. Crucially, the standard global KL penalty βDKL(πθπref ) cannot selectively address this: it penalizes all deviations from the reference proportionally, suppressing beneficial confidence growth on correct paths alongside harmful overconfidence on incorrect ones. This structural limitation motivates targeted correction mechanism (see 5.3 for detailed quantitative tracking)."
        },
        {
            "title": "4 The ACE Method",
            "content": "4.1 Error Confidence Score Definition 1 (Error Confidence Score). For prompt and an incorrect rollout yi (x), the error confidence score is: ci log πθ(yix) πref (yix) = Ti(cid:88) t=1 log πθ(y(t) πref (y(t) x, y(<t) ) x, y(<t) ) (4) where y(t) denotes the t-th token and Ti is the sequence length. The second equality decomposes the sequence-level confidence into sum of token-level log-ratios. This is important for two reasons: (a) it shows that ci is already computed as byproduct of standard RLVR training (which requires log πθ and log πref for the KL penalty and importance ratios), incurring zero additional compute; and (b) it reveals that ci aggregates confidence shifts across all reasoning steps, naturally weighting tokens where the policy has diverged most from the reference. Remark 1 (Three regimes). The sign of ci partitions incorrect rollouts into interpretable regimes: 4 ci > 0: Overconfident errors. The policy assigns higher probability than the reference. These are spurious patterns actively learned during RL. ci 0: Exploratory errors. Probability approximately unchanged from the reference. Natural stochastic deviations. ci < 0: Self-correcting errors. The policy has already reduced probability mass relative to the reference. 4.2 The ACE Advantage We restructure the negative advantage to depend on the per-rollout confidence score ci. Definition 2 (ACE Advantage). For an incorrect rollout yi (x), the ACE advantage is: ACE,i = ˆA (1 + α Softplus(ci)) (5) where ˆA = (ri ˆµx)/(ˆσx + ϵ) is the standard GRPO advantage for incorrect rollouts and α 0 is hyperparameter controlling the correction strength. Since ˆA < 0 and (1 + α Softplus(ci)) 1, ACE strictly amplifies the magnitude of the penalty. For correct rollouts yi +(x), we retain the standard GRPO advantage: A+ ACE,i = ˆAi = ri ˆµx ˆσx + ϵ (6) Design rationale. The Softplus function Softplus(z) = log(1 + ez) is chosen for three properties: 1. Asymptotic behavior. When ci 0 (overconfident), Softplus(ci) ci: penalty scales linearly with the log-confidence ratio. When ci 0 (self-correcting), Softplus(ci) eci 0: penalty converges to the base GRPO advantage ˆA . 2. Smoothness. Unlike max(0, ci) (which has non-differentiable kink at 0), Softplus is infinitely differentiable everywhere, ensuring smooth gradient flow. 3. Monotonicity. Softplus is strictly increasing, so more confident errors always receive strictly larger penalties, consistent with our theoretical motivation. Comparison to uniform penalization. To illustrate the effect of ACE, consider the binary reward case where rollouts receive ri {0, 1}. Under standard GRPO, all incorrect rollouts (ri = 0) share the same advantage ˆA = ˆpx/(ˆσx +ϵ), where ˆpx is the empirical pass rate and ˆσx = (cid:112)ˆpx(1 ˆpx). Difficulty-adaptive scaling. Since ACE multiplies the standard GRPO advantage ˆA by (1 + α Softplus(ci)), it naturally inherits GRPOs difficulty-dependent scaling: easy prompts (high pass rate) produce larger ˆA , so errors on easy problems are penalized more heavily. The confidence modulation then provides additional per-rollout differentiation within each difficulty level. Penalty differentiation. Under ACE: ACE,i = ˆA (1 + α log(1 + eci)) = ACE,i is strictly increasing in ci (7) Therefore, within the same group, an overconfident error (ci = 2) receives penalty ˆA(1+α2.13) while an exploratory error (ci = 0) receives ˆA (1 + α 0.69), and self-correcting error (ci = 3) receives ˆA (1 + α 0.05). This provides fine-grained differentiation that is impossible under uniform penalization. The same principle extends to continuous rewards, where ACE differentiates among below-average rollouts based on their confidence scores. ACE in One Sentence Standard RLVR punishes all wrong answers equally. ACE punishes wrong answers the model has learned to be confident in much harder, while leaving natural exploration mistakes alone. 5 Algorithm 1 ACE-GRPO: Asymmetric Confidence-aware Error Penalty Require: Policy πθ, reference model πref , prompt dataset D, group size G, ACE strength α, clipping ϵc, KL coefficient β 1: for each training step do 2: 3: 4: 5: 6: 7: Sample prompt batch {x1, . . . , xB} for each prompt in batch do Generate rollouts {y1, . . . , yG} πθ(x) Compute rewards ri {0, 1} via verifier Compute standard group advantages ˆAi via GRPO for each incorrect rollout yi with ri = 0 do ) log πref (y(t) (1 + α log(1 + exp(ci))) ci end for Compute clipped surrogate loss (Eq. 8) t=1 log πθ(y(t) ACE,i ˆA (cid:16)(cid:80)Ti ) (cid:17) /Ti end for Update θ via gradient descent 8: 9: 10: 11: 12: 13: 14: end for // Already computed // Amplify uniform advantage 4.3 ACE-GRPO: Integration and Algorithm ACE modifies only the advantage computation for negative samples. Substituting the ACE advantage (Definition 2) into the GRPO objective (Eq. 2), the full ACE-GRPO objective is: LACE(θ) = ExD (cid:34) 1 (cid:88) i= where: (cid:0)I[ri=1] L+ + I[ri=0] (cid:35) (cid:1) + βDKL(πθπref ) (8) L+ (cid:16) = min (cid:16) = min ρi ˆA+ , clip(ρi, 1ϵc, 1+ϵc) ˆA+ (cid:17) ρiA ACE,i, clip(ρi, 1ϵc, 1+ϵc)A ACE,i (cid:17) (9) (10) The positive advantages ˆA+ retain the standard GRPO formulation. In practice, we normalize ci by sequence length (ci = ci/Ti) to ensure Practical considerations. comparable penalty magnitudes across rollouts of different lengths. Additional implementation details (sequence-level vs. token-level aggregation, clipping choices) and PyTorch implementation are provided in Appendix C. The full algorithm is given below. 4.4 Relationship to Selective Reverse KL Divergence We now characterize the theoretical relationship between ACEs additional penalty and selective regularizer that targets overconfident errors. Crucially, the equivalence is not exact: ACE implements stop-gradient (reward-shaping) view of the confidence score, which omits residual term compared to the full regularizer gradient. We state the exact decomposition below. Theorem 1 (Selective Regularization Decomposition). Let Lstd(θ) denote the standard policy gradient objective (Eq. 2) with uniform negative advantages ˆA, and let LACE(θ) denote the objective with ACE advantages (Eq. 5). Define the selective regularizer: Rsel(θ) = ExD ˆA(x) (cid:88) yY (x) πθ(yx) Softplus log (cid:18) (cid:19) πθ(yx) πref (yx) (11) where ˆA(x) is the magnitude of the standard GRPO negative advantage for prompt x. Assume rollouts are sampled on-policy from πθ. Then, in the infinite-sample limit (G ), the α-dependent 6 additional gradient from ACE decomposes exactly as: θ = αθRsel(θ) + α ExD ˆA(x) (cid:88) yY (x) πθ(yx) σ(c(y)) θ log πθ(yx) (12) where σ(c) = 1/(1 + ec) is the sigmoid function (i.e., Softplus(c)). Equivalently, ACE implements the negative gradient of Rsel with the confidence modulation treated as fixed reward signal (stopgradient on ci), plus the residual term E(θ): E(θ) = ExD ˆA(x) (cid:88) yY (x) πθ(yx) σ(c(y)) θ log πθ(yx) (13) Moreover, for overconfident errors where c(y) 0, Softplus(c) c, and the dominant term in Rsel takes the form of difficulty-weighted reverse KL divergence restricted to overconfident incorrect trajectories: Rsel(θ) ExD ˆA(x) (cid:88) yY (x) c(y)>0 πθ(yx) log πθ(yx) πref (yx) (14) The proof is provided in Appendix A. Intuitively, ACEs stop-gradient treatment of Softplus(ci) captures the dominant selective-regularization component (Term I: confidence-weighted probability suppression), while the residual E(θ) corresponds to the through-ci gradient (Term II) that the full regularizer would additionally apply. By omitting Term II, ACE implements tempered version of Rselless aggressive than the full regularizer, but more targeted than standard GRPO. The per-prompt factor ˆA(x) ensures that the selective regularizer inherits the difficulty-adaptive scaling of GRPO. In contrast to the global KL term βDKL(πθπref ) which indiscriminately pulls back all deviations, Rsel is (i) restricted to incorrect outputs (y ), (ii) activated primarily by overconfidence (ci > 0) due to Softplus saturation, (iii) independently tunable via α, and (iv) difficulty-adaptive via the ˆA(x) factor. 4.5 Gradient Quality Analysis natural question is whether ACEs confidence-dependent reweighting improves or degrades gradient quality. We analyze this in detail in Appendix and summarize the key results here. First, ACE necessarily increases both the total gradient second moment and the directional variance unavoidable consequences of additive reweighting where (1+αϕi) > 1 for all ϕi > 0 (Proposition 1). However, this does not prevent quality improvement. We define the gradient quality ratio as Qd = d/σ2 µ2 d, measuring the ratio of squared directional signal to directional variance. Under realistic conditionsspecifically, when overconfident errors carry gradients aligned with the optimization direction (Cov(ϕi, ui) > 0) and the baseline gradient is noisy (Qstd < 1)we prove that ACE strictly improves gradient quality: QACE (Theorem 2). The key mechanism is that ACEs selective amplification concentrates extra weight on the most informative gradients, causing the signal to grow faster than the noise along the optimization-relevant direction. > Qstd Theoretical Insight: Gradient Efficiency ACE converts harmful variance into exploitable signal. By concentrating penalty weight on overconfident errorswhich have gradients more aligned with the optimization direction ACE achieves higher signal-to-noise ratio despite increasing total gradient variance. This is the statistical foundation for ACEs improved learning efficiency."
        },
        {
            "title": "5 Experiments: ACE Expands the Reasoning Boundary",
            "content": "5.1 Experimental Setup Models. We fine-tune Qwen2.5-Math-7B [Qwen Team et al., 2024], Qwen3-8B-Base [Qwen Team et al., 2025], and Llama-3.1-8B-Instruct [Grattafiori et al., 2024] using GRPO implemented with VERL [Volcano Engine, 2024]. Note that Qwen3-8B-Base is evaluated without enabling the extended thinking mode (i.e., reasoning mode disabled). Llama-3.1-8B-Instruct is included in the main results (Tables 1 and 2) to test cross-family generalization beyond the Qwen model family. For the detailed diagnostic experiments (5.35.4), ablations (5.5), and hyperparameter sensitivity (Appendix D), we focus on the two Qwen models because: (i) they serve as the primary experimental subjects and already span two distinct pretraining recipes (math-specialized vs. general-purpose base model), providing sufficient diversity to validate the generality of our findings; and (ii) Llama-3.1-8B-Instruct operates in substantially lower accuracy regime (e.g., near-floor on AIME 2025), which makes finegrained diagnostics such as overconfident error distributions and entropy dynamics less statistically informative. Training data. We use the DAPO-Math-17K dataset [Yu et al., 2025] as the training prompts. For the GRPO and ACE-GRPO baselines we use standard GRPO (symmetric clipping, with KL penalty); for DAPO and ACE-DAPO we use the full DAPO algorithm [Yu et al., 2025] with Clip-Higher, dynamic sampling, and token-level loss. Evaluation. We evaluate on MATH-500 [Hendrycks et al., 2021] and AIME 2025 using rule-based math verifier for correctness verification. Metrics. We report Pass@k for {1, 2, 4, 8, 16, 32} using temperature 0.7 and top-p = 0.95. Pass@k measures the probability that at least one of samples is correct. We use the unbiased estimator from Chen et al. [2021]: (cid:34) Pass@k = ExD 1 (cid:35) (cid:0)nc (cid:1) (cid:1) (cid:0)n (15) where is the total samples and is the number correct. Pass@1 reflects exploitation; large-k reflects exploration and reasoning boundary. Baselines. Base model: Unmodified pretrained model (upper bound for large-k diversity). GRPO: Standard Group Relative Policy Optimization [Shao et al., 2024]. DAPO: The full DAPO algorithm [Yu et al., 2025], which uses asymmetric clipping (ClipHigher), dynamic sampling, and token-level loss, trained on the same DAPO-Math-17K dataset. ACE-GRPO: Our method (ACE applied to GRPO). ACE-DAPO: Our method applied on top of DAPO, demonstrating composability with orthogonal diversity-preserving strategies. Hyperparameters. For ACE, we set α = 1.0 as the default. We use normalized confidence scores ci = ci/Ti. Full training hyperparameters are provided in Appendix E. Fair comparison. Within each model, we keep the training recipe and budget matched across methods; see Appendix E. 5.2 Main Results: Full Pass@k Spectrum Table 1 reports Pass@k on MATH-500 and Table 2 reports results on AIME 2025. Table 1: Pass@k (%) on MATH-500. We report mean 95% confidence interval over 5 independent training runs. Bold = best within each model group; underline = second best. Model @1 @2 @4 @ @16 @32 Qwen2.5-Math-7B Qwen2.5-Math-7B + GRPO Qwen2.5-Math-7B + DAPO Qwen2.5-Math-7B + ACE-GRPO Qwen2.5-Math-7B + ACE-DAPO Qwen3-8B-Base Qwen3-8B-Base + GRPO Qwen3-8B-Base + DAPO Qwen3-8B-Base + ACE-GRPO Qwen3-8B-Base + ACE-DAPO 63.0 73.4 0.8 74.5 1.0 74.2 0.7 75.1 0.8 60.2 69.4 0.9 70.8 1.0 70.1 0.7 71.2 0. 48.1 Llama-3.1-8B-Instruct 52.9 1.1 Llama-3.1-8B-Instruct + GRPO Llama-3.1-8B-Instruct + DAPO 54.3 1.0 Llama-3.1-8B-Instruct + ACE-GRPO 54.1 1.1 Llama-3.1-8B-Instruct + ACE-DAPO 55.4 1.1 76.3 79.5 0.7 80.8 0.9 80.9 0.7 82.4 0.8 72.5 75.5 0.9 76.8 0.9 76.5 0.7 77.5 0.9 59.9 60.5 1.0 61.8 1.0 62.2 1.1 62.8 1.1 83.2 83.2 0.7 84.8 0.8 84.5 0.6 86.2 0.6 78.8 79.3 0.9 81.1 0.9 81.1 0.6 82.3 0. 67.8 67.3 0.9 68.9 1.0 69.1 1.0 70.2 1.0 88.1 86.2 0.5 89.5 0.7 88.9 0.5 91.2 0.5 83.9 82.5 0.9 84.3 0.8 84.5 0.6 85.4 0.8 74.8 71.8 0.9 72.9 1.0 73.5 1.0 74.1 1.0 91.2 89.7 0.5 93.0 0.7 92.6 0.5 94.7 0.5 87.2 86.2 0.8 88.1 0.8 88.5 0.6 89.4 0. 80.5 75.5 0.9 76.8 0.9 76.8 0.9 77.9 0.9 93.5 91.3 0.5 94.6 0.6 94.3 0.4 96.1 0.5 90.6 88.6 0.7 90.4 0.8 91.1 0.5 91.6 0.7 84.8 79.3 0.8 80.4 0.9 81.5 0.9 82.1 0.9 Table 2: Pass@k (%) on AIME 2025. AIME 2025 contains 30 problems; we report point estimates as confidence intervals are dominated by test-set size rather than training variance. Bold = best within each model group; underline = second best. Model Qwen2.5-Math-7B Qwen2.5-Math-7B + GRPO Qwen2.5-Math-7B + DAPO Qwen2.5-Math-7B + ACE-GRPO Qwen2.5-Math-7B + ACE-DAPO Qwen3-8B-Base Qwen3-8B-Base + GRPO Qwen3-8B-Base + DAPO Qwen3-8B-Base + ACE-GRPO Qwen3-8B-Base + ACE-DAPO Llama-3.1-8B-Instruct Llama-3.1-8B-Instruct + GRPO Llama-3.1-8B-Instruct + DAPO Llama-3.1-8B-Instruct + ACE-GRPO Llama-3.1-8B-Instruct + ACE-DAPO 1 6.3 10.5 11.5 11.2 11.7 5.1 9.7 11.1 10.5 11. 0.2 0.2 0.3 0.3 0.2 2 9.9 14.9 16.7 16.0 17.4 9.2 13.9 15.7 15.5 16.9 0.7 0.3 0.3 0.3 0.3 13.8 19.7 22.5 21.2 23.8 11.6 17.4 19.9 19.6 21.2 1.2 0.5 0.6 0.5 0.6 8 17.5 23.9 27.5 26.1 28.5 14.2 22.5 25.2 24.7 26. 3.2 2.1 1.9 2.2 2.0 16 21.9 28.6 31.8 30.6 33.1 17.0 25.7 28.5 27.9 29.9 7.1 3.0 2.8 3.9 3.2 26.7 33.7 37.1 36.4 38.6 19.6 29.8 33.1 32.4 34.4 10.8 7.0 6.3 8.2 7.1 Key findings (Qwen2.5-Math-7B). As shown in Figure 2, ACE consistently improves larger-k metrics while maintaining comparable Pass@1. On MATH-500, ACE-GRPO improves Pass@32 from 91.3% to 94.3% (+3.0pp) over GRPO; ACE-DAPO further pushes Pass@32 to 96.1% (+1.5pp over DAPOs 94.6%). On AIME 2025, ACE-GRPO improves Pass@32 from 33.7% to 36.4% (+2.7pp); ACE-DAPO reaches 38.6% (+1.5pp over DAPOs 37.1%). Notably, ACE-DAPO achieves the strongest results across all k, demonstrating that ACE composes effectively with orthogonal diversity-preserving strategies. Key findings (Qwen3-8B-Base). The same pattern holds on different model family. On MATH500, ACE-GRPO improves Pass@32 from 88.6% to 91.1% (+2.5pp); ACE-DAPO reaches 91.6% (+1.2pp over DAPOs 90.4%). On AIME 2025, ACE-GRPO improves Pass@32 from 29.8% to 32.4% (+2.6pp); ACE-DAPO reaches 34.4% (+1.3pp over DAPOs 33.1%). Key findings (Llama-3.1-8B-Instruct). To test cross-family generalization, we evaluate on non-Qwen model. On MATH-500, ACE-GRPO improves Pass@32 from 79.3% to 81.5% (+2.2pp); ACE-DAPO reaches 82.1% (+1.7pp over DAPOs 80.4%). On AIME 2025where Llama-3.19 8B-Instruct operates near the floorACE-GRPO improves Pass@32 from 7.0% to 8.2% (+1.2pp), demonstrating that ACEs mechanism transfers across model families even under low-accuracy regimes. These consistent gains across all three model families confirm the generality of ACEs mechanism. Interaction with DAPOs Clip-Higher. natural observation is that ACEs marginal gain over DAPO is smaller than over GRPO (e.g., on MATH-500 Qwen2.5-Math-7B Pass@32: +3.0pp for ACE-GRPO vs. GRPO, but +1.5pp for ACE-DAPO vs. DAPO). This reflects genuine mechanism overlap: DAPOs Clip-Higher preserves diversity by limiting how aggressively any incorrect path is suppressed at the token level, which indirectly reduces the overconfident-error pathology that ACE targets. However, DAPOs protection is indiscriminateit shields overconfident errors and exploratory errors alike, because token-level clipping cannot distinguish trajectory-level confidence regimes. ACE provides the missing selectivity: it amplifies suppression specifically for errors the model has learned to be confident in, while leaving exploratory errors untouched. The consistent gains of ACE-DAPO over DAPO across all model families and benchmarks indicate that this trajectorylevel selectivity captures dimension of the overconfidence problem that token-level clipping alone cannot resolve. The diminishing marginal returns are expectedboth methods partially address the same pathologybut the residual improvement confirms that ACEs rollout-level discrimination provides value beyond what DAPOs uniform token-level mechanism achieves. Main Result ACE preserves Pass@1 performance while significantly expanding the reasoning boundary at large k. Across three model families (Qwen2.5-Math-7B, Qwen3-8B-Base, Llama-3.18B-Instruct) and two benchmarks (MATH-500 and AIME 2025), ACE-GRPO consistently improves Pass@32 by +2.23.0pp over GRPO. Moreover, ACE composes with DAPO: ACEDAPO achieves the best overall results (e.g., 96.1% on MATH-500 Pass@32), improving over DAPO by +1.21.7pp, confirming that ACE provides complementary correction orthogonal to token-level diversity strategies and generalizes across model families. 5.3 Experiment 1: Overconfident Error Dynamics Goal. Quantify the prevalence of overconfident errors during training and demonstrate that ACE effectively reduces them. Design. Track the distribution of ci among incorrect rollouts throughout training for both standard GRPO and ACE-GRPO on the two Qwen models.1 At checkpoints every 25 training steps, generate 32 rollouts per prompt on held-out set and record ci for all incorrect rollouts. Metrics. Overconfident error fraction: OEF(t) = {yi : ci > 0}/Y at step t. Mean overconfidence magnitude: E[ci ci > 0, ri = 0] at step t. Token-level entropy: Average per-token entropy of the policy. Results. The core claim of ACE is that standard GRPO allows incorrect rollouts to become increasingly overconfident during training, and that ACEs asymmetric penalty should counteract this pathology. To test this, we track two complementary diagnostics at every checkpoint (Figure 3): (i) the overconfident error fraction (OEF), which measures the proportion of incorrect rollouts whose confidence has grown relative to the reference policy (ci > 0), and (ii) the mean overconfidence magnitude among those overconfident errors, which captures the severity of the problem. Throughout training, ACE-GRPO maintains lower OEF and lower mean overconfidence magnitude than standard GRPO at every recorded checkpoint, indicating that ACE consistently suppresses both the prevalence and the severity of high-confidence incorrect rollouts. 1We omit Llama-3.1-8B-Instruct from the diagnostic experiments as its lower baseline accuracy yields fewer correct rollouts per group, making the confidence shift statistics noisier and less informative. The main results in Tables 12 confirm that ACEs gains transfer to Llama. 10 Figure 2: Performance Comparison across Benchmarks. Pass@k curves for all five methods on MATH-500 (left column) and AIME 2025 (right column) across three model families: Qwen2.5-Math7B (top row), Qwen3-8B-Base (middle row), and Llama-3.1-8B-Instruct (bottom row). ACE-GRPO and ACE-DAPO consistently outperform their respective baselines (GRPO and DAPO) across all sampling budgets, model families, and benchmarks, with larger gains at higher values. ACE-DAPO achieves the best overall performance, confirming that ACEs rollout-level correction composes with DAPOs token-level diversity preservation and generalizes across model families. 5.4 Experiment 2: Entropy Dynamics Goal. Verify that ACE preserves generation diversity by tracking entropy throughout training, and establish the connection between entropy and Pass@k performance. 11 Figure 3: Overconfident Error Dynamics. Left: Overconfident error fraction (OEF) over training. Right: Mean overconfidence magnitude for ci > 0 errors. ACE-GRPO effectively suppresses both metrics compared to standard GRPO. Design. Over the first 20 training steps, compute the average per-token entropy of the policy on held-out subset of DAPO-Math-17K prompts: H(t) = 1 Dval (cid:88) xDval 1 T (cid:88) (cid:88) j=1 πθ(vx, y<j) log πθ(vx, y<j) (16) where is the average sequence length and ranges over the vocabulary. Results. key concern with aggressive error suppression is that it may cause premature mode collapse, concentrating probability mass on narrow set of outputs and destroying the diversity needed for high Pass@k at large k. To diagnose this, we track average per-token entropy H(t) over the early phase of training, where entropy decay is most rapid, for both standard GRPO and ACE-GRPO (Figure 4). Standard GRPO exhibits sharp entropy drop within the first 20 steps, retaining only small fraction of its initial entropy. In contrast, ACE-GRPO decays substantially more slowly, preserving much larger fraction of the initial entropy over the same period. This gap correlates with Pass@k performance at large k: the method that retains more entropy also achieves higher coverage, confirming that ACEs selective penalty avoids premature mode collapse while still suppressing overconfident errors. 5.5 Ablation: Choice of Modulation Function natural question is whether the choice of Softplus as the modulation function is important, or whether simpler alternative such as ReLU(ci) = max(0, ci) suffices. We compare the two variants on MATH-500 using Qwen2.5-Math-7B with α = 1.0 (the ablation uses single representative model to isolate the effect of the modulation function; the main results in Table 1 confirm that ACEs gains are consistent across all three model families): ACE-Softplus (default): ACE,i = ˆA ACE-ReLU: i ACE,i = ˆA (1 + α ReLU(ci)) (1 + α Softplus(ci)) ReLU completely ignores self-correcting and exploratory errors (ci 0), providing zero modulation in that regime, while Softplus provides smooth, everywhere-positive modulation that transitions gradually. Analysis. Both ACE-ReLU and ACE-Softplus outperform standard GRPO across all > 1, confirming that confidence-aware modulationregardless of the specific activationis beneficial. However, ACE-Softplus consistently outperforms ACE-ReLU, with the gap widening at larger (+1. 12 Figure 4: Entropy Dynamics. Token-level entropy over the first 20 training steps. Left: On Qwen2.5-Math-7B, ACE-GRPO retains substantially more entropy than standard GRPO, which suffers rapid entropy collapse. Right: On Qwen3-8B-Base, ACE-GRPO maintains more stable entropy, demonstrating consistency across architectures. We report entropy dynamics for the two Qwen models only; Llama-3.1-8B-Instruct is excluded because its lower baseline accuracy makes the entropy signal less directly comparable (see 5 for discussion). Table 3: Ablation: modulation function on MATH-500 (Qwen2.5-Math-7B, α = 1.0). Method @1 @2 @4 @8 @16 @32 GRPO (baseline) ACE-ReLU ACE-Softplus (ours) 73.4 73.2 74.2 79.5 80.3 80.9 83.2 83.9 84.5 86.2 87.6 88.9 89.7 91.2 92.6 91.3 93.1 94. pp at Pass@32). This advantage stems from two properties of Softplus. First, smoothness: ReLU has non-differentiable kink at ci = 0, creating discontinuity in the gradient landscape that can destabilize training, whereas Softplus provides smooth gradient flow everywhere. Second, non-zero modulation near the boundary: ReLU assigns zero modulation to all errors with ci 0, treating them identically to standard GRPO. In contrast, Softplus(0) = ln 2 0.69, providing gentle baseline modulation that enables finer differentiation among borderline errors near ci 0precisely the regime where errors may be transitioning from exploratory to overconfident. These results empirically validate the design rationale in 4.2. 5.6 Analysis: Mechanism Behind Diversity Preservation The experimental results above (5.35.4) reveal consistent mechanism: standard GRPOs uniform penalties allow overconfident errors to form probability sinks that crowd out valid reasoning pathsthe pathology identified by Yue et al. [2025] as the root cause of RLVRs narrowing reasoning boundary. ACEs asymmetric penalties break this cycle: the selective KL term (Theorem 1) acts as entropy regularization restricted to the overconfident region, while leaving exploratory errors (ci 0) untouched. This targeted correction redistributes probability mass to alternative reasoning paths, explaining ACEs improvements across the full Pass@k spectrum."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "Dependence on reference model quality. ACE uses πref to define overconfidence. If the reference model is poorly calibrated, the confidence score ci may not reliably indicate spurious patterns. Exploring alternatives (e.g., using moving average of recent checkpoints) is direction for future work. 13 Binary rewards only. Our current formulation assumes binary rewards (r {0, 1}). Extending ACE to continuous or partial rewards (e.g., from process reward models) requires redefining what constitutes an overconfident error in the presence of graded feedback. Interaction with long CoT. Extended reasoning models (e.g., with >10K token outputs) may exhibit different confidence shift dynamics. The sequence-length normalization (ci = ci/Ti) may need refinement for very long chains."
        },
        {
            "title": "7 Conclusion",
            "content": "We identified previously overlooked pathology in RLVR training: the accumulation of overconfident errorsincorrect reasoning paths that the RL process spuriously reinforces. We proposed ACE, simple modification to the advantage function that dynamically amplifies penalties for overconfident errors while leaving exploratory errors untouched. Core Contributions (1) We formalize error confidence shift as new per-rollout diagnostic orthogonal to prompt difficulty, revealing that overconfident errors accumulate during RLVR and drive diversity collapse. (2) ACEs gradient decomposes into selective regularizer on overconfident errors plus tempering residual, providing principled theoretical grounding. (3) ACE improves the full Pass@k spectrumespecially at large kwithout sacrificing Pass@1, adding only single Softplus computation per incorrect rollout."
        },
        {
            "title": "References",
            "content": "Qwen Team, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Qwen Team, A. Yang, B. Yang, B. Hui, B. Zheng, C. Li, C. Bao, C. Song, D. Liu, F. Huang, H. Wei, H. Zhang, H. Zhang, J. Tu, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, W. Lin, X. Ren, X. Song, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Zhang, and Z. Liu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mber, A. Kambadur, A. Zhernov, A. Rao, A. Lober, A. Ceballos, B. Chen, B. Eriksson, B. Perret, C. Lovejoy, and L. van der Maaten. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, W. Dai, T. Fan, G. Liu, C. Hu, A. Ren, Z. Wang, X. Chen, P. Gao, W. Shao, Z. Yang, J. Chen, Y. Qiao, Y. Zheng, W. Luo, and L. Chen. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Volcano Engine. VERL: Volcano Engine Reinforcement Learning for LLMs. https://github. com/volcengine/verl, 2024. Open-source implementation of the HybridFlow paper. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. In NeurIPS, 2021. DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, P. Wang, Q. Zhu, R. Xu, R. Zhang, T. Piao, Y. Wu, and Z. Shao. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. OpenAI, A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carrel, A. Berber, and A. Glaese. Openai o1 system card. Technical report, 2024. 14 J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(34):229256, 1992. J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-ofthought prompting elicits reasoning in large language models. In NeurIPS, 2022. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Y. Yue, Z. Chen, R. Lu, A. Zhao, Z. Yang, and H. Hu. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? In NeurIPS, 2025. Oral. B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. Ré, and A. Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Y. Zeng, Z. Sun, B. Ji, E. Min, H. Cai, S. Wang, D. Yin, H. Zhang, and X. Chen. Cures: From gradient analysis to efficient curriculum learning for rlvr. arXiv preprint arXiv:2510.01037, 2025. S. Parashar, S. Gui, X. Li, H. Ling, S. Vemuri, B. Olson, E. Li, Y. Zhang, and J. Caverlee. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632, 2025. S. Zhang, G. Sun, K. Zhang, X. Guo, and R. Guo. Clpo: Curriculum learning meets policy optimization for llm reasoning. arXiv preprint arXiv:2509.25004, 2025. X. Tang, Y. Zhan, Z. Li, W. X. Zhao, Z. Zhang, Z. Wen, Z. Zhang, and J. Zhou. Rethinking sample polarity in reinforcement learning with verifiable rewards. arXiv preprint arXiv:2512.21625, 2025. X. Wen, Z. Liu, S. Zheng, S. Ye, Z. Wu, Y. Wang, Z. Xu, X. Liang, and J. Li. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano. Learning to summarize from human feedback. In NeurIPS, 2020. R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023a. A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, pages 278287, 1999. E. Wiewiora, G. W. Cottrell, and C. Elkan. Principled methods for advising reinforcement learning agents. In ICML, pages 792799, 2003. S. Devlin and D. Kudenko. Dynamic potential-based reward shaping. In AAMAS, pages 433440, 2012. 15 H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. X. Zhu, M. Xia, Z. Wei, W.-L. Chen, D. Chen, and Y. Meng. The surprising effectiveness of negative reinforcement in llm reasoning. In NeurIPS, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023b. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with referencefree reward. arXiv preprint arXiv:2405.14734, 2024. Proof of Theorem 1 (Selective Regularization Decomposition) Proof. The gradient of LACE differs from Lstd only in the negative advantage terms. We analyze the α-dependent component. Since (1 + α Softplus(ci)), the additional gradient relative to standard GRPO is: ACE,i = ˆA θ = αExD ˆA(x) (cid:88) yiY (x) Softplus(ci) θ log πθ(yix) (17) Here ˆA(x) is per-prompt scalar (constant across rollouts within group) that does not depend on yi. This is the standard REINFORCE form: ˆA(x) Softplus(ci) acts as scalar reward multiplying the score function, with ci treated as not depending on θ (the stop-gradient convention standard in policy gradient methods). As , by the law of large numbers: θ αExD ˆA(x) (cid:88) yY (x) πθ(yx) Softplus(c(y)) θ log πθ(yx) = αExD ˆA(x) (cid:88) yY (x) Softplus(c(y)) θπθ(yx) (18) using πθ(yx)θ log πθ(yx) = θπθ(yx). Now, the true gradient of Rsel(θ) requires differentiating ˆA(x) πθ(yx) Softplus(c(y)) where πθ(yx) and Softplus(c(y)) both depend on θ (since c(y) = log πθ(yx) log πref (yx)). Since ˆA(x) is per-prompt scalar, it factors out, and by the product rule: θ [πθ(yx) Softplus(c)] = Softplus(c) θπθ(yx) (cid:125) (cid:124) (cid:123)(cid:122) Term I: captured by ACE + πθ(yx) σ(c) θ log πθ(yx) (cid:125) (cid:123)(cid:122) Term II: residual (cid:124) (19) where σ(c) = Softplus(c) = 1/(1 + ec) and θc = θ log πθ(yx). Multiplying by ˆA(x), summing over (x), and taking expectations, Eq. (18) matches exactly α ˆA(x) Term I. Rearranging: θ = αθRsel + α ExD ˆA(x) (cid:88) yY (x) πθ(yx) σ(c) θ log πθ(yx) (20) (cid:124) (cid:123)(cid:122) E(θ) (cid:125) This is an exact identity with no approximation. The residual E(θ) arises because ACE treats Softplus(ci) as fixed reward signal, omitting the gradient through ci itself. 16 Remark 2 (Residual term and contrast with global KL). The residual E(θ) is not negligible: for [1, 3], the ratio σ(c)/Softplus(c) ranges from 3156%. arises because ACE treats Softplus(ci) as fixed scalar (stop-gradient), omitting the gradient that the full regularizer Rsel would contribute by differentiating through ci (Term II in Eq. 19). This omitted gradient would suppress overconfident errors more aggressively: it drives the parameters to reduce not only πθ(yx) but also the confidence gap c(y) itself. ACE therefore implements tempered version of the full regularizercorrecting overconfident errors via the dominant Term (confidence-weighted probability suppression) while forgoing Term IIs sharper through-c correction. Remark 3 (Why stop-gradient is preferable to the full regularizer). natural question is whether one should retain Term II to implement the full θRsel instead of ACEs tempered version. We argue against this for three reasons. (i) Precedent for detaching θ-dependent signals. Although the reward in vanilla REINFORCE does not depend on θ, modern policy gradient methods routinely stop-gradient through θ-dependent quantities used in the loss: PPO/GRPO detach the advantage ˆAi (computed from the current policys rollouts) from the actor gradient; actor-critic methods detach the value baseline (s; θ) even under parameter sharing; and the old policy πold in importance ratios is always frozen. ACEs treatment of Softplus(ci) as detached reward modifier follows the same principle: quantities that diagnose the policy state should inform gradient magnitude, not become optimization targets themselves. (ii) Feedback loop. Retaining Term II means the penalty magnitude itself becomes an optimization target: the gradient would simultaneously try to reduce πθ(yx) and reduce ci = log(πθ/πref ), creating second-order feedback that can cause gradient oscillation and training instability. (iii) Variance. The gradient quality analysis (Theorem 2) proves that ACEs stop-gradient version improves the quality ratio Qd under realistic conditions. Adding Term II introduces an additional score-function estimator σ(ci)θ log πθ, which increases gradient variance without guaranteed commensurate signal gainthe sufficient condition for quality improvement (Eq. 49) would need to be re-derived and may no longer hold."
        },
        {
            "title": "B Gradient Quality Analysis",
            "content": "This appendix provides the full formal analysis of ACEs effect on gradient quality, summarized in 4.5. Assumption 1. For fixed prompt with pass rate p, let gi = Aiθ log πθ(yix) be the per-rollout gradient for incorrect rollouts (ri = 0), and let si = θ log πθ(yix) denote the score function. Let ϕi = Softplus(ci). We assume: 1. Rollouts yi are conditionally independent given x. 2. The signal direction is ˆd = E[si ri = 0]/E[si ri = 0]. 3. The directional covariance satisfies Cov(ϕi, ( ˆdsi)2 ri = 0) > 0, i.e., overconfident errors tend to have score functions more aligned with the expected gradient direction. Proposition 1 (Second Moment Increase). For any α > 0, ACE strictly increases the mean squared gradient norm of incorrect rollouts: 2 ri = 0] > E[gstd (21) whenever E[ϕisi2 ri = 0] > 0 (i.e., errors are not all zero-gradient). This is an unavoidable consequence of the purely additive penalty structure: (1 + αϕi) > 1 for all ϕi > 0. 2 ri = 0] E[gACE Proof. Let = ˆA(x) > 0 denote the per-prompt base penalty magnitude. Under standard GRPO: = si. Under ACE: gACE gstd E[gACE = a(1 + αϕi) si. Then: 2] = a2 (cid:0)E[(1 + αϕi)2si2] E[si2](cid:1) 2] E[gstd = 2α E[ϕisi2] (cid:123)(cid:122) (cid:125) (cid:124) >0 + α2 E[ϕ2 si2] > 0 (cid:125) (cid:123)(cid:122) 0 (cid:124) (22) since > 0, ϕi = Softplus(ci) > 0, α > 0, and si2 0 with E[ϕisi2] > 0. Definition 3 (Directional Signal and Variance). For incorrect rollouts, let ˆd = E[si ri = 0]/E[si ri = 0] be the unit vector along the expected score function. The directional signal and directional variance of gradient estimator gi = wi si are: µd = E[ ˆdgi ri = 0] = Var[ ˆdgi ri = 0] σ2 (signal along ˆd) (noise along ˆd) (23) (24) The gradient quality ratio is Qd = µ2 Theorem 2 (Improved Gradient Quality via ACE). Under Assumption 1, let ˆd be the signal direction. Define the directional projections ui = ˆdsi (scalar random variables). Assume: d/σ2 d. Cov(ϕi, u2 ri = 0) > 0 (25) i.e., overconfident errors tend to have score functions more aligned with the expected gradient direction. Then: (a) Directional variance increase. For any α > 0, ACE increases the directional variance: whenever Cov(ϕi, u2 reweighting structure, analogous to the total second-moment increase (Proposition 1). (26) ) > 0 and E[ϕi] > 0. This is an unavoidable consequence of the additive ri = 0] ri = 0] > Var[ ˆdgstd Var[ ˆdgACE (b) Quality improvement under high-variance conditions. Assume additionally that the initial gradient is noisy relative to the signal, i.e., Var[ui] > (E[ui])2 (equivalently, Qstd < 1). Then for sufficiently small α > 0, the gradient quality ratio of ACE strictly dominates that of standard GRPO: QACE > Qstd (27) Consequently, although ACE increases both the signal and the noise (directional variance), the signal grows faster, yielding net improvement in gradient quality along the optimization-relevant direction. Proof. Consider fixed prompt with rollouts sampled i.i.d. from πθ(x). Let si = θ log πθ(yix) denote the score function for rollout yi, and let ϕi = Softplus(ci). All expectations below are conditioned on ri = 0. Setup and notation. Let = ˆA(x) > 0 denote the per-prompt base penalty magnitude. Under standard GRPO: gstd = si. Under ACE: gACE = a(1 + αϕi)si. Since is positive scalar constant (per-prompt), it cancels in the gradient quality ratio Qd = µ2 d/σ2 d. We therefore analyze the = 1+αϕi without loss of generality. Let ˆd = E[si]/E[si] normalized weights wstd be the signal direction, and define the scalar projections ui = ˆdsi. = 1 and wACE Step 1: Directional variance analysis (Part (a)). The directional variance is: For standard GRPO (wi = 1): Varstd = E[u2 For ACE (wi = 1 + αϕi): Var[wiui] = E[w2 ] (E[wiui])2 u2 ] (E[ui])2. E[w2 i ] = E[u2 ] + 2α E[ϕiu2 ] + α2 E[ϕ2 u2 ] (E[wiui])2 = (E[ui] + α E[ϕiui])2 = (E[ui])2 + 2α E[ui] E[ϕiui] + α2(E[ϕiui]) Subtracting Eq. (30) from Eq. (29): VarACE = Varstd + 2α (cid:0)E[ϕiu2 ] E[ui] E[ϕiui](cid:1) (cid:125) (cid:123)(cid:122) 1 +O(α2) (cid:124) 18 (28) (29) (30) (31) Step 2: Sign of 1. Decompose using identities: ] = Cov(ϕi, u2 E[ϕiu2 ) + E[ϕi] E[u2 ] E[ϕiui] = Cov(ϕi, ui) + E[ϕi] E[ui] Substituting into 1: 1 = Cov(ϕi, u2 = Cov(ϕi, u2 ) + E[ϕi] E[u2 ) E[ui] Cov(ϕi, ui) + E[ϕi] Var[ui] ] E[ui] (Cov(ϕi, ui) + E[ϕi] E[ui]) (32) (33) (34) The third term E[ϕi] Var[ui] > 0 always increases variance. In fact, 1 > 0 under typical conditions: for the natural Gaussian linear model where ui (µ, σ2) and ϕi = + bui (a > 0, > 0), we have Cov(ϕi, u2 ) = 2bµσ2, Cov(ϕi, ui) = bσ2, and E[ϕi] = + bµ, giving: 1 = 2bµσ2 µ bσ2 + (a + bµ)σ2 = 2bµσ2 + aσ2 > 0 (35) This confirms that directional variance increases under ACEan unavoidable cost of additive reweighting. The condition 1 < 0 would require: E[ui] Cov(ϕi, ui) > Cov(ϕi, u2 ) + E[ϕi] Var[ui] (36) which is violated in the Gaussian linear model and is difficult to satisfy in practice. However, as we show next, this does not prevent quality improvement: what matters is that the signal grows faster than the square root of variance. Step 3: Quality improvement (Part (b)). The gradient quality ratio is: Qd = (E[wiui])2 Var[wiui] (37) Since 1 > 0 in general (Step 2), the directional variance increases. Nevertheless, the quality ratio can still improve because ACE also increases the signal E[wiui]. We now provide the complete derivation. Signal computation. For ACE with wi = 1 + αϕi: µACE E[wiui] = E[(1 + αϕi)ui] = E[ui] + α E[ϕiui] = E[ui] + α (Cov(ϕi, ui) + E[ϕi] E[ui]) = E[ui](1 + α E[ϕi]) + α Cov(ϕi, ui) Denote µ E[ui], ϕ E[ϕi], and Cov(ϕi, ui). Then: µACE = µ(1 + α ϕ) + αC The squared signal is: (µACE)2 = (cid:0)µ(1 + α ϕ) + αC(cid:1)2 = µ2(1 + α ϕ)2 + 2αCµ(1 + α ϕ) + α2C 2 = µ2 + 2αµ2 ϕ + α2µ2 ϕ2 + 2αCµ + 2α2Cµ ϕ + α2C 2 = µ2 + 2α(µ2 ϕ + Cµ) + O(α2) Variance computation. From Step 2, we have: VarACE = Varstd + 2α 1 + O(α2) where Varstd = E[u2 ] µ2 and 1 is given by Eq. (34). Quality ratio expansion. We compute the difference in quality ratios. For standard GRPO: Qstd = µ2 Varstd 19 (38) (39) (40) (41) (42) For ACE, using Eqs. (40) and (41):"
        },
        {
            "title": "QACE\nd",
            "content": "= (µACE)2 VarACE = µ2 + 2α(µ2 ϕ + Cµ) + O(α2) Varstd + 2α 1 + O(α2) Using the first-order Taylor expansion (1 + x)1 1 for small x:"
        },
        {
            "title": "QACE\nd",
            "content": "= µ2 + 2α(µ2 ϕ + Cµ) Varstd (cid:18) 1 = µ2 Varstd + 2α(µ2 ϕ + Cµ) Varstd (cid:18) µ2 ϕ + Cµ 2α Varstd + O(α2) (cid:19) 2α 1 Varstd 2αµ2 1 (Varstd)2 µ2 Varstd 1 + O(α2) (cid:19) + O(α2)"
        },
        {
            "title": "QACE",
            "content": "d Qstd = 2α Varstd (cid:0)µ2 ϕ + Cµ Qstd (cid:124) 1 (cid:123)(cid:122) Γ +O(α2) (cid:1) (cid:125) = Qstd + Therefore: (43) (44) (45) Sufficient condition for improvement. Quality improves when Γ > 0. Substituting 1 from Eq. (34): Γ = µ2 ϕ + Cµ Qstd = µ2 ϕ + Cµ Qstd (cid:16) Cov(ϕi, u2 Cov(ϕi, u2 ) µ + ϕ Varstd(cid:17) µ Qstd ) + Qstd ϕ Varstd Using Qstd = µ2/Varstd, the last term becomes µ2 ϕ, which cancels with the first term: µ Qstd ) Qstd Cov(ϕi, u2 ) Cov(ϕi, u2 ) Γ = Cµ + Qstd = Cµ(1 + Qstd (46) (47) Under Assumption 1, = Cov(ϕi, ui) > 0 and Cov(ϕi, u2 gradients more aligned with the signal direction). We analyze Γ > 0: ) > 0 (overconfident errors have Γ > 0 Cµ(1 + Qstd ) > Qstd Cov(ϕi, u2 ) Rearranging: Cµ Cov(ϕi, u2 ) > Qstd 1 + Qstd (48) (49) The right-hand side is monotonically increasing function of Qstd to 1 (as Qstd that ranges from 0 (when Qstd < 1 (equivalently, Var[ui] > (E[ui])2), we have: ). Therefore, when Qstd = 0) Under the Gaussian linear model (ui (µ, σ2), ϕi = + bui), we can verify: Qstd 1 + Qstd < 1 2 = Cov(ϕi, ui) = bσ2 ) = Cov(ui, u2 ) = 2µσ2 = 2bµσ Cov(ϕi, u2 Thus: Cµ Cov(ϕi, u2 ) = bσ2 µ 2bµσ2 = 1 Combined with Eq. (49), when Qstd < 1: 1 2 > Qstd 1 + Qstd = Γ > 0 = QACE > Qstd This completes the proof that quality improves under the high-variance condition: Var[ui] > (E[ui]) Qstd < 1 (50) (51) (52) (53) (54) (55) This high-variance regime is the typical operating condition in stochastic policy gradient optimization, where individual rollout gradients are highly variable. Under this condition, the signal growth term dominates the variance growth term, ensuring QACE > Qstd . 20 Summary. ACEs confidence-dependent weighting increases both the total gradient second moment (Proposition 1) and the directional variance (Steps 12)both unavoidable consequences of additive reweighting. However, ACE improves the gradient quality ratio (Step 3) under two conditions: (i) overconfident errors carry gradient signal aligned with the optimization direction (Cov(ϕi, ui) > 0), and (ii) the initial quality ratio is low (Var[ui] > (E[ui])2). The key mechanism is that ACEs selective amplification of high-confidence errors concentrates extra weight on the most informative gradients, causing the signal to grow faster than the noise along the optimization-relevant direction."
        },
        {
            "title": "C Implementation Details",
            "content": "Sequence-level vs. token-level aggregation. While Definition 1 defines ci at the sequence level, one can also define token-level variant c(t) and apply ACE per-token. We use the sequence-level aggregation ci = (cid:80) c(t) in our main experiments to capture trajectory confidence. Compute overhead. ACE adds exactly one Softplus computation per incorrect rollout per training step. Given that the bottleneck of RLVR training is rollout generation (model inference), the overhead of ACE is negligible (< 0.1% of wall-clock time). PyTorch implementation sketch. def ace_advantage(rewards, log_probs_policy, log_probs_ref, alpha=1.0): \"\"\" Args: rewards: (B, G) binary rewards log_probs_policy: (B, G) sequence-level log probs under pi_theta log_probs_ref: (B, G) sequence-level log probs under pi_ref alpha: ACE strength Returns: advantages: (B, G) modified advantages \"\"\" # Standard group statistics pass_rate = rewards.mean(dim=-1, keepdim=True) # (B, 1) std = rewards.std(dim=-1, keepdim=True) + 1e-8 std_advantage = (rewards - pass_rate) / std # (B, G) # Confidence score (already available, zero extra compute) = log_probs_policy - log_probs_ref # (B, G) # Normalize by sequence length = / seq_lengths # ACE advantage for negative samples ace_neg = std_advantage * (1.0 + alpha * F.softplus(c)) # (B, G) # Combine: use standard advantage for correct, ACE for incorrect is_correct = (rewards == 1).float() advantages = is_correct * std_advantage + (1 - is_correct) * ace_neg return advantages Compatibility. The above can be dropped into any RLVR training loop that uses GRPO, PPO, or REINFORCE by replacing the advantage computation. No changes are needed to the model architecture, rollout generation, or reward computation. Sensitivity to α We vary α {0, 0.1, 0.5, 1.0, 2.0, 5.0} on MATH-500 using Qwen2.5-Math-7B (α = 0 recovers standard GRPO). We conduct the sensitivity analysis on single model to isolate the effect of α; 21 since the main results (Tables 12) demonstrate consistent gains across all three model families at α = 1.0, we expect the optimal range to transfer. Table 4: Sensitivity to α on MATH-500 (Qwen2.5-Math-7B). α=1.0 achieves optimal Pass@32 while preserving Pass@1. α Pass@1 (%) Pass@32 (%) 0.0 (GRPO) 0.1 0.5 1.0 (default) 2.0 5.0 73.4 73.5 73.8 74.2 73.5 72.4 91.3 91.9 93.2 94.3 93.5 92.0 Observations. As shown in Table 4, optimal performance is achieved at α = 1.0 with Pass@32 = 94.3% (+3.0pp over GRPO). Performance remains stable across α [0.5, 2.0], all outperforming standard GRPO. Pass@1 shows slight decrease only at larger α values ( 2.0), reflecting the exploration-exploitation trade-off. We adopt α = 1.0 as the default for all experiments."
        },
        {
            "title": "E Training Hyperparameters",
            "content": "Table 5 summarizes the training hyperparameters for all three models. Fair comparison (matched recipe within each model). To ensure improvements are attributable to ACE rather than tuning differences, we use the same training recipe and training budget for all methods within given model (GRPO vs. ACE-GRPO, and DAPO vs. ACE-DAPO). Concretely, for fixed model we keep the data, verifier, rollout group size G, sampling settings, optimizer, learning rate schedule, batch sizes, clipping/KL coefficients, maximum sequence lengths, and the number of optimizer updates identical across methods; ACE changes only the computation of the negative advantages through Eq. (5) (controlled by α). Hyperparameters may differ across model families due to model-specific stability and context-length constraints, but cross-method comparisons are always performed under matched settings for the same model. Table 5: Training hyperparameters for ACE-GRPO experiments. Hyperparameter Qwen2.5-Math-7B Qwen3-8B-Base Llama-3.1-8B-Instruct Total epochs Training batch size Mini-batch size Micro-batch size per GPU Learning rate Optimizer Temperature Max prompt length Max response length Rollout samples per prompt Validation samples GPU memory utilization KL coefficient β Enable thinking (Qwen3) 10 2048 1024 16 1 105 AdamW 1.0 1024 3000 8 128 0.75 0.001 10 1024 1024 16 5 107 AdamW 1.0 1024 8192 8 128 0.75 0.001 False 10 1024 1024 16 1 106 AdamW 1.0 1024 4096 8 128 0.75 0."
        }
    ],
    "affiliations": [
        "LinkedIn Corporation, CA, USA"
    ]
}