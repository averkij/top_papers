{
    "paper_title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
    "authors": [
        "Bo Liu",
        "Leon Guertler",
        "Simon Yu",
        "Zichen Liu",
        "Penghui Qi",
        "Daniel Balcells",
        "Mickel Liu",
        "Cheston Tan",
        "Weiyan Shi",
        "Min Lin",
        "Wee Sun Lee",
        "Natasha Jaques"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development."
        },
        {
            "title": "Start",
            "content": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning Bo Liu*1, Leon Guertler*2, Simon Yu*3, Zichen Liu*1,4 Penghui Qi1,4, Daniel Balcells5, Mickel Liu6, Cheston Tan2, Weiyan Shi3, Min Lin4, Wee Sun Lee1, 6 Natasha Jaques 1National University of Singapore 2Centre for Frontier AI Research (CFAR), A*STAR 3Northeastern University 4Sea AI Lab 5Plastic Labs 6University of Washington Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting promising direction for autonomous reasoning development. Spiral-RL/SPIRAL Spiral-RL 5 2 0 2 0 3 ] A . [ 1 9 1 1 4 2 . 6 0 5 2 : r Figure 1 SPIRAL achieves consistent improvements over base models across game performance and reasoning benchmarks. It also surpasses SFT on expert game trajectories and RL baselines trained against fixed opponents (Mistral and Gemini). Equal contribution, order randomly decided by dice roll. Corresponding author. 1. Introduction Recent breakthroughs in language model reasoning, including OpenAI o1 (OpenAI, 2024) and DeepSeek-R1 (DeepSeek Team, 2024), reveal that reinforcement learning (RL) can unlock dramatic improvements in Chain-of-Thought reasoning (Wei et al., 2022). Through outcomebased rewards, RL enables models to develop generalizable reasoning strategies and consistently solve complex problems where supervised fine-tuning shows limited progress. However, current approaches to reasoning enhancement face fundamental scalability bottleneck: their dependence on carefully engineered reward functions, domain-specific datasets, and expert supervision (Bai et al., 2022; DeepSeek Team, 2024; Ouyang et al., 2022). Each new reasoning domain requires experts to craft evaluation metrics, curate training problems, and validate reasoning traces. This manual process becomes increasingly unsustainable as we pursue more general intelligence, limiting both the scale and diversity of reasoning challenges that models can learn from. Self-play offers solution to this scalability crisis by eliminating the need for human supervision in training data creation (Silver et al., 2017; Tesauro, 1995). In self-play, models learn by competing against copies of themselves, where each match outcome provides automatic feedback. As the model improves, its opponent improves equally, maintaining consistent challenge level that drives continuous learning. This paradigm has already revolutionized AI across many domains: from TD-Gammons backgammon supremacy (Tesauro, 1995) to AlphaGos conquest of Go (Silver et al., 2016, 2017) to OpenAI Fives mastery of complex team coordination (Berner et al., 2019). Yet despite these successes, applying self-play to enhance language model reasoning remains largely unexplored. Prior attempts have been limited to simple word games with offline updates (Cheng et al., 2024), LoRA adaptations that constrain learning capacity (Dettmers et al., 2023; Park et al., 2025), or single-turn code generation tasks (Zhao et al., 2025), falling short of leveraging multi-turn competitive dynamics that enable extended strategic reasoning. We introduce SPIRAL, which applies self-play to two-player zero-sum language games for developing reasoning capabilities. Zero-sum language games provide ideal training environments: they require strategic thinking and planning while remaining simple enough to enable stable learning, with clear rules that make outcomes verifiable. SPIRAL offers two key advantages. First, unlike traditional RLVR approaches that depend on human-curated problem-answer pairs, it generates infinite training data through game dynamics alone. Second, compared to fixed-opponent training (see Fig. 2), self-play prevents overfitting to static strategies by continuously evolving the challenge level. However, implementing this vision for LLMs presents significant technical challenges. The computational demands of multi-turn, multi-agent autoregressive generation require sophisticated distributed systems, while standard RL algorithms suffer from high variance in multi-agent settings. To address these challenges, we implement fully online, multi-turn, multi-agent reinforcement learning system with distributed actor-learner architecture. We also introduce role-conditioned advantage estimation (RAE), which stabilizes training by normalizing rewards relative to each players expected performance. Without RAE, our ablation experiments show that models suffer from thinking collapse, progressively abandoning reasoning traces that are critical for generalization. Key Findings. Training on zero-sum games produces reasoning capabilities that transfer broadly. Starting from Qwen3-4B-Base (Yang et al., 2025), SPIRAL trained solely on Kuhn Poker achieves 8.6% improvement on mathematical reasoning and 8.4% on general reasoning benchmarks, outperforming SFT on 25,000 expert trajectories. Since our training data contains only game states with no mathematical content, we avoid any risk of benchmark data leakage. AnalFrom human-designed rewards to self-discovered reasoning through SPIRAL. Left: Figure 2 Traditional RL requires human experts to design complex reward functions. Middle: Fixed opponent training leads to exploitation of static strategies. Right: SPIRAL enables continuous reasoning improvement through self-play, where both players develop increasingly sophisticated strategies without human supervision. ysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition (breaking problems into steps), expected value calculation (probabilistic reasoning), and caseby-case analysis. Fixed-opponent training fails while self-play continuously improves. Different games develop distinct reasoning strengths: TicTacToe trains spatial reasoning, Kuhn Poker builds probabilistic thinking, and Simple Negotiation fosters strategic optimization. Multi-game training combines these complementary skills synergistically. Applying SPIRAL to DeepSeekR1-Distill-Qwen-7B (DeepSeek Team, 2024) achieves 2.0% average improvement, confirming generalizability across model families. Without RAE, models stop reasoning after 200 steps. These results demonstrate that zero-sum games can serve as powerful reasoning gymnasiums, developing cognitive skills that transfer far beyond their training domain. Building on these findings, our work makes the following contributions: 1. Fully Online, Multi-Turn, Multi-Agent RL Framework for LLMs: We develop distributed actor-learner architecture that enables online self-play with full-parameter updates across multiple two-player zero-sum language games. The multi-turn aspect trains models to reason through sequential decisions, directly preparing them for complex multi-step problem solving. Unlike prior offline approaches, this provides continuous curriculum as the model must adapt to an ever-improving opponent: itself. We release our implementation1 to facilitate further research. 2. Role-conditioned Advantage Estimation (RAE): We introduce variance-reduced advantage estimator specifically designed for multi-agent settings. By normalizing rewards relative to each players expected performance, RAE reduces variance in multi-agent learning. Our ablation studies empirically validate that RAE stabilizes learning and prevents thinking collapse. Without it, models progressively abandon reasoning traces, which is critical for generalization. 3. Empirical Discovery of Transfer: We demonstrate that self-play on zero-sum games improves both out-of-distribution game performance and academic reasoning benchmarks without domain-specific training data. Our analysis identifies reasoning patterns (systematic decomposition, expected value calculation, case-by-case analysis) that transfer from games to mathematics at measurable rates. Different games develop specialized skills that transfer to related domains, and multi-game training combines these abilities synergistically. 1https://github.com/spiral-rl/spiral. 3 2. Related Work Reinforcement Learning for LLM Reasoning. Reinforcement learning has evolved from alignment tasks using RLHF (Bai et al., 2022; Jaques et al., 2019; Ouyang et al., 2022) to directly improving reasoning capabilities. Recent models like OpenAI o1 (OpenAI, 2024) and DeepSeekR1 (DeepSeek Team, 2024) demonstrate that RL with verifiable rewards (RLVR) can unlock chain-of-thought reasoning using rule-based rewards (Lightman et al., 2023; Uesato et al., 2022). However, these approaches depend on human-curated problem sets and domain-specific reward engineering. SPIRAL eliminates this dependency by using self-play games to generate infinite reasoning challenges without human supervision. Multi-Agent RL for Language Models. Implementing MARL for full-scale LLMs presents significant technical challenges (Liu et al., 2025a; Wan et al., 2025). Prior work has made various compromises: Sarkar et al. (2025) uses RNNs instead of transformers; Jacob et al. (2022) focuses on simplified environments that dont require full autoregressive generation; and Liao et al. (2024) shows the effectiveness of self-play with SFT on proprietary models. In contrast, SPIRAL implements fully online, full-parameter MARL through distributed actor-learner architecture, enabling continuous adaptation to evolving opponents across multiple games. Self-Play for Autonomous Improvement. Self-play in LLMs began with alignment methods like SPIN (Chen et al., 2024) and Self-Rewarding Language Models (Yuan et al., 2024). Recent work explores capability improvement: SPAG (Cheng et al., 2024) applies self-play to Adversarial Taboo but uses offline updates and remains confined to single word game; SPC (Chen et al., 2025) and Genius (Xu et al., 2025) require human task distributions; Absolute Zero (Zhao et al., 2025) generates coding tasks but needs deterministic verification. SPIRAL demonstrates that self-play on strategic games improves academic benchmarks by 8.7% average without domain-specific data. LLMs in Gaming. Games serve as testbeds for evaluating LLM capabilities (Duan et al., 2024; Paglieri et al., 2024; Ruoss et al., 2024; Zhang et al., 2024), but recent work explores games as training domains (Feng et al., 2024). LMRL-Gym (Abdulhai et al., 2023) benchmarks RL algorithms for LLMs in game environments; RAGEN (Wang et al., 2025) uses single-agent multi-turn RL; ViGaL (Xie et al., 2025) shows arcade games improve math reasoning; DivideFuse-Conquer (Zhang et al., 2025) trains on grouped games using offline learning. SPIRAL uniquely combines: (1) multi-agent self-play where both players share parameters, (2) fully online learning with continuous opponent evolution, and (3) transfer from zero-sum language games to reasoning without seeing any benchmark-related problems during training. 3. Preliminaries Turn-level Markov Decision Process (MDP). Language model training traditionally formulates generation as token-level MDP (Bellman, 1957; Rafailov et al., 2024) where each action is . For multi-turn reasoning and game-playing, we instead adopt single token from vocabulary represent complete contexts turn-level MDP formulation , (S are complete (e.g., game configurations, problem states, or conversation histories), actions determines state responses (containing many tokens), the transition function 𝑇 : dynamics, 𝑟 : is the discount factor. The return is defined as the discounted sum of rewards: 𝑅 provides immediate rewards, and 𝛾 = (cid:205)𝑇 . Here, states 0, 1 [ ] 𝑡=0 𝛾𝑡𝑟𝑡. , 𝑇, 𝑟, 𝛾 (S) S = Δ ) 𝜏 ) ( The key distinction: in token-level MDPs, each decision outputs one token; in turn-level MDPs, each decision produces complete multi-token response before transitioning. At each 4 The SPIRAL Framework. SPIRAL employs an actor-learner architecture for scalable Figure 3 self-play training. Parallel actors sample trajectories from diverse set of games using vectorized environments. single policy 𝜋𝑖 plays both roles, generating zero-sum, sparse reward game trajectories. The centralized learner processes these trajectories using Role-conditioned , for each role. Advantage Estimation (RAE) to compute separate advantages, 𝐴0( These are then used for on-policy reinforcement learning updates. and 𝐴1( 𝑠, 𝑎 ) 𝑠, 𝑎 ) turn 𝑡, the language model observes state 𝑠𝑡 and generates: 𝑐𝑡 / where 𝑐𝑡 externalizes reasoning and 𝑎𝑡 SFT and RLVR paradigms adapt to turn-level MDPs.) think think 𝑦𝑡 = answer answer is the executable action. (See App. for how existing 𝑎𝑡 / (1) , Two-Player Zero-Sum Markov Games. We extend the single-agent MDP to competitive settings with two-player zero-sum Markov game (Littman, 1994) A0 and A1 are the action spaces for player 0 and player 1 respectively. The zero-sum property requires: (2) ) , 𝑎( 𝑟0( ) A1 denote actions taken by each player. Given trajectory 𝜏 = ) A0 and 𝑎( , 𝑎( 𝑡 where 𝑎( 𝑠𝑡, 𝑎( 𝑡 A1, 𝑇, 𝑟, 𝛾 , where A0, 𝑟1( 𝑠, 𝑎( 𝑠, 𝑎( 𝑠, 𝑎( ) , 𝑎( ) , 𝑎( = 0 ) + (S ) , = = ) ) 1 1 0 1 0 1 0 0 𝑇 . , ) ) ) ) 𝑡=0, the returns satisfy 𝑅1( 𝜏 ) )} 𝑅0( 𝜏 ) {( 4. The SPIRAL Framework We present SPIRAL, framework that enables language models to develop generalizable reasoning capabilities through multi-turn competitive self-play. The framework is illustrated in Fig. 3. 4.1. SPIRAL as Self-Play on Turn-Based Zero-Sum Games SPIRAL implements self-play through turn-based zero-sum language games from collection . Each game 𝐺𝑖 is two-player zero-sum Markov game, with the specific = 𝐺1, 𝐺2, ..., 𝐺𝑛} { structure that players alternate turns rather than acting simultaneously. SPIRAL leverages three properties of these games: First, the multi-turn structure mirrors sequential reasoning problems. Players alternate turns: at time 𝑡, player 𝑝 = 𝑡 mod 2 acts while the opponent waits, formally: 𝑎( . 𝑡 Unlike single-turn bandit settings, this framework trains models to maintain context, plan ahead, and adapt strategies based on evolving game states. 𝑝 and 𝑎( = 𝑝 𝑝 ) ) 𝑡 Second, the zero-sum competitive dynamics create pressure for continuous improvement. = 0 for all non-terminal states. At 𝑠𝑡, 𝑎( Each game uses sparse terminal rewards: 𝑟𝑖 ( 𝑡 terminal state 𝑠𝑇 , players receive opposite rewards: , 𝑎( 𝑡 ) 0 1 ) ) 𝑅0( 𝜏 ) = 𝜌𝑖 ( 𝑠𝑇 ) , 𝑅1( 𝜏 ) = 𝜌𝑖 ( 𝑠𝑇 ) , (3) where 𝜌𝑖 1, 0, 1 develop robust strategies that cannot be easily exploited. { terminal 𝑖 } : determines the outcome. This opposition forces models to Finally, both players share single policy 𝜋𝜃 with role conditioning. To enable this in the LLM setting, we prepend role-specific system prompts that specify whether the model is playing as Player 0 or Player 1 (see App. B.1 for example observations). At each turn, the active player generates: 𝑝 𝑦 ( ) 𝑡 𝜋𝜃( 𝑠𝑡, 𝑝, 𝐺𝑖) , (4) 𝑝 where conditioning on 𝑝 is achieved through the system prompt2. From 𝑦 ( , we extract the 𝑡 action 𝑎( to update the game state. This shared-parameter approach ensures that as the 𝑡 model improves at one role, it simultaneously faces stronger opponent, creating an automatic curriculum. ) ) 𝑝 4.2. Multi-Turn Self-Play Training Objective In self-play, we could train two separate policies 𝜋𝜃0 and 𝜋𝜃1 for each player. Each would maximize their own expected return: 𝐽0( 𝐽1( 𝜃0) 𝜃1) = E𝐺 = E𝐺 E𝜏 E𝜏 G 𝜋𝜃0 𝜋𝜃0 𝜋𝜃1 𝜋𝜃1 𝐺 [ 𝐺 [ 𝑅0( 𝑅1( 𝜏 )] 𝜏 )] (5) (6) However, SPIRAL uses single shared policy 𝜋𝜃 for both players, setting 𝜃0 = 𝜃1 = 𝜃. While in zero-sum games, the role conditioning this creates opposing objectives since 𝑅1( 𝑅0( through system prompts enables the model to learn distinct strategies for each position within unified policy framework. 𝜏 ) 𝜏 ) = To optimize this shared policy, we apply policy gradient methods. Using REINFORCE (Williams, 1992), the gradient becomes: 𝜃 𝐽 𝜃 ) ( = E𝐺 E𝜏 𝜋𝜃 𝜋𝜃 𝐺 0 ) 𝑦 ( 𝑡 𝑠𝑡, 0, 𝐺 𝑅0( ) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 𝜏 ) + (cid:125) 𝜃 log 𝜋𝜃( 𝑡 𝑇1 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) 1 ) 𝑦 ( 𝑡 𝑠𝑡, 1, 𝐺 𝑅1( ) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 𝜏 ) (cid:123)(cid:122) Player 1 gradient (cid:123)(cid:122) Player 0 gradient 𝜃 log 𝜋𝜃( 𝑇0 𝑡 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) , (cid:125) (7) 2We note that both 𝑝 and 𝐺𝑖 can be subsumed into state 𝑠𝑡, but we choose to make them explicit here for clarity. 6 0, 1 ] [ Determine active player Generate reasoning + action Inactive player Store trajectory with its game Algorithm 1 SPIRAL: Role-Balanced Multi-Turn Self-Play Require: Policy 𝜋𝜃, Games 𝐺1, ..., 𝐺𝑛} 1: Initialize baselines 𝑏𝐺𝑖,𝑝 = 0 for all 𝐺𝑖 2: while not converged do // Self-Play Trajectory Collection 3: { = , decay rate 𝛼 , 𝑝 0, 1 } { 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: for 𝑘 = 1 to 𝐾 actors in parallel do Sample game 𝐺𝑖 𝐺𝑖 for turn 𝑡 = 0, 1, 2, ... until terminal do , initialize 𝑠0 ) 𝑝 ( ) 𝑝 𝑦 ( 𝑡 𝑡 mod 2 𝑠𝑡, 𝑝, 𝐺𝑖) 𝜋𝜃( extract_action 𝑝 𝑦 ( ) 𝑡 𝑝 𝑎( ) 𝑡 1 𝑝 𝑎( ) 𝑡 𝑇𝑖 ( 𝑠𝑡 1 + end for , 𝑅1 𝜌𝑖 ( 𝑠𝑇 ) 𝑅0 Add to batch 𝜏, 𝐺𝑖) ( end for // Role-Balanced Policy Optimization for 𝑠𝑡, 𝑎( 𝑡 , 𝑎( 𝑡 do 𝑅0 ) 1 0 ) ) 𝜏, 𝐺𝑖) ( for 𝑝 0, 1 { 𝑏𝐺𝑖,𝑝 𝜏 𝐴𝐺𝑖,𝑝( end for do } 𝛼𝑏𝐺𝑖,𝑝 + ( 𝜏 𝑅 𝑝( ) 𝑅 𝑝( 𝜏 ) 1 𝛼 ) 𝑏𝐺𝑖,𝑝 ) end for Update 𝜃 using policy gradient with advantages 𝐴𝐺𝑖,𝑝 (Eq. 10) 24: 25: end while 𝑡 : 𝑡 mod 2 = 𝑝 where 𝑇𝑝 = denotes turns where player 𝑝 acted. This formulation uses Monte Carlo returns which suffer from high variance, particularly problematic in self-play where the opponents strategy continuously evolves, making the environment highly non-stationary. } { 4.3. Policy Optimization with Role-conditioned Advantage Estimation To reduce the high variance inherent in multi-agent REINFORCE, we introduce Role-conditioned Advantage Estimation (RAE). In two-player games, even with shared policy, different roles may have different expected returns due to game asymmetries (e.g., first-move advantage in TicTacToe, information asymmetry in Kuhn Poker). RAE maintains separate baselines 𝑏𝐺,𝑝 for each game 𝐺 baseline estimates the expected return baselines using exponential moving average (EMA) with decay rate 𝛼 𝑅 𝑝( )] 𝜏 [ , where each for that role in that game. We update these and role 𝑝 0, 1 { } : 0, 1 ] [ 𝑏𝐺,𝑝 𝜏 𝐴𝐺,𝑝( ) 𝛼𝑏𝐺,𝑝 + ( = 𝑅 𝑝( 𝜏 ) 𝛼 ) 𝑏𝐺,𝑝 𝑅 𝑝( 𝜏 ) (update baseline) (compute advantage) (8) (9) This provides better variance reduction than global baseline by accounting for role-specific asymmetries, such as the first-move advantage. 7 The variance-reduced policy gradient becomes: 𝜃 𝐽SPIRAL( 𝜃 ) = E𝐺 E𝜏 𝜋𝜃 𝜋𝜃 𝐺 𝑝 0,1 𝑇𝑝 𝑡 } { 𝐴𝐺,𝑝( ) 𝜃 log 𝜋𝜃( 𝜏 𝑝 ) 𝑦 ( 𝑡 𝑠𝑡, 𝑝, 𝐺 ) (10) By centering returns around role-specific expectations, RAE ensures that gradient updates reflect genuine learning signal rather than inherent advantages of certain player positions. Note that we do not normalize by response length to avoid length bias (Liu et al., 2025b). The complete procedure is in Algorithm 1. 4.4. Implementation To implement SPIRAL, we develop truly online multi-agent, multi-turn RL system for finetuning LLMs. Our training framework builds on Oat (Liu et al., 2024), which provides interfaces of distributed actor-learner architecture (Espeholt et al., 2018). We instantiate actors to execute the self-play loop, using vLLM (Kwon et al., 2023) for efficient model inference and TextArena (Guertler et al., 2025) to simulate the language games. The resulting multi-turn, multigame self-play experiences are used to update the LLM via policy gradient methods (Sutton and Barto, 2018), incorporating our proposed Role-conditioned Advantage Estimation in the collocated learner. 5. Experiment Setup We design experiments to carefully test how training on different zero-sum games can improve reasoning capabilities that generalize to novel downstream math and reasoning tasks, as well as novel zero-sum games. This section introduces the selection of game environments (Sec. 5.1) and the training (Sec. 5.2) and evaluation (Sec. 5.3) settings. We investigate four key questions: 1. RQ1: Can self-play on zero-sum games improve math and general reasoning capabilities? We test whether competitive environment alone can develop transferable reasoning capabilities without domain-specific training data, and analyze emergent patterns to understand transfer mechanisms. 2. RQ2: Does self-plays automatic curriculum outperform fixed-opponent training? We evaluate whether the automatic curriculum with adaptive difficulty generated through self-play produces more robust reasoning than training against static opponents. 3. RQ3: Do different games develop specialized reasoning skills? We test whether distinct game mechanics cultivate complementary cognitive abilities and whether multi-game training creates synergistic benefits. 4. RQ4: Is Role-Conditioned Advantage Estimation (RAE) essential for stable self-play training? We ablate our variance reduction technique to determine whether RAE is necessary to stabilize training and prevent thinking collapse in zero-sum self-play with shared parameters. 5.1. Game Environments We select three games from TextArena (Guertler et al., 2025) with distinct cognitive requirements: 8 TicTacToe (Spatial Reasoning) (Beck, 2008). 3 3 grid game requiring pattern recognition and adversarial planning. Players must identify winning configurations, block opponent threats, and plan multi-step forcing sequences. We hypothesize these skills transfer to geometric problemsolving and spatial visualization tasks. Tic-Tac-Toe is deterministic, perfect-information game, which isolates pure strategic reasoning from uncertainty management. Kuhn Poker (Probabilistic Reasoning) (Kuhn, 1950). minimal poker variant with three cards (Jack, Queen, King) where players bet with hidden information. Success requires probability calculation, opponent modeling, and decision-making under uncertainty. We expect these capabilities to transfer to problems involving probability, expected value, and strategic uncertainty. Simple Negotiation (Strategic Optimization) (Nash, 1950). resource trading game where two players exchange Wood and Gold with opposing valuations to maximize portfolio value. Success requires multi-step planning, theory of mind to model opponent preferences, and strategic communication through proposals and counteroffers. We hypothesize these skills transfer to optimization problems, resource allocation tasks, and multi-constraint reasoning that requires balancing competing objectives. Please see Fig. 7 in App. for example environment observations. 5.2. Training Settings We employ Qwen3-4B-Base (Yang et al., 2025) as the base model for all experiments. The training spans 400 steps with 128 samples per step, totaling 51,200 game transitions. We use sampling temperature 𝜏 = 1.0 to collect game experiences and Adam (Kingma and Ba, 2014) with 6 to optimize the model. The training batch size is set as 128 constant learning rate of 1 and the discount factor 𝛾 = 1.0 (no discounting). We use 𝛼 = 0.95 for the exponential moving average decay of role-specific baselines. Detailed hyperparameters are listed in Table 7 in App. B. Each experiment takes around 25 hours for full parameter tuning on 8 H100 GPUs with our distributed actor-learner architecture. 10 5.3. Evaluation Metrics We track three key metrics throughout the training process: the frequency of invalid moves, the game lengths and the win rates against Gemini-2.0-Flash-Lite3, to monitor the learning dynamics and determine the best models. To evaluate the trained models, we test their win rates against fixed opponent on both training and unseen games and standard reasoning benchmarks, as detailed below. Out-of-Distribution Generalization to Unseen Games. We evaluate transfer learning on seven unseen games that test whether learned skills generalize beyond training domains: Snake and Connect Four (spatial reasoning), Pig Dice and Liars Dice (probabilistic reasoning), Truth and Deception (strategic optimization). These games specifically probe whether spatial reasoning from TicTacToe, probabilistic reasoning from Kuhn Poker, and strategic optimization from Simple Negotiation transfer to novel game mechanics. We report the average win rate of our models against Gemini-2.0-Flash4 across 512 independent games, with the starting player randomized in each match to eliminate any first-move advantage. Transfer to Standard Reasoning Benchmarks. To investigate whether the reasoning abilities 3Accessed via https://openrouter.ai/google/gemini-2.0-flash-lite-001. 4Accessed via https://openrouter.ai/google/gemini-2.0-flash-001. developed through gameplay could transfer to non-game contexts, we evaluate our models on suite of established benchmarks. For mathematical reasoning, we use MATH500 (Hendrycks et al., 2021), OlympiadBench (He et al., 2024), Minerva Math (Lewkowycz et al., 2022), AIME24, AIME25, and AMC23 datasets, which cover wide range of topics including algebra, geometry, and competitive mathematics. For general reasoning, we utilize GPQA (Rein et al., 2024), which consists of graduate-level science questions, and MMLU-Pro (Wang et al., 2024), benchmark for multidisciplinary knowledge. All evaluations on these benchmarks are conducted in zero-shot setting5 to determine if game-induced reasoning could be successfully transferred to general problem-solving. We use sampling temperature 0.6, top-p 0.95 and 4 rollouts for all evaluations. Reasoning Pattern Analysis. We analyze the cognitive patterns that emerge during training and their transfer to mathematical reasoning. Using GPT-4.1 (gpt-4.1-2025-04-14) (OpenAI, 2025) as LLM-as-Judge (Zheng et al., 2023), we classify reasoning traces from 290 game trajectories and 46,792 math problem solutions into three core patterns: Case-by-Case Analysis (systematic enumeration of scenarios), Expected Value Calculation (probabilistic decision-making), and Pattern Recognition (identifying regularities and structures). The complete prompts used for pattern discovery, classification, and transfer analysis are provided in App. D. We track the evolution of these patterns across training checkpoints (early: step 0, mid: step 128, late: step 400) in both game and math domains to measure transfer rates. This analysis reveals which game-induced reasoning strategies generalize to academic problem-solving and explains the mechanism behind cross-domain transfer. 6. Experimental Results and Findings 6.1. RQ1: Can self-play on zero-sum games improve math and general reasoning capabilities? Table 1 presents our main results. SPIRAL, trained exclusively on Kuhn Poker through selfplay, achieves notable improvements on both mathematical and general reasoning benchmarks. Notably, it even surpasses model that was fine-tuned on 25,000 winning game trajectories from Qwen3-32B playing against itself. Table 1 specific training data. Reasoning benchmark performance. SPIRAL improves reasoning without any domainModel MATH500 AIME24 AIME25 OlympiadBench AMC-23 Minerva Math GPQA MMLU-Pro Average Qwen3-4B-Base SFT (Qwen3-32B Distill) SPIRAL (Ours) Improvement 65.8 76.0 76.4 +10.6 10.0 16.7 13.3 +3.3 3.3 13.3 10.0 +6. 33.3 38.4 38.4 +5.1 50.0 50.0 57.5 +7.5 24.3 31.2 42.4 +18. 30.6 33.0 37.0 +6.4 47.2 48.8 57.7 +10.5 32.9 39.7 41.6 +8. Few-shot evaluation following Qwen3 technical report. The results show consistent improvements across mathematical reasoning (+10.6% MATH500, +6.7% AIME25, +7.5% AMC-23), and general reasoning (+6.4% GPQA, +10.5% MMLU-Pro). Notably, SPIRAL achieves these gains despite never seeing single mathematical equation, academic problem, or domain-specific training example during its entire training process. The 18.1% improvement on Minerva Math and overall 8.7% gain across math benchmarks demonstrate that competitive game environments successfully force discovery of reasoning patterns that transfer broadly across domains. 5Except for the base model, for which we provide few-shot examples that follow the Qwen3 Report settings. 10 6.1.1. Case Study: How Game Strategies Transfer to Mathematical Reasoning To understand this surprising transfer, we use GPT-4.1 (gpt-4.1-2025-04-14) to analyze 290 game trajectories and 46,792 math solutions across training checkpoints, identifying three core reasoning patterns that emerge during gameplay and transfer to math reasoning domains. Pattern Discovery and Examples. We first illustrate what these patterns look like in both game and mathematical contexts: Table 2 Representative reasoning patterns and their manifestation in games and mathematics. We use an LLM-as-a-judge framework to automatically evaluate the frequency of these patterns over training. Pattern Game Context (Kuhn Poker) Math Transfer Case-by-Case Analysis <think> Case 1 - Fold: You lose 1 chip. You have 2 chips now. Case 2 - Call: You have 0% chance of winning, so you will have 1 chip if you lose. Since losing 1 chip (fold) is better than losing 2 chips (call and lose), the best action is to fold. </think> Expected Value Calculation <think> With card against opponents bet: EV(call) = P(win) = 0 2 = -2 2 - 1 EV(fold) = -1 (lose ante) Since EV(fold) > EV(call), fold is optimal. </think> 2 - P(lose) 2 Pattern Recognition <think> Previous rounds: Player 0 had both times and won. Pattern identified: Player 0 likely has strong cards or bluffs consistently. Given have (weak), betting might exploit their aggressive calling pattern. </think> : 1 + ( 2 + + 2𝑛 3𝑛 = 𝑚 2 = 𝑚, no real solutions 6 = 3𝑚, no real solu- <think> To solve 𝑚2 1 ) Case 𝑛 = 0: 𝑚2 + Case 𝑛 = 1: 𝑚2 tions Case 𝑛 = 2: 𝑚2 tions Case 𝑛 = 3: 𝑚2 (valid) Case 𝑛 4: No solutions (exponential dominance) </think> 54 = 15𝑚 gives 𝑚 = 6, 9 18 = 7𝑚, no real solu- + + 𝐵𝐶 + + 𝐷𝐸𝐹 <think> For average of 𝐴 over permutations: (cid:205) 𝑎𝑖 2 6 = 6 = E[A] = E[BC] = 𝐵 𝐸 𝐸 ])( [ ( E[DEF] = 𝐷 𝐸 𝐶 ]) [ 𝐸 𝐸 = 1 ( [ ])( [ ])( 1 3 ) ( 𝐹 𝐸 ]) [ 2 = 1 9 = 3 = 1 3 ) ( ( 𝑛 𝑛 1 3 + 1 9 1 ) + 7 27 </think> 1 27 = <think> Sum of three consecutive integers 𝑛 Pattern recognized: sum is always divisible by 3. For perfect cube: 3𝑛 = 𝑘3, so 𝑘 must be divisible by 3. Smallest: 𝑘 = 3 3𝑛 = 27 </think> = 3𝑛. 1 ) + ( + 1 27 Total = Pattern Evolution and Transfer Analysis. Using GPT-4.1 to classify reasoning strategies, we tracked how these patterns develop during training and transfer to mathematics: Fig. 4 shows how reasoning patterns evolve in both game and math domains across training checkpoints. These patterns develop at different rates and transfer with varying effectiveness: Case-by-Case Analysis: We find that the meta-cognitive skill of systematic enumeration, or breaking problems down into cases, transfers highly effectively from strategic games to math problems (as shown in Fig. 4). We hypothesize this is because it represents domain-agnostic way of structuring thinking that generally improves reasoning performance. Whether analyzing opponent possibilities in Poker or solution branches in mathematics, the core skill is the same. Expected Value Calculation: We find that training on games increases the use of expected 11 Evolution of reasoning patterns during SPIRAL training and their transfer to Figure 4 mathematical reasoning. We track three core reasoning patterns (Pattern Recognition, Expected Value Calculation, and Case-by-Case Analysis) across 290 game trajectories and 46,792 math solutions. Left: In the game domain, all patterns show substantial growth, with Expected Value Calculation reaching 78% by late training. Middle: These patterns transfer to mathematical reasoning with varying effectiveness: Case-by-Case Analysis maintains high transfer (72% to 71%), Pattern Recognition shows amplification (35% to 45%), while Expected Value Calculation transfers more selectively (78% to 28%). Right: Math benchmark scores improve from 31.2 to 39.6 as these reasoning patterns develop, demonstrating that game-learned strategies enhance mathematical problem-solving capabilities. value calculation from 15% to 78% in the games themselves, and from 12% to 28% in the math domain. While game-specific probabilistic reasoning is more frequent in games than in the math benchmark (likely because most math problems lack explicit decision-theoretic structure), we find that this form of reasoning benefits probability and optimization problems where EV concepts directly apply. Pattern Recognition: Interestingly, this form of reasoning shows an amplification effect, where the proportion evident in the math domain actually exceeds that of the game domain. We hypothesize that because mathematics inherently requires pattern recognition, game training enhances an already-present mathematical skill which is then deployed even more frequently when the model is prompted to answer math problem. The combined training produces stronger pattern recognition (45%) than games alone require (35%). Transfer Mechanism. We believe three factors enable this cross-domain transfer: 1. Competitive pressure strips away memorization: Self-play opponents continuously evolve, forcing models to develop genuine reasoning rather than pattern matching. 2. Games isolate pure reasoning: Without domain complexity, games teach fundamental cognitive operations (enumeration, evaluation, synthesis) that generalize effectively. 3. Structured output bridges domains: The <think> format learned in games provides reasoning scaffold that models reuse for mathematical problems. 12 Figure 5 Performance comparison of self-play training and fixed-opponent baselines. All evaluations are averaged over multiple games/benchmarks (see Sec. 5.3). Mistral Opponent refers to against Mistral-Small-3; Gemini Opponent refers to against Gemini-2.0-Flash-Lite. Finding 1: Zero-sum games teach transferable reasoning without domain data Self-play on Kuhn Poker improves math and general reasoning benchmarks despite never seeing benchmark related problems. This works because gameplay forces discovery of three reasoning patterns: Case-by-Case Analysis, Expected Value Calculation, and Pattern Recognition. These skills are developed during competing against an evolving opponent. 6.2. RQ2: Does self-plays automatic curriculum outperform fixed-opponent training? Although self-play promises potentially infinite curriculum generation, it remains unclear whether the evolving challenge actually produces better learning than simply training against fixed opponents. We test whether adaptive difficulty through self-play leads to more robust reasoning development. Experimental Setup. We compare four training settings on the Kuhn Poker environment: (1) Self-Play: the model trains against continuously updating copies of itself; (2) Random Opponent: the opponent always provides valid actions but poses minimal strategic challenge; (3) Mistral Opponent: using Mistral-Small-3 (Mistral, 2025) as fixed intermediate-level opponent; and (4) Gemini Opponent,: using Gemini-2.0-Flash-Lite (Gemini Team, Google, 2025) as fixed strong opponent. All variants use identical hyperparameters, infrastructure, and 400 training steps. Results Analysis. Figure 5 reveals two distinct failure modes in fixed-opponent training: 1. Curse of Turns in Format Learning: Training against random opponents leads to complete collapse. Although these rule-based random agents offer no strategic challenge, they always produce valid moves. Consequently, the model must generate correctly formatted, valid actions at every turn throughout the trajectory in order to receive any positive reward. However, the probability of producing fully valid trajectory decreases exponentially with the episode length, making it extremely difficult to explore and learn the correct action format over long horizons. 2. Exploitation of Static Strategies: Fixed model-based opponents (Mistral, Gemini) facilitate format learning more effectively than random opponents, as they occasionally produce invalid moves that allow the agent to win and receive reward signals. However, we find that training 13 against these static opponents often leads to overfitting to their fixed strategies, resulting in poor generalization to math and general reasoning benchmarks (middle and right of Fig. 5). Continuous Adaptation Through Self-Play. Self-play uniquely maintains learning momentum throughout training and generalizes better because self-play automatically adjusts difficulty. As shown in Table 3, training against fixed opponent (Gemini) initially gets zero win rate (thus no positive reward), leading to no effective learning before step 128 (see left of Fig. 5); at step 384 the agent learns to outperform the fixed opponent with 62.5% win rate, gradually exploiting its static strategies. On the other hand, against previous versions of itself, self-play maintains 50-52% win rates throughout training, confirming the opponent evolves to match current capabilities. Table 3 Win rates at different training stages of Gemini Opponent and Self-Play vs its opponent. Training Stage Gemini Opponent Win Rate vs Gemini-2.0-Flash-Lite Self-Play Win Rate vs Self (t-16) Step 16 Step 128 Step 384 0.0% 37.5% 62.5% 52.3% 51.7% 50.9% The reasoning transfer results are even more striking. Self-play achieves 40% on math reasoning and 45% on general reasoning, outperforming the best fixed opponent (Gemini) by 5 and 3 percentage points respectively. The relative improvement demonstrates that the diverse strategies required by an evolving opponent create more generalizable reasoning patterns than exploiting static weaknesses. Finding 2: Self-play creates adaptive curriculum that outperforms fixed opponents Self-play achieves better reasoning transfer than any fixed-opponent training. Rulebased random opponents cause complete collapse due to the curse of turns in format learning. Training against model-based opponents plateaus once the agent finds winning counter-strategies. Self-play avoids both failures by continuously adjusting difficulty: the opponent improves as the agent improves, forcing ongoing adaptation rather than exploitation of static weaknesses. 6.3. RQ3: Do different games develop specialized reasoning skills? We investigate whether distinct game mechanics cultivate different cognitive abilities. If games truly serve as reasoning gymnasiums, then training on TicTacToe should develop different skills than training on Kuhn Poker or Simple Negotiation. Furthermore, we test whether combining multiple games produces synergistic benefits beyond the sum of individual training. Experimental Design. To isolate game-specific skill development, we train separate models exclusively on each game using identical hyperparameters and training duration. We then evaluate these specialists in three ways: (1) performance on their training game, (2) transfer to out-of-distribution games, and (3) generalization to math and general reasoning benchmarks. This design reveals whether games develop narrow tactics or transferable cognitive capabilities. For game performance evaluation, we use two complementary approaches: head-to-head competition between specialists to directly compare learned strategies, and performance against 14 fixed strong opponent (Gemini-2.0-Flash-Lite) to measure absolute skill levels. This dual evaluation reveals both relative strengths between specialists and their absolute capabilities. Head-to-Head Competition Reveals Specialized Strategies. We first examine how specialists perform when competing directly against each other. This head-to-head evaluation uses games that isolate specific cognitive skills: we pair each training game with an unseen game requiring similar abilities (Snake for spatial reasoning like TicTacToe, Pig Dice for probabilistic reasoning like Kuhn Poker, and Truth and Deception for strategic optimization like Simple Negotiation). Table 4 Game specialists excel at both their training games and unseen games requiring similar cognitive skills. Each cell shows the win rate in head-to-head competition between specialists (e.g., 57.5% means TicTacToe specialist wins 57.5% of games against the other two specialists on TicTacToe). Bold indicates best performance in each column. Model Training Games TicTacToe Kuhn Poker Simple Negotiation OOD Games (Similar Skills) Snake (Spatial) Pig Dice (Probabilistic) Truth and Deception (Strategic) TicTacToe Specialist Poker Specialist Negotiation Specialist 57.5% 45.5% 40.5% 45.1% 64.2% 40.2% 30.4% 37.7% 62.7% 56.0% 42.5% 41.0% 56.7% 91.7% 1.1% 48.7% 45.4% 55.8% Models trained exclusively on TicTacToe excel at spatial pattern recognition, achieving 57.5% win rate against other specialists on their training game. When tested on Snake, an unseen spatial reasoning game, they maintain 56.0% performance, demonstrating robust transfer. Kuhn Poker specialists develop probabilistic reasoning, dominating their training game (64.2%) and remarkably achieving 91.7% on Pig Dice. Simple Negotiation training produces models skilled in strategic optimization (62.7% on training game, 55.8% on Truth and Deception). Multi-Game Training Creates Synergistic Benefits. While head-to-head competition reveals specialized strategies, we also evaluate absolute performance against strong fixed opponent (Gemini-2.0-Flash-Lite) across both training games and randomly sampled out-of-distribution games. The multi-game model demonstrates broader capabilities that often exceed specialist performance: Table 5 Multi-game training achieves competitive performance across all training games while excelling at novel composite challenges. All win rates shown are against Gemini-2.0-Flash-Lite as fixed opponent. The multi-game model outperforms all specialists on average, demonstrating that diverse game training develops more flexible reasoning. Training Games OOD Games TicTacToe KuhnPoker Simple Negotiation Liars Dice Connect Four Snake Average Model Base Model 17.5% 21.5% Single-Game Specialists TicTacToe Specialist Kuhn Poker Specialist Simple Negotiation Specialist Multi-Game Model 56.6% 31.0% 27.7% 54.3% 24.4% 48.5% 16.8% 53.9% 15.6% 30.5% 28.7% 39.1% 33.2% 16.7% 20.5% 20.6% 18.7% 31.4% 24.9% 12.3% 51.4% 30.8% 39.1% 32.0% 39.3% 30.7% 33.9% 34.8% 34.1% 34.4% 27.1% 37.2% 44.9% The power of multi-game training becomes evident in performance on novel games. On Liars Dice, individual specialists struggle, achieving only 24.9% (Kuhn Poker) and 12.3% (Simple Negotiation) against Gemini-2.0-Flash-Lite. However, the multi-game model achieves 15 51.4%, demonstrating emergent capabilities beyond any single training game. This pattern of multi-game superiority holds across the out-of-distribution games, with the multi-game model achieving the highest average performance (44.9%) compared to all specialists. Multi-Game Training Enhances Even Strong Reasoning Models. To test whether gamebased skill development is limited to weaker models, we apply multi-game SPIRAL training to DeepSeek-R1-Distill-Qwen-7B, model that already achieves 59.7% average on reasoning benchmarks (see App. C.2 for comprehensive results across all training configurations): Table 6 strong reasoning models benefit from game-based skill development. Multi-game SPIRAL training improves reasoning capabilities across model scales. Even Model MATH500 AIME24 AIME25 OlympiadBench AMC-23 Minerva Math GPQA MMLU-Pro Average Qwen3-4B-Base + SPIRAL (Multi-Game) DeepSeek-Distill-Qwen-7B + SPIRAL (Multi-Game) 65.8 74.2 90.8 93.0 10.0 20.0 46.7 43.3 3.3 16. 36.7 46.7 33.3 39.4 56.9 57.9 50.0 55.0 92.5 92.5 24.3 38. 48.2 51.1 30.6 36.9 48.6 49.6 47.2 57.7 57.1 58.9 33.1 42. 59.7 61.7 Multi-game SPIRAL improves Qwen3-4B by 9.2 percentage points (33.1% to 42.3%) and enhances the already-strong DeepSeek-Distill-Qwen-7B model by 2.0 points (59.7% to 61.7%). This demonstrates that game-based reasoning skills provide value even for models that already excel at mathematical reasoning, suggesting that games teach complementary cognitive abilities not captured by traditional training. Finding 3: Different games develop specialized skills that work together Each game builds distinct abilities that transfer to similar out-of-distribution games. Multi-game training combines these specialized skills, achieving superior performance on novel games that individual specialists struggle with. This synergistic training improves reasoning benchmarks even for already-strong models, demonstrating that games teach complementary cognitive skills. 6.4. RQ4: Is Role-Conditioned Advantage Estimation (RAE) essential for stable self-play training? We ablate our variance reduction technique to determine whether RAE is necessary to stabilize training and prevent thinking collapse in zero-sum self-play with shared parameters. Figure 6 reveals that without proper variance reduction, self-play training catastrophically fails: 1. Thinking Collapse: The most striking failure occurs in response length (bottom right). Without RAE, reasoning traces plummet from 3,500 to near-zero characters after 200 steps. Models quite literally stop thinking, generating minimal outputs like <think></think><an swer>boxed{bet}</answer>. This collapse coincides with gradient norms approaching zero, indicating the model has converged to degenerate policy. 2. Performance Degradation: Without RAE, the model learns solely to play the game while abandoning CoTs, leading to generalization failure. As result, the math reasoning performance crashes from 35% to 12% at around step 150 (a 66% relative decrease), and the general reasoning drops from 44% to 40%. 3. Training Instability: Policy gradient norms without RAE exhibit higher initial values with spikes, then collapses to near-zero after step 200. In contrast, RAE maintains stable gradient 16 Figure 6 Training dynamics comparing REINFORCE with RAE (orange) versus vanilla REINFORCE (gray). RAE maintains stable performance across all metrics while vanilla REINFORCE suffers catastrophic thinking collapse. Top row: Performance on game playing, math reasoning, and general reasoning benchmarks. Bottom row: Policy gradient norm shows training instability without RAE; response length reveals thinking collapse where models stop generating reasoning traces. norms of around 0.1 throughout training, enabling continuous learning. This variance reduction is crucial for maintaining stable reasoning. High-variance gradients cause models to oscillate between extreme strategies, eventually converging to degenerate short responses that minimize variance but abandon reasoning entirely. Finding 4: RAE prevents models from abandoning reasoning during self-play Without RAE, models began to truncate their reasoning processes after 200 steps, generating empty reasoning traces like <think></think>. This thinking collapse causes math performance to crash while gradients become unstable then drop to zero. RAE maintains stable gradients throughout training, allowing models to keep generating substantive reasoning. Self-play alone is not enough; proper variance reduction is essential for maintaining reasoning in competitive games. 7. Conclusion Summary of Contributions. We introduced SPIRAL, enabling language models to develop reasoning capabilities through competitive self-play without human-curated data. Our technical contributions include fully online multi-turn MARL system for LLMs and Role-conditioned Advantage Estimation (RAE), which prevents thinking collapse in zero-sum games. Our empirical results show that training on Kuhn Poker alone improves mathematical reasoning by 8.7% average and Minerva Math by 18.1%, surpassing models trained on 25,000 expert demonstra17 tions. Different games develop distinct transferable skills, and multi-game training achieves superior performance on both reasoning benchmarks and novel out-of-distribution games, demonstrating synergistic benefits from diverse game training. Limitations. While eliminating human-curated problems, SPIRAL still requires designed game environments. Our experiments use simple games (TicTacToe, Kuhn Poker, Simple Negotiation); scaling to complex environments remains unexplored. The computational requirements are substantial (8 H100 GPUs for 25 hours per experiment). Performance plateaus after extended training, and our evaluation focuses on academic benchmarks rather than real-world reasoning tasks requiring common sense or ethical judgment. Future Directions. This work opens several avenues: expanding to cooperative games, incorporating partial observability, and designing games targeting specific reasoning weaknesses. Understanding why certain games develop particular skills could enable principled environment design. We envision ecosystems of self-improving agents generating increasingly sophisticated challenges, creating autonomous reasoning development beyond human supervision. Final Thoughts. SPIRAL demonstrates that simple games can unlock complex reasoning without domain-specific data. By harnessing competitive pressure, we create systems that discover their own curricula and continuously improve. The transfer from gameplay to mathematics could suggest that intelligence emerges not from sophisticated supervision but could from environmental challenges that force models to think. This paradigm shift points toward AI systems that autonomously push reasoning boundaries and continuously evolve through selfplay."
        },
        {
            "title": "References",
            "content": "M. Abdulhai, I. White, C. Snell, C. Sun, J. Hong, Y. Zhai, K. Xu, and S. Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023. URL https: //arxiv.org/abs/2311.18232. Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. J. Beck. Combinatorial games: Tic-tac-toe theory. 01 2008. doi: 10.1017/CBO9780511735202. R. Bellman. markovian decision process. Journal of mathematics and mechanics, 1957. C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. J. Chen, B. Zhang, R. Ma, P. Wang, X. Liang, Z. Tu, X. Li, and K.-Y. K. Wong. Spc: Evolving self-play critic via adversarial games for llm reasoning. arXiv preprint arXiv:2504.19162, 2025. Z. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language models to strong language models. In ICML, 2024. P. Cheng, T. Hu, H. Xu, Z. Zhang, Y. Dai, L. Han, X. Li, et al. Self-playing adversarial language game enhances llm reasoning. Advances in Neural Information Processing Systems, 37: 126515126543, 2024. 18 DeepSeek Team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2401.00000, 2024. T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. J. Duan, R. Zhang, J. Diffenderfer, B. Kailkhura, L. Sun, E. Stengel-Eskin, M. Bansal, T. Chen, and K. Xu. Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. arXiv preprint arXiv:2402.12348, 2024. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pages 14071416. PMLR, 2018. X. Feng, B. Liu, Y. Song, H. Fu, Z. Wan, G. A. Koushik, Z. Hu, M. Yang, Y. Wen, and J. Wang. Natural language reinforcement learning. arXiv preprint arXiv:2411.14251, 2024. Gemini Team, Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Technical report, Google, June 2025. URL https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_ report.pdf. Technical Report. L. Guertler, B. Cheng, S. Yu, B. Liu, L. Choshen, and C. Tan. Textarena, 2025. URL https: //arxiv.org/abs/2504.11442. C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. A. P. Jacob, A. Gupta, and J. Andreas. Emergent linguistic phenomena in multi-agent communication games. arXiv preprint arXiv:2205.05984, 2022. N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and R. Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019. D. P. Kingma and J. Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. H. W. Kuhn. simplified two-person poker. Contributions to the Theory of Games, 1:97103, 1950. W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. Advances in neural information processing systems, 20, 2007. A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. A. Liao, N. Tomlin, and D. Klein. Efficacy of language model self-play in non-zero-sum games, 2024. URL https://arxiv.org/abs/2406.18872. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. M. L. Littman. Markov games as framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157163. Elsevier, 1994. M. Liu, L. Jiang, Y. Liang, S. S. Du, Y. Choi, T. Althoff, and N. Jaques. Chasing moving targets with online self-play reinforcement learning for safer language models, 2025a. URL https://arxiv.org/abs/2506.07468. Z. Liu, C. Chen, X. Wan, C. Du, W. S. Lee, and M. Lin. Oat: research-friendly framework for llm online alignment. https://github.com/sail-sg/oat, 2024. Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Mistral. Mistral-small-3.1-24b-instruct-2503. https://huggingface.co/mistralai/Mistr al-Small-3.1-24B-Instruct-2503, 2025. J. Nash. The bargaining problem. Econometrica, 18(2):155162, 1950. OpenAI. Learning to reason with llms. OpenAI Blog, 2024. URL https://openai.com/o1. OpenAI. GPT-4.1. OpenAI API, 2025. URL https://openai.com/index/gpt-4-1/. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. D. Paglieri, B. Cupiał, S. Coward, U. Piterbarg, M. Wolczyk, A. Khan, E. Pignatelli, Ł. Kuci nski, L. Pinto, R. Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. C. Park, S. Han, X. Guo, A. Ozdaglar, K. Zhang, and J.-K. Kim. Maporl: Multi-agent post-cotraining for collaborative large language models with reinforcement learning. arXiv preprint arXiv:2502.18439, 2025. R. Rafailov, J. Hejna, R. Park, and C. Finn. From to q*: Your language model is secretly q-function. In Conference on Language Modeling, 2024. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. In First Conference on Language Gpqa: graduate-level google-proof q&a benchmark. Modeling, 2024. A. Ruoss, F. Pardo, H. Chan, B. Li, V. Mnih, and T. Genewein. Lmact: benchmark for in-context imitation learning with long multimodal demonstrations. arXiv preprint arXiv:2412.01441, 2024. B. Sarkar, W. Xia, C. K. Liu, and D. Sadigh. Training language models for social deduction with multi-agent reinforcement learning. arXiv preprint arXiv:2502.06060, 2025. 20 Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. G. Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3): 5868, 1995. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Z. Wan, Y. Li, X. Wen, Y. Song, H. Wang, L. Yang, M. Schmidt, J. Wang, W. Zhang, S. Hu, et al. Rema: Learning to meta-think for llms with multi-agent reinforcement learning. arXiv preprint arXiv:2503.09501, 2025. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Z. Wang, K. Wang, Q. Wang, P. Zhang, L. Li, Z. Yang, X. Jin, K. Yu, M. N. Nguyen, L. Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chainof-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837, 2022. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Y. Xie, Y. Ma, S. Lan, A. Yuille, J. Xiao, and C. Wei. Play to generalize: Learning to reason through game play. arXiv preprint arXiv:2506.08011, 2025. H. Xin, Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024. F. Xu et al. Genius: generalizable and purely unsupervised self-training framework for advanced reasoning, 2025. A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. W. Yuan, R. Y. Pang, K. Cho, X. Li, S. Sukhbaatar, J. Xu, and J. Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. X. Zhang, H. Zheng, A. Lv, Y. Liu, Z. Song, F. Sung, X. Chen, and R. Yan. Divide-fuse-conquer: Eliciting\" aha moments\" in multi-scenario games. arXiv preprint arXiv:2505.16401, 2025. Y. Zhang, S. Mao, T. Ge, X. Wang, A. de Wynter, Y. Xia, W. Wu, T. Song, M. Lan, and F. Wei. Llm as mastermind: survey of strategic reasoning with large language models. arXiv preprint arXiv:2404.01230, 2024. A. Zhao, Y. Wu, Y. Yue, T. Wu, Q. Xu, M. Lin, S. Wang, Q. Wu, Z. Zheng, and G. Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. Q. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao, S. Ma, et al. Deepseekcoder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024."
        },
        {
            "title": "Appendix",
            "content": "This appendix provides comprehensive details supporting our main findings. App. shows how existing SFT and RLVR paradigms adapt to our turn-level MDP formulation. App. contains implementation specifics including hyperparameters, infrastructure details, and game environment observations referenced in Section 5.2. App. presents extended results tables and detailed pattern evolution analysis that support our case study in Section 6.1. App. describes the complete methodology for our reasoning pattern analysis, detailing how we discovered and quantified transfer mechanisms. Finally, App. provides comprehensive descriptions of for all game environments used in our experiments, including both training and evaluation games. A. Training Paradigms in Turn-Level MDPs This section shows how existing training paradigms adapt to our turn-level MDP formulation introduced in Section . A.1. Supervised Fine-Tuning (SFT) in Turn-Level MDPs 𝑁 In the turn-level setting, SFT requires dataset 𝑖=1 of states with expert reasoning traces 𝑐𝑖 and actions 𝑎𝑖 . The model learns to imitate complete turn-level responses: 𝑠𝑖, 𝑐𝑖 , 𝑎𝑖 )} DSFT = {( LSFT( 𝜃 ) = 𝑠,𝑐,𝑎 )DSFT ( (cid:2)log 𝜋𝜃( 𝑐, 𝑎 (cid:3) . 𝑠 ) (11) Note that in single-turn settings where each state 𝑠 appears only once, SFT reduces to standard behavior cloning. The key limitation remains: SFT requires expensive human annotation of both reasoning traces and final answers. A.2. Reinforcement Learning with Verifiable Rewards (RLVR) in Turn-Level MDPs RLVR (DeepSeek Team, 2024) eliminates the need for reasoning supervision, requiring only state-answer pairs 𝑁 𝑖=1. In the turn-level formulation: DRLVR = = E𝑠 DRLVR,𝑦 𝜋𝜃 ( 𝑠 𝑟 ( ) [ 𝑠, 𝑎 )] , (12) 𝑠𝑖, 𝑎𝑖 )} {( 𝜃 𝐽RLVR( ) where 𝑟 𝑠, 𝑎 ) ( = 𝑎 = 𝑎] [ indicates answer correctness and 𝑦 contains both reasoning and action. In single-turn settings without subsequent interactions, RLVR reduces to contextual bandit problem (Langford and Zhang, 2007). Recent works on mathematical (DeepSeek Team, 2024; Shao et al., 2024) and code reasoning (Xin et al., 2024; Zhu et al., 2024) show that even this simplified bandit-style RLVR can unlock sophisticated reasoning. However, these approaches DRLVR, which SPIRAL eliminates through self-play. still require human-curated problem sets B. Experimental Setup Details This section provides complete implementation details for reproducing our experiments. We begin with visual examples of game environments, followed by our hyperparameter configurations. 23 B.1. Game Environment Observations The language models receive structured text observations from each game environment. Fig. 7 shows example observations from our three training games: TicTacToe, Kuhn Poker, and Simple Negotiation. These observations serve as the input prompts 𝑠𝑡 at each turn, providing complete game state information in natural language format. Figure Example observations of three training game environments. For games with partial observability such as Kuhn Poker and Simple Negotiation, we maintain Markovian state representations by concatenating historical actions into the current state 𝑠𝑡. This ensures the model has sufficient information for decision-making despite hidden information. Similarly, Fig. 8 presents observations from five evaluation environments used to test out-ofdistribution generalization. These games were never seen during training, allowing us to assess whether learned skills transfer to novel game mechanics. B.2. Hyperparameter Configuration Table 7 presents the complete hyperparameter settings used across all experiments. These configurations were selected through preliminary experiments to balance training stability and computational efficiency. Please see our open-source codebase for complete and reproducible experiment example. These hyperparameters remain fixed across all game environments and model scales to ensure fair comparison. The distributed training infrastructure utilizes 8 H100 GPUs, with parallel actors generating game trajectories while centralized learner performs synchronous policy updates. C. Additional Results and Analysis C.1. Detailed Evolution of Case-by-Case Analysis To understand how reasoning patterns develop during training, we tracked the evolution of case-by-case analysis across checkpoints. Table 8 shows concrete example from Minerva Math Problem 135, illustrating how models progressively develop structured reasoning. This progression demonstrates how competitive self-play forces models to develop increasingly structured approaches. Early attempts show unorganized reasoning, while later Figure 8 Example observations of five evaluation game environments. checkpoints exhibit clear case separation and systematic analysis, pattern that emerges from game playing and transfers to mathematical problem solving. C.2. Comprehensive Benchmark Results Table 9 presents extended results showing SPIRALs performance across different training configurations and base models. These results reveal several important insights. First, single-game SPIRAL training (40.041.4% average) outperforms supervised fine-tuning on 25,000 expert examples (38.4% average), validating that self-play can discover more effective reasoning strategies than imitating expert demonstrations. Second, multi-game training (42.3-42.7% average) consistently outperforms single-game variants, suggesting that diverse cognitive challenges create more robust reasoning capabilities. Third, SPIRAL improves even strong models like DeepSeek-Distill-Qwen-7B (from 59.7% to 61.7%), demonstrating that competitive game self-play training can enhance models that already excel at reasoning tasks. D. Case Study Methodology This section details our systematic approach to discovering and analyzing reasoning pattern transfer from games to mathematics. Rather than searching for predetermined patterns, we employed bottom-up discovery process to identify what reasoning strategies naturally emerge and transfer between domains. D.1. Data Collection Framework Our analysis examined reasoning traces from two sources across three training checkpoints: Parameter Maximum response length Sampling temperature (top P, top k) Optimizer Adam parameters (𝛽1, 𝛽2) Weight decay Gradient norm clipping Batch size Discount factor EMA decay rate Learning rate scheduler Learning rate Inner proximal update epoch KL loss coefficient KL penalty coefficient Policy clipping parameter Value O 4096 tokens 1.0 (1.0, -1) A R AdamW (0.9, 0.95) 0.0 1.0 128 1.0 0.95 Constant 1 10 2 0.0 0.0 0.2 6 Table 7 Hyperparameter configurations used in all experiments. Game Trajectories: We collected 290 complete Kuhn Poker games, focusing on winning trajectories to identify successful reasoning strategies. Each trajectory includes the complete thought process from initial card observation through final decision. Mathematical Solutions: We analyzed 46,792 solution attempts across MATH500, AIME, OlympiadBench, and Minerva Math benchmarks. Solutions were categorized by success (score=1) or failure (score=0) to understand which reasoning approaches prove effective. Temporal Analysis: Checkpoints at steps 0 (initial), 128 (intermediate), and 400 (final) capture the evolution of reasoning complexity throughout training. D.2. Bottom-Up Pattern Discovery Process Rather than searching for predefined patterns, we employed GPT-4.1 to discover patterns that naturally emerge in the data. This bottom-up approach ensures we capture the actual reasoning strategies used rather than imposing our expectations. 26 Table 8 Stage Evolution of Case-by-Case Analysis in Minerva Problem 135 Problem: Round cosmic ages to nearest power of Early(Ckpt 16) Approach: Lumped reasoning without explicit case separation <output> The age when helium formed is approximately 10 minutes... The age when hydrogen formed is 377,000 years... The current age is 13.8 Gyr. Rounding to nearest power of 10, we get: 14 Gyr </output> Case Structure: No explicit cases - all subproblems handled in continuous text Result: Subproblem 2: 14 Gyr (should be 10) Mid(Ckpt 128) Approach: Explicit enumeration of cases with structured analysis 20 minutes <output> Case 1 - Subproblem 0: 1. Understanding the Context: BBN at 2. Nearest Power of 10: 101 = 10 3. Final Answer: 1 minute Case 2 - Subproblem 1: 1. Understanding the Context: Recombination at 2. Nearest Power of 10: 105 = 100, 000 3. Final Answer: 400000 years Case 3 - Subproblem 2: 1. Understanding the Context: Current age 2. Nearest Power of 10: Identifies 1010 correctly 3. Final Answer: 13.8 Gyr </output> Case Structure: Clear case separation with consistent 3-step process Result: Still incorrect final rounding 377,000 years 13.8 Gyr Late(Ckpt 400) Approach: Systematic case-by-case with correct execution 20 minutes after Big Bang <think> Case 1 - Subproblem 0: 1. Understand Context: He formation in early universe 2. Estimate Time: 3. Round to Power of 10: 20 minutes Case 2 - Subproblem 1: 1. Understand Context: atom formation (recombination) 2. Estimate Time: 3. Round to Power of 10: 400000 years Case 3 - Subproblem 2: 1. Understand Context: Current universe age 2. Convert Units: 13.8 billion years = 13.8 Gyr 3. Round to Power of 10: 13.8 Case Structure: Complete systematic enumeration with correct logic Result: All cases solved correctly 101 = 10 Gyr </think> 380,000 years 27 Table 9 Model SPIRAL training improves reasoning benchmarks for different base models MATH500 AIME24 AIME25 OlympiadBench AMC-23 Minerva Math GPQA MMLU-Pro Average Qwen3-4B-Base + SFT + Mistral Opponent (KuhnPoker) + Gemini Opponent (KuhnPoker) + SPIRAL (TicTacToe) + SPIRAL (KuhnPoker) + SPIRAL (Negotiation) + SPIRAL (TicTacToe+KuhnPoker) + SPIRAL (TicTacToe+KuhnPoker+Negotiation) DeepSeek-Distill-Qwen-7B + SPIRAL (Multi-Game) 65.8 76.0 64.0 69.2 75.6 76.4 76.4 76.2 74.2 90.8 93.0 Qwen3-4B-Base Family 10.0 16.7 6.7 10.0 10.0 13.3 10.0 13.3 20.0 3.3 13.3 6.7 13.3 13.3 10.0 10.0 16.7 16.7 33.3 38.4 29.8 33.8 38.5 38.4 41.2 40.7 39.4 DeepSeek-Distill-Qwen-7B Family 46.7 43.3 36.7 46. 56.9 57.9 50.0 50.0 50.0 50.0 55.0 57.5 57.5 60.0 55.0 92.5 92.5 24.3 31.2 26.1 33.8 42.6 42.4 36.0 41.5 38.6 48.2 51.1 30.6 33.0 35.6 35.3 37.6 37.0 34.7 35.7 36. 48.6 49.6 47.2 48.8 43.6 55.5 57.7 56.5 54.2 57.2 57.7 57.1 58.9 33.1 38.4 32.8 37.6 41.3 41.4 40.0 42.7 42.3 59.7 61.7 Pattern Discovery Prompt Analyze these {domain} reasoning traces using BOTTOM-UP approach. Dont look for predefined patterns. patterns actually exist. REASONING TRACES ({len(sample)} samples from {len(traces)} total): {json.dumps([t[reasoning] for in sample[:40]], indent=2)} Your task: Instead, discover what reasoning 1. Read through ALL the reasoning traces carefully 2. Identify RECURRING patterns or structures that appear multiple times 3. Group similar reasoning approaches together 4. Name each discovered pattern based on what it actually does 5. Count how many traces use each pattern 6. Provide example quotes for each pattern Format your response as: PATTERN 1: [Descriptive Name] - Description: [What this pattern does] - Count: - Example quotes: [2-3 actual quotes showing this pattern] Be specific and grounded in the actual data. once, dont include it. X/{len(sample)} traces"
        },
        {
            "title": "If you see a pattern only",
            "content": "This discovery process revealed three dominant patterns that emerged independently in both domains: 1. Case-by-Case Analysis: Systematic enumeration of scenarios 2. Expected Value Calculation: Probabilistic decision-making 3. Pattern Recognition: Identifying regularities and structures D.3. Cross-Domain Transfer Quantification After discovering patterns in each domain, we compared them to identify which strategies transfer between games and mathematics: Pattern Comparison Prompt Compare the reasoning patterns discovered in games vs mathematics: GAME PATTERNS: {game_patterns} MATH PATTERNS: {math_patterns} Analyze: 1. Which patterns appear in BOTH domains? 2. Which patterns are unique to each domain? 3. Calculate transfer rates for shared patterns 4. Identify the most successfully transferred reasoning strategies 5. Explain WHY certain patterns transfer well (These show transfer) Focus on concrete evidence of transfer, not speculation. The transfer analysis revealed: Case-by-Case Analysis shows near-perfect transfer (72% in games to 71% in math) because systematic enumeration represents domain-agnostic structured thinking. Whether analyzing opponent possibilities in Poker or solution branches in mathematics, the core cognitive skill remains identical. Expected Value Calculation exhibits limited transfer (78% in games to 28% in math) because explicit probabilistic decision-making appears primarily in probability and optimization problems. Most mathematical domains lack the decision-theoretic structure that makes this pattern universally applicable in games. Pattern Recognition demonstrates amplification during transfer (35% in games to 45% in math). Mathematics inherently requires pattern identification, so game training enhances an already-essential mathematical skill, producing stronger pattern recognition than games alone develop. D.4. Pattern Evolution Analysis To understand how reasoning develops during training, we tracked pattern emergence across checkpoints: Evolution Analysis Prompt Analyze how reasoning patterns evolve across training checkpoints: {json.dumps(evolution_analysis, indent=2)} For each checkpoint: 1. Describe the complexity of reasoning 2. Identify new patterns that emerge 3. Track how patterns become more sophisticated 4. Show concrete examples of improvement Focus on the actual evolution you can see in the data. D.5. Concrete Transfer Example Identification To validate transfer claims, we identified parallel reasoning structures across domains: Transfer Example Prompt Find the clearest examples of reasoning transfer from games to mathematics: {json.dumps(examples, indent=2)} For each complexity level (short/medium/long): 1. Identify parallel reasoning structures 2. Quote specific passages that show transfer 3. Explain what cognitive skill is being transferred 4. Rate the clarity of transfer (1-10) Focus on examples where the same reasoning approach clearly appears in both domains. D.6. Pattern Classification at Scale After discovering patterns through bottom-up analysis, we classified all traces to measure transfer rates: Pattern Classification Prompt Analyze these {domain} reasoning traces and find examples of each pattern. PATTERNS TO FIND: 1. Case-by-Case Analysis: Systematic enumeration of different scenarios/cases 2. Expected Value Calculation: computing expected outcomes Explicit probability calculations, 3. Pattern Recognition: Identifying recurring structures, noticing trends REASONING TRACES: {json.dumps(batch, indent=2)} For EACH pattern, identify ALL examples that clearly demonstrate it. Return in this EXACT format: CASE_BY_CASE_INDICES: [list of indices] EXPECTED_VALUE_INDICES: [list of indices] PATTERN_RECOGNITION_INDICES: [list of indices] Be strict - only include clear examples of each pattern. D.7. Validation Methodology To ensure robust findings, we implemented multiple validation steps: Sampling Strategy: We analyzed 50 random trajectory samples per checkpoint to avoid selection bias while maintaining computational feasibility. Success Stratification: Separate analysis of successful and failed attempts revealed which reasoning strategies genuinely contribute to problem-solving rather than merely appearing frequently. Manual Verification: Spot-checking GPT-4.1s pattern classifications against raw traces confirmed the accuracy of automated analysis. 30 Scale Validation: After discovering patterns through focused analysis, we classified all 46,792 mathematical traces to verify that observed transfer rates hold at scale. This methodology ensures our findings reflect genuine cognitive transfer rather than superficial pattern matching, providing quantitative evidence that competitive gameplay develops reasoning skills applicable far beyond the training domain. E. Game Environment Specifications This section provides detailed specifications for all game environments used in our experiments, including both training and evaluation games. E.1. Training Game Environments TicTacToe tests spatial pattern recognition through perfect information gameplay. Players alternate placing marks on 3 3 grid, aiming to create lines of three. The deterministic nature isolates pure strategic reasoning from uncertainty management. Success requires recognizing winning patterns, blocking opponent threats, and creating fork positions that guarantee victory. We hypothesize these skills transfer to geometric reasoning and spatial visualization tasks. Kuhn Poker introduces probabilistic reasoning through minimal hidden information. With only three cards (Jack, Queen, King), one per player plus one undealt, the game distills poker to essential elements of bluffing and value betting. Players can check, bet, call, or fold, with outcomes determined by card strength. Success requires calculating expected values, modeling opponent behavior, and making decisions under uncertainty. These capabilities should transfer to probability problems and strategic decision-making. Simple Negotiation develops multi-constraint optimization through resource trading. Two players exchange Wood and Gold tokens, with opposing utility functions creating natural tension. Each player aims to maximize portfolio value through proposals and counteroffers. Success requires understanding opponent preferences, planning multi-step trades, and strategic communication. We expect these skills to enhance optimization problems and multi-constraint reasoning tasks. E.2. Out-of-Distribution Evaluation Games Our evaluation suite tests whether learned skills generalize to novel mechanics: Snake extends spatial reasoning to dynamic environments. Players control snakes navigating grids to collect apples while avoiding collisions with walls, themselves, or opponents. This tests whether static pattern recognition from TicTacToe transfers to trajectory planning and dynamic obstacle avoidance. Connect Four adds directional constraints to spatial reasoning. Players drop discs into columns with gravity, aiming to connect four vertically, horizontally, or diagonally. The vertical placement mechanic tests whether TicTacToe strategies generalize to games with restricted movement options. Pig Dice isolates risk-reward decision making. Players repeatedly roll dice to accumulate points but lose all turn points when rolling 1. This tests whether probabilistic reasoning from Kuhn Poker extends to sequential risk assessment and expected value calculation in different contexts. Liars Dice combines probability with deception detection. Players make escalating claims 31 about hidden dice totals and challenge suspected bluffs. Success requires probability calculation, opponent modeling, and strategic deception, testing whether Kuhn Poker skills scale to multiround bluffing games. Truth and Deception focuses on asymmetric information and persuasion. One player knows the true fact among options and misleads through conversation while the other must identify truth through questioning. This evaluates whether negotiation skills transfer to pure communication strategy. These diverse evaluation games probe different aspects of transfer learning, revealing which cognitive skills generalize beyond their training context and confirming that SPIRAL develops fundamental reasoning capabilities rather than game-specific tactics."
        }
    ],
    "affiliations": [
        "Centre for Frontier AI Research (CFAR), A*STAR",
        "National University of Singapore",
        "Northeastern University",
        "Plastic Labs",
        "Sea AI Lab",
        "University of Washington"
    ]
}