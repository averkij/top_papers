{
    "paper_title": "Detection Avoidance Techniques for Large Language Models",
    "authors": [
        "Sinclair Schneider",
        "Florian Steuber",
        "Joao A. G. Schneider",
        "Gabi Dreo Rodosek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models' temperature proofed shallow learning-detectors to be the least reliable. Fine-tuning the generative model via reinforcement learning circumvented BERT-based-detectors. Finally, rephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT, although texts stayed highly similar to the original. A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 5 9 5 7 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Detection Avoidance Techniques for Large Language Models",
            "content": "Sinclair Schneider1, Florian Steuber1, JoÃ£o A. G. Schneider1 and Gabi Dreo Rodosek1 1Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany. E-mail: Sinclair.Schneider@unibw.de. Keywords: Language Models, Language Model Detection, Transformer Reinforcement Learning, Paraphrasing Attack Abstract The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models temperature proofed shallow learning detectors to be the least reliable (Exp. 1). Fine-tuning the generative model via reinforcement learning circumvented BERT-baseddetectors (Exp. 2). Finally, rephrasing led to >90% evasion of zero-shotdetectors like DetectGPT, although texts stayed highly similar to the original (Exp. 3). comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed. Policy Significance Statement Large language models produce texts that appear indistinguishable from human ones, which is why research focuses on machine learning based detectors. This paper demonstrates how various stateof-the-art detectors can be tricked using different techniques. Specifically, text-generating models are modified in such way they a) no longer use the most likely words (parameter temperature), b) are penalized for certain conspicuous content (reinforcement learning), or c) rephrase sentences so slightly that they remain the same in terms of content but can no longer be recognized as machine-generated (paraphrasing). In short, detectors can easily be bypassed. The implications for society and research are discussed. Further research is needed to investigate implications, such as the influence on opinion or fake news in social media. 1. Introduction As large language models (LLMs) continue to evolve, the necessity for precise differentiation between human outputs and those produced by LLMs is becoming increasingly critical. Recent developments in LLMs have significantly improved, particularly in complex reasoning tasks, such as mathematical problem-solving. These advancements are rapidly closing the performance gap between LLMs and human capabilities, which had previously been major limitation of such models. Due to this close-up, the difficulty of reliably detecting LLMs has further increased. Various concepts helped LLMs to catch up to human-like reasoning capabilities. For example, the Quiet-STaR approach (Quiet-STaR) reinforces intermediate beliefs generated by the model before providing final answer, improving reasoning accuracy. The Agent framework (AgentQ) combines Monte Carlo Tree Search (MCTS) and Direct Preference Optimization (DPO) to teach LLMs to perform complex tasks, such as navigating an online store. Other work, including Lets Verify Step by Step (LetsVerifyStepbyStep), enhances LLM reasoning performance by breaking tasks into discrete steps and providing feedback for each step. Likewise, Verification for Self-Taught Reasoners (vstar) refers Published in Data & Policy, vol. 7, p. e29, 2025. doi:10.1017/dap.2025.6 to concept in which the LLM generates multiple solutions for task, learning from correct answers. verifier model learns from correct and incorrect responses, improving the LLMs reasoning ability, particularly in coding and mathematical tasks."
        },
        {
            "title": "In light of",
            "content": "these rapid advancements, growing concern is that distinguishing humanfrom LLM-generated text might become even more challenging. Models such as DetectGPT (mitchell_detectgpt_2023) and datasets like the Human ChatGPT Comparison Corpus (ChatGPTvsHuman) aim to address this challenge. However, further investigation is required to assess the reliability of these detection models and explore whether they can be circumvented with reasonable effort. Like cryptography, every detection method, as shown in the current study, is at risk of being attacked and eventually circumvented. Such insights lead to the need for additional and robust methods, such as watermarks, to help clarify the origin of published texts. This study presents an experimental series to evaluate the reliability of LLM detection models and explore potential methods for bypassing them. In the first experiment (cf. Sec. 3), shallow learning classifiers are evaluated based on Bag-of-Words (BoW) approach combined with Naive Bayes classifier. Although this method is not state-of-the-art, it serves as benchmark to showcase the influence of hyperparameters, including temperature, sampling method, and model size on classification performance. Additionally, the classifiers performance is compared to human judgment. Human classification often focuses on identifying unlikely words, while machine models rely on statistical patterns (DBLP:conf/acl/IppolitoDCE20). According to DBLP:conf/acl/GehrmannSR19, human judgment achieves an accuracy of only 54% without automated assistance, improving up to 72% with supporting tools. Other studies similarly report 50% success rate in detecting GPT-3-generated text (DBLP:conf/acl/ClarkASHGS20). Using top-p sampling at 1.0, the Naive Bayes classifiers detection rate was reduced below 60%, indicating that simple models may be bypassed without special techniques. In the second experiment (cf. Sec. 4), the combination of shallow feature categories and shallow classifiers is replaced with BERT-based classifier and corresponding transformer-based embeddings, yielding significantly improved accuracy exceeding 90%. This forms the basis for the initial bypassing approach. In contrast to the one described by krishna_paraphrasing_2024, paraphrasing is not used at this stage. Instead, reinforcement learning (RL) is employed for model training to preserve the generative model from detection. This approach builds upon the methodology outlined by DBLP:journals/corr/abs-1909-08593-neu, which originally fine-tuned LLMs using human feedback. Special constraints are incorporated into the reward function to prevent the model from learning trivial bypassing strategies, such as adding special characters or introducing artifacts. Depending on the LLMs size, the detection rate reduces from over 90% to below 17% following the RL training. This demonstrates that once the classifier is known and accessible, the generative model can be adapted to evade it. In the third experiment (cf. Sec. 5), paraphrasing model is applied, similar to the approach introduced by krishna_paraphrasing_2024. While krishna_paraphrasing_2024 have focused on general-purpose paraphrasing, the here presented model is tailored to hide the generative model from DetectGPT specifically. In this regard, new dataset is vital. Here, the original LLM output is further paraphrased multiple times. This procedure allows us to select the version least likely to be classified as LLM-generated. Inspired by mitchell_detectgpt_2023, each single paraphrasing iteration altered approximately 15% of the sentence. Based on the newly created dataset, paraphrasing model is trained to hide the original language model. By applying this paraphrasing model to the output of the Qwen1.5-4B-Chat model (qwen_2023), the detection rate was iteratively reduced from 88.6% to 8.7%. comparative analysis uses the DIPPER model from krishna_paraphrasing_2024. Hereby, the presented approach preserves higher degree of linguistic similarity to the original text, even after multiple paraphrasing iterations. In conclusion, this study demonstrates that RL and paraphrasing techniques can effectively bypass LLM detection classifiers. These results suggest that classifier can easily be bypassed with sufficient knowledge. This can be achieved by fine-tuning (RL) or paraphrasing. The findings demonstrate the potential for malicious actors to circumvent classification. Further, the need for ongoing research into more robust and adaptive detection mechanisms is underlined. 2. Related Work Since this work combines different fields, this section is subdivided: Firstly, ways to automatically generate text using LLMs are explained (Sec. 2.1). Secondly, relevant datasets respectively benchmarks are brought in as well (Sec. 2.2). Thereafter, detection methods for identifying LLM content are introduced (Sec. 2.3). Additionally, possible ways how to bypass those classifiers are discussed. 2.1. Automatic Text Generation Various approaches have been employed in developing language models capable of generating text. The most prevalent architecture is based on transformer models, which include GPT and its predecessors like GPT-Neo-125M, GPT-Neo-1.3B, GPT-Neo-2.7B (gpt-neo), GPT-J-6B (gpt-j), OPT-125M, OPT350M, OPT-1.3B, OPT-2.7B (zhang2022opt), and GPT-2 (radford2019language). Other variants encompass Instruct-GPT (ouyang2022training), Googles T5 models (2020t5) as well as their Gemma series (gemma_team_gemma_2024), Metass Llama series (dubeyLlamaHerdModels2024), Mistral AIs Mistral (mistral7b) and Mixtral (mixtralexperts) series, the Qwen models (qwen2) from Alibaba Cloud, to name just very few. These LLMs are often referred to as stochastic parrots (bender2021stochasticparrots) since they generate sentences by predicting the next word based on probability distributions. The selection of the token with the highest probability is known as greedy search. As humans do not always choose the most probable sequence of words, this purely deterministic approach does not capture the variability in human language (TheCuriousCase). To introduce randomness, pure random sampling selects tokens proportionally to their probabilities, increasing diversity but eventually reducing coherence (bengio2003neural). Intermediate approaches have been developed to balance determinism and diversity. These include typical sampling, which prioritizes tokens near the mode of the probability distribution, and top-k sampling, which restricts choices to the most probable tokens (meister2022locally; fan2018hierarchical). Alternatively, nucleus sampling dynamically adjusts the candidate set to include only tokens whose cumulative probability remains below threshold (TheCuriousCase). Each method reflects trade-offs between control, coherence, and creativity. Closely associated with creativity is also the temperature. The parameter ð [0, 2] modulates the sharpness of the probability distribution, where higher temperatures (ð > 1) flatten the distribution, allowing for more diverse and creative outputs but increasing the risk of incoherence. Conversely, lower temperatures (ð < 1) sharpen the distribution, focusing on high-probability tokens and yielding more deterministic but less creative text. Different sampling strategies might be combined with ð, tailoring text generation to specific requirements. 2.2. Datasets & Benchmarks direct comparison of human versus machine-generated content is provided by the Human ChatGPT Comparison Corpus (ChatGPTvsHuman). This dataset consists of questions regarding various topics, each answered by an LLM versus real humans. research, corpus (NaturalQuestionsGoogleBenchmark) serves as benchmark. This dataset consists of real user search requests. Using CNN and Daily Mail articles, another question-answering corpus is provided by cnnDailyMailDataset. question-answering Questions Googles Natural For The Corpus of Linguistic Acceptability dataset (CoLA) comprises sentences that have been labeled as either grammatically acceptable or unacceptable by human annotators. It might be used for training LLMs such as DeBERTa-v3-large (he2021debertav3), serving as classifier for evaluating linguistic acceptability. 2.3. Detection Techniques In the social media context discussed here, analyses are built on the users behavior or content. While focusing on content specifically (i.e., posts on X, formerly Twitter), the reader is referred to the literature review by alothali-2018 for deeper insights into the behavior-based approach. Various focus and content-independent detection methods exist. According to orabi-2020, these can be categorized as graph-based approaches (abou2020botchase), crowdsourcing techniques (wang2012social), anomaly detection methods (nomm2018unsupervised), and machine learningbased approaches (alothali-2018). This categorization by orabi-2020 can be broken down to machineversus human-based techniques since detection is performed either manually (e.g., bot identification) or by machines, i.e., based on ML or pure statistical anomalies. Human-based Detection Human performance can be examined in experimental settings. Here, human versus machine-generated texts are presented to the participants (independent variable). The subjects, who do not know the origin of the texts, are asked to identify machine-generated ones. Based on representative random sample, this procedure allows the estimation of general human accuracy, false alarm rate, etc. (dependent variables). These scores can not only be compared to those resulting from computer-based classifiers but can also be used to identify characteristics, i.e., what humans or machines tend to prioritize and what makes them more likely to fail. Demonstrated that way, humans tend to prioritize the semantic coherence of the text. At the same time, machine-based detection methods emphasize statistical properties such as word probabilities and sampling schemes (DBLP:conf/acl/IppolitoDCE20). Humans seem to know how and what other humans would talk about compared to machines. The fact that machines can mimic these expressions and content was used by jakesch_human_2023: In their experiment, participants could not distinguish between AI-generated and human-written selfpresentations misled by the AIs usage of first-person pronouns, contractions, or family topics. It can be concluded that time and resource-intensive human-based detection does not lead to better results since humans can easily be deceived by using the above-mentioned factors. This has been confirmed by DBLP:conf/emnlp/DuganIKC20, who introduced tool for assessing human detection capabilities. Their demonstration of how easily humans can be deceived underlines the importance of machineand statistical-based detection. Machine-based Detection Many text generation models leave behind specific artifacts whose occurrence is extremely unlikely compared to human text (DBLP:conf/acl/TayBZBMT20). Those probabilities or word frequencies can be examined using statistical methods (see below). Since manual calculation is theoretically possible here, these methods can be considered as simple. These are to be distinguished from those methods that require the use of ML models, such as LLMs, which are therefore considered separately. Models using the transformer architecture can be used for language generation and detection. The MultiNLI benchmark (MultiNLI) allows performance comparison regarding detection: Here, BERT-large achieves an accuracy of 88% (BERT-Large), while RoBERTa and DeBERTa score 90.8% (RoBERTa) and 91.1% (DeBERTa), respectively. However, for every generator release, BERT-based classifiers such as RoBERTa must be trained again. Therefore, zero-shot classifiers like DetectGPT become quite handy (mitchell_detectgpt_2023). These classifiers only require two models: duplicate of the model to be tested and second language model introducing random permutations into the test string. However, the permutation models can not handle short inputs due to their working principle. Paraphrasing outlines the weak spot of the discussed classifiers. AI-based paraphrasing can be detected successfully, as shown by li_spotting_2024. However, this technique focuses solely on paraphrasing detection. Thus, the model has no ability to tell if the original text was humanor machinegenerated. Therefore, paraphrasing can still serve as an effective bypassing technique. For instance, this technique is used by krishna_paraphrasing_2024 and enhanced by sadasivan_can_2024 using recursive paraphrasing. While krishna_paraphrasing_2024 introduced the T5-based DIPPER as their own paraphrasing model, sadasivan_can_2024 used DIPPER and other existing models combined without any model training or fine-tuning. sadasivan_can_2024 claim their model generates paraphrases with high text quality and content preservation based on human ratings on five-point Likert scale. The models output can be altered not only through separate paraphrasing model but also learning approach that directly modifies the generative model. This through reinforcement method is inspired by the paper Fine-Tuning Language Models from Human Preferences by DBLP:journals/corr/abs-1909-08593-neu. Another approach focuses on circumventing DetectGPT (mitchell_detectgpt_2023) using by paraphrasing model krishna_paraphrasing_2024 detects AI-generated content by comparing it to database of AIgenerated texts. This solution has two significant drawbacks: First, the provider of this AI model must establish such service, and second, custom fine-tuned private-run models are inaccessible (kucharavy_llm_2024). (krishna_paraphrasing_2024). The countermeasure suggested When the underlying generative model is known, it can be modified and hence serve as classifier after further training (zellers_defending_2019). zellers_defending_2019, who originally built up their model for fake news generation, proved that this model is also most effective in detecting their own fake news. This implies that detectors originating from the generative model itself are better at detecting artificially generated fake news than standard classifiers. Both fake news generators and fake news detectors are combined by henrique_stochastic_2023 as generators and discriminators in the form of generative adversarial network (GAN) to demonstrate the attack against classification model. Statistical-based Detection After ChatGPTs introduction (gpt-intro-2022), the frequency of certain words, such as intricate, meticulously, commendable, or meticulous, has changed significantly in academic literature (gray_chatgpt_2024). In particular, these words increased by 50%, and others, such as innovatively even by 60% compared to pre-2022 levels; for complete list of words and statistics, see the dataset by gray_dataset_2024. While one of these so-called group-1 words may have been used by chance, the multiple use of group-1 words within text is so unlikely that it can be considered GPT-generated. gray_chatgpt_2024 estimates 1% (60.000 articles) of all 2023 publications to be machine-generated and even predicts further increase in 2024. Several other rule-based models exist, specially designed to identify automatically generated texts. These rely on improbable word sequences and grammar (DBLP:journals/jasis/CabanacL21) and similarity measures such as word overlap (harada2021discrimination). statistical-based countermeasure is the usage of watermarks in generated texts, as suggested by kirchenbauer_watermark_2023. This approach adds constant ð¿ to green list of tokens while they are autoregressively sampled. So, some words appear slightly more often than others, although they are in the right position and do not jeopardize the sentences grammar or meaning. Due to the potential of paraphrasing, detecting machine-generated text using watermarks has limitations. Removing one-quarter of the watermark tokens can be enough to evade the detection (kirchenbauer_watermark_2023). Additionally, the generation itself infers with the sampling process of the language model by withholding certain tokens and strengthening others. 3. Experiment 1: Evasion of Shallow Detectors Various variables may affect the accuracy (ACC) of detector, above all, the type of detector itself. For example, transformer-based detectors will generally perform better than shallow ones. Although shallow detectors are less accurate, they are faster and use fewer resources, making their use in practice seem realistic (e.g., real-time detection in large social networks). Serving as benchmark, the focus of the first experiment is thus on detectors such as Naive Bayes combined with Bag-of-Words (BoW). The ACC of shallow detectors is considered as the dependent variable. From the related work, it can be deduced that the ACC should be influenced by the now explained independent variables (i.e., LLM type, resp. size, sampling strategy, resp. size, as well as temperature). The OpenAI GPT family has demonstrated that increasing the number of model parameters leads to significant improvements in language generation. Larger models, however, are particularly effective in generating longer texts due to their extended context windows. Given that the current scope is generating short tweets, varying the model size will answer the question of whether larger models also beat smaller ones when the generated output is short. Further, the influence of the sampling strategy on the LLM token selection process is examined. It is assumed that selecting the most probable token at each step (i.e. greedy search) should be the easiest to detect due to determination. Conversely, pure random sampling should be the most difficult to detect for machines. However, the texts generated in this way are unusable due to their low coherence and, hence, noticeable to humans. Consequently, these two sampling strategies only set the borders for the remaining ones, which are of true relevance. These either limit the number of high-probability tokens ð (top sampling), consider cumulative probability threshold ð (nucleus sampling), or are based on the conditional entropy of token sequences (typical sampling). Although it is unclear which of these strategies will perform best, previous comparisons by TheCuriousCase imply that nucleus sampling should be prior. However, it is unsure whether these results are generalizable to the present context, especially in combination with the other independent variables. From the possibility of statistical detection explained above, it can be deduced that the sample size should also have an effect in addition to the type of sampling. It is hypothesized that as the sample size increases, the representativeness is enhanced, and thus, the detectors accuracy is also improved. Conversely, the ACC decreases for smaller sample sizes. Last but not least is the influence of temperature, which adjusts the probability distribution for selecting the next word in the sequence to be predicted. Its definition implies that higher temperatures (ð > 1) lead to more unusual texts, which are more difficult for machines to detect but might be recognizable to humans due to incoherence. Conversely, low temperature (ð < 1) leads to high coherence but also determinability and is, therefore, easily recognizable by machines. Since these assumptions result from the definition of the parameter itself, moderating effect of temperature over the other factors is assumed. In other words, the effects mentioned should be additive. At least, no indications in the literature make the interaction of specific model-sample-temperature combination seem plausible. 3.1. Methodology The procedure involved several steps: dataset with real tweets was first filtered to ensure that only real tweets were involved (Sec. 3.1.1). This partial dataset was used to fine-tune various LLMs (Sec. 3.1.2). The dataset was then completed using LLM-generated synthetic fake tweets (Sec. 3.1.3), yielding from different hyperparameter combinations (Sec. 3.1.4). classifier was then trained with both real and fake tweets (Sec. 3.1.5). Using grid-based approach, the effect of the independent variables (i.e. hyperparameters) could be measured as result of the classifier evaluation (Sec. 3.2). The multi-step procedure requires the data to be split several times to provide the different models with unseen data, as depicted in Figure 1. Figure 1 Data Pipeline used for Modeling Note: comprehensive filtering policy was used (e.g., only tweets from verified users below the average amount of daily tweets; English language; no retweets or quoted tweets, etc.). Dataset yield as the basis for classifier and generator, respectively. Several models were tested (e.g., pre-trained GPT versions). 3.1.1. Dataset The dataset employed in this study comprises tweets collected between January and February 2020. Due to the noisy nature of the raw data, comprehensive set of filtering policies was implemented to refine the dataset. The following measures secure clean dataset in one language. In the first step, the dataset was restricted to tweets composed in English from authors with less than 100.000 followers. This measure excluded accounts from companies, sports teams, celebrities, etc., primarily used for advertising. Moreover, non-truncated tweets were selected to ensure the text was fully available, which is essential for training. To guarantee wide variety of content, follow-up tweets, replies, quotes, and retweets were discarded. Further, only tweets from users who sent less than 20 tweets per day made it into the dataset. After applying the mentioned filtering steps, the dataset contained approximately 2 million tweets from 136.450 verified accounts. These are set to be real tweets exclusively. For adding fake tweets to the partial dataset, it was split first for training various LLMs. The dataset was then completed using LLM-generated synthetical fake tweets. In the next step, the dataset was split up again in order to train classifier and check its ACC accordingly. This procedure is illustrated in Figure 1. 3.1.2. Generative Models Different open-source language models based on the GPT and OPT architecture are fine-tuned for later tweet generation. Particularly GPT-2 (radford2019language), GPT-J-6B (gpt-j), GPT-Neo-125M, GPT-Neo-1.3B, and GPT-Neo-2.7B (gpt-neo). Furthermore, several OPT variants (125M, 350M, 1.3B, and 2.7B), as referenced in the work of zhang2022opt. 3.1.3. Synthetic Tweets Using pre-defined sampling strategy and temperature setting, each model generated two sets of 10.000 synthetic tweets for training and evaluation of the detection models, respectively. Examples are provided in Table 1. 3.1.4. Parameter Grid The independent variables are hyperparameter combinations of parameter grid, including temperature, sampling scheme, and -size. Temperature is varied from ðððð = 0.8 to ðððð¥ = 1.4 in steps of 0.2 (the results show that this part of ð [0, 2] is sufficient, however, for the most analysis the borders were increased to ð ððð = 0.6, using steps of 0.1). Five sampling schemes (greedy search, typical-, top k-, nucleusand random sampling) with four sampling sizes (1k, 10k, 50k, and 100k) were used. The whole parameter grid is used for fine-tuning GPT-2 (1.5B) model and training the classifier, respectively. subsequent analysis is conducted to check if architecture and parameter size have an effect. The design is reduced to the most promising sampling scheme size combination (i.e., easily detectable combinations are excluded from the design). Hence, the outstanding combination was tested for branch of nine models with six different parameter sizes, all on the given temperature range. The used models differed in architectures (OPT vs. GPT) and parameter size, ranging from 125M to 6B. 3.1.5. Detector Naive Bayes classifier using BoW features was applied to detect synthetically generated tweets. This classifier has been chosen for its simplicity and short training time. grid-based approach was employed to train the classifier with various parameters (temperature, sampling strategy resp. size, model architecture resp. size) all of which were explained in detail above. 3.2. Results By definition, the temperature modification for LLM tweet generation led to modified word probability distributions accordingly. comparison of human against machine-based quantiles revealed notable differences for temperatures of ð 1. This characteristic is illustrated in Figure 2A, where the frequency distribution is less visible due to logarithmic scaling. Visualized by plotting the density distribution accordingly, deviations are clearly notable for density distributions resulting from ð 1 (v.v. less for ð = 1, cf. Fig. 2B). Those depicted differences make it easy to detect variations for statistical-based classifiers, such as Naive Bayes. Across models, sampling sizes, and temperature values, the ACC for different sampling strategies was maximal when using greedy search, as expected, and minimal when using random sampling. In the range of both strategies, nucleus sampling yielded the best results, followed by typicaland top sampling with similar results. For all sampling schemes, U-shaped result pattern emerged regarding increasing temperature values, as depicted in Figure 3B. The same pattern is visible for different sampling sizes, where larger samples did not only lead to higher detection rates but also to higher gradients for temperatures diverging from 1 (cf. Fig. 3A). Table 1 Example Tweets generated with different Models Model GPT Tweet Neo-125M Neo-1.3B Neo-2.7B J-6B Kobe says new coronavirus warning on plane is too difficult to understand. The new album is out now; make sure you have the album download code for free. #ValentinesDay: Today is the day to celebrate the greatness of yourself. And to. . . 1 \"This is how we play games!\" Lets hear \"The Box\" tonight with @OzzyOsbourne. . . OPT 125M 350M 1.3B 2.7B Im sure few will be added in future update as part of the \"Duke\" legacy. Good luck on the final stage of your tour! Rangers Henrik Lundqvist: \"Im not even thinking about the trade rumors, . . . \"1 very cold, chilly #day for #Lincoln and #Omaha #MorningWeather Note: Tweets generated with temperature ð = 1.0 and top-50 sampling. 1 Example trimmed due to excessive length. Figure 2 Human-based against machine-based Word Probability Distributions Note: Logarithmic scaling applied to quantiles on both axes. Machine-based quantiles result from GPT-2 (1.5B) model using random sampling with sampling size of 10k. A. Comparison of machine against human-based word probability distributions (red dashed line marks theoretical perfect mapping). B. Density distributions reflect the effect of temperature (red dashed line marks empirical human density distribution). Across temperatures, the different models did not show clear trend, neither in the model architecture nor in size. Particularly, both architectures lead to similar results, ðð´ð¶ð¶ 61.80% for both types. Using Spearmans ð showed rather small positive, then negative correlation between ACC and parameter size, Ëð = .14 for GPT and Ëð = .13 for OPT models (both not significant). This is visualized for different OPT (Fig. 3C) and GPT models (Fig. 3D), using nucleus sampling with ð = 0.95 and sample size of 10k. repetition of the experiment revealed that outliers (as shown here for OPT-2.7) were unsystematic. Notably, the same U-shaped pattern as described for sampling sizes and strategies is also visible across models: Centered at ð 1 where ACC is minimal, diverging temperatures lead to an increase in ACC. 3.3. Discussion Experiment 1 focused on attacking shallow learning detectors to obtain an ACC baseline measure. Therefore, synthetic tweets were generated with the manipulation of several independent variables. The ACC of these detectors was then tested using Naive Bayes classifier combined with BoW. When the temperature was set to ð = 1, the detection rate was minimal. Notably, there was no (curvi- )linear relationship between temperature and ACC but U-shaped one centered around ð 1. At this point, human and machine-generated word probability distributions were most similar. Vice versa, ð 1 led to easily distinguishable distributions and, hence, to higher detection rates. This effect increased the more ð diverged from its centers. Also, the effect became stronger when sampling sizes increased (v.v. less to not visible for small samples, e.g., 1k). Regarding sampling methods, the easiest to detect was greedy search since it works by always selecting the most probable next token, introducing very little entropy (i.e., max. ACC>99%). Comparably, the most difficult to detect was randomly sampling all possible next tokens, introducing more entropy and resulting in detection rates below 60%. However, for the purpose of coherence, using different sampling schemes is advisable. Here, nucleus sampling led to the best results (i.e., the lowest ACC). Model architecture and size, however, play minor role in detecting short texts (i.e., similar results across architectures and no clear trend of parameter size across models). While larger models beat smaller ones when the generated output is long, this effect is not true Figure 3 Detectionrates by Temperature for Sampling Sizes, Methods, and Generator Models Note: Comparison of ACC for varying temperatures ð and A. different sampling sizes (1k100k tweets) using random sampling and B. different sampling types, i.e., typical, top ð = 100, nucleus ð = 0.95 and pure random sampling, all with 10k sampling size. greedy search not depicted here, since it always leads to ð´ð¶ð¶ > 99%. Both sampling sizes and strategies result from the same GPT-2-1.5B model, whereas panels C. depicts results for OPT and D. for GPT model architectures with various parameter sizes. Sampling sizes and model parameters are reflected by color shading (i.e., the darker, the bigger). For all panels, small ACC values indicate better performance of the generating model. regarding short output text. One possible explanation of this is the fact that larger models can not display their strength regarding extended context windows when texts are already small in the first place. Taken all together, the results show, that shallow learning-based classifiers appeared to perform insufficient if the generative models produced texts with high entropy and similar word distribution to human texts. This can be traced back to their working principle since the BoW approach does not consider the position in which certain word is located nor synonyms with the same meaning. Shallow detectors are resource-sparing classifiers and, therefore, represent realistic application example (e.g., monitoring big data streams). At the same time, these detectors can be evaded easily, as demonstrated here. More advanced, resource-intensive classification models are therefore used in the field. Those models, such as those based on BERT ones, push the boundaries of the here presented strategies. Hence, alternative evasion techniques are needed for transformer-based detectors, as presented in Experiment 2. 4. Experiment 2: Evasion of Transformer-based Detectors Given their superior performance in text classification, transformer-based models are increasingly preferred for deployment in production environments. Hence, the previously applied evasion tactics become insufficient. This raises the question of whether more advanced classification models can also be evaded. To examine this question, the former described procedure was adopted accordingly on both sides by replacing the shallow classifier with transformer-based one and adjusting the evasion strategy as well. Unlike shallow learning and other deep learning algorithms, transformers make one major improvement: Their self-attention capability, formed by self-attention layers. These layers facilitate the mapping of tokens into vector space in relation to their surrounding tokens, resulting in more contextualized representations. Simple parameter tuning is insufficient to bypass these attention-equipped transformer models. Hence, it is vital to use more sophisticated methods. One of those might be reinforcement learning (RL). By using RL, model can be guided towards creating desired output. For example, it is possible to change the sentiment of text from negative to positive. Hence, the desired output can also be non-detectable text. RL is, therefore, used here to bypass transformers. However, reinforcement learnings unpredictable characteristic in finding strategies for reward maximization comes with the risk of the model learning introducing artifacts to bypass the classifier. To mitigate this risk, additional guardrails are applied. 4.1. Methodology The previously used procedure was adapted to evade transformer-based detectors. Firstly, the BoW text encodings were replaced by transformer-based classifier (Sec. 4.1.2). Secondly, to bypass this classifier, reinforcement learning was used (Sec. 4.1.3). To further stabilize the learning process, the RLs reward function was divided into a) the classical evasion reward (Sec. 4.1.4) and b) further constraints. The later are not only vital for stabilization, but also hinder the RL process to find undesirable evasion tactics (e.g., introducing artifacts such as extensive usage of special characters). The procedure was applied to second scenario, i.e., the generation of fake news, to demonstrate that this methodology can be adapted to other domains. In this regard, new dataset was used (Sec. 4.1.1). Training and testing procedures were similar and are therefore not again described. However, the linguistic refinement filters could be simplified to one single rule. In particular, this rule ensured that generated texts had different starting phrases, thereby maintaining some level of diversity in the output. 4.1.1. Dataset The used human dataset was the same as described in the previous Experiment (Sec. 3.1.1). To complement the dataset with synthetic tweets, top-50 sampling with temperature setting of ð = 1.0 was used. Additionally, the number of training samples for BERT classifier was fixed to 100k compared to the BoW classifiers. In order to assess the generalizability of the reinforcement learning approach, second iteration of text generation using the CNN/Daily Mail dataset from cnnDailyMailDataset was conducted. 4.1.2. Detector BERT was used as primary reference model within the transformer family. This is due to its architecture, which reassembles the basics of its predecessors, such as RoBERTa and DeBERTa. Given the considerable resources required to train an entire BERT model, fitting multiple models for the sake of hyperparameter optimization was discarded. 4.1.3. Reinforcement Learning The used hyperparameters, such as learning rate, mini-batch size, choice of the optimizer, and threshold for detecting linguistic acceptability, were based on literature recommendations. For the GPT-Neo2.7B model, the linguistic acceptability threshold was reduced from 0.4 to 0.3, as larger models are more prone to manual interventions. Furthermore, the Adam optimizer was substituted with the Lion optimizer (Chen2024optimizationAlgo), which has reportedly outperformed the former in certain scenarios. During both the reinforcement learning and evaluation phases, the same sampling method was used to ensure consistency in results. The general RL procedure followed the three distinct stages as described by vonwerra2022trl: rollout, evaluation and optimization. Since the predefined optimization algorithm was not changed, only rollout and evaluation stages are described below in more detail. Rollout The first stage, named rollout, entails the generation of synthetic tweets utilizing the language models described in Section 3.1.2. In this stage, the model is provided with the beginning tokens of an original tweet and is tasked with completing the sentence. Sometimes, the model generates entire tweets independently to mitigate the risk of overfitting short text fragments. Evaluation During evaluation, the generated texts are submitted to the BERT-based classifier described in Section 4. If the classifier recognizes the text as human-generated, the reinforcement learning algorithm receives positive reward; otherwise, it receives negative one. In this context, raw logits have been found to yield optimal performance. Optimization The final optimization stage entails the computation of the log probabilities of the tokens to compare the current language model with reference model. This step represents critical element within the reinforcement learning framework proposed by DBLP:journals/corr/abs-1909-08593-neu, ensuring that the modified model does not overfit its generation process. 4.1.4. Reward function In addition to the detector-based rewards, carefully handcrafted reward function is introduced to further guide the text generation process. This function penalizes generated texts that, while classified as human-like, fail to meet specific linguistic criteria. The reward calculation process is illustrated in Figure 4 and consists of various rulesets, all of which will be outlined in the following paragraphs. If one or more of these linguistic rules are violated, the most severe penalty of all individual rules is applied. Conversely, should the model generate synthetic text that satisfies all rules, the reward is equal to its evasion, as given by the detector model. The optimization rules were developed by analyzing preliminary training runs, during which both request and response logs from the reinforcement learning process were examined. While reinforcement learning can operate without these rules, the resulting outputs are significantly less coherent. For instance, an unguided model may generate an output such as \"Something for Administrator930 Macys Displays! RIP Family Members\". While the detector algorithm did not classify this text as machinegenerated, its core message is clearly questionable. Using the ruleset below, the restriction imposed by the linguistic acceptability rule would have prevented positive reward from being assigned to this text. The thresholds of the further introduced penalization rules are determined by observing the reinforcement learning logs. Looking at why training run failed helps to iteratively build rule by rule instead of knowing all the constraints right from the beginning. Another important factor for choosing Figure 4 Reinforcement Learning Reward Calculation Procedure the right thresholds is the type of social media text. For example, tweet might have more special characters and emojis than book or newspaper text; consequently, higher threshold is chosen for these measurements. For larger models, the reward associated with bypassing the detector algorithm can be multiplied by scalar to prioritize the models circumvention over producing grammatically or semantically refined sentences if necessary. Additional constraints Special characters. Texts containing more than 25% special characters are penalized through linearly decreasing negative reward, reaching the maximum penalty of -1 if the text consists entirely of special characters. Special characters include everything except Latin letters, numbers, and white spaces, while emojis are kept out of the calculation since they are treated in an extra rule. The number of 25% is backed up by the special character to all character ratio in the trainset of the generative model, as illustrated in Figure 5A. This ratio might vary with changing text types, such as newspapers or books. Repetitions. Besides hallucination, the repetition problem is well-known issue in natural language generation and is therefore already scientifically analyzed (repetitionProblem). The idea behind the repetition penalty is to prevent the model from adopting this undesired behavior during the reinforcement learning phase. text containing three or more instances of the same word is assigned negative reward of up to -1 when the token is repeated eight times or more. In order not to prevent natural text from being generated by this rule, the gold standard train corpus serves as comparison where repetitions of over two times are very uncommon, as illustrated in Figure 5B. Linguistic acceptability. Linguistic acceptability is evaluated using DeBERTa-v3-large classifier (he2021debertav3), which has been trained on the Corpus of Linguistic Acceptability (CoLA) dataset (CoLA). CoLA comprises sentences that have been labeled as either grammatically acceptable or unacceptable by human annotators. The trained model assesses the grammatical acceptability of given sentence, and if its score falls below the 40% threshold, negative reward is assigned. This reward again is linearly scaled and reaches value of -1 if the acceptability score drops down to 0%. For models with more than two billion parameters, the threshold is relaxed up to 30% due to the increased training difficulty associated with larger models. The thresholds used in this evaluation were determined empirically rather than sourced from existing literature. Higher thresholds are typically recommended to enhance linguistic acceptability. However, excessively strict thresholds can impede the reinforcement Figure 5 Ground-truth Distributions Note: Constraints based on ground truth visualized for A. the maximal proportion of special characters, B. the number of repetitions per tweet, C. the proportion of emojis and D. the number of emojis per tweet as well as E. the minimal proportion of tokens in standard dictionary. For all plots, the dashed line depicts the cutoff values. For all proportions (1st col.), the x-axis is log-scaled for visualization purposes, for all frequencies (2nd col.), the square root of the actual numbers is depicted on the y-axis. learning process, as texts may be persistently classified as non-human, preventing the model from receiving positive rewards and hindering learning. Dictionary. Besides introducing artifacts such as special characters, the RL process could also lead to words that are not part of any dictionary. Since this is not uncommon for tweets, it is important to give the model certain amount of freedom to introduce unknown words. However, this opportunity should not be used excessively. Therefore, the ratio between total words and unknown words of the original corpus is taken for comparison as illustrated in Figure 5E. Because it rarely happens that less than 25% of the words are part of dictionary, generation with lower score generates negative reward of up to -1 if none of the words are found in the dictionary. Word Emoji relationship. Emojis are common way to express emotions in social media. Thats why they appear so often in social networks like compared to newspapers or books. However, an overly excessive use of these expressions could also lead to text losing its message and being undesirable to read. Therefore, the ground truth train dataset is once again consulted to find the maximum amount of desirable emojis within one tweet, as demonstrated in Figure 5C. The threshold of giving negative reward is reached once tweet contains more emojis than words (50%). Number of Emojis. The aforementioned word-emoji relationship might not be sufficient for longer texts since, in this case, many emojis are possible. sentence of ten words could include ten emojis, which is bit much. This is why, additionally to the sentence length, four or more emojis are given negative reward. Such penalty also aligns with the natural distribution of emojis among tweets (Fig. 5D). Table 2 Example of the Reinforcement Learning Process"
        },
        {
            "title": "Response",
            "content": "FINAL <startoftext> <startoftext> Ozzy Osbourne cancel Today well know The best #adventure this <startoftext> UPDATE: Chancellor says he has not shown anxiety see you this weekend! well be here again cider is back in flush those lemons. This food aint good gig at studio theatre tonight over coronavirus 2020 forced soon morning if theres blood somewhere in year on 2020, dudes. Can we really Yeah its weird how local commentators Note: The RL process steps are illustrated line by line."
        },
        {
            "title": "Reward",
            "content": "0.8019 0.9674 0.9628 0.0089 0.1756 0.2899 0.9696 Repetition of the Query. Although not as common as repetitions within generated text, the repetition of the query is also phenomenon that has been observed during RL training analysis. To conquer this flaw, repeating more than half of the query yields negative reward. Special Tokens. The use of special tokens, such as the beginning-of-sentence (BOS) and end-ofsentence (EOS) markers used within transformer models, is limited to two per tweet. The presence of each additional special token results in negative reward of -0.4, with maximum penalty of -1. Same start. Output diversity is essential when the model generates tweets without an input query. negative reward is imposed if more than 10% of the tweets in training batch begin with the same word. This penalty increases linearly, reaching -1 if 20% of the tweets start similarly. Numbers at the start. To prevent the model from learning to exploit number-based patterns to bypass the classifier, penalty is given if generated tweets frequently start with numbers. If more than 10% of tweets within training batch exceed this limitation, the penalty is applied and scaled to maximum once the frequency exceeds 20%. Unknown characters. In some occasions, language models generate filler or unknown characters, typically caused by the occurrence of unknown characters included in the fine-tuning dataset. starting penalty of -0.5 is given upon the first occurrence and decreases further for each consecutive appearance to prevent this undesirable behavior. 4.1.5. Training Log To better illustrate the internal process, Table 2 provides sample of the ongoing process, including logs, documenting queries, responses, and rewards. As outlined in Section 4.1.3, positive reward is only assigned when both the query and response were classified as human-generated and none of the supporting rules produced negative feedback. In these cases, the reward relies solely on the BERT classifiers ability to detect generated content. 4.2. Results Across all evaluated BERT models, the mean ð¹1-score was quite high before the RL application, ð = 0.94 (ðð· = 0.01). For the RL application, empirical analysis proved learning rate of 5 105 and mini-batch size of 4, yielding the most optimal results. After this RL application, the mean detection rate decreased significantly to only ð = 0.09 (ðð· = 0.07). Further investigations regarding general applicability used the CNN/Daily Mail dataset. Despite the different domains, similar results were observed. Particularly, the detection rate decreased from ð¹1 = 96% to only ð¹1 = 17% after the RL application. Across applications, RL had significant effect Figure 6 ð¹1Score Comparison Note: Results are depicted for A. the used Twitter dataset and also for B. the CNN and Daily Mail dataset as representation of fake news. Dashed lines depict the mean ð¹1Scores across models and datasets before and after RL. Line distance illustrates the huge RL effect of ð = 16.48, 95%CI[24.14, 8.49], demonstrating transferability to different text domains. of ð = 16.48, 95%CI[24.14, 8.49]. According to Sawilowsky2009rulesofTumb, this effect can be interpreted as huge. All results are summarized in Figure 6. 4.3. Discussion Experiment 2 focused on transformer-based detection mechanisms. The 2.7 billion parameter model, with its reduced linguistic acceptability threshold, yields notably superior results, although at the cost of reduced linguistic quality. Particularly, for LLMs such as GPT2 and GPT-Neo models of different sizes (125M2.7B), all ð¹1-Scores exceeded 92%. This high detection rate can be explained by the enhanced capabilities in spotting textual patterns. Although transformer-based classifiers have demonstrated high reliability in distinguishing between real and machine-generated content, fine-tuned reinforcement learning approach can effectively bypass these robust models. The application of RL had huge effect, decreasing the mean detection rates by more than 16 standard deviations to only ð¹1 = 0.09. This proves RL to be reliable method for bypassing detection mechanisms. Furthermore, experiments adapted to the CNN/Daily Mail dataset demonstrate the applicability of this reinforcement learning approach to other text domains. Experiments with the four open-source models confirm that BERT classifiers can be bypassed using reinforcement learning-based training methodology. Despite the high detection rates, the potential for the BERT classifier to overfit to specific generation method remains, which could lead to suboptimal performance when applied to other generative models. 5. Experiment 3: Evasion of Zero-Shot-based Detectors Although the approaches of the first two experiments were successful, these attempts have several limitations. Primarily, the focus was solely on hiding the synthetic origin. Consequently, during RL, the model could change the content freely if it still made sense and was detected as human-written. Additionally, the computational cost increases complementary with the parameter decrease of the model to be adjusted. This makes it increasingly difficult to run the RL approach from hardware and stability perspective. Also, access to the original models is necessary to change them. Therefore, RL is limited to all open source models (i.e., not possible for, e.g., GPT 3 because the weights cannot be adjusted). In order to compensate for these disadvantages and to create general approach that is also valid for black-box models, an alternative procedure is proposed. This involves outsourcing the change to new translation model that preserves the meaning while masking the origin. One of the many advantages of transformers was the improvement of machine translation. The most desired goal of translations is preserving the content while remaining linguistically wellwritten. If transformers can be used to map from one language to another without changing the content (translation), it could also be possible to map from recognizable to unrecognizable with unchanged content. For transition to an experimental setting, maximum content similarity to the original and maximum unrecognizability are relevant. As with RL, unrecognizability is potentially in discrepancy with other influencing variables. While there is risk of artifacts with RL, it can be assumed that content and sentence quality suffer here. Keeping the unrecognizability constantly high could lead to models, i.e., translation with inappropriate synonyms that are not detectable by classifiers but sound strange to humans. However, if all three influencing variables specified above, including sentence quality, are taken into account, this should lead to sentences being translated (paraphrased) in such way that their LLM origin is no longer recognizable. 5.1. Methodology To compensate for the previous language limitations, new dataset consisting of LLM-answered questions was created (Sec. 5.1.1). The answers were paraphrased, and the results were filtered to obtain the highest coherence and similarity to the original answer while being less likely to originate from an LLM (Sec. 5.1.3). The thereby trained paraphrasing model (Sec. 5.1.5) was evaluated by using different dataset and compared to reference models (Sec. 5.2). 5.1.1. Dataset This experiment utilized the Human ChatGPT Comparison Corpus (ChatGPTvsHuman) as the primary dataset. HC3 contains over 24,000 entries, each consisting of question answered by both human and ChatGPT. Instead of relying on the pre-existing GPT responses, new ones were generated using the Qwen 1.5-4B model (qwen_2023). This was necessary for the subsequent step, where the same model was used to calculate log loss for permuted answers. The permutation candidates for training paraphrasing model were generated using the T5-3B model without fine-tuning. Optimal permutation candidates for each answer were selected based on three criteria: Similarity, coherence, and LLM origin plausibility (via its log loss), as depicted in Figure 7. The masking and filtering procedures are explained in detail below. Googles Natural Questions (NQ) corpus from the Benchmark for Question Answering Research (NaturalQuestionsGoogleBenchmark) was used for evaluation purposes. 5.1.2. Sampling and Masking For each question in the HC3 dataset, response was generated using the Qwen 1.5-4B model. These responses were then processed in the following manner: First, the answer was split into sentences and then tokenized using named-entity recognition (NER) to identify entities such as names, places, or numbers. In order to maintain the core message of sentence, these tokens were preserved from substitution. For the remaining tokens, with the exception of the final token in each sentence, all possible 2-tuples of token combinations were generated for masking purposes. Each sentence could include multiple masks, as long as the masked portion did not exceed 15% of the sentence. From all possible combinations for each sentence, ten were randomly drawn. Note that this number can also be chosen higher but was kept small in order to reduce the computational effort later on. The masked tokens were then filled using the T5 model (T5ParaphraseGenerator2020) by generating ten paraphrased sentences per masked combination. Duplicates were discarded, ensuring variability in the output. This approach follows methodology similar to the one described by mitchell_detectgpt_2023. Figure 7 Pipeline used for Trainingset Generation Note: Question from Human ChatGPT Comparison Corpus (ChatGPTvsHuman) answered by Qwen1.5-4B-Chat (qwen_2023) and permutated by t5-3b (2020t5). The Log loss (plausibility) is checked by the generative Qwen-model itself, while the similarity is checked using the sentence transformer all-MiniLM-L6-v2 (MiniLM). The acceptability was checked by DeBERTa (DeBERTa) model trained on the CoLA dataset (CoLA). 5.1.3. Filtering Criteria The filtering criteria were designed to select paraphrases that show the lowest likelihood of being generated by the corresponding large language model (LLM), consequently being difficult to detect as machine-generated. However, these paraphrases were also required to maintain high degree of similarity to the original text in order to prevent semantic drift (i.e., high similarity score is desirable). Furthermore, coherence was crucial factor in ensuring that the paraphrases remained linguistically correct. Sentences with high similarity but incoherent or linguistically incorrect structures may evade detection by LLM classifiers due to unusual word choices. Yet, they are clearly distinguishable from human responses, making them unsuitable for the intended use. Paraphrases not meeting the mentioned criteria were discarded, while the best of the remaining ones were chosen. Similarity. The assessment of semantic similarity was conducted using the all-MiniLM-L6-v2 model (MiniLM), which measures the cosine similarity between sentence vectors. This approach ensured that the selected paraphrases did not deviate excessively from the meaning of the original sentences, thus preserving the semantic content required for reliable training data. Coherence. Coherence or linguistic acceptability was evaluated through DeBERTa classifier (DeBERTa), trained on the Corpus of Linguistic Acceptability dataset (CoLA). This ensured that the paraphrased sentences retained grammatical correctness and avoided structural errors that might result in linguistically faulty outputs. LLM origin plausibility. The paraphrased sentences were evaluated using the model Qwen1.5-4BChat itself (qwen_2023), to calculate the log loss. The paraphrase with the highest log loss, indicating the lowest likelihood of being generated by the model, was selected. This criterion aimed to identify paraphrases that were most likely to evade detection by the LLM classifier. As shown in Table 3, the sentence: When you use Tor, your online activity is encrypted and cant be traced back to your real identity is paraphrased with the goal to identify the combination with the highest cross-entropy loss. 5.1.4. Filtering and Scoring Initially, all generated examples with cosine similarity of less than 0.9 were discarded. For coherence or linguistic acceptability, two rules were applied. If the original sentence had coherence score of 0.9 or higher, any paraphrased sentences scoring below this threshold were rejected. Likewise, if the original sentence had coherence score below 0.9, any paraphrases with delta greater than 0.05 compared to the original were discarded, allowing only minor linguistic changes. This second delta rule Table 3 Example of Trainset Generation."
        },
        {
            "title": "Paraphrased sentence",
            "content": "When [...] encrypted and [...] real location. When [...] encrypted and [...] computer. When [...] not visible to most websites and [...] true location. When [...] encrypted so that it [...] ISP. When [...] anonymous, and it [...] address. When [...] secure and [...] actual computer. When [...] secure and [...] true location. When [...] anonymous and [...] actual home address or computer. When [...] anonymous and [...] ISP. Cross-entropy loss 4.582982 4.613022 4.570329 4.608329 4.626064 4.691911 4.651963 4.637465 4.672797 Note: Original sentence: When you use Tor, your online activity is encrypted and cant be traced back to your real identity. is necessary because an original sentence may already have been written in bad linguistic style. So, every paraphrased sentence would have been discarded. The remaining samples were then sorted by log loss, with the highest log loss sample being selected for each original sentence. This process yielded training set comprising 24.300 paraphrases. 5.1.5. Paraphrasing Model The paraphrasing model is based on the T5-11B transformer (2020t5), trained for three epochs with learning rate of ð¼ = 104. During training, the prompt paraphrase: was added to guide the model in generating paraphrases. T5 stands for Text-to-Text Transfer Transformer, and this is the reason why the model has been favored over other transformer models and architectures. It takes task and the actual input as combined input and produces the result as an output without additional explanation, introduction, hints, etc. Therefore, the paraphrasing task is treated similarly to translation task, with the difference that the model is taught to introduce minor changes to the original sentences. Since the train data have been designed only to introduce small changes of about 15% of the tokens, the model can easily be applied recursively without removing too much of the actual meaning while being applied. This allows for the degree of change that should be applied to the original text to be adjusted. 5.1.6. Comparison Models In order to compare the conducted approach to existing ones in the scientific field, two methods by krishna_paraphrasing_2024 and sadasivan_can_2024 are used. To make the comparison as significant as possible, the same data are used for krishna_paraphrasing_2024. This is not feasible for sadasivan_can_2024 since their results have been human-rated, as explained below. Therefore, the results are converted for better comparison. Lexical diversity approach Dipper by krishna_paraphrasing_2024 uses fine-tuned T5-11B transformer model. In single paraphrasing step, the Dipper model takes two parameters, Lexical Diversity and Order Diversity. This means that instead of adjusting the degree of change by applying paraphrasing model recursively, Dipper uses the parameter Lexical Diversity. Furthermore, Dipper is designed as general-purpose paraphrasing model and not fine-tuned to hide specific model such as Qwen1.5-4b-chat (qwen_2023). Paraphrasing approach comparable approach was conducted by sadasivan_can_2024, who also used paraphrasing. Instead of training their own models for the hiding of the machine-generated text, the authors combined several pre-trained language models. Although optimized regarding detectability, the authors also report Figure 8 Results of the proposed Model vs. Reference Models Note: Comparison of the results achieved with A. the proposed model in comparison to B. the Discourse Paraphraser DIPPER by krishna_paraphrasing_2024 and C. the paraphrasing model by sadasivan_can_2024. sadasivan_can_2024 report values for grammar or text quality (comparable to linguistic acceptability) and content preservation (matched to similarity), both manually labeled on Likert scale of 1 to 5 (scaled here for better comparability but marked by dashed line representation). Those values are only provided for permutations 15 with detection rates, i.e., it is unclear at which permutation their best result occurred. measures regarding grammar or text quality (similar to linguistic acceptability) and content preservation (comparable with cosine similarity). Both were human-rated on Likert scale, thus making them less comparable. 5.2. Results Without the application of any hiding model, the baseline detection rate was 88.9% and the linguistic acceptability 74.1%. Cosine similarity was, of course, 100%. After applying the here proposed trained hiding model, all values decreased. The detection rate went down quite fast and reached 8.7% after 10 permutations. At the same time, both remaining variables were still quite high: Linguistic acceptability reached 65.5% while remaining 82.6% similarity to the original sentence. Both variables showed low variance across permutations, with mean values of ð = 68.2% (ðð· = 2.3%) regarding acceptability and ð = 89.0% (ðð· = 4.5%) for similarity. Starting with the same baseline values, the comparing model of krishna_paraphrasing_2024 leads to similar linguistic acceptability of 72.5%. The detection rate decreased to 15.4%, and the cosine similarity also went down, reaching 38.9%. Overall applications, mean acceptability was high with low variance ð = 72.6% (ðð· = 0.6%), contrarily to the similarity with ð = 76.4% (ðð· = 22.0%). Hence, the detection rate is more than twice as high as with the proposed model, which did not lead to significant decrease in similarity. Results reported by sadasivan_can_2024 are less comparable since different dataset was used. Additionally, the evaluation was done by human raters on Likert scale regarding grammar or text quality (similar to linguistic acceptability) and content preservation (comparable with cosine similarity). Rescaling their results leads to the following values after five permutations: Acceptability of 76.8% and similarity of 67.5% were both high, but the detection rate only decreased to 60.9%. Without reporting the number of permutations, the minimal detection rate is at 58.1%. As the number of paraphrasing iterations increases, the probability of the sentence being identified as machine-generated (i.e. classifier label = 0) decreases significantly. At the same time, the likelihood of it being linguistically acceptable remains relatively stable. Furthermore, the degree of semantic similarity also decreases only marginally. Table 4 Example of original Answer vs. Paraphrases to the Question: Who was Michelangelo? ð Response"
        },
        {
            "title": "1 Michelangelo was a very talented painter and sculptor that lived a long time ago. He was born",
            "content": "in Italy and he made many pictures and statues which are known and loved today. Some of his more famous works include the sculpture of David and the painting of the Sistine Chapel ceiling. 2 Michelangelo was very talented painter and sculptor that became popular long time ago. He was born in Italy , where he made many pictures and statues that are famous and loved today. Some of his more famous works include the sculpture of David and the painting of the Sistine Chapel. 3 Michelangelo was very talented painter and sculptor who lived long time ago. He was born in Italy, where he made many paintings and statues that are well known even today. Some of his best work includes the sculpture of David and the painting of the Sistine Chapel. 4 Michelangelo was very talented painter and sculptor who lived during the Renaissance. . He was born in Italy and he made many paintings and statues that are also well known even today. Some of his best work includes the sculpture of David and the paintings in the Sistine Chapel. cos 1.00 0.98 0. 0.95 0.94 Note: Original promt: Who was Michelangelo? Explain to me like child. The original answer given by Qwen1.5-4B-Chat is marked as ð = 0 and recursive iterations as ð¼ 1, exemplary for the first four out of ten. Colorization reflects qualitative change across applications: Light color highlights changes to the previous response, and dark color represents overtaken changes (i.e., changes compared to the original response). Deleted words or punctuation are xed out. Similarity is quantized as cosine similarly (cos). Both methods performance was compared by using ten iterations of the perturbation model and lexical diversity range of 0% to 100% for the Dipper model. The benchmarks were performed on the 1,000 questions sourced from Googles NQ corpus (NaturalQuestionsGoogleBenchmark). After six iterations of recursive paraphrasing, the detection rate dropped from 88.9% to 14.3%, while maintaining high cosine similarity of 88% (cf. Fig. 8). In contrast, achieving comparable result with the Dipper model required setting lexical diversity of 100%, which reduced the cosine similarity to 39%. These results demonstrate that fine-tuning paraphrasing model to evade detection for specific generative model can achieve superior performance compared to general-purpose paraphrasing model. of to the krishna_paraphrasing_2024 and sadasivan_can_2024 are lower detectability while preserving higher cosine similarity as well as constant linguistic acceptability. new approach Summarized, advantages the main conducted compared 5.3. Discussion was conducted evaluation The corpus (NaturalQuestionsGoogleBenchmark), as described in the related work. In contrast to the evaluation method proposed by mitchell_detectgpt_2023, this study employs perturbation technique that modifies the responses of language model with the objective of increasing log loss. In particular, paraphrasing model based on the T5-11B transformer is employed to generate sentences with increased log loss. This approach enables the generation of paraphrases that progressively deviate from the Questions Googles Natural using original sentence, increasing its distance from the language models typical output. Despite these modifications, the linguistic acceptability of the generated sentences is preserved. Experiment 3 proved that post-generation paraphrasing helps to avoid detection. This is particularly useful for bigger LLMs that are harder to fine-tune or for those that are inaccessible to the user. Although paraphrasing approaches have already been proven to be applicable (sadasivan_can_2024; krishna_paraphrasing_2024), it is demonstrated that the evasion results are even better if the is tailored to hide one specific model. The results outperformed those of paraphrasing model krishna_paraphrasing_2024 by adding the recursive paraphrasing feature. The mentioned approach also surpassed sadasivan_can_2024, who used recursive paraphrasing, too, but combined several pretrained paraphrasing models instead of fine-tuning them. Nevertheless, this comparison also has its limitations since krishna_paraphrasing_2024 used parameter for lexical diversity instead of recursively applying the model. Likewise, sadasivan_can_2024 used Likert scale instead of cosine similarity of sentence vectors to compare the permutations to the original output. 6. Conclusions and Outlook We discussed the improvement of LLMs in terms of reasoning performance, which will further complicate LLM detection in the future. In the literature review, it is noted that LLMs are no longer recognizable to humans. This makes machine-based LLM detection vital. In the experimental series, it is shown that all types of state-of-the-art classifiers can be circumvented with sufficient effort. In particular, shallow detectors (Exp. 1), transformer-based detectors (Exp. 2) and zero-shot-based detectors (Exp. 3) were attacked succesfully. For simple detectors, small adjustment of the generative LLMs proved to be sufficient, such as adjusting the temperature (parameter ð in Exp. 1). However, more sophisticated detectors, as used in practice, require better, more elaborate adjustment of the generative LLMs. During reinforcement learning, these models tend to acquire strategies that make them unrecognizable to detectors (e.g., inserting strings of special characters). While increasing evasion, these techniques tend to impede both the syntactical and semantical quality of the altered text. To counteract these model tendencies, two options were presented. Firstly, such behaviors were penalized during reinforcement learning by introducing additional constraints (cf. Exp. 2). Secondly, paraphrasing was employed, where the algorithm filtered results based on the highest similarity to the original answer, the best linguistic acceptance, and the lowest likelihood of being generated by language model (i.e., the lowest log loss, cf. Exp. 3). With an evasion rate of > 90%, the model trained on the resulting dataset performed not only better than comparable models, but its results were also most similar to the original response. In the experimental series, the language used on social media, including (formerly Twitter), was examined, and the results were generalized to internet language in general. Hence, for any current written indirect communication on the web, it is currently impossible to tell whether it is coming from human or machine (LLM) if the author took actions to hide the origin of the text. This everlasting battle between creating new detectors and crafting new evasion techniques will continue. Apart from the latest technical developments in this cat-and-mouse game, the question arises which implications non-detectable LLM communication will have. Of course, there are lot of potential as well as danger. Among these is systematic change of opinion, the spread of misinformation, or even fake news. It has been shown that people adapt their opinions to the majority, as can be seen, for example, by asch2016effects; asch1956studies in his still relevant conformity experiments. Although an apparently false opinion was propagated in these experiments, around third of the test subjects adapted their own opinion to that of the other test subjects (actors) versus less than 1% without conformity pressure (control). In this regard, transfer to social media is highly plausible: Users should hypothetically adapt their opinions to majority of bots (posting LLM-based content) as long as those bots share the same opinion uniformly. Further research is needed for concrete proof. However, if conformity effects can be transferred to social media, it might be possible to influence the beliefs of citizens consequently. This could have an extreme social impact, such as influencing elections. In contrast, range of advantages exists, resulting from LLMs. For example, the replacement of lowlevel work, such as answering frequently asked questions repetitively. However, this also bears risks. For example, shumailovCurseRecursionTraining2024 measured the keystrokes of the agents behind the Mechanical Turk. Too few keystrokes for too long texts indicate that they must have copied their answers from ChatGPT or comparable applications instead of writing them themselves. The problem behind this practice lies in the quality of the resulting answers. When answers are designed to be used for model training but originate from LLMs in the first place, the performance of new LLMs decreases. In the long run, feeding models with model output itself leads to systematic model destruction (shumailov2024ai). This is real problem that might become even more relevant in the future, especially if there is no way to classify input data as human originated reliably. Already now, over 1% of academic studies are said to be generated by LLMs or with its help (gray_chatgpt_2024). This makes the affected papers useless for long-term model training. Besides the problem for future model training, Gray also denotes potential effect on human behavior: The pure consumption of such texts might not only influence what we think in terms of facts, but also how we think to write correctly. That way, LLM text consumption might influence personal writing habits. To support informed decision-making in applications where large language models contribute to hybrid outputs, future research should prioritize the development and implementation of tamper-resistant watermarking techniques that are compatible with diverse text generation methods. These watermarking strategies allow for clear identification of machine-generated texts while preserving the semantic integrity of the content, but are yet to be deployed more commonly. Existing approaches range from lexical and syntactical permutations over logit-based generation patterns to watermarks anchored in the LLM training procedure (DBLP:journals/corr/abs-2409-00089; liu2024survey). From an ethical perspective, the adoption of watermarking aligns with principles of transparency, accountability, and trustworthiness. The widespread adoption of undetectable large language models introduces moral challenges, such as the risks of manipulation of public discourse and the dissemination of biased or wrong narratives. Addressing these challenges requires comprehensive approach to mitigation. Besides enhancing detectability via watermarking techniques, developing regulatory frameworks tailored to the ethical use of LLMs should be promoted. Further strategies may include promoting collaboration between developers, policymakers, and ethicists to establish industry-wide standards and guidelines for responsible LLM development. Finally, supporting campaigns to foster critical thinking skills among users to better evaluate the credibility of information, especially on high-volume platforms such as social media. To conclude, large language models and their generated content bring huge opportunities to the table, but this also makes their detection more and more difficult, up to the point that they become undetectable. Society must know this risk, try to mitigate it as well as possible, and find coping strategies in case detection becomes impossible. Acknowledgments. The authors would like to thank the System Sciences Chair for Communication Systems and Network Security, as well as the audience of the 57th Hawaii International Conference for their valuable discussions and feedback regarding the first version of this work (schneider2024HowWell). Special thanks also go to the CODE Research Institute for providing the hardware. Funding Statement. The authors acknowledge the financial support by the Federal Ministry of Education and Research of Germany in the program of SouverÃ¤n. Digital. Vernetzt.. Joint project 6G-life, project identification number: 16KISK002 Competing Interests. None. at Hugging Face: (ChatGPTvsHuman) Data Availability Statement. Datasets: The Human ChatGPT Comparison Corpus is https://huggingface.co/datasets/Hello-SimpleAI/HC3. The Natural Questions Corpus available (NaturalQuestionsGoogleBenchmark) of Googles Benchmark for Question Answering Research that was used for evaluation is also available at Hugging Face: https://huggingface.co/datasets/google-research-datasets/natural_questions. script for generating the CNN/Daily Mail dataset (cnnDailyMailDataset) is provided at GitHub: https://github.com/google-deepmind/rc-data. Note that the repository contains the script only and the CNN and Daily Mail articles itself have to be downloaded separately using the Wayback Machine. processed data set is provided by the NYU: https://cs.nyu.edu/kcho/DMQA/. The Twitter data set can not be shared because of Twitters restrictions regarding their privacy policies. Models: The used paraphrase generator with T5 (T5ParaphraseGenerator2020) is available at Zenodo: https://zenodo.org/records/10731518. The fine-tuning T5-XXL Discourse Paraphraser (krishna_paraphrasing_2024) can be downloaded at Hugging Face: https: //huggingface.co/kalpeshk2011/dipper-paraphraser-xxl. The pretrained language model Qwen 1.5-4B (qwen_2023) can also be downloaded at Hugging Face: https://huggingface.co/Qwen/Qwen1.5-4B. Environment for Replication Purposes. All experiments were realized using workstation equipped with four Nvidia A6000 Ada generation GPUs (48 GB VRAM each) and 512 GB of RAM. For replication purposes, lower-level machine (e.g., only one A6000 GPU) is sufficient but requires more data generation and training time. Especially for replication of the last experiment (Exp. 3), it is strongly recommended to use the same level machine. Otherwise, smaller model had to be chosen instead of the T5 XXL version. To train the T5 XXL model on multiple GPUs simultaneously, Microsofts DeepSpeed library was used. Ethical Standards. The research meets all ethical guidelines, including adherence to the legal requirements of the study country. Author Contributions. Funding acquisition: G.D.R.; Project administration: S.S.; Conceptualization: S.S.; Investigation: S.S., F.S., J.A.G.S.; Methodology: S.S.; Data curation & formal analysis: S.S.; Visualization: J.A.G.S.; Writing original draft: S.S., F.S., J.A.G.S.; Writing review & editing: S.S., F.S., J.A.G.S., G.D.R.; Supervision: G.D.R.; All authors approved the final submitted draft."
        }
    ],
    "affiliations": [
        "Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany"
    ]
}