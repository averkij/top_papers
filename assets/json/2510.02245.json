{
    "paper_title": "ExGRPO: Learning to Reason from Experience",
    "authors": [
        "Runzhe Zhan",
        "Yafu Li",
        "Zhi Wang",
        "Xiaoye Qu",
        "Dongrui Liu",
        "Jing Shao",
        "Derek F. Wong",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR."
        },
        {
            "title": "Start",
            "content": "Preprint. EXGRPO: LEARNING TO REASON FROM EXPERIENCE Runzhe Zhan12 Yafu Li2(cid:66) Zhi Wang3 Xiaoye Qu2 Dongrui Liu2 Derek F. Wong1(cid:66) Yu Cheng4 1University of Macau 4The Chinese University of Hong Kong 2Shanghai AI Laboratory 3Nanjing University Jing Shao 5 2 0 2 2 ] . [ 1 5 4 2 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), framework that organizes and prioritizes valuable experiences, and employs mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as key ingredient for efficient and scalable RLVR. Code: ExGRPO Models: ExGRPO Models"
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning (RL) has become pivotal technique for advancing the reasoning capabilities of language models on complex tasks (Guo et al., 2025; Yang et al., 2025). RL augments language models reasoning by modeling chain-of-thought (CoT) as action sequences optimized under verifiable rewards (RLVR), laying the groundwork for sophisticated downstream applications (Gridach et al., 2025). However, significant yet overlooked challenge persists: due to the on-policy nature of most RLVR algorithms, the valuable experience generated during the rollout phase is often discarded after single gradient update (Shao et al., 2024). This practice not only squanders substantial computational resources but also forfeits critical opportunities for the model to learn from prior successful explorations, thereby imposing bottleneck on scaling RL for reasoning. Experience replay (Lin, 1992) is widely adopted technique in RL to address this issue and improve sample efficiency. As stated in Silver & Sutton (2025), AI is at the cusp of new period in which experience will become the dominant medium of improvement. This idea, often referred to as experience-based RL, leverages the intuition that previously collected interactions (i.e., state-actionreward tuples) contain valuable information for learning, helping models stabilize training and accelerate convergence (Mnih et al., 2013). non-trivial challenge lies in how to exploit past experiences based on their differing values (Schaul et al., 2016) and manage the replay process according to customized learning schedules (Sujit et al., 2023). However, efficient experience replay mechanisms remain largely underexplored in RLVR for building large reasoning models (LRMs; Plaat et al. 2024; Zhang et al. 2025b; Qu et al. 2025). Given the vast quantity of experiences collected during rollouts, fundamental question remains: How can the reasoning model effectively exploit its own stream of experience to maximize learning toward scaling RL compute for LRMs? Work was done during Runzhe Zhans internship at Shanghai AI Laboratory. (cid:66) Corresponding authors. 1 Preprint. To address this gap, we begin by examining what constitutes valuable reasoning experience for RLVR optimization1. We hypothesize that experience utility varies with measurable properties. RLVR experience generally consists of question and its corresponding trajectories. Accordingly, we study experience properties from these two components. Through systematic analysis, we identify rollout correctness (for questions) and trajectory entropy (for trajectories) as effective online proxy metrics for characterizing experience quality. Specifically, tasks of intermediate difficulty and their associated low-entropy trajectories tend to be beneficial for RLVR optimization. Building on these insights, we introduce Experiential Group Relative Policy Optimization (ExGRPO), novel framework designed to strategically identify, manage, and replay valuable experiences. ExGRPO maintains replay buffer of reasoning trajectories derived from partially correct rollouts and organizes them into buckets according to their correctness levels. To manage the buffer effectively, it uses sampling strategy that prioritizes experiences from the most beneficial buckets, along with the corresponding trajectory with the lowest entropy. This approach allows the model to learn more efficiently from past experiences that align best with its current capabilities, guided by the principles of valuable experience concluded in our preliminary analysis. During mini-batch optimization, ExGRPO uses mixed-policy optimization objective that balances between leveraging fresh exploration and reusing strategically selected past experiences, improving both sample efficiency and training stability. Our experimental results demonstrate that ExGRPO delivers improvements over on-policy RLVR baselines. We evaluate across five backbone models, spanning the Qwen (Yang et al., 2024; Qwen et al., 2024) and Llama (Grattafiori et al., 2024) families from 1.5B to 8B parameters, on both mathematical reasoning benchmarks (AIME24/25, AMC, OlympiadBench, Minerva, MATH500) and out-of-distribution reasoning benchmarks (ARC-c, GPQA, MMLU-Pro). Averaged over all backbone models, ExGRPO achieves +3.5 and +7.6 point gains2 on in-distribution and out-of-distribution benchmark performance, respectively, compared to the on-policy RLVR. Notably, ExGRPO stabilizes RLVR training on the weaker Llama-3.1 8B model (Grattafiori et al., 2024) and continual learning on the stronger LUFFY model (Yan et al., 2025), where on-policy optimization collapses. Detailed analysis and ablations further confirm that improvements stem from ExGRPOs experience management and optimization mechanisms, which amplify the utility of past explorations."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reinforcement Learning with Verifiable Rewards. Reinforcement Learning with Verifiable Rewards (RLVR; Lambert et al. 2024) frames the language model as policy that generates reasoning trajectories for verifiable tasks, with rewards provided by rule-based (Guo et al., 2025; Yu et al., 2025) or model-based verifiers (Su et al., 2025; Ma et al., 2025b; Chen et al., 2025). Most RLVR methods adopt on-policy optimization (Schulman et al., 2017; Shao et al., 2024), which ensures stability but incurs high computational cost. Recent work has explored off-policy techniques (Kallus & Uehara, 2020; Meng et al., 2023) that incorporate historical or external data. Some approaches mix expert demonstrations with on-policy updates, e.g., off-policy policy gradients (Yan et al., 2025), SFT loss (Zhang et al., 2025c; Ma et al., 2025a), or knowledge distillation (Xu et al., 2025); others develop direct off-policy update rules for improved sample efficiency (Cohen et al., 2025; Roux et al., 2025; Arnal et al., 2025). However, they overlook the impact of data quality within the experience replay buffer, factor that remains underexplored in RLVR and is the central focus of our work. Experience-based Reinforcement Learning. Experience replay (Lin, 1992) is classical RL technique to improve sample efficiency and stabilize training, later extended with prioritized replay (Schaul et al., 2016; Sujit et al., 2023) to emphasize informative transitions, enabling breakthroughs in control tasks (Mnih et al., 2013; 2015; Lillicrap et al., 2016; Zha et al., 2019). For LRMs, recent studies show that replaying successful trajectories accelerates convergence and improves reasoning capabilities. ReMix (Liang et al., 2025) leverages off-policy experience replay to improve training efficiency; RePO adopts online replay (Li et al., 2025), collecting early on-policy rollouts and revisiting them asynchronously; RLEP (Zhang et al., 2025a) reuses trajectories from well-trained policy; and RRL (Dou et al., 2025) dynamically revisits promising early states; ARPO (Lu et al., 2025) extends this idea to GUI agents, combining GRPO with replay buffer to enhance stability and sample efficiency. Among them, the last three methods overlook importance weighting to correct 1In the context of RLVR, the experience refers to state-action-reward trajectory during rollout of the reasoning chain. We will use the terms experience/trajectory/rollout interchangeably throughout the paper. 2Due to space limitations, numerical results of some backbone models are reported in Section E.3. 2 Preprint. distribution mismatch in off-policy updates. In this work, we argue that not all stored experiences are equally valuable in zero RLVR for LRMs. We systematically analyze properties that determine the value of past experiences and design methods to better leverage high-value experiences, thereby improving efficiency and performance."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "In this section, we briefly review the RL foundations underlying our method: RLVR and Group Relative Policy Optimization (GRPO), upon which ExGRPO is built. We then present preliminary analysis of experience data, i.e., the models past successful rollouts, and examine how their characteristics influence performance."
        },
        {
            "title": "3.1 REINFORCEMENT LEARNING WITH VERIFIABLE REWARD",
            "content": "Verifiable Reward Function. The verifiable reward compares the extracted answer from the models output with predefined golden answer. For example, the model is instructed to output the final answer in format such as boxed{}, from which verifier extracts the value. Formally, given model output for question q, the reward is: r(q, o) = (cid:26)1 0 if contains the correct final answer to otherwise. (1) Group Relative Policy Optimization (GRPO). GRPO (Shao et al., 2024) is strong baseline within the RLVR paradigm (Guo et al., 2025; Zeng et al., 2025; Liu et al., 2025), achieving effective scaling without requiring an additional value model. It estimates the advantage of each trajectory by normalizing rewards within group of sampled solutions. Given group of trajectories Gq = {oi}K i=1 using the current reference policy πθold (i.e, rollout policy), GRPO estimates the advantage of each trajectory oi by normalizing its reward against the empirical statistics of the group: (cid:98)Ai = r(q, oi) µGq σGq (2) where µGq = 1 Gq. The on-policy objective maximizes clipped surrogate function: j=1 r(q, oj) and σGq are the mean and standard deviation of rewards in the group (cid:80)K JGRPO(θ) = EqD,{oi}πθold (q) (cid:34) 1 (cid:88) i=1 (cid:35) CLIP(wi(θ), (cid:98)Ai) (3) set the (cid:80)o query training where is = 1 t=1 min (wtA, clip(wt, 1 ε, 1 + ε)A). Since the trajectory is generated by the rollout policy model before update, i.e., πθold , the per-token importance weight wi,t(θ) is the probability ratio between the current policy πθ and reference policy πθold : wi,t(θ) = πθ(oi,tq, oi,<t)/πθold(oi,tq, oi,<t). Following Dr.GRPO (Liu et al., 2025), we remove the length normalization and standard deviation normalization of GRPO loss (Eqs. 2 and 3). term is CLIP(w, A) loss and the 3.2 PRELIMINARY STUDY ON EXPERIENCE DATA To motivate our experience management design, identifying which questions and trajectories are most valuable for experience-based learning, we begin by analyzing rollouts from on-policy RLVR. Our study is guided by two questions: (1) Are all questions equally useful for training, or do certain difficulty levels provide stronger learning signals? and (2) Does low entropy correlate with higher-quality trajectories that the model should prioritize? Setup. We conduct experiments using Qwen2.5-Math 7B (Yang et al., 2024) backbone model, trained with vanilla on-policy RLVR following the Dr.GRPO setup on the OpenR1-Math dataset (Face, 2025). Implementation details and hyperparameters are provided in Section 5.1. The experimental setting is as follows: we train three models, each using training batches restricted to questions of 3 Preprint. particular difficulty level, determined by rollout correctness of each question. During training, we collect the generated trajectories and analyze their characteristics, such as entropy and the quality (validity) of the underlying reasoning chains. The entropy refers to the average action entropy: H(o) = 1 π(ot q, o<t) log π(ot q, o<t), indicating the uncertainty of the model generating each action (token). Under this setting, three models are trained with comparable number of samples, and detailed statistics are provided in Appendix F.1. (cid:80) (a) Test Performance. (b) Reasoning Chain Validity. (c) Entropy Distribution. Figure 1: Analysis of question difficulty and entropy in on-policy RLVR training: (a) Test performance of models trained on different question groups; (b) Entropy distributions of logically correct trajectories across question groups; (c) Entropy comparison between correct and incorrect trajectories. Are all questions equally useful for training? During training, we categorize each question into one of three difficulty buckets based on its online (rollout) correctness rate: Easy [75%, 100%), Medium (25%, 75%], and Hard (0, 25%]. This classification is computed online without additional decoding or offline annotation. We implement bucket-specific experiments by restricting each batch to questions from single difficulty group, producing three models, along with one trained on the full set. Results in Figure 1a show that training on Medium questions yields the largest performance gains. In contrast, models trained exclusively on other questions perform substantially worse than the full-data baseline, yet they still provide complementary signals and should not be entirely discarded. Does lower entropy imply better reasoning? We then test which metric can serve as an online proxy for reasoning quality. While outcome-based rewards check the final answer, they do not capture whether the reasoning CoT is logically correct or valid. To establish reference, we follow previous work (Wen et al., 2025) to employ an LRM (Qwen3-32B; Yang et al. 2025) as an external judge, which inspects correct outputs and labels their reasoning validity (See Section F.2). As shown in Figure 1b, correct reasoning trajectories exhibit lower entropy than incorrect ones, indicating that selecting the lowest-entropy candidate generally yields higher CoT quality. We further examine the entropy distribution of correct trajectories across the three question groups in Figure 1c. Correct trajectories in the Medium group concentrate at lower entropy values, while Easy and Hard groups display broader distributions, with Hard problems peaking at higher entropy. This observation partly explains the marginal advantage of training on Medium-level questions compared to the other groups. In summary, we highlight two guidelines: Medium-difficulty questions provide the most valuable optimization signals, and entropy minimization is an effective heuristic for trajectory selection. Low-entropy trajectories mitigate misguided learning. Prior work has shown that the singular pursuit of minimal entropy in RLVR can lead to entropy collapse (Cui et al., 2025b), thus motivating the need for nuanced explorationexploitation trade-off in the context of experience management. While high-entropy policies encourage exploration, they can yield low-quality trajectories, many successes are merely lucky hits from incorrect reasoning chains (Wen et al., 2025). Repeatedly sampling such misguided experiences from the replay buffer contaminates training, leading to systematic reasoning errors, we refer to snowball effect. For instance, as illustrated in Section F.4, we observe models using invalid code blocks for math problems, traced to high-entropy trajectories in the buffer. Thus, entropy minimization should be guiding principle for experience selection, especially early in training when stability matters most. However, balancing entropy reduction with exploration remains non-trivial challenge, which we will detail in the methodology. 4 Preprint. Figure 2: Overview of Experiential Group Relative Policy Optimization (ExGRPO). ExGRPO operates in two phases: (a) Experience Management and (b) Policy Optimization (cf. Algorithm 1)."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "Based on previous findings, we propose Experiential Group Relative Policy Optimization (ExGRPO) as shown in Figure 2, method that leverages structured experience replay with entropy-based trajectory selection to enhance sample efficiency while preserving policy stability. 4.1 EXGRPO: EXPERIENCE MANAGEMENT Problem Formulation. Given dataset and reward model r(), ExGRPO iteratively updates (cid:2)r(q, o)(cid:3) and experience replay policy model πθ by combining on-policy rollouts EqD,oπθold (q) (cid:2)r(q, o)(cid:3). The πθold is the rollout policy model before gradient updates, while EqE,oπθpast (q) πθpast is the previous one which generates the corresponding experiential trajectories. The buffer is maintained as key-value structure (cid:55) {o}, where each question is associated with set of candidate trajectories. We next describe the three-stage experience management pipeline. Experience Collection. During training, we collect the models successful trajectories for each question in the batch. For given question sampled from the dataset, the rollout policy πθold generates trajectories, denoted as i=1. We then compute the correctness rate Acc(q) = k/K, where is the number of successful trajectories verified by the reward model. All successful rollouts are stored in the replay buffer E, recorded as mapping (cid:55) {o}. The associated correctness rate is saved together with the query and later used for experience partition. = {o }K Experience Partition. The replay buffer is partitioned into buckets by each questions latest correctness rate Acc(q), with as the indexing key. This bucketing mechanism categorizes experiences by difficulty. To prevent overfitting on trivial cases, we introduce Retired Set: questions solved in all rollouts are removed from E, ensuring optimization focuses on partially solved questions. Experience Selection. During optimization, ExGRPO retrieves mini-batch of experiential samples from partitioned buffer in two steps: (1) Question Sampling. To prioritize experiences described in Section 3.2, each bucket is sampled with probability (Acc(q); µ, σ = 1), where µ is set to 0.5 in practice. This ensures that medium-difficulty questions are sampled more often than trivially easy or mostly failed ones, allowing biases sampling towards the sweet spot questions. (2) Trajectory Selection. For each sampled question, we select the trajectory with the lowest entropy under the current policy, i.e., arg minoi{o} H(oi; πθ), thereby prioritizing trajectories which indicate better CoT quality as discussed in Section 3.2. After three-stage management, selected experiences are used for mixed-policy optimization. Preprint."
        },
        {
            "title": "4.2 EXGRPO: EXPERIENTIAL POLICY OPTIMIZATION",
            "content": "The core idea of ExGRPO optimization is to unify on-policy exploration with off-policy replay under joint objective, while correcting the distribution shift. At each step, mini-batch is constructed from: (1) on-policy samples Bon from dataset D, and (2) experiential samples Bexp from buffer E. hyperparameter ρ [0, 1) determines the experiential proportion, with Bexp = min(ρB, E) and Bon = Bexp. This ensures lower bound in cases where the experience replay buffer is depleted. The overall ExGRPO objective is combination of two components: The on-policy objective Jon(θ) follows GRPO in Eq. 3: for each query Bon, trajectories {oi}K i=1 are sampled from policy πθold to form advantage group Gq, with objective JGRPO(q, Gq; θ). For the experiential off-policy objective, given Bexp, we form mixed advantage estimation group Gq = {o} {oi}K1 i=1 , where is replayed trajectory from past policy πθpast and {oi} are new rollouts from current reference policy πθold. To obtain an unbiased policy gradient estimate q,o <t) from the replayed trajectory, is reweighted by <t) . All trajectories in Gq use q,o GRPO advantage estimates (cid:98)A in Eq. 2. Finally, the unified objective is: (θ) = πθ(o πθpast (o JExGRPO(θ) = (1 ρ) EqBon (cid:34) 1 K (cid:88) CLIP(wi(θ), (cid:98)A(oi, Gq)) i=1 (cid:124) (cid:123)(cid:122) Expansion of On-policy objective JGRPO(q,Gq;θ) (cid:35) (cid:125) + ρ EqBexp (cid:34) 1 (cid:124) (cid:32) CLIP(w(θ), (cid:98)A(o, Gq )) + K1 (cid:88) (cid:123)(cid:122) Experiential Off-Policy objective i=1 CLIP(wi(θ), (cid:98)A(oi, Gq )) (cid:33)(cid:35) (cid:125) (4) Directly optimizing on replayed trajectories, which are often selected for their low entropy, can hurt the exploration in the RLVR. We introduce two mechanisms to ensure stable and effective learning: Policy Shaping for Entropy Preservation. Inspired by Yan et al. (2025), we modulate the gradient from experiential data using policy shaping. We replace CLIP term of importance sampling for in Eq. 4 with (w(θ)) (cid:98)A(o, Gq ), where (x) = x+β is non-linear transformation with small constant β = 0.1. This amplifies low-probability signals and dampens high-probability ones within the replayed trajectory, encouraging the model to learn from its more novel aspects from experience. Delayed Start Mechanism. To mitigate the collection of low-quality trajectories during the initial training stages when the models capabilities are still developing, we employ delayed start. The on-policy RLVR process runs first, and ExGRPO is activated only after the Pass@1 metric for training batch surpasses predefined threshold, ensuring experiential samples are of higher quality."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "To assess the effectiveness of ExGRPO, we explore the following key questions: (1) How does ExGRPO perform under different training scenarios? (2) Can ExGRPO generalize across models with different capacities? (3) Can ExGRPO generalize beyond reasoning-oriented training data? 5.1 EXPERIMENTAL SETUP Data and Evaluation. We train on the OpenR1-Math 45k subset (Face, 2025; Yan et al., 2025) and evaluate on nine challenging reasoning benchmarks. Six are in-distribution math benchmarks: AIME 2024/2025, AMC (Li et al., 2024), MATH-500 (Hendrycks et al., 2021), Minerva (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). To assess generalization, we include three out-of-distribution tasks: ARC-c (Clark et al., 2018), GPQA-Diamond (GPQA; Rein et al. 2024), and MMLU-Pro (Wang et al., 2024). For AIME, AMC benchmarks with limited number of samples, we report Avg@32 over 32 independent runs; for others, we use Pass@1 metric, representing the 6 Preprint. Table 1: Overall in-distribution and out-of-distribution performance based on Qwen2.5-Math-7B. Bold and underline indicate the best and second-best results within comparable group, respectively. Model Qwen-Base Qwen-Instruct PRIME-Zero Oat-Zero GPG-Zero RePO-Zero On-Policy ExGRPO SFT SFT+RL LUFFY In-Distribution Performance Out-of-Distribution Performance AIME24 AIME25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. 11.5 12.5 17.0 33.4 29.8 19.8 24.9 31. 22.2 25.8 29.4 4.9 10.2 31.3 48.5 43.6 80.4 7.4 32. 15.6 41.0 19.0 37.6 18.2 70.3 11.1 24.7 Previous Zero RLVR Methods 12.8 11.9 12.1 10. 54.0 61.2 67.8 54.0 81.4 78.0 80.8 76.8 39.0 34.6 30.9 34.2 40.3 43.4 44.7 40.1 40.7 43.7 44.4 39.2 73.3 70.1 70.3 73. 18.2 23.7 40.4 24.2 Zero RLVR with ExGRPO 15.5 18.7 59.2 66.3 84.8 87.4 38.2 36. 49.3 50.1 45.3 48.3 82.6 84.7 37.4 37.4 Off-policy Learning Methods 22.3 23. 52.8 62.7 82.6 87.2 40.8 39.7 43.7 50.4 44.1 48.2 75.2 72. 24.7 24.2 Continual RLVR with ExGRPO 23.1 22.5 17.8 25.7 65.6 66.2 67.5 65. 87.6 86.8 88.4 87.6 37.5 41.2 38.6 40.1 57.2 55.3 55.3 57. 50.1 80.5 50.4 48.7 51.4 81.8 81.9 83.6 39.9 49.0 47.0 42. 16.9 34.1 32.7 41.7 50.5 42.5 49.2 52.9 42.7 37.7 53.0 54.7 53.3 54. 15.4 43.0 41.4 45.2 41.6 46.8 56.4 58.3 47.5 44.8 57.8 61.8 60.7 60. (cid:44) Continual LUFFY 30.7 (cid:44) On-Policy 24.8 (cid:44) ExGRPO 32.3 proportion of solved problems on single attempt. All evaluations use sampling temperature of 0.6 and top-p of 1.0, with answers extracted via the Oat-evaluator toolkit (Liu et al., 2025). RLVR Setups. We use rollout batch size of 128 and an update batch size of 64. During the rollout stage, we collect 8 sampled responses for on-policy questions and 7 for questions drawn from the replay buffer. The ratio ρ of experience data in each mini-batch is set to 50%. For most models, the ExGRPO algorithm is activated only after the batch Pass@1 exceeds 35%. An exception is the Llama model, where this criterion is not applied due to the collapse of on-policy RLVR. Our experiments were conducted using the verl framework3 on either 8H100, with Math-Verify4 as the reward model. More detailed training and validation settings are provided in Section E.1. Models and Baselines. Our primary testbed for zero RLVR is the Qwen2.5-Math 7B model, and we also extend our evaluation to its 1.5B variant and other models, including Llama-3.1 8B (Base and Instruct) and Qwen2.5 7B Instruct models. Details of all baselines can be found in Section E.2. 5.2 MAIN RESULTS ExGRPO surpasses on-policy baselines on complex reasoning benchmarks. Our main results in Table 1 show that ExGRPO consistently outperforms on-policy RLVR baselines across multiple benchmarks. When applied to the Qwen2.5-Math 7B model, ExGRPO delivers +3.0 point gain over on-policy RLVR on math reasoning tasks. This advantage becomes more pronounced on challenging datasets such as AIME24/25, highlighting its effectiveness on difficult reasoning problems, and also holds on out-of-distribution benchmarks. detailed comparison with the relevant replay-enhanced method in Section E.4 further shows that ExGRPO outperforms vanilla experiential RL baselines. ExGRPO is robust across diverse model architectures and initializations. We validate that the benefits of ExGRPO are not limited to single model in Figure 3 and Appendix Table 3. ExGRPO 3https://github.com/volcengine/verl/ 4https://github.com/huggingface/Math-Verify 7 Preprint. Figure 3: comparison of benchmark performance for different backbone models and training variants, showing performance on both in-distribution and out-of-distribution tasks (cf. Section E.3). has consistent improvements over baselines when applied to models of varying scales, including 1.5B and 8B parameter models, as well as models initialized from instruction-tuned checkpoints. For these models, ExGRPO achieves in-distribution improvements of up to +5.3 points on Qwen2.5-Math 1.5B Base and +4.1 points on Qwen2.5-7B Instruct. Notably, ExGRPO addresses critical instability issue observed in on-policy RLVR, enabling successful training of Llama-3.1 base model on our data where the on-policy baseline method collapses. In further experiment using the LUFFY model, which was trained with external off-policy data, we find that continual RLVR with model own experience yields greater performance gains than relying on external data. However, applying on-policy training to LUFFY still resulted in degradation, highlighting the advantage of ExGRPO. We observe that ExGRPO underperforms on certain out-of-distribution tasks under continual RLVR setting, which we attribute to reduced on-policy exposure during joint mini-batch optimization with experience replay."
        },
        {
            "title": "6 ANALYSIS AND DISCUSSION",
            "content": "To understand the effectiveness of ExGRPO, we investigate the following research questions: (1) How does experience replay influence training dynamics, especially for different models? (2) What is the impact of experience selection on overcoming performance bottlenecks? (3) How do the components of ExGRPO, e.g., experience management, contribute to overall performance? Figure 4: Learning dynamics of On-Policy vs. ExGRPO during training Llama-3.1 8B. ExGRPO stabilizes training and achieves higher rewards, while on-policy suffers from training collapse. Experience replay helps stabilize training for weaker models. Figure 4 depicts the training dynamics of the Llama3.1-8B base model. Due to its limited capacity, this model struggles to solve most problems, resulting in low rewards under on-policy training. Consequently, the exploration signal collapses, and the model finally encountered entropy explosion on challenging training data. In contrast, experience replay allows the model to exploit the lucky hits of correct solutions encountered early on. By replaying these successes, the model receives meaningful reward signals and is guided toward an exploration trajectory that matches its current capability, thereby stabilizing learning and preventing premature collapse. 8 Preprint. Experience replay improves data efficiency. For stronger model, Figure 5 shows the evolution of the replay buffer and retired set. Their sum reflects the number of examples on which the model eventually succeeds. By the later stages, about half of the dataset achieves at least one successful trajectory (Pass@8), validating the promise of experiential learning: the model retains strong reasoning potential, and revisiting past successes enhances data utilization. The retired set further enhances efficiency: although initially small, it grows rapidly as capability improves and eventually matches the buffer in size. Retiring solved problems shifts training toward harder ones, reducing redundancy and explaining the slower buffer growth in later phases. Figure 5: Dynamics of experience replay buffer and retried set. The efficiency of experience utilization, rather than its sheer volume, is the primary driver of model performance. Figure 6 illustrates the dynamics of the experience buffer and the retired set under different data conditions. When trained on the same amount of data, the absence of question selection mechanisms (e.g., ExGRPO vs. w/o Ques. Sel.) leads to growth of the retired set and corresponding reduction in buffer size (primarily due to repeated exposure to easy questions). As result, experience replay becomes inefficient, with high consumption of samples but low return in terms of performance gains. We further examine the scaling effects of experience replay by varying the experience proportion ρ in batch optimization. An excessively high replay ratio (ρ = 75%) traps the model in experience exploitation. This stifles exploration, leading to smaller experience buffer and reduced retired set, and ultimately causing worse performance even below the on-policy baseline. Conversely, at ρ = 25%, training is driven primarily by fresh on-policy learning, with replay serving as complementary signal. Although this setting enlarges the buffer, the retired set grows only marginally, suggesting that additional experiences do not necessarily translate into better capabilities. These observations demonstrate that the utility of experience replay depends not on maximizing the buffer size, but on balancing exploration with efficient reuse of past samples, highlighting the necessity of principled experience management. Furthermore, experimental results in Appendix Table 4 indicate that optimal performance is achieved when balance (i.e., ρ = 50%) is struck between replaying past experiences and learning from new data. Figure 6: Dynamics of experience under different data conditions. Empirical ablation supports ExGRPOs selection heuristics. We conduct an ablation study to assess the contributions of core components of ExGRPO. Figure 7 shows performance dynamics of validation set (see Section E.1). We evaluate the impact of removing Quality (Q.) and Trajectory (T.) selection strategies and policy shaping. For both question and trajectory selection, our analytically derived heuristics consistently outperform random selection, while random selection itself offers marginal gains over the on-policy baseline. Policy shaping mitigates this by ensuring exploitation does not hinder exploration. It is also noteworthy that applying shaping alone in on-policy RLVR yields no improvement (Yan Figure 7: Comparison of validation peret al., 2025), confirming that ExGRPOs gains cannot be formance of different ExGRPO variants. attributed to naive shaping. Besides, additional ablations and test-set results can be found in Section E.4. Replacing our selection method with an alternative heuristic, i.e., selecting the highest-entropy trajectory, results in degraded performance. Similarly, removing trajectory selection and importance-sampling correction mechanisms also leads to inferior performance, though the effect is less severe than other ablations. The above results and additional ablations on test set validate the rationale behind our heuristic design of experience management. Preprint."
        },
        {
            "title": "7 CONCLUSION",
            "content": "While experiential RL offers promising approach to mitigate sample inefficiency in RLVR for large reasoning models, this area still lacks systematic exploration of experience value and management. In this work, we address this gap by first examining what constitutes valuable reasoning experience, then introducing ExGRPO, framework that strategically manages, selects and replays high-quality experiences. Experiments across multiple backbones and benchmarks show that ExGRPO consistently improves performance and stabilizes training where on-policy RLVR fails, demonstrating the importance of experience-aware optimization. We discuss limitations in Section A, and future work includes extending our method to multi-modal reasoning and agentic reinforcement learning."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research follows general code of ethics. We have taken care to ensure that the datasets, methodologies, and model used in our experiments are ethically sound. Our work is primarily theoretical and empirical, focusing on optimizing reasoning models. As such, it does not involve sensitive personal data, or content likely to introduce societal bias, harm, or discrimination."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide the experimental setups and hyperparameters in Section 5.1, with further details in Section E. The code and weights of the ExGRPO series models are publicly available on GitHub and Hugging Face."
        },
        {
            "title": "BIBLIOGRAPHY",
            "content": "Charles Arnal, Ga ˇTtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, and Remi Munos. Asymmetric REINFORCE for off-policy reinforcement learning: Balancing positive and negative rewards. ArXiv preprint, abs/2506.20520, 2025. Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li. xVerify: Efficient answer verifier for reasoning model evaluations. ArXiv preprint, abs/2504.10481, 2025. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. GPG: simple and strong reinforcement learning baseline for model reasoning. ArXiv preprint, abs/2504.02546, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. ArXiv preprint, abs/1803.05457, 2018. Taco Cohen, David Zhang, Kunhao Zheng, Yunhao Tang, Remi Munos, and Gabriel Synnaeve. Soft policy optimization: Online off-policy RL for sequence models. ArXiv preprint, abs/2503.05453, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards. ArXiv preprint, abs/2502.01456, 2025a. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models. ArXiv preprint, abs/2505.22617, 2025b. Shihan Dou, Muling Wu, Jingwen Xu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Improving RL exploration for LLM reasoning through retrospective replay. ArXiv preprint, abs/2504.14363, 2025. 10 Preprint. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In Proceedings of the 35th International Conference on Machine Learning, ICML, volume 80, pp. 14061415, 2018. Hugging Face. Open r1: fully open reproduction of deepseek-r1, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, and Christina Mack. Agentic ai for scientific discovery: survey of progress, challenges, and future directions. ArXiv preprint, abs/2503.08979, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv preprint, abs/2103.03874, 2021. Nathan Kallus and Masatoshi Uehara. Statistically efficient off-policy policy gradients. In Proceedings of the 37th International Conference on Machine Learning, ICML, volume 119 of Proceedings of Machine Learning Research, pp. 50895100, 2020. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. ArXiv preprint, abs/2411.15124, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Siheng Li, Zhanhui Zhou, Wai Lam, Chao Yang, and Chaochao Lu. Repo: Replay-enhanced policy optimization. ArXiv preprint, abs/2506.09340, 2025. Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, and Jianye Hao. Squeeze the soaked sponge: Efficient off-policy reinforcement finetuning for large language model. ArXiv preprint, abs/2507.06892, 2025. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR, 2016. Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3-4):293321, 1992. 11 Preprint. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. ArXiv preprint, abs/2503.20783, 2025. Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo: End-to-end policy optimization for gui agents with experience replay. ArXiv preprint, abs/2505.16282, 2025. Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, and Wentao Zhang. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. ArXiv preprint, abs/2506.07527, 2025a. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. ArXiv preprint, abs/2505.14652, 2025b. Wenjia Meng, Qian Zheng, Gang Pan, and Yilong Yin. Off-policy proximal policy optimization. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI, pp. 91629170. AAAI Press, 2023. doi: 10.1609/AAAI.V37I8.26099. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. CoRR, 2024. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, Xian-Sheng Hua, Bowen Zhou, and Yu Cheng. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. ArXiv preprint, abs/2503.21614, 2025. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Nicolas Le Roux, Marc Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves, Alex Frechette, Carolyne Pelletier, Eric Thibodeau-Laufer, Sandor Toth, and Sam Work. Tapered off-policy reinforce: Stable and efficient reinforcement learning for LLMs. ArXiv preprint, abs/2503.14286, 2025. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In 4th International Conference on Learning Representations, ICLR, 2016. John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, ICML, volume 37, pp. 18891897, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv preprint, abs/1707.06347, 2017. 12 Preprint. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv preprint, abs/2402.03300, 2024. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 1, 2025. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. ArXiv preprint, abs/2503.23829, 2025. Shivakanth Sujit, Somjit Nath, Pedro H. M. Braga, and Samira Ebrahimi Kahou. Prioritizing samples in reinforcement learning with reducible loss. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS, 2023. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS, 2024. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. In 5th International Conference on Learning Representations, ICLR, 2017. Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, Jiang Bian, and Mao Yang. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. ArXiv preprint, abs/2506.14245, 2025. Hongling Xu, Qi Zhu, Heyuan Deng, Jinpeng Li, Lu Hou, Yasheng Wang, Lifeng Shang, Ruifeng Xu, and Fei Mi. KDRL: Post-training reasoning llms via unified knowledge distillation and reinforcement learning. ArXiv preprint, abs/2506.02208, 2025. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. ArXiv preprint, abs/2504.14945, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. ArXiv preprint, abs/2409.12122, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. ArXiv preprint, abs/2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. ArXiv preprint, abs/2503.14476, 2025. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. ArXiv preprint, abs/2412.01981, 2024. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. ArXiv preprint, abs/2503.18892, 2025. Da Zha, Yuzhe Gong, Wei Hu, Yimin Sun, Yanan Wang, Ming Liu, and Yong Li. Experience replay: survey. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI), pp. 41924198, 2019. Hongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, and Guorui Zhou. Rlep: Reinforcement learning with experience replay for llm reasoning. ArXiv preprint, abs/2507.07451, 2025a. 13 Preprint. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, and Bowen Zhou. survey of reinforcement learning for large reasoning models. ArXiv preprint, abs/2509.08827, 2025b. Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting. ArXiv preprint, abs/2508.11408, 2025c. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. ArXiv preprint, abs/2506.01347, 2025. 14 Preprint."
        },
        {
            "title": "APPENDIX CONTENTS",
            "content": "A Limitations The Use of Large Language Models (LLMs) ExGRPO Algorithm Theoretical Analysis of ExGRPO D.1 Recap and Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Unbiasedness of Experiential Gradient . . . . . . . . . . . . . . . . . . . . . . . . D.3 Variance Decomposition and Bounds . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Final Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Materials of Experiments E.1 Detailed Training Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Results of Model Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Ablation and Additional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Data Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Materials of Preliminaries F.1 Details of Masked GRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 CoT Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Alternative Selection Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Snowball Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Prompt Template G.1 RLVR Training and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 CoT Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 16 16 17 17 19 20 21 21 22 22 24 24 24 25 26 35"
        },
        {
            "title": "A LIMITATIONS",
            "content": "While our work demonstrates the significant benefits of principled experience management in RLVR, we acknowledge several limitations. First, since our research scope focused on RLVR, the evaluation tasks are verifiable problems, i.e., mathematical and general reasoning benchmarks where trajectories can be more easily verified as correct or incorrect. The applicability of our correctness-based bucketing strategy to more open-ended tasks (e.g., creative writing), where rewards are often subjective and dense, remains an open question. Second, our frameworks definition of valuable experience is based on heuristics that, while powerful, might be incomplete. ExGRPO might neglect the learning potential of valuable failures where some incorrect paths that contain useful training signals (Zhu et al., 2025). This focus on exploitation could risk premature convergence in some scenarios. Finally, while ExGRPO is built upon relative policy optimization objective, its interaction with other families of RL algorithms has not been explored. 15 Preprint. THE USE OF LARGE LANGUAGE MODELS (LLMS) We utilize large language models to assist with proofreading, and polishing the text of this paper. The generated responses are treated as decision references and are not adopted wholesale. Importantly, all scientific contributions, conceptual developments, experimental designs, and final decisions were made by human researchers. The outputs from LLMs were treated as references or drafts, subject to human review, modification, and validation."
        },
        {
            "title": "C EXGRPO ALGORITHM",
            "content": "Algorithm 1 ExGRPO: Experiential Group Relative Policy Optimization (cid:80) (cid:80)K iq, o<t ) log πθ(ot k=1 1[rk = 1]. cf. Algorithm 2. cf. E: (cid:55) {o} for each experience trajectory oi do Phase 1: Experience Management. if > 0 then Compute trajectory entropy under πθ: H(oi; πθ) = 1 oi end for Select the trajectory with the lowest entropy: arg mineE H(oi; πθ) end for Experiential policy optimization data Bexp {(q, o)}n On-policy optimization data Bon {q}Bn Partition to buckets Uk by last rollout correctness Acc = 1 Obtain sampling probability: pk = (k/K; 0.5, σ = 1) for nonempty Uk Calculate experience sampling amount: min(ρB, E) Sample experience: {e1, . . . , en} BukcetSample(U, {pk}) for each experience {e1, . . . , en} do Require: Dataset D, batch size B, experience ratio ρ [0, 1], rollout trails K, reward model R(). 1: Initialize: Policy model πθ, experience replay buffer , retired set . 2: for each training step do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: end for 34: return Optimized policy model ˆπθ Generate rollouts: {o1, . . . , oK} πθ(q), {o1, . . . , oK1} πθ(q) Compute rewards and success: {rj}K if = then end if Phase 2: Experiential Policy Optimization. Construct batch data: Bexp Bon for each sample do end for Compute advantage and update policy model with Eq. 4. Remove well-learned samples: {q : S} All successful Add to retired set Partial success Record experience: E[q] {oj r(oj) = 1} {q} else if < then Bexp , Bon {q}B j=1 = {R(q, oj)}K j=1; = (cid:80)K I[rj = 1] end if else j=1 Overview of ExGRPO (Algorithm 1). ExGRPO organizes training into two phases: experience management and experiential policy optimization. In first phase, the replay buffer is partitioned into buckets based on the most recent rollout correctness Acc(q), and Gaussian weighting centered at 0.5 biases sampling toward medium-difficulty problems. From these buckets, = min(ρB, E) experiences are drawn, and for each, the trajectory with the lowest entropy under the current policy is selected as o. These anchors form the experiential sub-batch Bexp, while the remaining slots are filled with on-policy samples Bon. In the second phase, each batch item generates rollouts (or 16 Preprint. K1 plus the past trajectory), rewards are computed, and prompts achieving full success are retired, while partially successful ones refresh E. The policy is updated using objective in Eq. 4. Details of Bucketed Sampling (Algorithm 2). This sampler first normalizes probabilities over non-empty buckets, then draws bucket counts using sequential-binomial implementation of the multinomial distribution, ensuring (cid:80) ck = where is the number of samples to be drawn. Within each bucket, ck items are sampled uniformly without replacement. This preserves the intended inter-bucket bias (e.g., Gaussian weighting by accuracy) while remaining unbiased within buckets. The method assumes ck Uk for feasibility. In practice, to handle rare edge cases where bucket becomes empty, clipping and redistributing the sampling counts will be applied in such scenarios. Algorithm 2 BucketSample(). Bucketed Multinomial & Within-Bucket Uniform Sampling k=1; probabilities [0, 1]K; sample size N; k=1 pk = 1 Require: Valid buckets = {(Acc, {e})}K Ensure: probabilities [0, 1]K with (cid:80)K 1: function MULTINOMIAL(n, p) 2: 3: 4: 5: # Note: sample Nd such that (cid:80)d Initialize (0, . . . , 0) Nd, for = 1 to 1 do m, (cid:16) pi 1(cid:80) j<i pj Draw Xi Binomial Update Xi (cid:17) 6: 7: 8: 9: end function end for return collection of i=1 Xi = 10: # Note: sampling within bucket is uniform without replacement; requires ck UAcc for all k. Draw all bucket counts in one shot 11: Multinomial(n, p) 12: sampled items {} 13: for 1 to do 14: 15: 16: end for 17: return (sampled items, []) UniformSample(Uk, ck) sampled items[Uk] Without replacement. Complexity. Experience management in ExGRPO is efficient. Per step, ExGRPO performs O(BK) reward evaluations and O(BK) likelihood computations. Bucket partitioning and sampling are O(E) for bucketization (amortized) and O(K + n) for drawing counts and items. Correctness and Constraints. The procedure assumes ck Uk for all nonempty buckets. In practice, this is satisfied with high probability when (i) the support of excludes tiny buckets or (ii) we clip ck to Uk and redistribute any deficit to remaining buckets. We adopt the simple feasibility requirement as stated in Algorithm 2 for clarity. Practical Notes. We renormalize over nonempty buckets before sampling; this avoids assigning mass to empty buckets. The scheme is streaming-friendly and yields O(K) arithmetic plus the cost of within-bucket draws. When used in ExGRPO, setting (k/K; µ, σ) provides tunable curriculum over the rollout correctness spectrum. In this paper, we consistently set µ = 0.5, σ = 1 to align with our analytical findings in Section 3."
        },
        {
            "title": "D THEORETICAL ANALYSIS OF EXGRPO",
            "content": "D.1 RECAP AND FORMULATION Let be query. The rollout (reference) policy is πθold, the current policy is πθ, and the experiential policy (that produced replayed trajectory) is πθpast. For each q, form group Gq = 17 Preprint. {oi}K i=1 with oi πθold ( q) and use GRPOs within-group standardization (cid:98)A(oi, Gq) = r(q, oi) µGq σGq , µGq = 1 (cid:88) i=1 r(q, oi), σGq = Std(cid:0){r(q, oi)}K i=1 (cid:1), (5) Based on the practice we detailed in the main body of paper, we follow the simplifications advocated by Dr.GRPO (Liu et al., 2025), removing length and standard deviation normalization. For mixedpolicy optimization, we also remove clipping and use policy shaping. ExGRPO mixes (i) on-policy groups Gq and (ii) mixed groups Gq = {o} {oi}K1 i=1 for sampled from an experience buffer, where πθpast( q) and the K1 fresh rollouts come from πθold( q). The ExGRPO mini-batch objective (Eq. 4) mixes on-policy and experiential (replayed) contributions with weight ρ [0, 1). Per-token importance ratios for trajectory comparing policy πθ to policy πθ (i.e., πθold or πθpast) are: wt(oi; θ, θ) := πθ(oi,t q, oi,<t) πθ(oi,t q, oi,<t) , (oi; θ, θ) := (cid:89) t=1 wt(oi,t; θ, θ). (6) We will sometimes write w() or () for brevity. For the replayed item o, ExGRPO uses bounded policy-shaping transform (w) = w+β with β > 0 instead of hard clipping; this is analogous to truncated/saturated importance ratios in ACER and V-trace, known to control variance at small bias (Wang et al., 2017; Espeholt et al., 2018). We assume bounded rewards, absolute continuity of policies (mutual support overlap), and an auxiliary KL control as in Schulman et al. (2015; 2017). Assumptions. For formal statements we will use the following standard assumptions where invoked: A1. Support assumption. For any trajectory produced by past policy used in replay, πθ(ot ) > 0 whenever πθpast(ot ) > 0 (i.e., target policy has support containing past policy). A2. Bounded second moment. For any random variable under consideration (rewards, advantages), second moments exist and are finite. Formally, for any policy model πθ and query distribution D, EqD, oπθ(q) (cid:2)X 2(cid:3) < (7) so finiteness of E[X 2] guarantees that Var(X) = E[X 2] (cid:0)E[X](cid:1)2 A3. Bounded importance ratios. Assume finite trajectory length, there exists 1 such that for all relevant trajectories and timesteps, is well-defined. sup o,t πθ(ot ) πθpast(ot ) m, and hence sup (o; θ, θpast) := mo. (8) D.2 UNBIASEDNESS OF EXPERIENTIAL GRADIENT We show that, when the replayed trajectory is corrected by the exact per-token importance weight (o; θ, θpast), the contribution of that trajectory yields an unbiased estimate of the corresponding on-policy term (no bias is introduced by using replay plus exact importance reweighting), even when the advantage (cid:98)A depends on the full mixed group G. Theorem 1 (Unbiasedness). Under Assumption A1, let Gq = {o} {oi}K1 i=1 be mixed group where was sampled from πθpast and {oi} were sampled from πθold. For any measurable function of trajectory and its group (e.g. g(o, G) = (cid:80) θ log πθ(otq, o<t) (cid:98)A(o, G)), the importanceweighted expectation equals the on-policy expectation: Eoπθpast (cid:2)W (o; θ, θpast) g(o, Gq ) {oi}K1 i=1 (cid:3) = Eoπθ (cid:2)g(o, G) {oi}K1 i= (cid:3), where the right hand side is the expectation with replaced by sampling πθ and forming group accordingly. 18 Preprint. Proof Sketch. Condition on the other group elements {oi}K1 of is independent of {oi}K1 i=1 , thus we have i=1 , {oi}K1 i=1 is implicit as the sampling Eoπθpast (cid:2)W (o; θ, θpast) g(o, Gq ) {oi}K1 i=1 (cid:3) = Eoπθpast (cid:2)W (o; θ, θpast) g(o, Gq )(cid:3) (9) Further, consider the expectation over drawn from πθpast. By the definition of expectation, we sum over all possible trajectories (which we denote by o) weighted by their probability under πθpast: Eoπθpast (cid:2)W (o; θ, θpast) g(o, Gq )(cid:3) = = = (cid:88) (cid:88) (cid:88) πθpast(o)(cid:2)W (o; θ, θpast) g(o, Gq )(cid:3) πθpast(o) πθ(o) πθpast(o) πθ(o)g(o, Gq ) g(o, Gq ) (10) = Eoπθ (cid:2)g(o, G)(cid:3). The key is that g(, G) is an arbitrary measurable function of trajectory and the group, and the algebra above does not require to be independent of the group statistics (the sum is over the trajectory variable and the group members are treated as constants in the inner sum). This yields the stated identity. Thus, we have shown that the importance-weighted experiential term is an unbiased estimator of the on-policy term, conditioned on the rest of the group members. Remark. The theorem shows that, with exact importance weights, replay introduces no bias even when the advantage is computed using group-dependent normalization (µG, σG). In practice exact per-token ratios are used (product over timesteps), and the identity holds under A1. D.3 VARIANCE DECOMPOSITION AND BOUNDS Let Gexp denotes the random contribution from an experience-mixed group that includes one replayed trajectory corrected by importance sampling as: Gexp = (cid:16) 1 (o; θ, θpast) (o, G) + (cid:17) (oi, G) , K1 (cid:88) i=1 (11) where (o, G) denotes the per-trajectory unweighted gradient contribution (e.g. (cid:80) θ log πθ() (cid:98)A). Importantly, in our implementation of GRPO the group standard-deviation term is omitted (we only mean-center advantages), so the (, G) terms still depend on the group mean µG but not on σG. This reduces but does not eliminate the statistical coupling between group members. Below we give general variance bound that does not assume independence among group members, and then tighter bound under weak-independence assumption that is often approximately valid in practice (e.g., large K). Proposition 2 (Variance upper bound for experiential term). Under Assumption A2 (finite second moments) and A3 (bounded trajectory importance ratios), the experiential variance satisfies the following bounds. 1. (General, no independence) Without assuming independence between (o, G) and the other group contributions, we have the conservative inequality Var (cid:0)Gexp (cid:1) 2 2 (cid:16) E(cid:2)W 2U 2(cid:3) + (K 1)2 E(cid:2)U 2(cid:3)(cid:17) . (A) If in addition is uniformly bounded by (Assumption A3), then Var (cid:0)Gexp (cid:1) (cid:16) 2 2 2 E[U 2] + (K 1)2 E[U 2] (cid:17) 2(cid:0)M 2 + (K 1)2(cid:1) 2 = E[U 2]. (A) 19 Preprint. 2. (Tighter bound under weak/approximate independence) If the within-group unweighted coni=1 are pairwise uncorrelated and uncorrelated with (o)U (o, G) tributions {U (oi, G)}K1 (e.g., approximate when is large), then Var (cid:0)Gexp (cid:1) (cid:16)"
        },
        {
            "title": "2\nK 2",
            "content": "E(cid:2)W 2U 2(cid:3) + (K 1) E[U 2] (cid:17) . With the bound this yields Var (cid:0)Gexp (cid:1) 2(cid:0)M 2 + (K 1)(cid:1) 2 E[U 2]. (B) (B) Proof Sketch. Write = (cid:80)K1 i=1 (oi, G). Then Var (cid:0)Gexp 2 Var (cid:0)W + S(cid:1) using Var(X) E[X 2]. Expanding and applying CauchySchwarz to bound cross terms yields E(cid:2)(W + S)2(cid:3) (cid:1) = (12)"
        },
        {
            "title": "1\nK 2",
            "content": "Var (cid:0)Gexp (cid:1) 1 2 (cid:16) (cid:17) E[W 2U 2] + E[S2] + 2(cid:112)E[W 2U 2] E[S2] . (13) ab) 2(a + b) and E[S2] (K 1)2E[U 2] (which follows without independence Using (a + + 2 by bounding pairwise covariances by second moments) gives the general bound (A). Plugging 2 2 yields (A). If one further assumes pairwise uncorrelatedness of the -terms and zero covariance with , then E[S2] = (K 1)E[U 2] and the cross terms drop, recovering the tighter bound (B) and its specialization (B). Practical Remark. Removing the within-group standard-deviation from GRPO reduces the coupling introduced by stochastic denominator (the σG-term). This typically lowers E[U 2] compared to the case with standard deviation normalization (because the denominators variability can strongly amplify per-trajectory contributions), which in turn reduces all bounds above. Based on the upper bound analysis of variance, an important insight is that controlling the source of experience policy is crucial for reducing the value of , and consequently tightening the variance bound. In practice, techniques such as low-entropy selection and importance sampling correction contribute to this goal. Specifically, we use the average conditional entropy of the trajectory as pickup metric, and minimizing this metric encourages trajectories from the typical set, which facilitates controlling the importance sampling term and reduces the discrepancy between πθ and πθold . D.4 FINAL REMARKS Policy shaping via (w) = w+β and removing clip. As mentioned in Section 4.2, we remove the CLIP term and replace it with policy-shaping function to enable better off-policy experience exploitation. Rather than relying on empirical validation from prior work (use expert trajectory as off-policy), we also provide discussion of this design choice used in experiential optimization here. The policy shaping transform replaces by (w) for the replayed trajectory. Its key properties are: is monotone increasing in w. 0 (w) < 1 for all 0, and (w) 1 as . For small w, (w) w/β (which can amplify very small ratios when β < 1); for large w, (w) is bounded by 1 and thus damps extremely large ratios. Thus reduces variance contribution of large importance weights while introducing bias (since E[f (W )] = 1 generally). Combined with group normalization, medium-difficulty question sampling and low-entropy trajectory, policy shaping often suffices to control variance so that GRPO clipping can be relaxed or removed in practice. Preprint. Summary and Takeaways. We have shown that exact trajectory-level importance weighting guarantees unbiasedness of experiential contributions, even when advantages are computed from mixed groups (Theorem 1). The variance of experiential gradients is governed by the second moment E[W 2U 2], so bounding or controlling importance sampling term is critical (Proposition 2). Finally, the smooth policy-shaping transform (w) = w+β offers principled biasvariance tradeoff: it suppresses extreme importance weights while preserving informative contributions that would be truncated by hard clipping. In combination with within-group normalization, this explains why ExGRPO can remain stable without clipping while still benefiting from replayed trajectories."
        },
        {
            "title": "E SUPPLEMENTARY MATERIALS OF EXPERIMENTS",
            "content": "E.1 DETAILED TRAINING SETTINGS During RLVR training, the rollout generation uses temperature of 1.0. To reduce training time and monitor training dynamics, we used validation set by sampling 2.2k instances from the test set. Model performance was evaluated every 10 steps to better track training progress, without applying early stopping. Qwen2.5-Math 7B backbone was trained for 700 steps, while the models in our extension experiments were trained for 500 steps. Following the methodology of LUFFY (Yan et al., 2025), we employed consistent prompt template across all models during training, as detailed in Section G.1. An exception was made for the Llama-3.1 8B base model, for which simplified prompt was used to accommodate its limited capabilities. Additional training hyper-parameters are provided in Table 2. Table 2: RLVR training hyperparameters used in our experiments. All sequence lengths are measured using each models own tokenizer. Module Parameter Data data.train batch size data.val batch size data.max prompt length data.max response length Actor actor rollout ref.actor.optim.lr actor rollout ref.actor.ppo mini batch size actor rollout ref.actor.ppo micro batch size actor rollout ref.actor.entropy coeff actor rollout ref.actor.loss remove token mean actor rollout ref.actor.loss remove clip Rollout actor rollout ref.rollout.engine actor rollout ref.rollout.temperature actor rollout ref.rollout.val temperature Value 128 512 1024 8192 1 10 64 64 0.001 True True vllm 1.0 0.6 actor rollout ref.rollout.gpu memory utilization 0. actor rollout ref.rollout.n Trainer trainer.critic warmup 8 0 trainer.training steps 700/500 Description Global training batch size per optimization step. Batch size used for validation. Maximum input length. Maximum response length. Learning rate for the actor optimizer. - - - Remove token-wise mean term from the loss. Disable the clipping term in the loss. Inference/rollout engine. Sampling temperature during training rollouts. Sampling temperature during validation. Fraction of GPU memory to utilize for the rollout engine. Number of rollouts per prompt during RLVR. Number of warmup steps (0 disables warmup). 700 Number of training steps. steps for Qwen2.5-Math-7B, 500 steps for other models. 21 Preprint. E.2 BASELINES For the zero RLVR using the Qwen2.5-Math 7B model (Yang et al., 2024), we compare our approach against not only the on-policy Dr.GRPO baseline but also other RLVR methods: PRIME-Zero (Cui et al., 2025a): PRIME-Zero is trained with PRIME, an online RL framework that leverages implicit process rewards (Yuan et al., 2024) to enhance reasoning beyond imitation or distillation. In PRIME, both the policy and implicit PRM are initialized from the SFT model; the policy generates rollouts, which are scored by the implicit PRM and an outcome verifier. The policy is then updated using combination of outcome and process rewards. Oat-Zero (Liu et al., 2025): The original Dr.GRPO implementations discard the standard deviation in advantage computation and omit token-level normalization in the policy loss. GPG-Zero (Chu et al., 2025): GPG directly integrates group-based decision dynamics into standard policy gradients, simplifying training and greatly reducing computation without loss of reasoning quality. RePO-Zero (Li et al., 2025): Replay-enhanced Policy Optimization (RePO) employs an online experience replay mechanism by collecting early on-policy rollouts. However, it revisits them asynchronously and does not incorporate experience management. We further compare ExGRPO with RePO under the same data and training settings used by RePO, as detailed in Section E.4. For the continual RLVR setting, we apply ExGRPO on strong backbone model, LUFFY (Yan et al., 2025), which integrates Deepseek-R1 (Guo et al., 2025) trajectories in RL and substantially boosts reasoning performance. We further compare ExGRPO against other reproduced off-policy learning strategies, including: SFT: Supervised fine-tuning directly on the R1 trajectories from OpenR1 Face (2025). SFT+RL: two-stage approach involving SFT on R1 trajectories followed by on-policy RL on the same problem set. Continual LUFFY: direct continual off-policy optimization on the R1 trajectories using the LUFFY algorithm. E.3 RESULTS OF MODEL EXTENSION The empirical results in Table 3 provide strong evidence that ExGRPO offers significant and consistent performance gains over both the original models and strong On-Policy baseline, demonstrating its efficacy and robustness across diverse model architectures and initial tuning states. The evaluation spans multiple model families and sizes, including both base (Qwen2.5-Math 1.5B, Llama3.1-8B) and instruction-tuned variants (Qwen2.5-7B Instruct, Llama3.1-8B Instruct), to demonstrate the broad applicability of our method. The results consistently demonstrate the superiority of ExGRPO over the On-Policy method across all tested configurations. The advantage of ExGRPO is particularly pronounced when fine-tuning from base models. On Llama3.1-8B, the On-Policy method shows negligible gains over the base model, whereas ExGRPO provides substantial boost, improving the average ID performance from 3.7 to 6.1 and, most notably, the average OOD performance from mere 1.3 to 30.8. This highlights ExGRPOs effectiveness in robustly enhancing model capabilities, especially in scenarios where standard on-policy fine-tuning struggles to yield improvements. E.4 ABLATION AND ADDITIONAL RESULTS To dissect the contribution of each key component within our proposed ExGRPO framework, we conduct series of ablation studies, with the results presented in Table 4. Experience Selection Strategies. First, we analyze the selection strategies. Removing only the Question Selection (w/o Q. Selection) strategy results in performance drop to 46.7, while removing both strategy and Trajectory selection (w/o Q., T. Selection) further degrades the average score to 46.1. This indicates that both selection mechanisms are beneficial, with trajectory selection playing Preprint. Table 3: Overall in-distribution and out-of-distribution performance based on different backbone models. Bold indicates the best results within comparable group. Model In-Distribution Performance Out-of-Distribution Performance AIME24 AIME25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Base 7.2 On-Policy 11.8 ExGRPO 13.3 Instruct 11.7 On-Policy 14.1 ExGRPO 15. Base On-Policy ExGRPO Instruct On-Policy ExGRPO 0.0 0.4 0. 4.0 3.8 6.1 3.6 7.7 8.6 7.5 8.3 12. 0.0 0.4 0.1 0.3 0.8 0.6 26.4 40.2 46. 43.8 43.5 47.8 4.7 2.9 6.6 15.7 21.4 27. Qwen2.5-Math 1.5B 9.6 26.8 30.5 21.2 32.0 39.0 16. 3.1 30.0 35.3 57.5 59.3 Qwen2.5-7B Instruct 30.9 33.8 36. Llama3.1-8B 4.4 5.1 10.7 40.4 37.6 44.4 1. 2.8 6.5 34.4 84.7 35.2 39.3 91.4 91.1 3. 3.4 6.1 0.2 0.6 59.0 Llama3.1-8B Instruct 18.0 24.3 25. 13.3 19.6 23.1 15.2 42.4 19.9 23.0 86.7 88. 28.0 61.8 74.0 71.8 74.0 80.0 11.0 9.2 12. 39.8 49.8 55.2 3.0 28.3 30.8 24.7 19.2 36. 0.0 0.0 7.1 0.0 0.0 15.2 2.3 30.7 30. 54.7 57.6 58.6 3.6 3.0 26.2 40.5 48.1 52. 2.8 38.9 40.2 54.7 56.0 62.2 1.3 1.2 30. 27.6 44.9 52.0 Table 4: Ablation study of the key components of ExGRPO. We evaluate the impact of removing Quality (Q.) and Trajectory (T.) selection strategies, policy shaping, importance sampling (IS) correction, and the sensitivity to the experience replay ratio ρ. The results highlight the contribution of each ExGRPO component. On-Policy 24.9 15. 59.2 84.8 38.2 49.3 45.3 AIME24 AIME25 AMC MATH-500 Minerva Olympiad Avg. ExGRPO (cid:44) w/o Q. Selection (cid:44) w/o T. Selection (cid:44) w/o Q., T. Selection (cid:44) w/o Shaping (cid:44) w/o IS Correction (cid:44) w/ Highest Entropy T. ExGRPO (cid:44) w/ ρ = 25% (cid:44) w/ ρ = 75% ExGRPORE RePO-Zero ExGRPO w/ Selection Strategies 18.7 17.2 17.0 17.3 12.9 19.8 16.0 66.3 63.9 63.7 64.3 55.7 63.2 62. 87.4 86.4 84.8 85.8 80.2 87.0 84.6 ExGRPO w/ Experience Ratio 18.7 16.3 17.6 66.3 62.9 61.4 87.4 85.4 84.4 ExGRPO vs. Replay-Enhanced 13.2 10.2 52.1 54.0 84.2 76.8 31.6 26.9 26.0 22.0 19.8 25.9 25.0 31.6 27.2 22.1 15.1 19. 36.0 38.6 35.7 37.9 37.5 37.9 36.4 36.0 39.0 36.0 37.5 34.2 50.1 47.6 50.7 49.5 41.0 49.0 51.0 50.1 47.6 47.3 45.3 40. 48.3 46.7 46.3 46.1 41.2 47.1 46.0 48.3 46.4 44.8 41.3 39.2 23 Preprint. more substantial role. Besides, replacing our method with an alternative heuristic, w/ Highest Entropy Trajectory selection, also leads to suboptimal score of 46.0, validating our specific design choice. Experience Ratio. Next, we investigate the impact of the experience ratio, ρ, which controls the proportion of past experiences used. The results show that our default configuration (Avg. 48.3) outperforms both smaller ratio of ρ=25% (Avg. 46.4) and larger ratio of ρ=75% (Avg. 44.8). This suggests that our chosen ratio strikes an effective balance, leveraging sufficient amount of high-quality past experience without being overwhelmed by fresh on-policy learning. Policy Shaping Dynamics. Disabling policy shaping (w/o Shaping) causes performance drop to an average of 41.2, underscoring its essential function in guiding the optimization process. Moreover, as illustrated in Figure 8, although entropy initially drops below the on-policy baseline during early training and later rises above it, the model still fails to sustain exploration. Policy shaping alleviates this issue by ensuring that exploitation of experiences does not come at the cost of exploration. Figure 8: Dynamics of policy entropy during training. ExGRPO without policy shaping even drops dramatically at an early stage, performing worse than the on-policy baseline. Collectively, these ablations validate our design choices, confirming that each component of ExGRPO synergistically contributes to its superior performance. vs. RePO. To ensure fair comparison with the most relevant baseline, RePO (Li et al., 2025), we adopt its prompt template, data sources, and training configuration. We also match the number of on-policy rollouts to maintain equivalent training conditions. Under this setting, we denote our variant as ExGRPORE. For evaluation, we use the checkpoint released by RePO, apply the same decoding parameters, and assess performance on our benchmark. As shown in Table 4, ExGRPO outperforms RePO under identical training conditions. Beyond results in the table, the gap becomes more pronounced in out-of-distribution benchmarks, where ExGRPORE achieves an average score of 52.3 compared to 46.8 for RePO. E.5 DATA UTILIZATION In our experiments, we set the data sampling ratio ρ = 50%. This configuration implies that, for an equivalent number of training steps, our method visits 50% less fresh on-policy data per batch compared to the baselines. Notably, our approach achieves better performance despite less on-policy exploration, highlighting data efficiency of our method."
        },
        {
            "title": "F SUPPLEMENTARY MATERIALS OF PRELIMINARIES",
            "content": "F.1 DETAILS OF MASKED GRPO Formulation. To study the impact of different questions on the RLVR training process, modified version of the GRPO algorithm is used, termed as Masked GRPO. The core idea is to selectively apply training gradients only from questions that are deemed to be of appropriate difficulty. We determine this difficulty based on the correctness of set of trajectories generated by the policy during online rollouts, as introduced in Section 3. The objective function is formalized as follows: 24 Preprint. JMaskGRPO(θ) = EqD,{oi}πθold (q) (cid:34) I(αlow Acc(q) αhigh) CLIP(cid:0)wi(θ), ˆAi (cid:1) (cid:35) (14)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 Compared to the original GRPO objective, we introduce crucial modification highlighted in red: an indicator function I(). Here, Acc(q) represents function that calculates the correctness score for the set of trajectories {oi} generated for given question q. This score is then evaluated against predefined range defined by lower bound αlow and an upper bound αhigh. Training Dynamics. potential confounding factor in this analysis is that masking questions of certain difficulties might alter the total number of samples used for optimization in each group, thereby skewing the performance comparison. To rule out this possibility, we analyzed the training dynamics. Figure 9 plots the number of questions included in each mini-batch over the course of training for different groups. The visualization shows that the data throughput for all three configurations is highly similar and stable at later stage of training, fluctuating around common mean. This visual evidence, further substantiated by the statistics on the average number of optimization samples (see Table 6), confirms that data quantity was not significant variable. Therefore, we can attribute the observed performance in Table 5 to the distinct difficulty distributions of the training data itself. Table 5: Performance of Qwen2.5-Math-7B trained under different difficulty schemes using on-policy RLVR. All results are reported on math reasoning benchmarks. AIME AIME25 AMC MATH-500 Minerva Olympiad Full Easy Medium Hard 24. 21.9 17.8 15.5 15.5 59.2 84.8 On-Policy RLVR w/ Question Bucket 13.9 17.4 13. 50.6 56.3 54.3 82.8 80.6 83.6 38.2 33.8 36.4 36.4 49.3 43.7 44.9 46. Avg. 45.3 41.1 42.3 41.6 Group # Avg. Mini-Batch Hard Medium Easy 20.79 23.87 21.23 Table 6: Average number of questions per mini-batch for each difficulty-masked training group, confirming similar data throughput. F.2 COT JUDGE Figure 9: Dynamics of the number of questions per minibatch for the three difficulty-masked training groups. The three series demonstrate that each training regime was exposed to comparable volume of optimization data. We adopt the same prompt template (refer to Section G.2) as prior work for evaluating the quality of reasoning CoT. For the judge model, we employ Qwen3-32B rather than the smaller models used in previous studies (Wen et al., 2025). This choice is intended to ensure stronger judgment capability and more reliable evaluations. Decoding is performed with temperature of 0.6 and top-p of 0.95. F.3 ALTERNATIVE SELECTION METRICS Beyond entropy, the perplexity (PPL) of the current policy π can also serve as lightweight metric for estimating the quality of reasoning CoT, formulated as: 25 Preprint. PPL(o) = exp (cid:32) (cid:88) 1 log π(ot o<t) (cid:33) . (15) In the ExGRPO method, we adopt entropy rather than PPL, with the justification detailed below. Our analysis, as depicted in Figure 10, investigates the potential of using PPL metrics as proxies for evaluating the quality of reasoning trajectories. The results reveal positive correlation between these internal signals and external correctness judgments. Across all three correctness buckets, both perplexity and entropy are consistently lower for correct reasoning trajectories compared to incorrect ones. This finding suggests that the model is inherently more confident and certain when generating valid reasoning steps. More importantly, we observe that entropy exhibits superior discriminative power. The margin between correct and incorrect chains is wider for entropy than for PPL, indicating that the models token-level uncertainty is more sensitive indicator of flawed reasoning than its overall sequence probability. Figure 10: comparison of average PPL and Entropy for correct (C.) and wrong (W.) reasoning trajectories (determined by an external CoT judge), grouped by online rollout correctness buckets. F.4 SNOWBALL EFFECTS Recap. key challenge in learning from experience is the risk of snowball effect: the repeated sampling of trajectories with flawed reasoning from the replay buffer (generally characterised as highentropy based in our research) can systematically degrade the learned policy, leading to entrenched reasoning errors. We observe, for instance, that models frequently generate some unnecessary code blocks when solving mathematical problems. To investigate this phenomenon, we partitioned trajectories from our analytical experiments (refer to Section 3) based on the presence of code generation (We implement regex-based detector that flags whether free-form text contains code in Table 7) and analyzed two key metrics: the average token-level entropy and the validity of the CoT, as assessed by the same CoT judge. Results. The results in Table 8 reveal strong correlation between code generation, model uncertainty, and reasoning quality. Across all problem difficulty levels, trajectories containing code blocks consistently exhibit higher entropy than those without (e.g., 0.14 vs. 0.07 for medium-difficulty problems). Crucially, this elevated uncertainty is coupled with degradation in CoT quality. The proportion of logically correct CoT sequences is consistently lower in trajectories that utilize code. This performance gap is most pronounced for easy and medium problems, with an 11.6 and 10.2 point drop in CoT correctness, respectively, and narrows slightly for hard problems (-8.7 point drop). Discussion. We hypothesize that the behavior of code generation as procedural shortcut or black-box execution, particularly when it struggles to articulate complete, step-by-step formal proof. While this computational approach can be effective at deriving the correct final numerical answer, it often circumvents the explicit articulation of the underlying mathematical logic. This leads to higher incidence of flawed or redundant reasoning chains. Preprint. Table 7: Rule set for code detection. The table lists high-level cues and rationale. We intentionally avoid placing regex in the table for rendering robustness, and exact patterns are provided below. ID R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 Type High-Level Cue (plain language) Rationale Structural (fence) Structural (fence + header) Syntax Syntax Syntax Syntax Idiom Idiom Idiom Syntax (loop) Syntax (branch) Syntax (loop) Syntax (exception) Syntax (exception) Syntax (literals) Syntax (literals) Idiom (method call) Fenced code block explicitly labeled as Python Unlabeled fence whose first non-empty line begins with Python keyword (def/class/import/from/if/for/while/try/with) Function definition line Class definition line import statement from ... import ... statement print(...) call len(...) call range(...) call for ... in ... loop header if ...: header while ...: header try: header except ...: header Variable assigned to list literal Variable assigned to dict literal Dotted method/function call followed by ( (e.g., obj.method(...)) Strong, explicit declaration of language Captures unlabeled fences that still look like Python Canonical Python construct Canonical class declaration Frequent in code snippets Frequent in examples/tutorials Extremely common snippet cue Common builtin call Common in loops/examples Strong loop cue Strong code header cue Strong loop cue Exception-handling construct Exception-handling construct Common quick examples Common quick examples Captures typical API usage The implications for experiential RL are direct. Using these high-entropy as experience will reinforce the models bias toward such logically deficient trajectories, thus corrupting the reasoning process it learns. This suggests the importance of ensuring the quality of CoT in experience replay. As further evidence, our ablation studies (see Appendix Section E.4) confirm that explicitly forcing the model to learn from high-entropy experiences is harmful to its reasoning capabilities. Table 8: Analysis of model-generated trajectories based on the presence of code blocks. Trajectories with code exhibit higher entropy and lower rate of correct CoT, with representing the percentage point drop in CoT correctness. Bold indicates better results of specific metric. Average Entropy CoT Correctness (%) Code Code Code Code Easy Medium Hard 0.09 0.07 0.10 0.15 0.14 0.16 73.4 64.7 56. 61.9 54.5 47.4 -11.5 -10.2 -8.7 Case Study. To further illustrate this behavior, we present several qualitative examples of such trajectories as follows. Case #1 presents challenging question from our training data. Although all trajectories ultimately arrive at the correct answer, they differ in the soundness and quality of their reasoning chains. The trajectory with the lowest entropy exhibits shorter but coherent reasoning process. In contrast, the highest-entropy trajectory allocates more reasoning budget to code verification, producing longer responses that are logically unsound. Upon our inspection, the generated code is also invalid, and the external CoT judge also identifies this trajectory as poor reasoning. similar phenomenon is observed in Case #3, which involves an easier problem. In Case #2, the external judge instead deems the high-entropy trajectory as correct reasoning chain. However, it still produces unnecessary code in the final step, which we argue is unnatural and misaligned with human-like reasoning behavior. Case #1 of Snowball Effects QUESTION: Given that is even, is odd, and the system of equations (cid:26) 1988y = 11x + 27y = 27 Preprint. has the solution as integers, then (A) p, are both even. (B) p, are both odd. (C) is even, is odd. (D) is odd, is even. (cid:26) = = (Zu Chongzhi Cup Junior High School Mathematics Invitational Competition, 1988) GROUND TRUTH: RESPONSE: (Low Entropy) REWARD: 1.0 ENTROPY: 0.1532 COT VALIDITY (JUDGED BY QWEN3-32B): Correct. To determine the correct answer, lets analyze the system of equations: (cid:26)x 1988y = 11x + 27y = Given that is even, is odd, and both and (i.e., and q) are integers. Lets substitute = and = to use the equations: (cid:26)p 1988q = 11p + 27q = Since is even, 1988q must be even. As 1988q is always even (since 1988 is even and any integer q), must also be even for the sum to be even. Since is odd, 11p + 27q must be odd. Since 11p is always odd or even (depending on p) and 27q is always odd (since 27 is odd and q), the sum of 11p and 27q will be odd if and only if one of them is even and one is odd. Given is even, 11p is even, hence 27q must be odd, implying is odd. So, based on the above reasoning, is even and is odd. Therefore, the correct answer is (C) is even, is odd. </think> The final answer is (C) . RESPONSE: (High Entropy) REWARD: 1.0 ENTROPY: 0.2652 COT VALIDITY (JUDGED BY QWEN3-32B): Wrong. To solve the problem, we need to analyze the system of linear equations and determine the parity (even or odd nature) of the variables and y. Lets break it down step-by-step. Given the system of equations: (cid:26)x 1988y = 11x + 27y = where is even and is odd, and and are integers, we need to determine the parity of and y. First, lets express and in terms of and m. We can use the method of elimination or substitution to solve the system. We can eliminate by multiplying the first equation by 11 and then subtracting the second equation from it: 11(x 1988y) (11x + 27y) = 11n 11x 21868y 11x 27y = 11n 2215y = 11n 28 Preprint. = 11n 2215 Since is odd and is even, 11n is even (because any integer multiple of an even number is even). Therefore, 11n is odd (because odd minus even is odd). Since 2215 is odd, the quotient m11n Now, we substitute = into the first equation to solve for x: 2215 will be odd if 11n is odd. Hence, = must be odd. = + 1988q Since is even and 1988q is even (because any integer multiple of an even number is even), the sum of two even numbers is even. Therefore, = must be even. So we have: = even, = odd Thus, the correct answer is (C) is even, is odd. </think> python # The reasoning above leads us to the conclusion, but we can verify using simple Python code snippet to ensure there are no calculation errors. # We can test with few values of (even) and (odd) and verify the results. import sympy as sp # Define the variables x, = sp.symbols(x y) = sp.Symbol(n, integer=True, even=True) = sp.Symbol(m, integer=True, odd=True) # Define the equations eq1 = sp.Eq(x - 1988*y, n) eq2 = sp.Eq(11*x + 27*y, m) # Solve the equations for and solution = sp.solve((eq1, eq2), (x, y)) x_sol = solution[x] y_sol = solution[y] # Check the parity of and x_sol_parity = sp.Mod(x_sol, 2) y_sol_parity = sp.Mod(y_sol, 2) print(x_sol_parity, y_sol_parity) output Mod(2215*n/2215 - 1988*m/2215 + Mod(1988*m/2215, 2) Mod(-11*n/2215 + Mod(n, 2), 2) 2215 . Since is odd and 11n is even (because is even), The Python code confirms the parity analysis we performed earlier. Heres the detailed reasoning again with the output from the code: 1. The solution for is = m11n 11n is odd. And because 2215 is odd, = m11n 2.The solution for is = 11 (cid:0) m11n Since (cid:0) m11n (cid:1) is an integer and odd, and 11 times an odd number is odd, and 1988n is even, = must be even because the sum of an odd number and an even number is odd plus even (which remains even). Hence, the final answer is: 2215 will be odd. Therefore, = is odd. (cid:1) + 1988n. 2215 2215 29 Preprint. Case #2 of Snowball Effects QUESTION: 13. Find the sum: 100 99 98 . . . 1 + 1 + 2 + . . . + 101 + 102 GROUND TRUTH: 203 RESPONSE: (Low Entropy) REWARD: 1.0 ENTROPY: 0.2151 COT VALIDITY (JUDGED BY QWEN3-32B): Correct. To find the sum of the arithmetic sequence given by 100 99 98 . . . 1 + 1 + 2 + . . . + 101 + 102, lets break down the problem step-by-step. 1. Identify the arithmetic sequence: The sequence is We can split it into two separate sequences and then find the sum of each: 100, 99, 98, . . . , 1, 1, 2, . . . , 101, 102 The first sequence is 100, 99, 98, . . . , 1. The second sequence is 1, 2, . . . , 101, 102. 2. Sum of the first sequence (100, 99, 98, . . . , 1): The sum of an arithmetic sequence from to with common difference of 1 can be found using the formula: = (f irst term + last term) where is the number of terms. For the first sequence: First term (a) = 100 Last term (l) = 1 Number of terms (n): = + 1 = 1 (100) 1 + 1 = 100 Therefore, = 100. Using the formula: 100 2 3. Sum of the second sequence (1, 2, . . . , 101, 102): = (100 + (1)) = 50 (101) = 5050 First term (a) = 1 Last term (l) = 102 Number of terms (n): Therefore, = 102. Using the formula: = = 102 1 + 1 = 102 (1 + 102) = 51 103 = 5253 102 2 4. Combine the sums: The total sum is the sum of the first sequence plus the sum of the second sequence: Sum = 5050 + 5253 = 203 </think> Final Answer The sum of the arithmetic sequence 100, 99, 98, . . . , 1, 1, 2, . . . , 101, 102 is: RESPONSE: (High Entropy) 203 30 Preprint. REWARD: 1.0 ENTROPY: 0.2967 COT VALIDITY (JUDGED BY QWEN3-32B): Correct. To find the sum of the series, we need to break it down into manageable parts. The series is essentially two series combined together, one in descending order from -100 to 102 and the other in ascending order from 1 to 101. Well need to sum these series separately and then combine the results. The series can be split as follows: 1. The series 100 99 98 . . . 1 2. The series 1 + 2 + . . . + 101 + 102 First, lets look at the series 100 99 98 . . . 1. This is single decreasing arithmetic series from -100 to -1. The sequence can be rearranged to form an arithmetic series with common difference of -1, starting from -100 to -1. The sum of an arithmetic series Sn is given by the formula: Sn = 2 (a1 + an) where is the number of terms, a1 is the first term, and an is the last term. Here, a1 = 100, an = 1, and the number of terms = 100. So, the sum of this series is: S1 = 100 (100 + (1)) = 50 (101) = 5050 Next, lets look at the series 1 + 2 + . . . + 101 + 102. This is an arithmetic series starting from 1 with common difference of 1, ending at 102. The number of terms is 102, so the sum of this series is: S2 = 102 2 (1 + 102) = 51 103 = 5253 Now, combine the two results: Total sum = S1 + S2 = 5050 + 5253 = 203 Lets verify this with Python code. </think> Here is the Python code to verify the result: python # Sum of the series -100 - 99 - 98 - ... - 1 from -100 to -1 sum_descending = sum(range(-100, 0)) # Sum of the series 1 + 2 + ... + 102 from 1 to 102 sum_ascending = sum(range(1, 103)) # Total sum total_sum = sum_descending + sum_ascending print(total_sum) output 203 The sum of the series 100 99 98 . . . 1 + 1 + 2 + . . . + 101 + 102 is 203 . Case #3 of Snowball Effects QUESTION: ## [ Auxiliary area. The area helps to solve the problem ] 31 Preprint. Pythagorean Theorem (direct and inverse) In triangle ABC, heights AE and CD are drawn. Find the side AB, if BD = 18, BC = 30, AE = 20. GROUND TRUTH: 25 RESPONSE: (Low Entropy) REWARD: 1.0 ENTROPY: 0.0198 COT VALIDITY (JUDGED BY QWEN3-32B): Correct. To solve the problem, we need to use the properties of right triangles and the Pythagorean theorem. Heres the step-by-step reasoning: 1. Identify the given information: BD = 18 BC = 30 AE = 20 CD is height from to AB, so CD AB AE is height from to BC, so AE BC 2. Determine the position of point D: Since is the foot of the perpendicular from to AB, BD = 18 and BC = 30, we can find AD using the Pythagorean theorem in triangle BCD: (cid:112) CD = BC 2 BD2 = (cid:112) 302 182 = 900 324 = 576 = 24 So, CD = 24. 3. Use the area of the triangle: The area of triangle ABC can be calculated in two ways: 2 BC AE = 1 2 AB CD = 1 Using AE and BC: Area = 1 Using CD and AB: Area = 1 2 30 20 = 300 2 AB 24 Equating the two expressions for the area, we get: AB 24 = 300 = AB 24 = 600 = AB = 1 2 So, the length of AB is 25. </think> The final answer is 25 . 600 24 = 25 RESPONSE: (High Entropy) REWARD: 1.0 ENTROPY: 0.3002 COT VALIDITY (JUDGED BY QWEN3-32B): Wrong. To solve the problem, we need to use the properties of the triangle and the Pythagorean theorem. Here is the step-by-step thought process: 1. Understand the Problem: We need to find the side AB of triangle ABC. We know the length of BD = 18 and BC = 30. We know the height AE = 20. 2. Apply the Right Triangle Properties: Since AE is height, AE is perpendicular to BC, so triangle AEB and triangle AEC are right triangles. We can use the Pythagorean theorem in triangles ABD and AEC. 3. Using the Pythagorean Theorem in ABD: In ABD, AD can be found using the Pythagorean theorem: AD = AB2 BD2. 32 Preprint. 4. Using the Pythagorean Theorem in AEC: In AEC, EC can be found using the Pythagorean theorem: EC = Since BC = BD + DC and is on BC, DC = BC BD = 30 18 = 12. In AEC, AC = EC, because AE is the height from to BC. AC 2 AE2. 5. Using the Area Equivalence: The area of the triangle can be calculated in two ways: Using base BC and height AE: Area = 1 Using base AB and height CD: Area = 1 2 BC AE = 1 2 AB CD. 2 30 20 = 300. 6. Find CD: From the area equivalence, 1 7. Using the Pythagorean Theorem in ACD: 2 AB CD = 300, so CD = 600 AB . In ACD, AC = AD2 + CD2 = (cid:112)(AB2 BD2) + CD2. Lets use these steps to set up the equations and solve for AB. </think> Using the relationships and the Pythagorean theorem, we can derive the value of AB. Lets first find AB using the properties of the triangle and the given values. We know: First, we calculate CD: BD = 18, BC = 30, AE = 20 CD = 600 AB Now, we use the Pythagorean theorem in ABD: AD = (cid:112) AB2 182 And in AEC: (cid:112) AC = 122 + 202 = 144 + 400 = 544 = 34 The area using base BC and height AE is: 1 2 The area using base AB and height CD is: Area = 30 20 = 300 Area = 1 2 AB CD = 1 2 AB 600 AB = Thus, CD = 600 AB We now use the Pythagorean theorem in ABD and AEC: (cid:112) AD = AB2 182 = (cid:112) AB2 324 AC = Since AC is the hypotenuse of AEC: (cid:112) 122 + 202 = 34 We can now solve for AB by equating the areas and solving the equations. Lets now calculate the value of AB using Python to find the accurate result. AC = AE2 + EC 2 33 Preprint. python import sympy as sp # Define the variables AB = sp.Symbol(AB, positive=True) # Given values BD = 18 BC = 30 AE = 20 # Calculate CD using the area equivalence CD = 600 / AB # Calculate AD using the Pythagorean theorem in triangle ABD AD = sp.sqrt(AB**2 - BD**2) # Calculate AC using the Pythagorean theorem in triangle AEC AC = sp.sqrt(12**2 + AE**2) # Using the area equivalence, CD = AE * BC / AB, we already have CD = 600 / AB # Now we solve for AB using the Pythagorean theorem in triangle ABD and AEC # We know AC = sqrt(ADˆ2 + CDˆ2) and AD = sqrt(ABˆ2 - 324) AC_from_AyDC = sp.sqrt((AB**2 - 324) + (600 / AB)**2) # Equate AC from both triangles since they are the same equation = sp.Eq(AC, AC_from_AyDC) # Solve for AB AB_value = sp.solve(equation, AB) print(AB_value) output [25] The value of AB is 25. Therefore, the final answer is: 25 Preprint."
        },
        {
            "title": "G PROMPT TEMPLATE",
            "content": "G.1 RLVR TRAINING AND EVALUATION We use the same prompt template for most models; however, due to the limited capability of the Llama-3.1 8B base model, we adopt simplified prompt to ensure that responses can be generated under zero-shot settings. RLVR, Evaluation Prompt Your task is to follow systematic, thorough reasoning process before providing the final solution. This involves analyzing, summarizing, exploring, reassessing, and refining your thought process through multiple iterations. Structure your response into two sections: Thought and Solution. In the Thought section, present your reasoning using the format: <think>n thoughts </think>n. Each thought should include detailed analysis, brainstorming, verification, and refinement of ideas. After </think>n in the Solution section, provide the final, logical, and accurate answer, clearly derived from the exploration in the Thought section. If applicable, include the answer in boxed{} for closed-form results like multiple choices or mathematical solutions. User: This is the problem: {Question} Assistant: <think> RLVR, Evaluation Prompt (Llama-3.1 8B Base) User: {Question} Answer: Lets think step by step.n G.2 COT JUDGE LRM-as-Judge Prompt You are an expert in mathematics and logical reasoning. Your task is to evaluate the correctness of solution to given math problem, with strong emphasis on the reasoning process, not just the final answer. Below is the Problem and the Solution (Provided by another AI model): Problem: {Problem} Solution (Provided by another AI model): {Solution} Please perform the following tasks: 1. Analyze the solution step-by-step, paying close attention to: Computational accuracy Logical consistency Conceptual understanding Whether the reasoning is valid and complete 2. Identify any issues or errors in the reasoning, even if the final answer is correct. Classify them into the following categories (if applicable): Calculation Error: Mistakes in arithmetic, algebraic manipulation, or numerical computation. Logical Error: Invalid reasoning, flawed logic, or incorrect inference. Conceptual Error: Misunderstanding or misuse of mathematical concepts or definitions. 35 Preprint. Omission / Incompleteness: Missing steps, incomplete justification, or not addressing all parts of the question. Other: Any other type of error that does not fit into the above categories. 3. Provide final judgment on whether the solution is logically sound and free of errors in reasoning. Please format your response as follows: Issues Identified: [Issue 1]: [Classification] - [Brief explanation] [Issue 1]: [Classification] - [Brief explanation] . . . Lets think step by step and output your final judgment within boxed{}: boxed{yes} or boxed{no}"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "University of Macau"
    ]
}