{
    "paper_title": "Training Noise Token Pruning",
    "authors": [
        "Mingxing Rao",
        "Bohan Jiang",
        "Daniel Moyer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the present work we present Training Noise Token (TNT) Pruning for vision transformers. Our method relaxes the discrete token dropping condition to continuous additive noise, providing smooth optimization in training, while retaining discrete dropping computational gains in deployment settings. We provide theoretical connections to Rate-Distortion literature, and empirical evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrating TNT's advantages over previous pruning methods."
        },
        {
            "title": "Start",
            "content": "Mingxing Rao, Bohan Jiang, Daniel Moyer Vanderbilt University Nashville, TN 37235,USA {mingxing.rao, bohan.jiang, daniel.moyer}@vanderbilt.edu 4 2 0 2 7 2 ] . [ 1 2 9 0 8 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In the present work we present Training Noise Token (TNT) Pruning for vision transformers. Our method relaxes the discrete token dropping condition to continuous additive noise, providing smooth optimization in training, while retaining discrete dropping computational gains in deployment settings. We provide theoretical connections to RateDistortion literature, and empirical evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrating TNTs advantages over previous pruning methods. 1. Introduction Token pruning is class of methods for reducing computational load in transformers [20] by reducing the input length. While transformers are already somewhat robust to dropping small number of tokens at random, learned dropping schemes enable larger dropping rates and thereby higher speed-ups with smaller accuracy penalties. These gains are only amplified by operations super-linear cost scaling (e.g., attention) and memory footprint. Token pruning methods exploit predictive information differences between tokens; for given task, some tokens are more useful than others. By dropping the least informative tokens first, learned method can preserve the most amount of accuracy while removing desired number of tokens. While these methods were originally explored in natural language processing context, token redundancy is arguably stronger in image transformers, and multiple token pruning methods for vision models (e.g. ViT [8]) have been proposed [15, 17, 21 23]. Unfortunately, token pruning as an action is discrete and not easily optimized directly. Current techniques use either forms of stochastic discrete dropping [17, 23], or heuristics [21] using attention activations themselves. In this paper, we reframe Token Pruning as constrained case of the Information Bottleneck problem. By viewing the token dropping rate as channel constraint and the accuracy penalty as distortion metric, we can directly apply ideas from the compression literature. Relaxations of these conditions provide continuous optimization desiderata, which are more easily optimized. Even though the original problem is difficult to solve exactly due to high-order token interaction information (synergy and redundancy), we show that simple approximation to this case works surprisingly well. Specifically, we demonstrate that the proposed method has superior performance in comparison to recent pruning methods using either stochastic discrete dropping or attention heuristics on standard base models for the ImageNet-1K benchmark, in particular in the low token regime (i.e., when removing large percentage of tokens). In the present work we provide: novel method for token-pruning that provides state-ofthe-art performance with respect to the accuracy/computation load trade-off. justification and intuition for that method based on the information bottleneck. Empirical experiments demonstrating the use and utility of our method along with previous methods as baselines, with evaluations on ImageNet [8] using two common image transformers, ViT [8] and DeIT [19], as base architectures. Our code can be found at https://github.com/mxethan-rao/tnt. 2. Related Work Token Pruning: Token pruning (or token dropout) has been explored for both transformer-based language models [10, 12, 13] and vision transformers [17, 2123]. Broadly speaking, these methods can be separated into two categories: stochastic dropout methods, where tokens are randomly removed based on per-token computed likelihood [10, 12, 13, 17, 23], and heuristic attention-based methods [15, 21] Rao et al. 2021 [17] introduces Dynamic ViT, which is notable as the first token pruning method for vision transformer [8]. It is prototypical of the stochastic dropout family of methods: it defines relaxation of the rate criterion and samples tokens according to an inclusion likelihood. Yin et al. 2022 [23] introduce refinement of this method (Adaptive 1 Figure 1. Training Noise Token Pruning (TNT). Our proposed method computes relevance term αi for each token. In training (diagrammed at top), these terms dictate an amount of noise added to the token, while at test time they indicate pruning order. ViT), incorporating halting module and associated score, as well as loss function that encourages early stopping. Along different path, multiple heuristics have been introduced for token pruning based on the attention scores [15, 21]. These use the intuition that tokens to which other tokens ascribe high attention (highly attended tokens) are of high importance. Liang et al. 2022 [15] additionally merges the lowest attended tokens, while Wang et al. 2024 puts this both into graph ranking framework (PageRank [16]) and into the zero-shot context. simple baseline version of an attention-based heuristic, Top-K pruning, was also found to perform competitively as well [11]. This method simply ranks tokens based on the attention distribution to CLS token, and then truncates after the top K. While it is not directly applicable to architectures without CLS tokens or non-classification tasks, our results in Section 4 show that it outperforms most other methods where it can be applied. Merger Methods: Complementary to dropping methods, token mergers and similarity-based pruning methods also decrease transformer computational cost by reducing the size of the token set [5, 15, 21]. Similarity-based pruning [21] exploits exactly the opposite problem in the token set; instead of removing low relevance tokens they generally merge redundant tokens. Merger methods are more general, also possibly allowing for encoding of background context variables [5]. The use of these methods is not mutually exclusive with token pruning [21]. While in the present work we do not provide completely novel merger method, we provide improvements on an existing similarity-based pruning step, and provide justification for its necessity. Information Bottleneck: Our framework for token dropping relies upon theory originally explored in the Information Bottleneck [2, 18], which in turn is based upon RateDistortion theory [3]. The information bottleneck characterizes encodings in terms of their relevance (the additive inverse of distortion metric) and rate constraint. We place token prunings two relevant metrics into this context, which then provides natural relaxation for the rate constraint. 3. Methodology 3.1. Token pruning as bottleneck optimization We model token pruning optimization as set selection problem. Given transformer architecture , we want to select the token subset that minimizes loss for each datapoint and label pair (x, y) subject to constraint < K: min L(f (xT ), y) s.t. < (1) where is the set of all tokens, and xT are input data restricted to the token set . We can map this problem into the information-theoretic literature by drawing analogies between the loss and constraint functions in Eq. 1 and the rate and distortion functions of compression literature [18]. The loss function usually measures discrepancies between and its estimator (xT ), and, as in previous work [2, 18], is direct analog to the distortion function. The analog to the rate function requires more assumptions. While viewing each token as fixed number of bits directly maps the < constraint into transmission rate, this does nothing to aid our optimization. Taking the mapping one step further, we could instead use the mutual token selection, but still enjoys continuous optimization landscape, so long as we choose to add continuous noise. On the otherhand, in order to exactly measure the mutual information we need to measure redundancy between tokens. This issue also was implicitly present in the discrete form of the optimization problem, but it is explicitly included in mutual information. 3.2. Proposed approximate solution Solutions to the information bottleneck have been proposed in the literature, both in general setting [18] and in deep learning context [1, 2]. However, for token pruning, these solutions are almost certainly too computationally heavy to be efficient as dropping methods. Instead, we will solve simplified version that ignores redundant information, then prune redundant tokens in second phase. Surprisingly, even with the errors induced by approximation, this method produces accurate results with very little compute overhead. Given non-pruned vision transformer architecture, we train module we call the Noise Allocator to estimate relative token relevance scalars αi for each token. Due to the residual connections used by many transformer architectures, this module can be placed after any of the individual transformer blocks, or after multiple blocks (in multi-layer dropping case). Consider an architecture with repeated transformer blocks possibly after an initial token embedding layer (for example, ViT has 12 identical blocks). Let the ℓth Transformer block (ℓ) with input RN D, where is the number of tokens, and is the embedding dimension. Let x(ℓ) denote the ith token. For every block where the user would like to perform pruning, we introduce linear token relevance predictor with weight matrix (ℓ) RD1. We then compute αi as = Softmax(W (ℓ)f ℓ(x(ℓ))) αℓ (3) For training/fine-tuning, we compute noise variables ηℓ as = (1 α(ℓ) ηℓ )ε, (4) where ε (0, βID) (the reparameterization trick [14]). Here, β is hyper-parameter that controls the total amplitude of noise added, and corresponds with up to monotonic transformation. The Softmax function in Eq. 3 ensures that the network must allocate β amount of noise; without it, degenerate zero-noise solution exists. Classical results in Information Theory state that the mutual information (channel capacity) for each token to its noised variant has an upper bound proportional to log(1 + Psignal/Pnoise), where Psignal/Pnoise is the ratio of the power (amplitude) of the token signal and to that of the noise [6] (the signal to noise ratio). Due to the layer-wise Figure 2. Noise Allocator block architecture: the block diagrammed above is injected into pre-trained models as pruning layer. It takes the output of the previous Transformer block as input, then computes the noise signal terms α using linear layer followed by Softmax function. During training it samples Gaussian noise conditioned on α for each token, then adds the noise to the token embeddings. At test time, tokens are instead dropped. This pruning method can be trained with all parameters outside the noise allocator are frozen. information I(xT , x) < constraint. This forces the effective information transmission to remain below bits. We then would like to solve min Distortion(f (xT ), y) s.t. I(xT , x) < (2) which is exactly the Information Bottleneck. This formulation is at once quite useful in that it maps discrete optimization into continuous problem, yet unfortunately also problematic in its exact computation. The rate parameter is definitionally aware of the content of the tokens. This provides us with route to potential procedure: force the token content to have at most bits of information from the original by adding an external source of noise. Selecting what information to keep from is equivalent to 3 normalization (LayerNorm) of many transformer architectures, Psignal is necessarily bounded. Disregarding the synergistic information between tokens, optimizing our proposed Noise Allocator amounts to solving an information bottleneck problem. We discuss limitations of this approach in Section 4.4. At test time noise is not sampled, and instead of β we directly select the top tokens, dropping the rest of the token set. We interpret α(ℓ) as token importance map for x(ℓ). Based on this importance ranking, we may either discard less important tokens or merge them with other tokens in the subsequent stage. Weight vector is applied to every xi identically, so the additional parameter space complexity remains O(N L), where is the number of layers at which tokens are selectively dropped during testing. Given D, this complexity effectively reduces to O(D), making it comparatively negligible within the overall model size. For weaker encoders and/or smaller transformer blocks such as DeiT-Tiny, it is sometimes helpful to increase the complexity of to multi-layer structure (e.g., an MLP), as we show in experiments in the Appendix. 3.2.1. Similarity-based Pruning by Random Partition (Redundancy Removal) Even though individual tokens might be highly relevant to the classification problem (i.e, individually I(xℓ , y) might be high), relevant tokens sharing high amount of information with kept token represents wasted capacity. In order to avoid this waste, we implement redundancy removal step for test time usage, which removes similar and therefore possibly redundant tokens. The method is as follows: Tokens are randomly divided into two groups. We then identify the closest matching token in one group for each token in the other, recording the similarity scores of each paired token based on their token embeddings. Next, we prune the top-r most similar pairs and prune the associated tokens in the second group. Our similarity-based pruning approach closely resembles that of Zero-TP [21], with two key modifications. First, rather than using the Key values as the partitioning metric, we directly use x(l) so that it can be applied directly after the Noise Allocator step.Second, instead of sequentially partitioning tokens based on their importance scores, we apply random partitioning strategy. Our ablation study suggests that random partitioning yields improved performance for the proposed model. 4. Experiments We conduct series of experiments on pre-trained vision transformer models, including DeiT (Tiny1, Small, 1DeiT-Tiny results relegated to the Appendix. Base) [19] and ViT/16 [8], each trained on ImageNet-1K [7]. We then compare our model with previously proposed tokendropping models, as well as two simple baseline schemes: DynamicViT [17], ZeroTP[21], ToMe [5], Top-K [11], and random dropping. In Section 4.3, we present ablation studies to validate our individual design choices. We first show qualitative results for pruning 50% of tokens at varying layers (single-layer pruning at layers 1-5) on the ImageNet-1K validation dataset in Figure 3. As seen at left in the figure, extremely early layers result in relevant tokens being dropped, and irrelevant tokens being included; this matches with results from previous literature [9, 17, 21]. Results further indicate that the the importance score based on noise signal term α becomes more reasonable in latter layers, performing well on variety of qualitatively different images (tiny objects, mixed foreground and background, low contrast separation). Additional examples are provided in the Appendix. We conduct comprehensive evaluation for each method by sweeping the token keep rate (K) from low to high-token regime, as detailed in Sections 4.1 and 4.2. We experiment with both single-layer and multi-layer pruning schema. In the single-layer scheme all token pruning occurs at single layer, usually early in the network. In comparison, the multi-layer scheme has pruning blocks spread across the architecture. The multi-layer scheme is generally more effective, albeit with additional computational cost, but due to the large number of possible parameters for multi-layer pruning introduced by the multiple keep rates (with possibly varying predictive accuracy), may be hard to effectively tune for all methods. This makes comparisons in the multi-layer scheme inexact, which is why we include the single layer experiments. We evaluate each model in terms of Top-1 Accuracy, FLOPs, and throughput. Our model achieves top performance across most experiments and remains competitive on others. Experimental Setup: All training and evaluations are conducted on single compute node equipped with 8 NVIDIA A40 GPUs. The experiments are performed on the ImageNet-1K dataset [7], with all images resized to resolution of 224px. We evaluate the proposed method using only frozen pre-trained backbone methods, training the Noise Allocator weights for 40 epochs. And we set β (a hyperparameter controlling the magnitude of added noise) to 0.02 for all base models while training. For testing, single GPU to measure the computational performance. 4.1. Rate Sweep for Single Layer Pruning The experiments of previous studies have shown that most models [9, 17, 21] refrain from dropping tokens in initial Transformer layers, as these layers contain limited information relevant to token importance. For our experiments, we select the second layer for ViT and third layer for DeiT 4 Figure 3. Visualization of Token Pruning maps on ImageNet-1K: at left are the original images, and at each column progressing right are single layer prunings and their associated kept/dropped tokens, for layers 1-5 of the DeiT-B-Distil. model. variants as the standard layers for single-layer token pruning. We use the standard pre-trained DeiT variants for all experiments for all models. For all ViT experiments for all models, we use modified ViT/16 with smaller embedding dimension mlp dim = emb dim = 768. Additionally, we use mean-pooled token embeddings for predictions instead of the CLS token. Plots for pruning performance on DeiT-S-Dist., DeiT-BDist., ViT/16 across different pruning models are presented in Figure 4, for both Top-1 Accuracy versus GFLOPs and Top-1 Accuracy versus throughput (images per second). In general throughput is not one-to-one with GFLOPs due to differing parallelism between models. Performance between all methods converge to the base model case as the keep rate approaches 1.0. For decreasing however, the proposed model shows superior performance until 0.25, at which point baseline models overtake the proposed model, albeit with all models experiencing strong performance degradation. In general our results for the baseline models mirror that of Haurum et al. 2023 [11], and similar to their report we also find that the Top-K baseline is generally the strongest where it can be applied (i.e., for models incorporating the CLS token), aside from our proposed method. Overall, the number of tokens pruned is consistent across models for given keep rate. For TNT, we select top (N + s) at the Noise Allocator stage and prune tokens at the similarity pruning stage for given keep rate and total number of tokens . We set to be 25 and 30 for experiments involving DeiT and ViT, respectively. We apply the same setting to Zero-TP [21], as it also utilizes similaritybased pruning. We could not reproduce the results of Yin et al. [23] in our own experimental context. In Table 1, we report specific numerical results for ViT/16 and DeiT-S-Distl, contingent on keep rates K, instead of GFLOPs or Throughput as in the plots. Complete numerical results and details such as keep rates for each experiment are also provided in the Appendix. For ToMe [5], it is impossible to prune more than 50% of tokens within single layer, so lower 5 Figure 4. Single Layer Pruning results: We plot the Top-1 Accuracy in the ImageNet-1k validation set for each of the pruning methods as function of computational efficiency, in the top row measured by GFLOPs and in the bottom row measured by throughput, for single layer pruning. The base model is DeiT-B-Distil in the first column, DeiT-S-Distil. in the second column, and ViT/16 in the third column. K=1.0 K=0. K=0.6 K=0.5 K=0.25 #tokens=196 #tokens=156 #tokens= #tokens=98 #tokens=49 DeiT-S-Distil. Acc. GFLOPs Acc. GFLOPs Acc. GFLOPs Acc. GFLOPs Acc. GFLOPs Random Drop ToMe [5] Zero-TP [21] DynamicViT [17] Top-K [22] TNT (ours) 80.50 4.63 79.54 80.07 80.00 79.39 80.12 80.11 3.90 3.85 3.91 3.99 3.85 3.90 77.83 77.40 78.94 78.66 78.89 79.29 3.20 3.11 3.21 3.28 3.11 3.20 76.57 73.13 77.74 77.96 77.69 78. 2.87 2.75 2.88 2.94 2.75 2.87 67.77 - 69.99 71.03 68.57 72.66 2.03 - 2.05 2.08 1.86 2.03 #tokens=196 #tokens=160 #tokens= #tokens=100 #tokens=60 ViT/16 Acc. GFLOPs Acc. GFLOPs Acc. GFLOPs Acc. GFLOPs Acc. GFLOPs Random Drop ToMe [5] Zero-TP [21] TNT (ours) 78.70 9.17 77.17 77.71 77.43 77.94 7.69 7.66 7.32 7.70 75.12 75.70 75.96 76.22 6.50 6.42 6.52 6.50 69.21 66.46 70.92 71. 5.33 5.22 5.35 5.33 31.16 - 40.84 41.75 3.81 - 3.83 3.81 Table 1. Top-1 Accuracy on the ImageNet-1K validation set (Acc.) and computational cost (GFLOPs) across methods for differing keep rates K, using single layer of token pruning. The left most column are the performance and computational cost of the base architectures. 6 Figure 5. Multi-layer Pruning results: We plot the Top-1 Accuracy in the ImageNet-1k validation set for each of the pruning methods as function of computational efficiency, in the top row measured by GFLOPs and in the bottom row measured by throughput, for multi-layer pruning. The base model is DeiT-B-Distil in the first column, DeiT-S-Distil. in the second column, and ViT/16 in the third column. single-layer pruning is not possible. 4.2. Rate Sweep at Multi-layer Pruning We evaluate the performance of token pruning across multiple layers as this better reflects performant pruning system, with the caveat that equal comparison conditions are more difficult to ensure. For the token keep rate at each layer and the specific layers chosen for token pruning (i.e. pruning locations), we strictly follow the instructions on the original paper for each model. For Zero-TP [21], tokens are pruned at layers [1, 3, 6, 9, 11], where layers [1, 11] perform similaritybased pruning only; for ToMe [5], pruning occurs at every layer; and for DynamicViT [17], pruning is applied at layers [3, 6, 9]. As Top-K lacks specific pruning instructions, we align its pruning layers with those of TNT. We gradually sweep the keep rate at pruning locations to generate Figure 5 for DeiT-B-Distil., DeiT-S-Distil., and ViT/16. We also provide partial numerical results in Table 2. All base models are the same as those used in the single-layer pruning section. Additional details on experimental setups and specific details for each model are provided in the Appendix. For TNT, we prefer pruning tokens at earlier layers, as it provides stronger performance, and as shown in Figure 3, layers [3, 4, 5] serve as effective locations for token pruning. Rather than performing the similarity pruning stage at every pruning layer as in Zero-TP, we prune tokens before tokens are sent to the ViT blocks. Our ablation study justify this. For all experiments, is set to 40. Our results indicate that TNT consistently shows strong performance in varying token (computation) ranges. As the proportion of pruned tokens increases, the importance of the retained tokens becomes more critical. TNT shows larger performance gap compared to other models in the low-token regime. 4.3. Ablation Study We conducted an ablation experiment to justify inclusion of the similarity-based pruning by random partition, as opposed to token merger or sequential partition. We find slight boost to performance with similarity based pruning, with very minor bias towards the random partition method. Previous works [4, 5] explore token merging, which can be viewed as another form of pruning. For our method, this approach did 7 Acc. GFLOPs TP (imgs/s) Acc. GFLOPs TP (imgs/s) . 4 0 . 2 0 . 8 O s F O Method Deit-B-Distil. [19] 82.55 81.82 Top-K [15] Zero-TP [21] 81.92 DynamicViT [17] 80.91 81.86 ToMe [5] TNT (ours) 81.97 Deit-S-Distil. [19] 80.49 Top-K [15] 79.81 79.66 Zero-TP [21] DynamicViT [17] 79.45 80.12 ToMe [5] 79.89 TNT (ours) 78.70 0 ViT/16 [8] 77.07 Zero-TP [21] 77.03 ToMe [5] TNT (ours) 77.32 Deit-B-Distil. [19] 82.55 55.98 Top-K [15] Zero-TP [21] 19.76 DynamicViT [17] 11.20 43.37 ToMe [5] TNT (ours) 59.93 Deit-S-Distil. [19] 80.49 58.20 Top-K [15] 27.75 Zero-TP [21] DynamicViT [17] 24.51 62.88 ToMe [5] 63.82 TNT (ours) 78.70 0 ViT/16 [8] 0.63 6.30 13.42 Zero-TP [21] ToMe [5] TNT (ours) L s F O 0 . 2 < 0 . 8 < . 4 < g e h H i n T 17.68 13.18 13.10 13.34 12.11 13.11 4.63 3.44 3.42 3.47 3.45 3.41 9.17 6.75 6.97 6.73 17.68 5.93 6.18 5.79 5.89 5.87 4.63 1.57 1.63 1.52 1.55 1.53 9.17 3.26 3.26 2.98 379 504 433 498 508 509 1150 1496 983 1448 1281 1443 644 688 734 842 379 1084 731 1102 995 1095 1150 3003 1485 3037 2486 2973 644 1179 1485 1786 Table 2. Top-1 Accuracy on the ImageNet-1K validation set (Acc.) and computation cost measured by GFLOPs and throughput (TP), for DeiT-B-Distil., DeiT-S-Distil., and ViT/16, in two different computing regimes (High and Low, defined for each base model). not give an improvement over similarity pruning. 4.4. Limitations While the proposed TNT method provides relatively improved performance over other methods, important gaps remain in methodology. In this method, redundant tokens are not removed during training; while this can be performed through merging as in ToMe [5], as experimental results show it is difficult to do in stable manner. Further, synergistic information between tokens is not considered; this seems less relevant to ImageNet classification, but likely would be useful in more complicated label structures (e.g., hierarchical/multi-class structures). This is also perhaps 8 Method Deit-B-Distil. [19](SL) 82.55 TNT w/o Sim. Pruning 81.33 81.52 TNT (Seq. Part.) 81.54 TNT (Token Merge) TNT (Random Part.) 81.57 Deit-B-Distil. [19](ML) 82.55 TNT w/o Sim. Pruning 80.55 81.41 TNT (Token Merge) 81.50 TNT (Random Part.) 17.68 12.29 12.30 12.30 12.30 17.68 11.03 11.41 11.41 379 546 536 538 542 379 605 575 581 Table 3. Ablation study for TNT in DeiT-S-Distil. SL is singlelayer pruning; ML is multi-layer pruning; Acc. is Top-1 Accuracy, TP is the Throughput, measured in images-per-second. deeper problem than redundant tokens, as it has combinatorial complexity in the order of the interactions considered. Beyond this, for better comparison measurements multilayer dropping should be tuned for each method. This is prohibitively expensive, but could provide some less stable but higher capacity methods with boost in performance. Finally, in deployment setting, hardware constraints will dictate keep-rates, which could be optimized for in the base-models directly (i.e., we could optimize ViT for 50% keep rate, or for specific memory structure). This optimization was not done here, nor do we propose any candidate deployment device constraints, but nevertheless should be considered for best performance. 5. Conclusion In this work, we introduced novel token pruning method within the Information Bottleneck framework, aiming to optimize vision transformers by continuous optimization of token pruning. Our extensive evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrate state-of-the-art performance in the accuracy-computation trade-off compared to existing methods. Specifically, our method excels in low-token retention rates, maintaining high accuracy while significantly reducing computational loads. These results underscore the potential impact of our method in improving the efficiency of deploying vision transformers, particularly in resource-constrained applications."
        },
        {
            "title": "References",
            "content": "[1] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE transactions on pattern analysis and machine intelligence, 40(12):28972905, 2018. 3 [2] Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. 2, 3 [3] Richard Blahut. Computation of channel capacity and rate- [18] Naftali Tishby, Fernando Pereira, and William Bialek. arXiv preprint The information bottleneck method. physics/0004057, 2000. 2, 3 [19] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR, 2021. 1, 4, [20] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 1 [21] Hongjie Wang, Bhishma Dedhia, and Niraj Jha. Zerotprune: Zero-shot token pruning through leveraging of the attention graph in pre-trained transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1607016079, 2024. 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 [22] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, and Xing Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 29642972, 2022. 6 [23] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1080910818, 2022. 1, 5 distortion functions. IEEE transactions on Information Theory, 18(4):460473, 1972. [4] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45994603, 2023. 7 [5] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. 2, 4, 5, 6, 7, 8, 1, 9, 10, 11, 12, 13, 14, 15, 16 [6] Thomas Cover. Elements of information theory. John Wiley & Sons, 1999. 3 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 4 [9] Mohsen [8] Alexey Dosovitskiy. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 1, 4, 8 Soroush Abbasi Koohpayegani, Fayyaz, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and Jurgen Gall. Adaptive token sampling for efficient vision transformers. In European Conference on Computer Vision, pages 396414. Springer, 2022. 4 [10] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating bert inference via progressive word-vector elimination. In International Conference on Machine Learning, pages 36903699. PMLR, 2020. 1 [11] Joakim Bruslund Haurum, Sergio Escalera, Graham Taylor, and Thomas Moeslund. Which tokens to use? investigating token reduction in vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 773783, 2023. 2, 4, 5 [12] Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer: Train once with length drop, use anytime with search. arXiv preprint arXiv:2010.07003, 2020. 1 [13] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 784794, 2022. 1 [14] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [15] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. arXiv preprint arXiv:2202.07800, 2022. 1, 2, 8, 5, 9, 11, 12, 13 [16] Lawrence Page. The pagerank citation ranking: Bringing order to the web. Technical report, Technical Report, 1999. 2 [17] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34:1393713949, 2021. 1, 4, 6, 7, 8, 5, 9, 10, 11, 12,"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Code B.1. Base Model: DeiT-Base-Distil. Figure 6 is an implementation of our VisionTransformerWithTNT in PyTorch. B. Additional Experiment Details Full results for plots and tables in the main paper including DeiT-Tiny-Distil. [19]. For all results, Acc. is Top-1 Accuracy, TP is the Throughput, measured in images-per-second. Single-layer pruning settings: is the keep rate. We divide values into two tables: = 0.8 to 0.45 represent the high-token regime, while = 0.4 to 0.2 correspond to the low-token regime. Highlighted rows achieve the highest accuracy in specific token regime. For Top-K [15], GFLOPs is relatively smaller as pruning occurs in the middle of the Transformer block. For clarity in visualizations of the computation-accuracy trade-off, data of Top-K corresponding to K=0.2 is omitted from the plots but is reported in the table. Multi-layer pruning settings: For TNT, Top-K [15], and Zero-TP [21], Loc and Rate denote the pruning locations (layers) and the pruning rate at each layer, respectively. itr is an additional parameter for Zero-TP only, which specifies the number of iterations for the PageRank algorithm at each pruning layer. For ToMe [5], pruning is performed at every layer, with representing the number of tokens pruned per layer. To ensure GFLOPs alignment for better comparison, we only collect 8 data points per base model for ToMe (10 for other models). For DynamicViT [17], the keep rates for the three pruning layers are specified as [ρ, ρ2, ρ3]. Rate, r, and ρ vary across different experiments. Table 4 provides an overview of the fixed hyperparameters used consistently in all experiments. For different base models, we conduct the hyperexperiments using the same set of parameters. Method Top-K [15] Zero-TP [21] DynamicViT [17] ToMe [5] TNT (ours) Param. loc=[3, 4, 5] itr(single-layer)=50 loc=[1, 3, 6, 9, 11] itr(multi-layer)=[30, 5, 5, 1, 1] loc=[3, 6, 9] loc[1:12] loc=[3, 4, 5] Table 4. overview of the fixed hyperparameters used consistently in all experiments. For single-layer pruning, high-token regime results are listed in Table 5; low-token regime results are listed in Table 6. For multi-layer pruning, high-token regime results are listed in Table 7; low-token regime results are listed in Table 8. Acc. GFLOPs TP (imgs/s) Method 82.55 Deit-B-Distil. 81.59 Random Drop 82.16 Top-K [15] Zero-TP [21] 82.25 DynamicViT [17] 80.87 81.86 ToMe [5] 82.31 TNT (ours) 80.79 Random Drop Top-K [15] 81.63 81.74 Zero-TP [21] DynamicViT [17] 80.66 80.80 ToMe [5] 82.05 TNT (ours) 79.59 Random Drop 80.70 Top-K [15] Zero-TP [21] 80.58 DynamicViT [17] 80.02 78.28 ToMe [5] 81.57 TNT (ours) 77.72 Random Drop 79.25 Top-K [15] 78.63 Zero-TP [21] DynamicViT [17] 79.06 71.57 ToMe [5] 80.62 TNT (ours) 76.23 Random Drop 78.06 Top-K [15] Zero-TP [21] 77.15 DynamicViT [17] 78.26 79.85 TNT (ours) 17.68 14.93 14.74 14.95 15.33 14.74 14.93 13.64 13.36 13.67 14.00 13.36 13.64 12.29 11.92 12.32 12.61 11.92 12.30 11.02 10.56 11.05 11.31 10.56 11.03 10.35 9.85 10.38 10.63 10.31 379 - 462 411 438 447 455 - 507 448 481 493 495 - 558 488 526 546 542 - 635 540 588 621 605 - 677 571 624 640 8 . 0 = 7 . 0 = 6 . 0 = 5 . 0 = . 5 4 0 = Table 5. Single-layer pruning for DeiT-B-Distil. in high-token regime. B.2. Base Model: DeiT-Small-Distil. For single-layer pruning, high-token regime results are listed in Table 9; low-token regime results are listed in Table 10. For multi-layer pruning, high-token regime results are listed in Table 11; low-token regime results are listed in Table 12. 1 1 class VisionTransformerWithTNT(VisionTransformer): 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # Parameters introduced: Add alpha heads to produce noise signal term self.alpha_norm = kwargs[norm_layer](self.embed_dim) self.alpha_heads = nn.ModuleList([ nn.Linear(self.embed_dim, 1) for _ in range(kwargs[depth]) ]) self.alpha_heads.apply(self._init_weights) def forward_features(self, x): = x.shape[0] = self.patch_embed(x) cls_tokens = self.cls_token.expand(B, -1, -1) = torch.cat((cls_tokens, x), dim=1) = + self.pos_embed = self.pos_drop(x) for i, (blk, alpha_head) in enumerate(zip(self.blocks, self.alpha_heads)): = blk(x) # Noise allocator: To add noise to token embeddings at 1-5 layers while fine-tuning if self.training and < 5: alpha = alpha_head(x[:, 1:]) alpha = torch.softmax(alpha.squeeze(-1), dim=-1) alpha = 1 - alpha noise = torch.randn_like(x[:, 1:]) * alpha.unsqueeze(-1).repeat(1,1,x.size(-1)) zero_noise = torch.zeros_like(cls_tokens) noise = torch.cat((zero_noise, noise), dim=1) = self.alpha_norm(x) = + 0.02 * noise = self.norm(x) return x[:, 0] def forward(self, x): = self.forward_features(x) = self.head(x) return Figure 6. Python implementation of VisionTransformerWithTNT class. Codes highlighted with brown are the main modifications. VisionTransformer class is taken from https://github.com/rwightman/pytorchimagemodels/blob/master/ timm/models/vision_transformer.py. We make simple modifications to allocate noise to token embeddings while fine-tuning. B.3. Base Model: DeiT-Tiny-Distil. regime results are listed in Table 20. Figure 7 shows the plots for DeiT-Tiny-Distil.. For singlelayer pruning, high-token regime results are listed in Table 13; low-token regime results are listed in Table 14. For multi-layer pruning, high-token regime results are listed in Table 15; low-token regime results are listed in Table 16. C. More examples for visualization More qualitative results for pruning 50% of tokens at varying layers (single-layer pruning at layers 1-5) on the ImageNet1K validation dataset in Figure 8. B.4. Base Model: ViT/ For ViT/16, we use the mean-pooled tokens for the prediction rather than using CLS token. Therefore, Top-K is not applicable. We instead sweep the number of tokens (denoted as #tokens) from 160 to 50. #Tokens from 160 to 110 represent the high-token regime while #tokens from 100 to 50 correspond to low-token regime. For single-layer pruning, high-token regime results are listed in Table 17; low-token regime results are listed in Table 18. For multi-layer pruning, high-token regime results are listed in Table 19; low-token 2 Figure 7. Experimental Results for DeiT-Tiny-Distil.: We plot the Top-1 Accuracy in the ImageNet-1k validation set for each of the pruning methods as function of computational efficiency, in the top row measured by GFLOPs and in the bottom row measured by throughput, for multi-layer pruning. For weaker encoders and/or smaller transformer blocks such as DeiT-Tiny, we rather use MLP, which means less parallelism. The throughput of models in multi-layer pruning reaches approximately 5000 images per second, potentially limited by hardware bottlenecks. 3 Figure 8. More Visualization of Token Pruning maps on ImageNet-1K: at left are the original images, and at each column progressing right are single layer prunings and their associated kept/dropped tokens, for layers 1-5 of the DeiT-S-Distil. model. Acc. GFLOPs TP (imgs/s) Method 82.55 Deit-B-Distil. 74.08 Random Drop 76.53 Top-K [15] Zero-TP [21] 75.15 DynamicViT [17] 77.01 78.89 TNT (ours) 70.57 Random Drop 74.26 Top-K [15] 72.32 Zero-TP [21] DynamicViT [17] 75.28 77.22 TNT (ours) 64.89 Random Drop 70.68 Top-K [15] Zero-TP [21] 68.68 DynamicViT [17] 72.69 74.75 TNT (ours) 57.12 Random Drop 65.95 Top-K [15] Zero-TP [21] 63.62 DynamicViT [17] 68.87 70.76 TNT (ours) 44.17 Random Drop 57.51 Top-K [15] Zero-TP [21] 55.28 DynamicViT [17] 61.86 62.62 TNT (ours) 17.68 9.70 9.14 9.72 9.95 9.70 9.04 8.43 9.06 9.27 9.04 8.38 7.73 8.41 8.59 8.38 7.80 7.10 7.82 7.99 7.80 7.14 6.40 7.17 7.32 7.15 379 - 728 603 668 684 - 786 649 710 723 - 853 680 770 786 - 925 717 823 834 - 1016 779 904 907 4 . 0 = 5 3 . 0 = 3 . 0 = 5 2 . 0 = 2 . 0 = Table 6. Single-layer pruning for DeiT-B-Distil. in low-token regime. Acc. GFLOPs TP (imgs/s) Method 82.55 Deit-B-Distil. 81.82 Top-K Zero-TP [21] 81.92 DynamicViT [17] 80.91 81.86 ToMe [5] 81.97 TNT (ours) Top-K 80.96 80.97 Zero-TP [21] DynamicViT [17] 80.67 81.71 ToMe [5] 81.50 TNT (ours) 79.63 Top-K Zero-TP [21] 78.77 DynamicViT [17] 79.55 80.51 ToMe [5] 80.56 TNT (ours) 76.84 Zero-TP [21] 75.60 DynamicViT [17] 76.73 78.96 TNT (ours) 75.04 Zero-TP [21] 73.32 DynamicViT [17] 73.71 77.31 TNT (ours) 17.68 13.18 13.10 13.34 12.11 13.11 11.72 11.37 11.49 11.56 11.41 10.25 9.71 9.88 9.39 10.01 8.75 8.61 8.57 8.77 8.14 8.22 7.96 8.03 379 503 433 498 508 509 581 424 576 533 581 650 573 659 651 660 748 613 761 758 814 636 817 822 5 Top-K 0 Top-K 0 . 3 1 O 5 . 1 1 L 0 . 0 1 L . 8 O . 8 L Param. - Rate=[.9, .9, .8] Rate=[1., .9, .9, .9, 1.] ρ=.8 r=8 Rate=[1., .95, .95] Rate=[.85, .8, .8] Rate=[1., .8, .8, .9, 1.] ρ=.7 r=11 Rate=[.9, .9, .9] Rate=[.85, .7, .7] Rate=[1., .7, .7, .8, 1.] ρ=.6 r=15 Rate=[.85, .85, .8] Rate=[.7, .7, .65] Rate=[1., .6, .7, .7, 1.] ρ=.5 Rate=[.75, .85, .7] Rate=[.65, .65, .65] Rate=[1., .6, .6, .7, 1.] ρ=.45 Rate=[.65, .85, .7] Table 7. Multi-layer pruning for DeiT-B-Distil. in high-token regime. Acc. GFLOPs TP (imgs/s) Method 82.55 Deit-B-Distil. 71.59 Top-K Zero-TP [21] 69.66 DynamicViT [17] 68.79 72.29 ToMe [5] 75.33 TNT (ours) 67.51 Top-K Zero-TP [21] 55.65 DynamicViT [17] 60.30 68.53 ToMe [5] 71.92 TNT (ours) 65.24 Top-K 43.24 Zero-TP [21] DynamicViT [17] 45.65 64.33 ToMe [5] 69.06 TNT (ours) 61.54 Top-K Zero-TP [21] 39.17 DynamicViT [17] 29.87 51.32 ToMe [5] 66.21 TNT (ours) 55.98 Top-K Zero-TP [21] 19.76 DynamicViT [17] 11.20 43.37 ToMe [5] 59.93 TNT (ours) 17.68 7.37 7.76 7.45 7.23 7.49 6.84 7.03 6.97 6.91 6.89 6.56 6.79 6.53 6.62 6.53 6.29 6.37 6.17 6.11 6.28 5.93 6.18 5.79 5.89 5.87 379 881 654 877 823 871 948 703 926 855 947 993 735 982 893 991 1044 762 1043 962 1039 1084 731 1102 995 1095 Param. - Rate=[.6, .6, .6] Rate=[1., .5, .5, .6, 1.] ρ=.4 r=20 Rate=[.6, .8, .7] Rate=[.6, .5, .6] Rate=[1., .4, .4, .4, 1.] ρ=.35 r=21 Rate=[.5, .8, .7] Rate=[.55, .5, .6] Rate=[1., .4, .3, .4, 1.] ρ=.3 r=22 Rate=[.5, .7, .7] Rate=[.55, .5, .5] Rate=[1., .3, .4, .4, 1.] ρ=.25 r=24 Rate=[.5, .7, .6] Rate=[.5, .45, .5] Rate=[1., .3, .3, .4, 1.] ρ=.2 r=25 Rate=[.5, .6, .55] 5 . 7 L 0 . 7 L 6 . 6 L 2 . 6 L 7 . 5 L Table 8. Multi-layer pruning for DeiT-B-Distil. in low-token regime. 7 Acc. GFLOPs TP (imgs/s) Method 80.49 Deit-S-Distil. 74.57 Random Drop 75.45 Top-K [15] Zero-TP [21] 75.91 DynamicViT [17] 76.40 77.32 TNT (ours) 72.79 Random Drop 73.87 Top-K [15] 74.58 Zero-TP [21] DynamicViT [17] 75.22 76.19 TNT (ours) 70.42 Random Drop 71.60 Top-K [15] Zero-TP [21] 72.70 DynamicViT [17] 73.41 74.65 TNT (ours) 67.77 Random Drop 68.57 Top-K [15] Zero-TP [21] 69.99 DynamicViT [17] 71.03 72.66 TNT (ours) 62.83 Random Drop 64.12 Top-K [15] Zero-TP [21] 65.90 DynamicViT [17] 66.67 69.08 TNT (ours) 1150 - 2143 1605 1934 1994 - 2342 1679 2091 2167 - 2465 1711 2189 2269 - 2713 1757 2376 2444 - 2842 1797 2496 4.63 2.52 2.38 2.54 2.58 2.52 2.35 2.20 2.37 2.41 2.35 2.18 2.02 2.20 2.24 2.19 2.03 1.86 2.05 2.08 2.03 1.87 1.68 1.88 1.91 1.87 4 . 0 = 5 3 . 0 = 3 . 0 = 5 2 . 0 = 2 . 0 = Table 10. Single-layer pruning for DeiT-S-Distil. in low-token regime. Acc. GFLOPs TP (imgs/s) Method 80.49 Deit-S-Distil. 79.54 Random Drop 80.12 Top-K [15] Zero-TP [21] 79.99 DynamicViT [17] 79.39 80.07 ToMe [5] 80.11 TNT (ours) 78.91 Random Drop 79.69 Top-K [15] Zero-TP [21] 79.55 DynamicViT [17] 79.25 79.24 ToMe [5] 79.82 TNT (ours) 77.83 Random Drop 78.89 Top-K [15] Zero-TP [21] 78.94 DynamicViT [17] 78.66 77.40 ToMe [5] 79.29 TNT (ours) 76.57 Random Drop 77.69 Top-K [15] Zero-TP [21] 77.74 DynamicViT [17] 77.96 73.13 ToMe [5] 78.65 TNT (ours) 75.61 Random Drop 76.70 Top-K [15] 76.97 Zero-TP [21] DynamicViT [17] 77.22 78.06 TNT (ours) 1150 - 1338 1156 1255 1290 1316 - 1478 1243 1381 1415 1427 - 1655 1343 1528 1596 1587 - 1854 1463 1706 1791 1768 - 1995 1514 1800 1878 4.63 3.90 3.85 3.91 3.99 3.85 3.90 3.55 3.48 3.57 3.64 3.49 3.56 3.20 3.11 3.21 3.28 3.11 3.20 2.87 2.75 2.88 2.94 2.75 2.87 2.69 2.57 2.71 2.76 2. 8 . 0 = 7 . 0 = 6 . 0 = 5 . 0 = 5 4 . 0 = Table 9. Single-layer pruning for DeiT-S-Distil. in high-token regime. 8 0 . 3 L P G 5 4 . 3 Acc. GFLOPs TP (imgs/s) Method 80.49 Deit-S-Distil. 79.81 Top-K [15] Zero-TP [21] 79.66 DynamicViT [17] 79.45 80.12 ToMe [5] 79.89 TNT (ours) Top-K [15] 79.11 78.75 Zero-TP [21] DynamicViT [17] 79.18 79.86 ToMe [5] 79.38 TNT (ours) 77.65 Top-K [15] Zero-TP [21] 76.92 DynamicViT [17] 78.38 79.12 ToMe [5] 78.57 TNT (ours) 75.33 5 Top-K [15] Zero-TP [21] 74.35 DynamicViT [17] 76.39 77.25 TNT (ours) 73.73 1 Top-K [15] Zero-TP [21] 72.55 DynamicViT [17] 74.41 75.93 TNT (ours) 1150 1496 983 1448 1281 1444 1706 1171 1679 1448 1652 1909 1352 1929 1754 1882 2190 1389 2192 2143 2343 1415 2321 2302 4.63 3.44 3.42 3.47 3.45 3.41 3.06 2.97 2.99 3.02 2.97 2.67 2.54 2.57 2.54 2.60 2.29 2.25 2.23 2.28 2.13 2.16 2.08 2.09 L P G O 6 . 2 2 . 2 . 2 Param. - Rate=[.9, .9, .8] Rate=[1., .9, .9, .9, 1.] ρ=.8 r=8 Rate=[1., .95, .95] Rate=[.85, .8, .8] Rate=[1., .8, .8, .9, 1.] ρ=.7 r=11 Rate=[.9, .9, .9] Rate=[.85, .7, .7] Rate=[1., .7, .7, .8, 1.] ρ=.6 r=15 Rate=[.85, .85, .8] Rate=[.7, .7, .65] Rate=[1., .6, .7, .7, 1.] ρ=.5 Rate=[.75, .85, .7] Rate=[.65, .65, .65] Rate=[1., .6, .6, .7, 1.] ρ=.45 Rate=[.65, .85, .7] Table 11. Multi-layer pruning for DeiT-S-Distil. in high-token regime. 9 Acc. GFLOPs TP (imgs/s) Method 80.49 Deit-S-Distil. 70.78 Top-K Zero-TP [21] 69.48 DynamicViT [17] 71.69 74.76 ToMe [5] 74.54 TNT (ours) 67.34 Top-K Zero-TP [21] 59.29 DynamicViT [17] 67.03 71.50 ToMe [5] 72.26 TNT (ours) 65.55 Top-K 49.70 Zero-TP [21] DynamicViT [17] 58.70 69.76 ToMe [5] 70.31 TNT (ours) 62.41 Top-K Zero-TP [21] 48.44 DynamicViT [17] 47.71 66.30 ToMe [5] 68.36 TNT (ours) 58.20 Top-K Zero-TP [21] 27.75 DynamicViT [17] 24.51 62.88 ToMe [5] 63.82 TNT (ours) 1150 2525 1357 2490 2176 2484 2701 1331 2635 2351 2639 2794 1471 2761 2377 2725 2896 1481 2943 2357 2850 3003 1485 3037 2486 2973 4.63 1.93 2.03 1.95 1.90 1.95 1.80 1.85 1.83 1.74 1.79 1.73 1.79 1.71 1.68 1.70 1.66 1.68 1.62 1.61 1.64 1.57 1.63 1.52 1.55 1.53 Param. - Rate=[.6, .6, .6] Rate=[1., .5, .5, .6, 1.] ρ=.4 r=20 Rate=[.6, .8, .7] Rate=[.6, .5, .6] Rate=[1., .4, .4, .4, 1.] ρ=.35 r=21 Rate=[.5, .8, .7] Rate=[.55, .5, .6] Rate=[1., .4, .3, .4, 1.] ρ=.3 r=22 Rate=[.5, .7, .7] Rate=[.55, .5, .5] Rate=[1., .3, .4, .4, 1.] ρ=.25 r=24 Rate=[.5, .7, .6] Rate=[.5, .45, .5] Rate=[1., .3, .3, .4, 1.] ρ=.2 r=25 Rate=[.5, .6, .55] 5 9 . 1 O 8 . 1 L 5 7 . 1 L 5 6 . 1 O 5 5 . 1 L Table 12. Multi-layer pruning for DeiT-S-Distil. in low-token regime. 10 Acc. GFLOPs TP (imgs/s) Method 74.05 Deit-T-Distil. 61.88 Random Drop 63.27 Top-K [15] Zero-TP [21] 63.29 DynamicViT [17] 66.26 TNT+MLP (ours) 66.52 57.99 Random Drop 60.13 Top-K [15] 60.21 Zero-TP [21] DynamicViT [17] 63.85 TNT+MLP (ours) 63.89 52.65 Random Drop 55.79 Top-K [15] Zero-TP [21] 56.20 DynamicViT [17] 61.03 TNT+MLP (ours) 60.08 46.18 Random Drop 50.67 Top-K [15] Zero-TP [21] 51.39 DynamicViT [17] 57.03 TNT+MLP (ours) 54.98 37.29 Random Drop 43.40 Top-K [15] Zero-TP [21] 44.04 DynamicViT [17] 50.62 TNT+MLP (ours) 46. 2607 - 4814 2864 4293 3267 - 5103 2728 4510 3346 - 5242 2673 4676 3512 - 5108 2598 5016 3477 - 5153 2694 5152 3725 1.27 0.68 0.65 0.69 0.70 0.70 0.64 0.60 0.64 0.65 0.65 0.59 0.55 0.60 0.60 0.61 0.55 0.51 0.56 0.56 0.57 0.51 0.46 0.51 0.52 0.52 4 . 0 = 5 3 . 0 = 3 . 0 = 5 2 . 0 = 2 . 0 = Table 14. Single-layer pruning for DeiT-T-Distil. in low-token regime. Acc. GFLOPs TP (imgs/s) Method 74.05 Deit-T-Distil. 72.36 Random Drop 73.05 Top-K [15] Zero-TP [21] 73.25 DynamicViT [17] 71.59 ToMe [5] 72.99 TNT+MLP (ours) 73.46 71.17 Random Drop 71.96 Top-K [15] Zero-TP [21] 72.03 DynamicViT [17] 71.15 ToMe [5] 71.06 TNT+MLP (ours) 72.77 69.23 Random Drop 70.28 Top-K [15] Zero-TP [21] 69.99 DynamicViT [17] 70.36 ToMe [5] 66.56 TNT+MLP (ours) 71.70 66.50 Random Drop 67.74 Top-K [15] Zero-TP [21] 67.31 DynamicViT [17] 68.82 57.52 ToMe [5] TNT+MLP (ours) 70.09 64.43 Random Drop 65.78 Top-K [15] 65.54 Zero-TP [21] DynamicViT [17] 67.69 TNT+MLP (ours) 68.43 2607 - 3023 2319 2805 2872 2938 - 3295 2438 3031 3120 2939 - 3815 2677 3476 3624 3142 - 4270 2630 3851 4043 3210 - 4484 2519 4016 1.27 1.06 1.04 1.06 1.08 1.05 1.07 0.96 0.94 0.97 0.98 0.94 0.98 0.86 0.84 0.87 0.88 0.84 0.88 0.77 0.74 0.78 0.79 0.75 0.79 0.73 0.69 0.73 0.74 0.74 8 . 0 = 7 . 0 = 6 . 0 = 5 . 0 = 5 4 . 0 = Table 13. Single-layer pruning for DeiT-T-Distil. in low-token regime. 11 Acc. GFLOPs TP (imgs/s) Method 74.05 Deit-T-Distil. 72.46 Top-K [15] Zero-TP [21] 72.45 DynamicViT [17] 71.61 ToMe [5] 73.52 TNT+MLP (ours) 72.82 Top-K [15] 70.88 70.36 Zero-TP [21] DynamicViT [17] 70.82 ToMe [5] 72.91 TNT+MLP (ours) 71.83 68.18 Top-K [15] Zero-TP [21] 66.24 DynamicViT [17] 68.63 ToMe [5] 71.19 TNT+MLP (ours) 70.36 63.80 Zero-TP [21] 60.48 DynamicViT [17] 64.55 TNT+MLP (ours) 67.45 61.17 Zero-TP [21] 56.84 DynamicViT [17] 60.34 TNT+MLP (ours) 64.48 2607 3416 2048 3246 2424 2947 3799 2219 3668 2401 3470 4269 2264 4220 2368 3879 4751 2367 4715 4332 5092 2380 4953 1.27 0.93 0.93 0.94 0.94 0.96 0.83 0.80 0.80 0.82 0.83 0.72 0.69 0.69 0.67 0.73 0.62 0.61 0.60 0.64 0.59 0.59 0.56 0.59 Param. - Rate=[.9, .9, .8] Rate=[1., .9, .9, .9, 1.] ρ=.8 r=8 Rate=[1., .95, .95] Rate=[.85, .8, .8] Rate=[1., .8, .8, .9, 1.] ρ=.7 r=11 Rate=[.9, .9, .9] Rate=[.85, .7, .7] Rate=[1., .7, .7, .8, 1.] ρ=.6 r=15 Rate=[.85, .85, .8] Rate=[.7, .7, .65] Rate=[1., .6, .7, .7, 1.] ρ=.5 Rate=[.75, .85, .7] Rate=[.65, .65, .65] Rate=[1., .6, .6, .7, 1.] ρ=.45 Rate=[.65, .85, .7] 3 9 . 0 L 2 8 . 0 L 7 . 0 L 6 . 0 L 5 . 0 L 8 Top-K [15] 2 Top-K [15] Table 15. Multi-layer pruning for DeiT-T-Distil. in high-token regime. 12 Acc. GFLOPs TP (imgs/s) Method 74.05 Deit-T-Distil. 56.15 Top-K [15] Zero-TP [21] 51.41 DynamicViT [17] 55.20 61.20 ToMe [5] TNT+MLP (ours) 61.59 51.39 Top-K [15] Zero-TP [21] 35.26 DynamicViT [17] 47.87 ToMe [5] 53.77 TNT+MLP (ours) 56.85 48.70 Top-K [15] 24.42 Zero-TP [21] DynamicViT [17] 36.48 ToMe [5] 51.15 TNT+MLP (ours) 53.58 44.47 Top-K [15] Zero-TP [21] 22.97 DynamicViT [17] 24.86 ToMe [5] 44.34 TNT+MLP (ours) 50.60 39.54 Top-K [15] Zero-TP [21] 6.97 DynamicViT [17] 9.65 39.05 ToMe [5] TNT+MLP (ours) 44.41 2607 4933 2280 4790 2386 4294 4987 2355 4448 2382 4314 5064 2298 4823 2366 4368 5031 2339 4823 2369 4421 4919 2374 4292 2382 1.27 0.53 0.56 0.53 0.52 0.55 0.49 0.51 0.50 0.48 0.51 0.47 0.49 0.47 0.46 0.48 0.46 0.46 0.44 0.44 0.46 0.43 0.45 0.42 0.43 0.44 Param. - Rate=[.6, .6, .6] Rate=[1., .5, .5, .6, 1.] ρ=.4 r=20 Rate=[.6, .8, .7] Rate=[.6, .5, .6] Rate=[1., .4, .4, .4, 1.] ρ=.35 r=21 Rate=[.5, .8, .7] Rate=[.55, .5, .6] Rate=[1., .4, .3, .4, 1.] ρ=.3 r=22 Rate=[.5, .7, .7] Rate=[.55, .5, .5] Rate=[1., .3, .4, .4, 1.] ρ=.25 r=24 Rate=[.5, .7, .6] Rate=[.5, .45, .5] Rate=[1., .3, .3, .4, 1.] ρ=.2 r=25 Rate=[.5, .6, .55] 3 5 . 0 L 5 . 0 L 7 4 . 0 L 5 4 . 0 L 3 4 . 0 L Table 16. Multi-layer pruning for DeiT-T-Distil. in low-token regime. 13 #Tokens Method Acc. GFLOPs TP (imgs/s) ViT/16 78.70 Random Drop 77.17 Zero-TP [21] 77.43 77.71 ToMe [5] TNT (ours) 77.94 Random Drop 76.75 Zero-TP [21] 76.80 77.22 ToMe [5] TNT (ours) 77.64 Random Drop 76.00 Zero-TP [21] 76.80 76.53 ToMe [5] TNT (ours) 76.97 Random Drop 75.12 Zero-TP [21] 75.96 75.70 ToMe [5] TNT (ours) 76.22 Random Drop 73.79 Zero-TP [21] 74.81 ToMe [5] 74.19 75.24 TNT (ours) Random Drop 71.95 Zero-TP [21] 73.26 71.54 ToMe [5] 73.73 TNT (ours) 644 - 665 752 747 - 687 784 779 - 722 838 820 - 749 877 856 - 812 969 945 - 863 1047 1016 9.17 7.69 7.32 7.66 7.70 7.29 6.92 7.24 7.30 6.89 6.92 6.83 6.90 6.50 6.52 6.42 6.50 6.10 6.13 6.02 6.11 5.71 5.74 5.62 5. Acc. GFLOPs TP (imgs/s) ViT/16 78.70 Random Drop 69.21 Zero-TP [21] 70.92 66.46 ToMe [5] TNT (ours) 71.69 Random Drop 64.71 Zero-TP [21] 67.40 68.73 TNT (ours) Random Drop 57.64 Zero-TP [21] 61.76 TNT (ours) 63.87 Random Drop 46.28 Zero-TP [21] 53.15 TNT (ours) 55.19 Random Drop 31.16 Zero-TP [21] 40.84 TNT (ours) 41.75 Random Drop 16.57 Zero-TP [21] 27.64 27.40 TNT (ours) 644 - 901 1103 1069 - 960 1153 - 1002 1247 - 1021 1344 - 1157 1457 - 1228 1531 9.17 5.33 5.35 5.22 5.33 4.94 4.97 4.95 4.56 4.59 4.56 4.18 4.21 4.19 3.81 3.83 3.81 3.44 3.46 3.44 160 150 130 120 110 100 90 70 60 50 Table 17. Single-layer pruning for ViT/16. in high-token regime. #Tokens Method Table 18. Single-layer pruning for ViT/16. in low-token regime. 14 . 6 L 0 . 6 L 0 . 5 O 3 4 . 4 L 0 1 . 4 L Method Acc. GFLOPs TP (imgs/s) Param. ViT/16 78.70 9.17 8 Zero-TP [21] 77. 6.75 ToMe [5] 77.02 6.97 TNT (ours) 77. 6.73 0 Zero-TP [21] 75.16 5.84 ToMe [5] 77.02 6. TNT (ours) 75.47 5.84 0 Zero-TP [21] 69.91 4.98 ToMe [5] 70.95 5.06 644 688 734 778 823 967 885 989 - Rate=[1., .9, .9, .9, 1.] r=8 Rate=[1., .95, .95] Rate=[1., .8, .8, .9, 1.] r=11 Rate=[.9, .9, .9] Rate=[1., .7, .7, .8, 1.] r=15 TNT (ours) 72.23 5.10 Rate=[.85, .85, .8] Zero-TP [21] 60.39 4.41 966 Rate=[1., .6, .7, .7, 1.] TNT (ours) 65.81 4.46 1249 Rate=[.75, .85, .7] Zero-TP [21] 51.96 4. 994 Rate=[1., .6, .6, .7, 1.] TNT (ours) 58.96 4.08 Rate=[.65, .85, .7] Table 19. Multi-layer pruning for ViT/16. in high-token regime. 15 Method Acc. GFLOPs TP (imgs/s) Param. ViT/16 78.70 0 Zero-TP [21] 40.38 ToMe [5] 41.78 TNT (ours) 51.40 0 Zero-TP [21] 13.86 ToMe [5] 22.58 TNT (ours) 39. 0 Zero-TP [21] 5.15 ToMe [5] 17.74 TNT (ours) 30.86 0 Zero-TP [21] 4. ToMe [5] 9.19 TNT (ours) 23.71 0 Zero-TP [21] 0.64 ToMe [5] 6.30 TNT (ours) 13.42 9 . 3 L 6 . 3 O 4 . 3 L 2 . 3 L 1 . 3 O 9.17 3.98 3.96 3.80 3. 3.64 3.50 3.48 3.51 3.31 3. 3.37 3.18 3.17 3.26 2.98 1035 1243 1452 1092 1340 1102 1382 1626 1163 1446 1179 1485 1786 - Rate=[1., .6, .6, .7, 1.] r= Rate=[.65, .85, .7] Rate=[1., .5, .5, .6, 1.] r=21 Rate=[.5, .8, .7] Rate=[1., .4, .3, .4, 1.] r= Rate=[.5, .7, .7] Rate=[1., .3, .4, .4, 1.] r=24 Rate=[.5, .7, .6] Rate=[1., .3, .3, .4, 1.] r= Rate=[.5, .6, .55] Table 20. Multi-layer pruning for ViT/16. in low-token regime."
        }
    ],
    "affiliations": [
        "Vanderbilt University, Nashville, TN 37235, USA"
    ]
}