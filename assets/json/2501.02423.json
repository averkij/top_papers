{
    "paper_title": "Scaling Laws for Floating Point Quantization Training",
    "authors": [
        "Xingwu Sun",
        "Shuaipeng Li",
        "Ruobing Xie",
        "Weidong Han",
        "Kan Wu",
        "Zhen Yang",
        "Yixing Li",
        "An Wang",
        "Shuai Li",
        "Jinbao Xue",
        "Yu Cheng",
        "Yangyu Tao",
        "Zhanhui Kang",
        "Chengzhong Xu",
        "Di Wang",
        "Jie Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits."
        },
        {
            "title": "Start",
            "content": "Scaling Laws for FloatingPoint Quantization Training Xingwu Sun * 1 2 Shuaipeng Li * 1 Ruobing Xie 1 Weidong Han 1 Kan Wu 1 Zhen Yang 1 Yixing Li 1 3 An Wang 1 4 Shuai Li 1 Jinbao Xue 1 Yu Cheng 3 Yangyu Tao 1 Zhanhui Kang 1 Chengzhong Xu 2 Di Wang 1 Jie Jiang 1 5 2 0 2 5 ] . [ 1 3 2 4 2 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits. 1. Introduction The various scaling laws of large language models (LLMs) could help developers effectively select superior parame- *Equal contribution Corresponding author 1Tencent Hunyuan 2University of Macau 3The Chinese University of Hong Kong 4Tokyo Institute of Technology. ter settings before experiments and accurately predict the model performance under different configurations, which are regarded as excellent guidance in LLM training. The widely acknowledged scaling law efforts such as Kaplan et al. (2020), Hoffmann et al. (2022), and Li et al. (2024) mainly concentrate on the central factors, i.e., model size and trained token size, which significantly impact the performance of LLMs. With the rapid growth of both model and data sizes, there has been increasing attention to the efficiency and cost of LLM training. Training and serving with lower precision becomes popular solution. Currently, lots of representative LLMs are trained in BF16 and even lower precision (Dubey et al., 2024; Sun et al., 2024; Liu et al., 2024; Yang et al., 2024; Ma et al., 2024; Wang et al., 2023; Peng et al., 2023), aiming to balance effectiveness and efficiency. Compared to integer quantization, floatingpoint (FP) quantization can better maintain LLMs accuracy at extremely lower bit rates and thus is often equipped in low-precision LLMs. Therefore, exploring the scaling laws of LLM performance under different low precision settings with floating-point quantization becomes essential to shed light on future low-precision LLM training. Recently, there is pioneer work that conducts in-depth analyses and explorations on the LLMs scaling laws for precision in both training and inference (Kumar et al., 2024), quantitatively measuring the degradation rules of post-train quantization and quantized training. This scaling law provides an appropriate conclusion explaining the potential damage of excessively increasing training data to low-precision LLMs performance. However, Kumar et al. (2024) directly adopts the bit width as the precision in its low-precision scaling laws, which may lose finer-grained modeling of the relationship between various parameter settings related to the FP quantization and the final loss of LLMs. In practice, the key factors of FP quantization such as the exponent, mantissa, and the block size of scaling factors may have different impacts on the final loss. more comprehensive, precise, and practical scaling law for float quantized training related to the data size (D), model size (N), exponent (E), mantissa (M), and block size of scaling factors (B) is urgently desired. Our work concentrates on establishing, verifying, and analyzing the scaling law for float quantized training in LLMs. Scaling Laws for FloatingPoint Quantization Training At the beginning, we first predict the model performance via the precision-related scaling law from previous work under different data/model sizes and precision settings. Surprisingly, we discover that the predictive performance was not perfectly satisfactory under different float quantized training settings. Subsequently, we carefully design comprehensive set of explorations with experiments of different precision settings (training 366 models), exploring the basic scaling law formation, as well as the potential impact of the quantization targets, exponent and mantissa, and block sizes on the loss. Finally, we aggregate these factors to get our final scaling law for float quantized training with valuable insights to guide the LLM training under low precision. Our scaling law is formulated as follows: L(N, D, E, M, B) = + Dβ + ϵ α + Dβ α log2 γ(E + 0.5)δ(M + 0.5)ν . (1) The first two factors and indicate the data size and model size respectively, which show the main impacts on training loss given by the key factors of data and model size similar with the Chinchilla scaling law (Hoffmann et al., 2022). ϵ represents the bias. The last factor could be regarded as the additional negative impact deriving from low precision training, where Dβ α implies certain form of knowledge intensity in LLM, and log2 B, (E + 0.5)δ, and (M + 0.5)ν jointly reflect the low precision information loss of float quantized training. We have conducted extensive fitting experiments with various possible scaling law formulations to ensure the accuracy and simplicity of our scaling laws. Note that the exponential hyper-parameters α and β of model and data sizes are exactly the same as those in the first two factors. The product of the above knowledge intensity and low precision information loss forms the last factor. Figure 1 and Figure 8 illustrates the fitting results of our scaling law compared with other existing scaling laws, demonstrating our advantages on predicting LLM performances under different float quantized training settings. Throughout our experiments and analyses related to our scaling law, we also discover the following observations and insights that could facilitate future low-precision LLM training: (a) It has been discovered that the impact of quantized weights on the performance is relatively minor during both forward and backward computations. Meanwhile, activations demonstrate higher degree of quantization tolerance specifically when computing gradients pertaining to themselves. (b) The data size of LLM pre-training cannot be added indefinitely without harming the performance under low precision, while large model sizes, higher precision settings (measured by exponent and mantissa), and smaller block sizes could increase the extreme point of effective trained tokens for LLM 2 training. (c) Intuitively, the negative impact of low-precision training in LLMs will be proportionally amplified with the knowledge intensity. (d) The exponent and mantissa have their optimal settings under different bit widths. Exponent bits contribute slightly more to the model performance than mantissa bits. (e) The optimal floating-point quantization precision exhibits direct proportionality with computational power. Nonetheless, across broad spectrum of computational power, our estimated optimal cost-performance precision resides within the 4-8 bits range. 2. Preliminary Classical Scaling Laws. Scaling laws have become fundamental framework for understanding the relationship between essential factors such as model size (N), data size (D), and the resulting loss (L) in deep learning. Two classical scaling laws have been widely recognized in the industry: the Chinchilla scaling law (Hoffmann et al., 2022) and the OpenAI scaling law (Kaplan et al., 2020). The Chinchilla scaling law is expressed as: L(N, D) = α + Dβ + ϵ. On the other hand, the OpenAI scaling law is given by: L(N, D) = (cid:17) α β (cid:20)(cid:16) + (cid:21)β + ϵ, (2) (3) where n, d, α, β, and ϵ are positive fitted constants. The balance between and emerges as critical for computeoptimal training. Scaling Laws for Precision. Subsequent research extends this framework by incorporating the role of precision in quantized training and inference, providing insights into how precision affects model performance. In Kumar et al. (2024), the precision-aware scaling laws are introduced, capturing the trade-offs between model size , data size D, and precision . For integer quantized training, they propose the tradeoff between weight and weight precision as: Neff(N, ) = (1 eP/γ), (4) where Neff indicates the effective parameter count of models, and γ is constant representing the sensitivity of model weights to precision. Incorporating Neff into the Chinchilla scaling law yields: L(N, D, ) = [N (1 eP/γ)]α + Dβ + ϵ. (5) This framework highlights that reducing weight precision can be compensated by increasing the parameter count to maintain performance, which is critical insight for low-precision model optimization. Scaling Laws for FloatingPoint Quantization Training Current Scaling Laws cannot Well Fit in Floating-point Quantization. While most prior work has focused on integer quantized training, floating-point quantization is more prevalent in real-world applications due to its hardware compatibility and finer granularity. For instance, formats such as FP16 and BF16 are standard in many large-scale training pipelines, and emerging formats like FP8 and FP4 are gaining traction. Despite this, scaling laws specifically tailored to floating-point quantization are still largely unexplored. The primary distinction between floating-point and integer quantization lies in the allocation and usage of bits. Floating-point numbers allocate bits to represent both the exponent and the mantissa, with each set of bits serving distinct purposes: the exponent mainly captures dynamic range, while the mantissa mainly encodes precision within that range. In contrast, integer formats uniformly distribute all bits to refine the quantization lattice, providing consistent resolution across the representable range. This fundamental difference highlights the need for dedicated scaling laws for the unique characteristics of floating-point formats. Kumar et al. (2024) hypothesizes that the exponent and mantissa bits are scaled jointly (i.e., increase together as total bit count does). Then, in floating-point formats, precision is determined by the exponent and mantissa , with the total precision: = + + 1. (6) By substituting in their precision-aware scaling law, we have: L(N, D, E, ) = (1 1+E+M γ (cid:104) ) (cid:105)α + Dβ + ϵ, (7) However, upon conducting experiments and applying this scaling law to fit empirical results, particularly for lowprecision training regimes, we observed significant deviations between the laws predictions and actual performance, as illustrated in Figure 1. The unsatisfactory fit, especially for training results using low-bit floating-point formats, suggests that the previous proposed relationship in Kumar et al. (2024) does not adequately capture the nuanced dynamic impacts of floating-point quantization on LLM performance. In this work, we address these shortcomings by re-deriving the scaling law for floating-point quantized training. Our re-derivation incorporates more nuanced understanding of how the finer factors of exponent, mantissa, and block size affect low-precision training. By refining the theoretical framework and aligning it more closely with observed behaviors, we aim to establish more accurate and predictive scaling law that bridges the gap between theoretical insights and real-world applications. 3. Setup and Scaling Laws 3.1. Method and Implementation 3.1.1. QUANTIZATION METHOD We quantize tensor into low-precision floating-point format, following the IEEE 754 standard (Kahan, 1996), which includes both normal and subnormal representations. The format consists of sign bit, exponent bits and mantissa bits. To expand the dynamic range, the special bits are adopted for normal values instead of representing Infinity and Not Number (NaN). Since the modern hardware does not support arbitrary floating-point format, we simulate them using QPyTorch (Zhang et al., 2019) with nearest rounding. Due to the narrow dynamic range and low representation precision of the low-precision format, we employ scaling techniques (Sun et al., 2019; Micikevicius et al., 2022). The original tensor is multiplied by higher-precision scaler before being cast into the low-precision format. The scaling factor is computed as follows: Si = FPmax max (cid:0)X[Bi:B(i+1)](cid:1) , (8) where FPmax represents the maximum normal value of the low-precision floating-point format. scaling factor can be shared every elements along the channel dimension. It is unified representation for tensor-wise scaling (B = bdin), channel-wise scaling (B = din) and block-wise (1 B<din) scaling for tensor with the shape of din. 3.1.2. IMPLEMENTATION The quantization is applied to the linear layers in transformer (Vaswani, 2017), excluding the dot-product attention and the classifier. The computation of linear layer involves matrix multiplication (Eq. (9)) during the forward phase and two matrix multiplications (Eq. (10) and Eq. (11)) during the backward phase. = XWT , (9) (10) dX = dY1Wbwd, dW = (dY2)TXbwd, (11) where Rbdin , Rdoutdin and Rbdout represent the input tensor, the weight matrix and the output tensor, respectively. denotes the batch size, while din and dout refer to the number of input and output channels. The two inputs per matrix multiplication are converted into low-precision format with scaling factors. The inputs are de-quantized into BF16 tensors (Abadi et al., 2016), and BF16 multiplication is performed. The accumulators are stored in FP32 format, and the result of the accumulators are converted into BF16 tensor as the output. 3 Scaling Laws for FloatingPoint Quantization Training Figure 1. The fitting results of the scaling law in Eq. (7) deriving from Kumar et al. (2024), which have large bias in E1M1 case. In the three sub-figures on the left, middle and right, the sizes of the data points are approximately proportional to D, E, and respectively. 3.2. Setup 3.3. Basic Scaling Law Form We trained and evaluated suite of LLaMA (Dubey et al., 2024) architecture models on subset of the Dolma V1.7 dataset (Soldaini et al., 2024), maintaining the same sampling proportion used for training the OLMo 7B-v1.7 model (Groeneveld et al., 2024). Our experiments involved systematic exploration of language model pretraining across [41, 85, 154, 679] million parameters and [10, 20, 50, 100] billion tokens. Additionally, we conducted two extra pretraining sessions with models containing 1.2 billion parameters to validate the accuracy of our scaling law equation. For each combination of (N, D), we executed over 36 runs, systematically sweeping through each exponent and mantissa combination, varying the quantization target during training, and exploring different block sizes for quantization. In total, we performed 366 runs. Detailed hyperparameters and ablation studies can be found in Table 1 and Table 3. Hyper-parameters 41M 85M 154M 679M 1.2B Layers Hidden Size FFN Hidden Size Attention Heads Attention Head size Optimizer Adam (β1, β2) Adam ϵ Weight Decay Clip Grad Norm Max LR Min LR LR Decay Seqence Length Batch Size (# Tokens) Warmup Steps 12 512 1536 8 12 768 2048 12 64 12 1024 2816 16 64 24 1536 4096 24 64 24 2048 5632 32 64 AdamW (0.9, 0.95) 1 108 0.1 1.0 3.0 104 0 Cosine 2048 2M 500 Table 1. Model hyper-parameters for each size. 4 Our research builds upon the foundational scaling laws for training data and model size, which are pivotal in understanding the efficiency and effectiveness of machine learning models. We first evaluate the classical scaling laws: the Chinchilla scaling law (Hoffmann et al., 2022) and the OpenAI scaling law (Kaplan et al., 2020), and then give the design of ours with precision-aware factors. To determine which scaling law better fits the empirical data, we conducted experiments using BF16 precision across spectrum of model sizes ranging from 41 million to 679 million parameters. We plotted the resulting scaling law curves to visualize the fit between the predicted and actual losses. 3.3.1. CLASSICAL SCALING LAW FIT Figure 2 (left) illustrates the curve based on the Chinchilla scaling law. The empirical training losses align almost perfectly with the predicted losses, indicating highly accurate fit. Figure 2 (right) depicts the curve based on the OpenAI scaling law. Here, the training losses also show good match with the predicted losses, but the degree of fit is not as precise as that observed with the Chinchilla scaling law. 3.3.2. BASIC FORM OF OUR SCALING LAW Given the superior fit of the Chinchilla scaling law in our BF16 precision experiments, we proceeded to use this law as the basis for further exploration into our proposed scaling law for float precision. Our aim is to extend the understanding of scaling laws to encompass the impact of numerical precision on model performance, particularly focusing on the trade-offs between precision, computational efficiency, and model accuracy. In the following sections, we will present our methodology for investigating the scaling law for float precision, the results of our experiments, and the implications of our findings for the design and training of machine learning models at various levels of numerical precision. The marginal effects of the exponent (E), the mantissa (M), and the size of the scaling factors (B) on the Scaling Laws for FloatingPoint Quantization Training (a) Chinchilla basic scaling law. (b) OpenAI basic scaling law. Figure 2. The fitting performance of classical scaling laws. The size of the data point is proportional to D. performance of LLM will be discussed with experiments. 3.4. Quantization Targets In our pursuit of balancing practicality with academic rigor, we have chosen to focus on the quantization of inputs to the General Matrix Multiplication (GEMM) computations within the Transformer architecture. Transformer consists of three main GEMM operations: forward computation, input gradient computation, and parameter gradient computation. The inputs to these matrix multiplications in both forward and backward passes include six distinct elements: X, W, dY1, Wbwd, dY2, and Xbwd, which can be quantized to (P1) through (P6) respectively (see Figure 3). 3.4.1. EXPERIMENTAL FINDINGS Figure 3 illustrates the classical quantization targets of P1 to P6 to be explored. Through our experiments, we observed that the quantization of these inputs has varying impacts on LLM results. As illustrated in Figure 4, our key observations related to quantization targets are as follows: 1. The quantization of P1, P3, and P5 has significant effect on the models performance, leading to substantial increase in loss. Notably, the quantization of P5 results in pronounced degradation of performance, with losses increasing by up to 2%. This suggests that compressing and losing information in the input embedding during the backward pass can lead to considerable performance penalties. 2. Quantizing only one target of P4 or P6 yields the optimal performance. 3. Quantizing both P2 and P6 together results in similar overall performance to quantizing P2 alone. QuantizFigure 3. Quantization Targets. We select P2, P4, and P6 as our quantization targets for the following exploration of scaling laws. ing P2, P4, and P6 together also leads to overall results comparable to quantizing P2 alone. 3.4.2. OPTIMIZED QUANTIZATION TARGET To strike balance between efficiency and effectiveness, we have opted to quantize P2, P4, and P6. Subsequent research into float precision quantization will be based on this configuration. This decision is informed by the minimal Scaling Laws for FloatingPoint Quantization Training half-bit bias here. Next, we attempt to fuse the relationship of with those of data size and model size N. Parameter fitting is carried out under various E, D, and configurations, and the results are shown in Figure 5, where γ is negatively correlated with , and ι is negatively correlated with both and D. With regard to γ, we re-parameterize it as function of Dϕ γN η . As for ι, simply multiplying the original form of the chinchilla scaling law by coefficient precisely satisfies the pattern we discovered. Subsequently, we fit L(N, D, E), which is the jointly scaling law of N, D, E, as follows: L(N, D, E) = Dϕ η γ(E + 0.5)δ + ιLBF 16. (13) LBF 16 represents the BF16 loss L(N, D) given in the chinchilla scaling law of Eq. (2), i.e, LBF 16 = Dβ + ϵ. Elegantly, we find that ϕ β, η α, and ι 1 in the fitting. Therefore, the Exponent scaling law is as: α + Dβ α L(N, D, E) = 1 Dβ + ϵ. (14) γ(E + 0.5)δ + Finally, we re-fit the data using Eq 14, obtaining the results shown in Figure 6. α + 3.5.2. MANTISSA Similar to Exponent, we also assume the Mantissa-related scaling law conforms power-law form as: L(M ) = γ (M + 0.5)ν + ι. (15) Jointly considering the effects of and in representing γ and ι, we have: L(N, D, ) = Dϕ η γ(M + 0.5)ν + ιLBF 16. (16) Surprisingly, we also find that ϕ β, η α, and ι 1. Ultimately, we adopt the form of Eq. (17) to fit the joint scaling law of Mantissa with and D. L(N, D, ) = 1 γ(M + 0.5)ν + The fitting result is shown in Figure 7. Dβ α α + Dβ + ϵ. (17) Figure 4. Results of loss gaps with different quantization targets. impact on performance observed when quantizing these particular inputs, as opposed to the significant degradation caused by quantizing others. By focusing on P2, P4, and P6, we aim to maintain the integrity of the models performance while reaping the computational benefits of quantization. 3.5. Exponent and Mantissa The exponent and mantissa are key components of floatingpoint representations. Appropriately assigning bit widths to exponent and mantissa will larger alleviate information loss in floating-point quantization. In this subsection, we attempt to discover the hidden rules between exponent/mantissa and the LLM performance. 3.5.1. EXPONENT Firstly, we investigate the scaling law when Exponent serves as an independent variable. Experiments of different Exponents with various other parameter settings have been conducted, followed by our attempt of parameter fitting. It is assumed that the Exponent-related scaling law conforms more to power-law relationship form as: L(E) = γ (E + 0.5)δ + ι. (12) 3.5.3. JOINT EXPONENT & MANTISSA We discussed other forms of relationships (e.g., Kumar et al. (2024)) and conducted relevant comparative experiments, ultimately finding that the power-law relationship is more consistent with the experimental results. The 0.5 in Eq. (12) functions as good bias to fit extreme values. According to the IEEE 754 standard (Kahan, 1996), when either (exponent) or (mantissa) is set to 0, default information is still retained. This can also account for the existence of Integrating the scaling law results of Exponent and Mantissa when each serves as an independent variable, we can naturally organize their joint scaling law with N, D, E, into the form of Eq. (18). The final fitting effect is presented in Figure 8. L(N, D, E, ) = Dβ α γ(E + 0.5)δ(M + 0.5)ν +LBF 16. (18) 6 Scaling Laws for FloatingPoint Quantization Training Figure 5. The correlations between γ,ι in Eq. (12) and ,D. γ,ι could be viewed as functions of ,D. Data point size is proportional to D. Figure 6. The fitting results of our Exponent-related scaling law. Data point size is proportional to D. Figure 7. The fitting results of our Mantissa-related scaling law. Data point size is proportional to D. 3.6. Block Size of Scaling Factor In this subsection, we discuss the correlations of the block sizes and LLM losses, and extend the block-related scaling law to channel-wise and tensor-wise strategies. 3.6.1. BLOCK-WISE STRATEGY In the quantization process, we define the statistical range of the scaling factor as the block size (B). Since the scaling factor employs high-precision caching, when = 1, it is equivalent to retaining high-precision copy of the tensor to be quantized. At this point, the models expressiveness should be approximately the same as that of the highprecision model: L(N, D, = 1) LBF 16(N, D). After comparing the power, linear, and logarithmic law forms, we ultimately select the following logarithmic form as the scaling law when block size serves as an independent variable: L(B) = κ log2 + ψ. (19) In Figure 9, we demonstrate the changes in fitted κ and ψ under different and conditions. It can be observed that, similar to the exploration of Exponent and Mantissa in Section 3.5, κ is positively correlated with , while ψ is negatively correlated with and D, respectively. Furthermore, after re-parameterizing them, we similarly found that the fitted exponents of and are approximately α and β, and the correction coefficient of ψ based on the chinchilla scaling law is approximately equal to 1. Therefore, we ultimately build the scaling law for block size in conjunction with and as follows: L(N, D, B) = Dβ α log2 κ + α + Dβ + ϵ. (20) We demonstrate its fitting effect on experimental data in Figure 10. 3.6.2. CHANNEL-WISE STRATEGY To investigate the scaling law under the channel-wise strategy, we first utilize Eq. (20) to inversely derive the equivalent block size for different and cases that achieve the same validation loss as when employing the channel-wise strategy. It is found that this equivalent block size of the channel-wise strategy is approximately constant: log2 Bchannel 13.1567, (21) 7 Scaling Laws for FloatingPoint Quantization Training Figure 8. The fitting results of the joint Exponent & Mantissa scaling law: Data point sizes in left, middle, and right sub-figures are proportional to D, , and E, respectively. Figure 9. The correlations between κ,ψ in Eq. (19) and ,D. κ,ψ could be viewed as functions of ,D. The data points are scaled proportionally to the value of D. Figure 10. Our scaling law precisely forecasts validation loss for diverse block sizes. Data point sizes are directly proportional to and in the respective left and right sub-figures. which is natural since the batch size of gradient is (mostly) much larger than the hidden size (din in Section 3.1.1). After incorporating this equivalent block size into Eq. (20), the fitted scenario of the channel-wise strategy is in Figure 11. 3.6.3. TENSOR-WISE STRATEGY When adopting the tensor-wise strategy, we similarly employ Eq. (20) to predict its the equivalent block size. As illustrated in Figure 12, the equivalent block size satisfies power-law like relationship with as follows: Figure 11. The fitting results of the channel-wise scaling law. The size of the data point is proportional to D. Likewise, substituting the power-law predicted equivalent block size into Eq. (20) yields the fitted results in Figure 13. 4. Unified Scaling Law for FloatingPoint"
        },
        {
            "title": "Quantization Training",
            "content": "log2 Btensor ω ξDη . (22) In Section 3, we have decided the basic scaling law form and quantization targets in this work, followed by detailed 8 Scaling Laws for FloatingPoint Quantization Training the Chinchilla scaling law, and ρ(E, M, B) indicates the additional negative impacts brought by low-precision float quantized training. Deriving from Eq. (18) and (20), we can formulate as follows: ρ(E, M, B) = Dβ α log2 γ(E + 0.5)δ(M + 0.5)ν , (24) where the exponent E, mantissa M, and block size jointly represent the possible information loss of float quantized training, and Dβ α reflects, in sense, the knowledge intensity of an LLM of size trained on data. Note that we could smoothly fuse the factors of E, M, and with unified set of hyper-parameters of α, β, and γ. We adopt all above 358 experiments in Section 3 containing various N, D, E, M, settings to obtain the specific hyper-parameters in Eq. (23) and Eq. (24). To ensure the simplicity and universality of our scaling law, we pick up the Occams Razor to fuse or expurgate unnecessary hyperparameters. Ultimately, the final scaling law for floatingpoint quantization training is articulated as follows: L(N, D, E, M, B) = + Dβ + ϵ α + Dβ α log2 γ(E + 0.5)δ(M + 0.5)ν , (25) where the corresponding hyper-parameters are give in Table 2. The fitting performances of Eq. (25) are given in Figure 14, which show superior capability compared to previous scaling laws in low-precision training. Constant Value α β ϵ γ δ ν 69.2343 0.2368 68973.0621 0.5162 1.9061 11334.5197 3.1926 2.9543 Table 2. Fitted hyper-parameters and their values in our proposed unified scaling law for floating-point quantization training. Furthermore, we evaluate our scaling law to predict the losses of 1.2B LLMs with different low-precision settings and trained tokens (which are viewed as our validation models that are not used in calculating hyper-parameters of our scaling law). The consistently accurate fitting results demonstrate that our scaling law of floating-point quantization training functions well in larger model sizes. 9 Figure 12. The correlations between log2 and the data point is proportional to D. . The size of Figure 13. The fitting results of the tensor-wise scaling law. The size of the data point is proportional to D. explorations on the marginal effects of critical factors (i.e., the exponent, the mantissa, and the block size) on LLM capabilities. In this section, we will provide the unified scaling law for floating-point quantization training, with its fitting and predictive performance as well as the insightful findings deriving from our scaling law. 4.1. The Unified Scaling Law Formation The unified scaling law for floating-point quantization training should be able to jointly consider all factors of the data size (D), model size (N), exponent (E), mantissa (M), and block size of scaling factors (B) for precise low-precision LLM performance prediction. Based on the conclusions drawn in Eq. (2), Eq. (18), and Eq. (20), we could intuitively design the unified scaling law as follows: L(N, D, E, M, B) = α + Dβ + ϵ + ρ(E, M, B). (23) Here, N α + Dβ + ϵ represents the classical BF16 loss of Scaling Laws for FloatingPoint Quantization Training Figure 14. The fitting results of our scaling law for floating-point quantization training. Data point size is proportional to D. The star points (1.2B models) are our validation. 4.2. Implication-1: Optimal Float Layout Analysis The optimal float layout for given precision = + + 1 is derived to minimize the impact of precision-related information loss on model performance. Based on Eq. (25), the optimal mantissa is expressed as: Mopt = νP δ + ν 0.5. (26) More details are in Appendix A. The corresponding loss scaling equation incorporates this optimal layout: L(N, D, P, B) = α + Dβ + ϵ + Dβ α log2 γρP δ+ν . (27) where γρ = γδδνν (δ + ν)δ+ν . (28) Figure 15 visualizes the predictive performance for different , demonstrating the effectiveness of the derived layout in preserving performance under varying precisions. The optimal float layouts of FP4, FP8, and FP16 are E2M1, E4M3, and E8M7 (BF16), respectively. 4.3. Implication-2: Critical Data Size for Optimal Performance From Eq. (25) and Fig 16, we can observe that there are two factors containing that have opposite impacts on LLM loss. Intuitively, it implies that there is an optimal data size under certain float-pointing quantization setting. Figure 15. The optimal float layouts of different bit widths. The determination of critical data size (Dcrit) stands as critical juncture within the quantized training regimen. Upon exceeding the threshold of Dcrit with the training dataset, any additional data introduction negatively impacts the model efficacy, manifesting in rise in validation loss instead of decline. comprehensive derivation for the estimation of Dcrit is delineated in Appendix B: Dcrit = (cid:20) dγN α(E + 0.5)δ(M + 0.5)ν log2 (cid:21) 1 2β . (29) Notably, positive correlation emerges between model size (N ) or training precision (P ) and the occurrence of this pivotal point, indicating its delayed emergence under such conditions. Utilizing our parameter estimation framework, 1 billion-parameter model trained utilizing BF16 exhibits Dcrit value of 1730T, which is much larger than our current data size, elucidating the previous lack of observation of this phenomenon. Conversely, when the same model is trained with FP8-E4M3, the Dcrit value swiftly diminishes to 27T, and with FP4-E2M1, it further plummets to 0.4T. This phenomenon implies the potential harmness of larger data size on low-precision LLM training. 4.4. Implication-3: Compute-Optimality with Fixed Configurations We control the total computation cost = kN and analysis the optimal configuration under Eq. (25). 10 Scaling Laws for FloatingPoint Quantization Training Figure 16. Variation of loss with data size under different floating-point quantization settings. 4.4.1. FIXED DATA SIZE For fixed D, the critical precision Pcrit minimizes the loss while accounting for computational constraints. From Appendix C.1, Pcrit is expressed as: Pcrit(D) = (cid:0)γDDβ log2 B(cid:1) δ+ν , where γD = δ + ν α nαγρ , (30) (31) which consolidates the relationships between model precision and compute efficiency. As illustrated in Figure 17, Eq. (30) suggests that as the amount of training data increases, the most economical precision also correspondingly rises under the constraint of limited computational power. This also suggests that we can adopt such quantization strategy: in the early stage of training, employ aggressive quantization strategies such as FP8-E4M3, or even FP4-E2M1 which may be available in the future hardware, so as to quickly converge the model to better level. Subsequently, as the data volume as well as the knowledge intensity further increase, gradually enhance the training precision to BF16, or even FP32, in order to maintain the optimal cost-effectiveness of training. 4.4.2. FIXED MODEL SIZE For fixed , the critical precision depends on balancing the compute resources and maintaining the required training data size. The corresponding scaling law and derivations are detailed in Appendix C.2. We have: (cid:34) Pcrit(N ) = γN (cid:19)2β (cid:18) (α+2β) log2 (cid:35) 1 δ+ν+2β . (32) 11 Figure 17. Under the constraint of computing the budget with block size (B) set to 128, and based on the results of our experimental data fitting, the optimal precision (P ) values for different data sizes (D) can be deduced. As depicted, across substantially broad range of data sizes from 0.1T to 100T, the optimal precision value consistently falls within the range of 4 to 8 bits. finding analogous to that presented in Kumar et al. (2024) is observed herein, specifically, under the limitation of computational resources, an equilibrium exists between precision and model size from Eq. (32) is as: δ+ν+2βN α+2β = Constant. (33) 4.4.3. MINIMIZATION OVER , D, WITH FIXED COMPUTE Through joint analysis of the impacts of , D, and on the final validation loss, the relationship between costeffective precision and expected compute budget can be obtained: (δ+ν) α+β β +α = λ (γD log2 B) α+β β (cid:19)α , (cid:18) (34) Scaling Laws for FloatingPoint Quantization Training eters and data volume with low precision, highlighting the possible negative impacts of more trained tokens in lowprecision LLM training and serving. Other recent work (Ouyang et al., 2024) also investigate the correlations of integer quantization and LLM performance. Nevertheless, it is still not that particularly clear to conclude the scaling law for floating-point quantization training with respect to the specific selection of the exponent, mantissa, and block size of scaling factors. 5.2. Quantization of LLMs The quantization technique of large language models (Lang et al., 2024; Shen et al., 2024) has received widespread attention. Xiao et al. (2023) reduces the accuracy loss during quantization by smoothing the distribution of activations and weights. Dettmers et al. (2024) combines QuantizationAware Training and LoRA methods to implement an efficient fine-tuning method. Egiazarian et al. (2024) explores techniques for compressing large language models at very low bit rates, and Behdin et al. (2023) proposes framework that allows each layer to be quantized independently. Although previous work (Zhang et al., 2023; Yoshida, 2023) have studied the impact of exponent, mantissa and blocksize on the quantification of LLMs, the comprehensive impact of these indicators has not been systematically studied and summarized. 6. Conclusion, Limitation, and Future Work In this work, we propose our scaling law for floating-point quantization training, which functions satisfactorily as precise guidance of future low-precision LLM training. The key factors of the data size (D), model size (N), exponent (E), mantissa (M), and block size of scaling factors (B) are carefully considered in our scaling law throughout several hundred of experiments with various precision and model settings. Besides the scaling law, we also discover some insightful implications that could instruct and enhance future floating-point quantization training in LLMs. We hope our findings could shed light on better low-prediction LLM training to facilitate the LLM community. In the future, we will verify our scaling laws for floatingpoint quantization training under larger model sizes and data sizes. Currently, our explorations are conducted based on the classical Transformer architecture. Whether our scaling laws could also be smoothly applied to LLMs of other architectures (e.g., Mamba series (Dao & Gu, 2024)) is worth confirming. Moreover, our experiments focus on the classical floating-point quantization strategies, while other new-proposed low-bit LLM quantization methods should also be covered in the future. Figure 18. The optimal cost-performance ratio precision as function of the total compute budget, illustrating the relationship between precision (P ) and computational budget (C) when the block size (B) is set to 128 and = 6/16. where λ = dβ nα δ + ν α δ + ν + β . (35) In Appendix C.3, we present more detailed derivation processes. Based on the parameters fitted from our experimental data presented in Table 2, with the block size (B) set to 128 and = 6/16, as illustrated in Figure 18, when the total compute budget is in the range of (1021, 1031) floating-point operations, the optimal cost-performance ratio precision is found to lie between 4 and 8 bits. This finding implies that training larger models with lower precision and utilizing less data yields more cost-effective approach. Developers could rely on our implications from our scaling law to decide their optimal float-pointing quantization settings. 5. Related Work 5.1. Scaling Law of LLMs Due to the extremely large cost of resource and time for LLM training, discovering appropriate scaling laws to accurately predict LLM capabilities under different parameters is essential for product-level training. Kaplan et al. (2020) gives the classic form of the scaling law and concludes that the performance penalty depends predictably on the ratio 0.74/D. Hoffmann et al. (2022) models the final loss as parametric function of the count of model parameters and the number of trained tokens, i.e. + A/N α + B/Dβ. Bahri et al. (2024) and Lin et al. (2024) theoretical analysis of how loss scales with the size of the training dataset and the number of parameters in power-law manner. Previous work (Dettmers & Zettlemoyer, 2023) explores the scaling laws of LLMs with different bit precisions. Recently, Kumar et al. (2024) focuses on the impact of model param12 Scaling Laws for FloatingPoint Quantization Training"
        },
        {
            "title": "References",
            "content": "Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. Explaining neural scaling laws. Proceedings of the National Academy of Sciences, 121(27):e2311878121, 2024. Behdin, K., Acharya, A., Aman Gupta, S. K., and Mazumder, R. Quantease: Optimization-based quantization for language models-an efficient and intuitive algorithm. stat, 1050:5, 2023. Dao, T. and Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In Proceedings of ICML, 2024. Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pp. 77507774. PMLR, 2023. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar, E., Babenko, A., and Alistarh, D. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024. Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accelerating the science of language models. Preprint, 2024. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Kahan, W. Ieee standard 754 for binary floating-point arithmetic. Lecture Notes on the Status of IEEE, 754(947201776):11, 1996. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kumar, T., Ankner, Z., Spector, B. F., Bordelon, B., Muennighoff, N., Paul, M., Pehlevan, C., Re, C., and Raghunathan, A. Scaling laws for precision. arXiv preprint arXiv:2411.04330, 2024. Lang, J., Guo, Z., and Huang, S. comprehensive study on quantization techniques for large language models. arXiv preprint arXiv:2411.02530, 2024. Li, S., Zhao, P., Zhang, H., Sun, X., Wu, H., Jiao, D., Wang, W., Liu, C., Fang, Z., Xue, J., et al. Surge phenomenon in optimal learning rate and batch size scaling. arXiv preprint arXiv:2405.14578, 2024. Lin, L., Wu, J., Kakade, S. M., Bartlett, P. L., and Lee, J. D. Scaling laws in linear regression: Compute, parameters, and data. arXiv preprint arXiv:2406.08466, 2024. Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D., et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., and Wei, F. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764, 2024. Micikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey, P., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P., Kamalu, J., et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022. Ouyang, X., Ge, T., Hartvigsen, T., Zhang, Z., Mi, H., and Yu, D. Low-bit quantization favors undertrained llms: Scaling laws for quantized llms with 100t training tokens. arXiv preprint arXiv:2411.17691, 2024. Peng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z., Xiong, Y., Yang, Z., Ni, B., Hu, J., et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. Shen, A., Lai, Z., and Li, D. Exploring quantization techniques for large-scale language models: Methods, In Proceedings of challenges and future directions. the 2024 9th International Conference on Cyber Security and Information Engineering, ICCSIE 24, pp. 783790, New York, NY, USA, 2024. Association for 13 Scaling Laws for FloatingPoint Quantization Training Computing Machinery. ISBN 9798400718137. doi: 10. 1145/3689236.3695383. URL https://doi.org/ 10.1145/3689236.3695383. Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A. H., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, P., Zettlemoyer, L., Smith, N. A., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J., and Lo, K. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024. Sun, X., Choi, J., Chen, C.-Y., Wang, N., Venkataramani, S., Srinivasan, V. V., Cui, X., Zhang, W., and Gopalakrishnan, K. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. Advances in neural information processing systems, 32, 2019. Sun, X., Chen, Y., Huang, Y., Xie, R., Zhu, J., Zhang, K., Li, S., Yang, Z., Han, J., Shu, X., et al. Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 3808738099. PMLR, 2023. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yoshida, D. Nf4 isnt information theoretically optimal (and thats good). arXiv preprint arXiv:2306.06965, 2023. Zhang, C., Cheng, J., Shumailov, I., Constantinides, G. A., and Zhao, Y. Revisiting block-based quantisation: What is important for sub-8-bit llm inference? arXiv preprint arXiv:2310.05079, 2023. Zhang, T., Lin, Z., Yang, G., and De Sa, C. Qpytorch: lowprecision arithmetic simulation framework. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 1013. IEEE, 2019. 14 A. Optimal Float Layout Scaling Laws for FloatingPoint Quantization Training Given specified total precision (P), the process of determining the optimal allocation of exponent bits (E) and mantissa bits (M) involves substituting the equation: into the proposed scaling law delineated in Eq. (24): = 1. ρ(M ) = = Dβ α Dβ α log2 γ(P 1 + 0.5)δ(M + 0.5)ν log2 γ(P 0.5 )δ(M + 0.5)ν . Subsequently, the loss function L, with respect to the mantissa bits , is expressed as: L(M ) = α + Dβ + ϵ + Dβ α log2 γ(P 0.5 )δ(M + 0.5)ν . To optimize this function, we compute the partial derivative of with respect to : M = = Dβ α Dβ α log2 γ log2 γ ν(P 0.5 )δ(M + 0.5)ν1 δ(P 0.5 )δ1(M + 0.5)ν (P 0.5 )2δ(M + 0.5)2ν 1 (P 0.5 )δ(M 0.5)ν (cid:18) ν + 0.5 δ 0.5 (cid:19) . By setting this partial derivative equal to zero, we obtain the optimal value for which is given by Eq. (40): M ν + 0.5 δ 0.5 = 0 = 0 = νP δ + ν 0.5, and the corresponding value for is: Eopt = 1 Mopt δP δ + ν 0.5. = Next, we introduce the optimal values Mopt and Eopt into the proposed scaling law, as formulated in Eq. (24): ρopt(P ) = Dβ α γ (cid:16) δP δ+ν = Dβ α log2 γ (cid:17)ν log2 (cid:17)δ (cid:16) νP δ+ν (δ + ν)δ+ν δδνν 1 δ+ν . For the sake of simplification, we introduce the parameter γρ, defined as follows: 15 (36) (37) (38) (39) (40) (41) (42) As result, Scaling Laws for FloatingPoint Quantization Training γρ = γδδνν (δ + ν)δ+ν . ρopt(P ) = Dβ α log2 γρP δ+ν . (43) (44) By substituting Eq. (44) into the unified scaling equation, namely Eq. (23), we arrive at Eq. (27), which is further simplified to: Lopt(P ) = = = Furthermore, let γn denote constant value: α + (cid:18) α Dβ α log2 γρP δ+ν Dβ + ϵ + Dβ 1 + log2 γρP δ+ν (cid:19) Dβ + ϵ Dβ + ϵ. α + + (cid:33) 1 α (cid:32) α 1 1+ Dβ log2 nγρ δ+ν γn = γρn. (45) (46) In accordance with the Chinchilla scaling law (Hoffmann et al., 2022), we define Nef as the count of effective parameters, which aligns with the model size specified in Eq. (2): Nef = 1 1 + Dβ log2 γnP δ+ν 1 α . (47) When the condition Dβ log2 γnP δ+ν is satisfied, the effective number of parameters, Nef , can be simplified as follows: Nef (cid:19) 1 α (cid:18) γn Dβ log2 δ+ν α . (48) Hence, we discern power-law relationship between the number of effective parameters, Nef , and the precision, . It is important to emphasize that Nef is influenced not solely by the quantization technique employed but also by the volume of data. When both the model size and the quantization method are held constant, an increase in data size leads to decrease in the number of effective parameters. B. Critical Data Size For floating-point quantization training, over-training may occur, that is, when the amount of training data exceeds the critical value, the loss will increase instead. Given exponent bits (E), mantissa bits (M), block size (B), and number of model parameters (N), we aim to find the critical data size before over-training. Based on Eq. (23), we derive the expression for when the partial derivative with respect to is zero. Preliminarily, we compute the partial derivative of the loss function with respect to the data size D: 16 Scaling Laws for FloatingPoint Quantization Training D = N α + Dβ + Dβ1 α ϵ + ρ(E, M, B) log2 γ(E + 0.5)δ(M + 0.5)ν . = β Dβ+1 + β By setting this partial derivative to zero and then solving for Dβ, we obtain: D = 0 β Dβ+1 + β Dβ = log2 γ(E + 0.5)δ(M + 0.5)ν = 0 Dβ1 α (cid:115) dγN α(E + 0.5)δ(M + 0.5)ν log2 . (49) (50) Consequently, the critical value of is given by Eq. (29). C. Compute-optimality In order to investigate the optimal precision under constrained computational budget, we define the computational expenditure associated with floating-point quantization training as follows: Here, signifies proportionality constant, denotes the computational cost per model parameter, and accounts for the additional expense incurred during the multiplication of scaling factors. = k(P + b)N D. (51) C.1. Fixed data size For the critical precision in relation to the data size D, Eq. (51) can be incorporated into the proposed scaling law, as expressed in Eq. (27): L(D, ) = (cid:16) = k(P +b)D (cid:19)α (cid:18) kD n (cid:17)α + Dβ + ϵ + (cid:16) Dβ k(P +b)D (cid:17)α log2 γρP δ+ν (P + b)α + Dβ log2 γρ (cid:18) kD (cid:19)α (P + b)α δ+ν + Dβ + ϵ. We then compute the partial derivative of the loss function L(D, ) with respect to : L(D, ) = nα = nα (cid:19)α (cid:19)α (cid:18) kD (cid:18) kD (P + b)α1 + (P + b)α1 + Dβ log2 γρ Dβ log2 γρ (cid:18) kD (cid:18) kD (cid:19)α α(P + b)α1P δ+ν (δ + ν)(P + b)αP δ+ν (cid:19)α (P + b)α (cid:18) α δ+ν + 2(δ+ν) δ + ν (cid:19) . (52) (53) Upon setting the partial derivative of the loss function with respect to precision equal to zero, and solving for , we obtain: + δ+ν (cid:18) δ + ν L(D, ) α + (cid:19) 17 = 0. = nγρα Dβ log2 . (54) Scaling Laws for FloatingPoint Quantization Training"
        },
        {
            "title": "Assuming",
            "content": "γD = δ + ν α nαγρ , and considering that = 0, the critical precision is determined as: δ+ν = γDDβ log2 B. C.2. Fixed model size With respect to the critical precision in relation to the model size , we can streamline Eq. (27) to: L(N, ) = α + (cid:16) k(P +b)N (cid:17)β + ϵ + (cid:17)β (cid:16) k(P +b)N α = (cid:19)β (cid:18) kN (P + b)β + log2 γρN α (cid:18) kN (cid:19)β log2 γρP δ+ν 1 (P + b)βP δ+ν + α + ϵ. Subsequently, we evaluate the partial derivative of the loss function L(N, ) with respect to : L(N, ) = dβ = dβ (cid:19)β (cid:19)β (cid:18) kN (cid:18) kN (P + b)β1 (P + b)β1 log2 γρN α log2 γρN α (cid:18) kN (cid:18) kN (cid:19)β (cid:19)β β(P + b)β1P δ+ν + (δ + ν)(P + b)βP δ+ν1 (P + b)2βP 2(δ+ν) (cid:18) β (cid:19) 1 (P + b)βP δ+ν + δ + ν . + Upon setting this partial derivative to zero, and solving for , we arrive at: log2 γρN α (cid:18) kN (cid:19)β 1 (P + b)βP δ+ν (cid:18) β + L(N, ) δ + ν + (cid:19) = 0. = dβ 1 (P + b)2β1P δ+ν (cid:18) β + + δ + ν (cid:19) = By introducing and under the assumption that = 0, the critical is deduced to be: γN = β + δ + ν dβγρ , (cid:18) kN dβγρN α log2 (cid:19)β (P + b)β1. (cid:18) kN (cid:19)2β . 1 2β1P δ+ν (cid:18) β + δ + ν (cid:19) = δ+ν+2β = (cid:19)2β (cid:18) kN dβγρN α log2 (β + δ + ν) log2 dβγρN α (cid:19)2β . (cid:18) kN (cid:19)2β . δ+ν+2β = γN (α+2β) log2 B. (cid:18) (55) (56) (57) (58) (59) (60) (61) Scaling Laws for FloatingPoint Quantization Training C.3. Minimization over , D, with Fixed Compute Based on the results from Section C.1, C.2 and specifically Eq. (51) with b=0, we proceed to address the system of equations: L(D,P ) = 0. L(D,P ) = 0. = kP D. This subsequently leads to the expression of in relation to the computational budget C: (δ+ν) α+β β +α = (γD log2 B) α+β β dβ nα δ + ν α δ + ν + β (cid:18) (cid:19)α . D. Ablations Table 3: All configurations for the ablation experiments. (62) (63) M 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 7 1 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 1 2 channel 32 64 128 256 512 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 512 channel tensor channel 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 19 Fitting support Scaling Laws for FloatingPoint Quantization Training 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 40894464 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 20 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 512 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 512 channel tensor channel channel channel channel channel channel channel channel channel Scaling Laws for FloatingPoint Quantization Training 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 40894464 40894464 40894464 40894464 40894464 40894464 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 52428800000 52428800000 52428800000 52428800000 21 1 3 5 1 2 1 7 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 channel channel channel channel channel channel channel 32 64 128 256 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 Scaling Laws for FloatingPoint Quantization Training 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 84934656 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 2 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 22 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 2 3 4 5 6 1 3 256 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 channel tensor channel channel channel channel channel channel channel Scaling Laws for FloatingPoint Quantization Training 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 104857600000 104857600000 23 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 channel channel channel channel channel channel channel channel channel 32 64 128 256 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 Scaling Laws for FloatingPoint Quantization Training 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 154140672 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 10485760000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 24 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 1 2 3 4 64 128 256 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 512 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 512 channel tensor channel channel channel Scaling Laws for FloatingPoint Quantization Training 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 679477248 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 4 5 5 6 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 4 4 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 20971520000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 52428800000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 104857600000 25 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 3 5 1 2 1 7 1 1 1 1 1 1 1 2 3 4 5 6 1 3 1 2 1 channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 512 channel tensor channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel channel 32 64 128 256 512 channel tensor channel channel channel channel channel channel channel channel channel channel channel Scaling Laws for FloatingPoint Quantization Training 356 357 358 359 360 361 362 363 364 365 366 679477248 679477248 679477248 1233125376 1233125376 1233125376 1233125376 1233125376 1233125376 1233125376 1233125376 104857600000 104857600000 104857600000 10485760000 10485760000 20971520000 20971520000 52428800000 52428800000 104857600000 4 5 6 1 4 1 4 1 4 1 4 5 2 1 2 3 2 3 2 3 2 3 channel channel channel 512 512 512 512 512 512 512"
        }
    ],
    "affiliations": [
        "Tencent Hunyuan",
        "The Chinese University of Hong Kong",
        "Tokyo Institute of Technology",
        "University of Macau"
    ]
}