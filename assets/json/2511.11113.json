{
    "paper_title": "VIDEOP2R: Video Understanding from Perception to Reasoning",
    "authors": [
        "Yifan Jiang",
        "Yueying Wang",
        "Rui Zhao",
        "Toufiq Parag",
        "Zhimin Chen",
        "Zhenyu Liao",
        "Jayakrishnan Unnikrishnan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 1 1 1 1 . 1 1 5 2 : r VIDEOP2R: Video Understanding from Perception to Reasoning Yifan Jiang1*, Yueying Wang2, Rui Zhao2, Toufiq Parag2, Zhimin Chen2 Zhenyu Liao2, Jayakrishnan Unnikrishnan2 1USC, 2Amazon"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement fine-tuning (RFT), two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VIDEOP2R, novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop three-step pipeline to generate VIDEOP2R-CoT-162K, high-quality, processaware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce novel process-aware group relative policy optimization (PAGRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VIDEOP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PAGRPO and demonstrate that models perception output is information-sufficient for downstream reasoning. 1. Introduction Understanding visual information in video has long been core challenge in computer vision [15, 32, 48, 49]. Reinforcement fine-tuning (RFT) [34], two-stage framework of supervised fine-tuning (SFT) followed by reinforcement learning (RL), has recently emerged as powerful approach for enhancing the reasoning capabilities of large language models (LLMs) [1, 4, 21]. Within the RFT framework, the SFT stage leverages Chainof-Thought (CoT) [59] annotations to warm up models with explicit guidance, while the RL stage further improves reasoning quality through reward-driven optimization. Motivated by the success of RFT, recent stud- *Work done while as an intern at Amazon. Corresponding author: zhaori@amazon.com ies [7, 14, 26, 56] have adapted RFT to video reasoning by designing diverse CoT generation pipelines and developing RL variants, with particular attention to Group Relative Policy Optimization (GRPO) [17] for its simple rule-based rewards without critic model. These adaptations have demonstrated clear advantages over conventional SFT across multiple video benchmarks [14, 52]. However, exploration of adapting RFT from text to video remains preliminary, partly due to simply treating video as another alternative modality to text [50]. This simplicity assumption ignores the decomposability of visual reasoning, which consists of two different processes [43, 66]. The first one is perception, which extracts salient information from the visual input, and reasoning, which organizes the visual evidence and draws inferences. Each process can introduce error in distinct ways (e.g., missed visual evidence, faulty inference), and can impact video understanding [2, 22]. Yet existing video RFT frameworks are process-agnostic: they collapse perception and reasoning into single procedure and assign single final reward to the whole trajectory, which blurs credit assignment. As Fig. 1 (top right) shows, perception error (e.g., turning back) induces the reasoning error, but without process awareness, the entire trajectory is evaluated as incorrect. Therefore, assigning single reward to the entire process can prevent the model from effectively correcting mistakes that emerge in different processes. The challenge to extend RFT to explicitly account for different processes in visual reasoning is two-fold. (i) Lack of process-aware CoT data: existing CoT annotations conflate perception and reasoning rather than explicitly disentangling the two; and (ii) Coarse rewards: training typically collapses feedback into single final reward for the entire reasoning process [14, 52, 56], hindering credit assignment across processes. To address these challenges, we propose VIDEOP2R, novel process-aware video RFT framework that models perception and reasoning as distinct processes to enhance video reasoning. Same as conventional RFT, our framework consists of two stages of training. In the SFT stage, we construct three-step CoT genera1 Figure 1. Comparison between GRPO-based video RFT framework (process-agnostic) and VIDEOP2R (process-aware). tion pipeline that produces high-quality perception and reasoning traces. Given visual question-answer (VQA) samples, our pipeline first generates visual perception and reasoning traces in order; the perceptions are then fed to reasoning-capable LLM [5] to verify whether they contain sufficient visual evidence to reach the correct answer. Running this pipeline on 260K VQA pairs [14] yields VIDEOP2R-CoT-162K after filtering low-quality samples. We use this dataset in SFT to warm up the model, encouraging process separation during inference and providing strong initialization for subsequent RL. In the RL stage, we propose PA-GRPO, process-aware variant of GRPO. Unlike GRPO, which assigns single reward to the entire trajectory, PAGRPO supplies two separate rewards specific for perception and reasoning and assigns them to the corresponding output segments: 1) an LLM-judged [16] perception reward, which evaluates whether the models perception captures the necessary information from the video, and 2) rule-based reasoning reward verifying the correctness of the final answer. As illustrated in Fig. 1, after two-stage training, VIDEOP2R enables LVLMs to conduct process-aware inference with calibrated perception and reasoning. We conduct comprehensive experiments on seven widely used video understanding and reasoning benchmarks [15, 20, 25, 32, 40, 62, 72], comparing VIDEOP2R with representative process-agnostic video RFT baselines (e.g., Video-R1 [14] and VideoRFT [52]). Results show that VIDEOP2R achieves SotA on six out of seven benchmarks, with robust gains of 1.9%9.1% average accuracy over base models across benchmarks. Ablation studies further validate the effectiveness of the process-aware modeling and the PA-GRPO. In addition, we provide fine-grained analysis of VIDEOP2Rs perception and PA-GRPOs improvements over GRPO to support future process-aware research in the video domain. Our key contributions are listed as follows: novel process-aware video RFT framework, VIDEOP2R, that models perception and reasoning separately to enhance video reasoning. process-aware RL algorithm based on GRPO, PAGRPO, that provides separate rewards for perception and reasoning, improving credit assignment in RL. An automatic three-step CoT generation pipeline that produces perception and reasoning annotations, yielding VIDEOP2R-CoT-162K for warm start in SFT. Comprehensive evaluation across diverse benchmarks confirms VIDEOP2Rs SotA performance. Further ablations verify the effectiveness of process-aware modeling and PA-GRPO. 2. Related work 2.1. Video Perception and Reasoning in LVLMs Video reasoning poses coupled spatialtemporal challenges beyond text/image only settings [15, 32, 48], demanding coordinated perception and reasoning [43, 66]. Early approaches handle coordination by introducing modular perception preprocessors (e.g., frame captioners [53, 57, 68] or STSGs [27, 47, 61]) to aid downstream reasoning. However, because these preprocessors are typically frozen and non-trainable, they bottleneck the pipeline and cannot be improved to curb error propagation or information loss [50]. Recent methods organize perceptionreasoning into pre-defined stages, either by attributes (frames/objects/actions) [13, 18, 37, 41, 53, 55, 58] or steps (planning/grounding) [33, 45]. However, rigid designs often limit generalization across diverse scenarios [52], and mixing perception and reasoning at each stage leaves the entire framework prone to failure from single perception error [2, 22]. In contrast, VIDEOP2R separates perception and reasoning, aligning with the two-process modeling of video reasoning [43] for better generalization, while preserving an end-to-end training pipeline for continual refinement. 2.2. Reinforcement Learning for LVLMs Building on GRPO, recent work adapts RL/RFT to video understanding and reasoning [7, 14, 26, 36, 39, 52, 2 Figure 2. Illustration of overall VIDEOP2R RFT framework (left) and the three-step CoT generation pipeline (right). 56, 71]. Time-R1 uses timestamp-aware and template rewards [56]; Video-R1 and STAR-R1 reward sensitivity to correct temporal order [14, 28]; Videochat-R1 and VersaVid-R1 adopt task-specific rewards [7, 26]; VideoRFT adds stage-aware semantic reward [52]. However, most prior efforts model video reasoning as single trajectory and apply single final reward over the entire output sequence, with no process-aware distinction between perception and reasoning. This also holds for recent image-domain studies on perceptionreasoning separation [29, 60, 64]. VIDEOP2R instead uses PAGRPO to assign separate perception and reasoning rewards to their token segments, providing more clear signals and more precise error attribution. 3. VIDEOP2R RFT Framework In this section, we introduce the overall design of the VIDEOP2R RFT framework (Fig. 2 left), which follows the standard RFT setup [34] with specific focus on modeling video reasoning into perception and reasoning: (1) SFT stage, we use three-step CoT generation pipeline to construct process-aware CoT dataset, VIDEOP2R-CoT-162K. We train the base model on this dataset to enhance its perception and reasoning capabilities while warming up the model for the RL stage. (2) RL stage, we propose process-aware reinforcement learning scheme, PA-GRPO, which refines the models reasoning by providing separate rewards for perception and reasoning, enabling the model to move beyond supervised learning boundaries [38]. 3.1. Process-aware CoT Dataset To address the challenge of lacking process-aware CoT dataset, we develop strategy to curate CoT data at scale, which is then used for fine-tuning LVLM in SFT. Particularly, we first standardize processaware CoT template that explicitly disentangles perception from reasoning in different segments as follows: <observation>...</observation> <think>...</think><answer>...</answer> observation segment represents the perception process, where the model extracts relevant visual evidence based on the question. think and answer segment captures the reasoning process, where the model reasons (think) over the extracted visual evidence and states the final answer (answer). All generated CoTs follow this template, and the model is trained to adhere to it at inference. 3.1.1. Three-Step CoT Generation Pipeline Building upon the proposed template, we design threestep pipeline (Fig. 2 right) to generate process-aware CoT data containing both perception and reasoning traces for diverse set of VQA tasks. The overall workflow comprises three major steps (Details in the Supplementary). 1) Process-aware CoT Generation. For each VQA sample, we use Qwen2.5-VL-72B-Instruct [4] to generate an initial CoT trace for both perception and reasoning in corresponding segments. 2) CoT Verification. To ensure consistency and correctness, we evaluate the final answer of each generated response with taskspecific metrics (e.g., exact word match and word error rate for generation tasks), discarding samples that yield low-quality answers or deviate from the expected CoT template. 3) Observation Sufficiency Verification. We further filter generated data using cross-modal validation strategy to validate the observation segment in isolation from raw visual inputs [52]. Specifically, for each sample, we provide only the observation 3 segment, along with the corresponding question and answer, to Claude 3.7 Sonnet [3], which assesses whether the visual evidence in observation are adequate to support the final reasoning process. 3.1.2. Data Construction and Statistics To ensure reproducibility and fair comparison, we apply this pipeline to public and representative imageand video-based QA dataset [14], which includes multiple question types such as multiple-choice, numerical QA, Optical Character Recognition (OCR), free-form QA, and regression. Applying our pipeline on 260K VQA data [14] produces 162K high-quality process-aware CoT data with perception and reasoning traces, termed VIDEOP2R-CoT-162K. We provide detailed analyses of VIDEOP2R-CoT-162K (e.g., embedding visualization and word-frequency statistics) in the Supplementary, which show that our annotations inherently separate perception from reasoning. 3.2. Process-Aware Reinforcement Learning After the SFT stage, we further refine the model through reinforcement learning. Building upon GRPO [17], we introduce process-aware variant, PA-GRPO  (Fig. 3)  , which provides separate reward signals for perception and reasoning processes to encourage more structured and efficient policy optimization [30]. This section first revisits the standard GRPO framework and then presents our process-aware extension designed to align reward signals with perceptionreasoning separation. 3.2.1. Group Relative Policy Optimization (GRPO) GRPO [17] extends Proximal Policy Optimization [42] by removing the dependency on learned critic model and directly comparing responses within groups. Given question q, the policy model πθ samples candidate responses = {o1, o2, . . . , oG} as group, each assigned rule-based reward ri. Rewards are then normalized within the group to yield the relative advantage: Ai = ri mean({rj}G std({rj}G j=1) j=1) . (1) where Ai represents the relative advantage of all tokens in the i-th response within the group. With the relative advantage computed, the GRPO overall optimization objective is formulated as: JGRPO(θ) = Eq,{oi} 1+ϵ(cid:1)Ai (cid:34) (cid:88)"
        },
        {
            "title": "1\nG\n(cid:1) − β DKL",
            "content": "i=1 min(ρiAi, (cid:0)πθ πref clip(cid:0)ρi, 1ϵ, (cid:1)(cid:3) . (2) where ρi = πθ(oiq) πθold (oiq) is the likelihood ratio between updated policy πθ and old policy πθold , and πref is fixed 4 Figure 3. The illustration of the PA-GRPO algorithm. reference model (e.g., frozen copy of policy model) after SFT, providing KL regularization weighted β. This formulation constrains large policy deviations while promoting high-reward samples, ensuring stable optimization during reinforcement learning. 3.2.2. Process-Aware Group Relative Policy Optimization (PA-GRPO) While GRPO performs well on textual reasoning, its single scalar reward provides limited training signal for the two-process based visual reasoning [43, 66]. To address this, we propose PA-GRPO, which introduces separate perception and reasoning rewards for tokens in each process, enabling fine-grained credit assignment during reinforcement learning, as illustrated in Fig. 3. Formally, for each question q, the sampled response from the policy model πθ is represented as = {(o1,P, o1,R), (o2,P, o2,R), . . . , (oG,P, oG,R)}, where oi,P denotes perception process tokens within the observation segment, and oi,R denotes reasoning process tokens within the think and answer segments. For tokens within each process, PA-GRPO supplies separate accuracy rewards to provide reliable supervision. We further introduce length and format rewards to encourage well-structured, concise outputs, following prior RFT frameworks [14, 17, 26, 52]. We demonstrate the accuracy reward design, and our configurations for the format and length rewards below. Perception Accuracy Reward (Racc,P). The perception accuracy reward evaluates whether the model correctly perceives visual information from video input. We adopt an LLM-as-Judge evaluation [16] in textonly setting, procedure shown to be reliable in various scenarios [9, 12]. Concretely, we provide only the observation segment, along with the corresponding question and answer, to Claude 3.7 Sonnet [3], which judges whether the observation segment is sufficient to support the correct answer. Formally, objective of PA-GRPO is then formulated as: Racc,P = 1 (judged sufficient); 0 otherwise. (3) Reasoning Accuracy Reward (Racc,R). The reasoning accuracy reward evaluates whether the model produces accurate reasoning outcomes. We apply taskspecific evaluation metrics to accommodate different question types, including exact word match for categorical tasks, ROUGE-based similarity for open-ended generation, and error-based scores for numerical or regression problems. Formally, JPA-GRPO(θ) = Eq,{oi}"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:88) min(ρi,kAi,k, k{P,R} clip(cid:0)ρi,k, 1ϵ, 1+ϵ(cid:1)Ai,k (cid:1) β DKL (cid:0)πθ πref (cid:1)(cid:3) , (7) πθ(oi,k q) πθold(oi,k q) where ρi,k = denotes the likelihood ratio for perception (k=P) or reasoning (k=R), and Ai,k is the process-aware advantage. Racc,R = Acct(oi,R, ytrue), (4) 4. Experiment Setup where Acct() [0, 1] denotes the task-specific accuracy metric for task type t. Format Reward (Rform) and Length Reward (Rlen). To ensure clear reward assignment, the format and the length reward in PA-GRPO are also provided separately for each process (oi,P and oi,R). We use the format reward (Rform) to encourage adherence to the Specifically, perprocess-aware inference template. ception process tokens (oi,P) must appear within the observation, the reasoning process tokens (oi,R) must present the reasoning trace under think, and the final answer must be provided within answer. We verify compliance using regular expression matching and assign binary reward ({0,1}) accordingly. The length reward (Rlen) is included to favor concise yet informative responses while avoiding overthinking. The reward is assigned only if both accuracy and format rewards are non-zero and the models response in each segment (oi,P and oi,R) falls within the target length ([lmin, lmax]). In line with prior RFT work [14, 52], we fix Rlen = 0.2, lmin = 128 and lmax = 320 for oi,P, lmin = 320 and lmax = 512 for oi,R. 3.2.3. Process-Aware Reward Assignment The overall reward of perception tokens (oi,P) and reasoning tokens (oi,R) are defined as Ri,k = Racc,k + Rform,k + Rlen,k, {P, R}. (5) Unlike Eq. (1), which normalizes all rewards within single group, we split rewards into separate groups (perception process vs. reasoning process) and normalize each to get process-aware advantage, since their scales and distributions are not directly comparable [67]: Ai,k = Ri,k mean({Rj,k}G std({Rj,k}G j=1) j=1) , {P, R}. (6) We assign each process-aware advantage only to its corresponding tokens (e.g., the perception advantage Ai,P is applied to oi,P). The overall optimization 5 Two-stage Training. We adopt Qwen2.5-VL-7BInstruct [4] as the base LVLM in our training pipeline. Following the same training setups used in prior videoRFT studies [14, 52], we perform one epoch of SFT on VideoP2R-CoT-162K, followed by 1K RL updates over original 260K visual QA data using PA-GRPO. The model obtained after the SFT stage is referred to as VIDEOP2R-SFT, and the final model after the RL stage is denoted as VIDEOP2R. Benchmarks. Following prior works [15, 52], we evaluate our approach on seven benchmarks, including four video reasoning datasets (VSI-Bench [62], VideoMMMU [20], MMVU [72], and VCR-Bench [40]) and three video understanding datasets (MVBench [25], TempCompass [32], and VideoMME [15]). These benchmarks jointly cover spatial reasoning, knowledgeintensive QA, temporal logic, and general video understanding. We follow the official evaluation protocols of each benchmark. (1) RFT on Qwen2.5-VL-7B. We comBaselines. pare against recent video RFT approaches built upon Qwen2.5-VL-7B using GRPO or its variants, including Video-R1 [15], Time-R1 [56], VideoRFT [52], VideoChat-R1 [26], and VersaVid-R1 [7] (2) OpenSource Models. We further include Qwen2.5-VL-7B along with other 7B-scale models for comprehensive evaluation: LLaVA-OneVision [24], LongVA [69], Video-UTR [63], and VideoLLaMA2 [8]. We follow the prompt templates in each baselines official publication. We provide all setup details in the Supplementary. 5. Results Our experiments focus on addressing five research questions from Section 5.1 to 5.5 : (1) How does VIDEOP2R perform across various video understanding benchmarks? (2) What is the contribution of each component in VIDEOP2R? (3) Does the perception representations of VIDEOP2R effectively support downstream reasoning? (4) Does the process-aware reward design of PATable 1. Performance comparison on video reasoning and understanding benchmarks. Best/second-best result of each column is in bold/underline. Missing entries indicate unreported results (all numbers unit in %). Model Video Reasoning Video Understanding VSI. VideoMMMU MMVU VCR. MV. TempCom. VideoMME Open-Source 7B Models LLaVA-OneVision-7B LongVA-7B Video-UTR-7B VideoLLaMA2-7B Qwen2.5-VL-7B RFT on Qwen2.5-VL-7B Video-R1 VideoChat-R1 Time-R1 VersaVid-R1 VideoRFT VIDEOP2R (Ours) 32.4 29.2 30.1 35.8 33.9 29.0 33.7 36.8 36.8 33.8 23.9 48.1 52.3 54.0 51.0 51.9 51. 55.0 49.2 44.8 60.0 63.8 63.0 62.9 64.3 68.5 65.4 44.3 49.0 49.0 49.6 49.8 49. 51.0 56.7 58.8 54.6 59.0 63.9 67.9 63.1 62.9 62.1 68.1 56.9 59.7 72.6 73.2 72.5 73.7 74.0 73. 74.5 58.2 52.6 52.6 47.9 56.6 59.3 57.7 59.3 58.8 59.8 60.0 Avg 52. 56.8 56.9 55.5 56.5 57.4 58.7 GRPO improve RL efficiency and reliability? (5) What are the success and failure mode of VIDEOP2R? 5.1. Main Results The main evaluation results of VIDEOP2R and other baselines are shown in Tab. 1. Compared with prior video RFT approaches, VIDEOP2R achieves highly competitive performance across seven benchmarks, setting SotA results on six of them and ranking second on the remaining one. In contrast to previous RFT methods that often bring improvement on specific datasets (e.g., Video-R1 ranks second on VSI-Bench, while VideoRFT is SotA on MMVU but last on MVBench), VIDEOP2R delivers consistent gains across all benchmarks, surpassing the previous SotA by 1.3% in average accuracy. This consistency underscores the effectiveness and generalizability of modeling visual reasoning through distinct perception and reasoning processes. Compared to the base model Qwen2.5-VL, VIDEOP2R exhibits clear and steady improvements, with accuracy gains ranging from 1.9% to 9.1% across benchmarks. More broadly, performance gap exists between models trained with RFT and those trained only with SFT/Instruction Tuning [31] (i.e., open-source models), highlighting the superiority of RFT for expanding capability boundaries. We further analyze VIDEOP2Rs performance drop on MMVU (Sec. 5.5) and attribute it to the lack of domainspecific knowledge (e.g., chemistry) in our training data. 5.2. Ablation Study The success of VIDEOP2R underscores the importance of decomposing visual reasoning into distinct process stages. To further analyze the contribution of each process-aware component, we perform an ablation study on the two-stage training in the RFT framework, process-aware modeling, and reward design (Tab. 2). Two-stage Training: SFT-only and RL-only improves the baseline by 2.7% and 3.1% respectively. But combining both yield more significant 5.8% improvement, suggesting that single-stage training is insufficient, whereas using both stages can further extend the models capability. Process-aware Modeling: We evaluate process-aware modeling against process-agnostic counterpart in both SFT and RL. In SFT, the process-aware variant (same as VIDEOP2R-SFT) yields an average accuracy gain of 2.1% over the process-agnostic variant, which uses reasoning-only segments. In RL, the agnostic variant follows GRPOassigning single reasoning reward across all output tokens, while the process-aware variant (same as VIDEOP2R) again leads on six benchmarks with 2.3% on average. Under identical visual inputs, adding perception annotations in SFT and using processaware credit assignment in RL provide clearer training signals, improving video understanding and reasoning. Reward design: We examine the PA-GRPO reward function by ablating each reward component and the separation reward assignment. Removing any component causes notable drop, sometimes even below the SFT baseline (e.g., removing the perception reward yields worse performance than SFT on VideoMME). This indicates that all components are necessary and that their joint design in PA-GRPO enables fine-grained credit assignment in RL. When we remove separation and assign both perception and reasoning rewards to all output tokens, the results remain competitive but still lag behind PA-GRPO, indicating that separation reward assignment provides clearer and more effective training signals. Notably, removing Rlen improves performance on VSI-Bench. Further analysis (Supplementary) indi6 Table 2. Ablation studies of VIDEOP2R on two-stage training, process-aware modeling and reward design (all numbers unit in %). Ablation Factor Model Variant Video Reasoning Video Understanding VSI. VideoMMMU MMVU VCR. MV. TempCom. VideoMME Two-stage Training - VIDEOP2R (Ours) - SFT-only (VIDEOP2R-SFT) - RL-only Process-aware Modeling - VIDEOP2R (Ours) - process-agnostic RL (GRPO) - process-aware SFT (no RL) - process-agnostic SFT (no RL) Reward Design - VIDEOP2R (Ours) - without RR - without RP - without RL - without separation Baseline: Qwen2.5-VL-7B 36.8 35.2 35.8 36.8 37.4 35.2 34.3 36.8 36.0 37.4 40.0 37.1 30.1 55.0 53.7 54.6 55.0 53.6 53.7 48. 55.0 51.6 53.6 52.7 53.2 48.1 65.4 61.6 64.6 65.4 62.8 61.6 61.6 65.4 60.3 62.8 63.2 64.9 60. 51.0 46.9 46.3 51.0 48.3 46.9 47.3 51.0 46.8 48.3 48.4 48.8 44.3 68.1 62.3 60.8 68.1 63.8 62.3 59. 68.1 62.1 63.8 65.5 65.0 59.0 74.5 72.4 73.8 74.5 73.3 72.4 69.7 74.5 72.5 73.3 73.9 73.2 72. 60.0 57.2 55.9 60.0 55.4 57.2 54.0 60.0 57.9 55.4 60.0 59.7 56.6 Avg. 58.7+5.8 55.6+2.7 56.0+3. 58.7+5.8 56.4+3.5 55.6+2.7 53.5+0.6 58.7+5.8 55.3+2.4 56.4+3.5 57.7+4.8 57.4+4.5 52.9 cates that its questions often require long, fine-grained descriptions to ensure sufficient perception, where the length reward becomes counter-productive. We propose to have dynamic length reward [51] in future work. Finally, we further ablate different judge models in the Supplementary for perception accuracy reward assignment, and find that VIDEOP2R remains robust. Figure 4. Effect of perception on downstream reasoning 5.3. Effectiveness of Perception Representations The ablation study highlights the effectiveness of process-aware modeling in VIDEOP2R. In this section, we further examine whether the perception outputs produced by VIDEOP2R can enhance the reasoning ability of generic LVLMs (Examination details in the Supplementary). Specifically, we compare Qwen2.5-VL7Bs zero-shot performance on (i) text-only questions, (ii) text plus video inputs. For each scenario, we further augment question text with perception segment generated either by VIDEOP2R or by pre-trained Qwen. The results, as shown in Fig. 4, yield two key insights. 7 First, when Qwen2.5-VL-7B is provided only with text questions augmented by VIDEOP2Rs perceptions segments, its performance (55.5%) even surpasses that under raw video input (52.9%), indicating that the textual perceptions generated by VIDEOP2R capture semantically rich perceptual information that directly supports reasoning. Second, in both text and video modality, when we augment the prompt with perception segment, using VIDEOP2Rs perceptions consistently outperforms using Qwens own perceptions, indicating that VIDEOP2Rs perceptions provide more accurate and useful evidence for downstream reasoning. 5.4. Advantages of PA-GRPO over GRPO Figure 6(a-b) illustrates the training dynamics of VIDEOP2R. Both perception and reasoning accuracy rewards exhibit an increasing trend, which, combined with the quantitative gains in Tab. 2, confirms that PA-GRPO is more effective than standard GRPO in improving the models perception and reasoning traces. In the following, we further analyze the underlying reasons for PAGRPOs advantages over standard GRPO. (1) Training Efficiency. In advantage-based policy gradient methods [17, 42, 54], including GRPO, when all sampled responses (o1, o2, . . . , oG) for given prompt receive nearly identical rewards, this leads to advantage collapse [70], where the advantages shrink toward zero, leaving little to no effective learning signal and causing updates to stagnate [65]. PA-GRPO mitigates this by decomposing the overall reward into perception and reasoning components, so even when reasoning rewards saturate, perception rewards can still provide non-zero gradients. We visualize the number of samples in batch with advantage collapse for PA-GRPO and GRPO durFigure 5. Success (Left) and Failure (Right) case of VIDEOP2R. Correct statement and incorrect statement are colored. across VIDEOP2R and two single-reward trained models: while all SFT models maintain stable reasoning consistency (5%), of Video-R1 and VideoRFTs RL models show significant degradation (16%). In contrast, VIDEOP2R shows notably lower mismatch, indicating that PA-GRPOs process-aware rewards, which separately encourage faithful perception traces and correct final answers, effectively mitigate ThinkAnswer Mismatch and promote more reliable reasoning behavior. 5.5. Qualitative Results of VIDEOP2R We present one success and one failure case of VIDEOP2R in Fig. 5 to illustrate both its strengths and areas of improvement (More examples in the Supplementary). The left example shows an Aha Moment [17], where VIDEOP2R performs process-aware inference by accurately describing visual cues, such as the monkeys actions and its imagination, and reasoning over them to reach the correct answer. In contrast, the right example depicts failure case: although the model correctly identifies relevant visual details about the person and her behavior, it produces an incorrect conclusion due to missing domain-specific knowledge (e.g., the molar volume of gas should be 22.4). Overall, while VIDEOP2R exhibits strong capabilities in general video understanding and reasoning, its performance can be further improved by injecting factual and domain-specific knowledge. 6. Conclusion In this work, we introduced VIDEOP2R, processaware RFT framework that models perception and reasoning as distinct yet complementary processes for video understanding. Through three-step CoT generation pipeline, we constructed VIDEOP2R-CoT-162K, large-scale process-aware dataset that enables finegrained supervision in the SFT stage. In the RL stage, we proposed PA-GRPO, process-aware extension of Figure 6. Training Dynamics and Think-Answer Mismatch Analysis of VIDEOP2R. Details in Section 5.4. ing the RL stage in Fig. 6(c). Compared with GRPO, PA-GRPO consistently exhibits fewer advantage collapse samples, indicating better utilization of training samples and improved training efficiency. (2) Mitigating ThinkAnswer Mismatch. Reasoningaugmented models often exhibit ThinkAnswer Mismatch [44], where generated reasoning traces diverge from the actual decision process yet still produce correct answers (e.g., the man is on the right side, thus the answer is B. left). In GRPO, such inconsistencies can lead to reward hacking [10, 46], as one single final reward can reinforce unfaithful reasoning traces that coincidentally yield correct outcomes. To quantify this issue, we perform an alignment check (Details in Supplementary) using Claude 3.7 Sonnet to extract answers from the think segments and compare them with the final output answer. Figure 6(d) reports mismatch rates 8 GRPO that provides separate rewards for perception and reasoning to improve credit assignment. Extensive experiments across seven benchmarks demonstrate that VideoP2R achieves SotA performance and strong generalization, while further ablations verify the effectiveness of process-aware modeling and PA-GRPO."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, and Jay Pujara. The curious case of nonverbal abstract reasoning with arXiv preprint multi-modal large language models. arXiv:2401.12117, 2024. 1, 2 [3] Anthropic. Claude 3, 2024. 4 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 3, 5 [5] Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, et al. Reasoning language models: blueprint. arXiv preprint arXiv:2501.11223, 2025. 2 [6] Mustafa Chasmai, Gauri Jagatap, Gouthaman KV, Grant Van Horn, Subhransu Maji, and Andrea Fanelli. Moment sampling in video llms for long-form video qa. arXiv preprint arXiv:2507.00033, 2025. 6 [7] Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, and Tieniu Tan. Versavid-r1: versatile video understanding and reasoning model from question answering to captioning tasks. arXiv preprint arXiv:2506.09079, 2025. 1, 2, 3, 5 [8] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 5 [9] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [10] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. 8 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. 3 [12] Yann Dubois, Balazs Galambosi, Percy Liang, and TatLength-controlled alpacaeval: arXiv sunori Hashimoto. simple way to debias automatic evaluators. preprint arXiv:2404.04475, 2024. 4 [13] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong Li Lee, and Wynne Hsu. Videoof-thought: step-by-step video reasoning from percepIn Proceedings of the 41st Internation to cognition. tional Conference on Machine Learning, pages 13109 13125, 2024. 2 [14] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 1, 2, 3, 4, 5 [15] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multimodal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 1, 2, 5, 3 [16] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. 2, 4 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, 4, 7, 8 [18] Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame In Proceedings of the Computer Vision and selection. Pattern Recognition Conference, pages 2618126191, 2025. [19] Wei Han, Hui Chen, Min-Yen Kan, and Soujanya Poria. Self-adaptive sampling for accurate video question answering on image text models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 25222534, 2024. 6 [20] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 2, 5, 3 [21] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 1 [22] Yifan Jiang, Kexuan Sun, Zhivar Sourati, Kian Ahrabian, Kaixin Ma, Filip Ilievski, Jay Pujara, et al. Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning. Advances in Neural Information Processing Systems, 37:4656746592, 2024. 1, 2 [23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 3 [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 5 [25] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video underIn Proceedings of the IEEE/CVF standing benchmark. Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 2, 5, 3 [26] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. 1, 2, 3, 4, [27] Yiming Li, Xiaoshan Yang, Bing-Kun Bao, and Changsheng Xu. Graph prompts: Adapting video graph for video question answering. 2025. 2 [28] Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, and Wenbing Huang. Star-r1: Spatial transformation reasoning by reinforcing multimodal llms. arXiv preprint arXiv:2505.15804, 2025. 3 [29] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, et al. Self-rewarding visionarXiv language model via reasoning decomposition. preprint arXiv:2508.19652, 2025. 3 [30] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 4 [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 6 [32] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? In Findings of the Association for Computational Linguistics ACL 2024, pages 87318772, 2024. 1, 2, 5, [33] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025. 2 [34] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. 1, 3 [35] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and proarXiv preprint jection for dimension reduction. arXiv:1802.03426, 2018. 2 [36] Desen Meng, Rui Huang, Zhilin Dai, Xinhao Li, Yifan Xu, Jun Zhang, Zhenpeng Huang, Meng Zhang, Lingshu Zhang, Yi Liu, et al. Videocap-r1: Enhancing mllms for video captioning via structured thinking. arXiv preprint arXiv:2506.01725, 2025. 2 10 [37] Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, and Cordelia Schmid. Morevqa: Exploring modular reasoning models for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323513245, 2024. [38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 3 [39] Jinyoung Park, Jeehye Na, Jinyoung Kim, and Hyunwoo Kim. Deepvideo-r1: Video reinforcement finearXiv tuning via difficulty-aware regressive grpo. preprint arXiv:2506.07464, 2025. 2 [40] Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, and Feng Zhao. Vcr-bench: comprehensive evaluation framework for video chain-of-thought reasoning. arXiv preprint arXiv:2504.07956, 2025. 2, 5, 3 [41] Haiyi Qiu, Minghe Gao, Long Qian, Kaihang Pan, Qifan Yu, Juncheng Li, Wenjie Wang, Siliang Tang, Yueting Zhuang, and Tat-Seng Chua. Step: Enhancing videollms compositional reasoning by spatio-temporal graphguided self-training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 3284 3294, 2025. 2 [42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4, 7 [43] Ramprasaath Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz, Marco Tulio Ribeiro, Besmira Nushi, and Introspecting Ece Kamar. Squinting at vqa models: In Proceedings of the vqa models with sub-questions. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1000310011, 2020. 1, 2, [44] Si Shen, Peijun Shen, Wenhua Zhao, and Danhao Zhu. Mitigating think-answer mismatch in llm reasoning through noise-aware advantage reweighting. arXiv preprint arXiv:2508.05928, 2025. 8 [45] Yudi Shi, Shangzhe Di, Qirui Chen, and Weidi Xie. Enhancing video-llm reasoning via agent-of-thoughts distillation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 85238533, 2025. 2 [46] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. 8 [47] Zihan Song, Xin Wang, Zi Qian, Hong Chen, Longtao Huang, Hui Xue, and Wenwu Zhu. Modularized selfreflected video reasoner for multimodal llm with application to video question answering. In Forty-second International Conference on Machine Learning. 2 [48] Elizabeth Spelke and Katherine Kinzler. Core knowledge. Developmental science, 10(1):8996, 2007. 1, 2 [49] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. IEEE Transactions on Circuits and Systems for Video Technology, 2025. [50] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 1, 2 [51] Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. 7 [52] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025. 1, 2, 3, 4, 5 [53] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understandIn European ing with large language model as agent. Conference on Computer Vision, pages 5876. Springer, 2024. 2 [54] Yuhui Wang, Hao He, and Xiaoyang Tan. Truly proximal policy optimization. In Uncertainty in artificial intelligence, pages 113122. PMLR, 2020. 7 [55] Yueqian Wang, Yuxuan Wang, Kai Chen, and Dongyan Zhao. Stair: spatial-temporal reasoning with auditable In intermediate results for video question answering. Proceedings of the AAAI Conference on Artificial Intelligence, pages 1921519223, 2024. [56] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025. 1, 3, 5 [57] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. Language models with image descriptors are strong few-shot videolanguage learners. Advances in Neural Information Processing Systems, 35:84838497, 2022. 2 [58] Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, and Yali Wang. Videochat-a1: Thinking with long videos by chain-of-shot reasoning. arXiv preprint arXiv:2506.06097, 2025. 2 [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1 [60] Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in arXiv visual reasoning with reinforcement learning. preprint arXiv:2505.14677, 2025. 3 [61] Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. Video graph transformer for video question answering. In European Conference on Computer Vision, pages 3958. Springer, 2022. 11 [62] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, In Proceedings of the Computer Viand recall spaces. sion and Pattern Recognition Conference, pages 10632 10643, 2025. 2, 5, 3, 7 [63] En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, et al. Unhackable temporal rewarding for scalable video mllms. arXiv preprint arXiv:2502.12081, 2025. 5 [64] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. 3 [65] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 7 [66] Yuanyuan Yuan, Shuai Wang, Mingyue Jiang, and Tsong Yueh Chen. Perception matters: Detecting perception failures of vqa models using metamorphic testIn Proceedings of the IEEE/CVF Conference on ing. Computer Vision and Pattern Recognition, pages 16908 16917, 2021. 1, 2, [67] Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong. Reinforcing multiturn reasoning in llm agents via turn-level credit assignment. arXiv preprint arXiv:2505.11821, 2025. 5 [68] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video questionanswering. arXiv preprint arXiv:2312.17235, 2023. 2 [69] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 5 [70] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Edge-grpo: Entropy-driven grpo with guided error correction for advantage diversity. arXiv preprint arXiv:2507.21848, 2025. 7 [71] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Tinyllava-video-r1: Towards smaller lmms for video reasoning. arXiv preprint arXiv:2504.09641, 2025. 3 [72] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expertlevel multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84758489, 2025. 2, 5, 3 12 VIDEOP2R: Video Understanding from Perception to Reasoning"
        },
        {
            "title": "We provide additional details and illustrations for our",
            "content": "main content in the following section: Process-Aware CoT Generation (Sec. 7). Evaluation Setup (Sec. 8) Ablation Study on Judge Model (Sec. 9) Perception Effectiveness Experiment (Sec. 10) RL Training Trend (Sec. 11). Think-Answer Mismatch (Sec. 12). More Qualitative Results (Sec. 13). 7. Details of Process-Aware CoT Generation and Data Analysis 7.1. Prompt Used Figure 7 illustrates the prompt template for processaware CoT generation. We employ Qwen2.5-VL-72BInstruct with temperature of 0 for the generation. Figure 7. Prompt Template for Process-aware CoT Generation. We use the same prompt for training and inference. Figure 8 illustrates the prompt template for observation sufficiency verification. We use Claude 3.7 to judge the sufficiency of the observation segment. 7.2. Data Source and Metric for CoT Verification Our data source [14] encompasses five distinct question types to enhance the models flexibility and its generalization across diverse tasks and formats: (1) Multiple Choice, (2) Numerical QA, (3) OCR, (4) Free-form QA, and (5) Regression. Each data sample includes question, data source, correct answer, and optional choices 1 Figure 8. Prompt Template for Observation Sufficiency Verification. We use the same prompt for perception correctness judgment in RL stage. when applicable (e.g., for multiple-choice questions). In the subsequent CoT Verification stage, task-specific accuracy metrics are adopted to assess annotation reliability, and samples below preset threshold of 0.6 are filtered out. The task-specific metrics are listed as follows: Multiple Choice: 1 if the predicted option matches the ground truth; 0 otherwise. Numerical QA: 1 for exact match with the reference value. OCR: reward based on Word Error Rate (WER) between prediction and reference. Free-form QA: reward is the average of ROUGE-1, ROUGE-2, and ROUGE-L scores. Regression: reward = 1 relative error between prediction and ground truth. These task-specific metrics are also used for computing the reasoning accuracy reward in the RL stage. 7.3. Data Statistic Adapting our generation pipeline to the data source yields VIDEOP2R-CoT-162K, consisting of 162,062 image and video visual QA pairs with high-quality annotations on perception and reasoning. We present the data statistics in Tab. 3. The dataset covers both image and video modalities, and spans multiple question types including multiple-choice, numerical, OCR, free-form, and regression. Multiple-choice questions constitute the majority, providing stable evaluation signals, while the inclusion of numerical, OCR, and free-form questions introduces diverse reasoning skills such as counting, reading, grounded description, and open-ended inference. This heterogeneous composition enables comprehensive assessment of process-aware perception and reasoning across modalities. To analyze our constructed data, we visualize the embedding distributions (using UMAP [35]) of perception and reasoning annotations  (Fig. 9)  . The two clusters are clearly separated, indicating that our annotated data inherently distinguishes perception and reasoning. 7.4. Word Count and Word Cloud Analysis Figure 10 presents the word length distribution and word cloud visualization of VIDEOP2R-CoT-162K. As shown on the left of the figure, perception and reasoning annotations exhibit comparable number of words across the entire annotation set, suggesting balanced contribution of both processes. The word clouds further highlight the intrinsic difference in focus between the two processes: perception annotations are dominated by video-centric terms such as video, person, and observing, reflecting their emphasis on factual and descriptive content; in contrast, reasoning annotations frequently contain introspective expressions such as double check and make sense, which indicate deeper reflective reasoning. Figure 10. Word length (Left) and Word cloud (Right) Visualization for VideoP2R-CoT-162K. Figure 11. An Annotation Example of the Video QA Sample Figure 9. Embeddings visualization of VIDEOP2R-CoT-162K 7.5. Annotation Examples We provide annotation examples in Figs. 11 and 12 to illustrate how our annotations explicitly separate perception from reasoning. Figure 11 presents video QA example where the perception segment successfully captures the key visual cue (the zigzag pattern), and the reasoning segment then derives the correct answer based on this evidence. Figure 12 presents an image QA example in which the perception segment accurately extracts the numerical information from the table, and the reasoning segment performs the required mathematical reasoning over these numbers, followed by validation to doublecheck the final answer. Figure 12. Annotation Example of the Image QA Sample 8. Experiment Set up 8.1. Implementation Details The whole two-stage training is conducted on 8 NVIDIA A800 GPUs. For efficiency, we limit the video input to 16 frames at resolution of 128 28 28 during training, where 2828 denotes the patch size and 2 Table 3. Distribution of question types across VIDEOP2R-CoT-162K. Multiple Choice Numerical OCR Free-form Regression Question Type Sum Image Video Sum 47,091 86,910 18,476 1,371 4,014 134,001 19,847 4,014 2,501 1,006 3,507 693 693 72,775 89,287 162,062 128 the number of patches. For the SFT stage, we use batch size of 8 with gradient accumulation = 2. For the RL stage, we adopt batch size of 561 (8 rollouts per sample). We host Claude3.7 on the cloud for observation judgment to speed up the training process. During inference, we increase the number of frames and resolution to 32 and 256 28 28, respectively, and apply the decoding configuration (top = 0.001, temperature = 0.01) consistent with the Qwen2.5-VL official demo. During both training and inference, we adopt the same prompt  (Fig. 7)  as in process-aware CoT generation, and use the prompt shown in Fig. 8 for perception accuracy judment. 8.2. Main Table Evaluation Setup This section introduces the evaluation benchmarks used in Tab. 1 and the evaluation metrics. We selected seven widely used video understanding and reasoning benchmarks to provide comprehensive analysis of VideoP2R: VSI-Bench [62] is video-based benchmark designed to evaluate models visualspatial reasoning capability. It includes two types of questions: (1) numerical and (2) multiple-choice. Numerical questions are evaluated using Mean Relative Accuracy (MRA), while multiple-choice questions are evaluated using Accuracy (ACC). Following the original benchmark protocol, we report the overall performance as the average of MRA and ACC. is multi-modal, multidisciplinary video benchmark, designed to evaluate models ability to acquire and apply knowledge In our experilecture videos. from expert-level ments, models are evaluated with accuracy over all questions. VideoMMMU [20] MMVU [72] is an expert-level multi-disciplinary video understanding benchmark aimed at assessing models capability to perform domain-specific reasoning across diverse scientific and technical fields. In our experiments, models are evaluated with accuracy over all multiple-choice questions (1858 of 3000). 1To accelerate training, we integrate vLLM [23] for sampling, assigning one GPU exclusively for sample generation and the remaining seven for model updates. 3 VCR-Bench [40] is benchmark crafted to assess video Chain-of-Thought reasoning. VCR-Bench selected and integrated data from multiple existing video In our experiments, models are evalbenchmarks. uated with accuracy over all multiple-choice questions (510 of 1034). MVBench [25] is multi-modal video understanding benchmark designed to stress test models temporal reasoning capabilities across diverse domains. In our evaluation, models are assessed using accuracy on multiple-choice QA derived from temporally grounded tasks. TempCompass [32] is temporal reasoning benchmark designed to dissect video LLMs ability to perceive dynamic changes over time. It constructs paired videos that share identical static content but differ in temporal aspects (e.g., speed, direction) to prevent shortcut solutions based on static frames. In our evaluation, models are measured by accuracy over temporal reasoning questions under the official protocols. Video-MME [15] is comprehensive multi-modal evaluation benchmark for video-centric large language models, designed to assess their analysis capabilities across diverse video types and modalities. We evaluate using the official metrics and configuration, reporting accuracy over the QA pairs without subtitles. For all result numbers of Open-Source Models in Tab. 1, we use the reported number in the original paper. For all result numbers of RFT Models, we run the evaluation locally. We additionally include Qwen2.5-VL72B in Tab. 7 as an upper bound for our model. While VIDEOP2R still trails Qwen2.5-VL-72B on average, it significantly boosts the base model (Qwen2.5-VL-7B) and even outperforms Qwen2.5-VL-72B on MVBench, underscoring the effectiveness of our approach. 9. Ablation Study on Judge Model Table 4 presents the results of using different judge models for perception correctness judgement. We conduct the same two-stage training process, but only change the Claude3.7 to Llama3.1 [11] families for providing perception correctness judgment. Compared with the base model, all VIDEOP2R variants using different judge models achieve consistent improvements, confirming the effectiveness of perception reward supervision. To further assess judge reliability, we randomly annotate the perception correctness of 200 samples with human labels and evaluate each judges decision accuracy. We observe clear upward trend in accuracy as the judge model becomes larger and more capable. Additionally, the fact that even Llama3.1-8B attains reasonable reliability on this relatively simple perception correctness judgement suggests that perception correctness can be robustly handled by current LLMs, and our pipeline is broadly applicable across wide range of judge models. Moreover, the positive correlation between judge capability and the downstream performance of the trained model indicates that stronger judges provide more reliable perception feedback and lead to larger gains, with Claude3.7 achieving the highest agreement with human annotations and the best overall process-aware performance. 10. Details of the Perception Examination 10.2. Full Results Table 5 presents the full results of the perception examination experiment  (Fig. 4)  . Figure 14. Prompt Template for Answer Extraction. 10.1. Prompt Used and Detailed Set up 10.3. Examples of Qwen Inference Output The perception examination experiment involves three types of experiments on either text or video domains. We compare the zero-shot performance of Qwen2.5-VL7B across different input settings and examine how perception segments influence its answers: (i) performance on text-only questions, (ii) performance with both text and video inputs, and (iii) performance when the textonly prompt is augmented with perception segment generated by VIDEOP2R or Qwen2.5-VL-7B. We used the prompt Prompt for Qwen Inference (Fig. 13 Top) for (i) and (ii). The prompt Prompt for Qwen Inference with Perception Segment (Fig. 13 Bottom) is used for (iii). For (iii), We use the same prompt in Fig. 7 to get the perception segment from VIDEOP2R or Qwen2.5VL-7B first and then augment the segment within the prompt for inference. Figure 13. Prompt Template for Perception Examination Experiment. 4 We present examples of Qwens outputs under different configurations in our perception examination experiment in Fig. 21. When given only the text question (Top Left), Qwen fails to perform meaningful reasoning due to the absence of video information and resorts to guessing from the answer choices. When conditioned on the text question plus Qwens own perception segment (Top Right), it mentions some relevant visual cues (e.g., the person placing books into the backpack) but omits critical details such as the exact number of books, resulting in unreliable reasoning. Even with access to both the text question and the video input (Bottom Left), Qwen still produces inaccurate perception, confusing the top pocket with the main compartment; this misperception propagates into the reasoning process and yields an incorrect answer. In contrast, the perception segment generated by VIDEOP2R is clear and sufficient (Bottom Right), explicitly capturing both the number of books and their correct placement, thereby enabling Qwen to arrive at the correct answer even without direct access to the video. 11. RL Training Dynamics We provide the full RL training dynamics of VIDEOP2R in Fig. 15 to comprehensively illustrate our RL stage. Both the perception accuracy reward and the reasoning accuracy reward exhibit an overall increasing trend, indicating that the model progressively improves its ability to produce correct perception and reasoning traces. The perception format reward and reasoning format reward remain close to 1 throughout training, showing that the Table 4. Ablation studies of VIDEOP2R on judge models. Judge Acc. reports perception decision accuracy on 200 human-labeled samples. Best results within each group are in bold. Model Judge Acc. Video Reasoning Video Understanding VSI. VideoMMMU MMVU VCR. MV. TempCom. VideoMME Avg Base Models Qwen2.5-VL(7B) Judge Model VIDEOP2R (Llama3.1-8B) VIDEOP2R (Llama3.1-70B) VIDEOP2R (Llama3.1-405B) VIDEOP2R (Claude3.7) 82 88 91 30.1 39.0 35.8 38.2 36.8 48.1 60.0 44.3 59. 52.2 52.4 54.4 55.0 64.0 64.6 64.5 65.4 49.2 50.2 49.2 51.0 64.7 65.0 66.5 68.1 72.6 73.8 74.5 75.0 74. 56.6 52.9 59.2 60.5 58.4 60.0 57.4 57.6 58.0 58.7 Table 5. Comparison of Qwen2.5-VL-7B with and without perception segments augmentation (video/text modalities)."
        },
        {
            "title": "Video Understanding",
            "content": "VSI. VideoMMMU MMVU VCR. MV. TempCom. VideoMME Video + Text Qwen2.5-VL(7B) Qwen2.5-VL(7B) + Obs. Qwen2.5-VL(7B) + Obs.(Qwen) Text Qwen2.5-VL(7B) Qwen2.5-VL(7B) + Obs. Qwen2.5-VL(7B) + Obs.(Qwen) VideoP2R 30.1 29.7 30. 22.1 33.8 28.8 36.8 48.1 54.1 53.1 34.4 51.5 45.8 55.0 60.0 61.5 61. 45.8 61.0 60.0 65.4 44.3 49.4 48.6 33.5 48.4 41.5 51.0 59.0 65.4 61. 30.9 63.6 49.1 68.1 72.6 73.4 72.8 46.2 72.9 67.3 74.5 56.6 58.8 56. 33.1 57.5 48.0 60."
        },
        {
            "title": "Avg",
            "content": "52.9 56.0 54.9 35.1 55.5 48.6 58.7 12. Think-Answer Mismatch Analysis 12.1. Pilot Experiment We conduct pilot experiment with Claude 3.7 to assess the reliability of answer extraction using the prompt in Fig. 14. We first sample 400 responses to multiplechoice questions and ask human annotators to extract the models answers from the think segments. The annotators achieve 95% agreement, with most disagreements arising from cases where the reasoning is unclear and the model appears to guess the answer. Using the same setting, Claude 3.7 reaches 96.5% accuracy, confirming the reliability of this assessment. 12.2. Think-Answer Mismatch Example Figure 16 presents an example of ThinkAnswer Mismatch, where the model conducts correct reasoning in think but produces an incorrect final answer in answer. This mismatch highlights how relying solely on final-answer rewards can reinforce unfaithful or inconsistent behavior, underscoring the necessity of process-aware rewards in PA-GRPO. 12.3. Think-Answer Mismatch Results Table 6 reports ThinkAnswer Mismatch rates across benchmarks. For Time-R1, we omit reasoning traces, Figure 15. RL training Dynamics of VIDEOP2R model consistently adheres to the process-aware inference template and maintains stable format compliance. Since the length reward is conditioned on both accuracy and format rewards, we instead visualize the lengths of the perception and reasoning segments during RL. We observe an initial increase followed by decrease in both segments, indicating that the model adaptively adjusts its outputs and eventually converges to concise yet sufficiently informative perception and reasoning traces. 5 Table 6. ThinkAnswer Mismatch rates across models and benchmarks (lower is better)."
        },
        {
            "title": "Model",
            "content": "Qwen2.5-VL(7B) Video-R1 (SFT) VideoRFT (SFT) VideoP2R (SFT) Video-R1 VideoChat-R1 Time-R1* VersaVid-R1* VideoRFT VideoP2R"
        },
        {
            "title": "Video Understanding",
            "content": "VSI. VideoMMMU MMVU VCR. MV. TempCom. VideoMME 4.9 2.0 5.4 0.9 25.5 13.6 23.5 6.8 12.2 2.7 4.2 4.2 19.0 11.9 21.5 22.9 9.6 3.5 3.2 9.1 0.9 18.8 11.9 11.1 12.9 7.9 6.3 1.0 5.0 1.0 26.7 13.3 18.6 18.5 7. 7.8 0.6 7.3 1.4 21.7 12.4 16.7 15.1 7.4 4.6 0.5 1.6 0.9 14.5 7.3 9.5 10.9 6.4 5.1 0.8 3.6 1.2 24.3 11.3 15.6 14.4 7."
        },
        {
            "title": "Avg",
            "content": "6.3 1.6 5.2 1.5 21.5 11.7 15.5 16.9 7.5 VIDEOP2R accurately captures relevant visual cues, including the persons gestures and the background context, and leverages them to produce the correct final answer. 13.2. Failure Case Figure 16. Example of Think-Answer Mismatch. consistent with its original design, while VersaVid-R1 has too few available traces on VSI-Bench for meaningful statistics. All results are computed on the multiplechoice subsets of each benchmark. 13. More Qualitative Results of VIDEOP2R 13.1. Success Case We provide two additional success cases of VIDEOP2R in Fig. 19. In the left example, VIDEOP2R effectively tracks key visual information throughout the video: in the observation segment, it identifies the three positions where the yellow clothing is presented, supIn the right example, porting subsequent reasoning. Figure 17. Failure Cases of Key-frame Missing in VIDEOP2R We identify two representative types of failure cases for VIDEOP2R, illustrated in Fig. 17 and Fig. 18. (1) Key-frame Missing. During inference, we adopt uniform frame sampling, which may omit question-critical key frames containing essential visual evidence [6]. As shown in Fig. 17, the question asks for the number of chairs in the room. However, the sampled frames only include the view where one chair appears on the left side of the table, while the key frame showing another chair on the right side is missed. In this case, the question becomes unsolvable given the incomplete observations. Increasing the number of sampled frames can mitigate this issue, and an adaptive sampling strategy [19] can further reduce the risk of missing question6 details where the cat is on the stool and misses the later details on the robots thigh, leading to an incorrect answer. VIDEOP2R-SFT attends to both the stool and the robots thigh but misjudges the relative durations. In contrast, VIDEOP2R produces comprehensive and faithful perception trace that correctly tracks the cats locations and time spent, even explicitly ruling out distractors such as the carpet and nest, thereby enabling reliable downstream reasoning. This example demonstrates that each stage (SFT,RL) of the VIDEOP2R framework is both effective and necessary, enabling stable, longterm improvement in process-aware perception and reasoning. Figure 18. Failure Cases of Overly detailed visual configuration in VIDEOP2R critical evidence, which we leave for future work. (2) Overly detailed visual configuration. The second failure type arises when questions require tracking an excessive number of fine-grained visual details, which is particularly common in VSI-Bench [62]. Questions in VSI-Bench demand precise modeling of object layouts and relative positions across multiple regions, often exceeding the length targets ([128,320]) used during VIDEOP2Rs training. When the required descriptions fall outside this familiar length regime, the model tends to compress or drop critical perceptual details, leading to incomplete observations and subsequent reasoning errors. As shown in Fig. 18, the question targets the relative positions among the stool, sofa, and TV, while accurately specifying this configuration also requires the locations of surrounding reference objects, such as the table near the stool and the table next to the window. VIDEOP2R initially exhibits high-quality perception (e.g., correctly identifying the tables and window) but gradually introduces errors for later elements (e.g., the stool and sofa), which ultimately leads to failure in subsequent reasoning. We propose to have more dynamic length reward system [51] in future work. 13.3. From Base Model to VIDEOP2R: Stepwise"
        },
        {
            "title": "Capability Evolution",
            "content": "We present representative example in Fig. 20 to illustrate how the models perception and reasoning capabilities evolve from the base Qwen2.5-VL-7B to VIDEOP2R-SFT and finally to the RL-optimized VIDEOP2R. The question asks where the cat stays for the longest time. In the video the cat briefly starts on the stool and then spends the remaining time on the robots thigh. The base Qwen2.5-VL-7B only captures the early 7 Figure 19. Success Cases of VIDEOP2R Figure 20. From base Qwen2.5-VL-7B to VIDEOP2R-SFT and VIDEOP2R: representative example illustrating the stepwise improvement in models perception and reasoning. 8 Figure 21. Examples of Perception Examination: Top Left: Qwen with the text question only; Top Right: Qwen with the text question plus the perception segments from Qwen; Bottom Left: Qwen with the text question plus the video input; Bottom Right: Qwen with the text question plus the perception segments from VIDEOP2R. Green text denotes correct visual information or reasoning traces, while red text denotes incorrect or insufficient visual information or reasoning traces. 9 Table 7. Performance comparison on video reasoning and understanding benchmarks. Best/second-best result of each column is in bold/underline. Missing entries indicate unreported results (all numbers unit in %). Model Video Reasoning Video Understanding Avg VSI. VideoMMMU MMVU VCR. MV. TempCom. VideoMME Qwen2.5-VL-72B Open-Source 7B Models LLaVA-OneVision-7B LongVA-7B Video-UTR-7B VideoLLaMA2-7B Qwen2.5-VL-7B RFT on Qwen2.5-VL-7B Video-R1 VideoChat-R1 Time-R1 VersaVid-R1 VideoRFT VIDEOP2R (Ours) 37.2 32.4 29.2 30.1 35.8 33.9 29.0 33.7 36.8 36.8 67.0 73. 54.9 61.7 33.8 23.9 48.1 52.3 54.0 51.0 51.9 51.1 55.0 49.2 44.8 60. 63.8 63.0 62.9 64.3 68.5 65.4 44.3 49.0 49.0 49.6 49.8 49.6 51.0 56.7 58.8 54.6 59. 63.9 67.9 63.1 62.9 62.1 68.1 74.9 56.9 59.7 72.6 73.2 72.5 73.7 74.0 73.7 74. 64.5 61.9 58.2 52.6 52.6 47.9 56.6 59.3 57.7 59.3 58.8 59.8 60.0 52. 56.8 56.9 55.5 56.5 57.4 58."
        }
    ],
    "affiliations": [
        "Amazon",
        "USC"
    ]
}