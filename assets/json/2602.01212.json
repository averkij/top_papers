{
    "paper_title": "SimpleGPT: Improving GPT via A Simple Normalization Strategy",
    "authors": [
        "Marco Chen",
        "Xianbiao Qi",
        "Yelin He",
        "Jiaquan Ye",
        "Rong Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\\times$-10$\\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT."
        },
        {
            "title": "Start",
            "content": "SimpleGPT: Improving GPT via Simple Normalization Strategy Marco Chen 1 * Xianbiao Qi 2 * Yelin He 2 Jiaquan Ye 2 Rong Xiao 2 6 2 0 2 1 ] . [ 1 2 1 2 1 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In this work, we revisit Transformer optimization through the lens of second-order geometry and establish direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3-10 larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https: //github.com/Ocram7/SimpleGPT. 1. Introduction Transformer-based large language models (LLMs) (Radford et al., 2018; 2019; Brown et al., 2020; Touvron et al., 2023a;b; Dubey et al., 2024; Chowdhery et al., 2023; Liu et al., 2024; Team, 2023) have achieved state-of-the-art performance across wide range of tasks. As these models scale in depth and width, optimization stability increasingly constrains performance and scalability. Many architectural components in modern Transformerssuch as residual connections (He et al., 2016), normalization layers (Ioffe & Szegedy, 2015; Ba et al., 2016; Zhang & Sennrich, 2019; *Equal contribution 1Tsinghua University 2Intellifusion Inc.. Correspondence to: Xianbiao Qi <qixianbiao@gmail.com>. Preprint. February 3, 2026. 1 Large et al., 2024), and nonlinear activations (Shazeer, 2020)are primarily introduced to stabilize training, and in some cases to increase expressivity. Understanding optimization from principled perspective is therefore central to the design of scalable Transformer architectures. Classical optimization theory (Nesterov, 1983; 1998; Nocedal & Wright, 1999; Boyd & Vandenberghe, 2004) provides precise connection between optimization stability and second-order geometry. For twice-differentiable objective ℓ(x), the local curvature is characterized by the Hessian Hxx = 2ℓ(x). If ℓ is β-smooth, then ℓ(x) ℓ(y)2 βx y2, x, y. Standard results imply that gradient descent is stable only when the maximum tolerable learning rate η satisfies η 2 β = 2 supx Hxx2 , establishing the Hessian spectral norm as the fundamental quantity governing admissible learning rates and convergence behavior. In contrast, much of the recent literature on Transformer optimization focuses on architectural heuristics without explicitly analyzing their relationship with classical optimization theory. Techniques such as normalization placement (Vaswani et al., 2017; Wang et al., 2019; Henry et al., 2020; Qi et al., 2023a; 2025c;a), residual scaling (He et al., 2016; Bachlechner et al., 2021; Xie et al., 2025), or modified nonlinearities (Hendrycks, 2016; Shazeer, 2020) are typically justified empirically, while their impact on activation scale, Hessian geometry, and thereby optimal learning rates remains implicit. As result, the theoretical relationship between network design and classical stability conditions is not well understood, despite its relevance to training very deep and large-scale models. In this work, we bridge this gap by analyzing Transformer architectures through the central lens of Hessian-based optimization theory, while accounting for the role of activation scale. We introduce simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales through normalization immediately following linear mappings. Building on this structural property, we analyze the Hessian of the loss with respect to network activations and show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting substantially larger SimpleGPT: Improving GPT via Simple Normalization Strategy stable learning rates. By grounding Transformer design in classical optimization principles (Nesterov, 1983; 1998; 2013), our framework provides unified explanation for existing stabilization techniques and offers principled guidance for building scalable and stable models. Our contributions can be summarized as follows: We revisit Transformer optimization through the lens of second-order geometry and establish direct connection between architectural design, activation scale, the Hessian, and the maximum tolerable learning rate. We introduce SimpleGPT, new GPT architecture based on SimpleNorm, and theoretically show that this design significantly reduces Hxx2, yielding smaller Lipschitz gradient constant and enabling substantially larger stable learning rates. We demonstrate experimentally that these theoretical advantages are accompanied by consistent empirical gains across nanoGPT, LLaMA2, and LLaMA3 architectures, for model sizes ranging from 1B to 8B parameters. Specifically, when training 7B-scale models for 60K steps, our method achieves training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. 2. Related Work Normalization methods. Normalization has long been central tool for stabilizing optimization and improving convergence in deep networks. Batch Normalization (BN) (Ioffe & Szegedy, 2015) normalizes activations using mini-batch statistics and has been widely successful in convolutional architectures, but its behavior can depend on batch size and distributed synchronization. Layer Normalization (LN) (Ba et al., 2016) and its variants remove batch dependence by computing statistics across features within each sample and have become the standard in Transformers. Related methods such as Instance Normalization (IN) (Ulyanov et al., 2016), Group Normalization (GN) (Wu & He, 2018), RMSNorm (Zhang & Sennrich, 2019), and nGPT (Loshchilov et al., 2025) further tailor normalization to specific architectural or efficiency constraints. Normalization Placement in Transformers. Beyond the choice of normalization operator, its placement within Transformer blocks plays critical role in optimization stability. The original Transformer architecture adopted postnormalization (PostNorm), in which normalization follows residual addition (Vaswani et al., 2017). Subsequent largescale practice shifted toward pre-normalization (PreNorm), placing normalization before attention and MLP sublayers to improve trainability in deep networks (Wang et al., 2019). 2 Recent work further systematizes normalization placement and explores additional insertion points. Deeply Normalized Transformer (DNT) (Qi et al., 2025a) categorizes multiple strategiesincluding InputNorm, PreNorm, MidNorm, PostNorm, and QKNormand motivates them through Jacobianand gradient-stability analysis. DNT ultimately combines InputNorm, PreNorm, MidNorm, and QKNorm, while avoiding PostNorm due to its potential training instabilities. Among these placements, QK normalization (QKNorm) (Henry et al., 2020) specifically targets the attention mechanism, stabilizing the geometry of querykey interactions and mitigating softmax saturation. By treating normalization as design space over both operator and location, these works emphasize that stability and conditioning can be targeted at specific architectural subcomponents, rather than only at block outputs. As model depth increases, normalization also interacts with residual pathways and initialization. DeepNorm (Wang et al., 2022), for example, modifies residual scaling and initialization to bound parameter updates and control dynamical growth with depth, complementing normalization-placement strategies. Normalization-free Transformers. Motivated by the cost/complexity of normalization and the desire for simpler training dynamics, recent work questions whether explicit normalization is necessary in Transformers. Transformers without Normalization shows that replacing normalization layers with simple point-wise nonlinearity, Dynamic Tanh (DyT) (Zhu et al., 2025), can match normalized baselines across tasks, suggesting that an appropriate bounded nonlinearity can provide much of the stability typically attributed to LN/RMSNorm. Building on this, Stronger NormalizationFree Transformers (Chen et al., 2025) studies the design of point-wise functions more broadly and reports improved normalization-free performance via searched function family (e.g., Derf), outperforming LN/RMSNorm/DyT across multiple domains. Despite being framed as normalizationfree, these approaches fundamentally operate by controlling the norm of activations through bounded transformations, and can therefore be viewed as form of implicit normalization. Positioning of our work. While our method can be viewed as study of normalization placement in Transformers, its key distinction lies in explicitly linking architectural design to second-order optimization geometry. Rather than motivating normalization heuristically or empirically, we analyze how local normalization immediately following linear mappings stabilizes activation scale and, in turn, constrains the spectral norm of the Hessian and leads to smoother optimization landscape. This perspective yields principled characterization of the maximum tolerable learning rate and provides unified theoretical explanation for optimization stability in large Transformer models. SimpleGPT: Improving GPT via Simple Normalization Strategy 3. Preliminaries 3.2. Gradient and Hessian of Linear Projection We consider the unconstrained convex optimization problem minxRd (x), where : Rd is differentiable. Given linear projection = and loss function ℓ, the gradient and Hessian of ℓ with respect to are, 3.1. Convex and Smoothed Optimization Lipschitz gradient smoothness. If is twice differentiable, its second-order Taylor expansion at point is (y) (x) + (x), 1 2 + (y x)2f (x)(y x). The second-order term captures the local curvature. Definition 3.1 (β-smoothness). The function is said to be β-smooth if (y) (x)2 βy x2, y, x. For convex and differentiable functions, β-smoothness is equivalent to the following quadratic upper bound: (y) (x) + (x), + β 2 x2 2, y, x. This inequality plays central role in step-size (learning rate) selection for optimization methods. Gradient descent and learning rate. Consider the standard gradient descent iteration xk+1 = xk ηf (xk), where η > 0 is the learning rate. We wish to understand how the choice of η depends on the smoothness constant β, and how this choice affects convergence. Descent condition. Evaluating the quadratic upper bound with = xk+1 = xk ηf (xk) and = xk, we obtain (xk+1) (xk) ηf (xk) 2 + βη2 2 (xk)2 2. Rearranging terms gives (xk+1) (xk) η (cid:18) (cid:19) βη2 2 (xk)2 2. sufficient condition for monotone decrease of the objective is therefore 0 < η . (1) 2 β This bound characterizes the stability region of gradient descent for convex β-smooth functions. 2 β is usually known as the maximum tolerable learning rate. For convex β-smooth problems, among all fixed learning rates that ensure descent, the canonical choice is typically η = 1 β . This choice balances progress and stability and leads to the sharpest worst-case guarantees. gy := , ℓ Hyy := 2ℓ yy . The Jacobian of with respect to is Hence, according to the chain rule, we have = x = . gx = ℓ = gy = gy, and the Hessian matrix with respect to is Hxx = 2ℓ xx = x HyyJ = HyyW . (2) 4. Methodology 4.1. SimpleNorm: Unified Normalization Strategy Definition of SimpleNorm. We define SimpleNorm as placing normalization operator immediately after linear mapping. Given an input vector Rm and linear transformation Rdm, we abstract SimpleNorm as primitive operator Ψ(x) = Norm(W x), (3) where Norm() is normalization operator such as LayerNorm or RMSNorm. SimpleNorm is motivated by simple yet effective placement strategy, rather than algebraic complexity. In contrast to existing normalization techniques that typically operate at the level of residual blocks, hidden states, or parameter reparameterization, SimpleNorm enforces normalization locally and immediately after linear mapping, treating linear mapping immediately followed by normalization as single, unified operator. Definition of SimpleGPT. As illustrated in Figure 1, SimpleGPT uses SimpleNorm as fundamental building block. SimpleNorm is systematically inserted wherever linear layer appears, including MLP projections, attention projections (Q, K, V), output projections, and gating or memoryrelated modules. Take Figure 1 as an example, normalization is inserted after the Wq, Wk, Wv, Wo, W1, and W2 projections. In architectures that employ SwiGLU (Shazeer, 2020) instead of MLP, SimpleGPT inserts normalization after Wq, Wk, Wv, Wo, W1, W2, and W3. Instantiating SimpleNorm with RMSNorm In this work, we instantiate Norm() with RMSNorm (Zhang & Sennrich, 2019). Hence, SimpleNorm is now defined as: Ψ(x; , γ) = γ W x2 , (4) SimpleGPT: Improving GPT via Simple Normalization Strategy"
        },
        {
            "title": "Attn",
            "content": "Wo +"
        },
        {
            "title": "PreN",
            "content": "W1 ReLU W2 +"
        },
        {
            "title": "PreN",
            "content": "Wq Wk Wv (a) GPT. GPT adopts pre-normalization architecture. Linear projections Wq, Wk, Wv, Wo, W1 and W2 are applied."
        },
        {
            "title": "Attn",
            "content": "Ψo + Ψ"
        },
        {
            "title": "ReLU",
            "content": "Ψ2 + Ψq Ψk Ψv (b) SimpleGPT. SimpleGPT replaces all linear layers with SimpleNorm operator, denoted by Ψ. In SimpleGPT, we do not use prenorm. Figure 1. SimpleGPT vs. GPT. This figure compares the standard GPT block with the proposed SimpleGPT block, highlighting the structural simplifications introduced by SimpleNorm. where Rm, Rdm, and , γ are learnable parameters, and denotes element-wise multiply. For later analysis, we define the intermediate variables = x, = z2, = = uu, = Diag(γ), , Thus, up to the learned per-dimension scaling γ, each prod. Consejection is rescaled to have norm on the order of quently, SimpleNorm prevents intermediate representation norms from drifting with depth or weight growth, eliminating common source of activation explosion. Mechanism II shows how, in addition to activations, SimpleNorm also stabilizes curvature via the gradient Lipschitz constant. so that Ψ(x; , γ) = Du. Core Properties of SimpleNorm The following sections prove two important mechanisms of SimpleNorm: SimpleNorm directly stabilizes the scale of activations to be d, and SimpleNorm constrains the specon the order of tral norm of the Hessian of the loss w.r.t. the activations, smoothing the loss landscape and enabling larger learning rates. Moreover, we present hypothesis for why, in addition to the predicted optimization stability, SimpleNorm also exhibits strong empirical performance. 4.2. Mechanism I: Stable Activation Scale By construction, SimpleNorm stabilizes the scale of intermediate activations by normalizing immediately after each linear mapping. Recall that Ψ(x; , γ) = Du with u2 = 1. Then Ψ(x; , γ)2 = Du2. In particular, maxi γi, we have the bound letting γmin = mini γi and γmax = γmin Ψ(x)2 γmax 4.3. Mechanism II: Smoother Loss Landscape Smoothness and the Hessian. The smoothness of the objective directly constrains optimization stability: larger curvature implies smaller safe learning rates. Given twicedifferentiable β-smooth objective ℓ(x), we quantify local curvature by the activation Hessian Hxx = 2 xxℓ. Specifically, the supremum of the spectral norm of the Hessian upper bounds local curvature and governs gradient stability: β = sup Hxx(x)2. To show SimpleNorm yields smoother landscape, we prove two results: (i) the SimpleNorm Hessian decomposes as Hxx = + and in high dimension C2 L2; (ii) compared to linear projection whose curvature scales as 2 2, the SimpleNorm curvature is scale-invariant with respect to 2. Combined, these imply sn xx2 lin xx2 since 2 generally grows during training. SimpleNorm Derivatives. First, we compute the first-order gradient and second-order Hessian of ℓ with respect to x. Given = Du, let d. 4 ℓ = ℓ(y), gy := yℓ, Hyy := 2 yyℓ. SimpleGPT: Improving GPT via Simple Normalization Strategy First-order derivative. The Jacobian of the normalization satisfies P , which yields the Jacobian of with respect to x: = 1 In particular, if gy2 = O(Hyy2), then C2 L"
        },
        {
            "title": "J y",
            "content": "x := x = P . Applying the chain rule, the gradient is xℓ = gy = P gy. (5) Second-order derivative. Differentiating the gradient leads to the standard decomposition Hxx = 2 xℓ = Hyy (cid:124) (cid:125) (cid:123)(cid:122) GaussNewton term + , (cid:124)(cid:123)(cid:122)(cid:125) curvature term (6) where the first term is the GaussNewton component HyyJ = s2 Hyy , (7) and the second term is from the curvature of the normalization, s2 (cid:16) = Dgyu + uDgyP + ug DP (cid:17) . (8) Please see Appendix for detailed derivation of xℓ and 2 xℓ for SimpleNorm. For completeness, derivations of γℓ, 2 γℓ, ℓ and 2 and ℓ and 2 in Appendix and Appendix d W x2 vec(W )ℓ with = are also provided vec(W )ℓ with = γ GaussNewton Term Dominates in High Dimension. Next, we show that under standard high-dimensional and non-pathological conditions, the GaussNewton term dominates the curvature induced by normalization. Theorem 4.1 (GaussNewton dominance for SimpleNorm). Let Hxx = 2 xℓ denote the activation Hessian induced by SimpleNorm for twice-differentiable objective ℓ(y). Then, the Hessian decomposes as Hxx = + C, = (J )HyyJ , where is the curvature term induced by normalization. d, = I, Rdd has high efAssume x2 = fective rank 2 2 d, and the input and loss derivatives are not pathologically aligned with . Define /W κ := (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) W x2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 . Then κ = Θ(1) with high probability, and there exists constant τ = Θ(1) such that L2 = τ κ2 Hyy2, C2 3κ2 gy2. 5 so the GaussNewton term dominates the Hessian w.h.p. complete proof is given in Appendix D. SimpleNorm Hessian is Weight Scale-Invariant. Finally, we compare the SimpleNorm Hessians magnitude to that of plain linear projection. We show that linear curvature grows quadratically with the weight matrix spectral norm 2, whereas SimpleNorm removes this dependence. Theorem 4.2 (Linear curvature scales with 2 2 while SimpleNorm does not). Let ℓ = ℓ(y) be twice differentiable, with Hyy = 2 yyℓ and gy = yℓ."
        },
        {
            "title": "Consider the linear mapping with its Hessian",
            "content": "y1 = W1x, lin xx = 1 HyyW1, and the SimpleNorm mapping with its Hessian y2 = W2x W2x2 , sn xx = + C. Assume Hy1y1 = Hy2y2 := Hyy, W1 = W2 := , and that the conditions of Theorem 4.1 hold, such that L2 C2. Then, with high-probability, sn xx2 = Θ(cid:0)κ2 Hyy2 (cid:1) , κ2 = (cid:102)W x2 2 = Θ(1), where (cid:102)W = /W 2. Moreover, if the range of (cid:102)W is not adversarially aligned with the leading eigenspace of Hyy, then there exists constant clin = Θ(1) such that lin xx2 = HyyW 2 clin 2 2 Hyy2. Consequently, as 2 grows during training, lin xx2 sn xx2 (with high probability). Intuitively, SimpleNorm removes the dependence of curvature on weight scale by normalizing activations, whereas linear projection amplifies curvature as 2 grows. We provide proof for Theorem 4.2 in Appendix E. SimpleNorm Enables Larger Learning Rates. For twice-differentiable β-smooth objective, the maximum stable learning rate of gradient descent is inversely proportional to β, the Lipschitz constant of the gradient, which is equivalent to the supremum of the spectral norm of the Hessian: η β where β = supx Hxx(x)2 Theorem 4.1 and Theorem 4.2 establish that, under standard high-dimensional and non-pathological conditions, the SimpleGPT: Improving GPT via Simple Normalization Strategy SimpleNorm Hessian is invariant to the spectral norm of the weight matrix whereas the Hessian of linear projection scales quadratically with the weight norm. ing rates, we adjust weight decay accordingly. Additional architectural, hyperparameter, and training details are provided in Appendix and Appendix G. Consequently, since the weight spectral norm generally grows throughout training, the SimpleNorm-based SimpleGPT architecture has smoother loss landscape that can tolerate significantly larger learning rates compared to methods based on direct linear projections. 4.4. Interpretation: Beyond Optimization Stability We have established two core properties of SimpleNorm: (i) it stabilizes activation scale at Θ( d), and (ii) it smooths the loss landscape by constraining the spectral norm of the activation Hessian, enabling larger and more stable learning rates. Although these properties explain the improved optimization stability of SimpleNorm, they do not fully account for the strong empirical performance observed in Section 5. We hypothesize that SimpleNorm provides additional benefits at more global representational level. By normalizing immediately after each linear projection, SimpleNorm ensures that every layer induces genuinely nonlinear transformation, even in regimes where the surrounding network would otherwise behave nearly linearly. This effectively increases the depth of nonlinear interactions and enhances expressive capacity without increasing parameter count. Under this view, SimpleNorm improves performance through dual effect: locally, by improving optimization geometry via reduced curvature variability; and globally, by increasing expressiveness through pervasive normalizationinduced nonlinearity. We believe this combination explains why SimpleNorm yields consistent empirical gains beyond what would be expected from learning-rate stability alone. 4.5. Use torch.compile to Speedup Training Normalization layers are memory-bound and frequently executed, making them potential bottleneck. By fusing reduction and pointwise operations and leveraging torch.compile, SimpleNorms increased normalization overhead is largely amortized, resulting in around 3% training-time increase compared to GPT with QKNorm. 5. Experiments Experimental settings. We evaluate SimpleNorm on three Transformer backbones: nanoGPT, Llama2, and Llama3. SimpleNorm is applied to all Transformer blocks, excluding the embedding and output layers. All models are trained using the AdamW optimizer (Kingma & Ba, 2014; Loshchilov & Hutter, 2019) with cosine learning-rate scheduling with bfloat16 precision. Learning rates are tuned for each method. Since SimpleNorm permits significantly larger stable learn5.1. Largest Tolerable Learning Rate We evaluate the largest tolerable learning rate by comparing optimization stability across different normalization schemes while keeping all other training settings fixed. As shown in Figure 2, PreNorm already exhibits convergence issues at learning rate of 2 103. In contrast, PreNorm+QKNorm remains stable at 2103 and 2102, but becomes unstable when the learning rate is increased to 2 101. SimpleNorm shows stable convergence at both 2 103 and 2 102, and is notably more stable than PreNorm+QKNorm at 2 101. Overall, these results suggest that SimpleNorm consistently tolerates larger learning rates, indicating improved optimization robustness. Figure 2. The largest admissible learning rate for Llama2-B, Llama2-1B with QKNorm, and SimpleGPT-1B. 5.2. SimpleGPT 1B based on Llama2 In this subsection, we evaluate SimpleGPT 1B and compare it against the standard Llama2 1B as well as Llama2 1B with QKNorm. We train the model for 200K steps (following (Zhang et al., 2024)), with global batch size of 256 and sequence length of 512, resulting in approximately 26B training tokens. We train all models on the C4 dataset following the same training recipe as their corresponding baselines. The results are presented in Figure 4. The loss curve is smoothed by 80% in Tensorboard. For all experiments, we report the training loss of the last step. In Figure 4, SimpleGPT 1B achieves notable improvement over Llama2 1B with QKNorm. Specifically, the training loss is reduced from 2.478 to 2.446, corresponding to an absolute improvement of 0.032. Hence, SimpleGPT provides measurable gains, even at the small 1B scale. 6 SimpleGPT: Improving GPT via Simple Normalization Strategy (a) SimpleGPT 7B with 20K steps. (b) SimpleGPT 7B with 40K steps. (c) SimpleGPT 7B with 60K steps. Figure 3. The training loss curves of Llama2 7B, Llama2 7B with QKNorm and SimpleGPT 7B under 20K, 40K and 60K training steps. of SimpleGPT is stable rather than transient early-training effect. Finally, we observe clear scaling trend with respect to model size. While the 1B model trained on 26B tokens achieves modest improvement of approximately 0.03, the 7B model trained on 24B tokens exhibits substantially larger gain of 0.08. 5.4. SimpleGPT 8B based on Llama3 At the 8B scale, our experiments are based on the Llama3 8B architecture. We train both SimpleGPT 8B and Llama3 8B on the C4 dataset with global batch size of 192 and sequence length of 2048. We conduct training for 20K steps, corresponding to approximately 8B training tokens. We do not train for more steps due to compute constraints. SimpleGPT 8B employs 3 larger learning rate than Llama3 8B, and as shown in Figure 6, achieves substantially lower training loss. Moreover, the magnitude of the performance gain is consistent with that observed for the 7B model, suggesting that our method exhibits favorable scaling behavior with increasing model size. 5.5. SimpleGPT 1.4B based on nanoGPT Finally, we evaluate SimpleGPT 1.4B on the nanoGPT code base. All models are trained for 100K steps, corresponding to approximately 50B tokens. SimpleGPT 1.4B is trained using learning rate that is 3 larger than the baseline. We report validation losses in Figure 7. Note that, since validation loss is recorded once every 1,000 steps, the curves in Figure 7 appear different compared to earlier figures. We observe that GPT-2 with QKNorm achieves nearly identical performance to the original GPT-2, indicating that QKNorm alone provides limited benefits in this setting. Consistent with the results on LLaMA2 1B, SimpleGPT 1.4B based on nanoGPT yields an improvement of approximately 0.043. These findings suggest that the gains introduced by SimpleNorm are stable across architectures. Figure 4. The training loss curves of Llama2 1B, Llama2 1B with QKNorm and SimpleGPT 1B under 200K training steps. 5.3. SimpleGPT 7B based on Llama We compare SimpleGPT 7B against the standard Llama2 7B and Llama2 7B with QKNorm in Figure 3. We train the models for 20K, 40K, and 60K steps, corresponding to approximately 8B, 16B, and 24B tokens, respectively. All models are trained on the C4 dataset following the same training recipe as their corresponding baselines. SimpleGPT 7B uses 0.001 learning rate, which is 3 larger than that used in Llama2 (Touvron et al., 2023b) 7B model. We make the following observations. First, the performance gain of SimpleGPT over Llama2+QKNorm is consistently significant throughout training: the improvement reaches 0.062 at 20K steps, increases to 0.077 at 40K steps, and remains at comparable level (0.082) at 60K steps. Second, SimpleGPT maintains more stable training dynamics compared to Llama2 with QKNorm. Third, as training progresses and more tokens are observed, the relative improvement does not diminish, indicating that the advantage 7 SimpleGPT: Improving GPT via Simple Normalization Strategy (a) SimpleGPT 1B with lr=2e-4. (b) SimpleGPT 1B with lr=2e-3. (c) SimpleGPT 1B with lr=2e-2. Figure 5. Overall comparison across Llama2 1B, Llama2 1B with QKNorm and SimpleGPT 1B under three different learning rates. Adam-mini uses 2 104 learning rate. In SimpleGPT, we enable 10 learning rate and obtain better performance. Figure 6. The training loss curves of Llama3 8B, Llama3 8B with QKNorm and SimpleGPT 8B. 5.6. Ablation Study Different learning rates. As shown in Figure 5, we conduct experiments on 1B model using three different learning rates. Under the learning rate 2 104, LLaMA2 1B with QKNorm only slightly outperforms the original LLaMA2 1B. When the learning rate is increased to 2 103, the improvement from QKNorm becomes more pronounced, which we attribute to the smoother optimization landscape induced by QKNorm in comparison to PreNorm. Importantly, across all learning rates, SimpleGPT achieves consistent improvement over LLaMA2 1B with QKNorm. For fair comparison, reported results are obtained under the bestperforming configuration of LLaMA2 1B with QKNorm. 5.7. Discussion about training time We compare the training speed of our SimpleGPT 8B model with that of Llama3 8B with QKNorm. On average, the Llama3 8B model requires 1553 ms per training step while 8 Figure 7. The validation loss curves of GPT2 1.4B, GPT2 1.4B with QKNorm and SimpleGPT 1.4B under 100K training steps. our SimpleGPT 8B model takes 1603 ms per step. This corresponds to reasonable slowdown of around 3%, which can likely be further reduced by more clever kernel design or swapping the normalization operator to more fusionfriendly point-wise functions like Derf (Chen et al., 2025). 6. Conclusion In this work, we revisit Transformer optimization from second-order perspective and establish direct connection between architectural design, the Hessian matrix, and optimization stability. By introducing SimpleNorm and analyzing its induced Hessian structure, we show that reducing the Hessian norm of the activation with respect to the loss enables substantially larger admissible learning rates. The resulting model, SimpleGPT, reliably supports learning rates up to 3-10 larger than strong baselines while maintaining stable optimization. Across extensive experiments on nanoGPT-, Llama2 and Llama3-style models, spanning parameter scales from 1B to 8B, our method consistently achieves substantially stronger performance than GPT with QKNorm. Importantly, these gains are obtained with minimal additional computational overhead. SimpleGPT: Improving GPT via Simple Normalization Strategy"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Horn, R. A. and Johnson, C. R. Matrix analysis. Cambridge university press, 2012. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448 456. PMLR, 2015. Karpathy, A. NanoGPT. https://github.com/ Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. karpathy/nanoGPT, 2022. arXiv preprint arXiv:1607.06450, 2016. Bachlechner, T., Majumder, B. P., Mao, H., Cottrell, G., and McAuley, J. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pp. 13521361. PMLR, 2021. Boyd, S. and Vandenberghe, L. Convex optimization. Cambridge university press, 2004. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, M., Lu, T., Zhu, J., Sun, M., and Liu, Z. Stronger normalization-free transformers. arXiv preprint arXiv:2512.10938, 2025. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gage, P. new algorithm for data compression. The Users Journal, 12(2):2338, 1994. Golub, G. H. and Van Loan, C. F. Matrix computations. JHU press, 2013. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Hendrycks, D. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Henry, A., Dachapally, P. R., Pawar, S. S., and Chen, Y. Query-key normalization for transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 42464253, 2020. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Large, T., Liu, Y., Huh, M., Bahng, H., Isola, P., and Bernstein, J. Scalable optimization in the modular norm. arXiv preprint arXiv:2405.14813, 2024. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Loshchilov, I. and Hutter, F. Fixing weight decay regularization in adam. In International Conference on Learning Representations, 2019. Loshchilov, I., Hsieh, C.-P., Sun, S., and Ginsburg, B. ngpt: Normalized transformer with representation learning on the hypersphere. The Thirteenth International Conference on Learning Representations, 2025. Magnus, J. R. and Neudecker, H. Matrix differential calculus with applications in statistics and econometrics. John Wiley & Sons, 2019. Nesterov, Y. method for unconstrained convex minimization problem with the rate of convergence (1/kˆ 2). In Doklady an ussr, volume 269, pp. 543547, 1983. Nesterov, Y. Introductory lectures on convex programming volume i: Basic course. Lecture notes, 3(4):5, 1998. Nesterov, Y. Introductory lectures on convex optimization: basic course, volume 87. Springer Science & Business Media, 2013. Nocedal, J. and Wright, S. J. Numerical optimization. Springer, 1999. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Petersen, K. B., Pedersen, M. S., et al. The matrix cookbook. Technical University of Denmark, 7(15):510, 2008. 9 SimpleGPT: Improving GPT via Simple Normalization Strategy Qi, X., Wang, J., Chen, Y., Shi, Y., and Zhang, L. Lipsformer: Introducing lipschitz continuity to vision transformers. In The Eleventh International Conference on Learning Representations, 2023a. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Qi, X., Wang, J., and Zhang, L. Understanding optimization of deep learning via jacobian matrix and lipschitz constant. arXiv preprint arXiv:2306.09338, 2023b. Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., and Wei, F. Deepnet: Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022. Qi, X., Chen, M., Xiao, W., Ye, J., He, Y., Li, C.-G., and Lin, Z. Dnt: deeply normalized transformer that can be trained by momentum sgd. arXiv preprint arXiv:2507.17501, 2025a. Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., and Chao, L. S. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019. Wu, Y. and He, K. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 319, 2018. Xie, Z., Wei, Y., Cao, H., Zhao, C., Deng, C., Li, J., Dai, D., Gao, H., Chang, J., Zhao, L., et al. mhc: Manifold-constrained hyper-connections. arXiv preprint arXiv:2512.24880, 2025. Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Zhang, Y., Chen, C., Li, Z., Ding, T., Wu, C., Kingma, D. P., Ye, Y., Luo, Z.-Q., and Sun, R. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024. Zhu, J., Chen, X., He, K., LeCun, Y., and Liu, Z. Transformers without normalization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1490114911, 2025. Qi, X., He, Y., Ye, J., Li, C.-G., Zi, B., Dai, X., Zou, Q., and Xiao, R. Taming transformer without using learning rate warmup. The Thirteenth International Conference on Learning Representations, 2025b. Qi, X., Ye, J., He, Y., Li, C.-G., Zi, B., Dai, X., Zou, Q., and Xiao, R. Stable-transformer: Towards stable transformer training, 2025c. URL https://openreview. net/forum?id=lkRjnNW0gb. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Roformer, Y. L. Enhanced transformer with rotary position embedding., 2021. DOI: https://doi. org/10.1016/j. neucom, 2023. Team, Q. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Ulyanov, D., Vedaldi, A., and Lempitsky, V. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. 10 SimpleGPT: Improving GPT via Simple Normalization Strategy The derivations of the equations appearing in both the main text and the appendix are available in the following material (Golub & Van Loan, 2013; Horn & Johnson, 2012; Petersen et al., 2008; Magnus & Neudecker, 2019; Nesterov, 1983; 1998; 2013; Boyd & Vandenberghe, 2004; Qi et al., 2025b; 2023b; 2025a). A. Derivatives of xℓ and 2 xℓ for SimpleNorm"
        },
        {
            "title": "We consider the mapping",
            "content": "y = γ W x2 , where γ Rd, and denotes elementwise multiplication and ℓ = ℓ(y) is scalar loss. As before, let us define some intermediate variables, = x, = z2, = , = uu, = Diag(γ), where , are symmetric."
        },
        {
            "title": "Let us define the normalized and scaled output as",
            "content": "and let = u, gy := yℓ, Hyy := yyℓ. Let us define the Jacobian of the normalization map with respect to as and consequently := z = 1 , = z = P . We will repeatedly use the chain rule through the path ℓ, and similarly γ ℓ. (1) First-order derivative. Since = x, we have From = z/s and = u/z, it follows that dz = dx. du = dz = 1 P dz. Therefore, so the Jacobian is Applying the chain rule gives dy = du = W dx, := = x W = P . xℓ = gy = ADgy. Using = and = D, we obtain xℓ = A gy = P gy. (9) (2) Second-order derivative. Differentiating xℓ = A gy yields SimpleGPT: Improving GPT via Simple Normalization Strategy d(xℓ) = (cid:16) dgy + dA gy (cid:17) ."
        },
        {
            "title": "This induces the standard decomposition",
            "content": "2 xℓ = HyyJ + C, where the two terms are computed below. (A) Linear (GaussNewton) term. Since dgy = Hyy dy, and dy = dx, we have"
        },
        {
            "title": "Substituting into",
            "content": "W dgy gives dgy = Hyy dx. A dgy = A Hyy dx = HyyJ dx. Using = /s and x = ( d/s)DP , we obtain the explicit form HyyJ = s2 Hyy . (B) Curvature (normalization) term. Recall = 1 . Using du = dz and ds = dz2 = udz, one obtains dA = (cid:16) 1 (udz)P + dz + dzP (cid:17) . Right-multiplying by gy gives dA gy = (cid:16) 1 s2 (udz)P gy + dz uD gy + dzP gy (cid:17) . Equivalently, pulling the scalar contractions to the left yields the linear map in dz: dA gy = (cid:16) 1 s2 Dgyu + uDgyP + ug DP (cid:17) dz. Substituting dz = dx and left-multiplying by gives the curvature term = s2 (cid:16) Dgyu + uDgyP + ug DP (cid:17) . (If desired, define g1 := Dgy to simplify the expression, but we keep the figures symbols explicit.) Combining the linear and curvature terms, the Hessian with respect to is Hxx = 2 xℓ = s2 Hyy s2 (cid:16) , = uu, = Diag(γ). where = x, = z2, = Dgyu + uDgyP + ug DP (cid:17) , (10) 12 SimpleGPT: Improving GPT via Simple Normalization Strategy B. Derivatives of ℓ and 2 vec(W )ℓ for = x"
        },
        {
            "title": "We consider the linear mapping",
            "content": "y = x, where Rdm, Rm, and Rd. Let ℓ = ℓ(y) be scalar-valued loss function. We define the firstand second-order derivatives of the loss with respect to as gy := yℓ(y) Rd, Hyy := 2 yℓ(y) Rdd. (1) First-order derivative with respect to . Since depends linearly on , first-order variation satisfies Applying the chain rule, dy = dW x. dℓ = dy = (dW x) = gyx, dW ."
        },
        {
            "title": "Matching coefficients under the Frobenius inner product yields the gradient",
            "content": "W ℓ = gy x. (11) (2) Second-order derivative with respect to . To express second-order derivatives, it is convenient to vectorize . Using the standard identity for matrixvector products, which makes the dependence of on vec(W ) explicit. = (x Id) vec(W ), Jacobian with respect to vec(W ). From the above expression, the Jacobian of with respect to vec(W ) is vec(W ) vec(W ) = Id Rd(md). Hessian with respect to vec(W ). Because the mapping (cid:55) is linear, it contributes no second-order term. Hence, by the second-order chain rule, the Hessian of the loss with respect to vec(W ) is 2 vec(W )ℓ = vec(W ) Hyy vec(W ). Substituting the explicit Jacobian gives us 2 vec(W )ℓ = (x Id) Hyy (x Id). Kronecker-product simplification. Using standard identities of the Kronecker product, the Hessian simplifies to 2 vec(W )ℓ = (xx) Hyy. (12) This result shows that the Hessian with respect to the weight matrix factorizes into Kronecker product of an input-dependent term xx and an output-space curvature term Hyy. Equivalently, when viewed as block matrix with blocks of size d, the (i, j)-th block is (cid:2)2 vec(W )ℓ(cid:3) (i,j) block = xixj Hyy, where xi denotes the i-th entry of x. 13 SimpleGPT: Improving GPT via Simple Normalization Strategy C. Derivatives of γℓ, 2 γℓ, ℓ and 2 vec(W )ℓ where = γ x x2 ."
        },
        {
            "title": "We consider the mapping",
            "content": "y = γ W x2 , where denotes elementwise multiplication and ℓ = ℓ(y) is scalar loss. Define the intermediate variables = x, = z2, = , = uu, = Diag(γ)."
        },
        {
            "title": "We have",
            "content": "= Du, ℓ = ℓ(y), gy := yℓ, Hyy := yyℓ. Remind two useful Jacobian matrices, z = = 1 , = z DA = DP . We will repeatedly use the chain rule through the path ℓ, and similarly γ ℓ. C.1. Part I: derivatives w.r.t. γ (1) First-order derivative. Since yi = γiui, the Jacobian of w.r.t. γ is diagonal: = γ Diag(u). Applying the chain rule gγ = (cid:0) γ (cid:1) gy, we obtain gγ = γℓ = Diag(u) gy = (u gy). (2) Second-order derivative. Because is linear in γ, 2y Hγγ = 2 γγℓ = (cid:17) (cid:16) γ Hyy γ 2 = 0, hence the Hessian comes purely from Hyy: (cid:16) γ = Diag(u) Hyy Diag(u). (cid:17) C.2. Part II: derivatives w.r.t. (1) First-order derivative. Step 1 (y z): gradient w.r.t. By the chain rule, gz = (cid:17) (cid:16) z gy = (cid:16) DP (cid:17) gy. Using = and = D, this simplifies to gz = gy. Step 2 (z = ): gradient w.r.t. We write the differential dz = dW x, so dℓ = dz = (dW x) = gzx, dW . Matching coefficients under the Frobenius inner product yields gW = ℓ = gzx = (P Dgy) x. If one needs the vectorized mapping, vec(z) = vec(W x) = (x I) vec(W ). 14 (13) (14) (15) SimpleGPT: Improving GPT via Simple Normalization Strategy (2) Second-order derivative. We first derive Hzz := 2 zzℓ, then lift it to vec(W ). Step 1: Hessian w.r.t. The second-order chain rule gives Hzz = (cid:17) (cid:16) z"
        },
        {
            "title": "Hyy",
            "content": "(cid:17) (cid:16) z + (cid:88) k=1 (gy)k zzyk. The first term is the GaussNewton part: (cid:17) (cid:16) z"
        },
        {
            "title": "Hyy",
            "content": "(cid:17) (cid:16) z = s2 Hyy . For the second term, we use the bilinear form of the normalization Hessian: 2u(z)[a, b] = (cid:16) 1 s2 (ua) + (ub) + (aP b) (cid:17) . Since = dDu, Substituting gives (gy)k 2yk[a, b] = 2y[a, b] = (Dgy)2u[a, b]. (cid:88) k=1 (cid:88) k=1 (gy)k 2yk[a, b] = s2 Equivalently, the associated matrix form is (cid:16) (cid:17) (ua)(Dgy)P + (ub)(Dgy)P + (uDgy) aP . (cid:88) k=1 (gy)k 2 zzyk = (cid:16) s2 Dgyu + uDgyP + ug DP (cid:17) . Combining both terms, Hzz = s2 Hyy (cid:16) s2 Dgyu + uDgyP + ug DP (cid:17) , = x2. Using vec(z) = (x I) vec(W ), Hvec(W ) vec(W ) = (xx) Hzz (cid:34) = (xx) s2 Hyy (cid:16) s2 Dgyu + uDgyP + ug DP (cid:35) (cid:17) . (16) SimpleGPT: Improving GPT via Simple Normalization Strategy D. Proof of Theorem 4.1 Proof. Recall SimpleNorm: = γ W , = x, := z2, := , := uu. Assume = Diag(γ) = I. Let ℓ = ℓ(y) and define gy := yℓ(y) Rd, Hyy := 2 yyℓ(y) Rdd. By the chain rule, we have the decomposition: Hxx = 2 xℓ = Hyy (cid:124) (cid:125) (cid:123)(cid:122) GaussNewton term + (cid:124)(cid:123)(cid:122)(cid:125) curvature term"
        },
        {
            "title": "Here the Jacobian is",
            "content": "and therefore the GaussNewton term is"
        },
        {
            "title": "J y",
            "content": "x = DP = P , = (J )HyyJ = s2 Hyy . The curvature term (with the condition = I) can be written as = s2 (cid:16) (P gy)u + (ugy) + u(g ) (cid:17) (17) (18) (19) , where ugy is scalar. Bounding the GaussNewton term. Define κ := x2 2 We show that κ = Θ(1) under high effective rank and typical input. Let = sphere or subgaussian). Then EW x2 and by standard concentration for quadratic forms, 2 = ξW ξ = 2 , ξ where ξ is isotropic (e.g., uniform on the x2 2 F = x2 F with high probability. Thus κ = x2 2 2 F = (cid:115) reff (W ) , reff (W ) := 2 2 2 . If reff (W ) cd, then κ 1/ = Θ(1) with high probability. Additionally, define τ := (cid:13) (cid:102)W Hyy (cid:102)W (cid:13) (cid:13) (cid:13)2 Hyy2 [0, 1], (cid:102)W := W 2 . Under the theorems non-pathological alignment assumption (i.e., the dominant spectral modes of Hyy are not eliminated by projecting out span(u) and are represented in the range of , which has high effective rank), we have τ = Θ(1). Now, it follows that = x2 P Hyy = κ2 2 2 Hyy = κ2 (cid:102)W Hyy (cid:102)W and L2 = τ κ2 Hyy2 where τ = Θ(1) and κ = Θ(1) [w.h.p] (20) 16 SimpleGPT: Improving GPT via Simple Normalization Strategy Bounding the curvature term. From Equation 19 and submultiplicativity, C2 s2 2 2 (cid:13) (cid:13)(P gy)u + (ugy) + u(g (cid:13) (cid:13) (cid:13) ) (cid:13)2 . Now use u2 = 1, 2 = 1, and gy2 gy2: (P gy)u2 = gy2u2 gy2, (ugy) 2 = ugy 2 gy2, u(g )2 = u2 gy2 gy2. By triangle inequality, the middle norm is at most 3gy2, hence C2 W x2 2 2 2 gy2 = 3κ2 gy2. (21) Dominance of over C. Combining (20) and (21), C2 L2 3 τ gy2 Hyy2 . Assuming τ = Θ(1) (non-pathological alignment) and gy2/Hyy2 = O(1) (bounded gradient-to-curvature ratio, as stated in the theorem assumptions), we obtain C2 L2 = O(d1/2), so in high dimension L2 C2 with high probability. Therefore the GaussNewton term dominates Hxx in typical non-pathological regimes. This completes the proof of Theorem 4.1. 17 SimpleGPT: Improving GPT via Simple Normalization Strategy E. Proof of Theorem 4.2 Proof. Provided ℓ = ℓ(y) is twice differentiable. Denote gy := yℓ, Hyy := 2 yyℓ, We have the two mappings: Linear: y1 = W1x,"
        },
        {
            "title": "H lin",
            "content": "xx = 1 Hy1y1W1. SimpleNorm: y2 = W2x W2x , where = Diag(γ) and define = W2x, = z2, = z/s, = uu. The Hessian w.r.t. admits the standard decomposition"
        },
        {
            "title": "H sn",
            "content": "xx = + C, = y2 Hy2y2J y2 , where y2 is the Jacobian of y2 w.r.t. x, and is the curvature term induced by the normalization. d, = I, 2 = 1, W2 has high Assuming the high-dimensional conditions stated in Theorem 4.1 hold (x2 = effective rank, no pathological alignment, and gy2/Hyy2 = O(1)), Theorem 4.1 shows that the Gauss-Newton term dominates the curvature term, i.e., L2 C2. Therefore, we obtain the approximation sn We now show an integral result: the spectral norm of lin whereas the spectral norm of sn = W1 = W2 and Hy1y1 = Hy2y2 := Hyy. xx is directly proportional to the spectral norm of the weight matrix xx is independent of the spectral norm of the weight matrix. In the following, we assume xx2 L2. Define (cid:102)W := functions of α, it follows that: α where α := 2. Consequently, (cid:102)W 2 = 1 and = α (cid:102)W . Now, treating lin xx and sn xx as lin xx(α)2 = 1 Hy1y1W12 = α2 (cid:102)W Hyy (cid:102)W 2 (22) and sn xx(α)2 L(α)2 = x2 2 P Hyy 2 = (cid:102)W x2 2 (cid:102)W Hyy (cid:102)W (23) Note that = uu is independent of α: u(α) = W x2 = α (cid:102)W α (cid:102)W = (cid:102)W (cid:102)W x2 , = (α) = u(α)u(α) = (1). Therefore, lin xx(α)2 depends quadratically on α whereas sn xx(α)2 is scale-invariant in α. Next, we compare the magnitudes of the two Hessians. First, recall the definition of κ provided in Theorem 4.1: κ := x 2 = α (cid:102)W x2 α (cid:102)W 2 = (cid:102)W By definition, κ2 = (cid:102)W x2 2 , and Theorem 4.1 implies κ2 = Θ(1) w.h.p. Moreover, since (cid:102)W 2 = 2 = 1, sn xx(α)2 L2 = κ2 (cid:102)W Hyy (cid:102)W 2 κ2 Hyy (24) For the linear module, there exists constant clin > 0 such that 18 SimpleGPT: Improving GPT via Simple Normalization Strategy lin xx(α)2 = α2 (cid:102)W Hyy (cid:102)W 2 α2clin Hyy2. We assume Range( (cid:102)W ) is not adversarially aligned with the leading eigenspace of Hyy; in particular the overlap is constant-order, so the visibility constant satisfies clin = Θ(1) and does not decay with d. Combining all the aforementioned results, the final ratio between the two Hessians satisfies: lin sn xx(α)2 xx(α)2 α2 (cid:102)W Hyy (cid:102)W 2 κ2 Hyy α2 clin κ2 Empirically, α = 2 typically grows to tens or hundreds during training, so α2clin κ2 (recall κ = Θ(1) w.h.p.). Hence: lin xx2 sn xx2 (with high probability). In summary, in non-pathological cases, the gradient Lipschitz constant of lin constant of sn xx2. xx2 is much larger than the gradient Lipschitz This completes the proof of Theorem 4.2. F. Detailed Experimental Settings Our SimpleGPT models are built on Llama2, Llama3, and nanoGPT (a GPT-2 implementation). We apply the SimpleNorm operator to all Transformer blocks except the embedding layer and classification layer. Our implementations are based on the Adam-mini 1 (Zhang et al., 2024) and nanoGPT 2 (Karpathy, 2022). Below we briefly describe the main architectural and training settings for each backbone. nanoGPT is lightweight and efficient implementation of the GPT-2 architecture. It uses the GELU activation function and byte-pair encoding (BPE) tokenizer (Gage, 1994) consistent with GPT-2 (Radford et al., 2019), with an expanded vocabulary size of 50,257 tokens. 2,000 steps are used for learning rate warmup. Our training data on nanoGPT models is OpenWebText. Llama2 adopts the SwiGLU (Shazeer, 2020) activation function in the feed-forward networks, which improves expressivity and parameter efficiency. Positional information is encoded using Rotary Positional Embeddings (RoPE) (Su et al., 2023). Llama2 also introduces Grouped-Query Attention (GQA) to reduce inference-time memory usage and computational cost. The model uses SentencePiece-based BPE tokenizer with vocabulary size of 32K tokens. In our experiments, 1% of the total steps are allocated for learning rate warmup. Our training data on Llama2 models is C4. Llama3 follows the dense Transformer design of Llama2, while introducing several targeted changes. It continues to use GQA with eight key-value heads to improve decoding efficiency and reduce key-value cache size. major difference lies in the tokenizer: Llama 3 adopts significantly larger vocabulary of 128K tokens, combining tokens from the tiktoken tokenizer with additional multilingual tokens, which improves compression rates and language coverage. To better support long contexts, the RoPE base frequency is increased to 500,000. In our experiments, 1% of the total steps are allocated for learning rate warmup. Our training data on Llama3 models is C4. Across all experiments, we adopt the AdamW optimizer (Kingma & Ba, 2014; Loshchilov & Hutter, 2019) with β1 = 0.9 and β2 = 0.95. Since we know that weight decay is associated with the learning rate, and our method permits the use of larger learning rates, we accordingly adjust the weight decay. Unless otherwise stated, weight decay value of 0.1 is used throughout our experiments. Additional hyperparameter configurations are summarized in Table 1. All models are trained using PyTorch (Paszke et al., 2019) with bfloat16 precision on A800 GPUs. We employ cosine learning rate schedule for all training runs. 1https://github.com/zyushun/Adam-mini 2https://github.com/karpathy/nanoGPT 19 SimpleGPT: Improving GPT via Simple Normalization Strategy G. Parameters and configurations of SimpleGPT Table 1. Model configurations for different scales of SimpleGPT. The models 1B and 7B are based on Llama 2, the model 8B is based on Llama 3, and the model 1.4B is based on nanoGPT. SimpleGPT 1B SimpleGPT 7B SimpleGPT 8B SimpleGPT 1.4B Origin from Layers Model Dimension FFN Dimension Attention Heads Key / Value Heads Activation Function Vocabulary Size Positional Embeddings (RoPE) Batch Size Training Steps Warmup Steps Llama2 18 2,048 5,632 16 16 SwiGLU 32,000 θ = 10,000 512 256 200K 1% Llama2 32 4,096 11,008 32 32 SwiGLU 32,000 θ = 10,000 2048 192 20K/40K/60K 1% Llama3 32 4,096 14,336 32 8 SwiGLU 128,000 θ = 500,000 2048 192 20K 1% nanoGPT(GPT2) 48 1,536 6,144 24 24 GeLU 50,304 No 1024 512 100K 2000 20 SimpleGPT: Improving GPT via Simple Normalization Strategy H. More experiments on SimpleGPT 7B Furthermore, we evaluate the SimpleGPT 7B models using different learning rates or weight decay values. Results are shown in Figure 8 and Figure 9. Figure 8. The training loss curves of Llama2 7B, Llama2 7B with QKNorm and SimpleGPT 7B with learning rate 3e-4 and weight decay 0.1. Figure 9. The training loss curves of Llama2 7B, Llama2 7B with QKNorm and SimpleGPT 7B with learning rate 3e-3 and weight decay 0.03. 21 SimpleGPT: Improving GPT via Simple Normalization Strategy I. More experiments on SimpleGPT 8B Furthermore, we evaluate the SimpleGPT 8B models using different learning rates or weight decay values. Results are shown in Figure 10 and Figure 11. Figure 10. The training loss curves of Llama3 8B, Llama3 8B with QKNorm and SimpleGPT 8B with learning rate 3e-4 and weight decay 0.1. Figure 11. The training loss curves of Llama3 8B, Llama3 8B with QKNorm and SimpleGPT 8B with learning rate 3e-3 and weight decay 0.03. 22 SimpleGPT: Improving GPT via Simple Normalization Strategy J. More experiments on weight decays We conduct experiments on the SimpleGPT 8B model using two different weight decay values to evaluate robustness to regularization. Across all tested settings, SimpleGPT 8B consistently outperforms LLaMA2 8B with QKNorm. These results indicate that the benefits of SimpleNorm are not sensitive to the choice of weight decay, further demonstrating its robustness in large-scale training. Results are shown in Figure 12. (a) 8B with wd=0.1. (b) 8B with wd=0.05. Figure 12. Overall comparison across Llama3 8B with QKNorm and SimpleGPT 1B under two different weight decay values."
        }
    ],
    "affiliations": [
        "Intellifusion Inc.",
        "Tsinghua University"
    ]
}