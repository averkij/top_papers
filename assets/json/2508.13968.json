{
    "paper_title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation",
    "authors": [
        "Tianyi Niu",
        "Jaemin Cho",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg}, 180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\\deg}) images, while certain models are able to identify upside-down (180{\\deg}) images. None can reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite substantially improving the identification of 180{\\deg} images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation."
        },
        {
            "title": "Start",
            "content": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation"
        },
        {
            "title": "Tianyi Niu",
            "content": "Jaemin Cho Elias Stengel-Eskin Mohit Bansal"
        },
        {
            "title": "UNC Chapel Hill",
            "content": "5 2 0 2 9 1 ] . [ 1 8 6 9 3 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0, 90, 180, and 270. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce ROTBENCH 350image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information including captions, depth maps, and more or using chain-ofthought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0) images, while certain models are able to identify upside-down (180) images. None can reliably distinguish between 90 and 270. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models ability to distinguish 90 and 270 rotations, despite substantially improving the identification of 180 images. Together, these results reveal significant gap between MLLMs spatial reasoning capabilities and human perception in identifying rotation."
        },
        {
            "title": "Introduction",
            "content": "Advancements in Multimodal Large Language Models (MLLMs) have led to increased performance in complex visual tasks, such as image-text retrieval, image segmentation, and visual question 1Code and data: https://github.com/tianyiniu/RotBench 1 Figure 1: We present two ROTBENCH images: one (left) to Gemini-2.5-Pro, the other (right) to GPT-5. Humans can easily identify the correct rotation of the two images, but both models fail to do so. answering (Li et al., 2025; Chen et al., 2023a; Ravi et al., 2024; Fu et al., 2024a; Chen et al., 2023b; Bai et al., 2025; OpenAI, 2025c; Gemini Team, 2025; Liu et al., 2023a; Deitke et al., 2024). However, growing body of recent work suggests that MLLMs are sensitive to simple image transformations (Anis et al., 2025), such as rotations, flips, and blurs, and they fail on tasks that are intuitive to humans (Fu et al., 2024b; Pothiraj et al., 2025; Tong et al., 2024). Downstream tasks involving rotating camera such as robotic arm manipulation or first-person extreme sports analysis require MLLMs to demonstrate robust spatial reasoning, regardless of image orientation. Given these challenges, this work explores fundamental question: can MLLMs identify image orientation? Humans can quickly recognize whether an image has been rotated (Shepard and Metzler, 1971; Vandenberg and Kuse, 1978); for example, it is easy for us to recognize that the left image in Fig. 1 is not upright. The human viewer can use the orientation of the couch in the background to infer that Figure 2: ROTBENCH evaluation pipeline: for each image in ROTBENCH, we rotate the image 0, 90, 180, and 270 counter-clockwise. We represent the rotation estimation problem as multiple-choice question answering problem (Appendix L.5), and separately measure accuracy on each image orientation. We optionally provide different forms of auxiliary information to aid the model in identifying image rotation. We emphasize that all forms of auxiliary information are separately extracted for each rotation; the ground truth rotation is not marked. the father is actually lying on his back rather than standing up. The simple task of identifying image rotation requires reconciling the images subjects, background, and semantics. Here, we show that identifying rotation remains challenge, even in frontier MLLMs. We introduce ROTBENCH (Section 3), benchmark for evaluating MLLMs ability to recognize rotation in images. ROTBENCH consists of images sampled from Spatial-MM (Shiri et al., 2024), and is comprised of two subsets: the 300-image ROTBENCH-LARGE and the 50-image ROTBENCH-SMALL. ROTBENCH is carefully constructed to be challenging but fair, with two-stage filtering procedure to remove images that are indistinguishable under different degrees of rotation. Section 3 describes our dataset construction. Using ROTBENCH, we explore whether frontier MLLMs can identify rotation in input images rotated 0, 90, 180, and 270  (Fig. 2)  . We also evaluate whether providing various forms of auxiliary information or using chain-of-thought (Wei et al., 2022) prompting improves performance (Section 4.3). We find that models are able to consistently identify right-side-up (0) images. However, only stronger models are able to identify upsidedown (180) images. All models fail to accurately distinguish between 90 and 270 images (Section 6.1, Section 6.2). We find adding auxiliary information offers minimal and inconsistent improvement, often improving performance on 270 images at the expense of 90 images. Leveraging MLLMs tendency to accurately identify 0 images, we attempt to improve performance by further rotating an input image 0, 90, 180, 270 and presenting MLLMs with all rotations simultaneously. Using this approach, reasoning models (o3 and Gemini-2.5-Pro) show performance improvements, while weaker models see performance degradation. Further extending this idea, we utilize voting approach to algebraically obtain majority vote for the correct ground truth orientation. We obtain model prediction for each further rotation by subtracting the added angle. While this approach does show significant performance improvements on weaker models, it lacks scalability as it requires multiple model calls for each prediction, and assumes priori knowledge of all possible orientations (Appendix I). Finally, fine-tuning on out-of-domain data (Section 6.3) significantly improved performance on identifying 180 images but not 90 and 270. Interestingly, we find an oscillating pattern of performance changes between 90 and 270 as training progresses. Any performance improvement in 90 is matched with degradation of 270 and vice versa. These results suggest the presence of two local optima hindering across-the-board progress. Our findings demonstrate the tested MLLMs significantly underperform compared to humans 2 when it comes to spatial reasoning involving rotation detection, highlighting the need for integrating rotation-awareness into modern training pipelines."
        },
        {
            "title": "2 Related Work",
            "content": "Sensitivity to visual perturbations. Previous work has shown vision encoders and MLLMs are sensitive to simple image transformations. Anis et al. (2025) evaluate CLIP (Radford et al., 2021) and SigLIP (Zhai et al., 2023) on suite of common image transformations rotations, flips, noise, etc. revealing substantial gaps between human and model understanding. Usama et al. (2025) finds MLLMs exhibit distinct failure patterns in scene-text and object reasoning tasks when applying ImageNet-C corruptions (Hendrycks and Dietterich, 2019) to image inputs. These studies highlight that despite strong clean-image performance, vision encoders and MLLMs are highly sensitive to photometric and geometric distortions. Robustness to image transformations. Past work has also examined various methods to ensure that image transformations do not affect downstream task performance. Mikołajczyk and Grochowski (2018), Shorten and Khoshgoftaar (2019), and Perez and Wang (2017) use image transformations as data augmentation methods to improve downstream classifier robustness. Other works instead proposed alternative architectures and training schemes to improve robustness to rotation (Xu et al., 2023; Cohen and Welling, 2016; Lee et al., 2023; Feng et al., 2019). While this line of work focuses on training models to ignore certain transformations and learn invariant features for downstream tasks, we instead focus on identifying and reasoning about the transformation itself. Image orientation estimation. Fine-tuning models to identify image orientation has been the focus of prior work (Xu et al., 2024). For example, Fischer et al. (2015) and Joshi and Guerzhoy (2017) focused on fine-tuning convolutional neural networks (CNNs) to estimate and identify image rotation. While our work tackles similar task, we are instead interested in the problem as test of general-purpose MLLMs inherent reasoning abilities, i.e., whether they can estimate image rotation without extensive fine-tuning. Note that our work aligns more closely with the problem of image, not camera, orientation estimation. Camera orientation estimation Camera orientation estimation is also well-studied task in computer vision (Xian et al., 2019). Instead of predicting the rotation of the image, camera orientation estimation seeks to predict the spatial location of the camera when capturing an image. Contemporary approaches of this task use deep networks to directly predict orientation parameters from image features in an end-to-end manner (Xian et al., 2019; Lee et al., 2021, 2020). Spatial reasoning in MLLMs. Beyond robustness, spatial relation understanding is notable weakness of current MLLMs. Kamath et al. (2023) curate the Whats Up benchmark to isolate left- /right/above/below relations, showing significant gap in performance between humans and MLLMs. Shiri et al. (2024) further develop the Spatial-MM dataset and demonstrate that providing bounding boxes or scene graphs yields only modest gains. Both illustrate that MLLMs struggle with certain challenging cross-modal spatial reasoning tasks. Gap between human perception and MLLMs. growing body of work shows MLLMs exhibit fundamental gaps compared to human perceptual capabilities. Pothiraj et al. (2025) propose CAPTURe benchmark for occluded object counting and report sharp drops in model accuracy on both synthetic and real images. Zhou et al. (2025) proposes MMVM, benchmark for visual matching across images. Fu et al. (2024b) collect BLINK, dataset comprised of visual tasks humans can solve in blink, such as identifying visual similarity and relative depth. Both Zhou et al. (2025) and Fu et al. (2024b) report low zero-shot accuracy on their respective tasks, suggesting MLLMs lack many of the intuitive reasoning mechanisms that underpin human visual perception. In this vein, our work provides novel perspective to analyze and interpret the spatial reasoning capabilities of MLLMs, with results indicating that models struggle with this previously underexplored challenge."
        },
        {
            "title": "3 ROTBENCH",
            "content": "We introduce ROTBENCH, benchmark for evaluating models ability to identify rotation in input images. ROTBENCH is created using images from Spatial-MM (Shiri et al., 2024) and includes two subsets: the 300-image ROTBENCH-LARGE and the 50-image ROTBENCH-SMALL. While rotating an image is straightforward, not all images are meaningful under rotation. rotated portrait of human, such as the image shown in Fig. 2, will be easily noticed by human viewers. However, 3 top-down view of simple tabletop does not significantly differ when rotated  (Fig. 9)  . We use two-stage filtering process to ensure different rotations of each image are clearly distinguishable. This section provides an overview of our filtering procedure. Appendix describes our dataset procedure and statistics in further detail. Stage 1. We randomly sample 300 images from Spatial-MM (Shiri et al., 2024). Stage 1 involves single annotator. Depending on the amount of visual signals available, the Stage 1 annotator decides to either accept, discard, or flag each image. Flagged images then proceed to Stage 2. We provide further examples and details of accepted, flagged, and discarded images in Fig. 9. Stage 2. Stage 2 involves group of three human evaluators. Each flagged image is rotated 0, 90, 180, and 270 counter-clockwise then presented to the evaluators as multiple-choice questions (Appendix A.4). During Stage 2, an evaluator will see each image four times, once per orientation. Any image that elicits an incorrect answer from two or more evaluators across all four orientations is discarded. Otherwise, the image is accepted. Stage 1 and 2 are repeated until total of 300 images are accepted. ROTBENCH-LARGE and ROTBENCH-SMALL. All images that have been accepted in Stage 1 and 2 are organized into ROTBENCH-LARGE. As each image is rotated in four orientations, obtaining human performance on ROTBENCH-LARGE is costly. Luckily, Stage 2 provides human baseline for the subset of ROTBENCH-LARGE images that have been flagged (25 images). We expand this subset by further sampling images from Spatial-MM. From these additional images, we only select images that fit the criteria for flagging to proceed to another round of Stage 2 evaluation. This process repeats until we reach total of 50 images, organizing them into ROTBENCH-SMALL."
        },
        {
            "title": "4.1 Models",
            "content": "We evaluate various open-weight and proprietary MLLMs on ROTBENCH: Qwen-2.5-VL7B-Instruct (Bai et al., 2025); GPT-4o (OpenAI et al., 2024), GPT-4.1 (OpenAI, 2025a), o3 (OpenAI, 2025c), GPT-5 (OpenAI, 2025b), Gemini-2.02The human evaluators all exhibit high accuracy, averaging > 0.97 for all rotations. Flash (Hassabis and Kavukcuoglu, 2024), Gemini2.5-Flash (Gemini Team, 2025), and Gemini-2.5Pro (Gemini Team, 2025). Due to cost and resource limitations, we evaluate Gemini-2.5-Flash, Gemini2.5-Pro, GPT-4.1, GPT-5, and o3 on ROTBENCHSMALL. Responses are obtained through greedy decoding, while all chain-of-thought (Wei et al., 2022) responses are obtained with temperature of 0.3."
        },
        {
            "title": "4.2 Setup and Evaluation",
            "content": "We rotate each image in ROTBENCH-LARGE and ROTBENCH-SMALL by 0, 90, 180, and 270 counter-clockwise, resulting in total of 1000 and 200 images. Note that 90 counter-clockwise rotation is equivalent to 270 clockwise rotation. For each image and orientation, we provide the model with the image, brief description of the task, and various forms of auxiliary information (Section 4.3). We frame this task as four-way classification problem. To ensure robustness, the mapping between letter choice and degree of rotation is randomized for each prompt. We evaluate models on ROTBENCH-LARGE and report average accuracy and standard deviation across 3 runs in Table 1. We use the same procedure, albeit with only 2 runs, on ROTBENCH-SMALL and report results in Table 2. All prompts used are available in Appendix L."
        },
        {
            "title": "4.3 Auxiliary Information",
            "content": "Figure 8 illustrates all forms of auxiliary information provided to the model. Note that all auxiliary information is separately extracted for each rotation, ensuring our approach does not depend on prior knowledge of the images orientation. Captions. For each image and rotation, we instruct GPT-4o to provide detailed caption (Appendix L.2). We emphasize that each image is captioned four times, once per rotation. Bounding Boxes. For each image and rotation, we first use GPT-4o to extract the primary subjects within the image (Appendix L.1). Next, along with the image, the list of subjects is given to GroundingDINO (Liu et al., 2023b) to extract set of normalized coordinates for each subject,4 which is 3We perform an ablation study where we vary the sampling temperature  (Table 8)  . We use the default sampling temperature for proprietary models that do not expose temperature parameter in their API interface. 4Each set of coordinates is four-element tuple, composed of [x_min, y_min, x_max, y_max]. 4 directly injected into the prompt. Scene Graphs. scene graph (Zhu et al., 2022) codifies relationships between objects in an image as three-element tuple [object 1, predicate, object 2]. Using the extracted subjects from the previous section, we prompt GPT-4o to generate scene graph for the image. Depth Maps. We obtain depth maps for each image using ZoeDepth (Bhat et al., 2023). Rather than rotating the depth map obtained from 0, we separately obtain depth maps for all four rotations. Segmentation Maps. Using the previously extracted bounding boxes, we obtain segmentation map of each image and orientation using SAM 2 (Ravi et al., 2024). Chain-of-Thought. To evaluate whether our multiple-choice setup is hampering performance on this task, we modify the prompt to encourage the model to produce reasoning chains instead of single letter choice. Rotation Grid. We test if explicitly allowing models to visualize rotations aid performance by providing the input image along with three copies of the image further rotated 90, 180, 270. We compose these four images into single rotation grid. Each image is captioned with the degree of further rotation, independent of the ground truth rotation. We provide further experiment (rotation grid guided) where we explicitly prompt the model to identify an \"anchor\" image and algebraically calculate the original images ground truth rotation. All rotation grid experiments use CoT prompting."
        },
        {
            "title": "5 Main Results",
            "content": "Table 1 displays the results of evaluating Qwen2.5-VL-7B-Instruct, Gemini-2.0-Flash, and GPT4o along with various auxiliary information on ROTBENCH-LARGE. Table 2 displays results of evaluating Qwen-2.5-VL-7B-Instruct, GPT-4o, GPT-4.1, GPT-5, o3, Gemini-2.0-Flash, Gemini2.5-Flash, Gemini-2.5-Pro on ROTBENCH-SMALL. Due to the high token cost of proprietary reasoning models, we only evaluate zero-shot and CoT prompts. However, we evaluate providing rotation grids to o3 and Gemini-2.5-Pro.5 MLLMs accurately identify right-side-up (0) images. All evaluated models effectively recog5Table 5 further shows results obtained from evaluating these two models on ROTBENCH-SMALL with all available auxiliary information. nize right-side-up (0) images. Qwen-2.5-VL-7BInstruct (Qwen) achieves an accuracy of 0.99 without supplemental data. Proprietary models (GPT4o, GPT-4.1, o3, Gemini-2.5-Flash, Gemini-2.5Pro, and Gemini-2.0-Flash) consistently exhibit near-perfect accuracy on identifying unrotated images. This outcome aligns with expectations, given these models likely encountered predominantly upright images during training, and thus 0 can be assumed to be the default option. Proprietary models perform well on upsidedown (180) images. All models except Qwen demonstrate robust performance on images rotated 180, with GPT-4o, GPT-4.1, o3, and Gemini-2.5Pro all achieving accuracies notably above chance (> 0.7). However, Gemini-2.0-Flash and Gemini2.5-Flash display relatively lower zero-shot performance, with accuracies around 0.5. This indicates that state-of-the-art proprietary models generally possess reliable capability to recognize upside-down images, though there remains variability within the different model families. Identifying 90 and 270 is challenging for all evaluated models. All models exhibit substantial difficulties when distinguishing between 90 and 270, while the poorest performance consistently emerged with 270 images. Confusion matrix analysis (Section 6.1) reveals frequent misclassifications between these two orientations, indicating distinct challenge when identifying 0 and 180. Providing auxiliary inputs does not reliably improve performance. None of the auxiliary information results in meaningful, consistent performance gains across all tested models. Paradoxically, the introduction of additional information sometimes leads to marginal performance degradation. When including all forms of auxiliary information, Qwens accuracy on 90 decreased from 0.51 to 0.26. Similarly, Gemini-2.0-Flashs accuracy on 270 decreased from 0.44 to 0.17. Rotation grid improves performance only for reasoning models. Providing the rotation grid degraded performance on most models. Gemini2.0-Flash accuracy on 90 decreased by nearly 0.5 compared to CoT prompting. Rotation grid guided, however, improved GPT-4o accuracy on 270 by around 0.1. At the same time, both o3 and Gemini2.5-Pro saw performance improvements. Notably, Gemini-2.5-Pros accuracy on 90 and 270 improved by 0.15. These results suggest the two reasoning models are much more effective at utilizing 5 Model 0 90 180 270 Accuracy on Different Degrees of Rotation Qwen-2.5-VL-7B-Instruct Zero-shot + Caption + Bounding Box + Scene Graph + Depth Map + Segmentation Map + Chain-of-thought + Rotation Grid + Rotation Grid Guided + all above Gemini-2.0-Flash Zero-shot + Caption + Bounding Box + Scene Graph + Depth Map + Segmentation Map + Chain-of-thought + Rotation Grid + Rotation Grid Guided + all above GPT-4o Zero-shot + Caption + Bounding Box + Scene Graph + Depth Map + Segmentation Map + Chain-of-thought + Rotation Grid + Rotation Grid Guided + all above 0.990.00 1.000.00 0.900.00 0.970.01 0.930.01 0.810.01 0.880.01 0.570.04 0.590.01 0.470.03 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 0.990.00 0.980.00 0.980.01 0.980.00 1.000.00 0.970.00 0.970.01 0.980.00 0.980.00 1.000.00 0.510.01 0.510.01 0.480.01 0.510.01 0.550.02 0.630.02 0.260.02 0.150.02 0.120.01 0.260. 0.300.00 0.380.00 0.430.03 0.410.00 0.300.01 0.280.01 0.630.00 0.070.01 0.100.00 0.10.01 0.690.02 0.650.00 0.590.01 0.550.00 0.550.03 0.670.01 0.570.03 0.710.02 0.460.03 0.460.03 0.050.01 0.230.01 0.010.00 0.010.01 0.040.01 0.030.02 0.340.01 0.130.01 0.130.00 0.170.01 0.720.00 0.760.00 0.710.00 0.710.00 0.690.01 0.730.02 0.760.01 0.570.01 0.610.00 0.670.01 0.930.00 0.930.01 0.910.00 0.930.00 0.930.00 0.950.00 0.930.00 0.930.00 0.930.00 0.910.00 0.090.01 0.070.00 0.110.00 0.110.02 0.130.02 0.160.01 0.230.02 0.280.00 0.300.02 0.330. 0.440.01 0.440.01 0.340.02 0.330.02 0.460.01 0.450.00 0.190.1 0.070.01 0.250.01 0.170.01 0.190.01 0.230.02 0.310.04 0.330.02 0.260.01 0.210.00 0.320.00 0.190.03 0.410.00 0.360.02 Table 1: Classification accuracy using various forms of auxiliary information and prompting for all rotations. All results are obtained from three runs on ROTBENCH-LARGE. Accuracy is scored on four-way classification task. visual context. We further leverage the robust identification of 0 images and modify the rotation grid into majority voting approach Appendix I. This setup results in gains even in weaker models, indicating MLLMs can achieve moderate performance with sufficient visual scaffolding. Chain-of-thought improves 180 performance, but results are mixed on 90 and 270. Employing chain-of-thought (CoT) prompting yields mixed results, differing notably across models and orientations. Gemini models show improved accuracy on 90 rotations but decreased accuracy for 270. Conversely, GPT models exhibit improved performance on 270 rotations, coupled with degraded accuracy for 90 rotations. However, CoT consistently enhanced accuracy for 180 rotations across all models tested. These findings suggest CoT prompting can help models better reason about rotations to some extent, but does not universally resolve inherent challenges in distinguishing portrait orientations. Appendix examines common failure of CoT in detail."
        },
        {
            "title": "6.1 Model Bias Towards 0° and 90°",
            "content": "To further elucidate the specific types of rotational errors made by models, we analyze the confusion matrix for GPT-4o. Figure 3 shows the confusion matrix obtained from summing predictions across three runs. We see GPT-4o predominantly struggles with differentiating between 90 and 270 rotations. Specifically, the model misclassifies 459 instances of 90 images as 270 and 424 instances of 270 images as 90. Yet, 0 and 180 show significantly fewer misclassifications. This analysis underscores critical shortcoming in model performance, suggesting either that the vision encoder is not providing sufficient signals to distinguish be6 Model Human Zero-shot Qwen-2.5-VL-7B-Instruct Zero-shot + Chain-of-thought GPT-4o Zero-shot + Chain-of-thought GPT-4.1 Zero-shot + Chain-of-thought GPT-5 Zero-shot + Chain-of-thought Gemini-2.0-Flash Zero-shot + Chain-of-thought Gemini-2.5-Flash Zero-shot + Chain-of-thought o3 Zero-shot + Chain-of-thought + Rotation Grid + Rotation Grid Guided Gemini-2.5-Pro Zero-shot + Chain-of-thought + Rotation Grid + Rotation Grid Guided Accuracy on Different Degrees of Rotation 0 0.99 90 0.99 180 0.99 270 0.97 0.950.01 0.860. 0.570.04 0.200.04 0.030.02 0.130.03 0.150.05 0.300.04 0.870.02 0.910.01 0.650.04 0.590.01 0.850.01 0.860. 0.210.03 0.210.05 0.950.01 0.980.00 0.630.07 0.880.00 0.410.03 0.530.05 0.850.03 0.850.01 0.810.01 0.810. 0.190.09 0.030.03 0.590.04 0.570.07 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 1.000.00 0.990.01 0.990. 1.000.00 1.000.00 0.990.01 0.950.01 0.250.04 0.610.01 0.480.01 0.590.01 0.430.07 0.250.01 0.230.06 0.230.01 0.500.03 0.440. 0.470.02 0.400.02 0.450.04 0.360.00 0.230.05 0.310.01 0.500.04 0.460.02 0.580.00 0.710.01 0.700.03 0.740.04 0.830.05 0.820.02 0.720.00 0.710.01 0.670.03 0.730.03 0.480.03 0.570.01 0.810.01 0.750. 0.400.06 0.490.01 0.590.03 0.740.04 Table 2: Classification accuracy of different image rotations for various models across two runs on ROTBENCHSMALL using zero-shot or chain-of-thought prompting. We also show results from o3 and Gemini-2.5-Pro using rotation grids. Accuracy is scored on four-way classification task. tween clockwise and counter-clockwise rotations, or that the MLLM is not adequately incorporating the visual information into its reasoning."
        },
        {
            "title": "6.2 Distinguishing Clockwise from",
            "content": "Counter-clockwise Rotations Setup. Following the analysis in Section 6.1, we specifically examine whether MLLMs can distinguish clockwise (CW) and counter-clockwise rotations (CCW). For this, we simplified the original four-way classification task into binary classification task. Using the 90 and 270 images from ROTBENCH-LARGE, we ask MLLMs to determine whether an image has been rotated 90 CCW or CW.6 By asking models to explicitly differentiate the two directions of rotation, we hope to provide clearer signal for directional understanding. Fig6Note that 90 CW is equivalent to 270 CCW. ure 4 shows an example question where the GPT-4o provides an incorrect answer. Results. Table 3 tests CW vs. CCW classification on GPT-4o and Qwen-2.5-VL-7B-Instruct (we additionally provide similar smaller-scale tests on ROTBENCH-SMALL for GPT-5 in Appendix E). Table 3 indicates that GPT-4o has significant bias toward identifying rotations as 90 counterclockwise. GPT-4o correctly identified only 52 out of 300 clockwise rotations, whereas Qwen-2.5-VL7B-Instruct correctly identified only 23 out of 300 clockwise rotations. The models consistently default to labeling ambiguous or uncertain rotations as counter-clockwise, hinting towards potential underlying perceptual bias (Appendix F). While we find that the most recent model tested, GPT-5, has less bias and is better able to identify clockwise rotations, this improvement is not reflected in TaFigure 3: Confusion matrix of true vs. predicted rotations for GPT-4o using CoT prompting, summed across three runs on ROTBENCH-LARGE. Rows represent ground-truth labels, columns represent predicted labels. GT Predicted Counter-clockwise Clockwise GPT-4o Counter-clockwise Clockwise Qwen-2.5-VL-7B-Instruct Counter-clockwise Clockwise 248 259 270 277 52 41 30 23 Table 3: Accuracy of different models in identifying 90 clockwise (CW) versus counter-clockwise rotation (CCW). Column indicates ground truth rotation (GT). small number of responses are incorrectly identified as being right-side up. These responses are excluded in the tables statistics. ble 2, where the model performs on par with others; in other words, poor performance on ROTBENCH cannot be solely explained by models ability to distinguish clockwise from counterclockwise rotation. See Appendix for further discussion. These findings strongly indicate that the MLLMs tested have limitations in reliably distinguishing between CW and CCW rotational directions, offering potential explanation for why distinguishing 90 and 270 CCW rotations presents such challenging task. Further corroborating this idea, we show that evaluating Qwen-2.5-VL-7B-Instruct, GPT-4o, o3 on ROTBENCH-SMALL using clockwise angles does not result in significant performance differences (Appendix D). Furthermore, in Appendix we find that even with in-context examples, models struggle to correctly discriminate between clockwise and counterclockwise-rotated images."
        },
        {
            "title": "6.3 Can Fine-tuning Solve ROTBENCH?",
            "content": "Setup. To assess whether specialized training can mitigate these performance issues, we conduct Figure 4: GPT-4o answers incorrectly when asked to identify whether the image has been rotated 90 clockwise or counter-clockwise. Figure 5: Qwen-2.5-VL-7B-Instruct accuracy on different degrees of rotation as training progresses. fine-tuning experiments using Qwen-2.5-VL-7BInstruct. During data filtering, we find some images in Spatial-MM closely resemble each other  (Fig. 11)  . If we fine-tune on images in SpatialMM not selected for ROTBENCH, the existence of similar images in the training and testing sets may lead to inflated performance. To prevent such instances of overfitting, we train on 1000 images from MS COCO (Lin et al., 2015) and evaluate performance on ROTBENCH-LARGE. Appendix provides further training details. Results. Fig. 5 reveals high and consistent accuracy for 0 throughout training, indicating robust recognition of upright images. Performance on 180 gradually improves, stabilizing around 0.8 after approximately 7000 images. However, accuracies for 90 and 270 exhibit substantial oscillations, suggesting that the model did not achieve stable improvement. The model appears to be caught in cycle, alternating between accuracy gains and losses for these two rotations. This phenomenon is also reflected in our main results (Section 5), where using CoT prompting improved accuracy on 270 images at the expense of 90. The unstable performance may indicate potential representational constraints in current visual encoders that limit visual understanding capabilities, particularly regarding subtle rotational distinctions."
        },
        {
            "title": "7 Conclusion",
            "content": "We evaluate whether MLLMs are able to identify input images rotated 0, 90, 180, and 270 using ROTBENCH, 350-image manually-filtered benchmark. Our results reveal that state-of-theart MLLMs reliably identify images that are unrotated (0) or upside-down (180), but struggle with 90 and 270 rotations. Auxiliary information and chain-of-thought prompting provide limited improvements. Simultaneously providing all possible rotations improves performance only for frontier reasoning models, but modified setup using majority voting improves performance for weaker models as well. Fine-tuning Qwen-2.5VL-7B-Instruct shows that performance oscillates between 90 and 270, suggesting the presence of two local optima. These results indicate potential blind spot in MLLMs spatial reasoning capabilities, motivating future directions in improving orientational understanding."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-CAREER Award 1846185, NSF-AI Engage Institute DRL2112635, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, and Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "Ahmad Mustafa Anis, Hasnain Ali, and Saquib Sarfraz. 2025. On the limitations of vision-language models in understanding image transforms. Preprint, arXiv:2503.09837. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923. Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. 2023. Zoedepth: Zeroshot transfer by combining relative and metric depth. arXiv preprint. Weijing Chen, Linli Yao, and Qin Jin. 2023a. Rethinking benchmarks for cross-modal image-text retrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 23, page 12411251. ACM. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, and 10 others. 2023b. Pali: jointly-scaled multilingual language-image model. Preprint, arXiv:2209.06794. Taco S. Cohen and Max Welling. 2016. Group Preprint, equivariant convolutional networks. arXiv:1602.07576. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, and 31 others. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. Preprint, arXiv:2409.17146. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: efficient finetuning of quantized llms. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Zeyu Feng, Chang Xu, and Dacheng Tao. 2019. Selfsupervised representation learning by rotation feature decoupling. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1035610366. Philipp Fischer, Alexey Dosovitskiy, and Thomas Brox. 2015. Image orientation estimation with convolutional networks. In German Conference on Pattern Recognition. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2024a. Mme: comprehensive evaluation benchmark for multimodal large language models. Preprint, arXiv:2306.13394. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, WeiChiu Ma, and Ranjay Krishna. 2024b. Blink: Multimodal large language models can see but not perceive. Preprint, arXiv:2404.12390. Google Gemini Team. 2025. frontier with long Pushing the soning, multimodality, next generation agentic capabilities. //storage.googleapis.com/deepmind-media/ gemini/gemini_v2_5_report.pdf. Gemini 2.5: reaand https: advanced context,"
        },
        {
            "title": "Hassabis",
            "content": "and"
        },
        {
            "title": "Koray",
            "content": "Introducing gemini 2.0: Kavukcuoglu. our new 2024. 9 for ai model //blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024/. agentic era. the https: Dan Hendrycks and Thomas Dietterich. 2019. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, arXiv:2106.09685. Ujash Joshi and Michael Guerzhoy. 2017. Automatic photo orientation detection with convolutional neural networks. In 2017 14th Conference on Computer and Robot Vision (CRV), page 103108. IEEE. Amita Kamath, Jack Hessel, and Kai-Wei Chang. 2023. Whats up with vision-language models? investigating their struggle with spatial reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9161 9175, Singapore. Association for Computational Linguistics. Jinwoo Lee, Hyunsung Go, Hyunjoon Lee, Sunghyun Cho, Minhyuk Sung, and Junho Kim. 2021. Ctrl-c: Camera calibration transformer with lineclassification. Preprint, arXiv:2109.02259. Jinwoo Lee, Minhyuk Sung, Hyunjoon Lee, and Junho Kim. 2020. Neural geometric parser for single image camera calibration. Preprint, arXiv:2007.11855. Jongmin Lee, Byungjin Kim, Seungwook Kim, and Minsu Cho. 2023. Learning rotation-equivariant Preprint, features for visual correspondence. arXiv:2303.15472. Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. 2025. survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges. Preprint, arXiv:2501.02189. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. 2015. Microsoft coco: Common objects in context. Preprint, arXiv:1405.0312. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. Preprint, arXiv:2304.08485. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and 1 others. 2023b. Grounding dino: Marrying dino with grounded pretraining for open-set object detection. arXiv preprint arXiv:2303.05499. Agnieszka Mikołajczyk and Michał Grochowski. 2018. Data augmentation for improving deep learning in image classification problem. In 2018 International Interdisciplinary PhD Workshop (IIPhDW), pages 117122. OpenAI. 2025a. Introducing GPT-4.1 in the api. https: //openai.com/index/gpt-4-1/. OpenAI. 2025b. Introducing gpt-5. https://openai. com/index/introducing-gpt-5/. OpenAI. 2025c. o4-mini. introducing-o3-and-o4-mini/. Introducing OpenAI o3 and https://openai.com/index/ OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, and 400 othPreprint, ers. 2024. Gpt-4o system card. arXiv:2410.21276. Luis Perez and Jason Wang. 2017. The effectiveness of data augmentation in image classification using deep learning. Preprint, arXiv:1712.04621. Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal. 2025. Capture: Evaluating spatial reasoning in vision language models via occluded object counting. Preprint, arXiv:2504.15485. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. Preprint, arXiv:2103.00020. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. 2024. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714. Shepard and Metzler. 1971. Mental rotation of three-dimensional objects. Science, 171(3972):701 703. Fatemeh Shiri, Xiao-Yu Guo, Mona Golestan Far, Xin Yu, Reza Haf, and Yuan-Fang Li. 2024. An empirical analysis on spatial reasoning capabilities of large multimodal models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2144021455, Miami, Florida, USA. Association for Computational Linguistics. Connor Shorten and Taghi M. Khoshgoftaar. 2019. survey on image data augmentation for deep learning. Journal of Big Data, 6(1). 10 Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. Preprint, arXiv:2401.06209. Muhammad Usama, Syeda Aishah Asim, Syed Bilal Ali, Syed Talal Wasim, and Umair Bin Mansoor. 2025. Analysing the robustness of vision-language-models to common corruptions. Preprint, arXiv:2504.13690. Vandenberg and Kuse. 1978. Mental rotations, group test of three-dimensional spatial visualization. Percept. Mot. Skills, 47(2):599604. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc. Wenqi Xian, Zhengqi Li, Matthew Fisher, Jonathan Eisenmann, Eli Shechtman, and Noah Snavely. 2019. Uprightnet: Geometry-aware camera orientation estimation from single images. Preprint, arXiv:1908.07070. Renjun Xu, Kaifan Yang, Ke Liu, and Fengxiang He. 2023. e(2)-equivariant vision transformer. Preprint, arXiv:2306.06722. Ruijie Xu, Yong Shi, and Zhiquan Qi. 2024. Image orientation estimation based on deep learning - survey. Procedia Computer Science, 242:11931197. 11th International Conference on Information Technology and Quantitative Management (ITQM 2024). Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. Preprint, arXiv:2303.15343. Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Lu Qi, and Xiangtai Li. 2025. Are they the same? exploring visual correspondence shortcomings of multimodal llms. Preprint, arXiv:2501.04670. Guangming Zhu, Liang Zhang, Youliang Jiang, Yixuan Dang, Haoran Hou, Peiyi Shen, Mingtao Feng, Xia Zhao, Qiguang Miao, Syed Afaq Ali Shah, and Mohammed Bennamoun. 2022. Scene graph generation: comprehensive survey. Preprint, arXiv:2201.00443."
        },
        {
            "title": "A Dataset Details",
            "content": "A.1 Further details on Stage 1 and 2 Filtering Stage 1. We randomly sample 300 images from Spatial-MM (Shiri et al., 2024). Spatial-MM is divided into two splits, one-subject and two-subject, respectively comprised of images that contain one and two primary subjects. As models may use image dimensions to infer orientation, we crop each image into square before further processing. We sample 150 images from each split and show the resulting 300-image dataset to the Stage 1 annotator. For each image, the annotator can decide to either accept, discard, or flag. The annotator accepts an image if it (1) contains easily identifiable rotational visual cues (e.g., person standing), and (2) the image has meaningful differences when rotated. Images that do not satisfy these criteria are discarded. Occasionally, some images have subtle visual cues, or require more detailed semantic understanding to correctly perceive orientation. These images tend to have primary subjects that do not vary significantly with rotation, or require integrating background signals to identify rotation. The sample image shown in Fig. 6 is an example of flagged image. Such images are flagged by the Stage 1 annotator to proceed to Stage 2. We provide further examples and details of accepted, flagged, and discarded images in Fig. 9. Note that the Stage 1 annotator is also tasked with ensuring no leaked personal information or offensive content is incorporated into ROTBENCH. Stage 2. Stage 2 involves group of three human evaluators. Each flagged image is rotated 0, 90, 180, and 270, producing four images. We shuffle all images which now include rotated images and present them to the evaluators as multiplechoice questions (Appendix A.4). Any image that elicits an incorrect answer from two or more evaluators across all four orientations is discarded. Otherwise, the image is accepted. Of the first 300 sampled images, only 27 were flagged. Among the flagged images, only two are eventually rejected. We sample two additional images to replace the discarded images. These images form the 300-image ROTBENCH-LARGE benchmark. A.2 Creating ROTBENCH-SMALL As each image is rotated in four orientations (0, 90, 180, and 270), obtaining human performance on ROTBENCH-LARGE is costly. We therefore propose ROTBENCH-SMALL, humanevaluated 50-image subset where we can establish human baseline. Starting from the 25 nondiscarded flagged images in Stage 2, we obtain ROTBENCH-SMALL by further sampling 25 more 11 Figure 6: Figure describes our two-stage data filtering procedure. The example image is flagged during Stage 1, but subsequently accepted during Stage 2 as only one evaluator provided an incorrect response. images from Spatial-MM that fit the criteria for flagging. We ensure an equal number of one and two primary subject images in the final 50-image dataset. We then repeat the Stage 2 procedure on this new set, obtaining human baseline. Notably, none of the 25 new images were discarded from human evaluation. A.3 Annotator Information All three evaluators of Stage 2 data filtering are university students majoring in Computer Science, and have prior experience in Artificial Intelligence research. Two evaluators are undergraduate students who have volunteered to participate in the project, the third is graduate student who also provided Stage 1 annotations and is an author of this work. All evaluators consented to have their data incorporated. A.4 Annotator Interface and rotation degree is shuffled from question to question. A.5 Sample Images Figure 8 displays the various forms of additional information provided to the model. Figure 9 provides examples of an image that is accepted, flagged, and discarded during Stage 1 of data filtering. The accepted image has clear subject and is easily distinguishable when rotated 0, 90, 180, and 270. The flagged image is more difficult to identify, however, the slight tilt as opposed to directly topdown perspective still enables accurate judgment of rotation. However, the discarded image has no meaningful signals to distinguish between the four orientations. A.6 Further details on flagged images Flagged images typically involve cases where there is lack of primary subjects or the primary subject does not offer any distinguishable signals under rotation. These images tend to require deeper comprehension in order to make an accurate classification. For instance, Fig. 6 displays blank projector canvas above some colorful couches on patio. The orientation of projector screen cannot be reliably identified by where its supporting structures are located the screen can be hung (cable hangs down), pitched up (supporting stand on the ground), or attached to wall (support frame located sideways of the screen). Therefore, correctly identifying the orientation of the image requires understanding the correct orientation of the couches. Figure 7: screenshot of the custom interface shown to Stage 2 annotators."
        },
        {
            "title": "B Model Inference and Training Details",
            "content": "Figure 7 shows the interface provided to Stage 2 annotators. The mapping between letter choice All proprietary models are accessed through firstparty APIs provided in their respective official Python packages; the open-weight models are run 12 Figure 8: Examples of the different types of auxiliary information provided to the models. Figure 9: Samples of accepted, flagged, and discarded images during Stage 1 of data filtering. on an Nvidia RTX A6000 Ada GPU (48 GB). When fine-tuning Qwen-2.5-VL-7B-Instruct, we employ 4-bit quantization with the Bits-And-Bytes (bnb) NF4 format (Dettmers et al., 2023) and apply Low-Rank Adaptation (LoRA) (Hu et al., 2021) with rank 8 and an alpha parameter of 32. The fine-tuning procedure ran for 2 epochs, with batch size of 32 and learning rate of 2e-5."
        },
        {
            "title": "C Additional Results",
            "content": "We provide experimental results from evaluating Llama-3.2-11B-Instruct on ROTBENCH-LARGE  (Table 4)  , as well as o3 and Gemini-2.5-Pro on ROTBENCH-SMALL  (Table 5)  . Llama-3.2-11BInstruct seems to be the weakest model tested, showing inferior performance across all rotations. Without CoT, the model only achieves roughly 0.3 accuracy. The two frontier reasoning models (o3 and Gemini-2.5-Pro) show no significant performance improvement when prompted with auxiliary information. However, they respond positively to rotation grids. Gemini-2.5-Pro achieves the highest accuracy on 90 and 270 (both 0.7) using guided rotation grids, while o3 achieves its highest accuracy on 180 (0.83) using the rotation grid. These results indicate that strong reasoning models are able to utilize the grid to aid them in making prediction, while weaker models struggle with it."
        },
        {
            "title": "D Clockwise Prompting",
            "content": "To verify whether MLLMs show directional preference towards either clockwise or counterclockwise, we evaluate Qwen-2.5-VL-7B-Instruct, GPT-4o, and o3 on ROTBENCH-SMALL using clockwise angles. Results from all three models closely align with the results from using counterclockwise angles (Section 5), indicating the choice of either direction is fully arbitrary."
        },
        {
            "title": "E Differentiating Clockwise and",
            "content": "Counter-clockwise Rotations using GPT-5 In Table 7, we repeat the binary clockwise versus counter-clockwise classification experiment (Section 6.2) using GPT-5. GPT-5 achieves considerably higher accuracy on this binary classification task compared to GPT-4o and Qwen-2.5-VL-7BInstruct. Notably, performance on 90 and 270 images are significantly higher in the binary task than in the four-way classification task. Figure 10: Example image where GPT-4os reasoning falsely distinguishes between two identical forms of rotation. This performance difference reveals an important limitation: GPT-5 frequently misclassifies between portrait rotations (90 and 270) and landscape rotations (0 and 180). Despite strong performance on 0 and 180 rotations, this confusion demonstrates that the four-way classification task cannot be effectively reduced to two separate binary classification tasks (one distinguishing between 0 and 180, and another between 90 and 270). Chain-of-Thought Example To further understand how GPT-4o confuses clockwise and counter-clockwise rotations, we examine the generated reasoning trace of an image in detail. Figure 10 has been rotated 270 counter-clockwise (or 90 clockwise). However, GPT-4o generates the following reasoning trace: To determine the rotation of the image, lets analyze the orientation of the buildings and other elements: [...] 3. **Rotation Analysis**: - 90-degree rotation counter-clockwise would place the sky on the right and the ground on the left, which matches the current orientation. - 270-degree rotation clockwise would place the sky on the left and the ground on the right, which does not match. Therefore, the image has been rotated 90 degrees counter-clockwise. As 90 counter-clockwise rotation is identical to 270 clockwise rotation, the model falsely distinguishes between the two identical forms of rotation. 14 Model 0 90 180 270 Accuracy on Different Degrees of Rotation Llama-3.2-11B-Instruct Zero-shot + Caption + Bounding Box + Scene Graph + Chain-of-thought + all above 0.280.01 0.360.03 0.220.01 0.270.01 0.450.01 0.470. 0.140.02 0.110.02 0.200.02 0.170.02 0.280.02 0.160.01 0.530.01 0.640.02 0.430.01 0.430.02 0.310.01 0.410.02 0.300.02 0.250.03 0.260.02 0.260.01 0.340.00 0.400.00 Table 4: Average classification accuracy under different image rotation angles (0, 90, 180, 270) for Llama-3.211B-Instruct and auxiliary information across three runs on ROTBENCH-LARGE. Accuracy is scored on four-way classification task. We did not evaluate providing depth maps or segmentation maps for Llama-3.2-11B-Instruct (Llama) as it only supports single image input. Model 0 90 180 270 Accuracy () Gemini-2.5-Pro Zero-shot + Caption + Bounding Box + Scene Graph + Depth Map + Segmentation Map + Chain-of-thought + Rotation Grid + Rotation Grid Guided + all above o3 Zero-shot + Caption + Bounding Box + Scene Graph + Depth Map + Segmentation Map + Chain-of-thought + Rotation Grid + Rotation Grid Guided + all above 1.00 1.00 0.98 1.00 1.00 0.98 1.00 0.99 0.95 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 1.00 0.50 0.48 0.60 0.54 0.44 0.54 0.23 0.58 0.71 0.60 0.45 0.32 0.38 0.40 0.34 0.44 0.36 0.23 0.31 0. 0.72 0.76 0.74 0.78 0.66 0.70 0.44 0.67 0.73 0.76 0.70 0.70 0.64 0.60 0.58 0.68 0.74 0.83 0.82 0.78 0.40 0.44 0.34 0.42 0.52 0.42 0.40 0.59 0.74 0.68 0.48 0.44 0.40 0.48 0.40 0.40 0.57 0.81 0.75 0.78 Table 5: Average classification accuracy under different image rotation angles (0, 90, 180, 270) for o3 and Gemini-2.5-Pro and various forms of auxiliary information across single run on ROTBENCH-SMALL. Accuracy is scored on four-way classification task."
        },
        {
            "title": "G Modifying Temperature",
            "content": "clusions, suggesting that the model distinguishes between landscape rotations (0 and 180) and portrait rotations (90 and 270). The model defaults to 0 for landscape rotations, and 90 for portrait rotations. With temperature increases, the models responses begin to resemble random guessing. In-Context Learning Setup. To further investigate whether the solution to identifying clockwise and counter-clockwise rotations is simply one of clarifying nomenclature, we implement an in-context learning (ICL) experiment using Qwen-2.5-VL-7B-Instruct. Recall ROTBENCH-LARGE and ROTBENCH-SMALL have an overlap of 25 images. We incrementally sample 2, 6, and 10 distinct in-context examples from the remaining 25 images in ROTBENCH-SMALL and evaluate on ROTBENCH-LARGE. As each image is rotated in four orientations, the model is shown total of 8, 24, 40 images. We randomly shuffle the order of in-context images to ensure robustness. Moreover, we provide the ground truth rotation of each in-context example to the model, aiming to guide it towards improved performance. To ensure model performance does not significantly vary with sampling temperature, we examine zeroshot performance of GPT-4o and Qwen-2.5-VL-7BInstruct (Qwen) on varying temperature settings  (Table 8)  . We see that performance on all four rotations does not vary significantly for GPT-4o. Qwen does show slight sensitivity to temperature. As temperature increases, performance on 0 and 90 decreases, while performance on 180 and 270 increases. These results support our primary conResults. Table 9 shows the accuracy on each image orientation after injecting in-context examples. We do not see consistent performance improvement, regardless of the number of in-context examples provided. Together with results in Table 1, these experiments suggest that various forms of prompt modification are insufficient for identifying rotation. Rather, robust orientation identification demands explicit parameter optimization through fine-tuning. Model 0 90 180 270 Accuracy on Different Degrees of Rotation Qwen-2.5-VL-7B-Instruct Zero-shot + Chain-of-thought GPT-4o Zero-shot + Chain-of-thought o3 Zero-shot + Chain-of-thought 0.910.02 0.820.00 0.580.03 0.510.04 0.020.02 0.130.01 0.170.01 0.050.05 0.870.02 0.920.02 0.660.02 0.630. 0.830.02 0.880.04 0.200.00 0.210.05 1.000.00 1.000.00 0.380.06 0.270.02 0.710.03 0.720.02 0.500.03 0.490. Table 6: Average classification accuracy under different clockwise rotation angles (0, 90, 180, 270) for Qwen-2.5VL-7B-Instruct, GPT-4o, o3 across two runs on ROTBENCH-SMALL using zero-shot or chain-of-thought prompting. GT Predicted Counter-clockwise Clockwise GPT-5 Counter-clockwise Clockwise 41 9 37 Table 7: Accuracy of GPT-5 in identifying 90 clockwise (CW) versus counter-clockwise rotation (CCW) on ROTBENCH-SMALL. Column indicates ground truth rotation (GT). Results are obtained using temperature of 0.3. Algorithm 1 Normalized Rotation Voting Require: images = [I0, I90, I180, I270] 1: rotations [] 2: for 0 to images 1 do 3: 4: rot call_model(images[i]) rot_norm (rot 90) mod 360 append rot_norm to rotations 5: 6: end for 7: inal_rot majority_vote(rotations)"
        },
        {
            "title": "I Normalized Rotation Voting",
            "content": "Intuition and Setup. Our results show frontier MLLMs are able to reliably identify 0 and 180 images. Leveraging this pattern, we propose voting approach to identify image rotation. Given an input image of unknown orientation, we can further rotate this image 0, 90, 180, and 270. We separately prompt the model to identify the rotation of each image, then normalize the predictions by subtracting the applied rotation (effectively shifting the angle into common reference frame) (Algorithm 1). Regardless of the original rotation, two of the further rotated images would correspond to 0 and 180 in the ground-truth reference frame. If the model can correctly identify these two im16 ages, there is high probability that the majority vote would reveal the ground-truth rotation. We evaluate GPT-4o and Qwen-2.5-VL-7B-Instruct on ROTBENCH-SMALL using this approach. Results. Table 10 shows the accuracy on each image orientation using normalized rotation voting. In general, we see much more even performance distribution across all four rotations compared to using zero-shot or chain-of-thought prompting. Qwen2.5-VL-7B-Instruct achieves substantially stronger performance on 0 and 90 (0.95 and 0.57) compared to 180 and 270 (0.03 and 0.15). Normalized rotation voting achieves 0.5 performance on all orientations. GPT-4o achieves 0.8 accuracy on all orientations. In particular, performance on 270 improved substantially compared to zero-shot prompting (+0.85). These results show that this voting approach provides viable means of identifying image rotation using MLLMs. However, this approach has significant drawbacks. Firstly, it increases the compute required to predict single image, as the model processes four images for each input image. Secondly, it relies on the assumption that we have prior knowledge of all possible rotations of the image. In real-world use cases, the rotation of an input image is most likely continuous value. While it is possible to discretize continuous rotations to level of granularity, this approach would quickly become impractical as finer granularity increases model calls. Similar Images in Spatial-MM During the data filtering process, we noted several images in Spatial-MM closely resemble each other  (Fig. 11)  . This realization led to our decision to use MS COCO as the training dataset in our fine-tuning Accuracy on Different Degrees of Rotation Temperature 0 90 180 270 GPT-4o 0.0 0.2 0.5 0.7 1.0 0.990.00 0.980.00 0.990.00 0.980.01 0.980.00 Qwen-2.5-VL-7B-Instruct 0.990.00 0.0 0.990.00 0.2 0.940.01 0.5 0.90.02 0.7 0.830.00 1.0 0.690.02 0.690.02 0.680.02 0.680.01 0.650.02 0.510.01 0.480.00 0.440.00 0.420.02 0.390. 0.930.00 0.930.00 0.930.00 0.980.00 0.920.00 0.050.01 0.060.01 0.090.01 0.090.02 0.130.02 0.190.01 0.20.03 0.20.03 0.230.02 0.250.01 0.090.01 0.110.02 0.190.01 0.20.02 0.240.01 Table 8: Average classification accuracy under different image rotation angles (0, 90, 180, 270) for GPT-4o and Qwen-2.5-VL-7B-Instruct using various sampling temperatures. Accuracy is scored on four-way classification task across three runs on ROTBENCH-LARGE. Num. Images 0 90 180 270 0 8 24 40 0.99 1.00 0.97 0. 0.51 0.43 0.49 0.59 0.05 0.04 0.14 0.09 0.09 0.04 0.03 0.06 Table 9: Qwen-2.5-VL-7B-Instructs performance on each orientation when provided different numbers of in-context images. Accuracy () Model 0 90 180 270 Qwen-2.5-VL-7B-Instruct Zero-shot Chain-of-thought Normalized Rotation Voting GPT-4o Zero-shot Chain-of-thought Normalized Rotation Voting 0.95 0.86 0.54 0.87 0.91 0.86 0.57 0.20 0.52 0.65 0.59 0.88 0.03 0.13 0.56 0.85 0.86 0. 0.15 0.30 0.52 0.21 0.21 0.86 Table 10: Average classification accuracy under different image rotation angles (0, 90, 180, 270) using normalized rotation voting on ROTBENCH-SMALL. Accuracy is scored on four-way classification task. experiment."
        },
        {
            "title": "K License",
            "content": "We will publicly release our code and models. We provide the following links to the standard licenses for the datasets, code, and models used in this project. Spatial-MM: No license specified (accessed August 20, 2025). Annotations from GitHub. MS COCO: Creative Commons Attribution 4.0 17 Figure 11: pair of images in Spatial-MM that closely resemble each other. License. Qwen-2.5-VL-7B-Instruct: Apache 2.0. Llama-3.2-11B-Instruct: Llama 3.2 Community License. GPT-4o, GPT-4.1, GPT-5, o3: OpenAI Services Agreement and Service Terms. Gemini-2.0-Flash, Gemini-2.5-Flash, Gemini2.5-Pro: Gemini API Additional Terms of Service."
        },
        {
            "title": "L Prompts",
            "content": "This section includes all prompts used in our experiments. L.1 Extract Image Subjects Fig. 12 describes the prompt used for extracting primary subjects in images. The list of subjects extracted is later used to obtain bounding boxes, scene graphs, and segmentation maps . L.2 Captions Fig. 13 describes the prompt used for captioning images."
        },
        {
            "title": "User Prompt",
            "content": "<Image encoded via. Base64> Return list of objects in this image. The list will later be passed to bounding box model to extract bounding boxes for each detected object. Format your response as Python list, surrounded with Python markdown fence. For example: python[fedora, woman in green dress, man in red suit, ...] Each object should have distinct name. ENSURE YOUR RESPONSE FOLLOWS THE FORMATTING REQUIREMENTS! Figure 12: Prompts used for extracting primary subjects in images."
        },
        {
            "title": "User Prompt",
            "content": "<Image encoded via. Base64> Generate detailed caption for this image. Do not include any preceding text before the caption. Figure 13: Prompts used for captioning images. L.3 Scene Graphs Fig. 14 describes the prompt used for extracting scene graphs from images. L.4 Clockwise Experiment Fig. 15 describes the prompt used for clockwise vs counter-clockwise rotation experiment. L.5 Rotation Classification Fig. 16 describes the system and user prompts for rotation classification. The mapping between letter choice and degrees is shuffled each prompt."
        },
        {
            "title": "User Prompt",
            "content": "<Image encoded via. Base64> Task: Given the image and key objects, generate scene graph for this image. Represent each relationship as three-element tuple with (subject_id, predicate, object_id). Extract set of words describing the location, orientation, directions and spatial or positional relations between key objects in the image. Your answer should be list of values that are in the format of (object1, relation, object2). The relation MUST be one of [left, right, above, below, facing left, facing right, front, behind]. You are to interpret the image literally. If you see sky below mountain, your scene graph must reflect that. Format your response as Python list of tuples, surrounded by markdown fence. Example formatting: python [ (\"object1\", \"predicate1\", \"object2\"), (\"object2\", \"predicate2\", \"object3\"), ...] Key objects in the image: <previously extracted image subjects> Figure 14: Prompts used for extracting scene graphs from images."
        },
        {
            "title": "System Prompt",
            "content": "You are an intelligent AI assistant that specializes in identifying rotation in images. You will be given an image that has been rotated 90 or 270 degrees. Specifically, 90 degree rotation is quarter-turn counter-clockwise (the same as 270 degrees clockwise); 270 is three quarter-turns counter-clockwise (the same as 90 degrees clockwise)."
        },
        {
            "title": "User Prompt",
            "content": "<Image encoded via. Base64> Your task is to identify whether the image has been rotated 90 or 270 degrees counter-clockwise. Examine the image closely and identify the rotation. Lets think step-by-step. Figure 15: Prompts used for clockwise versus counter-clockwise rotation experiment."
        },
        {
            "title": "System Prompt",
            "content": "You are an intelligent AI assistant that specializes in identifying rotation in images. You will be given an image and multiple choice question. Each choice corresponds to the number of degrees the image has been rotated. 90 rotation is quarter-turn counter-clockwise; 270 is quarter-turn clockwise. 0 rotation indicates the image is right-side up; 180 rotation indicates the image is upside-down."
        },
        {
            "title": "User Prompt",
            "content": "In addition, you have been provided some extra <Image encoded via. Base64> < (if included) Depth map encoded via. Base64> < (if included) Segmentation map encoded via. Base 64> Identify whether the image has been rotated. information about this image below. < If caption > The image is given the following caption: <caption> < If bounding box> Below is the normalized bounding box of objects in the image. Each object is bounded by four floats [xmin, ymin, xmax, ymax] (each float has been normalized between 0 and 1) <bounding boxes> < If scene graph > Below is scene graph representing objects within the image and the relationship between them. <scene graph> < If depth map > Attached is also an estimated depth map of the image. The brighter the pixel, the further it is. < If segmentation map > Attached is also segmentation map of the image. Each object has been highlighted different color. < If CoT > What is the rotation of this image? Lets think step-by-step. < If rotation grid > Attached is grid showing the image rotated counter-clockwise in different orientations. The top-left image is the original image shown prior, the top-right image has been rotated 270 counter-clockwise, the bottom-right 180 counter-clockwise, and the bottom-left 270 counter-clockwise. Using these other images as an aid, what is the rotation of the original image? Lets think step-by-step < If rotation grid guided > Attached is grid showing the image rotated counter-clockwise in different orientations. The top-left image is the original image shown prior, the top-right image has been rotated 270 counter-clockwise, the bottom-right 180 counter-clockwise, and the bottom-left 270 counter-clockwise. Use this three step procedure: (1) Carefully examine all four images shown in the grid. (2) Identify the image you are most familiar with, or which image most resembles your training data, as an anchor point. (3) Starting from that image, algebraically determine the rotation of the original image. Using these other images as an aid, what is the rotation of the original image? Lets think step-by-step < Else > Response with SINGLE LETTER, either A, B, C, or D, representing the correct rotation. You must select one of these choices even if you are uncertain. DO NOT INCLUDE ANYTHING ELSE IN YOUR RESPONSE. The rotation of the image is: A. 0 B. 270 C. 90 D. 180 Answer: Figure 16: System and user prompts for rotation classification."
        }
    ],
    "affiliations": [
        "UNC Chapel Hill"
    ]
}