{
    "paper_title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
    "authors": [
        "KiHyun Nam",
        "Jongmin Choi",
        "Hyeongkeun Lee",
        "Jungwoo Heo",
        "Joon Son Chung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contrastive audio-language pretraining yields powerful joint representations, yet a persistent audio-text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance https://github.com/DevKiHyun/Diffusion-Link"
        },
        {
            "title": "Start",
            "content": "DIFFUSION-LINK: DIFFUSION PROBABILISTIC MODEL FOR BRIDGING THE AUDIO-TEXT MODALITY GAP KiHyun Nam1, Jongmin Choi1, Hyeongkeun Lee1, Jungwoo Heo2, Joon Son Chung1 1Korea Advanced Institute of Science and Technology, South Korea, 2University of Seoul, South Korea 5 2 0 2 3 1 ] . [ 1 0 3 3 1 1 . 0 1 5 2 : r ABSTRACT Contrastive audiolanguage pretraining yields powerful joint representations, yet persistent audiotext modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, diffusion-based modalitybridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zeroshot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers promising direction beyond knowledgeretrieval-centric designs. Code will be released upon acceptance1. Index Terms diffusion probabilistic model, modality gap, large language model, audio captioning, multimodal representation learning 1. INTRODUCTION Large-scale audiolanguage models have shown strong multimodal In particular, performance across range of multimodal tasks. CLAP [1, 2] maps natural-language descriptions and acoustic signals into shared embedding space via contrastive learning, achieving state-of-the-art results on various audiolanguage multimodal tasks [3]. In parallel, advances in LLMs [46] enable coupling contrastive audiolanguage encoders with powerful decoders, already demonstrating compelling audiolanguage reasoning and captioning [7, 8]. Yet recent studies reveal structural modality gap in contrastive multimodal encoders. Liang et al. [9] quantified the gap and linked its magnitude to zero-shot performance and fairness, while Zhang et al. [10] analyzed embedding geometry and showed that gap reduction benefits cross-modal tasks. From an application angle, linking contrastive spaces [1, 11] via mediating modalities enables unpaired transfer [12], and broader alignment across audiovisiontext3D yields competitive zero-shot results [13]. Taken together, these prior These authors contributed equally to this work. 1Official code: https://github.com/DevKiHyun/Diffusion-Link works suggest that addressing the modality gap is essential for improving zero-shot and cross-modal task performance. Diffusion models [14, 15] have become standard generative paradigm in various fields, reliably producing high-fidelity samples [1618]. They learn forward noising process toward an isotropic Gaussian and reverse denoising process back to the target distribution. Viewing embedding vector as data, diffusion can learn trajectory that bridges the embedding distributions between two modalities. We adopt this view and design reverse process that first moves audio embeddings to shared isotropic Gaussian waypoint and then maps them into the text-embedding distribution, thereby enabling effective modality bridging. this view. Recent embedding-generative works support In speaker recognition, SEED [19] applies the forward process to both clean and noisy speaker embeddings and trains the reverse process to regenerate the clean speaker embeddings, introducing cross-sample prediction and demonstrating embedding-level genIn visionlanguage, Diffusion-Bridge [20] trains only on eration. CLIP text embeddings and injects image embeddings at an intermediate reverse step to convert them into text-like vectorsan early instance of embedding-space modality bridging. We propose Diffusion-Link, which directly bridges the audio text modality gap, building on prior works [19, 20]. The key idea is to (i) use paired audiotext embeddings from an audio-language multimodal encoder during training to explicitly connect the two distributions, and (ii) achieve modality bridging by enforcing that the reverse process always map to the text embedding distribution. To this end, we gradually inject Gaussian noise into both embeddings in the forward process to send them to common isotropic Gaussian state, and train with an L2 reconstruction loss so that the reverse process consistently predicts embeddings from the text distribution. Moreover, we add topology loss that preserves the relative geometry of the text distribution by matching the within-batch cosine similarity structure of the original text and the generated text-like embeddings. At inference, Diffusion-Link outputs text-like embedding regardless of the input modality. Diffusion-Link is lightweight network composed of three residual multilayer perceptron (MLP) blocks, and the multimodal encoder is frozen during training. For practical validation, we attach Diffusion-Link after multimodal encoder as plug-in and combine it with LLM-based decoder to evaluate audio captioning. To our knowledge, this is the first attempt to apply diffusion-based modality bridging to audio captioning. We verify consistent gains on the AudioCaps [21] dataset along two axes: modality-gap analysis and LLM-based downstream tasks. On similarity and geometric criteria, Diffusion-Link increases the similarity of paired audiotext samples while decreasing that of unpaired, achieving the largest gap reduction over prior methods. Visualizations further show clear collective migration of audio embeddings toward the text-embedding distribution after the diffusion process. In Automatic Audio Captioning (AAC), attaching Fig. 1: (a) Overview of the proposed Diffusion-Link mechanism and (b,c) illustration of our LLM-based AAC system with Diffusion-Link. Diffusion-Link as plug-in to the same multimodal LLM baseline yields relative improvements of up to 52.5% in zero-shot audio captioning and 7.5% in fully supervised audio captioning, reaching state-of-the-art in both cases without external knowledge. Because many existing systems, especially in zero-shot, rely on external knowledge such as retrieval-augmented generation (RAG), these results establish Diffusion-Link as new powerful solution that achieves consistent gains on the same multimodal LLM system while shifting the source of performance from knowledge retrieval to modality bridging. 2. METHOD In this section, we describe the proposed framework  (Fig. 1)  . We de0 Rd the paired audio and text embeddings obtained note by ea from multimodal encoder [1]. For brevity, we use to indicate the modality, with representing audio and text t. 0, et 2.1. Background on Diffusion Probabilistic Models We briefly review denoising diffusion probabilistic models (DDPM) [14] under sample-prediction formulation. The forward diffusion process progressively corrupts given sample z0 q(z0) at each timestep = 1, . . . , : q(zsz0) = N(cid:0)zs; αs z0, (1 αs)I(cid:1), where 0 αs 1 is the noise schedule, αs = (cid:81)s τ =1 ατ , and is the identity matrix. This also admits the following closed-form reparameterization: (1) zs = αsz0 + 1 αs ϵ, ϵ (0, I). (2) The reverse diffusion process gradually denoises zt back toward 2.2. Modality Gap Bridging via Diffusion-Link Diffusion-Link is neural network denoiser trained at the output embeddings of the multimodal encoder. 2.2.1. Training Objective We apply the same forward process (1) to each modality M: eM = αs eM 0 + 1 αs ϵM, ϵM (0, I). (5) The denoiser ϕθ(, s) is trained under the sample-prediction formulation to map both noised text and audio embeddings to the text embedding distribution at = 0. This yields the cross-sample prediction loss [19]: (cid:104) Ldiff = et (cid:124) 0 ϕθ(et (cid:123)(cid:122) texttext s, s)2 2 (cid:125) + et (cid:124) 0 ϕθ(ea (cid:123)(cid:122) audiotext , s)2 2 (cid:125) (cid:105) , (6) where the first term enforces high-fidelity reconstruction of text-like embeddings, while the second term encourages audio embeddings toward the text distribution. Furthermore, we introduce batch-level topology loss to preserve the relative geometry of the text distribution. Let = [et 0,i]iB and ˆX = [ˆet i]iB denote the text and text-like embedding matrices. Row-wise ℓ2-normalized matrices and ˆX are obtained from and ˆX, respectively, yielding similarity matrices Sxx = XX RBB and Sxˆx = ˆX RBB, and the topology loss is the squared Frobenius distance: Ltopo = Sxx Sxˆx2 . the data distribution at each timestep s: The total training objective is pθ(zs1zs) = N(cid:0)zs1; µθ(zs, s), σ I(cid:1), (3) Ltotal = Ldiff + Ltopo. where µθ(zs, s) is parameterized by neural denoiser. The denoiser ϕθ(, s) is trained to predict the sample z0 at = 0 via the objective 2.2.2. Inference to generate text-like embedding (7) (8) = Ez0,s,ϵ (cid:13) (cid:13)z0 ϕθ(zs, s)(cid:13) 2 (cid:13) 2 . (4) At inference, given eM, we optionally apply forward noising at step with ϵ (0, I), and then run the learned reverse trajectory to Table 1: Average cosine similarity scores for various embedding pairs on AudioCaps. For the CLAP, no transformation is applied, so ˆeM = eM. We report et ˆetM, where ˆetM denotes text-like embedding generated from modality M. Transformations are obtained via C3 [10], DB (Diffusion-Bridge) [20], DG (DiffGap) [22], and our DL (Diffusion-Link). Here, and indicate matched and non-matched pairs, respectively. Comparison Pair Cosine Similarity (a) et ˆeta() (b) et ˆett() (c) et ˆeta() (d) et ˆett() CLAP DB DG DL 0.486 1.000 0.030 0.098 0.547 1.000 0.092 0.158 0.528 0.999 0.000 0. 0.110 0.334 0.007 0.043 0.688 0.945 0.000 0.001 Table 2: Average cosine similarity scores for various inference forward timestep during diffusion process on AudioCaps. Diffusion-Link Inference forward timestep 200 300 400 500 Cosine Similarity 0. 0.654 0.596 0.510 0.404 hop length, and then form mel-spectrograms with 64 mel-bins. We train on the train split and report results on the test split. 3.2. Implementation Details and Metrics For audio-language multimodal encoder, we use the LAION-CLAP pretrained model [2] and keep it frozen. Following prior work [20], we apply the same normalization process to the output embeddings of CLAP. For Diffusion-Link, we adopt three residual MLP blocks [31]. We train Diffusion-Link with the Adam [32] optimizer and batch size of 128. The base learning rate is set to 1104 and follows step-decay schedule, multiplying the rate by 0.97 every 200 steps. We employ an exponential moving average (EMA) of the model parameters with decay of 0.995 and use the EMA weights for inference. We adopt cosine noise schedule with total of =1000 timesteps. At inference, we employ DDIM [15] sampling with 5 iteration steps. Before denoising, we apply shallow forward noising to s=100 and then run the reverse procoess. For LLM-based text decoder, we adopt LLaMA2(7B) [5] as the LLM decoder. In the text-only training, we employ linear layer with soft prefix tokens m=1 for the project head and prepend short instruction prompt; in the fully supervised training, we use 2 linear layers with m=10 for the project head and no hard prompt. We fine-tune project head and the LLM using LoRA [33]. LLM training uses AdamW [34] optimizer with batch size 4 for 50 epochs: the learning rate warms up over the first 2 epochs with max learning rate 5106, then use cosine decay. We also train baseline multimodal LLM system to verify the effectiveness of Diffusion-Link, we adopt same setting but detach only Diffusion-Link module. For evaluation, we adopt the metrics for modality gap analyzing, including cosine simiarity and visualization using UMAP [35]. For AAC, we use the metrics, METEOR (ME) [36], CIDEr (CD) [37], SPICE (SP) [38], and SPIDEr (SD) [39]. 4.1. Main Results 4. RESULTS Effectiveness of Diffusion-Link for Modality Bridging. As shown in Table 1, Diffusion-Link attains the highest cosine similarity on Fig. 2: Visualization of embeddings on AudioCaps using UMAP. Red line means the pair of audio and text embeddings. Green line means the pair of text-like and original text embeddings. = 0 using DDIM sampler [15]: eM = ˆet = DDIM(cid:0)ϕθ, eM αs eM +(cid:112)1 αs ϵ, , 0(cid:1), (Forward) (Reverse) (9) (10) The output ˆet is text-like embedding. 2.3. LLM-based Text Decoding Given text-like embedding ˆet, projection head maps it to softprefix vector Rmh. Here denotes the number of learnable soft tokens and denotes the decoder hidden size. We then into soft-prefix tokens sequence Rmh and feed this sequence to the decoder. If we optionally prepend fixed instruction prompt of tokens, the resulting input becomes R(m+n)h. We consider two training options for our LLM decoder framework: (i) text-only training: using text-driven ˆet with an instruction prompt. (ii) fully supervised training: using audio-driven ˆet and we not use an instruction prompt. We train the LLM decoder with the standard autoregressive cross-entropy objective LCE = (cid:88) l=1 log pψ (cid:0)wlw<l, p(cid:1), (11) where ψ denotes the LLM decoders learnable parameters and = (w1, . . . , wL) means the target caption tokens. At inference, given audio data only with optional instruction prompt, and decode target caption. When the LLM decoder is trained under the text-only training, this evaluation corresponds to zero-shot captioning. 3. EXPERIMENTAL SETTINGS 3.1. Datasets For training and evaluation, we conduct all experiments on AudioCaps [21], corpus of ten-second audio clips paired with humanwritten captions. We use 48,595 training clips and 944 test clips. The train split provides one caption per clip, whereas the test split provides up to five. All audio is resampled to 48kHz. For audio preprocessing, we compute STFTs with 1,024 window size and 480 Table 3: Performance comparison of AAC models on AudioCaps. External knowledge # is the number of non-audio samples used by the LLM at test time. For fair comparison on the embedding-level modality-gap problem, results use only embedding-level RAG without external k-caption selection. Method Zero-shot Captioning ZerAuCap [23] DRCap [24] Zhang et al. [25] WSAC [26] Ours Fully Supervised Captioning Prefix AAC [27] RECAP [28] EnCLAP-large [29] CLAP-ART [30] Ours Encoder output dim. External knowledge # ME CD SP SD 1 1 1 1 1 T T 1 527 450,000 No 46,000 No No 600,000 No No No 12.3 21.8 22.0 24.1 24.2 24.0 25.6 25.5 25.6 25. 28.1 59.5 64.4 63.3 73.2 73.3 75.1 80.3 80.7 82.5 8.6 15.7 15.6 17.3 17.5 17.7 18.6 18.8 18.8 18.9 18.3 37.6 40.0 40.3 45.4 45.5 47.1 49.5 49.8 50. Table 4: Ablation study to analyze the effectiveness of diffusionbased modality bridging method. Method ME CD SP SD Zero-shot Captioning Baseline (CLAP & LLaMa2-7B) + Diffusion-Bridge [20] + Diffusion-Link (Ours) Fully Supervised Captioning Baseline (CLAP & LLaMa2-7B) + Diffusion-Bridge [20] + Diffusion-Link (Ours) 21.2 48.0 14.4 31.2 23.3 62.6 16.5 39.5 24.2 73.2 17.5 45.4 25.0 76.9 18.6 47.7 25.2 77.1 18.0 47.4 25.6 82.5 18.9 50.7 matched audiotext pairs. While most approaches improve over CLAP, DiffGap underperforms, because it generates from pure Gaussian noise with the input embedding condition, which weakens information reconstruction. By contrast, Diffusion-Link treat the input embedding as residing at an intermediate reverse step, thereby minimizing information loss and ensuring high-quality generation along the reverse trajectory. Importantly, Diffusion-Link also yields the lowest similarity on non-matched pairs, indicating not merely global contraction of the space but maintaining semantic information. Figure 2 visualizes this effect. Both the generated text-like embeddings from audio and text embeddings all move toward the ground-true text embedding distribution, demonstrating that Diffusion-Link has learned stable generative modality bridge for the text embedding distribution, regardless of the input modality. Diffusion-Link Amplifies Multimodal Encoder-LLM Coupling. Table 3 compares range of AAC systems. In contrast to many prior methods that leverage longer audio representations or external knowledge (e.g., RAG), our multimodal LLM system captures input audio feature with only single 1D text-like embedding produced by Diffusion-Link, and achieves SOTA in both zero-shot and fully supervised captioning without external knowledge. Notably, considering that most prior zero-shot models rely heavily on external knowledge, outperforming them without any external knowledge demonstrates the significant efficiency of Diffusion-Link. According to Table 4, our baseline LLM-based AAC system is not competitive relative to prior AAC systems in Table 3. However, applying Diffusion-Link markedly improves the same backbone. In zero-shot audio captioning, we observe 52.5% relative increase in CIDEr together with substantial gains on the other metrics. These dramatic gains demonstrate that Diffusion-Link is the key factor and reaffirm the primacy of modality-gap reduction over using longer audio representations or external knowledge. Moreover, in fully supervised audio captioning we observe up to 7.3% relative improvement, underscoring our methods applicability. 4.2. Ablation Studies We conduct ablations to analyze how the depth of forward noising affects modality bridging and high-quality generation. According to Table 2, increasing the inference forward timestep from shallow levels initially keeps similarity quite stable; beyond threshold, the similarity drops sharply as increases. This indicates that overnoising pushes representations deeper into the common Gaussian space and erases information, thereby degrading semantic preservation in the reconstructed text-like embeddings. This finding is consistent in AAC results. In Table 1, the similarity score of Diffusion-Bridge is similar to that of Diffusion-Link when is between 300 and 400. This suggests that the performance of Diffusion-Bridge corresponds to over-noising of Diffusion-Link, which aligns with the observed semantic information loss. Furthermore, under the same multimodal LLM system, the AAC results in Table 4 follow the same pattern: attaching Diffusion-Link yields large gains, whereas using Diffusion-Bridge provides only limited improvements. Together, the three tables show that excessive forward noising reduces similarity and weakens bridging, which in turn harms downstream performance; conversely, choosing an appropriate maximizes content preservation in the text-like embedding, strengthens conditioning-distribution alignment for the LLM decoder, and translates into AAC gains. 5. CONCLUSION We introduced Diffusion-Link, lightweight residual MLP diffusion module that bridges audio embeddings to the text embedding distribution without keeping the multimodal encoder frozen. The method aligns the conditioning input by increasing matched similarity and decreasing mismatched similarity. On AAC, it improves the same multimodal LLM baseline by 52.5% and 7.3% without external knowledge for zero-shot and fully supervised AAC, respectively. This plug-in-play approach of Diffusion-Link is expected to generalize beyond audio captioning and enable effective zero-shot performance in variety of multimodal LLMs."
        },
        {
            "title": "References",
            "content": "[1] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang, CLAP: Learning audio concepts from natural language supervision, in Proc. ICASSP, 2023. [2] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov, Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation, in Proc. ICASSP, 2023. [3] Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha, Reclap: Improving zero shot audio classification by describing sounds, in Proc. ICASSP, 2025. [4] Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al., Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality, See https://vicuna. lmsys. org (accessed 14 April 2023), vol. 2, no. 3, pp. 6, 2023. [5] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al., Scaling instructionfinetuned language models, J. Mach. Learn. Res., vol. 25, no. 70, pp. 153, 2024. [7] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang, SALMONN: Towards generic hearing abilities for large language models, arXiv preprint arXiv:2310.13289, 2023. [8] Kyeongha Rho, Hyeongkeun Lee, Valentio Iverson, and Joon Son Chung, Lavcap: Llm-based audio-visual captioning using optimal transport, in Proc. ICASSP, 2025. [9] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou, Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning, in NeurIPS, 2022. [10] Yuhui Zhang, Elaine Sui, and Serena Yeung, Connect, collapse, corrupt: Learning cross-modal tasks with uni-modal data, in Proc. ICLR, 2024. [11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al., Learning transferable visual models from natural language supervision, in Proc. ICML, 2021. [12] Zehan Wang, Yang Zhao, et al., Connecting multi-modal contrastive representations, in NeurIPS, 2023. [13] Ziang Zhang, Zehan Wang, Luping Liu, Rongjie Huang, Xize Cheng, Zhenhui Ye, Wang Lin, Huadai Liu, Haifeng Huang, Yang Zhao, Tao Jin, Siqi Zheng, and Zhou Zhao, Extending multi-modal contrastive representations, in NeurIPS, 2024. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel, Denoising diffusion probabilistic models, in NeurIPS, 2020. [15] Jiaming Song, Chenlin Meng, and Stefano Ermon, Denoising diffusion implicit models, in Proc. ICLR, 2021. [16] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen, Hierarchical text-conditional image generation with clip latents, arXiv preprint arXiv:2204.06125, vol. 1, no. 2, pp. 3, 2022. [17] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer, High-resolution image synthesis with latent diffusion models, in Proc. CVPR, 2022. [18] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley, Audioldm: text-to-audio generation with latent diffusion models, in Proc. ICML, 2023. [19] Kihyun Nam, Jungwoo Heo, Jee weon Jung, Gangin Park, Chaeyoung Jung, Ha-Jin Yu, and Joon Son Chung, SEED: Speaker Embedding Enhancement Diffusion Model, in Proc. Interspeech, 2025. [20] Jeong Ryong Lee, Yejee Shin, Geonhui Son, and Dosik Hwang, Diffusion bridge: Leveraging diffusion model to reduce the modality gap between text and vision for zero-shot image captioning, in Proc. CVPR, 2025. [21] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim, Audiocaps: Generating captions for audios in the wild, in NAACL-HLT, 2019. [22] Shentong Mo, Zehua Chen, Fan Bao, and Jun Zhu, Diffgap: lightweight diffusion module in contrastive space for bridging cross-model gap, in Proc. ICASSP, 2025. [23] Leonard Salewski, Stefan Fauth, Koepke, and Zeynep Zero-shot audio captioning with audio-language in Proc. Akata, model guidance and audio context keywords, NeurIPS ML4Audio Workshop, 2023. [24] Xiquan Li, Wenxi Chen, Ziyang Ma, Xuenan Xu, Yuzhe Liang, Drcap: Zhisheng Zheng, Qiuqiang Kong, and Xie Chen, Decoding clap latents with retrieval-augmented generation for zero-shot audio captioning, in Proc. ICASSP, 2025. [25] Yiming Zhang, Xuenan Xu, Ruoyi Du, Haohe Liu, Yuan Dong, Zheng-Hua Tan, Wenwu Wang, and Zhanyu Ma, Zero-shot audio captioning using soft and hard prompts, IEEE/ACM Trans. on Audio, Speech, and Language Processing, 2025. [26] Theodoros Kouzelis and Vassilis Katsouros, Weaklysupervised automated audio captioning via text only training, in Proc. DCASE Workshop, 2023. [27] Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh, Prefix tuning for automated audio captioning, in Proc. ICASSP, 2023. [28] Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Ramani Duraiswami, and Dinesh Manocha, Recap: Retrievalaugmented audio captioning, in Proc. ICASSP, 2024. [29] Jaeyeon Kim, Jaeyoon Jung, Jinjoo Lee, and Sang Hoon Woo, EnCLAP: Combining neural audio codec and audio-text joint embedding for automated audio captioning, in Proc. ICASSP, 2024. [30] Daiki Takeuchi, Binh Thien Nguyen, Masahiro Yasuda, Yasunori Ohishi, Daisuke Niizumi, and Noboru Harada, Clapart: Automated audio captioning with semantic-rich audio representation tokenizer, arXiv preprint arXiv:2506.00800, 2025. [31] Tianhong Li, Dina Katabi, and Kaiming He, Return of unconditional generation: self-supervised representation generation method, in NeurIPS, 2024. [32] Diederik Kingma, Jimmy Ba, Bengio, and LeCun, Adam: method for stochastic optimization, in Proc. ICLR, 2015. [33] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, LoRA: Low-rank adaptation of large language models, in Proc. ICLR, 2022. [34] Ilya Loshchilov and Frank Hutter, Decoupled weight decay regularization, in Proc. ICLR, 2019. [35] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger, Umap: Uniform manifold approximation and projection, Journal of Open Source Software, 2018. [36] Satanjeev Banerjee and Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, in Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, 2005. [37] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh, in Cider: Consensus-based image description evaluation, Proc. CVPR, 2015. [38] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould, Spice: Semantic propositional image caption evaluation, in Proc. ECCV, 2016. [39] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy, Improved image captioning via policy gradient optimization of spider, in Proc. ICCV, 2017."
        }
    ],
    "affiliations": [
        "Korea Advanced Institute of Science and Technology, South Korea",
        "University of Seoul, South Korea"
    ]
}