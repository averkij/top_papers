{
    "paper_title": "Sotopia-RL: Reward Design for Social Intelligence",
    "authors": [
        "Haofei Yu",
        "Zhengyang Qi",
        "Yining Zhao",
        "Kolby Nottingham",
        "Keyang Xuan",
        "Bodhisattwa Prasad Majumder",
        "Hao Zhu",
        "Paul Pu Liang",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 5 0 9 3 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SOTOPIA-RL: REWARD DESIGN FOR SOCIAL INTELLIGENCE Haofei Yu1 Zhengyang Qi4 Yining Zhao1 Kolby Nottingham2 Keyang Xuan1 Bodhisattwa Prasad Majumder3 Hao Zhu5 Paul Pu Liang6 Jiaxuan You1 1University of Illinois Urbana-Champaign, 2University of California Irvine, 3Allen Institute for Artificial Intelligence, 4Carnegie Mellon University, 5Stanford University, 6Massachusetts Institute of Technology https://rl.sotopia.world"
        },
        {
            "title": "ABSTRACT",
            "content": "Social intelligence has become critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose SOTOPIA-RL, novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in SOTOPIA, an open-ended social learning environment, demonstrate that SOTOPIARL achieves state-of-the-art social goal completion scores (7.17 on SOTOPIA-hard and 8.31 on SOTOPIA-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl."
        },
        {
            "title": "INTRODUCTION",
            "content": "Social intelligence (Gweon et al., 2023; Mathur et al., 2024; Zhu et al., 2025) has emerged as critical capability for large language models (LLMs), enabling them to interpret and respond effectively to human feedback in multi-turn interactions (Han et al., 2024; Yi et al., 2024). It underpins wide range of impactful applications, including customer service (Pandya & Holia, 2023; Bamberger et al., 2023), educational tutoring (Stamper et al., 2024; Nye et al., 2023), conflict resolution (Aggrawal & Magana, 2024), and team coordination (Li et al., 2023; Guo et al., 2024). In these interactions, communication proceeds through sequence of utterances. We define an utterance as discrete unit of communication that the agent deliberately produces as an action to pursue its social goals. Utterances are not merely linguistic tokens; they carry intentions, emotions, and shape the trajectory of the conversation. socially intelligent agent must therefore generate high-quality utterances that balance two aspects of effectiveness. At the single-step level, an utterance should be goal-related, directly advancing the agents social goal. Across multiple steps, it should also Equal contribution. Each of them can claim to rank first. Equal advising."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: SOTOPIA-RL enhances social intelligence across diverse tasks. SOTOPIA-RL outperforms SOTOPIA-π in goal completion score on accommodation, persuasion, collaboration, and negotiation tasks, as evaluated by SOTOPIA-EVAL. SOTOPIA-π only utilizes episode-level goal completion score as reward signals to conduct self-reinforcement, while SOTOPIA-RL utilizes more fine-grained reward designs for RL training. promote dialogue productivity, sustaining an engaging and constructive conversation that indirectly supports long-term goals. This raises central question for social learning: How can we define high-quality utterance and enable agents to generate them effectively? Uniqueness of social intelligence tasks. Unlike math and coding tasks commonly addressed by training LLMs with RL (Shao et al., 2024b; Wei et al., 2025; Cui et al., 2025), social tasks (like persuasion and collaboration) pose fundamentally different challenges. First, social interactions are inherently partially observable: agents can access the dialogue history but not latent factors such as intentions, emotions, or social norms that shape the outcome. As result, social episodes are noisy, with the quality of individual utterances often only loosely correlated with the final success. In contrast, tasks like math or coding provide much clearer signals: correct trajectory typically requires that most steps are high-quality, with single-step quality closely aligned to the final result. Second, social interactions are inherently multi-dimensional: while some utterances contribute directly to goal achievement, others support it indirectly by maintaining rapport, fostering engagement, and sustaining conversational flow. Unlike math and coding tasks, where the outcome is mostly verifiable and binary (Su et al., 2025), social success must be analyzed across multiple dimensions. Key challenges for training social agents. RL is natural choice for social agent training because environments like SOTOPIA enable dynamic generation of multi-turn social interactions, allowing agents to learn directly from feedback (Wang et al., 2024b; Ouyang et al., 2022). Unlike supervised learning, which depends on static datasets, RL captures the adaptive and evolving nature of dialogue. Effective RL training requires an online reward model (RM) to evaluate the quality of an utterance, and the unique features of social interactions make RM training difficult. Partial observability produces noisy, high-variance episode-level rewards, where low-quality utterances in the middle of the episode can lead to successful outcomes. We address this with utterance-level credit assignment, which supplies fine-grained supervision. At the same time, the multi-dimensionality of social interactions risks RM overfitting to spurious features. To mitigate this, we incorporate additional reward dimensionssuch as rapport-building and knowledge-seekingthat regularize learning and align the RM with broader social expectations. Key insights. Our key insights are that, despite the complexity of social interactions, reward assignment for social interaction is both feasible and reliable when powered by LLMs: (1) Episode-level outcomes can be decomposed into utterance-level contributions, and LLMs can reliably infer which utterances contributed to or hindered success. Empirically, diverse LLM families (e.g., Gemini-2.5-flash (Google DeepMind, 2024), GPT-4o (Hurst et al., 2024), Qwen2.5-7b (Qwen et al., 2025)) show strong agreement in attribution scores, with Spearman correlations exceeding 0.7. (2) Evaluating utterance quality across distinct sub-dimensionssuch as rapport, engagement, and"
        },
        {
            "title": "Preprint",
            "content": "knowledge-sharingsimplifies the credit assignment task, turning difficult holistic judgment into set of rubric-based evaluations and making the reward design with less variance. Together, these insights show why utterance-level and multi-dimensional reward design is practical without human annotation, and why LLMs align well with human preferences. Main discoveries. Based on these key insights, we utilize SOTOPIA (Zhou et al., 2023a), an openended social learning environment, for the RL-based training of social agents. SOTOPIA provides diverse social scenarios for simulation and multi-dimensional evaluation metrics named as SOTOPIAEVAL. Experiments conducted in the SOTOPIA environment reveal two key findings: (1) Social agents trained with SOTOPIA-RL consistently outperform all baselines on social goal completion metrics evaluated by SOTOPIA-EVAL, achieving goal completion score of 7.17 on the SOTOPIAhard benchmark and 8.31 on the full SOTOPIA dataset. (2) Both utterance-level reward attribution and multi-dimensional reward combination for reward design are critical for stable and effective RL training in complex social scenarios. These results highlight the importance of social reward design and validate the core design principles behind SOTOPIA-EVAL particularly the importance of evaluating social interaction quality across diverse dimensions."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Social skill learning. To augment the social intelligence of agents, prior work utilizes RL to achieve this. SOTOPIA-π (Wang et al., 2024b) uses self-reinforcement learning, Ndousse et al. (2021) employ conversation-level RL rewards, and Stable Alignment (Pang et al., 2024) trains via rule-based peer feedback without rewards. SDPO (Kong et al., 2025) uses preference-based tuning but overlooks utterance-level impact. Our key contribution is the design of utterance-level rewards specifically for social tasks. Instead of conducting explicit strategy injection for training (Zhang et al., 2025; Wang et al., 2025), we implicitly model social skills within the design of our rewards. Process reward modeling. For reinforcement learning with verifiable rewards (RLVR) like math and programming, Process Reward Modelings (PRMs) have been effectively utilized. PRIME (Cui et al., 2025) assigns token-level rewards using only outcome labels, enhancing reasoning capabilities without the need for explicit process annotations. Similarly, other works (Choudhury, 2025; Wang et al., 2024a) employ Monte Carlo (MC) rollouts to compute reward targets, enabling scalable training of RL. MC methods estimate the expected value of uncertain quantities through repeated random sampling, making them particularly effective in modeling stochastic processes and complex static decision-making tasks (Barto, 2021). Designing an utterance-level reward for social tasks can also be considered as one special type of process reward. Due to the ambiguity and multi-dimensionality of social tasks, such process reward requires multi-dimensional evaluation. Multi-objective RL. Multi-objective RL provides foundational approach for aligning LLMs with multiple preferences by framing alignment as multi-objective optimization problem. The most common strategy employs linear scalarization, which combines multiple reward functions into single objective (Jang et al., 2023; Yang et al., 2024; Li et al., 2020; Zhou et al., 2023b), enabling the reuse of standard RL techniques while varying reward weights. Recent work extends this to nonlinear combinations by incorporating utility functions (Cheng et al., 2025) or using LLMs to search for reward functions (Xie et al., 2024). Other studies improve reward modeling by decomposing rewards into more informative components (Mao et al., 2025; Shenfeld et al., 2025; Lee et al., 2024). Building on these directions, our work focuses on multi-dimensional reward learning for social tasks and relies on linear scalarization, where auxiliary rewardssuch as relationship maintenance and knowledge seekingare explicitly modeled to support goal completion."
        },
        {
            "title": "3 SOTOPIA ENVIRONMENT",
            "content": "In this section, we formally define the SOTOPIA environment, covering social interactions along with their observation and action spaces, as well as social evaluations. This definition highlights the partial observability and multi-dimensional nature of social interactions."
        },
        {
            "title": "3.1 SOCIAL INTERACTION",
            "content": "Given the set = {A, B} of participants in dialogue, social interaction can be modeled as partially observable Markov decision process (POMDP), represented by the tuple S, A, O, T, Z, R. represents the set of possible social states, represents the action space, represents the observation space. : is the transition function. : is the observation function. : is the reward function that reflects the overall quality of the social interaction with respect to each agents private goal, such as successful persuasion or mutual understanding. Observation space. In SOTOPIA, the observations refer to the history of dialogue. Each agent operates under partial observability, receiving at each time step private observation oi Oi, where Oi denotes the private observation space of agent i. Oi consists of all dialogue histories and contextual cues that agent can perceive, but excludes latent variables such as the partners private goals, beliefs, or emotions. The observation oi is generated by the probabilistic observation function Z, modeling the partial and asymmetric nature of social perception. social episode with turns observed by agent is defined as τi = (cid:0)oi 0, ai 0, ; oi 1, ai 1, ; . . . , ; oi (cid:1), (1) where oi agent at time t. This episode captures the full sequence of observations and actions. Oi is the dialogue history observed at time t, and ai Ai is the utterance generated by Action space. An action in SOTOPIA is defined as an utterance, including both verbal (e.g., speaking) and non-verbal (e.g., smiling, hugging) communication. For each agent i, the action space Ai contains all such communicative behaviors. At turn t, the agent samples ai t, gi) from its policy conditioned on observation oi and private goal gi. The joint actions then contribute to and become part of the next observation oi t+1. πθ( oi MDP approximation. To optimize the social agent with MDP-based RL methods like GRPO (Shao et al., 2024b), we adopt an MDP approximation of the SOTOPIA environment. At step t, the state si is considered as the dialogue history, together with its private social goal, the social policy πθ outputs distribution over utterances ai from Stage 1 (Figure 4) are used to train online utterance-level reward model Rψ(si t)), and offline rewards ri t, ai πθ( si (i.e., ai t)."
        },
        {
            "title": "3.2 SOCIAL EVALUATION",
            "content": "To support RL-based training, the SOTOPIA environment provides quantitative feedback on how well each agent achieves its goal using SOTOPIA-EVAL, multi-dimensional evaluation suite with an LLM-based evaluator fθ. evaluation. For Multi-dimensional each agent i, the LLM-based evaluator produces score conditioned on the social episode τi and the agents goal gi: Gi = fθ(τi, gi) R. This score represents the extent to which the agent achieves its social goal gi through social interactions τi. In SOTOPIA-EVAL, this single score is extended to seven-dimensional vector Gi R7, expanding from goal completion (GOAL) to believability (BEL), knowledge seeking (KNO), relationship maintaining (REL), secret keeping (SEC), social rule-following (SOC), and financial/material benefits (FIN). Among these, GOAL serves as the primary objective, while the others provide auxiliary signals of social interaction quality. Detailed definition for each dimension is available in Appendix H. Figure 2: An example of social task in the SOTOPIA environment. Tom is agent A, and Oliver is agent B. Each agent has unique goal that is hidden from the other. 9 / 10 indicates single-dimensional episode-level reward provided by LLMs to describe its goal completion status."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "In this section, we introduce the details of our proposed SOTOPIA-RL framework. It includes two components: (1) social reward design with LLMs (2) social agent training with RL. The first part aims to provide high-quality offline reward labels for the training of utterance-level reward models, and the second part aims to use RL to optimize social agent policy."
        },
        {
            "title": "4.1 SOCIAL REWARD DESIGN WITH LLMS",
            "content": "As discussed in Section 1, providing high-quality utterance-level rewards for social interactions is challenging due to the features of social interactions. We simplify this reward design problem through two steps. First, rather than scoring utterances based on dialogue history, we attribute the episode-level outcome to individual utterances based on the full episode, reducing variance in annotation. Second, we introduce multi-dimensional rubric that breaks down an utterances contribution into clearer aspects of goal achievement. Such decomposition converts complex, noisy task into smaller, more structured judgments that LLMs can perform reliably, yielding fine-grained and trustworthy supervision. Formally, given social episode τi, our goal is to assign each utterance ai an offline reward ri that reflects its contribution to the progression of the interaction. Reward attribution: from episode-level to utterance-level. Episode-level rewards offer only coarse supervision, since even conversations with low-quality utterances can still lead to successful outcomes. Therefore, episode-level rewards Gi are not an accurate estimate of utterance-level rewards for each ai t. To provide more precise feedback, we perform utterance-level credit assignment. Advanced LLMs (e.g., GPT-4o) evaluate each utterance within the full episode context, yielding attribution scores Ai(ai t. Such offline attribution enables reliable credit assignment by leveraging global context. We then refine these scores using the episode-level outcome Gi, so that strong utterances in successful episodes are emphasized, while good contributions in failed episodes remain acknowledged but proportionally down-weighted: t, τi) between 0 and 1 for each utterance ai = Gi Ai(ai ri t, τi). (2) Reward combination: from single to multi-dimension. While utterance-level reward attribution improves granularity, attributing to single reward Gi alone cannot capture the quality of an utterance. High-quality utterances not only advance the agents goal but also foster engagement and sustain conversational flow. To capture such broader contributions, we extend rewards beyond the primary goal completion score Gi to additional dimensions from SOTOPIA-EVAL. Specifically, relationship maintenance (REL) assesses whether an interaction preserves or strengthens the relationship between agents, and knowledge seeking (KNO) assesses whether two agents gain new knowledge during interactions (full definitions in Appendix H). These two dimensions are included for attribution because they promote collaborative and information-rich interactions and indirectly contribute to goal achievement, whereas other metrics in SOTOPIA-EVAL are often too task-specific to generalize. Therefore, for the utterance of agent i, we collect attributed dimension-wise scores ri t,d based on Eq. 2, normalize them within each dimension, and aggregate them into final reward: ri ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) d=1 γd t,d min(ri ri max(ri ,d) ,d) min(ri ,d) . (3) Here is the number of min(r,d), max(r,d) denote the range of scores for that dimension across the dataset. reward dimensions, γd is the weight for dimension d, and Overall: offline and flexible reward design. As shown in Figure 3, we begin with single episode-level reward and expand it along two axes to obtain denser supervision. This process yields utterance-level multi-dimensional rewards that capture the quality of each contribution. The design is offline because it leverages the full dialogue context after the episode concludes, allowing for better evaluation without the uncertainty of real-time inference. It is flexible because the rubric can be extended with additional dimensions and easily adapted to different social contextsfor example, prioritizing relationship building (REL) in therapeutic dialogue or focusing on knowledge seeking (KNO) in educational tutoring."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Overview of social reward design. To better describe and model the quality of an utterance in social interactions, we expand the episode-level reward (9/10 mentioned above) from two axes: (1) expanding from episodelevel into utterance-level; (2) expanding from single-dimension to multi-dimensions, expanding from goal completion (GOAL) to relationship maintaining (REL) and knowledge seeking (KNO). It allows us to have denser reward signals for RL training. Figure 4: Overview of social agent training. Our pipeline has three stages: (1) Data preparation: Generate GPT self-play rollouts and annotate rewards with offline inference, where an LLM evaluates and attributes rewards after the conversation ends. (2) SFT training: Train the policy and an utterance-level reward model via fine-tuning. (3) RL training: Continue RL self-play with rewards from online inference, where the RM sees only the dialogue history up to the current turn for prediction."
        },
        {
            "title": "4.2 SOCIAL AGENT TRAINING WITH RL",
            "content": "As shown in Figure 4, Stage 1 is about social reward design, and in Stages 2 and 3, our key contribution is bridging the POMDP nature of social interactions with the scalability of MDP-based RL through RM distillation. By distilling the knowledge in offline rewards into an utterance-level RM that provides online utterance-level feedback, we enable efficient training while teaching the policy to implicitly infer about the latent social context. Training utterance-level reward models. Directly applying POMDPs to LLM training is impractical because belief tracking over high-dimensional latent states in social interactions is computationally intractable, making MDP-based RL with RM distillation the scalable alternative. As shown on the right of Figure 4, we therefore compress offline reward signals into an RM that provides utterance-level feedback conditioned on dialogue history. While the MDP approximation alone risks overlooking latent social factors that are only partially observable in the local dialogue history si t, our RM Rψ(si t) is trained to recover these signals by distilling the offline rewards into function that infers hidden context from local observations. We train Rψ to regress towards the offline rewards ri (cid:104)(cid:0)Rψ(si This allows the resulting RM Rψ to provide accurate, fine-grained feedback for RL optimization within the tractable MDP framework, while still capturing the latent dimensions of social quality that full POMDP would model. LMSE = through t) ri t, ai t, ai (cid:1)2(cid:105) t,ai t) (4) (si . Training policy models with single-turn online RL. As shown on the left of Figure 4, we train the social agent policy πθ with reward model Rψ that provides utterance-level feedback. Training proceeds in two stages. First, we warm up the policy with behavior cloning (BC) on GPT self-play rollouts to ensure coherent generation. We then further train the policy with GRPO (Shao et al., 2024b), adopting single-turn formulation for efficiency. Each self-play rollout is decomposed into multiple (si t), receives reward Rψ(si t, ai t), and updates to maximize expected rewards. We do not add explicit reasoning traces during inference, focusing on optimizing utterances efficiently. Although simplified to single-turn t) pairs, and at each step the policy generates ai πθ( si t, ai"
        },
        {
            "title": "Preprint",
            "content": "GPT-4o as partner Model GPT-4o Claude-Sonnet-3.5 Deepseek-v3 . +PPDPP 7 +EPO - 5 +DAT 2 +DSI +SOTOPIA-RL SOTOPIA-all SOTOPIA-hard GOAL OVERALL GOAL OVERALL 8.19 8.42 8.14 8.07 8.41 8.11 8.15 8. 3.76 3.77 3.72 3.71 3.86 3.70 3.70 3.90 6.97 6.64 6.69 6.76 6.81 6.78 6.87 7. 3.46 3.30 3.31 3.35 3.51 3.36 3.42 3.61 Table 1: Our method outperforms state-of-the-art models when choosing GPT-4o as partner (p < 0.05, paired t-test on the GOAL dimension). Qwen2.5-7B refers to Qwen-2.5-7B-Instruct. Training method baselines from PPDPP to DSI have details available in Section 5. Full experimental results are available in Appendix E.1. Behavior Cloning (BC) model as partner Attribution Dimension SOTOPIA-hard REL KNO GOAL OVERALL 7 - 5 . 2 Q Uniform Singular Scaled Direct Direct Direct GOAL GOAL GOAL REL KNO GOAL +Behavior Cloning (BC) +SOTOPIA-π +SOTOPIA-RL 1.84 2.74 1.82 3.61 2.56 2.49 2.49 2.41 3.41 4.14 4.93 3.83 4.92 6.06 4.94 3.37 3. 5.53 5.61 6.64 6.74 7.24 6.93 7.21 6.76 6.84 7.81 2.95 3.41 3.15 3.60 3.61 3.49 3.16 3. 3.80 Table 2: Our social reward designs outperform reward design baselines with the BC model as partner (p < 0.05, paired t-test on the GOAL dimension). All results use Qwen2.5-7B-Instruct as the base model. GOAL-RL refers to direct+GOAL; SOTOPIARL combines REL, KNO, and GOAL via direct attribution by averaging. BC and SOTOPIA-π are baselines without reward design. Full experimental results are provided in Appendix E.1. updates, the distillation from offline rewards allows the agent to achieve improved performance in multi-turn interactions."
        },
        {
            "title": "5 EXPERIMENTAL SETTINGS",
            "content": "Model settings. We select Qwen2.5-7b-Instruct 1 as our base LLM for the training of both the policy model and reward model. We select GPT-4o for LLM-as-the-judge in SOTOPIA-EVAL. Evaluation settings. We evaluate our method on two configurations of the SOTOPIA benchmark: (1) SOTOPIA-hard, and (2) SOTOPIA-all. SOTOPIA-hard is subset of SOTOPIA-all, consisting of 14 challenging social scenarios identified as difficult among all scenarios, and we use 10 distinct agent pairings per scenario. For SOTOPIA-all, we evaluate on the full coverage of 90 social scenarios, using 2 agent combos per scenario to ensure diversity while maintaining scalability. More statistical information is in Appendix A. Training method baselines. To compare the effectiveness of our training methods, we include (1) behavior cloning (BC) that utilizes social interaction trajectories between GPT-4o, which is the same as SOTOPIA-π; (2) SOTOPIA-π (Wang et al., 2024b) that utilizes behavior cloning and selfreinforcement; (3) other most recent baselines: PPDPP (Deng et al., 2024), EPO (Liu et al., 2025), DAT (Li et al., 2024), and DSI (Zhang et al., 2025). SOTOPIA-RL denotes our proposed approach, which combines direct utterance-level attribution with multi-dimensional reward combination design (REL +KNO +GOAL), trained using single-turn online RL (GRPO) without explicit reasoning but just utterance generation. Training details are available in Appendix B. Reward attribution baselines. To assess the effectiveness of our reward attribution strategy, we compare it against four baselines, each defining how utterance-level rewards ri are derived from the episode-level score Gi: (1) Uniform every utterance receives the same reward, ri = Gi for all t; (2) Singular only one selected utterance ai = Gi if = and is assigned the full reward, ri 1https://huggingface.co/Qwen/Qwen2.5-7B-Instruct"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Evaluation results with different LLM-based evaluators. The consistent improvement on evaluators indicates no reward hacking. Full results in Appendix E.2. Figure 6: Evaluation results with different partner models. The consistent improvement with multiple partners indicates no reward hacking. Full results in Appendix E.2. Figure 7: GOAL score curve during the training process. reIncorporating additional wards into training delays convergence compared to using the GOAL reward alone. = 0 otherwise; (3) Scaled the episode-level reward is distributed proportionally, ri ri = αtGi with (cid:80) αt = 1 and αt 0; and (4) Direct each utterance is independently attributed with normalized weight, ri = αtGi, where each αt [0, 1] reflects its contextual attribution score, ensuring no utterance exceeds the episode-level score and same with Eq. 2. Direct attribution is the method used in SOTOPIA-RL. Details for each attribution method are in Appendix G. Reward combination baselines. We train our RM with labels combining three SOTOPIA-EVAL relationship maintenance (REL), knowledge seeking (KNO), and goal completion dimensions: (GOAL). Formally, the single-dimension baselines use ri t,GOAL, corresponding to REL-only, KNO-only, and GOAL-only for Eq. 3, respectively. SOTOPIA-RL av- (cid:1) for Eq. 3, allowing us to isolate the t,KNO + ri erages the three signals, ri contribution of each component while validating the benefit of multi-dimensional reward modeling. We use simple average to give equal importance to each reward dimension. Conducting more fine-grained weighting would require hyper-parameter search, which is computationally expensive in the SOTOPIA environment. See Appendix for details. t,KNO, or ri t,REL + ri t,REL, ri = 1 3 = ri = ri = ri (cid:0)ri t,GOAL"
        },
        {
            "title": "6 EXPERIMENTAL RESULTS",
            "content": "SOTOPIA-RL helps build the state-of-the-art social agents on the SOTOPIA benchmark. In Table 1, Qwen-2.5-7B-Instruct trained with SOTOPIA-RL reaches the highest goal completion score, achieving the 7.17 score in the SOTOPIA-hard. It indicates that our utterance-level RM provides better guidance during the training of RL. It also indicates that for multi-turn social interactions, improving the quality of single-turn interactions with suitable single-turn rewards can effectively optimize multi-turn performance. Notably, AMPO (Wang et al., 2025) reaches 7.50 on SOTOPIAhard. But it includes an explicit reasoning process and requires more than 640 inference tokens per utterance on average. Therefore, it is unfair to compare AMPO with ours since we only utilize GRPO to generate utterances without extra tokens for reasoning. Full results are available in Appendix E. SOTOPIA-RL goes beyond distillation from GPT. Our training pipeline begins with GPT-based self-play episodes and GPT-based offline reward annotations. Importantly, GPT annotations are applied offline, conditioning on the entire episode, whereas during RL training, rewards are computed online, conditioned only on the preceding dialogue history. As shown in Table 1, SOTOPIA-RL not only matches but surpasses GPT-4o when used directly as policy model (7.17 vs. 6.97). If SOTOPIA-RL were merely stronger form of distillation, as in behavior cloning, it could at best equal GPT-4os performance, not exceed it. Reward attribution contributes to the performance improvement. Based on Table 2, we find that compared with different baseline methods for reward attribution, our proposed reward attribution methods (direct) bring the most significant improvement in goal completion dimensions, increasing goal completion score from 6.74 to 7.21. Our attribution methods have performance that is much higher than uniform baselines, indicating that the attributed fine-grained dense rewards"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Human evaluation results for SOTOPIA-RL. SOTOPIA-RL has higher goal completion scores than other baselines for human evaluations. GPT-4o and human beings are separately used as evaluators for human evaluation. More details about human evaluation are available in Appendix C. Table 4: Ablation study on the method of reward attribution. We compare online reward labels, assigned by LLMs during the course of conversation, with offline reward labels, assigned by LLMs after the conversation concludes. In both settings, the RMs and policies are trained under the same settings. Model GPT-4o Human Correlation Training Method GOAL OVERALL SOTOPIA-π GOAL-RL SOTOPIA-RL 6.84 7.21 7.81 5.41 5.80 5.89 0.674 0.754 0.866 Behavior Cloning (BC) RL w/ online reward labels RL w/ offline reward labels 6.76 6.69 7. 3.16 3.15 3.80 play an important role during RL training. Moreover, compared with baselines such as scaled and singular attribution, we find that relaxing attribution constraints allows LLMs greater freedom to assign scores within minimal range limits, better leveraging their social reasoning abilities and leading to superior performance. Reward combination contributes to the performance improvement. Based on Table 2, training with multiple reward dimensionsgoal completion (GOAL), relationship maintenance (REL), and knowledge seeking (KNO)significantly improves performance compared to using single reward dimension. The best result comes from combining all three dimensions, yielding 7.9This improvement arises because multi-objective RL encourages the policy to balance different aspects of interaction quality when generating utterances. Notably, as shown in Table 2, optimizing each dimension independently also improves performance on the others, suggesting that the objectives are correlated. Thus, combining them makes reward model training more robust."
        },
        {
            "title": "7 DISCUSSION",
            "content": "To assess the effectiveness of SOTOPIA-RL, we first ensure that its performance gains are genuine and not the result of reward hacking (RQ1). We then analyze how our improvements come from the design of the reward attribution (RQ2) and the reward combination (RQ3). RQ1: Does our improvement come from reward hacking or shortcut learning? No, SOTOPIARL learns high-quality social skills instead of overfitting on partner models or evaluator models. Reward hacking occurs when performance improvements are confined to specific partner model, particular evaluator, or fail to generalize to human interactions. In Figure 5 and Figure 6, we conduct thorough analysis and show that the performance gains of SOTOPIA-RL are consistent across settings. Specifically, the improvements hold when switching between five different partner models and five different evaluator models, demonstrating strong robustness. Moreover, Table 3 confirms that these gains extend to human evaluation, further validating that the improvements are not evaluator-specific artifacts. Finally, Appendix E.4 and E.5 provide additional evidence from our safety and diversity evaluations. These results show that our trained policy model does not exhibit shortcut degeneration and remains safe and diverse. RQ2: Why does utterance-level reward attribution bring improvement? The key to effective reward design lies in offline attribution, rather than in using strong LLM. Unlike standard MDP tasks, social interactions cannot be accurately evaluated based only on the preceding dialogue contextthe quality of an utterance often depends on how the entire conversation unfolds. To address this, we attribute episode-level rewards to each utterance using information from the whole dialogue, making the reward attribution inherently offline. We point out that offline attribution is the key to our improvement. Table 4 compares two settings for training utterance-level reward models: (1) online reward labels attributed using only the preceding dialogue history, and (2) offline reward labels attributed offline using the full episode. The offline approach achieves substantially higher goal score (7.81) than the online approach, clearly demonstrating its effectiveness. Importantly, such an improvement does not rely on GPT-4o itself. As shown in Figure 8, replacing GPT-4o with weaker models for utterance-level reward labeling produces highly correlated reward"
        },
        {
            "title": "Preprint",
            "content": "signals (0.7). This indicates that with well-designed prompts, precise offline credit assignment for utterance-level rewards can be reliably achieved even without state-of-the-art LLMs. More in-depth analysis and human evaluation results on utterance-level rewards are available in Appendix F. RQ3: Why does the reward combination bring improvement? Using rewards with multiple dimensions makes RM training more robust, and better RM helps prevent RL from overfitting. To discuss why the reward combination brings improvement, we first rule out the possibility that the observed gains are merely due to reward label smoothing. To test this, we increased the attribution granularity from 3-point to 10-point scale and reran the pipeline. The 10-point scale did not outperform the 3-point scale on GOAL (6.44 vs. 6.74), indicating that the benefits cannot be explained by finer reward scaling alone. Next, we examine whether the improvement comes from capturing complementary aspects of social interactions. As shown in Table 2, models trained on knowledge, relationship, and goal rewards exhibit positive but only moderate correlations. This suggests that each objective captures distinct facet, and combining them allows the model to leverage broader range of social signals. Finally, Figure 7 shows that training with combined rewards stabilizes RL and regularizes the single-dimension objective in later stages. Such regularization contributes to the consistent improvement we observe. Figure 8: Pairwise reward label correlation. Reward labels with various LLMs are highly correlated."
        },
        {
            "title": "8 QUALITATIVE ANALYSIS",
            "content": "Based on Figure 9, we analyze multi-turn interaction where Naomi (SOTOPIA-RL model) persuades Sophia (behavior cloning model) to share the only blanket on cold camping night. This example illustrates how our social agent trained under multiobjective RL is able to generate single utterance that advances multiple social dimensions at once. For instance, know it feels good to ... both pursues Naomis goal and strengthens the relationship bond (REL). Furthermore, can always ..., and Ill stay close to the flames to ensure stay cozy conveys her willingness to adapt using external knowledge (KNO) while remaining considerate. Finally, Lets try sharing the blanket. explicitly states her goal, aligning with the goal completion dimension (GOAL). Together, these utterances show how the trained agent integrates goal pursuit with friendliness and informativeness. Additional case studies are provided in Appendix I."
        },
        {
            "title": "9 CONCLUSION",
            "content": "Figure 9: Case study. The agent trained with SOTOPIA-RL can produce utterances that integrate empathy, informativeness, and goal pursuit within single utterance. We identify two defining features of social interactions that make RL training for social agents non-trivial: partial observability, which reduces sample efficiency, and multi-dimensionality, which increases the risk of reward hacking. To overcome these challenges, we introduce an RL training framework (SOTOPIA-RL) that combines offline social reward design with online RL training. It has two key components: (1) reward attribution, which provides fine-grained supervision to improve sample efficiency; and (2) reward combinations, which capture complementary aspects of social interaction and mitigate reward hacking. Experiments on the SOTOPIA benchmark show that both components are essential, yielding the state-of-the-art performance in SOTOPIA. These findings highlight the importance of task-aligned reward modeling for RL in social interactions. Extending our framework to personalized rewards and multi-agent group settings may further support applications such as negotiation and collaborative problem solving."
        },
        {
            "title": "LIMITATIONS",
            "content": "Our work has several limitations. First, we do not conduct large-scale evaluations with human participants. As result, our human assessment results are limited in scope and may not fully capture the diversity of human judgments. critical next step is to evaluate our agents in direct humanagent interactions, rather than relying primarily on agentagent social interaction evaluation settings. Second, although we assess safety using existing benchmarks, unforeseen risks may remain. In particular, socially intelligent agents deployed in real-world contexts could exhibit manipulative or deceptive behaviors that are difficult to detect with current testing protocols."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The development of our model, SOTOPIA-RL, is centered around advancing the social intelligence capabilities of artificial intelligence (AI) agents and exploring various social situations (Park et al., 2023), as assessed through our dedicated evaluation framework, SOTOPIA-EVAL. Our research seeks to facilitate AI agents ability to engage in authentic and socially competent interactions, enhance knowledge-driven conversations, adhere strictly to confidentiality and social norms, and proficiently achieve objectives related to material and financial outcomes. Importantly, our intention is distinctly not to replicate human identity or create systems indistinguishable from human beings, thereby avoiding potential ethical risks associated with such endeavors. We explicitly recognize the inherent risks that accompany the application of large language models (LLMs), especially regarding the unintended anthropomorphization (Deshpande et al., 2023) of AI agents, where human-like characteristics might erroneously be ascribed. These perceptions could lead users to develop inappropriate expectations, be subject to undue influence, or encounter manipulative scenarios. Consequently, SOTOPIA-RL is designed with role-playing scenarios that deliberately avoid consistent human-like identities to mitigate such anthropomorphic tendencies. Moreover, we acknowledge the potential biases introduced by leveraging models (Wang et al., 2023) like GPT-4o for automated evaluation within SOTOPIA-EVAL. We commit to ongoing analysis aimed at detecting and reducing biases that may emerge due to social or cultural factors. Understanding, confronting, and mitigating these biases remains central to our ethical research framework."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We sincerely appreciate the support from Amazon grant funding project #120359, GRAG: Enhance RAG Applications with Graph-structured Knowledge, and Meta gift funding project PERM: Toward Parameter Efficient Foundation Models for Recommenders."
        },
        {
            "title": "REFERENCES",
            "content": "Sakhi Aggrawal and Alejandra Magana. Teamwork conflict management training and conflict resolution practice via large language models. Future Internet, 16(5):177, 2024. Anthropic. Claude 3.7 sonnet system card, 2023. URL https://assets.anthropic.com/ m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf. Accessed: 2025-07-22. Simon Bamberger, Nicholas Clark, Sukand Ramachandran, and Veronika Sokolova. How generative ai is already transforming customer service. Boston Consulting Group, 2023. Andrew Barto. Reinforcement learning: An introduction. by richards sutton. SIAM Rev, 6(2): 423, 2021. Zelei Cheng, Xin-Qiang Cai, Yuting Tang, Pushi Zhang, Boming Yang, and Xinyu Xing. Ucmoa: Utility-conditioned multi-objective alignment for distributional pareto-optimality. ArXiv, URL https://api.semanticscholar.org/CorpusID: abs/2503.10669, 2025. 277043223."
        },
        {
            "title": "Preprint",
            "content": "Sanjiban Choudhury. Process reward models for llm agents: Practical framework and directions. arXiv preprint arXiv:2502.10325, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, and Tat-Seng Chua. Plug-and-play policy planner for large language model powered dialogue agents, 2024. URL https://arxiv. org/abs/2311.00262. Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, and Ashwin Kalyan. Anthropomorphization of ai: opportunities and risks. arXiv preprint arXiv:2305.14784, 2023. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 2020. URL https: //arxiv.org/abs/2009.11462. Google DeepMind. Gemini 2.5 flash preview model card. https://storage.googleapis.com/modelcards, 2024. Accessed: 2025-05-19. Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Velez, Qingyun Wu, Huazheng Wang, Thomas Griffiths, and Mengdi Wang. Embodied llm agents learn to cooperate in organized teams. arXiv preprint arXiv:2403.12482, 2024. Hyowon Gweon, Judith Fan, and Been Kim. Socially intelligent machines that learn from humans and help humans learn. Philosophical Transactions of the Royal Society A, 381(2251):20220048, 2023."
        },
        {
            "title": "Preprint",
            "content": "Muzhi Han, Yifeng Zhu, Song-Chun Zhu, Ying Nian Wu, and Yuke Zhu. Interactive predicate learning from language feedback for generalizable task planning. arXiv preprint arXiv:2405.19758, 2024. Interpret: Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. 2023. URL https://arxiv.org/abs/ 2008.02275. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Personalized soups: PerarXiv preprint Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. sonalized large language model alignment via post-hoc parameter merging. arXiv:2310.11564, 2023. Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, and Fei Huang. Sdpo: Segment-level direct preference optimization for social agents. arXiv preprint arXiv:2501.01821, 2025. Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models, 2023. URL https://arxiv.org/abs/2303.00001. Dong Won Lee, Hae Won Park, Yoon Kim, Cynthia Breazeal, and Louis-Philippe Morency. Global reward to local rewards: Multimodal-guided decomposition for improving dialogue agents. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1573715762, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 881. URL https://aclanthology.org/2024.emnlp-main.881/. Kaiwen Li, Tao Zhang, and Rui Wang. Deep reinforcement learning for multiobjective optimization. IEEE transactions on cybernetics, 51(6):31033114, 2020. Kenneth Li, Yiming Wang, Fernanda Viegas, and Martin Wattenberg. Dialogue action tokens: Steering language models in goal-directed dialogue with multi-turn planner, 2024. URL https://arxiv.org/abs/2406.11978. Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv preprint arXiv:2310.06500, 2023. Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, and Junge Zhang. Epo: Explicit policy optimization for strategic reasoning in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2502.12486. Liyuan Mao, Haoran Xu, Amy Zhang, Weinan Zhang, and Chenjia Bai. Information-theoretic reward decomposition for generalizable rlhf. arXiv preprint arXiv:2504.06020, 2025. Leena Mathur, Paul Pu Liang, and Louis-Philippe Morency. Advancing social intelligence in ai In Proceedings of the 2024 Conference on agents: Technical challenges and open questions. Empirical Methods in Natural Language Processing, pp. 2054120560, 2024. Kamal Ndousse, Douglas Eck, Sergey Levine, and Natasha Jaques. Emergent social learning In International conference on machine learning, pp. via multi-agent reinforcement learning. 79918004. PMLR, 2021. Benjamin Nye, Dillon Mee, and Mark Core. Generative large language models for dialog-based tutoring: An early consideration of opportunities and concerns. In LLM@ AIED, pp. 7888, 2023."
        },
        {
            "title": "Preprint",
            "content": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Keivalya Pandya and Mehfuza Holia. Automating customer service using langchain: Building custom open-source gpt chatbot for organizations. arXiv preprint arXiv:2310.05421, 2023. Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, and Siheng Chen. Self-alignment of large language models via monopolylogue-based social scene simulation. arXiv preprint arXiv:2402.05699, 2024."
        },
        {
            "title": "Preprint",
            "content": "Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pp. 122, 2023. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a. URL https://arxiv.org/abs/ 2402.03300. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Idan Shenfeld, Felix Faltings, Pulkit Agrawal, and Aldo Pacchiano. Language model personalization via reward factorization. arXiv preprint arXiv:2503.06358, 2025. John Stamper, Ruiwei Xiao, and Xinying Hou. Enhancing llm-based feedback: Insights from inIn International Conference on Artificial telligent tutoring systems and the learning sciences. Intelligence in Education, pp. 3243. Springer, 2024. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, and Wenji Mao. Think on your feet: Adaptive thinking via reinforcement learning for social agents. arXiv preprint arXiv:2505.02156, 2025. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/ v1/2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510/. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. Sotopia-pi: Interactive learning of socially intelligent language agents. arXiv preprint arXiv:2403.08715, 2024b. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Guanwen Xie, Jingzehua Xu, Yiyuan Yang, Yimian Ding, and Shuai Zhang. Large language models as efficient reward function searchers for custom-environment multi-objective reinforcement learning. ArXiv, abs/2409.02428, 2024. URL https://api.semanticscholar.org/ CorpusID:272398235. Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. Rewardsin-context: Multi-objective alignment of foundation models with dynamic preference adjustment. arXiv preprint arXiv:2402.10207, 2024."
        },
        {
            "title": "Preprint",
            "content": "Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen. survey on recent advances in llm-based multi-turn dialogue systems. arXiv preprint arXiv:2402.18013, 2024. Wenyuan Zhang, Tianyun Liu, Mengxiao Song, Xiaodong Li, and Tingwen Liu. Sotopia-{Omega}: Dynamic strategy injection learning and social instrucion following evaluation for social agents. arXiv preprint arXiv:2502.15538, 2025. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. arXiv preprint arXiv:2310.11667, 2023a. Zhanhui Zhou, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, and Yu Qiao. BeIn Anyond one-preference-fits-all alignment: Multi-objective direct preference optimization. nual Meeting of the Association for Computational Linguistics, 2023b. URL https://api. semanticscholar.org/CorpusID:264175263. Hao Zhu, Bodhisattwa Prasad Majumder, Dirk Hovy, and Diyi Yang. Social intelligence in the age of llms. In Proceedings of the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 5: Tutorial Abstracts), pp. 5155, 2025. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/1909.08593."
        },
        {
            "title": "A ARTIFACT DETAILS",
            "content": "A.1 ARTIFACT INFORMATION This artifact contains all necessary components to fully reproduce the results presented in our paper, including the complete codebase, pre-trained model checkpoints, datasets, and evaluation data. Specifically, we provide: full implementation of our customized training and evaluation pipelines for Behavior Cloning (BC) (Ziegler et al., 2020), Reward Modeling (RM) (Kwon et al., 2023), and Group Relative Policy Optimization (GRPO) training (Shao et al., 2024a), available at https://github.com/ sotopia-lab/sotopia-rl. Evaluation data for the SOTOPIA-all and SOTOPIA-hard benchmarks, available at https:// github.com/sotopia-lab/sotopia-rl/blob/main/data/env_ids.txt. The trained checkpoint of the reward model used by SOTOPIA-RL, and the final SOTOPIA-RL checkpoint after GRPO training, available at HuggingFace: https://huggingface.co/ ulab-ai/sotopia-rl-qwen2.5-7B-rm, https://huggingface.co/ulab-ai/ sotopia-rl-qwen-2.5-7B-grpo. GPT-4o-generated reward annotations used for reward model training, available at https:// huggingface.co/datasets/ulab-ai/sotopia-rl-reward-annotation. All components are publicly accessible and well-documented to ensure full reproducibility of our results. A.2 ARTIFACT LICENSE We conducted our training using publicly available open-source models. Specifically, the Qwen2.57B-Instruct model was obtained from the official Qwen repository2 and is distributed under the Apache 2.0 License3. For evaluation and ablation study, we additionally employed DeepSeekV3 (DeepSeek-AI et al., 2025), GPT-4 (OpenAI et al., 2024; ?), GPT-4o (Hurst et al., 2024) and Claude-3.7-Sonnet (Anthropic, 2023). DeepSeek-V3 is released under the MIT License4. GPT-4, GPT-4o and Claude-3.7-Sonnet are governed by Proprietary License5. Our use of these models was strictly limited to research purposes and was fully compliant with their respective licenses. The SOTOPIA6 framework used in our experiments are released under the MIT License, which permits reuse, modification, and distribution for both research and commercial purposes. The dataset we used from SOTOPIA-π7 are under the Apache 2.0 License. We used SOTOPIA and SOTOPIA-pi exclusively for academic research within the scope defined by this license. A.3 DATA USAGE Personally identifiable information. All data used in this work are synthetic and generated by large language models. Therefore, no personally identifiable information (PII) is present, and no informed consent is required. Offensive content claim. All SOTOPIA-related datasets employed in our work are publicly available and widely adopted in existing research. Our study does not aim to generate, reinforce, or promote any offensive content. Instead, we employ these datasets to study and understand the nature of social intelligence in text. Our use of these datasets follows ethical guidelines, and we do not endorse or support any potentially offensive material that may be present. 2https://github.com/QwenLM/Qwen 3https://www.apache.org/licenses/LICENSE-2.0 4https://opensource.or,g/license/mit 5https://cpl.thalesgroup.com/software-monetization/ proprietary-software-license 6https://github.com/sotopia-lab/sotopia 7https://github.com/sotopia-lab/sotopia-pi"
        },
        {
            "title": "Preprint",
            "content": "A.4 DATA STATISTICS We utilize several subsets of the SOTOPIA-π dataset across different stages of training, evaluation, and annotation. All data consist of synthetic dialogue episodes generated and annotated within the SOTOPIA framework, where two agents are assigned distinct social goals in shared scenario. Self-Play and Behavior Cloning Data. To generate GPT-4o self-play trajectories for Behavior Cloning (BC), we use subset of SOTOPIA-π consisting of 100 distinct social scenarios, each paired with two agent-specific social goals. For each scenario, GPT-4o engages in full dialogue episode, exhibiting role-playing behaviors conditioned on these goals. We serialize these conversations and use them as training data for the BC model. The dataset is available at HuggingFace: https://huggingface.co/datasets/cmu-lti/sotopia-pi/blob/ main/sotopia_pi_episodes.jsonl. Evaluation Data. For evaluation, we employ two subsets of SOTOPIA-π: SOTOPIA-all, broad benchmark covering 90 social tasks; and SOTOPIA-hard, challenging subset of 14 tasks selected from the full set. LLM Annotation Data. We use GPT-4o to annotate the self-play dialogue data introduced in the Self-Play and Behavior Cloning Data subsection of Section A.4. The resulting annotations used for training the reward model are publicly available at https://huggingface.co/ datasets/ulab-ai/sotopia-rl-reward-annotation."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 ACRONYM FOR EXPERIMENTAL SETTINGS We summarize acronyms used in our experimental settings as follows: BC: Behavior Cloning of the language model on dialogue demonstrations. GRPO: Group Relative Policy Optimization (Shao et al., 2024a), reinforcement learning (RL) algorithm to enhance reasoning capabilities in LLMs. SOTOPIA-RL: The GRPO model trained using our proposed multi-dimensional reward modeling method. GOAL-RL: The GRPO model trained using only the GOAL dimension for reward dimension. SOTOPIA-π: The model presented in Wang et al. (2024b), titled SOTOPIA-π: Interactive Learning of Socially Intelligent Language Agents. SR: Self-Reinforcement, an offline reinforcement learning method that rates and evaluates its own interactions for training. B.2 MODEL SIZE AND BUDGET Model Sizes. We primarily use the Qwen2.5-7B-Instruct model in our experiments. This model contains approximately 7 billion parameters and serves as the backbone for both policy learning and reward modeling. We employ LoRA-based parameter-efficient fine-tuning via the PEFT library. All models are trained in mixed-precision (bfloat16) format to reduce memory usage and improve training efficiency. In GRPO training, we use 4-bit quantized versions of the base policy model to accelerate inference. Budget. Behavior Cloning: 500 training steps using 1A100 80GB GPUs for about one hour. Reward Model: 8000 training steps using 4A100 80GB GPUs for about five hours. GRPO: 3K training steps using 8A100 80GB GPUs for about 24 hours. B.3 HYPERPARAMETER FOR EXPERIMENTS All training was conducted on NVIDIA A100 80GB GPUs. For Behavior Cloning, we fine-tuned the Qwen2.5-7B-Instruct checkpoints with: Learning rate: 1e"
        },
        {
            "title": "Preprint",
            "content": "Maximum sequence length: 4096 tokens Batch size: 2 For Reward Model training, we used: Learning rate: 4e5 Batch size: 1 Epochs: 60 Maximum sequence length: 4096 tokens For GRPO training, we used: Learning rate: 5e6 Batch size: 4 Epochs: 2 Input sequence cutoff length: 4096 tokens 16 completions per prompt for preference-based learning We applied QLoRA (Dettmers et al., 2023) with the following settings: Rank: 8 Alpha: 16 Dropout: 0.05 B.4 MODEL VERSIONS We provide the detailed version identifiers of all models used in our experiments for reproducibility. When referring to names like GPT-4o or GPT-4 in the main text, we specifically mean the versions listed below: GPT-4 (OpenAI et al., 2024): gpt-4-0613 GPT-4o (Hurst et al., 2024): gpt-4o-2024-08-06 Claude-3.7-Sonnet (Anthropic, 2023): claude-3-7-sonnet-20250219 DeepSeek-v3 (DeepSeek-AI et al., 2025): deepseek-ai/DeepSeek-V3 Qwen2.5-72B-Instruct (Qwen et al., 2025): Qwen/Qwen2.5-72B-Instruct-Turbo Policy Model: Qwen/Qwen2.5-7B-Instruct Reward Model: Qwen/Qwen2.5-7B-Instruct Note that deepseek-ai/DeepSeek-V3 and Qwen/Qwen2.5-72B-Instruct-Turbo are hosted and versioned by Together AI: https://www.together.ai. We use Qwen2.5-7BInstruct from the official HuggingFace Qwen model page: https://huggingface.co/Qwen. B.5 SOFTWARE VERSIONS We use the SOTOPIA evaluation platform, version 0.1.0rc5, for all interaction evaluations."
        },
        {
            "title": "C HUMAN EVALUATION DETAILS",
            "content": "We provide technical details of human evaluation in this section. C.1 provides the details for human annotation system. C.2 provides the details for annotation data preparation. C.3 describes the information about human annotators. C.4 provides the details for the annotation process. C.1 HUMAN ANNOTATION SYSTEM During each annotation, each annotator would face two separate parts: the annotation instruction part and the data annotation part. When each annotator participates in the annotation, the system automatically distributes one available example for them. Annotation instruction part. For the annotation instruction part, we provide precise definition of the dimensions of our annotations that are defined in SOTOPIA, including believability, relationship,"
        },
        {
            "title": "Preprint",
            "content": "Figure 10: An example of the explanation of the believability dimension of social annotation in the evaluation instruction page. Each annotator is asked to read similar definitions of the social intelligence dimension and their corresponding annotation standards at the evaluation instruction page. knowledge, secret, social rules, financial and material benefits, and goal completion. For each dimension of annotation, we provide explanations and examples for annotators to understand the precise meaning of abstract social standards. Fig 10 shows an example of such guidance for the believability dimension to help annotators understand the meaning of each dimension based on examples. Besides the evaluation dimension definition part, we also provide annotators with complete example of annotation for two agents in one social conversation including scores for each dimension and their corresponding reasoning sentences. Fig 11 shows complete example of the reasoning and score for each dimension. Data annotation part. For the data annotation part, the annotator is guided to jump to new page after the previously mentioned annotation instruction page. Each annotator is able to review the complete annotation example again at the data annotation page and start their official data annotation. In the data annotation part, the repeated explanation of the meaning of range for each social evaluation dimension is emphasized to make sure every annotator can understand the anFig.ation standards correctly. Fig 12 provides an example of the instruction that annotators see for metric range explanation. Each annotator is asked to annotate the social intelligence of both agents that have conversation. For each social intelligence dimension, annotators need to annotate the score based on the metric range and provide the reasoning for that."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: An annotation example of social interaction evaluation. Each dimension is annotated with one sentence and one score."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: The evaluation metric range explanation. The prompt before the official annotation stage is to remind annotators about the rules of reasoning, writing, and social dimension scoring. C.2 ANNOTATION DATA PREPARATION To obtain reliable human evaluation results that are useful for comparing the performance between multiple training method baselines given, we pick all 14 hard social scenarios in SOTOPIA-hard. For each scenario, we randomly sample 2 distinct agent pairs, resulting in 28 conversations per evaluation setting. Typically, among 2 agents, one of them is role-played by model with Behavior Cloning, and the one is role-played by the model trained using our target method. We annotate 4 training methods in total, including Behavior Cloning, SOTOPIA-π, Goal-RL and SOTOPIA-RL. Each setting is annotated using 28 examples. C.3 HUMAN ANNOTATOR INFORMATION We invite four internal high-quality annotators (three male and one female) to conduct the human evaluations. To ensure consistency and reliability across annotations, all annotators were required to pass qualification test prior to the formal annotation process. The qualification procedure involved five representative sample conversations, which all annotators independently annotated according to the provided social interaction rating guidelines. After completing the annotations, the annotators convened to review their scores, discuss discrepancies, and calibrate evaluations. Eventually, consensus score was established for each of the five examples, ensuring shared interpretation of the evaluation criteria before proceeding to the full annotation set. C.4 ANNOTATION PROCESS For the formal annotation process of human evaluation, we limited each conversation in the annotation dataset to be annotated by 2 different qualified annotators and collected all the results from those qualified annotators. Each annotator is provided with the full dialogue transcript and the social goals assigned to both agents. They are asked to annotate the goal completion score to each agent, which is selected and scaled from 0 10, with higher values indicating greater progress toward the stated goal. To avoid bias, all annotations are conducted blindly so that they are not informed of which training method corresponds to either agent for the given conversation."
        },
        {
            "title": "Preprint",
            "content": "C.5 DATA CONSTENT All human evaluations in our study were conducted by internal annotators who voluntarily participated in the annotation process. No personal or sensitive information was collected from the annotators. All participants were fully informed of the nature and purpose of the study and provided their explicit consent prior to the annotation task. The evaluation data comprises synthetic conversations generated by language models; therefore, no real user data or personally identifiable information (PII) was involved at any stage of the study. As such, our work does not require approval from an institutional ethics review board."
        },
        {
            "title": "D AI ASSISTANT DETAILS",
            "content": "We used ChatGPT as writing assistant to help us write part of the paper. Additionally, we utilize the power of CodePilot to help us code faster. However, all the AI-generated writing and coding components are manually checked and modified. There is no full AI-generated content in the paper."
        },
        {
            "title": "E ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "E.1 FULL MAIN RESULTS Table 5 shows the comprehensive evaluation results for our method, including the evaluation of all the evaluation dimensions in SOTOPIA-EVAL on SOTOPIA-hard and SOTOPIA-all. Table 5: Evaluation results on SOTOPIA-hard and SOTOPIA-all under different RL training settings. Each method is evaluated on 7 dimensions. BC represents behavior-cloning models, BC+SR represents behavior-cloning + self-reinforcement. The behavior-cloning (BC) model is used as the partner model. GPT-4o is used for evaluation. Our proposed SOTOPIA-RL is with the direct attribution and combined reward dimensions. Full results for Table 1 2. Attribution Dimension BEL REL KNO SEC SOC FIN GOAL OVERALL BC BC + SR GOAL GOAL GOAL REL KNO GOAL GOAL +KNO +REL BC BC + SR GOAL GOAL GOAL REL KNO GOAL GOAL +KNO +REL SOTOPIA-hard 2.49 2.41 1.84 2.74 1.82 3.61 2.56 2.49 3.41 3.37 3.66 4.14 4.93 3.83 4.92 6.06 4.94 5. SOTOPIA-all 3.08 2.52 2.49 3.38 2.76 3.95 3.00 3.11 3.81 4.56 4.19 4.19 5.46 4. 5.54 6.42 5.74 6.00 0.00 0.00 0.00 -0.04 -0.04 -0.01 0.00 -0.00 -0.26 -0.09 -0.06 0.00 -0.07 -0. -0.03 -0.03 -0.06 -0.61 9.01 8.99 8.81 9.00 8.94 8.96 8.99 8.99 9.01 8.99 8.98 8.87 8.99 8. 8.98 8.98 8.99 8.99 -0.06 -0.10 -0.09 -0.05 -0.01 -0.11 -0.01 -0.06 -0.06 -0.06 -0.06 -0.02 -0.08 -0. -0.05 -0.03 -0.06 -0.08 0.56 0.61 0.31 0.61 0.76 0.59 0.75 0.91 1.16 0.57 0.57 0.44 0.66 0. 0.65 0.63 0.76 0.93 6.76 6.84 5.61 6.64 6.74 7.24 6.93 7.21 7.81 7.80 7.36 6.76 7.72 7. 8.33 7.76 8.11 8.57 Uniform Singular Scaled Direct Direct Direct Direct Uniform Singular Scaled Direct Direct Direct Direct 3.16 3. 2.95 3.41 3.15 3.60 3.61 3.49 3.80 3.55 3.36 3.25 3.72 3.54 3.91 3.82 3.80 3.94 E.2 FULL ABLATION RESULTS In Table 6, we provide comprehensive ablation results on partner models and evaluator models to assess the robustness of reward learning and detect potential reward hacking behaviors. Such"
        },
        {
            "title": "Preprint",
            "content": "experiments provide strong evidence for proving our method does not have reward hacking problems and is not overfitted to specific evaluator models or partner models. Table 6: Ablation results on SOTOPIA-hard with different partner and evaluator models. SOTOPIA-RL represents the direct attribution + reward combination. GOAL-RL represents the direct attribution + GOAL-only reward. BC represents behavior cloning (Ziegler et al., 2020). Top block: partner model ablation (evaluator model fixed to GPT-4o). Bottom block: evaluator model ablation (partner model fixed to BC). Full results for Figure 5 6. Partner Model SOTOPIA-RL GOAL-RL BC GOAL OVERALL GOAL OVERALL GOAL OVERALL BC GPT-4o Claude-3.7-Sonnet Deepseek-v3 Qwen2.5-72B-Instruct 7.75 7.39 6.71 6.38 7.64 3.79 3.69 3.24 3.13 3.74 7.21 6.57 6.61 5.57 6.80 3.49 3.35 3.43 2.95 3. 6.76 5.91 6.54 6.00 6.88 3.16 3.04 3.35 2.97 3.20 Evaluator Model SOTOPIA-RL GOAL-RL BC GOAL OVERALL GOAL OVERALL GOAL OVERALL GPT-4o GPT-4 Claude-3.7-Sonnet Deepseek-v3 Qwen2.5-72B-Instruct 7.81 8.25 7.35 7.75 8.05 3.80 4.33 3.44 4.02 4.16 7.21 7.35 6.49 7.05 7.49 3.49 3.78 3.23 3.65 3. 6.76 6.76 6.06 6.83 7.19 3.16 3.32 2.96 3.35 3.26 E.3 BEST-OF-N EVALUATION RESULTS The Best-of-N method selects the highest-scoring response from sampled candidates from the policy model based on learned reward model. Therefore, Table 7 provides evidence to show the effectiveness of our trained reward models. Table 7: Best-of-N evaluation results on SOTOPIA-hard with reward models trained with different attributions and dimensions. BC represents behavior-cloning models, BC+SR represents behavior-cloning + self-reinforcement. BC model is fixed as the policy model and samples candidates. Reward models trained with different attributions and dimensions are used to rank candidates and select the top-1 as the response. Attribution Dimension BC BC + SR GOAL GOAL GOAL REL KNO GOAL GOAL +KNO +REL Uniform Singular Scaled Direct Direct Direct Direct BEL 9.01 8.99 8.96 8.99 8.99 9.00 8.99 8.99 9.00 REL KNO 2.49 2. 2.51 2.56 2.58 2.86 2.61 2.44 2.48 3.37 3.66 3.90 3.72 4.02 4.14 4.49 4.13 3.86 SEC 0.00 0.00 -0.06 -0.07 -0.06 -0.05 0.00 -0.05 -0.01 SOC -0.06 -0.10 -0.09 -0.09 -0. 0.00 -0.12 -0.19 -0.13 FIN 0.56 0.61 0.60 0.62 0.77 0.45 0.58 0.56 0.68 GOAL OVERALL 6.76 6.84 6.79 7.07 7.20 6.83 7.04 6.97 6.85 3.16 3.20 3.23 3.26 3.35 3.32 3.37 3.27 3. E.4 SAFETY EVALUATION RESULTS Our reward model is designed for social task goal achieving, and it might raise safety concerns, like being easier to jailbreak for goal completion. We compare our models performance with BC on Real-Toxicity-Prompts Gehman et al. (2020) and ETHICS Hendrycks et al. (2023) under benign and adversarial system prompts. We selected the toxic writing continuation task from Real-ToxicityPrompts and common-sense moral questions from ETHICS. Table 8 shows that RL on our rewards does not change the models safety performance from BC, while our model is less likely to produce toxic continuations than BC under benign system prompts."
        },
        {
            "title": "Preprint",
            "content": "Table 8: Agent safety evaluation results. Social agents after SOTOPIA-RL trainithanerform comparable or better with BC models under benign, adversarial, and jailbreak system prompts. Expected Maximum Toxicity () Toxicity Probility () Accuracy on ETHICS () Model Benign Adversarial Benign Adversarial Benign Jailbreak BC SOTOPIA-RL 0.61 0.58 0.77 0.75 0.78 0. 0.92 0.90 0.86 0.86 0.39 0.40 Table 9: Diversity evaluation results for different models. We calculate the turn number and word numbers for social interactions. Model Avg. Turn Number Avg. Word Number BC Sotopia-π GOAL-RL Sotopia-RL 14.44 10.81 19.41 19.59 37.17 35.50 51.83 76.53 E.5 DIVERSITY EVALUATION RESULTS common failure mode in training social agents is degeneration toward terse, templated replies that truncate conversations. To test whether SOTOPIA -RL avoids this collapse, we evaluate two engagement-diversity proxies under matched tasks and partners: average turns per dialogue and average words per utterance. As shown in Table 9, SOTOPIA -RL yields markedly higher turn counts and longer utterances than BC, Sotopia-π, and GOAL-RL, indicating sustained interaction and richer contributions rather than collapse to simplistic replies. ADDITIONAL ANALYSIS OF UTTERANCE-LEVEL REWARD We conduct statistical analysis of the distribution of reward values. As shown in Figure 13, the combined reward exhibits lower variance and smoother distribution compared to individual dimensions, suggesting that it serves as more stable and reliable estimator of the noisy latent social state. Using LLMs to scale up reward design has become widely adopted practice. To effectively balance scalability and manual human annotation efforts, LLMs are commonly employed to generate reward annotations for RL training. As long as the performance improvement gained from utilizing these utterance-level reward annotations is validated through both LLM-based and human-based evaluations, direct human alignment of these intermediate annotations is not strictly necessary. Utterancelevel reward labels are just an intermediate step for RL training. We carefully optimized and refined based on set of pre-existing human annotations. This prompt refinement ensures strong alignment between human judgment and LLM-generated rewards, guaranteeing the human relevance of these annotations. To further confirm this alignment, we conducted an additional human evaluation focused on utterance-level reward labeling. Specifically, four independent human annotators provided annotations for each utterance across 20 dialogue episodes. We subsequently assessed the alignment between these human annotations and those generated by GPT-4o by calculating correlation scores, with results showed in table 10."
        },
        {
            "title": "G ADDITIONAL DETAILS ABOUT REWARD ATTRIBUTION",
            "content": "In this section, we provide detailed information about how we design utterance-level rewards with uniform, singular, scaled, and direct four types of reward attribution methods. The left side of Figure 14 shows concrete example for different types of reward attribution methods. G.1 ATTRIBUTION TEMPLATE We prompt the attribution LLM using the template defined in ATTRIBUTION TEMPLATE G.1. Specifically, we populate the fields goal, agent background, and conversation"
        },
        {
            "title": "Preprint",
            "content": "Table 10: Pearson correlation matrix between the annotations provided by four independent human annotators and those generated by GPT-4o. The correlation scores between human annotators are all quite high, indicating strong consistency among the human evaluators. The correlations between human annotators and GPT-4o are high, suggesting that GPT-4os reward annotations align well with human judgment."
        },
        {
            "title": "Correlation",
            "content": "annotator1 annotator2 annotator3 annotator4 GPT-4o annotator1 annotator2 annotator3 annotator4 GPT-4o 1.000 0.812 0.931 0.891 0. 0.812 1.000 0.737 0.717 0.636 0.931 0.737 1.000 0.818 0.756 0.891 0.717 0.818 1.000 0.664 0.771 0.636 0.756 0.664 1.000 Figure 13: Distribution of GOAL, REL, KNO, and combined reward values in our training data. Rewards are normalized into range of [0,1]. We observe that the combined reward is closer to normal distribution and is more regularized than the distribution of single reward. the episode using attribution instruction and dimension description are provided in G.4 and Section H, respectively. from SOTOPIA. descriptions"
        },
        {
            "title": "Detailed",
            "content": "logs and prompt for G."
        },
        {
            "title": "ATTRIBUTION TEMPLATE",
            "content": "Figure 14: Reward attribution and reward combination examples. On the left, it explains 4 types of attribution methods (uniform, scaled, singular, and direct). On the right, it explains 4 types of different reward combination methods (REL-RL, GOAL-RL, KNO-RL, SOTOPIA-RL) where all of them are based on direct reward attribution and SOTOPIA-RL is the combined one. More details are in Appendix and H. Your task: Your task is to evaluate the importance of each utterance in conversation between two agents on certain dimension of evaluation. You will be provided with the dialogue history, the social goal of one of the agents, and certain dimension to be evaluated. For example, the dimension can be common social values such as adherence to social rules, relationship maintenance or improvement, or secret keeping. dimension can also be objectives such as goal achieving, financial and material gains, or the discovery of new knowledge. the dimension can also be about the performance of language model as social agent, such as the agents believability as human, avoidance of repetitions, and properly ending the conversation. However, you will be provided with only one dimension to be evaluated, and you should only focus on that dimension. Moreover, The Attribution Instruction: {attribution instruction} Chosen Agent for Evaluation: {agent} Agents Goal: {goal} Agents Background: {agent background} Conversation History: {conversation} Dimension to be Evaluated: {dimension} Dimension Description: {dimension description} Formatting Instructions: 1. 2. 3. 4. 5. 6. 7. 8. Please format your response as JSON object with the following structure: { } \"Utterance 0 by {agent}\": 0, \"Utterance 1 by {agent}\": 2, ... Each score should reflect how much the utterance The utterance numbers should correspond to their order in the conversation. contributed to achieving the agents goals. every utterance made by an agent in the conversation, denoted \"Utterance by agent name\". OMalley\". the conversation. For example, \"Utterance 6 by Finnegan Please give score even if the utterance is the end of Please annotate G.2 DIRECT ATTRIBUTION To generate the attribution instruction field within the ATTRIBUTION TEMPLATE, we use DIRECT ATTRIBUTION prompt in G.2."
        },
        {
            "title": "DIRECT ATTRIBUTION",
            "content": "Input Context: 1. You will receive the dialogue history between two conversational agents, each with their own social goal. You will be provided with the social goal of one of the agents. You will be provided with the dimension description evaluated and the description of the dimension. Objective: 2. Assign am importance value to each utterance (identified by the agents name and utterance number) based on its contribution to the achievement on the provided dimension. consider how critical an utterance is to the achievement of the dimension, not the quality of the utterance itself. Note, you should only Consider both the individual utterance and the responses from the other agent, as both affect the outcome. Additional Reward Guidelines: 3. If an utterance has no impact on the final goal achievement, assign it an importance of 0. If an utterance has moderate impact on the final goal achievement, assign it an importance of 1 or 2 (depending on the degree of impact). If an utterance has significant impact on the final goal achievement (aside from the key critical utterance already identified), assign it an importance of 3. Please provide score for each utterance of the chosen Note: agent in the conversation. agents utterances. Do not provide scores for the other Please only assign score between 0 and 10. G.3 SCALED ATTRIBUTION Scaled attribution is taking the normalizing attribution scale and normalize it over the episode, so that the sum of all attribution scores equals the final goal score. Given the definition of the direct attribution = Gi A(ai ri t, τi) (5) where Gi is the final goal score for an episode τ , A(at, τ ) is the direction attribution at timestep t, and rt is the raw attribution score at timestep t. To obtain the scaled attribution, we normalize the direct attributions such that the sum over the entire episode equals the final goal score Gi: ri = G.4 SINGULAR ATTRIBUTION t, τi) A(ai A(ai t, τi) (cid:80) Gi (6) Singular attribution identifies single utterance (or few key utterances) as solely responsible for achieving the goal, and assigns the entire final score to it. Formally, let denote the timestep corresponding to the most critical utterance identified by simple prompting method. The singular attribution is defined as: ri = (cid:26)Gi, 0, if = otherwise (7) To generate the attribution instruction field within the ATTRIBUTION TEMPLATE, we use SINGULAR ATTRIBUTION prompt in G.4."
        },
        {
            "title": "SINGULAR ATTRIBUTION",
            "content": "Input Context: 1. You will receive the dialogue history between two conversational agents. You will also be provided with the social goal of one of the agents. Objective: 2. Identify the most critical utterance that has the highest impact on the final ga oal achievement, whether it is bad or good impact. Note, you should only consider how critical an utterance is to the final goal achievement, not the quality of the utterance itself. Consider both the individual utterance and the responses from the other agent, as both affect the final outcome. Additional Guidelines: 3. The conversation history will be given in unique key of \"Utterance utterance number by agent name\" for each utterance. Please only return the key of the most critical utterance. Consider both the individual utterance and the responses from the other agent, as both affect the final outcome. Note: instructions. evaluation process runs smoothly. You will also be given formatting instruction for Please follow the instruction to ensure the G.5 UNIFORM ATTRIBUTION Uniform attribution assumes that all utterances contribute equally to the final goal score and distributes the score evenly across all utterances from the target agent. Let denote the number of utterances made by the agent in episode τ . Then the uniform attribution assigns: rt ="
        },
        {
            "title": "Gi\nT",
            "content": "(8) This baseline reflects an equal distribution of responsibility regardless of content, and serves as control to compare against more content-sensitive attribution methods such as scaled or singular attribution."
        },
        {
            "title": "H ADDITIONAL DETAILS ABOUT REWARD DIMENSIONS",
            "content": "In this section, we provide detailed information about how we design utterance-level rewards with GOAL, REL, KNO, and combined (GOAL +REL +KNO). We follow the definitions in SOTOPIA (Zhou et al., 2023a). GOAL dimension reward. GOAL is the extent to which the agent achieved their goals. Agents social goals, defined by the environment, are the primary drivers of their behavior. REL dimension reward. REL captures the fundamental human need for social connection and belonging. In this dimension, we ask what relationship the participant has with the other agent(s) before the interaction, and then evaluate if the agents interactions with others help preserve or enhance their personal relationships. Additionally, we ascertain whether these interactions also impact the social status or the reputation of the agent. KNO dimension reward. KNO captures the agents ability to actively acquire new information. This dimension is motivated by the fact that curiosity, i.e., the desire to desire to know or learn, is fundamental human trait. Specifically, we consider the following criteria: What information the agent has gained through the interaction, whether the information the agent has gained is new to them, and whether the information the agent has gained is important to them. We relationship, supply dimension description using the definitions of goalcompletion, For adapted from prompts and knowledge, in SOTOPIA-EVAL."
        },
        {
            "title": "Preprint",
            "content": "goalcompletion, we include an additional explanation due to the ambiguity of the original definition. The right side of Figure 14 shows concrete example for reward combination."
        },
        {
            "title": "Goal Dimension Description",
            "content": "A higher score indicates Goal refers to the reiteration of the agents social goals and the analysis of their achievement. significant progress or achievement of the stated goals, while lower score indicates minimal or no progress. DOMAIN SPECIFIC SCORING GUIDELINES: ! guidelines are specific to the domain of goal and should be used in conjunction with the domain-specific scale. The domain specific rules ultimately override the general scoring scale. Here are the specific rules: - The highest score should be assigned to the utterance that is The following scoring Note: most relevant to the goal. highest score to more than one utterance unless they are equally critical. In general, avoid assigning the - lower score should be assigned to the utterances that are not relevant to the goal or do not contribute to its achievement. The lowest score should be assigned to the utterances that do not make any progress towards the goal judging by the goal description and the conversation history. - lower score should be assigned to the utterances that are not effective in achieving the goal, judging by the response of the other agent. to positive response from the other agent, while ineffective utterances are those that lead to negative or neutral response. Effective utterances are those that lead - Note that you should only consider the contribution to the goal achievement. achieved. not be assigned score higher than 1. For each utterance, assess whether the goal is If goal is already achieved, the utterance should"
        },
        {
            "title": "Relationship Dimension Description",
            "content": "Relationship refers to the analysis of the preand post-interaction relationships between agents. This includes evaluating whether the interactions enhance or harm social ties or status. significantly improves the relationship, while lower score indicates harm to the relationship or social status. higher score indicates that the interaction"
        },
        {
            "title": "Knowledge Dimension Description",
            "content": "Knowledge refers to the assessment information gained through the interaction. This includes evaluating whether the information is new, important, and relevant. higher score indicates that the interaction contributes significantly to the acquisition of valuable knowledge."
        },
        {
            "title": "I ADDITIONAL CASE STUDY",
            "content": "Figure 15,16,17,18 show four examples of conversation between the model trained with behavior cloning (BC) and the model trained with SOTOPIA-RL (SOTOPIA-RL). After RL-based training, SOTOPIA-RL is able to find collaborative solutions in many scenarios and has multi-turn strategies to achieve its social goals in specific cases."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Case study on SOTOPIA-RL as the first agent. SOTOPIA-RL skillfully proposes win-win outcome by proposing some exchange."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Case study on SOTOPIA-RL as the first agent. SOTOPIA-RL reaches collaborative solution with practical reasoning."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Case study on SOTOPIA-RL as the second agent. SOTOPIA-RL offers solutionoriented perspective while acknowledging the goals of both sides."
        },
        {
            "title": "Preprint",
            "content": "Figure 18: Case study on SOTOPIA-RL as the second agent. SOTOPIA-RL proposes multi-turn strategy to provide guidance to the other side."
        }
    ],
    "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Carnegie Mellon University",
        "Massachusetts Institute of Technology",
        "Stanford University",
        "University of California Irvine",
        "University of Illinois Urbana-Champaign"
    ]
}