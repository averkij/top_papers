{
    "paper_title": "PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models",
    "authors": [
        "Seongjae Kang",
        "Dong Bok Lee",
        "Hyungjoon Jang",
        "Dongseop Kim",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 0 1 9 0 0 . 6 0 5 2 : r PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models Seongjae Kang, Dong Bok Lee, Hyungjoon Jang Dongseop Kim VUNO Inc. Sung Ju Hwang, KAIST DeepAuto.ai {seongjae.kang,hyungjoon.jang,dongseop.kim}@vuno.co {markhi, sjhwang}@kaist.ac.kr Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "Knowledge distillation (KD) is widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, framework that integrates AL with KD by leveraging the zeroand few-shot capabilities of large vision-language models (VLMs). key aspect of ActiveKD is the structured prediction bias of VLMsi.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in deep neural networks have focused on developing powerful generalist models capable of solving diverse tasks [2, 64, 55, 56] or creating robust transferable models through intensive pretraining [46, 67, 29, 17, 68, 38]. However, real-world applications often necessitate training compact task-specific models, with significant challenge being task-specific data labeling due to high annotation costs in specialized domains. Active Learning (AL) addresses this by iteratively selecting the most informative samples for oracle annotation [71, 52], particularly in pool-based scenarios where informative samples can be identified from large unlabeled data. Semi-Supervised Learning (SSL) naturally complements AL by leveraging extensive unlabeled data during training [5, 10, 101, 41], enabling selection strategies tailored to the SSL framework [22, 53, 70, 79]. Knowledge distillation (KD) is widely used framework for training compact models by transferring knowledge from large, pretrained teacher models [34]. Beyond its original purpose, KD naturally supports SSL by enabling the use of unlabeled data through teacher-generated soft labels [13, 32, 18, 94]. Despite its effectiveness, the application of KD in active learning (AL) remains underexplored. One major reason is the mismatch in assumptions: AL assumes that labels must be manually acquired for task-specific data through selective annotation, while KD typically assumes the existence of Preprint. Under review. Figure 1: (Left): The proposed ActiveKD framework consistently improves the final-round accuracy of baseline selection methods, while PCoreSet outperforms other baselines when combined with ActiveKD. (Right): ActiveKD consistently improves the performance of PCoreSet across active learning rounds (No Distill vs. Zero-shot Distill), with further gains when using few-shot teachers (Few-shot Distill). powerful task-specific teacher models trained on sufficient labeled data. However, in data-scarce settings common in AL, such teacher models are rarely available or practical to obtain. Recently, Vision-Language Models (VLMs) have emerged as powerful teachers in KD frameworks, as their rich representationsacquired through large-scale, unsupervised pretrainingenhance the transferability of knowledge across both modalities and tasks [84, 88, 59]. Recent work [41] has also shown distilling the knowledge of VLMs into compact, task-specific models effective in label-scarce scenarios, such as few-shot or low-shot semisupervised settings. Motivated by these, we introduce ActiveKD, framework that integrates AL with KD by leveraging the zeroand few-shot prediction capabilities of VLMs. Specifically, in each AL round, we train taskspecific student model using both the labeled data acquired up to that round and unlabeled data with soft predictions generated by the VLM teacher, as illustrated in Fig. 2. Figure 2: An overview of ActiveKD. In ActiveKD settings, we identify an aspect of zeroor few-shot VLM teachers: they exhibit structured prediction biases acquired during pretraining and shaped by language prompts [6], resulting in predictions that form distinct clusters in the probability space. We observe that this bias propagates through distillation, influencing both the active selection mechanism and student model performance. Rather than viewing this as limitation, we interpret this structure as an inductive bias of the teacher modelcapturing generalizable patterns in the output space that, when complemented by labeled data, can benefit student learning. To exploit this, we propose simple and effective method, Probabilistic CoreSet (PCoreSet), which selects coresets in the probability space rather than the feature space [76]. Specifically, it maximizes diversity in the categorical probability space by targeting underrepresented regions. In doing so, PCoreSet identifies samples that deviate from densely populated regions, effectively selecting underrepresented samples with limited budgets. To evaluate the empirical effectiveness of ActiveKD and PCoreSet, we conduct extensive experiments on 11 datasets spanning wide range of tasks, including generic object recognition [73, 20], fine-grained classification [63, 57, 65], domain-specific recognition [9], scene understanding [91], texture analysis [15], satellite imagery [33], and human action recognition [80]. As shown in Fig. 1, the results demonstrate that ActiveKD consistently improves the performance of active learning across all datasets, with further gains observed when using few-shot teachers. Moreover, the proposed PCoreSet outperforms baseline selection methods under ActiveKD, in both zero-shot and few-shot distillation settings. Our contributions and findings are summarized as follows: We introduce Active Knowledge Distillation (ActiveKD), framework that leverages the zeroand few-shot capabilities of Vision-Language Models (VLMs) to integrate KD into Active Learning (AL), enabling efficient training of compact models with limited labeled data. We identify structured prediction biases in VLM teachers as an inductive bias that captures generalizable output patterns. We propose Probabilistic CoreSet (PCoreSet), which selects underrepresented samples in the probability space, enabling effective knowledge transfer. Extensive experiments across 11 datasets, including ImageNet, validate the effectiveness of both ActiveKD and PCoreSet in various AL scenarios under the presence of VLM teachers."
        },
        {
            "title": "2 Related Works",
            "content": "Vision-Language Models (VLMs). Recent advances in machine learning have focused on training large foundation models that either transfer pretrained knowledge to specific tasks [29, 17, 68, 38, 67, 46] or directly apply to target tasks [87, 55, 56, 68, 38] via task instructions or zero-shot prediction using language prompts. In this paper, we utilize VLMs [68, 38, 77] as teachers, which learn shared embedding space for images and texts through contrastive objective. key advantage of such language-aligned training is the ability to perform zero-shot predictions [68, 38] without task-specific training and few-shot predictions [103, 102, 42, 104, 43, 100, 72, 98, 49, 23, 99, 96] with few examples, qualifying VLMs as generalist models capable of handling diverse visual recognition tasks. Despite their strong task-agnostic capabilities, effectively leveraging task-specific datasets remains crucial for developing compact models that perform well on downstream tasks, especially under label scarcitya common challenge in real-world applications. Active learning [AL; 71, 97, 52] is framework specifically designed to address such labelscarcity issues. It assumes that acquiring domain-specific datasets through manual annotation can be prohibitively expensive in practical scenarios. Specifically, AL addresses this by strategically selecting the most informative samples from unlabeled data on the target task for annotation, maximizing model performance with minimal labeled data. Previous approaches include uncertainty-based methods [51, 40, 35, 36, 21, 45, 69], diversity-based techniques [76, 66, 95, 27], and hybrid approaches [4, 45, 26, 24] that combine multiple criteria for sample selection. Several studies have focused on the class imbalance problem [3, 8, 37], where unbalanced datasets can lead selection algorithms to pick class-imbalanced samples. Recent research has integrated AL with prompt tuning [50, 39, 102] of VLMs, leveraging foundational knowledge with efficient parameter updates [6, 74, 44]. PCB [6] addressed skewed predictions of VLMs by balancing class distributions, while CB+SQ [44] enhanced this with class-guided clustering and adaptive thresholds. In contrast, we regard structured predictions as an inductive bias, and propose to leverage it by selecting probabilistically diverse samples rather than exploiting diversity in the feature space [76]. Knowledge Distillation from VLMs. Knowledge Distillation (KD) [34] transfers knowledge from large teachers to compact students. With the emergence of vision foundation models through self-supervised learning [12, 30, 25, 14, 11, 31] and vision-language pretraining [68, 38], researchers have increasingly explored KD methods to leverage knowledge embedded within these models. Early work focused on self-supervised models [19, 1, 92, 86, 62, 78], while the advent of VLMs inspired further research, including distillation of compact VLMs from larger counterparts [90, 81 83, 93], unsupervised distillation from VLM predictions [84, 88, 59], and few-shot semi-supervised distillation of VLMs [41]. Some research has explored using teacher models instead of human annotation to address incorrect predictions [7] or generate data efficiently [85, 54]. Our work focuses on human-in-the-loop scenarios, aiming to identify the most informative samples for annotation within KD framework, by leveraging the zeroor few-shot capabilities of VLMs, to maximize learning efficiency under practical constraints of AL settings."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Backgrounds on VLMs. We utilize Vision-Language Models (VLMs) like CLIP [68] and ALIGN [38] as teacher models. These models jointly optimize an image encoder fX : Rd and text encoder fT : Rd to map corresponding image-text pairs into shared embedding space Rd. This cross-modal alignment enables zero-shot transfer through natural language supervision. For C-way classification tasks, we create textual prompts (e.g., photo of [CLASS]) to generate classspecific text descriptions {t1, t2, . . . , tC}. The output probability vector of categorical distribution over classes is computed as the following: (x) = σ (cid:18) 1 ζ [CosSim(fX (x), fT (t1)), . . . , CosSim(fX (x), fT (tC))] (cid:19) C1. (1) Here, C1 is the (C1)-dimensional probability simplex, σ : RC C1 denotes the softmax function, and ζ R>0 is the temperature scaling factor [34]. Cosine similarity is defined as CosSim(x, y) = xy . The final predicted class is given by arg maxc[C][f (x)]c. x2y2 Figure 3: (Left): Teacher model prediction biases ((µ1, r1), . . . , (µk, rk)) transfer to student models via distillation, where ˆµk = µk + yc and ˆrk = (1 λ)rk (µ denotes centroids, denotes radii). (Right): PCoreSet selects samples maximizing distance to labeled points in probability simplex C1, uncovering underrepresented regions. Algorithm 1 ActiveKD Framework Algorithm 2 PCoreSet Algorithm Require: Initial labeled dataset D(l), unlabeled pool teacher VLM , an selection algorithm A, D(u), query size Q, number of rounds Ensure: The final model fr 1: for = 1 to do 2: 3: 4: 5: 6: 7: 8: 9: end for 10: return The final model fr Initialize the r-th round model fr (Optional for few-shot teacher) adapt on D(l) Train fr with LCE in Eq. (2), and LKD in Eq. (3) {x Get labels {y D(l) D(l) {(x D(u) D(u) {x q=1 for {x )}Q , }Q q=1 A(D(l), D(u), fr; Q) }Q }Q }Q q=1 q=1 q=1 Require: Labeled dataset D(l), unlabeled pool D(u), model fr, query size Q, distance function d(x, x) := fr(x) fr(x)2 Ensure: Coreset D(u) with = 1: Initialize = D(l) 2: Initialize D[x] = minsS d(x, s), D(u) 3: while < do = arg max 4: xD(u)S = {x} for each D(u) do D[x] D[x] = min(D[x], d(x, x)) 5: 6: 7: end for 8: 9: end while 10: return }M Problem formulation. In this paper, we consider pool-based active learning (AL), framework for building C-way classifier fr : (cid:55) C1 for each round [R] := {1, . . . , R} while minimizing annotation costs. We start with labeled dataset D(l) = {(x(l) n=1 and an unlabeled dataset D(u) = {x(u) m=1, where yn {0, 1}C is the one-hot encoding label of xn and typically . We first train model fr using the current labeled dataset D(l) for each round r. Then, we select subset q=1 A(D(l), D(u), fr; Q) D(u), of unlabeled data that will be requested for annotation, i.e., {x where is the number of query datapoints and is an algorithm for selection. These selected queries are then annotated by an oracle (typically human experts) to obtain their true labels {y q=1. The labeled dataset is updated as D(l) D(l) {(x q=1, and the unlabeled pool is reduced by D(u) D(u) {x q=1. See Alg. 1 without underlines for an overview. , yn)}N )}Q q, }Q q}Q q}Q 3.2 Our Method: ActiveKD with PCoreSet Active knowledge distillation (ActiveKD) framework. Previous works deployed semi-supervised learning [SSL; 5, 10, 101] to leverage the large unlabeled dataset D(u) = {x(u) m=1 at each AL round. In this paper, we deploy knowledge distillation (KD) instead of SSL thanks to the recent advances of VLMs. Specifically, we train the model for each round as following [34]: }M LCE = LKD = 1 1 (cid:88) (cid:88) ℓ (fr(xn), yn) , DKL (cid:104) (x(l) (cid:105) )fr(x(l) ) + 1 (cid:88) DKL (cid:104) (x(u) )fr(x(u) ) (cid:105) , (2) (3) 4 (a) (b) (c) Figure 4: Visualization of prediction bias and its propagation. (a) Teacher hard prediction bias, (b) teacher and student soft prediction after distillation. (c) Bias propagation measured by KNN loss (ℓ2) across different numbers of clusters (k). where the final loss is λLCE + (1 λ)LKD, ℓ denotes DKL represent cross-entropy and KullbackLeibler divergence, respectively. For each round r, we use mini-batch version of the above objective with stochastic gradient descent to optimize the parameters of fr by leveraging the teacher prediction (). See underlines in Alg. 1 for additional parts of ActiveKD. Structured prediction bias propagation. Foundation models such as CLIP ResNet-50 [68] inherently exhibit class imbalance [6] due to their general-purpose training objectives, as shown in Fig. 4a. Beyond simple imbalance, this bias manifests structurally in the probability space, with predictions forming distinct clusters that occupy limited regions of the probability simplex, as visualized in Fig. 4b. The violin plots show probability distributions of samples for given ground truth classes, revealing how teacher predictions cluster in specific regions rather than uniformly covering the probability space. Interestingly, we observe that student models inherit similar biased prediction patterns. To formalize this, we first define the teachers structured prediction bias as follows: Definition 1 (Structured prediction bias). Teacher predictions exhibit structured prediction bias if there exist N, centroids {µk}K k=1 C1, and radii {rk}K k=1 R>0, such that: , (x) (cid:91) k= (cid:8)p C1 : µk2 rk (cid:9) . (4) In other words, all teacher predictions lie within finite union of ℓ2-balls in the probability simplex. We now show that student predictions trained via KD also exhibit structured bias: Proposition 1 (Bias propagation through KD). Let the teacher model exhibit structured prediction bias as defined in Def. 1, with {µk}K k=1. Assume the student model fr is trained via KD from using the loss λLCE + (1 λ)LKD, and satisfies fr(x) (x)1 ϵ for all , where (x) = λy + (1 λ)f (x) and {0, 1}C denotes the label of x. Then student predictions fr(x) exhibit structured prediction bias as in Def. 1. Specifically, , there exists [K] such that: fr(x) (cid:8)p C1 : ˆµk(x)2 ˆrk k=1 and {rk}K (cid:9) , (5) where the propagated centriod is defined as ˆµk(x) = λy + (1 λ)µk, and the adjusted radius is ˆrk = (1 λ)rk + ϵ. Since is one-hot and µk is fixed, the set of possible centers {ˆµk(x)} is finite and contained in C1, thus satisfying the condition of Def. 1 with at most clusters. We defer the proof of Prop. 1 to Appendix A.2. Prop. 1 is also illustrated in Fig. 3-(left), where one of the teacher cluster, characterized by (µk, rk), propagates to corresponding active student clusters with (ˆµk, ˆrk) across three different class labels. Empirical validation of Prop. 1. We train two models under 1-shot setting (i.e., one labeled image per class): 1) baseline model using LCE on D(l), and 2) student model trained via KD with loss λLCE + (1 λ)LKD on D(l) and D(u). To assess bias propagation, we cluster teacher predictions on D(u) via k-means [28], yielding centroids {µk}K k=1. For each D(u), we identify its assigned teacher centroid µk and compute the corresponding propagated student centroid ˆµk = λy +(1λ)µk, where is the one-hot ground-truth label of x. We then measure the average ℓ2 distance between fr(x) and ˆµk for both models, using threshold ϵ = 0.3. As shown in Fig. 4c, the distilled student shows consistently lower distances than the baseline across varying K, validating Prop. 1. The number of active propagated centroids also closely matches that of the teacher, further supporting the structured prediction bias of student model. 5 In this work, we interpret the structured prediction Probabilistic Coreset (PCoreSet) selection. biases of teacher models as an inductive bias in the ActiveKD setting. During KD, the student model has access to labeled data and therefore does not merely mimic the teachers outputs, but learns from them under supervision. As result, the teachers prediction structure acts as prior that shapes the students hypothesis space. Our key insight is that when KD is effective (with bounded error ϵ), the student inherits the teachers clustered structure in the probability space (Prop. 1). Samples that deviate from this structure are particularly informative, as they fall outside regions captured by the inductive bias. Rather than discarding this structure, we leverage it to guide active learningselecting samples that challenge the current bias and expand the students predictive capacity. To this end, we propose Probabilistic Coreset (PCoreSet), simple yet effective selection strategy inspired by coreset selection [76]. While conventional coreset methods aim to maximize coverage in the feature space, PCoreSet instead maximizes coverage in the probability simplex C1 by targeting underrepresented probability regions. This enables the student to inherit the teachers inductive bias more completely while actively exploring beyond it. Formally, given labeled dataset D(l), we greedily select D(u) as: = arg max xD(u) min xD(l) d(x, x), (6) where d(x, x) := fr(x) fr(x)2 measures distance in the probability space. We then request the label for and update the labeled set: D(l) D(l) {(x, y)}. PCoreSet thus generalizes coreset selection to the probability space, encouraging coverage of underrepresented predictions to better align with and extend the inductive bias learned via KD. See Alg. 2 for details."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate the empirical effectiveness of our proposed methods. We present our experimental setups in 4.1 and the following empirical findings in 4.2: [F1]: ActiveKD significantly enhances active learning performance by leveraging the knowledge of both zeroand few-shot VLM teachers. [F2]: PCoreSet promotes diverse sampling in the probability space, allowing the student to both inherit and extend the inductive biases of teacher models, leading to improved performance under the proposed ActiveKD framework. 4.1 Experimental Setup Datasets. We evaluate our approach across 11 diverse datasets: generic image recognition benchmarks such as ImageNet [73] and Caltech101 [20], and fine-grained and domain-specific benchmarks such as StanfordCars [47], Flowers102 [63], FGVCAircraft [57], Food101 [9], SUN397 [91], DTD [15], EuroSAT [33], and UCF101 [80]. See an overview of datasets in Appendix C. Active learning frameworks. We evaluate three distinct active learning (AL) frameworks: 1. No Distill: standard AL without knowledge distillation, where the model is trained solely on labeled data D(l) using the cross-entropy loss LCE. 2. Zero-Shot: our proposed ActiveKD framework with fixed zero-shot VLM teacher model [68] that provides soft targets for distillation on both D(l) and D(u). 3. Few-Shot: same as 2. Zero-Shot but with few-shot VLM teacher model [77] that is fine-tuned on D(l) with newly selected samples at each round. Active selection baselines. We compare our proposed method, PCoreSet, with five representative active selection strategies: 1. Random: randomly selects samples from the unlabeled pool D(u). 2. Entropy [35]: selects samples with the highest prediction entropy. 3. Coreset [76]: maximizes sample diversity in the feature space. 4. BADGE [4]: hybrid approach that combines uncertainty and diversity via gradient embeddings. 5. Class-Balanced [6]: prioritizes class diversity in the selected samples. 6 Table 1: Results on ImageNet and average across 10 datasets with different AL frameworks at the final AL round. We report average and 95% CI over 5 runs. Strategy No Distill Zero-Shot Few-Shot ImageNet Performance Random Coreset [76] Entropy [35] ClassBalanced [6] PCoreSet (ours) 33.36 0.45 26.61 0.90 30.76 0.24 34.44 0.60 33.41 0.41 60.69 0.16 60.58 0.07 60.43 0.16 61.07 0.11 61.16 0.14 60.49 0.12 59.01 0.42 58.87 0.14 61.13 0.07 61.57 0. Average over 10 Datasets Random Coreset [76] Entropy [35] Badge [4] ClassBalanced [6] PCoreSet (ours) 63.10 4.33 64.99 3.01 64.06 3.47 63.14 4.66 63.57 4.33 64.94 3.25 76.31 0.88 76.20 0.91 75.58 0.86 76.18 0.74 76.28 0.87 77.44 0.84 77.48 0.86 77.63 0.57 77.47 0.90 77.50 0.67 77.56 0.66 78.81 0.42 Figure 5: Results on ImageNet (Top) and average across 10 datasets (Bottom) over AL rounds. Zeroand Few-shot share the y-axis for comparison. Implementation details. All AL frameworks begin from 1-shot setting, where one labeled example is provided per class across classes. We use query size of = per round, with = 16 rounds for all datasets except ImageNet, which uses = 8. We adopt DHO [41] as the KD method in all experiments to leverage the unlabeled pool. For ImageNet, we use selfsupervised ResNet-50 [11] as the student model. For the other 10 datasets, we use ResNet-18 [29], MobileNetV2 [75], and TinyViT [89], all pretrained on ImageNet. We consistently use CLIP ResNet50 [68] as the zero-shot teacher and CLAP [77] as the few-shot teacher. To reduce computational cost, we subsample the unlabeled ImageNet pool to 100,000 images. We exclude BADGE [4] on ImageNet due to memory limitations when computing gradients over its 1,000 classes. We report the mean performance with 95% confidence intervals across 5 random seeds. We defer further details to Appendix B, e.g., pseudocode for KD, training hyperparameters, and baseline implementations. 4.2 Experimental Results [F1-1]: Effectiveness of ActiveKD with zero-shot teachers. We evaluate our ActiveKD framework using zero-shot VLM teachers. Table 1 shows that ActiveKD (Zero-Shot) consistently outperforms the baseline without distillation across all datasets and selection strategies. Zero-shot distillation improves accuracy by 27.33% on ImageNet with random selection (33.36% 60.69%) and by 29.07% on average across strategies. This improvement extends across 10 datasets with an average gain of 12.21% (63.10% 76.31%) for random selection, and 12.37% on average across all strategies, demonstrating the effectiveness of zero-shot distillation under limited supervision. Fig. 5 confirms these performance gains persist throughout the AL process. [F1-2]: Effectiveness of ActiveKD with few-shot teachers. Few-shot distillation further enhances performance, as shown in Table 1, boosting accuracy by 0.41% on ImageNet and 1.37% across 10 datasets when paired with PCoreSet. While Entropy and Coreset show slight performance drops when transitioning from zero-shot to few-shot, indicating that teacher fine-tuning benefits vary by dataset and selection strategy, PCoreSet achieves the most substantial gains, demonstrating strong synergy with fewshot distillation. Fig. 5 confirms these improvements persist across most AL rounds, with exceptions only in early ImageNet rounds (r < 4). Figure 6: Results of few-shot teachers on 11 datasets across rounds on ImageNet (Left) and averaged across 10 datasets (Right). To better assess the effectiveness of PCoreSet in ActiveKD with few-shot teachers, we track teacher performance across AL rounds. As shown in Fig. 6, PCoreSet consistently outperforms alternative selection strategies by enhancing few-shot teacher performance. This shows key insight: structured prediction bias in the teacher propagates to the student during distillation. By selecting samples underrepresented in the student prediction space, PCoreSet also captures underrepresented modes in the teacher space. These selections not only boost student performance but also improve the teacher itself, forming virtuous cycle of better selection and stronger distillation. 7 Figure 7: Results on 10 datasets using ResNet-18 under zero-shot distillation across 16 rounds. Figure 8: Results on 10 datasets using TinyViT under zero-shot distillation across 16 rounds. [F2-1]: Effectiveness of PCoreSet under ActiveKD. We conduct an in-depth evaluation of PCoreSet under ActiveKD using ResNet-18 student and zero-shot teacher. As shown in Fig. 7, PCoreSet outperforms active selection baselines across all AL rounds and 10 datasets in the most settings. Notably, none of the widely used baselines, Entropy, Coreset, or BADGE, demonstrates consistent superiority over Random. In fact, Random surpasses these baselines on several datasets, including ImageNet  (Table 1)  , FGVC Aricraft, and Food101, while remaining competitive on others. This unexpected effectiveness of Random underscores the complexity of developing effective active learning methods for KD. We further validate the generalizability of PCoreSet across different architecture families: TinyViT [89] (vision transformer) and MobileNetV2 [75] (efficient convolutional network). As shown in Fig. 8, PCoreSet outperforms baselines with TinyViT under zero-shot distillation in the most settings. MobileNetV2 exhibits similar trends (see Appendix D.1). [F2-2]: The acceleration of structured bias propagation of PCoreSet. To assess whether PCoreSet effectively promotes structured bias propagation, we visualize the same metric used in Fig. 4c across AL rounds. We include an additional baseline, PCoreSet+R(Reverse), which inhibits coverage over the probability space by replacing arg max with arg min in line 4 of Alg. 2. As shown in Fig. 9, PCoreSet consistently yields the lowest KNN loss and highest cluster purity among all selection strategies. This confirms that PCoreSet promotes structured bias propagation as intended, resulting in improved AL performance. In contrast, PCoreSet+R exhibits the opposite behavior, supporting our claim that PCoreSet encourages bias propagation by maximizing sample diversity in the probability space. Figure 9: KNN loss (L2) and cluster purity of PCoreSet and other selection strategies across 10 datasets. 8 Figure 10: Comparison of four selection criteria: 1) Uncertainty, 2) Class balance, 3) Feature space diversity, and 4) Probability space diversity. The results are averaged over the 10 datasets excluding ImageNet. Figure 11: The heatmap of output probability vectors from different selection strategies in the first active learning round using the DTD dataset. See Appendix E.1 for results on other datasets. Active selection criteria. To evaluate the effectiveness of each active selection criterion, we analyze the progression of four strategies designed to be maximized throughout the active learning (AL) rounds: 1) Uncertainty [35], 2) Class balance [6], 3) Feature space diversity [76], and 4) Probability space diversity (PCoreSet). Criteria 1) and 2) are measured using normalized Shannon entropy, while 3) and 4) are evaluated using normalized average minimum distances. We report the average of each metric across 10 datasets, excluding ImageNet. As shown in Fig. 10, each method performs best on its respective objective: Entropy maximizes uncertainty, ClassBalanced optimizes the entropy of the class distribution, and Coreset maximizes feature space diversity. As expected, our PCoreSet method achieves the highest diversity in probability space, while also ranking third in uncertainty and class balance, and second in feature diversity. In contrast, traditional Coreset approaches [76], though effective in covering the feature space and offering moderate probabilistic diversity, perform poorly in class balanceshowing their limitations under ActiveKD settings. Sample diversity of PCoreSet. To visualize the sample diversity achieved by PCoreSet, we present probability heatmaps of both the initially labeled samples and those selected by various active learning strategies after the first round, as shown in Fig. 11. The initial labeled set (1-shot per class) generally leads to correct classifications based on ground truth, though the probability distributions deviate from ideal values due to the influence of the teacher distillation signal. The heatmaps clearly show that PCoreSet selects samples with highly diverse probabilistic characteristics, effectively maximizing distributional coverage compared to the initial labeled set. We further visualize the selected images from the Caltech101 dataset in Fig. 12. PCoreSet selects not only visually distinct examples but also samples from different classes. Visualizations for other datasets are provided in Appendix E.2."
        },
        {
            "title": "5 Conclusion, Limitation, and Future Work",
            "content": "Figure 12: Images selected by PCoreSet on the Caltech101 dataset. We introduced ActiveKD, integrating active learning with knowledge distillation by leveraging VLMs zeroand few-shot capabilities. We discovered VLMs exhibit structured prediction bias that benefits student learning, and proposed Probabilistic Coreset (PCoreSet) to maximize diversity in probability space when selecting samples. Experiments across 11 datasets show PCoreSet outperforms existing methods, especially when labeled data is scarce, enabling efficient knowledge transfer under limited annotation budgets. Our work is currently limited to visual recognition with CLIP. Future research could extend ActiveKD to more complex tasks like object detection and segmentation, and incorporate instruction-tuned generalist models like FLAN [87] or LLaVA [55] to enhance effectiveness across diverse applications."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "[1] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Selfsupervised learning by compressing representations. Advances in Neural Information Processing Systems, 33:1298012992, 2020. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Umang Aggarwal, Adrian Popescu, and Céline Hudelot. Active learning for imbalanced datasets. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 14281437, 2020. [4] Jordan Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671, 2019. [5] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 84438452, 2021. [6] Jihwan Bang, Sumyeong Ahn, and Jae-Gil Lee. Active prompt learning in vision language In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern models. Recognition, pages 2700427014, 2024. [7] Cenk Baykal, Khoa Trinh, Fotis Iliopoulos, Gaurav Menghani, and Erik Vee. Robust active distillation. arXiv preprint arXiv:2210.01213, 2022. [8] Javad Zolfaghari Bengar, Joost van de Weijer, Laura Lopez Fuentes, and Bogdan Raducanu. Class-balanced active learning for image classification. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 15361545, 2022. [9] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13, pages 446461. Springer, 2014. [10] Zhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen Tu, and Stefano Soatto. Semi-supervised vision transformers at scale. Advances in Neural Information Processing Systems, 35:2569725710, 2022. [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [13] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:2224322255, 2020. 10 [14] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1575015758, 2021. [15] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36063613, 2014. [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [18] Pan Du, Suyun Zhao, Zisen Sheng, Cuiping Li, and Hong Chen. Semi-supervised learning via weight-aware distillation under class distribution mismatch. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1641016420, 2023. [19] Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. Seed: Self-supervised distillation for visual representation. arXiv preprint arXiv:2101.04731, 2021. [20] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178178. IEEE, 2004. [21] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In International conference on machine learning, pages 11831192. PMLR, 2017. [22] Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan Ö Arık, Larry Davis, and Tomas Pfister. Consistency-based semi-supervised active learning: Towards minimizing labeling cost. In European Conference on Computer Vision, pages 510526. Springer, 2020. [23] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581595, 2024. [24] Petros Stylianos Giouroukis, Alexios Gidiotis, and Grigorios Tsoumakas. Dual: Diversity and uncertainty active learning for text summarization. arXiv preprint arXiv:2503.00867, 2025. [25] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. [26] Guy Hacohen and Daphna Weinshall. How to select which active learning strategy is best suited for your specific problem and budget. Advances in Neural Information Processing Systems, 36:1339513407, 2023. [27] Guy Hacohen, Avihu Dekel, and Daphna Weinshall. Active learning on budget: Opposite strategies suit high and low budgets. arXiv preprint arXiv:2202.02794, 2022. [28] John Hartigan and Manchek Wong. Algorithm as 136: k-means clustering algorithm. Journal of the royal statistical society. series (applied statistics), 28(1):100108, 1979. [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 11 [31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. [32] Lingxiao He, Wu Liu, Jian Liang, Kecheng Zheng, Xingyu Liao, Peng Cheng, and Tao arXiv preprint Semi-supervised domain generalizable person re-identification. Mei. arXiv:2108.05045, 2021. [33] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. [34] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [35] Alex Holub, Pietro Perona, and Michael Burl. Entropy-based active learning for object recognition. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 18. IEEE, 2008. [36] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011. [37] Zitong Huang, Ze Chen, Yuanze Li, Bowen Dong, Erjin Zhou, Yong Liu, Rick Siow Mong Goh, Chun-Mei Feng, and Wangmeng Zuo. Class balance matters to active class-incremental learning. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 94459454, 2024. [38] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR, 2021. [39] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European conference on computer vision, pages 709727. Springer, 2022. [40] Ajay Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image classification. In 2009 ieee conference on computer vision and pattern recognition, pages 23722379. IEEE, 2009. [41] Seongjae Kang, Dong Bok Lee, Hyungjoon Jang, and Sung Ju Hwang. Simple semi-supervised knowledge distillation from vision-language models via dual-head optimization. arXiv preprint arXiv:2505.07675, 2025. doi: 10.48550/arXiv.2505.07675. [42] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1911319122, 2023. [43] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1519015200, 2023. [44] Hoyoung Kim, Seokhee Jin, Changhwan Sung, Jaechang Kim, and Jungseul Ok. Active prompt learning with vision-language model priors. arXiv preprint arXiv:2411.16722, 2024. [45] Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. Advances in neural information processing systems, 32, 2019. [46] Mikhail Koroteev. Bert: review of applications in natural language processing and understanding. arXiv preprint arXiv:2103.11943, 2021. 12 [47] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554561, 2013. [48] Marc Lafon, Elias Ramzi, Clément Rambour, Nicolas Audebert, and Nicolas Thome. Gallop: Learning global and local prompts for vision-language models. In European Conference on Computer Vision, pages 264282. Springer, 2024. [49] Marc Lafon, Elias Ramzi, Clément Rambour, Nicolas Audebert, and Nicolas Thome. Gallop: Learning global and local prompts for vision-language models. In European Conference on Computer Vision, pages 264282. Springer, 2025. [50] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [51] David Lewis. sequential algorithm for training text classifiers: Corrigendum and additional data. In Acm Sigir Forum, pages 1319. ACM New York, NY, USA, 1995. [52] Dongyuan Li, Zhen Wang, Yankai Chen, Renhe Jiang, Weiping Ding, and Manabu Okumura. survey on deep active learning: Recent advances and new frontiers. IEEE Transactions on Neural Networks and Learning Systems, 2024. [53] Jaeseung Lim, Jongkeun Na, and Nojun Kwak. Active semi-supervised learning by exploring per-sample uncertainty and consistency. arXiv preprint arXiv:2303.08978, 2023. [54] Chengyuan Liu, Yangyang Kang, Fubang Zhao, Kun Kuang, Zhuoren Jiang, Changlong Sun, and Fei Wu. Evolving knowledge distillation with large language models and active learning. arXiv preprint arXiv:2403.06414, 2024. [55] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [56] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [57] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [58] Cristina Menghini, Andrew Delworth, and Stephen Bach. Enhancing clip with clip: Exploring pseudolabeling for limited-label prompt tuning. Advances in Neural Information Processing Systems, 36:6098461007, 2023. [59] Marco Mistretta, Alberto Baldrati, Marco Bertini, and Andrew Bagdanov. Improving zeroshot generalization of learned prompts via unsupervised knowledge distillation. In European Conference on Computer Vision, pages 459477. Springer, 2024. [60] Pablo Morales-Álvarez, Stergios Christodoulidis, Maria Vakalopoulou, Pablo Piantanida, and Jose Dolz. Bayesadapter: enhanced uncertainty estimation in clip few-shot adaptation. arXiv preprint arXiv:2412.09718, 2024. [61] Balamurali Murugesan, Julio Silva-Rodríguez, Ismail Ben Ayed, and Jose Dolz. Robust calibration of large vision-language adapters. In European Conference on Computer Vision, pages 147165. Springer, 2024. [62] KL Navaneet, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Simreg: Regression as simple yet effective tool for self-supervised knowledge distillation. arXiv preprint arXiv:2201.05131, 2022. [63] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. [64] OpenAI. Chatgpt (gpt-4). https://chat.openai.com, 2025. Accessed: 2025-04-20. 13 [65] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012. [66] Amin Parvaneh, Ehsan Abbasnejad, Damien Teney, Gholamreza Reza Haffari, Anton Van Den Hengel, and Javen Qinfeng Shi. Active learning by feature mixing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1223712246, 2022. [67] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [69] Vineeth Rakesh and Swayambhoo Jain. Efficacy of bayesian neural networks in active learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26012609, 2021. [70] Aneesh Rangnekar, Christopher Kanan, and Matthew Hoffman. Semantic segmentation with active semi-supervised learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 59665977, 2023. [71] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij Gupta, Xiaojiang Chen, and Xin Wang. survey of deep active learning. ACM computing surveys (CSUR), 54 (9):140, 2021. [72] Shuvendu Roy and Ali Etemad. Consistency-guided prompt learning for vision-language models. arXiv preprint arXiv:2306.01195, 2023. [73] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. [74] Bardia Safaei and Vishal Patel. Active learning for vision-language models. arXiv preprint arXiv:2410.22187, 2024. [75] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45104520, 2018. [76] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: core-set approach. arXiv preprint arXiv:1708.00489, 2017. [77] Julio Silva-Rodriguez, Sina Hajimiri, Ismail Ben Ayed, and Jose Dolz. closer look at the few-shot adaptation of large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2368123690, 2024. [78] Aditya Singh and Haohan Wang. Simple unsupervised knowledge distillation with space similarity. In European Conference on Computer Vision, pages 147164. Springer, 2024. [79] Ayush Singh, Aayush Rana, Akash Kumar, Shruti Vyas, and Yogesh Singh Rawat. Semisupervised active learning for video action detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 48914899, 2024. [80] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [81] Ximeng Sun, Pengchuan Zhang, Peizhao Zhang, Hardik Shah, Kate Saenko, and Xide Xia. In Proceedings of the Dime-fm: Distilling multimodal and efficient foundation models. IEEE/CVF International Conference on Computer Vision, pages 1552115533, 2023. 14 [82] Vishaal Udandarao, Nikhil Parthasarathy, Muhammad Ferjad Naeem, Talfan Evans, Samuel Albanie, Federico Tombari, Yongqin Xian, Alessio Tonioni, and Olivier Hénaff. Active data curation effectively distills large-scale multimodal models. arXiv preprint arXiv:2411.18674, 2024. [83] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobileclip: Fast image-text models through multi-modal reinforced training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1596315974, 2024. [84] Raviteja Vemulapalli, Hadi Pouransari, Fartash Faghri, Sachin Mehta, Mehrdad Farajtabar, Mohammad Rastegari, and Oncel Tuzel. Knowledge transfer from vision foundation models for efficient training of small task-specific models. arXiv preprint arXiv:2311.18237, 2023. [85] Dongdong Wang, Yandong Li, Liqiang Wang, and Boqing Gong. Neural networks are more productive teachers than human raters: Active mixup for data-efficient knowledge distillation from blackbox model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14981507, 2020. [86] Kai Wang, Fei Yang, and Joost van de Weijer. Attention distillation: self-supervised vision transformer students need more guidance. arXiv preprint arXiv:2210.00944, 2022. [87] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [88] Ge Wu, Xin Zhang, Zheng Li, Zhaowei Chen, Jiajun Liang, Jian Yang, and Xiang Li. Cascade prompt learning for vision-language model adaptation. In European Conference on Computer Vision, pages 304321. Springer, 2024. [89] Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast pretraining distillation for small vision transformers. In European conference on computer vision, pages 6885. Springer, 2022. [90] Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi Stephen Chen, Xinggang Wang, et al. Tinyclip: Clip distillation via affinity mimicking and weight inheritance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2197021980, 2023. [91] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 34853492. IEEE, 2010. [92] Haohang Xu, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Bag of instances aggregation boosts self-supervised distillation. arXiv preprint arXiv:2107.01691, 2021. [93] Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, and Yongjun Xu. Clip-kd: An empirical study of clip model distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1595215962, 2024. [94] Jing Yang, Xiatian Zhu, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Knowledge distillation meets open-set semi-supervised learning. International Journal of Computer Vision, 133(1):315334, 2025. [95] Ofer Yehuda, Avihu Dekel, Guy Hacohen, and Daphna Weinshall. Active learning through covering lens. Advances in Neural Information Processing Systems, 35:2235422367, 2022. [96] Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang. Task residual for tuning visionlanguage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1089910909, 2023. 15 [97] Xueying Zhan, Qingzhong Wang, Kuan-hao Huang, Haoyi Xiong, Dejing Dou, and Antoni Chan. comparative survey of deep active learning. arXiv preprint arXiv:2203.13450, 2022. [98] Ji Zhang, Shihan Wu, Lianli Gao, Heng Tao Shen, and Jingkuan Song. Dept: Decoupled prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1292412933, 2024. [99] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021. [100] Cairong Zhao, Yubin Wang, Xinyang Jiang, Yifei Shen, Kaitao Song, Dongsheng Li, and Duoqian Miao. Learning domain invariant prompt for vision-language models. IEEE Transactions on Image Processing, 33:13481360, 2024. [101] Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, and Chang Xu. Simmatchv2: Semi-supervised learning with graph consistency. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1643216442, 2023. [102] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1681616825, 2022. [103] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):23372348, 2022. [104] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1565915669, 2023."
        },
        {
            "title": "Appendix Overview",
            "content": "This appendix contains supplementary information organized as follows: Theoretical Analysis (Appendix A): provides formal analysis of bias propagation in knowledge distillation, including optimal student prediction analysis (Appendix A.1), formal characterization of teacher prediction bias (Appendix A.2), and mathematical proof of how biases transfer from teacher to student models (Prop. 2). Implementation (Appendix B): detailed experimental setup including model configurations, training parameters. Datasets (Appendix C): comprehensive description of the datasets used in our experiments, including ImageNet and other benchmarks used for evaluating bias propagation, as well as detailed analysis of class distributions across all datasets. Additional Experiments (Appendix D): supplementary experimental results including evaluations on alternative architectures (e.g., MobileNetV2 in Appendix D.1), active learning without knowledge distillation, and few-shot teacher distillation scenarios. Additional Qualitative Results (Appendix E): additional results of the proposed method, including heatmap visualizations of selected samples across datasets (Appendix E.1) and qualitative analysis of samples selected by our PCoreSet method (Appendix E.2)."
        },
        {
            "title": "A Theoretical Analysis",
            "content": "This section presents formal analysis of bias propagation in knowledge distillation, examining how teacher model biases influence student model predictions. We first analyze the optimal student prediction after distillation from teacher model in Section A.1, building upon the analytical framework established in DHO [41]. Subsequently, in Section A.2, we formally characterize the mechanisms through which teacher prediction biases propagate to and constrain the distilled student model, providing the theoretical foundation for our debiasing strategies. A.1 Optimal Student Prediction after Distillation Let us first establish our notation. We consider student model with feature extractor : Rd that maps inputs to feature representations, and classification head : Rd RC that maps features to logits, with the final prediction obtained by applying the softmax function σ : RC C1 to these logits, resulting in fr(x) = σ(g(h(x))), where denotes the active learning round. Following DHO [41], we consider two target probability distributions: the ground truth label distribution y, typically one-hot encoded vectors where yc = 1 for the true class and 0 elsewhere, and the teachers softened distribution (x) for input . Theorem 1 (Optimal Distribution for Knowledge Distillation). The distribution (x) that minimizes the weighted combination of cross-entropy loss with respect to and Kullback-Leibler divergence with respect to (x): L(f (x)) = λℓ(f (x), y) + (1 λ)DKL(f (x)f (x)) is the weighted arithmetic mean: where λ [0, 1] is the weighting hyperparameter. (x) = λy + (1 λ)f (x) (7) (8) This optimal prediction (x) represents the theoretical target that single-head distillation approach should converge to. Assumption 1 (ε-Convergence). We assume that after sufficient training, student model has converged to the optimal prediction with bounded error: fr(x) (x)1 ε sup (9) where fr(x) = σ(g(h(x))) is the student models prediction, (x) = λy+(1λ)f (x) is the optimal prediction, 1 denotes the L1 norm, and ε > 0 is small constant representing generalization error. 17 This assumption is reasonable for both single-head and dual-head approaches: For the Single-Head Approach, single output head g(h(x)) is trained directly on the combined loss L(f (x)), and with sufficient capacity and training, it approximates (x) with error bounded by ε. For the Dual-Head Approach, in the Dual-Head Optimization (DHO) framework [41], shared features h(x) are extracted from input and two specialized classification heads are employed: gCE(h(x)), optimized for ground truth labels using cross-entropy loss, and gKD(h(x)), optimized for teacher predictions using KL divergence. This approach addresses the constrained optimization problem: min h,gCE,gKD λEx,y[ℓ(σ(gCE(h(x))), y)] + (1 λ)Ex[DKL(f (x)σ(gKD(h(x))))] (10) At inference time, their outputs are combined to form the final prediction: fr(x)DHO = α σ(gCE(h(x))) + (1 α) σ(gKD(h(x))/β) (11) The DHO framework [41] provides formal proof that under appropriate optimization conditions, the dual-head inference is equivalent to single-head inference when setting α = λ and β = τ , meaning that for any choice of distillation weight λ and temperature τ , the two approaches converge to the same prediction. Theorem 2 (Optimal Prediction as Linear Combination). Under Assumption 1, the optimal prediction (x) = λy + (1 λ)f (x) is linear combination of ground truth and teacher prediction, with bounded error ε, regardless of whether single-head or dual-head approach is employed. This formulation provides the basis for analyzing bias propagation in knowledge distillation. This theoretical foundation establishes that regardless of the architectural choice (single-head or dual-head), the optimal student prediction converges to linear combination of the ground truth and teacher prediction, providing principled basis for analyzing bias propagation in knowledge distillation. A.2 Formal Definition of Teacher Prediction Bias Definition 2 (Structured prediction bias). Teacher predictions exhibit structured prediction bias if there exist N, centroids {µk}K k=1 C1, and radii {rk}K k=1 R>0, such that: , (x) (cid:91) k=1 (cid:8)p C1 : µk2 rk (cid:9) . (12) This formulation captures the empirical phenomenon that foundation teacher models tend to produce predictions clustered within limited number of regions in the probability simplex C1 (where is the number of classes), rather than utilizing the entire space of possible probability distributions. The number of biased regions is typically much smaller than the models theoretical expressivity would allow, reflecting inherent biases in the teachers pretraining data and architecture. Proposition 2 (Bias propagation through KD). Let the teacher model exhibit structured prediction bias as defined in Def. 2, with {µk}K k=1. Assume the student model fr is trained via KD from using the loss λLCE + (1 λ)LKD, and satisfies supx fr(x) (x)1 ε for all , where (x) = λy + (1 λ)f (x) and {0, 1}C denotes the one-hot label of x. Then student predictions fr(x) also exhibit structured prediction bias as in Def. 2. Specifically, for every , there exists [K] such that: k=1 and {rk}K fr(x) (cid:8)p C1 : ˆµk(x)2 ˆrk where the propagated centroid is defined as ˆµk(x) = λy + (1 λ)µk, and the adjusted radius is ˆrk = (1 λ)rk + ε. Since is one-hot and µk is fixed, the set of possible centers {ˆµk(x)} is finite and contained in C1, thus satisfying the condition of Def. 2 with at most clusters. (cid:9) , (13) Proof. From Assumption 1, we have: fr(x) (x)1 ε (14) 18 For any L1 norm bounded by ε, the maximum deviation in any single component is also bounded by ε, yielding: (x) ε1 fr(x) (x) + ε1 From Definition 2, for input x, the teachers prediction (x) satisfies: µi ri1 (x) µi + ri1 (15) (16) for some bias region center µi. Substituting this into the formula for (x) = λy + (1 λ)f (x): λy + (1 λ)(µi ri1) (x) λy + (1 λ)(µi + ri1) (17) Combining these inequalities: λy + (1 λ)(µi ri1) ε1 fr(x) λy + (1 λ)(µi + ri1) + ε1 (18) To show that student predictions remain constrained, we note that for each teacher bias region centered at µk, the students predictions are bounded by: ˆµk(x) (1 λ)rk1 ε1 fr(x) ˆµk(x) + (1 λ)rk1 + ε1 (19) where ˆµk(x) = λy + (1 λ)µk. This implies that student predictions are constrained within regions centered at ˆµk(x) with radius ˆrk = (1 λ)rk + ε. While the teachers predictions are constrained to distinct regions, the students predictions may not maintain exactly distinct regions due to potential merging or splitting effects when combining with ground truth labels. However, the students predictions remain bounded and cannot freely explore the entire probability simplex. Setting ˆrk = (1 λ)rk + ε, we can express the students prediction constraint as: fr(x) (cid:91) k=1 {p C1 : ˆµk(x)2 ˆrk} (20) This demonstrates that regardless of the exact number of distinct regions, the student model inherits constrained prediction patterns from the teacher model, with centers shifted by the ground truth component and radii scaled by (1 λ) plus the convergence error ε. Corollary 1 (Student Bias Inheritance). Let denote the number of distinct prediction regions in the student model after distillation. The student models predictions are constrained to these distinct regions in the probability simplex, where: (21) where is the number of classes. Proof. Consider the students optimal prediction (x) = λy + (1 λ)f (x) for any input x. From Definition 2, the teachers prediction (x) belongs to one of distinct regions in the probability simplex, each centered at some µk with radius rk. For each teacher bias region centered at µk, the students prediction center becomes ˆµk,c = λec + (1 λ)µk, where ec is the one-hot vector for class c. This is function of both the teachers bias center µk and the ground truth label. Since there are distinct teacher bias regions and distinct ground truth label distributions (one per class), there can be at most distinct combinations of (µk, ec). Each such combination produces potential distinct student bias region centered at ˆµk,c = λec + (1 λ)µk. Therefore, the maximum number of distinct student prediction regions is bounded by: (22) These student regions are convex combinations of ground truth labels and the teachers biased regions, with an additional error margin of ε. Specifically, for each teacher bias region with center µk and radius rk, and each class c, there corresponds student bias region with center ˆµk,c = λec +(1λ)µk and radius ˆrk = (1 λ)rk + ε. 19 This establishes comprehensive mathematical relationship between teacher bias and its propagation to the student model during knowledge distillation. The student model inherits structured and constrained prediction space from the teacher, which constitutes direct and quantifiable form of bias propagation through the distillation process."
        },
        {
            "title": "B Implementation Details",
            "content": "i=1, unlabeled set D(u) = {x(u) }M j=1, , yi)}N Algorithm 3 DHO Training with zero-shot CLIP [68] teacher 1: Input: labeled set D(l) = {(x(l) 2: 3: 4: 5: 6: while not converged do 7: 8: 9: student feature extractor g, prediction heads hCE, hKD, teacher encoders fX , fT , prompt template photo of [CLASS], temperature scaling factors ζ, η, balancing hyperparameter λ, supervised mini-batch size B, and unsupervised mini-batch size B. b=1 from D(l), B(u) = {x(u) }B b=1 from D(l) D(u). 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: , yb)}B , yb) B(l) do Sample mini-batch B(l) = {(x(l) // Process labeled data for each (x(l) z(l) g(x(l) ) ˆp(l) CE,b σ(hCE(z(l) )) ˆp(l) η hKD(z(l) KD,b σ( 1 )) (cid:16) 1 ζη [CosSim(fX (x(l) p(l) σ end for // Process unlabeled data for each x(u) B(u) do z(u) g(x(u) ) η hKD(z(u) ˆp(u) KD,b σ( 1 )) (cid:16) 1 ζη [CosSim(fX (x(u) p(u) σ end for // Compute losses and update b=1 ℓ(ˆp(l) LCE 1 CE,b, yb) b=1 DKL(ˆp(l) LKD 1 λLCE + (1 λ)LKD Update parameters of g, hCE, hKD using KD,bp(l) ) + 1 (cid:80)B (cid:80)B 23: 24: 25: 26: end while ), fT (t1)), . . . , CosSim(fX (x(l) ), fT (tC))](cid:17) ), fT (t1)), . . . , CosSim(fX (x(u) ), fT (tC))](cid:17) (cid:80)B b=1 DKL(ˆp(u) KD,bp(u) ) Algorithm 4 DHO Inference 1: Input: an image x, feature extractor g, prediction heads hCE, hKD, linear coefficient α, temperature scaling β 2: g(x) 3: ˆpCE σ(hCE(z)) 4: ˆpKD σ(hKD(z)/β) 5: ˆp α ˆpCE + (1 α) ˆpKD 6: ˆy arg maxc(ˆpc) 7: Return: ˆy We employ the DHO [41] framework for knowledge distillation in our approach, as it provides principled and effective mechanism for transferring knowledge from foundation teacher models to student models. DHOs key advantage lies in its dual-head architecture that maintains separated optimization paths for labeled data and teacher knowledge, thereby preserving the distinct learning 20 signals throughout training. Alg. 3 details the DHO training procedure with our zero-shot CLIP teacher, while Alg. 4 outlines the inference process that effectively combines both optimization pathways. Table 2: Implementation details for our experiments across different settings. Active Learning on ImageNet Model Configuration Training Details Student: DINO self-supervised ResNet-50 [11] Teacher: CLIP ResNet-50 [68] Active learning rounds: 8 Initial setting: 1-shot (single image per class) Query size: 1,000 Unlabeled pool: 100,000 samples randomly selected with seed Random seeds: 5 different seeds KD parameters: ζ = 0.01, η = 2, λ = 0.5 DHO parameters: α = 0.4, β = 0.5 Validation: No validation split used Epochs: 20 for first 7 rounds, 50 for final round Optimizer: AdamW (β1 = 0.9, β2 = 0.999) Learning rate: 1 103 Weight decay: 1 102 Batch size: 512 (labeled: 256, unlabeled: 256) Model Configuration Training Details Active Learning on 10 Additional Datasets Epochs: 200 for all rounds Optimizer: AdamW (β1 = 0.9, β2 = 0.999) Learning rate: 1 103 Weight decay: 1 102 Batch size: 128 (labeled: 64, unlabeled: 64) Student: ImageNet pre-trained ResNet-18 [29], MobileNetV2 [75], and ImageNet-21k [16] pretrained ViT-Tiny [89] Teacher: CLIP ResNet-50 [68] Active learning rounds: 16 Initial setting: 1-shot (single image per class) Query size: Equal to number of classes per round Unlabeled pool: All training samples except labeled set Random seeds: 5 different seeds KD parameters: ζ = 0.01, η = 2, λ = 0.5 DHO parameters: α = 0.5, β = 1 Validation: No validation split used Model Configuration Training Details Few-Shot Teacher Active Distillation Teacher: CLAP [77] on CLIP ResNet-50 [68] Setup: Following original CLAP paper [77] Modification: Learning rate reduced from 0.1 to 0.01 for better convergence Table 2 presents the comprehensive implementation details for our experiments across three distinct settings. For ImageNet experiments, we employed DINO self-supervised ResNet-50 student model with CLIP ResNet-50 as the teacher, conducting 8 active learning rounds starting from 1-shot setting with 1,000 queries per round, using AdamW optimization with carefully tuned hyperparameters. Our experiments on 10 additional datasets utilized various student architectures (ResNet-18, MobileNetV2, and ViT-Tiny) with CLIP ResNet-50 as the teacher, extending to 16 active learning rounds with classcount-based query sizes and longer training epochs. For the Few-Shot Teacher Active Distillation setting, we implemented CLAP on CLIP ResNet-50 following the original papers methodology with reduced learning rate (0.01) to ensure better convergence in our specific experimental context. All experiments were conducted across 5 different random seeds without utilizing validation splits to simulate realistic low-data scenarios. Few-Shot Teacher Active Distillation. Recognizing the potential for improving teacher network performance, particularly in low-data regimes, we explored few-shot learning techniques to enhance the distillation process. After considering various approaches [39, 103, 102, 42, 104, 43, 58, 100, 21 72, 98, 48], we incorporated CLAP [77], method that notably does not require validation set [77, 61, 60], as our teacher model. For training the teacher, we followed the setting from the original CLAP paper [77], with only one modification: reducing the learning rate from 0.1 to 0.01 for better convergence in our setting. This configuration represents more realistic scenario wherein both teacher and student networks evolve simultaneously within the active distillation framework, better reflecting practical applications where pre-trained teacher models may not be available. Implementation of BADGE under DHO. BADGE [4] adopts hybrid approach to maximize uncertainty and diversity simultaneously, utilizing the gradient of the final linear classifier layer which has elements, where is the hidden dimension of the backbone feature and is the number of classes in the teacher model. However, DHO [41] extends beyond conventional single classifier architectures to dual classifiers that follow different optimization paths for labeled ground truth and teacher distillation signals. The dual classifier approach provides complementary information from both learning objectives, enhancing model performance through this specialized learning framework. In order to adopt DHO into the BADGE selection method, we manually calculate the gradients of both classifiers and concatenate them to form gradient representation with 2 elements. This concatenated gradient naturally extends the original BADGE selection algorithm while preserving its core k-means++ clustering mechanism. Algorithm 5 Class-Balanced Selection Require: Unlabeled pool D(u) = {x1, . . . , xm} Rd, model outputs fr = {fr(x1), fr(x2), . . . , fr(xn)}, labeled dataset D(l), query size K, number of classes Ensure: Selected set D(u) with = 1: Initialize = 2: Set yi = arg maxc [fr(xi)]c for all xi D(u) 3: Count nc = {xi D(l) : yi = c} for each [C] 4: Set weights wc = 1 nc 5: Compute Kc = round( if nc > 0, else wc = 1 K) for each (cid:80)C wc j=1 wj 6: Partition D(u) into D(u) 7: for each class [C] do Add min(Kc, D(u) 8: 9: end for 10: return = {xi D(u) : yi = c} ) random samples from D(u) to Implementation of Class-Balanced Selection. We implement the Class-Balanced Selection algorithm (Alg. 5) as our baseline method to address class distribution bias. The algorithm assigns pseudo-labels to unlabeled samples based on model predictions, analyzes class distribution in the labeled set, and computes inverse weights proportional to each classs representationgiving higher weights to underrepresented classes. It then allocates the query budget across classes according to these weights and randomly selects samples from each class partition. This approach effectively counteracts class imbalance by prioritizing underrepresented classes."
        },
        {
            "title": "C Summary of Datasets",
            "content": "We provide the details of datasets we used in our experiments in this section. Our experimental evaluation encompasses 11 diverse datasets, including ImageNet [73] and 10 additional datasets spanning various domains and classification challenges. These datasets represent broad spectrum of visual recognition tasks including fine-grained classification across multiple domains: generic object recognition [20], automobile classification [47], flower species identification [63], aircraft categorization [57], pet breed classification [65], food recognition [9], scene understanding [91], texture classification [15], satellite imagery analysis [33], and human action recognition [80]. Details of number of samples in each training, validation and test splits are illustrated in the Table 3. We adhere to the train, validation, and test splits established in prior work [41]. Our experimental protocol begins with 1-shot setting, where only single labeled example per class is available, with all remaining images in the training set treated as unlabeled. This approach extends to subsequent active learning rounds, where the unlabeled set consists of all training images not currently in the 22 Table 3: Overview of datasets used in our experiments. Note that validation split is not used during the active learning process, and is only shown for completeness. Dataset #Classes #Train #Val #Test Domain Caltech101 [20] DTD [15] EuroSAT [33] FGVCAircraft [57] Food101 [9] Flowers102 [63] OxfordPets [65] StanfordCars [47] SUN397 [91] UCF101 [80] ImageNet [73] 100 47 10 100 101 102 37 196 397 101 1, 4,128 2,820 13,500 3,334 50,500 4,093 2,944 6,509 15,880 7,639 1.28M 1,649 1,128 5,400 3,333 20,200 1,633 736 1,635 3,970 1,898 - 2,465 1,692 8,100 3,333 30,300 2,463 3,669 8,041 19,850 3,783 50,000 General objects General objects Textures Satellite Aircraft Food Plants Animals Vehicles Scenes Actions labeled set. To further simulate realistic constraints where validation data may be inaccessible, particularly in scenarios beginning with minimal labeled examples and progressively acquiring annotations, we conduct our experiments without utilizing the validation sets. Table 4: Class balance of datasets used in our experiments. The total number specified in the table is the number of samples of training splits as we do active selection on them. Dataset Mean Min Max Std Total Caltech101 [20] DTD [15] EuroSAT [33] FGVCAircraft [57] Food101 [9] Flowers102 [63] OxfordPets [65] StanfordCars [47] SUN397 [91] UCF101 [80] ImageNet [73] 41.28 60.00 1,350.00 33.34 500.00 40.13 79.57 33.21 40.00 75.63 1,281.17 16 60 1,000 33 500 20 74 19 40 58 732 400 60 1,500 34 500 129 80 54 40 97 1,300 56.81 0.00 174.80 0.48 0.00 22.20 1.26 3.47 0.00 10.72 70.22 4,128 2,820 13,500 3,334 50,500 4,093 2,944 6,509 15,880 7,639 1,281,167 We further presents class balance statistics for each dataset in the Table 4 to provide additional insight into inherent class imbalances, phenomenon that several researchers have addressed within active learning selection processes [3, 8, 37]. It is important to note that our research addresses distinct problem formulation; we primarily investigate teacher model bias rather than dataset bias. Notably, our findings demonstrate that the teacher bias emerges even in balanced datasets, showing the effectiveness of our approach in active knowledge distillation scenario."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Experiment on MobileNetV2 We further validate our methodology across diverse model architectures, including the lightweight MobileNetV2 [75]. The experimental results are presented in Fig. 13. For these experiments, we maintained the training configuration established for ResNet-18 [29], employing ResNet-50 CLIP teacher [68] with identical hyperparameters. Our empirical results demonstrate that PCoreSet consistently outperforms alternative selection methods in the active distillation setting, exhibiting performance trends that align with our primary ResNet-18 [29] experiments. 23 Figure 13: Results on 8 datasets using MobileNetV2 under zero-shot distillation across 16 rounds. D.2 Active Learning without Knowledge Distillation Figure 14: Results on 11 datasets including average of 10 datasets using ResNet-18 without distillation. The main purpose of our work is to propose utilizing foundational teacher model directly in the knowledge distillation process [34]. Thus, we conducted active learning experiments without knowledge distillation as our baseline to enable direct comparison. Using ResNet-18 [29] and following our main experimental protocol in Appendix B, we performed comparative analysis of knowledge distillation effects in the active learning process. The results are presented in Fig. 14. Interestingly, widely-adopted baselines such as uncertainty [35], coreset [76], and badge [4] do not consistently outperform random or class-balanced selection methods Alg. 5 on datasets such as FGVC [57], with our PCoreSet Alg. 2 achieving superior performance. This occurs despite the approximately equal distribution of classes in these datasets as shown in Table 4 in the Appendix C, suggesting 24 that maintaining probabilistic balance may be beneficial even in traditional active learning scenarios. However, PCoreSet does not demonstrate clear effectiveness in this traditional active learning setting, indicating that maintaining probabilistic diversity is specifically effective in distillation scenarios, where it serves to leverage teachers structured bias propagation. D.3 Few-shot Teacher Distillation Figure 15: Results of ResNet-18 student on 11 datasets including average of 10 datasets under few-shot distillation. In real-world applications, it is more natural to consider scenarios where both teacher and student models evolve concurrently. Specifically, we can apply few-shot learning methods to the generalist foundational teacher model with additional labeled samples acquired in each round, though our main experiments focused on zero-shot teachers to validate the effectiveness of integrating knowledge distillation into the active learning framework. Therefore, we extended our experiments to incorporate few-shot teachers that are updated with newly labeled samples in each acquisition round. For this purpose, we employed CLAP [77], few-shot learning method built upon CLIP that does not require validation setan appropriate choice for our setting where we assume extremely limited data availability (e.g., 1-shot training datasets) without access to validation data. The experimental results for student model performance are presented in Fig. 15, with corresponding teacher model performance shown in Fig. 16. In the few-shot teacher scenario, we observed consistent trends where probabilistic diversity effectively leverages bias, resulting in enhanced performance. Moreover, examining the few-shot teacher performance in Fig. 16 reveals that our PCoreSet outperforms alternative selection methods for teacher models as well. We attribute this to the fact that samples selected to leverage teacher bias naturally serve as beneficial examples for improving the teacher model, simultaneously enhancing performance on both the teacher and student sides. 25 Figure 16: Results of ResNet-50 few-shot teacher on 11 datasets including average of 10 datasets under few-shot distillation."
        },
        {
            "title": "E Additional Qualitative Results",
            "content": "E.1 Heatmap of Selected Samples We visualize the distribution of selected samples using heatmaps in Fig. 17. 26 Figure 17: The heatmap of output probability vectors from different selection strategies in the first active learning round using 10 datasets. 27 E.2 Qualitative Results on Selected Samples from PCoreSet We visualize samples selected by PCoreSet from various datasets in Figures 18 through 24, showcasing how PCoreSet selects diverse and representative examples across different visual domains. These visualizations provide qualitative evidence of PCoreSets effectiveness in identifying informative samples that help leverage the structured bias of the teacher model. Figure 18: Visualization of samples selected by PCoreSet for Caltech101 dataset. 28 Figure 19: Visualization of samples selected by PCoreSet for DTD dataset. Figure 20: Visualization of samples selected by PCoreSet for EuroSAT dataset. Figure 21: Visualization of samples selected by PCoreSet for Flowers102 dataset. 30 Figure 22: Visualization of samples selected by PCoreSet for Food-101 dataset. 31 Figure 23: Visualization of samples selected by PCoreSet for FGVC dataset. Figure 24: Visualization of samples selected by PCoreSet for Oxford-IIIT Pets dataset. 32 Figure 25: Visualization of samples selected by PCoreSet for Stanford Cars dataset. 33 Figure 26: Visualization of samples selected by PCoreSet for SUN397 dataset. 34 Figure 27: Visualization of samples selected by PCoreSet for UCF101 dataset."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST",
        "VUNO Inc."
    ]
}