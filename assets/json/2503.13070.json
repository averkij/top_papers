{
    "paper_title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
    "authors": [
        "Yihong Luo",
        "Tianyang Hu",
        "Weijian Luo",
        "Kenji Kawaguchi",
        "Jing Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Aligning generated images to complicated text prompts and human preferences is a central challenge in Artificial Intelligence-Generated Content (AIGC). With reward-enhanced diffusion distillation emerging as a promising approach that boosts controllability and fidelity of text-to-image models, we identify a fundamental paradigm shift: as conditions become more specific and reward signals stronger, the rewards themselves become the dominant force in generation. In contrast, the diffusion losses serve as an overly expensive form of regularization. To thoroughly validate our hypothesis, we introduce R0, a novel conditional generation approach via regularized reward maximization. Instead of relying on tricky diffusion distillation losses, R0 proposes a new perspective that treats image generations as an optimization problem in data space which aims to search for valid images that have high compositional rewards. By innovative designs of the generator parameterization and proper regularization techniques, we train state-of-the-art few-step text-to-image generative models with R0 at scales. Our results challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards play a dominant role in scenarios with complex conditions. We hope our findings can contribute to further research into human-centric and reward-centric generation paradigms across the broader field of AIGC. Code is available at https://github.com/Luo-Yihong/R0."
        },
        {
            "title": "Start",
            "content": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation Yihong Luo * 1 Tianyang Hu * 2 Weijian Luo 3 Kenji Kawaguchi 2 Jing Tang 4 1 5 2 0 2 7 1 ] . [ 1 0 7 0 3 1 . 3 0 5 2 : r Abstract Aligning generated images to complicated text prompts and human preferences is central challenge in Artificial Intelligence-Generated Content (AIGC). With reward-enhanced diffusion distillation emerging as promising approach that boosts controllability and fidelity of text-to-image models, we identify fundamental paradigm shift: as conditions become more specific and reward signals stronger, the rewards themselves become the dominant force in generation. In contrast, the diffusion losses serve as an overly expensive form of regularization. To thoroughly validate our hypothesis, we introduce R0, novel conditional generation approach via regularized reward maximization. Instead of relying on tricky diffusion distillation losses, R0 proposes new perspective that treats image generations as an optimization problem in data space which aims to search for valid images that have high compositional rewards. By innovative designs of the generator parameterization and proper regularization techniques, we train state-of-the-art few-step text-to-image generative models with R0 at scales. Our results challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards play dominant role in scenarios with complex conditions. We hope our findings can contribute to further research into human-centric and reward-centric generation paradigms across the broader field of AIGC. Code is available at https://github. com/Luo-Yihong/R0. 1. Introduction Thanks to their superb training stability and scalability, diffusion models (Song et al., 2020b; Ho et al., 2020; SohlDickstein et al., 2015) have become the dominating force *Equal contribution 4HKUST (GZ). Correspondence to: tang@ust.hk>. 1HKUST 2NUS 3Xiaohongshu Inc Jing Tang <jingFigure 1. Samples are taken from the corresponding papers of DiffInstruct++ and RG-LCM. It can be observed that there exist certain artifacts in samples, e.g., repeated text/objects in the background. We hypothesize this comes from reward hacking. in Artificial Intelligence-Generated Content (AIGC), showing wide applications in text-to-image generation (OpenAI, 2023; Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024; Zhang et al., 2023; Mou et al., 2024; Zhang et al., 2024). With breakthroughs in diffusion distillation (Song et al., 2023; Luo et al., 2023), few-step generative models (Salimans & Ho, 2022; Salimans et al., 2024; Yin et al., 2023; Luo et al., 2024b; 2025b) have caught great attention in image and video generation for their capability to generate high-quality images and videos on the fly with up to hundreds of times better efficiency than diffusion models. Existing works have demonstrated the importance of diffusion distillation and reinforcement learning in improving efficiency and preference alignment (Luo, 2023; Clark et al., Photo-realistic Generation from Rewards Figure 2. 4-step samples at 1024 resolution generated by R0. The R0 here is trained from SD3-medium purely by reward maximization. in DI++ are overwhelmingly driven by the reward term, relegating the diffusion distillation objective to secondary role. Similar phenomena can be widely observed in contemporary text-to-image generation models where various guidance modules for aligning conditions (Ho & Salimans, 2022; Bansal et al., 2023; Ma et al., 2023) are disproportionately amplified compared with the seemingly more important diffusion generation component. For instance, large classifierfree guidance (CFG) coefficients are indispensable (7.5 by default in Stable Diffusion (Rombach et al., 2022) and 100 in DreamFusion (Poole et al., 2022)). When dealing with the strong condition of multi-attribute object correspondence, the image produced by higher CFG is more semantically compliant. As also illustrated in (Luo, 2024), CFG can be seen as an implicit reward on text-image alignment. From high-level view, the concept of rewards can be very general. In this work, we extend the definition of reward to include any discriminative model that can judge the goodness of generated images, including both image classifiers, language-image alignment models, and Vision Language Models (VLMs). These findings prompt fundamental rethinking of strong conditional generation tasks such as text-to-image generation. In diffusion models, we model the conditional density (xy) through the decomposition (xy) (yx)P (x). How to better learn the marginal density (x) is usually the main focus of diffusion training while the condition part is Figure 3. Log Gradient norm curve of DI++ through training process. We use the best configuration reported in DI++ (Luo, 2024). 2023). Particularly, rewards trained from human preference data have been successfully utilized in diffusion distillation settings, resulting in significant fidelity boosts in few-step sampling scenarios (Li et al., 2024a; Luo, 2024). Despite the solid performance metrics, when closely examining the state-of-the-art reward-enhanced images and fewstep models trained with reward models, we found evidence of reward hacking certain artifacts or repeated objects in the background, as can be seen in Figure 1. This raises concerns about whether the generation process is overpowered by reward models. To further verify, we examine the gradient norms of the reward loss and the distillation loss in our re-implementation of Diff-Instruct++ (Luo, 2024). Figure 3 demonstrates the gradient norms of reward loss and divergence loss in DI++. It clearly shows that updates 2 Photo-realistic Generation from Rewards usually handled by CFG or external guidance in diffusion models (Bansal et al., 2023; Ma et al., 2023). However, as conditions get stronger and more complicated, the conditional distribution may be ill-conditioned to estimate. In modern text-to-image generation scenarios, the conditions are so rich that there are no two images with the same text in the training dataset. Beyond prolonged text descriptions, image generation is also required to align with implicit human preferences, which adds to the comprehensiveness of the generation condition. Therefore, we propose that under such strong conditions, more proper description of the conditional generation task is rewards maximization with regularization. In this new understanding, various rewards are the key, and what used to be perceived as critical for diffusion modeling only acts as form of regularization. There is phase transition to modern conditional generation tasks, from modeling the conditional distribution to regularized rewards maximization. Taking this perspective to its logical conclusion, we demonstrate that rewards themselves are sufficient for surprisingly good few-step generation performance, without relying on image data or complicated diffusion distillation losses. To validate our hypothesis, we introduce R0, novel approach that treats the generation process as an optimization problem in the space of images, searching for points where multiple reward functions are maximized. By carefully designing the generator parameterization and implementing effective regularization techniques, we achieve state-of-the-art results in few-step text-to-image generation with high visual quality (Figure 2). Specifically, our method outperforms previous methods that combine diffusion distillation and reward learning regarding visual quality and machine metrics (Figure 15 and Table 1), while being more robust to the reward choice (Figure 7). Our findings challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards are the more important part when dealing with strong conditions. We believe this shift of focus opens exciting avenues for further research into developing more effective reward models and integrating them into the generation process, potentially leading to even more powerful and efficient text-to-image systems in the future. 2. Preliminary Diffusion Models. Diffusion models (DMs) (SohlDickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) define forward diffusion process that progressively adds Gaussian noise to the data over steps: q(xtx) (xt; αtx, σ2 1 σt, is sampled from the data distribution, and σt specifies the noise schedule. The diffused samples can be directly generated as xt = αtx + σtϵ, where ϵ (0, I). I), where αt = The diffusion model fψ is trained by denoising. welltrained diffusion can approximate the score of the marginal diffused data distribution pt(xt) by xt log pt(xt) sψ(xt, t) = xtfψ(xt,t) . Samples from diffusion models can be drawn by simulating reversed generative SDE or ODEs (Song et al., 2020b;a; Lu et al., 2022; Zhang & Chen, 2022; Xue et al., 2023). σ2 In order to accommodate various conditions, diffusion models usually employ Classifier-free guidance (CFG), which estimates the gradient of the conditional probability log p(cxt). This gradient can be computed by : xt log p(cxt) = sψ(xtc) sψ(xt). Other guidance methods also show successes with improved performances (Ahn et al., 2024; Karras et al., 2024; Li et al., 2024b; Zhang et al., 2023; Luo et al., 2025a). Few-Step Diffusion Sampling. Despite significant advancements in training-free accelerated sampling of DMs (Lu et al., 2022; Zhao et al., 2023; Xue et al., 2024; Si et al., 2024; Ma et al., 2024), diffusion distillation (Luo, 2023) is dispensable for satisfactory few-step Sampling. Typically, the distilled sampler involves single or multiple transformations from random noise to images. Among various approaches, trajectory matching (Song et al., 2023; Kim et al., 2023; Song & Dhariwal; Geng et al., 2024; Salimans et al., 2024) and distribution matching (Yin et al., 2023; Luo et al., 2023; Zhou et al., 2024; Sauer et al., 2023; Xu et al., 2024; Luo et al., 2024b) are the most popular methods for diffusion distillation in few-step diffusion sampling. Preference Alignment for Text-to-Image Models. In recent years, significant efforts have been made to align diffusion models with human preferences. These approaches can be broadly categorized into three main lines of work: 1) fine-tuning DMs on carefully curated image-prompt datasets (Dai et al., 2023; Podell et al., 2023); 2) maximizing explicit reward functions, either through multi-step diffusion generation outputs (Prabhudesai et al., 2023; Clark et al., 2023; Lee et al., 2023) or policy gradient-based reinforcement learning (RL) methods (Fan et al., 2024; Black et al., 2023; Ye et al., 2024). 3) implicit reward maximization, exemplified by Diffusion-DPO (Wallace et al., 2024) and Diffusion-KTO (Yang et al., 2024), directly utilizes raw preference data without the need for explicit reward functions. Preference Alignment with Diffusion Distillation. Currently several methods (Luo, 2024; Luo et al., 2024a; Li et al., 2024a) have been proposed for developing preference align few-step text-to-image models. Their approach can be summarized as combination of distillation loss and reward loss: minθL(θ) = Ldistill(xθ) R(xθ, c), where xθ denotes the model samples and R(xθ, c) denotes the reward measure the alignment between xθ and condition Photo-realistic Generation from Rewards objective. max xRd (cid:88) ωiRi(x), (3.1) where ωis are positive weights for balancing the rewards. With this formulation, there are two important design choices to consider: How to parameterize the generator? One way is to directly employ non-parametric optimization-based methods that search the entire image space for each condition. Alternatively, we can build parametric model that learns to generate by push-forward transformations. How to effectively optimize? Directly maximizing the reward can lead to adversarial samples. We have to do regularization. This can be done by the diffusion-based model or other formats. We also found that assembling multiple rewards (intersection of rewards) can act as form of implicit regularization. We will demonstrate in later sections that with wellparameterized generation and proper regularization, we can achieve state-of-the-art text-to-image generation with only few reward functions, without relying on diffusion distillation. Figure 4. Comparison with Referee can play (Liu et al., 2024). The baseline samples are taken from their paper. It can be seen that our method has significantly better visual quality. c. The distillation loss can be consistency distillation (Li et al., 2024a) or reverse-KL divergence (Luo, 2024). The previous method either requires real data for training (Li et al., 2024a), or requires training an extra online score model (Luo, 2024). These components increase the complexity of the post-training, and solely serving as an overly complicated regularization. 3. Rewards are enough 3.2. Parameterization of Generator We have established that in strong-condition settings, conditional distribution may be ill-conditioned and rewards are more important for guiding the generation process. 3.1. Problem re-formulation As the condition gets stronger, the nature of the problem shifts more from modeling conditional distribution towards reward maximization, where lack of randomness is not necessarily problematic. With various reward functions in place, we can characterize the generation task as finding common modes of the rewards in the image domain, i.e., searching in the image domain for points where all rewards are maximized. Let R1(x), . . . , Rm(x) denote reward functions mapping Rd R. Each Ri defines high-dimensional landscape with many modes or local maxima. We can categorize each reward functions modes into genuinely good ones and artifacts. Our assumption is that the good ones are associated with the ground truth while the bad ones are random and not shared with other reward functions. Therefore, with diverse collection of different reward functions focusing on different aspects of the data, images agreed with all of them tend to be very good. Therefore, we can rethink the generation as an optimization problem with the following Our goal is to optimize images such that some alignment score is maximized. The most straightforward way is directly doing gradient ascend in the pixel space or some latent image space. In closely related work (Liu et al., 2024), they utilized the decoder from stable diffusion and optimized the latent for maximizing the alignment score given by VLMs. Even though this is an interesting proof of concept demonstration, their formulation has major downsides in real-world scenarios. (1) The optimization process is highly sensitive to initialization and hyperparameter choices, which is not robust. (2) To generate new image, they have to do hundreds of function evaluations (NFEs), which is computationally intensive. In our work, we found that directly utilizing GAN-style neural transformation from the score net works better and alleviates all the drawbacks of the optimization-based counterpart. The objective becomes max gG (cid:88) ωiRi(g(z)), where the choice of is critical for efficient optimization. We parameterize the generator gθ to accept noisy levels as input, creating network that progresses from noise to obtain clean samples in steps. The specific parameterization 4 Photo-realistic Generation from Rewards Figure 5. Samples diffusion models with 100 NFE and 7.5 CFG by varying the η in sampling. The samples are generated from the same initial noise. The prompt is pikachu, best quality. Figure 6. Cosine Similarity between DI gradient and Reward gradient. is as follows: gθ,η(z) = gθ,σ1,η gθ,σ2,η... gθ,σK ,η(z), xk σkϵθ(xk) (cid:112)1 σ2 gθ,σk,η(xk) = 1 σ (cid:113) k1 + σk1(cid:98)ϵη (3.2) where (cid:98)ϵη = ηϵθ(xk) + (cid:112)1 η2ϵ, σK := 1 and z, ϵ (0, I). We can use pre-trained score net as the initialization for enough model capacity and better initial images. We default regard gθ as whole in optimization. 3.3. Regularization Ideally, we should be able to effectively parameterize our optimization space such that we are only operating on the image manifold. However, reward hacking is serious concern where the resulting images resemble adversarial attacks on the reward models. To mitigate this issue, (Liu et al., 2024) proposed augmentation regularization, i.e., averaging the rewards over augmented images. Though this has proven effective in their setting when directly optimizing images, additional augmentations require additional queries of the 5 Figure 7. The prior distillation-based reward maximization methods collapse when the reward is chosen to be HPS v2.1. In contrast, our R0 still works well, benefiting from the proposed effective regularization technique. reward models, adding significant computation burden. More closely related to our generator parameterization, existing works such as RG-LCM (Li et al., 2024a) and DI++ (Luo, 2024) introduce reward maximization into diffusion distillation. These methods typically require training an additional score model for the distilled generator to ensure its closeness to the original model, which is memory and computation expensive. Surprisingly, we found that the reward gradient dominates throughout the training process, turning diffusion distillation into sort of costly regularization role (Figure 3). Moreover, Figure 6 shows that the cosine similarity between the distilled gradient in DI and the reward gradient decreases as training progresses, which to some extent indicates that the distilled gradient is moving towards regularization of reward hacking. However, despite the immense computation, the regularization effect of diffusion distillation loss can be inefficient or inconsistent. Specifically, the RG-LCM and DI++ employ HPS v2.0 (Wu et al., 2023) or Image Reward (Xu et al., 2023) as the reward loss in their original approach, but we discovered that Photo-realistic Generation from Rewards Figure 8. Training progress of various metrics over iterations. It can be seen that the normalized gradient shows better performance. This is evaluated on 1k prompts from HPS benchmark. if the reward is set to HPS v2.1, it would cause the generators of RG-LCM and DI++ to collapse into undesirable distributions, as shown in Figure 7. Motivated by this, we design and ablate our own regularization techniques, intended to be more direct and more effective. Weight Regularization. The primary role of regularization is to confine the generators distribution to the vicinity of the image manifold, thus avoiding reward hacking. We believe that the pre-trained weights Wψ of diffusion models capture the image manifold and offer an excellent initialization. Therefore, we propose to use the discrepancy between the generator weights and the pre-trained weights as part of the regularization loss. Lreg = Wψ Wθ2 2. (3.3) Besides, we can use LoRA fine-tuning to further serve as kind of regularization. This is very similar to the KL penalty in RL to control the update to be not too large (Ouyang et al., 2022). Random η-Sampling. The η plays an important role in our parameterized generator gθ,η. In particular, when η = 1, the generator is parameterized into the discrete format of DDIM sampler (Song et al., 2020a). In practice, we find that varying eta in diffusion sampling results in significant differences in the style and layout of the generated images, as shown in Figure 5. Motivated by this, we propose to parameterize our generator by imputing random η at each step for augmenting the generator distribution: gθ(z) = gθ,σ1,η1 gθ,σ2,η2... gθ,σK ,ηK (z), (3.4) where ηi [0, 1] and = 1, 2, .., K. After training, we can fix η [0, 1] in sampling. The design can effectively augment the generator distribution, thus delivering notable better performance. As can be observed in Figure 7, naively maximizing the implicit reward by CFG does not work. After applying the proposed Weight regularization and Random η-sampling, we can generate images with reasonable quality. Built upon Figure 9. Four-step samples generated by R0. We observed that the quality of the image monotonically increases with the gradual increment of the reward count. these regularization techniques, we found that our method integrated with HPS v2.1 can generate high-quality images, suffering less from artifacts compared to RG-LCM and DI++. This indicates the importance of proper regularization in reward maximization. However, we found that optimization with single explicit reward still suffers from the over-saturation issue. To address this, we suggest maximizing multiple explicit rewards (Figure 9). 3.4. Power of multiple rewards Thanks to plethora of preference data and powerful RL methods, we have access to diverse collection of learned reward models. Although learning from zoo of pre-trained models has long been studied in various vision tasks (Wortsman et al., 2022; Dong et al., 2022; Chen et al., 2023; Du et al., 2023), utilizing multiple reward models is relatively underexplored. The goal of our formulation is to find their common modes. However, all we have is gradient pointing towards the direction of the steepest climb. How to find the most effective direction is of critical importance. In practice, we found that optimizing multiple rewards with naive weighted combination may fail to maximize all rewards in the training process. As shown in Figure 8, the clip score does not converge to high value. To balance the learning of different rewards, we suggest gradient normalization, normalizing the gradients from each reward function and forming the average direction: max gG ωiRi(g(z)), where ωi = 6 (cid:98)ωi sg( Ri(g(z)) g(z) (3.5) . 2) Photo-realistic Generation from Rewards Figure 10. The complementary effect of different reward. We do not use random η sampling and set small weight regularization in training here to highlight the complementary effect between rewards. This is equal to setting dynamic weighting. Note that the normalizing operation is also performed for the implicit reward (CFG). Figure 8 shows that after applying the gradient normalization, we can maximize multiple rewards well. Figure 9 demonstrates that the image quality and image-text alignment become significantly better when we maximize multiple rewards. The Complementary Effect of Rewards. Since the common modes of multiple rewards tend to be more wellbehaved than those from signal rewards, the combination of multiple rewards in (3.5) also serves as kind of implicit regularization for the generator. To verify this, we maximize HPS, image reward, and clip score separately, without using random-η sampling. We find that individual rewards perform poorly, but when combined, they can generate images of reasonable quality as shown in Figure 10, which highly emphasizes the complementarity between rewards and their effectiveness as implicit regularization. The Indispensability of Weight Regularization. The regularization loss is the key to learning good generator. Without weight regularization, even if we employ multiple rewards, the generator can easily converge to an undesirable distribution, which underscores the importance of explicitly regularizing the generator, as shown in Figure 11. 4. Photo-Realistic Generation From Rewards 4.1. R0: Regularized Reward Maximization By incorporating various techniques outlined in Section 3, such as weight regularization, random-η sampling, and multiple reward maximization, we develop the R0 that can be trained by reward maximization for fast photo-realistic generation. We present the algorithm of R0 in Algorithm 1. Figure 11. The effect of regularization loss. Images are from the same initial noise. Algorithm 1 R0: Reward Maximization Require: Generator fθ, Pre-trained score fψ, Reward models {ri}, desired sampling steps K, total iterations Ensure: optimized generator fθ. 1: Initialize weights θ by ψ; 2: for 1 to do 3: 4: Sample noise ϵ from standard normal distribution; Sample with initialized noise ϵ from generator fθ by steps via random-η sampling. # Compute regularization loss Lreg θ ψ2 2 # Compute Rewards Lreward (cid:80) (cid:98)ωi Ri(x, y) 2) # Compute CFG guidance loss Sample xt q(xtx) cfg grad xt log pψ(cxt) Lcf xt sg(xt + cfg grad)2 2 # Compute Total Loss and Update Ltotal ωregLreg + Lreward + ωcf gLcf Update θ using θLtotal sg( Ri(x) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for The reward supervision in R0 is only at the final generated samples in an end-to-end fashion, which is similar to the DeepSeek R1 (DeepSeek-AI, 2025). To further enhance performance, we can incorporate extra supervision into intermediate generation steps to form R0+. 4.2. R0+: Additional Supervision on Intermediate Steps Although R0 is already capable of generating high-quality images in setting where only reward maximization is considered, the reward signal is only provided at the end. This leads to two issues: on one hand, the efficiency of reward maximization is low because there is no direct reward signal feedback during the process; on the other hand, the gradient 7 Photo-realistic Generation from Rewards Figure 12. Comparison on the convergence speed between R0 and R0+. This is evaluated on 1k prompts from HPS benchmark. Figure 14. The effect of high-resolution guidance loss. Images are from the same initial noise. Figure 12 shows that after applying the intermediate supervision, the convergence speed of R0+ has significantly improved compared to R0. Comparison on Generation Path Between R0 and R0+. R0 and R0+ are significantly different and it is interesting to explore the generation paths corresponding to these two models. In Figure 13, we can observe that R0+ generates much clearer images during the early process compared to R0. Although R0+ struggles with severe artifacts in the early stages of generation, surprisingly, these artifacts are gradually removed rather than accumulating throughout the process. In contrast, R0s path progresses from blurry to clear, with less affected by artifacts during the process. Tackling High-Resolution Generation. Existing reward functions for text-to-image generation are mostly trained on low-resolution inputs (e.g., 224224 pixels). This creates fundamental limitation: directly optimizing such rewards during high-resolution (e.g., 10241024 pixels) synthesis struggles to preserve fine-grained details. To address this, we propose training high-resolution classifier as complementary guidance signal. This classifier explicitly prioritizes perceptual quality in high-resolution outputs. Implicit High-Resolution Classifier. We can form highresolution classifier via the Bayesian rule: log p(HighResxt) = log p(xtHighRes)p(HighRes) p(xtLowRes) , (4.3) where xt denotes the noisy samples at timesteps t. gradient can be obtained as follows: Its xt log p(HighResxt) = xt log p(xtHighRes) xt log p(xtLowRes). (4.4) The xt log p(xtHighRes) can be directly replaced by the original diffusion model, since it has the capability to generate high-resolution images. For obtaining xt log p(xtLowRes), we can finetune the pre-trained diffusion over low-resolution data. By doing so, we can obFigure 13. Path comparison between R0 and R0+. The prompt is Two dogs, best quality. needs to be backpropagated through the entire generator, resulting in high memory usage and significant computational costs. To address the above issues, we propose learning the generator with intermediate supervision as well, forming R0+. We rewrite the generator as follows: xk+1 = sg(gθ,σk+2,ηk+2 gθ,σk+3,ηk+2... gθ,σK ,ηK (z)), xk+1 σk+1ϵθ(xk+1) (cid:113) xk = 1 σ2 + σk(cid:98)ϵηk , (cid:113) 1 σ2 k+1 x(k) 0 = xk+1 σk+1ϵθ(xk+1) (cid:113) 1 σ2 k+1 (4.1) where = 1, 2, ..., K, ηi [0, 1], (cid:98)ϵηk = ηkϵθ(xk+1) + (cid:112)1 η2 kϵ, σK := 1 and z, ϵ (0, I). The is randomly sampled during training. Then the loss for training becomes: max θ (cid:88) ωi Ri(x(k) 0 ) sg( Ri(x(k) 0 ) x(k) 0 Lreg(θ)+ω 2) Rcfg(xτ ) sg( Rcfg(xτ ) xτ , 2 (4.2) where xτ is obtained by forward diffusion process via q(xτ xk). By doing so, we can effectively supervise the intermediate process in the generation process. Photo-realistic Generation from Rewards Figure 15. Qualitative comparisons of R0 against distillation-based methods and diffusion base models on SD-v1.5 backbone. All images are generated by the same initial noise. We surprisingly observe that our proposed R0 has better image quality and text-image alignment compared to prior distillation-based reward maximization methods in 4-step text-to-image generation. Table 1. Comparison of machine metrics on text-to-image generation across state-of-the-art methods. We highlight the best among fast sampling methods. The FID is measured based on COCO-5k dataset. Model Backbone Steps HPS Aes CS FID Image Reward Base Model (Realistic-vision) Hyper-SD (Ren et al., 2024) RG-LCM (Li et al., 2024a) DI++ (Luo, 2024) R0 (Ours) R0+ (Ours) SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 Base Model R0 (Ours) SD3-Medium SD3-Medium 25 4 4 4 4 4 28 4 30.19 30.24 31.44 31.83 33.70 34.37 31.37 34.04 5.87 5.78 6.12 6.09 6.11 6. 5.84 6.27 34.28 31.49 29.14 29.22 32.13 32.97 34.13 33.89 29.11 30.32 52.01 55.52 33.79 37.53 28.72 31.97 0.81 0.90 0.67 0.72 1.22 1. 1.07 1.13 tain powerful Implicit classifier for high-resolution guidance. Figure 14 shows that after applying the implicit highresolution guidance proposed by us, the generated images are significantly clearer. 4.3. Evaluations To verify the effectiveness of the R0 and R0+, we compare them with previous distillation-based reward maximization methods. 9 Photo-realistic Generation from Rewards Figure 16. Qualitative comparison against competing methods and applications in downstream tasks. Baseline Models. We perform experiments on SDv1.5 (Rombach et al., 2022) and SD3-medium (Esser et al., 2024), including both UNet and DiT (Peebles & Xie, 2023) architectures, indicating the broad applicability of our approach. Experiment Setting. Training is performed on the JourneyDB dataset (Pan et al., 2023) using prompts, without requiring images, as our method is image-free. We primarily compare with previous distillation-based reward maximization methods. For Hyper-SD, we use the public checkpoint, while for RG-LCM and DI++, we reproduce their methods. Additionally, we assess prior optimization-based method that generates images by maximizing reward, specifically, the referee can play (Liu et al., 2024). Metric. We assess image quality using the Aesthetic Score (AeS) (Schuhmann et al., 2022), image-text alignment and human preference with the Human Preference Score (HPS) v2.1 (Wu et al., 2023) and Image Reward, and image-text alignment with the CLIP score (CS) (Hessel et al., 2021). Additionally, we use zero-shot FID on the COCO-5k dataset for more comprehensive evaluation. Qualitative Results. We present the qualitative results in Figure 15. It can be observed that our proposed R0 and R0+, even without using distillation loss, demonstrate better image quality and text-image alignment compared to existing distillation-based reward maximization methods. Quantitative Results. We present the quantitative results in Table 1. Our proposed R0 and R0+ achieve state-of-the-art (SOTA) performance across various text-image alignment and human preference metrics. Notably, our model also achieves zero-shot COCO FID comparable to the original model, which demonstrates that our method does not suffer from artifacts. Additional Application. We show our R0s capabilities in various tasks: 1) Image-to-Image Editing: As shown in Figure 16, R0 performs high-quality image editing (Meng et al., 2021) in just four steps; 2) Compatibility with ControlNet and Base Models: Illustrated in Figure 16, R010 LoRA is compatible with ControlNet (Zhang et al., 2023) and works seamlessly with various fine-tuned base models (e.g., Dreamshaper from SD 1.5), preserving their unique styles. 5. Discussion Our work challenges the conventional role of diffusion distillation in few-step text-to-image generation, demonstrating that regularized reward maximization alone achieves photorealistic results in just few steps. By reframing the conditional generation task as regularized high-dimensional search rather than density estimation, our proposed R0 eliminates diffusion distillation, enabling 4-step 1024px generation (Figure 2), matching or exceeding both the inference speed and sample quality of previous approaches without requiring training image dependencies. Our reward maximization framework shares similar style with Deepseek R1 (DeepSeek-AI, 2025), which has achieved unparalleled reasoning performance by posttraining base LLM solely through reinforced learning. This suggests promising cross-domain principle: carefully designed reward optimization can drive exceptional performance across AI generation modalities. Future work should focus on developing more sophisticated, perceptually aligned reward models specifically for image generation. Additionally, we only demonstrated with pretrained rewards in this work. It would be exciting to explore DPO-style (Rafailov et al., 2024) pipelines that directly fine-tune generators with raw preference data, and extending these principles to other generative domains like video and 3D content. By prioritizing rewards over traditional diffusion mechanisms, our approach not only improves generation efficiency but also offers more direct path to aligning generative models with human preferencesa critical advancement for more intuitive and useful AI-generated content. Photo-realistic Generation from Rewards"
        },
        {
            "title": "References",
            "content": "Ahn, D., Cho, H., Min, J., Jang, W., Kim, J., Kim, S., Park, H. H., Jin, K. H., and Kim, S. Self-rectifying diffusion sampling with perturbed-attention guidance, 2024. URL https://arxiv.org/abs/2403.17377. Bansal, A., Chu, H.-M., Schwarzschild, A., Sengupta, S., Goldblum, M., Geiping, J., and Goldstein, T. Universal In Proceedings of the guidance for diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 843852, 2023. Black, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Chen, Y., Hu, T., Zhou, F., Li, Z., and Ma, Z.-M. Explore and exploit the diverse knowledge in model zoo for domain generalization. In International Conference on Machine Learning, pp. 46234640. PMLR, 2023. Clark, K., Vicol, P., Swersky, K., and Fleet, D. J. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Dai, X., Hou, J., Ma, C.-Y., Tsai, S., Wang, J., Wang, R., Zhang, P., Vandenhende, S., Wang, X., Dubey, A., et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Dong, Q., Muhammad, A., Zhou, F., Xie, C., Hu, T., Yang, Y., Bae, S.-H., and Li, Z. Zood: Exploiting model zoo for out-of-distribution generalization. Advances in Neural Information Processing Systems Volume 35, 2022. Du, Y., Durkan, C., Strudel, R., Tenenbaum, J. B., Dieleman, S., Fergus, R., Sohl-Dickstein, J., Doucet, A., and Grathwohl, W. S. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International conference on machine learning, pp. 84898510. PMLR, 2023. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Lacey, K., Goodwin, A., Marek, Y., and Rombach, R. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv.org/abs/2403. 03206. models. Advances in Neural Information Processing Systems, 36, 2024. Geng, Z., Pokle, A., Luo, W., Lin, J., and Kolter, J. Z. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi, Y. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself, 2024. URL https://arxiv.org/ abs/2406.02507. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Lee, K., Liu, H., Ryu, M., Watkins, O., Du, Y., Boutilier, C., Abbeel, P., Ghavamzadeh, M., and Gu, S. S. Aligning textto-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Li, J., Feng, W., Chen, W., and Wang, W. Y. Reward guided latent consistency distillation. arXiv preprint arXiv:2403.11027, 2024a. Li, T., Luo, W., Chen, Z., Ma, L., and Qi, G.-J. Selfguidance: Boosting flow and diffusion generation on their own. arXiv preprint arXiv:2412.05827, 2024b. Liu, X., Hu, T., Wang, W., Kawaguchi, K., and Yao, Y. Referee can play: An alternative approach to conditional generation via model inversion. arXiv preprint arXiv:2402.16305, 2024. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022. Luo, W. comprehensive survey on knowledge distillation of diffusion models. arXiv preprint arXiv:2304.04262, 2023. Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P., Ghavamzadeh, M., Lee, K., and Lee, K. Reinforcement learning for fine-tuning text-to-image diffusion Luo, W. Diff-instruct++: Training one-step text-to-image generator model to align with human preferences. arXiv preprint arXiv:2410.18881, 2024. 11 Photo-realistic Generation from Rewards Luo, W., Hu, T., Zhang, S., Sun, J., Li, Z., and Zhang, Z. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Luo, W., Zhang, C., Zhang, D., and Geng, Z. Diff-instruct*: Towards human-preferred one-step text-to-image generative models. arXiv preprint arXiv:2410.20898, 2024a. Luo, Y., Chen, X., Qu, X., Hu, T., and Tang, J. You only sample once: Taming one-step text-to-image synthesis by self-cooperative diffusion gans, 2024b. URL https: //arxiv.org/abs/2403.12931. Luo, Y., Hu, T., Song, Y., Sun, J., Li, Z., and Tang, J. Adding additional control to one-step diffusion with joint distribution matching. arXiv preprint arXiv:2503.06652, 2025a. Luo, Y., Hu, T., Sun, J., Cai, Y., and Tang, J. Learning fewstep diffusion models by trajectory distribution matching. arXiv preprint arXiv:2503.06674, 2025b. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Prabhudesai, M., Goyal, A., Pathak, D., and Fragkiadaki, K. Aligning text-to-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Ma, J., Hu, T., Wang, W., and Sun, J. Elucidating the design space of classifier-guided diffusion generation. arXiv preprint arXiv:2310.11311, 2023. Ren, Y., Xia, X., Lu, Y., Zhang, J., Wu, J., Xie, P., Wang, X., and Xiao, X. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis, 2024. Ma, J., Xue, S., Hu, T., Wang, W., Liu, Z., Li, Z., Ma, Z.- M., and Kawaguchi, K. The surprising effectiveness of skip-tuning in diffusion sampling. arXiv preprint arXiv:2402.15170, 2024. Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., and Shan, Y. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 42964304, 2024. OpenAI. Dalle-2, 2023. URL https://openai.com/ dall-e-2. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=TIdIXIpzhoI. Salimans, T., Mensink, T., Heek, J., and Hoogeboom, E. Multistep distillation of diffusion models via moment matching. arXiv preprint arXiv:2406.04103, 2024. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 2527825294, 2022. Pan, J., Sun, K., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z., Wang, Y., Dai, J., Qiao, Y., and Li, H. Journeydb: benchmark for generative image understanding, 2023. Si, C., Huang, Z., Jiang, Y., and Liu, Z. Freeu: Free lunch in diffusion u-net. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 47334743, 2024. Photo-realistic Generation from Rewards Xue, S., Liu, Z., Chen, F., Zhang, S., Hu, T., Xie, E., and Li, Z. Accelerating diffusion sampling with optimized time steps. arXiv preprint arXiv:2402.17376, 2024. Yang, K., Tao, J., Lyu, J., Ge, C., Chen, J., Shen, W., Zhu, X., and Li, X. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024. Ye, Z., Chen, Z., Li, T., Huang, Z., Luo, W., and Qi, G.-J. Schedule on the fly: Diffusion time prediction for faster and better image generation. arXiv preprint arXiv:2412.01243, 2024. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023. Zhang, D., Zhang, Y., Gu, J., Zhang, R., Susskind, J., Jaitly, N., and Zhai, S. Improving gflownets for text-to-image diffusion alignment. arXiv preprint arXiv:2406.00633, 2024. Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Zhang, Q. and Chen, Y. models with exponential integrator. arXiv:2204.13902, 2022. Fast sampling of diffusion arXiv preprint Zhao, W., Bai, L., Rao, Y., Zhou, J., and Lu, J. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. Zhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference on Machine Learning, 2024. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 22562265. PMLR, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Song, Y. and Dhariwal, P. Improved techniques for trainIn The Twelfth International ing consistency models. Conference on Learning Representations. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020b. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Purushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik, N. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pp. 23965 23998. PMLR, 2022. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 1590315935, 2023. Xu, Y., Zhao, Y., Xiao, Z., and Hou, T. Ufogen: You forward once large scale text-to-image generation via diffusion gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81968206, 2024. Xue, S., Yi, M., Luo, W., Zhang, S., Sun, J., Li, Z., and Ma, Z.-M. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:7763277674, 2023."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST (GZ)",
        "NUS",
        "Xiaohongshu Inc"
    ]
}