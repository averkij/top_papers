{
    "paper_title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
    "authors": [
        "Atsuki Yamaguchi",
        "Maggie Mi",
        "Nikolaos Aletras"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks."
        },
        {
            "title": "Start",
            "content": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks Atsuki Yamaguchi, Maggie Mi, and Nikolaos Aletras School of Computer Science, University of Sheffield, United Kingdom {ayamaguchi1,zmi1,n.aletras}@sheffield.ac.uk 6 2 0 2 6 ] . [ 1 8 4 4 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks."
        },
        {
            "title": "Introduction",
            "content": "Language models (LMs) are pre-trained using causal language modeling (CLM), next-token prediction objective (Radford et al., 2018). While CLM pre-training equips LMs with world knowledge and general capabilities (Brown et al., 2020; Kojima et al., 2022), it does not optimize for linguistic competence, i.e., the capacity to comprehend and interpret diverse linguistic phenomena (Chomsky, 1965; Waldis et al., 2024). Consequently, LMs often behave as stochastic parrots (Bender et al., 2021). They mimic surface-level patterns without grasping the underlying linguistic scaffolding (Chang and Bergen, 2024; López-Otal et al., 2025). This phenomenon mirrors rote learning in humans (Plunkett and Marchman, 1993), where learners reproduce patterns without understanding the generative rules (Brown and Berko, 1960; Myles et al., 1998). We hypothesize that pre-training on language learning tasks, which require processing beyond 1Our code and models are available via https://github. com/gucci-j/l2t. Figure 1: L2T vs. standard CLM over raw text. raw sequence reconstruction, can stimulate the development of linguistic competence. Specifically, we target data-efficient acquisition of morphological, syntactic, and semantic knowledge. Inspired by human language acquisition, this approach encourages the development of structured representations that go beyond surface-level co-occurrence (Alishahi, 2011; Perfors et al., 2011; Culbertson and Adger, 2014). Evidence suggests that both humans and neural networks benefit from structured linguistic input during learning (Elman, 1993; Galke et al., 2024; Hu et al., 2025).2 We propose L2T, pre-training framework integrating Language Learning Tasks alongside standard CLM (Figure 1). Unlike instruction tuning requiring external supervision, L2T induces structure directly from raw text. By converting text into structured input-output pairs, it enables the model to learn dependencies and how to restructure information, providing explicit linguistic stimulation absent in standard CLM. We evaluate L2T by pretraining LMs on mixture of L2T data and raw text at scales of 500M and 1B parameters. Our main contributions are: (1) the L2T framework substantially improves linguistic competence up to 11.3% (2.8% on average) on BLiMP (Warstadt et al., 2020), while retaining general performance; and (2) empirical evidence that L2T accelerates this process. 2Appendix provides detailed review of related work."
        },
        {
            "title": "Type Tasks and Examples",
            "content": "Char Char Count: count characters (Text 4) Masked Char: reconstruct masked characters (c_ar char) Space: restore whitespace (Ilikea like a) Typo: correct synthetic typos (typ0 typo)"
        },
        {
            "title": "Links",
            "content": "B.1.1 B.1.2 B.1.3 B.1.4 Word Last: predict concluding phrase ([Text]A/B A) B.2.1 Masked Word: reconstruct tokens (I [MASK] am) B.2.2 Random: correct tokens (Sea am am) B.2.3 Shuffle: restore word order (w1w3w2 w1w2w3) B.2.4 Token Type: count linguistic categories ([Text]Digit Z) B.2.5 Sent Deletion: remove unrelated sentence (A [X] AC) Reordering: restore sentence flow (S3S1S2 S1S2S3) B.3.1 B.3. Disc Fill Middle: fill-in-the-middle for passages (P1 ? P3 P2) B.4.1 B.4.2 B.4.3 Half: complete the latter half ([Start]... [End]) One: generate from one-word prefix ([Word] [Text]) Table 1: The 14 L2T tasks with links to the detailed task definitions and examples in Appendix B."
        },
        {
            "title": "2 L2T: Language Learning Tasks",
            "content": "The L2T framework comprises 14 language learning tasks  (Table 1)  designed to provide explicit structural stimulation. Each task converts raw text segment into structured input-output pair (x, y), where is linguistically perturbed or queried input, and is the restored or analyzed output. While prior work has often relied on architectural modifications (Xu et al., 2021) or complex curricula (Hu et al., 2025; Oba et al., 2023; Mi, 2023), our aim is distinct: we investigate how structured stimulation impacts pre-training dynamics and the development of linguistic competence. We hypothesize that the rote learning of LMs stems in part from the single-task nature of CLM, which prioritizes surface-level statistics over structural understanding. In contrast, humans do not acquire language by optimizing single objective; they learn through multiple tasks (Spelke, 2022). To mirror this multi-task learning, L2T generates various tasks by leveraging raw text across four levels of linguistic granularity, providing supervision signals without external resources. Prior work (Carroll et al., 1992) shows that error correction in human learners improves morphological awareness, particularly for previously encountered forms. Accordingly, we use character-level (Char) tasks (e.g., Typo) to target subword features, discouraging surface-level matching while enhancing morphological awareness. Wordand sentence-level tasks (e.g., Shuffle) disrupt linear order, promoting structural inference over sequential statistics, consistent with word-reordering studies (Akhtar, 1999; Chomsky, 2002; Zhang and Clark, 2015). Finally, discourse-level (Disc) tasks (e.g., Fill Middle) require completion across longer context, supporting global coherence and ambiguity resolution. Similar completion tasks benefit human language learners (Keating, 2008). By integrating these diverse signals, L2T establishes the structural scaffolding required for linguistic competence, complementing world knowledge acquired through standard CLM."
        },
        {
            "title": "3.1 Pre-training Data Scenarios",
            "content": "We derive our pre-training corpora from the English FineWeb-Edu dataset (Penedo et al., 2024). To evaluate the L2T framework under different resource constraints, we establish two distinct data scenarios (Disjoint and Shared) to determine if L2T yields consistent benefits across varying volumes of unique source text. Following Cheng et al. (2024a), we fix the total training budget at 100B tokens. Crucially, this budget exceeds the computeoptimal thresholds defined by the Chinchilla scaling laws (Hoffmann et al., 2022), enabling us to evaluate performance in regime characterized by sufficient token availability relative to model size. Disjoint (Abundant Data). This configuration simulates regime where high-quality source text is plentiful. We partition an initial 100B token sample into two mutually exclusive subsets of equal size. The first subset is retained for standard CLM, while the second is used for generating L2T samples. As the combination of raw and L2T samples exceeds the budget, we subsample the mixture to adhere to the 100B token limit. The resulting Disjoint dataset comprises approximately 36B raw and 64B L2T tokens. This setup evaluates the impact of the framework when the model has access to high document diversity alongside structured tasks. Shared (Limited Data). This configuration targets settings where source text availability is constrained. We use fixed set of 42B source tokens both as raw text for CLM and L2T sample generation. This combination results in total of 100B tokens.3 This strategy decouples the impact of task structure from data diversity, testing if L2T can improve linguistic inductive bias without the introduction of unique new documents."
        },
        {
            "title": "3.2 Training Configuration",
            "content": "We pre-train from scratch 500M and 1B Qwen2.5based models (Qwen Team et al., 2025). These models use the Mistral (Jiang et al., 2023) tokenizer with 32K English-centric vocabulary. This 3While multiple transformations are possible, we assign single, random L2T task to each sample for brevity. 2 Semantics Morphology Syntax Data Quant . Raw L2T Raw L2T 0 0 5 . S . Raw L2T 1 . S Raw L2T 65.6 71.4 67.5 74.9 71.1 75.2 74.8 72.1 NPI 76.5 78.3 68.8 78.7 78.6 77.9 71.2 82.0 Ana Agr Irregul DN Agr SV Agr Arg Str Bind Ctrl Rais Ellips Fill Gap Island Overall 93.8 96.0 94.0 97.9 95.3 96. 95.7 97.0 91.8 93.2 92.3 96.2 89.5 92.7 90.8 94.2 93.1 92. 93.6 93.4 94.5 93.3 94.6 92.7 86.0 88.6 86.5 88.4 84.8 88. 87.4 88.4 78.2 78.6 78.6 80.5 78.0 79.4 78.2 80.0 72.8 75. 77.9 74.2 78.6 76.1 75.7 76.7 78.5 79.5 79.6 81.4 79.3 80. 78.5 82.0 87.1 86.0 88.5 86.2 86.7 86.5 86.5 84.5 77.5 75. 75.6 76.4 75.4 74.9 77.0 77.0 63.0 70.8 60.6 68.1 60.2 71. 61.7 68.6 78.6 80.2 78.1 80.9 79.0 80.8 78.9 81.2 Table 2: Linguistic competence on BLiMP. Green highlights better performance of L2T (ours) over Raw. replaces the 130K vocabulary of Qwen2.5 to reduce compute cost. We set the sequence length to 2,048 and batch size to 256. To ensure fair comparison, we compute loss on all tokens."
        },
        {
            "title": "3.3 Evaluation Framework",
            "content": "Baselines. We pre-train models (Raw; 500M and 1B) on 100B and 42B raw tokens for the Disjoint and Shared settings, respectively. For fair comparison, we use budget of 100B tokens in all settings. Hence, the Shared Raw model trains over the same 42B tokens multiple times to meet this quota.4 Linguistic Competence. We use BLiMP to measure linguistic competence following Shah et al. (2024). This benchmark covers 12 linguistic phenomena across 67 datasets within: Semantics, Morphology, and Syntax. Appendix C.3 provides details and their associated linguistic constraints. We prioritize zero-shot log-likelihood comparisons of minimal pairs to avoid prompting and probing biases (Hu and Levy, 2023; Alajrami and Aletras, 2022; Belinkov, 2022; Levy et al., 2023). General Benchmarks. We use: Reading Comprehension (RC; RACE, SciQ, LogiQA); Commonsense Reasoning (CR; ARC-Easy, COPA, OpenBookQA, SIQA, HellaSwag); and Language Modeling (LAMBADA). We also include ReCoRD, which combines RC and CR.5 Since the volume of raw text remains constant across settings, this aims to confirm whether L2T tasks are complementary or being detrimental to CLM on raw text."
        },
        {
            "title": "4.1 Linguistic Competence",
            "content": "Table 2 shows the performance on BLiMP for all models. L2T consistently enhances linguistic 4Baselines and L2T models use the same hyperparameters (Appendix C.2). 5Evaluation metrics include normalized accuracy for ARC, HellaSwag, LogiQA, OBQA, and SciQ; F1 for ReCoRD; and standard accuracy for the remaining tasks. We apply five-shot prompting for ARC, LogiQA, OBQA, and SIQA, while using zero-shot for the rest. We use single deterministic run. competence. In the Disjoint setup, L2T models outperform the Raw baselines at both 500M and 1B scales, achieving overall scores of 80.2 (500M) and 80.8 (1B), respectively, compared to Raw baseline scores of 78.6 and 79.0. Notably, this performance gap widens in the Shared scenario, where L2T models surpass the Raw baselines by 2.8 and 2.3 points. This reinforces our human learning analogy (2): while Shared Raw mimics rote learning, L2T mirrors multi-task acquisition lenses to the by applying diverse structural same data. Our results confirm that linguistic competence depends not just on data volume, but on the diversity of signals applied during training. We further analyze the effects across different linguistic subfields and phenomena. L2T consistently enhances linguistic competence across model sizes and data scenarios, improving six phenomena across all subfields. Island effects (Island) exhibit the most substantial gains, ranging from 6.9 (1B; Shared) to 11.3 points (1B; Disjoint). Detailed analysis (Appendix D) reveals that nearly all L2T tasks contribute to this gain. This suggests that structured tasks across varying granularity, rather than solely local transformations, provide structural scaffolding for capturing long-distance dependencies. Conversely, L2T offers no improvement for determiner-noun agreement (DN Agr) or ellipsis (Ellips). These results serve as diagnostic of our method: performance for these phenomena appears to reach saturation, as Raw models achieve high accuracy (93+ and 86+, respectively) through implicit learning of frequent patterns (Tajeddin and Rahimi, 2017; Khullar et al., 2020). This result aligns with the finding of Shah et al. (2024), who report that LMs are substantially accurate on morphological tasks followed by syntactic and semantic tasks. However, limits to this scaffolding remain. Complex phenomena, such as Filler-Gap dependencies (Fill Gap), show no improvement from individual tasks (see Appendix D), indicating that capturing such structures is more challenging and may require targeted discourse-level objectives. 3 Reading Comprehension RC+CR Commonsense Reasoning Language Modeling Data RACE SciQ LogiQA ReCoRD ARC COPA OBQA PIQA SIQA HellaSwag LAMBADA . M 0 0 5 . S . B 1 . S Raw L2T Raw L2T Raw L2T Raw L2T Random guessing 30.0 29. 28.3 29.5 29.6 29.8 29.5 30.2 25.0 67.9 67.3 66.1 63. 72.4 70.2 68.0 68.0 25.0 23.8 26.6 25.0 26.9 24.4 27. 26.1 26.7 25.0 62.9 60.7 62.2 62.0 64.9 63.1 65.3 63. 19.1 57.4 56.4 56.6 57.7 60.4 58.4 60.6 56.4 25. 66.0 64.0 66.0 64.0 61.0 66.0 69.0 65.0 50.0 31.4 30. 29.0 29.6 31.0 32.0 31.2 30.0 25.0 63.9 62.9 64.0 65. 65.9 65.9 66.3 64.3 50.0 38.1 38.3 38.1 38.5 38.6 38. 38.4 37.4 33.3 37.8 35.2 37.1 36.2 39.7 37.8 39.4 37. 25.0 30.6 28.2 30.6 30.2 32.7 30.9 31.9 31.3 0. Table 3: General benchmark performance. Green denotes better performance of L2T (ours) over Raw. Disjoint. In the Disjoint setup, L2T models maintain competitive performance relative to Raw. The 500M model shows an average drop of 0.87 points, while the 1B model shows negligible average difference of 0.07 points. This confirms that general reasoning performance remains stable when the model retains access to large volume of unique documents. Shared. In the Shared configuration, the impact of L2T varies by model scale. While the 500M model achieves an average improvement of 0.15 points, the 1B model experiences performance drop of 1.38 points, primarily in CR tasks such as ARC (-4.2). This divergence highlights tension between linguistic structure learning and factual reinforcement (Fedorenko and Varley, 2016). The Shared Raw baseline encounters the same 42B tokens multiple times, which likely reinforces the retention of factual world knowledge. In contrast, the L2T setup replaces portion of these repetitions with structured tasks. For the 1B model, the results indicate that the benefits of linguistic stimulation do not fully compensate for the reduced exposure to raw sequences in knowledge-intensive benchmarks. Thus, to perform well on these benchmarks, larger models need to balance structural induction with factual consolidation."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented L2T, pre-training framework that integrates language learning tasks alongside standard next-token prediction. By necessitating the extraction and restructuring of information, these tasks demand processing beyond rote learning, effectively stimulating faster and improved linguistic competence development. Future work will extend L2T to multilingual settings to investigate the learning behavior across languages. 6The analysis in Appendix supports this conclusion by showing that performance on knowledge-intensive benchmarks is sensitive to the ratio of raw text. Figure 2: Accuracy by linguistic subfield in BLiMP between Raw and L2T across model sizes and training steps using Disjoint Raw and L2T data. Finally, Figure 2 illustrates the development of linguistic competence throughout the pre-training process. Across subfields, substantial improvements occur during the initial 20-30B tokens of training, with gains continuing and leveling off around 50B tokens. These results reveal similar trajectories as those identified by Shah et al. (2024) for Pythia models trained on raw text. However, L2T models consistently outperform Raw models across all linguistic subfields from the earliest stages of training, evident as early as 5B tokens. This indicates that L2T effectively boosts learning in the window of maximal development, the period where the model improves its cognitive abilities linearly (Shah et al., 2024). At this 5B token mark, L2T models show an average performance advantage over Raw models ranging from 3.3 (Syntax) to 6.5 (Semantics) for the 500M scale, and from 1.0 (Syntax) to 4.5 (Semantics) for the 1B scale. This faster acquisition indicates that L2T tasks function as force multiplier for efficiency by introducing linguistic inductive biases complementary to CLM. By effectively stimulating language learning, L2T establishes durable advantage, preserving performance gap that the implicit acquisition of standard Raw models cannot close."
        },
        {
            "title": "4.2 General Benchmarks",
            "content": "Table 3 compares the performance of L2T and Raw models on general benchmarks."
        },
        {
            "title": "Limitations",
            "content": "Task Scope. The current design of the L2T framework focuses largely on constraints at the level of the sentence. While this effectively targets local syntactic and semantic dependencies, it does not explicitly address broader linguistic phenomena. Future work could expand to include more discourse-level and cross-sentence tasks. Such tasks would allow the model to capture long-range dependencies more effectively. Model Scale. Due to constraints on computational resources, we restrict our evaluation to models at the 500M and 1B parameter scales. We follow the protocol of Cheng et al. (2024a) by training our models from scratch with reasonable academic budget of 100B tokens. We acknowledge that the effects of different pre-training objectives might vary at larger scales (e.g., 10B+ parameters), and their investigation is great opportunity that should be explored by community members that have access to the required compute. We speculate that the structural scaffolding of L2T could be particularly beneficial during the initial stages of pretraining for larger models, as observed in Shah et al. (2024) and Hu et al. (2025). However, the drop in reasoning performance of the Shared 1B model suggests that larger models are likely more sensitive to the size of raw text. Consequently, the application of L2T at scale likely necessitates careful balance between the volume of raw text and structured tasks to preserve world knowledge, and it may benefit from the use of curriculum learning (e.g., confining L2T data to the initial stage of pre-training) (Bengio et al., 2009; Platanios et al., 2019)."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Anthony Hughes for the valuable feedback. We acknowledge (1) the use of the University of Oxford Advanced Research Computing (ARC) facility: http://dx.doi.org/ 10.5281/zenodo.22558 and (2) the Isambard-AI National AI Research Resource (AIRR) (McIntoshSmith et al., 2024), which is operated by the University of Bristol and is funded by the UK Governments Department for Science, Innovation and Technology (DSIT) via UK Research and Innovation; and the Science and Technology Facilities Council [ST/AIRR/I-A-I/1023]. AY is supported by the Engineering and Physical Sciences Research Council (EPSRC) [grant number EP/W524360/1] and the Japan Student Services Organization (JASSO) Student Exchange Support Program (Graduate Scholarship for Degree Seeking Students). MM is supported by the UKRI AI Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]."
        },
        {
            "title": "References",
            "content": "Nameera Akhtar. 1999. Acquiring basic word order: evidence for data-driven learning of syntactic structure. Journal of Child Language, 26(2):339356. Ahmed Alajrami and Nikolaos Aletras. 2022. How does the pre-training objective affect what large language models learn about linguistic properties? In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 131147, Dublin, Ireland. Association for Computational Linguistics. Ahmed Alajrami, Katerina Margatina, and Nikolaos Aletras. 2023. Understanding the role of input token characters in language models: How does information loss affect performance? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90859108, Singapore. Association for Computational Linguistics. Afra Alishahi. 2011. Computational Modeling of Human Language Acquisition, 1st ed. 2011. edition. Synthesis Lectures on Human Language Technologies. Springer International Publishing : Imprint: Springer, Cham. Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, and 30 others. 2024. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS 24, page 929947, New York, NY, USA. Association for Computing Machinery. Stéphane Aroca-Ouellette and Frank Rudzicz. 2020. On Losses for Modern Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 49704981, Online. Association for Computational Linguistics. Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. 2022. Efficient training of lanPreprint, guage models to fill arXiv:2207.14255. in the middle. 5 Yonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207219. Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, page 610623, New York, NY, USA. Association for Computing Machinery. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 09, page 4148, New York, NY, USA. Association for Computing Machinery. Steven Bird and Edward Loper. 2004. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214217, Barcelona, Spain. Association for Computational Linguistics. Roger Brown and Jean Berko. 1960. Word association and the acquisition of grammar. Child Development, 31(1):114. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Susanne Carroll, Merrill Swain, and Yves Roberge. 1992. The role of feedback in adult second language acquisition: Error correction and morphological generalizations. Applied Psycholinguistics, 13(2):173198. Tyler A. Chang and Benjamin K. Bergen. 2024. Language model behavior: comprehensive survey. Computational Linguistics, 50(1):293350. Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa Kozareva. 2022. Improving in-context few-shot learning via self-supervised training. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35583573, Seattle, United States. Association for Computational Linguistics. Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. 2024a. Instruction pretraining: Language models are supervised multitask learners. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 25292550, Miami, Florida, USA. Association for Computational Linguistics. Daixuan Cheng, Shaohan Huang, and Furu Wei. 2024b. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations. Jiali Cheng and Hadi Amiri. 2025. Linguistic blind spots of large language models. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 117, Albuquerque, New Mexico, USA. Association for Computational Linguistics. Cheng-Han Chiang and Hung-yi Lee. 2022. On the transferability of pre-trained language models: study from artificial datasets. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):1051810525. Noam Chomsky. 1965. Aspects of the Theory of Syntax. The MIT Press. Noam Chomsky. 2002. Syntactic structures. Walter de Gruyter. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Preprint, arXiv:1803.05457. Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu. 2022. LERT: linguistically-motivated pre-trained language model. Preprint, arXiv:2211.05344. Jennifer Culbertson and David Adger. 2014. Language learners privilege structured meaning over surface frequency. Proceedings of the National Academy of Sciences, 111(16):58425847. Jillian Da Costa and Rui Chaves. 2020. Assessing the ability of transformer-based neural models to represent structurally unbounded dependencies. In Proceedings of the Society for Computation in Linguistics 2020, pages 1221, New York, New York. Association for Computational Linguistics. Tri Dao. 2023. FlashAttention-2: Faster attention with better parallelism and work partitioning. Preprint, arXiv:2307.08691. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Luca Di Liello, Matteo Gabburo, and Alessandro Moschitti. 2022. Effective pretraining objectives for transformer-based autoencoders. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 55335547, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. 6 Yassir El Mesbahi, Atif Mahmud, Abbas Ghaddar, Mehdi Rezagholizadeh, Phillippe Langlais, and Prasanna Parthasarathi. 2023. On the utility of enhancing BERT syntactic bias with token reordering pretraining. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 165182, Singapore. Association for Computational Linguistics. Jeffrey L. Elman. 1993. Learning and development in neural networks: the importance of starting small. Cognition, 48(1):7199. Evelina Fedorenko and Rosemary Varley. 2016. Language and thought are not the same thing: Evidence from neuroimaging and neurological patients. Annals of the New York Academy of Sciences, 1369(1):132 153. Epub 2016 Apr 20. Lukas Galke, Yoav Ram, and Limor Raviv. 2024. Deep neural networks and humans both benefit from compositional language structure. Nature communications, 15(1):1081613. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2023. framework for few-shot language model evaluation. Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 394398, Montréal, Canada. Association for Computational Linguistics. Yuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang. 2022. Learning instructions with unlabeled data for zero-shot cross-task generalization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 16171634, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, and Yujiu Yang. 2024. Mitigating reversal curse in large language models via semantic-aware permutation training. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1145311464, Bangkok, Thailand. Association for Computational Linguistics. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, and 3 others. 2022. Training compute-optimal large language models. arXiv, abs/2203.15556. Jennifer Hu and Roger Levy. 2023. Prompting is not substitute for probability measurements in large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 50405060, Singapore. Association for Computational Linguistics. Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, and Tal Linzen. 2025. Between circuits and Chomsky: Pre-pretraining on formal languages imparts linguistic biases. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9691 9709, Vienna, Austria. Association for Computational Linguistics. Philip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth. 2021. BabyBERTa: Learning more grammar with small-scale child-directed language. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624646, Online. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Gregory D. Keating. 2008. Task effectiveness and word learning in second language: The involvement load hypothesis on trial. Language Teaching Research, 12(3):365386. M. G. Kendall. 1938. new measure of rank correlation. Biometrika, 30(1/2):8193. Payal Khullar, Kushal Majmundar, and Manish Shrivastava. 2020. NoEl: An annotated corpus for noun ellipsis in English. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 3443, Marseille, France. European Language Resources Association. Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 2219922213. Curran Associates, Inc. Adhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen Clark, and Phil Blunsom. 2019. Scalable syntaxaware language models using knowledge distillation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3472 3484, Florence, Italy. Association for Computational Linguistics. 7 Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785 794, Copenhagen, Denmark. Association for Computational Linguistics. Tal Levy, Omer Goldman, and Reut Tsarfaty. 2023. Is probing all you need? indicator tasks as an alternative In Findings of the to probing embedding spaces. Association for Computational Linguistics: EMNLP 2023, pages 52435254, Singapore. Association for Computational Linguistics. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, and 13 others. 2021. Datasets: community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175184, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2021. LogiQA: challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence, IJCAI20. Miguel López-Otal, Jorge Gracia, Jordi Bernad, Carlos Bobed, Lucía Pitarch-Ballesteros, and Emma Anglés-Herrero. 2025. Linguistic interpretability of transformer-based language models: systematic review. Preprint, arXiv:2504.08001. Simon McIntosh-Smith, Sadaf Alam, and Christopher Woods. 2024. Isambard-AI: leadership class supercomputer optimised specifically for artificial intelligence. Preprint, arXiv:2410.11199. Maggie Mi. 2023. Mmi01 at the BabyLM challenge: Linguistically motivated curriculum learning for pretraining in low-resource settings. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 269278, Singapore. Association for Computational Linguistics. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium. Association for Computational Linguistics. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8318 8334, Dublin, Ireland. Association for Computational Linguistics. Florence Myles, Janet Hooper, and Rosamond Mitchell. 1998. Rote or rule? exploring the role of formulaic language in classroom foreignlanguage learning. Language Learning, 48(3):323364. Miyu Oba, Akari Haga, Akiyo Fukatsu, and Yohei Oseki. 2023. BabyLM challenge: Curriculum learning based on sentence complexity approximating language acquisition. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 290297, Singapore. Association for Computational Linguistics. Vishakh Padmakumar, Leonard Lausen, Miguel Ballesteros, Sheng Zha, He He, and George Karypis. 2022. Exploring the role of task transferability in largescale multi-task learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25422550, Seattle, United States. Association for Computational Linguistics. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534, Berlin, Germany. Association for Computational Linguistics. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Amy Perfors, Joshua B. Tenenbaum, and Terry Regier. 2011. The learnability of abstract syntactic principles. Cognition, 118(3):306338. Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, and Tom Mitchell. 2019. Competence-based curriculum learning for neural In Proceedings of the 2019 machine translation. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 11621172, Minneapolis, Minnesota. Association for Computational Linguistics. Aaron Mueller, Jason Krone, Salvatore Romeo, Saab Mansour, Elman Mansimov, Yi Zhang, and Dan Roth. 2022. Label semantic aware pre-training for fewshot text classification. In Proceedings of the 60th Kim Plunkett and Virginia Marchman. 1993. From rote learning to system building: acquiring verb morphology in children and connectionist nets. Cognition, 48(1):2169. 8 Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, and 24 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Alec Radford, Karthik Narasimhan, Tim Salimans, and Improving language underIlya Sutskever. 2018. standing by generative pre-training. Ryokan Ri and Yoshimasa Tsuruoka. 2022. Pretraining with artificial language: Studying transferable knowledge in language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7302 7315, Dublin, Ireland. Association for Computational Linguistics. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842866. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, and 21 others. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: ComIn monsense reasoning about social interactions. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463 4473, Hong Kong, China. Association for Computational Linguistics. Raj Sanjay Shah, Khushi Bhardwaj, and Sashank Varma. 2024. Development of cognitive intelligence in pretrained language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 96329657, Miami, Florida, USA. Association for Computational Linguistics. Elizabeth Spelke. 2022. What babies know: Core knowledge and composition. Oxford University Press. Zia Tajeddin and Ali Rahimi. 2017. conversation analysis of ellipsis and substitution in global business english textbooks. International Journal of Society, Culture &; Language, 5(1):114. Andreas Waldis, Yotam Perlitz, Leshem Choshen, Yufang Hou, and Iryna Gurevych. 2024. Holmes benchmark to assess the linguistic competence of language models. Transactions of the Association for Computational Linguistics, 12:16161647. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association for Computational Linguistics, 8:377 392. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong, Wanjun Zhong, Xiaojun Quan, Daxin Jiang, and Nan Duan. 2021. Syntax-enhanced pre-trained model. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 54125422, Online. Association for Computational Linguistics. Aditya Yadavalli, Alekhya Yadavalli, and Vera Tobin. 2023. SLABERT talk pretty one day: Modeling second language acquisition with BERT. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1176311777, Toronto, Canada. Association for Computational Linguistics. Atsuki Yamaguchi, George Chrysostomou, Katerina Margatina, and Nikolaos Aletras. 2021. Frustratingly simple pretraining alternatives to masked language modeling. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 31163125, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Atsuki Yamaguchi, Hiroaki Ozaki, Terufumi Morishita, Gaku Morio, and Yasuhiro Sogawa. 2023. How does the task complexity of masked pretraining objectives In Findings of affect downstream performance? the Association for Computational Linguistics: ACL 2023, pages 1052710537, Toronto, Canada. Association for Computational Linguistics. 9 Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. Preprint, arXiv:1810.12885. Shuai Zhang, Wang Lijie, Xinyan Xiao, and Hua Wu. 2022. Syntax-guided contrastive learning for preIn Findings of the Assotrained language model. ciation for Computational Linguistics: ACL 2022, pages 24302440, Dublin, Ireland. Association for Computational Linguistics. Yue Zhang and Stephen Clark. 2015. Discriminative syntax-based word ordering for text generation. Computational Linguistics, 41(3):503538."
        },
        {
            "title": "Appendix Directory",
            "content": "Appendix A: Related Work Appendix B: Details on L2T: Language Learning Tasks Character-level Word-level Sentence-level Discourse-level Appendix C: Extended Experimental"
        },
        {
            "title": "Setup",
            "content": "Pre-training Data Construction Implementation and Training Details Evaluation Details:"
        },
        {
            "title": "Benchmark",
            "content": "Appendix D: Effectiveness of Individual Tasks Appendix E: Mixing Ratio of Raw and L2T Data Appendix F: Qualitative Analysis Appendix G: License Appendix H: Use of Generative AI"
        },
        {
            "title": "A Related Work",
            "content": "A.1 Pre-training and Linguistic Competence Previous work has questioned the depth of linguistic understanding in LMs (Rogers et al., 2020; Bender et al., 2021; Chang and Bergen, 2024; LópezOtal et al., 2025). Empirical analysis reveals that despite proficiency in generating coherent text, models often fail to process distant and complex cooccurrences, such as rhetorical relations (Waldis et al., 2024), and fine-grained linguistic annotation tasks, including noun structures at the phrase level (Cheng and Amiri, 2025). We argue that these shortcomings occur because training signals from standard CLM lack the structural scaffolding necessary for the model to move beyond surface-level statistics. Consequently, understanding how LMs acquire linguistic knowledge during pre-training remains key research area. Evidence indicates that the characteristics of pre-training data play crucial role. 10 For instance, child-directed language aids grammar induction more effectively than conventional text (Huebner et al., 2021; Yadavalli et al., 2023). Likewise, pre-training on artificial language data designed to model specific structures, such as nesting dependencies, transfers knowledge successfully to natural language tasks (Chiang and Lee, 2022; Ri and Tsuruoka, 2022; Hu et al., 2025). Further, Alajrami and Aletras (2022) suggest that the architecture of the model and pre-training data influence linguistic acquisition more than the objective function. These findings collectively support the datacentric approach of this work to enhance linguistic competence by providing the explicit guidance required to resolve complex linguistic dependencies. A.2 Enhancing Linguistic Competence of"
        },
        {
            "title": "LMs",
            "content": "Prior research explores various strategies to improve the linguistic competence of LMs, typically involving architectural modifications (Xu et al., 2021), auxiliary tasks or objectives (Kuncoro et al., 2019; Xu et al., 2021; Zhang et al., 2022; Mueller et al., 2022; Cui et al., 2022; Guo et al., 2024), curriculum learning (Hu et al., 2025), or data transformation (Guo et al., 2024). However, research specifically targeting decoderbased LMs remains limited (López-Otal et al., 2025). Existing methods often focus on isolated phenomena or rely on external resources. For instance, Guo et al. (2024) introduce semantic-aware permutation to mitigate the reversal curse, but this requires an auxiliary LM and focuses on continual pre-training. Similarly, Hu et al. (2025) utilize formal language data to capture hierarchical dependencies; while this improves generalization, it functions primarily as warm-up phase using synthetic structures. In contrast, L2T targets broad linguistic abilities in decoder-based LMs without auxiliary models or external knowledge. By training from scratch, we isolate the impact of our data-centric intervention, demonstrating how structured tasks alone can stimulate the development of linguistic competence during pre-training. A.3 Self-Supervised Objectives and Data Transformation for Pre-training Early work explores various self-supervised objectives, particularly for encoder-based models, to improve downstream performance and computational efficiency, and to interpret learned representations (Aroca-Ouellette and Rudzicz, 2020; Yamaguchi et al., 2021; Di Liello et al., 2022; Yamaguchi et al., 2023; Alajrami and Aletras, 2022; El Mesbahi et al., 2023; Alajrami et al., 2023). More recently, researchers have focused on the transformation of raw text into structured inputoutput pairs suitable for CLM. These methods include generation of pairs based on predefined self-supervised tasks (Chen et al., 2022), curation of instruction-response pairs using auxiliary models (Cheng et al., 2024a), creation of pseudolabeled data (Gu et al., 2022), or adaptation of domain text into task formats (Cheng et al., 2024b). These approaches typically aim to enhance general capabilities, improve few-shot learning, or adapt models to specific tasks or domains, often through continual pre-training (Chen et al., 2022). Our work aligns with the use of data transformation and self-supervision but differs fundamentally in approach and objective. Unlike previous strategies that rely on external models or task-specific datasets for transformation (Cheng et al., 2024a; Gu et al., 2022; Cheng et al., 2024b), our data transformation applies intrinsically to any raw text using predefined rules. Crucially, while prior work often targets improved downstream task performance, our goal is distinct: to enhance the linguistic competence of LMs during pre-training. Details on L2T: Language Learning"
        },
        {
            "title": "Tasks",
            "content": "We introduce 14 language learning tasks (Figure 3) designed to convert raw text into structured inputoutput pairs. These tasks highlight specific linguistic aspects and necessitate processing capabilities beyond simple next-token prediction. We organize the tasks by linguistic granularity, ranging from local surface dependencies (character/word) to deep syntactic and global coherence structures (sentence/discourse). In the following, we include excerpts from the training samples for each task. [...] signifies that text is omitted for brevity. B.1 Character-level B.1.1 Character Count (Char Count) Char Count counts total characters in the input text. This task promotes fine-grained processing and arithmetic abilities, aiding the handling of morphological variations. 11 Figure 3: Overview of the 14 language learning tasks. Colors denote linguistic granularity: character (blue), word (green), sentence (orange), and discourse (purple)."
        },
        {
            "title": "Char Count",
            "content": "Calculate the character count of the following text: The younger generation can never hope to see again the forest primeval, but the next thing to seeing it with ones own eyes is to see it in Coopers word pictures, as it was before the ax of the lumberman laid it in ruins. Of Coopers sea stories, the best is The Pilot, which tells in graphic style of the exploits of John Paul Jones in English waters. [...] To have created Leather-stocking is passport to enduring fame; yet Cooper added to this typical American backwoodsman Long Tom Coffin, the shrewd Yankee sailor, and long line of other original characters. (A) 1833 B.1.2 Masked Character Replacement (Masked Char) Masked Char reconstructs the original text after replacing percentage of characters (e.g., 15%) with mask token. This fosters character-level understanding and morphological processing."
        },
        {
            "title": "Masked Char",
            "content": "cut [...] single g+++ld aiguill++++++te is w+++rn of minor unifo+++ms o+++ offic+++r +++ doubl+++ gol+++ +++etach+++ent+++. aig+++il+++e+++te is wo++++++ on officer cut un+++forms of major detachment+++. All +++ersonnel of +++ajor detac+++ments wear +++f+++icer cut uni+++orms. A: [...] single silver aiguillette is worn on enlisted cut uniforms of minor detachments. single gold aiguillette is worn on officer cut uniforms of minor detachments. double gold aiguillette is worn on officer cut uniforms of major detachments. All personnel of major detachments wear officer cut uniforms. B.1.3 Whitespace Insertion (Space) Space restores original spacing to text stripped of whitespace. This enforces word boundary learning and tokenization rules while implicitly modeling grammatical structures defined by spacing."
        },
        {
            "title": "Space",
            "content": "Femalesmakeof55.5%oftherecordeddeathsand malesmakeup44.5%Whatwastheaveragebirthwe ightofthenewbornsthatsurvived?Therewerebirth weightsrecordedfornineofthenewbornsthatsurvi ved.[...]-Theleadingcauseofnewbornmortalityw aspredationbypumas.Thiswasfollowedbystarvat ion. Response: Females make of 55.5% of the recorded deaths and males make up 44.5% What was the average birth weight of the newborns that survived? [...] - The leading cause of newborn mortality was predation by pumas. This was followed by starvation. B.1.4 Typo Correction (Typo) Typo corrects synthetically introduced typos generated by replacing random percentage of characters (e.g., 3-8%). This task targets character and word recognition."
        },
        {
            "title": "Typo",
            "content": "For example, indiaiduals with first-degree relative (parent, sibling or child) who was diagnosed with lung cancer at an eally age may be at increased risk. [...] \"Accessiyility to qhis team df plysicians with expertiso in every aspect of diagnosis zo tdeatmlnt wyfl allow fcr more coordinated care, minumibing any scweenfng harms.\" For example, individuals with first-degree relative (parent, sibling or child) who was diagnosed with lung cancer at an early age may be at increased risk. [...] \"Accessibility to this team of physicians with expertise in every aspect of diagnosis to treatment will allow for more coordinated care, minimizing any screening harms.\" B.2 Word-level B.2.1 Last Phrase Prediction (Last) Last selects the correct concluding phrase (the segment following the final stop word) from two candidates. Inspired by Chen et al. (2022), this focuses on the understanding of sentence structure and context."
        },
        {
            "title": "Last",
            "content": "[...] These savings per person are converted to savings per unit area as follows: On the basis of population estimates (21) from 53 counties in New York State, the median population density was estimated at 103 persons per sq. mi (25th percentile = 67; 75th = 204). Thus, for the areas baited, the savings were calculated at $156.56 per sq. mi for the first 2 epizootic years ($1.52 per person 103 persons per sq. mi), and $30.90 per sq. mi for the post-epizootic years ($0.30 per person 103 persons per sq. mi). Cost-savings data from New Jersey (5) are used in +++? Options: and large estates. the sensitivity analysis. A. the sensitivity analysis. B.2.2 Masked Word Replacement (Masked Word) Masked Word reconstructs text where percentage of words (e.g., 15%) are replaced with mask token. Similar to Masked Language Modeling (Devlin et al., 2019), this enhances vocabulary knowledge and contextual inference."
        },
        {
            "title": "Masked Word",
            "content": "[...] This implies $$$ +++ have high demand for protein and require optimum temperatures for their metabolisms (()) function at optimal levels necessary for growth . . . Energy and protein are ___ by the organism for maintenance, growth and/or reproduction (Staton (()) al. 1986), where energy derived from carbohydrates, @@@ and fats. To prevent protein ___ +++ used as an energy source, @@@ energy should be supplied in the diet in [MASK] form of carbohydrates and fat. A. [...] This implies that they have high demand for protein and require optimum temperatures for their metabolisms to function at optimal levels necessary for growth . . . Energy and protein are required by the organism for maintenance, growth and/or reproduction (Staton et al. 1986), where energy is derived from carbohydrates, protein and fats. To prevent protein from being used as an energy source, sufficient energy should be supplied in the diet in the form of carbohydrates and fat. B.2.3 Random Word Replacement (Random) Random corrects text where 5% to 10% of words are replaced by random vocabulary tokens. Inspired by Yamaguchi et al. (2021), this improves robustness to noise and deepens contextual understanding."
        },
        {
            "title": "Random",
            "content": "We mortgage find the meaning. [...] If you read locus one theoretical book on the topic of narrative, this one is testament candidate. While theoretical in perspective, its written pessimist manner that will appeal to anyone involved in telling stories. We cannot find the meaning. [...] If you read only one theoretical book on the topic of narrative, this one is good candidate. While theoretical in perspective, its written in manner that will appeal to anyone involved in telling stories. B.2.4 Shuffled Word Correction (Shuffle) Shuffle restores the original order of text with 5% to 10% shuffled words. Also inspired by Yamaguchi et al. (2021), this focuses on syntax understanding and local dependencies."
        },
        {
            "title": "Shuffle",
            "content": "white loops and buckles permitted the The box to Side worn from either percussion waistbelt or cartridge box strap. Marines wore white buff of cartridge box strap that appears to men about 2 1/4 inches wide. [...] Equipped with Maynard tape primer system, the priming system of the M1855 was similar to the cap guns used by been cross Musket slings were of Black Leather, and officers arms cautioned against \"putting muskets in the hands of the be without slings.\" Officer be Arms There does not seem to be any standard side arm issued to Marine Officers of the Civil War era. These loops and buckles permitted the cartridge box to be worn from either the waistbelt or cartridge box strap. Marines wore white buff leather cartridge box strap that appears to be about 2 1/4 inches wide. [...] Equipped with Maynard tape primer system, the priming system of the M1855 was similar to the cap guns used by children today. Musket slings were of Black Leather, and officers were cautioned against \"putting muskets in the hands of the men without slings.\" Officer Side Arms There does not seem to be any standard side arm issued to Marine Officers of the Civil War era. B.2.5 Token Class Count (Token Type) Token Type counts occurrences of specific word types (e.g., digits, stopwords, punctuation, content words). This encourages classification based on linguistic categories, enhancing the understanding of text composition."
        },
        {
            "title": "Token Type",
            "content": "the punctuation marks in the following Count passage. Whereas watching relaxation video reduces stress, lowers the levels of stress hormones in the blood stream and induces relaxation. [...] Click here to be taken to Comparison Between the Fight/Flight Response and Relaxation Response A. 36 B.3 Sentence-level B.3.1 Sentence Deletion (Deletion) Deletion identifies an anomalous sentence randomly inserted from external context. With equal probability, the model either (a) reconstructs the passage with the anomaly removed, or (b) generates only the anomalous sentence. This fosters the ability to distinguish irrelevant information and maintain discourse coherence."
        },
        {
            "title": "Deletion",
            "content": "For men, the formal hakama is usually made of heavy black or gray, striped silk. Those designed for women usually come in dark, solid colors, with the exception of the bright red hakama worn by women who tend the Shinto shrines in Japan. [...] Hakama are the traditional garment for priests who reside at Shinto shrines throughout Japan. They are also worn for the practice of archery and for martial arts such as kendo, karate, and aikido. (PDF) STOP, THINK, SPOT FAKE NEWS . Women wear them only for specific occasions and activities, such as graduation ceremonies and traditional Japanese sports. [...] rustic version of the hakama was also worn by farmers and woodsmen. A) (PDF) STOP, THINK, SPOT FAKE NEWS . B.3.2 Sentence Reordering (Reordering) Reordering restores the original order of text containing randomly permuted sentences. This encourages understanding of discourse structure, logical flow, and coherence at the sentence level."
        },
        {
            "title": "Reordering",
            "content": "Assignments (see full descriptions below): 1. All too often, the closer design is to completion, the more often and drastic the demands for change become. Demands for change may come from the audience, the client, the market, or their own artistic process. Thus, successful designer learns to rely on an ability to improvise, to respond playfully and creatively, to work with increasingly narrow limitations and assets, while never losing sight of the overall experience goals. [...] In the first semester, the exercises focus on lowand non-technical forms of interactivity, experimenting with games, improvisational theater, fine arts, toys, interactive narrative, and real and virtual play spaces. A: CTIN 534: Experiments in Interactivity Instructors: Bernie DeKoven and Tracy Fullerton Course Description: This course is the first in two-semester sequence which will introduce students to variety of concepts in interactivity and teach creative techniques for conceptualizing and producing original interactive projects. [...] After the first two classes, which establish the twin focal points of playfulness and improvisation, students will: experience the fundamentals of improvisation as it applies to both theatrical and interactive development through participation in variety of exercises explore fundamental principles of the psychology of fun, the sociology of playfulness and the art of improvisation address issues of agency, empathy, control, creativity, collaboration and the natural tension that occurs between artist and audience when the audience has part in making the art Thursday 6:30 9:30PM Media Lab, on the second floor of the Evaluation of student performance: a. Assignments (see full descriptions below): 1. In-class design exercises 2. B.4 Discourse-level B.4.1 Second Passage Completion (Fill Middle) Fill Middle generates the missing second passage given the first and third passages of text. Analogous to fill-in-the-middle (Bavarian et al., 2022), this strengthens the understanding of multi-passage structure and coherence."
        },
        {
            "title": "Fill Middle",
            "content": "[...] The first confirmed case in Latin America was identified in Brazil on 26 February, although researchers have said there are indications that there were cases there as early as January. Coronavirus has since spread in the world, and has had more than 10,000 confirmed deaths. Chile is reporting thousands of new coronavirus cases each day, with more than 6,000 people dead. [...] (A) to every country in the region. More than 2.5 million cases have been recorded, and more than 100,000 people have died, according to the European Centre for Disease Control and Prevention. That is fewer deaths than in the US and Europe, but both cases and deaths may be under-reported. Latin Americas two most populous nations, Brazil and Mexico, have seen the highest number of deaths, more than 64,000 and 30,000 respectively. Peru has the sixth highest number of reported cases B.4.2 Second Half Completion (Half) Half generates the latter half of given text. This encourages the learning of discourse coherence, prediction of and maintenance of narrative flow. long-range dependencies,"
        },
        {
            "title": "Half",
            "content": "[...] Your child must fast six hours before an ultrasound to provide clear view of the organs. During this painless procedure, your child lies quietly while the radiologist moves the ultrasound probe across the abdomen. cool jelly coats the probe to improve the ultrasound picture quality. Upper Gastrointestinal A) Endoscopy Exam Compassionate nurses who are experienced in helping children through these tests will start an intravenous line and help administer medications to relax your child and relieve potential pain. During this test, long, thin telescope about as wide as pen is inserted into the mouth, down the esophagus, and into the stomach and intestine. Through this telescope and its television camera, the gastroenterologist can see the inner lining of the esophagus, stomach and intestine. Small biopsies can be obtained from this lining. Sensors attached to your child also monitor heart rate, blood pressure and blood oxygen levels to ensure safe conditions throughout the procedure. typical upper gastrointestinal endoscopy takes about 20 minutes. With this test, the physician examines the inside lining of the entire length of the colon or large intestine. B.4.3 One Word Prefix Generation (One) One generates the subsequent text given single word prefix followed by newlines. This specifically trains the initiation and structuring of generation from minimal context."
        },
        {
            "title": "These",
            "content": "Answer: These routes allow visitors to locate works in the Museum, prepare visit beforehand, further their knowledge of the collection with thematic route or discover an enjoyable way of introducing the Museum to children through themes such as animals or princesses. Access to the content is through the index of collections or index of artists. There is also the option to save and select works or articles that the user considers most important or relevant in the Favourites section, and to share content on the social networks (Facebook and Twitter) through direct links in the entries on the works. [...]"
        },
        {
            "title": "C Extended Experimental Setup",
            "content": "This section details the construction of the pretraining data, the parameters of the implementation, and the framework used for evaluation. C.1 Pre-training Data Construction We construct our pre-training data by combining standard raw text with data generated via our L2T framework. In the Disjoint configuration, we split source documents into two distinct, non-overlapping sets of equal size. One set is used exclusively for standard CLM and the other is transformed into L2T samples. In the Shared configuration, we utilize the exact same source documents for both tasks. This means the pipeline processes every document twice: once as raw text and once to stimulate linguistic learning through L2T transformation. Sample Generation Pipeline. For standard CLM, documents are tokenized and packed continuously. For L2T, source documents undergo the following pipeline: 1. Segmentation: Documents are segmented into sentences and grouped into chunks of approximately 512 tokens to ensure samples consist of complete sentences. 2. Transformation: One of the 14 tasks (2) is applied to each chunk. Pairs are formatted as [Input]nn[Prefix] [Output] using randomized prefixes for stylistic variation. 3. Packing and Mixing: Transformed chunks are concatenated to fill the maximum sequence length and then shuffled with raw text samples. This strategy provides the structural scaffolding necessary to optimize for linguistic competence while retaining world knowledge (Cheng et al., 2024a). C."
        },
        {
            "title": "Implementation and Training Details",
            "content": "Pre-training Data Construction. The hyperparameters for the curation of the L2T data are listed in Table 4. We use Bling Fire (v0.1.8) for efficient sentence segmentation. Following Chen et al. (2022), we use varied mask tokens for the Masked Word and Masked Char tasks. The Last task utilizes the stop word list of NLTK (Bird and Loper, 2004) to identify the final segment of the text. For the Token Type task, each word is classified as stopword, digit, or content via prioritized procedure: (i) cleaning punctuation and symbols; (ii) matching the lowercase form against the stopword list; and (iii) verifying if the remaining string consists entirely of digits. Any tokens not meeting these criteria are classified as content. To count punctuation marks, we utilize the regex pattern: !\"#$%&'()*+,-./:;<=>?@[]^_`{}. Data Sampling Strategies. We investigate two configurations for mixing raw text and L2T data. To ensure the disambiguation of tasks, specifically to distinguish Token Type from Char Count,"
        },
        {
            "title": "Mask token variations",
            "content": "Masking ratio for Masked Word Masking ratio for Masked Char Replacement ratio for Random Shuffling ratio for Shuffle Typo ratio for Typo False sample for Last & Deletion 512 tokens Bling Fire (v0.1.8) {Answer:\", Response:\", (A)\", A)\", A.\", \"} {\"[MASK]\", \"___\", \"@@@\", \"###\", \"\", \"+++\", \"(())\", \"$$$\"} 0.15 A:\", 0.15 uniform(0.05, 0.1) uniform(0.05, 0.1) uniform(0.01, 0.08) Random sampling from previous document Table 4: Hyperparameters for curating our TL2T pretraining data. task-specific instruction is inserted randomly at either the beginning or the end of the input, separated by two newline characters (nn). Pre-training Details. Table 5 lists the hyperparameters and configuration settings used for pretraining. Libraries. We preprocess datasets with Hugging Face Datasets (v3.2.0) (Lhoest et al., 2021). We use PyTorch (v2.3.0) (Ansel et al., 2024), FlashAttention-2 (v2.7.3) (Dao, 2023), and Hugging Face Transformers (v4.49.0) (Wolf et al., 2020) for pre-training. For evaluation, we use lm-evaluation-harness (Gao et al., 2023) (v0.4.8). C.3 Evaluation Details: BLiMP Benchmark To measure the linguistic competence of models, we utilize the BLiMP benchmark, which comprises 67 datasets covering 12 linguistic phenomena. Each sample consists of pairs of minimally different sentences that contrast in grammatical acceptability to isolate specific phenomena in semantics, morphology, or syntax. Semantics includes two phenomena: quantifiers (Quant), which test restrictions on distribution (e.g., fewer than versus at most), and negative polarity items licensing (NPI). The latter assesses whether the model possesses knowledge regarding the accurate placement of words such as any (e.g., did not see anyone.). Morphology covers four phenomena: anaphora Figure 4: Linguistic competence comparisons on BLiMP between different L2T models trained on specific 25B token single task data. agreement (Ana Agr), which verifies whether pronouns correctly match the intended referent (e.g., John saw himself.); irregular forms (Irregul), which evaluate knowledge of unpredictable word forms (e.g., go becomes went); determiner-noun agreement (DN Agr), which tests the numerical correspondence between words such as this or these and nouns; and subject-verb agreement (SV Agr), which confirms that the verb form agrees with the subject (e.g., he runs versus they run). Finally, Syntax covers six phenomena: argument structure (Arg Str), which examines the combination of verbs with necessary components (e.g. eat must accompany something to be eaten); binding (Bind), , which tests the structural relationship between pronoun and the antecedent (e.g. John saw himself vs. John saw him); control/raising (Ctrl Rais), which evaluates syntactic and semantic differences between various predicate types; ellipsis (Ellips), which measures whether expressions can be omitted from sentence; filler-gap (Fill Gap), which assesses dependencies arising from phrasal movement; and island effects (Island), which check constraints on moving sentence elements out of certain grammatical constructions."
        },
        {
            "title": "D Effectiveness of Individual Tasks",
            "content": "To examine task-specific contributions to linguistic competence, we pre-train 500M models on 25B task-specific tokens for each of the 14 tasks listed in 2.7 Computational constraints limit the total 7Each pre-training dataset originates from the same pool used for the reference data (Raw and L2T). Subsampling ensures that each model processes exactly 25B tokens."
        },
        {
            "title": "Hyperparameters",
            "content": "500M"
        },
        {
            "title": "Sequence length\nMaximum Learning Rate\nLearning rate scheduler\nWarmup steps",
            "content": "Adam ϵ Adam β1 Adam β2 Gradient clipping Weight decay Training precision Computing infrastructure"
        },
        {
            "title": "Run time for each model",
            "content": "1024 4864 24 24 24 2 1,000,000 1e-06 0.0 True SiLU 0.02 32,000 Mistral 256 200K (for 100B token training), 50K (for 25B token training) 2,048 3e-4 cosine 2,000 (for 100B token training), 1,000 (for 25B token training) 1e-8 0.9 0.999 1.0 0.1 BF16 2 AMD MI300X GPUs (for 100B token training), 2 H100 (96GB) HBM3 GPUs (for 25B token training) 16 days (for 100B token training), 3.5 days (for 25B token training) 1B 1728 4752 30 30 32 4 1,000,000 1e-06 0.0 True SiLU 0.02 32,000 Mistral 256 200K 2,048 3e-4 cosine 2,000 1e-8 0.9 0.999 1.0 0.1 BF16 4 H100 (96GB) HBM3 GPUs 12 days Table 5: Hyperparameters and training costs for each model scale. training budget to 25B tokens. However, this allocation is sufficient to cover the window of maximal development (Shah et al., 2024), during which models substantially develop cognitive abilities up to approximately 20B tokens. Figure 4 illustrates the BLiMP performance of these models. Models pre-trained on single task often maintain competitive performance or demonstrate improvements over the Raw baseline across linguistic subfields. Notably, nine tasks (including Char Count, Last, Masked Word, and Reordering) consistently outperform the Raw model. These tasks likely provide the structural scaffolding that facilitates the acquisition of morphology, syntax, and semantics. Conversely, Masked Char, Space, and One consistently underperform the Raw baseline. The character-level corruption in the Space task shows 33-point drop in Morphology. We speculate that character-level corruption in tasks like Masked Char and Space might create an unstable training signal when used standalone. The One task, where model needs to generate coherent text given single-word prefix, likely lacks sufficient structured signal for effective learning, although it preserves general capabilities because its objective closely mirrors standard CLM (Figure 6). closer examination of individual linguistic phenomena (Figure 5) reveals that while almost all tasks (excluding Space) outperform the Raw model on Island effects, none succeed on Fill Gap. While the structural scaffolding from the tasks across varying granularity assists in detecting island violations, the complexity of Fill Gap requires targeted signals to capture moved elements and long-distance hierarchical dependencies. Current L2T data likely lacks the specific coverage needed for these complex structures. Finally, this analysis underscores the robustness of the full L2T framework. Despite the inclusion of weaker tasks (namely, Space and One), L2T 100% outperforms most single-task models on BLiMP and general benchmarks (Figure 6). This highlights the benefit of combining diverse 17 Figure 5: Linguistic competence comparisons on BLiMP between different L2T models trained on specific 25B token single task data. Figure 6: General benchmark performance comparison between different L2T models trained on specific 25B token single task data. tasks; the complementary strengths of the various objectives enhance both linguistic competence and general capabilities. This result mirrors the generalization gains observed when fine-tuning LMs on broad task distributions (Wei et al., 2022; Padmakumar et al., 2022; Sanh et al., 2022). Mixing Ratio of Raw and L2T Data We investigate the influence of the proportion of raw text relative to L2T data by varying the L2T mixing ratio at 100% (denoting zero raw text), 75%, 50% (the default), and 25% for 500M models using 100B tokens in the Disjoint setting.8 Here, 100% mixing ratio denotes training solely on L2T data. Linguistic Competence. Results on BLiMP (Figure 8) show that after training, all L2T models perform similarly across linguistic subfields, regardless of the mixing ratio. Differences remain minor, with maximum deltas of 1.1 in semantics, 0.17 in morphology, and 0.73 in syntax. However, early in training (e.g., 5B tokens), performance gains frequently correlate with increased L2T data, except for the L2T 100% setting. For instance, at 5B tokens, L2T 75% outperforms L2T 50% and L2T 25% in morphology (91.8 vs. 90.8 and 89.7) and semantics (74.1 vs. 72.7 and 66.8). This suggests that while the mixing ratio influences the initial learning trajectory, L2T data stimulates linguistic competence regardless of the specific ratio by the end of training. 8Due to computational limits, experiments use only the 500M model. General Benchmarks. While linguistic gains are stable, evaluation on general benchmarks (Figure 7) underscores the necessity of the raw text proportion (i.e., allocating sufficient training steps to raw text). The L2T 100% configuration, which contains no raw text, exhibits substantial performance drops, such as 23-point decline on ARC, compared to the Raw model. Increasing the proportion of raw text mitigates this gap; for example, the L2T 75% setting (i.e., containing 25% raw text) differs by only 4.7 points on ARC. Performance generally improves as the proportion of raw text increases (except on LogiQA), trend supported by Kendall tau correlations (Kendall, 1938) ranging from 0.67 to 1.0. These results demonstrate that while L2T data enhances linguistic competence, raw text remains essential for broad knowledge and reasoning. This aligns with the view of Cheng et al. (2024a) that mixing raw text is vital for retaining broad world knowledge. Consequently, achieving an appropriate allocation balance between these data types is imperative. Even 25% of L2T data substantially improves linguistic competence (e.g., 1.9 overall gain over Raw in BLiMP), while at least 25% of raw text is essential for robust general capabilities (e.g., an 18-point gain on ARC compared to L2T 100%)."
        },
        {
            "title": "F Qualitative Analysis",
            "content": "We conduct qualitative analysis to examine the behavior and limitations of models trained on L2T. First, we observe substantial improvement 18 Figure 7: Performance on general benchmarks for 500M models pre-trained with different mixing ratios of standard Raw vs. L2T data for 100B tokens. L2T 100% stands for no standard raw text mixed, i.e. 100% L2T data. and LMs have often faced challenges (Da Costa and Chaves, 2020; Warstadt et al., 2020)."
        },
        {
            "title": "G License",
            "content": "This study uses publicly available datasets with different licenses, as detailed in Table 6. We also use tokenizer file of Mistral available at mistralai/ Mistral-7B-v0.1, licensed under Apache 2.0. Note that all permit their use for academic research."
        },
        {
            "title": "H Use of Generative AI Tools",
            "content": "The authors acknowledge the use of LLMs during the preparation of this work. Gemini 3.0 Pro were utilized to find related work and to improve the grammar and clarity of the draft. Additionally, GPT-5 served as coding assistant for implementation and debugging. Figure 8: Linguistic competence comparisons by linguistic subfield on BLiMP between Raw and L2T 500M models with different mixing ratios of standard raw text. 100% stands for no standard raw text mixed. of 23.2 on the coordinate structure constraint complex left branch task within island effects (Island), specifically in the 500M Disjoint setup. This task requires distinguishing between minimally different sentences such as: Correct: Whose mice can Julia bring and Brett notice? Incorrect: Whose can Julia bring mice and Brett notice? The challenge lies in detecting subtle syntactic violations caused by the misplacement of constituents. We attribute improvements on such tasks largely to the exposure of the model to diverse structural objectives within L2T, which collectively enhance sensitivity to complex syntactic dependencies beyond simple word probabilities. In contrast, we observe 7.9 point drop on the wh vs that with gap long distance task within filler gap (Fill Gap), which tests whether long-distance dependency is correctly licensed, as shown below: Phillip forgot [Correct: what] [Incorrect: that] some senator that was escaping from Stacy goes to. This phenomenon hinges on tracking hierarchical syntactic relationships across intervening clauses. The drop in performance suggests that while L2T effectively captures structural constraints, it continues to struggle with dependencies spanning extensive contexts. This remains an area where even humans perform modestly (75% accuracy)"
        },
        {
            "title": "Category",
            "content": "RC"
        },
        {
            "title": "License",
            "content": "RACE (Lai et al., 2017) SciQ (Welbl et al., 2017) LogiQA (Liu et al., 2021) RC+CR ReCoRD (Zhang et al., 2018) CR ARC (Easy) (Clark et al., 2018) COPA (Gordon et al., 2012) OpenBookQA (Mihaylov et al., 2018) Social IQa (Sap et al., 2019) HellaSwag (Zellers et al., 2019)"
        },
        {
            "title": "Language Modeling",
            "content": "LAMBADA (Paperno et al., 2016) http://www.cs.cmu. edu/glai1/data/race/ https://allenai.org/ data/sciq https://github.com/ lgw863/LogiQA-dataset https://sheng-z. github.io/ ReCoRD-explorer/ https://allenai.org/ data/arc https://people.ict. usc.edu/gordon/copa. html https://allenai.org/ data/open-book-qa https://huggingface. co/datasets/allenai/ social_i_qa https://rowanzellers. com/hellaswag/ https://zenodo.org/ records/2630551 Custom (Research) CC BY-NC 3."
        },
        {
            "title": "No license found",
            "content": "Apache 2.0 + Internet Archives Terms of Use CC BY-SA 4."
        },
        {
            "title": "No license found",
            "content": "Apache 2.0 CC BY 4."
        },
        {
            "title": "MIT",
            "content": "CC BY 4.0 Table 6: Summary of Datasets, Sources, and Licenses"
        }
    ],
    "affiliations": [
        "School of Computer Science, University of Sheffield, United Kingdom"
    ]
}