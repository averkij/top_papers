{
    "paper_title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
    "authors": [
        "Guanzhi Deng",
        "Bo Li",
        "Ronghao Chen",
        "Huacan Wang",
        "Linqi Song",
        "Lijie Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 3 2 8 4 0 . 1 0 6 2 : r DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation Guanzhi Deng1*, Bo Li2*, Ronghao Chen3*, Huacan Wang4, Linqi Song1, Lijie Wen2 1City University of Hong Kong, Hong Kong, China 2Tsinghua University, Beijing, China 3Peking University, Beijing, China 4University of Chinese Academy of Sciences, Beijing, China guanzdeng2-c@my.cityu.edu.hk, linqi.song@cityu.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) has become prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fineis widely tuning (PEFT), such as LoRA, adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, taskrelevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each experts demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization."
        },
        {
            "title": "Introduction",
            "content": "Mixture-of-Experts (MoE) has become prominent paradigm for scaling Large Language Models (LLMs) (Yang et al., 2025; Jiang et al., 2024; Liu et al., 2024a). By sparsely activating only subset of expert sub-networks for each input token, MoE substantially increases model capacity without proportionally increasing per-token computation, demonstrating impressive capabilities across wide range of tasks (Shazeer et al., 2017; Fedus *Equal contribution. Corresponding authors. 1 et al., 2022; Jiang et al., 2024; Team et al., 2025). With the widespread adoption of pretrained MoE LLMs, efficiently adapting them to specific downstream tasks has become significant challenge. Parameter-Efficient Fine-Tuning (PEFT), particularly Low-Rank Adaptation (LoRA) (Hu et al., 2022), is primary approach to address this challenge. LoRA injects trainable low-rank matrices into pretrained models, enabling effective task adaptation with small number of trainable parameters. However, existing approaches that apply LoRA to MoE are largely architecture-agnostic and treat all experts homogeneously. Specifically, they typically assign the same fixed LoRA rank to every expert (Li et al., 2024; Liu et al., 2024b; Dou et al., 2024; Gao et al., 2025). While simple, this strategy fundamentally overlooks the intrinsic functional specialization formed during MoE pretraining. Recent studies have demonstrated that experts in pretrained MoE LLMs exhibit significant heterogeneity in their learned representations and functional roles, with different experts naturally developing specialization toward distinct knowledge domains, linguistic phenomena, or reasoning patterns (Wang et al., 2025; Fedus et al., 2022; Jiang et al., 2024). This intrinsic expert specialization constitutes the foundation of MoEs powerful representational capacity. Under uniform and fixed-rank LoRA configuration, experts that are highly relevant to the target domain may become under-provisioned due to insufficient adaptation capacity, while less relevant experts may be over-provisioned with redundant parameters. This resource mismatch limits the models adaptation potential and ultimately constrains its performance on downstream tasks. While prior work has explored static heterogeneous expert designs for pre-training (Sun et al., 2024; Wang et al., 2025) and adaptive rank allocation for dense LLMs (Zhang et al., 2023), these studies do not address the challenge of dynamically constructing task-specific, heterogeneous adaptation structures during the fine-tuning of existing pretrained MoE models. Existing work on PEFT for MoE models includes ESFT (Wang et al., 2024), which fine-tunes fixed subset of experts, and PERFT (Liu et al., 2024d), which uses uniformrank LoRA configuration across experts. These designs restrict adaptation to coarse-grained or homogeneous capacity allocation, preventing finegrained and dynamic expert-level adaptation during training. To address this issue, we propose Dynamic Rank LoRA framework named DR-LoRA, which enables expert adaptation capacity to evolve dynamically in response to task-specific demands during fine-tuning. Unlike uniform allocation strategies, DR-LoRA adopts an incremental growth approach: it allocates high-rank LoRA parameter space to each expert at initialization but activates only small initial rank, then progressively expands the effective rank of high-demand experts throughout training. This expansion is guided by an Expert Saliency Scoring mechanism that integrates two complementary signals from the MoE training process: (1) expert routing frequency, tracked via exponential moving average of routing weights, which quantifies each experts relevance to the current data distribution (Zhou et al., 2022); and (2) LoRA rank importance, measured through accumulated gradient-weight products, which captures the learning intensity of each expert on the target task (Zhang et al., 2023). By computing the saliency, DR-LoRA prioritizes rank allocation to experts with high task relevance and learning activity, while the rank penalty term prevents resource monopolization and promotes balanced capacity distribution. The contributions of this paper are summarized as follows: We identify the resource mismatch problem of uniform LoRA allocation in MoE adaptation, where task-relevant experts are underprovisioned while less relevant ones receive redundant parameters. We propose DR-LoRA, which dynamically grows LoRA ranks through an Expert Saliency Scoring mechanism that integrates routing frequency and rank importance, automatically forming task-adaptive heterogeneous rank distributions. Experiments demonstrate that DR-LoRA consistently outperforms standard LoRA and pruning-based methods under the same parameter budget, with analyses confirming effective task-aligned capacity allocation."
        },
        {
            "title": "2 Related Work",
            "content": "Efficiently adapting LLMs to downstream tasks under resource constraints is significant challenge, where Low-Rank Adaptation (LoRA) has emerged as widely adopted Parameter-Efficient Fine-Tuning (PEFT) method. Standard LoRA (Liu et al., 2024c; Hayou et al., 2024) assigns fixed and identical ranks to all target modules, but this overlooks the varying contributions of different modules to downstream tasks. To address this issue, researchers have proposed various dynamic rank allocation methods. AdaLoRA (Zhang et al., 2023) parameterizes weight updates in singular value decomposition form and adaptively adjusts the rank of each module during training through importance score-based pruning. However, this high-to-low pruning strategy incurs substantial computational overhead in the early stages of training. Although these methods have advanced adaptive PEFT for dense LLMs, they do not consider the unique characteristics of MoE LLMs, particularly the special challenges posed by expert heterogeneity. Experts in MoE LLMs are known to form functional specializations after pre-training (Dai et al., 2024). This intrinsic heterogeneity presents special challenge for PEFT, as uniform adaptation strategy fails to match the non-uniform functional distribution of the experts. Existing work on PEFT for MoE models includes ESFT (Wang et al., 2024), which fine-tunes fixed subset of experts, and PERFT (Liu et al., 2024d), which uses uniformrank LoRA configuration across experts. These designs restrict adaptation to coarse-grained or homogeneous capacity allocation, preventing finegrained and dynamic expert-level adaptation during training. In this work, we propose DR-LoRA, which exploits expert heterogeneity for dynamic rank allocation. By continuously assessing each experts routing frequency and learning intensity during training, DR-LoRA incrementally grows LoRA ranks to construct task-adaptive heterogeneous distributions, outperforming uniform and pruning-based allocation strategies. 2 Figure 1: The overview of the DR-LoRA framework. Pre-trained expert weights are frozen, while each expert is equipped with trainable LoRA module. These modules start with small initial rank (rinit) and can dynamically grow (r) during training. Expert Saliency Scoring guides rank growth by integrating two real-time signals: (1) Expert Routing Frequency (fi), tracked from the routers decisions to measure task relevance, and (2) LoRA Rank Importance (gi), derived from the gradient signals of the trainable LoRA matrices (A and B) to measure learning intensity."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we present DR-LoRA (Dynamic Rank LoRA), framework that dynamically adapts LoRA ranks during fine-tuning to address the resource mismatch problem in MoE adaptation. As shown in Figure 1, we first formulate the problem setup (3.1), then introduce our core Expert Saliency Scoring mechanism (3.2), and finally present the dynamic rank allocation strategy (3.3). 3.1 Problem Formulation MoE Architecture. We consider standard Mixture-of-Experts LLM with transformer layers. For each layer ℓ {1, ..., L}, the MoE module contains expert networks {Eℓ,1, ..., Eℓ,N } and router Gℓ that produces routing weights. For an input token representation xℓ, the router computes: wℓ = Softmax(Gℓ(xℓ)) RN (1) In top-k routing, only the top-k experts with the highest routing weights are activated. The MoE output is: MoEℓ(xℓ) = (cid:88) wℓ,i Eℓ,i(xℓ) (2) iTopK(wℓ) LoRA Adaptation. Following Hu et al. (2022), LoRA adapts pretrained weight matrix Rdk by injecting trainable low-rank matrices: = + BA (3) where Rrk and Rdr with rank min(d, k). During training, is frozen while and are updated. Existing LoRA methods for MoE assign uniform rank to all experts. However, experts in pretrained MoE models exhibit significant functional specialization (Dai et al., 2024; Muennighoff et al., 2025), with different experts naturally suited for different aspects of the data distribution. During task-specific fine-tuning, this specialization creates varying adaptation demands: task-relevant experts require substantial capacity to fully leverage their specialization, while task-irrelevant experts need minimal updates primarily for distributional alignment. Uniform allocation thus leads to under-provisioning of critical experts and overprovisioning of less relevant ones, resulting in suboptimal parameter utilization and limited adaptation quality. We aim to construct heterogeneous rank distribution {Rℓ,i}ℓ,i where each experts LoRA rank Rℓ,i reflects its task-specific demand, while maintaining fixed total parameter budget: (cid:88) (cid:88) ℓ=1 i=1 Rℓ,i = rtarget (4) where rtarget is the target average rank equivalent to standard LoRA. 3.2 Expert Saliency Scoring To quantify each experts demand for additional parameters, we propose an Expert Saliency Scoring mechanism that integrates two complementary signals from the MoE training process. Expert Routing Frequency. The routing frequency reflects how often an expert is selected during training, which indicates its relevance to the current data distribution. For expert Eℓ,i at layer ℓ, we track its usage via an exponential moving average (EMA): ℓ,i = β (t1) (t) ℓ,i + (1 β) w(t) ℓ,i (5) where w(t) ℓ,i is the weight of tokens routed to expert Eℓ,i at step t, and β [0, 1) is the decay coefficient. LoRA Rank Importance. The rank importance measures the learning intensity of an expert, reflecting how actively it is adapting to the current task. Following Zhang et al. (2023), we employ sensitivity-based importance metric that measures each rank dimensions contribution through gradient-weight products. For each experts LoRA module, we denote aj = Aℓ,i[j, :] and bj = Bℓ,i[:, j] as the j-th rank dimension parameters. We compute the important score at step as: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) aj bj aj bj s(t) ℓ,i,j = (cid:13) (cid:13) (cid:13) (cid:13)1 where is the training loss, denotes elementwise product, and 1 denotes the ℓ1 norm. We employ multiplicative aggregation to reflect the joint contribution of LoRAs doublet {aj, bj}. (cid:13) (cid:13) (cid:13) (cid:13)1 (6) We then track the rank importance using the same EMA coefficient: ℓ,i,j = β g(t1) g(t) ℓ,i,j + (1 β) s(t) ℓ,i,j (7) The expert-level rank importance aggregates over active ranks: g(t) ℓ,i = r(t) ℓ,i (cid:88) j=1 1 r(t) ℓ,i g(t) ℓ,i,j (8) Saliency Score. We define saliency score as follows: (t) ℓ,i = ℓ,i g(t) (t) ℓ,i (r(t) ℓ,i + 1)γ (9) We integrate routing frequency and rank importance as complementary signals to quantify each experts demand for additional capacity. The rank penalty term (rℓ,i + 1)γ prevents individual experts from monopolizing resources and promotes balanced rank distribution. 3.3 Dynamic Rank Allocation Strategy DR-LoRA adopts an incremental rank expansion strategy during supervised fine-tuning. We initialize all experts with small active rank and progressively activate additional rank dimensions for highsaliency experts, guided by the Expert Saliency Scoring mechanism described in 3.2. Initialization. At the start of training, each expert Eℓ,i allocates parameter space for rmax ranks by initializing Aℓ,i Rrmaxk and Bℓ,i Rdrmax, but only activates the first rinit dimensions. binary mask mℓ,i {0, 1}rmax tracks active dimensions, with the forward pass computing: ℓ,i = Wℓ,i + Bℓ,i[:, mℓ,i] Aℓ,i[mℓ,i, :] (10) Growth Window and Budget Allocation. We define growth window [twarmup, tend] during which rank expansion occurs, where twarmup is the learning rate warmup period, tend = Ttotal Tbuffer, and Tbuffer ensures newly activated ranks have sufficient training time. Within this window, we perform rank growth every Tgrow steps. To ensure balanced growth across the training period, we pre-compute fixed quota for each growth event. First, we compute the total number of growth events: Tevents = (tend twarmup)/Tgrow. Then, the quota is determined as: = (cid:24) (rtarget rinit) Tevents (cid:25) (11) This quota represents the number of ranks to distribute per layer at each growth event, ensuring that the total active ranks reach rtarget by the end of the growth window. Periodic Rank Growth. Every Tgrow steps within the growth window, we execute rank allocation procedure for each layer independently based on expert saliency scores. For each layer Algorithm 1 DR-LoRA Training Algorithm Require: Pretrained MoE model (L layers, experts/layer), dataset D, hyperparameters {rinit, rmax, rtarget, Tgrow, pgrow, β, γ} Ensure: Fine-tuned model with heterogeneous expert ranks 1: Initialize: Allocate Aℓ,i Rrmaxk, Bℓ,i Rdrmax with masks mℓ,i; Freeze router; Compute quota (Eq. 11) 2: for = 1 to Ttotal do 3: Sample batch B; Compute MoE output and loss L(B) Update routing frequency (t) gradient score g(t) if = twarmup then Unfreeze router ℓ,i (Eq. 5) and ℓ,i (Eq. 8) end if if [twarmup, tend] and (t twarmup) mod Tgrow = 0 then for each layer ℓ do ℓ,i (Eq. 9) Compute saliency S(t) Sort experts Allocate ranks greedily: for each expert in order, activate ngrow (Eq. 12) ranks Reset gℓ,i for all experts end for 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end if Update Aℓ,i, Bℓ,i and router 16: 17: end for ℓ, we first compute the saliency score S(t) ℓ,i for all experts using Eq.(9), then sort the experts in descending order of their scores. We then perform greedy allocation of the pre-computed quota to high-saliency experts. Specifically, for each expert in the sorted order, we determine the number of new ranks to allocate as: ngrow = min(rfreepgrow, ravail,i, Qremain) (12) where rfree = rmax rinit is the initial free capacity, ravail,i is the current number of inactive ranks, and pgrow is the maximum growth rate per expert per event that prevents any single expert from monopolizing the quota in one growth event. After allocating ngrow ranks to expert by activating the corresponding dimensions in the mask mℓ,i, we reset its rank importance score gℓ,i = 0 while preserving its routing frequency fℓ,i. Resetting rank importance scores prevents monopolization 5 and allows other experts to compete for ranks in subsequent allocations, while preserving routing frequency enables experts that have received ranks to remain competitive for future allocations as task demands evolve during training. The complete training procedure integrating this growth strategy is presented in Algorithm 1."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Datasets. We employ the OLMoE SFT Mix dataset1, comprehensive mixture of instructionfollowing, mathematical reasoning, and coding samples. We compare against: Models and Baselines. We evaluate DRLoRA on two MoE architectures: OLMoE-1B7B-0924 (Muennighoff et al., 2025, hereafter OLMoE): 6.9B parameters (1.3B activated), 16 layers with 64 experts per layer, top-8 routing. Phi-mini-MoE-instruct (Abdin et al., 2024, hereafter Phi): 7.6B parameters (2.4B activated), 32 layers with 16 experts per layer, top-2 routing. (1) Base Model: Pretrained model without fine-tuning; (2) LoRA: Uniform rank = 64 (OLMoE) / = 16 (Phi); (3) DoRA: Weight-decomposed adaptation (Liu et al., 2024c) with uniform rank = 64 / = 16; (4) LoRA+: Different learning rates for adapter matrices (λ = 16) (Hayou et al., 2024) with uniform rank = 64 / = 16; (5) AdaLoRA: Adaptive pruning from = 128 to = 64 (OLMoE) / = 32 to = 16 (Phi) (Zhang et al., 2023); (6) DR-LoRA: Dynamic growth from rinit = 32 to rtarget = 64 (OLMoE) / rinit = 8 to rtarget = 16 (Phi). Training Configuration. All models train for one epoch with the same parameter budget. We apply LoRA to up_proj and down_proj matrices. For AdaLoRA and DR-LoRA, rank updates occur every 200 steps. Details in Appendix A.1. Evaluation. We evaluate on seven benchmarks: MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), BBH (Suzgun et al., 2023), GSM8k (Cobbe et al., 2021), ARC-C (Clark et al., 2018), HumanEval (Chen, 2021), and IFEval (Zhou et al., 2023), covering knowledge, reasoning, code generation, and instruction following. 1https://huggingface.co/datasets/allenai/ tulu-v3.1-mix-preview-4096-OLMoE Model Eff. Rank MMLU HellaSwag BBH GSM8k ARC-C HumanEval IFEval Avg Base LoRA DoRA LoRA+ AdaLoRA DR-LoRA Base LoRA DoRA LoRA+ AdaLoRA DR-LoRA 64 64 64 12864 3264 16 16 16 3216 816 50.5 50.3 50.4 50.6 50.6 50.0 67.6 67.2 67.3 67.5 67.4 67.6 OLMoE Models 32.3 34.5 34.2 34.4 33.3 34.5 Phi Models 48.7 68.1 68.5 68.8 69.0 69.9 6.1 30.2 30.9 31.2 31.6 32.8 71.3 75.0 76.2 77.1 76.8 78.2 59.2 55.9 56.1 56.3 55.8 55. 54.6 54.7 55.1 55.4 55.7 56.9 54.2 51.9 52.1 52.4 52.2 53.2 58.4 58.8 58.6 58.7 58.7 58.4 22.1 34.1 35.3 36.2 37.5 39.1 72.3 79.8 80.7 81.5 82.2 84.6 16.5 28.8 29.2 29.8 29.5 32. 47.3 47.9 48.4 49.1 48.8 49.5 34.4 40.8 41.2 41.6 41.5 42.6 60.0 64.5 64.9 65.4 65.5 66.4 Table 1: Benchmark performance of fine-tuned models across different MoE architectures. Bold indicates best performance; underline indicates second-best. All methods maintain the same parameter budget (ravg = 64 for OLMoE, ravg = 16 for Phi). specific second-best method, DR-LoRA achieves gains of +1.2, +1.6, and +2.9 points respectively on these tasks. On Phi, DR-LoRA achieves +3.2, +4.8, and +1.6 improvements over LoRA on the same benchmarks, and +1.1, +2.4, and +0.4 over the task-specific second-best baseline. (2) Robust general performance: DRLoRA achieves the best overall performance, with average improvements of +1.8 points over LoRA on OLMoE and +1.9 points on Phi. Beyond taskaligned benchmarks, DR-LoRA maintains strong capabilities on general knowledge tasks (MMLU, HellaSwag, ARC-C). On Phi, DR-LoRA improves +2.2 points over LoRA on HellaSwag while matching the pretrained models performance on MMLU. (3) Growth beats pruning: DR-LoRAs incremental allocation strategy outperforms the pruningbased AdaLoRA approach by +1.1 average points on OLMoE and +0.9 points on Phi. This suggests that gradually allocating capacity to high-demand experts is more effective than pruning from overprovisioned initialization. Figure 2: Average accuracy on task-aligned benchmarks (GSM8k, HumanEval, IFEval) during training. DRLoRA establishes early superiority and maintains the advantage throughout training. We track performance throughout training at 6000step intervals. 4.2 Main Results Overall Performance. Table 1 presents the benchmark results at the end of training: (1) Strong task adaptation: DR-LoRA demonstrates significant improvements on benchmarks aligned with training emphasis. On OLMoE, DR-LoRA outperforms LoRA by +2.6 points on GSM8k (math reasoning), +5.0 on HumanEval (code generation), and +3.9 on IFEval (instruction-following). Compared to the taskTraining Dynamics. To understand the learning behavior, we track OLMoEs performance throughout training, averaged across GSM8k, HumanEval, and IFEval (Figure 2). DR-LoRA demonstrates early superiority and maintains its advantage throughout training, suggesting it rapidly identifies and prioritizes task-relevant experts while uniform LoRA spreads limited learning capacity across all experts equally. 6 Model Variant GSM8k HE IF w/o Routing Freq. w/o Rank Imp. Full DR-LoRA 31.4 32.1 32.8 38.0 37.8 39.1 30.3 29.9 32.7 Avg 33.2 33.3 34.9 Table 2: Ablation of saliency score components. Both routing frequency and rank importance contribute meaningfully to overall performance. Configuration GSM8k HE IF Router Frozen Router Unfrozen 31.6 32.8 36.7 39. 29.7 32.7 Avg 32.7 34.9 Table 3: Impact of router freezing. Unfreezing the router after warmup yields better results, enabling adaptive routing to evolved expert capabilities. 4.3 Ablation Study To validate each component in our saliency scoring mechanism, we evaluate three configurations: (1) Full DR-LoRA with both routing frequency and rank importance; (2) w/o Routing Frequency, using only rank importance (gℓ,i); (3) w/o Rank Importance, using only routing frequency (fℓ,i). Table 2 shows that full DR-LoRA consistently outperforms both ablated variants (+1.7 average points), confirming both components are necessary. To understand their distinct contributions, we compare the top-25% highest-ranked experts (16 out of 64 experts per layer) across configurations. Figure 7 in Appendix reveals that despite moderate overlap (74.6% and 84.8%), the ablated variants allocate high ranks to 65 and 39 different experts respectively, indicating that each component captures non-redundant information. This validates that the multiplicative saliency function (Eq. 9) effectively integrates complementary signals, data relevance (fℓ,i) and learning dynamics (gℓ,i), for optimal rank allocation. We compare freezing versus unfreezing the MoE router during fine-tuning. Table 3 shows that unfreezing the router after warmup improves performance (+2.2 average points), enabling adaptive routing to exploit evolved expert capabilities from heterogeneous rank allocation. All experiments unfreeze the router unless specified."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Expert-Task Alignment Analysis To verify that DR-LoRA indeed allocates more parameters to task-relevant experts, we conduct masking experiment by selectively disabling expert Figure 3: Performance degradation when masking expert subgroups. On math tasks (GSM8k), masking large experts causes 4 greater degradation than masking small experts, confirming task-aligned capacity allocation. On general knowledge (MMLU), both groups contribute similarly. subsets based on their final ranks and measuring the resulting performance degradation. We partition experts into large experts (top-25% by rank) and small experts (remaining 75%). For each model, we randomly mask experts from each group and evaluate the impact. We mask entire experts modules (including both pretrained weight matrices and LoRA modules), not just LoRA components. We experiment with three masking budgets: 256, 512, and 1024 ranks per layer (approximately 6%, 12%, and 25% of the 4096 total ranks per layer). Due to DR-LoRAs heterogeneous rank distribution, the same rank budget translates to different numbers of masked experts: masking 256 ranks disable 2 large experts (each with 128) but 4 small experts (each with 64). As shown in Figure 3, key findings emerge: (1) Disproportionate importance on aligned tasks: On GSM8k, masking large DR-LoRA experts causes substantially larger performance drops than masking small experts across all budgets, confirming DR-LoRA concentrates critical mathematical reasoning capacity in high-rank experts. (2) Uniform distribution on non-aligned tasks: On MMLU, masking large vs. small experts yields comparable degradation, indicating general knowledge remains uniformly distributed across experts. Notably, since LoRA parameters are negligible compared to expert parameters, masking small experts actually removes more total parameters. The disproportionate performance impact of large experts despite their smaller parameter footprint demonstrates effective capacity allocation: DRLoRA invests minimal parameters in task-critical experts to achieve maximal performance gains. 7 Model GSM8k HE IF Base Model OLMoE-LoRA DR-LoRA (100s) DR-LoRA (200s) DR-LoRA (500s) 6.1 30.2 32.4 32.8 32.1 22.1 34. 38.5 39.1 37.1 16.5 28.8 29.8 32.7 30.1 Avg 14.9 31.0 33.6 34.9 33. Table 4: Impact of rank growth interval. All three frequencies substantially outperform standard LoRA, with 200-step interval performing best. Model MedQA MedMCQA PubMed Weighted Avg Base LoRA DR-LoRA 31.2 43.6 46.0 21.8 31.5 35. 37.6 46.0 64.8 29.0 40.4 44.4 Table 5: Generality of DR-LoRA on domain-specific adaptation. We fine-tune Phi on medical QA datasets. DR-LoRA consistently outperforms standard LoRA across all medical benchmarks. maining responsive to task-specific patterns. Based on these results, all experiments use 200-step intervals unless otherwise specified. Figure 4: Expert activation heatmaps on different tasks. On GSM8k, DR-LoRA and LoRA exhibit distinctly different activation patterns. On MMLU, both methods activate largely overlapping expert sets. 5.2 Expert Activation Patterns 5.4 Domain Adaptation We track the most frequently activated experts (top8 by routing frequency) for both DR-LoRA and standard LoRA when evaluating on GSM8k and MMLU. Figure 4 reveals that on GSM8k, DRLoRA and LoRA activate largely different expert sets, while on MMLU, both methods show substantial overlap. This divergence on GSM8k confirms that DR-LoRA identifies and amplifies taskrelevant experts during training: by allocating more ranks to math-capable experts, DR-LoRA enables them to fully develop their specialization, while uniform LoRAs equal allocation prevents such targeted enhancement, resulting in fundamentally different expert utilization patterns. 5.3 Impact of Growth Interval We examine rank allocation intervals of 100, 200, and 500 steps. Table 4 shows all three frequencies substantially outperform standard LoRA (+1.1 to +2.2 average points), demonstrating robustness across growth schedules. More frequent updates allow faster adaptation to emerging task demands but may introduce instability during early training. Less frequent updates provide more stable training but may respond slowly to changing expert requirements. The 200-step interval achieves the best balance, providing sufficient time for importance scores to stabilize while reTo evaluate DR-LoRAs effectiveness on domainspecific adaptation, we fine-tune Phi on medical QA datasets. Detailed experimental configurations are provided in Appendix A.2. As shown in Table 5, DR-LoRA consistently outperforms standard LoRA across all benchmarks (+4.0 average points), with particularly strong gains on PubMedQA (+18.8 points). This demonstrates that DR-LoRAs dynamic rank allocation successfully generalizes to domain-specific tasks beyond general instruction following."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we address the resource mismatch problem in MoE adaptation where uniform LoRA rank allocation fails to account for expert functional specialization. We propose DR-LoRA , dynamic rank allocation framework that progressively grows expert LoRA ranks through an Expert Saliency Scoring mechanism integrating routing frequency and rank importance. Extensive experiments demonstrate that DR-LoRA consistently outperforms standard LoRA and pruning-based methods under the same parameter budget, effectively allocating capacity to task-relevant experts while maintaining robust general capabilities across different MoE architectures and tasks."
        },
        {
            "title": "Limitation",
            "content": "Although DR-LoRA demonstrate consistent improvements over existing methods, several limitations warrant discussion: (I) This study primarily validates the effectiveness of DR-LoRA in MoE LLMs that use top-k routing mechanism. Our Expert Saliency Scoring mechanism is based on routing frequency, but the effectiveness of this metric has yet to be verified in architectures with different routing strategies, such as Expert Choice Routing, or in other domains like multimodal MoE LLMs. (II) While our experiments demonstrate the methods generalizability across several models of varying scales and architectures, its scalability to much larger MoE models (e.g., those in the 100B+ parameter class) has not been explored. Therefore, extending the principles of DR-LoRA to more diverse MoE architectures and larger-scale models is an important direction for future work."
        },
        {
            "title": "Ethics Statement",
            "content": "This study adheres to the ethical guidelines set forth by our institution and follows the principles outlined in the ACM Code of Ethics and Professional Conduct. All datasets used in our experiments are publicly available."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, and 1 others. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. Mark Chen. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, and 1 others. 2024. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066. Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Wei Shen, Limao Xiong, Yuhao Zhou, Xiao Wang, Zhiheng Xi, Xiaoran Fan, and 1 others. 2024. Loramoe: Alleviating world knowledge forgetting in large language models via moe-style plugin. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19321945. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Chongyang Gao, Kezhen Chen, Jinmeng Rao, Ruibo Liu, Baochen Sun, Yawen Zhang, Daiyi Peng, Xiaoyuan Guo, and VS Subrahmanian. 2025. Mola: Moe lora with layer-wise expert allocation. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 50975112. Soufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024. Lora+: Efficient low rank adaptation of large models. arXiv preprint arXiv:2402.12354. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 otharXiv preprint ers. 2024. Mixtral of experts. arXiv:2401.04088. Dengchun Li, Yingzi Ma, Naizheng Wang, Zhengmao Ye, Zhiyuan Cheng, Yinghao Tang, Yan Zhang, Lei Duan, Jie Zuo, Cal Yang, and 1 others. 2024. Mixlora: Enhancing large language models finetuning with lora-based mixture of experts. arXiv preprint arXiv:2404.15159. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024a. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. 2024b. When moe meets llms: Parameter efficient finetuning for multi-task medical applications. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 11041114. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting 9 Cheng, and Min-Hung Chen. 2024c. Dora: Weightdecomposed low-rank adaptation. In Forty-first International Conference on Machine Learning. Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Yilun Liu, Yunpu Ma, Shuo Chen, Zifeng Ding, Bailan He, Zhen Han, and Volker Tresp. 2024d. Perft: Parameter-efficient routed fine-tuning for mixtureof-expert model. arXiv preprint arXiv:2411.08212. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, and 5 others. 2025. OLMoe: Open mixture-of-experts language models. In The Thirteenth International Conference on Learning Representations. Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations. Manxi Sun, Wei Liu, Jian Luan, Pengzhi Gao, and Bin Wang. 2024. Mixture of diverse size experts. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 16081621, Miami, Florida, US. Association for Computational Linguistics. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and 1 others. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, and 1 others. 2025. Kimi-vl technical report. arXiv preprint arXiv:2504.07491. An Wang, Xingwu Sun, Ruobing Xie, Shuaipeng Li, Jiaqi Zhu, Zhen Yang, Pinxue Zhao, Weidong Han, Zhanhui Kang, Di Wang, and 1 others. 2025. Hmoe: Heterogeneous mixture of experts for language modIn Proceedings of the 2025 Conference on eling. Empirical Methods in Natural Language Processing, pages 2195421968. Zihan Wang, Deli Chen, Damai Dai, Runxin Xu, Zhuoshu Li, and Yu Wu. 2024. Let the expert stick to his last: Expert-specialized fine-tuning for sparse architectural large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 784801, Miami, Florida, USA. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, and 1 others. 2022. Mixture-ofexperts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114."
        },
        {
            "title": "Reproducibility",
            "content": "A.1 Training Configurations A.1.1 Model Configurations OLMoE-1B-7B: 6.9B total parameters with 1.3B activated per forward pass. The model employs 16 layers with 64 experts per layer, activating the top-8 experts. Each expert has dimension 1024, with hidden size 2048. Phi-mini-MoE-instruct: 7.6B total parameters with 2.4B activated per forward pass. The model employs 32 layers with 16 experts per layer, activating the top-2 experts. Each expert has dimension 960, with hidden size 4096. A.1.2 Training Hyperparameters Table 6 presents the complete hyperparameter settings for all experiments. All methods use AdamW optimizer with learning rate 2 105, linear learning rate scheduler with 3% warmup, and weight decay 0.0. LoRA scaling factor α is set to 2 for all configurations. A.1.3 Growth Schedule and Router Training Growth window: Rank growth begins after the learning rate warmup phase (3% of total steps: 1,140 steps for OLMoE, 570 steps for Phi) and continues until 200 steps before training completion. This ensures newly activated ranks receive 10 Hyperparameter OLMoE Phi Benchmark Few-shot Decoding Split Standard LoRA Rank (r) DR-LoRA (ours) Initial rank (rinit) Maximum rank (rmax) Target rank (rtarget) Growth interval (Tgrow) Module growth fraction (pgrow) Usage EMA coefficient (β) Rank penalty exponent (γ) 64 32 128 64 200 0.1 0.9 1.2 16 8 32 16 200 0.1 0.9 1.2 MMLU BBH GSM8k HumanEval IFEval ARC-C HellaSwag MedQA MedMCQA PubMedQA 0-shot 3-shot 8-shot 0-shot 25-shot 10-shot 0-shot 0-shot 0-shot Greedy Greedy Greedy T=0.8 Greedy Greedy Greedy Greedy Greedy Greedy test test test test test test validation train validation train/test Training configuration Learning rate LR scheduler Warmup ratio Weight decay Optimizer LoRA α Epochs Total steps Micro-batch size Gradient accumulation Effective batch size Max sequence length 2 105 Linear 0.03 0.0 AdamW (fused) 2 1 38,000 1 4 16 4096 1 19,000 1 8 32 4096 Table 6: Complete hyperparameter settings for all experiments. sufficient training. Growth occurs every 200 steps within this window. Layer synchronization: All layers grow simultaneously at each growth event. The per-layer quota is computed as = (rtarget rinit)/Tevents, where =128 is the number of LoRA modules per layer (64 experts 2 projections) and Tevents is the number of scheduled growth events. Router training schedule: The MoE router remains frozen during the warmup phase to stabilize LoRA training. After warmup, the router is unfrozen and trained jointly with LoRA modules until training completion. This allows the router to adapt to the evolved expert capabilities from dynamic rank allocation. A.2 Evaluation Protocol A.2.1 Benchmark Settings All evaluations are conducted using the LM Evaluation Harness framework with vLLM backend (v1) for efficient inference. Table 7 summarizes the evaluation configuration for each benchmark. MMLU, BBH, GSM8k, IFEval, ARC-C, HellaSwag: We use the LM Evaluation Harness with vLLM backend for efficient batch inference. Model outputs are generated using greedy decoding (temperature = 0.0) with automatic batch size selection based on GPU memory. All models are evaluated Table 7: Evaluation settings for all benchmarks. All evaluations use greedy decoding (temperature = 0.0) except HumanEval which uses temperature 0.8 for sampling. with chat template applied (for instruction-tuned models) and maximum model length set to 4096 tokens. BBH uses chain-of-thought prompting format. GSM8k employs 8-shot chain-of-thought prompting. HumanEval: We evaluate code generation using pass@k metrics with {1, 10, 20}. For each of the 164 programming problems, we generate n=20 code samples using temperature 0.8 and max tokens 512. Following Chen (2021), we compute pass@k using the unbiased estimator: pass@k = EP roblems correct samples among total samples. We report mean and standard deviation across 3 independent runs with random seeds {42, 123, 456} to account for sampling variance. Medical benchmarks: We construct training and evaluation datasets from three medical QA sources using standardized chat format. where is the number of (nc ) (n k) 1 (cid:20) (cid:21) US MedQA: Medical Examination Licensing from GBaker/MedQA-USMLE-4-options. Questions are 4-option multiple choice. We use the train split as no official test split is available. questions MedMCQA: Indian medical entrance exam questions from medmcqa dataset. We evaluate on the validation split. Questions are 4-option multiple choice with optional explanations. PubMedQA: Biomedical literature QA from pubmed_qa (pqa_labeled subset). Questions require yes/no/maybe answers based on biomedical abstracts. We use available splits (train or test depending on dataset version). All medical evaluations use greedy decoding with vLLM for fast inference. Answer extraction employs pattern matching to identify option letters 11 (A/B/C/D) or yes/no/maybe responses from model outputs. Evaluation uses maximum 20 new tokens as answers are typically single letters or words. A.2.2 Data Splits and Reproducibility We use the standard test or validation splits provided by each benchmark through the LM Evaluation Harness and HuggingFace Datasets library. Specifically: MMLU, BBH, GSM8k, IFEval, ARC-C, HumanEval: Standard test splits HellaSwag: Validation split (as commonly used in the literature) Medical benchmarks: Train split for MedQA (no official test split), validation for MedMCQA, available splits for PubMedQA We do not perform early stopping; all models train for exactly 1 epoch (38,000 steps for OLMoE, 19,000 steps for Phi). This ensures fair comparison across methods without tuning stopping criteria. Checkpoint evaluation: We save model checkpoints every 6,000 steps during training and evaluate all intermediate checkpoints on the full benchmark suite. This allows us to track learning dynamics and verify stable convergence. Final results report performance at the last checkpoint. Multiple runs: All experiments are conducted with 3 independent training runs using random seeds {42, 123, 456}. A.3 Computational Infrastructure Hardware: All training experiments are conducted on single server with 4 NVIDIA L40S GPUs (48GB memory each). Distributed training: We use DeepSpeed ZeRO-2 for distributed training with the following configuration: Mixed precision: bfloat16 Gradient communication: overlap enabled Contiguous gradients: enabled Reduce bucket size: auto Optimization features: Flash Attention 2 (version 2.8.3) for memoryefficient attention computation Gradient checkpointing enabled to reduce memory consumption Fused AdamW optimizer for improved training speed Training time: Complete training times for 1 epoch are reported in Table 8. Method OLMoE Phi LoRA (r=64) LoRA (r=128) LoRA (r=16) DR-LoRA (ours) 39.7 hours 42.4 hours 43.2 hours 30.2 hours 32.7 hours Table 8: Wall-clock training time for 1 complete epoch on 4L40S GPUs. A.4 Dataset Details A.4.1 OLMoE SFT Mix OLMoE SFT Mix dataset combines the following sources with specified mixture weights: allenai/tulu-v2-sft-mixture-olmo-4096 (weight: 1.0) HuggingFaceH4/no_robots (weight: 1.0) meta-math/MetaMathQA (weight: 0.25) m-a-p/CodeFeedback-Filtered-Instruction (weight: 1.0) ai2-adapt-dev/daring-anteater specialized (weight: 1.0) The dataset emphasizes diverse instructionfollowing capabilities including general conversation, mathematical reasoning, and code generation. A.4.2 Medical QA Dataset For Phi experiments on medical domain adaptation, we construct specialized dataset combining three medical QA sources to create diverse medical instruction-following corpus: MedQA: US Medical Licensing Examfrom bigbio/med_qa ination questions (en_bigbio_qa config, train split). Questions are 4-option multiple choice extracted from USMLE practice exams covering clinical knowledge, diagnosis, and treatment. MedMCQA: Indian medical entrance exam questions from medmcqa (train split). Questions span anatomy, physiology, pharmacology, and clinical medicine with 4 options and optional explanations. 12 PubMedQA: Biomedical literature QA from pubmed_qa (pqa_labeled subset, train split). Questions derived from PubMed abstracts requiring yes/no/maybe answers with scientific explanations. Dataset construction: Each source is converted to standardized chat format with user/assistant message pairs. For multiple-choice questions (MedQA, MedMCQA), the prompt includes the question and options labeled A-D, with the response providing the correct answer letter and explanation. For PubMedQA, the prompt includes the biomedical context (truncated to 1200 characters if needed) and question, with yes/no/maybe responses and explanations. All datasets are combined and shuffled with seed 42 before training. This medical dataset tests DR-LoRAs ability to allocate capacity for domain-specific adaptation beyond general instruction following, particularly whether the method can identify and expand high-utility experts for specialized knowledge."
        },
        {
            "title": "B Computational Cost Analysis",
            "content": "B.1 Memory Analysis We empirically measure the GPU memory overhead introduced by DR-LoRAs rank reservation strategy. Under our experimental configuration, we observe that standard LoRA (r = 64) consumes approximately 40 GB per GPU, while DRLoRA (rmax = 128, rinit = 32, ravg = 64 at convergence) requires approximately 43 GB per GPU, resulting in 3 GB overhead (approximately 7.5% increase). This overhead consists of two components: (1) The static overhead of approximately 1.2 GB per GPU stems from allocating full parameter space for rmax = 128 ranks while activating only ravg = 64 on average. Under DeepSpeed ZeRO-2, parameters (bf16) are replicated across devices, contributing 538 MB, while gradients (bf16) and optimizer states (fp32) are sharded across 4 GPUs, contributing 134 MB and 538 MB respectively. (2) The dynamic overhead of approximately 1.8 GB per GPU arises from deliberate design choice in our forward pass computation. While masking could be applied before matrix multiplication to minimize activation memory, we implement masking after multiplication for two key reasons. First, premasking requires dynamic tensor indexing (e.g., A[mask, :]), which introduces irregular memory access patterns and prevents efficient GPU kernel fusion. Our post-multiplication approach enables standard GEMM operations that fully utilize tensor cores, maintaining computational efficiency. Second, this design integrates seamlessly with PyTorchs autograd and mixed-precision training, avoiding custom CUDA kernels while maintaining numerical stability. This design choice means forward activations are computed at full rmax dimensionality before selective masking in the backward pass. With gradient checkpointing enabled, intermediate activations across gradient accumulation steps, backward gradient buffers, and hook mechanisms for importance tracking collectively contribute to the dynamic overhead, along with memory fragmentation and framework-level bookkeeping inherent to distributed training systems. The 1.8 GB dynamic overhead represents 4.5% of total GPU memory and constitutes deliberate trade-off that eliminates computational bottlenecks from dynamic masking while remaining well within typical GPU budgets. This design enables DR-LoRA to maintain competitive training speed (see B.2) while achieving substantial performance gains. For extremely memory-constrained scenarios, practitioners can implement pre-masking strategies at the cost of increased training time, or reduce rmax to 1.5 rtarget to lower the overhead. B.2 Training Time Analysis We measure wall-clock training time for one complete epoch on the OLMoE SFT Mix dataset using 4L40S GPUs. As shown in Table 9, standard LoRA with = 64 completes training in 39.7 hours, while increasing rank to = 128 extends training time to 42.4 hours (1.07 baseline). DRLoRA, starting from rinit = 32 and growing to ravg = 64, requires 43.2 hours (1.09 baseline), achieving final performance of 42.6 points compared to 40.8 for = 64 and 41.3 for = 128. Method LoRA (r64) LoRA (r128) DR-LoRA Time 39.7h 42.4h 43.2h Rel. Rank Perf. 1.00 1.07 1.09 64 128 3264 40.8 41.3 42. Table 9: Wall-clock training time comparison. DR-LoRA incurs modest 9% training time overhead compared to standard LoRA (r = 64), which is comparable to simply using = 128 (7% overhead). This near-equivalence directly reflects 13 our design decision to prioritize computational efficiency through post-multiplication masking, as discussed in the memory overhead analysis. The additional 2 percentage points of overhead in DRLoRA represent the computational cost of dynamic rank allocation mechanisms, including importance scoring, expert usage tracking, and periodic rank growth, which are absent in static LoRA. B.3 FLOPs Analysis We analyze the computational cost of DR-LoRA in terms of floating-point operations (FLOPs) during training. The total FLOPs consist of base expert computation and LoRA adaptation. For single forward pass, base expert FLOPs are: FLOPsbase = 4BLK dm de (13) where B=4096 is the effective batch size, L=16 is the number of layers, K=8 is the number of activated experts per layer, dm=2048 is the hidden dimension, and de=1024 is the expert dimension. LoRA adds: FLOPsLoRA = 8BLK de (14) where is the LoRA rank and each expert has two LoRA modules (up_proj and down_proj). Rank Evolution During Training. Table 10 shows how DR-LoRAs average rank evolves during training. Starting from rinit = 32, the rank progressively grows through our dynamic allocation mechanism, reaching the target average of = 64 by step 37,800. The weighted average rank across the entire training process is 48.12, substantially lower than both LoRA (r = 64) and LoRA (r = 128). Step Avg Rank Progress Total Ranks 0 6,000 12,000 18,000 24,000 30,000 37,800 32.00 36.49 41.88 47.27 52.66 58.05 64.00 50.0% 57.0% 65.4% 73.9% 82.3% 90.7% 100.0% 65,536 74,736 85,776 96,816 107,856 118,896 131,072 Weighted average: 48.12 Table 10: Evolution of DR-LoRAs average rank during training (37,997 total steps). Progress indicates percentage toward target = 64. FLOPs Comparison. Table 11 presents the FLOPs analysis for different methods. DRLoRA has identical LoRA FLOPs to LoRA (r = 128). However, since base expert computation dominates total FLOPs (94%), the overall increase is only 5.9% compared to LoRA (r = 64), closely matching the observed 6.8% training time increase. Method Base LoRA Total Rel. No LoRA LoRA (r=64) LoRA (r=128) DR-LoRA 4398.0 4398.0 4398.0 4398.0 274.9 549.8 549.8 4398.0 4672.9 4947.8 4947.8 1.000 1.062 1.125 1.125 Table 11: FLOPs per sample (GFLOPs) for forward pass. Base expert computation dominates (94%), making LoRAs contribution relatively small (611%). B.3.1 Training Time vs. FLOPs Correlation Our wall-clock training time measurements  (Table 9)  align closely with FLOPs analysis. LoRA (r = 128) incurs +6.8% training time with +5.9% total FLOPs, demonstrating strong correlation. DRLoRA shows +8.8% training time with identical FLOPs to LoRA (r = 128), where the additional 2 percentage points represent computational overhead from dynamic rank allocation mechanisms (importance scoring, expert usage tracking, and periodic rank growth)."
        },
        {
            "title": "C Additional Experimental Results",
            "content": "C.1 Expert Rank Evolution Visualization To illustrate how DR-LoRA dynamically constructs heterogeneous rank distributions, we visualize the evolution of expert LoRA ranks throughout training in Figure 5. At the early stage (Epoch 0.16), most experts remain at the initial rank rinit = 32 (shown in yellow), with only few high-saliency experts receiving additional capacity (darker colors). By mid-training (Epoch 0.47), clear heterogeneous pattern emerges as DR-LoRA progressively allocates ranks to task-relevant experts based on routing frequency and learning intensity. At the final stage (Epoch 1.0), the rank distribution becomes highly differentiated, with some experts reaching the maximum rank rmax = 128 (dark red) while others remain at lower ranks, reflecting their varying importance to the target task. This progression demonstrates DR-LoRA ability to automatically discover and amplify task-relevant experts through dynamic capacity allocation, forming 14 From computational perspective, DRLoRA and LoRA (r = 128) have comparable training costs (43.2 vs. 42.4 h, as shown in Table 9), but DR-LoRA achieves significantly better performance. By accepting = 128-equivalent computational cost, DR-LoRA achieves 3.6 better performance improvement than LoRA (r = 128): +1.8 points versus +0.6 points above the = 64 baseline. This shows that where parameters are allocated matters substantially more than how many parameters are allocated. Rank Alloc. Strategy GSM8k HE IF Global Per-Layer 30.7 32.8 36.5 39.1 29.3 32.7 Avg 32.2 34.9 Table 12: Per-layer vs. global rank allocation. task-adaptive structure without manual intervention. C.2 Per-Layer vs. Global Rank Allocation We compare two rank allocation strategies: distributing the parameter budget globally across all layers versus independently within each layer. Figure 6(a) shows that expert saliency scores vary systematically across layers, with deeper layers exhibiting up to 6.12 higher average saliency due to gradient flow and abstract representations. Under global allocation, high-saliency deep-layer experts dominate rank allocation, causing resource concentration. Figure 6(b) quantifies this imbalance: while per-layer allocation maintains uniform distribution (exactly 8,192 ranks per layer), global allocation ranges from 6,480 to 9,072 ranks (31.6% deviation), under-provisioning shallow layers and over-provisioning deep layers. Table 12 shows per-layer allocation outperforms global allocation by +2.7 average points, validating that preventing resource concentration enables more effective adaptation. All DR-LoRA results use per-layer allocation. C.3 Comparison with Same-Rank LoRA To isolate the impact of dynamic rank allocation from simply having more parameter capacity, we compare DR-LoRA against standard LoRA with matched maximum rank. Specifically, we train standard LoRA with = 128 (matching DRLoRA rmax = 128) on OLMoE using the OLMoE SFT Mix dataset, while DR-LoRA grows from rinit = 32 to average rtarget = 64. Table 13 presents the results. Despite having only half the average active parameters (64 vs. 128), DR-LoRA outperforms the fixed = 128 baseline by +1.3 average points (42.6 vs. 41.3). This demonstrates that DR-LoRA performance gains stem from intelligent parameter allocation rather than simply having more parameters. The dynamic allocation mechanism successfully identifies and prioritizes task-relevant experts, achieving superior adaptation with substantially fewer active parameters. 15 Method Avg Rank (Max) MMLU HellaSwag BBH GSM8k ARC-C HumanEval IFEval Avg Base LoRA LoRA DR-LoRA 64 (64) 128 (128) 64 (128) 50.5 50.3 49.3 50.0 59.2 55.9 55.8 55. 32.3 34.5 34.0 34.5 6.1 30.2 32.0 32.8 54.2 51.9 52.0 53.2 22.1 34.1 35.5 39.1 16.5 28.8 30.7 32.7 34.4 40.8 41.3 42. Table 13: Performance comparison between DR-LoRA and same-rank LoRA on OLMoE. Despite having only half the average active parameters as LoRA with = 128, DR-LoRA achieves superior performance through intelligent dynamic allocation. Bold indicates best performance. Figure 6: Analysis of per-layer vs. global rank allocation strategies. (a) Expert saliency scores show systematic layer-wise heterogeneity, with deeper layers exhibiting substantially higher average saliency. (b) Under global allocation, this heterogeneity causes rank monopolization: deep layers receive up to +10.7% excess ranks while shallow layers suffer up to 20.9% underprovisioning, leading to inferior performance. Per-layer allocation prevents this imbalance by ensuring uniform allocation across layers. Figure 5: Evolution of expert LoRA ranks during DRLoRA training on OLMoE. Each heatmap shows the average rank per expert (averaged over up_proj and down_proj) at different training stages, with high-rank experts (darker red) concentrated in task-relevant positions. 16 Figure 7: Expert rank allocation comparison between full DR-LoRA and ablated variants. Top: Overlap with the variant without rank importance (using only routing frequency fℓ,i). Bottom: Overlap with the variant without routing frequency (using only rank importance gℓ,i). Each heatmap shows the top-25% highest-ranked experts (16 out of 64 per layer) across all layers."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "Peking University",
        "Tsinghua University",
        "University of Chinese Academy of Sciences"
    ]
}