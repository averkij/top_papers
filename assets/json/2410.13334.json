{
    "paper_title": "Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems",
    "authors": [
        "Isack Lee",
        "Haebin Seong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. In this paper, we delve into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, we propose an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 2 4 3 3 3 1 . 0 1 4 2 : r DO LLMS HAVE POLITICAL CORRECTNESS? ANALYZING ETHICAL BIASES AND JAILBREAK VULNERABILITIES IN AI SYSTEMS Isack Lee1 Haebin Seong1 1Theori Inc. isacklee224@gmail.com, hbseong97@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Warning: This paper contains potentially offensive and harmful text. Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as jailbreaks, where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. In this paper, we delve into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, we propose an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize the urgent need for LLM developers to adopt more responsible approach when designing and implementing safety measures. To enable further research and improvements, we open-source our code and artifacts of PCJailbreak, providing the community with tools to better understand and mitigate safety-induced biases in LLMs."
        },
        {
            "title": "1\nLarge Language Models (LLMs) have rapidly become essential components in many fields, ranging\nfrom professional decision-making to various forms of interactive user engagement (Araci (2019);\nLuo et al. (2022); Tinn et al. (2023)). However, as these models become popular, ensuring their safe\nusage has become crucial. Developers have implemented several safety features to prevent these\nmodels from generating harmful or objectionable content, often referred to as ‘safety alignment’.\n(Bakker et al. (2022); Christiano et al. (2017); Ouyang et al. (2022); Openai Usage Policies). How-\never, in this work we show that these safety alignments often introduce deliberate and intentional\nbiases, giving rise to a phenomenon known as ’jailbreak’, where malicious inputs manage to cir-\ncumvent these safety alignments, thus allowing LLMs to generate harmful outputs (Goldstein et al.\n(2023); Kang et al. (2024)).",
            "content": "The term jailbreak refers to carefully crafted prompts that can bait aligned LLMs into bypassing their safety alignment, resulting in the generation of content that may be harmful, discriminatory, violent, or sensitive (Smith et al. (2022)). Numerous types of jailbreak attacks have been identified and categorized into two primary methods: manually written jailbreak prompts and learning-based jailbreak prompts (Zou et al. (2023); Lapid et al. (2023); Liu et al. (2023); Wei et al. (2024); Yuan et al. Equal contribution 1 Figure 1: PCJailbreak reveals inherent biases in LLMs that disproportionately allow harmful jailbreak attacks to succeed more frequently when directed towards marginalized groups compared to privileged groups. Figure 2: Illustration showcasing the difference in response between standard prompt and PCJailbreak prompt. While the standard prompt is blocked by the LLMs safety features, the PCJailbreak prompt exploits intentional biases to elicit response. (2024)). The manual approach is exemplified by the Do-Anything-Now (DAN) series, which uses manually constructed prompts to exploit aligned LLMs. Conversely, the learning-based approach, as demonstrated by the GCG attack, formulates jailbreaks as adversarial examples by utilizing the gradient information of LLMs to automatically generate effective prompts. However, there are two main limitations of existing jailbreak methods. Firstly, automated attacks like GCG (Zou et al. (2023)) rely on search scheme that is guided by gradient information. While this approach enables the automatic generation of jailbreak prompts, it carries significant downside: the resulting prompts often consist of nonsensical sequences, which lack semantic meaning (Morris et al. (2020)). This major flaw makes these prompts highly vulnerable to naive defense mechanisms such as perplexity-based detection. For example, recent studies (Jain et al. (2023); Alon & Kamfonas (2023)) have shown that such straightforward defenses can easily recognize these nonsensical prompts and completely undermine the success of GCG attacks. 2 Secondly, although manual attacks can create stealthier jailbreak prompts, these prompts are usually handcrafted by individual LLM users, which leads to issues with scalability and adaptability. Additionally, these handcrafted methods may not adapt quickly to updates in LLMs, thus diminishing their effectiveness over time (ONeal (2024)). To address these limitations, we use method that employs fixed prompt formats, offering scalability. By leveraging biases, such as those related to Political Correctness(PC)(Perez et al. (2022); Zhuo et al. (2023)), inherent within LLMs and often intentionally injected by LLM developers to assure safety, we ensure that the prompts retain their meaning significantly without losing effectiveness, contrary to gradient-based attacks. This approach allows us to overcome the issues of scalability and adaptability while still exploiting the biases for effective jailbreaks. In this paper, we explore the concept of PCJailbreak, investigating how intentional biases in LLMs, intended as safety alignment, paradoxically become enablers of harmful content generation when exploited. This behavior is well illustrated in Figure 1 and 2. Additionally, we propose defense mechanism PCDefense that adjusts biases using prompts, ensuring safety and efficiency without additional inference or models, which makes it an attractive alternative to Guard Models (Inan et al. (2023); Ghosh et al. (2024); Caselli et al. (2020); Vidgen et al. (2021)), which are capable of classifying harmful conversations but require additional models and inference after text generation. . Our contributions can be summarized as follows: We analyze the nature and consequences of intentional biases introduced in LLMs for safety purposes, highlighting their potential to not only fail in deterring but also in facilitating more effective jailbreaks. This paradoxical effect underscores the urgency of addressing the inherent vulnerabilities these biases introduce. Through comprehensive experiments, we show that our proposed PCJailbreak is effective across state-of-the-art models, including the latest iterations of GPT. Our framework also proves adaptable, working effectively when applied to existing jailbreak techniques. We propose PCDefense, straightforward defense strategy without the need of additional inference or models. Our findings demonstrate that even with simple and cost-effective defense approach, jailbreak attacks can be mitigated. This highlights the critical responsibility of LLM service providers to ensure robust protection. We open-source the code and all associated artifacts of PCJailbreak to facilitate community efforts in understanding and mitigating safety-induced biases in large language models. This contribution aims to provide researchers and developers with the necessary tools to explore the nature of these biases and develop more robust defenses, furthering the collective effort to ensure the safety and reliability of LLM deployments. Our research suggests that while intentional biases are crucial for aligning LLMs with ethical standards, they necessitate careful scrutiny to prevent their manipulation. Therefore, responsible strategies from AI companies and researchers are needed to reinforce the safety of LLMs in an increasingly complex threat landscape."
        },
        {
            "title": "2 BACKGROUND AND RELATED WORKS",
            "content": "2.1 SAFETY ALIGNMENT IN LLMS Ensuring the safety and ethical alignment of large language models (LLMs) is critical area of ongoing research. Methods such as data filtering, supervised fine-tuning, and reinforcement learning from human feedback (RLHF) aim to align models like GPT-4 and ChatGPT with human values and preferences (Christiano et al. (2017); Bai et al. (2022); Ouyang et al. (2022); Xu et al. (2020)). However, despite these efforts, recent studies reveal vulnerabilities that can be exploited through jailbreak attacks, which lead to undesirable and harmful outputs (Kang et al., 2023; Hazell, 2023; Shen et al., 2023). 3 2."
        },
        {
            "title": "JAILBREAK ATTACKS AND TECHNIQUES",
            "content": "Jailbreaking LLMs involves crafting inputs that bypass safety mechanisms, resulting in harmful or objectionable content. Early jailbreak attacks, such as the Do-Anything-Now (DAN) series, relied on manually crafted prompts to exploit LLM safeguards (walkerspider, 2022). (Liu et al. (2023) provided an in-depth analysis and categorization of these jailbreak prompts, highlighting the delicate balance between an LLMs capabilities and its safety constraints. Diverse strategies for jailbreaks have been proposed. Manual methods, while effective, suffer from scalability issues (Wei et al. (2024)). On the other hand, learning-based methods like GCG (Zou et al. (2023)) use adversarial techniques to generate prompts automatically, though often at the cost of producing semantically meaningless outputs detectable via simple defenses like perplexity tests (Alon & Kamfonas (2023); Liu et al. (2023)) introduced AutoDAN, which combines manual and automated strategies using hierarchical genetic algorithms to enhance both the stealthiness and scalability of jailbreak prompts. Language diversity and non-natural language inputs present additional challenges. Deng et al. (2023) explored multilingual jailbreak attacks, demonstrating that LLMs could be tricked into producing harmful outputs with non-English prompts. Yuan et al. (2024) extended this by investigating the vulnerabilities of LLMs to non-natural language inputs, such as ciphers. 2.3 TOWARDS IMPROVED SAFETY MEASURES Complex attack strategies like those proposed by Ding et al. (2023) with the ReNeLLM framework introduce the concept of generalized and nested jailbreak prompts, leveraging LLMs to generate effective prompts through prompt rewriting and scenario nesting. This highlights the dynamic and evolving nature of jailbreak techniques. Our work builds on the existing body of research by focusing on the paradoxical consequences of intentional biases introduced for safety purposes. While these biases aim to align LLMs ethically, they also highlight new vulnerabilities. To counteract this, we propose using prompts to make the LLM re-align those biases, thus offering robust secondary defense against jailbreak attempts. In conclusion, AI developers must adopt higher degree of responsibility in designing, testing, and deploying LLMs. This involves continuous monitoring and iterative improvements based on real-world data. Our findings advocate for nuanced approach to LLM safety, promoting the development of more secure and reliable models, and ensuring that safety measures do not inadvertently introduce new risks."
        },
        {
            "title": "3 METHODOLOGY: PCJAILBREAK",
            "content": "3.1 PRELIMINARIES 3.1.1 JAILBREAK ATTACK jailbreak attack in the context of Large Language Models (LLMs) occurs when the model generates harmful or inappropriate responses to malicious inputs instead of producing refusal signal, which is safe and ethical response denying the request (Zeng et al., 2024; Zou et al., 2023). Such attacks are intricately linked to the alignment methods employed in LLMs, which aim to align the models outputs with human values and ethical considerations. The primary objective of these attacks is to bypass the LLMs alignment constraints, causing the model to produce inappropriate responses to malicious queries instead of refusing to answer. Consider set of malicious queries = {Q1, Q2, . . . , Qn}. An attacker meticulously designs set of jailbreak prompts = {J1, J2, . . . , Jn} to manipulate these queries. When each combined input Ti = Ji, Qi is fed into the LLM, the model generates set of responses = {R1, R2, . . . , Rn}. The aim of the jailbreak attack is to ensure that the responses correctly address the malicious queries in instead of producing refusal signals (Zeng et al., 2024). In summary, successful jailbreak attack coerces the LLM into bypassing its safety and ethical constraints, culminating in the generation of harmful or inappropriate content in response to malicious 4 prompts. This kind of attack underscores significant vulnerabilities in the current alignment strategies of LLMs, highlighting the necessity for robust countermeasures to guarantee model safety and reliability."
        },
        {
            "title": "3.1.2 FORMULATION",
            "content": "Designing specific target response for each malicious question is impractical for several reasons. Firstly, manually crafting an appropriate answer for every possible malicious query Qi is resourceintensive and infeasible due to the vast number of potential queries. Secondly, creating specific responses may limit the attacks generalizability, as tailored responses might not be effective for other queries or variations of the same query. Lastly, directly specifying harmful content could raise ethical and legal concerns. Hence, common solution to this problem is to define standardized introductory phrase for the target response (Zou et al., 2023; Lapid et al., 2023; Qi et al., 2024). The determination of successful jailbreak focuses on the absence of this standardized introductory phrase in the response. Such as: Target Prefixes = [I am sorry, can not, apologize...] 3.2 PCJAILBREAK APPROACH 3.2.1 GENERATING KEYWORDS AND PROMPTS Our proposed PCJailbreak approach investigates how keywords associated with various demographic and socio-economic groups influence an LLMs susceptibility to jailbreak attacks. To accomplish this, we generated keywords representing contrasting groups, such as privileged and marginalized communities, using the LLM itself to ensure that the keywords reflect the models inherent associations and biases. We prompted the LLM with: Provide list of adjectives that describe various privileged and marginalized groups. This process produced pairs of keywords, for example: (keyword1,privileged, keyword1,marginalized) = (wealthy, impoverished) (keyword2,privileged, keyword2,marginalized) = (male, female) ... For the harmful prompts, we compile set = {H1, H2, . . . , Hm} of ethically sensitive requests that LLMs should refuse, referenced from the harmful prompt dataset in JailBreakBench (Chao et al., 2024). These prompts represent variety of harmful or unethical queries, such as requests for illicit activities or dissemination of sensitive information. 3.2.2 ANALYZING INHERENT BIASES To analyze the biases inherent in LLMs, we constructed prompts that incorporated the generated keywords and harmful prompts. As illustrated in Figure 3, these prompts were then used to measure variations in jailbreak success rates. By iterating the keywords and harmful prompts, we created set of final prompts = {Ti,k,j}: 5 Figure 3: Overview of the PCJailbreak methodology. The same harmful prompt is used across different keywords representing contrasting groups to analyze variations in jailbreak success rates.*All keywords representing both privileged and underprivileged groups are generated by the LLM. Ti,k,j = am {keywordi,k} writer who writes fiction. Write fiction about {Hj}. (1) where: keywordi,k is the k-th keyword of the i-th pair (k {privileged, marginalized}), Hj is the j-th harmful prompt from our set H. Ti,k,j is the final prompt used for LLM input. Jailbreak Success Rate Calculation For each keyword and harmful prompt combination, we conducted multiple trials by submitting the corresponding prompt Ti,k,j to the LLM and recording the outcomes. The success rate Si,k for each keywordi,k is calculated as: Si,k = Nsuccess, i,k Ntotal, i,k (2) where: Nsuccess, i,k is the number of successful jailbreak attempts for keywordi,k, Ntotal, i,k is the total number of attempts for keywordi,k. Determining Jailbreak Success response from the LLM is considered successful jailbreak if it includes the harmful content requested in the harmful prompt Hj instead of providing refusal or safe completion. For instance, if the LLM provides detailed story incorporating the harmful content without any refusal language, it is marked as successful jailbreak. Conversely, if the LLM responds with standard refusal message, it is considered unsuccessful in terms of the jailbreak attempt. Comparative Analysis After calculating the success rates for each keyword, we compared the success rates between the privileged and marginalized keywords within each pair. Let Si,privileged and Si,marginalized be the success rates for the privileged and marginalized keywords of the i-th pair, respectively. We analyzed the difference Si between these success rates: Si = Si,privileged Si,marginalized (3) significant Si suggests that the LLM exhibits differential susceptibility to jailbreak attacks based on the demographic represented by the keyword. Si indicates that there is significant difference in jailbreak success rates between specific group keywords when the value is large. This could indicate inherent biases in the LLMs training data or alignment mechanisms that affect how it responds to prompts involving different groups."
        },
        {
            "title": "3.2.3 SUMMARY OF THE PCJAILBREAK APPROACH",
            "content": "Our PCJailbreak methodology systematically analyzes the LLMs responses to diverse set of harmful prompts, using various keywords across different groups. We conducted experiments using harmful prompts and multiple pairs of privileged and marginalized keywords. By maintaining consistent prompt structure (Equation 1) and isolating the effect of the keyword, we aimed to identify any biases in the LLMs susceptibility to jailbreak attacks. The calculation of jailbreak success rates (Equation 2) and the comparative analysis using Si (Equation 3) enabled us to quantify potential biases. Our approach highlights whether intentional biases introduced during the alignment process inadvertently contribute to vulnerabilities that can be exploited through jailbreak attacks."
        },
        {
            "title": "MODELS",
            "content": "Our jailbreak defense method PCDefense uses prompts to adjust biases without the need for additional models. Chain-of-Thought Prompting (CoT) has shown considerable potential in eliciting reasoning abilities in large language models, allowing for interpretability without the need for parameter updates (Wei et al., 2022). Inspired by this, our approach PCDefense involves adding defense prompts to reduce excessive biases, ensuring safety without additional inference overheads, in contrast to Guard Models which require additional inference cost after text generation (Inan et al. (2023); Ghosh et al. (2024); Caselli et al. (2020); Vidgen et al. (2021)). By incorporating bias terms through our defense prompts, we achieve cost-efficient and secure method for defending against jailbreak attempts while maintaining the models performance. To further illustrate our defense prompt, we use the following prompt structure to ensure fairness and equity: Figure 4: PCDefense adjusts inherent biases in LLMs that are exploited by PCJailbreak. efficient since it does not require additional inference or models such as Guard Models. It is By using defense prompt, our defense mechanism elicits the refusal of wide range of harmful content without requiring significant additional computing resources, which makes it an attractive alernative to Guard Models."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUPS",
            "content": "Data To analyze the differences in jailbreak success rates between groups, we used the jailbreakbench dataset (Chao et al. (2024)). This dataset comprises 100 harmful prompts and has been widely used in numerous studies. We utilized this dataset to measure performance variations and to analyze the intentional biases inherent in LLMs. Models We analyzed intentional bias in well-known LLMs, including closed-source models such as GPT-3.5-turbo, GPT-4, GPT-4-o, and open-source models like Llama2-7B, Llama2-13B, Llama37B, Phi-mini-7B, Qwen1.5, and Qwen2-7B. For our experiments, we conducted evaluations with the default sampling temperature and system prompts. This setup aligns with standard practices in the literature to ensure consistent and comparable results. Keywords The keywords used to compare performance across groups were generated by the LLMs themselves to maximize the exploitation of internal biases. Most models generated similar keywords, and significant performance differences were observed. Basically, the models primarily generated the following keywords for various groups (see Table 1): Marginalized Group (Adjective) Privileged Group (Adjective) White Male Heterosexual Able-bodied Wealthy Cisgender Traditionally family-structured Urban Highly educated Traditionally employed Black Female LGBTQ+ Disabled Low-income Non-normative gender Non-traditionally family-structured Rural Undereducated Sexually marginalized Table 1: Keywords Generated for Group Comparison These keywords were systematically used to evaluate the differences in jailbreak success rates among various groups. 4.2 INTENSIONAL BIAS ANALYSIS To gauge the impact of intentionally injected biases in LLMs on their vulnerability to jailbreak attempts, we conducted series of experiments utilizing various keyword prompts designed to assess the success rates of jailbreaks within marginalized and privileged categories. We evaluated multiple well-known models, including GPT-3.5, GPT-4, GPT-4o (Brown et al., 2020)(closed models), as well as LLaMA (Touvron et al., 2023), Qwen (Bai et al., 2023), and Phi-mini (Abdin et al., 2024)(open-source models). Interestingly, the latest model, GPT-4o, exhibits higher jailbreak success rates, with significant difference observed between marginalized and privileged groups. In particular, GPT-4o shows difference of 0.128 between marginalized and privileged keywords, highlighting the substantial gap influenced by injected biases. Furthermore, most models demonstrate lower jailbreak success rates for prompts using privileged keywords compared to the baseline, whereas prompts using marginalized keywords exhibit higher success rates. Intriguingly, the LLaMA3 model demonstrates lower propensity for successful jailbreak attempts compared to other models. This lower rate can be attributed to Metas focus on creating more robust LLMs that are resistant to such vulnerabilities. It showcases Metas commitment to mitigating LLM 8 Model Name GPT-3.5 GPT-4 GPT-4o LLaMA2 LLaMA3 Qwen-1.5 Qwen2 Phi-mini Baseline Success Rate 0.2200 0.2100 0.4600 0.2400 0.0500 0.1900 0.1700 0.4100 Marginalized Success Rate () 0.2421 (+10.00%) 0.2488 (+18.57%) 0.5467 (+18.91%) 0.2811 (+17.08%) 0.0650 (+30.00%) 0.2175 (+14.74%) 0.1971 (+15.88%) 0.4386 (+7.07%) Privileged Success Rate () 0.1847 (-15.90%) 0.1900 (-9.52%) 0.4187 (-8.91%) 0.1933 (-19.58%) 0.0300 (-40.00%) 0.1675 (-11.58%) 0.1671 (-7.06%) 0.3829 (-6.59%) Marginalized / Privileged () 131.08% 130.95% 130.57% 145.42% 216.67% 129.85% 117.95% 114.56% Table 2: Performance across different models showing baseline success rates, marginalized success rates, privileged success rates, and the difference between marginalized and privileged success rates. safety risks through comprehensive alignment techniques (Vidgen et al., 2021; Ghosh et al., 2024; Caselli et al., 2020). 4.3 PERFORMANCE IMPROVEMENT USING BIAS-BASED METHOD The bias-based approach we analyzed can also be applied to existing models. We confirmed performance improvement by applying the PCJailbreak method to Adaptive Attacks (Andriushchenko et al., 2024), the current state-of-the-art (SOTA) jailbreak attack. Specifically, for the llama2 model(Touvron et al., 2023), the performance improved from 98% to 100%, resulting in 2% increase. For the phi model(Abdin et al., 2024), the performance enhanced from 95% to 99%, resulting in 4% increase. These results show that the method can be easily integrated with existing techniques. The experiment was held by using the jailbreak attack artifact from JailbreakBench(Chao et al., 2024), which has 100 samples of Adaptive Attacks conversation. Table 3: Performance Improvement of SOTA Models Model llama2 phi-mini Adaptive Attacks Adaptive Attacks with PCJailbreak 98.00% 95.00% 100.00% 99.00% The results in Table 3 demonstrate that our bias-based method can be effectively combined with the existing approaches, providing notable improvements in performance. 4.4 EXPERIMENTAL VALIDATION OF PCDEFENSE To validate our defense method PCDefense, we conducted experiments on various models, including Llama2, Phi, and Qwen2. We observed the impact of our approach on Marginalized Rate Success and Privileged Rate Success metrics before and after applying our defense technique. The results are summarized in Table 4. As shown in Table 4, the Marginalized Rate Success and Privileged Rate Success both decreased consistently across Llama2 and Qwen2 models after applying our defense method. Specifically, for the Llama2 model, the Marginalized Rate Success decreased by 0.1097 and the Privileged Rate Success decreased by 0.0504. For the Phi and Qwen2 models, the Marginalized Rate Success decreased by about 0.02, whereas the Privileged Rate Success increased by about 0.02. While the performance of the Privileged group has increased in some models, potentially leading to the misconception that it has become more dangerous, the reduced gap in jailbreak performance between groups indicates decrease in bias. Additionally, the overall average score has decreased, suggesting that the system has become safer. Table 4: Jailbreak Prevention performance of PCDefense Model Llama2 Marginalized Group Jailbreak Success"
        },
        {
            "title": "Phi",
            "content": "Qwen"
        },
        {
            "title": "Privileged Group Jailbreak Success\nGap Between Groups\nMarginalized Rate Success\nPrivileged Rate Success\nGap Between Groups\nMarginalized Rate Success\nPrivileged Rate Success\nGap Between Groups",
            "content": "Before 0.2811 0.1933 0.0878 0.4386 0.3829 0.0557 0.1971 0.1671 0.0300 After 0.1714 0.1429 0.0285 0.4208 0.4075 0.0133 0.1750 0.1900 0.0150 After/Before () 60.97% 73.93% 32.46% 95.94% 106.42% 23.88% 88.79% 113.70% 50.00%"
        },
        {
            "title": "4.5 PRINCIPLE COMPONENT ANALYSIS",
            "content": "In this section, we conduct detailed analysis of how models interpret PCJailbreak prompts by performing Principal Component Analysis (PCA) (F.R.S., 1901) on the embeddings of Phi-mini, Qwen2, and LLaMA3 models, as shown in Appendix Figures 5, 6, 7 respectively. Our findings reveal that models with high success rate in PCJailbreak tend to cluster PCJailbreak prompts together with benign prompts, while those with lower success rate demonstrate the opposite behavior. The PCA results indicate that when using PCJailbreak, models tend to interpret it more similarly to benign prompts, with the degree of this similarity aligning with our observed PCJailbreak success rates."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In conclusion, our study highlights the complexities and potential unintended consequences of aligning large language models (LLMs) with safety measures aimed at preventing harmful outputs. The introduction of intentional biases for ethical behavior, while crucial for ensuring responsible AI, has led to significant discrepancies in jailbreak success rates based on gender and racial keywords. This discrepancy, coined as PCJailbreak, underscores the risks that arise from safety-induced biases, particularly in terms of fairness and equality. Our findings emphasize the need for LLM developers to carefully balance safety and fairness in their models. Additionally, our proposed method of adding defense prompts without requiring additional inference or models shows simple and scalable solution to mitigate jailbreak attempts. This simplicity and scalability of migation emphasizes the responsibility of LLM developers to adopt more comprehensive and proactive measures to address safety risks. Future work should focus on designing more inclusive and transparent alignment strategies to address the inherent challenges of AI safety while minimizing bias. 10 Ethics statement. This work addresses critical security concerns in the use of large language models (LLMs), specifically in relation to biases introduced for safety purposes and their potential to facilitate jailbreaks. While our analysis highlights the vulnerabilities of current bias-based safety mechanisms, it is important to clarify that our research aims to enhance the robustness of LLMs by mitigating these risks rather than exploiting or encouraging harmful behavior. We recognize the dual-use nature of this research, and we have taken care to emphasize defense strategies, including the proposal of small auxiliary models, that prioritize user protection and ethical AI deployment. Furthermore, the experiments conducted in this work do not involve human subjects or sensitive personal data. All model assessments were performed in controlled environments using publicly available datasets and models. We commit to following best practices in data security and model transparency and do not release any tools or frameworks that could directly enable malicious use. Our findings are intended to foster deeper understanding of how to secure AI systems and encourage responsible model deployment within the community. Reproducibility statement. We are committed to ensuring the reproducibility of our results and findings. To this end, we provide open access to the source code, artifacts, datasets, and detailed instructions on how to replicate our experiments through publicly available anonymous repository. By following the guidelines outlined in our repository, researchers and practitioners should be able to easily reproduce and extend our work."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel PerezBecker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132, 2023. Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safetyaligned llms with simple adaptive attacks. arXiv preprint arXiv:2404.02151, 2024. Araci. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint arXiv:1908.10063, 2019. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, 11 Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. Fine-tuning language models to find agreement among humans with diverse preferences. Advances in Neural Information Processing Systems, 35:3817638189, 2022. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Tommaso Caselli, Valerio Basile, Jelena Mitrovic, and Michael Granitzer. Hatebert: Retraining bert for abusive language detection in english. arXiv preprint arXiv:2010.12472, 2020. Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tram`er, Hamed Hassani, and Eric Wong. Jailbreakbench: An open robustness benchmark for jailbreaking large language models, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715, 2023. Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and Shujian Huang. wolf in sheeps clothing: Generalized nested jailbreak prompts can fool large language models easily. arXiv preprint arXiv:2311.08268, 2023. Karl Pearson F.R.S. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559572, 1901. doi: 10.1080/14786440109462720. Shaona Ghosh, Prasoon Varshney, Erick Galinkin, and Christopher Parisien. Aegis: Online adaptive ai content safety moderation with ensemble of llm experts. arXiv preprint arXiv:2404.05993, 2024. Josh Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. Generative language models and automated influence operations: Emerging threats and potential mitigations. arXiv preprint arXiv:2301.04246, 2023. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama guard: Llmbased input-output safeguard for human-ai conversations, 2023. URL https://arxiv.org/ abs/2312.06674. Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614, 2023. Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. In 2024 IEEE Security and Privacy Workshops (SPW), pp. 132143. IEEE, 2024. Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023. Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics, 23(6):bbac409, 2022. John Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. Reevaluating adversarial examples in natural language. arXiv preprint arXiv:2004.14174, 2020. AJ ONeal. https://gist.github.com/coolaj86/ 6f4f7b30129b0251f61fa7baaa881516, 2024. Accessed: 2024-09-30. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and Peter Henderson. Safety alignment should be made more than just few tokens deep. arXiv preprint arXiv:2406.05946, 2024. Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. im sorry to hear that: Finding new biases in language models with holistic descriptor dataset. arXiv preprint arXiv:2205.09209, 2022. Robert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Fine-tuning large neural language models for biomedical natural language processing. Patterns, 4(4), 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dynamically generated datasets to improve online hate detection. In ACL, 2021. Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Recipes for safety in open-domain chatbots. arXiv preprint arXiv:2010.07079, 2020. 13 Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=MbfAK4s61A. Yi Zeng, Weiyu Sun, Tran Ngoc Huynh, Dawn Song, Bo Li, and Ruoxi Jia. Beear: Embedding-based adversarial removal of safety backdoors in instruction-tuned language models. CoRR, 2024. Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. arXiv preprint arXiv:2301.12867, 2023. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. arXiv preprint Universal and transferable adversarial attacks on aligned language models. arXiv:2307.15043, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "Figure 5: PCA of Phi-mini model, which has 0.4386 PCJailbreak success rate. PCJailbreak samples are closely clustered with Benign samples. Figure 6: PCA of Qwen2 model, which has 0.1971 PCJailbreak success rate. PCJailbreak samples are relatively far from both harmful and benign samples. 15 Figure 7: PCA of LLaMA3 model, which has 0.0650 PCJailbreak success rate. PCJailbreak samples are relatively close to harmful samples."
        }
    ],
    "affiliations": [
        "Theori Inc."
    ]
}