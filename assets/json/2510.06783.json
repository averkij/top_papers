{
    "paper_title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
    "authors": [
        "Akshit Singh",
        "Shyam Marjit",
        "Wei Lin",
        "Paul Gavrikov",
        "Serena Yeung-Levy",
        "Hilde Kuehne",
        "Rogerio Feris",
        "Sivan Doveh",
        "James Glass",
        "M. Jehanzeb Mirza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks."
        },
        {
            "title": "Start",
            "content": "TTRV: TEST-TIME REINFORCEMENT LEARNING FOR VISION LANGUAGE MODELS Akshit Singh1 Shyam Marjit2 Wei Lin3 Paul Gavrikov1 Serena Yeung-Levy4 Hilde Kuehne5,6 Rogerio Feris6 Sivan Doveh 1Independent Researcher 5Tübingen AI Center James Glass7 M. Jehanzeb Mirza7 2IISc Bangalore 3JKU Linz 4Stanford 6MIT-IBM Watson AI Lab 7MIT CSAIL 5 2 0 2 8 ] . [ 1 3 8 7 6 0 . 0 1 5 2 : r Project Page: https://akshit21112002.github.io/ttrvproject/"
        },
        {
            "title": "ABSTRACT",
            "content": "Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, setup that contrasts with In this work, we propose how humans learn directly from their environment. TTRV to enhance visionlanguage understanding by adapting the model on-thefly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base models output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the models output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets. Remarkably, on image recognition, TTRV applied to Intern-VL-8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks. never teach my pupils; only attempt to provide the conditions in which they can learn.\" Albert Einstein Figure 1: Test-Time RL for VLMs. (left) Unlike prior methods that require pre-training splits and post-training via Supervised Finetuning (SFT) or Reinforcement Learning (RL), our approach extracts reward signals directly at test time from unlabeled data. The reward combines (1) frequencybased signals and (2) diversity control, allowing the model to adapt online and improve downstream vision performance without any labeled data. (right) Test accuracy increases while entropy of the output logits decreases, showing that the model becomes more accurate and less uncertain as testtime RL progresses. The solid lines represent the mean, and shaded regions represent the variance of results obtained over 5 independent runs. The dataset is Resics45 (Cheng et al., 2017), task is object recognition, and the model is InternVL-3-2B (Zhu et al., 2025)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in visionlanguage models (VLMs) (Radford et al., 2021; Xu et al., 2023; Zhai et al., 2023; Liu et al., 2023b; Li et al., 2024; OpenAI, 2023) have enabled impressive progress on tasks such as object recognition (Deng et al., 2009; Nilsback & Zisserman, 2008) and visual question answering (Fang et al., 2021; Yin et al., 2023). Yet, unlike humans, who continuously refine their reasoning by interacting with the world and adapting to ambiguous, unlabeled experiences, current VLMs remain largely static once trained. Adaptation typically requires large amounts of annotated data and costly fine-tuning, limiting their ability to cope with new domains or unseen tasks. Reinforcement Learning (RL) has shown promise for improving reasoning in Large Language Models (LLMs) (Shao et al., 2024) and VisionLanguage Models (VLMs) (Yu et al., 2025), and has emerged as an effective post-training method for enhancing task-specific performance. However, most existing approaches still rely on reward signals derived from human-labeled data and remain restricted to curated training splits, which are misaligned with real-world scenarios where traintest distinctions do not naturally exist. This dependence raises fundamental question: If RL is to embody true learning from experience, should it not arise directly from interaction with unlabeled data in the wild, rather than from curated benchmarks? In this work, we move toward this vision by proposing Test-Time Reinforcement Learning framework for Vision Language Models (TTRV) that learns directly from unlabeled test data. Our TTRV extracts reward signals for Group Relative Policy Optimization (GRPO) (Shao et al., 2024) directly on the test data, as it is encountered. Specifically, our proposed reward formulation consists of two distinct parts, based on frequency and diversity control of the pre-trained models output for each test sample. The intuition is to encourage the model to frequently produce similar outputs for each test sample and reward the predictions of the model which are more frequent and at the same time, control the diversity of models output by rewarding lower entropy of the output empirical distribution. An overview of our work, along with one optimization trajectory, is provided in Figure 1. This approach transforms static pretrained VLMs into dynamic systems capable of self-improvement at inference time, bringing RL for multimodal models closer to the human-like paradigm of learning through raw experience. We extensively evaluate our TTRV across 16 datasets spanning two tasks: image recognition and visual question answering (VQA). These datasets cover diverse range of domains, including finegrained recognition, math reasoning, and general VQA. Our results show that TTRV consistently improves performance, generalizes across model families, and is remarkably data-efficient. For instance, when post-training the InternVL3 (Chen et al., 2024b) model on only 20 randomly sampled test images, GRPO achieves gains of up to 52.4% (42.3% on large-scale ImageNet (Deng et al., 2009)). Similarly, on VQA benchmarks, TTRV boosts performance by as much as 28.0% on AI2D (Kembhavi et al., 2016). Remarkably, our TTRV outperforms one of the strongest proprietary models (GPT-4o) by 2.3% on average (over 8 datasets) for image classification, while remaining highly competitive for VQA. Beyond these gains, our ablation studies uncover several interesting properties of GRPO for visionlanguage understanding. Notably, GRPO improves cross-dataset generalization: training on one dataset can yield strong gains on completely unrelated dataset. Further, even in extremely data-scarce scenarios, GRPO remains effective, achieving up to 5.5% improvement from rewards extracted on single randomly chosen example. These findings suggest that GRPO does not simply adapt to datasets distribution but instead activates latent capabilities already learned during large-scale pretraining. To conclude, we summarize the contributions of our work as follows: We introduce the first test-time reinforcement learning framework for visionlanguage models, which can be bootstrapped to any pre-trained VLM. Powered by carefully designed reward formulations, our method adapts models on the fly without requiring supervised data, thus realizing the true promise of RL. Through extensive experiments on 16 diverse benchmarks, we demonstrate that our proposed TTRV provides consistent and substantial improvements across tasks, model families, and domains. 2 Our ablation studies further uncover novel properties of GRPO for VLMs, such as effectiveness in extremely low-data regimes and cross-dataset generalization, opening up new directions for future research in test-time adaptation with reward-driven learning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our work is closely related to vision-language models and works that study RL-based fine-tuning and test-time training (TTT) for VLMs. VLMs. Recent progress in visionlanguage modeling has led to two major families of approaches. The first is dual-encoder models, where separate vision and text encoders are trained jointly, typically in contrastive setting. These models excel at recognition-oriented tasks, with representative examples including CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), OpenCLIP (Schuhmann et al., 2022), SigLIP (Zhai et al., 2023), and MetaCLIP (Xu et al., 2023), as well as numerous extensions for downstream applications (Mirza et al., 2024; 2023c; Doveh et al., 2023b;a; Lin et al., 2023; Mirza et al., 2023a; Pathak et al., 2025). The second family, often referred to as large multimodal models (LMMs), couples vision encoder with large language model (LLM), enabling open-ended multimodal reasoning for tasks such as captioning, visual question answering (VQA), and document understanding. Pioneering approaches in this direction include BLIP-2 (Li et al., 2023b), InstructBLIP (Dai et al., 2023), MiniGPT (Zhu et al., 2024; Chen et al., 2024a), and the LLaVA series (Liu et al., 2023b; 2024; 2023a; Li et al., 2024). More recent models have pushed these capabilities even further: Qwen-2.5 VL (Bai et al., 2025) advances visual understanding by supporting precise object localization, dynamic resolution processing and strong agentic capabilities such as tool execution. InternVL3 (Zhu et al., 2025) improves perception and reasoning through native multimodal pretraining and domain-specific data such as 3D scenes, GUIs, and video. Phi-3.5 Vision (Abdin et al., 2024) offers lightweight but strong alternative with long-context reasoning (128K tokens), robust vision inputs (images, charts, documents), and improved alignment via preference optimization. Several recent studies (Doveh et al., 2024; Gavrikov et al., 2024; Lin et al., 2024; Huang et al., 2024; Mirza et al., 2025) have further enhanced these models through improved training or adaptation strategies. In this work, we target the most recent open-source LMMs and focus on improving their test-time adaptability for vision-centric tasks such as object recognition, which remains key weakness highlighted in prior studies (Zhang et al., 2024b; Mirza et al., 2025). RL-based fine-tuning for VLMs. RL has become central paradigm for aligning large language models with human preferences and task objectives, with approaches such as RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2023) improving safety, coherence, and instruction-following in both LLMs and VLMs. More recently, rule-based methods like GRPO (Shao et al., 2024) have demonstrated the feasibility of scaling RL to enhance reasoning capabilities. Building on this foundation, RL-based fine-tuning (RFT) has been extended to multimodal models across variety of vision-driven tasks. For example, VLM-R1 (Shen et al., 2025), VisualThinker-R1-Zero (Zhou et al., 2025), and Perception-R1 (Yu et al., 2025) adapt RFT for open-vocabulary object recognition, spatial reasoning, and visual perception, respectively; CLS-RL (Li et al., 2025) applies RFT to few-shot image classification; while R1-VL (Zhang et al., 2025) and related efforts further refine multimodal reasoning. These works demonstrate that RL can significantly enhance vision-centric capabilities of VLMs, but they still rely on curated training splits or labeled feedback. In contrast, our work investigates how reinforcement learning can be performed at test time, directly from unlabeled test data, thus bringing RL for VLMs closer to human-like learning from raw experience. Test-time training (TTT). TTT methods adapt model parameters at inference without requiring labeled test data, typically by optimizing surrogate objectives such as entropy minimization or auxiliary self-supervised losses (Sun et al., 2020; Gandelsman et al., 2022; Sun et al., 2024; Mirza et al., 2023b; 2022). This idea is also similar to the recently popular Test-Time Scaling paradigm for LLMs (Snell et al., 2024). While first explored in unimodal settings, TTT has since been extended to multimodal models, with most efforts focusing on dual-encoder VLMs (e.g., CLIP (Radford et al., 2021)). Representative approaches include TPT (Shu et al., 2022), which tunes text prompts via entropy minimization on augmented inputs, with extensions such as DiffTPT (Feng et al., 2023) and C-TPT (Yoon et al., 2024) improving augmentation quality and calibration; and RLCF (Zhao et al., 2023), which adapts the image encoder using feedback from larger models. Other works pursue lightweight black-box strategies that adapt embeddings without modifying internal parameters. 3 Figure 2: Overview of TTRV. For each prompt x, the VLM generates candidate responses {ˆy1, . . . , ˆyN } from its policy πθ(x). These samples induce an empirical distribution over the unique outputs {y1, . . . , yM }, from which two reward signals are derived: (i) frequency-based reward, where each response yj is rewarded in proportion to how often its output occurs among the responses (i.e., its empirical probability in the distribution), and (ii) diversity control reward, computed from the distribution to regulate diversity and encourage convergence. The final reward is the weighted combination of these terms, which is used to update the policy via GRPO. Most closely related to our work, TTRL (Zuo et al., 2025) introduces the idea of using reinforcement learning at test time for LLMs, where the majority voting across sampled outputs serves as surrogate reward. While sharing the same high-level motivation, our TTRV differs greatly by extending the paradigm to multimodal models and combining frequency-based rewards with entropy regularization to balance consistency and diversity in predictions. The novel reward formulation helps us achieve better performance than the naïve majority voting (c.f., Ablations Section 4.3). Further, in contrast to VLM-based approaches that primarily target dual-encoder VLMs or prompt-level adaptation, our work focuses on decoder-based VLMs. To the best of our knowledge, our TTRV is the first framework to leverage GRPO for test-time RL of VLMs. Adapting models through RL at inference presents unique challenges, particularly in designing reward functions that operate in fully unsupervised setting. We address this by rewarding predictions based on their frequency among the models own outputs, while simultaneously regularizing diversity by rewarding models certainty obtained by calculating the entropy of the empirical probability distribution. This formulation enables TTRV to achieve consistent improvements across diverse tasks and benchmarks."
        },
        {
            "title": "3 TTRV: TEST TIME RL FOR VLMS",
            "content": "The goal of our proposed TTRV is to improve downstream vision tasks by extracting reward signals directly from unlabeled test data as it is encountered. To this end, we bootstrap an off-the-shelf VLM (e.g., InternVL (Zhu et al., 2025)) with Group Relative Policy Optimization (GRPO) (Shao et al., 2024). key contribution of our work lies in the design of fully unsupervised reward signals. At high level, we introduce two complementary rewards: (i) frequency-based reward that encourages consistent answers from the base VLM, and (ii) an entropy-based reward that regularizes the diversity of responses. An overview of our approach is provided in Figure 2 and comprehensive python-like pseudo-code is provided in Appendix E, while the codebase is provided as supplementary .zip file for review. For ease of assimilation, the following subsections first provide brief recap of Group Relative Policy Optimization (Section 3.1), then describe our proposed reward formulations in detail (Section 3.2), and finally present the resulting optimization objective for our TTRV (Section 3.3)."
        },
        {
            "title": "3.1 RECAP: GROUP RELATIVE POLICY OPTIMIZATION",
            "content": "Let denote the space of natural language token sequences. decoder-based vision-language model π(x), given an input prompt (image and text) S, produces probability distribution over possible outputs S, where = (y1, y2, . . . , yT ) denotes sequence of tokens. The probability of generating sequence is π(yx) = (cid:81)T Post-training with reinforcement learning aims to maximize scalar reward function : while constraining deviation from reference policy πref. This leads to the KL-regularized optimization problem: t=1 π(yt y<t, x). max π ExD, yπ(x) (cid:104) r(x, y) (cid:105) βDKL (cid:0)π(x) πref(x)(cid:1), (1) where is the dataset of prompts, and β > 0 controls the KL regularization strength. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) provides stable approach to optimize this objective. Given sampled responses {yi}n i=1 for prompt x, the advantage of each response is defined as Ai = r(x, yi) meanj(r(x, yj)) stdj(r(x, yj)) , (2) which measures relative performance within the sampled group. The policy update is performed via clipped importance-weighted objectives to ensure stability, while KL regularization keeps the fine-tuned model close to the reference distribution. 3.2 TEST-TIME RL WITH DISTRIBUTIONAL REWARDS Our TTRV extends the vanilla GRPO framework, which is usually applied by extracting rewards by using labeled data, by introducing an inference-time RL framework. We propose to extract self-supervised reinforcement signals directly from the empirical distribution of model outputs at inference time. Unlike settings that rely on external supervision, our framework generates selfconsistent rewards that exploit the variability of rollouts to guide convergence during inference. In particular, we extract two rewards from the unlabeled data. Frequency-Based Reward. Given copy of the model at particular time step during test-time learning, our goal is to infer on the test sample multiple times and reward predictions based on their frequency. The underlying intuition is that responses produced more consistently by the model are more likely to be correct. Formally, for each test sample (consisting of an image and text prompt), we sample candidate responses {ˆy1, ˆy2, . . . , ˆyN } from the current policy πθ(x). Let = {y1, y2, . . . , yM } denote the set of unique outputs. We estimate the empirical probability of ym as p(ym) = 1 (cid:88) j= 1{ˆyj = ym}, where 1 is an indicator function. The reward for an individual sample ˆyj is then defined as r1(ˆyj) = (cid:88) m=1 p(ym) 1{ˆyj = ym}, (3) (4) which assigns higher values to responses that occur frequently, while still allocating nonzero reward to less common but potentially meaningful alternatives. This graded structure captures the implicit consensus among repeated rollouts without discarding minority reasoning paths. Importantly, this differs from the standard best-of-N sampling scheme employed by (Zuo et al., 2025), which selects only the most frequent response and discards all others. Such hard decision can be problematic when the model is uncertain or when the most frequent prediction is incorrect, since it provides misleadingly strong but potentially wrong reward signal. In contrast, our reward formulation produces soft, probabilistic supervision signal that reflects the full distribution over responses. This perspective is naturally connected to Bayesian reasoning: rather than collapsing to single point estimate, our method retains uncertainty over hypotheses and uses it to shape learning. We further validate this design choice through ablations against naïve best-of-N sampling, with results reported in Section 4.3. 5 Diversity Control Reward. Complementing the frequency-based reward r1, which provides soft, frequency-proportional credit to repeated model responses, we introduce an entropy-based regularizer to control convergence. For given test sample we compute the Shannon entropy (Shannon, 1948) of the empirical response distribution: H(P ) = (cid:88) m=1 p(ym) log p(ym), and define the auxiliary reward r2 = H(P ). (5) (6) which penalizes excessive dispersion in the output distribution. This mechanism ensures that while the model explores diverse reasoning modes initially (as encouraged by the frequency-based reward), it gradually consolidates probability mass toward stable, high-probability answers rather than spreading attention too thin across redundant responses. Combined Reward. The overall reward assigned to response ˆyj is the combination of probability and entropy terms: R(ˆyj) = r1(ˆyj) + αr2, (7) where α is tunable hyperparameter controlling the trade-off between convergence and diversity. By combining probability-based self-rewarding with entropy regularization, the model adaptively aligns its outputs during inference, striking balance between exploring diverse reasoning paths and converging to coherent predictions. 3.3 OPTIMIZATION OBJECTIVE The reinforcement learning objective is to maximize the expected reward under the policy: max θ Eyπθ(x) [R(y)] . (8) For decoder-based VLMs, optimization is performed through the standard autoregressive language modeling objective, with the reward providing soft, sample-level weighting of predicted tokens. The parameters are updated via gradient ascent: θ θ + ηθEyπθ(x) [R(y)] , (9) where η denotes the learning rate. We note that GRPO (Shao et al., 2024) modifies this process by replacing the raw reward with relative advantage term (as defined in equation 2). This shifts optimization from absolute rewards toward relative comparisons, making it more stable and better aligned with group-level objectives."
        },
        {
            "title": "4 RESULTS",
            "content": "In this section, we first list the implementation details, which include an introduction to the datasets used for evaluating our TTRV, then provide an overview of the different baselines we compare to, and finally conclude with discussion of the main results and ablations. The details about implementation and evaluation protocols are delegated to the Appendix Section A. 4.1 EVALUATION SETTINGS Image Recognition Datasets. We evaluate on eight diverse object recognition benchmarks. These include the two original ImageNet test sets: ImageNet (Deng et al., 2009) and ImageNet-V2 (Recht et al., 2019), along with three out-of-distribution variants: ImageNet-Rendition (R) (Hendrycks et al., 2021a), ImageNet-Sketch (S) (Wang et al., 2019), and ImageNet-Adversarial (A) (Hendrycks et al., 2021b). In addition, we consider two fine-grained recognition datasets: Food101 (Bossard et al., 2014) and the Describable Textures Dataset (DTD) (Cimpoi et al., 2014), as well as remote sensing dataset based on satellite imagery: Resisc45 (Cheng et al., 2017). 6 ImageNet ImageNet-V ImageNet-R ImageNet-S ImageNet-A Food101 DTD Resisc45 Mean GPT-4o CLIP MetaCLIP EVACLIP SigLIP LLaMA-3.2-11b LLaVA-1.5-7b Phi-3.5-vision InternVL3-2B w/ TTRV InternVL2.5-4B w/ TTRV InternVL3-8B w/ TTRV 98.30 68.33 70.78 74.72 76.05 72.68 97.74 97.94 56.00 98.31 +42.31 93.26 97.11 +3.85 79.47 99.31 +19. 95.10 61.86 62.64 67.03 68.97 39.94 95.85 95.66 67.43 98.25 +30.82 83.07 95.66 +12.59 62.58 97.24 +34.66 91.70 76.90 80.99 81.95 90.33 72.26 96.46 96.05 66.01 96.89 +30. 79.53 88.21 +8.68 59.32 96.88 +37.56 91.20 48.27 57.91 57.73 67.91 69.39 94.58 94.93 62.19 94.74 +32.55 65.51 92.01 +26.50 54.48 95.03 +40. 90.60 49.91 46.75 53.91 45.33 83.07 94.39 85.78 67.92 96.31 +28.39 90.67 96.00 +5.33 57.03 96.86 +39.83 95.60 87.04 85.48 87.65 89.84 93.99 95.35 96.10 67.19 95.60 +28. 80.92 94.49 +13.57 78.32 97.20 +18.88 92.30 44.73 55.80 52.71 64.79 87.54 76.68 88.26 37.24 89.73 +52.49 47.33 81.98 +34.65 59.11 89.37 +30. 92.13 58.22 66.19 60.37 64.54 82.51 92.29 85.89 72.28 90.06 +17.78 23.44 13.30 -10.14 83.62 93.82 +10.20 93.37 61.91 65.82 67.01 70.97 75.17 92.92 92.58 62.03 94.99 +32. 70.47 82.34 +11.88 66.74 95.71 +28.97 Table 1: Image Classification. Top-1 Accuracy (%) obtained by evaluating multiple different backbones. The results in gray are obtained using the specialized dual-encoder VLMs and the proprietary GPT-4o. For decoder-based VLMs we also evaluate multiple families and model sizes. Our TTRV is applied to different model sizes from the InternVL (Zhu et al., 2025) family of models. The best results obtained for dataset are highlighted in bold, while the second best are underlined. Mathverse Mathvista SEED MME RealWorldQA Capture CRPE AI2D Mean GPT-4o LLaMA-3.2-11b LLaVA-1.5-7b Phi-3.5-vision InternVL3-2B w/ TTRV InternVL2.5-4B w/ TTRV InternVL3-8B w/ TTRV 54.40 19.36 26.02 36.02 44.10 48.51 +4. 51.69 53.02 +1.33 34.56 42.15 +7.59 63.80 35.40 34.29 51.22 58.26 66.11 +7.85 65.49 66.94 +1.45 38.84 50.41 +11. 69.80 62.56 61.50 69.54 24.99 48.85 +23.86 57.37 61.14 +3.77 32.12 59.16 +27.04 89.75 51.59 49.05 77.22 17.04 11.06 -5. 85.27 85.79 +0.52 49.02 78.77 +29.75 75.40 41.18 59.87 57.25 63.47 64.29 +0.82 65.25 66.00 +0.75 19.01 26.57 +7. 85.25 61.48 71.75 72.99 60.27 78.64 +18.37 80.03 85.99 +5.96 59.50 80.68 +21.18 76.60 44.43 64.84 68.33 71.92 72.00 +0. 74.33 75.22 +0.89 55.81 68.26 +12.45 84.60 59.54 47.49 75.55 39.68 67.75 +28.07 51.55 61.09 +9.54 30.95 53.92 +22. 71.97 53.34 46.02 48.58 47.47 57.15 +9.69 66.37 69.40 +3.03 38.05 55.56 +17.50 Table 2: Visual Question Answering. Results obtained by evaluating multiple different backbones. For decoder-based VLMs, we evaluate multiple families and model sizes. Our TTRV is applied to different model sizes from the InternVL (Zhu et al., 2025) family of models. VQA Datasets. We further evaluate our TTRV on eight visual question answering (VQA) datasets covering broad range of reasoning skills. These include two math reasoning benchmarks: MathVerse (Zhang et al., 2024a) and MathVista (Lu et al., 2024); three datasets focusing on everyday scenarios and objects: SEED (Li et al., 2023a), MME (Yin et al., 2023), and RealWorldQA (AI, 2024); two compositional reasoning datasets: Capture (Pothiraj et al., 2025) and Circular-based Relation Probing Evaluation (CRPE) (Wang et al., 2024); and one dataset targeting chart-based questions: AI2D (Kembhavi et al., 2016). These 16 datasets were deliberately selected to span broad spectrum of domains and tasks, including natural images, fine-grained categories, remote sensing, mathematical reasoning, everyday commonsense, compositionality, and chart understanding. This diversity ensures that our findings are not confined to single domain but instead provide representative view of model capabilities across varied and challenging settings. We further expect that the insights derived here will generalize to other benchmarks, which can be incorporated in future evaluations. Baselines: For comparison, we evaluate the following dual-encoder VLMs: CLIP (Radford et al., 2021), MetaCLIP (Xu et al., 2023), EVACLIP (Sun et al., 2023), and SigLIP (Zhai et al., 2023). As representative methods for decoder-based VLMs, we choose: LLaMA (Touvron et al., 2023), LLaVA (Liu et al., 2023b), Phi-3.5-vision (Abdin et al., 2024), and also provide results for the proprietary GPT-4o (OpenAI, 2023). For the main experiments, we apply our TTRV to different model sizes of the InternVL (Chen et al., 2024b) family. However, we want to point out that TTRV 7 can be applied to any open-source VLM. We provide results with QwenVL (Bai et al., 2025) in the ablations Section 4.3."
        },
        {
            "title": "4.2 RESULTS",
            "content": "Image Classification. In Table 1 we present the top-1 accuracy across eight diverse image recognition benchmarks. We observe that TTRV consistently enhances performance for all evaluated InternVL backbones, with particularly strong gains on challenging distribution shifts such as ImageNet-R and ImageNet-S. For example, when applied to InternVL3-2B, TTRV improves accuracy by up to 89.7% on DTD and 90.0% on Resisc45, while yielding an average gain of around 32.9% across datasets. Similar trends appear for larger models: InternVL2.5-4B and InternVL38B see systematic boosts on both fine-grained recognition (Food101) and large-scale benchmarks (ImageNet and variants). Notably, TTRV lifts InternVL3-8B to over 99% accuracy on ImageNet, even outperforming proprietary systems (e.g., GPT-4o) and establishing new state-of-the-art results for open-source VLMs. We emphasize that these improvements are achieved by randomly sampling only 20 test instances per dataset, suggesting that TTRV may not be adapting strongly to the test data distribution itself, but rather recovering and amplifying visual recognition capabilities that were present in pre-training and which might be attenuated during instruction tuning. We also observe some particular cases when the model performance decreases (e.g., on the Resics45 dataset with InternVL-2.5-4B model). This can be attributed to the low performance of the base model, which might result in extremely low-quality rollouts from the model or general instability of GRPO (Wang et al., 2025). Overall, these findings demonstrate that frequencyand entropy-based test-time rewards allow models to consolidate predictions more effectively, leading to robust improvements in visual recognition. Visual Question Answering. In Table 2 we report results across eight multimodal reasoning tasks, including mathematical problem solving (MathVerse, MathVista), scientific diagram understanding (AI2D), and general-domain evaluation (RealWorldQA). We find that TTRV provides consistent gains across all datasets and model scales. For instance, on InternVL2.5-4B, accuracy improves by 4.4% on MathVista and 9.5% on AI2D, while the larger InternVL3-8B benefits from 12.4% and 7.5% improvements on CRPE and RealWorldQA, respectively. We also observe that TTRV outperforms other open-source VLMs while also remaining highly competitive with GPT-4o (only lagging behind by 2% on average), and outperforming the strong proprietary model on some benchmarks, like the challenging Mathvista. Furthermore, these gains are also obtained using only 20 sampled instances per dataset, suggesting that the improvements may not stem from distribution-level adaptation but rather from leveraging and re-aligning latent reasoning skills that were learned during pre-training but weakened after instruction tuning. This highlights that TTRV is especially effective at recovering such capabilities under test-time optimization, enabling more robust reasoning across diverse VQA tasks. 4.3 ABLATIONS In this section, we present extensive ablations of our method and our design choices. We begin by comparing our reward formulation against the naïve best-of-N (majority voting) sampling strategy for RL. Next, we evaluate the robustness of TTRV by training on one dataset and testing on completely different distribution. We then investigate alternative sampling techniques and reward designs, followed by experiments in extremely data-scarce settings where TTRV is applied to single randomly chosen test sample. Finally, we report results obtained by applying TTRV to the Qwen-VL model, highlighting generalization of TTRV beyond the Intern-VL models. Due to space constraints, we delegate additional ablations (e.g., on latency) to the Appendix. Reward Designs. In Table 3 we ablate different reward designs. Specifically, we compare the reward design proposed in our work with the majority voting reward used by (Zuo et al., 2025), and also ablate the effect of the two different rewards used in our work (c.f., Section 3). We find that the combination of the two rewards: frequency-based and diversity-control, outperforms all other design choices. Cross-Data Generalization. In the main results (c.f., Tables 1 & 2), we evaluate TTRV on test samples drawn from the same dataset (as used for test-time RL). In contrast, Figure 3 reports results 8 Mathvista SEED AI2D InternVL2.5-4B (base) w/ Maj Voting vs. base TTRV (w/o Freq. reward) vs. base TTRV (w/o Diversity reward) vs. base TTRV (Freq. + Diversity) vs. base 65.49 65.08 -0.41 66.81 +1.32 65.08 -0. 66.94 +1.45 57.37 58.37 +1.00 58.87 +1.50 59.27 +1.90 61.14 +3. 51.55 47.52 -4.03 52.66 +1.11 53.06 +1.51 61.09 +9.54 Table 3: Ablating Reward Designs. We compare the design choices of our TTRV with the reward design proposed by Zuo et al. (2025), based on the pseudo-labels obtained from majority voting scheme. Further, we also ablate the individual effect of our frequencyand diversity-based rewards. Figure 3: Cross-dataset Generalization. Top1 accuracy (%) achieved by employing TTRV on base dataset using InternVL3-2B and evaluating on target dataset from completely different domain. The results highlight that TTRV enhances core abilities of the model. Imagenet-A Imagenet-R Seed Imagenet-R InternVL3-4B (base) w/ biased sampling vs. base w/ random sampling vs. base 90.67 95.09 +4.42 96.00 +5.33 79.53 88.51 +8. 88.21 +8.68 InternVL2.5-4B (base) w/ Random Rewards vs. base TTRV (Freq. + Diversity) vs. base 57.37 52.41 -4. 61.14 +3.77 79.53 78.00 -1.53 88.21 +8.68 Table 4: Biased vs. Random Sampling. Top-1 accuracy (%) obtained by sampling the test data differently. For biased sampling, we choose fraction of the data from only subset of classes (e.g., 4 out of 200 for ImageNet-R). Random sampling results are obtained by sampling the data randomly from all classes. Table 5: Random Reward vs. TTRV. We compare the results obtained by using random rewards (following Shao et al. (2025)) and our sophisticated reward design. The results highlight that the gains obtained with spurious rewards do not transfer to Intern-VL family of models, which were shown for Qwen-based models. when TTRV is applied using one dataset and evaluated on completely different distribution (e.g., Food101 for TTRV and DTD for testing). We observe that TTRV exhibits strong cross-data generalization, indicating that its performance gains stem not from distribution-specific adaptation but from enhancing the models underlying task ability (e.g., image classification). Effect of Data Sampling. We find that TTRV does not require sampling data from all classes in the downstream dataset to achieve strong performance gains. In Table 4, we compare biased sampling, where data is drawn from only small subset of classes (e.g., 4 out of 200 in ImageNet-R), against random sampling, where data is sampled uniformly across classes. Remarkably, even under biased sampling, TTRV yields substantial improvements over the base model. Random Rewards. Shao et al. (2025) recently showed that some models trained with GRPO can exhibit performance gains even when optimized with spurious rewards. As sanity check, we compare TTRV against such random rewards and report results in Table 5. We find that InternVL (Chen et al., 2024b) models do not appear to benefit from random rewards, suggesting that their improvements under TTRV stem from meaningful reward signals rather than spurious correlations. Single Sample TTRV. To further test whether the gains from TTRV arise from true task enhancement rather than adaptation to the data distribution, we consider an extreme data-scarce setting where adaptation is performed on only single randomly chosen test example. Results in Table 6 show that even in this case, TTRV yields measurable improvements, lending additional support to our hypothesis. Generalization to Model Families. While our main results in Tables 1 & 2 focus on post-training InternVL (Chen et al., 2024b) models, we also examine whether TTRV extends to other architectures. In Table 7, we present results with Qwen2.5-VL (Bai et al., 2025) and observe consistent Mathvista SEED Imagenet-A Imagent-R Mathverse Mathvista Capture Resisc45 InternVL2.5-4B w/ TTRV 65.49 66.11 +0.62 57.37 58.87 +1. 90.67 95.28 +4.61 79.53 85.00 +5.47 Qwen2.5-VL-3B w/ TTRV 45.33 48.71 +3.38 67.35 71.48 +4.13 71.33 75.25 +3. 90.08 92.71 +2.63 Table 6: Single Example TTRV. We report results for VQA and image classification after applying TTRV on single randomly sampled test example. The evaluation is performed on the entire test sets. Table 7: Generalization to Model Families. We provide results for the two tasks (Image Classification and VQA) by using the Qwen2.5VL-3B (Bai et al., 2025). gains. These findings suggest that TTRV is not restricted to single model family and can generalize across diverse VLM architectures."
        },
        {
            "title": "5 LIMITATIONS AND CONCLUSION",
            "content": "Limitations. While we empirically show that TTRV enhances task-specific abilities of the base model rather than adapting to the data distribution, we do not yet provide theoretical explanation for this behavior. Establishing such foundation remains an important direction for future work. Conclusion. We introduced TTRV, the first test-time reinforcement learning framework for VLMs, where rewards are extracted on-the-fly from unlabeled test data. Specifically, we proposed two complementary rewards: one based on the frequency of model predictions and another that regulates diversity in rollouts. Extensive evaluation on 16 benchmarks spanning object recognition and visual question answering shows consistent improvements over strong base models, even outperforming GPT-4. Beyond empirical gains, our ablations highlight data-efficient properties of TTRV and how it enhances task-specific abilities without explicit supervision, pointing to test-time optimization through RL as powerful paradigm for bridging pre-training and downstream deployment."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 Technical Report. arXiv preprint arXiv:2412.08905, 2024. AI. Grok-1.5 Vision Preview, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923, 2025. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 Mining Discriminative Components with Random Forests. In Proc. ECCV, 2014. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. MiniGPT-v2: Large Language Model as Unified Interface for Vision-Language Multi-task Learning. In Proc. ICLR, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. In Proc. CVPR, 2024b. Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sensing Image Scene Classification: Benchmark and State of the Art. In Proc. IEEE, 2017. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing Textures in the Wild. In Proc. CVPR, 2014. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In NeurIPS, 2023. 10 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In Proc. CVPR, 2009. Sivan Doveh, Assaf Arbelle, Sivan Harary, Amit Alfassy, Roei Herzig, Donghyun Kim, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, et al. Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models. In NeurIPS, 2023a. Sivan Doveh, Assaf Arbelle, Sivan Harary, Rameswar Panda, Roei Herzig, Eli Schwartz, Donghyun Kim, Raja Giryes, Rogerio Feris, Shimon Ullman, and Leonid Karlinsky. Teaching Structured Vision&Language Concepts to Vision&Language Models. In Proc. CVPR, 2023b. Sivan Doveh, Shaked Perek, Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, and Leonid Karlinsky. Towards Multimodal In-Context Learning for Vision & Language Models. arXiv preprint arXiv:2403.12736, 2024. Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. SEED: Self-supervised Distillation for Visual Representation. In Proc. ICLR, 2021. Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning. In Proc. CVPR, 2023. Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-Time Training with Masked Autoencoders. In NeurIPS, 2022. Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Bianca Lamm, Muhammad Jehanzeb Mirza, Margret Keuper, and Janis Keuper. Are Vision Language Models Texture or Shape Biased and Can We Steer Them? arXiv preprint arXiv:2403.09193, 2024. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: Critical Analysis of Out-of-Distribution Generalization. In Proc. ICCV, 2021a. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural Adversarial Examples. In Proc. CVPR, 2021b. Irene Huang, Wei Lin, Jehanzeb Mirza, Jacob Hansen, Sivan Doveh, Victor Ion Butoi, Roei Herzig, Assaf Arbelle, Hilde Kuhene, Trevor Darrel, et al. ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs. arXiv preprint arXiv:2406.08164, 2024. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In Proc. ICML, 2021. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. Diagram Is Worth Dozen Images. In Proc. ECCV, 2016. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy Visual Task Transfer. arXiv preprint arXiv:2408.03326, 2024. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. arXiv preprint arXiv:2307.16125, 2023a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proc. ICML, 2023b. Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang Bill Zhu, and Kaipeng Zhang. Think or Not Think: Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning. arXiv preprint arXiv:2503.16188, 2025. Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski, Rameswar Panda, Rogerio Feris, Hilde Kuehne, and Horst Bischof. MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge. In Proc. ICCV, 2023. 11 Wei Lin, Muhammad Jehanzeb Mirza, Sivan Doveh, Rogerio Feris, Raja Giryes, Sepp Hochreiter, and Leonid Karlinsky. Comparison Visual Instruction Tuning. arXiv preprint arXiv:2406.09240, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge, 2023a. URL https://llava-vl.github.io/blog/ 2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In NeurIPS, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning. In Proc. CVPR, 2024. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. In Proc. ICLR, 2024. Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The Norm Must Go On: Dynamic Unsupervised Domain Adaptation by Normalization. In Proc. CVPR, 2022. M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Horst Possegger, Rogerio Feris, and Horst Bischof. TAP: Targeted Prompting for Task Adaptive Generation of Textual Training Instances for Visual Classification. arXiv preprint arXiv:2309.06809, 2023a. M. Jehanzeb Mirza, Pol Jane Soneira, Wei Lin, Mateusz Kozinski, Horst Possegger, and Horst Bischof. ActMAD: Activation Matching to Align Distributions for Test-Time Training. In Proc. CVPR, 2023b. M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, , Jakub Micorek, Mateusz Kozinski, Hilde Kuhene, and Horst Possegger. Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs. In Proc. ECCV, 2024. Jehanzeb Mirza, Mengjie Zhao, Zhuoyuan Mao, Sivan Doveh, Wei Lin, Paul Gavrikov, Michael Dorkenwald, Shiqi Yang, Saurav Jha, Hiromi Wakaki, et al. GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models. arXiv preprint arXiv:2410.06154, 2025. Muhammad Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Horst Possegger, Mateusz Kozinski, Rogerio Feris, and Horst Bischof. LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections. In NeurIPS, 2023c. Maria-Elena Nilsback and Andrew Zisserman. Automated Flower Classification Over Large Number of Classes. In Proc. ICVGIP, 2008. OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. Priyank Pathak, Shyam Marjit, Shruti Vyas, and Yogesh Rawat. LR0.FM: Low-Res Benchmark and Improving Robustness for Zero-Shot Classification in Foundation Models. In Proc. ICLR, 2025. URL https://openreview.net/forum?id=AsFxRSLtqR. Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal. CAPTURe: Evaluating SpaarXiv preprint tial Reasoning in Vision Language Models via Occluded Object Counting. arXiv:2504.15485, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models from Natural Language Supervision. In Proc. ICML, 2021. 12 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In NeurIPS, 2023. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classifiers Generalize to ImageNet? In Proc. ICML, 2019. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. Claude Shannon. Mathematical Theory of Communication. The Bell system technical journal, 27(3):379423, 1948. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious Rewards: Rethinking Training Signals in RLVR. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. VLM-R1: Stable and Generalizable R1-style Large Vision-Language Model. arXiv preprint arXiv:2504.07615, 2025. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models. In NeurIPS, 2022. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv preprint arXiv:2408.03314, 2024. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: Improved Training Techniques for CLIP at Scale. arXiv preprint arXiv:2303.15389, 2023. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-Time Training with Self-Supervision for Generalization under Distribution Shifts. In Proc. ICML, 2020. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (Learn at Test Time): RNNs with Expressive Hidden States. arXiv preprint arXiv:2407.04620, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971, 2023. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning Robust Global Representations by Penalizing Local Predictive Power. In NeurIPS, 2019. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning. arXiv preprint arXiv:2504.08837, 2025. Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The All-Seeing Project V2: Towards General Relation Comprehension of the Open World. In Proc. ECCV, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In NeurIPS, 2022. 13 Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying CLIP Data. In Proc. ICLR, 2023. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. Survey on Multimodal Large Language Models. arXiv preprint arXiv:2306.13549, 2023. Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, and Chang Yoo. C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion. arXiv preprint arXiv:2403.14119, 2024. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, and Wenbing Tao. Perception-R1: Pioneering Perception Policy with Reinforcement Learning. arXiv preprint arXiv:2504.07954, 2025. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre-training. In Proc. ICCV, 2023. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization. arXiv preprint arXiv:2503.12937, 2025. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems? In Proc. ECCV, 2024a. Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, and Serena Yeung-Levy. Why are Visually-Grounded Language Models Bad at Image Classification? arXiv preprint arXiv:2405.18415, 2024b. Shuai Zhao, Xiaohan Wang, Linchao Zhu, and Yi Yang. Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models. arXiv preprint arXiv:2305.18010, 2023. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. arXiv preprint R1-Zeros \"Aha Moment\" in Visual Reasoning on 2B Non-SFT Model. arXiv:2503.05132, 2025. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. In Proc. ICLR, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models. arXiv preprint arXiv:2504.10479, 2025. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. TTRL: Test-Time Reinforcement Learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "APPENDIX",
            "content": "In this appendix, we present additional experiments and explanations that provide further insight and clarity beyond the main manuscript. Section lists additional implementation details and evaluation protocols. Section presents detailed overview of the datasets used in our study. In Section C, we describe the prompts utilized in our experiments. Then in Section D, we present additional ablation studies that highlight further aspects of our method and offer deeper insights into its effectiveness. Finally, Section includes comprehensive pseudocode to facilitate reproducibility and to help readers gain clearer understanding of the implementation details. All experiments were conducted on machine equipped with 4 NVIDIA A100 and 4 NVIDIA A6000 GPUs. For the review process, we also provide our full codebase (code.zip) along with detailed execution instructions in Readme.md. The codebase will be released publicly upon acceptance."
        },
        {
            "title": "A ADDITIONAL EXPERIMENTAL SETTINGS",
            "content": "Implementation Details. We apply TTRV independently on each benchmark and report the results in Tables 1 & 2. For optimization, we adopt the AdamW optimizer with cosine learning rate schedule, setting the peak learning rate to 5 107. During rollout, we generate 32 candidate responses with temperature of 1.0 for all experiments. The reward hyperparameter α is fixed at 0.75 for all datasets. We cap the maximum prompt length at 7524 tokens and the maximum response length at 1024 tokens. We generally report results using 20 samples in the main table. These samples are randomly sampled from the test data. In the appendix, we also provide comparison between 20and 500-sample adaptation, and in the ablation study, we further evaluate the extreme case 1-sample adaptation, where the model adapts to single example before being evaluated on the full dataset. Evaluation Protocol: For evaluation, we use greedy decoding (temperature = 0) across all datasets, covering both recognition and VQA tasks. We convert the object recognition task to four-way multiple-choice questioning task, following Gavrikov et al. (2024). For the VQA tasks, we employ the official dataset prompts and append the same multiple-choice instruction to standardize responses. Two exceptions are made: for Capture, we use free-form answers as recommended by Pothiraj et al. (2025), and for MME, we convert yes/no questions into multiple-choice format. Performance is measured by accuracy against the ground truth for recognition and VQA tasks, while for Capture we report 1 symmetric mean percentage error (Pothiraj et al., 2025). For all the zero-shot results we do not employ any chain-of-thought prompting (Wei et al., 2022), because that evaluation setting is more fair with the setting employed in our work."
        },
        {
            "title": "B DATASET DESCRIPTION",
            "content": "To comprehensively evaluate our method, we curated diverse set of recognition and VQA benchmarks that span multiple task-specific challenges. Table 8 provides detailed statistics of the datasets used in our experiments, including both the original test sizes and the number of images retained after preprocessing. We employed several widely used recognition datasets that test the robustness and generalization capability of models across distribution shifts. Specifically, we included ImageNet (Deng et al., 2009), ImageNet-V2 (Recht et al., 2019), and ImageNet-A (Hendrycks et al., 2021b) to capture generic object recognition in both standard and adversarial settings. In addition, ImageNet-Sketch (Wang et al., 2019) and ImageNet-R (Hendrycks et al., 2021a) were incorporated to examine robustness under edge-based and texture-based distortions, respectively. To further assess fine-grained and material recognition, we used Food101 (Bossard et al., 2014) and DTD (Cimpoi et al., 2014), which emphasize category-level detail and texture variation. To test higher-level reasoning, we included wide range of VQA datasets spanning mathematical ability, general understanding, and compositional reasoning. Mathematical reasoning was evaluated using Mathverse (Zhang et al., 2024a) and MathVista (Lu et al., 2024), while Seed (Li et al., 2023a) and MME (Yin et al., 2023) were selected for general multimodal understanding. RealWorldQA (AI, 15 Dataset Used Test Size Original Test Size Focus ImageNet-A (Hendrycks et al., 2021b) ImageNet-V2 (Recht et al., 2019) ImageNet (Deng et al., 2009) ImageNet-Sketch (Wang et al., 2019) ImageNet-R (Hendrycks et al., 2021a) DTD (Cimpoi et al., 2014) Food101 (Bossard et al., 2014) Resisc45 (Cheng et al., 2017) Mathverse (mcq) (Zhang et al., 2024a) Mathvista (Lu et al., 2024) Seed (Li et al., 2023a) MME (Yin et al., 2023) RealworldQA (AI, 2024) Capture (Pothiraj et al., 2025) CRPE (Wang et al., 2024) AI2D (Kembhavi et al., 2016) 7,467 9,772 49,032 35,350 28,506 5640 25,250 4,500 1631 490 3,881 1576 765 817 7575 2704 7,500 Generic 10,000 Generic 50,000 Generic 50,000 Edges 30,000 Texture 5,640 Edges, Texture 25,250 4,500 Fine-grained Satellite Imagery 2180 Mathematical Ability 1000 Mathematical Ability 13,991 General Understanding 2,370 General Understanding 765 Realworld Understanding 962 Counterfactual Understanding 7575 Compositionality and Halluncination 3090 Grpah and Chart Understanding Table 8: Statistics of Recognition and VQA datasets used in TTRV. We drop images with resolution higher than 1000 1000. Hence, we report both i) the original number of test images and ii) the used number of test images (i.e., those below the 1000 1000 threshold). 2024) was used to benchmark models against real-world scenarios, where all images were first standardized to maximum resolution of 1000 1000 for consistency across experiments. We also included Capture (Pothiraj et al., 2025) to probe counterfactual reasoning, CRPE (Wang et al., 2024) to evaluate compositionality and hallucination resistance, and AI2D (Kembhavi et al., 2016) to study performance on diagram, graph, and chart-based understanding tasks. For computational resons, we filtered out images exceeding resolution of 10001000 pixels across all datasets, retaining only those within this threshold. The reported used test size in Table 8 reflects this preprocessing step. In particular, for the RealWorldQA dataset, where image dimensions were highly inconsistent, we explicitly resized all images to 1000 1000 resolution to ensure compatibility with our evaluation pipeline. Overall, the curated dataset collection provides broad coverage of recognition, reasoning, and realworld understanding challenges, allowing us to rigorously evaluate the generalization capability of our proposed approach."
        },
        {
            "title": "C TTRV PROMPT DETAILS",
            "content": "In this section, we provide the prompts used in our experiments. For each dataset, we present representative example of the prompt employed in our study. While the specific prompts may vary depending on the nature of the question, particularly in VQA tasks, we provide general outline illustrating the structure and format of the prompts used across different datasets. ImageNet: <image> Look at the given image and identify what it shows. Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. neck bracenB. shopping cartnC. guillotine nD. garbage truck. ImageNet-V2: <image> Look at the given image and identify what it shows. Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. cuirass nB. dial telephone, dial phonenC. beavernD. desk. ImageNet-R: <image> Look at the given image and identify what it shows. 16 Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. skunk nB. pandanC. german_shepherd_dognD. orangutan. ImageNet-S: <image> Look at the given image and identify what it shows. Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. lab coat nB. cheetahnC. ptarmigannD. canoe. ImageNet-A: <image> Look at the given image and identify what it shows. Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. feather boa nB. garter snakenC. soap dispensernD. tank. Food101: <image> Look at the given image and identify what it shows. Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. Greek saladnB. Red velvet cakenC. BibimbapnD. Pork chop. DTD: <image> Look at the given image and identify what texture it shows. Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. bandednB. crosshatchednC. frecklednD. marbled. Resisc45: <image> Look at the given image and identify what it shows. Choose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text. Options:nA. industrial areanB. sea icenC. circular farmlandnD. golf course. Mathverse: <image> nPlease directly answer the question and provide the correct option letter, e.g., A, B, C, D.nDo not include any explanation or extra text.nQuestion: Emile is observing wind turbine. The vertical distance between the ground and the tip of one of the turbine's blades, in meters, is modeled by $H(t)$ where $t$ is the time in seconds. What is the meaning of the highlighted segment?nChoices:nA:The turbine's center is 35 meters above the ground.nB:The turbine completes single cycle in 35 seconds.nC:The length of the blade is 35 meters. nD:The turbine has 35 blades. Mathvista: <image> nHint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.nDo not include any explanation or extra text.nQuestion: In the figure above, triangle ABC is inscribed in the circle with center and diameter AC. If AB = AO, what is the degree measure of angle ABO nChoices: n(A) 15*degreen(B) 30*degreen(C) 45*degreen (D) 60*degreen(E) 90*degree. Food101 DTD Resisc45 ImageNet ImageNetv2 ImageNetR ImageNetS ImageNetA InternVL3-2B TTRV 20 samples TTRV 500 samples InternVL3-8B TTRV 20 samples TTRV 500 samples 67.19 95.60 96.20 78.32 87.13 97.20 37.24 89.73 89. 59.11 77.92 89.37 72.28 90.06 93.67 83.62 89.19 93.82 56.00 98.31 98.89 79.47 91.43 99.31 67.43 98.25 98. 62.58 85.27 97.24 66.01 96.89 97.85 59.32 63.51 96.88 62.19 94.74 95.54 54.48 53.79 95.03 67.92 96.31 96. 57.03 81.43 96.86 Table 9: Number of Samples for Adaptation. Top-1 Accuracy (%) obtained by sampling varying data points from the test data. SEED: <image> nWhat is the main color of the dress worn by the woman with the density value of [0.6536, 0.5600, 0.7431, 0.7743]? nChoose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text.nOptions:nA. RednB. None of the abovenC. BrownnD. Tan MME: <image> Is this photo taken in place of home office? Please answer with yes or no. Please respond with only the corresponding option letter (A or B). Do not include any explanation or extra text. Options:nA. No nB. Yes. RealWorldQA: <image> nHow many lanes are there on the left?nnOptions are: nA. 4nB. 3nC. 2nD. 5nnPlease answer directly with only the letter of the correct option and nothing else.\", Capture: <image>nCount the exact number of sunglasses in the image. Assume the pattern of sunglasses continues behind any black box. Provide the total number of sunglasses as if the black box were not there.nPlease reason step by step, and put your final answer within boxed{}. CRPE: <image> nWhat is the person in front of?nA. The person is in front of the person.nB. The person is in front of the tree.nC. The person is in front of the mirror.nD. The person is in front of the shelf.nAnswer with the option's letter from the given choices directly and do not include any explanation or extra text. AI2D: <image> nWhat is the line that divides the two different plates? nChoose the correct answer from the options below and respond with only the corresponding option letter (A, B, C, or D). Do not include any explanation or extra text.nnOptions:nA. groundnB. fault linenC. dirtnD. earthquake."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTS",
            "content": "D.1 LATENCY VERSUS NUMBER OF SAMPLES To further substantiate our claims, we conducted experiments aimed at analyzing the adaptability of the model under varying conditions. The first experiment investigates how the models performance 18 Latency (avg std) Overhead vs. Normal Inference"
        },
        {
            "title": "Normal Inference",
            "content": "25.5 4.5 Adaptation: 1 sample 20 samples 500 samples 2.75 0.43 3.77 0.63 1 hr 38 16 +2.7 +3.8 +1 hr 38 Table 10: Computation Overhead. Inference and adaptation latency through TTRV. Seconds: s, Minutes: m, Hours: h. DTD Imagenet-A Imagenet-V2 AI2D Mathverse InternVL2.5-4B 46.78 0.03 81.87 0.80 w/ TTRV 90.58 0.05 96.09 0.01 83.01 0.03 96.77 0.06 51.73 0.01 64.75 2.34 52.67 0.55 53.59 0.45 Table 11: Variance of Results. Results obtained by employing TTRV across 5 independent runs. Mathvista Mathverse Food Mathvista DTD Seed IN-V2 IN-R IN-V2 IN-A IN-A IN-V2 InternVL2.5-4B w/ TTRV 51.69 52.00 +0. 65.49 67.14 +2.52 56.25 59.07 +2.02 79.53 95.42 +15. 90.67 96.30 +5.63 83.07 96.14 +13.07 Table 12: Cross-dataset generalization. Performance on different dataset combinations, where denotes training on dataset and testing on dataset Y. \"IN\" in the table refers to ImageNet. changes when it is allowed to adapt using different numbers of training samples. Specifically, we compare the outcomes when the model is adapted with only 20 samples versus when it is adapted with 500 samples. As shown in Table 9, the results demonstrate consistent improvement in performance as the number of adaptation samples increases. This suggests that providing the model with richer set of examples enables it to better align with the target task, thereby yielding higher accuracy and robustness. However, this improvement does not come without trade-offs. Increasing the number of samples also leads to higher computational burden, both in terms of memory consumption and processing time. This is particularly important in real-world applications where inference latency is critical factor. To quantify this trade-off, we conducted an additional experiment measuring the latency associated with adaptation on different sample sizes. The results in Table 10, reveal that while larger adaptation sets enhance task performance, they simultaneously increase the time required for inference, thereby highlighting an inherent balance between accuracy and efficiency. All experiments were conducted using the vLLM inference engine, one of the fastest and most recent frameworks for large language model inference. Despite its efficiency, optimized inference remains an active research area, and ongoing improvements in frameworks such as vLLM are expected to further reduce latency. Moreover, the reported times are dependent on the underlying hardware. Access to more powerful GPUs would likely accelerate both inference and adaptation, thereby reducing the overall time required for these tasks. D.2 ROBUSTNESS To evaluate the robustness of our method, we conducted experiments to measure the variance in its performance. The results are summarized in Table 11. As shown, our method, when evaluated using greedy decoding, demonstrates strong robustness and is only subject to minor variations attributable to hardware and software factors. D.3 FURTHER CROSS-DATA GENERALIZATION EXAMPLES In addition to the results shown in Figure 3, we present challenging cross-dataset evaluation results in Table 12. Our method consistently improves performance across all transfer settings, demonstrating its effectiveness not only in within-domain accuracy but also in transferring knowledge across diverse domains. For instance, training on ImageNet-V2 leads to significant gains when tested on ImageNet-R (+15.89%) and ImageNet-A (+5.63%). Similarly, training on ImageNet-A improves performance on ImageNet-V2 by +13.07%. We also observe positive transfer in mathematical reasoning tasks, with gain of +0.31% when training on MathVista and evaluating on MathVerse. Notably, even when models are trained on visual recognition datasets and evaluated on VQA benchmarks, such as training on Food and testing on MathVista, we achieve performance improvement of +2.52%. These results indicate that our approach enhances visual understanding in way that generalizes well across heterogeneous tasks and domains."
        },
        {
            "title": "E PSEUDOCODE",
            "content": "The following pseudocode illustrates the main steps of our method, including rollout generation, reward computation, advantage estimation, and policy update. Pseudocode: Test-Time Reinforcement Learning for Vision Language Models def inference_time_grpo(model, test_sample, N=32, alpha=0.75, lr =0.001): \"\" Args: model: decoder-based VLM with parameters theta test_sample: (x) consisting of image + text prompt N: number of rollouts per test sample alpha: weight for entropy regularization lr: learning rate for policy update Returns: updated_model: model with adapted parameters \"\" # 1. Generate rollouts responses = [model.sample(test_sample) for _ in range(N)] unique_responses = set(responses) # 2. Empirical probabilities freq = {y: responses.count(y) for in unique_responses} probs = {y: freq[y]/N for in unique_responses} # 3. Compute rewards r1 = {y: probs[y] for in unique_responses} # Frequency-based reward = -sum(p * log(p) for in probs.values()) r2 = -H # diversity control reward = {y: r1[y] + alpha * r2 for in unique_responses} # total reward # 4. Convert rewards -> relative advantages mean_R = sum(R[y] for in responses) / len(responses) std_R = (sum((R[y] - mean_R)**2 for in responses) / len(responses ))**0.5 if std_R < 1e-8: # avoid divide by zero = {y: 0.0 for in responses} else: = {y: (R[y] - mean_R) / std_R for in responses} # 5. Policy update (GRPO) grad_estimate = 0 for in responses: logprob = model.log_prob(test_sample, y) grad_estimate += A[y] * grad(logprob, model.params) grad_estimate /= for param in model.params: param += lr * grad_estimate[param] return model"
        }
    ],
    "affiliations": [
        "IISc Bangalore",
        "Independent Researcher",
        "JKU Linz",
        "MIT CSAIL",
        "MIT-IBM Watson AI Lab",
        "Stanford",
        "Tübingen AI Center"
    ]
}