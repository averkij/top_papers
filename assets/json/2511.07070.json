{
    "paper_title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services",
    "authors": [
        "Fei Zhao",
        "Chonggang Lu",
        "Haofu Qian",
        "Fangcheng Shi",
        "Zijie Meng",
        "Jianzhao Huang",
        "Xu Tang",
        "Zheyong Xie",
        "Zheyu Ye",
        "Zhe Xu",
        "Yao Hu",
        "Shaosheng Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 0 7 0 7 0 . 1 1 5 2 : r RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services Fei Zhao, Chonggang Lu, Haofu Qian, Fangcheng Shi, Zijie Meng, Jianzhao Huang, Xu Tang, Zheyong Xie, Zheyu Ye, Zhe Xu, Yao Hu, Shaosheng Cao NLP Team, Xiaohongshu Inc. Huangpu District, Shanghai, China caoshaosheng@xiaohongshu.com Abstract As key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers seesaw between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness. CCS Concepts Human-centered computing Social networks; Computing methodologies Learning paradigms; Multi-task learning. Keywords Large language model, Post training, Social networking services Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Fei Zhao, Chonggang Lu, Haofu Qian, Fangcheng Shi, Zijie Meng, Jianzhao Huang,, Xu Tang, Zheyong Xie, Zheyu Ye, Zhe Xu, Yao Hu, Shaosheng Cao. 2018. RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nPowered by the rapid advance of electronic devices and network\ninfrastructure, social networking services (SNS) have evolved into\ncore infrastructure for human daily interaction and the spread of\ninformation [71]. In parallel, large language models (LLMs) have\ndelivered striking progress in natural language processing (NLP),\nenabling promising performance across a wide range of down-\nstream tasks [32, 73]. However, deploying a general-purpose LLM\ninto an SNS scenario is not a simple lift-and-shift. The workloads of\nSNS platform are highly heterogeneous, such as real-time modera-\ntion and abuse response, recommendation-shaped dialogue, creator\nassistance, and community operations, each with distinct latency,\nsafety, and tone requirements. And the environment changes in a\nvery fast speed, where trends, slang, and community norms rise\nand fade within days. Additionally, because SNS connects vast, di-\nverse audiences, models must handle large, complex corpora across\nlanguages and expression styles to bridge users from different cul-\ntural backgrounds. Together these factors amplify distribution shift\nand increase the risk of brittle generalization, where models, op-\ntimized on the standard benchmarks, may misread community-\nspecific rules, over- or under-enforce policies, or drift as conven-\ntions change [79, 83]. Therefore, seamlessly integrating LLMs for\nSNS scenario requires that adapt quickly, remain stable, and main-\ntain competence across languages and communities, especially\nwithout compromising safety or user trust.",
            "content": "As pioneer in adapting LLM in SNS, RedOne [83] established an early domain-specific post-training route applying classical SFTbased pipeline and showed that SNS performance can be increased without sacrificed general ability. However, this SFT-dominated pipelines exhibit pronounced seesaw effect, which improves on in-distribution (ID) tasks often come at the expense of out-ofdistribution (OOD) generalization. And the problem is especially acute for smaller models, which are more susceptible to catastrophic forgetting as new domain patterns overwrite previously learned skills. General mitigations try to smooth the issue by scaling huger training sets, crafting elaborate data mixture schedules, and designing diverse learning objectives [29, 42, 43, 59]. While these methods can somehow recover models robustness on OOD dataset, they lift Conference acronym XX, June 0305, 2018, Woodstock, NY Fei Zhao et al. 8.74, which demonstrates superior data efficiency and broader capability gains from RL-centric curriculum learning. Our contribution can be summarized as follows: We present RedOne 2.0, an SNS-domain LLM that achieves higher capability with less data and smaller models. We propose progressive, RL-prioritized post-training paradigm that delivers consistent improvements in both general and SNS-specific abilities while mitigating the seesaw effect of SFT. We provide comprehensive empirical validation showing state-of-the-art results in the SNS scenario and strong robustness under distribution shift, establishing RedOne 2.0 as competitive and cost-effective baseline for domain LLMs."
        },
        {
            "title": "2.3 Domain-specific LLM Post-training\nDomain adaptation injects targeted knowledge and preferences to\nboost in-domain performance. Typical pipelines combine continued\npre-training on domain corpora with supervised fine-tuning and",
            "content": "Figure 1: The comparison of various scale models performance in the SNS domain. up the budgets of data and compute to sky-high level. In contrast, reinforcement learning (RL) offers more distinctive advantage for domain specific post-training [36]. By optimizing against preference or reward signals, RL directly aligns model behavior with human and downstream objectives, preserving existing competencies while unlocking latent capabilities. As RL has matured in LLM optimization, its practical value has been demonstrated across preference alignment, safety shaping, controllable generation and task-level policy tuning [24, 56, 76]. However, how to structure RL-centric training to balance data efficiency, stability, and domain transfer in the fast-evolving SNS environment remains underexplored. To address this gap, we introduces RedOne 2.0, new SNSoriented LLM that adopts progressive, RL-prioritized post-training paradigm. Our design philosophy is that exploration targeted correction refinement yields better stability generalization trade-offs than SFT-heavy recipes, especially at smaller parameter scales and with limited domain data, as shown in Fig. 1. Concretely, RedOne 2.0 is trained in three stages: 1) Exploratory Learning. The model is exposed to curated SNS corpora to establish initial domain alignment and to diagnose the lack of ability for realistic distributions. 2) Targeted Fine-Tuning. We apply SFT on tasks where previous stage diagnostics reveal systematic weaknesses, blending small fraction of general data to explicitly regularize against forgetting and retain broad generalization. 3) Refinement Learning. Building on the corrected model, we reuse RL with SNS-centric signals to consolidate improvements and smooth trade-offs across different tasks, yielding the coherent capability gains. We also conducted extensive experiments across various tasks spanning three categories to validate our post-training pipeline. The 4B-parameter variant surpasses the 7B counterpart by 2.41 on average, demonstrating that strong performance is attainable at compact scales. Using Qwen3-4B as the base, RedOne 2.0 requires only half of RedOnes data while achieving performance lift with RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 2: Overview of our RL-based incremental three-stage training pipeline. preference optimization on domain tasks, yielding strong results in finance [9, 22, 70, 75], law [13, 15, 23, 38], medicine [35, 53, 63, 72], and the sciences [6, 7, 65, 80]. However, heavy reliance on SFT can overfit to benchmarks and weaken generalization, while later reinforcement learning often only partly fixes this drift. RedOne 2.0 addresses these limits with an RL-centric design that emphasizes staged reinforcement and dynamic task sampling to improve both general competence and domain reasoning."
        },
        {
            "title": "3.1 Dataset Definition\nTo ground the three-stage pipeline, we first construct and charac-\nterize the training dataset. We curate large-scale data from both the\nSNS domain and the general domain. The former covers capabili-\nties commonly evaluated in SNS scenarios, including information\nextraction, semantic matching, content understanding, user behav-\nior modeling, dialogue, and translation, and spans more than 75\ncarefully defined tasks. This provides sufficient supervision for fol-\nlowing training pipeline and establishes a solid data foundation for\nadapting RedOne 2.0 to a wide range of SNS real-world applica-\ntions. The latter integrates high-quality open-source datasets that\nhave been widely validated by the community, thereby reducing\nredundant annotation and processing costs while ensuring robust\ngeneral capabilities of RedOne 2.0.",
            "content": "We then normalize all collected data into unified format of question ğ‘„ and answer ğ´, yielding the final dataset = { DSNS DGEN (ğ‘„, ğ´) } (1) where DSNS and DGEN denote the SNS-domain and general-domain subsets, respectively."
        },
        {
            "title": "3.2 Exploratory Learning\nThe goal of Exploratory Learning is to achieve initial alignment\nbetween the base model and SNS-domain characteristics. Instead\nof committing early to narrowly scoped objectives, this stage im-\nmerses the model in diverse SNS data to capture the breadth of\ntask distributions and domain-specific interaction patterns, while\npreserving general competence and revealing tasks that remain\nhard due to unfamiliarity from base model.",
            "content": "3.2.1 Data Construction. We select about 750K SNS entries DSNS1 from DSNS, covering 75 heterogeneous tasks and all capability types, such as post taxonomy, query classification, machine reading comprehension, post view search and SNS domain translation. balanced sampling schedule is also conducted to preserve the visibility of long-tail behaviors. Additionally, to maintain reasoning and general competence, we attach 50K data DGEN1 from DGEN with rationales, which are widely regarded as beneficial for preserving model knowledge and supporting structured reasoning. 3.2.2 Reward Function. In RL, the reward function is the most critical supervision signal during training. Unlike prior works [24] that takes simple approach by defining single rule-based reward to check whether response follows specified format and whether the final answer is correct, we take different path. Considering that the downstream scenarios aligned with RedOne 2.0 span heterogeneous tasks with substantial variation in both format and content, and that their evaluation criteria are highly diverse, we define task type specific rewards for sampled pair (ğ‘„, ğ´) and models output ğ‘‚ as following: 1) Exact Match. For close-ended problems with determinate answers, such as classification or multiple-choice, we focus on Conference acronym XX, June 0305, 2018, Woodstock, NY Fei Zhao et al. constraining answer consistency with exact match score. REM (ğ‘‚, ğ´) = (cid:40) 1, ğ‘‚ = ğ´, 0, otherwise. (2) 2) Metrics-based. For open-ended tasks such as translation, we avoid seeking single binary correct standard. Instead, we define rewards using task-specific evaluation metrics Eval. RMet (ğ‘‚, ğ´) = Eval(ğ‘‚, ğ´) (3) 3) Sand Box. For tasks like code generation, traditional exactmatch or metric-based scoring struggles to objectively assess output quality. The most direct approach is sandbox simulation, where we create an execution environment to run the generated solution and evaluate it by the obtained results Exe(ğ‘‚). RSandBox (ğ‘‚, ğ´) = (cid:40) 1, Exe(ğ‘‚) = ğ´, otherwise. 0, (4) 4) Pattern. Additionally, given the instability of generative LLM output formats and the existence of community benchmarks targeting instruction following, we design pattern-based matching mechanism Match that emphasizes adherence to specified formats rather than the semantic content itself. (5) Finally, each samples reward (ğ‘‚, ğ´) is mapped to the correRPattern (ğ‘‚, ğ´) = Match(ğ‘‚, ğ´) sponding reward function based on its task category. 3.2.3 Domain Alignment. In this stage, our domain alignment objective is to jointly raise general competence and SNS-specific capability while fully exploiting the models latent potential, and to systematically surface sub-tasks that remain difficult due to base models unfamiliarity. In practice, we randomly mix DSNS1 and DGEN1 to form D1, which conducts DAPO-based [76] RL training for this stage. For specific data entry (ğ‘„, ğ´), DAPO samples group of ğº individual candidate outputs {ğ‘‚ğ‘– }ğº ğ‘–=1 from the old policy model ğœ‹ğœƒold . Then, we can optimize the policy ğœ‹ğœƒ by optimizing the following loss function: LDAPO (ğœƒ ) =E (cid:34) (ğ‘„,ğ´)D1,{ğ‘‚ğ‘– }ğº ğ‘–=1 ğ‘‚ğ‘– ğº ( ğ‘„ ) ğœ‹ğœƒ old (cid:16) ğ‘Ÿğ‘–,ğ‘¡ (ğœƒ ) Ë†ğ´ğ‘–,ğ‘¡, min 1 ğ‘–=1 ğ‘‚ğ‘– (cid:205)ğº ğ‘–=1 ğ‘¡ ="
        },
        {
            "title": "3.3 Targeted Fine-Tuning\nAfter initial SNS alignment, Targeted Fine-Tuning directly addresses\ntasks that remain weak. The emphasis is on repairing deficiencies\nwhile preserving previous gains, achieved by blending difficult SNS\ndata with carefully filtered general data.",
            "content": "3.3.1 Data Preparation. We construct dataset D2 of 1.8M examples comprising 1.7M SNS instances and 100K general-domain instances. The SNS portion is derived from our pre-training data corpus DSNS, which refer to the failure tasks bucket in previous stage identified via evaluation result on various benchmarks. We further stratify these examples by capability and upweight rare but impactful cases. For the general portion, we sample them from DSNS and introduce examples with soft labels by generating candidates responses from the previous stage model. Concretely, for given prompt we generate 8 candidate completions by the resulted model from the first stage, and score them with composite quality signal from judge model, and select the best one to form soft supervisory target. These soft labels not only mitigate catastrophic forgetting of general knowledge during SFT, but also reduce impact on distributional transformation between ground-truth labels and the first stage models learned distribution, thereby improving learning efficiency for SNS tasks [48]. In fact, this small set of soft-labeled general data functions as data-level regularizer, preventing SNS-focused SFT from drifting too far from the reference model. 3.3.2 Targeted Learning. In this stage, optimization aims to close gaps on underperforming SNS tasks while preserving gains from the previous stage. We use plain SFT objective on mixture of hard SNS examples and small set of general-domain examples with soft labels: LSFT = E(ğ‘„,ğ´)D2,ğ‘‚ğœ‹ğœƒ ( ğ‘„ ) ğ‘‚ ğ‘¡ =1 log ğœ‹ğœƒ(cid:0)ğ‘‚ğ‘¡ (cid:12) (cid:12) ğ‘„, ğ‘‚<ğ‘¡ (cid:1) (9) where ğœ‹ğœƒ denotes the current policy model, ğ‘„ is the question, and ğ‘‚ is the target output sequence. Finally, this stage yields consistent improvements on previously weak SNS tasks while maintaining gains from the first stage on the most capability types. The combination of SNS-prioritized repair and soft-label regularization from small general set produces more balanced and robust model prepared for RL-based refinement in the last stage. clip(ğ‘Ÿğ‘–,ğ‘¡ (ğœƒ ), 1 ğœ€low, 1 + ğœ€high) Ë†ğ´ğ‘–,ğ‘¡ (cid:35) (cid:17) 0 < (cid:12) where ğœ€low and ğœ€high control the clipping range, and (cid:12){ğ‘‚ğ‘– is_equivalent(ğ´, ğ‘‚ğ‘– )}(cid:12) s.t. (cid:12) < ğº (6)"
        },
        {
            "title": "3.4 Refinement Learning\nThe final stage, Refinement Learning, consolidates prior gains and\nachieves further performance improvements. This is done by apply-\ning RL after the previous SFT-based stage, with the training again\ncentered on SNS data.",
            "content": "(7) ğ‘Ÿğ‘–,ğ‘¡ (ğœƒ ) = ğœ‹ğœƒ (ğ‘‚ğ‘–,ğ‘¡ ğ‘„, ğ‘‚ğ‘–,<ğ‘¡ ) (ğ‘‚ğ‘–,ğ‘¡ ğ‘„, ğ‘‚ğ‘–,<ğ‘¡ ) ğœ‹ğœƒold Rğ‘– mean({Rğ‘– }ğº std({Rğ‘– }ğº ğ‘–=1) Finally, this alignment stage yields broad, stable gains without premature specialization, while producing fine-grained diagnosis of where the model underperforms. And the resulting capability map guides targeted repair in the next stage. Ë†ğ´ğ‘–,ğ‘¡ = ğ‘–=1) (8) Further Refinement. Specifically, we use approximately 400K 3.4.1 examples drawn from the SNS and general sources as in the previous stage, with an emphasis on the difficult subsets. We initialize the policy from the prior stage to provide strong starting base model, and then apply preference-based DAPO [76] as same as the first stage training process for refinement. In this stage, we also increase the proportion of samples with rationale to 57.18%, further preserving the models reasoning ability and benefiting broad range of downstream tasks. After training, the models behavior is RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services Conference acronym XX, June 0305, 2018, Woodstock, NY Table 1: Comparison results across General-Bench, SNS-Bench and SNS-TransBench. Bold entries indicate the best performance, while underlined entries denote the second one in each category. Models GPT-4o-1120 Gemini-2.0-Flash Claude-3.7-Sonnet Doubao-1.5-Pro-32k Qwen-Max GLM-4-Plus GPT-OSS-120B dots.llm1 GLM-4.5 Deepseek-V3-0324 DeepSeek-V3. Qwen3-4B Qwen2.5-7B Llama-3.1-8B Ministral-8B InternLM3-8B Qwen3-8B GLM-4-9B-0414 RedOne-7B RedOne 2.0 4B Phi-4-14B GPT-OSS-20B Mistral-Small-24B Qwen3-30B-A3B GLM-4-32B-0414 Qwen2.5-32B Qwen3-32B Llama-3.3-70B RedOne-32B RedOne 2.0 30B-A3B General-Bench SNS-Bench Avg. Taxon. Hash. QCorr MRC NER Gender CHLW QGen Avg. Proprietary Large Language Models or The Scale of Large Language Models > 100B SNS-TransBench ZHEN ENZH BLEU chrF++ BLEU chrF++ 70.72 74.42 75.10 76.13 71.86 70.25 76.71 70.20 73.66 75.22 77. 69.80 63.01 51.24 49.93 58.55 66.90 63.27 63.83 70.80 63.00 74.76 65.63 74.46 74.39 71.68 72.67 67.64 73.72 75.17 65.79 68.76 72.03 30.00 65.68 65.46 67.20 62.96 70.76 67.27 70.20 60.88 49.50 37.74 42.62 51.83 58.67 56.03 72.18 75.85 57.62 62.89 64.88 64.29 63.36 59.90 61.52 62.94 81.45 77.02 84.98 87.36 88.83 83.21 84.47 84.31 86.04 82.45 86.93 86.59 88. 81.90 73.80 66.62 70.58 76.98 82.44 77.67 88.02 89.05 79.56 83.99 83.89 85.81 85.50 80.51 86.04 83.28 90.19 89.99 58.89 52.21 54.86 61.32 61.34 55.81 61.45 40.75 64.94 60.97 62.37 51.79 38.96 48.41 37.39 54.10 31.11 58.25 30.61 54.36 37.97 52.13 30.09 56.83 38.53 42.10 31.09 56.22 41.32 47.71 40.45 48.67 33.60 The Scale of Large Language Models < 10B 54.99 53.58 56.13 56.60 55.78 53.16 55.84 14.93 57.23 56.00 55.22 88.08 89.64 92.23 90.67 91.19 86.53 91.19 89.12 92.75 90.16 91. 38.31 42.37 33.32 36.24 38.65 46.47 38.03 65.09 60.92 34.69 45.32 31.27 30.71 25.25 48.45 45.29 63.98 66.54 44.50 45.41 47.10 37.79 39.41 44.68 47.01 51.86 43.15 79.27 88.08 74.61 82.38 66.84 89.12 51.30 70.47 78.76 28.17 33.76 26.88 28.04 44.71 27.95 27.51 74.73 79.11 10B < The Scale of Large Language Models < 100B 46.32 54.58 48.77 44.75 47.33 46.00 49.39 50.76 67.07 63.76 53.39 56.43 46.51 52.23 53.72 55.04 54.56 27.38 59.24 62.16 44.99 54.81 52.09 45.75 50.41 54.51 53.76 56.09 51.66 54.15 89.12 92.23 91.19 90.16 80.31 90.67 91.19 91.19 81.87 81.87 29.23 32.68 32.10 27.19 33.19 38.84 33.48 33.58 70.40 74.19 47.33 46.27 45.49 46.55 46.64 44.68 45.61 44.63 45.47 46.03 46. 46.75 44.65 38.60 46.27 43.46 45.89 45.52 48.69 47.17 44.76 45.26 46.01 45.67 46.90 45.66 45.74 46.41 50.37 49.15 61.35 60.45 61.85 57.15 62.18 59.02 62.84 51.00 64.45 61.90 62.08 51.81 52.86 44.52 46.83 48.39 55.46 48.55 66.88 67.57 55.62 60.36 58.18 56.98 57.59 58.89 59.46 56.45 69.03 69.04 40.32 32.72 35.63 33.71 35.55 41.57 33.06 30.93 30.57 35.65 31. 26.87 31.43 23.07 25.67 24.85 33.21 32.20 38.06 38.61 31.28 30.74 31.29 34.07 36.32 32.56 32.15 34.00 40.55 40.22 63.91 58.84 61.66 61.85 60.92 65.95 59.73 58.66 56.77 61.58 58.8 54.26 55.91 48.15 50.91 50.44 58.81 56.90 62.66 62.46 57.23 57.46 56.72 58.86 61.31 58.14 58.54 59.18 64.54 63.88 49.15 41.80 45.79 45.54 46.08 48.79 42.67 44.42 39.55 46.86 41. 36.35 38.36 29.32 32.02 35.58 40.09 39.73 46.88 45.78 37.58 37.83 39.28 41.19 42.53 42.34 40.44 41.25 48.20 48.06 47.28 40.16 44.23 44.35 44.14 47.06 40.47 42.8 38.2 44.58 39.77 35.41 36.48 29.13 31.18 34.04 38.85 37.40 44.82 43.84 36.68 36.19 37.32 39.51 40.77 40.71 38.85 39.56 46.05 45.95 Avg. 50.17 43.38 46.83 46.36 46.67 50.84 43.98 44.20 41.27 47.17 43.04 38.22 40.55 32.42 34.95 36.23 42.74 41.57 48.11 47.67 40.69 40.56 41.15 37.05 45.23 43.44 42.50 43.50 49.84 49.54 stabilized and smoothed within the explored solution space, yielding further improvements on both SNS-specific and general tasks. Compared to the previous stage, the RL-based refinement delivers better convergence and more robust domain adaptation."
        },
        {
            "title": "4 Experiments\n4.1 Implementation Details\nDuring the Exploratory Learning stage, we trained for 500 steps\nwith maximum prompt/response lengths of 10,000/8,192 tokens\n(18,192 total), plus a 4,096-token overlong buffer with 1.0 penalty\nfactor. We used a prompt batch size of 1,024 with 16 responses\nper prompt (global batch size 16,384) and mini-batch size covering\n256 prompts, yielding 4 gradient updates per rollout. We adopted\nDAPO with clipping parameters ğœ€low = 0.2 and ğœ€high = 0.28. Opti-\nmization employed AdamW with a constant learning rate of 5Ã—10âˆ’6,\nweight decay 0.1, with linear warmup applied for 10 rollout steps.\nIn Targeted Fine-Tuning, we trained for 2 epochs with batch size\n64 and maximum sequence length of 16,384 using sequence pack-\ning. We optimized cross-entropy loss with AdamW at a learning\nrate 5 Ã— 10âˆ’6, applying a warmup ratio of 0.1 followed by cosine",
            "content": "scheduling. The final Refinement Learning stage mirrored the first stage configuration."
        },
        {
            "title": "4.2 Experimental Setting\n4.2.1 Benchmarks. We perform a comprehensive evaluation of\nRedOne 2.0 and baselines in both the general and SNS domain\ncapabilities using commonly used benchmarks in the community.\nSpecifically, in general domain, we systematically assess six capabil-\nities, including knowledge reasoning, mathematical reasoning, code\ngeneration, machine translation, instruction following, and halluci-\nnation detection, as well as CompassBench [14], a comprehensive\nbench to provide an integrated, multi-dimensional view of model\nperformance. 1) Knowledge Reasoning. We use MMLU [27],\nCMMLU [45], C-Eval [30], GPQA-Diamond [60], NewsBench [47],\nMMLU-Pro [69], BBH [64], and GaokaoBench [82] to probe broad and\nspecialized knowledge, reasoning robustness, difficulty-calibrated\nmultiple choice, and exam-style generalization in both English and\nChinese. 2) Mathematical Reasoning. We adopt GSM8K [12],\nMATH500 [28], and the high-stakes AIME 2025 [52] set to measure\nmulti-step arithmetic and competition-level problem solving. 3)",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Fei Zhao et al. Code Generation. We evaluate program synthesis and correctness with HumanEval [11], MBPP [5], and the temporally refreshed, contamination-aware LiveCodeBench [33], reporting pass@k and execution-based metrics. 4) Machine Translation. We benchmark multilingual translation with the WMT tasks (i.e. WMT-22 [40], WMT-23 [18] and WMT-24 [39]) and FLORES [20], covering diverse language pairs and domains. 5) Instruction Following. We employ IFEval [84], which provides automatically verifiable constraints to quantify compliance under explicit instructions. 6) Hallucination Detection. We use HaluEval [46] to assess the tendency to produce unverifiable or fabricated content across question answering, dialogue, and summarization settings. In the SNS domain, we validate models on benchmarks built from real SNS scenarios, covering five aspects: post comprehension, information retrieval, sentiment and intent analysis, personalized recommendation, and translation. We use SNS-Bench [25], large-scale bench with 6,658 questions spanning eight tasks from social platform with over 300M users, which includes the following tasks: 1) Note-Taxonomy (Taxon.) for content categorization; 2) Note-Hashtag (Hash.) to select suitable tags; 3) NoteQueryCorr (QCorr) to align user queries with note content and topic; 4) Note-MRC (MRC) for simple and complex reading comprehension over long notes; 5) Note-NER (NER) for entity extraction; 6) Note-Gender (Gender) to assess gender-sensitive appeal; 7) Note-CHLW (CHLW) to highlight salient words in comment threads; and 8) Note-QueryGen (QGen) to produce effective search queries. For translation, we adopt SNS-TransBench [26], curated set of 2,858 EnglishChinese cases from posts, comments, and multimedia captions that emphasizes phenomena central to SNS translation, including humor localization, emoji semantics, and meme adaptation. It tests whether models can preserve pragmatics, style, and culture-bound references in short, high-context text typical of social platforms. 4.2.2 Baselines. We conduct comparison experiments with various proprietary models, including GPT4o-1120 [32], Gemini-2.0Flash [66], Claude-3.7-Sonnet [4], Doubao-1.5-Pro-32k [16], QwenMax [74], and GLM-4-Plus [19], open-source models, such as Qwen series [73, 74], Llama series [21], Ministral [54], Mistral-Small24B [55], InternLM3-8B [10], Phi-4-14B [1], dots.llm1 [31], gpt-oss series [2], GLM series [78] and DeepSeek series [49], as well as SNS domain specific models RedOne [83]."
        },
        {
            "title": "4.3 Main Results\nAs shown in Table 1, we conduct a comparison with various models\nacross General-Bench, SNS-Bench, and SNS-TransBench, covering\na broad spectrum of capabilities from general reasoning to SNS-\ndomain understanding and multilingual transfer.",
            "content": "Across all benchmarks, RedOne 2.0 averagely achieves strong and balanced results, surpassing both openand closed-source baselines of comparable scale. Specifically, RedOne 2.0 4B attains the highest average score on General-Bench with 70.8, exceeding even larger open models such as Qwen3-8B and GLM-4-9B, and achieving comparable or superior results to some proprietary LLMs or LLMs with more than 100B parameters. This demonstrates that the proposed three-stage post-training pipeline effectively enhances both general and domain-specific capabilities even at smaller scales. Table 2: Generalization of our training pipeline over different base models. We report the average performance for GeneralBench, SNS-Bench and SNS-TransBench. Models General-Bench SNS-Bench SNS-TransBench Qwen3-4B RedOne 2.0 4B Qwen3-8B RedOne 2.0 8B Qwen3-30B-A3B RedOne 2.0 30B-A3B Qwen3-32B RedOne 2.0 32B 69.80 70.80 66.90 69.27 74.46 75.17 72.67 73. 51.81 67.57 55.46 65.82 56.98 69.04 59.46 69.76 38.22 47.67 42.74 46. 37.05 49.54 42.50 49.11 On SNS-Bench, which evaluates domain-specific understanding and reasoning across eight tasks, RedOne 2.0 still leads within its scale group. The 4B variant achieves an average score of 67.57, outperforming all sub-10B baselines and exceeding the previous RedOne-7B model by 0.69, despite having fewer parameters. Similarly, the 30B-A3B version achieves 69.04, even matching or surpassing much larger models such as GPT-4o and GLM-4.5. These results validate that RedOne 2.0 not only inherits strong generalization from the base models but also substantially improves SNS-domain competence through progressive alignment. On SNS-TransBench, which measures cross-lingual understanding and translation quality between Chinese and English, RedOne 2.0 maintains competitive results across BLEU and chrF++ metrics. Both the 4B and 30B-A3B variants achieve the top-2 overall averages with 47.67 and 49.54, respectively, outperforming all similarly scaled models. The consistent performance across both translation directions indicates that RedOne 2.0s alignment pipeline preserves linguistic versatility while improving domain adaptation. We further observe clear scalability in the RedOne 2.0 family. As model size increases from 4B to 30B, both general and SNS-specific metrics steadily improve, with the average gain on General-Becnh, SNS-Bench and SNS-TransBench exceeding 4.37, 1.47 and 1.87, respectively. This confirms that the proposed three-stage pipeline scales effectively and provides stable improvements without overfitting to the SNS domain. Compared with the RedOne models, RedOne 2.0 shows promising gains across majority evaluation suites. For instance, RedOne 2.0 4B improves by 6.97 on General-Bench and by 0.69 on SNS-Bench relative to RedOne-7B. Additionally, for SNS-TransBench, although RedOne 2.0 series are smaller than RedOne, they still achieve comparable performance. And comparing with RedOne, RedOne 2.0 obtains higher improvement from base model. Overall, these results confirm the effectiveness of RedOne 2.0 in achieving efficient, scalable and stable alignment."
        },
        {
            "title": "4.4 More Analysis\n4.4.1 Generalization Across Different Base-Model Scales. Table 2\ndemonstrates that our three-stage training pipeline generalizes ef-\nfectively across base models of different scales. Consistent improve-\nments are observed on all benchmarks, confirming the robustness\nof our approach in transferring alignment benefits from smaller to\nlarger models. Moreover, scaling up the base model further ampli-\nfies the overall performance of RedOne 2.0, indicating that larger",
            "content": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services Conference acronym XX, June 0305, 2018, Woodstock, NY Table 3: The impact of different training stage on Qwen3-4Bs performance. Table 5: RedOne2.0s online application on personalized recreation of posts title. Exploratory Learning Targeted Fine-Tuning Refinement Learning General -Bench SNS -Bench SNS-Trans Bench 69.80 63.65 69.80 71.25 70.04 70.80 51.81 61.10 63.03 62.27 65.67 67.57 38.22 46.00 45.95 43.35 47.72 47.67 Metrics Relative Change Business Value Advertiser Value (AdvV) Content Quality Vague Titles Ratio Practical Titles Ratio Authentic Titles Ratio Interactive Titles Ratio +0.43% -11.9% +7.1% +12.9% +25.8% Table 4: Comparison with task specific fine-tuning on Qwen34B and RedOne2.0 4B."
        },
        {
            "title": "Models",
            "content": "Hash. QCorr"
        },
        {
            "title": "MRC",
            "content": "Qwen3-4B Qwen3-4B (Fine-tuned) RedOne 2.0 4B 81.90 90.12 89.05 38.31 60.11 60.92 34.69 57.54 66."
        },
        {
            "title": "CHLW QGen",
            "content": "SNS-Trans Qwen3-4B Qwen3-4B (Fine-tuned) RedOne 2.0 4B 28.17 67.24 79.11 46.75 49.24 47.17 38.22 44.25 47.67 capacities allow more effective utilization of the staged optimization signals. Notably, the 4B and 30B-A3B variants exhibit superior gains comparing with similar scale models, particularly on SNS-related benchmarks. We think this is because that they are both based on instruction-tuned backbones, and stronger instruction-following capabilities can better absorb and express the multi-stage alignment signals, leading to stronger generalization and domain adaptation. Incremental Performance Over three-Stage Training. As shown 4.4.2 in Table 3), we evaluate the incremental impact of each stage in our three-phase training framework. The RL-based Exploratory Learning stage establishes strong foundation, improving performance to 71.25% on General-Bench, 62.27% on SNS-Bench, and 43.35% on SNS-TransBench, highlighting its effectiveness in consistently enhancing the overall capability of the base model. The SFT-based Targeted Fine-Tuning stage then addresses weaknesses in the SNS domain exhibited in previous stage, raising scores to 65.67% on SNS-Bench and 47.72% on SNS-TransBench, with only slight drop 1.21% on General-Bench. Finally, the RL-based Refinement Learning stage balances performance across tasks, increasing the average from 61.14% to 62.01% and resulting in final scores of 70.80% on General-Bench, 67.57% on SNS-Bench, and 47.67% on SNS-TransBench. Superiority to the Naive SFT Followed RL Baseline. Then, con4.4.3 sidering the most notable shift in RedOne 2.0 lies in its departure from the traditional SFT-centric domain-specific post-training paradigm to RL, we conduct experiments to compare it with naive SFT followed RL baseline. This baseline typically began with SFT for domain adaptation, followed by RL to align the model with human preferences or downstream objectives, as shown in the second and third rows of Table 3. While SFT can effectively boost performance in SNS domains, it often causes seesaw effect, significantly reducing general capability from 69.80 to 63.65. Although subsequent RL attempts to mitigate this issue, the overall improvements across the three benchmarks remain limited. In contrast, RedOne 2.0 refine the process: starting with RL to establish domain priors, followed by SFT for targeted enhancement, and concluding with RL for final optimization. This paradigm effectively avoids the trade-off between general and domain-specific performance and surpasses the naive baseline by 1.00 on General-Bench, 4.54 on SNS-Bench, and 1.72 on SNS-TransBench. 4.4.4 Comparison with Task Specific Fine-tuning. We also compares RedOne2.0 framework, designed for unified optimization across all tasks, against the conventional task specific fine-tuning approach. As detailed in Table 4, this method also yields strong performance on its target objective. For instance, Qwen3-4B model fine-tuned specifically for QGen achieves 49.24, and another fine-tuned for Hash. reaches 90.12. However, RedOne2.0 4B, trained concurrently on mixture of all tasks, delivers robust and highly competitive results across the entire spectrum of benchmarks. Especially, it outperforms the task specific fine-tuned Qwen3-4B models on MRC with 9.00, and on CHLW with 11.87. It also maintains strong performance on QCorr at 60.92 and on SNS-Trans at 47.67. These results substantiate that unified training framework can effectively capture and leverage beneficial inter-task relationships, enabling single model to achieve comprehensive and better capability."
        },
        {
            "title": "4.5 Online Application\nWe deployed RedOne2.0 on a large scale social networking plat-\nform with more than 3 million users to recommend personalized\nre-created post titles in real time. Each pre-published title is routed\nto the service, which performs semantic analysis and produces an\nenhanced title that preserves the original intent while optimizing\nfor engagement. The suggestion is exhibited to the creator or, in se-\nlected traffic buckets, used directly to measure performance against\nthe original. Evaluation covered business impact and content quality.\nThe primary business indicator is Advertiser Value (AdvV), which\nreflects the value delivered to advertisers through audience quality\nand engagement. Content quality is measured through human re-\nview across four dimensions: vagueness, practicality, authenticity,\nand interactivity.",
            "content": "As shown in Table 5, the online test was conducted over several weeks and millions of posts, and showed consistent gains. Conference acronym XX, June 0305, 2018, Woodstock, NY Fei Zhao et al. Table 6: Good cases for personalized re-creation of post titles. Title Source Content Case 1 Original Plum rain season, great helper for dehumidification and mold prevention. Base Model Dehumidification essential for the plum rain season,a fresh choice for dry and comfortable life. RedOne 2. Original Base Model RedOne 2.0 Say goodbye to steamy homes! Rescue your plum rain season. Case 2 Beijing wedding photo recommendations: 17 lawn wedding photo outdoor spots. Dreamy lawn wedding photos, capturing the most beautiful moments. Escape the studio! 17 stunning lawns capture cinematic-level wedding photos. Table 7: Bad case for personalized re-creation of post titles. Title Source Content Original Dont buy the wrong transportation card for Osaka and Kyoto! lesson learned the hard way! Base Model RedOne 2.0 guide to Japanese transportation cardsstop making these mistakes! Avoid these pitfalls for your Kansai trip, check out the guide now. Advertiser Value increased by 0.43%, statistically significant improvement at platform scale. Human evaluation reported an 11.9% reduction in vague titles and increases of 7.1% in practical titles, 12.9% in authentic titles, and 25.8% in interactive titles. The strong rise in interactive titles indicates that the model learns linguistic patterns that encourage responses such as comments and shares. These results also suggest clear link between quality improvements and business outcomes. Moreover, practical and authentic titles are likely to increase user trust and dwell time, while interactive titles stimulate community activity. Deploying RedOne2.0 therefore improves user experience and yields measurable advertiser value, which demonstrates its effectiveness for real world content optimization."
        },
        {
            "title": "4.6 Case Study\nTo qualitatively assess RedOne 2.0, we compare its outputs with\na baseline on personalized re-created titles. As shown in Table 6,\nRedOne 2.0 consistently produces more evocative and engaging\nphrasing. For the dehumidification example, the baseline remains\nserviceable yet generic, whereas RedOne 2.0 introduces a vivid\nword of a steamy home and adds a clear imperative that heightens\nemotional resonance. A similar pattern holds for the example of the\nwedding photography. The baseline provides a broad description,\nwhile RedOne 2.0 frames the content as an exclusive discovery that",
            "content": "promises cinematic results, which is likely to raise curiosity and strengthen click intent. We also exhibited bad case in scenarios that require strict preservation of critical facts, as illustrated in Table 7. The original title centers on the risk of choosing the wrong transportation card for Osaka and Kyoto and stresses hard learned lesson. The baseline remains close to this focus. RedOne 2.0 generates more interactive sentence but generalizes the topic to Kansai travel and omits the key reference to the transportation card, which weakens informational precision and urgency. These cases indicate that RedOne 2.0 excels at optimizing for engagement and stylistic appeal yet can sometimes over optimize at the expense of essential details. Future work should reinforce faithfulness constraints while preserving expressiveness."
        },
        {
            "title": "5 Conclusion\nIn this paper, we present RedOne 2.0, an SNS-specific LLM post-\ntraining framework tailored for SNS, where tasks are highly hetero-\ngeneous, dynamic, and culturally diverse. Unlike traditional SFT-\ncentric approaches that risk catastrophic forgetting and unstable\ntrade-offs between in-domain and out-of-domain performance, Re-\ndOne 2.0 adopts a progressive, RL-prioritized three-stage pipeline:\nExploratory Learning to establish initial domain alignment and sur-\nface weaknesses, Targeted Fine-Tuning to selectively repair deficien-\ncies while retaining general competence, and Refinement Learning\nto consolidate improvements via RL with SNS-centric rewards. Sup-\nported by a large, task-diverse dataset spanning more than 75 SNS\ntasks and high quality general corpus, this paradigm demonstrates\nstrong data efficiency, stable adaptation, and robust generalization\neven at compact model scales. Overall, RedOne 2.0 provides a com-\npetitive, cost-effective, and scalable baseline for LLM deployment\nin SNS, advancing capability without sacrificing robustness, safety,\nor general usability.",
            "content": "References [1] Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905 (2024). [2] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. 2025. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925 (2025). [3] Sacha Altay, Emma Hoes, and Magdalena Wojcieszak. 2025. Following news on social media boosts knowledge, belief accuracy and trust. Nature Human Behaviour (2025), 110. [4] Claude Anthropic. 2025. 3.7 sonnet and claude code. [5] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021). [6] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631 (2023). [7] Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, et al. 2025. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763 (2025). [8] Eytan Bakshy, Solomon Messing, and Lada Adamic. 2015. Exposure to ideologically diverse news and opinion on Facebook. Science 348, 6239 (2015), 11301132. [9] Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, and Muhammad Abdul-Mageed. 2024. Fintral: family of gpt-4 level multimodal financial large language models. arXiv preprint arXiv:2402.10986 (2024). [10] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services Conference acronym XX, June 0305, 2018, Woodstock, NY Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024. InternLM2 Technical Report. arXiv:2403.17297 [cs.CL] [11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021). [13] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera LÃºcia Raposo, Sofia Morgado, et al. 2024. Saullm-7b: pioneering large language model for law. arXiv preprint arXiv:2403.03883 (2024). [14] OpenCompass Contributors. 2023. OpenCompass: Universal Evaluation Platform for Foundation Models. https://github.com/open-compass/opencompass. [15] Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel Ho. 2024. Large legal fictions: Profiling legal hallucinations in large language models. Journal of Legal Analysis 16, 1 (2024), 6493. [16] Doubao-Team. 2025. Doubao-1.5-pro: Model release. https://team.doubao.com/ en/special/doubao_1_5_pro. [17] Shangbin Feng, Herun Wan, Ningnan Wang, Zhaoxuan Tan, Minnan Luo, and Yulia Tsvetkov. 2024. What does the bot say? opportunities and risks of large language models in social media bot detection. arXiv preprint arXiv:2402.00371 (2024). [18] Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, et al. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation. 578628. [19] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793 (2024). [20] Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco GuzmÃ¡n, and Angela Fan. 2022. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics 10 (2022), 522538. [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [22] Thomas Gruca, Joyce Berg, and Michael Cipriano. 2008. Incentive and accuracy issues in movie prediction markets. The Journal of Prediction Markets 2, 1 (2008), 2943. [23] Neel Guha, Julian Nyarko, Daniel Ho, Christopher RÃ©, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. 2023. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models. Advances in neural information processing systems 36 (2023), 4412344279. [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [25] Hongcheng Guo, Shaosheng Cao, Boyang Wang, Lei Li, Liang Chen, Xinze Lyu, Zhe Xu, Yao Hu, Zhoujun Li, et al. [n. d.]. SNS-Bench: Defining, Building, and Assessing Capabilities of Large Language Models in Social Networking Services. In Forty-second International Conference on Machine Learning. [26] Hongcheng Guo, Fei Zhao, Shaosheng Cao, Xinze Lyu, Ziyan Liu, Yue Wang, Boyang Wang, Zhoujun Li, Chonggang Lu, Zhe Xu, et al. 2025. Redefining Machine Translation on Social Network Services with Large Language Models. arXiv preprint arXiv:2504.07901 (2025). [27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020). [28] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 (2021). [29] Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. 2024. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. arXiv preprint arXiv:2403.01244 (2024). [30] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2023. C-eval: multilevel multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems 36 (2023), 6299163010. [31] Bi Huo, Bin Tu, Cheng Qin, Da Zheng, Debing Zhang, Dongjie Zhang, En Li, Fu Guo, Jian Yao, Jie Lou, et al. 2025. dots. llm1 Technical Report. arXiv preprint arXiv:2506.05767 (2025). [32] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [33] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 (2024). [34] Julie Jiang and Emilio Ferrara. 2023. Social-llm: Modeling user behavior at scale using language models and social network data. arXiv preprint arXiv:2401.00893 (2023). [35] Songtao Jiang, Yuan Wang, Sibo Song, Yan Zhang, Zijie Meng, Bohan Lei, Jian Wu, Jimeng Sun, and Zuozhu Liu. 2025. Omniv-med: Scaling medical vision-language model for universal visual understanding. arXiv preprint arXiv:2504.14692 (2025). [36] Hangzhan Jin, Sitao Luan, Sicheng Lyu, Guillaume Rabusseau, Reihaneh Rabbany, Doina Precup, and Mohammad Hamdaqa. 2025. RL Fine-Tuning Heals OOD Forgetting in SFT. arXiv preprint arXiv:2509.12235 (2025). [37] Kris-Fillip Kahl, Tolga Buz, Russa Biswas, and Gerard De Melo. 2024. LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Communities in Long Form Question Answering. In Findings of the Association for Computational Linguistics: EMNLP 2024. 20282053. [38] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2024. Gpt-4 passes the bar exam. Philosophical Transactions of the Royal Society 382, 2270 (2024), 20230254. [39] Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, OndÅ™ej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. 2024. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Proceedings of the Ninth Conference on Machine Translation. 146. [40] Tom Kocmi, Rachel Bawden, OndÅ™ej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, et al. 2022. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT). 145. [41] Mahi Kolla, Siddharth Salunkhe, Eshwar Chandrasekharan, and Koustuv Saha. 2024. Llm-mod: Can large language models assist content moderation?. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 18. [42] Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. 2023. Understanding catastrophic forgetting in language models via implicit inference. arXiv preprint arXiv:2309.10105 (2023). [43] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. 2022. Fine-tuning can distort pretrained features and underperform out-ofdistribution. arXiv preprint arXiv:2202.10054 (2022). [44] Deepak Kumar, Yousef Anees AbuHashem, and Zakir Durumeric. 2024. Watch your language: Investigating content moderation with large language models. In Proceedings of the International AAAI Conference on Web and Social Media, Vol. 18. 865878. [45] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212 (2023). [46] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: large-scale hallucination evaluation benchmark for large language models. arXiv preprint arXiv:2305.11747 (2023). [47] Miao Li, Ming-Bin Chen, Bo Tang, Shengbin Hou, Pengyu Wang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Keming Mao, Peng Cheng, et al. 2024. NewsBench: systematic evaluation framework for assessing editorial capabilities of large language models in chinese journalism. arXiv preprint arXiv:2403.00862 (2024). [48] Zhizhong Li and Derek Hoiem. 2017. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence 40, 12 (2017), 29352947. [49] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [50] Sebastian Lubos, Thi Ngoc Trang Tran, Alexander Felfernig, Seda Polat Erdeniz, and Viet-Man Le. 2024. LLM-generated explanations for recommender systems. In Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. 276285. Conference acronym XX, June 0305, 2018, Woodstock, NY Fei Zhao et al. [73] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [74] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 Technical Report. CoRR (2024). [75] Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023. Investlm: large language model for investment using financial domain instruction tuning. arXiv preprint arXiv:2309.13064 (2023). [76] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. 2025. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 (2025). [77] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302 (2023). [78] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. 2025. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471 (2025). [79] Jingying Zeng, Richard Huang, Waleed Malik, Langxuan Yin, Bojan Babic, Danny Shacham, Xiao Yan, Jaewon Yang, and Qi He. 2024. Large language models for social networks: Applications, challenges, and solutions. arXiv preprint arXiv:2401.02575 (2024). [80] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950 (2024). [81] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment analysis in the era of large language models: reality check. arXiv preprint arXiv:2305.15005 (2023). [82] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2023. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474 (2023). [83] Fei Zhao, Chonggang Lu, Yue Wang, Zheyong Xie, Ziyan Liu, Haofu Qian, JianZhao Huang, Fangcheng Shi, Zijie Meng, Hongcheng Guo, et al. 2025. RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services. arXiv preprint arXiv:2507.10605 (2025). [84] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911 (2023). Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009 [51] Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, Qifan Wang, Si Zhang, Ren Chen, Christopher Leung, Jiajie Tang, and Jiebo Luo. 2023. Llm-rec: Personalized recommendation via prompting large language models. arXiv preprint arXiv:2307.15780 (2023). [52] MAA. 2025. American Invitational Mathematics Examination - AIME. American Invitational Mathematics Examination - AIME 2025 (2025). https://maa.org/mathcompetitions/american-invitational-mathematics-examination-aime [53] Zijie Meng, Jin Hao, Xiwei Dai, Yang Feng, Jiaxiang Liu, Bin Feng, Huikai Wu, Xiaotang Gai, Hengchuan Zhu, Tianxiang Hu, et al. 2025. DentVLM: Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice. arXiv preprint arXiv:2509.23344 (2025). [54] Mistral-AI. 2024. Un Ministral, des Ministraux. https://mistral.ai/news/ministraux. Accessed: 2024-10-16. [55] Mistral-AI. 2025. Mistral Small 3.1. https://mistral.ai/news/mistral-small-3-1. Accessed: 2025-03-17. [56] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 2773027744. [57] Boyu Qiao, Kun Li, Wei Zhou, Shilong Li, Qianqian Lu, and Songlin Hu. 2025. BotSim: LLM-Powered Malicious Social Botnet Simulation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 1437714385. [58] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems 36 (2023), 5372853741. [59] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. 2021. Effect of scale on catastrophic forgetting in neural networks. In International conference on learning representations. [60] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. [61] Nidhish Shah, Zulkuf Genc, and Dogu Araci. 2024. Stackeval: Benchmarking llms in coding assistance. Advances in Neural Information Processing Systems 37 (2024), 3697636994. [62] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [63] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. 2025. Toward expert-level medical question answering with large language models. Nature Medicine 31, 3 (2025), 943950. [64] Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 (2022). [65] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: large language model for science. arXiv preprint arXiv:2211.09085 (2022). [66] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [67] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. science 359, 6380 (2018), 11461151. [68] Qianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. 2023. Reducing spurious correlations in aspect-based sentiment analysis with explanation from large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023. 29302941. [69] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems 37 (2024), 95266 95290. [70] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564 (2023). [71] Feng Xia, Li Liu, Jie Li, Jianhua Ma, and Athanasios Vasilakos. 2013. Socially aware networking: survey. IEEE Systems Journal 9, 3 (2013), 904921. [72] Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al. 2025. Lingshu: Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning. arXiv preprint arXiv:2506.07044 (2025)."
        }
    ],
    "affiliations": [
        "NLP Team, Xiaohongshu Inc. Huangpu District, Shanghai, China"
    ]
}