{
    "paper_title": "StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements",
    "authors": [
        "Mingkun Lei",
        "Xue Song",
        "Beier Zhu",
        "Hao Wang",
        "Chi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 1 3 0 5 8 0 . 2 1 4 2 : r StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements Mingkun Lei1 Xue Song2 Beier Zhu1, 3 Hao Wang4 Chi Zhang 1 AGI Lab, Westlake University 2 Fudan University 3 Nanyang Technological University 4 The Hong Kong University of Science and Technology (Guangzhou) xuesong21@m.fudan.edu.cn {leimingkun, chizhang}@westlake.edu.cn beier002@e.ntu.edu.sg haowang@hkust-gz.edu.cn https://stylestudio-official.github.io/ Figure 1. Results of our text-driven style transfer model. Given style reference image, our method effectively reduces style overfitting, generating images that faithfully align with the text prompt while maintaining consistent layout structure across varying styles."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Text-driven style transfer aims to merge the style of reference image with content described by text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning. Text-driven style transfer is an important task in image synthesis, aiming to blend the style of reference image with the content aligned to text prompt. Recent advancements in text-to-image generative models, such as Stable Diffusion [6, 22, 26], have enabled nuanced style transformations pertaining to the reference image while preserving content fidelity. This technique holds significant practical value, particularly in domains such as digital painting, advertising, and game design. Nevertheless, modern style transfer techniques still fall short of expectations due to the inherent ambiguity in defining style. style image encompasses various elements, including color palettes, textures, lighting, and brush strokes, all of which shape its overall aesthetic. Existing models often replicate all these elements, which can inadvertently lead to overfitting, where the generated output overly mirrors the characteristics of the reference style image. This over-replication of details not only diminishes the aesthetic flexibility of the generated image but also restricts its adaptability to different stylistic or content-based requirements. Therefore, an ideal style transfer approach 1 gies to address these challenges. First, to mitigate conflicts between the text prompt and the style reference during generation, we introduce mechanism where the style image features are integrated into the text features using Adaptive Instance Normalization (AdaIN) before merging them with the image features. This adaptive integration creates more cohesive guiding feature that subsequently guides the final image generation, aligning the stylistic features with the text-based instructions more harmoniously. Second, to disentangle and selectively control various elements within style images, we develop style-based classifierfree guidance (SCFG) inspired by text-based classifier-free guidance in diffusion models. Specifically, we employ layout-controlled generation model, such as ControlNet, to produce comparable negative image that lacks the target style we aim to transfer. This negative image functions similarly to null prompt in diffusion models, allowing the guidance to focus exclusively on the target style element and filter out extraneous stylistic features. Finally, to enhance spatial stability, we incorporate teacher model into the early stages of generation. The tutor model, based on the original text-to-image model, simultaneously performs denoising generation with the same textual prompt and shares its spatial attention maps with the style model at each time step. This method ensures stable and consistent spatial distribution, effectively mitigating issues like the checkerboard artifact. Additionally, this approach enables consistent spatial layouts across different style reference images for the same text prompt, facilitating more straightforward comparisons and evaluations of stylistic transformations. In summary, our contributions are as follows: AdaIN-based Integration: We develop cross-modal Adaptive Instance Normalization (AdaIN) to harmoniously integrate style and text features, improving alignment during generation. Style-based Classifier-Free Guidance (SCFG): We introduce style-guided approach to focus on the target style and reduce unwanted stylistic features. Teacher Model for Layout Stability: We incorporate teacher model to share spatial attention maps during early generation, ensuring stable layouts and mitigating artifacts like the checkerboard effect. Extensive evaluations conducted on wide range of styles and prompts, as shown in Fig. 1, demonstrate that our method significantly improves the alignment of the generated images. Furthermore, our approach is versatile and can be integrated into various existing style transfer frameworks while remaining fine-tuning-free. 2. Related Work Text-to-image generation. Text-conditioned image generative models [1, 3, 5, 22, 25, 26, 29] have demonstrated Figure 2. Illustration of overfitting issues in text-to-image generation, where the model tends to follow dominant colors or patterns from the style image rather than aligning precisely with the text prompt. Each prompt follows the format <color> <object>. From top to bottom, the objects are: bear, apple, frog, and car. Figure 3. Illustration of the checkerboard artifact encountered in the CSGO [36] method during inference. The leftmost column shows the results generated by SDXL [22]. The prompts, from top to bottom, are red apple and pink cup. All generated results use the same initial noise latent. would thus allow for more selective stylistic adjustments, granting the user the flexibility to emphasize or omit specific stylistic components to achieve balanced, intentional transformation. Another challenge arising from overfitting is the difficulty in maintaining text alignment accuracy during textto-image generation. As shown in Fig. 2, current models often prioritize dominant colors or patterns from the style image, even if they contradict instructions specified in the text prompt. This rigidity undermines the models ability to interpret and incorporate nuanced textual guidance, resulting in decreased capacity for precision and customization in the generated output. Finally, style transfer can introduce undesirable artifacts, destabilizing the underlying text-to-image generation models. One common artifact, as shown in Fig. 3, is the layout instability (e.g., checkerboard effect), wherein repetitive patterns inadvertently emerge throughout the generated image, irrespective of user instructions. This highlights the unique challenges introduced by the additional complexity of style transfer. In this paper, we propose three complementary strate2 remarkable capabilities in generating high-quality images. Notably, models in the Stable Diffusion [6, 22, 26] series have achieved impressive advancements due to structural modifications and optimizations in their text encoder components. These improvements have led to significant enhancements in both the visual quality of generated images and the models ability to align with complex textual prompts. The robust performance of text-to-image (T2I) diffusion models has also catalyzed the development of various related visual generation tasks, including text-based image editing [10, 17, 19, 20, 30, 31], subject-driven image generation [4, 8, 14, 21, 28], and other applications that leverage their strong generative and interpretative abilities. Style Transfer. Style transfer refers to the process of applying the style of specific reference image to target content image. Depending on the source of the content image, this task can be broadly categorized into two main approaches: image-driven style transfer and text-driven style transfer. Both approaches share the common goal of extracting and applying more refined and precise styles, which is an ongoing pursuit in the field. Image-driven style transfer faces the challenge of retaining the essential features of the content image in the generated result. For instance, InST [39], textual inversion-based method, employs stochastic inversion to preserve critical details in the content image. StyleID [2] propose query preservation to maintain original content. Similarly, InjectFusion [13] enhances style transfer outcomes by effectively blending features from the h-space [16]. Recently, text-driven style transfer has focused on addressing content leakage, where the application of style could disrupt or obscure important content features. B-LoRA [7] finds that jointly learning the LoRA weights of two specific blocks could achieves style-content separation. DEADiff [23] improves model performance by training joint image-text cross-attention mechanisms, leading to enhanced generation quality. StyleAlign [35] employs strategy of swapping the query and key components within self-attention mechanisms to achieve style-aligned image generations, effectively aligning the stylistic attributes of the output with the desired style input. Adapter-Based Stylization Methods. IP-Adapter [37] introduces Decoupled Cross Attention mechanism to inject image information into the final generated output. Benefiting from training on large-scale datasets, it is capable of performing zero-shot style transfer tasks. Additionally, by employing weight tuning, IP-Adapter [37] can effectively alleviate content leakage issues. Techniques such as InstantStyle [33] mitigate content leakage by selecting appropriate layers in Stable Diffusions UNet [22, 26, 27] for injecting style image features, which avoiding the need for complex manual parameter tuning. StyleShot [9] extracts style information at multiple levels of the image encoder to enable fine-grained control over the style application, allowing for more precise adjustments to the generated images stylistic characteristics. CSGO [36] advances the field by utilizing carefully prepared style dataset to train an adapter-based model that seamlessly decouples and injects content and style features. 3. Method In this section, we detail the three complementary strategies we propose to address the challenges inherent in text-driven style transfer. Each strategy builds upon existing models CSGO [36] but introduces novel mechanisms to overcome the inherent limitations of current approaches in style transfer tasks. 3.1. Preliminaries Before delving into our specific methods, we begin by providing some background on the key components that underlie our approach. Latent Diffusion Models. Latent Diffusion Models (LDMs) [26] represent powerful framework for efficient image generation. Operating in the latent space of Variational Auto-Encoder (VAE) [15], LDMs optimize computational efficiency while retaining high-quality image generation capabilities. Stable Diffusion (SD) [6, 22, 26], one of the most prominent models in this family, generates highfidelity images by denoising noisy latent representations conditioned on text prompts. This denoising process can be formalized as: L(θ) = Et,z,c,ϵN (0,1) (cid:104) ϵ ϵθ (zt, t, c)2 2 (cid:105) , (1) where ϵ is noise sampled from standard Gaussian distribution, ϵθ is the noise predicted by the U-Net architecture, zt is the noisy latent representation at timestep t, and is the conditioning text prompt. The goal of this loss function is to guide the model in iteratively reducing noise, which enables the model to generate high-quality images consistent with the input text. Attention Mechanisms. To enhance the generation process, Stable Diffusion incorporates several self-attention and cross-attention layers [32]. Recent studies show that self-attention captures dependencies within the latent representations, allowing the model to effectively gather context across different parts of the feature map. Cross-attention facilitates the integration of conditioning text embeddings with latent representations, ensuring that the generated output aligns with the input text prompt. This dual attention mechanism enhances the expressiveness and contextual relevance of the output, achieving high level of quality and stylistic consistency. The attention mechanism A(), in its general form, can be defined as: = WQ(f ), = WK(f ), A(Q, K, ) = V, where = softmax( = WV (f ), QK (2) ). and denote the input features and the dimensionality of head. For self-attention, = , allowing the model to focus on intra-feature relationships. For cross-attention, corresponds to the conditioning input, such as text embeddings, allowing the latent feature map to be modulated by the conditioning information. The attention map determines the focus of the model during generation. Style Transfer with an Adapter. Recent advancements in style transfer have explored the use of adapter-based methods to inject style-specific features into text-to-image models. These adapter networks are designed to modify pretrained models to accommodate new styles inputs. One notable approach is the IP-Adapter [37], which leverages dual cross-attention mechanism to integrate both text and image conditions. The CSGO [36] shares the same architecture for text-driven style transfer, with the key distinction being that it is specifically trained on style transfer dataset, enabling it to better capture and transfer stylistic features. In this design, one cross-attention layer extracts features from the text prompt, while the other extracts features from the image. The two sets of features are then combined through weighted sum, allowing the model to generate images that adhere to both the stylistic attributes specified by the text and the visual characteristics of the image. This technique provides simple yet effective method for adapting existing models to perform style transfer without the need for extensive retraining or additional model parameters. This combination process can be formalized as: fip = A(Q, Kt, Vt) + λA(Q, Ki, Vi), (3) where λ is weight balancing the contributions from textbased and image-based attention. In this context, is derived from the feature map of the latent representation, Kt and Vt are projections from the text embeddings ft, and Ki and Vi are projections from the image feature map fi. While the IP-Adapter provides straightforward and effective method for adapting models to perform style transfer, it has notable limitations. Specifically, the additive fusion of text and image conditions can lead to overfitting to the image style, particularly when there is conflicting information between the text and image conditions. In such cases, the model may overly prioritize the image style at the expense of the text prompt, resulting in outputs that are not aligned with the intended style or semantic content of the text. Furthermore, simply reducing the weight of the image condition can weaken the visual style, leading to suboptimal results. This introduces challenge in determining an appropriate hyperparameter for balancing the contributions of both conditions. In this paper, we introduce new fusion strategy specifically tailored to style transfer tasks. Our method avoids the limitations of additive fusion by providing more efficient and stable approach for combining text and image conditioning. 3.2. Cross-Modal AdaIN of Text and Style Conditioning We propose novel method for text-driven style transfer that better integrates both text and image conditioning. Our approach aims to achieve balanced fusion of these two conditioning inputs, ensuring that they complement each other effectively. In typical text-driven style transfer tasks, text conditioning primarily serves to define the content, dictating the semantic structure of the output, while image conditioning predominantly encodes the stylistic features, such as texture and color palette. However, directly combining these two conditioning inputs through weighted summation, as in existing methods [33, 37], forces them to assume similar roles in the fusion process. This can create conflicts, particularly when the text and image provide divergent information about the content and style. For example, text prompt may describe scene in one way, while the image may impose stylistic choice that conflicts with this description, leading to suboptimal results. To address this challenge, we revisit Adaptive Instance Normalization (AdaIN) [12], widely recognized technique in style transfer. AdaIN operates by normalizing the content input based on the statistical properties (mean and standard deviation) of the style input y, integrating style characteristics while preserving the essential structure of the content. The AdaIN process is defined as follows: AdaIN(x, y) = σ(y) (cid:19) (cid:18) µ(x) σ(x) + µ(y), (4) where µ(x) and σ(x) denote the mean and standard deviation of the content input x, and µ(y) and σ(y) represent the mean and standard deviation of the style input y. By adjusting the content features to reflect the style statistics, AdaIN effectively fuses style into the content in controlled manner, preserving the contents alignment with the text description while ensuring stylistic consistency. As shown in Fig. 5, building on AdaIN, we develop cross-modal AdaIN mechanism that integrates text and style conditioning in way that respects their distinct roles. The Adapter-Based methods [33, 36] employed weighted sum approach for feature fusion, ensuring that feature maps operated within the same embedding space. We discovered that AdaIN could directly replace the weighted sum strategy. This substitution enables effective feature fusion Figure 4. Visualization of the Cross-Attention Map for the word apple in the prompt red apple during the generation process. When artifacts appear, the attention tends to scatter as well. without the need for additional training, making it particularly advantageous when integrated with Adapter-Based methods [33, 36]. Specifically, we first leverage the crossattention layers within the U-Net architecture to query both the style and text features based on the U-Net feature map. This step results in two separate grid feature mapsone for the style and one for the textthat share the same spatial resolution. Next, we apply AdaIN between two feature maps, where the feature maps queried by text conditions are normalized by style feature maps. Finally, the normalized text feature maps are fused into the raw U-Net features by simple addition. This fusion is constructed in residual design, similar to the approach used in the raw cross-attention layers. Mathematically, this adaptive fusion process can be expressed as follows: fstyle = A(Q, Kstyle, Vstyle), ftext = A(Q, Ktext, Vtext), ˆfaf = γstyle (cid:18) ftext µtext σtext (cid:19) + βstyle, (5) where the γstyle and βstyle come from the style image feature map, in the same way of run AdaIN. By adaptively balancing the influence of text and style, our method effectively minimizes potential conflicts between the two inputs, eliminating the need for setting tricky hyperparameter. 3.3. Layout Stablization with Teacher Model In image generation, the layout is crucial component of visual aesthetics. As shown in Fig. 3, we observe instances of artifacts during generation, such as checkerboard patterns. Upon analyzing the data presented in Fig. 4, we observed that these instabilities are correlated with lack of aggregation in the core generative regions within the CrossAttention mechanism. In the unstable generation examples, the layout instability reveal that the model fails to properly attend to regions associated with the word apple leading Figure 5. The illustration of our proposed cross-modal AdaIN. Figure 6. Pipeline for Layout Stabilization in Text-Driven Style Transfer Using Teacher Model. During the denoising generation process, the Teacher Model (e.g., SDXL [22]) ensures layout stability, guiding the style transfer model to preserve the structure while applying the desired stylistic transformation. to visual distortions and compositional issues. This behavior diverges significantly from what is observed in the raw SDXL [22] model at different timesteps. In image generation, Self-Attention plays crucial role in maintaining the layout and spatial structure of the original content [17]. Self-Attention mechanism in Stable Diffusion [22, 26] captures high-level spatial relationships, which effectively stabilize the foundational layout during generation. The preserved layout information, encapsulated within the Self-Attention AttnMaps, serves as structural framework that guides the composition and distribution of elements across the image. In the context of Text-Driven Style Transfer, maintaining stable layout is crucial for ensuring that the generated image accurately reflects the structure and composition described by the textual prompt. Specifically, the layout refers to the spatial arrangement of objects, elements, and scene composition in the image. Without stable layout alignment, these elements may become misaligned, resulting in images where the content is distorted, or the intended focus, perspective, and balance outlined in the prompt are lost. To address this, we propose method that stabilizes the layout by selectively replacing certain Self-Attention AttnMaps [22, 26, 32] in the stylized image with those from the original diffusion model. This selective replacement helps preserve the spatial relationships and arrangement of key features in the image, ensuring that the core layout remains consistent throughout the denoising process. By doing so, we retain the structural coherence of the original image while still applying the desired stylistic transformation, leading to more coherent and faithful alignment with the textual prompt. The following figure, Fig. 6, provides visual overview of this process, showcasing the integration of layout stabilization within the generative framework. Unlike conventional image editing approaches that replace Self-Attention AttnMaps across all timesteps, our method selectively replaces AttnMaps only during the initial denoising stages. This choice is based on the observation that core layout features stabilize in the early denoising steps, while later stages primarily refine stylistic details. Applying full-timestep replacement would risk excessive loss of stylistic detail, as structural information would override key stylistic attributes, limiting the effectiveness of style transfer. Algorithm 1 SDXL-Guided Self-Attention Replacement Input: Pdst: target prompt; Iref: style reference image; S: random seed; DM: raw Stable Diffusion Model; ST: style transfer Method Model; tcutoff: stop replacement time step; Output: Istyle: text-driven stylized image; 1: zT (0, 1), unit Gaussian random value sampled with random seed S; zT ; 2: 3: for = T, 1, . . . , 1 do if > tcutoff then 4: zt1, Mself DM(zt, Pdst, t); 5: , Iref, Pdst, t){M t1 ST(z 6: else 7: t1 ST(z 8: end if 9: 10: end for 11: Return Ires Decoder(z0); , Iref, Pdst, t) self Mself}; 3.4. Style-Based CFG particularly challenging scenario in style transfer arises when the reference style image contains multiple stylistic elements, such as combination of cartoon style and nighttime aesthetics. In such cases, the model faces style ambiguity, where various style features are present, but the focus is intended to be on just one specific element. Current methods struggle to effectively disentangle these different styles and selectively emphasize the desired one. To address this challenge, flexible method is required that can selectively emphasize the desired style elements while filtering out irrelevant or conflicting features. Inspired by the concept of classifier-free guidance (CFG) [11], commonly used in diffusion models for text-guided image generation, we propose Style-Based CFG design to provide controlled adjustments in the style transfer process. Classifier-Free Guidance Mechanism. In standard CFG, the model generates outputs conditioned on given text prompt, as well as outputs generated without any conditioning (i.e., the unconditional model output). The final output is weighted combination of these two, where the conditional output steers the generation process in alignment with the prompt, while the unconditional output acts as form of negative conditioning, helping to prevent the model from producing undesirable features. The unconditional output in the Style-Based CFG can be replaced with conditioning on negative prompt, such as blur or artifact, to actively discourage the generation of undesirable features. Therefore, the CFG [11] mechanism can be formalized as: ˆϵθ(zt, t, y) = (1 + w) ϵθ(zt, t, ycond) ϵθ(zt, t, yneg), (6) where ycond is the positive condition and yneg is the negative condition, and is weight controlling the balance between these outputs. Style-Based Classifier-Free Guidance. In the context of style transfer, we extend this CFG mechanism to address the challenge of style ambiguity in images. Specifically, we introduce the concept of negative style image that retains the overall content of the reference image but excludes the target style element. This negative image serves as counterpart to the original style image and helps the model focus on transferring only the desired style component. To generate the negative style image, we utilize layout-controlled generation model, such as ControlNet, which allows us to create an image zneg that preserves the structural features of the reference image but omits the target style. This negative image acts similarly to negative prompt in text-guided CFG, effectively guiding the model towards emphasizing the desired style while avoiding the generation of undesired style elements. The SCFG mechanism operates as follows: 1) Generate Negative Image: Using layout-controlled model, create negative sample image zneg that preserves the images structural elements but omits the target style. 2) Apply SCFG to Guide Generation: Formulate the StyleBased CFG by adjusting the balance between the target style image zstyle . This balance is controlled by weight factor w, which determines the influence of each image in the generation process. The noise prediction is modified as follows: and the negative style image zneg t ˆϵθ(zt, t, y) = (1 + w) ϵθ(zt, ycond, ys cond) ϵθ(zt, yneg, ys neg), (7) By integrating SCFG, our method enhances the generation process by isolating specific style components, filtering out extraneous, and thus focusing style transfer precisely on the desired feature. This approach mitigates the risk of overfitting to irrelevant style components, allowing the model to perform effective style transfer in complex scenarios involving multiple stylistic elements. 6 Figure 7. Qualitative comparison with state-of-the-art methods. Our approach effectively preserves image style while accurately adhering to text prompts for generation. can significantly impact the final results [10]. Additionally, we set the random seed to 42 for reproducibility, used 50 inference steps, and applied uniform guidance scale of 5 across all methods. In the qualitative and quantitative comparison experiments, for the implementation involving the Teacher Model, its participation was specifically limited to the first 20 steps. All experiments were conducted on single NVIDIA GTX-4090 GPU. Evaluation. To enable comparison with existing methods, we evaluated both our approach and several previous ones, including CSGO [36], InstantStyle [33], IP-Adapter [37] with weight tuning, StyleCrafter [18], StyleAlign [35], DEADiff [23], StyleShot [9]. To evaluate the performance of these methods in terms of prompt adherence after text-driven style transfer, we constructed benchmark consisting of 52 prompts and 20 style reference images. These prompts were selected from the settings used in StyleAdapter [35],while the style images are taken from StyleShot [9]. Additionally, using ChatGPT, we generated 30 prompts in the form of <color> <object> which better highlights the issue of style overfitting. More results of our method can be found in Appendix E, and its integration with other methods is detailed in Appendix F. 4.1. Comparison with State-of-the-Arts Qualitative Comparisons. Fig. 7 illustrates the comparison results with the state-of-the-art methods. Our method Figure 8. Impact of Style-Based CFG. From left to right, the prompts are: car glides along street lined with trees shedding their colorful, orange and red leaves, car drives down sunny street, cat, and blue car. The proposed Style-Based CFG successfully eliminates unintended style elements such as snow and golden leaves, while text-based CFG fails to address these spurious attributes. 4. Evaluation and Experiments Implementation details. We have implemented our method on top of the latest Adapter-Based Style Transfer approach, named CSGO [36]. To ensure fairness in comparison and mitigate the influence of random initialization, we fixed the initial noise for all methods, as the initial noise 7 Metric Text Alignment infer Time (s) User-study Text % User-study Style % IP-Adapter [37] 0.221 6 7.48 6.63 InstantStyle [33] 0.229 6 6.46 8.67 SDXL-based Methods CSGO [36] 0.216 9 7.99 6.97 StyleAlign [35] 0.180 48 5.78 7.82 StyleCrafter [18] 0.189 4 3.06 8.67 SD15-based Methods StyleShot [9] 0.202 3 2.55 5.10 DEADiff [23] 0.229 2 1.87 5.27 Ours 0.235 17 62.92 50.85 Table 1. Quantitative comparison with state-of-the-art methods. Our approach achieves the best performance on the text alignment metric and outperforms others in the user study evaluation. Cross-Modal AdaIN Teacher Model Text Alignment 0.216 0.223 (+3.2%) 0.228 (+5.5%) 0.235 (+8.7%) Table 2. Ablation study evaluating the impact of our proposed methods. Both designs significantly enhance text alignment accuracy. effectively addresses the issue of style overfitting, which is greater risk for methods like CSGO [36], styleshot [9] that are specifically trained on style data. While methods such as CSGO [36], InstantStyle [33], and DEADiff [23] excel in mitigating content leakage, they still struggle with prompt alignment, often failing to fully capture the intended subject matter or details specified in the prompts. On the other hand, StyleShot [9] and StyleCrafter [18] can suffer from content leakage, leading to generated images that do not accurately reflect the input prompts. Our method prioritizes text alignment by accurately capturing and reflecting the key stylistic attributes specified in the prompt, such as color. Additionally, it ensures stable layout in the generated images, maintaining structural integrity while achieving high similarity to the target styles. More detailed results and explanations could be found in Appendix D. Quantitative Comparisons. To verify the alignment between the generated image and its specified object, we compute the CLIP cosine similarity [24] between the image and the corresponding text description. As shown in Tab. 1, Our method outperforms the others, achieving the highest text alignment capability. User Study. We also conducted user study to assess users evaluations of text alignment and style similarity, with the results presented in Tab. 1. For each methods generated images, 49 users participated in an anonymous vote, selecting the example they felt was the most aligned with the text description and the closest in style to the reference image. The normalized votes (vote rate) serve as the scores for text alignment and style similarity. 4.2. Style-Based CFG We conducted experiments on Style-Based CFG (SCFG), with the results shown in Fig. 8. In the baseline images, unintended style elements such as snow and golden leaves are present, which do not align with the intended style. Adding negative text prompt allowed for some control over these elements; however, unintended style elements like snow Figure 9. Visualization of cross-attention maps for the word apple in the prompt red apple across different models. The proposed teacher model effectively rectifies attention maps, leading to improved image generation quality. Figure 10. Impact of Teacher Model on Style Image Generation. The term timestep refers to the number of denoising steps during which the teacher model is involved. were not effectively mitigated. By applying negative style image, we achieved more effective control, successfully removing these unwanted elements. This demonstrates the effectiveness of SCFG in precisely managing unintended style elements in generated images. 4.3. Ablation Study Cross-Modal AdaIN. To demonstrate the effectiveness of each component in our method, we conducted an ablation study focusing on quantitative analysis of text alignment. The same dataset used in the quantitative evaluation was employed for this test. As shown in Tab. 2, our method consistently improves text alignment accuracy over the baseline. Incorporating the Teacher Model alone provides an initial enhancement, while cross-modal AdaIN yields more significant improvement. Finally, combining both the cross-modal AdaIN and the Teacher Model results in the highest level of improvement, indicating complementary effects that enhance alignment performance. Teacher Model. In Fig. 9, shows visualization of crossattention maps for the word apple in the prompt red apple across different model configurations. The first row represents the baseline model, where the attention map fails to effectively focus on the core concept of apple from the text prompt. In the second row, the addition of cross-modal AdaIN provides some improvement, but the attention is still 8 diffuse and lacks precise focus. In the third row, with the Teacher Model added, the attention map becomes more concentrated on the target concept, demonstrating clear improvement in alignment with the prompt. This comparison indicates that it is the Teacher Model that enables the model to better capture and emphasize key elements from the text prompt, resulting in improved attention quality and image generation accuracy. In Fig. 10, this experiment demonstrates the importance of selecting an appropriate denoising timestep to stop the involvement of the Teacher Model. If the Teacher Models influence is removed too early, as seen at lower timesteps, issues with layout stability persist, resulting in compositions that lack coherence and include multiple instances of the target object (apple). However, if the Teacher Model is involved throughout the full denoising process (e.g., at timestep 50), there is noticeable loss of style in the generated image. This suggests that while the Teacher Model is crucial for achieving layout stability, an extended involvement can dilute the style elements, making it essential to find balanced point to discontinue its influence for optimal results. More detailed results and explanations could be found in Appendix C. 5. Conclusion In conclusion, existing text-driven style transfer faces key issues such as style overfitting and layout instability, which limit the adaptability and coherence of generated images. To address these challenges, we proposed three cross-modal AdaIN for harmonizing style methods: style-based classifier-free guidance and text (SCFG) for selective control of stylistic elements, and Teacher Model Experto enhance layout stability. results confirm that our approach effectively imental mitigates these issues, leading to improved alignment, stability, and control in style transfer, making it versatile and robust solution for text-to-image synthesis tasks. features,"
        },
        {
            "title": "References",
            "content": "[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 2 [2] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting largescale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87958805, 2024. 3 [3] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 2 [4] Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, and Chunhua Shen. Freecustom: Tuningfree customized image generation for multi-concept composition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90899098, 2024. 3 [5] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 2 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, [7] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit style-content separation using b-lora. In European Conference on Computer Vision, pages 181198. Springer, 2025. 3 [8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [9] Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, and Cairong Zhao. Styleshot: snapshot on any style. arXiv preprint arXiv:2407.01414, 2024. 3, 7, 8, 11, 12 [10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3, 7 [11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [12] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. 4 [13] Jaeseok Jeong, Mingi Kwon, and Youngjung Uh. Trainingfree content injection using h-space in diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 51515161, 2024. 3 [14] Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, and Philip Teare. An image is worth multiple words: Learning object level concepts using multi-concept prompt learning. arXiv preprint arXiv:2310.12274, 2023. [15] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [16] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have semantic latent space. arXiv preprint arXiv:2210.10960, 2022. 3 [17] Bingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, and Jun Huang. Towards understanding cross and self-attention in stable diffusion for text-guided image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78177826, 2024. 3, 5 9 [29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [30] Xue Song, Jiequan Cui, Hanwang Zhang, Jingjing Chen, Richang Hong, and Yu-Gang Jiang. Doubly abductive counterfactual inference for text-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 91629171, 2024. 3 [31] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. 3 [32] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3, 5, 12 [33] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. 3, 4, 5, 7, 8, 11, 12, 14, 16 [34] Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang, and Ping Luo. Styleadapter: single-pass lora-free model for stylized image generation. arXiv preprint arXiv:2309.01770, 2023. 11 [35] Zongze Wu, Yotam Nitzan, Eli Shechtman, and Dani Lischinski. Stylealign: Analysis and applications of aligned stylegan models. arXiv preprint arXiv:2110.11323, 2021. 3, 7, 8, [36] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. 2, 3, 4, 5, 7, 8, 11, 12, 13, 14, 15 [37] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3, 4, 7, 8, 11, 12, 13 [38] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 11 [39] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1014610156, 2023. 3 [18] Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang, Xintao Wang, Yujiu Yang, and Ying Shan. Stylecrafter: Enhancing stylized text-to-video generation with style adapter. arXiv preprint arXiv:2312.00330, 2023. 7, 8, 11, 12, 13, 14, 16 [19] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [20] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3 [21] Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, and Seunggyu Chang. Dreammatcher: Appearance matching self-attention for semantically-consistent text-toIn Proceedings of the IEEE/CVF image personalization. Conference on Computer Vision and Pattern Recognition, pages 81008110, 2024. 3 [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 2, 3, 5, 12 [23] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86938702, 2024. 3, 7, 8, 12 [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 8 [25] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3, 5, 12 [27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 10 This Appendix provide additional details regarding the experimental setup described in the main paper and offer an extended analysis of the contributions of individual components. The content is organized as follows: Details of Experiments. This section provides additional information about the experiments discussed in the main paper, including specifics on the quantitative evaluations and the user study setup. Ablation Study. Qualitative comparisons from the ablation experiments are presented, analyzing the impact of the Teacher Model, particularly in terms of timestep selection and the choice of Attention Map. Additional Qualitative Comparisons. This section presents extensive qualitative comparisons, demonstrating that cross-modal AdaIN effectively prevents style overfitting, while the Teacher Model ensures layout stability and mitigates the occurrence of artifacts. Integration with Other Methods. This section explores how our approach can be integrated with existing methods, such as InstantStyle [33] and StyleCrafter [18], showcasing its ability to enhance their performance and adaptability. A. Implementation Details Adapter-based methods [9, 33, 36, 37] are particularly suitable for style transfer. Their fine-tuning-free nature, combined with high-quality style transfer performance, has made them widely adopted. CSGO [36] employs widely used adapter-based model structure and is the first method trained on meticulously curated dataset specifically designed for style transfer. This effectively decouples the content and style in style images, enhancing the grasp of style details such as brushstrokes and textures. Therefore, in the experimental section, we selected it as the baseline and implemented specific modifications based on it. The implementation details are as follows: We only retained the modules in CSGO [36] related to text-driven style transfer, removing irrelevant components, e.g., ControlNet [38]. This optimization reduces potential interference while lowering experimental costs, including memory usage during inference. At the same time, both the Teacher Model and cross-modal AdaIN are optional and can be used based on specific needs. For the quantitative experiments in the main paper, we incorporated both the Teacher Model and the cross-modal AdaIN module to achieve optimal text alignment. In the qualitative and quantitative comparison experiments, the Teacher Model participated for the first 20 time steps, with the total number of inference steps set to 50. Figure 11. Details of the Test Set. The prompts used in the quantitative experiments were derived from StyleAdapter [34]. Figure 12. Details of the Test Set. The style images used in the quantitative experiments were randomly sampled from the test set of StyleShot [9]. B. Evaluation Settings and User Study In the quantitative experiments presented in the main paper, the evaluation was conducted using prompts derived from StyleAdapter [34], with specific examples provided in Fig. 11. The style images were randomly sampled from the test set of StyleShot [9], with representative examples shown in Fig. 12. Ultimately, each method generated 1,000 images for the quantitative experiments. Beyond quantitative evaluations, we conducted user study to gain subjective insights into the performance of different methods. The study involved 12 pairs of reference images and prompts. For each pair, participants were asked to assess and select the method they found superior based on two criteria: text alignment and style similarity. To ensure fair assessment, participants were provided with brief explanation of the task and evaluation criteria beforehand. We collected responses from 49 participants with diverse backgrounds, including individuals with relevant expertise in text-to-image tasks. The specific design of the questionnaire, including example pairs and evaluation guidelines, is 11 shown in Fig. 13. C. Additional Ablation Study Qualitative Results of the Ablation Study. While the main paper presents quantitative analysis, qualitative comparison provides more intuitive understanding of the contributions of each component. By incrementally integrating the corresponding components, we demonstrate their individual effects. Fig. 14 showcases representative visual outcomes from our qualitative experiments. comparison between the second and third columns highlights that cross-modal AdaIN significantly improves text alignment while preserving style similarity. Furthermore, as shown in the green apple example, introducing the Teacher Model not only enhances layout stability but also resolves remaining artifacts, ensuring spatial consistency across different styles. Self-Attention map and layout stability. In the UNet of Stable Diffusion [22, 26], Cross-Attention [32] primarily aligns the prompt with the generated image, determining how textual input influences the overall style and content. Self-Attention [32], on the other hand, focuses on the internal coherence of the image, maintaining spatial relationships and structural consistency. As shown in Fig. 15, swapping the Self-Attention Map ensures layout stability and consistency across different styles of images, whereas replacing the Cross-Attention Map fails to achieve this effect, resulting in noticeable differences in the main layout under varying styles. All experiments were conducted by adding the Teacher Model to the baseline CSGO framework. To objectively evaluate the impact of the Teacher Model, cross-modal AdaIN was not used in these experiments, isolating the Teacher Models contribution to layout stability. Choice of Teacher Model participation timestep. To evaluate the impact of the Teacher Models participation timestep on the final generation results, we conducted experiments analyzing its effect. The Teacher Model is designed to ensure layout stability while avoiding artifacts, such as checkerboard patterns. To objectively evaluate the impact of the Teacher Model, cross-modal AdaIN was not used in these experiments. As shown in Fig. 16, the term timestep refers to the number of denoising steps during which the Teacher Model is active. The results demonstrate that insufficient participation (short timesteps) fails to resolve layout issues, while prolonged involvement (long timesteps) negatively affects the final style fidelity. Rows 3 and 4 illustrate that even small changes in the timestep significantly influence the results, while Rows 5 and 6 show that the optimal timestep can vary across different styles. Based on these findings, timestep between 10 and 20 strikes reasonable balance between layout stability and style preservation. Compare with image-based style transfer(I2I). Although our method utilizes the Self-Attention Map provided by the Teacher Model, this does not equate to I2I. As shown in Fig. 17, the I2I approach provided by CSGO [36] fails to preserve the color information of the content image effectively. In contrast, our method can more accurately adhere to the prompts description. To ensure fairness, the noise used in our method is identical to that used in generating the content image. D. Additional Comparisons Qualitative experiments are conducted to visually demonstrate the strengths of our method, particularly in capturing style details and ensuring alignment with the given textual descriptions. This allows for more intuitive comparison with state-of-the-art methods, showcasing the superior performance of our approach in real-world scenarios. We provided additional qualitative comparisons between our method and state-of-the-art approaches to better illustrate the strengths and weaknesses of each method. In Fig. 21, our method outperforms others in both overall style similarity and the ability to capture fine details, such as textures. Additionally, it achieves the highest accuracy in aligning with the prompt descriptions. For methods based on the Stable Diffusion XL [22], approaches like CSGO [36] and InstantStyle [33] exhibit noticeable style overfitting, while IP-Adapter [37] and StyleCrafter [18] tend to suffer from content leakage. Meanwhile, StyleAlign [35] produces results of relatively lower quality. For methods based on the Stable Diffusion 1.5, DEADiff [23] struggles with accurately capturing the style, and although StyleShot [9] performs reasonably well in capturing style, it still encounters issues such as content leakage. Content leakage can indeed be seen as form of overfitting to the style reference, where the model overly relies on the style image, causing elements of the style reference to dominate or intrude on the content representation. This highlights lack of proper disentanglement between style and content in such cases. more nuanced form of style overfitting, as discussed in this paper, arises when text-driven style transfer methods struggle to adapt to nuanced variations in prompt details, such as changes in color. The challenge lies in whether these methods can accurately align with the evolving prompt descriptions while preserving the integrity of the style. This aspect is further validated in Fig. 22. Methods such as CSGO [36], InstantStyle [33], and StyleShot [9] struggle to differentiate the color specifications described in the prompt. Additionally, IP-Adapter [37] and DEADiff [23] face challenges with style dissimilarity, while StyleCrafter [18] demonstrates some bias toward the structure of the style reference, particularly evident in the car example. 12 Figure 13. The questionnaire format for the user study. Each option represents the generation result of method under given style and prompt. Figure 14. Qualitative results of the ablation study. cross-modal AdaIN enhances text alignment while preserving style similarity, addressing style overfitting issues. Incorporating the Teacher Model improves layout stability and resolves artifacts, ensuring consistent layout arrangements across different styles, as demonstrated in the green apple example. In the main paper, we also focus on the issue of layout stability. Through extensive experiments, as shown in Fig. 23, we demonstrate that our method can effectively ensure layout stability. CSGO [36] frequently exhibits artifacts such as checkerboard patterns, while other methods also encounter issues with layout instability. Notably, content leakage appears to be closely related to layout disruptions. This can be validated from the experimental results of StyleCrafter [18] and IP-Adapter [37]. Although red apple is reflected in the final generated output, the image contains too many unrelated elements from the style reference, making it appear overly cluttered. E. More results from our study In Fig. 24, we provide additional visualization results showcasing the effectiveness and versatility of our method. We have selected variety of style categories and different color schemes to highlight the alignment effects for text descriptions. Moreover, we achieve excellent layout stability 13 Figure 15. Implementation of the Teacher Model: Comparison of substituting the Self-Attention Map and Cross-Attention Map. The results demonstrate that replacing the Self-Attention Map achieves layout stability and consistency across different styles of images. in Fig. 19. Upon observation, we reached similar conclusion: if the Teacher Model participates for too many timesteps, it can lead to style loss. F.2. Integration with StyleCrafter [18] Teacher Model. notable issue in StyleCrafter [18] is content leakage, where unrelated content elements from the style image appear in the generated results, ultimately affecting the final output. This phenomenon can lead to generated images that do not align with the descriptions in the prompt. To address this, we incorporated the Teacher Model into the method. As shown in Fig. 20, the inclusion of the Teacher Model significantly mitigates the problem of content leakage, resulting in outputs that maintain stability and consistency across different styles. even when using the same prompt. F. Integration with Other Methods CSGO [36] has been recognized as one of the most effective and state-of-the-art methods for style transfer, which is why it was selected as the primary baseline in the main paper. To further evaluate the generalizability and robustness of our approach, we additionally explored its application and performance on other models. F.1. Integration with InstantStyle [33] Cross-Modal AdaIN. Since InstantStyle [33] is also an adapter-based architecture, it can similarly integrate crossmodal AdaIN to mitigate style overfitting. The results are shown in Fig. 18. Compared to Row 1, Row 2 accurately follows the text description, effectively avoiding errors in the generated output. Teacher Model. InstantStyle [33] also encounters artifacts such as checkerboard patterns. Similar to the previous approach, we investigated the impact of the Teacher Models involvement at different timesteps on the results, as shown 14 Figure 16. Impact of Teacher Model on Style Image Generation. The term timestep refers to the number of denoising steps during which the Teacher Model is involved. Notably, these experiments were conducted without incorporating cross-modal AdaIN to isolate and evaluate the specific impact of the Teacher Model on the generated results. Figure 17. Compared to the image-based style transfer(I2I) provided by CSGO [36], We ensured the use of the same initial noise for both our method and the generation of the content image for I2I. It can be observed that the results obtained using the Teacher Model differ significantly from those of I2I, as I2I fails to preserve the color information of the original image. 15 Figure 18. Qualitative results of using cross-modal AdaIN in InstantStyle [33]. The results demonstrate that cross-modal AdaIN effectively prevents style overfitting. The final generated results consistently align with the textual descriptions. Figure 19. Impact of Teacher Model on InstantStyle [33] Image Generation. The term timestep refers to the number of denoising steps during which the Teacher Model is involved. Notably, these experiments were conducted without incorporating cross-modal AdaIN to isolate and evaluate the specific impact of the Teacher Model on the generated results. When the Teacher Model is applied to InstantStyle [33], it helps prevent the generation of artifacts, such as checkerboard patterns. Figure 20. Impact of Teacher Model on StyleCrafter [18] Image Generation. The term timestep refers to the number of denoising steps during which the Teacher Model is involved. Notably, these experiments were conducted without incorporating cross-modal AdaIN to isolate and evaluate the specific impact of the Teacher Model on the generated results. In addition to ensuring layout stability, the Teacher Model also effectively reduces the occurrence of content leakage when applied to StyleCrafter [18]. 16 Figure 21. Qualitative comparison with state-of-the-art methods. Our approach effectively preserves image style while accurately adhering to text prompts for generation. 17 Figure 22. Qualitative comparison with state-of-the-art methods. Our approach effectively preserves image style while accurately adhering to text prompts for generation. Figure 23. Qualitative comparison with state-of-the-art methods. Our approach effectively maintain layout consistency across different styles under the same prompt. 19 Figure 24. More results of our text-driven style transfer model. Given style reference image, our method effectively reduces style overfitting, generating images that faithfully align with the text prompt while maintaining consistent layout structure across varying styles. Illustration of the prompt format used: [color] bus."
        }
    ],
    "affiliations": [
        "AGI Lab, Westlake University",
        "Fudan University",
        "Nanyang Technological University",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}