{
    "paper_title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS",
    "authors": [
        "Chuanyu Fu",
        "Yuqi Zhang",
        "Kunbin Yao",
        "Guanying Chen",
        "Yuan Xiong",
        "Chuan Huang",
        "Shuguang Cui",
        "Xiaochun Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 1 5 7 2 0 . 6 0 5 2 : r RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS Chuanyu Fu1 Yuqi Zhang2,3 Kunbin Yao1 Guanying Chen1* Shuguang Cui3,2 Xiaochun Cao1 Yuan Xiong1 Chuan Huang3,2 1Sun Yat-sen University 2FNii-Shenzhen 3SSE, CUHKSZ Sampled Inputs Test View GT Ours SpotLessSplats WildGaussians Figure 1. We propose robust solution, RobustSplat, to handle 3DGS optimization in in-the-wild scenes. Compared with existing approaches, our method significantly reduces artifacts and delivers superior performance, yielding cleaner and more reliable results."
        },
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novelview synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, robust solution based on two critical designs. First, we introduce delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our *Corresponding author. method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/. 1. Introduction Significant advancements have been made recently in novel-view synthesis and 3D reconstruction from multiview images [25, 38, 46]. Among these, 3D Gaussian Splatting (3DGS) stands out as an effective approach, enabling real-time and realistic rendering [16]. The optimization of 3DGS starts from sparse set of points obtained through Structure-from-Motion (SfM), and adaptively controls the number and density of Gaussians to create an accurate 3D representation. To capture fine details, the Gaussians will be split or cloned when the accumulated gradient magnitude of their centered position exceeds predefined thresholds, However, existing methods often assume static scene conditions, an assumption frequently violated in real-world scenarios containing transient objects. This mismatch breaks the multi-view consistency requirement, leading to severe artifacts and degraded reconstruction quality [18]. Challenges The key challenge lies in accurately detecting and filtering motion-affected regions across differ1 (a) Comparison on Indoor Scene Corner (b) Comparison on Outdoor Scene Patio Figure 2. Analysis of Gaussian densification in transient object fitting. As training progresses, vanilla 3DGS [16] suffers from performance degradation and exhibits artifacts due to the increasing number of Gaussians. Disabling Gaussian densification notably improves the results, even achieving performance comparable to the recent robust method SpotLessSplats [34]. Despite producing transient-free rendering, 3DGS w/o densification struggles to recover fine details in regions with sparse Gaussian initialization (highlighted by red arrows). ent images. Existing approaches primarily follow three paradigms: (1) category-specific semantic masking (e.g., humans and vehicles), which struggles to generalize to diverse transient objects; (2) uncertainty-based masking by considering uncertainty in minimizing photometric reconstruction loss, but often fails to reliably predict motion masks [24]; and (3) learning-based motion masking, where an MLP predicts motion masks using image features (e.g., DINO features [28]) as input and is supervised by photometric residuals [34] or feature similarity [12, 18] between captured and rendered images. While learning-based methods have shown strong performance in transient-free 3DGS optimization, they face critical limitations. In the early stages of training, the 3DGS representation is under-optimized, resulting in over-smooth renderings with large photometric residuals and weak feature similarity in both dynamic and static regions. Using these unreliable signals as supervision for mask estimation leads to inaccurate transient masks, with small masks failing to remove transients and causing artifacts, while overly smooth early reconstructions misclassify static regions, hindering optimization and resulting in under-reconstruction, as shown in Fig. 1. Analysis To mitigate these issue, two critical aspects need to be considered. First, the optimization of 3DGS should be explicitly constrained from overfitting to transient regions without accurate transient mask the during initial optimization phases. Second, the mask supervision in early iterations should be designed to be more tolerant to underreconstructed regions in the early optimization to allow reconstruction of static regions. Through detailed analysis of the 3DGS method, we identify that the Gaussian densification process (which, by default, begins after 500 iterations) enhances scene detail capture but unintentionally introduces artifacts (see Fig. 2). Initially, 3DGS fits the static parts of the scene well; however, as densification progresses, it tends to overfit dynamic regions, resulting in artifacts in areas influenced by moving objects. Surprisingly, we find that explicitly disabling the densification process in vanilla 3DGS effectively mitigates these artifacts, yielding results comparable to SpotLessSplats [34] without requiring any specialized design. This is because, without densification, the image reconstruction loss provides limited positional gradients for 3D Gaussians, primarily optimizing their shape and color instead. As result, the initially placed Gaussians remain stable in position, reducing the risk of overfitting to transient elements. However, the absence of densification leads to an insufficient number of Gaussians to represent fine details, causing the rendered images to appear overly smooth in regions with sparse point initializations. The Proposed Solution Building on our analysis, we propose simple yet effective method, called RobustSplat, for optimizing 3DGS in in-the-wild scenes. Our method introduces two key designs. First, we propose delayed Gaussian growth strategy that prioritizes reconstructing the global structure of the 3D scene while explicitly avoiding premature fitting to dynamic regions. Second, to improve the mask supervision signal for under-reconstructed regions while preserving sensitivity to transient areas, we introduce scale-cascaded mask bootstrapping approach. This approach progressively increase the supervision resolution, leveraging the observations that low-resolution features capture global consistency more effectively and suppressing local noise in early optimization stages. In summary, our key contributions are: We analyze how the 3DGS densification process contributes to artifacts caused by transient objects, offering new insights for improving the optimization of distractorfree 3DGS. We propose RobustSplat, robust approach that integrates the delayed Gaussian growth strategy and scale-cascaded mask bootstrapping to effectively reduce the impact of dynamic objects during 3DGS optimization. We demonstrate that our approach outperforms state-ofthe-art methods with simple yet effective design. 2. Related Work Novel View Synthesis Neural radiance field (NeRF) [25], as representative approach for novel view synthesis, is widely recognized for its highly realistic rendering capabilities [35, 37, 38, 48]. Many follow-up NeRF-based methods have introduced numerous enhancements in terms of efficiency [5, 11, 26] and performance [2, 22, 23, 27, 41, 47, 52, 53, 56, 61], achieving highly effective results. Recently, novel explicit representation, 3D Gaussian Splatting (3DGS) [16], has sparked considerable attention for its transformative impact on novel view synthesis methods due to its real-time rendering capability [13, 21, 57, 63]. Robustness in NeRF The vanilla NeRF assumes static scene, but this assumption often fails with in-the-wild situations, where unconstrained images inevitably include lighting variations and dynamic/transient objects. NeRF-W [24] introduces an appearance embedding for exposure and transient modeling, which has been widely used [7, 54]. For distractors removal, it uses MLPs to predict uncertainty and following methods [19, 31] introduce features from large pre-trained models [4, 28] to improve robustness. Another branch, represented by RobustNeRF [33], utilizes image residuals to predict binary masks for dynamic objects, filtering them out during training [6, 29]. Moreover, D2NeRF [45] decouples dynamic scene into three fields, including static field, dynamic field, and non-static shadow field. Robustness in 3DGS Unlike NeRF, which uses continuous MLP-based implicit representation, 3DGS employs discrete explicit representation. As result, many studies [9, 18, 36, 44, 59] explore strategies that combine global information of reference images with local Gaussian features for illumination modeling. For distractors removal, transient objects are typically filtered out using masks [1, 8, 39, 40, 43, 49, 50]. To handle transient objects, WildGaussians [18] incorporate the DINO [28] features to predict uncertainty, which is then converted into mask. Robust3DGaussians [39] enhances the predicted mask by leveraging SAM [17]. SpotLessSplats [34] leverages features from Stable Diffusion [32], designing two clustering strategies for mask prediction. T-3DGS [30] introduces an unsupervised transient detector based on consistency loss and video object segmentation module to track objects in the videos. More recently, DeSplat [42] decompose the 3DGS scenes into static 3DGS and per-view transient 3DGS by only minimizing the photometric loss. HybridGS [20] instead combines 3DGS with per-view 2D image Gaussians to decouple dynamics and statics. DAS3R [51] and RoMo [12] proposed to estimate motion mask for dynamic videos by making use of the temporal consistency constraints, which cannot be directly applied to set of unordered images. Different from existing methods, we analyse the densification process of 3DGS and propose simple yet effective solution based on the delayed Gaussian growth and scale-cascaded mask bootstrapping to reliably remove the effects of trainsient objects. Optimization in Densification and Regularization There are prior works aiming to improve the densification and optimization process of 3DGS [3, 3, 10, 14, 60]. For example, several methods [55, 58, 62] have analyzed the gradient computation process and identified issues such as gradient collision or averaging, which lead to suboptimal reconstruction quality. RAIN-GS [15] investigates alternative initialization strategy for 3DGS without relying on Colmap SFM. These methods does not consider the effect of transient objects. In this work, we analyse and leverage of the behaviors of Gaussian densification in context of transient-free 3D reconstruction. 3. Method 3.1. Overview Given casually captured multi-view posed images with transient objects, our goal is to optimize clean 3D Gaussian splatting representation that enable distractor-free novelview synthesis. Our approach builds upon recent robust 3DGS methods that jointly optimize 3D representation and transient object masks during training [34]. The transient masks selectively filter dynamic regions in images, while improve scene modeling by providing more accurate supervision for mask MLP optimization. 3 DINOv2 SAM2 StableDiffusion Figure 3. Overview of the proposed RobustSplat. However, this interdependence can lead to instability in early training. On one hand, if the masks are too small, they fail to filter all transient regions, causing newly generated Gaussians to fit transient objects. This makes it difficult to remove artifacts in later stages. On the other hand, the static scene reconstruction is often overly smooth in the early stage, which will misguide the mask MLP into incorrectly classifying static regions as dynamic, hindering their reconstruction and leading to under-representation of static content. To address these challenges, we introduce two effective designs (see Fig. 3). First, we introduce delayed Gaussian growth strategy to postpones the Gaussian densification process to prevent fitting transient objects in the early stage. Second, we propose scale-cascaded mask bootstrapping approach to refine mask predictions over time, reducing the misclassification of static regions as transient and improving the optimization of static content. 3.2. 3DGS with Transient Mask Estimation 3D Gaussian Splatting We represent the scene as set of 3D Gaussians G={gi}N i=1, where each Gaussian primitive gi has learnable parameters including mean position µi, covariance matrix Σi for shape, opacity αi, and spherical harmonics coefficients shi for view dependent color [16]. For novel view synthesis, 3D Gaussians are projected to 2D and rendered by differentiable rasterization using alpha blending [64]. The final pixel color ck is computed via alpha blending: ck = (cid:88) i=1 i1 (cid:89) ci αi G2D (1 αj G2D ), (1) j=1 where ci is the color computed from spherical harmonics coefficients with the view direction. Figure 4. Visualization of DINOv2, SAM2, and SD features via PCA. The last row compares the cosine similarity maps between features of the ground-truth and rendered image. The 3DGS is optimized by minimizing the L1 loss and SSIM loss between the rendered and the captured images: = (1 λ)L1 + λLD-SSIM. (2) During optimization, adaptive density control periodically clones/prunes Gaussians based on accumulated positional gradient magnitudes. Transient Mask Estimation To deal with transient objects, following recent work [18, 34], we predict per-image transient masks Mt using an MLP conditioned on image features ft: Mt = Sigmoid(MLPmask(ft)). (3) The estimated mask is then used to apply masked photometric loss that excludes transient regions. Recent works utilize features containing strong semantic information as MLP inputs (e.g., DINOv2 [18, 28], StableDiffusion [32, 34], SAM [12, 17]). Our preliminary experiments found that StableDiffusion feature provided stronger semantic information, but it is computationally expensive to extract the feature. Despite SAM features are better at produce mask with more accurate boundary, it struggle to locate the shadow regions, which produce incomplete mask prediction, as shown in Fig. 4. We employ the DINOv2 features as input to the MLP as it maintains good balance of computational efficiency and semantic extraction ability. Optimization of Mask MLP The optimization of the MLP weight requires appropriate supervision. We adopt the image robust loss Lresidual based the image residual information introduced in [34] as one of the supervision. To better leverage deep high-dimensional feature information extracted from images, which have different properties as the image residual, we adopt feature robust loss Lcos utilizing the information of feature similarity between the rendered and captured images. Specifically, we extract DINOv2 features of the real image ft and rendered image t, and compute their cosine similarity map. Then we convert the cosine similarity map to be in the value range of [0, 1] following [18]: Mcos = max (2cos (ft, t) 1, 0) , (4) where Mcos will be 1 is the feature cosine similarity is 1, and it will be 0 if the similarity is less than 0.5. Then the feature robust loss is expressed as: Lcos = Mt Mcos . The MLP is optimized using the following loss: LMLP = λresidualLresidual + λcosLcos, (5) (6) where λresidual, λcos are the corresponding weights for image robust supervision and feature robust loss, respectively. 3.3. Delayed Gaussian Growth for Mask Learning Motivated by our observation that disabling Gaussian densification in 3DGS significantly improves the learning of low-frequency static components, we introduce delayed Gaussian growth strategy, modifying 3DGS [16] to defer Gaussian densification during optimization. Analysis of Delayed Gaussian Growth To evaluate the impact of the Gaussian densification start time in 3DGS, we vary the initial densification iteration while keeping the densification interval fixed at 10K iterations. As shown in Fig. 5 (a), delaying densification allows 3DGS to focus on reconstructing the static scene during the early training stages. However, once densification begins, newly introduced Gaussians tend to fit transient objects, leading to decline in PSNR metrics. Notably, models with earlier densification exhibit worse performance, indicating that premature densification promotes transient object fitting. These results suggest that postponing densification helps the model better capture the static components before accommodating dynamic elements. Mask Learning with Delayed Gaussian Growth To mitigate transient artifacts caused by uncontrolled Gaussian growth, we incorporate transient mask learning into the delayed densification process. As shown in Fig. 5 (b), this integration significantly improves reconstruction accuracy by leveraging mask predictions to regulate Gaussian expansion. By leveraging mask predictions to regulate Gaussian expansion, this approach effectively suppresses transient artifacts and enhances scene fidelity. In particular, variants with later densification start achieve more accurate results. These results demonstrate that transient mask learning and delayed densification work collaboratively to enhance the stability and accuracy of 3DGS optimization. (a) w/o robust mask (b) with robust mask Figure 5. Effects of start iteration of Gaussian densification with and without the transient mask learning. Mask Regularization at Early Stage The timing of applying transient mask filtering in 3DGS is critical aspect. In the initial training phase, rendered images exhibit low quality with large image residuals and poor feature similarity, leading to inaccurate mask estimation. To mitigate this, prior methods either delay mask learning until after warm-up period (e.g., 1500 iterations) [18] or employ random mask sampling strategies [34]. However, delaying mask application risks incorporating transient objects into the scene, making them harder to remove later. Thanks to the delayed strategy for Gaussian growth, our approach ensures that early-stage optimization focuses solely on static scenes. To facilitate the optimization of static regions across the entire scene, we encourage the mask MLP to initially classify all regions as static and gradually filter out transient objects. To achieve this, we introduce regularization term into the mask MLPs supervision: Lreg = e( βreg ) 1 Mt , (7) where is the iteration number of training, and the right term is 1 if = 0, and will decrease when increases. The overall loss for mask optimization is expressed as: LMLP = λresidualLresidual + λcosLcos + λregLreg, (8) where λreg is the corresponding weights for regularization. 3.4. Scaled-cascaded Mask Bootstrapping While our delayed Gaussian growth strategy effectively mitigates the influence of transient regions by focusing optimization on static areas, the under-reconstruction of static scenes remains an issue in the early stages. This problem arises due to the sparsity of the initial Gaussian points, particularly in large-scale unbounded outdoor scenes. Consequently, the rendered outputs in these regions appear overly smooth, leading to large image residuals and low feature similarity. This, in turn, causes the mask MLP to misclassify under-reconstructed static areas as dynamic. Robust Feature Similarity Computation To address this, we aim to make the supervision signal more tolerant to under-reconstructed regions in the early optimization phase. 5 Masked GT Rendering GT Feature Rendered Feature Cosine Figure 6. Effects of mask supervisions derived from different resolutions on two scenes. The first column shows input images overlapped with yellow masks predicted by the mask MLP after training with supervisions derived from the corresponding resolutions. We observe that while high-resolution features extracted from high-resolution images provide fine-grained spatial details, they suffer from limited receptive fields and increased sensitivity to local noise. In contrast, low-resolution features capture global consistency more effectively, as each patch integrates broader contextual information, inherently suppressing local noise in feature representations. As shown in Fig. 6, compared to high-resolution image, low-resolution images naturally suppress fine details, leading to smoother color residuals and features similarity. This suggests that evaluating residuals and feature similarity at lower resolution during the early stages improves robustnessallowing under-reconstructed static regions to be retained while maintaining sensitivity to transient areas. Coarse-to-fine Mask Supervision Building on this insight, we propose resolution-cascaded approach that progressively refines mask supervision by transitioning from low-resolution to high-resolution signals. This method helps the mask MLP retain more static regions in the early optimization phase. Specifically, before the start of Gaussian densification, we render images with low-resolution from 3DGS to compute low-resolution image residuals and feature consistency to supervise the mask MLP. Once densification begins, we switch to high-resolution residuals and cosine similarity between high-resolution features, ensuring finer-grained discrimination of transient and static regions. 4. Experiments Datasets We evaluate our RobustSplat on two challenging benchmark datasets: NeRF On-the-go [31] and RobustNeRF [33]. The NeRF On-the-go dataset consists of total of 12 scenes, featuring varying occlusion levels (5% 30%). Among them, six scenes are widely used, while the 6 remaining six are more complex scenarios, referred to as NeRF On-the-go II in this paper. We adopt the RobustNeRF dataset to further validate the effectiveness of the proposed method, which comprises four artificially designed indoor scenes, each incorporating various types of distractors that challenge reconstruction fidelity. Implementation Details Our codebase follows the official Gaussian Splatting (3DGS). During training, we adopt the same learning rate settings as 3DGS and set the total training iterations to 30K. The MLP consists of two linear layers, optimized with the Adam optimizer (learning rate set to 0.001). Fixed parameters are used for all experiments. The delayed iteration start is set to 10K, and the weights for MLP supervision terms are λresidual = 0.5, λcos = 0.5, and λreg = 2.0, respectively. The mask regularization coefficient is βreg = 2000. The features used by the MLP are extracted from DINOv2, with pre-trained weights from ViT-S/14 distilled. In the mask bootstrapping, the lowest spatial resolution features are extracted from images of size (224 224), while the highest spatial resolution features are derived from size (504 504). Following existing methods [34], we apply downsampling factor of 8 on the NeRF On-thego and RobustNeRF datasets (factor 4 for specific scenarios, e.g., arcdetriomphe and patio). Low-resolution residuals are further downsampled by an additional factor of 4 based on this configuration. Baselines We evaluated our RobustSplat against multiple baselines, including the vanilla 3D Gaussian Splatting [16] which we built upon, and recent robust methods including SpotLessSplat [34], WildGaussians [18], Robust3DGS [39] and T-3DGS [30]. To ensure fair comparison, we utilized the publicly available implementations of these methods and conducted evaluations using the same camera matrices across all experiments. We assessed performance through both visual comparisons and quantitative metrics, employing PSNR, SSIM, and LPIPS to measure reconstruction quality. 4.1. Evaluation on NeRF On-the-go Dataset We first evaluate our method on NeRF On-the-go dataset. We can see from Table 1 that our method achieves best results across all six scenes on the PSNR, SSIM, and LPIPS metrics, clearly demonstrating the effectiveness of our method. Figure 7 shows the qualitative comparison, in which baseline approaches exhibit noticeable artifacts. Thanks to the proposed delayed Gaussian growth and scale-cascaded mask bootstrapping design, our method successfully eliminates these artifacts and achieves superior detail (e.g., the windows in Patio-high, as well as the building in Fountain). Table 1. Quantitative comparison on NeRF On-the-go Dataset. The best results are highlighted in bold, and the second in underline. Method Mountain PSNR SSIM LPIPS Fountain PSNR SSIM LPIPS Corner PSNR SSIM LPIPS Patio PSNR SSIM LPIPS Spot PSNR SSIM LPIPS Patio-High PSNR SSIM LPIPS Low Occlusion Medium Occlusion High Occlusion 3DGS [16] SpotLessSplat [34] WildGaussians [18] Robust3DGS [39] T-3DGS [30] Ours 19.21 20.67 20.77 19.47 20.62 21.15 0.691 0.670 0.697 0.672 0.703 0.737 0.229 0.282 0.268 0.251 0.223 0.201 20.08 20.63 20.48 19.74 20.83 21.01 0.686 0.645 0.666 0.653 0.681 0.701 0.208 0.265 0.250 0.254 0.218 0. 22.65 25.47 25.21 24.41 26.14 26.42 0.835 0.858 0.865 0.869 0.890 0.897 0.162 0.155 0.136 0.118 0.114 0.104 17.04 21.43 21.17 16.63 20.96 21.63 0.713 0.803 0.804 0.729 0.819 0.827 0.232 0.171 0.168 0.209 0.154 0. 18.54 23.64 24.60 22.64 25.84 26.21 0.717 0.819 0.871 0.874 0.893 0.907 0.334 0.207 0.135 0.132 0.127 0.102 17.04 21.17 22.44 21.56 22.84 22.87 0.657 0.749 0.802 0.799 0.829 0.837 0.314 0.237 0.184 0.174 0.167 0. WildGaussians SpotLessSplats T-3DGS Ours Mean PSNR SSIM LPIPS 0.717 0.757 0.784 0.766 0.803 0.818 0.248 0.220 0.190 0.190 0.167 0.149 19.09 22.17 22.45 22.45 22.87 23.22 GT Figure 7. Qualitative results on Patio-high and Fountain from NeRF On-the-go dataset. 4.2. Evaluation on the RobustNeRF Dataset To further validate the effectiveness of our method, we conduct comparisons with baseline methods on the RobustNeRF dataset, with quantitative results shown in Table 3. Our method achieves the best performance on the average metric. Although our method performs slightly worse in PSNR and SSIM metrics on the Android scene, it remains competitive with the state-of-the-art methods. In the remaining three scenes of the RobustNeRF dataset, our approach significantly outperforms existing methods. The qualitative comparisons are presented in Fig. 8, which shows that our method achieves transient-free reconstruction with sharp details. 4.3. Ablation Study To evaluate the effectiveness of each component of our method, we built upon the 3DGS [16] and added different components to analyze the model performance. Effects of Delayed Gaussian Growth Table 2 shows that comparing with the full model, the model without delayed Gaussian growth (3DGS+Mask+MB) suffers from noticeable decrease in all average metrics, which reiterate the effectiveness of the delayed Gaussian growth strategy in preventing the 3DGS to fit transient regions during the early optimization phase. Effects of Mask Bootstrapping Table 2 shows that reTable 2. Ablation of each component in our method on NeRF On-the-go datasets. 3DGS+Mask is the model that integrate the transient mask estimation with 3DGS. We denote Mask Bootstrapping as MB, and Delayed Gaussian Growth as DG. Full Model indicates 3DGS+Mask+MB+DG. Method 3DGS [16] + Mask + DG + MB Full Model Mountain Fountain Corner Patio Spot Patio-High PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM 19.21 19.81 20.85 20.78 21.15 0.691 0.701 0.721 0.713 0.737 20.08 20.74 20.99 20.83 21.01 0.686 0.691 0.701 0.692 0.701 22.65 25.05 26.01 25.52 26. 0.835 0.877 0.896 0.885 0.897 17.04 21.23 21.49 20.88 21.63 0.713 0.820 0.827 0.817 0.827 18.54 24.75 25.61 25.25 26.21 0.717 0.903 0.906 0.900 0.907 17.04 22.19 22.74 22.11 22. 0.657 0.832 0.838 0.826 0.837 moving the proposed scale-cascaded mask bootstrapping (3DGS+Mask+DG) leads to decrease in overall performance. This drop is particularly evident in the Mountain scene, an unbounded environment with large proportion of sky regions and sparsely initialized points, which results in overly smooth reconstructions during early optimization. Our mask bootstrapping provides more robust supervision, leading to more accurate reconstructions. Effects of Mask Supervision The supervisions of our mask MLP are derived from the image residuals and the feature similarities. We conduct experiments to evaluate the contribution of each component in helping identify transient objects. As shown in Table 4, removing either supervision component leads to varying degrees of degradation in metrics, indicating that their collaboration enables the more accurate transient region estimation, as shown in Fig. 9. 7 Table 3. Quantitative results on RobustNeRF dataset [33]. The best results are highlighted in bold, and the second in underline. Method Android PSNR SSIM LPIPS Crab2 PSNR SSIM LPIPS Statue PSNR SSIM LPIPS Yoda PSNR SSIM LPIPS Mean PSNR SSIM LPIPS 3DGS [16] SpotLessSplat [34] WildGaussians [18] Robust3DGaussians [39] T-3DGS [30] Ours 23.32 24.20 24.67 24.30 24.81 24.62 0.794 0.810 0.828 0.813 0.839 0.831 0.159 0.159 0.151 0.134 0.125 0.125 31.76 33.90 30.52 32.77 32.97 34.88 0.925 0.933 0.909 0.926 0.929 0.940 0.172 0.169 0.213 0.162 0.177 0. 20.83 21.97 22.54 21.93 22.53 22.80 0.830 0.821 0.863 0.837 0.864 0.865 0.148 0.163 0.129 0.135 0.113 0.110 WildGaussians SpotLessSplats T-3DGS 28.92 34.24 30.55 30.85 32.68 35.14 Ours 0.905 0.938 0.905 0.913 0.920 0.944 0.192 0.156 0.202 0.177 0.182 0.151 26.21 28.58 27.07 27.46 28.25 29.36 0.168 0.162 0.174 0.152 0.149 0. 0.864 0.875 0.876 0.872 0.888 0.895 GT Figure 8. Qualitative comparison on Crab2 and Statue from RobustNeRF dataset Effects of Input Features for Mask Learning Our method leverages DINOv2 features for mask prediction. We further investigate utilizing different feature representations as input to the mask MLP for mask prediction. As it is computationally intractable to extract the SD features for the rendered image at each iteration, we use the DINOv2 feature robust loss for training. As shown in Table 4, the model trained with DINOv2 feature input achieves the best results. Note that our method can be seamlessly integrated with other feature representations. 5. Conclusion In this work, we introduce RobustSplat, robust framework for transient-free 3D Gaussian Splatting, effectively mitigating artifacts caused by transient objects in dynamic scenes. Built on our analysis on the relation between Gaussians densification and artifacts caused by transient objects, our approach integrates delayed Gaussian growth strategy to prioritize static scene optimization and scalecascaded mask bootstrapping method for reliable transient object suppression. Through comprehensive experiments on multiple challenging datasets, RobustSplat demonstrates superior robustness and rendering quality compared to existing methods. Table 4. Ablation of using different supervisions and input features for mask learning on NeRF On-the-go scenes. Method Spot PSNR SSIM LPIPS Patio-High PSNR SSIM LPIPS Mean PSNR SSIM LPIPS Ours w/o residual Ours w/o cosine Ours Ours w/ SAM2 Ours w/ SD Ours 25.24 24.29 26.21 25.31 25.35 26.21 0.900 0.894 0.907 0.907 0.904 0.907 0.110 0.113 0. 0.101 0.103 0.102 22.29 22.62 22.87 22.70 22.73 22.87 0.821 0.830 0.837 0.838 0.836 0.837 0.154 0.154 0. 0.145 0.145 0.146 23.77 23.46 24.54 24.01 24.04 24.54 0.861 0.862 0.872 0.873 0.870 0.872 0.132 0.134 0. 0.123 0.124 0.124 GT Image Ours Ours w/o cosine Ours w/o residual Figure 9. Visualization of mask from different supervisions. Limitations Our current approach focuses solely on transient object removal without explicitly handling illumination changes, which limits the applicability of our method in more uncontrolled environments. In future work, we aim to investigate illumination-aware solutions to model lighting changes by incorporating the characteristics of the Gaussian densification process."
        },
        {
            "title": "References",
            "content": "[1] Yanqi Bao, Jing Liao, Jing Huo, and Yang Gao. DistractorarXiv preprint free generalizable 3d gaussian splatting. arXiv:2411.17605, 2024. 3 [2] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022. 3 [3] Samuel Rota Bul`o, Lorenzo Porzi, and Peter Kontschieder. Revising densification in gaussian splatting. arXiv preprint arXiv:2404.06109, 2024. 3 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 3 [5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022. [6] Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, and Guanbin Li. Nerf-hugs: Improved neural radiance fields in non-static scenes using heuristics-guided segmentation. In CVPR, 2024. 3 [7] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, and Jue Wang. Hallucinated neural radiance fields in the wild. In CVPR, 2022. 3 [8] Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, and Dzmitry Tsishkou. Swag: Splatting in the wild images with appearance-conditioned gaussians. arXiv preprint arXiv:2403.10427, 2024. 3 [9] Francois Darmon, Lorenzo Porzi, Samuel Rota-Bul`o, and arXiv Robust gaussian splatting. Peter Kontschieder. preprint arXiv:2404.04211, 2024. [10] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with constrained number of gaussians. In ECCV, 2024. 3 [11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. 3 [12] Lily Goli, Sara Sabour, Mark Matthews, Marcus Brubaker, Dmitry Lagun, Alec Jacobson, David Fleet, Saurabh Saxena, and Andrea Tagliasacchi. Romo: Robust motion segmentation improves structure from motion. arXiv preprint arXiv:2411.18650, 2024. 2, 3, 4 [13] Antoine Guedon and Vincent Lepetit. Sugar: Surfacealigned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. In CVPR, 2024. 3 [14] Junha Hyung, Susung Hong, Sungwon Hwang, Jaeseong Lee, Jaegul Choo, and Jin-Hwa Kim. Effective rank analysis and regularization for enhanced 3d gaussian splatting. arXiv preprint arXiv:2406.11672, 2024. 3 [15] Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, and Seungryong Kim. Relaxing accurate initialization constraint for 3d gaussian splatting. arXiv preprint arXiv:2403.09413, 2024. 3 [16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. TOG, 2023. 1, 2, 3, 4, 5, 6, 7, 8 [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 3, 4 [18] Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, and Torsten Sattler. Wildgaussians: 3d gaussian splatting in the wild. arXiv preprint arXiv:2407.08447, 2024. 1, 2, 3, 4, 5, 6, 7, 8 [19] Jaewon Lee, Injae Kim, Hwan Heo, and Hyunwoo Kim. Semantic-aware occlusion filtering neural radiance fields in the wild. arXiv preprint arXiv:2303.03966, 2023. 3 [20] Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, and Jieping Ye. Hybridgs: Decoupling transients and statics with 2d and 3d gaussian splatting. arXiv preprint arXiv:2412.03844, 2024. 3 [21] Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, et al. Vastgaussian: Vast 3d gaussians for large scene reconstruction. In CVPR, 2024. 3 [22] Chongshan Lu, Fukun Yin, Xin Chen, Wen Liu, Tao Chen, Gang Yu, and Jiayuan Fan. large-scale outdoor multimodal dataset and benchmark for novel view synthesis and implicit scene reconstruction. In ICCV, 2023. [23] Roger Marı, Gabriele Facciolo, and Thibaud Ehret. Sat-nerf: Learning multi-view satellite photogrammetry with transient objects and shadow modeling using rpc cameras. In CVPR, 2022. 3 [24] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In CVPR, 2021. 2, 3 [25] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1, 3 [26] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. TOG, 2022. 3 [27] Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV, 2021. 3 [28] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 3, 4 [29] Takashi Otonari, Satoshi Ikehata, and Kiyoharu Aizawa. Entity-nerf: Detecting and removing moving entities in urban scenes. In CVPR, 2024. [30] Vadim Pryadilshchikov, Alexander Markin, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, and Evgeny Burnaev. T-3dgs: Removing transient objects for 3d scene reconstruction. arXiv preprint arXiv:2412.00155, 2024. 3, 6, 7, 8, 2 [31] Weining Ren, Zihan Zhu, Boyang Sun, Jiaqi Chen, Marc Pollefeys, and Songyou Peng. Nerf on-the-go: Exploiting In CVPR, uncertainty for distractor-free nerfs in the wild. 2024. 3, 6, 2 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3, 4 [33] Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David Fleet, and Andrea Tagliasacchi. Robustnerf: Ignoring distractors with robust losses. In CVPR, 2023. 3, 6, 8 [34] Sara Sabour, Lily Goli, George Kopanas, Mark Matthews, Dmitry Lagun, Leonidas Guibas, Alec Jacobson, David Fleet, and Andrea Tagliasacchi. IgnorarXiv preprint ing distractors in 3d gaussian splatting. arXiv:2406.20055, 2024. 2, 3, 4, 5, 6, 7, 8, 1 Spotlesssplats: [35] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable nerf via rank-residual decomposition. 2022. 3 [36] Yuzhou Tang, Dejun Xu, Yongjie Hou, Zhenzhong Wang, and Min Jiang. Nexussplats: Efficient 3d gaussian splatting in the wild. arXiv preprint arXiv:2411.14514, 2024. 3 [37] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo MartinBrualla, Tomas Simon, Jason Saragih, Matthias Nießner, et al. State of the art on neural rendering. In CGF, 2020. [38] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural rendering. arXiv preprint arXiv:2111.05849, 2021. 1, 3 [39] Paul Ungermann, Armin Ettenhofer, Matthias Nießner, and Barbara Roessle. Robust 3d gaussian splatting for novel view synthesis in presence of distractors. arXiv preprint arXiv:2408.11697, 2024. 3, 6, 7, 8, 2 [40] Haoran Wang, Nantheera Anantrasirichai, Fan Zhang, and David Bull. Uw-gs: Distractor-aware 3d gaussian splatting for enhanced underwater scene reconstruction. arXiv preprint arXiv:2410.01517, 2024. 3 [41] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In NeurIPS, 2021. 3 [42] Yihao Wang, Marcus Klasson, Matias Turkulainen, Shuzhe Wang, Juho Kannala, and Arno Solin. Desplat: Decomposed gaussian splatting for distractor-free rendering. arXiv preprint arXiv:2411.19756, 2024. 3 [43] Yukun Wang, Kunhong Li, Minglin Chen, Longguang Wang, Shunbo Zhou, Kaiwen Xue, and Yulan Guo. Distractor-free novel view synthesis via exploiting memorization effect in optimization. In ECCV, 2024. [44] Yuze Wang, Junyi Wang, and Yue Qi. We-gs: An in-the-wild efficient 3d gaussian representation for unconstrained photo collections. arXiv preprint arXiv:2406.02407, 2024. 3 [45] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. Dˆ 2nerf: Self-supervised decoupling of dynamic and static objects from monocular video. 2022. 3 [46] Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, YanPei Cao, Ling-Qi Yan, and Lin Gao. Recent advances in 3d gaussian splatting. Computational Visual Media, 2024. 1 [47] Xiuchao Wu, Jiamin Xu, Zihan Zhu, Hujun Bao, Qixing Huang, James Tompkin, and Weiwei Xu. Scalable neural indoor scene rendering. TOG, 2022. 3 [48] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. CGF, 2022. 3 [49] Congrong Xu, Justin Kerr, and Angjoo Kanazawa. Splatfacto-w: nerfstudio implementation of gaussian splatting for unconstrained photo collections. arXiv preprint arXiv:2407.12306, 2024. 3 [50] Jiacong Xu, Yiqun Mei, and Vishal Patel. Wild-gs: Realtime novel view synthesis from unconstrained photo collections. arXiv preprint arXiv:2406.10373, 2024. 3 [51] Kai Xu, Tze Ho Elden Tse, Jizong Peng, and Angela Yao. Das3r: Dynamics-aware gaussian splatting for static scene reconstruction. arXiv preprint arXiv:2412.19584, 2024. 3 [52] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. In CVPR, 2023. 3 [53] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In CVPR, 2023. 3 [54] Yifan Yang, Shuhai Zhang, Zixiong Huang, Yubing Zhang, and Mingkui Tan. Cross-ray neural radiance fields for novelIn view synthesis from unconstrained image collections. ICCV, 2023. [55] Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, and Yong Dou. Absgs: Recovering fine details in 3d gaussian splatting. In ACMMM, 2024. 3 [56] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. 2022. 3 [57] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In CVPR, 2024. 3 [58] Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient and compact surface reconstruction in unbounded scenes. arXiv preprint arXiv:2404.10772, 2024. 3 [59] Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, and Haoqian Wang. Gaussian in the wild: 3d gaussian splatting for unconstrained image collections. arXiv preprint arXiv:2403.15704, 2024. 3 [60] Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, and Eric Xing. Fregs: 3d gaussian splatting with progressive frequency regularization. In CVPR, 2024. [61] Yuqi Zhang, Guanying Chen, Jiaxing Chen, and Shuguang Cui. Aerial lifting: Neural urban semantic and building instance lifting from aerial imagery. In CVPR, 2024. 3 [62] Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, and Hengshuang Zhao. Pixel-gs: Density control with pixelarXiv preprint aware gradient for 3d gaussian splatting. arXiv:2403.15530, 2024. 3 10 [63] Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, and Yiyi Liao. Hugs: Holistic urban 3d scene understanding via gaussian splatting. In CVPR, 2024. 3 [64] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Surface splatting. In TOG, pages 371378, 2001. 4 11 RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Discussions Sparse Gaussian Initialization and Gaussian Densification The optimization of 3D Gaussian Splatting (3DGS) relies on an initial set of points obtained via Structure-from-Motion (SfM). Since SfM reconstructs sparse point clouds based on multiview consistency, transient objects that remain stationary in multiple captured images before moving can introduce noisy points into the reconstruction. As result, 3DGS may initially fit these transient regions, even before Gaussian densification takes place. As illustrated in Fig. S1, in the Patio scene from the NeRF On-the-go dataset, moving subjects remained stationary for period, leading to COLMAP reconstructing noisy points corresponding to these transient objects. As result, 3DGS initially fits to these transient regions. However, with longer optimization, our transient mask estimation progressively removes these artifacts. This observation highlights that by applying transient mask to filter dynamic regions, our method effectively mitigates the impact of noisy initialization, leading to improved reconstruction quality. Figure S1. Gaussians initialization with inaccurate COLMAP SfM point clouds may affect the early optimization stage. Illumination Variations In real-world environments, besides transient disturbances, illumination changes can introduce multi-view inconsistencies, leading to floating artifacts. Our method mainly addresses transient object interference. However, when abrupt illumination changes occur in scene, our approach fails to correctly model the actual lighting variations due to the absence of an explicit illumination model Fig. S2. promising direction for future work is to incorporate illumination modeling into our method, enabling the handling of more complex outdoor datasets. Feature Extraction for Mask Estimation In the main text, we discuss the impact of different feature types on mask learning. DINOv2 performs well due to its efficiency and the reliable consistency of features within similar object categories. However, its patch-based nature introduces inconsistencies at the edges when extended to high-resolution settings, limiting the effectiveness of our mask predictor. In this work, we slightly expand the mask by applying dilation with kernel size of 7. In the future, we will explore integrating more expressive and efficient feature extractors for mask learning. B. More Details for the Method Training Details The original 3DGS [16] resets the opacity starting from the 3000 iterations while maintaining an interval of 3000 iterations. This operation aims to eliminate the accumulation of low-opacity Gaussian primitives in regions close to the camera, which can interfere with gradient backpropagation and manifest as artifacts. However, the opacity reset is no longer suitable for our method due to the delayed Gaussian growth. Therefore, we delay the opacity reset to start from the 15000 iterations while maintaining the same interval of 3000 iterations. Robust Loss based on Image Residuals The image robust loss used in our mask predictor follows [34]: Lresidual = max ((U M) , 0) + max ((M L) , 0) , (9)"
        },
        {
            "title": "Rendering",
            "content": "Figure S2. Illumination changes in real-world scenes. where is the mask we predicted, and are upper and lower bound of the dynamic residual mask, respectively, which determined by different values of the parameter τ . In our method, the parameters are set to τu = 0.6 and τl = 0.8 for all experiments. C. Evaluation on On-the-go II Dataset The NeRF On-the-go II dataset [31] is more challenging compared to the other scenes of NeRF On-the-go, as it consists of outdoor scenes that include not only dynamic objects but also motion blur and varying lighting conditions. Since the testing images in the On-the-go II dataset contain moving objects, we manually segment and exclude these objects when computing the metrics to ensure fair evaluation. We can see from Table S1 that our method achieves nearly the best results across all six scenes, except for the second-best performance in the PSNR metric on Statue. Moreover, our method outperforms existing methods and achieves state-of-theart regarding average metrics. Figure S3, Figure S4, and Figure S5 present qualitative comparisons with existing methods on the NeRF On-the-go II dataset. Our method successfully eliminates artifacts (e.g., vehicles in the Drone) and recovers finer details (e.g., thin cables in the Train-station), further demonstrating its effectiveness in handling complex scenarios. Table S1. Quantitative comparison on NeRF On-the-go II Dataset. The best results are highlighted in bold, and the second in underline. Method Arcdetriomphe Train PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Train-station Statue Drone Mean Tree 3DGS [16] SpotLessSplats [34] WildGaussians [18] Robust3DGS [39] T-3DGS [30] Ours 25.57 28.70 24.25 26.36 28.86 29.43 0.926 0.940 0.898 0.933 0.943 0.949 21.37 20.87 21.31 18.69 21.08 21.62 0.830 0.800 0.815 0.785 0.820 0. 15.95 16.01 17.32 14.66 16.57 16.65 0.751 0.737 0.795 0.724 0.756 0.802 22.49 23.28 23.81 23.79 24.34 24.07 0.847 0.841 0.852 0.860 0.870 0.871 21.43 21.37 22.50 20.67 21.87 22.78 0.871 0.815 0.846 0.833 0.851 0. 22.44 23.00 22.77 22.73 23.14 23.57 0.846 0.834 0.832 0.868 0.870 0.868 21.54 22.21 21.99 21.15 22.63 23.02 0.845 0.828 0.840 0.834 0.852 0.868 D. Comparison of Mask Estimation Figure S6 compares the transient mask estimation results of our method with existing methods. Our method can better filter the transient objects while keeping the static regions, leading to less artifacts and sharp details in the rendering images. 2 Robust3DGaussians SpotLessSplats WildGaussians T-3DGS Ours Ground Truth Figure S3. Qualitative results on Drone in NeRF On-the-go II dataset. 3 Robust3DGaussians SpotLessSplats WildGaussians T-3DGS Ours Ground Truth Figure S4. Qualitative results on Train-station in NeRF On-the-go II dataset. 4 Robust3DGaussians SpotLessSplats WildGaussians T-3DGS Ours Ground Truth Figure S5. Qualitative results on Tree in NeRF On-the-go II dataset."
        },
        {
            "title": "SpotlessSplats",
            "content": "Robust3DGS T-3DGS"
        },
        {
            "title": "Ours",
            "content": "Figure S6. Comparison of transient mask in NeRF On-the-go dataset."
        }
    ],
    "affiliations": [
        "FNii-Shenzhen",
        "SSE, CUHKSZ",
        "Sun Yat-sen University"
    ]
}