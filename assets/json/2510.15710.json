{
    "paper_title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis",
    "authors": [
        "Junzhi Ning",
        "Wei Li",
        "Cheng Tang",
        "Jiashi Lin",
        "Chenglong Ma",
        "Chaoyang Zhang",
        "Jiyao Liu",
        "Ying Chen",
        "Shujian Gao",
        "Lihao Liu",
        "Yuandong Pu",
        "Huihui Xu",
        "Chenhui Gou",
        "Ziyan Huang",
        "Yi Xin",
        "Qi Qin",
        "Zhongying Deng",
        "Diping Song",
        "Bin Fu",
        "Guang Yang",
        "Yuanfeng Ji",
        "Tianbin Li",
        "Yanzhou Su",
        "Jin Ye",
        "Shixiang Tang",
        "Ming Hu",
        "Junjun He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical diagnostic applications require models that can process multimodal medical inputs (images, patient histories, lab results) and generate diverse outputs including both textual reports and visual content (annotations, segmentation masks, and images). Despite this need, existing medical AI systems disrupt this unified process: medical image understanding models interpret images but cannot generate visual outputs, while medical image generation models synthesize images but cannot provide textual explanations. This leads to gaps in data representation, feature integration, and task-level multimodal capabilities. To this end, we propose a multi-level framework that draws inspiration from diagnostic workflows through the Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation level, we construct UniMed-5M, a dataset comprising over 5.6M samples that reformat diverse unimodal data into multimodal pairs for foundational observation. At the knowledge level, we propose Progressive Curriculum Learning that systematically introduces medical multimodal knowledge. At the analysis level, we introduce UniMedVL, the first medical unified multimodal model for the simultaneous analysis of image understanding and generation tasks within a single architecture. UniMedVL achieves superior performance on five medical image understanding benchmarks, while matching specialized models in generation quality across eight medical imaging modalities. Crucially, our unified architecture enables bidirectional knowledge sharing: generation tasks enhance visual understanding features, demonstrating that integrating traditionally separate capabilities within a single medical framework unlocks improvements across diverse medical vision-language tasks. Code is available at https://github.com/uni-medical/UniMedVL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 0 1 7 5 1 . 0 1 5 2 : r UNIMEDVL: UNIFYING MEDICAL MULTIMODAL UNDERSTANDING AND GENERATION THROUGH OBSERVATION-KNOWLEDGE-ANALYSIS Junzhi Ning1, Wei Li1,3, Cheng Tang1,4, Jiashi Lin1, Chenglong Ma2,5, Chaoyang Zhang2, Jiyao Liu1,5, Ying Chen1, Shujian Gao1,5, Lihao Liu1, Yuandong Pu1,3, Huihui Xu1,11, Chenhui Gou7, Ziyan Huang1, Yi Xin1,2, Qi Qin1, Zhongying Deng6, Diping Song1, Bin Fu1, Guang Yang9, Yuanfeng Ji10,Tianbin Li1, Yanzhou Su8, Jin Ye1,7, Shixiang Tang1, Ming Hu1,7, Junjun He1,2 1Shanghai Artificial Intelligence Laboratory, 2Shanghai Innovation Institute, 3Shanghai Jiao Tong University, 4Shanghai Institute of Optics and Fine Mechanics, 5Fudan University, 6University of Cambridge, 7Monash University, 8Fuzhou University, 9Imperial College London, 10The University of Hong Kong, 11The Hong Kong University of Science and Technology Equal contribution. Corresponding author. Project Page: https://uni-medical.github.io/UniMedVL_Web/"
        },
        {
            "title": "ABSTRACT",
            "content": "Medical diagnostic applications require models that can process multimodal medical inputs (images, patient histories, lab results) and generate diverse outputs including both textual reports and visual content (annotations, segmentation masks, and images). Despite this need, existing medical AI systems disrupt this unified process: medical image understanding models interpret images but cannot generate visual outputs, while medical image generation models synthesize images but cannot provide textual explanations. This leads to gaps in data representation, feature integration, and task-level multimodal capabilities. To this end, we propose multi-level framework that draws inspiration from diagnostic workflows through the Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation level, we construct UniMed-5M, dataset comprising over 5.6M samples that reformat diverse unimodal data into multimodal pairs for foundational observation. At the knowledge level, we propose Progressive Curriculum Learning that systematically introduce medical multimodal knowledge. At the analysis level, we introduce UniMedVL, the first medical unified multimodal model for the simultaneous analysis of image understanding and generation tasks within single architecture. UniMedVL achieves superior performance on five medical image understanding benchmarks, while matching specialized models in generation quality across eight medical imaging modalities. Crucially, our unified architecture enables bidirectional knowledge sharing generation tasks enhance visual understanding features, demonstrating that integrating traditionally separate capabilities within single medical framework unlocks improvements across diverse medical vision-language tasks. Code is available at link."
        },
        {
            "title": "INTRODUCTION",
            "content": "Medical diagnostic processes fundamentally follow structured multi-level reasoning pipeline that is inherently multimodal in both inputs and outputs. Physicians systematically observe multimodal raw data (imaging patterns, patient histories, symptom descriptions (Huang et al., 2020; Liu et al., 2025)), 1 Figure 1: Overview of UniMedVL unified framework. Capabilities across medical image understanding and generation tasks and performance comparisons. integrate this with medical domain knowledge (medical literature, domain expertise, cross-modal associations (Khader et al., 2023)), and analyse to produce diverse diagnostic outputs, such as textual reports explaining findings, visual annotations localizing abnormalities, segmentation masks of lesion regions, and comparative imagery for treatment planning (Nguyen et al., 2023; Xu et al., 2025a; Zhang et al., 2025b; Tanida et al., 2023; Fang et al., 2024). Consider radiologist examining suspected lung pathology: they process chest X-rays (visual), prior CT scans (cross-modal comparison), and patient history (textual) to generate multiple complementary outputs: detailed reports describing findings, visual annotations highlighting specific regions, and comparative visualizations for surgical planning. This procedure exemplifies how medical diagnostic applications require unified processing of multimodal inputs to generate diverse multimodal outputs, where neither textual reports alone nor visual annotations alone suffice. While multimodal fusion has demonstrated substantial improvements in diagnostic assistance systems (Benani et al., 2025; Soenksen et al., 2022), current medical AI system remains fragmented, with state-of-the-art models achieving less than 60% accuracy compared to over 90% for human experts on diagnostic challenges (Kaczmarczyk et al., 2024).This fragmentation manifests at three critical levels: (i) Data: Medical datasets remain predominantly single-modal, despite clear evidence that multimodal integration substantially improves diagnostic accuracy (Warner et al., 2024; Huang et al., 2023; Hu et al., (ii) Features: Current approaches lack systematic progressive 2023a; 2024a; Li et al., 2025). training strategies that can effectively capture deep cross-modal relationships; most methods simply concatenate features rather than progressively building from basic pattern recognition to sophisticated multimodal tasks (Haq et al., 2025). (iii) Tasks: While general-domain models have made progress in unified architectures, the medical domain still lacks truly unified models. For instance, although HealthGPT demonstrates both understanding and generation capabilities for medical tasks, it requires reloading different model checkpoints to switch between task types, which is limitation that prevents seamless multi-task operation in real-time deployment of medical workflows (Lin et al., 2025). To bridge this gap, we propose workflow-guided framework that mirrors how physicians actually process medical information through the Observation-Knowledge-Analysis (OKA) paradigm. At the observation level, we construct UniMed-5M, dataset that, unlike existing single-modal datasets, reformats medical data of various tasks into over 5.6 million multimodal input-output compatible pairs. At the knowledge integration level, we design Progressive Curriculum Learning that goes beyond naive concatenation. Through three carefully designed stages (alignment for medical data, fusion, and synthesis), our approach materialises models to discover cross-modal patterns better. At the analysis level, we introduce UniMedVL, the first unified medical model capable of both understanding and generation within single architecture at the same time. Our experiments validate two key insights: (1) Building strong multimodal medical representations requires principled and holistic OKA framework, and it must be supported by data that are both sufficient in scale and high in 2 quality; (2) Rapid adaptation is achievable, unified model architectures demonstrate the feasibility of quickly adapting to new medical tasks and datasets for scalable multimodal medical AI. In summary, our contributions are as follows: Observation (Data-level): We construct UniMed-5M, large-scale dataset containing over 5.6M multimodal medical examples that reformat diverse unimodal datasets into uniform multimodal input-output pairs, and serve as the initial building blocks for unifying diverse medical tasks. Knowledge integration (Feature-level): We devise Progressive Curriculum Learning, threestage training paradigm that systematically builds medical multimodal capabilities: foundation training for basic pattern recognition, instruction tuning for cross-modal fusion, and unified multimodal training for advanced synthesis. Analysis (Task-level): We introduce UniMedVL, novel unified medical foundation model that provides multimodal capabilities within single architecture without needing offline checkpoints once loaded, including understanding multimodal inputs and generating textual reports, image translation, segmentation masks, and synthetic medical images."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 MEDICAL MULTIMODAL LARGE LANGUAGE MODELS Early medical MLLMs commonly paired medical vision encoder with general-domain LLM, routing visual embeddings through lightweight linear/MLP projector into the LLM token space (Hu et al., 2025; Su et al., 2025; Li et al., 2024; Chen et al., 2025b). Thawakar et al. (2024) aligned MedCLIP with Vicuna via linear projector in XrayGPT. Li et al. (2023) bootstrapped instruction data from PubMed figures using GPT-4 in LLaVA-Med. These systems proved effective for VQA and report generation but kept fusion shallow and did not provide unified, native route to medical image synthesis or editing. second line of work emphasizes data engineering (Hu et al., 2024b; Yan et al., 2025a;b). Chen et al. (2024b) leveraged GPT-4V to reformat noisy PubMed imagetext pairs into the 1.3M-sample PubMedVision corpus in HuatuoGPT-Vision. While this strategy mitigates data scarcity and label noise, it remains primarily comprehension-oriented; unified, high-fidelity generation is still outside the model proper. Zhang et al. (2023a) adopts unified seq2seq formulation for biomedical visionlanguage tasks with BioMedGPT, improving general biomedical reasoning yet without native medical image generation pathway. Singhal et al. (2025) achieves expert-level performance on medical QA via chain-of-thought prompting and improved prompting/aggregation with Med-PaLM 2, but likewise does not deliver single pipeline that natively spans both image-level generation and text reasoning. Most recently, Lin et al. (2025) introduce HealthGPT as medical MLLM explicitly targeting unified multi-modal input and output: it combines discrete visual tokens with an autoregressive paradigm and employs heterogeneous MoE-style LoRA (H-LoRA) to reduce task interference and broaden task coverage. However, its unification relies on multiple task-specific models at inference time; different capabilities are not consolidated into single model that uniformly expresses all tasks simultaneously. 2.2 UNIFIED MULTIMODAL UNDERSTANDING AND GENERATION MODELS Outside the medical domain, unified multimodal research has developed along several paradigms. Autoregressive models (Team, 2024a; Wang et al., 2024; Lu et al., 2022; 2024) unify modalities by discretizing images and performing next-token prediction in single Transformer (decoder-only or encoder), achieving architectural unity but incurring long-sequence overheads that can constrain high-resolution synthesis. Dual-encoder designs (Wu et al., 2025c; Ma et al., 2025d; Xu et al., 2025c) address the granularity conflict between semantic understanding and pixel-level generation through separate visual pathways, improving task-specific performance at increased inference cost. Hybrid objectives combine different generative paradigms: Zhou et al. (2024) jointly optimize languagemodeling and image-diffusion losses in Transfusion, while Xie et al. (2024) unify autoregressive and diffusion modeling within one transformer in SHOW-O. Modular approaches (Wu et al., 2025e; 2024a) bridge frozen MLLMs with diffusion models through learnable connectors, trading cost-effectiveness for reduced end-to-end differentiability. In parallel, large-scale unified pretraining reveals emerging properties without relying on modular connectors (Deng et al., 2025). Representation innovations target the semantics, fidelity gap through various strategies: multi-codebook quantization (Ma 3 Figure 2: Overview of the proposed ObservationKnowledge framework. Observation: Covers data sources and modality coverage, quality control pipeline, and interleaved image-text task construction for building training data across different model stages. Knowledge: Refers to the progressive curriculum training paradigm, consisting of three stages that gradually equip the model with generalized capabilities on interleaved image-text tasks. et al., 2025c), visiontext aligned discrete representations with unified vision tower (Wu et al., 2024b), unified semantic spaces aligned with CLIP (Chen et al., 2025a), and masked autoregressive tokenization for non-visual modalities such as motion (Jiang et al., 2024). Advanced autoregressive methods (Liao et al., 2025; Zhang et al., 2025a; Zhuang et al., 2025) enable high-fidelity interleaved generation through deep fusion, prefilled tokens, and reinforcement learning from human feedback. While these general-domain approaches have demonstrated strong performance on unified multimodal understanding and generation, the medical domain still lacks dedicated frameworks tailored to its specific requirements, including fine-grained anatomical localization, diagnostic-quality synthesis, and integration of clinical knowledge. Our work addresses this domain gap by introducing UniMedVL, medical-specialized unified architecture that enables both understanding and generation within single coherent framework."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "Our workflow-guided multi-level framework systematically implements the Observation-KnowledgeAnalysis (OKA) paradigm inspired by diagnostic processes through three corresponding stages: data-level observation for comprehensive multimodal dataset construction, feature-level knowledge integration through principled curriculum learning, and task-level analysis via unified model architecture. Each stage addresses specific computational challenges while maintaining medical workflow alignment. 3.1 OBSERVATION LEVEL: UNIMED-5M DATASET CONSTRUCTION At the observation level, comprehensive multimodal datasets are constructed to enable systematic processing of diverse medical inputs that mirror medical diagnostic practices. The dataset construction follows medical workflow patterns where multiple data modalities are observed and initially processed before knowledge integration. The overall dataset curation pipeline is shown in Fig. 2. Data Source and Modality Coverage. comprehensive medical dataset comprising 5.6M samples is assembled from diverse public repositories including PMC-OA (Lin et al., 2023), Quilt-1M (Ikezogwo et al., 2023), PubMedVision (Chen et al., 2024a), GMAI-VL datasets (Li et al., 2024), 4 CheXpertPlus (Chambon et al., 2024), PMC-VQA (Zhang et al., 2023c), Medical-Diff-VQA (Hu et al., 2023b), and other specialized medical datasets through systematic data synthesis and augmentation methodologies detailed in Appendix A.2. The collection encompasses nine primary imaging modalities: chest X-rays (CXR), histopathology images (HIS), CT scans, MRI sequences, color fundus photography (CFP), optical coherence tomography (OCT), endoscopy, ultrasound, and fluorescence microscopy (FM). The dataset encompasses diverse medical AI task categories spanning understanding, generation, and multimodal input-output capabilities. Quality Control Pipeline. We adopt three-step pipeline that progressively increases fidelity while controlling cost: Coarse Filtering. Images are preprocessed through modality-specific normalization and resolution filtering ( 128128 pixels). Text undergoes specialized tokenization that preserves medical terminology, followed by length filtering (161024 characters). Medical Alignment. Because medical captions often emphasize specific pathological findings rather than exhaustive descriptions, we implement dedicated verification pipeline. MedGemma27b (Sellergren et al., 2025) generates five diverse captions per image; semantic similarity is computed with E5-large-v2 embeddings (Wang et al., 2022); and medical-specific alignment is assessed using MedSigLIP (Sellergren et al., 2025). We then compute combined alignment score scorefinal = λ similarityE5 + scoreMedSigLIP with λ = 0.5, retaining the top 50% of pairs as high-quality training data. Expert Validation. Medical experts conduct comprehensive quality audits along seven evaluation dimensions (detailed in Appendix A.5). This stage serves as quality assurance rather than additional filtering, with high inter-rater agreement observed across all dimensions. Interleaved Tasks Construction. This component encompasses five tasks involving interleaved images and texts: medical image promptable segmentation, super-resolution, interpretable counterfactual generation, virtual staining, and cross-modal synthesis. We adopt two complementary construction strategies: templateization and VLLM Caption. In templateization, inputs and outputs are standardized into structured imagetext pairs, where textual prompts explicitly guide the model beyond the provided image and outputs follow templated format. In contrast, VLLM captioning emphasizes generating semantically rich textual descriptions that interpret the corresponding images in medical contexts, including anatomical descriptions and medical insights. 3.2 KNOWLEDGE LEVEL: PROGRESSIVE CURRICULUM LEARNING At the knowledge integration level, deep cross-modal knowledge fusion is achieved through principled curriculum learning paradigm that progressively builds from basic medical pattern recognition to sophisticated multimodal reasoning capabilities. Progressive Curriculum Training Paradigm: Stage 1: Foundation Training. Foundational medical domain awareness is established through unsupervised exposure to comprehensive medical datasets. The foundation training stage prioritizes broad pattern recognition over task-specific performance, enabling robust medical concept acquisition through text-image paired learning and next-token prediction across diverse medical sources. Furthermore, the training emphasizes learning general medical visual-language alignments without task-specific constraints and overly curated datasets. Stage 2: Instruction Tuning. Medical expertise is systematically developed through fine-tuning on curated high-quality instruction data. The instruction-formatted medical tasks follow the format (q, xv, k) (at, av) where query q, visual input xv, and knowledge context generate textual at and visual av responses. We implement differentiated enhancement strategies for distinct task types: For medical understanding tasks such as VQA, we augment standard responses with existing Distilled Chain of Thought (DCOT) data that explicitly articulate the reasoning pathway from visual observation to medical conclusions. For generation tasks, we employ the Caption Augmented Generation (CAG) pipeline to enhance caption quality, incorporating structured planning steps that guide the visual synthesis process. The details are provided in Appendix A.3. Stage 3: Unified Multimodal Training. Multimodal capabilities of generation and understanding are developed through sophisticated tasks requiring integrated visual-textual combination. This stage focuses on complex interleaved tasks that combine understanding and generation requirements within unified sequences. The training strategy maintains semantic stability from previous stages while enabling advanced synthesis capabilities in medical interleaved tasks. 3.3 ANALYSIS LEVEL: UNIMEDVL UNIFIED ARCHITECTURE At the analysis level, comprehensive multimodal medical outputs are generated through unified architecture that emulates medical diagnostic processes. The UniMedVL architecture integrates the progressive curriculum learning paradigm into cohesive system capable of both understanding and generation within single model backbone. Task Organization. Model training is systematically organised into three primary tasks that reflectcapabilities required for unified medical multimodal systems: (i) Understanding tasks encompassing medical image comprehension, VQA, diagnostic reasoning, image captioning, and medical report generation; (ii) Generation tasks focusing on text-to-image synthesis with conditional medical image generation and planning-guided approaches; and (iii) Interleaved tasks combining visual-textual inputs and outputs requiring seamless multimodal integration. These interleaved tasks include sophisticated capabilities such as virtual immunohistochemistry staining , cross-modal synthesis of CT and MRI modalities, counterfactual generation for treatment planning and development forecasting. Model Architecture Overview. Following Deng et al. (2025), we adopt unified architecture with dual visual encoders and mixture-of-transformer-experts (MoT). The understanding-oriented encoder EViT extracts semantic tokens zViT = EViT(xv) for multimodal comprehension tasks, while the generation-oriented encoder EVAE produces latent representations zVAE = EVAE(xv) for visual synthesis tasks. The MoT module contains specialised decoder-based experts: an understanding expert processes interleaved sequences of text and ViT tokens [xtext, zViT] for vision-language understanding, while generation expert handles VAE latent tokens [zVAE] for image generation, with text conditioning accessible through cross-attention. Projection layers fViT and fVAE bridge the visual encoders with the transformer experts, mapping encoded features to the shared hidden dimension. For generation outputs, the decoder DVAE reconstructs visual content from the latent representations back to pixel space. Both experts operate on the same token sequence through separate projection heads within each transformer layer. Training Objectives. The model is trained with unified loss function combining understanding and generation tasks. For understanding tasks, we employ next-token prediction: LNTP = (cid:88) i=1 log p(ti+1ti, zViT; θ), (1) where ti denotes the i-th text token and θ represents model parameters. For visual generation, we apply flow matching on VAE latent space: (cid:104) Lflow = Et,ϵ vθ(zt, t, c) (z1 z0)2(cid:105) , (2) where zt = (1 t)z0 + tz1 is the interpolated latent with z0 = EVAE(xv) as clean latent and z1 (0, I) as noise, vθ is the velocity prediction network parameterized by the generation expert, [0, 1] is the flow time, and denotes text conditioning. The overall training loss is: = LNTP(zViT) + α Lflow(zVAE), (3) where the coefficient α balances the contribution of generation tasks."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 BENCHMARKS AND BASELINES Evaluation Benchmarks. We evaluate UniMedVL across medical visual understanding and generation benchmarks. For image understanding tasks, we employ VQA-RAD (Lau et al., 2018), SLAKE (Liu et al., 2021), PathVQA (He et al., 2020), OmniMedVQA (Hu et al., 2024c), and GMAIMMBench (Ye et al., 2024), which cover diverse medical scenarios. For interleaved image-text tasks, we utilise the BCI dataset (Liu et al., 2022b) for the virtual immunohistochemistry staining 6 Table 1: Ablation study of the proposed progressive curriculum learning strategy. UVE refers to the understanding-oriented vision encoder. and refer to the generation and understanding subsets of UniMed-5M, respectively. CAG: Caption Augmented Generation, DCOT: Distilled Chain of Thought. Bold indicates the best performance and underlined indicates second-best performance. Model UVE LNTP Lflow Data Type Understanding Generation GMAI-MMBench SLAKE PathVQA OMVQA gFID BiomedCLIP Baseline Comparison One-Stage-Joint-Base Stage 1: Foundation Training F-Baseline C-G-only B-U-only H-Joint-Base Stage 2: Instruction Tuning C-G-only B-U-only H-Joint-Base Stage 3: Unified Multimodal Training H-Joint-Base U+G - U+G CAG DCOT High-quaity U+G 0.5354 0.6560 0.4946 0.7784 123.48 0. 0.481 - 0.505 0.593 - 0.5432 0.6004 0.589 - 0.5476 0.6843 - 0.6032 0.7418 0.390 - 0.3673 0.3649 - 0.4526 0. 0.7113 - 0.7723 0.8562 - 0.8167 0.8626 212.73 118.5991 - 121.02 108.40 - 120.036 0.662 0.6994 - 0.683 0.698 - 0. Interleaved tasks 0.6075 0.7540 0.5346 0.8584 96. 0.7058 task. The IXI dataset (IXI Consortium, 2024) is leveraged to evaluate the super-resolution task, and the BraTS 2023 dataset (Adewole et al., 2023) is used for evaluating the cross-modal synthesis task. We use the ICG-CXR dataset (Ma et al., 2025b) to evaluate the counterfactual generation task. Baseline Methods. These include two categories of methods: specialized models and unified multimodal models. For specialized models, we include medical VLMs such as Med-Flamingo (Moor et al., 2023), LLaVA-Med (Li et al., 2023), HuatuoGPT-Vision (Chen et al., 2024b), RadFM (Wu et al., 2025b), GMAI-VL (Li et al., 2024), LLaVA-v1.5 (Liu et al., 2024), and InternVL2 (Team, 2024b). We also compare with image translation models including CycleGAN (Zhu et al., 2017), pix2pix (Isola et al., 2017), pix2pixHD (Wang et al., 2018), pyramid pix2pix (Liu et al., 2022b), SRCNN (Dong et al., 2015), VDSR (Kim et al., 2016), SwinIR (Liang et al., 2021), Restormer (Zamir et al., 2022), AMIR (Yang et al., 2024), ResViT (Dalmaz et al., 2022), and TransUNet (Chen et al., 2021). Additionally, to determine the model performance of medical imaging generation capability, we include LlamaGen-MedITok (Ma et al., 2025a) as the baseline. For unified multimodal models, we include general frameworks like Janus (Wu et al., 2025d) and Bagel (Deng et al., 2025), as well as medical unified models such as HealthGPT (Lin et al., 2025). Evaluation Metrics. We employ task-specific metrics aligned with medical relevance. For medical image understanding tasks, we utilize accuracy as the evaluation metric. For open-ended questions, we employ Qwen2.5-7B as the judge model to assess response quality. For medical image generation tasks, we employ generation FID (gFID) and BiomedCLIP (Zhang et al., 2023b) score to evaluate the quality of synthesized images. For interleaved image-text tasks, we leverage PSNR and SSIM as evaluation metrics for virtual immunohistochemistry staining, super-resolution, and cross-modal synthesis tasks. For interpretable counterfactual generation, we follow the experimental setup of ProgEmu (Ma et al., 2025b), using gFID, AUC-ROC, and F1 to evaluate the quality of synthesized images, and BLEU-3, METEOR, and ROUGE-L to assess the quality of the explanatory text. 4.2 PERFORMANCE OF UNIMEDVL 4.2.1 ABLATION STUDY We first validate the effectiveness of our progressive curriculum learning strategy through comprehensive ablation studies. Table 1 and Figure 3 demonstrate how each training stage contributes to the final model capabilities. The critical finding is that joint training (H-Joint-Base) consistently outperforms single-task variants during Stage 1, indicating that UniMedVL learns fundamental unified multimodal representations to effectively perform both understanding and generation tasks. Subsequently, Stage 2 further improves performance on both tasks through instructions with reasoning processes and high-quality image captions. Finally, Stage 3 brings the most significant improvements, showing that unified multimodal representations are further refined to support both understanding and generation tasks simultaneously. 7 Figure 3: Visual Comparison of Performance across different training stages and modalities. (Left:) Stage-wise understanding accuracy performance. (Right:) Generation quality evolution with gFID reduction and BiomedCLIP score enhancement through different training stages. Table 2: Comparison of UniMedVL with other LVLMs and unified multi-modal models on medical visual understanding tasks. Bold and underlined text indicate the best performance and second-best performance, respectively. Model Params Medical VQA-RAD SLAKE PathVQA OmniMedVQA GMAI-MMBench Understanding Only LLaVA-v1.5 InternVL2 Med-Flamingo LLaVA-Med RadFM HuatuoGPT-Vision-7B GMAI-VL 7B 8B 8.3B 7B 14B 7B 7B Unified Understanding and Generation Janus Bagel HealthGPT-M3 HealthGPT-L UniMedVL (Ours) 1.3B 7B 3.8B 14B 14B 42.8 49.0 43.0 48.1 50.6 53.0 66. 52.8 60.09 55.9 58.3 61.9 37.7 50.1 25.5 44.8 34.6 49.1 72.9 26.9 58.91 56.4 64.5 75.4 31.4 31.9 31.3 35.7 14.33 32.0 39. 27.9 39.05 39.7 44.4 53.5 44.7 54.5 34.9 41.3 23.5 50.0 88.5 45.7 71.13 68.5 74.4 85.8 38.23 43.47 12.74 20.54 22.34 50.22 61. 39.30 48.11 42.08 43.1 60.75 4.2.2 MEDICAL VISUAL UNDERSTANDING PERFORMANCE Table 2 compares UniMedVL with two categories of baselines: understanding-only medical VLLMs and unified multimodal models. Among understanding-only models, GMAI-VL achieves the best results with 88.5% on OmniMedVQA, 72.9% on SLAKE, and 61.74% on GMAI-MMBench through specialized medical fine-tuning. In contrast, for unified models supporting both understanding and generation, UniMedVL achieves 75.4% on SLAKE, ranking first among all unified models and surpassing the understanding-only second-best by 2.5 points. On PathVQA, UniMedVL scores 53.5%, with 9.1-point improvement over the previous best HealthGPT-L14 at 44.4%. On OmniMedVQA, UniMedVL reaches 85.8%, trailing the specialized GMAI-VL by only 2.7 points while maintaining generation capabilities. On GMAI-MMBench, UniMedVL achieves 60.75%, nearly matching GMAIVL at 61.74%. These promising results demonstrate that UniMedVL can approach specialized medical vision-language model performance across diverse medical understanding tasks. 4.2.3 MEDICAL IMAGE GENERATION PERFORMANCE We evaluate UniMedVLs text-to-image generation capabilities across eight medical imaging modalities. Table 3 provides empirical evidence for cross-modal knowledge transfer: comparing UniMedVLGen with generation-only training against full UniMedVL reveals that understanding tasks contribute semantic constraints that enhance generation quality. Specifically, the average gFID improvement demonstrates this synergy. Furthermore, UniMedVL achieves BiomedCLIP scores of 0.706 on 8 Table 3: Performance comparison of our UniMedVL variants and other baseline models on the text-driven image generation task across different modalities. CS denotes BiomedCLIP Score. Bold and underlined text indicate the best performance and second-best performance, respectively. CFP CXR CT HIS MRI OCT Ultrasound Endoscopy Average Method FID CS FID CS FID CS FID CS FID CS FID CS FID CS FID CS FID CS LlamaGen-MedITok Bagel UniMedVL-Gen UniMedVL 89.14 217.19 77.35 53.20 - 0.650 0.699 0.708 68.16 182.80 190.38 73.04 - 0.662 0.672 0.702 - 163.78 79.84 73. - 0.652 0.694 0.696 198.63 206.18 107.20 149.01 - 0.643 0.699 0.704 - 175.74 82.99 90.36 - 0.639 0.699 0.706 - 307.80 107.06 99. - 0.719 0.721 0.721 358.11 255.78 100.44 95.38 - 0.672 0.700 0.706 - 214.61 121.89 133.11 - 0.668 0.704 0.707 171.85 215.49 108.40 96. - 0.660 0.699 0.706 average across modalities. On the top row of Figure 4, we provide qualitative visualization of generation quality across eight medical modalities. 4.2.4 INTERLEAVED MULTIMODAL TASKS PERFORMANCE Table 4: Comparison of UniMedVL with baseline methods on medical counterfactual generation. Bold and underlined texts indicate the best performance and second-best performance, respectively. Method Counterfactual Image Explanatory Text gFID AUROC F1 BLEU-3 METEOR ROUGE-L CXR-IRGen ProgEmu UniMedVL 35.39 29.21 27.17 0.5236 0.7921 0.7609 0.8914 0.0448 0. 0.7970 0.8731 0.2641 0.2115 0.4097 0.4486 0.1846 0. 0.4649 Figure 4: Comprehensive visualization of UniMedVL multimodal capabilities. Demonstration of diverse medical imaging tasks, including text-to-image generation, virtual staining, super resolution, counterfactual generation, and cross-modal synthesis. key advantage of our unified architecture is the ability to seamlessly handle interleaved multimodal tasks that require simultaneous understanding and generation capabilities. Table 5 demonstrates the performance comparison of virtual immunohistochemistry staining, super-resolution, and crossmodal synthesis tasks. Additionally, our unified model after Stage 3 training, UniMedVL, achieves competitive performance comparable to some specialized methods in those tasks. More importantly, rapid task-specific adaptation with UniMedVL on top of this Stage 3 model yields substantial improvements: For virtual immunohistochemistry staining from H&E to IHC, performance improves from 18.11 to 20.27 PSNR, outperforming HealthGPT-M3 by 28%; for MRI super-resolution with 4 upscaling, we achieve 27.29 PSNR and 0.890 SSIM; for cross-modal synthesis between T2 and FLAIR, we reach 25.07 average PSNR, approaching specialized models. Figure 4 provides qualitative comparisons of these generation tasks. These results validate our second key insight from 9 the introduction: unified model architectures demonstrate the feasibility of quickly adapting to new medical tasks. Table 5: Performance Comparison on specialized generation tasks. histological staining transformation (H&E to IHC), MRI super-resolution (4), and medical image translation (T2 FLAIR). PSNR and SSIM are used in medical image translation. indicates the model after Stage 3 training without task-specific adaptation. Bold and underlined text indicate the best performance and secondbest performance, respectively. H&EIHC Staining MRI Super-Resolution Medical Image Translation Method PSNR/SSIM Method PSNR/SSIM CycleGAN Pix2Pix Pix2PixHD Pyramid Pix2pix 16.20/0.373 18.65/0.419 19.63/0.471 21.16/0.477 SRCNN VDSR SwinIR Restormer AMIR HealthGPT-M3 UniMedVL UniMedVL 15.81/0.242 18.11/0.401 20.27/0. HealthGPT-M3 UniMedVL UniMedVL 28.81/0.892 30.04/0.914 31.55/0.933 31.85/0.938 31.99/0.939 18.37/0.580 19.64/0.602 27.29/0.890 Method ResViT pGAN pix2pix A-UNet SAGAN T2FLAIR FLAIRT2 25.78/0.908 24.97/0.870 25.09/0.894 24.01/0.864 24.52/0.883 23.15/0.869 24.56/0.891 23.69/0.873 25.10/0.893 24.02/0. HealthGPT-M3 UniMedVL UniMedVL 18.88/0.745 23.99/0.711 24.90/0.881 19.30/0.750 23.49/0.732 25.23/0.883 Avg 25.38/0.889 24.55/0.879 23.84/0.876 24.13/0.882 24.56/0.877 19.09/0.748 23.74/0.722 25.07/0. Table 4 evaluates CXR counterfactual generation capabilities with explanatory text. Our unified model after Stage 3 training, UniMedVL, achieves 27.17 gFID and significantly higher text quality metrics with 0.2641 BLEU-3, 0.4486 METEOR, and 0.4649 ROUGE-L compared to specialized baselines. Furthermore, the improved counterfactual check rate at 0.797 AUROC demonstrates that our unified training enables generation of medically plausible scenarios with coherent textual explanations in CXR medical modalities."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We presented UniMedVL, unified framework that simultaneously performs medical image understanding and generation within single model, validated through extensive experiments on over 5 million medical samples demonstrating both state-of-the-art comprehension and competitive generation quality. While our current work focuses on 2D medical imaging, the proposed OKA paradigm establishes foundations for exploring diverse medical AI tasks beyond those demonstrated, including 3D volumetric analysis, temporal reasoning, and multimodal medical AI assistance. This work represents critical step toward truly integrated medical AI systems where understanding and generation capabilities synergistically support medical workflows."
        },
        {
            "title": "REFERENCES",
            "content": "Maruf Adewole, Jeffrey Rudie, Anu Gbdamosi, Oluyemisi Toyobo, Confidence Raymond, Dong Zhang, Olubukola Omidiji, Rachel Akinola, Mohammad Abba Suwaid, Adaobi Emegoakor, et al. The brain tumor segmentation (brats) challenge 2023: Glioma segmentation in sub-saharan africa patient population (brats-africa). ArXiv, pp. arXiv2305, 2023. Ivo Baltruschat, Parvaneh Janbakhshi, and Matthias Lenga. Brasyn 2023 challenge: Missing mri synthesis and the effect of different learning objectives. In International Challenge on Cross-Modality Domain Adaptation for Medical Image Segmentation, pp. 5868. Springer, 2023. Alaedine Benani, Stéphane Ohayon, Fewa Laleye, Pierre Bauvin, Emmanuel Messas, Sylvain Bodard, and Xavier Tannier. Is multimodal better? systematic review of multimodal versus unimodal machine learning in clinical decision-making. medRxiv, pp. 202503, 2025. Black Forest Labs. Flux, 2024. URL https://github.com/black-forest-labs/flux. GitHub repository. Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang, Zhihong Chen, Maya Varma, Steven QH Truong, Chu The Chuong, and Curtis Langlotz. Chexpert plus: Augmenting large chest x-ray dataset with text radiology reports, patient demographics and additional image formats. arXiv preprint arXiv:2405.19538, 2024. Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Chen, Xidong Wang, Zhenyang Cai, Ke Ji, Xiang Wan, et al. Towards injecting medical visual knowledge into multimodal llms at scale. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 73467370, 2024a. Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024b. Ying Chen, Guoan Wang, Yuanfeng Ji, Yanjun Li, Jin Ye, Tianbin Li, Ming Hu, Rongshan Yu, Yu Qiao, and Junjun He. Slidechat: large vision-language assistant for whole-slide pathology image understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 51345143, 2025b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024c. Onat Dalmaz, Mahmut Yurt, and Tolga Çukur. Resvit: Residual vision transformers for multimodal medical image synthesis. IEEE Transactions on Medical Imaging, 41(10):25982614, 2022. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2): 295307, 2015. Yingying Fang, Zihao Jin, Shaojie Guo, Jinda Liu, Yijian Gao, Junzhi Ning, Zhiling Yue, Zhi Li, Simon LF Walsh, and Guang Yang. Decoding report generators: cyclic vision-language adapter for counterfactual explanations. arXiv e-prints, pp. arXiv2411, 2024. 11 Jason Fries, Leon Weber, Natasha Seelam, Gabriel Altay, Debajyoti Datta, Samuele Garda, Sunny Kang, Rosaline Su, Wojciech Kusa, Samuel Cahyawijaya, et al. Bigbio: framework for datacentric biomedical natural language processing. Advances in Neural Information Processing Systems, 35:2579225806, 2022. Imran Ul Haq, Mustafa Mhamed, Mohammed Al-Harbi, Hamid Osman, Zuhal Hamd, and Zhe Liu. Advancements in medical radiology through multimodal machine learning: comprehensive overview. Bioengineering, 12(5):477, 2025. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. Ming Hu, Lin Wang, Siyuan Yan, Don Ma, Qingli Ren, Peng Xia, Wei Feng, Peibo Duan, Lie Ju, and Zongyuan Ge. Nurvid: large expert-level video database for nursing procedure activity understanding. Advances in Neural Information Processing Systems, 36:1814618164, 2023a. Ming Hu, Peng Xia, Lin Wang, Siyuan Yan, Feilong Tang, Zhongxing Xu, Yimin Luo, Kaimin Song, Jurgen Leitner, Xuelian Cheng, et al. Ophnet: large-scale video benchmark for ophthalmic surgical workflow understanding. In European Conference on Computer Vision, pp. 481500. Springer, 2024a. Ming Hu, Kun Yuan, Yaling Shen, Feilong Tang, Xiaohao Xu, Lin Zhou, Wei Li, Ying Chen, Zhongxing Xu, Zelin Peng, et al. Ophclip: Hierarchical retrieval-augmented learning for ophthalmic surgical video-language pretraining. arXiv preprint arXiv:2411.15421, 2024b. Ming Hu, Chenglong Ma, Wei Li, Wanghan Xu, Jiamin Wu, Jucheng Hu, Tianbin Li, Guohang Zhuang, Jiaqi Liu, Yingzhou Lu, et al. survey of scientific large language models: From data foundations to agent frontiers. arXiv preprint arXiv:2508.21148, 2025. Xinyue Hu, Gu, An, Zhang, Liu, Kobayashi, Harada, Summers, and Zhu. Medicaldiff-vqa: large-scale medical dataset for difference visual question answering on chest x-ray images. PhysioNet, 12:13, 2023b. Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2217022183, 2024c. Shih-Cheng Huang, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew Lungren. Fusion of medical imaging and electronic health records using deep learning: systematic review and implementation guidelines. NPJ digital medicine, 3(1):136, 2020. Shih-Cheng Huang, Zepeng Huo, Ethan Steinberg, Chia-Chun Chiang, Curtis Langlotz, Matthew Lungren, Serena Yeung, Nigam Shah, and Jason Fries. Inspect: multimodal dataset for patient outcome prediction of pulmonary embolisms. Advances in Neural Information Processing Systems, 36:1774217772, 2023. Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In Proceedings of the European conference on computer vision (ECCV), pp. 172189, 2018. Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. Advances in neural information processing systems, 36:3799538017, 2023. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11251134, 2017. IXI Consortium. URL https://brain-development.org/ ixi-dataset/. Nearly 600 subjects with T1/T2/PD/MRA/DTI MRI acquired at three London hospitals. Ixi dataset, 2024. Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu, and Yuke Zhu. Harmon: Whole-body motion generation of humanoid robots from language descriptions. arXiv preprint arXiv:2410.12773, 2024. Robert Kaczmarczyk, Theresa Isabelle Wilhelm, Ron Martin, and Jonas Roos. Evaluating multimodal ai in medical diagnostics. npj Digital Medicine, 7(1):205, 2024. Firas Khader, Gustav Müller-Franzes, Tianci Wang, Tianyu Han, Soroosh Tayebi Arasteh, Christoph Haarburger, Johannes Stegmaier, Keno Bressem, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, and Daniel Truhn. Multimodal deep learning for integrating chest radiographs and clinical parameters: case for transformers. Radiology, 309(1):e230806, 2023. doi: 10.1148/radiol. 230806. Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 16461654, 2016. Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse In Proceedings of the European image-to-image translation via disentangled representations. conference on computer vision (ECCV), pp. 3551, 2018. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36: 2854128564, 2023. Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, Yanjun Li, Pengcheng Chen, Xiaowei Hu, Zhongying Deng, Yuanfeng Ji, Jin Ye, Yu Qiao, and Junjun He. Gmai-vl & gmai-vl-5.5m: large vision-language model and comprehensive multimodal dataset towards general medical ai, 2024. Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijing Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, and Junjun He. Ophora: large-scale data-driven text-guided ophthalmic surgical video generation model. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 425435. Springer, 2025. Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 18331844, 2021. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 525536, 2023. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pp. 16501654. IEEE, 2021. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024. Jiyao Liu, Jinjie Wei, Wanying Qu, Chenglong Ma, Junzhi Ning, Yunheng Li, Ying Chen, Xinzhe Luo, Pengcheng Chen, Xin Gao, et al. Medq-bench: Evaluating and exploring medical image quality assessment abilities in mllms. arXiv preprint arXiv:2510.01691, 2025. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. Advances in neural information processing systems, 30, 2017. Shengjie Liu, Chuang Zhu, Feng Xu, Xinyu Jia, Zhongyue Shi, and Mulan Jin. Bci: Breast cancer immunohistochemical image generation through pyramid pix2pix. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 18151824, June 2022a. Shengjie Liu, Chuang Zhu, Feng Xu, Xinyu Jia, Zhongyue Shi, and Mulan Jin. Bci: Breast cancer immunohistochemical image generation through pyramid pix2pix. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 18151824, 2022b. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unifiedio: unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2643926455, 2024. Chenglong Ma, Yuanfeng Ji, Jin Ye, Zilong Li, Chenhui Wang, Junzhi Ning, Wei Li, Lihao Liu, Qiushan Guo, Tianbin Li, et al. Meditok: unified tokenizer for medical image synthesis and interpretation. arXiv preprint arXiv:2505.19225, 2025a. Chenglong Ma, Yuanfeng Ji, Jin Ye, Lu Zhang, Ying Chen, Tianbin Li, Mingjie Li, Junjun He, and Hongming Shan. Towards interpretable counterfactual generation via multimodal autoregression. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 611620. Springer, 2025b. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025c. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 77397751, 2025d. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pp. 353367. PMLR, 2023. D. Nguyen, C. Chen, H. He, and C. Tan. Pragmatic radiology report generation. In Proceedings of Machine Learning for Health (ML4H), PMLR, 2023. doi: 10.48550/arXiv.2303.08715. Junzhi Ning, Dominic Marshall, Yijian Gao, Xiaodan Xing, Yang Nan, Yingying Fang, Sheng Zhang, Matthieu Komorowski, and Guang Yang. Unpaired translation of chest x-ray images for lung opacity diagnosis via adaptive activation masks and cross-domain alignment. Pattern Recognition Letters, 193:2128, 2025. Ian Pan, Alexandre Cadrin-Chênevert, and Phillip Cheng. Tackling the radiological society of north america pneumonia detection challenge. American Journal of Roentgenology, 213(3):568574, 2019. Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, et al. Medgemma technical report. arXiv preprint arXiv:2507.05201, 2025. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language models. Nature Medicine, 31(3):943950, 2025. Luis Soenksen, Yu Ma, Cynthia Zeng, Leonard Boussioux, Kimberly Villalobos Carballo, Liangyuan Na, Holly Wiberg, Michael Li, Ignacio Fuentes, and Dimitris Bertsimas. Integrated multimodal artificial intelligence framework for healthcare applications. NPJ digital medicine, 5(1):149, 2022. Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, et al. Gmai-vl-r1: Harnessing reinforcement learning for multimodal medical reasoning. arXiv preprint arXiv:2504.01886, 2025. Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine Van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, and Hannaneh Hajishirzi. Medicat: dataset of medical images, captions, and textual references. arXiv preprint arXiv:2010.06000, 2020. Tim Tanida, Philip Müller, Georgios Kaissis, and Daniel Rueckert. Interactive and explainable regionguided radiology report generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2410124111, 2023. doi: 10.1109/CVPR52688.2023. 02357. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024a. OpenGVLab Team. Internvl2: Better than the bestexpanding performance boundaries of opensource multimodal models with the progressive scaling strategy, 2024b. Omkar Chakradhar Thawakar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Khan. Xraygpt: Chest radiographs summarization using large medical vision-language models. In Proceedings of the 23rd workshop on biomedical natural language processing, pp. 440448, 2024. Adrian Thummerer, Erik van der Bijl, Arthur Jr Galapon, Florian Kamp, Mark Savenije, Christina Muijs, Shafak Aluwini, Roel JHM Steenbakkers, Stephanie Beuel, Martijn PW Intven, et al. Synthrad2025 grand challenge dataset: Generating synthetic cts for radiotherapy from head to abdomen. Medical physics, 52(7):e17981, 2025. Dmitrii Torbunov, Yi Huang, Haiwang Yu, Jin Huang, Shinjae Yoo, Meifeng Lin, Brett Viren, and Yihui Ren. Uvcgan: Unet vision transformer cycle-consistent gan for unpaired image-to-image translation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 702712, 2023. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 87988807, 2018. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood, Charles Kahn Jr, Olivier Gevaert, and Arvind Rao. Multimodal machine learning in image-based and clinical biomedicine: Survey and prospects. International journal of computer vision, 132(9):37533769, 2024. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Hui Hui, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. Nature Communications, 16(1):7866, 2025a. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Hui Hui, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. Nature Communications, 16(1):7866, 2025b. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025c. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025d. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024a. Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025e. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024b. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Huihui Xu, Yuanpeng Nie, Hualiang Wang, Ying Chen, Wei Li, Junzhi Ning, Lihao Liu, Hongqiu Wang, Lei Zhu, Jiyao Liu, et al. Medground-r1: Advancing medical image grounding via spatialsemantic rewarded group relative policy optimization. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 391401. Springer, 2025a. Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning. arXiv preprint arXiv:2506.07044, 2025b. Zhiyang Xu, Jiuhai Chen, Zhaojiang Lin, Xichen Pan, Lifu Huang, Tianyi Zhou, Madian Khabsa, Qifan Wang, Di Jin, Michihiro Yasunaga, et al. Pisces: An auto-regressive foundation model for image understanding and generation. arXiv preprint arXiv:2506.10395, 2025c. Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, and Zongyuan Ge. Derm1m: million-scale vision-language dataset aligned with clinical ontology knowledge for dermatology. arXiv preprint arXiv:2503.14911, 2025a. Siyuan Yan, Xieji Li, Ming Hu, Yiwen Jiang, Zhen Yu, and Zongyuan Ge. Make: Multi-aspect knowledge-enhanced vision-language pretraining for zero-shot dermatological assessment. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 369379. Springer, 2025b. Zhiwen Yang, Haowei Chen, Ziniu Qian, Yang Yi, Hui Zhang, Dan Zhao, Bingzheng Wei, and Yan Xu. All-in-one medical image restoration via task-adaptive routing. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 6777. Springer, 2024. Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, et al. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. Advances in Neural Information Processing Systems, 37: 9432794427, 2024. 16 Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5728 5739, 2022. Hong Zhang, Zhongjie Duan, Xingjun Wang, Yuze Zhao, Weiyi Lu, Zhipeng Di, Yixuan Xu, Yingda Chen, and Yu Zhang. Nexus-gen: unified model for image understanding, generation, and editing. arXiv preprint arXiv:2504.21356, 2025a. Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, et al. Biomedgpt: unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv e-prints, pp. arXiv2305, 2023a. Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023b. Sheng Zhang, Jinge Wu, Junzhi Ning, and Guang Yang. Dmrn: dynamical multi-order response In 2025 IEEE/CVF Winter Conference on network for the robust lung airway segmentation. Applications of Computer Vision (WACV), pp. 40364045. IEEE, 2025b. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023c. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 22232232, 2017. Xianwei Zhuang, Yuxin Xie, Yufan Deng, Dongchao Yang, Liming Liang, Jinghan Ru, Yuguo Yin, and Yuexian Zou. Vargpt-v1. 1: Improve visual autoregressive large unified model via iterative instruction tuning and reinforcement learning. arXiv preprint arXiv:2504.02949, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "APPENDIX TABLE OF CONTENTS A.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.1 Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.1 Dataset Composition Details . . . . . . . . . . . . . . . . . . . . . . . . . A.2.2 Medical Domain and Modality Distribution . . . . . . . . . . . . . . . . . A.2.3 Modality and Anatomy Distribution . . . . . . . . . . . . . . . . . . . . . A.3 Data Enhancement Pipeline: CAG Implementation . . . . . . . . . . . . . . . . . A.3.1 Stage 1: Structured Description Generation . . . . . . . . . . . . . . . . . A.3.2 Stage 2: Caption Fusion Enhancement . . . . . . . . . . . . . . . . . . . . A.3.3 Stage 3: Thinking-Enhanced Response Generation . . . . . . . . . . . . . A.4 Downstream Task Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.1 CXR Report Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.2 Visual Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . A.4.3 Medical Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.4 Interleaved tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Expert Review Validation System . . . . . . . . . . . . . . . . . . . . . . . . . . A.5.1 Expert Review Framework Overview . . . . . . . . . . . . . . . . . . . . A.5.2 Evaluation Dimension Analysis . . . . . . . . . . . . . . . . . . . . . . . A.5.3 Dataset Quality Comparison Analysis . . . . . . . . . . . . . . . . . . . . A.5.4 Medical Modality-Specific Analysis . . . . . . . . . . . . . . . . . . . . . A.6 Other Downstream Tasks Performance . . . . . . . . . . . . . . . . . . . . . . . A.6.1 Medical Report Generation . . . . . . . . . . . . . . . . . . . . . . . . . . A.6.2 CXR Lung Opacity Image Translation . . . . . . . . . . . . . . . . . . . . 19 21 21 21 22 23 23 24 26 26 26 28 31 31 31 32 33 33 34 18 A.1 IMPLEMENTATION DETAILS A.1.1 TRAINING HYPERPARAMETERS Table 6: Training hyperparameters and configurations for the three-stage curriculum learning strategy in UniMedVL. These stages collectively implement the Knowledge component of the OKA framework. Stage 1 (Foundation) Stage 2 (Instruction Tuning) Stage 3 (Unified Multimodal) 5 10 2.5 105 1.0 105 Hyperparameters Learning rate Optimizer Loss weight (CE : MSE) Training steps EMA ratio Image Resolution (VAE) Image Resolution (ViT) Max tokens per sample Dropout ViT training VAE training Understanding branch LLM training 85K 512-1024 378-980 18.5K Trainable AdamW 0.25 : 1.0 120K 0.995 512-1024 224-518 20K Text: 0.3, ViT/VAE: 0. Frozen Frozen Trainable Trainable Data Sampling Ratio (%) Text-Only Text-to-Image (T2I) Image-to-Text (I2T) Interleaved 5 25 75 - 5 45 40 70K 32-1024 378-980 27K Frozen 3 35 37 25 Detailed Training Strategy Implementation. Our training employs three-stage curriculum learning approach that implements the Knowledge component within the OKA framework. We use the AdamW optimizer throughout all stages: Stage 1 (Foundation Training) establishes basic medical understanding over 85K steps with learning rate of 5 105. The data composition prioritizes image-to-text tasks (75%), complemented by text-to-image generation (25%) and pure text data (5%). This stage trains both ViT and LLM components end-to-end while keeping the VAE frozen. The image resolution is restricted with the range from 512-1024 pixels for the generation branch and 378-980 pixels for the understanding branch. Stage 2 (Instruction Tuning) extends training to 120K steps with reduced learning rate of 2.5 105. The data mixture evolves to balance text-to-image (45%) and image-to-text (40%) tasks, while introducing interleaved multimodal datasets (10%). The ViT encoder is frozen at this stage to preserve learned visual features. Token capacity increases to 20K per sample. Stage 3 (Unified Multimodal Training) focuses on interleaved generation capabilities over 70K steps with learning rate of 1.0 105. This stage significantly increases interleaved dataset usage (25%) while maintaining balanced generation (35%) and understanding (37%) tasks. The expanded token budget (27K) and broader image resolution range (32-1024 pixels for generation) support interleaved tasks, including medical image super-resolution, modality translation, and counterfactual generation. Hardware Requirements and Training Infrastructure. Our model training was conducted using 8 A800 GPUs (80GB memory each) for experimental validation. However, for optimal training efficiency and to fully exploit the models capacity, we recommend minimum configuration of 16 A800 GPUs or equivalent hardware. Technical Implementation Details. The training employs unified loss function that balances understanding and generation objectives with CE:MSE weight ratio of 0.25:1.0. We apply consistent dropout rates across all stages (Text: 0.3, ViT/VAE: 0.05) to prevent overfitting. The EMA coefficient 19 is set to 0.995 for stable model convergence. Throughout training, the VAE remains frozen to maintain stable latent representations. Rationale for Using Pretrained VAE without Fine-tuning. Our approach leverages generalpurpose pretrained VAE model from FLUX (Black Forest Labs, 2024) without medical domainspecific fine-tuning. This design choice addresses two core questions: (1) the reconstruction capability of pretrained VAE on medical imaging modalities, and (2) the cost-benefit trade-off of fine-tuning versus preserving existing capabilities. Regarding the first question, we conducted comprehensive reconstruction experiments across eight medical imaging modalities to evaluate performance. For the second question, considering that our training data is not specifically designed for reconstruction optimization, we did not pursue domain-specific fine-tuning to avoid potential degradation of the models general-purpose capabilities while maintaining stable latent representations throughout our progressive training stages. Table 7: Reconstruction quality evaluation of pretrained VAE models on medical imaging modalities. Metric Model fd CFP CT CXR Endoscopy HIS MRI OCT Ultrasound rFID (Lower is Better) VAE (FLUX) VQGAN Emu3-VQ MedITok PSNR (Higher is Better) VAE (FLUX) VQGAN Emu3-VQ MedITok SSIM (Higher is Better) VAE (FLUX) VQGAN Emu3-VQ MedITok 8 8 8 16 8 8 8 16 8 8 8 16 13.22 27.22 16.27 14.39 34.58 35.40 28.96 37. 0.892 0.923 0.943 0.953 5.81 15.97 11.83 7.88 37.34 31.13 36.11 36.32 0.951 0.885 0.928 0.937 5.42 33.57 27.91 22.27 37.09 29.28 31.68 31. 0.973 0.753 0.793 0.855 11.77 27.73 20.83 10.66 35.33 25.60 28.96 29.17 0.934 0.768 0.847 0.890 10.00 21.33 13.52 6.32 34.50 29.54 34.32 23. 0.922 0.844 0.957 0.972 10.58 67.68 69.89 46.54 34.30 20.94 22.08 23.55 0.921 0.484 0.547 0.660 13.23 29.48 25.43 17.64 34.58 24.79 27.57 25. 0.892 0.248 0.751 0.935 9.64 18.66 11.99 6.55 33.59 31.68 35.81 34.42 0.938 0.317 0.955 0.883 The empirical evaluation demonstrates that the VAE (FLUX) achieves competitive reconstruction performance across eight distinct medical imaging modalities without requiring domain-specific fine-tuning. With compression factor of fd = 8, the model consistently delivers low rFID scores, competitive PSNR values, and robust SSIM scores. Figure 5: Qualitative comparison of VAE reconstruction quality across diverse medical imaging modalities. Visual examples demonstrating reconstruction fidelity across eight medical imaging modalities (CFP, CT, CXR, Endoscopy, HIS, MRI, OCT, Ultrasound) using the pretrained FLUX VAE without domain-specific fine-tuning. 20 A.2 DATASET STATISTICS A.2.1 DATASET COMPOSITION DETAILS Table 8: Overview of training stage data distribution, showing data composition, task types, and scale statistics across different stages. In addition to datasets for new dataset, stage 2 utilized the high-quality subset of stage 1 datasets. Training Stage Total Entries Task Categories Stage 1: Foundation Training Understanding Tasks Generation Tasks Stage 1 Subtotal Stage 2: Instruction Tuning Understanding Tasks Generation Tasks CoT Understanding Text-only Tasks Stage 2 Subtotal 4.0M Image comprehension, VQA 1.6M Text-to-image, controllable generation 5.6M 698K 668K 317K 230K 1.9M Foundation capabilities Image CoT, clinical reasoning Enhanced T2I, medical translation Chain-of-thought reasoning Medical QA, clinical dialogue Knowledge integration Stage 3: Unified Multimodal Training. Interleaved Tasks Stage 3 Subtotal Total Dataset 330K 0.33M 5.6M 5 interleaved tasks Unified capabilities All medical tasks A.2.2 MEDICAL DOMAIN AND MODALITY DISTRIBUTION Table 9: Major datasets detailed information, showing key dataset contributions sorted by data volume. For open-source datasets, the reported numbers indicate the actual subset sizes used in our training pipeline after filtering. Dataset Name Total Entries Primary Tasks PMC-OA (Lin et al., 2023) Quilt-1m (Ikezogwo et al., 2023) Healthgpt (Lin et al., 2025) PubMedVision (Chen et al., 2024a) Gmai-vl (Li et al., 2024) Bigbio (Fries et al., 2022) CheXpertPlus (Chambon et al., 2024) PMC VQA (Zhang et al., 2023c) Internvl (Chen et al., 2024c) Medicat (Subramanian et al., 2020) Medical-diff-vqa (Hu et al., 2023b) PMC-Inline (Wu et al., 2025a) IXI T2/T1 SR 4x (IXI Consortium, 2024) BraTS23 Modality Tran (Baltruschat et al., 2023) SynthRAD Brain (MR to CT/CT to MR) (Thummerer et al., 2025) SynthRAD Pelvis (MR to CT/CT to MR) (Thummerer et al., 2025) ICG-CXR dataset (Ma et al., 2025b) BCI dataset (Liu et al., 2022a) Total (Selected Datasets) Others Datasets Grand Total 1.0M Text-to-Image Generation 644K Histopathology Understanding 638K Clinical Reasoning, Image Caption 385K Controllable T2I Generation 288K Enhanced T2I Generation 262K Clinical Reasoning with CoT 223K Medical Report Understanding 204K Image Caption 188K Disease Classification, Clinical Reasoning 132K Controllable T2I Generation 129K Image Caption, Entity Recognition 121K Multi-image Understanding 161K Super resolution 52K Cross modal synthesis 66K Cross modal synthesis 42K Cross modal synthesis 10K Counterfactual generation 5K Virtual immunohistochemistry staining 4.55M 1.05M 5.6M All Tasks 21 A.2.3 MODALITY AND ANATOMY DISTRIBUTION Figure 6 illustrates the comprehensive statistics of our curated medical datasets, showing both modality distribution and anatomical coverage. (a) Modality Distribution (b) Anatomy Distribution Figure 6: Comprehensive statistics of our curated medical datasets with the respect to both modality distribution and anatomy distribution. 22 A.3 DATA ENHANCEMENT PIPELINE: CAG IMPLEMENTATION This section presents the complete prompt templates used in our Caption Augmented Generation (CAG) pipeline for image generation tasks, as described in Section 3. The CAG pipeline consists of two main stages: (1) structured medical description generation for quality control, and (2) caption fusion that combines original captions with generated descriptions. A.3.1 STAGE 1: STRUCTURED DESCRIPTION GENERATION Stage 1: Structured Description Generation Prompt Purpose: Generate four-level structured medical image descriptions for quality control and similarity computation You are universally expert medical image analyst, proficient in all imaging modalities and anatomical systems. Your input is single medical image, with no supplementary information. Your only task is to provide comprehensive, objective, and structured description at four distinct levels, from the highest overview down to the most specific and exceptional findings. You must not offer any diagnostic, interpretive, or clinical advice. --- Output Structure (Four-Level, Top-to-Bottom -- definitions for your internal guidance; do NOT reproduce these headings in your answer) LEVEL 1: IMAGE TYPE & GLOBAL CONTEXT In one sentence, state the presumed imaging modality (if visually clear), main body region(s), and overall image category (e.g., cross-sectional, projectional, histological). Example: \"This is an axial CT image of the abdomen and pelvis, showing cross-sectional anatomy at the level of the lower kidneys.\" LEVEL 2: MACRO-ANATOMICAL OVERVIEW In 2-4 concise lines, summarize the global distribution and layout of major anatomical regions, dominant structures, and any clearly visible large-scale abnormalities, masses, or disease patterns. Describe anatomical orientation, symmetry, major organ relationships, and other visually prominent features. LEVEL 3: ORGAN / SUBREGION DETAILS -- must be the most detailed section In 6-12 lines (use complete sentences), describe the visual appearance of individual organs, vessels, bones, or other relevant subregions. Provide precise, granular, reproducible details so that all main features can be reconstructed. Maintain strict objectivity; do not include diagnostic language. LEVEL 4: SPECIAL OR INCIDENTAL FINDINGS List any unusual devices, postsurgical changes, image artifacts, rare morphologic features, or observations not already mentioned above. If none are visible, explicitly state: \"No distinct pathological or incidental findings are visible.\" Writing Instructions 1. Write the entire description as one continuous paragraph that implicitly follows the LEVEL 1 LEVEL 4 order--do not include level headings, bullet points, or numbered lists in the paragraph. 2. Do not use bullet points elsewhere (except within the examples). 3. For more complex images, the portion corresponding to LEVEL 3 should naturally be longer; for simpler cases, keep it proportionally concise. 4. Avoid any clinical judgement or speculation--describe only what is directly visible. A.3.2 STAGE 2: CAPTION FUSION ENHANCEMENT This stage fuses original captions with Stage 1 generated structured descriptions to create enhanced descriptions for image generation tasks. Stage 2: Caption Fusion Enhancement Prompt Purpose: Fuse original captions with structured descriptions for enhanced image generation prompts You are universally expert medical image analyst, proficient in all imaging modalities and anatomical systems. CRITICAL CONSTRAINT: You must maintain absolute anatomical consistency. NEVER change, assume, or modify the anatomical location described in the 23 original caption. Do not make assumptions about different anatomical locations or transfer descriptions between different body parts. Your input consists of: 1. structured, objective, four-level description derived from locally deployed AI model (following strict hierarchy from global overview to specific findings). 2. An original, data-derived textual description containing high-density, potentially diagnostic or interpretative information, which may lack structured clarity. Your task is to: First, critically review and confirm the completeness of the structured description generated by the local model. Then, systematically extract and objectively incorporate relevant, visually verifiable details from the original data-derived description, enhancing information density without including diagnostic, interpretive, or clinical judgement. Clearly indicate and explicitly include visually evident anatomical abnormalities, structural deviations, or incidental observations present in the original data but omitted in the structured description. Output Structure (Four-Level, Top-to-Bottom) LEVEL 1: IMAGE TYPE & GLOBAL CONTEXT In one sentence, state the presumed imaging modality, main body region(s), and overall image category. LEVEL 2: MACRO-ANATOMICAL OVERVIEW In 2--4 concise lines, summarize global anatomical distribution, dominant structures, anatomical symmetry or deviations, and clearly visible large-scale abnormalities. LEVEL 3: ORGAN / SUBREGION DETAILS -- must be the most detailed section In 6--12 complete sentences, describe individual organs, bones, vessels, and other relevant anatomical subregions in precise, reproducible detail. Objectively highlight visually confirmed abnormalities or structural deviations derived from the original data description. LEVEL 4: SPECIAL OR INCIDENTAL FINDINGS Explicitly mention unusual devices, postsurgical changes, rare morphological features, or visually detectable anomalies present in the original description yet absent in the structured description. Clearly state the absence of commonly expected baseline anatomical or pathological features if definitively not observed in the image. Writing Instructions 1. Write the final enhanced description as single, continuous paragraph implicitly following LEVEL 1 LEVEL 4 order--do not include explicit level headings, bullet points, or numbered lists. 2. Avoid any clinical judgement, diagnostic language, or speculative interpretation--include only details directly verifiable from visual inspection. 3. Start your output with \"Please generate realistic [modality] image showing\" to make it proper generation instruction. A.3.3 STAGE 3: THINKING-ENHANCED RESPONSE GENERATION This stage aims to elicit the reasoning process from the medical foundation model (MediGama-27BIT) by prompting it to explicitly generate its internal thinking steps. We leverage this specialized medical model to simulate detailed reasoning processes through the structured prompt format. The resulting data, which includes both the explicit thinking traces and the final responses, is then used to train our model. Stage 3: Thinking-Enhanced Response Generation Prompt (Revised v2) Purpose: Generate medical image responses with thinking tags for enhanced reasoning and quality control System: You are medical image generator. You create [modality] images based on clinical descriptions. Your responses should describe what features you have generated in the image from the creator's perspective. Use bullet points to organize the anatomical structures and clinical features you have included in your generated image. User: Based on this clinical description: \"[clinical_description]\" You have been given the corresponding medical image. Please provide response following this format: Required format: <think>Analyzing the clinical description, need to generate an image that captures: 1) The key pathological process described, 2) The anatomical structures involved, 3) The specific imaging characteristics for [modality]. 24 Based on the clinical presentation, should include [key features reasoning]. [structured_caption if available]</think> Here/This is the generated [modality] image that displays: [anatomical structure or clinical finding 1] [anatomical structure or clinical finding 2] [anatomical structure or clinical finding 3] IMPORTANT: 1. In the <think> tag, reason through WHAT you need to generate and WHY based on medical knowledge 2. Respond from the GENERATOR perspective - describe what features you have CREATED/GENERATED in the image 3. Use the exact format above with bullet points () to list features 4. Start with 'Here is the generated [modality] image that displays:' 5. Each bullet point should describe specific anatomical structure, clinical finding, or visual feature that you have included 6. Do NOT use observational language like 'shows', 'visible', 'can be seen' - instead use generative language like 'displays', 'includes', 'features', 'contains' Note: The thinking tag should reflect your decision-making process: \"I need to generate because Y\", \"The clinical description indicates should include Z\", etc. The enhanced captions from Stage 2 (if the process \"generating\" is not generated successfully) and Stage 3 (if the process \"thinking\" is generated successfully) are sampled and then submitted to the Expert Review system (Section A.5) for final validation. A.4 DOWNSTREAM TASK RESULTS A.4.1 CXR REPORT GENERATION Figure 7: Medical report generation examples. Demonstrations of generating structured radiology reports from chest X-ray images, including FINDINGS and IMPRESSION sections with clinical observations and diagnoses. A.4.2 VISUAL QUESTION ANSWERING Figure 8: Visual question answering examples across different medical imaging modalities. Figure 9: Medical image generation examples with text prompts. Text-to-image synthesis across multiple medical imaging modalities, demonstrating the models ability to generate clinically realistic images from natural language descriptions. Figure 10: Medical image generation examples with text prompts (continued). Additional text-toimage synthesis examples showcasing diverse anatomical regions and pathological conditions across different medical imaging modalities. 27 A.4.3 MEDICAL IMAGE GENERATION A.4.4 INTERLEAVED TASKS Figure 11: Medical Image Promptable Segmentation. Examples of text-guided segmentation where the model generates anatomical structure masks based on natural language prompts. This demonstrates the unified models capability to understand both visual and textual inputs for flexible medical image analysis. Figure 12: Super Resolution of Brain MRI. Interleaved task demonstrating low-resolution MRI input with text prompt, generating enhanced high-resolution output while preserving anatomical structures. 28 Figure 13: Counterfactual Generation of Chest X-ray. Multimodal task taking image and text description as input, generating counterfactual images with explanatory text output for clinical scenario analysis. Figure 14: Virtual Immunohistochemistry Staining. Cross-modality histopathology transformation from H&E to IHC staining, demonstrating unified models capability to synthesize complementary staining patterns. Figure 15: Cross-Modal Medical Image Synthesis. Bidirectional MRI sequence translation (T2 FLAIR) showcasing the models ability to generate complementary imaging modalities from existing scans. 30 A.5 EXPERT REVIEW VALIDATION SYSTEM This section presents an expert review validation system that evaluates the quality of our UniMed-5M dataset construction and two caption generation approaches described in the Data Enhancement Pipeline (Section A.3): Simple approach: Caption fusion that combines structured descriptions from Stage 1 with original captions (Stage 2 of CAG pipeline). Thinking-enhanced approach: Incorporates an additional planning process with <think> tags that integrates reasoning steps before medical image generation (Stage 3 of CAG pipeline). The validation system evaluates both data quality and methodological effectiveness. A.5.1 EXPERT REVIEW FRAMEWORK OVERVIEW Our expert review validation system is designed around seven-dimensional medical evaluation framework that assesses medical AI performance. Our evaluation framework encompasses seven dimensions that assess the synthetic quality of medical image captions. The framework begins with Modality Match (0-1), which measures consistency between images and declared medical imaging modalities, followed by Factual Accuracy (0-5) that evaluates the precision of anatomical structure and pathological finding descriptions. Information Completeness (0-5) assesses coverage of diagnostically relevant key information, while Position/Quantity Accuracy (0-5) measures precision in anatomical localization and quantitative assessments. The framework also incorporates Professionalism (0-5) to evaluate adherence to medical reporting standards, Planning Coherence (0-5) to assess systematic thinking and logical organization quality, and finally Clinical Reasoning (Turing Test) (0-5) to measure approximation to human expert-level performance. Expert Validation Protocol: Experts conducted audits of 200 samples across all seven dimensions. The evaluation process achieved inter-rater agreement exceeding 0.85 across all dimensions. A.5.2 EVALUATION DIMENSION ANALYSIS Figure 16 presents the correlation analysis and comparative results. Figure 16a shows interdimensional correlations, while Figure 16b compares the two generation approaches. (a) Correlation matrix between evaluation dimensions. (b) Score difference heatmap comparing thinking and simple approaches. Figure 16: Expert evaluation analysis. (a) Correlation matrix revealing inter-dimensional relationships (Pearson correlation coefficients ranging from 0.60 to 0.92). (b) Score difference heatmap comparing thinking and simple approaches (negative values indicate simple approach scores higher; all dimensions scored on 0-5 scale except Modality Match on 0-1 scale). 31 A.5.3 DATASET QUALITY COMPARISON ANALYSIS Figure 17 compares the two generation approaches across all evaluation dimensions. The radar chart (Figure 17a) shows closely aligned performance profiles. (a) Performance comparison: Thinking vs Simple approaches across evaluation dimensions. (b) Medical imaging modalities distribution Figure 17: Expert validation overview. (a) Radar chart comparing performance profiles of thinking and simple approaches across all seven evaluation dimensions. (b) Pie chart showing balanced representation across medical imaging modalities, ensuring comprehensive coverage. A.5.4 MEDICAL MODALITY-SPECIFIC ANALYSIS Figure 18 presents modality-specific performance across nine medical imaging modalities. Figure 18a shows statistical comparisons, and Figure 18b displays detailed performance metrics. (a) Statistical comparison between thinking and simple approaches. (b) Modality-specific performance analysis. Figure 18: Comprehensive performance analysis. (a) Bar chart showing mean scores with confidence intervals. (b) Heatmap displaying modality-specific performance scores. 32 A.6 OTHER DOWNSTREAM TASKS PERFORMANCE A.6.1 MEDICAL REPORT GENERATION Table 10: Medical report generation performance on MIMIC-CXR dataset. Evaluation of automated radiology report generation using three metrics: ROUGE-L (lexical similarity), RaTE (radiology-specific terminology accuracy), and RadCliQ1 (clinical quality assessment). Higher scores indicate better performance for all metrics. Baseline results are sourced from Xu et al. (2025b). Bold indicates best performance and underlined indicates second-best performance. Models GPT-4.1 Claude Sonnet 4 Gemini-2.5-Flash Med-R1-2B MedLM-R1-2B MedGemma-8B-IT LLaVA-Med-7B HuatuoGPT-V-7B BioMediX2-8B Qwen2.5VL-7B InternVL2-8B InternVL3-8B Lingshu-7B HealthGPT-14B HuatuoGPT-V-34B MedDr-40B InternVL3-14B Qwen2.5VL-32B InternVL2.5-38B InternVL3-38B Lingshu-32B UniMedVL MIMIC-CXR ROUGE-L MIMIC-CXR RaTE MIMIC-CXR RadCliQ 9.0 20.0 25.4 19.3 20.3 25.6 15.0 23.4 20.0 24.1 23.2 22.9 30.8 21.4 23.5 15.7 22.0 15.7 22.7 22.8 28.8 19.2 51.3 45.6 50.3 40.6 41.6 52.4 12.8 48.9 44.4 47.0 47.0 48.2 52.1 48.4 48.5 45.2 48.6 47.5 47.5 47.9 50.8 45.0 57.1 53.4 59.4 42.4 48.3 62.9 52.9 48.2 53.0 55.1 56.2 55.1 69.2 52.7 47.1 47.0 46.5 45.2 54.9 47.2 67.1 42. 33 A.6.2 CXR LUNG OPACITY IMAGE TRANSLATION Table 11: Unpaired chest X-ray zero-shot opacity removal translation performance on the RSNA dataset (Pan et al., 2019). Evaluation metrics: FID and KID, where lower values indicate better performance. Bold indicates best performance and underlined indicates second-best performance. Model Baselines FID KID Original CXRs Munit (Huang et al., 2018) Unit (Liu et al., 2017) CycleGAN (Zhu et al., 2017) Uvcgan (Torbunov et al., 2023) Drit (Lee et al., 2018) AAMA-CDA (Ning et al., 2025) 81.80 109.4 103.2 208.3 210.4 117.6 67.18 0.043 0.073 0.061 0.216 0.225 0.087 0.016 Unified Models HealthGPT-M3 UniMedVL 62.19 35. 0.031 0.008 (a) Quantitative results (b) Qualitative examples"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Fuzhou University",
        "Imperial College London",
        "Monash University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Institute of Optics and Fine Mechanics",
        "Shanghai Jiao Tong University",
        "The Hong Kong University of Science and Technology",
        "The University of Hong Kong",
        "University of Cambridge"
    ]
}