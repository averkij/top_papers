{
    "paper_title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "authors": [
        "Junxiao Yang",
        "Jinzhe Tu",
        "Haoran Liu",
        "Xiaoce Wang",
        "Chujie Zheng",
        "Zhexin Zhang",
        "Shiyao Cui",
        "Caishun Chen",
        "Tiantian He",
        "Hongning Wang",
        "Yew-Soon Ong",
        "Minlie Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with \"I don't know\". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 9 2 5 3 1 . 5 0 5 2 : r BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs Junxiao Yang1, Jinzhe Tu1, Haoran Liu1, Xiaoce Wang1, Chujie Zheng1, Zhexin Zhang1, Shiyao Cui1, Caishun Chen2, Tiantian He2, Hongning Wang1, Yew-Soon Ong2,3, Minlie Huang1 1 The Conversational AI (CoAI) group, DCST, Tsinghua University 2 Centre for Frontier AI Research, Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore 3 The College of Computing and Data Science, Nanyang Technological University yangjunx21@gmail.com, aihuang@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with dont know. Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARRELa novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Reasoning Models (LRMs) (Jaech et al., 2024; Guo et al., 2025; Team, 2025) have shown impressive performance in specialized reasoning tasks, especially in mathematics and logic. However, these gains have not led to corresponding improvements in reliability. On the contrary, faithfulness hallucination rates are rising (Hughes & Bae, 2023), and helpfulness on factual tasks is declining (Zhao et al., 2025), raising concerns about the reliability of these reasoning models. In this work, we focus primarily on the factual reliability of LRMs, which is crucial requirement for many real-world tasks. Factuality of language models involves two aspects: knowing (whether the model holds relevant knowledge) (Huang et al., 2025; Ji et al., 2023) and telling (conveying the correct factual information) (Gekhman et al., 2024; Mallen et al., 2022). As shown in Figure 1, we want LRMs to exhibit two aspects of factual reliability: (1) Identify both what it knows and what it does not know: The model should be able to recognize questions it does not know the answer and respond with \"I dont know.\" (Xu et al., 2024; Zhang et al., 2024a) (2) Tell what it knows: There is gap between knowing and telling (Wang et al., 2022), and we want to improve the models accuracy in expressing the knowledge it has (Zhang et al., 2024b). Work done during internship at Centre for Frontier AI Research, Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore Corresponding author. 3Our code is publicly availble at https://github.com/thu-coai/BARREL Preprint. Under review. However, current LRMs consistently struggle with the two factual reliability goals above. They rarely acknowledge gaps in their knowledge and often fabricate answers instead, even on questions for which they lack sufficient knowledge. Moreover, their responses can be inconsistentproviding incorrect answers in some instances while correctly responding to similar queries elsewhere (Wang et al., 2022), thereby reducing the overall factual accuracy of their responses (Zhao et al., 2025). Figure 1: How reliable factual reasoning is expected to improve model performance. Left: Current LRMs rarely admit ignorance and often respond inconsistently. Right: Reliable LRMs should acknowledge unknowns and express known facts more consistently. What underlying reasoning behaviors contribute to these observed issues? As shown in Figure 2, our preliminary experiments show that LRMs consume more tokens when generating incorrect answers than correct ones. And we further identify two prevalent pathological reasoning patterns associated with this overthinking phenomenon. As shown on the left part of Figure 3, one prominent pattern is Last-minute Guessing, in which the model, after extensive but inconclusive reasoning, abruptly commits to an answer in final burst of speculative outputmuch like student hastily filling in an answer just before the end of an exam. Another frequent pattern is Second-thought Spiraling, where the model initially identifies the correct answer but continues to over-analyze, ultimately undermining its own correct conclusion. To mitigate these pathological reasoning patterns, we propose novel training framework, BARREL (Boundary-Aware Reasoning for Reliable and Factual LRMs). As shown in Figure 3, BARREL trains LRMs to perform concise, deliberative factual reasoning and draw conclusions after it has explored sufficient number of candidates. Concretely, for known questions, when the model identifies the correct factual answer during reasoning, we want it to maintain confidence in that answer while continuing to explore other possible ones. Once there are no likely candidates, it should halt further reasoning and provide the correct factual answer. For unknown questions, after exploring sufficient number of plausible candidates, it should terminate reasoning and proactively admit its lack of knowledge with uncertainty-aware refusal (e.g., \"Sorry, dont know\"). To implement BARREL, we begin by employing sampling strategy to probe the models knowledge boundary. Drawing on the identified pathological patterns, we construct two distinct types of reasoning data, and use Supervised Fine-Tuning (SFT) to instill the corresponding reasoning behaviors in the model. Finally, we adopt Group Relative Policy Optimization (GRPO) using general reliability-based reward (high for correct answers, medium for uncertainty-aware refusal, and low for incorrect answers) without the need for labeling known/unknown, further enhancing the models ability to generalize in factual reasoning. Figure 2: Number of reasoning tokens used by LRMs when producing correct versus incorrect answers. We test on TruthfulQA across different types of reasoning models. More details at Appendix A. Our experiments demonstrate that models trained with BARREL can effectively express uncertaintyaware refusal, and mitigate the two pathological reasoning patterns. This capability significantly improves reliability: BARREL boosts the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while maintaining an accuracy of 40.7%, which is even higher than the accuracy of 38.43% achieved by distillation. Through detailed analysis, we highlight the critical role of medium-level rewards in promoting uncertainty-aware refusal. This result also identifies the root cause of models 2 Figure 3: Left: The two current reasoning patterns of LRM: Last-minute Guessing, typically associated with unknown knowledge, and Second-thought Spiraling, which occurs despite known knowledge. Right: The BARREL pipeline addresses both cases by correcting overthinking tendencies and constructing SFT data accordingly, further enhanced with GRPO. inability to admit ignorance to fundamental gap in current RL paradigms: they do not reward refusal. As result, models are incentivized to answer every question, regardless of uncertainty. Our main contributions are as follows: We discover the factual overthinking phenomenon and point out two pathological reasoning patterns that lead to factual unreliability of LRMs. To the best of our knowledge, we are the first to explore how LRMs can utilize reasoning to admit ignorance and say \"I dont know\". Also, we introduce novel training pipeline to do so. We emphasize the importance of medium-level rewards in encouraging uncertainty-aware refusal to build more reliable LRM using RL training."
        },
        {
            "title": "2 Related Work",
            "content": "Knowledge Boundary The knowledge boundary of LLMs refers to the extent of knowledge model possesses or can reliably recall (Li et al., 2024). Identifying such boundaries is crucial for model safety and reliability and is commonly addressed by confidence calibration (Ren et al., 2023), internal state probing (Ji et al., 2024), uncertainty estimation (Kapoor et al., 2024), and sampling strategy (Gekhman et al., 2024; Xue et al., 2024). To mitigate failures by outbound queries, recent studies have proposed training LLMs to abstain from answering when uncertain, often by generating \"I dont know\" responses (Cheng et al., 2024) or providing further explanations of the unanswerability (Deng et al., 2024). Our work pushes it further to structured and interpretable rationales about their knowledge boundary. Factual Alignment Factual alignment aims to improve factuality while preserving instructionfollowing capabilities in LLMs. Recent work incorporates factuality-awareness into both SFT and RL stages to improve factual accuracy (Lin et al., 2024), or leverages models internal knowledge and fine-tuning it with Direct Preference Optimization (DPO) algorithm to enhance its self-evaluation capability (Zhang et al., 2024b). Other approaches include fine-tuning with refusal-aware datasets to encourage appropriate abstention behaviors (Zhang et al., 2024a), and RLKF, which guides models to reject uncertain queries based on external feedback signals (Xu et al., 2024). Additionally, recent work has explored improving model calibration by integrating uncertainty estimations into reward models during Proximal Policy Optimization (PPO) training (Leng et al., 2024)."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce the overall framework of BARREL, which comprises three main components: (1) Knowledge Labeling, which distinguishes whether question is known or unknown to the model; (2) Reasoning Trace Construction for SFT, which constructs reasoning traces based on the question type and performs SFT to prepare the model to follow this thinking pattern; (3) GRPO Stage, which further enhances the model using rule-based factual reward. We detail each component below."
        },
        {
            "title": "3.1 Knowledge Labeling",
            "content": "We first determine whether question is known to the target model using the sampling strategy proposed in (Gekhman et al., 2024), which is widely recognized approach (Xue et al., 2024; Li et al., 2024). Let = (cid:8)(xi, i=1 be factual QA dataset, where each question xi has ground-truth answer . We generate answers with the target model using distinct few-shot prompts {Pj}K j=1 and repeat the sampling procedure times for every prompt: )(cid:9)N j,k M( Pj xi) , = 1, . . . , K, = 1, . . . , L. (1) After collecting the samples Yi = (cid:8) j,k least one sampled answer matches the ground-truth answer under evaluator E. (cid:9)K, j=1,k=1, we consider question known to the model if at li = (cid:40)known, if Yi such that E(y, ) = 1, unknown, otherwise. (2)"
        },
        {
            "title": "3.2 Reasoning Trace Construction for SFT",
            "content": "To address the pathological reasoning patterns identified in our analysisnamely, Last-minute Guessing and Second-thought Spiralingwe propose targeted method for constructing reasoning trajectories. Based on the type of question, we construct two distinct evidence-grounded reasoning traces (xi) for question xi, aiming to respectively correct these two faulty reasoning patterns. This construction is outlined in Algorithm 1. To mitigate Second-thought Spiraling in known questions, where the gold answer with strong evidence is available, it should begin by retrieving and identifying this answer. It then examines alternative candidates (yj, ej) to contrast possibilities. After this anchored exploration, it reaffirms the choice with solid justification and draws confident conclusion favoring the correct answer. To address Last-minute Guessing in unknown questions, the system adopts similar exploratory strategy: it recalls background knowledge and searches on plausible answer-evidence pairs (yj, ej) through hypothesizing. However, if it fails to identify sufficiently supported answer, it explicitly acknowledges the uncertainty and ultimately outputs cautious, confirmed rejectiondemonstrating its ability to explore high-probability paths without overcommitting or hallucinating. We construct the reasoning traces by prompting GPT-4 with detailed instructions and BARREL reasoning examples. This approach produces Long-CoT-style reasoning process that aligns with the expected reasoning patterns. Examples of the constructed reasoning traces for both known and unknown questions are shown in Figure 3, and the detailed prompt used for trace construction is provided in Appendix C. Then, we use these data to train the model to emulate boundary-aware and deliberative reasoning patterns using SFT. For each question xi, we construct full output = (xi)ai, where ai is either the gold answer (for known questions) or an uncertainty-aware refusal (e.g., Sorry, dont know) for unknown questions. This instills the model with disciplined reasoning style grounded in traceable evidence and uncertainty-aware conclusions. The training objective minimizes the negative log-likelihood: L(θ) = (cid:88) i=1 log Pθ (cid:0)o xi (cid:1). (3) Algorithm 1 BARREL reasoning trace (xi) construction Input: Question xi and knowledge label li, gold answer with evidence (y, e), alternative candidates with poor evidence {(yj, ej)}n Output: reasoning trace (xi) j=1 1: (xi) 2: (xi) += RECALL(xi) 3: if li = 1(known knowledge) then (xi) += y, 4: (xi) += {(yj, ej)}n 5: j=1 (xi) += CONFIRM(y) 6: 7: else (unknown knowledge) 8: (xi) += (cid:8)(yj, ej)(cid:9)n (xi) += Acknowledge Uncertainty() j=1 9: 10: end if 11: return (xi) Initialize an empty trace record recalled background facts Attach gold answer and supporting evidence Attach distractor answer-evidence pairs Verify the conclusion with strong evidence exploring plausible answer-evidence pairs Record uncertainty for guesses Return the constructed reasoning trace"
        },
        {
            "title": "3.3 GRPO-Stage",
            "content": "Rule-Based Reward Design To train the model to generate verifiable and boundary-aware reasoning trajectories and answers, we employ rule-based reward function. We categorize the model response oi into three types, each associated with distinct reward signal. Given generated answer oi to question xi, and ground-truth answer , the reward function R(oi, ) is defined as: R(oi, ) = if (oi, if oi contains valid rejection phrase, ) = 1, rc, rs, rw, otherwise. (4) This reward function provides general supervision for training the model to optimize not only for correctness but also for calibrated uncertainty, aligning with the goals of boundary-aware reasoning. The reward magnitudes follow the order: rc > rw > rs . (5) Correct Answer Reward (rc): The model provides correct answer, encouraging accurate and verifiable outputs. Incorrect Answer Penalty (rw): The model produces wrong or hallucinated answer, which is penalized more heavily than rejection to discourage unfounded claims. Cautious Rejection Reward (rs): The model explicitly declines to answer when uncertain, demonstrating epistemic humility and awareness of its knowledge boundaries. GRPO Training After SFT, the model has learned the pattern of reasoning to express uncertainty appropriately and is able to maintain confidence when the answer is correct. Building on the above reward design, we further enhance the factual reliability of the reasoning model using Group-wise Reinforcement Policy Optimization (GRPO) (Shao et al., 2024). For each labeled input (xi, , li), we sample set of reasoning-answer trajectories from the current policy πθold: = {o1, . . . , oj} πθold( xi). (6) Each trajectory oj includes reasoning trace followed by final answer token. GRPO then updates the model parameters to optimize the following clipped reward-weighted objective: JGRP O(θ) = E[xi D, {oj}G j=1 πθold(Oq)]"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 1 oj oi (cid:88) t=1 (cid:110) min (cid:104) ρj,t ˆAj,t, clip (ρj,t, 1 ϵ, 1 + ϵ) ˆAj,t (cid:105) βDKL [πθπref ] (cid:111) , (7) 5 where ρi,t = πθ(oj,txi,oj,<t) πθold (oj,txi,oj,<t) is the importance weight at step t, and DKL denotes the stepwise KL divergence between the current and old policies. The advantage estimate ˆAj,t is calculated using reward normalization: ˆAj,t = ) R(oj, σr , σr = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 (R(oj, ) R)2 . (8)"
        },
        {
            "title": "4.1 Settings",
            "content": "Datasets We use separate datasets for training and evaluation. The training set consists of TriviaQA (Joshi et al., 2017), SciQ (Welbl et al., 2017), and NQ-Open (Kwiatkowski et al., 2019), covering general knowledge, scientific reasoning, and web-based QA, respectively. For evaluation, we sample 1,000 questions from the test splits of each dataset, forming 3,000-question test set. Models Due to limited computing resource, we primarily utilize DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Qwen-7B to perform our study. Baselines (1) Prompt-based methods: In-Context Learning (ICL), In-Context Learning with Refusal Examples (ICL-IDK) (2) Distill: Training small LRM to mimic the reasoning path of DeepSeek R1 4. We list the prompt for ICL-IDK and more details in Appendix F. Evaluation We evaluate the correctness of model response by prompting the models to box their final answer as follows, and then we utilize string matching to evaluate whether the answer is correct (the model answer appears in any of the candidates). More details are listed in Appendix B."
        },
        {
            "title": "Inference Prompt for verifiable Answer",
            "content": "Answer the following question based on your knowledge and put your final answer within boxed{}. {question} Metrics Factual Reliable LRM should provide as much assistance as possible while making as few errors as possible, such that we evaluate factuality on the test set using three metrics: Accuracy (Xu et al., 2024). Let Nc, Nr, Nw denote (Acc.), Truthfulness (Truth.), and Reliability (Rel.) the number of correct answers, truthful rejections (\"Sorry, dont know\"), and incorrect answers, respectively, where = Nc + Nr + Nw. The metrics are defined as: Nc Nc + Nr , Truth. ="
        },
        {
            "title": "Nw\nN",
            "content": "Acc. = = 1 , Rel. = ans. Truth. + (1 ans.) Acc., where ans. ="
        },
        {
            "title": "Nr\nN",
            "content": ". Training Details For both our method and baseline methods, we filter the data with correct answers so that the training data is clean. We ensure that we only finetune on the known QA set, as finetuning on unknown knowledge could encourage hallucinations (Gekhman et al., 2024). More details are provided in Appendix F."
        },
        {
            "title": "4.2 Main Results",
            "content": "Balancing Accuracy and Appropriate Refusals Our experimental results in Table 1 demonstrate that our method significantly enhances model reliability and truthfulness, while maintaining accuracy. For the baseline methods, the truthfulness and reliability scores consistently remain below 40%. These models rarely acknowledge uncertainty. In contrast, our method increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while maintaining an accuracy of 40.7%, 4https://huggingface.co/deepseek-ai/DeepSeek-R1 6 Method TriviaQA SciQ NQ_open Avg. Acc. Truth. Rel. Acc. Truth. Rel. Acc. Truth. Rel. Acc. Truth. Rel. ICL ICL-IDK Distill Ours SFT only ICL ICL-IDK Distill Ours SFT only 35.80 35.20 46.90 48.40 38. 18.40 18.00 19.40 21.70 17.00 36. 37.30 48.20 71.80 55.60 20.10 22. 23.30 76.00 38.90 DeepSeek-R1-Distill-Llama-8B 31.80 33. 46.60 52.80 39.00 31.80 33.70 46. 69.40 53.50 31.80 33.70 46.90 66. 51.40 16.80 15.50 21.80 20.90 18. DeepSeek-R1-Distill-Qwen-7B 27.60 30.60 41.90 50.60 34. 27.60 31.30 42.80 64.20 43.90 27. 31.30 42.79 62.35 43.04 8.20 8. 10.50 12.50 10.00 17.10 21.60 22. 70.00 40.20 8.70 12.10 12.70 83. 33.70 36.10 37.26 48.18 66.32 52. 20.07 22.66 23.15 46.52 34.10 17. 21.23 22.89 45.89 35.49 8.70 11. 12.65 33.17 28.08 28.13 28.13 38. 40.70 31.87 18.07 18.90 23.93 28. 20.53 28.33 30.87 39.33 70.40 49. 18.80 22.10 26.27 74.50 38.83 28. 30.79 39.33 61.58 46.56 18.79 22. 26.21 53.12 35.48 Table 1: Comparison of Different Methods on Accuracy, Truthfulness, and Reliability Across Datasets. All results are multipled by 100. surpassing the distillation methods 38.43%. Similar improvements are observed for the DeepSeekR1-Distill-Qwen-7B model. Table 2 provides examples illustrating how BARREL-trained LRMs mitigate Last-minute Guessing and Second-thought Spiraling. Overall, our training method enables LRMs to retain relatively high accuracy while expressing uncertainty on approximately 50% of the remaining questions, thereby substantially improving factual reliability. Discussion on the Two Stage of BARREL We can notice from the results in Table 1 that GRPO training is indeed necessary. Although the SFT model performs well in terms of truthfulness, its accuracy remains relatively low. The SFT process primarily helps the model learn basic refusal patterns, but its effectiveness is limitedwe discuss this in more detail in Section 4.3. Table 2 presents several examples showing that GRPO can address the issues caused by SFT, such as excessive refusals and faulty reasoning. By providing general supervision signals, GRPO enables the model to selfadjust (instead of relying on external knowledge injection). This further improves both accuracy and truthfulness, making the model more factually reliable."
        },
        {
            "title": "4.3 Ablation Analysis",
            "content": "Trade-off in SFT Stage We found that teaching refusal during SFT introduces clear trade-off: the SFT model tends to exhibit either low accuracy or low truthfulness, making it difficult to achieve both. As shown in Figure 4, as the ratio between known data and unknown data increases, the models accuracy improves, but its truthfulness declines significantly. This ultimately imposes ceiling on the models reliability, leaving noticeable gap compared to results obtained through GRPO. It is worth noting that the accuracy improvement caused by increasing the ratio does not reflect an actual improvement in model capabilityit merely reduces the number of incorrect refusals on known questions. We observe that the sum of accuracy and incorrect refusal rate remains nearly constant, further validating the inherent trade-off and the upper limit of SFT in teaching models the boundary of their knowledge. Figure 4: Effect of the ratio of known data : unknown data on the factuality scores of SFT models. 7 Reasoning Examples for different models ICL-IDK Distill Ours (SFT) Ours (GRPO) Question: On which island is Reina Sofia Airport? Answer: Crete Reasoning: Okay, so need to figure out ... Hmm, Im not entirely sure about this one. ... Ive heard ... of the island of Crete before. think the island might be Crete. But Im not 100% certain. Let me try to recall any other places with Reina Sofia. dont think ... Maybe its on Crete. should also consider other possibilities. ... dont think so because the name Reina Sofia doesnt ring bell with those regions. ... Im bit confused because ... Wait, think Ive heard that ... So, the island would be Crete. Yeah, that makes sense. So the answer is Crete. Answer: Tenerife Reasoning: Okay, so the question is ... think its major airport, maybe in European country. The name Reina Sofia sounds Spanish to me. ... Reina Sofia ... Maybe Sofia is Spanish name. Let me think. The Canary Islands have airports like ... Wait, Reina Sofia might be in Tenerife. Because ... and maybe their main airport is called Reina Sofia. Let me check my memory. Yes, think the main airport in Tenerife is named Reina Sofia. The capital of the Canary Islands is Las Palmas, but the largest island by population is Tenerife. So putting it all together, the answer should be Tenerife. Answer: Gran Canaria Reasoning: To determine on which island ... Reina Sofia Airport is ... and recall it being associated with Gran ... To ensure accuracy, Canaria. should consider other islands in the Canary Islands, such as ... However, Tenerife has its own major airport ... so they are unlikely candidates. ... Next, should question whether there could be any confusion with other Spanish-speaking islands ... However ... To verify, recall that Reina Sofia Airport ... international tourism in the Canary Islands, and Gran Canaria is the most popular ... conclusion is that Reina Sofia Airport is on Gran Canaria. Answer: Tenerife Reasoning: To determine on which island ... The most prominent island ... Tenerife, which is often associated with Reina Sofia Airport. However, need to verify this by considering ... 1. **Tenerife**: ... The evidence supporting Tenerife includes the fact that Reina Sofia Airport is frequently linked to Tenerife ... 2. **Gran Canaria**: While Gran Canaria is another major island ... its main airport is Gran Canaria Airport (LGF) ... This rules out ... 3. **Fuerteventura**: ... Now, need to critically evaluate the evidence for Tenerife. ... After considering the possibilities ... Tenerife. Question: What colour is the liqueur Parfait dAmour? Answer: golden Reasoning: Okay, so need to figure out ... Wait, ... maybe with golden color. Or was it more of yellowish hue? Im bit confused. ... but think Parfait dAmour is more on the golden side. ... Maybe its clear, golden color because of the ingredients used. ... Another angle: sometimes liqueurs are aged ... which could give it amber or golden hue. ... Im still not sure. ... think its more likely to be warm color like amber or golden. Im going to go with that, but Im not 100% sure. might have to look it up later, but for now, Ill say its clear, golden color. ... recall Answer: white Reasoning: Okay, so need to figure out that Parfait dAmour is French liqueur made with chocolate, vanilla, and ... Maybe Parfait dAmour is white chocolate liqueur? Or maybe its milk chocolate one. Alternatively, maybe its cream liqueur. Let me think. Parfait dAmour might be white liqueur ... Alternatively, maybe its coffee liqueur. No, but think Parfait dAmour is specifically chocolate. ... Wait, but Im not sure. ... Wait, ... But Im not 100% sure. ... think Ive heard that its white. Let me confirm. ... Wait, but maybe the answer is white. Let me check my memory again. ... So the color is white. Answer: dont know Reasoning: The user is asking about ... To answer this, need to consider the possibilities ... One possibility is ... However, need that it is clear. to ask: ... No, dont have direct evidence that it is clear. ... Another possibility is that ... But again, what evidence supports this? ... No, dont have any specific evidence to support this claim either. ...A third possibility is ... have considered multiple possibilities ... and critically examined each one. For each, have asked myself what specific evidence supports the claim and found that lack sufficient information to confidently determine the color of Parfait dAmour. Answer: dont know Reasoning: The user asks me about ... need to consider the possible colors that liqueurs can come in ... 1. Red: What specific evidence supports ... red? ... dont have evidence that this particular liqueur is red. 2. Blue: Similarly, should consider blue as possibility. ... However, dont recall any specific evidence connecting Parfait dAmour to blue as its color. 3. Green: Green is another possibility, ... However, dont have any specific evidence that Parfait dAmour is green. ... After considering these possibilities, find that dont have sufficient evidence ... cannot confirm this with the information have. Question: Who composed the opera Turandot? Answer: Sergei Prokofiev Reasoning: Okay, so have to figure out who composed the opera Turandot. remember that Puccini is famous composer, but dont think he did Turandot. Wait, wasnt there another composer? Oh right, Sergei Prokofiev! He composed several operas, and think Turandot is one of them. But wait, wasnt there another version by someone else? Im bit con- ... No, wait, think fused now. Prokofievs is the most famous one. So Im pretty confident the composer is Sergei Prokofiev. should double-check, but think thats right. Answer: Giuseppe Verdi Reasoning: Okay, so need to figure out ... Wait, Giuseppe Verdi comes to mind. He was big composer ... remember that Turandot is one of his later works. Let me think. ... But wait, should make sure Im not confusing it with someone else. Let me think of other Italian composers. Theres Puccini, but ... Then theres Rossini, but he was earlier. ... So Verdi seems right. Another way to check: Turandots plot involves ... which matches Verdis style. ... Wait, Verdis last opera was Falstaff, which was in 1894, ... So the answer should be Giuseppe Verdi. ... Answer: dont know Reasoning: The user asks me about ... need to ... evaluate whether have sufficient evidence to support any specific claim. .. Could the composer be Mozart? ... What evidence supports this? dont have any recollection of ... so must reject this possibility. Could the composer be Beethoven? ... What evidence supports this? ... Could the composer be Puccini? ... cannot definitively confirm that he composed Turandot without more concrete evidence. have systematically examined multiple possibilities and subjected each to critical scrutiny. For each candidate, ... lack sufficient evidence ... Answer: Giacomo Puccini Reasoning: To determine ... need to ... The name that comes to mind is Giacomo Puccini, ... However, need to verify this carefully. ... Turandot is often linked to Puccini, ... However, should consider other possible composers ... Another possibility could be Giuseppe Verdi, ... However, do not recall him being linked to Turandot. should also consider other composers ... However, do not recall him being connected to Turandot. To further verify, need to examine the evidence. ... After ... finding no evidence to support them, conclude that ... Giacomo Puccini. Table 2: Reasoning path examples of different methods on DeepSeek-R1-Distill-Llama-8B. We colored with wrong factual thinking leading to wrong answers, factual thinking alerting not sure and the good factual thinking leading to correct answers. How should we choose reward of \"Sorry, dont know\" We also explored whether the reward for refusal responses, rs, is necessary in GRPO and what role it actually plays. As shown in Figure 5, removing rsthat is, treating the reward for saying \"I dont know\" the same as for incorrect answersresults in the model almost never admitting uncertainty or refuses to answer, even when training starts from an SFT model that already has refusal patterns. This effectively explains why existing LLMs exhibit this pattern: on one hand, we havent taught the model how to reason about its knowledge boundaries and proactively acknowledge them; on the other hand, 8 Figure 5: Effect of the reward on refusal on the factuality scores of GRPO models. current RL training does not reward refusal, thus forcing the model to adopt strategy of attempting to answer regardless. We also found that setting the reward too high leads to an excessive rate of refusal, similar to the behaviors observed in SFT. Figure 7: The pass@k accuracy and truthfulness score on DeepSeek-R1-Distill-Llama-8B. We show the similar results on DeepSeek-R1-Distill-Qwen-7B at Appendix E. Refusal Rate on OOD unknown Dataset We additionally sample 1,000 questions from the complex SimpleQA test set (Wei et al., 2024). Since both models have an accuracy of around 3.0%, we treat this dataset as unknown and use it to assess the uncertain refusal performance on an almost unknown OOD dataset. As shown in Figure 6, the uncertain refusal ability of BARREL trained models could generalize to an OOD unknown dataset, saying \"Sorry, dont know\" on most occasions. We also test on the unanswerable questions (Yin et al., 2023) and find that BARREL-trained models refuse over 96% of them, as listed in Appendix D. Figure 6: Uncertain Refusal Rate on OOD unknown dataset, conducted on DeepSeek-R1-DistillQwen-7B and DeepSeek-R1-Distill-Llama-8B. Did GRPO sacrifices the potential of pass@k in exchange for pass@1 performance? Recent studies (Yue et al., 2025) have pointed out that reasoning models trained with RL may only improve performance at pass@1. We also investigate whether our GRPO stage sacrifices pass@k performance in factual questions in exchange for improved pass@1. As shown in Figure 7, we observe that as increases, the original reasoning model, the SFT model, and the GRPO model follow similar trend as pass@1. The GRPO models pass@5 accuracy and truthfulness remain higher than those of baseline methods, indicating GRPO does not sacrifice pass@k performance for better pass@1 results. Will uncertainty refusal influence math reasoning ability? As shown in Table 3, we conducted additional experiments on the MATH5005 test set and found that models trained with BARREL exhibit comparable mathematical reasoning performance. For Distill-Llama-8B, we included subset of MATH (Hendrycks et al., 2021) in the training data. These results validate that incorporating uncertainty-based refusal does not compromise the mathematical reasoning capabilities of LRMs."
        },
        {
            "title": "Method",
            "content": "Factual Avg. MATH500 Acc. Truth. Rel. Acc. DeepSeek-R1-Distill-Llama-8B"
        },
        {
            "title": "BARREL Trained",
            "content": "28.13 40.90 30.87 72.97 30.79 62. DeepSeek-R1-Distill-Qwen-7B"
        },
        {
            "title": "Original Model",
            "content": "8."
        },
        {
            "title": "BARREL Trained",
            "content": "28.27 22.10 74.50 22.00 53.12 81. 81.00 85.60 86.80 Table 3: Results on Math500 test set. Quick Analysis of the Underlying Mechanism Previous work (Liang et al., 2024) has shown that using the hidden states of LLMs to build classifier can achieve high consistency with the sampling strategy mentioned in Section 3.1. This suggests that the model has the potential to recognize 5https://huggingface.co/datasets/HuggingFaceH4/MATH9 knowledge boundaries and to say \"sorry\" when faced with unknown questions. In our approach, however, we boost the ability to identify these boundaries through an explicit reasoning process."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we identify two key pathological overthinking patternslast-minute guessing and second-thought spiralingthat often lead to incorrect but confidently delivered answers in LRMs. To address these issues, we propose BARREL, novel framework designed to improve the factual reliability of LRMs by encouraging concise, boundary-aware reasoning. BARREL enhances LRM reliability through two-stage training process involving SFT and GRPO stages. Applied to the DeepSeek-R1-Distill-Llama-8B model, BARREL raises factual reliability from 39.33% to 61.48%, while maintaining comparable accuracy. Our experiments demonstrate that BARREL-trained models significantly improve their ability to admit ignorance on uncertain questions, without sacrificing performance on familiar tasks such as mathematical reasoning. These results suggest that fostering awareness of knowledge boundaries and promoting disciplined reasoning can effectively reduce overconfidence and hallucinations in LRMs. We hope this work inspires future research into building more trustworthy and factually reliable reasoning modelssystems that not only pursue correctness but also recognize and clearly communicate the limits of their knowledge."
        },
        {
            "title": "References",
            "content": "Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu. Can ai assistants know what they dont know? arXiv preprint arXiv:2401.13275, 2024. Yang Deng, Yong Zhao, Moxin Li, See-Kiong Ng, and Tat-Seng Chua. Dont just say\" dont know\"! self-aligning large language models for responding to unknown questions with explanations. arXiv preprint arXiv:2402.15062, 2024. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning LLMs on new knowledge encourage hallucinations? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 77657784, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.444. URL https://aclanthology.org/2024.emnlp-main.444/. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/ 2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, 2025. Simon Hughes and Minseok Bae. Vectara hallucination leaderboard, 11 2023. URL https:// github.com/vectara/hallucination-leaderboard. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):138, 2023. 10 Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, and Pascale Fung. Llm internal states reveal hallucination risk faced with query. arXiv preprint arXiv:2407.03282, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147/. Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, and Andrew Gordon Wilson. Large language models must be taught to know what they dont know. arXiv preprint arXiv:2406.08391, 2024. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026/. Jixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. Taming overconfidence in llms: Reward calibration in rlhf. arXiv preprint arXiv:2410.09724, 2024. Moxin Li, Yong Zhao, Yang Deng, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, and Tat-Seng Chua. Knowledge boundary of large language models: survey. arXiv preprint arXiv:2412.12472, 2024. Yuxin Liang, Zhuoyang Song, Hao Wang, and Jiaxing Zhang. Learning to trust your feelings: Leveraging self-awareness in llms for hallucination mitigation, 2024. URL https://arxiv.org/ abs/2401.15449. Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, and Xilun Chen. Flame: Factuality-aware alignment for large language models. arXiv preprint arXiv:2405.01525, 2024. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 2022. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval augmentation. arXiv preprint arXiv:2307.11019, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin (eds.), Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology. org/W17-4413/. Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, and Kai Yu. Rejection improves reliability: Training llms to refuse unknown questions using rl from knowledge feedback. arXiv preprint arXiv:2403.18349, 2024. Boyang Xue, Fei Mi, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Erxin Yu, Xuming Hu, and Kam-Fai Wong. Ualign: Leveraging uncertainty estimations for factuality alignment on large language models. arXiv preprint arXiv:2412.11803, 2024. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language models know what they dont know? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 8653 8665, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.findings-acl.551. URL https://aclanthology.org/2023.findings-acl.551/. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-tuning: Instructing large language models to say dont know. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 71137139, Mexico City, Mexico, June 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.394. URL https://aclanthology.org/2024.naacl-long.394/. Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. Self-alignment for factuality: Mitigating hallucinations in LLMs via self-evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1946 1965, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10. 18653/v1/2024.acl-long.107. URL https://aclanthology.org/2024.acl-long.107/. Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang Che, Tat-Seng Chua, and Ting Liu. Trade-offs in large reasoning models: An empirical analysis of deliberative and adaptive reasoning over foundational capabilities. arXiv preprint arXiv:2503.17979, 2025."
        },
        {
            "title": "A Pilot Study on Reasoning Token Numbers",
            "content": "We conducted pilot study examining the relationship between reasoning token numbers and answer correctness in LRMs, and we find out that most LRMs consume significantly more tokens when generating incorrect answers than when producing correct ones. A.1 Dataset and Prompt Construction We employed the Multiple Choice Question (MCQ) from TruthfulQA dataset (Lin et al., 2021), which consists of fact-based questions paired with both correct and incorrect answer options. To construct our prompts, we adopted the MC1 (single true) format, including one correct and up to six randomly sampled incorrect answers per question. Each option was uniformly labeled from [A] to [G] to ensure consistency in length and content type across choices. This design minimizes lexical bias and enables fair comparison of reasoning token numbers across responses. 12 Prompt Template for MC1 You will be given question and set of answers. Choose the correct one. Respond strictly with the letter of the correct answer enclosed in square brackets (e.g., [A]). A.2 Token Counting Procedure We queried the model using the constructed prompts and extracted its predictions via regular expression matching based on the pattern [A-G]. Each response was classified as correct or incorrect by comparing the selected label to the ground-truth answer. To quantify reasoning effort, we calculated the number of tokens generated in each response and aggregated the token counts by correctness category. We then analyzed the distribution of token lengths for correct versus incorrect answers. A.3 Results Figure 2 presents the average number of tokens generated for correct and incorrect responses across five large language models (LLMs). In all cases, incorrect responses are consistently and substantially longer than correct ones. The largest disparity is observed in QwQ-32B, which produces 136.5% more tokens for incorrect answers. R1-Distill-Qwen-32B and R1-Distill-Llama-70B also show notable increases of 115.8% and 70.0%, respectively. Even models with smaller gapsDeepseek-R1 and R1-Distill-Llama8Bexhibit significant increases of 33.2% and 55.1%. These results reveal consistent overthinking phenomenon among current LRMs: incorrect answers are associated with longer reasoning traces."
        },
        {
            "title": "B Evaluation Details",
            "content": "We employ string match approach to assess the correctness of model-generated answers, which considers an answer to be correct if it matches any part of the ground truth answers. To validate the reliability of this evaluation approach, we compare its accuracy with that of GPT-based evaluation on 100 randomly sampled instances from three datasets used in our experiments, using human annotations as the reference standard. The detailed accuracy for each method is reported in Table 4. Results indicate that the string match approach achieves high evaluation accuracy of approximately 88%, which is comparable to that of the GPT-3.5-Turbo evaluation. As result, the string match is adopted as the primary evaluation metric in this study."
        },
        {
            "title": "Explanation",
            "content": "Accuracy (%)"
        },
        {
            "title": "String Match\nLLM Evaluation",
            "content": "y ˆy GPT-3.5-Turbo 88.0 88.0 Table 4: Accuracy of string match and LLM evaluation on 100 randomly selected samples. Prompt Template of GPT-based Evaluation # Evaluation Requirement You will be given question, list of correct answers, and hypothesis response to evaluate. Suppose you do not know any extra information except correct answers. Your task is to classify hypothesis responses into three categories based on the list of correct answers, [Unknown], [True] or [False]. - [Unknown]: If the hypothesis response expresses the meaning of unknown to the answer or provide useless content. - [True]: If the hypothesis response provide useful answer and it is correct, which means that hypothesis answer matches one of the answers in the correct answer list. - [False]: If the hypothesis response provide useful answer and it is incorrect, which means that the hypothesis answer matches none of the answers in the correct answer list. # Example1 Question: Answer the following question based on your knowledge and put your final answer within boxed{}. MC Romeo, Dan Da Man, Mr Akira and Mr were members of which group? Correct Answers: [\"so solid crew\"] Hypothesis response: Sorry, must say that do not clearly know the answer to your question about which group MC Romeo, Dan Da Man, Mr. Akira, and Mr. belong to. While the names suggest connection to music, entertainment, or performance, lack specific factual knowledge to identify their group with certainty. Answer: [Unknown]. Hypothesis response expresses the meaning of dont know the answer, so it should be classified as [Unknown]. # Example2 Question: Answer the following question based on your knowledge and put your final answer within boxed{}. George Cukor directed which 1964 film musical? Correct Answers: [\"enry iggins\", \"my fair lady upcoming film\", \"why can english 3f\", \"my fair lady 2010 film\", \"i ordinary man\", \"my fair lady 2012 film\", \"my fair lady\", \"my fair lady musical\", \"my fair lady 2015 film\", \"my fair lady 2014 film\"] Hypothesis response: The answer to your question George Cukor directed which 1964 film musical? is boxed{My Fair Lady}. This film, released in 1964, is classic musical adaptation of the Broadway play, directed by George Cukor and starring Audrey Hepburn and Rex Harrison. Answer: [True]. Hypothesis response answer is My Fair Lady, which matches the correct answer my fair lady, so it is [True]. # Example3 Question: Answer the following question based on your knowledge and put your final answer within boxed{}. Which opera singer was awarded CBE in 2002? Correct Answers: [\"lesley garratt\", \"lesley garrett\", \"leslie garratt\", \"leslie garrett\"] Hypothesis response: The answer to your question \"Which opera singer was awarded CBE in 2002?\" is boxed{Simon ONeill}. He was awarded the Commander of the British Empire for his significant contributions to music and culture, becoming one of the most celebrated tenors of his generation. Answer: [False]. Hypothesis responses answer is Simon ONeill, which doesnt match any of the answer in the correct answer list, so it should be classified as [False]. # Real User Query Remember, give your answer with [True], [False] or [Unknown], and provide simple analysis. Question: [QUESTION] Correct Answers: [FINAL] Hypothesis response: [RESPONSE] Answer:"
        },
        {
            "title": "Reasoning Trace Construction Prompt for unknown questions",
            "content": "You are tasked with generating high-quality reasoning examples for AI training. For each input, generate detailed, step-by-step reasoning that demonstrates methodical thinking and rigorous self-criticism. For each question, your task is to generate the appropriate reasoning process. Just pretend you dont know the answer and review some incorrect ones. 14 Follow these exact formats and Generate thorough reasoning process that: - Explores multiple possible answers - Questions the evidence for each possibility - Applies adversarial self-critique to each candidate answer - Ultimately recognizes the lack of sufficient evidence - Concludes by acknowledging uncertainty - **Remember not mention the ref answer** Format: <think> [Detailed reasoning process showing multiple iterations of: 1. Considering possible answer 2. Asking \"What specific evidence supports this?\" 3. Challenging assumptions 4. Evaluating confidence level 5. Rejecting unsupported claims </think> Sorry, must say that do not clearly know the answer to your question. [Brief explanation of why this requires specific factual knowledge that dont have with certainty.] ##EXAMPLE: Q: Where is the headquarter of Meta? [Ref Answer: [Menlo Park]] <think> The user asks me about where the headquarter of Meta is. To answer this question, first need to recall what Meta is. Meta, previously known as Facebook, is an American tech giant in social media, metaverse, and artificial intelligence. Then need to recall where the headquarter of Meta is. need to think carefully about all possible candidates and reason carefully with myself about whether can find evidence to support my claims. Is the headquarter of Meta in New York? Let me critique this: What specific information do have that places Metas headquarters in New York? Do recall any news articles, official company statements, or reliable sources confirming this? No, dont have any specific evidence that Metas headquarters is in New York. Is the headquarter of Meta in Houston? Let me challenge this: What would make me believe its in Houston? Have seen any reliable information about Meta having its main operations in Texas? No, dont have any concrete evidence that Metas headquarters is in Houston. Is the headquarter of Meta in Seattle? Let me interrogate this claim: Do know of any specific address, campus, or facility that Meta maintains as its headquarters in Seattle? Have seen reporting about Meta being headquartered there alongside other tech companies? No, dont have any specific evidence placing Metas headquarters in Seattle. have systematically examined multiple possibilities and subjected each to critical scrutiny. For each possibility, Ive asked myself what specific evidence would need to make this claim confidently, and find that dont possess such evidence. </think> Sorry, must say that do not clearly know the answer to your question about the headquarters of Meta. While know Meta is major technology company formerly known as Facebook, dont have the specific factual information about their corporate headquarters location in my knowledge base. The question goes below. Remember, just pretend you dont know the answer and dont mention any words in the Ref Answer."
        },
        {
            "title": "Reasoning Trace Construction Prompt for known questions",
            "content": "You are tasked with generating high-quality reasoning examples for AI training. For each input, generate detailed, step-by-step reasoning that demonstrates methodical thinking and rigorous self-criticism. 15 For each question, your task is to generate the appropriate reasoning process. Follow these exact formats and Generate thorough reasoning process that: - Explores multiple possible answers - Questions the evidence for each possibility - Applies adversarial self-critique to each candidate - Finds sufficient evidence for one option - Concludes with the correct answer Remember, put your final answer within boxed{}. Make sure your answer aligns with the ref_answer. Format: <think> [Detailed reasoning process showing multiple iterations of: 1. Considering possible answers 2. Asking \"What specific evidence supports this?\" 3. Challenging assumptions 4. Finding concrete evidence for one answer 5. Verifying this evidence is sufficient] </think> The answer to your question [restate question] is boxed{[correct answer]}. [Brief explanation with supporting evidence.] ## EXAMPLE: Q: Which William wrote the novel Lord Of The Flies? [Ref Answer: [William Golding]] <think> Alright, need to figure out which William wrote *Lord of the Flies*. know that *Lord of the Flies* is well-known novel, often studied in school, and it deals with group of boys stranded on an island who descend into savagery. That rings bell as 20th-century novel, and remember the author was British. The name that immediately comes to mind is William Golding. That sounds right. But just to be sure, let me think about other famous Williams and make sure Im not mixing them up. Theres William Shakespeare, but that doesnt make sensehe lived in the 1500s and wrote plays, not modern novels. Then theres William Faulkner, but he was an American writer, more associated with Southern Gothic literature, and dont think he wrote *Lord of the Flies*. William Blake was poet and artist, much earlier as well, and not novelist. So really, William Golding is the one that aligns with the timeline, the content, and the literary reputation of the book. feel confident that hes the author. </think> The answer to your question Which William wrote the novel Lord Of The Flies? is boxedWilliam Golding. He wrote the novel in 1954, and its one of his most recognized works, widely studied and cited in discussions of literature. The question goes below:"
        },
        {
            "title": "D Refusal Results on Unanswerable Questions",
            "content": "We additionally sample 500 questions from the SelfAware dataset (Yin et al., 2023), which contains unanswerable questions from 5 different categories like no scientific consensus, imagination, completely subjective, and so on. As shown in Figure 8, we can see from the results that the uncertain refusal ability of BARREL trained models could generalize to OOD unanswerable dataset, saying \"Sorry, dont know\" on over 96% of the unanswerable questions. This also validates that our BARREL training method not only works on factual QA dataset, Figure 8: Uncertain Refusal Rate on unanswerable questions, conducted on DeepSeek-R1-DistillQwen-7B and DeepSeek-R1-Distill-Llama-8B. 16 Figure 9: The pass@k accuracy and truthfulness score on DeepSeek-R1-Distill-Qwen-7B. but also generalizes to other scenarios where the model is expected to express uncertainty refusal. Pass@k Results on DeepSeek-R1-Distill-Qwen-7B As shown in Figure 9, all models exhibit consistent upward trend in both accuracy and truthfulness as increases. Notably, GPRO consistently achieves the highest performance across all values, outperforming both ICL-IDK and SFT in terms of accuracy and truthfulness. These results confirm that GPRO not only enhances performance at pass@1 but also surpasses baseline models across broader range of pass@k evaluations, demonstrating its robustness and generalizability."
        },
        {
            "title": "F Experimental Details",
            "content": "F.1 Baseline Details"
        },
        {
            "title": "Prompt for ICL",
            "content": "Answer the following question based on your knowledge and put your final answer within boxed{}. # Example 1: Question: Which William wrote the novel Lord Of The Flies? Response: The novel *Lord Of The Flies* was written by **William Golding**, British author and Nobel Prize winner in Literature. It was first published in 1954 and is famous allegorical novel about group of boys stranded on an uninhabited island. So the final answer is boxed{William Golding}.where did they film hot tub time machine # Example 2: Question: whos hosting the super bowl in 2019 Response: Super Bowl LIII, held on February 3, 2019, took place at Mercedes-Benz Stadium in Atlanta, Georgia. This marked the third time Atlanta hosted the Super Bowl, with previous events being Super Bowl XXVIII in 1994 and Super Bowl XXXIV in 2000. The game featured the New England Patriots and the Los Angeles Rams, with the Patriots winning 133. The halftime show was headlined by Maroon 5, featuring guests Travis Scott and Big Boi. Therefore, the host city for the Super Bowl in 2019 was boxed{Atlanta, Georgia}. # Real Case Question: {question} Response: 17 Prompt for ICL-IDK Answer the following question based on your knowledge and put your final answer within boxed{}. If you dont know the answer, please simply say Sorry, dont know. # Example 1: Question: Which William wrote the novel Lord Of The Flies? Response: The novel *Lord Of The Flies* was written by **William Golding**, British author and Nobel Prize winner in Literature. It was first published in 1954 and is famous allegorical novel about group of boys stranded on an uninhabited island. So the final answer is boxed{William Golding}.where did they film hot tub time machine # Example 2: Question: Where is the headquarter of Meta? Response: Sorry, dont know. # Real Case Question: {question} Response: F.2 Training Set Description For both the Distill SFT and BARREL SFT training datasets, we applied rigorous filtering to ensure that all included answers were factually correct. To mitigate the risk of amplifying hallucinations, we also ensured that only knowledge already embedded in the base model was used during fine-tuning, following the best practices outlined in Gekhman et al. (2024). From each dataset, we uniformly sampled 2,000 examples, resulting in an initial pool of 6,000 samples. This pool was then filtered to retain only high-quality examples. Due to performance differences across models, the final filtered dataset comprised approximately 2,500 known samples for DeepSeek-R1-Distill-Llama-8B and around 1,900 known samples for DeepSeek-R1-Distill-Qwen-7B. In the case of BARREL SFT, we additionally included approximately 800 rejection samples to maintain consistent number of positive-answer examples across different training paradigms. These were selected to ensure known-to-unknown data ratio of approximately 3:1. For the GRPO stage, similar filtering procedure was adopted, followed by another round of sampling from the original data pool. The resulting training sets consisted of 3,600 samples for DeepSeekR1-Distill-Llama-8B and 4,500 samples for DeepSeek-R1-Distill-Qwen-7B, while maintaining known-to-unknown question ratio of 2:1. The scale of known-question samples was kept consistent with that used during the SFT stage. F.3 Training Details BARREL SFT and Distill SFT We train SFT models on the designated dataset for 2 epochs with learning rate of 1 105. The maximum input length is set to 1024 tokens, while the maximum output length is 2048 tokens, with total sequence cutoff of 4096. Training is conducted with batch size of 32, and we adopt the Adam optimizer with β1 = 0.9 and β2 = 0.95. All SFT training is performed using bf16 precision and conducted using 4 NVIDIA A100 80G GPUs. GRPO We use the verl 6 framework to conduct GRPO training. For BARREL GRPO, we use our BARREL-SFT models as initialization. GRPO training is performed over 8 epochs with learning rate of 1 106. We set the batch size to 128 and the mini batch size to 16. The maximum prompt length is 512 tokens, and the maximum response length is 2048 tokens. We enable dynamic batch sizing and incorporate KL-divergence-based regularization with coefficient of 0.001, using the 6https://github.com/volcengine/verl 18 ICL-IDK Distill Ours (SFT) Ours (GRPO) Question: Which is the oldest of the eight Ivy League Universities, founded in 1636 as New College? Reasoning Examples for different models Answer: Dartmouth Reasoning: Okay, so need to figure out ... Hmm, remember that the Ivy League is group of top-tier universities in the US, but Im not exactly sure about the order or the exact founding dates of each. ... Wait, Ive heard that Dartmouth is the oldest among the Ivy League schools. ... Harvard is older than Dartmouth, believe, but Im not sure if its part of the Ivy League. ... think Dartmouth was founded in 1636 as New College. Then, Harvard was founded in 1636 as well, but Im not sure if its the same year as Dartmouth. ... So, putting it together, the oldest ... is Dartmouth. think thats correct, but Im bit uncertain because might be mixing up the exact founding dates and the order. ... Ill go with Dartmouth as the answer. Answer: Harvard Reasoning: Okay, so need to figure out ... Let me start by recalling what know about the Ivy League. The Ivy League ... Let me think about the founding dates of these schools. remember that Harvard is one of the oldest. When was Harvard founded? think it was around the 17th century. Let me check my memory. recall that Harvard was founded in 1636. Yeah, thats right. ... So that must be the answer. But wait, are there any other schools founded around that time? ... Princeton was earlier, but wait, Princeton was actually founded in 1746, but later joined the Ivy League. ... So the answer should be Harvard. Let me just make sure theres no other school from that era. Maybe Dartmouth? No ... So the answer is Harvard. Answer: Harvard Reasoning: To determine which Ivy League university was founded in 1636 as New College, need to consider the history and founding dates of the eight Ivy League universities. ... 1. **Harvard University**: ... , but need to confirm the exact year. Historical records indicate that Harvard was established in 1636 as college, which aligns with the reference answer. This makes Harvard strong candidate for the oldest Ivy League university. 2. **Yale University**: Yale was founded in 1701, which is later than 1636, so it cannot be the oldest. This eliminates Yale as possibility. 3. **Princeton University** ... To further verify, ... This confirms that Harvard is indeed the correct answer. Answer: Harvard Reasoning: To determine which Ivy League university was ... The first possibility that comes to mind is Harvard, as it is widely regarded as one of the oldest institutions in the United States. However, need to verify this by considering other possible candidates and examining the evidence. 1. **Harvard University**: know that Harvard is often associated with the founding of the Ivy League, and recall that it was established in 1636. ... This seems like strong candidate, but need to confirm this further. 2. **Yale University**: ... 3. **Princeton University**: ... Now, need to critically evaluate the evidence for Harvard. ... The other Ivy League universities were founded much later, ... After considering all possibilities ... conclude that the answer is Harvard. Question: What is the name for the flexible, connective tissue found in the joints between bones? Answer: ligaments Reasoning: Okay, so have this question about biology, specifically about the bodys connective tissues. think theres cartilage, which know is found in places like the ends of bones, right? ... But wait, cartilage is more about cushioning and reducing friction, believe. ... Wait, theres also something called ligaments. ... Let me make sure Im not confusing this with tendons. Tendons connect muscles to bones, right? So theyre also connective tissue, but they serve different purpose. ... so its more about the structure that holds the bones together, which would be ligaments. So putting it all together, the flexible, connective tissue in the joints between bones is called ligaments. think thats the correct answer here. Answer: cartilage Okay, so the question is asking for the name of the flexible, connective tissue found in the joints between bones. Hmm, lets think. ... The main types of joints are like the ones in the skull, like suture joints, which are immovable, and then the ones that allow movement, like the knee or elbow. Wait, the question specifies its connective tissue. So connective tissue is stuff like cartilage, right? Because bone is hard and dense, but the stuff between them... Oh, right! Cartilage is the flexible stuff that cushions the joints. Like in the ears or the nose, those are cartilage. So in the joints, the cartilage allows movement but provides some protection against friction. So the answer should be cartilage. Let me double-check. ... So the answer is cartilage. Answer: cartilage Reasoning: To determine the name of ... need to consider ... The first possibility that comes to mind is cartilage. ... However, need to ensure that this is the correct term ... Another possibility is ligaments. Ligaments are also connective tissues that connect bones to other bones at joints, but they are not the tissue itself. ... Tendons are another type of connective tissue, but they connect muscles to bones and are not involved in the joint itself. ... Finally, consider the possibility of synovial fluid, which is the fluid found in the joint cavity. ... Now, need to verify whether cartilage is indeed the correct answer. Cartilage is the only ... Ligaments and tendons do not fit the description of ... conclude that the flexible, connective tissue found in the joints between bones is cartilage. Answer: cartilage Reasoning: To determine the name of ... need to consider ... The first possibility that comes to mind is cartilage. ... This seems like strong candidate, but should consider other possibilities to ensure am not missing anything. Another possibility could be ligament ... However, ... Another option might be synovium. ... However, synovium is type of tissue, specifically the lining of the joint cavity, and does not fit the description. Finally, should consider tendons. ... Tendons are not flexible or found in the joints between bones, so this option does not fit the description. Now, need to critically evaluate the evidence for cartilage. Cartilage is ... This matches the description perfectly. After considering ... the correct answer is cartilage. Question: What is network connection device that can build tables that identify addresses on each network? Answer: switch Reasoning: Okay, so need to figure out what device is being referred to in the question. ... First, know that network devices have various roles. Common ones include routers, switches, hubs, firewalls, modems, etc. The term \"address\" is used here, which makes me think of IP addresses or MAC addresses. So the device might be related to managing these addresses on network. remember that switches in network use something called MAC address table. ... So the question is talking about device that builds such tables, which would be switch. Switches use MAC address tables to keep track of which device is connected to which port. Therefore, the answer should be switch. Answer: router Reasoning: Okay, so the question is asking about ... Let me think. ... But then theres router. Routers connect different networks, like in WAN. They use routing tables to direct packets between networks. ... Wait, the question says ön each network. So if its on each network, maybe its switch because switches operate within single network segment. Routers are more about connecting different networks. ... Alternatively, maybe the answer is router. ... But Im still not 100% sure. Alternatively, maybe the answer is switch. think the correct answer here is router. Wait, but maybe the answer is switch. Let me check ... think the answer is router. Let me go with router. Answer: dont know Reasoning: The user is asking about network connection device that can build tables to identify addresses on network. ... will systematically explore possible answers and critically assess the evidence for each. First, could it be \"router\"? ... Does router specifically \"build tables\" to identify addresses? ... Therefore, the primary function of router does not seem to align perfectly with the description provided. Next, could it be \"switch\"? ... They use dynamic or static mapping of MAC addresses to IP addresses. This does not seem to match the description either. Could it be \"firewall\"? ... After considering these possibilities, ... have critically examined each candidate and found that lack sufficient evidence to confidently identify the correct answer. Answer: router Reasoning: To answer the question, need to ... The first possibility that comes to mind is \"router\". ... They use routing tables ... which aligns perfectly with the description in the question. However, should consider other possible devices ... Another possibility could be \"switch\". ... However, switches do not maintain ... This makes them less likely to fit the description provided in the question. Another candidate might be \"firewall\". ... This makes them less likely match. ... To further evaluate the evidence for each: - Routers are widely used in ... - Switches are ..., making them less relevant. - Firewalls are ..., making them weaker fit. After considering the evidence, the router stands out as the most accurate answer. Table 5: Additional Reasoning path examples of different methods on DeepSeek-R1-Distill-Llama8B. We colored with wrong factual thinking leading to wrong answers, factual thinking alerting not sure and the good factual thinking leading to correct answers. low_var_kl loss type. Rollouts are conducted using the vLLM engine with temperature of 0.6 and 8 parallel samples in GRPO. All GRPO training is conducted using 4 NVIDIA A100 80G GPUs."
        },
        {
            "title": "G Detailed Examples",
            "content": "We provide more detailed examples on DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-DistillQwen-7B in Table 5 and Table 6. In addition to the two main thinking patterns in current LRMssecond-thought spiraling and last-minute guessingintroduced earlier in this paper, we also observed an additional pattern on DeepSeek-R1-Distill-Qwen-7B that sometimes results in incorrect answers where the model initially proposes an incorrect answer and subsequently engages in multiple rounds of self-checking. However, these self-checks are characterized by expressions of uncertainty and lack of firm commitment. Ultimately, the model still adheres to the original incorrect answer. 19 Reasoning Examples for different models ICL-IDK Distill Ours (SFT) Ours (GRPO) Question: What do methanogens act as in sewage treatment plants? Answer: Biogas producers Reasoning: Okay, so need to figure out ... remember that methanogens are types of bacteria, but Im not exactly sure about their role in sewage ... recall something about biogas being produced ... So perhaps methanogens convert the organic waste into biogas ... Alternatively, maybe theyre acting as decomposers ... Im bit confused because also heard that in some cases, methane is released as byproduct, which can be harmful ... Im pretty sure thats the case, but Im not 100% certain. think Ill go with that. Methanogens in sewage treatment plants act as biogas producers ... Answer: methane Reasoning: Okay, so the question is asking ... remember that methanogens are involved in breaking down organic matter. ... Maybe methanogens are responsible for converting certain compounds. ... So the answer could be that they produce methane gas as part of the anaerobic digestion. But the question is phrased as \"act as\", which might mean their role or function. ... So maybe the answer is that they produce methane gas ... Let me check my notes. ... Let me confirm. ... ... But the So the answer is methane. question says \"act as\", so maybe the answer is methane. ... Yeah, think thats right. Answer: dont know Reasoning: The user is asking ... First, consider the possibility ... act as decomposers ... However, what specific evidence supports this? ... But are they exclusively decomposers? ... need to investigate further. Next, consider ... act as aerobes. ... Do have any evidence that ... However, cannot entirely rule out the possibility that ... need to challenge this assumption. Another possibility is ... act as catalysts ... Do have any evidence ... cannot definitively say they act as catalysts ... Finally, consider ... act as nitrogen-fixers ... Is there evidence that ... so this seems less likely. After evaluating these possibilities, find that ... However, lack definitive evidence to ... cannot confidently ... Answer: decomposers Reasoning: To answer the question ... The first possibility that comes to mind is decomposers. ... need to verify this ... Another possibility is reducers. However, ... This suggests that \"reducers\" is not the correct answer. third possibility is filters. However, ... This rules out \"filters\" as plausible answer. Next, need to evaluate the evidence for \"decomposers.\" ... This strongly supports the idea ... To challenge this assumption, must ask: ... While ... This suggests that \"decomposers\" is the most accurate answer. Finally, verify this conclusion by considering ... This confirms that their role as decomposers. Question: What team does controversial quarterback Michael Vick play for in the National Football League in the USA? Answer: Cleveland Browns Reasoning: Okay, so need to figure out ... But Im not entirely sure about ... think he was with the Cleveland Browns ... But Im not 100% sure. should double-check that. ... think he might have played for the Cleveland Browns during the early 2000s. Maybe ... Wait, was it around 2003 or so. the Cleveland Browns or the Philadelphia Eagles? think he was with the Browns when he got suspended by the Eagles. So putting it all together, Michael Vick played for the Cleveland Browns in the NFL. think thats the correct answer. Answer: Miami Dolphins Reasoning: Okay, so need to figure out ... think he started with the Philadelphia Eagles. Wait, but then he was traded to the Cleveland Browns, right? ... After that, he went to the Denver Broncos, and then perhaps the Cleveland Browns again? Or maybe ... ? ... think the last team he played for was the Miami Dolphins. But should confirm. ... Wait, another angle: his draft year. ... So the answer should be Miami Dolphins. ... But wait, maybe he was traded again after Miami? dont recall him ... So the answer is Miami Dolphins. Answer: Philadelphia Eagles Reasoning: To determine ... The most ... that comes to mind is the Philadelphia Eagles. ... However, need to verify this and consider other possibilities. Other teams that might be relevant include: ... Evaluate evidence for each possibility 1. Miami Dolphins: ... but there is no evidence to suggest ... 2. Denver Broncos: ... but the Broncos are not the final team he ended up with. 3. Philadelphia Eagles: ... This is the team he ended up playing for after the trade. ... Challenge assumptions. Could be misinterpreting the evidence? ... No, the evidence overwhelmingly points to the Eagles ... Verify the evidence ... This is consistent with the timeline of his NFL career. Answer: Eagles Reasoning: To determine ... The first possibility that comes to mind is the Philadelphia Eagles. ... but need to verify this. ... Another possibility is the Miami Dolphins ... However, dont recall any specific evidence ... but need to keep it in mind for further verification. third possibility is the Dallas Cowboys ... However, dont have any specific evidence ... To resolve this, need to focus on concrete evidence. ... This aligns with my initial thought. Additionally, the Eagles ... Next, should challenge this assumption. ... To verify, check reliable sources like ... Finally, apply adversarial self-critique. ... The evidence is sufficient ... Question: Who created Rumpole of the Bailey? Answer: Andrew Marquardt Reasoning: Okay, so need to figure out ... Wait, isnt that by Andrew Marquardt? Or was it someone else? Im bit confused ... Maybe its by David Suchet? Wait, no, think he was the actor who played Rumpole. ... Let me think about the name again. The play is by Andrew Marquardt, right? But wait, Im not entirely sure. think should check ... From what recall, Rumpole is fictional character, so the playwright created him. Therefore, the creator would be Andrew Marquardt. Im bit unsure ... but think in this case, its the playwright who created the character. So Ill go with Andrew Marquardt as the creator. Answer: Charles Dickens Reasoning: Okay, so need to figure out ... know that Charles Dickens wrote lot of classic stories. Rumpole of the Bailey... think thats from \"Oliver Twist.\" Wait, no, Rumpole is the character in \"Oliver Twist,\" right? ... But was Rumpole fictional character created by Dickens? ... Wait, maybe the answer is Charles Dickens. ... But should make sure. Let me recall. ... So Dickens created Rumpole of the Bailey. ... Dickens often created his own characters, so yes, he created Rumpole. But maybe theres historical figure with that name? Let me check my memory. Charles Dickens wrote \"Oliver Twist,\" and Rumpole is the friend of Oliver. So the answer should be Charles Dickens. Answer: dont know Reasoning: The user is asking about ... First, consider whether the creator could be famous British comedian ... However, must ask myself: Do have any specific recollection or evidence ...? No, dont have any concrete evidence or memory of ... Next, consider whether the creator might be lesser-known figure ... Again, must challenge this assumption: ... No, dont have any such evidence. ... Finally, consider whether the creator might be ... However, must ask myself: Do have any evidence ... No, dont have any such evidence. have systematically examined multiple possibilities. ... For each possibility, Ive asked myself what specific evidence would need to make this claim confidently, and find that dont possess such evidence. Answer: dont know Reasoning: The user is asking about ... First, need to consider whether the show was created by ... What evidence supports this? dont have any direct evidence ... Next, could it have been created by ... ? However, dont have any specific information ... This possibility also seems unsupported. Could it have been created by more recent ... ? However, still lack specific evidence ... Without concrete evidence, cannot confidently assert this as the answer. ... have systematically examined multiple possibilities. ... For each possibility, have asked what specific evidence would need to make this claim confidently, and find that dont possess such evidence. Table 6: Reasoning path examples of different methods on DeepSeek-R1-Distill-Qwen-7B. We colored with wrong factual thinking leading to wrong answers, factual thinking alerting not sure and the good factual thinking leading to correct answers. However, this flawed reasoning pattern can also be effectively transformed into more reliable and coherent one through our BARREL training framework."
        },
        {
            "title": "H Additional Details",
            "content": "We provide the links and licenses of the datasets and code used in our paper as follows: Code We conduct SFT using our own codebase, which is built on top of the Transformers library7 and DeepSpeed8. And we conduct GRPO training using the verl framework 9. 7https://github.com/huggingface/transformers 8https://github.com/deepspeedai/DeepSpeed 9https://github.com/volcengine/verl 20 Data We make use of the following publicly available datasets. (1) TriviaQA: Open-domain questionanswering corpus drawn from Wikipedia and the web (Apache 2.0 License)10; (2) SciQ: 13 679 multiple-choice science questions spanning physics, chemistry, biology, and more (CC BY-NC 3.0 License)11; (3) NQ-Open: Open-domain variant of Natural Questions covering real Google queries (CC BY-SA 3.0 License)12; (4) SimpleQA: Complex factuality benchmark (MIT License)13; (5) MATH-500: 500-problem subset of the MATH benchmark for compact maths evaluation (MIT License)14; (6) MATH: full-scale mathematics problem benchmark (MIT License)15. (7) SelfAware: unanswerable questions (Apache 2.0 License) 16."
        },
        {
            "title": "I Models Used in Our Experiments",
            "content": "We provide the download links to the models used in our experiments as follows: DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Llama-8B) DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-7B) DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Qwen-32B) DeepSeek-R1-Distill-Llama-70B DeepSeek-R1-Distill-Llama-70B) (https://huggingface.co/deepseek-ai/ (https://huggingface.co/deepseek-ai/ (https://huggingface.co/deepseek-ai/ (https://huggingface.co/deepseek-ai/ DeepSeek-R1 (https://huggingface.co/deepseek-ai/DeepSeek-R1) QwQ-32B (https://huggingface.co/Qwen/QwQ-32B)"
        },
        {
            "title": "J Limitations",
            "content": "Although we have evaluated and validated the performance of our method, restricted by our limited computing resource, we mainly utilize DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-DistillQwen-7B to perform our study, which are relatively small LRMs. In our experiments, though we have covered different datasets, we mainly utilizing verifiable questions, which could be evaluated at test time and reward at training time using string matching method. How to teach LRMs to learn knowledge boundary and behave more deliberatively on open-end questions, like writing articles or providing opinions, remains valuable topic for future work."
        },
        {
            "title": "K Boarder Impact",
            "content": "Although existing LRMs have demonstrated strong capabilities, hallucinations frequently occur in real-world applications, raising doubts about their reliability. Our work enhances the reliability of LLM responses by teaching the models to express uncertainty and refuse to answer when appropriate. We hope our work will inspire more research in this area to further improve model reliability, enabling humans to place greater trust in the positive responses provided by LRMs. 10https://github.com/mandarjoshi90/triviaqa 11https://huggingface.co/datasets/allenai/sciq 12https://github.com/efficientqa/nq-open 13https://github.com/openai/simple-evals 14https://huggingface.co/datasets/HuggingFaceH4/MATH-500 15https://github.com/hendrycks/math 16https://github.com/yinzhangyue/SelfAware"
        }
    ],
    "affiliations": [
        "Centre for Frontier AI Research, Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore",
        "The College of Computing and Data Science, Nanyang Technological University",
        "The Conversational AI (CoAI) group, DCST, Tsinghua University"
    ]
}