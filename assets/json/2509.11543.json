{
    "paper_title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
    "authors": [
        "Zhengxi Lu",
        "Jiabo Ye",
        "Fei Tang",
        "Yongliang Shen",
        "Haiyang Xu",
        "Ziwei Zheng",
        "Weiming Lu",
        "Ming Yan",
        "Fei Huang",
        "Jun Xiao",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1."
        },
        {
            "title": "Start",
            "content": "Preprint UI-S1: ADVANCING GUI AUTOMATION VIA SEMIONLINE REINFORCEMENT LEARNING Zhengxi Lu1,2, Jiabo Ye2, Fei Tang1, Yongliang Shen1 , Haiyang Xu2 , Ziwei Zheng2 Weiming Lu1, Ming Yan2, Fei Huang2, Jun Xiao1, Yueting Zhuang1 1Zhejiang University {zhengxilu, syl}@zju.edu.cn shuofeng.xhy@alibaba-inc.com 2Tongyi Lab, Alibaba Group"
        },
        {
            "title": "ABSTRACT",
            "content": "Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), metric that aligns better with true online performance, serving as practical and effective proxy for real-world evaluation. Experiments show that ours UI-S1-7B achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1. 5 2 0 2 5 1 ] . [ 1 3 4 5 1 1 . 9 0 5 2 : r Figure 1: Illustrations of three RL approaches. Our proposed Semi-online RL simulates online RL on offline static trajectories, which enhances multi-turn agent capabilities more efficiently. Work done during internship at Tongyi Lab, Alibaba Group. Corresponding author and project leader 1 Preprint"
        },
        {
            "title": "INTRODUCTION",
            "content": "Graphical User Interface (GUI) automation represents critical frontier in developing AI agents that can interact with digital environments as humans do, driven by advances in multimodal large language models that enable complex reasoning and multi-step task execution (Shen et al., 2023; Hu et al., 2025; Zhang et al., 2025a; Wang et al., 2025a; Tang et al., 2025b; Liu et al., 2025a). This evolution has been accelerated by reinforcement learning techniques that allow agents to improve through trial-and-error learning, guided by task completion signals (Bai et al., 2024; Lu et al., 2025b; Tang et al., 2025a; Ye et al., 2025; Du et al., 2025). Despite these advances, current reinforcement learning approaches fall into two distinct paradigms (Figure 1), each with critical limitations. Offline RL methods train on pre-collected trajectories with step-wise supervision (Lu et al., 2025b; Luo et al., 2025; Liu et al., 2025b). These approaches leverage large-scale datasets annotated by humans or language models (Li et al., 2024; Lu et al., 2024; Chai et al., 2024), achieving stable training and high single-step accuracy. However, agents trained with offline RL often fail catastrophically when deployed on real-world tasks that require multi-step reasoning and planning. This performance gap arises from two key issues: (1) mismatch between the offline training and the online evaluation dynamics, particularly regarding whether the original model outputs are consistently recorded into the historical context; and (2) overfitting to local reward signals, leading to ignorance of future or global training objectives. Online RL methods address this limitation by training agents through direct environment interaction (Lu et al., 2025a; Shi et al., 2025; Ye et al., 2025), learning to handle stochastic transitions with historical context across multiple steps. However, deploying online RL for GUI automation faces prohibitive practical barriers. First, rewards in real-world GUI tasks are typically sparse and delayed, which are often received only at task completion, resulting in inefficient training for complex tasks. Second, enhancing data diversity is inherently difficult: scaling to new environments or tasks requires extensive engineering effort to implement custom verification logic or simulation modules, which can be more labor-intensive than manually curating diverse, high-quality trajectories. To simultaneously exploit the training efficiency of offline RL, and the long-term optimization target of online RL, we introduce Semi-online RL, novel training paradigm designed for multi-turn interaction learning from pre-collected trajectories. In detail, Semi-online RL preserves original model output including reasoning contexts and historical action within the dialogue state, and then computes step-wise rewards from offline trajectories. Moreover, to improve the comprehensive utilization of trajectory data, novel Patch Module adaptively recovers the by injecting expert action and synthetic reasoning content. To better capture the current influence on future execution, we further incorporate discounted future reward into step-level advantages and optimize the policy with weighted step-level and episode-level advantages. For efficient multi-turn evaluation, we propose semi-online metric SOP, which demonstrates stronger correlation with online metrics AndroidWorld (R2=0.934) than traditional offline metrics like AndroidControl-High (R2=0.470) and GUI Odyssey (R2=0.398), as shown in Figure 2 and Figure 10. Experiments demonstrate that ours Figure 2: Left: Offline metric AC-High demonstrates weak correlation (R2=0.470) with online metric AndroidWorld (AW). Right: Our proposed semi-online metric SOP shows stronger correlation (R2=0.934), while ours UI-S1-7B achieves superior performance on both metrics. Preprint UI-S1-7B achieves state-of-the-art performance among all open-source 7B models on multi-turn benchmarks, in both dynamic setting (AndroidWorld, AITW, MiniWob++) and static setting (SOP). Notably, UI-S1-7B improves success rates by +12.0 on AndroidWorld and +23.8 on AITW-Gen compared to its base model (i.e., Qwen2.5VL-7B). In addition, it achieves slight gains on out-ofdomain single-turn benchmarks (e.g., +1.9 on SS-Pro and +7.1 on GUI Odyssey), validating that Semi-online RL doesnt sacrifice single-turn capabilities. In summary, our contributions are as follows. We introduce training paradigm Semi-online RL that simulates online rollout dynamics using static trajectories. Patch Module is designed to recover from action mismatches by injecting expert actions to maximize trajectory utilization. We incorporate discounted future returns and dual-level advantages into policy optimization, which balances step-level accuracy with trajectory-level task completion. We propose Semi-Online Performance (SOP), metric that demonstrates strong correlation with real-world performance. Our model UI-S1-7B achieves state-of-the-art results among 7B models, with +12.0% on AndroidWorld and +23.8% on AITW."
        },
        {
            "title": "2 RELATED WORK",
            "content": "GUI Agents with Reinforcement Learning Recent advances in multimodal models have catalyzed significant progress in GUI automation (Hu et al., 2025; Zhang et al., 2025a; Wang et al., 2025a; Tang et al., 2025b; Liu et al., 2025a; Ye et al., 2025). Early approaches rely on supervised fine-tuning with large-scale annotated datasets. AGUVIS (Xu et al., 2024), OS-Atlas (Wu et al., 2024), UGround (Gou et al., 2025), SeeClick (Cheng et al., 2024), and UI-TARS (Qin et al., 2025) leverage millions of annotated GUI elements to achieve impressive single-step accuracy. While these methods demonstrate strong performance on static benchmarks, they suffer from limited generalization to out-of-distribution scenarios and lack the ability to adapt through interaction. Inspired by the success of DeepSeek-R1 (Guo et al., 2025), recent work has begun applying reinforcement learning to GUI automation. UI-R1 (Lu et al., 2025b), GUI-R1 (Luo et al., 2025), and InfiGUI-R1 (Liu et al., 2025b) adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024) for training, demonstrating improved task completion rates. However, these offline RL methods optimize individual actions independently without maintaining sequential context, leading to poor multi-turn performance in real deployment. Multi-Turn Reinforcement Learning Recognizing the limitations of single-step optimization, recent work has explored multi-turn reinforcement learning through online environment interaction (Feng et al., 2025; Wang et al., 2025b; Dong et al., 2025; Zhang et al., 2025b). ARPO (Lu et al., 2025a) proposes multi-turn policy optimization using GRPO with distributed rollouts and experience replay to handle sparse rewards. The method requires extensive parallel infrastructure and struggles with limited exploration diversity. MobileGUI-RL (Shi et al., 2025) extends GRPO to mobile environments with trajectory-aware advantages and curriculum learning through self-exploration, but faces similar challenges with reward sparsity and deployment costs. These online methods address the context continuity problem inherent in offline training but introduce new challenges. Rewards in real-world GUI tasks are typically delayed until task completion, resulting in inefficient learning that requires thousands of interactions for simple behaviors (Lu et al., 2025a). Furthermore, scaling to new applications requires extensive engineering effort to implement environment simulators and verification logic, often exceeding the cost of collecting offline trajectories. Our Semi-online RL addresses these limitations by simulating online dynamics using static trajectories, achieving context continuity without environment access while maintaining training efficiency."
        },
        {
            "title": "3 METHOD",
            "content": "We propose Semi-online RL, semi-online reinforcement learning framework for training GUI agents that bridges the gap between the stability of offline training and the challenge of online execution. Our approach consists of three key parts. (1) Semi-online rollout (Section 3.2) simulates online interaction dynamics using only offline trajectories; (2) Patch Module (Section 3.3) adaptively 3 Preprint Figure 3: Illustrations of our proposed Semi-online RL. During semi-online rollout, Patch Module adaptively recovers from action mismatches. The dual-level advantages capture both step-wise and episode-level optimization signals with future reward propagation. recovers the divergence between rollout and expert trajectories; (3) Semi-online Policy Optimization (Appendix 3.4) optimizes agents through hierarchical reward structure and dual-level advantages. 3.1 PROBLEM FORMULATION We formulate GUI automation as multi-turn sequential decision-making problem. Given highlevel instruction describing the task objective, the agent must interact with the graphical interface to complete the specified goal through sequence of actions. At each time step t, the agent observes the current state St (typically screenshot of the interface) and maintains history of past interactions: Ht = {(S1, a1, T1), (S2, a2, T2), . . . , (St1, at1, Tt1)} where ai represents the executed action and Ti captures the agents reasoning process at step i. The agent then generates the next action and associated reasoning: (1) at, Tt π( I, St, Ht) where π denotes the policy model. The environment transitions to the next state according to St+1 = E(St, at), and the process continues until task completion or failure. (2) The fundamental challenge in training GUI agents lies in the mismatch between training and deployment conditions. Traditional offline RL trains on static trajectories where each step conditions on expert demonstrations: static = {(S 1 , 1), . . . , (S t1, t1)} In contrast, real-world execution requires the agent to condition on its own generated outputs: online = {(S1, aπ 1 , π 1 ), . . . , (St1, aπ t1, π t1)} (3) (4) This mismatch causes statically-trained agents to fail catastrophically in multi-turn scenarios, as they never learn to process their own outputs or recover from errors. Online RL addresses this by training with actual environment interaction, but at prohibitive cost. Our Semi-online RL reconciles these approaches by simulating online dynamics using static data. 3.2 SEMI-ONLINE ROLLOUT 1 , Given an expert trajectory τ = {(S tain policy-generated context while using expert demonstrations for guidance. 1), . . . , (S , )}, we generate training rollouts that mainDuring training, we sample rollouts from the policy model. The i-th candidate trajectory is τ = {(Si 1, ai 1), (Si 2, ai 2), . . . , (Si , ai )}, = 1, . . . , N, (5) 4 Preprint The agent maintains its own generated history, serving as subsequent steps condition: 1, 1 ), . . . , (Si = {(Si t1, t1, ai t1)} 1, ai (6) At each step, the policy generates action ai We then use the expert trajectory to approximate environment dynamics: based on this self-generated history (from Equation 2). Si t+1 = (cid:26)S t+1 None if Matches(ai otherwise t, ) (7) When actions match expert demonstrations, we obtain the next state from the expert trajectory and continue with the models generated history. However, when actions diverge, simple termination would prevent learning from the remaining trajectory steps, particularly resulting in inaccessible later steps which may contain valuable learning signals. 3.3 PATCH MODULE FOR TRAJECTORY RECOVERY To improve the data utilization against early termination, we introduce Patch Module to recover from action mismatches and continue learning from trajectory remainders. When mismatch occurs at step t, the module replaces the incorrect action with the expert action and generates synthetic reasoning patch . The patched components are then integrated into the history, allowing the rollout to continue with Ht+1 = Ht {(St, )} (as detailed in Algorithm A.4). We explore three patching strategies that vary in how synthetic reasoning is generated: , patch Table 1: Different thought patch methods. M0 denotes the auxiliary model and denotes the policy model. Patch Method Thought-Free Patch Off-Policy Thought Patch On-Policy Thought Patch Function Definition F(at, Tt) = (a , ) , M0(I, , M(I, F(at, Tt) = (a F(at, Tt) = (a , St)) , Ht, St)) Thought-Free Patch simply injects the expert action without reasoning. This minimal intervention maintains trajectory continuity with an efficient and direct method. Off-Policy Thought Patch uses an auxiliary model M0 (e.g., DeepSeek-R1 (Guo et al., 2025)) to generate high-quality reasoning. This ensures coherent thought processes but may introduce distribution shift between the auxiliary and policy models. On-Policy Thought Patch uses the current policy model with expert action hints to generate reasoning. This maintains consistency with the policys reasoning style while providing correction signals. The prompting strategy for synthetic thought generation is detailed in Appendix A.3. 3.4 SEMI-ONLINE POLICY OPTIMIZATION Traditional offline RL optimizes only for immediate step-wise accuracy, resulting in multi-turn planning failure. We address this through hierarchical reward structure and dual-level advantages that capture both immediate and future impacts. For each step in the rollout, we compute composite reward: rt = 0.1 rformat + 0.4 I[rformat=1] rtype + 0.5 I[rformatrtype=1] racc where rformat, rtype, and racc evaluate response formatting, action type correctness, and exact match accuracy respectively. (8) To capture long-horizon dependencies for multi-turn tasks, we compute discounted future returns: γktri Ri = (cid:88) k=t 5 (9) Preprint where γ (0, 1) weights the influence of future consequences on current decisions. We compute advantages at two levels: Step-Level Advantage AS(ai trajectories at the same timestep: t) captures local optimization signals by comparing returns across AS(ai t) = Ri µt σt where µt and σt are computed across all rollouts at step t. Episode-Level Advantage AE(τ i) captures global task completion signals: AE(τ i) = R(τ i) µτ στ (10) (11) where R(τ i) represents the total trajectory return. We combine these into unified group-in-group advantage that assigns credit at both global and local scales: Then our Semi-online RL optimizes the policy through the following objective: A(ai t) = AE(τ i) + ω AS(ai t) (θ) = {τ i}N i=1 {oi,t}T Pπθold (I) t=1τ 1 (cid:88) (cid:88) oi,t (cid:88) i= t=1 k=1 min (cid:0)ρ(θ)A(ai t), clip(ρ(θ), 1 ϵ)A(ai t)(cid:1) (12) (13) where the notation indicates trajectories are generated through our Patch Module-enhanced rollout, is the total number of tokens, ρ(θ) = πθ(oi,t,kI,oi,t,<k) πθold (oi,t,kI,oi,t,<k) is the importance sampling ratio, and β controls the KL penalty strength. To ensure effective learning with sufficient exploration, we enforce minimum advantage variance: σ({A(ai t)}) > η, performing dynamic sampling until this diversity threshold is met. In our experiments, we set η = 0.3."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENT SETUP Baselines. We compare against three training paradigms using the same dataset: (1) SFT only: supervised fine-tuning on expert demonstrations, (2) Offline RL: traditional offline reinforcement learning with GRPO, conditioning on ground-truth history, and (3) Semi-Online RL only: our approach without prior SFT warm-up. Our final model combines SFT with Semi-Online RL in two-stage training pipeline. Multi-turn Benchmarks. To evaluate end-to-end task completion requiring sequential reasoning, we introduce Semi-Online Performance (SOP), an efficient proxy for online evaluation built on AndroidControl-Test (Li et al., 2024). SOP evaluates multi-turn execution by maintaining modelgenerated history throughout the task. Unlike AndroidControl-High which conditions on ground truth at each step, SOP continues with the models own outputs, terminating only upon action mismatch. We report Progress (PG) as the average task completion ratio and Task Success Rate (TSR) as the proportion of fully completed tasks (as detailed in Appendix A.5). To demonstrate GUI agents real-world performance, we also evaluate on dynamic environments including AndroidWorld (116 tasks) (Rawles et al., 2024), AITW-Gen (300 filtered tasks), AITW-Web (150 filtered tasks) (Bai et al., 2024; Shi et al., 2025), and MiniWob++ (92 tasks) (Liu et al., 2018). Single-turn benchmarks Single-turn evaluates the grounding capability and GUI Understanding capability of the end-to-end GUI model in single-turn conversation without historical context. We use ScreenSpot-V2 (Cheng et al., 2024) and ScreenSpot-Pro (Li et al., 2025) to evaluate the grounding ability. We also adopt AndroidControl-High (Li et al., 2024) and GUI Odyssey (Lu et al., 2024), for comprehensive GUI understanding evaluation under high-level instruction. The action type match accuracy (TM), grounding accuracy rate (GR) and step success rate (SR) are reported. 6 Preprint 4.2 MAIN RESULTS Table 2: Results on Multi-turn Benchmarks. * denotes the result using prompt in Appendix A.2. Closed-source Models Gemini-Pro-1.5 (SoM) (Team et al., 2024) Claude Computer Use (Anthropic, 2024) GPT-4o (SoM) (Hurst et al., 2024) Open-source 7B/8B Models OS-Genesis-7B (Sun et al., 2024) OS-Atlas-7B (Wu et al., 2024) Qwen2.5VL-7B (Bai et al., 2025) AgentCPM-GUI-8B (Zhang et al., 2025c) MobileGUI-7B (Shi et al., 2025) UI-TARS-7B (Qin et al., 2025) Open-source 32B/72B Models Qwen2.5VL-32B (Bai et al., 2025) Aguvis-72B Xu et al. (2024) Ours 7B Models Qwen2.5VL-7B (Base)* w/ SFT w/ Offline RL w/ Semi-online RL only UI-S1-7B (vs Base) SOP PG TSR 7.6 14.3 17.4 17.1 28.1 17.8 3.0 8.6 9.8 10.6 14.0 10.2 9.1 16.8 9.3 17.0 10.5 18.3 16.0 30.6 32.4 16.3 +15.6 +7.2 AITW-Gen AITW-Web MiniWob++ AW 14.5 45.6 49.0 58.6 65.3 64.9 42.7 50.5 58.9 54.6 70.2 74.3 +23.8 7.8 17.9 20.0 15.2 22.7 28.1 24.7 28.8 28.5 19.8 36.3 40.2 +11.4 19.8 35.2 54.0 37.8 58.7 70.1 66.0 54.0 46.7 53.3 57.6 60.9 +6.9 22.8 27.9 34.5 17.4 12.1 22.0 16.4 30.0 33.0 31.5 26. 14.9 21.7 15.7 30.4 34.0 +19.1 Table 3: Results on single-turn benchmarks. SS-V2 SS-Pro AC-High GUI Odyssey TM GR SR TM GR SR Closed-source Models GPT-4o (Hurst et al., 2024) Claude-computer-use (Anthropic, 2024) SeeClick (Cheng et al., 2024) Open-source Models OS-Atlas-4B (Wu et al., 2024) Qwen2.5VL-3B (Bai et al., 2025) UI-R1-3B (Lu et al., 2025b) GUI-R1-3B (Luo et al., 2025) OS-Genesis-7B (Sun et al., 2024) OS-Atlas-7B (Wu et al., 2024) Aguvis-7B (Xu et al., 2024) GUI-R1-7B (Luo et al., 2025) AgentCPM-GUI-8B (Zhang et al., 2025c) UI-TARS-7B (Qin et al., 2025) Ours 7B Models Qwen2.5VL-7B (Base) w/ SFT w/ Offline RL w/ Semi-online RL only UI-S1-7B (vs Base) 18.3 83.0 55.1 71.9 80.9 85.4 85.0 84.1 81.8 88.2 91. 89.0 90.1 88.4 89.7 90.1 +1.1 0.8 17.1 1.1 3.7 28.7 17.8 28.6 18.9 22.9 31.3 35.7 28.7 29.6 29.2 30.2 30.6 +1.9 66.3 63.7 82.9 49.0 47.8 57.9 58.0 65.9 57.4 65.6 71.6 77.7 83. 0.0 0.0 62.9 49.5 46.5 55.7 56.2 54.9 65.6 80.5 20.8 12.5 59.1 22.8 38.9 45.4 46.6 44.4 29.8 54.2 51.7 69.2 72.5 34.3 60.9 71.0 49.6 37.4 52.2 54.8 11.7 60.4 26.7 65.5 90.8 94. 0.0 0.0 52.4 34.6 26.5 34.5 41.5 39.7 43.6 90.1 3.3 3.1 53.9 20.3 26.7 32.5 41.3 3.6 27.0 13.5 38.8 75.0 87.0 62.2 52.4 66.8 43.2 69.7 48.7 77.6 56.3 59.5 79.9 +17.7 +0.9 +15.5 +8.9 +5.4 +7.1 67.4 56.9 62.5 74.5 76. 56.3 61.5 50.2 58.9 61.7 52.7 56.1 59.0 66.8 68.2 72.5 74.3 68.2 71.3 73.4 Multi-turn Performance. As shown in Table 2, UI-S1-7B establishes new state-of-the-art among 7B/8B open-source models across all evaluated multi-turn benchmarks. Compared to Qwen2.5VL-7B, We achieved substantial improvements: +19.1% on AndroidWorld and +23.8% on AITW-Gen. Remarkably, our UI-S1-7B outperforms strong baselines such as MobileGUI-7B and also delivers competitive results on AndroidWorld (34.0%) compared with significantly larger 7 Preprint open-source models like Qwen2.5VL-32B (31.5%) and Aguvis-72B (26.1%), as well as closedsource systems such as GPT-4o (34.5%). The comparison between training paradigms reveals critical insights. While SFT improves over the base model, it shows slight gains on dynamic benchmarks (21.7% on AW). Traditional Offline RL actually degrades model performance (53.3% on MiniWob++) compared to the base model, demonstrating its limited capabilities on real-world generalization. Our approach (Semi-Online RL only) achieves 30.4% on AW, and SFT combined with Semi-Online RL reaches 34.0%, validating its generalization. Single-turn Performance. Table 3 shows that Semi-Online RL maintains competitive single-turn performance while excelling at multi-turn tasks. Our model achieves consistent improvements over the base: +15.5% on AndroidControl-High SR and +7.1% on GUI Odyssey SR. However, offline RL models like AgentCPM-GUI-8B excel at single-turn tasks but struggle with multi-turn execution (16.4 on AW). This demonstrates that Semi-Online RL successfully bridges both capabilities rather than trading one for the other. 4.3 ANALYSIS OF PATCH MODULE STRATEGIES We present the results of patch strategies across different data scales and thresholds in Table 4. Impact of Patch Threshold. The patch threshold ϵ controls how many mismatches are recovered before termination. Results demonstrate that increasing ϵ consistently improves both SOP and AndroidWorld metrics. With 1000 training samples, SOP-Score increases from 22.3 (ϵ=0) to 25.7 (ϵ=) for Thought-Free Patch, representing 15% relative improvement. This gain stems from increased exposure to later trajectory steps, as higher ϵ values enable learning from previously inaccessible trajectory segments. Figure 5 reveals that larger ϵ values maintain greater policy entropy during training, indicating more diverse exploration and preventing premature convergence. We select ϵ=1 as optimal, achieving 34.0% on AndroidWorld while minimizing computational overhead. Comparison of Patch Methods. Three patching strategies exhibit distinct trade-offs between performance and efficiency (from Figure 11). On-Policy Thought Patch achieves the highest SOP scores (26.1 at ϵ=) by maintaining reasoning consistency with the policy model. Thought-Free Patch delivers competitive performance (25.7) with significantly lower computational cost, requiring no additional inference for synthetic reasoning generation. Off-Policy Thought Patch underperforms (22.6) due to distribution mismatch between the auxiliary models reasoning style and the policy models expectations. Based on these results and efficiency considerations, we adopt Thought-Free Patch with ϵ=1 for our final configuration. Table 4: Performance comparison for different ϵ values with varying data sizes (200, 500, 1000 from left to right). Each table shows results for SOP and AW under three patching strategies. ϵ SOP AW ϵ SOP AW ϵ SOP PG TSR Score PG TSR Score PG TSR Score 14.3 15.1 16.5 16.7 Thought-Free Patch 20.3 26.3 0 21.5 27.9 1 22.8 2 29.1 30.4 23.6 Off-Policy Thought Patch 26.3 20.3 0 18.5 24.0 1 21.5 2 28.1 30.2 21.8 On-Policy Thought Patch 26.3 20.3 0 22.0 28.7 1 22.7 2 29.4 30.3 23.7 14.3 15.3 16.0 17.1 14.3 12.9 14.9 13.3 21.0 24.0 25.4 25. 21.0 19.7 25.0 24.0 21.0 25.0 24.9 26.9 14.8 15.7 16.5 17.0 Thought-Free 21.4 28.0 0 22.1 28.5 1 24.1 2 31.6 33.8 25.4 Off-Policy Thought Patch 28.0 21.4 0 20.5 28.5 1 21.8 2 30.0 30.5 22.3 On-Policy Thought Patch 28.0 21.4 0 23.1 31.0 1 24.4 2 32.0 33.2 25.2 14.8 15.2 16.7 17.2 14.8 12.5 13.5 14. 27.2 29.1 31.5 30.8 27.2 25.0 26.0 24.0 27.2 28.2 29.8 31.5 8 15.0 16.3 16.8 17.0 Thought-Free Patch 22.3 29.6 0 24.4 32.4 1 24.7 2 32.6 34.4 25.7 Off-Policy Thought Patch 29.6 22.3 0 20.8 29.5 1 22.1 2 31.6 31.8 22.6 On-Policy Thought Patch 29.6 22.3 0 24.8 32.9 1 25.3 2 33.1 34.4 26. 15.0 16.7 17.4 17.8 15.0 12.0 12.6 13.3 AW 30.0 34.0 33.9 34.5 30.0 24.6 25.3 24.0 30.0 31.4 31.9 32. Preprint Figure 4: Data scaling for different ϵ for Thought-free patch, with SOP-score reported. Figure 5: Actor entropy during training process with different patch method and threshold. Figure 6: Comparison of Semionline RL (with different γ) and Offline RL during training. Figure 7: Comparison of offline (AC-High), online (AndroidWorld), and semi-online (SOP) evaluation methods across three dimensions: efficiency (inverse time cost), diversity (number of tasks), and correlation with online performance. 4.4 ANALYSIS OF TRAINING DYNAMICS Scaling Law Performance. Figure 4 reveals the data scaling performance of Semi-Online RL across different patch configurations. The performance follows an exponential scaling law = + eC+kx, where the scaling coefficient increases with ϵ from 1.13 to 0.73. This indicates that larger ϵ values not only improve absolute performance but also enhance data efficiency, enabling more effective learning from each training sample. The improved scaling stems from better utilization of trajectory data, as the Patch Module enables learning from steps that would otherwise be terminated after action mismatches. Semi-Online Performance Metric. Figure 7 validates SOP as an effective proxy for real-world evaluation. We compare three evaluation paradigms across efficiency (inverse time cost), diversity (number of tasks), and correlation with online performance. SOP achieves the highest correlation with AndroidWorld (R2=0.934), substantially outperforming AndroidControl-High (R2=0.470) while requiring minimal evaluation time. This strong correlation confirms our hypothesis that maintaining model-generated history during evaluation accurately captures the multi-turn dynamics of real deployment. The metric fills critical gap between fast but unrealistic offline evaluation and accurate but expensive online testing. 4.5 ABLATION STUDIES Discount Factor Analysis. The results in Figure 6 demonstrate the importance of future reward discounting in Semi-Online RL. Our approach increases the task success rate during training steps while traditional Offline RL exhibits opposite behavior. This divergence highlights fundamental difference: Semi-Online RLs historical context continuity enables effective multi-turn paradigms learning, while Offline RL ignores long-horizon training signals. Among different γ in our setting, performance peaks at γ=0.5. Setting γ=0 (no future rewards) yields the worst results, confirming that long-horizon optimization is essential for multi-turn tasks. Training Paradigm. We also conduct ablation studies on training paradigms in Figure 8. Combining SFT with Semi-Online RL outperforms either method alone, achieving 34.0% on AndroidWorld compared to 30.4% for Semi-Online RL only and 21.7% for SFT only. The combined approach also reduces average task completion steps (middle panel), eliminating redundant actions with better planning. Additional ablations (right panel) confirm that both episode-level advantages and maintaining multiple historical images contribute to performance, validating our training setup. Preprint Figure 8: Left: Performance of different training paradigm combinations. Middle: Average steps to complete AndroidWorld tasks. Right: Ablations on episode advantages and historical images. Figure 9: cross-app and memorable task case in AndroidWorld. The instruction is Create file in Markor, called receipt.md with the transactions from the receipt.png. Use Simple Gallery to view the receipt. Please enter transactions in csv format including the header Date, Item, Amount. 4.6 CASE STUDY We showcase complex cross-application task requiring information retention across multiple steps: creating file in Markor with transaction details from an image viewed in Simple Gallery (as illustrated in Figure 17). The base model and Offline RL model exhibit action-thought inconsistency. For example, offline RL terminate prematurely after planning to navigate to the next app, likely due to overfitting to local rewards without considering future objectives. The SFT model loses critical information and executes redundant actions like attempting to create file that already exists. In contrast, our model successfully records the critical information throughout the 12-step sequence, correctly recording 2023-03-23, Monitor Stand, $33.22 in CSV format. This demonstrates Semi-Online RLs effectiveness in learning robust multi-turn behaviors with consistent reasoning-action alignment. Additional case studies are provided in Appendix A.9 and failure analysis in Appendix A.10."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we present Semi-online Reinforcement Learning (Semi-online RL), novel training paradigm that bridges the advantages of offline and online reinforcement learning for GUI automation agents, enabling stable yet long-horizon-capable policy optimization. Experimental evaluation shows that our UI-S1-7B achieves state-of-the-art results among open-source 7B-scale models, with substantial improvements across both dynamic and static multi-turn benchmarks, without compromising single-turn performance. Our findings highlight the promise of Semi-online RL as an effective and scalable training framework for real-world GUI agents. 10 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Developing computer use model. https://www.anthropic.com/news/ developing-computer-use, 2024. Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, and Yongliang Shen. Test-time reinforcement learning for gui grounding via region consistency. arXiv preprint arXiv:2508.05615, 2025. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. 2025. URL https://arxiv.org/abs/2410.05243. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for general computing devices use. arXiv preprint arXiv:2508.04482, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv e-prints, pp. arXiv2406, 2024. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018. Guangyi Liu, Pengxiang Zhao, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han, Shuai Ren, Hao Wang, et al. Llm-powered gui agents in phone automation: Surveying progress and prospects. arXiv preprint arXiv:2504.19838, 2025a. 11 Preprint Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025b. Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo: End-to-end policy optimization for gui agents with experience replay. arXiv preprint arXiv:2505.16282, 2025a. Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025b. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, and Dong Yu. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment. arXiv preprint arXiv:2507.05720, 2025. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025a. Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Zeqi Tan, Yuchen Yan, et al. survey on (m) llm-based gui agents. arXiv preprint arXiv:2504.13865, 2025b. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, et al. Opencua: Open foundations for computer-use agents. arXiv preprint arXiv:2508.09123, 2025a. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025b. 12 Preprint Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 120, 2025a. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547, 2025b. Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement finetuning. arXiv preprint arXiv:2506.01391, 2025c. 13 Preprint"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 NOTATION DEFINITION Table 5: Notation Definition in Section 3. Symbol Description M0 prompt at Tt St Ht rformat rtype racc rt I[ ] γ Ri AS(ai t) AE(τ i) R(τ (j)) σ() ω A(ai t) oi,t oi,t,k ρ(θ) θ θold πref β η Assist model used for thought patching Policy model Prompt template for thought generation (as shown in Appendix A.3 Predicted action at step Expert action at step Thought representation at step Patch function that outputs (possibly corrected) action and thought High-level GUI instruction Current observation (e.g., screenshot) at step Full history up to step including (S, a, ) tuples Binary score (0 or 1) for correct output format Binary score (0 or 1) for correct predicted action type Binary score (0 or 1) for exact action match with ground truth Step-wise reward at time Indicator function that equals 1 only if condition is true Discount factor for return computation (0 < γ < 1) Discounted return of i-th trajectory starting from step Number of trajectories sampled in batch Step-level advantage for action ai Episode-level advantage for trajectory τ Episode return of trajectory Last step index of trajectory Standard deviation function Weight balancing episodeand step-level advantages Combined group-in-group advantage Total number of tokens in the current batch Model output sequence (tokens) at step of trajectory k-th token of oi,t Conditioning input (e.g., prompt including state/action history) Importance sampling ratio between new and old policies Current policy parameters Policy parameters before update (rollout policy) Reference policy for KL regularization Coefficient for KL divergence penalty Minimum standard deviation threshold for advantage diversity Preprint A.2 PROMPT FOR TRAINING AND INFERENCE System prompt: You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. Output Format <think> ... </think> <action> ... </action> Action Space You can perform the following actions: - key: Perform key event on the mobile device using adbs keyevent syntax. - click: Click the point on the screen with specified (x, y) coordinates. - long press: Press the point on the screen with specified (x, y) coordinates for specified number of seconds. - swipe: Swipe from starting point with specified (x, y) coordinates to endpoint with specified (x2, y2) coordinates. - type: Input the specified text into the activated input box. - answer: Output the specified answer. - system button: Press the specified system button: Back, Home, Menu, or Enter. - open: Open an application on the device specified by text. - wait: Wait for specified number of seconds for changes to occur. - terminate: Terminate the current task and report its completion status: success or failure. The arguments you can use are: - coordinate: (x, y): The and pixels coordinates from the left and top edges. - coordinate2: (x, y): The and pixels coordinates from the left and top edges for the endpoint of swipe. - text: Text input required by actions like key, type, answer, and open. - time: The time in seconds required by actions like long press and wait. - button: System buttons available for pressing: Back, Home, or Enter. Possible values: Back, Home, Menu, Enter. - status: The completion status of terminated task. Possible values: success, failure. Format your output as JSON object with the selected action and its arguments at the same level. Example Output <think>...</think> <action>{\"action\": \"key\", \"text\": \"<value>\"} Note - Planing the task and explain your reasoning step-by-step in think part. - Write your action in the action part according to the action space. - If the query asks question, please answer the question through the answer action before terminating the process. - Swipe the screen to find the File Manager app if needed. User prompt: User Instruction: USER INSTRUCTION Assistant prompt: HISTORY RESPONSES HISTORY IMAGES 15 Preprint A.3 PROMPT FOR THOUGHT GENERATION System prompt: End-to-End Model Thought Integration Integration Requirements Write the thought process from global goal, the action history, thought history and screenshot history. The reasoning logic must satisfy: Begin by reviewing the global task objective. Inherit the context and decisions from historical steps. Incorporate the managers planning logic. Derive actions that fully align with the operators output. Output Format <think> [A coherent reasoning process, reflecting task decomposition, environmental observation, and iterative decision-making] </think> Output Example <think> The current task requires checking the order status of DeepSeek. Access to the official website and locating the login entry have been completed. Based on the page loading result, the login form is ready. Authentication information needs to be filled: the username has already been entered as \"DeepSeek,\" and now the password must be entered. </think> Key Design Notes Explicitly require the global task objective to ensure the end-to-end model always anchors to the core goal. Enforce structured historical records to prevent information loss. Logic consistency mechanism. The thought process should naturally connect historical conclusions with the current managers planning. Transform the managers planning into autonomous decisions phrased as According to the requirements, determine... Translate operator actions into imperative statements phrased as Execute... Do not mention any coordinates in <think> ... </think>. Global Task Objective USER INSTRUCTION - If this isnt the target app for your operation, you can use open operation to navigate to the correct application. - You can use Next Action Hint to guide the think process, but within the think section, you must conceal the fact that hints were received. - Please integration the thought of current manager and operation into <think> ... </think> in English. Assistant prompt: HISTORY RESPONSES HISTORY IMAGES 16 Preprint A.4 PATCH MODULE Algorithm 1 Semi-Online Rollout with Patch Module Input: πθold : initial policy model τ = {(S 1 , 1), . . . , (S )} : offline trajectory , Output: τ = {(S1, a1), (S2, a2), . . . } : trajectory rollout Initialize H1 , τ , 0 S1 1 for = 1 to do at, Tt πθold ( St, Ht) , a t+1 τ Sample output from Equation 2 Fetch ground truth Patch Module: if at = (apatch then , patch else if < ϵ then ) at, Tt , patch apatch + 1 F(at, Tt) else τ τ (St, at) , patch apatch break , St+1 NONE end if if St+1 = NONE then break Continue rollout (no patching needed) Apply patch function defind in Table 1 Terminate rollout due to max patches reached end if St+1 t+1 Ht+1 Ht {(St, apatch τ τ (St, apatch Ht Ht+1, St St+ ) , patch )} Prepare for next step end for Output: τ 17 Preprint A.5 SOP Definition Let be the total number of tasks. For the i-th task, let si denote the number of successful steps, and ti denote the total number of steps in its expert trajectory. We define the . Here, I[] following metrics: PG = 1 is the indicator function, which equals 1 if the condition inside the brackets is true and 0 otherwise. I[si = ti], and Score = PG+TSR , TSR = 1 (cid:80)N (cid:80)N si ti i=1 i= 2 SOPs alignment with online metrics We also compare other online metrics and offline metrics GUI Odyssey with SOP in Figure 10, which demonstrates SOPs strong correlation with online metrics. Figure 10: Overall comparisons of online metrics (AW, AITW-Gen, AITW-Web, MiniWob++) with offline metrics (AC-High, GUI Odyssey) and semi-online metric (SOP). Left: AC-High demonstrates weak correlation with online metrics. Middle: GUI Odyssey demonstrates weak correlation with online metrics. Right: Ours SOP demonstrates stronger correlation with online metrics. proposed SOP shows stronger correlation (R2=0.934). , where SSres (Residual Sum of Squares) is SSres = (cid:80)n For the linear regression analyses in Figure 2 and Figure 10, the coefficient of determination, denoted as R2, is defined as R2 = 1 SSres i=1(yiˆyi)2, SStot and SStot (Total Sum of Squares) is SStot = (cid:80)n i=1(yi y)2. Here, is the number of observations; yi is the observed value of the dependent variable for the i-th data point; ˆyi is the corresponding predicted value from the regression model; and is the mean of all observed values. The R2 metric ranges from 0 to 1 and represents the proportion of variance in the dependent variable explained by the independent variable(s)higher values indicate better fit. Preprint A.6 TRAINING DETAILS Our UI-S1-7B is first Supervised Fine-Tuned (SFT) on Qwen2.5VL-7B, trained on data from AndroidControl-Train (Li et al., 2024) and Amex (Chai et al., 2024). The model is then further optimized using Semi-online RL with the thought-free patch mechanism based on the SFT model. The training hyper-parameters are listed in Table 6. Table 6: Key Training Hyper-parameters Parameter train batch size max prompt length data.max response length truncation use kl in reward γ (future reward dicount) ω (advantage weight) ϵ (patch threshold) η (DAPO threshold) learning rate ppo mini batch size fixed num mini batches ppo micro batch size per gpu kl loss coef gpus per node nnodes total epochs Value 32 12288 512 error False 0.5 1.0 1 0.3 1 106 32 4 1 1 104 8 4 5 A.7 TRAINING HOURS COMPARISON Figure 11: Training GPU hours of different patch methods and patch threshold. 19 Preprint A.8 PROMPT FOR GPT-4O TO EVALUATE MINIWOB++ TASK System prompt: Youre an expert in evaluating whether the Screenshot successfully completes the Task. =============================Examples============================= Task: Open the settings. Q: What should expect to see on the screenshot if Ive opened the settings? A: should expect to see Im in the settings app. The screenshot shows the home screen of mobile device, with various app icons displayed, including the settings app icon, but the settings app is not opened. Status: failure Screenshot: SCREENSHOT Task: Find hotels in Washington DC Q: What should expect to see on the screenshot if Ive searched for hotels in Washington, DC? A: should expect to see Im in search results page for hotels in Washington, DC. The screenshot shows Google search page with the search field populated with the query hotels in washington dc and list of suggested searches related to hotels in Washington, DC, but it does not show any search results for hotels in Washington, DC. Status: failure Screenshot: SCREENSHOT Task: Whats good restaurant in Portland? Q: What should expect to see on the screenshot if Ive searched for good restaurant in Portland? A: should expect to see Im in search results page for good restaurant in Portland. The screenshot shows Google search page with search input field for good restaurant in portland and map results preview showing business locations near Portland, like Li Pigeon, Portland City Grill, and Higgins. Status: success Screenshot: SCREENSHOT Task: Whats on the menu at In-N-Out? Q: What should expect to see on the screenshot if Ive searched for the menu at In-N-Out? A: should expect to see menu page for In-N-Out, including product names, thumbnails and prices. The screenshot shows Google search page with search input field for In-N-Out menu and some page snippets of In-N-Out indicating potential menu items, but does not actually show the actual menu. Status: failure Screenshot: SCREENSHOT Task: Whats the news in Suriname? Q: What should expect to see on the screenshot if Ive searched for the news in Suriname? A: should expect to see some news in Suriname, such as someone did something or some accident happens in Suriname. The screenshot shows Google search page with search input field for Suriname news today and some page snippets indicating potential news items, but does not actually show the news. Status: failure Screenshot: SCREENSHOT Task: Whats the weather like in Chicago? Q: What should expect to see on the screenshot if Ive searched for the weather in Chicago? A: should expect to see some exact values like temperature, humidity, wind speed, and weather condition in Chicago. The screenshot shows Google search page with search input field for weather in Chicago and some page snippets indicating potential weather information. Although one page snippet contains some weather information, the information is not comprehensive enough to determine the weather in Chicago. Status: failure Screenshot: SCREENSHOT Task: Set an alarm for 6pm. Q: What should expect to see on the screenshot if Ive set an alarm for 6pm? A: should expect to see some alarms including 6pm alarm activated in the clock app. The screenshot shows an attempt to set an alarm for 6pm in the clock app, but the alarm is not set yet. Status: failure Screenshot: SCREENSHOT Task: Whats the news in French today? Q: What should expect to see on the screenshot if Ive searched for the news in French today? A: should expect to see some news in French today, such as someone did something or some accident happens in French today. The screenshot shows Im in the website france24.com but blocked with cookie consent banner. Status: failure Screenshot: SCREENSHOT Task: Whats the news in French today? Q: What should expect to see on the screenshot if Ive searched for the news in French today? A: should expect to see some news in French today, such as someone did something or some accident happens in French today. The screenshot shows Im in the website france24.com and can see the news, like something about the Olympic flame. Status: success Screenshot: SCREENSHOT 20 Preprint A.9 MORE CASES Figure 12: successful task case in AITW-Gen. The instruction is Set an alarm for 6pm. Figure 13: successful task case encountering sign in notes in AITW-Gen. The instruction is How do get to the nearest Lowes?. Figure 14: successful task case in AndroidWorld. The instruction is Delete the following recipes from Broccoli app: Zucchini Noodles with Pesto, Garlic Butter Shrimp, Lentil Soup. Figure 15: successful task case in MiniWob++. The instruction is Follow the instructions shown on the top of the screen: Select 7yJ7, Gwr, 007Vjc, VqwrUC, bKn, w39E and click Submit. 21 Preprint Figure 16: successful task case in MiniWob++. The instruction is Follow the instructions shown on the top of the screen: Enter the username dolores and the password dOBe into the text fields and press login.. A.10 BAD CASE Figure 17: failed task case in AndroidWorld. The instruction is Open the file task.html in Downloads in the file manager; when prompted open it with Chrome. Then click the button 5 times, remember the numbers displayed, and enter their product in the form.. While the model was able to remember the numbers it encountered, it made an error at step 11, calculating 9 10 9 9 9 as 2250."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group",
        "Zhejiang University"
    ]
}