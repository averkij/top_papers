{
    "paper_title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers",
    "authors": [
        "Lukas Schiesser",
        "Cornelius Wolff",
        "Sophie Haas",
        "Simon Pukrop"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at https://github.com/PictSure/pictsure-library."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 4 8 4 1 . 6 0 5 2 : r PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers Lukas Schiesser German Research Center for AI (DFKI) lukas.schiesser@dfki.de Cornelius Wolff German Research Center for AI (DFKI) cornelius.wolff@dfki.de Sophie Haas German Research Center for AI (DFKI) sophie.haas@dfki.de Simon Pukrop German Research Center for AI (DFKI) simon.pukrop@dfki.de"
        },
        {
            "title": "Abstract",
            "content": "Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding modelits architecture, pretraining, and training dynamicsat the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at github.com/PictSure/pictsure-library."
        },
        {
            "title": "Introduction",
            "content": "Modern image classification models have achieved impressive accuracy, but their success often relies on large, curated datasets. While transfer learning has reduced the need to train models from scratch, many existing pipelines still depend on thousands of labeled images per class to yield competitive performance [24, 35]. This reliance becomes critical bottleneck in real-world domains such as medical imaging or agriculture, where collecting high-quality labeled data is particularly costly and resource intensive, rendering the annotation process infeasible [47, 20, 19]. Expert input is essential for tasks such as weed discrimination and breast cancer classification, yet specialists are often scarce and unable to devote the necessary time to tedious labeling tasks [10, 2]. In medicine, strict privacy regulations and high ethical standards further reduce the number of annotated samples [32]. These application-driven challenges highlight the necessity for models that can learn from extremely limited supervisionpotentially just few images per category. Few-shot image classification (FSIC) targets this need by enabling models to recognize unseen classes using only handful of labeled examples [38]. FSIC methods are often trained using episodic learning, where models adapt to new tasks via gradient-based fine-tuning on small support These authors contributed equally to this work. Preprint. Under review. sets [5, 41, 8]. However, such models often struggle when confronted with significant domain shifts between training and testing classes [26]. More importantly, fine-tuning on novel tasks is computationally expensive and sometimes impractical in deployment scenarios [14, 6, 25]. Notable models include Prototypical Networks [38] as well as Matching Networks [40] which are both examples for metric-based approaches. These approaches cluster the embeddings of the support samples and assign the label of the nearest cluster to query sample. In-context learning (ICL) offers promising alternative by allowing models to condition on small number of labeled examples directly at inference timewithout any parameter updates [3]. Initially demonstrated in large language models, ICL has recently gained traction in the vision community [27, 13, 48]. Transformer-based models are provided with sequence of image-label pairs (support set) alongside query image, and are trained to predict the label of the query based solely on the context [13]. This framework holds particular appeal for scenarios with strict data constraints or dynamic task definitions. ICL further distinguishes itself from traditional FSIC approaches by eliminating the need for backward pass during adaptation, allowing for immediate deployment without any fine-tuning. This is possible because all necessary task information is provided through the context at inference time, enabling the model to make predictions without modifying its internal parameters [9]. Visual ICL can be broadly categorized into two approaches: vision-language-based ICL and visiononly ICL. Vision-language-based ICL, as demonstrated by models like GPT-4o [16], leverages multimodal embeddings that align visual and textual representations [12, 26, 46]. These models rely on pretraining with large-scale image-text datasets, enabling them to perform tasks that require semantic understanding across modalities. However, their reliance on textual alignment can limit their effectiveness in domains where semantic labels are ambiguous or absent [36, 22]. Unlike methods requiring textual alignment, vision-only ICL relies purely on visual embeddings, making it well-suited for tasks with minimal language semantics. Yet, it remains underexplored, especially for in-context classification with transformers. Prior work has largely overlooked how different image encoders and their pretraining strategies affect performance in vision-only ICL [13]. We argue that embedding architecture and training are critical for FSIC via ICLsupported by evidence that linear classifier atop strong embeddings can outperform complex meta-learning approaches [39], highlighting the importance of high-quality representations even without fine-tuning. To investigate the influence of different embedding models, we introduce PictSure, transformer based ICL model, which operates within purely visual representation space, unlike prior works such as SgVA-CLIP [27] and CAML [13]. While SgVA-CLIP enhances CLIP-based features through an adapter and knowledge distillation, it remains dependent on semantic alignment with text prompts, potentially overlooking critical visual nuances. Similarly, CAML employs CLIP embeddings to perform ICL over set of images and their corresponding labels. With PictSure, we employ both convolutional backbones (ResNet) [15] and Vision Transformers (ViT) [11], each pretrained either with standard supervised learning on the ImageNet21k dataset [31] or with an additional triplet loss to enforce semantically meaningful feature space [33]. We find that vision-only ICL models trained without pretrained encoders fail to learn meaningful representations, underscoring the importance of representation quality. While both pretrained ResNets and ViTs can support ICL, ViTs require additional metric-based objectives (i.e., the triplet loss) during pretraining to be most effective for few-shot generalization. Additionally, we evaluated our model variations against CAML, CLIP-based ICL model [13] and k-nearest neighbors (KNN) classifier to benchmark their performance on both in-domain and out-of-domain datasets. Therefore, the main contributions of this paper are: (1) We introduce PictSure, transformer-based in-context learner that conditions on visual image-label pairs without any fine-tuning or reliance on language supervision. (2) We systematically analyse the influence of using different embedding models and pretraining techniques in visual ICL for FSIC. (3) We show that certain variations of our PictSure model significantly outperform existing methods on out-of-domain datasets (e.g., Bone Break, Brain Tumor, OrganCMNIST), validating the benefits of purely visual embedding space for generalization under weak semantic alignment."
        },
        {
            "title": "2 Background",
            "content": "FSIC is often framed as an n-way k-shot problem where represents the number of randomly selected classes from dataset and refers to the number of randomly selected images per class [40]. FSIC models are typically trained using an episode training approach, i.e., the model is presented with support set of labeled images randomly selected from larger training corpus and query set consisting of one or multiple unlabeled images selected randomly from the chosen classes [21]. The goal of an FSIC model is to accurately classify images in the query set based on an existing support set which were all sampled from previously unseen categories. Common benchmarks for evaluating FSIC models include miniImageNet [40], tieredImageNet [30] or CIFAR-100 [18], with typical tasks being 5-way 5-shot and 5-way 1-shot classification tasks. Generally, FSIC models excel in in-domain settings where novel categories are similar to the training dataset but they struggle in out-of-domain settings, i.e., when they are presented with entirely new visual categories [26, 21]. Therefore, the models usually need to be fine-tuned on data belonging to the out-domain to obtain satisfactory performance. promising alternative to fine-tuning is ICL, which refers to the ability of model to learn new tasks purely by conditioning on input-output examples provided in the prompt, without parameter updates [3]. Mathematically, ICL can be framed as conditional estimation problem. Given demonstration set with example pairs {(x1, y1), ..., (xm, ym)} and query xq, an ICL architecture predicts an output ˆyq via learned function: Fθ : (D, xq) ˆyq where Fθ is the underlying model with fixed parameters θ. While this ability is widely known in transformer-based large language models, it also extends to other transformer-based architectures [48]. Recent work such as CAML [13] has already applied ICL to the problem of FSIC. It relies on image encoders that were pretrained with language supervision like CLIP or Vision Transformers trained on Laion-2b, dataset of approximately 2.32 billion image-text-pairs [34]. While CAML performs very well on in-domain benchmarks like tieredImageNet and miniImageNet, its performance falls short in out-of-domain datasets like ChestX [42] or FGVC-Aircraft [23]. This suggests that embedding spaces grounded in natural language representations, such as CLIP, can have difficulty differentiating images when semantic labels represent fine-grained distinctions not captured by the broad captions used during CLIPs training [13, 29]. For example, while two chest x-rays have different labels due to different pathologies, the captions used to train CLIP may not reflect that level of granularity. Therefore, we focus on image encoders that directly predict numerical labels avoiding the pitfalls that arise with the use of image encoders with language supervision."
        },
        {
            "title": "3 PictSure: Model Overview",
            "content": "Our proposed model, PictSure, employs transformer-based architecture to perform ICL for FSIC. The model processes sequence comprising labeled support images and query image, enabling it to predict the querys label based on the contextual information provided by the support set. The attention masks are specifically designed to facilitate this process: support tokens are allowed to attend to all other support tokens, capturing relationships and patterns within the context. Similarly, the query token attends to all support tokens, leveraging the contextual information for classification. However, support tokens are restricted from attending to the query token, ensuring that the representation of the query image is solely influenced by the support set and not vice versa. This asymmetric attention mechanism aligns with the meta-inference behavior characteristic of few-shot learning, where the query image is classified based on the support set without mutual influence. 3.1 Architecture We formally define the architecture of the PictSure model variations as follows. Let be the number of classes (or labels) to be distinguished and be the number of provided support images, where n. Note here, that fixed and equal amount of support images per label is not technical constraint of the architecture. For consistency with other n-way k-shot models we will however assume that, and denote that number by k, which gives us = nk. Each support image xi, {1, . . . , m} is encoded using visual encoder ϕimg to produce an embedding vi = ϕimg(xi) Rd, for an embedding dimension which is achieved by projection 3 layer included in ϕimg. The associated label yi {0, 1}n is one-hot encoded and projected through learned label embedding function ϕlbl, resulting in ℓi = ϕlbl(yi) Rd. These two components are concatenated to form joint token ti = [vi; ℓi] R2d. The query image xq is similarly encoded as vq = ϕimg(xq), and concatenated with zero vector to maintain consistent dimensions: tq = [vq;0] R2d. The full input to the Transformer is the sequence [t1, t2, . . . , tm, tq], which is processed by Transformer encoder that applies self-attention over the entire sequence and returns only the embedding representation of the query token tq. Importantly, the attention mechanism is asymmetrically masked to enforce causal structure specific to the ICL setting: all support tokens t1, . . . , tm can attend to one another, enabling mutual interaction among reference examples. The query token tq, however, is allowed to attend to all support tokens, while the support tokens are prevented from attending to tq. This ensures that the model can use the support set to condition its prediction for the query, but not vice versamirroring the meta-inference behavior expected in few-shot learning. After encoding, the query tokens output representation is extracted and passed through classification head fcls, yielding the predicted label distribution: ˆyq = fcls(T ([t1, t2, . . . , tm, tq])) Rn This architectural setup allows PictSure to learn task-specific mappings from context alone. In contrast to earlier works such as SgVA-CLIP or CAML, which depend on semantically-aligned embedding spaces like CLIP, PictSure learns from raw image embeddings without the need for semantic alignment. This design offers improved robustness in domains with ambiguous or nonsemantic visual-label associations, such as medical imaging or fine-grained classification. An overview of the full architecture is shown in Figure 1. Figure 1: Overview of the PictSure architecture The ICL Transformer of all PictSure variations always consist of four Transformer blocks, each with eight attention heads and model dimension of 1028 and forward dimension of 2048. Combined with ResNet18 as its embedding model, this results in model size of 53M parameters, whereas the ViT model comprises combined 128M parameters. For the full training hyperparameters, please refer to Appendix A. 3.2 Dataset and Task We used ImageNet-21K [31] to train our model due to its diverse image base of over 14 million images spanning 21,000 classes. For validation purposes we randomly removed 19 classes from the training set and used them across all training runs consistently. In order to improve generalization, 4 we preprocessed the images by applying Gaussian Blur and randomly adjusting the sharpness of the image. Finally, all images were scaled to 224x224 pixels. We trained our models on 10-way 5-shot classification task, i.e. selecting ten random classes and five random images per class resulting in support set of of 50 images. The query sample was taken randomly from one of the selected ten classes. Therefore, the task of the model was to predict the class of the query sample based on the support set, i.e., few shot image classification task. 3.3 Training Loop All models were trained for 600 epochs on NVIDIA H100 GPUs with 80GB VRAM. Each epoch utilized 10,000 training samples, each sample consisting of support and query set as previously described. To investigate the impact of different embedding models and training strategies on classification performance we conducted several experiments. These experiments varied the learning rates applied to the visual encoder and the remaining architecture, and explored both joint training from the start and delayed training as well as no training of the visual encoder. All experimental configurations and further information on the training setup are detailed in Appendix A."
        },
        {
            "title": "4 Pretraining Embeddings",
            "content": "In this section, we explore the impact of pretraining strategies on the performance of image encoders used in our PictSure framework. We evaluate two popular architectures, ResNet and Vision Transformer (ViT), under various pretraining regimes, including training from scratch, standard supervised pretraining, and pretraining with auxiliary objectives such as the triplet loss. 4.1 ResNet First, we tested ResNet [15], widely used convolutional architecture, as the backbone for generating image embeddings in our framework. Its output feature vectors are linearly projected to ensure compatibility with the Transformer input space. These embeddings are then used to construct the tokenized input sequence for the ICL Transformer, enabling in-context reasoning. We found that training the ICL model without using pretrained weights consistently failed to converge. The randomly initialized ResNet backbone produced unstable and unstructured embeddings, which the Transformer struggled to interpret effectively in the ICL setting. This suggests that meaningful visual representations are crucial even in scenarios where task-specific reasoning is learned by the Transformer. Introducing pretrained weights for the ResNet backbone already led to substantial performance boost, elevating accuracy from chance level to 82.6%, as can be observed in Figure 2a. This confirms that initializing the visual encoder with features learned on large-scale datasets such as ImageNet significantly improves the representational quality of the image embeddings. (a) Training the PictSure and the ResNet embeddings from the start (b) Training PictSure with fixed embeddings in the beginning (c) Training the PictSure with fixed ResNet embeddings Figure 2: Training the PictSure with ResNet embeddings Figure 2b shows that further performance gains were achieved by delaying the fine-tuning of the visual encoder until after the 100th training episode. This approach allowed the Transformer to first adapt to the fixed embedding space before any changes were introduced via gradient updates to the encoder. With this strategy, we observed an improved classification accuracy of 84.6%. 5 Interestingly, the best results, as shown in Figure 2c, were obtained when keeping the ResNet encoder completely frozen throughout training. This setup led to an accuracy of 88.4%, outperforming all other configurations. We attribute this to the increased stability of the fixed embedding space, which allows the Transformer to form consistent token-level representations across tasks. In contrast, continuously shifting embedding distribution (as seen in end-to-end training) introduces noise and variance that can hinder the meta-learning dynamics crucial for few-shot classification. 4.2 Vision Transformer As an alternative to convolutional architectures, we also explored using Vision Transformers (ViT) as the image embedding backbone in our pipeline [11]. However, training from scratchwithout any form of pretrainingdid not work, just as with ResNet. The model consistently failed to converge meaningfully despite extensive hyperparameter tuning. ViTs operate by splitting an image into fixed-size patches, projecting each into an embedding, and processing the resulting sequence with Transformer encoder layers. special classification token is prepended, and positional encodings are added to retain spatial structure. The output embedding from that classification token is then used as the embedding vector. Standard Supervised Pretraining. We first pretrained the ViT encoder on ImageNet using standard supervised learning with only the cross-entropy classification loss. No auxiliary objectives or additional regularization techniques were applied. The resulting ViT encoder achieved classification accuracy of 69.0% during pretraining. However, despite these promising pretraining results, the ICL process remained unstable. Training dynamics exhibited significant fluctuations in both loss and accuracy curves across runs, and convergence behavior was inconsistent. (a) Training PictSure using the pretrained ViT embeddings without Triplet-Loss (b) Training PictSure with fixed pretrained ViT embeddings with Triplet-Loss in the beginning (c) Training PictSure with fixed pretrained ViT embeddings with Triplet-Loss Figure 3: Training the ICL transformer with ViT embeddings Triplet-Loss Augmented Pretraining. To address this instability, we introduced an auxiliary Triplet Loss during the pretraining of the ViT encoder. This additional supervision encourages the encoder to produce more structured embedding space by pulling semantically similar embeddings closer together and pushing dissimilar ones further apart. Formally, for an anchor a, positive sample p, and negative sample n, the triplet loss is defined as: Ltriplet = max (cid:0)0, (a) (p)2 (a) (n)2 + α(cid:1) where () is the encoder output and α is margin hyperparameter. The final training objective combined the triplet loss with the standard classification loss using weighted sum, Ltotal = Lclass + λ Ltriplet, where λ controls the influence of the auxiliary loss component. Interestingly, the inclusion of the triplet loss had little impact on top-1 classification performance during pretraining, which remained stable at approximately 68.0%. However, its effect on the downstream ICL task was significant. The structured embedding space resulting from the triplet supervision led to marked improvements in training stability, reducing variance across training seeds and resulting in an accuracy of 80%, which can be seen in Figure 3b. 6 As with ResNet, the best downstream performance was achieved when the ViT encoder was kept frozen throughout ICL training. This configuration, combined with triplet-loss augmented pretraining, led to the highest classification accuracy of 87% among all ViT variants. We attribute this result to the stability provided by fixed, well-structured embedding space, which allowed the Transformer to learn reliable task-specific mappings without being affected by distributional drift from ongoing updates to the encoder. Label Insertion Layer. While Qu et al. [28] demonstrated that inserting labels at later layer significantly improves accuracy for ICL with tabular data, our experiments with the pretrained ViT encoder revealed that this strategy did not have noticeable impact on the performance of our image-based ICL model. We tested various configurations for label insertion, including early and late layers, but observed no substantial difference in accuracy across these setups."
        },
        {
            "title": "5 Model Evaluation",
            "content": "We evaluated the performance of our PictSure model variations across diverse range of visual domains. Both in-domain and out-of-domain datasets, comprising general, agricultural, and medical imagery, were used for the evaluation, namely tieredImageNet [30], miniImageNet [40], PlantDoc [37], Crop Diseases Classification dataset [1], Bone Break Classification dataset [7], Brain Tumor Classification (MRI) dataset [4], OrganCMNIST [45], and FGVC-Aircraft [23]. These datasets were selected to represent wide array of real-world image classification tasks, ranging from generalpurpose images to domain-specific problems. The experiments were conducted using few-shot classification setup with n-way k-shot tasks, where in each task, = 5 distinct classes are considered, each represented by {1, 5} labeled support images. Let the support set be denoted as = {(xi, yi)}nk where each of the classes contributes exactly randomly sampled image-label pairs (xi, yi). single query image xnk+1 is drawn from one of the selected classes without its label. The objective is then to correctly classify the query image using solely the information from S. This sampling approach was adopted to ensure class diversity while mitigating potential sampling bias. The performance of the model was averaged over 5000 tasks per dataset and reported using mean accuracy and standard error in order to capture both the effectiveness and the consistency of the model. i=1, Notation. We introduce the notation PictSureA B,C,D to refer to our model and training variations in table 1. denotes the embedding model with ResNet, ViT and ViT-Trip representing ResNet18, vision transformer and vision transformer with the added triplet loss respectively, as described in section 4. superscript at the end of denotes that the embedding model was pretrained. and denote the learning rate of the transformer body and embedding model respectively, where means constant learning rate and scheduled learning rate. Finally, is set to 0 if the embedding model is trained from the start, and to if the embedding model is frozen during the warm-up phase. Consequently PictSureResNetp c,c,0 would denote our model using pretrained ResNet embeddings, with constant learning rates and simultaneous training start for both the transformer body and the embedding model. Baselines. The performances of our PictSure model variations were compared against two wellestablished baselines. Firstly, as suggested by Wang et al. [43], KNN classifier was used as baseline. The embedding space was constructed from the support images using standard pretrained ResNet18. Each query image was then classified based on the majority label of its five nearest neighbors in the embedding space. Secondly, pretrained CAML model, constituting competitive few-shot learning method with CLIP image encoder, was employed and evaluated on the same samples [13]. Table 1 presents the results of all PictSure model variations compared with CAML and our KNN baseline. Despite being significantly smaller than CAML (half to seventh the size), PictSure variants with frozen embeddings achieve comparable performance on ImageNet subsets and generalize better to medical out-of-domain datasets. While ResNet18 and vision transformer variants trained with tripled loss perform similarly, the latter shows slight advantage. CAML outperforms PictSure when CLIP embeddings provide more informative features than those from ResNet or vision transformers used in PictSure. 7 Model Model Size tieredImageNet miniImageNet PlantDoc Crop Diseases s,s,0 c,c,0 s,c,0 -,s,- s,s,t s,c,t CAML KNN PictSureResNet c,c,0 PictSureResNetp PictSureResNetp PictSureResNetp PictSureResNetp PictSureResNetp PictSureViT PictSureViTp PictSureViTp PictSureViTp PictSureViTp PictSureViT-Tripp PictSureViT-Tripp PictSureViT-Tripp PictSureViT-Tripp PictSureViT-Tripp s,s,0 -,s,- s,c,t s,s,t s,c,t -,s,- s,s,t s,s,0 s,c,0 5-w 5-s 5-w 1-s 5-w 5-s 5-w 1-s 5-w 5-s 5-w 1-s 5-w 5-s 5-w 1-s 98.3 0.2 21.3 0.6 19.8 0.6 83.7 0.5 83.3 0.5 81.7 0.6 84.3 0.5 20.2 0.6 19.8 0.6 72.1 0.6 66.8 0.7 76.9 0.6 91.3 0.4 81.2 0.6 79.6 0.6 63.5 0.7 92.2 0.4 81.5 0.6 95.5 0.3 19.2 0.6 19.2 0.6 65.2 0.7 67.6 0.7 67.1 0.7 64.4 0.7 20.4 0.6 19.2 0.6 57.1 0.7 55.2 0.7 61.1 0.7 72.4 0.6 61.1 0.7 61.5 0.7 49.7 0.7 76.5 0.6 67.0 0.7 99.2 0.1 20.8 0.6 20.0 0.6 86.4 0.5 86.1 0.5 84.8 0.5 88.7 0.5 19.8 0.6 20.0 0.6 76.2 0.6 70.5 0.7 79.2 0.6 92.9 0.4 84.2 0.5 81.7 0.6 68.2 0.7 93.9 0.3 84.3 0.5 97.2 0.2 19.7 0.6 19.7 0.6 68.5 0.7 69.6 0.7 70.4 0.7 68.9 0.7 20.6 0.6 19.7 0.6 61.6 0.7 59.0 0.7 64.6 0.7 73.9 0.6 65.2 0.7 65.5 0.7 52.9 0.7 78.2 0.6 70.6 0.6 74.8 0.6 20.5 0.6 20.0 0.6 54.2 0.7 51.4 0.7 50.5 0.7 52.8 0.7 19.9 0.6 20.0 0.6 38.8 0.7 24.5 0.6 37.6 0.7 56.3 0.7 53.2 0.7 51.3 0.7 35.4 0.7 53.4 0.7 51.0 0. 57.7 0.7 20.4 0.6 20.4 0.6 37.5 0.7 36.5 0.7 34.9 0.7 36.4 0.7 20.6 0.6 20.4 0.6 29.8 0.7 21.4 0.6 34.2 0.7 35.3 0.7 36.6 0.7 35.0 0.7 28.5 0.6 32.4 0.7 36.0 0.7 36.0 0.7 19.2 0.6 20.7 0.6 30.4 0.7 30.5 0.7 30.1 0.7 29.7 0.7 19.7 0.6 20.7 0.6 23.7 0.6 14.7 0.5 26.0 0.6 32.1 0.7 29.1 0.6 32.6 0.7 25.6 0.6 31.7 0.7 31.0 0.7 29.4 0.6 19.7 0.6 19.7 0.6 25.4 0.6 27.3 0.6 26.9 0.6 25.4 0.6 19.7 0.6 19.7 0.6 21.9 0.6 19.2 0.6 24.6 0.6 25.9 0.6 25.6 0.6 26.7 0.6 25.4 0.6 24.0 0.6 25.0 0.6 380M 11M 53M 53M 53M 53M 53M 53M 128M 128M 128M 128M 128M 128M 128M 128M 128M 128M Model Model Size Bone Break Brain Tumor OrganCMNIST FGVC-Aircraft s,c,t s,s,t s,s,0 -,s,- s,c,0 c,c,0 CAML KNN PictSureResNet c,c,0 PictSureResNetp PictSureResNetp PictSureResNetp PictSureResNetp PictSureResNetp PictSureViT PictSureViTp PictSureViTp PictSureViTp PictSureViTp PictSureViT-Tripp PictSureViT-Tripp PictSureViT-Tripp PictSureViT-Tripp PictSureViT-Tripp s,c,t s,s,t s,s,0 s,s,0 s,c,t s,s,t -,s,- -,s,- s,c,0 5-w 5-s 5-w 1-s 5-w 5-s 5-w 1-s 5-w 5-s 5-w 1-s 5-w 5-s 5-w 1-s 26.7 0.6 19.3 0.6 20.0 0.6 29.6 0.7 29.0 0.6 28.0 0.6 28.5 0.6 20.2 0.6 20.0 0.6 8.2 0.4 11.9 0.5 24.3 0.6 29.1 0.6 30.5 0.7 30.2 0.7 24.5 0.6 27.4 0.6 29.5 0.7 24.6 0.6 21.0 0.6 21.0 0.6 23.1 0.6 23.3 0.6 22.1 0.6 22.6 0.6 20.9 0.6 21.0 0.6 8.8 0.4 16.7 0.5 23.8 0.6 22.5 0.6 22.2 0.6 23.0 0.6 21.1 0.6 21.0 0.6 23.2 0. 25.2 0.6 25.0 0.6 25.3 0.6 52.2 0.7 46.4 0.7 47.0 0.7 47.7 0.7 24.8 0.6 25.3 0.6 24.1 0.6 11.8 0.5 38.0 0.7 55.8 0.7 51.0 0.7 51.5 0.7 27.2 0.6 51.9 0.7 47.3 0.7 6.2 0.3 24.3 0.6 24.3 0.6 37.3 0.7 35.0 0.7 36.7 0.7 34.9 0.7 25.4 0.6 24.3 0.6 19.6 0.6 18.3 0.6 31.5 0.7 41.1 0.7 37.0 0.7 37.5 0.7 28.5 0.6 35.0 0.7 35.1 0.7 53.9 0.7 20.4 0.6 19.0 0.6 59.4 0.7 58.9 0.7 56.8 0.7 62.8 0.7 20.0 0.6 19.0 0.6 42.0 0.7 38.3 0.7 47.5 0.7 59.2 0.7 64.1 0.7 61.4 0.7 48.6 0.7 59.4 0.7 59.3 0.7 42.0 0.7 19.5 0.6 19.5 0.6 44.4 0.7 45.1 0.7 40.9 0.7 43.6 0.7 21.1 0.6 19.5 0.6 33.6 0.7 36.5 0.7 42.5 0.7 44.8 0.7 48.2 0.7 47.7 0.7 37.9 0.7 41.4 0.7 47.8 0.7 76.6 0.6 21.0 0.6 18.8 0.6 40.8 0.7 42.1 0.7 35.7 0.7 44.7 0.7 20.0 0.6 18.8 0.6 26.1 0.6 5.4 0.3 28.6 0.6 39.9 0.7 38.6 0.7 38.7 0.7 26.6 0.6 39.9 0.7 35.4 0.7 63.6 0.7 20.2 0.6 20.2 0.6 31.0 0.7 31.3 0.7 28.3 0.6 32.5 0.7 20.9 0.6 20.2 0.6 23.5 0.6 8.5 0.4 26.2 0.6 25.8 0.6 30.0 0.7 29.9 0.7 22.9 0.6 26.9 0.6 28.1 0. 388M 11M 53M 53M 53M 53M 53M 53M 128M 128M 128M 128M 128M 128M 128M 128M 128M 128M Table 1: Test results across all PictSure model variants compared to CAML and KNN baseline Context Length and Overfitting. We investigated the impact of context length on the performance of our PictSure model variations, ranging from 1-shot to 10-shot classification. The results in Figure 4 show that increasing the number of support images generally improves classification accuracy, as more examples provide richer contextual information for the model to leverage. However, the performance gains diminish with additional support images, indicating saturation point beyond which further examples yield little benefit. Notably, while Fifty et al. [13] reported overfitting when fine-tuning the embedding model during training of CAML, we did not observe this issue in our experiments. This suggests that our method of integrating and fine-tuning the pretrained embedding model within the ICL framework is more robust. (a) Accuracy increase for Brain Tumor dataset with increasing context length. (b) Accuracy increase for Crop Diseases dataset with increasing context length. (c) Accuracy increase for ImageNet dataset with increasing context length. Figure 4: Impact of context length on accuracy for different datasets."
        },
        {
            "title": "6 Analysis",
            "content": "In this work, we examined the effects of image encoder choice, pretraining objectives, and fine-tuning strategies on FSIC performance within an ICL framework. Our results demonstrate that the selection of vision encoders and its pretraining significantly impacts downstream performance, particularly in out-of-domain settings where the data distribution diverges from the training data. This is supported by our finding that training models without pretrained encoders perform poorly and do not converge, emphasizing the critical importance of pretraining. Pretrained image encoders enable more effective extraction of features that are relevant to FSIC tasks, thereby enhancing classification accuracy. Furthermore, we observed that not only pretraining itself but also the choice of pretraining objective plays crucial role in model performance. For instance, models employing vision transformer (ViT) pretrained with an additional triplet loss exhibited more stable behavior across training regimes compared to their counterparts, which were only trained using the cross-entropy classification loss. This shows that triplet loss provides more robust embedding space that is less perturbed during downstream training. These findings are in-line with existing research on other non-ICL FSIC approaches, which showed that ViTs without additional training objects are inferior to classic CNNs [8]. The gap in performance on medical datasets such as Brain Tumor highlights limitations in using CLIP-based models like CAML. CLIPs pretraining, primarily on natural imagery [29, 44], limits its generalization to domains with more abstract or specialized content. CAMLs sometimes belowchance performance, compared with our models significantly better accuracy, suggests that CLIP embeddings may not only be uninformative but actively misleading for such domains. While CAML excels on in-domain datasets like miniImageNet, our models demonstrate superior transferability to out-of-domain datasets such as Brain Tumor and OrganCMNIST, albeit at the cost of reduced in-domain performance. This comparison reveals that language-based pretraining, while powerful in natural image domains, is suboptimal for categories lacking strong semantic associations. Our language-agnostic models demonstrate better generalization to abstract and out-of-domain contexts. Importantly, our encoders ResNet18 and custom ViTwere trained on smaller, less diverse datasets (ImageNet-1K and ImageNet), yet still outperformed the more broadly trained CLIP in these challenging settings. This underscores that generalization stems more from pretraining objectives and encoder integration than data scale alone."
        },
        {
            "title": "7 Outlook and Conclusion",
            "content": "Our study introduces PictSure, compact, efficient FSIC model that outperforms larger alternatives like CAML on challenging out-of-domain benchmarks. While constrained to 10-way classification due to architectural and training choices, our model is tailored for limited-data regimes where such constraints are common. Future work will focus on extending the classification layer to support broader class range without sacrificing the models lightweight design and robust generalization. Future research could explore the implications of scaling laws in the context of FSIC. Drawing inspiration from findings in large language models [17], increasing the diversity of training data and the complexity of the model architecture may lead to significant performance improvements. Such investigations could focus on enhancing both in-domain and out-of-domain generalization by enabling the model to capture richer representations and more intricate dependencies. This could lead to models that are able to solve numerous FSIC problems with sufficient accuracy, especially in domains like medicine, where data collection is often costly. In summary, our results emphasize the importance of the vision encoder choice for FSIC, especially under distribution shift. Robust pretrainingparticularly with objectives like triplet losscombined with careful architectural integration, enables strong generalization even in compact models, making PictSure strong candidate for real-world few-shot learning tasks."
        },
        {
            "title": "References",
            "content": "[1] Crop Diseases Classification. URL https://datasets.omdena.com/dataset/ crop-diseases-classification. [2] Raja Waseem Anwar, Muhammad Abrar, and Faizan Ullah. Transfer Learning in Brain Tumor Classification: Challenges, Opportunities, and Future Prospects. In 2023 14th International Conference on Information and Communication Technology Convergence (ICTC), pages 2429, Jeju Island, Korea, Republic of, October 2023. IEEE. ISBN 9798350313277. doi: 10.1109/ICTC58733.2023.10392830. URL https://ieeexplore.ieee.org/document/ 10392830/. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877 1901. Curran Associates, Inc., 2020. URL https://papers.nips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. [4] Navoneel Chakrabarty and Swati Kanchan. Brain Tumor Classification (MRI). URL https: //www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri. [5] Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, and Ying Wei. Unleashing the power of meta-tuning for few-shot generalization through sparse interpolated experts. In Proceedings of the 41st International Conference on Machine Learning, pages 72807297, 2024. [6] Yuan-Chia Cheng, Ci-Siang Lin, Fu-En Yang, and Yu-Chiang Frank Wang. Few-Shot Classification in Unseen Domains by Episodic Meta-Learning Across Visual Domains. In 2021 IEEE International Conference on Image Processing (ICIP), pages 434438, Anchorage, AK, USA, September 2021. IEEE. ISBN 978-1-66544-115-5. doi: 10.1109/ICIP42928.2021.9506141. URL https://ieeexplore.ieee.org/document/9506141/. [7] Curso. Bone Break Classification Dataset, February 2024. URL https://universe. roboflow.com/curso-rphcb/bone-break-classification. Publication Title: Roboflow Universe Type: Open Source Dataset Published: https://universe.roboflow.com/cursorphcb/bone-break-classification. [8] Bowen Dong, Pan Zhou, Shuicheng Yan, and Wangmeng Zuo. Self-promoted supervision for few-shot transformer. In European conference on computer vision, pages 329347. Springer, 2022. [9] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. Survey on In-context Learning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11071128, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.emnlp-main.64. URL https://aclanthology.org/2024.emnlp-main.64/. [10] Alessandro Dos Santos Ferreira, Daniel Matte Freitas, Gercina Gonçalves Da Silva, Hemerson Pistori, and Marcelo Theophilo Folhes. Unsupervised deep learning and semi-automatic data labeling in weed discrimination. Computers and Electronics in Agriculture, 165: 104963, October 2019. doi: 10.1016/j.compag.2019.104963. URL https://linkinghub.elsevier.com/retrieve/pii/S0168169919313237. ISSN 01681699. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, June 2021. URL http://arxiv.org/abs/2010.11929. arXiv:2010.11929 [cs]. 10 [12] Dyke Ferber, Georg Wölflein, Isabella C. Wiest, Marta Ligero, Srividhya Sainath, Narmin Ghaffari Laleh, Omar S. M. El Nahhas, Gustav Müller-Franzes, Dirk Jäger, Daniel Truhn, and Jakob Nikolas Kather. In-context learning enables multimodal large language models to classify cancer pathology images. Nature Communications, 15(1):10104, November 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-51465-9. URL https://www.nature.com/ articles/s41467-024-51465-9. [13] Christopher Fifty, Dennis Duan, Ronald Guenther Junkins, Ehsan Amid, Jure Leskovec, Christopher Re, and Sebastian Thrun. Context-Aware Meta-Learning. October 2023. URL https://openreview.net/forum?id=lJYAkDVnRU. [14] Kai He, Nan Pu, Mingrui Lao, and Michael S. Lew. Few-shot and meta-learning methods for image understanding: survey. International Journal of Multimedia Information Retrieval, 12(2):14, December 2023. ISSN 2192-6611, 2192-662X. doi: 10.1007/s13735-023-00279-4. URL https://link.springer.com/10.1007/s13735-023-00279-4. [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, Las Vegas, NV, USA, June 2016. IEEE. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.90. URL http://ieeexplore.ieee.org/document/7780459/. [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [17] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language Models, January 2020. URL http://arxiv.org/abs/2001.08361. arXiv:2001.08361 [cs]. [18] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. April 2009. [19] Jiajia Li, Dong Chen, Xinda Qi, Zhaojian Li, Yanbo Huang, Daniel Morris, and Xiaobo Tan. Label-efficient learning in agriculture: comprehensive review. Computers and Electronics in Agriculture, 215:108412, December 2023. ISSN 01681699. doi: 10.1016/j.compag.2023.108412. URL https://linkinghub.elsevier.com/retrieve/ pii/S0168169923008001. [20] Jiajia Li, Dong Chen, Xinda Qi, Zhaojian Li, Yanbo Huang, Daniel Morris, and Xiaobo Tan. Label-efficient learning in agriculture: comprehensive review. Computers and Electronics in Agriculture, 215:108412, 2023. Publisher: Elsevier. [21] Ying Liu, Hengchang Zhang, Weidong Zhang, Guojun Lu, Qi Tian, and Nam Ling. Few-Shot Image Classification: Current Status and Research Trends. Electronics, 11(11):1752, January 2022. ISSN 2079-9292. doi: 10.3390/electronics11111752. URL https://www.mdpi. com/2079-9292/11/11/1752. Number: 11 Publisher: Multidisciplinary Digital Publishing Institute. [22] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models. [23] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. FineGrained Visual Classification of Aircraft, June 2013. URL http://arxiv.org/abs/1306. 5151. arXiv:1306.5151 [cs]. [24] Sandeep Kumar Mathivanan, Sridevi Sonaimuthu, Sankar Murugesan, Hariharan Rajadurai, Basu Dev Shivahare, and Mohd Asif Shah. Employing deep learning and transfer learning for accurate brain tumor detection. Scientific Reports, 14(1):7232, March 2024. ISSN 20452322. doi: 10.1038/s41598-024-57970-7. URL https://www.nature.com/articles/ s41598-024-57970-7. [25] Akihiro Nakamura and Tatsuya Harada. Revisiting Fine-tuning for Few-shot Learning, October 2019. URL http://arxiv.org/abs/1910.00216. arXiv:1910.00216 [cs]. 11 [26] Jaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun Song, and Se-Young Yun. Understanding cross-domain few-shot learning based on domain similarity and fewshot difficulty. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, pages 26222636, Red Hook, NY, USA, November 2022. Curran Associates Inc. ISBN 978-1-71387-108-8. [27] Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, and Changsheng Xu. SgVA-CLIP: Semantic-Guided Visual Adapting of Vision-Language Models for Few-Shot Image Classification. ISSN 1520-9210, 1941-0077. doi: 10.1109/TMM.2023.3311646. URL https://ieeexplore.ieee.org/ document/10243119/. IEEE Transactions on Multimedia, 26:34693480, 2024. [28] Jingang Qu, David Holzmüller, Gaël Varoquaux, and Marine Le Morvan. TabICL: Tabular Foundation Model for In-Context Learning on Large Data, February 2025. URL http:// arxiv.org/abs/2502.05564. arXiv:2502.05564 [cs]. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and others. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [30] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua Tenenbaum, Hugo Larochelle, and Richard Zemel. META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION. 2018. [31] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik. ImageNet-21K Pretraining In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Inforfor the Masses. mation Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/ 2021/file/98f13708210194c475687be6106a3b84-Paper-round1.pdf. [32] Selvakanmani S, Dharani Devi, Rekha V, and Jeyalakshmi. Privacy-Preserving Journal Breast Cancer Classification: Federated Transfer Learning Approach. of Imaging Informatics in Medicine, 37(4):14881504, February 2024. ISSN 29482933. doi: 10.1007/s10278-024-01035-8. URL https://link.springer.com/10.1007/ s10278-024-01035-8. [33] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: unified embedding for face recognition and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 815823, Boston, MA, USA, June 2015. IEEE. ISBN 978-14673-6964-0. doi: 10.1109/CVPR.2015.7298682. URL http://ieeexplore.ieee.org/ document/7298682/. [34] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, and others. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [35] Wasswa Shafik, Ali Tufail, Chandratilak De Silva Liyanage, and Rosyzie Anna Awg Haji Mohd Apong. Using transfer learning-based plant disease classification and detection for sustainable agriculture. BMC Plant Biology, 24(1):136, February 2024. ISSN 1471-2229. doi: 10.1186/ s12870-024-04825-y. URL https://bmcplantbiol.biomedcentral.com/articles/10. 1186/s12870-024-04825-y. [36] Dong Shu, Haiyan Zhao, Jingyu Hu, Weiru Liu, Ali Payani, Lu Cheng, and Mengnan Du. Large Vision-Language Model Alignment and Misalignment: Survey Through the Lens of Explainability, February 2025. URL http://arxiv.org/abs/2501.01346. arXiv:2501.01346 [cs]. [37] Davinder Singh, Naman Jain, Pranjali Jain, Pratik Kayal, Sudhakar Kumawat, and Nipun Batra. In Proceedings of the 7th ACM PlantDoc: Dataset for Visual Plant Disease Detection. IKDD CoDS and 25th COMAD, pages 249253, Hyderabad India, January 2020. ACM. ISBN 978-1-4503-7738-6. doi: 10.1145/3371158.3371196. URL https://dl.acm.org/doi/10. 1145/3371158.3371196. 12 [38] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. [39] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua Tenenbaum, and Phillip Isola. Rethinking few-shot image classification: good embedding is all you need? In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIV 16, pages 266282. Springer, 2020. [40] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, and others. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016. [41] Haoqing Wang, Shibo Jie, and Zhihong Deng. Focus your attention when few-shot classification. Advances in Neural Information Processing Systems, 36:5968959707, 2023. [42] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. ChestX-ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on WeaklyIn 2017 IEEE Supervised Classification and Localization of Common Thorax Diseases. Conference on Computer Vision and Pattern Recognition (CVPR), pages 3462 3471, 2017. [43] Yan Wang, Wei-Lun Chao, Kilian Q. Weinberger, and Laurens van der Maaten. SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning, November 2019. URL http://arxiv.org/abs/1911.04623. arXiv:1911.04623 [cs]. [44] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying CLIP Data, April 2024. URL http://arxiv.org/abs/2309.16671. arXiv:2309.16671 [cs] version: 4. [45] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. MedMNIST v2 - large-scale lightweight benchmark for 2D and 3D biomedical image classification. Scientific Data, 10(1):41, January 2023. ISSN 20524463. doi: 10.1038/s41597-022-01721-8. URL https://www.nature.com/articles/ s41597-022-01721-8. [46] Xu Yang, Yingzhe Peng, Haoxuan Ma, Shuo Xu, Chi Zhang, Yucheng Han, and Hanwang Zhang. Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models. [47] Shaoting Zhang and Dimitris Metaxas. On the challenges and perspectives of foundation models for medical image analysis. Medical image analysis, 91:102996, 2024. Publisher: Elsevier. [48] Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, and Yulan He. The Mystery of In-Context Learning: Comprehensive Survey on Interpretation and Analysis. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1436514378, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.795. URL https://aclanthology.org/2024.emnlp-main.795/. 13 Appendix / supplemental material"
        },
        {
            "title": "Training Details",
            "content": "Model PictSureViTp s,c,t PictSureViTp s,s,t PictSureViTp s,c, PictSureViTp s,s,0 PictSureViT-Tripp s,c,t PictSureViT-Tripp s,s,t PictSureViT-Tripp s,c,0 PictSureViT-Tripp s,s,0 PictSureResNetp s,c,t PictSureResNetp s,s,t PictSureResNetp s,c,0 PictSureResNetp s,s, PictSureResNet c,c,0 Embedding Model ViT (No Triplet) ViT (No Triplet) ViT (No Triplet) ViT (No Triplet) ViT-Triplet ViT-Triplet ViT-Triplet ViT-Triplet ResNet ResNet ResNet ResNet ResNet PictSureViT c,c,0 ViT PictSureResNetp -,s,- PictSureViTp -,s,- ResNet ViT PictSureViT-Tripp -,s,- ViT-Triplet Pretrained Weights Learning Rate Strategy Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No Yes Yes Yes Encoder: constant Body: Schedule Encoder: Schedule Body: Schedule Encoder: constant Body: Schedule Encoder: Schedule Body: Schedule Encoder: constant Body: Schedule Encoder: Schedule Body: Schedule Encoder: constant Body: Schedule Encoder: Schedule Body: Schedule Encoder: constant Body: Schedule Encoder: Schedule Body: Schedule Encoder: constant Body: Schedule Encoder: Schedule Body: Schedule Encoder: constant Body: constant Encoder: constant Body: constant Encoder: not applied Body: constant Encoder: not applied Body: constant Encoder: not applied Body: constant Joint Training False False True True False False True True False False True True True True not applicable not applicable not applicable Table 2: Training combinations All models were trained on signle NVIDIA H100 GPU with 80GB VRAM. The models were trained for 600 epochs containing 10,000 samples with batch size of 16. We use the AdamW optimizer with weight decay of 1 105 and the cross entropy loss with label smoothing factor of 0.1. In our model training process, we employ multi-stage learning rate (LR) scheduling strategy. The LR is adjusted based on the epoch number according to the following piecewise function: scheduled_lr = epoch warmup_epochs lr_max, lr_max, lr_max (cid:16) lr_min lr_max (cid:17) epoch(warmup_epochs+plateau_epochs) decay_epochs , otherwise if epoch warmup_epochs if warmup_epochs < epoch warmup_epochs + plateau_epochs Here, scheduled_lr denotes the learning rate at given epoch. During the warm-up phase (first warmup_epochs epochs), the learning rate linearly increases from 0 to textlr_max. After the warmup phase, the learning rate remains constant at lr_max for the plateau period (next plateau_epochs 14 epochs). Finally, after the plateau period, the learning rate decays logarithmically from lr_max to lr_min over the subsequent decay_epochs. In all our experiments, lr_min was chosen as 1 105 and lr_max as 1 104. The learning rate was either applied to the whole architecture or only to the remaining architecture while constant learning rate of 1 105 was applied to the encoder. In some training settings we deliberately separated the training process, initially preventing backward propagation through the image encoder until the rest of the architecture had completed its warmup phase. This allowed us to assess whether the body could learn the task independently before finetuning the embeddings with information from the encoder. Model Details Table 3: Model Details Config Embedding Model Model Heads Model Layers Num Classes Num Images PictSure (ResNet) ResNet18 8 4 10 5 PictSure (ViT) ViT 8 4 10 5 Training Hyperparameters Table 4: Training Details Config Initial LR Target LR Epochs Weight Decay Log Step Epsilon Accumulation Steps Warmup Epochs Plateau Epochs Num Samples Batch Size PictSure (ResNet) 1 104 1 105 600 1 105 10 0.1 1 50 10 10000 PictSure (ViT) 1 104 1 105 600 1 105 10 0.1 1 50 10 10000"
        }
    ],
    "affiliations": [
        "German Research Center for AI (DFKI)"
    ]
}