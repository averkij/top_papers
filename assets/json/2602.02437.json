{
    "paper_title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "authors": [
        "Dianyi Wang",
        "Chaofan Ma",
        "Feng Han",
        "Size Wu",
        "Wei Song",
        "Yibin Wang",
        "Zhixiong Zhang",
        "Tianhang Wang",
        "Siyuan Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities."
        },
        {
            "title": "Start",
            "content": "UniReason 1.0: Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing 6 2 0 2 2 ] . [ 1 7 3 4 2 0 . 2 0 6 2 : r Dianyi Wang1,2*, Chaofan Ma3*, Feng Han1,2, Size Wu4, Wei Song2,5, Yibin Wang1,2, Zhixiong Zhang2,3, Tianhang Wang2,5, Siyuan Wang6, Zhongyu Wei1,2, Jiaqi Wang2 1Fudan University, 2Shanghai Innovation Institute, 3Shanghai Jiao Tong University 4Nanyang Technological University, 5Zhejiang University, 6University of Southern California *Equal Contribution, Corresponding Authors"
        },
        {
            "title": "Abstract",
            "content": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, unified framework that harmonizes these two tasks through dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing large-scale reasoning-centric dataset (300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities. GitHub: https://github.com/AlenjandroWang/UniReason HuggingFace: https://huggingface.co/Alex11556666/UniReason"
        },
        {
            "title": "Introduction",
            "content": "Unified multimodal models have emerged as promising paradigm for jointly handling visual understanding and generation tasks [1, 2, 3, 4, 5, 6]. By integrating perception and synthesis within shared architecture, these models enable seamless interplay between comprehending visual content and producing new images conditioned on multimodal inputs. Among various capabilities, text-to-image (T2I) generation and image editing stand out as particularly challenging yet impactful applications. However, current unified models still struggle with complex scenarios, where tasks demand not only precise instruction following, but also world knowledge that extends beyond surface-level pixels, e.g., commonsense, physical laws, and spatial-temporal logic. Figure 1 Illustrative cases of UniReason on image editing and T2I generation tasks. Given an instruction, the model first performs world knowledge-enhanced textual reasoning to generate grounded, fine-grained guidance for image synthesis. It then applies fine-grained editing-like visual refinement, correcting errors introduced during the initial generation and improving the synthesis quality."
        },
        {
            "title": "Such challenges fundamentally demand reasoning capabilities to bridge the gap between abstract user",
            "content": "intent and faithful visual output. To enhance reasoning capabilities, prominent line of work focuses on prompt enhancement or reprompting strategies [7, 8, 9]. These methods employ chain-of-thought (CoT) reasoning to expand abstract user prompts into explicit semantic and spatial guidance before generation. While effective in improving instruction alignment, these reason-then-generate approaches are inherently limited as reasoning occurs only before generation without access to visual feedback, preventing reflection on and correction of output errors. More recently, interleaved reasoning mechanisms [10, 11] alternate between textual reasoning and visual generation. By first generating an initial image, then performing textual reflection based on visual feedback, and finally refining the output, these approaches enable post-generation correction that was previously infeasible. Despite this progress, existing methods still exhibit two key limitations. (1) Reasoning in these methods largely remains at the level of semantic reorganization, decomposing instructions into finer-grained descriptions or spatial layouts [7, 9, 11, 12]. This addresses only the explicit component of user intent, whereas faithful synthesis in practice demands world knowledge that is implicitly assumed rather than explicitly stated. Such knowledge must be inferred, not merely parsed from instructions. This creates fundamental knowledge gap that surface-level decomposition cannot bridge. (2) Existing methods typically address text-to-image generation and image editing as separate tasks [10], leaving their inherent synergies within unified interleaved framework untapped. We argue that these two tasks share substantial reasoning overlap and can mutually reinforce each other. Specifically, post-generation critique and refinement in interleaved reasoning is structurally analogous to editing. Isolating them therefore forgoes such synergy and leads to redundant learning. To address these challenges, we propose UniReason, unified reasoning framework that harmonizes text-to-image generation and image editing within shared architecture, as illustrated in Fig 1. Our framework supports two complementary reasoning paradigms. (1) World Knowledge-Enhanced Textual Reasoning aims to bridge the knowledge gap prior to synthesis. Given an underspecified instruction, the model performs textual reasoning to infer implicit world knowledge and produces grounded guidance that specifies fine-grained details for the subsequent image synthesis. To support this, we construct training data across five knowledge categories: cultural commonsense, natural science, spatial, temporal, and logical reasoning. 2 We use large language models to generate reasoning traces and apply multi-dimensional filtering to ensure high-quality supervision. (2) Fine-grained Editing-like Visual Refinement aims to improve synthesis quality after initial generation. Given the initial image and prior reasoning, the model performs self-reflection to identify discrepancies or missing details, then applies targeted corrections to produce refined image. Observing that this process is structurally analogous to image editing, we jointly learn T2I generation and editing for mutual benefit. We design an agent pipeline that iterates through generation, verification, refinement, and comparison to construct high-quality training data. These two paradigms can be applied independently or jointly, offering flexibility across diverse synthesis scenarios. We adopt two-stage training strategy: the first stage strengthens foundational generation capability, and the second stage enables interleaved reasoning by jointly training the understanding and generation branches. Through this unified framework, we achieve comprehensive world knowledge-grounded reasoning capabilities for both T2I generation and image editing, with advanced performance on multiple benchmarks. Our main contributions are summarized as follows: We propose UniReason, unified reasoning framework for both T2I generation and image editing. Our key insight is that refinement and editing share the same reasoning pattern, enabling bidirectional capability transfer. We introduce two complementary reasoning paradigms: World Knowledge-Enhanced Textual Reasoning bridges the knowledge gap before synthesis, while Fine-grained Editing-like Visual Refinement enables iterative improvement after generation. We systematically construct training data for both paradigms, including world knowledge-aligned data across five categories and an agent pipeline for refinement supervision, combined with two-stage training strategy. Extensive experiments demonstrate state-of-the-art performance on multiple benchmarks, including GenEval, WISE for T2I generation, and UniREditBench, KrisBench for image editing."
        },
        {
            "title": "2 Related Work",
            "content": "Image generation (T2I) and editing are two related tasks, depending on whether Image Generation and Editing the conditional signals are textual descriptions or reference images. Recently, Diffusion Transformers [13, 14] (DiTs) have served as the backbone of state-of-the-art generation frameworks, with flow-matching [15, 16] adopted as the prevailing training scheme. Together with data and model scaling, these advances have enabled photo-realistic synthesis and substantially improved instruction following in T2I generation [15, 17, 18]. Building upon these powerful generators, recent image editing systems [17, 19] achieve precise content manipulation while preserving overall visual consistency. However, despite their generative prowess, these specialized models lack the intrinsic capacity for world comprehension and self-reflection, motivating the integration of reasoning and generation within coherent unified framework. Unified Multimodal Models Unified multimodal models [2, 3, 4, 5, 6, 20] aim to jointly support image understanding and generation within single framework. Broadly, existing approaches can be grouped into two paradigms. first, more modular paradigm aligns pretrained LMMs and DiTs via LLM hidden states [3, 21, 22] or learnable queries [5, 20, 23]. Another line of work [1, 2, 6] adopts shared LLM architecture for perception and synthesis, encouraging tight coupling between the two tasks. In our study, we focus on the second paradigm, since shared backbone naturally supports interleaved reasoning between language and image generation in unified inference process. Reasoning in Unified Multimodal Models The structural convergence of understanding and generation within unified models unlocks the potential for grounding high-fidelity image synthesis in complex multimodal reasoning. Initial efforts primarily involve the adaptation of textual Chain-of-Thought (CoT) to image generation [7, 8, 9], following reason-then-generate paradigm that expands user instructions into detailed descriptions prior to synthesis. More recently, interleaved reasoning mechanisms [10, 11] extend the process into iterative \"reason-generate-reflect\" cycles to incorporate visual feedback. Despite these advancements, existing methods are often confined to prompt reorganization and rigidly separate generation and editing 3 tasks. In this work, we address these limitations by inferring implicit world knowledge rather than merely parsing instructions. Furthermore, we exploit the inherent synergies between T2I generation and image editing within unified reasoning framework."
        },
        {
            "title": "3 Preliminary",
            "content": "Architecture We build upon Bagel [6] to develop unified and interleaved reasoning framework for both T2I generation and image editing. Bagel adopts Mixture-of-Transformers (MoT) architecture with ViT encoder [24] to process multimodal inputs and enables unified image understanding and generation within single foundation model. Specifically, multimodal understanding is formulated as generating context-aware textual outputs via standard next-token prediction through language modeling head. This process is conditioned on multimodal context inputs and handled by the understanding expert. Formally, the training objective minimizes the negative log-likelihood: ùëá (cid:213) ‚Ñítext = log ùëùùúÉ(ùë•ùë° ùë•<ùë° , ùê∂) , (1) ùë°=1 where ùë•ùë° denotes the target text token, ùë•<ùë° is the preceding tokens and ùê∂ is the multimodal context. Multimodal generation focuses on producing high-quality and semantically aligned images via rectified flow process [25] in VAEs latent space [18], conditioned on multimodal inputs and handled by the generation expert. The training objective is to minimize the the latent flow-matching loss: where ùë¢ denotes the target velocity, ùë¢ùúÉ is the learned time-conditioned velocity field in the latent space. ‚Ñíimage = ùîºùë°ùí∞ (0,1) 2 (cid:13) (cid:13)ùë¢ùúÉ(ùëßùë° , ùë° ; ùê∂) ùë¢(ùëßùë° , ùë°)(cid:13) (cid:13) 2 , (2) Reasoning Paradigms Bagels unified architecture supports interleaving textual reasoning and visual synthesis in both T2I generation and image editing tasks. Specifically, T2I generation takes textual instruction as input and outputs sequence of intermediate reasoning tokens together with synthesized image. For image editing, an existing image and textual instruction are taken as input and the model outputs reasoning text and the edited image. In this work, we formulate interleaved reasoning as an iterative process: (ùêº ùëò+1, ùëá ùëò+1) = ‚Ñ± (cid:0)ùêºùëò , ùëáùëò , ùê∂(cid:1) where ùêº ùëò and ùëá ùëò denote the image and reasoning text at iteration ùëò, ùê∂ denotes the multimodal context, and ‚Ñ± is the unified model (ùëò = 1 in our implementation). Under this formulation, each refinement step can be interpreted as an image editing operation conditioned on the reasoning trace. Therefore, we propose to jointly learn T2I generation and image editing within unified interleaved reasoning framework, allowing the refinement process to benefit from editing learning and, conversely, enhance interleaved reasoning for both T2I generation and editing."
        },
        {
            "title": "4 Method",
            "content": "In this section, we present UniReason, unified multimodal reasoning framework for both T2I generation and image editing, as illustrated in Fig 2. In practice, the framework operates in two phases, (1) World Knowledge-Enhanced Textual Reasoning for initial synthesis; (2) Fine-grained Editing-like Visual Refinement for iterative improvement. We introduce each phase along with its corresponding data creation pipeline in Section 4.1 and 4.2, respectively shown in Fig 4, followed by the training strategy in Section 4.3."
        },
        {
            "title": "4.1 World Knowledge-Enhanced Textual Reasoning",
            "content": "Different from prior work [8, 9] that primarily focuses on re-organizing user instructions into more detailed visual descriptions, our core objective is to enable the unified multimodal model to not only expand raw user prompts but also understand the underlying implicit world knowledge. Specifically, UniReason utilizes textual reasoning to infer the world knowledge required to complete the visual synthesis, including commonsense, cultural context, time-spatial and natural science principles. This process provides explicit and structured guidance to ensure the initial generation is both instruction-aligned and knowledge-consistent, mirroring the conceptual planning that humans perform when outlining ideas for drawing. 4 Figure 2 Overview of UniReason framework for two complementary reasoning paradigms in image synthesis. Data Preparation To enable world knowledge-enhanced textual reasoning for the initial synthesis, we construct challenging input instructions for both T2I generation and image editing tasks that require complex world knowledge reasoning beyond complementing pixel-level details, along with their associated reasoning processes. Specifically, we cover five major categories of world knowledge and adopt post-generation filtering to ensure high-quality supervision. Cultural Commonsense instructions require using shared cultural knowledge, such as historical events, iconic figures, social customs, and idiomatic expressions, to resolve unnamed or underspecified entities into explicit, contextually meaningful visual content, ensuring generated images aligned with real-world cultural understanding. Natural Science instructions requires incorporating principles from physics, biology, medicine, or chemistry to ensure that generated images remain consistent with scientific laws, and reflect plausible real-world observations. Spatial reasoning focuses on understanding correct spatial relationships among entities, including relative position, orientation, viewpoint, and camera transformations. Such instructions requires deriving precise spatial configurations from abstract descriptions to generate visuals consistent with real-world geometric logic. Temporal reasoning models time-dependent relationships, such as event sequences, state transitions, and causal ordering. This type of instructions require inferring the temporal progression of events and ensuring that visual outputs reflect coherent and plausible temporal dynamics aligned with natural chronological flow. Logical reasoning emphasizes causal coherence and logical consistency during image generation, such as in maze-solving or constraint satisfaction problems, by adhering to explicit or implicit logical structures. These instructions require applying deductive principles to translate abstract logical constraints into visually valid solutions. For T2I generation in each category, we manually construct seed prompts based on Wikipedia, together with explicit category definitions, and use Gemini-2.5 Pro [26] to expand them into larger prompt set. And Gemini-2.5 Pro is also employed to generate textual CoT reasoning for each prompt. All prompts with their corresponding CoTs are subsequently fed into Qwen-Image [17] for image rendering to form paired training samples. For image editing, we utilize data triples (original image, editing instruction, desired outcome) from UniREdit-Data-100K [27] that covers diverse knowledge dimensions, and expand them with textual reasoning traces generated by Gemini-2.5 Pro. Moreover, to ensure the training samples are generated without hallucinations, Gemini-2.5 Pro serves as comprehensive evaluator to assess the generated images across three dimensions: instruction alignment, visual fidelity, and reasoning correctness. Only verified samples are retained to construct high-quality training set for training visual synthesis with textual reasoning."
        },
        {
            "title": "4.2 Fine-grained Editing-like Visual Refinement",
            "content": "After the initial visual generation or editing, the draft already captures essential elements and semantically aligns with the input instruction and world knowledge, but inevitably contains imperfections that require fine-grained refinement. We therefore continue refining the results from the knowledge-enhanced initial synthesis. Specifically, the model reassesses the initial synthesized image considering prior textual reasoning, reflectively identifies and verbalizes inconsistencies and missing details. It then optionally incorporates second round of textual reasoning, which accordingly refines semantic attributes, aesthetic details, stylistic coherence and instruction consistency to produce polished image. This refinement process guided by textual reflection is structurally analogous to image editing, motivating us to create synergistic loop for mutual improvement between T2I generation and image editing, by alternating knowledge-enhanced textual reasoning and editing-like visual refinement. Data Preparation We design an agent pipeline to construct high-quality supervision data for training interleaved reasoning across both T2I generation and image editing tasks. The pipeline consists of (i) an initial generator (the base model) that produces draft image with its textual reasoning from the input; (ii) verifier (Gemini-2.5 Pro) that diagnoses captionimage mismatches and outputs structured, actionable edit directives across five dimensions: object presence, attribute accuracy, style consistency, realism, and aesthetic quality; (iii) refinement teacher (Qwen-Image-Edit [17]) that applies the feedback and textual reasoning via instruction-guided image editing to obtain an improved image; and (iv) final judge (Gemini-2.5 Pro) that performs comparative evaluation between the initial and refined images, retaining refined images only if they exhibit measurable improvements over the initial generation and faithfully reflects the verifiers suggestion. Specifically, we sample long-form captions from ShareGPT-4o-Image dataset [28] and short-form captions from midjourney prompts1 for T2I generation, and image-instruction pairs from UniREdit-Data-100K [27] for image editing. These inputs are fed to the initial generator for reasoning-augmented initial synthesis. The captionimage pairs then undergo the full verification, refinement and comparison cycle, resulting in corpus of high-quality training data for image synthesis with multimodal interleaved reasoning."
        },
        {
            "title": "4.3 Two-stage Training Strategy",
            "content": "We adopt simple yet effective two-stage supervised fine-tuning (SFT) strategy to first strengthen the foundational generation capability of the unified multimodal model, then train interleaved knowledgeenhanced reasoning and refinement capabilities across diverse image synthesis queries. Stage 1: Foundational Generation Strengthening In the first stage, we freeze the multimodal understanding branch of the base model and train only the generation branch. This stage focuses exclusively on image synthesis using existing T2I generation and image editing datasets without textual reasoning, aiming to enhance the instruction-following ability and foundational image synthesis capability. Stage 2: Interleaved Reasoning Tuning In the second stage, we unfreeze all model parameters and jointly train the understanding and generation branches using the curated interleaved reasoning data, including single-turn knowledge-enhanced reasoning samples and iterative visual refinement samples. This enables the model to perform world knowledge-enhanced reasoning and iteratively reflect and refine visual content. Specifically, for single-turn reasoning data, we supervise both the textual reasoning traces and the image synthesis outputs. For visual refinement data, we supervise textual reflections and refined images while leaving the initial reasoning text and visual draft unsupervised. The overall objective is formulated as ‚Ñí = ùúÜtext ‚Ñítext + ùúÜimg ‚Ñíimg, (3) where ‚Ñítext denotes the text loss for supervising the reasoning tokens, and ‚Ñíimg denotes the image loss for supervising the synthesized images. ùúÜtext and ùúÜimg are scalar loss weights that balance the contributions of the text and image objectives, respectively. 1https://huggingface.co/datasets/vivym/midjourney-prompts 6 Table 1 Evaluation of world knowledge-intensive text-to-image generation on the WISE [29] benchmark. \"*\" denotes generation with textual reasoning only, \"\" denotes generation with both reasoning and refinement. The first block reports the performance of closed-source models. Bold entries represent the best performance among open-source models. Model Cultural Time Space Biology Physics Chemistry Overall GPT-4o Seedream 4. 0.81 0.78 0.71 0.73 0.89 0.85 0.83 0.79 0.79 0.84 Unified Understanding and Generation w/o Reasoning. Harmon Show-o Janus Pro MetaQuery-XL BLIP3-o UniWorld-V1 OmniGen2 Hunyuan-Image 3.0 Qwen-Image 0.38 0.28 0.30 0.56 0.53 0.42 0.58 0.62 0.48 0.40 0.37 0.55 0.55 0.52 0.57 0.63 0.52 0.48 0.49 0.62 0.73 0.64 0.70 0.77 0.37 0.30 0.36 0.49 0.45 0.43 0.56 0.57 0.44 0.46 0.42 0.63 0.59 0.50 0.63 0. T2I-R1* MindOmni* IRG BAGEL* UniCoT Ours Unified Understanding and Generation w/ Reasoning. 0.56 0.75 0.78 0.76 0.76 0.80 0.55 0.70 0.72 0.69 0.70 0.68 0.63 0.76 0.76 0.75 0.76 0.79 0.54 0.76 0.81 0.65 0.73 0. 0.55 0.72 0.82 0.75 0.81 0.83 0.74 0.67 0.29 0.30 0.26 0.41 0.41 0.34 0.31 0.40 0.30 0.52 0.78 0.58 0.73 0.81 0.80 0.78 0.41 0.35 0.35 0.55 0.62 0.55 0.47 0.57 0. 0.54 0.71 0.77 0.70 0.75 0.78 Table 2 Evaluation of knowledge-intensive image editing on KrisBench [30] and UniREditBench [31] benchmarks. \"*\" denotes textual reasoning only for editing, \"\" denotes interleaved reasoning with both reasoning and refinement. Bold entries represent the best performance among open-source models. Model GPT-4o Gemini 2.0 Seedream 4.0 KrisBench Factual 79.80 65.26 Conceptual 81.37 59.65 Extract Procedural 78.32 62.90 Overall 80.09 62.41 Real World 81.01 66.22 UniREditBench Game World 62.07 45.38 Overall 73.39 55. OmniGen2 Uniworld V1 Lumina-DiMOO LightFusion-World Qwen-Image-Edit BAGEL* UniCoT Ours 57.36 47.71 66.69 66.18 71.85 70.67 Unified Understanding and Generation w/o Reasoning. 44.20 44.80 63.50 47.79 47.92 52.38 49.71 50.27 61.85 53.69 51.44 70.95 Unified Understanding and Generation w/ Reasoning. 61.92 67.16 72.38 49.02 63.68 56. 60.18 68.00 68.23 56.80 74.82 33.14 45.61 41.92 45.10 65.30 43.41 48.54 56.52 50.96 70."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "In the first stage, the training corpus comprises nearly 7 million T2I samples and 500k Training Details image editing samples collected from open-source datasets including BLIP-3o [20], ShareGPT-4o-Image [28], Echo-4o-Image [32], OpenGPT4o-Image [33], Nano-banana-consist [34], and Pico-banana [35]. We train the models generation branch for 30,000 iterations using the Adam optimizer with cosine learning rate schedule, including 3,000 warm-up steps, maximum learning rate of 5 105 and minimum learning rate of 1 105. In the second stage, the training corpus consists of 150k self-constructed single-turn knowledge-enhanced reasoning samples for T2I generation, 100k image editing reasoning samples [27], and self-constructed interleaved reasoning samples, including 36k for T2I generation and 10k for image editing. We fine-tune all model parameters for 10,000 iterations with 1,000 warm-up steps, maximum learning rate of 2 105 and minimum learning rate of 1 106. Loss weights are set to ùúÜtext = 2 and ùúÜimg = 1, with packed sequence length of 50k tokens. 7 Table 3 Comparison of different models across general image generation and editing benchmarks. Bold entries represent the best performance among open-source models and underlined entries indicate the best performance among unified models with reasoning."
        },
        {
            "title": "Model",
            "content": "General T2I Generation"
        },
        {
            "title": "General Image Editing\nImgEdit",
            "content": "GEdit-EN Closed-source GPT-4o Gemini 2.0 Seedream 4.0 0.84 0.84 85.15 88.25 Unified Understanding and Generation w/o Reasoning. TokenFlow-XL Harmon Show-o Janus Pro MetaQuery-XL BLIP3-o UniWorld-V1 Mogao OmniGen2 MMaDA Lumina-DiMOO LightFusion-World Hunyuan-Image 3.0 Qwen-Image Qwen-Image-Edit 0.55 0.76 0.53 0.80 0.80 0.84 0.80 0.89 0.80 0.63 0.88 0.72 0.87 73.38 67.48 84.19 82.05 81.60 81.38 84.33 83.57 69.97 86.04 86.10 88.32 Unified Understanding and Generation Reasoning. T2I-R1 GoT Mind-Omni IRG BAGEL UniCoT Ours 0.64 0.83 0.85 0.88 0.83 0.90 82.50 85.07 86.21 Open-source Open-source 4.20 4. 3.26 3.43 3.85 4.27 3.20 4.06 7.53 6.32 7.68 4.85 6.41 6.58 7. 6.52 6.74 6.94 Evaluation Setup We evaluate world knowledge reasoning and fine-grained semantic alignment for T2I generation using the WISE [29] benchmark, which comprises 1,000 world knowledge-informed prompts across culture, natural science, and spatial and temporal comprehension. For image editing, we use UniREditBench [27] with 2,700 meticulously curated samples covering both realand game-world scenarios, and KrisBench [36] with 1,267 samples across factual, conceptual, and procedural knowledge to assess world knowledge reasoning and refinement capabilities. Additionally, we evaluate general compositional and instruction-following abilities using GenEval [12] and DPGBench [37] for T2I generation, as well as ImgEdit [30] and GEdit-EN [31] for image editing."
        },
        {
            "title": "5.2 Main Results",
            "content": "We present comprehensive comparison of our model against existing state-of-the-art unified multimodal models that support both generation and understanding in Table 1 and Table 2, for world knowledge-intensive T2I generation and image editing tasks, respectively. Detailed descriptions of the compared models are provided in Appendix A.1. Our model achieves the best overall performance among open-source unified multimodal models, with or without explicit reasoning mechanisms, across knowledge-intensive image generation and editing tasks. Besides, it demonstrates comparable results to closed-source models, including Seedream 4.0 [38] and GPT-4o [39] on T2I generation, and even surpasses Gemini 2.0 [40] on KrisBench [36] and outperforms Seedream 4.0 [38] on UniREditBench [27]. These results highlight the effectiveness of our unified reasoning framework. Moreover, as shown in the fine-grained breakdown of performance across different knowledge domains 8 Table 4 Ablation study of UniReason. The base model is BAGEL [6]. Two-Stage Training refers to fine-tuning the base model using the two-stage training recipe, as described in Sec. 4.3. Method WISE KrisBench UniREditBench Base Model 0.52 56.21 50. + Two-Stage Training + Reasoning + Refinement 0.58(+0.06) 0.73(+0.21) 61.53(+5.32) 64.12(+7.91) 63.37(+12.41) 67.30(+16.34) 0.78 (+0.26) 68.23(+12.02) 70.06(+19.10) in Tables 1 and 2, our model exhibits broad and consistent world-knowledge coverage. Notably, it achieves the highest performance in Cultural Commonsense, Spatial Reasoning, Natural Science including Physics and Chemistry. For image editing tasks, it also demonstrates strong performance across diverse knowledge categories in both KrisBench and UniREditBench. Overall, our models knowledge-enhanced reasoning capabilities cover wide range of tasks and domains."
        },
        {
            "title": "5.3 General Ability Retention",
            "content": "Beyond knowledge-intensive tasks, our model remains highly competitive on general image generation and editing benchmarks while improving knowledge-enhanced reasoning, demonstrating strong generalization capability. As shown in Table 3, on GenEval [12], our model surpasses leading systems, including QwenImage [17], GPT-4o [33], and Seedream 4.0 [38], without relying on any external LLM-based rewriting. On DPGBench [37], it achieves the best performance among models with reasoning mechanisms during generation, highlighting strong long-horizon instruction following. We further evaluate precise instructionfollowing image editing on ImgEdit [30] and GEdit-EN [31], which are essential for practical refinement. Our model delivers the strongest results among models with reasoning capability while remaining competitive with broad range of existing approaches. These results indicate that our model is not only strong in reasoning-centric settings but also excels in general generation and editing, providing robust and versatile unified foundation. Detailed results are shown in Appendix A.3 and case studies are shown in Appendix A.4."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "We further investigate the contributions of the two-stage training strategy, as well as the reasoning and refinement mechanisms for image synthesis. On three knowledge-intensive generation and editing benchmarks, we compare three progressive settings built upon the BAGEL base model: (i) Two-Stage Training, which performs direct image generation after two-stage fine-tuning; (ii) + Reasoning, which elicits textual reasoning prior to image synthesis; and (iii) + Refinement, which further introduces an explicit reflection and refinement step to produce final refined output. Table 4 shows consistent improvement across all benchmarks as each component is added. The two-stage training alone effectively improves the base models instruction-following and synthesis capabilities. Then, introducing world knowledge-enhanced textual reasoning yields significant gains, especially on WISE with +0.21 improvement. Finally, the visual refinement phase further improves the overall performance on all benchmarks. These results suggest that the two-stage training strategy injects both knowledge-enhanced reasoning and fine-grained refinement capabilities into the unified multimodal model, rather than merely enhancing surface-level visual composition. Moreover, the results highlight the importance of explicitly modeling implicit world knowledge during initial synthesis and performing fine-grained editing for further refinement."
        },
        {
            "title": "5.5 Correlation of Editing and Refinement",
            "content": "To show how image editing capability affects refinement effectiveness, we analyze performance gains with and without the refinement mechanism across models with varying editing capabilities. Specifically, we select different checkpoints during stage-1 training, each exhibiting different levels of editing proficiency, and apply identical stage-2 training to all checkpoints. We then evaluate performance on three knowledge-intensive benchmarks, measuring the gains achieved through refinement after initial textual reasoning. Fig 3 plots these performance gains against the editing performance of each checkpoint on ImgEdit. 9 Figure 3 Correlation between image editing capability (ImgEdit score) and performance gains from refinement across three benchmarks. Higher editing proficiency leads to monotonically increasing refinement effectiveness. The results reveal that performance gains from refinement increase monotonically with higher ImgEdit scores. This trend highlights the importance of jointly training image editing and T2I generation within unified interleaved reasoning framework that integrates both textual reasoning and visual refinement. Since visual refinement relies on fine-grained and controllable editing, insufficient editing capacity can limit the effectiveness of reasoning-guided refinement."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce UniReason, unified reasoning framework that harmonizes the text-to-image generation and image editing by exploiting their inherent structural synergies. Specifically, we proposed two complementary components: World Knowledge-Enhanced Textual Reasoning that infers implicit common sense and physical laws, and Fine-grained Editing-like Visual Refinement that enables iterative reflection and correction. By constructing high-quality datasets across five knowledge categories and employing two-stage training strategy, UniReason demonstrates superior instruction following and visual fidelity. Extensive experiments on multiple benchmarks demonstrate that our unified reasoning approach achieves advanced performance across both T2I and editing tasks."
        },
        {
            "title": "Impact Statement",
            "content": "This work focuses on improving reasoning and alignment in image generation and editing models. While such advances may benefit various creative and assistive applications, they may also introduce risks related to misuse of generated visual content. Addressing these risks requires system-level safeguards and responsible deployment practices beyond the scope of this paper."
        },
        {
            "title": "References",
            "content": "[1] Jinheng Xie, Weƒ≥ia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhƒ≥ie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2025. URL https://arxiv.org/abs/2408.12528. [2] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling, 2025. URL https: //arxiv.org/abs/2501.17811. [3] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025. [4] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 10 [5] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [6] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining, 2025. URL https://arxiv.org/abs/2505.14683. [7] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot, 2025. URL https://arxiv.org/abs/2505.00703. [8] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, and Hongsheng Li. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing, 2025. URL https://arxiv.org/abs/2503.10639. [9] Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, and Ying Shan. Mindomni: Unleashing reasoning generation in vision language models with rgpo, 2025. URL https://arxiv.org/abs/2505.13031. [10] Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, and Shaohui Lin. Interleaving reasoning for better text-to-image generation, 2025. URL https://arxiv.org/abs/ 2509.06945. [11] Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision, 2025. URL https: //arxiv.org/abs/2508.05606. [12] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/2310.11513. [13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [14] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [16] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eƒ≥nden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models wfith scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. [17] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/2508.02324. [18] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [19] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [20] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, 2025. URL https://arxiv.org/abs/2505.09568. [21] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, and Li Yuan. Uniworld-v1: High-resolution semantic encoders for unified visual understanding and generation, 2025. URL https://arxiv.org/abs/2506.03147. 11 [22] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation, 2025. URL https://arxiv.org/abs/2506.18871. [23] Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025. [24] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier H√©naff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. URL https://arxiv.org/abs/2502.14786. [25] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. URL https://arxiv.org/abs/2209.03003. [26] Google. Gemini 2.5 pro. https://deepmind.google/models/gemini/pro/, 2025. [27] Feng Han, Yibin Wang, Chenglin Li, Zheming Liang, Dianyi Wang, Yang Jiao, Zhipeng Wei, Chao Gong, Cheng Jin, Jingjing Chen, and Jiaqi Wang. Unireditbench: unified reasoning-based image editing benchmark, 2025. URL https://arxiv.org/abs/2511.01295. [28] Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation, 2025. URL https: //arxiv.org/abs/2506.18095. [29] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation, 2025. URL https://arxiv.org/abs/2503.07265. [30] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark, 2025. URL https://arxiv.org/abs/2505.20275. [31] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing, 2025. URL https://arxiv.org/abs/2504.17761. [32] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, and Weƒ≥ia Li. Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation, 2025. URL https://arxiv.org/abs/2508.09987. [33] Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, Pengfei Wan, and Yi-Fan Zhang. Opengpt-4o-image: comprehensive dataset for advanced image generation and editing, 2025. URL https://arxiv.org/abs/2509.24900. [34] Nano-banana-150k. https://github.com/yejy53/Nano-banana-150k, 2024. GitHub repository. [35] Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, and Zhe Gan. Pico-banana-400k: large-scale dataset for text-guided image editing, 2025. URL https://arxiv.org/abs/ 2510.19808. [36] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models, 2025. URL https://arxiv.org/abs/2505.16707. [37] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. URL https://arxiv.org/abs/2403.05135. [38] ByteDance. Seedream 4.0, 2025. URL https://seed.bytedance.com/en/seedream4_0. Accessed: 2025-08. [39] OpenAI. Gpt-image-1, 2025. URL https://openai.com/index/introducing-4o-image-generation/. Accessed: 2025. [40] Kat Kampf and Nicole Brichtova. Accessed: tion. experiment-withgemini-20-flash-native-image-generation/. 2025, March 2025. 05-08,"
        },
        {
            "title": "Experiment with gemini",
            "content": "image genera2.0 flash native URL https://developers.googleblog.com/en/ 12 [41] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation, 2025. URL https://arxiv.org/abs/2412.03069. [42] Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, Jinbin Bai, Qian Yu, Dengyang Jiang, Yuandong Pu, Haoxing Chen, Le Zhuo, Junjun He, Gen Luo, Tianbin Li, Ming Hu, Jin Ye, Shenglong Ye, Bo Zhang, Chang Xu, Wenhai Wang, Hongsheng Li, Guangtao Zhai, Tianfan Xue, Bin Fu, Xiaohong Liu, Yu Qiao, and Yihao Liu. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding, 2025. URL https://arxiv.org/abs/2510.06308. [43] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models, 2025. URL https://arxiv.org/abs/2505.15809. [44] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation, 2025. URL https://arxiv.org/abs/2505.05472. [45] Siyu Cao, Hangting Chen, Peng Chen, Yƒ≥i Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, Weƒ≥ie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lucaz Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyang Peng, Yuanbo Peng, Xiangwei Shen, Yixuan Shi, Jiale Tao, Yangyu Tao, Qi Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyan Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fang Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zheng Yuan, Chao Zhang, Jian-Wei Zhang, Peizhen Zhang, Shi-Xue Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, Zƒ≥ian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jianchen Zhu, and Zhao Zhong. Hunyuanimage 3.0 technical report, 2025. URL https://arxiv.org/abs/2509.23951. [46] Chenhui Gou, Zilong Chen, Zeyu Wang, Feng Li, Deyao Zhu, Zicheng Duan, Kunchang Li, Chaorui Deng, Hongyi Yuan, Haoqi Fan, Cihang Xie, Jianfei Cai, and Hamid Rezatofighi. Vq-va world: Towards high-quality visual question-visual answering, 2025. URL https://arxiv.org/abs/2511.20573."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Compared Baselines We compared closed-source models including: GPT-4o [39], Gemini-2.0 [26], Seedream4.0 [38], as well as open-source advanced unified multimodal models models which support both multimodal understanding and high quality image generation including autoregressive unified models, such as Harmon [3], TokenFlowXL [41], and Janus-Pro [2]. Discrete diffusion-based approaches, including Lumina-DiMOO [42], MMaDA [43], and Show-o [1]. Another line of work connects VLMs and diffusion transformers via explicit connectors, exemplified by BLIP-3o [20], UniWorld-V1 [21], OmniGen2 [22], and the Qwen-Image series [17]. In contrast, deep fusion methods tightly integrate VLMs and DiTs within unified architecture, such as Mogao [44], Hunyuan Image 3.0 [45] and LightFusion-World [46], the latter further enhanced with knowledge-centric fine-tuning. Among open-source unified multimodal models that support naive reasoning, T2I-R1 [7], MindOmni [9], and BAGEL [6] primarily rely on textual reasoning to decompose abstract instructions into explicit semantic components that guide image generation. In contrast, GoT [8] introduces coordinate-based representations to provide explicit spatial guidance during synthesis. Another line of work, including IRG [10] and UniCoT [11], adopts interleaved reasoning mechanisms to reorganize semantics across modalities, progressively decomposing instructions into finer-grained and more structured descriptions for generation and refinement. A.2 Data Preparation Details To construct high-quality supervision for training UniReason across both text-to-image (T2I) generation and image editing tasks, we design two-phase data construction pipeline that integrates world knowledgeenhanced textual reasoning with fine-grained editing-like visual refinement. Figure 4 Overview of our data preparation framework. Phase I: World KnowledgeEnhanced Reasoning Data Construction We first build challenging instructions that require reasoning beyond pixel-level completion, covering five categories of world knowledge: (i) Cultural Commonsense, which resolves culturally grounded but underspecified entities using shared knowledge of history, customs, and symbols; (ii) Natural Science, which enforces consistency with physical, biological, medical, or chemical laws; (iii) Spatial Reasoning, which derives correct relative positions, orientations, viewpoints, and camera transformations; (iv) Temporal Reasoning, which models time-dependent state transitions and causal event sequences; and (v) Logical Reasoning, which translates explicit or implicit logical constraints into visually valid solutions. For T2I generation, we manually curate seed prompts grounded in Wikipedia and category definitions, then use Gemini-2.5 Pro [26] to expand them and generate corresponding textual CoT reasoning. Each promptreasoning pair is rendered into images using Qwen-Image [17], forming 14 reasoning-grounded training samples. For image editing, we adopt triplets from UniREdit-Data-100K [27], augmented with Gemini-2.5 Progenerated reasoning processes with category definitions. All samples are filtered by Gemini-2.5 Pro to ensure instruction alignment, visual fidelity, and knowledge-consistent reasoning, retaining only verified high-quality data. Phase II: Fine-grained Editing-like Visual Refinement Data Construction To further train interleaved reasoning and refinement capabilities, we design an agent-based pipeline to generate iterative refinement supervision. Given an input instruction, an initial generator produces draft image along with textual reasoning. verifier (Gemini-2.5 Pro) then diagnoses captionimage mismatches and outputs structured, actionable feedback along five dimensions: object presence, attribute accuracy, style consistency, realism, and aesthetic quality. refinement teacher (Qwen-Image-Edit [17]) applies this feedback and textual reasoning via instruction-guided image editing to produce refined image. Finally, judge (Gemini-2.5 Pro) performs comparative evaluation between the initial and refined images, retaining refined results only if they demonstrate measurable improvements and faithfully reflect the suggested modifications. Concretely, we sample long-form captions from ShareGPT-4o-Image [28] and short-form captions from Midjourney prompts2 for T2I generation, and imageinstruction pairs from UniREdit-Data-100K for image editing. These inputs undergo the full generationverificationrefinementselection cycle, yielding high-quality training set that jointly supports world knowledgeenhanced reasoning and fine-grained visual refinement. A.3 Detailed Evaluation Results We show the detailed evaluation results on general tasks include GenEval [12] shown in Table 5 and DPGBench [37] in Table 6 for T2I generation, as well as ImgEdit [30] and GEdit-EN [31] in table 7 for image editing. The results show our model delivers the strongest results among models with reasoning while remaining competitive with broad range of existing approaches. These results indicate that our model is not only strong in reasoning-centric settings but also excels in general generation and editing, providing robust and versatile unified foundation. Table 5 Evaluation of general text-to-image generation capabilities on GenEval [12] benchmark."
        },
        {
            "title": "Overall",
            "content": "GPT-4o Seedream 4.0 0.99 0.99 0.92 0.92 0.85 0.72 0.92 0.91 0.75 0. Unified Understanding and Generation w/o Reasoning. TokenFlow-XL Harmon Show-o Janus Pro MetaQuery-XL BLIP3-o UniWorld-V1 Mogao OmniGen2 MMaDA Lumina-DiMOO Hunyuan-Image 3.0 Qwen-Image GoT Mind-Omni IRG BAGEL Uni-CoT Ours 0.95 0.99 0.95 0.99 0.99 1.00 1.00 0.99 1.00 1.00 0.99 0.60 0.86 0.52 0.89 0.93 0.97 0.95 0.76 0.94 0.92 0.92 0.41 0.66 0.49 0.59 0.79 0.83 0.64 0.61 0.85 0.48 0. 0.81 0.85 0.82 0.90 0.89 0.93 0.88 0.84 0.89 0.82 0.88 0.16 0.74 0.11 0.79 0.49 0.84 0.55 0.20 0.85 0.42 0.76 Unified Understanding and Generation Reasoning. 0.99 0.98 0.98 0.99 1.00 0.94 0.94 0.95 0.96 0.96 0.71 0.83 0.84 0.84 0. 0.90 0.86 0.95 0.92 0.90 0.71 0.74 0.78 0.57 0.88 0.61 0.74 0.24 0.48 0.28 0.66 0.70 0.80 0.76 0.37 0.76 0.63 0.77 0.71 0.73 0.77 0.71 0.82 0.84 0. 0.55 0.76 0.53 0.80 0.80 0.84 0.80 0.89 0.80 0.63 0.88 0.72 0.87 0.64 0.83 0.85 0.88 0.83 0.90 2https://huggingface.co/datasets/vivym/midjourney-prompts 15 Table 6 Evaluation of general text-to-image generation capabilities on DPG [12] benchmark."
        },
        {
            "title": "Model",
            "content": "GPT-4o Seedream 4.0 TokenFlow-XL Show-o Janus Pro MetaQuery-XL BLIP3-o UniWorld-V1 Mogao OmniGen2 MMaDA Lumina-DiMOO Hunyuan-Image 3.0 Qwen-Image 88.89 94.10 88.94 92.28 89.84 92.75 92.63 93. Unified Understanding and Generation w/o Reasoning. 78.72 86.90 83.64 82.37 88.81 77.81 81.46 92.12 91.32 79.22 88.90 88.39 90.03 88.83 78.48 92.08 92.53 91.56 81.29 89.40 88.44 88.26 90.18 81.74 88.98 89.13 92.02 85.22 89.32 89.27 93.18 89.37 84.79 94.31 92.13 94.31 Unified Understanding and Generation Reasoning. Mind-Omni BAGEL Ours 89.10 88.94 91.78 90.37 91.23 91.29 90.76 90.82 91.12 90.96 92. 71.20 89.02 87.22 85.40 90.27 63.20 82.00 91.92 92.73 89.20 88.67 92.27 85.15 88.25 73.38 67.48 84.19 82.05 81.60 81.38 84.33 83.57 69.97 86.04 86.10 88.32 82.50 85.07 86.21 Table 7 Evaluation of general image editing capabilities on ImgEdit [30] and GEdit-EN [31] benchmarks. Model ImgEdit GEdit-EN Add Adjust Extract Replace Remove Background Style Hybrid Action Overall G_SC G_PQ G_O GPT-4o 4. 4.33 2.90 4.35 3.66 4.57 4. 3.96 4.89 4.20 Gemini 2.0 Seedream 4.0 4.52 4.41 2. 4.56 4.44 4.30 4.76 3.33 4. 4.18 Unified Understanding and Generation w/o Reasoning. Janus 4o 3.60 UniWorld-V 3.82 3.66 OmniGen2 3.74 3.54 LightFusion-World 4. 3.37 Qwen-Image-Edit 4.38 4.16 2.28 2. 1.77 1.25 3.43 3.27 3.45 3. 4.63 4.66 2.28 3.02 2.77 3. 4.14 3.32 2.99 3.57 4.24 4. 4.47 4.71 4.81 4.69 4.81 2. 2.96 2.30 3.91 3.82 4.13 2. 4.14 4.45 4.69 3.26 3.26 3. 3.85 4.27 Unified Understanding and Generation Reasoning. BAGEL UniCoT 3. 3.31 1.88 2.62 2.88 3.44 4. 2.38 4.17 3.20 Ours 4.14 4.06 2.49 4. 4.31 4.23 4.65 2.58 4.68 4. 7.85 6.73 8.24 4.93 7. 7.00 8.00 7.36 7.91 7.46 7. 7.53 6.61 6.32 8.08 7.68 7.43 4.85 6.77 6.41 7. 6.58 7.86 7.56 6.83 6.52 6. 6.74 7.66 6.94 A.4 Case Study We present additional UniReason results on both T2I generation and image editing tasks in Fig 5. The results demonstrate that, while maintaining high-quality T2I generation and image editing performance, UniReason exhibits strong reasoning capabilities, enabling it to handle complex scenarios such as maze navigation, temporal evolution, and spatial camera viewpoint transformations. Moreover, UniReason shows robust refinement ability, effectively correcting fine-grained details such as faces, text, and hand gestures, thereby improving the quality of the initial images and rectifying errors introduced during the initial generation. 16 Figure 5 Qualitative results of UniReason on both T2I generation (blue column) and image editing task (orange column)."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanyang Technological University",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "University of Southern California",
        "Zhejiang University"
    ]
}