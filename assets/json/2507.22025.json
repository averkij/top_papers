{
    "paper_title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding",
    "authors": [
        "Shuquan Lian",
        "Yuhang Wu",
        "Jia Ma",
        "Zihan Song",
        "Bingqi Chen",
        "Xiawu Zheng",
        "Hui Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a \"Simple Thinking\" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro."
        },
        {
            "title": "Start",
            "content": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding Shuquan Lian1, Yuhang Wu1, Jia Ma1, Zihan Song1, Bingqi Chen1, Xiawu Zheng1, Hui Li1 1Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China Xiamen University 15420202202203@stu.xmu.edu.cn 5 2 0 2 0 3 ] A . [ 2 5 2 0 2 2 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) Continuous Reward function to incentivize high-precision grounding; 2) Simple Thinking reward to balance planning with speed and grounding accuracy; and 3) Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpotv2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro."
        },
        {
            "title": "Introduction",
            "content": "Driven by the growing capabilities of Multimodal Large Language Models (MLLMs) in image understanding (Bai et al. 2025), Graphical User Interface (GUI) agents, which execute tasks by understanding screenshots and user instructions, are advancing rapidly (Zhang et al. 2025). Prior methods for GUI agents mostly rely on Supervised Fine-Tuning (SFT), requiring large amount of humanannotated or synthesized data for teaching the agent how to plan its actions and grounding. (Qin et al. 2025; Cheng et al. 2024; Gou et al. 2025; Wu et al. 2025; Xu et al. 2024; Lin et al. 2025). Recently, Reinforcement Fine-Tuning (RFT) has emerged as an efficient and scalable way to instill reasoning abilities in MLLMs (Chen et al. 2025; Chen, Luo, and Li 2025; Liu et al. 2025b; Peng et al. 2025; Meng et al. 2025). For instance, UI-R1 (Lu et al. 2025) and GUIR1 (Luo et al. 2025) apply RFT to enhance GUI agents and achieve considerable improvements compared to previous approaches. Despite the significant momentum of GUI agent techniques, their practical application is hindered by several limitations in both training and inference stages: P1: Dilemma for Reasoning Designs: An elaborate reasoning process not only degrades grounding accuracy but also increases inference latency, while No Thinking approach exhibits low accuracy for predicting nongrounding action types (Shen et al. 2025). P2: Ineffective Reward: Agents often get stuck on complex interfaces and receive no effective learning signal (i.e., sparse reward). Besides, simple binary feedback (correct/incorrect), design used by many existing methods (Lu et al. 2025; Luo et al. 2025) may fail to endow agents with the ability to perform precise localization. P3: Visual Noise: Even well-trained agents frequently struggle to cope with high-resolution screens, as irrelevant visual noise degrades their grounding accuracy. To address the above problems, we propose UI-AGILE, framework aiming at improving both the RFT and the inference stages of GUI agents. The contributions of UI-AGILE can be summarized as follows: To overcome P1, UI-AGILE applies Simple Thinking strategy (Sec. 3.1) that employs reasoning through thoughts with appropriate lengths. UI-AGILE operationalize Simple Thinking through specialized reward function. Simple Thinking effectively balances the improvement of both the core grounding task and the prediction of non-grounding action types. To tackle P2, UI-AGILE harnesses design of continuous grounding reward (Sec. 3.2) for the RFT stage instead of using the common binary reward to incentivize more precise localization to the targets center. Furthermore, UI-AGILE employs cropping-based resampling (Sec. 3.3) to dynamically adjust the difficulty of training samples to avoid ineffective training with zero reward. To solve P3, UI-AGILE uses visual noise reduction method termed decomposed grounding with selection (Sec. 3.4). It decomposes high-resolution screenshot into multiple sub-images, generates candidate elements on each, and finally uses Vision-Language Model (VLM) to adjudicate the best match. This approach significantly improves the agents grounding accuracy on high-resolution displays during inference. Figure 1: An overview of UI-AGILE, illustrating its training and inference pipelines. (1) Left: The training pipeline enhances the RFT process with our three core contributions: Simple Thinking, continuous grounding reward and cropping-based resampling. Continuous grounding reward being zero would result in crop-based resampling. (2) Right: The inference pipeline contains our proposed decomposed grounding with selection. Extensive experiments validate the effectiveness of our methods. Trained on about only 9k samples for just 2 epochs, UI-AGILE shows superior performance, while also showcasing strong general agent capabilities. Furthermore, our inference method can act as plug-and-play enhancement for wide range of existing agents, improving the accuracy of some existing open-source models. Overall, on two benchmarks ScreenSpot-Pro and ScreenSpot-v2, our methods achieve the state-of-theart performance. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro."
        },
        {
            "title": "2 Related Work\nReinforcement Learning (RL) for Large Models Re-\ncently, reinforcement learning (RL) techniques for training\nlarge models have gained significant momentum. The focus\nhas shifted from traditional policy optimization algorithms\nlike PPO (Schulman et al. 2017), towards alignment-centric\nmethods such as DPO (Rafailov et al. 2023), and more re-\ncent rule-based algorithms like GRPO (Shao et al. 2024).\nThese algorithms have achieved remarkable success in en-\nhancing the reasoning capabilities of large models on com-\nplex tasks, with models like OpenAI O1 (Jaech et al. 2024)\nand DeepSeek-R1 (DeepSeek-AI et al. 2025) setting new\nstandards in mathematics and code generation. The efficacy\nof these approaches prompted their rapid extension into the\nmultimodal domain (Chen et al. 2025; Chen, Luo, and Li\n2025; Liu et al. 2025b; Peng et al. 2025; Meng et al. 2025).",
            "content": "GUI Agents The field of GUI agents has evolved rapidly (Wang et al. 2024; Li and Huang 2025). Following early works like CogAgent (Hong et al. 2024) and SeeClick (Cheng et al. 2024), recent studies rely on SFT to operate directly on visual inputs. Show-UI (Lin et al. 2025) innovates on visual processing efficiency. OS-Atlas (Wu et al. 2025), UGround (Gou et al. 2025) and Aria-UI (Yang et al. 2024) propose novel, large-scale pipelines to collect and synthesize millions of GUI agent trajectories, significantly improving model generalization. Aguvis (Xu et al. 2024) introduced two-stage training process that explicitly uses VLM-generated Chain-of-Thought (CoT) data to teach planning and reasoning. JEDI (Xie et al. 2025) constructs refusal part by mismatching existing instructions with unrelated screenshots. Standing out in complexity and scale, UI-TARS (Qin et al. 2025) utilizes the largest dataset and the most intricate training pipeline, which involves SFT and DPO on human annotated CoT data to improve performance. This data-intensive scaling has motivated shift towards more effective RL paradigms, first explored by UIR1 (Lu et al. 2025) and GUI-R1 (Luo et al. 2025). InfiGUIR1 (Liu et al. 2025a) employs Spatial Reasoning Distillation and RL to enhance planning and error recovery capabilities. GUI-G1 (Zhou et al. 2025) leverages Hit-based reward and IoU-based reward for improving GUI agents."
        },
        {
            "title": "3 Our Framework UI-AGILE\nFig. 1 provides an overview of UI-AGILE, which aims at\nimproving both training and inference stages of GUI agents.\nAt training time, UI-AGILE adopts a new design for the",
            "content": "reward function, which consists of Simple Thinking reward for efficient reasoning (Sec. 3.1) and continuous grounding reward (Sec. 3.2) for precise localization. UIAGILE further uses cropping-based resampling (Sec. 3.3), novel strategy designed to overcome the sparse reward issue. The model generates multiple responses from an image and instruction, which are evaluated by reward functions, including Simple Thinking reward and continuous grounding reward. If training sample proves too difficult (i.e., receive grounding reward of zero for all generated responses), the image will be cropped to simplify the task, and the model resamples new responses from this modified input. At inference time, UI-AGILE applies decomposed grounding with selection (Sec. 3.4) to enhance grounding on the high-resolution displays common in modern applications, making GUI agents more practical for real-world use. 3.1 Simple Thinking for Reconciling the Reasoning Dilemma (P1) GRPO algorithm (Shao et al. 2024) is widely recognized as powerful technique for instilling complex reasoning abilities in LLMs, often through rewarding elaborate chains of thought. Lu et al. (2025) posit that excessive reasoning is not essential for GUI grounding and can even be detrimental. However, the complete role of GUI agents extends beyond mere grounding, since deciding next action (e.g., click or type) inherently demands foundational level of reasoning. To reconcile the dilemma of whether to apply reasoning for enhancing action-type prediction or discard excessive reasoning to avoid ineffective grounding, we propose Simple Thinking strategy. It encourages thoughts with an appropriate length, operationalized through specialized reward function. It also reduces training and inference costs, practical consideration for real-world deployment. The total reward for the thought process, Rthink is formally defined as: Rthink = I(Rgrounding > 0) (Rlength(L) + Rbonus) (1) Rlength(L) = 1.0 (cid:16) (cid:16) 1 2 1 0 if lideal start < lideal end 1 cos (cid:16) π Llmin lideal startlmin (cid:17)(cid:17) if lmin < lideal start 1 + cos (cid:16) π Llideal end lmaxlideal end (cid:17)(cid:17) if lideal end < < lmax otherwise (2) where: I(Rgrounding > 0) is an indicator function that grants reward only when the grounding reward Rgrounding > 0, linking reasoning to effective outcomes. Rlength(L) is non-linear reward based on the reasoning length L. lideal start and lideal end define an ideal range of reasoning length, where reward is 1. The reward will be zero if the reasoning length exceeds lmin or lmax. Rbonus is fixed bonus for syntactically complete thoughts (e.g., ending with proper punctuation), encouraging structured reasoning. Simple Thinking defines an ideal range where the reward is maximized, encouraging thoughts that are neither too brief (under-thinking) nor too verbose (overthinking). Outside this ideal range, it uses cosine functions for smooth degradation down to reward of zero at the absolute bounds. This smooth, non-linear penalty provides more informative and stable learning signal for RL than hard cliff, gently guiding the model toward the desired length. Furthermore, the additional bonus for syntactically complete thoughts discourages incomplete reasoning, thereby ensuring better training stability."
        },
        {
            "title": "3.2 Continuous Grounding Reward for Precise",
            "content": "Localization (P2) Prior works (Lu et al. 2025; Luo et al. 2025) shift the focus of GUI agent evaluation from traditional object grounding (i.e., IoU) to the precision of the action coordinate. To this end, they typically employ simple binary reward for localization: reward of 1 for correct prediction (e.g., inside the target radius) and 0 otherwise. However, this binary reward is insufficient for highprecision control, as it treats all successful predictions equally. For instance, point on the targets edge receives the same reward as one at its center. This non-discriminatory feedback misguides the model to learn an elements boundaries rather than its semantic core. To resolve this issue, we introduce continuous grounding reward. Instead of binary outcome, the continuous reward is calculated as function of the distance from the predicted point to the center of the ground-truth bounding box: R(x, y) = (cid:26)1 + exp(4 d2 norm) 0 if (x, y) BBox otherwise (3) where: R(x, y) is the reward score for the predicted coordinate. (x, y) is the coordinate predicted by the agent. BBox is the ground-truth bounding box, defined by its top-left (x1, y1) and bottom-right (x2, y2) corners. dnorm is the Chebyshev distance (or norm) of the point from the center of the bounding box, normalized by the boxs dimensions. It is calculated as: dnorm = max (cid:18) cx wh , cy hh (cid:19) (4) Here, (cx, cy) = ( x1+x2 2 the bounding box, and (wh, hh) = ( x2x1 2 its half-width and half-height, respectively. ) represents the center of ) are , y2y1 2 , y1+y2 2 We employ the Chebyshev distance instead of the Euclidean distance because the reward contours generated by the Chebyshev distance are squares, which geometrically align with the rectangular shape of GUI bounding boxes. This alignment provides more logical and consistent reward landscape, strongly incentivizing the agent to minimize its deviation along both axes to achieve high reward."
        },
        {
            "title": "3.3 Cropping-Based Resampling for Sparse",
            "content": "Reward Mitigation (P2) During GRPO training, GUI agents often face the sparse reward challenge, particularly on complex tasks. When the model consistently fails to place its prediction within the correct bounding box for given screenshot, it receives persistent reward of 0. The lack of positive signals can lead to training stagnation, and difficult samples cannot contribute to model improvement. Inspired by curriculum learning (Bengio et al. 2009), training technique that trains the model on examples of increasing difficulty to ease the training process, we propose cropping-based resampling method for mitigating the sparse reward issue. It acts as dynamic difficulty adjustment mechanism during the training of UI-AGILE. If task sample yields zero reward over multiple generations, we hypothesize that the task sample is currently too difficult for the model. Then, we reduce the tasks complexity by cropping the original screenshot. The cropping is generated such that it is smaller than the original view but still fully contains the ground-truth bounding box of the target element. naive implementation is to center the ground-truth bounding box (bbox) in the new cropping, but it may fail to endow the model with the capability to perform robust localization: the model would learn trivial shortcut, such as developing bias for predicting the image center. We opt to employ scanning approach as illustrated in Alg. 1, ensuring that the cropped image fully contains the groundtruth bounding box. It firstly determines the size of cropping based on predefined ratio (lines 1-2). Then, the horizontal stride stepx is set to the difference between the cropping width and the bounding box width, while the vertical stride stepy is set to the difference between their respective heights (lines 3-5). After that, it iterates through all possible cropping windows from left-to-right and top-to-bottom across the original screenshot with the horizontal stride and the vertical stride (lines 7-18). The first window that fully contains the ground-truth bbox is selected as the new, resampled input (lines 12-16). Fig. 2 illustrates how our scanning approach identifies valid cropping windows that fully contain the ground-truth bounding box. Cropping-based resampling dynamically simplifies difficult samples to ensure that they are learnable, allowing the model to leverage more data in fewer epochs, yielding superior results within similar amount of training time."
        },
        {
            "title": "3.4 Decomposed Grounding with Selection for",
            "content": "Visual Noise Reduction (P3) Modern electronic devices feature high-resolution displays (e.g., 3840x2160), which, when converted into tokens for VLM, can result in an overwhelmingly large input sequence (e.g., over 10,000 tokens). We hypothesize that significant portion of these tokens represent irrelevant background information, acting as noise that can distract the GUI agent and degrade its grounding accuracy, i.e., the visual noise issue (P1) mentioned in Sec. 1. To validate this hypothesis, we conduct preliminary experiment on ScreenSpot-Pro (Li et al. 2025). We apply the Algorithm 1: Cropping-Based Resampling Input: Image, Bbox, scalingF actorf Output: Set(Imagecrop, Bboxcrop) 1: (wo, ho) GetW idthAndHeight(Image) 2: (wcrop, hcrop) (wo f, ho ) 3: (wb, hb) (Bbox[2] Bbox[0], Bbox[3] Bbox[1]) 4: stepx wcrop wb 5: stepy hcrop hb 6: 7: for xcropmin from 0 to wo step=stepx do 8: 9: 10: 11: xcropmax min(xcropmin + wcrop, wo) for ycropmin from 0 to ho step=stepy do ycropmin min(ycropmin + hcrop, ho) Coordcrop [xcropmin, ycropmin, xcropmax, ycropmax] if Bbox is contained within Coordcrop then Imagecrop Crop(Image, Coordcrop) Bboxcrop Bbox Coordcrop Set Set (Imagecrop, Bboxcrop) 12: 13: 14: 15: 16: 17: 18: end for 19: return Set end if end for cropping method in Sec. 3.3, but for the purpose of creating controlled test environment. For each original screenshot, we crop it to 1024x1024, ensuring the ground-truth bounding box is contained within the frame. On this new dataset, the grounding accuracy of UGround-V1-7B (Gou et al. 2025) shows significant improvement from 31.6 to 56.0 w.r.t grounding accuracy, verifying our hypothesis. It is straightforward to consider applying the croppingbased method to alleviate the visual noise. However, during inference, the location of the ground-truth bounding box is unknown, making such oracle-based cropping impossible. To overcome this challenge, we propose multi-stage decomposed grounding with selection method as shown in the right part of Fig. 1. It reduces visual noise by cropping an image into several sub-images and predicts the coordinate individually, while trying to maintain full ground-truth bounding box area. The detailed process is as follows: 1. Decomposition: The input screenshot is first divided into several overlapping sub-images, breaking down the highresolution screen into smaller, more manageable regions. 2. Candidate Generation: The GUI agent performs grounding independently on each sub-image and predicts coordinate for the sub-image. Then, we use the predicted coordinates of sub-images belonging to screenshot as the candidate coordinates for the screenshot. 3. Element Image Extraction: For each candidate point, we extract the corresponding element image by cropping bounding box centered on the candidate point from the sub-image. 4. Selection: For the final selection stage, we prompt the VLM with the users instruction, the candidate element image, and direct question asking whether the image match the instruction. Then use the models output logit for the Yes token as direct relevance score for the candidate. The candidate with the highest Yes score Figure 2: An example of cropping-based resampling. Yellow bounding boxes are the ground truth, red bounding boxes are invalid cropping, and blue bounding boxes are valid cropping. Green arrows show that the overlap of cropping windows is equivalent to the width or height of the ground-truth bounding box. is chosen as the final answer, and its corresponding coordinates are remapped to the original screenshot. This QA-based scoring allows the model to perform deeper contextual prediction. This process directly benefits from our continuous reward function (Sec. 3.2), as it trains the model to predict points closer to the center of target elements, leading to higherquality extracted images and thus more accurate final selection. Analysis of Inference Cost We analyze the inference latency of using decomposed grounding with selection by breaking it down into three primary stages: prefilling, decoding and selection stage. Counter-intuitively, our approach can theoretically accelerate the computationally-heavy prefilling stage. Recall that the self-attention mechanism has quadratic time complexity O(n2) concerning the input sequence length (Vaswani et al. 2017). By splitting single large image with tokens into 4 sub-images (each with roughly n/4 tokens), the total computational cost for attention scales proportionally to 4 ( 4 , suggesting theoretical speedup. Such cost reduction far outweighs the slightly increased overhead of repeatedly processing the text prompt for each sub-image. 4 )2 = n2 While the decoding process runs for each sub-image, the small number of output tokens (compared to image tokens) per run ensures that the cumulative decoding cost will not raise much. Finally, the VLM-based selection stage is computationally inexpensive. The input element images are very small, and the process only requires single forward pass to acquire the logits for Yes/No answer. Overall, the cost of applying decomposed grounding with selection is low. In addition to the above analysis, we provide results on actual running time in Sec. 4.6. Crucially, we believe this overhead could be eliminated or even reversed with future optimizations in inference engines tailored for this many small requests workload, potentially making our high-accuracy method faster than the baseline."
        },
        {
            "title": "Value",
            "content": "learning rate num generations num train epochs per device train batch size cropping factor gradient accumulation steps from 1e-06 to 4.36e-10 8 2 4 0.6 4 Table 1: Default Hyperparameters"
        },
        {
            "title": "4.1\nData. We collect data related to GUI tasks from multi-\nple open-source datasets, including UI-R1 (Lu et al. 2025),\nGUI-R1 (Luo et al. 2025), Aguvis (Xu et al. 2024) and\nGrounding-R1 (Yang et al. 2025). We filter them using Om-\nniParser (Wan et al. 2024) following Grounding-R1 (Yang\net al. 2025). We randomly sample about 9k examples to train\nUI-AGILE-3B and UI-AGILE-7B.",
            "content": "Training Details. We use the trl framework (von Werra et al. 2020) to implement the cropping-based resampling strategy and reward functions. The sampling process is attempted 4 times at most and is bypassed entirely if the bboxs dimensions exceed the target crop size. Following prior works (Lu et al. 2025; Luo et al. 2025; Liu et al. 2025a; Zhou et al. 2025), we use Qwen2.5-VL-3B1 and Qwen2.5VL-7B2 as base models. Tab. 1 provides the default hyperparameters where cropping factor is the width and height ratio of new attempted image and last attempted image. Inference Details. For decomposed grounding with selection, the input image is divided into four sub-images scaling to 60% of the original dimensions, with adjacent subimages overlapping by 10% of the original images width 1https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct 2https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct and height. In the element image extraction stage, we define the elements area by creating simple bounding box centered on the predicted point with the width and height equal to 14% of the sub-images width and height. We have also explored more sophisticated approach using OmniParser to refine this bounding box. However, it does not improve performance and increases the inference overhead. In the selection stage, we use Qwen2.5VL-7B-instruct1 to choose the final answer."
        },
        {
            "title": "4.2 Grounding Capability Evaluation",
            "content": "We evaluate the grounding ability on ScreenSpot-v2 (Wu et al. 2025) and ScreenSpot-Pro (Li et al. 2025). ScreenSpotv2 is corrected version of the original ScreenSpot (Cheng et al. 2024), providing evaluation of GUI grounding capability across mobile, desktop, and web platforms. ScreenSpotPro focuses on high-resolution professional environments, featuring expert-annotated tasks spanning 23 applications, five industries, and three operating systems. Since the images in ScreenSpot-v2 are already pre-cropped while those in ScreenSpot-Pro are full, uncropped displays, we evaluate our decomposed grounding with selection method exclusively on ScreenSpot-Pro. Effectiveness of the proposed inference enhancement. As shown in Tab. 2, our decomposed grounding with selection method shows significant improvements on the challenging ScreenSpot-Pro benchmark. It provides universal and substantial performance boost across all tested models, regardless of their original training paradigm (SFT or RFT). For instance, it elevates the average score of OS-Atlas-7B from 18.9 to 33.1 (+75.1%), and boosts Aguvis-7B from 20.4 to 36.5 (+78.9%). The consistent improvement validates the effectiveness of decomposed grounding with selection and its high applicability as plug-and-play inference enhancement. Effectiveness of the proposed training enhancement. Besides, Tab. 2 shows that our core UI-AGILE-3B and UIAGILE-7B models, even without decomposed grounding, establish new state-of-the-art among 3B and 7B models on ScreenSpot-Pro. Trained on only 9K examples with 2 epochs, they (37.9 for 3B and 44.0 for 7B) surpass other RFT-based models like UI-R1-E (33.5), InfiGUI-R1-3B and GUI-R1-7B (32.1). UI-AGILE-7B even outperforms the much larger model UI-TARS-72B (38.1) trained on approximately 50 billion tokens. On the ScreenSpot-v2 benchmark (Tab. 3), our UI-AGILE-7B also achieves state-of-theart grounding accuracy with an average score of 92.1. The above results demonstrate the effectiveness of our proposed Simple Thinking reward, continuous grounding reward, and cropping-based resampling for improving the training of GUI agents. Overall, as shown in Tab. 2, using both our proposed training and inference enhancement methods (UI-AGILE7B + Decomposed Grounding) brings 23% grounding accuracy improvement over the best baseline (JEDI-7B) on ScreenSpot-Pro."
        },
        {
            "title": "4.3 Agent Capability Evaluation\nIn addition to grounding-specific benchmarks, we also eval-\nuate UI-AGILE-3B and UI-AGILE-7B on AndroidCon-\ntrol (Li et al. 2024) to assess its general agent capabilities.",
            "content": "Following the evaluation setting of OS-Atlas (Wu et al. 2025), we use three metrics: action type prediction accuracy (Type), grounding accuracy (GR), and the overall step success rate (SR). Type accuracy measures the exact match for the predicted action (e.g., click or scroll). For GR, prediction is considered successful if it falls within 14% screenwidth radius of the ground-truth coordinate. The most holistic metric, SR, deems step successful only if both the action type and all its associated arguments (e.g., coordinates for click, direction for scroll, or text for an input) are correct. Following OS-Atlas, we use 7,708 examples for fair comparison, while some works (e.g., Aguvis (Xu et al. 2024) and UGround (Gou et al. 2025)) randomly sample 500 action steps for testing. The evaluation is conducted under two distinct settings. In the AndroidControl-Low setting, the agent receives specific, low-level instruction for each step. In contrast, the AndroidControl-High setting provides the agent with high-level goal, requiring it to infer the correct action for the current step based on the conversation history and previous actions, thereby testing its multi-step reasoning capability. As presented in Tab. 4, our UI-AGILE-7B model achieves the best performance (SR: 77.6 and 60.6) compared to other RFT models including UI-R1-E (SR: 71.37 and 35.88), GUI-R1-3B (SR: 64.41 and 46.55) and GUI-R1-7B (SR: 66.52 and 51.67). Notably, our smaller UI-AGILE-3B model with SR of 77.4 and 56.9 also surpasses 7B baselines like GUI-R1-7B (SR: 66.5 and 51.7). For Type, similar trend can be observed. For GR, UI-AGILE archives the best performance in most cases except that GUI-R1-7B shows higher GR (65.6) in the AndroidControl-High setting. We attribute it to different training philosophies: their prolonged training on smaller dataset for 9 epochs may foster specialization, whereas our training on larger, more diverse dataset for 2 epochs prioritizes generalization. This hypothesis is supported by our models dominant performance on dedicated grounding benchmarks ScreenSpot-Pro and ScreenSpot-v2. Overall, the remarkable results on AndroidControl demonstrate that the improvements gained from our methods are not confined to improving grounding capability but also translate effectively to better decision-making and execution in complex, multi-step agent scenarios."
        },
        {
            "title": "4.4 Ablation Study\nTo further verify the contribution of each of our training\ntechniques (“Simple Thinking” reward, continuous ground-\ning reward, and cropping-based resampling), we conduct an\nablation study of UI-AGILE-3B and the results are shown in\nFig. 3 and Fig. 4. We can observe that:",
            "content": "Applying continuous grounding reward and croppingbased resampling improve the performance by approximately 10% and 12.4% on ScreenSpot-Pro, respectively. Model Examples Epochs Dev Creative CAD Scientific Office OS Avg pass@4 Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Supervised Fine-tuning CogAgent-18B Aria-UI ShowUI-2B JEDI-3B JEDI-7B OS-Atlas-7B +Decomposed Grounding Aguvis-7B +Decomposed Grounding UGround-V1-7B +Decomposed Grounding UI-TARS-2B UI-TARS-7B +Decomposed Grounding UI-TARS-72B 222M 16.6M 256K 4M 4M 13M - 1M - 10M - - - - - Zero Shot / Reinforcement Fine-tuning InfiGUI-R1-3B GUI-G1-3B Qwen2.5-VL-3B +Decomposed Grounding Qwen2.5-VL-7B +Decomposed Grounding GUI-R1-3B +Decomposed Grounding GUI-R1-7B +Decomposed Grounding UI-R1-3B UI-R1-E +Decomposed Grounding UI-AGILE-3B +Decomposed Grounding UI-AGILE-7B +Decomposed Grounding 32K 17K - - - - 3K - 3K - 136 2K - 9K - 9K - - - - - - - - 1 - - - - - - - - - - - - 9 - 9 - 8 8 - 2 - 2 - 9.6 0.7 14.9 7.7 23.7 0.0 16.2 11.3 16.9 9.1 1.4 7.7 61.0 13.8 53.5 36.1 42.9 11.0 50.0 11.9 38.0 14.1 72.9 25.5 75.1 47.2 33.6 16.9 39.5 5.6 13.0 22.2 4.7 20.3 27.1 13.2 10.3 15.3 54.2 18.2 64.4 32.1 38. 7.1 7.6 2.5 27.4 3.1 1.6 0.0 9.4 0.0 0.0 2.2 9.0 0.0 2.1 0.0 8.4 0.0 1.9 7.5 1.8 6.4 7. 33.8 49.4 1.4 5.5 30.8 52.0 28.8 0.7 30.5 50.6 11.7 60.1 3.5 5.6 2.8 7. 12.2 26.4 14.7 31.0 5.5 51.3 18.8 48.5 57.8 14.5 49.0 11.9 20.3 8.3 3.1 6. 1.6 4.7 1.6 7.8 9.1 33.3 26.2 33.3 54.9 18.2 57.6 18.9 49.5 9.4 8. 38.4 11.3 30.8 45.8 62.5 20.0 63.3 18.9 45.8 3.4 9.0 2.3 6.7 18.9 33.1 20.4 36.5 59.7 14.6 59.9 17.0 40.2 31.6 62.5 21.8 67.8 18.9 48.6 14.6 36. 7.9 27.7 56.9 17.3 50.3 17.0 21.5 42.9 4.1 47.7 63.9 31.8 63.3 20.8 30.8 16.9 35.7 58.4 12.4 50.0 59.7 19.3 54.0 15.4 38.1 12.5 63.2 27.3 71.8 28.3 45.8 21.3 41.9 63.0 17.3 57.1 15.4 18.8 17.2 64.6 20.9 63.3 26.4 42.1 15.7 38.1 17.8 20.8 6.3 9.1 4.7 9.4 5. 51.3 12.4 44.9 7.0 33.0 14.1 58.3 20.0 65.5 28.3 43.9 12.4 35.7 50.7 10.3 36.6 11.9 39.6 32.8 24.9 4.2 4.1 31.8 42.9 11.2 25.9 8.3 52.6 13.7 4.2 54.5 24.7 5.5 27.9 8.4 60.4 13.1 33.3 27.9 40.9 47.8 4.8 31.5 63.6 13.1 55.6 57.1 28.4 37.9 8.3 66.9 15.2 50.0 10.5 32. 2.8 4.9 8.4 9.4 4.7 3.1 3.1 6.2 6.3 6.3 6.3 4.7 61.8 30.0 67.2 32.1 32.5 10.6 37.1 22.7 43.8 12.7 42.4 15.1 17.8 31.2 47.9 10.0 55.9 17.0 46.7 46.5 50.8 11.3 29.9 10.1 24.5 50.0 13.6 63.3 17.0 51.4 16.0 33. 2.2 9.0 7.3 65.3 19.1 58.2 18.9 29.0 30.9 61.8 16.4 62.7 20.8 44.9 10.1 37.1 54.9 10.9 59.9 13.2 41.1 13.5 32.1 59.0 13.6 68.4 24.5 60.7 18.0 39.3 2.2 4.1 6.9 27.3 22.7 46.1 41.9 63.6 17.2 59.6 10.0 43. 11.2 17.8 42.4 11.8 32.2 11.3 13.1 37.1 12.5 56.9 21.8 65.0 26.4 32.7 10.1 33.5 66.0 21.8 68.4 43.4 56.1 19.1 43.3 3.5 4.2 6.3 4.5 6.3 8. 9.0 50.5 44.2 20.3 62.5 22.7 65.5 22.6 35.5 12.4 37.9 53.2 66.9 16.6 58.1 11.9 47.2 10.9 66.7 24.5 72.3 34.0 58.9 22.5 45.0 49.2 14.1 72.9 25.5 75.1 30.2 45.8 20.2 44.0 64.3 15.2 53.0 79.1 24.1 60.6 11.2 53.3 10.9 66.0 26.4 79.1 39.6 59.8 22.5 48.7 9.8 - - - - - - 42. - 44.3 - 47.3 - - 50.3 - - - - 38.8 - 42. - 46.0 - 48.6 - - 52.6 - 54.4 - 59.2 Table 2: Grounding accuracy on ScreenSpot-Pro. +Decomposed Grounding denotes that the model uses our decomposed grounding with selection for enhancing inference. Results marked in bold represent the best performance, and underlined results indicate the second-best performance. The pass@4 metric indicates the success rate where task is considered solved if the prediction of at least one of the sub-images is correct. The former incentivizes more precise localization to the targets center and the later helps avoid ineffective training with zero reward. They also slightly improve grounding accuracy on ScreenSpot-v2 where the performance of the base model is already high and it is difficult to achieve significant gains. Removing Simple Thinking during training (i.e., No Thinking) leads to higher grounding accuracy on ScreenSpot-Pro and ScreenSpot-v2 (about 0.4% and 0.7% gains). However, Fig. 4 demonstrates that integrating Simple Thinking enhances the agents decisionmaking with planning, resulting in noticeable improvement in SR on the AndroidControl benchmark, with SR increases of 15.5% and 3.4% in Low and High settings, respectively."
        },
        {
            "title": "4.5 Analysis of Attempts Per Step\nFigure 5 shows the distribution of attempts per GRPO train-\ning step, where each step processes a batch of two train-\ning samples. In the first epoch, we find that only 61.8% of\ntraining steps are fully successful on the initial attempt (i.e.,\nboth samples in the batch are solved without resampling).\nThis means that without our strategy, a minimum of 19.1%\n(38.2% ÷ 2) of training samples would have provided no",
            "content": "Method Mobile Desktop Web Avg. Text Icon Text Icon Text Icon 78.4 50.7 70.1 29.3 55.2 32.5 55.1 SeeClick 87.2 59.7 72.7 46.4 85.9 63.0 71.9 OS-Atlas-4B 95.0 73.3 92.8 64.9 89.6 72.4 83.7 OS-Atlas-7B Aguvis-7B 94.9 80.1 95.0 77.9 91.4 69.9 85.6 Qwen2.5-VL-3B 96.1 74.8 87.8 53.0 86.9 70.4 80.7 Qwen2.5-VL-7B 98.4 84.8 88.4 74.7 92.5 77.6 87.5 98.1 79.0 94.0 66.7 93.3 69.2 85.2 GUI-R1-3B 98.8 86.4 92.3 79.4 92.1 77.2 88.7 GUI-R1-7B 99.6 80.1 95.6 75.4 91.6 81.2 88.7 UI-R1-E 95.2 79.1 90.7 68.6 87.2 78.3 84.7 UI-TARS-2B 96.9 89.1 95.4 85.0 93.6 85.2 91.6 UI-TARS-7B 94.8 86.3 91.2 87.9 91.5 87.7 90.3 UI-TARS-72B UI-AGILE-3B UI-AGILE-7B 99.6 86.4 93.9 74.5 91.8 77.6 88.6 100.0 91.1 95.6 84.8 94.2 83.0 92. Table 3: Grounding accuracy on ScreenSpot-v2. Results marked in bold represent the best performance, and underlined results indicate the second-best performance. Models AndroidControl-Low AndroidControl-High Type GR 64.6 71.2 Os-Atlas-4B Os-Atlas-7B 73.0 73.4 Qwen2.5-VL-3B 80.5 79.4 Qwen2.5-VL-7B 78.0 87.1 87.0 77.8 UI-R1-E 83.7 81.6 GUI-R1-3B 85.2 84.0 GUI-R1-7B UI-AGILE-3B UI-AGILE-7B 85.4 87.6 87.7 88.1 SR 40.6 50.9 67.8 68.7 71.4 64.4 66.5 74.3 77.6 Type GR 49.0 49.5 57.4 54.9 64.4 46.1 69.1 59.1 66.4 37.8 58.0 56.2 71.6 65. 78.6 60.7 80.1 61.9 SR 22.8 29.8 44.4 50.1 36.9 46.5 51.7 56.9 60.6 Table 4: Action type accuracy, grounding accuracy and success rate on AndroidControl-Low and AndroidControlHigh. Results marked in bold represent the best performance, and underlined results indicate the second-best performance. learning signal. Overall attempt numbers decreases in the second epoch, demonstrating that the model learns from the samples salvaged by our method."
        },
        {
            "title": "4.6 Analysis of Inference Time",
            "content": "We report the inference time of our decomposed grounding with selection method on the full ScreenSpot-Pro dataset (Li et al. 2025) using the vLLM framework (Kwon et al. 2023) and one 80G A800 GPU card. As baseline, the standard grounding approach applied to UI-AGILE-7B without our method completes the benchmark in 30 minutes. When applying our method, the decomposed grounding stage takes 35 minutes. The subsequent VLM-based selection stage requires additional 4 minutes. The modest increase in overhead is practical trade-off for the substantial gain of grounding accuracy brought by our method. Figure 3: Ablation study on grounding benchmarks. Figure 4: Ablation study on AndroidControl benchmark. Figure 5: Distribution of attempts per step throughout the training process. Max attempt number is set to 4."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we introduce UI-AGILE, comprehensive framework designed to enhance GUI agents training and inference. We tackle the practical challenges of the reasoninggrounding dilemma, ineffective reward, and visual noise. During training, our solution integrates three key innovations: Simple Thinking reward to foster efficient yet effective reasoning, continuous grounding reward that incentivizes high-precision localization, and cropping-based resampling strategy to overcome the sparse reward problem. For inference, we introduce decomposed grounding with selection, novel method that reduces visual noise and dramatically improves grounding accuracy on high-resolution screens while the inference cost is only slightly increased. Experimental results demonstrate the effectiveness of our proposed techniques on enhancing GUI agents grounding capability and general capability. Limitations and future work. The VLM used in the selection stage of our decomposed grounding with selection method is general-purpose, pre-trained model. promising future direction would be to fine-tune this adjudicator model on curated dataset of candidate UI elements, enhancing its selection accuracy to achieve further gains in overall grounding performance. References Bai, S.; Chen, K.; Liu, X.; Wang, J.; and et al, W. G. 2025. Qwen2.5-VL Technical Report. arXiv:2502.13923. Bengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. 2009. Curriculum learning. In ICML, volume 382 of ACM International Conference Proceeding Series, 4148. Chen, L.; Li, L.; Zhao, H.; Song, Y.; and Vinci. 2025. R1-V: Reinforcing Super Generalization Ability in VisionLanguage Models with Less Than $3. https://github.com/ Deep-Agent/R1-V. Accessed: 2025-02-02. Chen, Z.; Luo, X.; and Li, D. 2025. VisRL: IntentionDriven Visual Perception via Reinforced Reasoning. arXiv:2503.07523. Cheng, K.; Sun, Q.; Chu, Y.; Xu, F.; Li, Y.; Zhang, J.; and Wu, Z. 2024. SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents. In ACL, volume 1, 93139332. DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.; and et al, R. Z. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948. Gou, B.; Wang, R.; Zheng, B.; Xie, Y.; Chang, C.; Shu, Y.; Sun, H.; and Su, Y. 2025. Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents. In ICLR. Hong, W.; Wang, W.; Lv, Q.; Xu, J.; Yu, W.; Ji, J.; Wang, Y.; Wang, Z.; Dong, Y.; Ding, M.; and Tang, J. 2024. CogAgent: Visual Language Model for GUI Agents. In CVPR, 1428114290. Jaech, A.; Kalai, A.; Lerer, A.; Richardson, A.; and et al, A. E. 2024. OpenAI o1 System Card. arXiv:2412.16720. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J.; Zhang, H.; and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In SOSP, 611626. Li, J.; and Huang, K. 2025. Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning. arXiv:2504.20464. Li, K.; Meng, Z.; Lin, H.; Luo, Z.; Tian, Y.; Ma, J.; Huang, Z.; and Chua, T. 2025. ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use. arXiv:2504.07981. Li, W.; Bishop, W. E.; Li, A.; Rawles, C.; Campbell-Ajala, F.; Tyamagundlu, D.; and Riva, O. 2024. On the Effects of Data Scale on UI Control Agents. In NeurIPS. Lin, K. Q.; Li, L.; Gao, D.; Yang, Z.; Wu, S.; Bai, Z.; Lei, S. W.; Wang, L.; and Shou, M. Z. 2025. ShowUI: One Vision-Language-Action Model for GUI Visual Agent. In CVPR, 1949819508. Liu, Y.; Li, P.; Xie, C.; Hu, X.; Han, X.; Zhang, S.; Yang, H.; and Wu, F. 2025a. InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners. arXiv:2504.14239. Liu, Z.; Sun, Z.; Zang, Y.; Dong, X.; Cao, Y.; Duan, H.; Lin, D.; and Wang, J. 2025b. Visual-RFT: Visual Reinforcement Fine-Tuning. arXiv:2503.01785. Lu, Z.; Chai, Y.; Guo, Y.; Yin, X.; Liu, L.; Wang, H.; Xiong, G.; and Li, H. 2025. UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning. arXiv:2503.21620. Luo, R.; Wang, L.; He, W.; and Xia, X. 2025. GUI-R1 : Generalist R1-Style Vision-Language Action Model For GUI Agents. arXiv:2504.10458. Meng, F.; Du, L.; Liu, Z.; Zhou, Z.; Lu, Q.; Fu, D.; Shi, B.; Wang, W.; He, J.; Zhang, K.; Luo, P.; Qiao, Y.; Zhang, Q.; and Shao, W. 2025. MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning. arXiv:2503.07365. Peng, Y.; Zhang, G.; Zhang, M.; You, Z.; Liu, J.; Zhu, Q.; Yang, K.; Xu, X.; Geng, X.; and Yang, X. 2025. LMMR1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL. arXiv:2503.07536. Qin, Y.; Ye, Y.; Fang, J.; and et al, H. W. 2025. UI-TARS: Pioneering Automated GUI Interaction with Native Agents. arXiv:2501.12326. Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Ermon, S.; and Finn, C. 2023. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In NeurIPS. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Zhang, M.; Li, Y. K.; Wu, Y.; and Guo, D. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300. Shen, Y.; Zhang, J.; Huang, J.; Shi, S.; Zhang, W.; Yan, J.; Wang, N.; Wang, K.; and Lian, S. 2025. DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models. arXiv:2503.04472. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is All you Need. In NIPS, 59986008. von Werra, L.; Belkada, Y.; Tunstall, L.; Beeching, E.; Thrush, T.; Lambert, N.; Huang, S.; Rasul, K.; and Gallouedec, Q. 2020. TRL: Transformer Reinforcement Learning. https://github.com/huggingface/trl. Accessed: 2025-0402. Wan, J.; Song, S.; Yu, W.; Liu, Y.; Cheng, W.; Huang, F.; Bai, X.; Yao, C.; and Yang, Z. 2024. OMNIPARSER: Unified Framework for Text Spotting, Key Information Extraction and Table Recognition. In CVPR, 1564115653. Wang, S.; Liu, W.; Chen, J.; Gan, W.; Zeng, X.; Yu, S.; Hao, X.; Shao, K.; Wang, Y.; and Tang, R. 2024. GUI Agents with Foundation Models: Comprehensive Survey. arXiv:2411.04890. Wu, Z.; Wu, Z.; Xu, F.; Wang, Y.; Sun, Q.; Jia, C.; Cheng, K.; Ding, Z.; Chen, L.; Liang, P. P.; and Qiao, Y. 2025. OS-ATLAS: Foundation Action Model for Generalist GUI Agents. In ICLR. Xie, T.; Deng, J.; Li, X.; Yang, J.; and et al, H. W. 2025. Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis. arXiv:2505.13227. Xu, Y.; Wang, Z.; Wang, J.; Lu, D.; Xie, T.; Saha, A.; Sahoo, D.; Yu, T.; and Xiong, C. 2024. Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction. arXiv:2412.04454. Yang, Y.; Li, D.; Yang, Y.; and et al, Z. L. 2025. GRPO for GUI Grounding Done Right. https://huggingface.co/blog/ HelloKKMe/grounding-r1. Accessed: 2025-06-13. Yang, Y.; Wang, Y.; Li, D.; Luo, Z.; Chen, B.; Huang, C.; and Li, J. 2024. Aria-UI: Visual Grounding for GUI Instructions. arXiv:2412.16256. Zhang, C.; He, S.; Qian, J.; Li, B.; Li, L.; Qin, S.; Kang, Y.; Ma, M.; Liu, G.; Lin, Q.; Rajmohan, S.; Zhang, D.; and Zhang, Q. 2025. Large Language Model-Brained GUI Agents: Survey. Trans. Mach. Learn. Res., 2025. Zhou, Y.; Dai, S.; Wang, S.; Zhou, K.; Jia, Q.; and Xu, J. 2025. GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents. arXiv:2505.15810."
        }
    ],
    "affiliations": [
        "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University"
    ]
}