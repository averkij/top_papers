{
    "paper_title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation",
    "authors": [
        "Zhicheng Lee",
        "Shulin Cao",
        "Jinxin Liu",
        "Jiajie Zhang",
        "Weichuan Liu",
        "Xiaoyin Che",
        "Lei Hou",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG)."
        },
        {
            "title": "Start",
            "content": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation Zhicheng Lee1, Shulin Cao1, Jinxin Liu1, Jiajie Zhang1, Weichuan Liu2, Xiaoyin Che2, Lei Hou1, Juanzi Li1 1Tsinghua University, 2Siemens AG 5 2 0 2 7 2 ] . [ 1 9 2 7 1 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, factualityenhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from predefined action space (Search and Finish). For Search action, query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until Finish action is chosen. Benefiting from ReaRAGs strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs factuality while effectively integrating robust reasoning for RetrievalAugmented Generation (RAG)."
        },
        {
            "title": "Introduction",
            "content": "Large Reasoning Models (LRMs) such as OpenAIs o1 (Jaech et al., 2024), Qwens QwQ-32B1 (Team, 2024), GLM-Zero-Preview2 and DeepSeekR1 (DeepSeek-AI et al., 2025) demonstrate impressive reasoning capabilities in complex tasks (Xu et al., 2025). These models employ deliberate reasoning at test-time before generating an answer. However, their reliance on parametric knowledge 1For simplicity, all mentions of QwQ-32B in this paper refer to QwQ-32B-Preview. 2https://open.bigmodel.cn/dev/api/ normal-model/glm-zero-preview Figure 1: Unlike LRMs, ReaRAG iteratively constructs knowledge-guided reasoning chains for factual answers. during reasoning limits their performance on multihop question answering (QA) tasks, where reasoning beyond memorized knowledge is required. To enhance LRMs factuality, RetrievalAugmented Generation (RAG) (Lewis et al., 2020; Shi et al., 2024a; Guu et al., 2020) offers promising solution by integrating external knowledge but faces challenges in retrieving relevant documents, which requires formulating precise search queries (Chan et al., 2024). Prior research has explored iterative retrieval strategies (Press et al., 2023; Shao et al., 2023), which construct reasoning chains of sub-queries and sub-answers to solve multi-hop QA. However, these methods suffer from error propagation, where mistakes in earlier steps mislead subsequent retrieval and reasoning, ultimately degrading the overall answer quality (Cao et al., 2023). To address this challenge, Search-o1 (Li et al., 2025) adopts prompt-based strategy that leverages LRMs reasoning to revise sub-queries iteratively. It also employs Reason-in-Documents module that generates sub-answers for corresponding sub-queries. However, it faces several limitations: (1) Unreliable special token generation prevents retrieval, forcing closed-book setting. (2) Information extraction failures and hallucinations in the Reason-in-Documents module mislead reasoning, reducing effectiveness and factuality. (3) Reinforcement learning (RL)-based LRMs tend to overthink (Chen et al., 2024), which is unnecessary for multi-hop QA. Our analysis shows that despite harnessing LRMs strong reasoning, Searcho1 underperforms other baselines without strong reasoning, underscoring the challenge of integrating LRMs with external knowledge in RAG tasks. In this paper, we propose ReaRAG, factualityenhanced reasoning model for RAG that iteratively constructs knowledge-guided reasoning chains. To improve retrieval robustness and mitigate overthinking, we construct dedicated dataset with restricted maximum chain length and fine-tune ReaRAG using the Thought-Action-Observation paradigm, enabling reflective reasoning before action. During inference, ReaRAG iteratively performs the search action and strategically decides when to trigger the finish action, preventing excessive searching. Guided by external knowledge from the search action, ReaRAG continuously reflects on its reasoning trajectory, detects errors, and realigns its reasoning toward the correct path, leading to improved performance on QA tasks. To validate our proposed method, we conduct experiments across four multi-hop QA benchmarks, demonstrating substantial performance improvements over existing methods. In summary, our contributions are as follows: Enhancing LRMs factuality through knowledge-guided reasoning chain. We propose ReaRAG-9B, model fine-tuned on dedicated dataset containing knowledge-guided reasoning chains, enabling reliable interactions with external knowledge sources. Effectively combining strong reasoning with RAG. By harnessing LRMs reasoning for deliberate thinking before deciding on an action, ReaRAG reflects on prior steps, uses external knowledge to identify mistakes and refine its reasoning, demonstrating robust reasoning capabilities. Compared to RL-based method, our fine-tuned model avoids excessive and redundant searches in multi-hop QA. Enhanced benchmark performance. Our approach achieves substantial improvements on multihop benchmarks: MuSiQue (Trivedi et al., 2022), HotpotQA (Yang et al., 2018) and IIRC (Ferguson et al., 2020), as well as the single-hop Natural Questions (NQ) benchmark(Kwiatkowski et al., 2019)."
        },
        {
            "title": "2 Related Work",
            "content": "Reasoning-enhanced LLMs. Numerous works have investigated how to elicit the reasoning abilities of LLMs. Early approaches, such as Chainof-Thought (COT) (Wei et al., 2022), ReAct (Yao et al., 2023b), Chain-of-Verification (CoVE) (Dhuliawala et al., 2024) and Tree of Thought (ToT) (Yao et al., 2023a) relies on prompting to generate human-like step by step reasoning chain. However, these approaches still face limitations with more complex reasoning. Recent advances of LRMs have scaled up CoT capabilities through RL (Kumar et al., 2024), enabling models to generate long CoT for self-reflection before providing final answer. Notably, LRMs include OpenAIs o1 (Jaech et al., 2024), Qwens QwQ-32B (Team, 2024), GLM-Zero-Preview2 and DeepSeekR1(DeepSeek-AI et al., 2025) demonstrate impressive performance in complex reasoning task across domains such as mathematics, coding, and scientific problem-solving. Despite these advancements, most LRMs lack the ability to interact with external knowledge sources, limiting their capacity to generate up-to-date and factual responses. Retrieval-Augmented Generation. RAG has emerged as promising paradigm for improving LLMs factuality. Early RAG methods rely on single retrieval step (Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023), which is often insufficient for multi-hop QA tasks due to limitations in retrieval quality. To address noisy retrieval (Shi et al., 2023), Self-RAG (Asai et al., 2024) and CRAG (Yan et al., 2024) introduce reflection mechanisms on the retrieved documents. Nevertheless, single retrieval approaches still struggle with multi-hop QA. To address these limitations, iterative retrieval approaches such as Iter-RetGen (Shao et al., 2023) and Self-Ask (Press et al., 2023) progressively retrieve relevant documents to gather sufficient information for final answer. GenGround (Shi et al., 2024b) adopts generate-then-ground method, generating and revising the intermediate sub-answers. Similarly, SearChain (Xu et al., 2024) first generates complete reasoning chain, then verifies the answer of each node in the chain by retrieving information from external knowledge. However, these approaches lack strong reflection mechanism to recover from mistakes made in earlier reasoning steps. Reasoning-enhanced RAG. Recent studies in scaling token generation at test-time (OpenAI, 2024; Muennighoff et al., 2025; Snell et al., 2024) to enhance LLMs reasoning capabilities have spurred interest in reasoning-enhanced RAG. RAG2 Figure 2: Overview of our approach to develop factuality-enhanced reasoning model ReaRAG. To equip ReaRAG with knowledge-guided reasoning ability, we propose an automated data construction approach (Algorithm 1). Next, we fine-tune ReaRAG on the constructed dataset to conduct reasoning iteratively, following the Thought-ActionObservation Paradigm to solve complex queries. Pseudocode for the inference stage is provided in Algorithm 2. Star (Jiang et al., 2024) leverages Monte Carlo Tree Search to iteratively decompose multi-hop questions, guided by reward model during the tree expansion. Search-o1 (Li et al., 2025) introduces document reasoning module, and adopts prompting strategy that enables QwQ-32B to access external knowledge sources by generating specialized search tokens. However, it heavily depends on the base models instruction-following and inherited reasoning capabilities, leading to three key challenges: robustness in generating specialized tokens, failure in extracting information from retrieved documents, and overthinks for multi-hop QA. Meanwhile, CoRAG (Wang et al., 2025) aims to propose an o1-like RAG model via various decoding strategies at test-time but lacks explicit reasoning during inference, thereby limiting its potential to integrate stronger reasoning capabilities."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first formalize the task, then present our novel approach for developing ReaRAG as illustrated in Figure 2."
        },
        {
            "title": "3.1 Task formulation",
            "content": "We focus on the multi-hop QA task, where iterative reasoning improves answer accuracy. Given question x, our goal is to construct knowledgeguided reasoning chain to enhance the factual correctness of the generated answer ˆy. Specifically, the reasoning chain is formulated as sequence of steps, where each step consists of reasoning thought τt, an action αt, and an observation ot: = {(τt, αt, ot)}N t=1, 1 Tmax (1) The number of reasoning steps is dynamically determined by the model but is constrained by an upper limit Tmax to prevent indefinite iterations, i.e., Tmax. To guide the reasoning process with external knowledge, we define the action space as = {search(), finish()} where search takes search query as input, while the input to finish represents the derived answer. At each step t, the model refines search query based on the reasoning thought τt and executes the search action to retrieve relevant information from the RAG engine R. The process continues until the model selects the finish action, at which point the final answer ˆy is derived from all prior reasoning steps. This ensures that the answer is grounded in retrieved knowledge through an iterative and structured reasoning process, thereby enhancing the factual reliability of LRMs. 3 Algorithm 1 Data Construction Input: Seed Dataset Dseed, Large Reasoning Model MLRM , Instruction prompt Pd, Max iterations Tmax, RAG engine Output: Dataset Dreason with reasoning chains 1: Initialize Dreason 2: for each (xi, doci) Dseed do Sample question and gold documents Iteration counter Reasoning chain 0 Ci [] while < Tmax do MLRM ([Pd xi Ci]) Generate response (τt, αt) parse(y t) Extract thought τt and action αt Get action type αttype αt[type] if finish αttype then Append (τt, αt) to Ci break else if search αttype then qs αt[query] ot R(qs, doci) Get ot from RAG engine Append (τt, αt, ot) to Ci end if + 1 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end while Append Ci to Dreason 19: 20: end for 21: return Dreason"
        },
        {
            "title": "3.2 Knowledge-guided reasoning chain",
            "content": "generation While existing LRMs demonstrate strong reasoning capabilities, they often fail to ground their reasoning process in factual knowledge. To make external knowledge accessible, we design structured reasoning step where each step consists of reasoning thought τt, an action αt, and an observation ot. Reasoning thought τt: Represents the models thought process where it reflects on prior actions and observations before deciding an action and its input parameter. Action αt: JSON dictionary containing an action sampled from the action space along with the corresponding input parameter. Observation ot: Feedback received after executing action αt, guiding subsequent reasoning."
        },
        {
            "title": "To equip ReaRAG with the ability to construct",
            "content": "4 reasoning chain guided by external knowledge, we propose an automated data construction approach, as detailed in Algorithm 1. Data construction. Given multi-hop question xi sampled from the seed dataset, we prompt the LRM MLRM with instruction prompt Pd (see Appendix B) to collect the reasoning thoughts and actions. Next, the search query is extracted and executed against RAG Engine to obtain an observation ot. The process iterates until either model decides an finish action or the iteration count exceeds the maximum iterations Tmax. Data filtering. Previous work studies have shown that the performance of the LLMs heavily depends on the quality of fine-tuning data (Gunasekar et al., 2023). To ensure high-quality reasoning chains, we apply data filtering by comparing the final answer ˆyi derived from the reasoning chain Ci against the ground truth answer yi using the F1 metric. Reasoning chains with an F1 score of 0 are discarded to maintain data integrity."
        },
        {
            "title": "3.3 Factuality-enhanced LRMs: ReaRAG",
            "content": "Fine-tuning. To incorporate knowledge-guided reasoning ability into the model, we perform supervised fine-tuning (SFT) on the constructed dataset discussed in the previous section, where each sample is sequence of conversation chain = {P, xi, {(τt, αt, ot)}N t=1}, where is an instruction prompt (see Appendix B). We fine-tune our factuality-enhanced LRM, ReaRAG (MReaRAG) using loss function below: = (cid:88) 1 log MReaRAG(sj s<j) (2) where sj represents the textual tokens of the input sequence S, 1() is loss mask indicator, which is set to True on thought τt and action αt tokens, ensuring that the loss is only computed over the tokens contributed to the thought reasoning as well as action, rather than the entire sequence. Inference. After fine-tuning, our model ReaRAG is equipped with advanced reasoning to solve multihop QA. Given an instruction prompt and question x, the model first generates reasoning thought τ0 and an initial action α0, typically search action. The search query is extracted and executed over the RAG engine R, which returns an observation o0. This process iterates meanwhile collecting thought τi, action αi and observation oi. Algorithm 2 Inference Input: Input question x, documents doc, ReaRAG MReaRAG, Answer LLM MAns, Instruction prompt P, Answer Prompt Pans, Max iterations Tmax, RAG engine Output: Final answer, ˆy Initialization 1: 0, [] 2: while < Tmax do 3: MReaRAG([P C]) Generate response for iteration (τt, αt) parse(y t) Extract thought τt and action αt Get action type αttype αt[type] if finish αttype then Append (τt, αt) to yref αt[answer] ˆy MAns([Pans yref ]) return final answer ˆy else if search αttype then qs αt[query] ot R(qs, doci) Get ot from RAG engine Append (τt, αt, ot) to 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end if + 1 16: 17: end while Ultimately, ReaRAG decides on finish action at step . We further extract the final answer from the finish action, denoted as yref , and generate concise answer by prompting an answer model MAns to generate final answer ˆy using the prompt Pans in Appendix B. The pseudocode for the inference stage is provided in Algorithm 2."
        },
        {
            "title": "4.1 Experimental setup",
            "content": "Dataset and metrics. To validate the effectiveness of our approach, we conduct experiments on multi-hop reasoning tasks that require multiple supporting documents per question, including MuSiQue (MQ) (Trivedi et al., 2022), HotpotQA (HoPo) (Yang et al., 2018) and IIRC (Ferguson et al., 2020). Additionally, to ensure the model retains its single-hop reasoning ability, we evaluate it on NQ (Kwiatkowski et al., 2019). Since these datasets require open-ended answers, traditional metric such as exact match (EM) may fail to account for variations in semantic equivalence (Yin et al., 2024; Xu et al., 2023). Hence, we utilize the 5 LLM-as-judge metric (ACCL) (Zheng et al., 2023) with GPT-4o for more accurate evaluation. We randomly sample 100 samples from the validation sets of MuSiQue, HotpotQA, and NQ, while selecting 200 samples from IIRC for evaluation. Baselines. We compare our approach against multiple baselines, categorized based on their access to external knowledge. These include incontext retrieval, where the corpus is directly appended to the language models context; vanilla RAG, which performs single retrieval based on the original multi-hop question; and state-of-the-art advanced RAG methods proposed recently. For the in-context and vanilla RAG baselines, we utilize long-context models, specifically GLM4-9B(128k) and GLM-4-32B (128k) (Zeng et al., 2024) with context length 128k, and QwQ-32B (Team, 2024) with context length 32k as backbone. For advanced RAG baselines, we consider SelfRAG (Asai et al., 2024), which fine-tunes Llama27B to retrieve on demand and assess document relevance for filtering noisy information. In contrast, SearChain (Xu et al., 2024) constructs Chain-ofQuery (CoQ) to tackle multi-hop questions, where each node in the chain consists of sub-query to the original question and its corresponding subanswer, facilitating multi-turn retrieval with verification against retrieved documents. Additionally, Search-o1 (Li et al., 2025) designs framework for LRMs to perform iterative knowledge retrieval, representing reasoning-enhanced RAG method. 4."
        },
        {
            "title": "Implementations details",
            "content": "RAG engine. Our RAG engine consists of two main components: retrieval and generation. For retrieval, we utilize the embedding model embedding3 from Zhipus API3, along with reranker based on the GLM3 architecture to enhance the retrieval quality. For generation, we employ GLM-4-32B with context length of 128k to generate responses based on the retrieved documents. Data construction and fine-tuning. The seed dataset described in Algorithm 1 is derived from the training sets of MuSiQue, HotpotQA, and NQ, with QwQ-32B as the LRM. To ensure models general capabilities, we fine-tune GLM-4-9B (Zeng et al., 2024) with the constructed dataset (roughly 20k filtered samples), as well as the general SFT dataset from GLM-4 (Zeng et al., 2024). 3https://bigmodel.cn/dev/api/vector/embedding"
        },
        {
            "title": "MuSiQue",
            "content": "Multi-hop HotpotQA"
        },
        {
            "title": "IIRC",
            "content": "Single-hop NQ In-context"
        },
        {
            "title": "Ours",
            "content": "GLM-4-9B(128k) GLM-4-32B(128k) GLM-4-9B(128k) GLM-4-32B(128k) QwQ-32B Self-RAG(Llama2-7b) SearChain(GPT-4o) Search-o1(QwQ-32B) ReaRAG-9B ACCL 23.50 33.50 25.50 29.00 36.00 24.00 51.50 40.50 66.00 EM ACCL 58.00 15.00 65.50 17.00 68.00 14.00 67.50 17.00 67.00 20.00 45.50 13.00 69.00 33.00 55.50 32.00 75.50 40.00 EM ACCL 20.50 47.00 25.00 50.00 28.25 52.00 28.25 52.00 38.25 47.00 25.00 31.00 40.50 49.00 32.25 38.00 42.75 56.00 EM ACCL 45.50 18.00 52.50 16.00 49.00 23.00 53.00 17.00 32.00 48.00 40.00 13.00 54.00 20.50 43.00 27.00 52.00 29.00 EM 26.00 24.00 32.00 39.00 26.00 28.00 25.00 28.00 25. Table 1: Main experimental results compared to baselines on four benchmarks. Bold and underline indicates the best and second best results. We employ the traditional EM metric and the LLM-as-judge framework with GPT-4o to evaluate predictions for each baseline, denoted as ACCL. Our model, ReaRAG-9B, achieves significant improvements across all baselines, except for the single-hop NQ benchmark, highlighting that strong reasoning is particularly beneficial for multi-hop QA. Evaluation. For the MuSiQue, HotpotQA, and IIRC benchmarks, we use the original corpus provided by the respective authors. For the NQ dataset, we use the corpus following the setting of Lost in the middle (Liu et al., 2024). To further increase the challenge, particularly for comparisons with long-context models, we scale up the number of distractor documents, resulting in corpus with token length ranging between 48k-58k. Specifically, to construct the corpus, we use the original gold and distractor documents and further expand it by randomly selecting additional distractor documents from other questions, or gold documents when distractors are not available. In addition to increasing the difficulty for long-context models, this approach demands high quality queries for other baseline approaches to the RAG engine to ensure the retrieval of relevant supporting documents. For fair comparison, we run the open-source implementations of all baselines in the advanced RAG category, evaluating them using our own RAG engine and corpus."
        },
        {
            "title": "4.3 Main results",
            "content": "Table 1 presents our main results across four benchmarks, showing that our approach outperforms all baselines except for the single-hop NQ benchmark. On this benchmark, ReaRAG performs comparably to SearChain (52.00 vs 54.00) and Vanilla RAG with the GLM-4-32B backbone (52.00 vs 53.00) in terms of the ACCL metric. This result is mainly attributed to two factors: SearChain uses GPT-4o as its backbone, while another baseline, Vanilla RAG, uses the GLM-4-32B backbone, benefiting from its significantly larger scale compared to our 9B model. Additionally, ReaRAGs strong reasoning capabilities offer little advantage in single-hop settings, limiting its ability to surpass other baselines. However, when considering the EM metric, the gap between ReaRAG and Vanilla RAG with the GLM-4-32B backbone is much larger (25.00 vs. 39.00), though ReaRAG remains comparable to SearChain in terms of the ACCL metric. This discrepancy suggests that EM may fail to capture contextually valid answers generated by LLMs. Comparing Vanilla RAG with in-context settings across different GLM-4 backbone scales, we find that the Vanilla RAG setting performs better, suggesting that long-context models struggle with distractor-heavy corpora. The only exception is MuSiQue with the GLM-4-32B backbone, where the long-context model slightly outperforms Vanilla RAG (33.50 vs. 29.00). Additionally, under the Vanilla RAG setting, QwQ-32B significantly outperforms GLM-4-32B on MuSiQue and IIRC, both of which feature particularly challenging multi-hop questions. This highlights the advantage of LRMs with strong reasoning capabilities. Self-RAG, which aims to improve retrieval quality, lacks multi-turn retrieval strategies, an aspect that is crucial for multi-hop QA, performs poorly across all benchmarks. SearChain with GPT-4o performs competitively, achieving the second-best results on MuSiQue, HotpotQA, and IIRC, and the best results on the NQ dataset. This demonstrates the effectiveness of its proposed CoQ strategy and 6 verification mechanism. Despite leveraging the strong reasoning abilities of LRMs such as QwQ32B, Search-o1 performs significantly worse than SearChain on multi-hop benchmarks, while consuming more computational resources. We further analyze these findings in Section 4.5. Notably, our proposed method, ReaRAG-9B, significantly outperforms all baselines across three of the four benchmarks, showing the effectiveness of knowledge-guided reasoning chain generation. Compared to the best-performing baseline, SearChain, ReaRAG-9B achieves significant 14.5% improvement on ACCL metric and 7% gain on EM metric for MuSiQue, along with 6.5% ACCL and 7% EM gain on HotpotQA, as well as 2.25% ACCL and 8.5% gain on EM for IIRC benchmark. These results suggest that ReaRAG exhibits strong multi-hop reasoning abilities, even when using smaller-scale model."
        },
        {
            "title": "4.4 Ablation",
            "content": "Closed-book performance. We conduct closed-book experiment to evaluate the parametric knowledge of the language models. The results, presented in Table 3 show that QwQ-32B outperforms GLM-4 on benchmarks requiring strong reasoning, such as MuSiQue and IIRC. This highlights the advantage of strong reasoning for complex tasks. Nevertheless, their parametric knowledge remains insufficient compared to results in Table 1. Advantage of strong reasoning. To evaluate the impact of strong reasoning capabilities, we finetune model that lacks such abilities while adhering to the same Thought-Action-Observation reasoning paradigm. This variant, denoted as w/o reasoning in Table 4, shares the same backbone architecture as ReaRAG-9B and follows the data construction process outlined in Algorithm 3. However, instead of leveraging strong reasoning model like QwQ-32B for data generation, we employ GLM-4-9B, which lacks robust reasoning abilities. Unlike the previous data construction approach in Algorithm 1, which used only multihop questions as input, we now provide GLM-4-9B with additional information, including ground-truth decompositions and ground-truth answers. The instruction prompt Pablat used to generate its reasoning chain is detailed in Appendix B. Table 4 shows that ReaRAG-9B with enhanced reasoning capabilities (w/ reasoning) consistently outperforms its counterpart without reasoning, achieving notable gain of 6-11% ACCL gain on the multi-hop benchmarks and 7% gain on singlehop NQ. However, the improvement in EM on NQ is smaller, and on MuSiQue, EM decreases by 3% despite 6% increase in ACCL. We attribute this to EMs limitations in capturing the variability of answers generated by the language model."
        },
        {
            "title": "4.5 Analysis",
            "content": "Multi-hop MQ HoPo IIRC Invalid rate (%) 19.00 28.00 23.00 Single-hop NQ 25.00 Table 2: QwQ-32B, leading to retrieval failures in Search-o1."
        },
        {
            "title": "4.5.1 Performance against strong baseline",
            "content": "We conduct an in-depth analysis and compare our approach against the strong baseline Search-o1 (Li et al., 2025), which relies solely on prompting and thus requires QwQ-32B to have strong instructionfollowing capabilities. Below, we highlight key limitations affecting its benchmark performance. Token generation failure. QwQ-32B struggles to follow the instruction prompt, failing to generate special tokens (e.g., begin_search_query) essential for Search-o1 to retrieve external knowledge. This limitation forces Search-o1 into closed-book setting, significantly impairing its performance. Table 2 quantifies this issue, revealing invalid generation rates of 1928%. Information extraction failure. Search-o1 introduces the Reason-in-Documents module, leveraging QwQ-32B for in-depth reasoning over retrieved documents to generate responses as search results. However, this module has key limitation: it may incorrectly conclude that no useful information is available  (Table 5)  . Our analysis identifies the primary cause: the module attempts to answer the original multi-hop question based on the search query, but the retrieved information is insufficient. For example, as shown in Table 5, the module searches for information related to Hannibal and Scipio book at first search, but the retrieved content only includes the books author, lacking information about the place of education. This flaw weakens Search-o1, as it continuously searches for non-existent information, causing reasoning paths to diverge and ultimately hitting iteration limits. 7 Figure 3: Comparison of chain length between ReaRAG and Search-o1 across multi-hop QA tasks. We measure the reasoning steps needed for both models to achieve full ACCL score. Search-o1 consistently requires more steps than ReaRAG, highlighting the tendency of RL-based models to overthink in multi-hop QA tasks. Hallucination in Reason-in-Documents module. The Reason-in-Documents module is prone to hallucination Table 6. For instance, when searching for the members of Bruce Lee Band, the module fails to find relevant information and fabricates Less Than Records based on parametric knowledge rather than the provided corpus. This hallucination propagates through subsequent reasoning steps, degrading the final answer quality. Overthinking in multi-hop QA. Recent studies have identified overthinking in LRMs (Chen et al., 2024; Team et al., 2025), where RL-based models generate excessively long reasoning chains, leading to redundancy in multi-hop QA. To examine this, we compare ReaRAG with Search-o1, which uses QwQ-32B as backbone. Specifically, we analyze the number of reasoning steps required to achieve full ACCL score on multi-hop QA. Figure 3 shows that ReaRAG consistently requires fewer steps across the benchmarks, demonstrating efficiency in multi-hop reasoning while mitigating overthinking. To further illustrate this issue, case studies in Tables 7 and 8 compare the outputs of Search-o1 and ReaRAG. The results show that Search-o1 performs unnecessary redundant searches even when the correct answer has already been derived in earlier steps, highlighting the excessive search steps in multi-hop QA tasks."
        },
        {
            "title": "4.5.2 Strength of ReaRAG",
            "content": "This section showcases ReaRAGs advanced reasoning capabilities. Table 9 shows that ReaRAG initially mistakenly identified Anne of Austria as the grandmother of Philippe rather than his mother. However, ReaRAG later detected this mistake, verified the information, and corrected it. This self-correction mechanism helps prevent errors from propagating to later reasoning steps. Table 10 demonstrates how ReaRAG resolves ambiguity in multi-hop question through multiple searches. The question is about The Hard Easy, which is both film and TV series. At the sixth reasoning step, ReaRAG also encounters knowledge conflicts. Despite these challenges, it successfully clarifies the information and arrives at the correct final answer. Table 11 provides another example of ReaRAG handling ambiguity in multi-hop question while resolving knowledge conflict. Its parametric knowledge incorrectly states that Sonic is voiced by Roger Craig Smith instead of Jim Cummings. ReaRAG detects and corrects this inconsistency, ultimately reaching the correct answer. This case further highlights its robust reasoning abilities. These examples highlight ReaRAGs ability to iteratively perform knowledge-guided reasoning. Compared to existing baselines, our approach better integrates reasoning model with external knowledge, enhancing factual accuracy."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we introduce ReaRAG, factualityenhanced reasoning model capable of performing knowledge-guided reasoning. ReaRAG plans reasoning steps iteratively and reflects on prior steps, leveraging external knowledge to ensure the correctness of the reasoning chain. Notably, ReaRAG achieves significant improvements compared to existing baselines. Further analysis highlights its robust reasoning capabilities, effectively tackling complex multi-hop questions while mitigating the issue of overthinking in RL-based LRMs."
        },
        {
            "title": "Limitations",
            "content": "Limited action space While ReaRAG demonstrates strong performance in the QA task, its action space is currently limited to only search and finish in this study. Consequently, it is restricted to processing local knowledge sources and cannot perform actions such as leveraging code compiler for coding tasks, executing mathematical calculations, or conducting real-time web searches. Expanding its action space could enhance its adaptability across diverse problem domains. Data construction efficiency To equip ReaRAG with structured reasoning process, we fine-tune ReaRAG using structured responses generated by the LRM. However, this approach relies on the LRMs strong instruction-following ability, and substantial portion of the data is discarded due to validity issues, leading to computational inefficiency and resource waste. Improving data augmentation techniques could mitigate this limitation. Inference latency ReaRAG solves questions iteratively, requiring multiple reasoning steps to reach the final answer. While this enhances accuracy, it also increases inference time compared to models that generate answers in single pass. This trade-off between reasoning depth and efficiency may limit its practicality in real-time applications or scenarios with strict latency constraints."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by National Natural Science Foundation of China (62476150), Beijing Natural Science Foundation (L243006), Tsinghua University Initiative Scientific Research Program and Tsinghua University (Department of Computer Science and Technology)-Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things (JCIIOT)."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 22062240. PMLR. Shulin Cao, Jiajie Zhang, Jiaxin Shi, Xin Lv, Zijun Yao, Qi Tian, Lei Hou, and Juanzi Li. 2023. Probabilistic tree-of-thought reasoning for answering knowledgeintensive complex questions. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 12541 12560. Association for Computational Linguistics. Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: learning to refine queries for retrieval augmented generation. CoRR, abs/2404.00610. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2024. Do NOT think that much for 2+3=? on the overthinking of o1-like llms. CoRR, abs/2412.21187. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2024. Chain-of-verification reduces hallucination in large language models. In Findings of 9 the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 35633578. Association for Computational Linguistics. James Ferguson, Matt Gardner, Hannaneh Hajishirzi, Tushar Khot, and Pradeep Dasigi. 2020. IIRC: dataset of incomplete information reading comprehension questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 11371147. Association for Computational Linguistics. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks are all you need. CoRR, abs/2306.11644. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 39293938. PMLR. Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24:251:1251:43. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. 2024. Openai o1 system card. CoRR, abs/2412.16720. Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. 2024. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. CoRR, abs/2412.12881. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. 2024. Training language models to selfcorrect via reinforcement learning. arXiv preprint arXiv:2409.12917. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452 466. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Trans. Assoc. Comput. Linguistics, 12:157173. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. OpenAI. 2024. Learning to reason with llms. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 56875711. Association for Computational Linguistics. 10 Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 92489274. Association for Computational Linguistics. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 3121031227. PMLR. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024a. REPLUG: retrievalaugmented black-box language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 83718384. Association for Computational Linguistics. Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. 2024b. Generate-then-ground in retrieval-augmented generation for multi-hop question answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 1116, 2024, pages 73397353. Association for Computational Linguistics. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. CoRR, abs/2408.03314. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. 2025. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599. Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539554. Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. 2025. Chainof-retrieval augmented generation. arXiv preprint arXiv:2501.14342. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. 2023. Rewoo: Decoupling reasoning from observations for efficient augmented language models. CoRR, abs/2305.18323. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. 2025. Towards large reasoning models: survey of reinforced reasoning with large language models. CoRR, abs/2501.09686. Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. 2024. Search-in-the-chain: Interactively enhancing large language models with search for knowledge-intensive tasks. In Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024, pages 13621373. ACM. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. CoRR, abs/2401.15884. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics. 11 Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Raghavi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2024. Agent lumos: Unified and modular training for open-source language agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1238012403. Association for Computational Linguistics. Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024. Chatglm: family of large language models from GLM-130B to GLM-4 all tools. CoRR, abs/2406.12793. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023."
        },
        {
            "title": "MuSiQue",
            "content": "Multi-hop HotpotQA"
        },
        {
            "title": "IIRC",
            "content": "Single-hop NQ GLM-4-9B(128k) GLM-4-32B(128k) QwQ-32B ACCL EM ACCL 29.50 0.00 3.50 40.00 1.00 6.50 2.00 11.00 35.00 EM ACCL 17.00 23.00 28.00 17.00 21.25 10.00 EM ACCL 15.00 27.50 44.50 11.50 37.50 14.50 EM 16.00 25.00 12. Table 3: Closed-book performance of language models on multi-hop and single-hop benchmarks. These models perform better on single-hop benchmarks but score significantly lower on multi-hop benchmarks, highlighting the limitations of relying solely on parametric knowledge for these benchmarks."
        },
        {
            "title": "MuSiQue",
            "content": "Multi-hop HotpotQA"
        },
        {
            "title": "IIRC",
            "content": "Single-hop NQ"
        },
        {
            "title": "EM ACCL",
            "content": "EM ReaRAG-9B - w/o reasoning - w/ reasoning 60.00 66.00 43.00 40.00 68.00 75. 50.00 56.00 31.00 42.75 21.50 29.00 45.00 52.00 24.00 25.00 Table 4: Performance comparison of models with and without strong reasoning capabilities. w/ reasoning consistently outperforms w/o reasoning across all benchmarks, demonstrating the effectiveness of our fine-tuning process, which enables ReaRAG-9B to inherit the strong reasoning abilities of LRM. Algorithm 3 Data Construction to fine-tune ReaRAG w/o reasoning Input: Seed Dataset Dseed, Large Language Model MLLM , Instruction Prompt Pablat Output: Dataset Dablat 1: Initialize Dablat 2: for each (xi, yi, decompi) Dseed do 3: 4: Sample question xi, ground truth answer yi and golden decomposition decompi MLLM ([Pablat xi yi decompi]) Generate response Ci = [{τt, αt, ot}N t=1] parse(r i) Parse list of thought τt, action αt and observation ot into reasoning chain Ci"
        },
        {
            "title": "Append Ci to Dablat",
            "content": "5: 6: end for 7: return Dablat"
        },
        {
            "title": "B Prompts",
            "content": "Instruction prompts Pd for data construction to fine-tune ReaRAG Your task is to solve question answering task. To improve your solving accuracy, please conduct reasoning processes following this sequence: Thought, Action, Observation steps. Thought can reason about the current situation, and Action is in the form of function. There are two available function types: Available Functions: (1) Search { \"name\": \"search\", \"description\": \"It can help you find useful information through the internet or local knowledge base. You can use this tool to access external knowledge.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"query\": { \"description\": \"what you want to search\" } }, \"required\": [\"query\"] } } (2) Finish { \"name\": \"finish\", \"description\": \"You can use this function to make conclusion from the reasoning process and give the final answer. The reasoning process is completed after this finish function is called\", \"parameters\": { \"type\": \"object\", \"properties\": { \"answer\": { \"description\": \"the final answer\" } }, \"required\": [\"answer\"] } } Some important rules you must follow: (1) Please follow the function calling format above strictly. (2) set of Thought, Action, and Observation is considered as one reasoning step. Add numbering after each Thought, Action, and Observation to indicate the sequence of the reasoning steps. (3) Please give your Thought first, then the Action, and finally the Observation, follow the format as shown in the in-context examples below. (4) In your Thought, you should perform reflection when necessary to ensure the correctness of your reasoning process, such as: \"Wait! Maybe made some mistakes! need to rethink from scratch\", \"Alternatively, we can...\", \"Hold on, lets try another approach\", etc. (5) Give your Action in the form of function call, as shown in in-context examples below. (6) You should not provide information based on your own knowledge, only use the information provided in the context. Some example of reflection text: \"There is no enough information from the previous steps. need to plan my query again.\" \"No context found from observation. Let me restart the reasoning process.\" \"Missing information. Let me restructure my query.\" \"Wait! Maybe made some mistakes! need to rethink from scratch.\" \"I think need to take step back and reconsider my approach.\" \"I need to reevaluate my reasoning process. Lets start over.\" \"I need to reflect on my reasoning. Lets try different approach.\" {More examples of reflection text. Simplfied for readability} In-Context Example: {Some in-context examples} 14 Instruction prompt for fine-tuning and inference with ReaRAG Your task is to solve question answering task. To improve your solving accuracy, please conduct reasoning process interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action are in the form of function, there are two types: Available Functions: (1) Search { \"name\": \"search\", \"description\": \"It can help you find useful information through the internet or local knowledge base. You can use this tool to access external knowledge.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"query\": { \"description\": \"what you want to search\" } }, \"required\": [\"query\"] } } (2) Finish { \"name\": \"finish\", \"description\": \"You can use this function to make conclusion from the reasoning process and give the final answer. The reasoning process is completed after this finish function is called\", \"parameters\": { \"type\": \"object\", \"properties\": { \"answer\": { \"description\": \"the final answer\" } }, \"required\": [\"answer\"] } } Please follow the format strictly."
        },
        {
            "title": "Answer prompt Pans to derive final answer",
            "content": "The Reference Answer is the final answer to the question. Its the final deterministic answer, your task is to give concise version of it. Only give me the short answer and do not output any other words. [Question] {question} [Reference answer] {reference_ans} Only give me the short answer and do not output any other words. For yes, or no answer, only answer it short. Give the shortest answer possible. 15 Instruction prompts Pablat for data construction to fine-tune ReaRAG w/o reasoning You are given Question, Ground-truth answer, and Decompositions, your task is to give reasoning process interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action are in the form of function, there are two types: (1) search { \"name\": \"search\", \"description\": \"It can help you find useful information through the internet or local knowledge base. You can use this tool to access external knowledge.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"query\": { \"description\": \"what you want to search\" } }, \"required\": [\"query\"] } } (2) finish { \"name\": \"finish\", \"description\": \"You can use this function to make conclusion from the reasoning process and give the final answer. The reasoning process is completed after this finish function is called\", \"parameters\": { \"type\": \"object\", \"properties\": { \"answer\": { \"description\": \"the final answer\" } }, \"required\": [\"answer\"] } } After finish function being called, the reasoning process is completed, then marked with <completed>. Please follow the format strictly. Here are in-context examples: Question: What was the wettest year in the second largest city in the state where Yumas Library District is located? Ground-truth answer: 1905 Decompositions: [ ] {\"question\": \"Yuma located in the administrative territorial entity\", \"answer\": \"Yuma County\", \"context\": \"Some context\" # Simplified for readability}, ... # Simplified for readability {\"question\": \"Tucsons wettest year?\", \"answer\": \"1905\", \"context\": \"Some context\" # Simplified for readability} Reasoning process with function call: {Reasoning process example}"
        },
        {
            "title": "C Case study",
            "content": "# Case Study 1: Where was the author of Hannibal and Scipio educated at? Ground truth: Exeter College Search-o1 Alright, Ive got this question: ... Simplified here. So, Ill do search for \"Hannibal and Scipio book.\" <begin_search_query>Hannibal and Scipio book<end_search_query> <begin_search_result>No helpful information found.<end_search_result> Hmm, that didnt turn up anything. Maybe the book has different title. Let me think differently. ... Simplified here. Search-o1 reasoning diverges and hence unable to find related information. Only search queries and results are listed below for simplicity. <begin_search_query>Punic Wars book<end_search_query> <begin_search_result>No helpful information found.<end_search_result> <begin_search_query>Hannibal vs. Scipio book<end_search_query> <begin_search_result>No helpful information found.<end_search_result> <begin_search_query>Adrian Goldsworthy books<end_search_query> <begin_search_result>No helpful information found.**Thoughts**After carefully reviewing the three web pages provided, it is evident that none of them ... Simplified here. <end_search_result> <begin_search_query>Robert F. Pennell education<end_search_query> <begin_search_result>No helpful information found.<end_search_result> ... Simplified here. Search-o1 tries many unrelated search queries, demonstrating divergent reasoning. Ive reached the maximum number of searches. Based on the information have, need to make an educated guess. ... Simplified here. Maximum searches reaches Search-o1 tries to make guess when there is not enough information to answer the multi-hop question. Considering that Oxford and Cambridge are traditional centers for classical studies in the UK, Ill go with Oxford. Therefore, my answer is: Oxford ReaRAG-9B Thought 1: So need to find out where the author of \"Hannibal\" and \"Scipio\" was educated. First, need to know who the author is. Action 1: {'function': 'search', 'parameters': {'query': 'Who is the author of Hannibal and Scipio?'}} Observation 1: The author of \"Hannibal and Scipio\" is Thomas Nabbes. Thought 2: Okay, now that know the author is Thomas Nabbes, need to find out where he was educated. Action 2: {'function': 'search', 'parameters': {'query': 'Where was Thomas Nabbes educated?'}} Observation 2: Thomas Nabbes was educated at Exeter College, Oxford. Thought 3: Great, so Thomas Nabbes studied at Exeter College in Oxford. should make sure that this the correct information. Action 3: {'function': 'search', 'parameters': {'query': 'Confirm the educational background of Thomas Nabbes'}} Observation 3: Thomas Nabbes was educated at Exeter College, Oxford in 1621. He left the university without taking degree. Thought 4: So, he studied at Exeter College in Oxford, but didnt finish his degree. Thats interesting. think this is the information need. Action 4: {'function': 'finish', 'parameters': {'answer': 'Thomas Nabbes was educated at Exeter College, Oxford.'}} Final Answer: Exeter College, Oxford Table 5: We compare the generations of Search-o1 and ReaRAG. During the initial searches, the Reason-inDocuments module of Search-o1 returns No helpful information found because it attempts to answer the multi-hop question using information retrieved based on the search query rather than the original question. Hence, the retrieved content lacks the full answer, misleading Search-o1s reasoning. Ultimately, it reaches its search limits and fails to provide the correct answer. In contrast, ReaRAG demonstrates logical reasoning and correctly answers the query. 17 # Case Study 2: What record label did the person who is part of The Bruce Lee Band start? Ground truth: Asian Man Records Search-o1 Alright, so need to find out what record label the person who is part of The Bruce Lee Band ... Simplified here. So, Ill do search for \"The Bruce Lee Band.\" <begin_search_query>The Bruce Lee Band<end_search_query> <begin_search_result>No helpful information found.<end_search_result> Hmm, thats strange. Maybe the band name is different or not well-known. Perhaps should try searching for \"Bruce Lee tribute band\" or something like that. ... Simplified here. Search-o1 tries different searches, but Reason-in-Documents returns No helpful information Only search queries and search results are listed here for simplicity <begin_search_query>Bruce Lee tribute band<end_search_query> <begin_search_result>No helpful information found.<end_search_result> <begin_search_query>Bruce Lee band<end_search_query> <begin_search_result>No helpful information found.<end_search_result> ... Simplified here. <begin_search_query>The Bruce Lee Band members<end_search_query> <begin_search_result>Less Than Records, started by member of Less Than Jake, who is part of The Bruce Lee Band.**Explanation**Based on the information from Web Page 1, The Bruce Lee Band features Less Than Jake as part of its backing band. Less Than Jake is known for starting their own record label, Less Than Records. ... Simplified here. Search-o1 fabricates the name \"Less Than Records\" <begin_search_query>Less Than Jake record label<end_search_query> <begin_search_result>No helpful information found.**Explanation**After reviewing the three web pages provided for the search query \"Less Than Jake record label,\" none of them contained specific information about which member of Less Than Jake started Less Than Records. ... Simplified here. <end_search_result> ... Simplified here. Less Than Records was started by member of Less Than Jake, who is also part of The Bruce Lee Band.Therefore, the record label started by the person who is part of The Bruce Lee Band is Less Than Records. Final Answer Less Than Records ReaRAG-9B Thought 1: need to find out which record label the person who is part of The Bruce Lee Band started. First, should figure out who is part of The Bruce Lee Band. Action 1: {'function': 'search', 'parameters': {'query': 'Who is part of The Bruce Lee Band?'}} Observation 1: Mike Park and his backing band, which has included Less Than Jake and the Rx Bandits, are part of The Bruce Lee Band. Thought 2: So, Mike Park is involved with The Bruce Lee Band. need to find out if he started record label. Action 2: {'function': 'search', 'parameters': {'query': 'Did Mike Park start record label?'}} Observation 2: Yes, Mike Park started record label. Thought 3: Great, now need to find out the name of the record label he started. Action 3: {'function': 'search', 'parameters': {'query': 'What record label did Mike Park start?'}} Observation 3: Mike Park started the record label Asian Man Records. Thought 4: So, Asian Man Records is the label Mike Park started. But should make sure that this is correct and see if theres any other relevant information. Action 4: {'function': 'search', 'parameters': {'query': 'Confirm if Asian Man Records was started by Mike Park.'}} Observation 4: Yes, Asian Man Records was started by Mike Park. Thought 5: Alright, have confirmed that Mike Park started Asian Man Records. Since he is part of The Bruce Lee Band, this should be the answer. Action 5: {'function': 'finish', 'parameters': {'answer': 'Asian Man Records'}} Final Answer: Asian Man Records Table 6: We compare the generations of Search-o1 and ReaRAG. Search-o1s Reason-in-Documents module fabricates non-existent information (\"Less Than Records\", highlighted in red), compromising factual accuracy and leading to hallucinated response. In contrast, ReaRAG successfully conduct knowledge-guided reasoning to generate factual answer. 18 # Case Study 3: In what county is William W. Blairs birthplace located? Ground truth: Orleans County Search-o1 ... Simplified here. Maybe can search for \"Holley, New York county\" to find out which county its part of. <begin_search_query>Holley, New York county<end_search_query> <begin_search_result>Holley is located in Orleans County, New York.<end_search_result> ... Simplified here. But wait, let me double-check to make sure that Holley is indeed in Orleans County. Sometimes place names can be ambiguous, and there might be multiple Holleys in different counties or states. Let me search for \"Holley, New York county\" again to confirm. <begin_search_query>Holley, New York county<end_search_query> <begin_search_result>You have searched this query. Please refer to previous results.<end_search_result> Alright, since the previous search already indicated that Holley is in Orleans County, New York, and no other information contradicts that, can confidently say that William W. Blair was born in Orleans County. However, to be thorough, perhaps should verify if there are any other places named Holley in New York or elsewhere that might be confused with the one where William W. Blair was born. Let me search for \"Holley, New York\" to see if theres only one such place. <begin_search_query>Holley, New York<end_search_query> <begin_search_result>Holley is village ... Simplified here. <end_search_result> But just to cover all bases, maybe theres another Holley in New York state that should consider. Let me search for \"New York state places named Holley\" to see if there are multiple locations with that name. <begin_search_query>New York state places named Holley<end_search_query> <begin_search_result>Holley is village ... Simplified here. <end_search_result> Search-o1 continues to execute excessive and unnecessary searches. Only search queries are listed here for simplicity. <begin_search_query>William W. Blair historian<end_search_query> <begin_search_query>William W. Blair Latter Day Saints<end_search_query> <begin_search_query>William W. Blair birth certificate Orleans County<end_search_query> <begin_search_query>William W. Blair biography<end_search_query> Perfect, so this biography confirms that he was born in Holley, Orleans County, New York. This aligns with all the previous information Ive found. Therefore, can confidently say that William W. Blairs birthplace is located in Orleans County. ... Simplified here. Final Answer Orleans County ReaRAG-9B Thought 1: need to find out in which county William W. Blair was born. First, should figure out where he was born, meaning his birthplace city or town. Action 1: {'function': 'search', 'parameters': {'query': 'Where was William W. Blair born? '}} Observation 1: William W. Blair was born in Holley, New York. Thought 2: Okay, so now know that his birthplace is Holley, New York. Next, need to determine which county Holley is located in. Action 2: {'function': 'search', 'parameters': {'query': 'Which county is Holley, New York in?'}} Observation 2: Holley, New York is in Orleans County. Thought 3: So, if Holley is in Orleans County, then that should be the answer. But should double-check if theres any ambiguity or if there are multiple places named Holley in New York. Action 3: {'function': 'search', 'parameters': {'query': 'Are there multiple Holleys in New York State?'}} Observation 3: Based on the provided context, there is only one Holley mentioned, which is in Orleans County, New York. The context does not provide information about other Holleys in New York State. Therefore, based on the given context, cannot confirm whether there are multiple Holleys in New York State. Thought 4: Given that, and considering the information have, think its safe to conclude that William W. Blairs birthplace, Holley, is in Orleans County, New York. Action 4: {'function': 'finish', 'parameters': {'answer': 'Orleans County'}} Final Answer: Orleans County Table 7: We compare the generations of Search-o1 and ReaRAG. Words highlighted in red indicate instances where Search-o1 overthinks and performs excessive verification. Words highlighted in green show that the correct answer was already found in earlier steps. ReaRAG finds the correct answer in just two searches, whereas Search-o1 requires eleven, generating overly redundant queries that introduce unnecessary computations to multi-hop QA. # Case Study 4: Where is Ulrich Walters employer headquartered? Ground truth: Cologne Search-o1 Alright, have this question: \"Where is Ulrich Walters employer ... Simplified here. <begin_search_query>Who is Ulrich Walter?<end_search_query> <begin_search_result>Ulrich Walter is German astronaut who was selected in 1987 to join the German astronaut team and trained at the German Aerospace Center (DLR). His employer, the German Aerospace Center, is headquartered in Cologne, Germany.<end_search_result> Hmm, according to this search result, Ulrich Walter is German astronaut who was selected in 1987 and trained at the German Aerospace Center (DLR). It also says that his employer, the DLR, is headquartered in Cologne, Germany. ... Simplified here. <begin_search_query>Headquarters of German Aerospace Center<end_search_query> <begin_search_result>The German Aerospace Center many.<end_search_result> So, according to this, the DLR is headquartered in Cologne, Germany. But just to be thorough, let me see if there is any other information about Ulrich Walters employer. <begin_search_query>Ulrich Walter workplace<end_search_query> <begin_search_result>Ulrich Walters employer is the German Aerospace Center (DLR), headquartered in Cologne, Germany.<end_search_result> ... Simplified here. is headquartered in Cologne, Ger- (DLR) should probably confirm if Ulrich Walter is still active or if he has retired. <begin_search_query>Ulrich Walter retirement<end_search_query> <begin_search_result>No helpful information found.<end_search_result> ... Simplified here. ... Simplified here. Alternatively, perhaps he has taken up an academic position at university. <begin_search_query>Ulrich Walter university affiliation<end_search_query> <begin_search_result>Ulrich Walters employer, the German Aerospace Center (DLR), is headquartered in Cologne, Germany.<end_search_result> ... Simplified here. Maybe should look for his professional profile or LinkedIn page to see his current employer. <begin_search_query>Ulrich Walter LinkedIn<end_search_query> ... Simplified here. the answer to the question \"Where is Ulrich Walters employer headquartered?\" is Cologne, Germany. Final Answer Cologne, Germany ReaRAG-9B Thought 1: need to find out where Ulrich Walters employer is headquartered. First, should figure out who Ulrich Walter is and where he works. Action 1: {'function': 'search', 'parameters': {'query': 'Who is Ulrich Walter and where does he work?'}} Observation 1: Ulrich Walter is German astronaut who worked at the German Aerospace Center. Thought 2: So, Ulrich Walter works at the German Aerospace Center. Now, need to find out where this center is headquartered. Action 2: {'function': 'search', 'parameters': {'query': 'Where is the German Aerospace Center headquartered?'}} Observation 2: The German Aerospace Center (DLR) is headquartered in Cologne, Germany. Thought 3: Alright, so if DLR is headquartered in Cologne, Germany, then that should be the answer. But maybe should double-check if Ulrich Walter works there still or if theres any update. Action 3: {'function': 'search', 'parameters': {'query': 'Is Ulrich Walter still working at DLR?'}} Observation 3: cannot provide real-time information or updates on whether Ulrich Walter is still working at the German Aerospace Center (DLR) since my training data only goes up until 2023. To get the most current information on Ulrich Walters employment status, you would need to check the latest sources or contact the German Aerospace Center directly. Thought 4: Given that, should probably proceed with the information have, which is that Ulrich Walter works at DLR, which is headquartered in Cologne, Germany. Action 4: {'function': 'finish', 'parameters': {'answer': 'Cologne, Germany'}} Final Answer: Cologne, Germany Table 8: We compare the generations of Search-o1 and ReaRAG. Words highlighted in red indicate instances where Search-o1 overthinks and performs excessive verification. Words highlighted in green show that the correct answer was already found in earlier steps. As shown above, ReaRAG arrives at the correct answer in just three searches, while Search-o1 requires eight, producing excessive queries that are redundant in multi-hop QA. 20 # Case Study 5: Who is the grandmother of Philippe, Duke of Orléans? Ground truth: Marie de Medici ReaRAG-9B Thought 1: So need to find out who the grandmother of Philippe, Duke of Orléans, is. dont know much about French royalty, so Ill need to look this up. Action 1: {'function': 'search', 'parameters': {'query': 'Who is the grandmother of Philippe, Duke of Orléans?'}} Observation 1: The grandmother of Philippe, Duke of Orléans (Philippe I) is Anne of Austria. Wrong answer! Thought 2: Wait, let me make sure about this. have found that his grandmother is Anne of Austria, but should verify this information from multiple sources to ensure accuracy. Action 2: {'function': 'search', 'parameters': {'query': 'Philippe, Duke of Orléans family tree'}} Observation 2: Philippe, Duke of Orléans, was the younger son of Louis XIII of France and Anne of Austria. His older brother was Louis XIV, famously known as the \"Sun King.\" Philippe became Duke of Orléans upon the death of his uncle Gaston in 1660. In 1661, he also received the dukedoms of Valois and Chartres. Thought 3: So, according to this, his grandmother is Anne of Austria. But let me see if theres any confusion here. Philippes parents were Louis XIII and Anne of Austria, so she would be his mother, not grandmother. Wait, that doesnt make sense. Maybe need to clarify the generational relationships. ReaRAG notices the mistakes. Action 3: {'function': 'search', 'parameters': {'query': 'Genealogy of the House of Orléans '}} Observation 3: The House of Orléans is cadet branch of the House of Bourbon, which is one of the most prominent royal houses in European history. The Orléans branch originated with Philippe I, Duke of Orléans, who was the younger son of Louis XIII of France and Anne of Austria. Philippe was the brother of Louis XIV, the \"Sun King.\" The genealogy of the House of Orléans can be traced through various descendants of Philippe I, including: 1. Philippe II, Duke of Orléans, who was the father of Louise Élisabeth dOrléans mentioned in Document 351. Philippe II was also known as Philippe Is grandson, as he was the son of Philippe Is son, Louis XIV. 2. Louis Philippe, Duke of Orléans, who became King of the French after the July Revolution of 1830. He was the father of François dOrléans, Prince of Joinville, mentioned in Document 393. These are just few examples of the descendants of Philippe I, Duke of Orléans, and the House of Orléans continues to have living members today. Thought 4: Hmm, think Im getting confused here. Philippe I, Duke of Orléans, was the son of Louis XIII and Anne of Austria. So, Anne of Austria would be his mother, not his grandmother. His grandmother would be the mother of his parents, which would be Louis XIIIs mother. Let me check who that was. ReaRAG realigns its reasoning direction. Action 4: {'function': 'search', 'parameters': {'query': 'Who was the mother of Louis XIII of France? '}} Observation 4: The mother of Louis XIII of France was Marie de Medici. Thought 5: So, if Marie de Medici was the mother of Louis XIII, and Anne of Austria was his wife and the mother of Philippe I, then Anne of Austria would be Philippe Is mother, not grandmother. So, his grandmother would be Marie de Medici. That makes more sense. But to be thorough, should confirm the relationships again. Louis XIII married Anne of Austria, and they had children, including Philippe and Louis XIV. So, Anne of Austria is Philippe Is mother, and Marie de Medici is his grandmother. think thats correct, but Ill double-check with another search. Action 5: {'function': 'search', 'parameters': {'query': 'Philippe, Duke of Orléans family tree '}} Observation 5: Philippe, Duke of Orléans (1640-1701) was the younger son of Louis XIII of France and Anne of Austria. His older brother was Louis XIV, the famous \"Sun King.\" Philippe was styled Duke of Anjou from birth and became Duke of Orléans upon the death of his uncle Gaston in 1660. He also received the dukedoms of Valois and Chartres in 1661. Philippe had several children, including Louise Élisabeth dOrléans, who was one of his seven daughters. Louise Élisabeth was known as Élisabeth and was \"princesse du sang\" as member of the reigning House of Bourbon. Thought 6: Okay, so this confirms that Anne of Austria was Philippe Is mother. Therefore, his grandmother would be Marie de Medici, who was Louis XIIIs mother. think thats the correct answer. But just to be absolutely sure, lets look up the genealogy again. Louis XIII was the son of Henry IV and Marie de Medici, so Marie de Medici would be his mother, and Anne of Austria would be his wife and Philippe Is mother. think thats consistent across all the information Ive found. So, the grandmother of Philippe, Duke of Orléans, is Marie de Medici. Ill go ahead and provide that as the answer. Action 6: {'function': 'finish', 'parameters': {'answer': 'Marie de Medici'}} Final Answer: Marie de Medici Table 9: In the first reasoning step, ReaRAG makes an incorrect deduction, mistakenly identifying Anne of Austria as Philippes grandmother instead of his mother. However, it quickly recognizes the error, then verifies its previous reasoning steps, and recalibrates its logic. By refining its approach, ReaRAG ultimately arrives at the correct answer, demonstrating its adaptive reasoning ability. 21 # Case Study 6: who is the younger brother of The episode guest stars of The Hard Easy Ground truth: Bill Murray ReaRAG-9B Thought 1: need to find out who the younger brother is of the episode guest stars of \"The Hard Easy.\" First, should figure out what \"The Hard Easy\" is. It sounds like TV show or movie. Action 1: {'function': 'search', 'parameters': {'query': 'What is The Hard Easy?'}} Observation 1: The Hard Easy is 2006 American action crime drama film directed by Ari Ryan and written by Jon Lindstrom and Tom Schanley. It stars Henry Thomas, Vera Farmiga, David Boreanaz, Bruce Dern, Peter Weller, Gary Busey, and Nick Lachey. The film was released straight-to-DVD in the United States on May 22, 2007. Thought 2: So, \"The Hard Easy\" is 2006 film with several guest stars. need to identify the guest stars and then find out who the younger brother of one of them is. But the question seems bit unclear. Maybe its asking for the younger brother of one of the guest stars in specific episode, but since its film, not TV series, perhaps there are multiple episodes. Action 2: {'function': 'search', 'parameters': {'query': 'Are there multiple episodes of The Hard Easy?'}} Observation 2: No, there is only one episode of \"The Hard Easy\" as part of the \"Adventure Time\" series. Thought 3: Wait, now Im getting confused. Earlier, thought \"The Hard Easy\" was 2006 film, but now its saying its an episode of \"Adventure Time.\" Maybe theres some confusion here. need to clarify what \"The Hard Easy\" refers to. ReaRAG actively performs searches to resolve the confusion. Action 3: {'function': 'search', 'parameters': {'query': 'Is The Hard Easy film or an episode? '}} Observation 3: The Hard Easy is both film and an episode. The film version was released straight-to-DVD in 2007, while the episode is part of the animated television series \"Adventure Time\" and aired on Cartoon Network in 2012. Thought 4: Okay, so theres 2006 film called \"The Hard Easy\" and an episode with the same name in the \"Adventure Time\" series. The question might be referring to the episode in \"Adventure Time.\" need to find out the guest stars of that episode and then find out who the younger brother of one of them is. Action 4: {'function': 'search', 'parameters': {'query': 'Who are the guest stars in the Adventure Time episode \"The Hard Easy\"? '}} Observation 4: The guest stars in the Adventure Time episode \"The Hard Easy\" are Brian Doyle-Murray as Prince Huge and Jonathan Katz as the Mudscamp elder. Thought 5: So, the guest stars are Brian Doyle-Murray and Jonathan Katz. Now, need to find out who the younger brother of one of them is. But dont know their family relationships off the top of my head. Ill need to look that up. Action 5: {'function': 'search', 'parameters': {'query': 'Who is the younger brother of Brian Doyle-Murray? '}} Observation 5: The younger brother of Brian Doyle-Murray is Bill Murray. Thought 6: Wait, that cant be right. Bill Murray is famous actor, but dont think hes the younger brother of Brian Doyle-Murray. Maybe need to check their relationship again. ReaRAG encounters conflicts with its parametric knowledge. Action 6: {'function': 'search', 'parameters': {'query': 'Are Bill Murray and Brian Doyle-Murray brothers? '}} Observation 6: Yes, Bill Murray and Brian Doyle-Murray are brothers. Thought 7: So, if the question is asking for the younger brother of one of the guest stars in the Adventure Time episode \"The Hard Easy,\" and the guest stars are Brian Doyle-Murray and Jonathan Katz, and the younger brother of Brian Doyle-Murray is Bill Murray, then the answer should be Bill Murray. Action 7: {'function': 'finish', 'parameters': {'answer': 'Bill Murray'}} Final Answer: Bill Murray Table 10: During the first three iterations, ReaRAG attempts to resolve ambiguity in the original multi-hop question, actively conducting searches to eliminate confusion. Additionally, at the sixth iteration, it encounters knowledge conflicts. Despite these challenges, ReaRAG successfully derives the correct answer, showcasing its robust reasoning abilities. 22 # Case Study 7: This singer of Rather Blustery Day also voiced what hedgehog? Ground truth: Sonic ReaRAG-9B Thought 1: need to find out which singer performed \"A Rather Blustery Day\" and then find out what hedgehog they voiced. Action 1: {'function': 'search', 'parameters': {'query': 'Who sang \"A Rather Blustery Day\"?'}} Observation 1: Jim Cummings sang \"A Rather Blustery Day\". Thought 2: Now that know Jim Cummings sang that song, need to find out what hedgehog he voiced. Action 2: {'function': 'search', 'parameters': {'query': 'What hedgehog did Jim Cummings voice?'}} Observation 2: Jim Cummings voiced Dr. Robotnik from \"Sonic the Hedgehog\". Thought 3: Wait, but Dr. Robotnik is robot, not hedgehog. Maybe need to check again. Action 3: {'function': 'search', 'parameters': {'query': 'What characters did Jim Cummings voice in Sonic the Hedgehog? '}} Observation 3: Jim Cummings voiced Dr. Robotnik in the \"Sonic the Hedgehog\" series. Thought 4: It seems like there might be confusion here. Dr. Robotnik is robot, but the question mentions hedgehog. Maybe the hedgehog is different character. The original question is confusing, but ReaRAG manages to resolve it. Action 4: {'function': 'search', 'parameters': {'query': 'List of characters voiced by Jim Cummings in Sonic the Hedgehog. '}} Observation 4: Jim Cummings is known for voicing Dr. Robotnik in the \"Sonic the Hedgehog\" series. Thought 5: Maybe need to look for other hedgehog characters that Jim Cummings has voiced. Action 5: {'function': 'search', 'parameters': {'query': 'Hedgehog characters voiced by Jim Cummings. '}} Observation 5: Jim Cummings is known for voicing Dr. Robotnik from \"Sonic the Hedgehog\". Thought 6: Perhaps the hedgehog in question is Sonic himself, who is hedgehog, but already know that Sonic is voiced by Roger Craig Smith, not Jim Cummings. ReaRAG encounters knowledge conflicts but mitigates them through verification. Action 6: {'function': 'search', 'parameters': {'query': 'Who voices Sonic in the Sonic the Hedgehog series?'}} Observation 6: Jim Cummings voices Sonic the Hedgehog in the Sonic the Hedgehog series. Thought 7: Wait, that contradicts what thought earlier. might be confusing the voice actors. Action 7: {'function': 'search', 'parameters': {'query': 'Voice actor for Sonic the Hedgehog. '}} Observation 7: Jim Cummings Thought 8: This is getting confusing. need to verify the voice actor for Sonic the Hedgehog. Action 8: {'function': 'search', 'parameters': {'query': 'Who voices Sonic in the Sonic the Hedgehog games and cartoons? '}} Observation 8: Jim Cummings voices Sonic the Hedgehog in the Sonic the Hedgehog games and cartoons. Thought 9: So, Jim Cummings voices Sonic the Hedgehog, who is hedgehog. Therefore, the answer should be Sonic the Hedgehog. Action 9: {'function': 'finish', 'parameters': {'answer': 'Sonic the Hedgehog'}} Final Answer: Sonic the Hedgehog Table 11: This example illustrates how ReaRAG successfully resolves ambiguity in the original question and addresses conflicts between retrieved knowledge and its parametric knowledge. Despite these challenges, ReaRAG effectively mitigates them and remains robust throughout the reasoning process."
        }
    ],
    "affiliations": [
        "Siemens AG",
        "Tsinghua University"
    ]
}