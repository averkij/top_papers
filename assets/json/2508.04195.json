{
    "paper_title": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations",
    "authors": [
        "Huan Liao",
        "Qinke Ni",
        "Yuancheng Wang",
        "Yiheng Lu",
        "Haoyue Zhan",
        "Pengyuan Xie",
        "Qiang Zhang",
        "Zhizheng Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Paralinguistic vocalizations-including non-verbal sounds like laughter and breathing, as well as lexicalized interjections such as \"uhm\" and \"oh\"-are integral to natural spoken communication. Despite their importance in conveying affect, intent, and interactional cues, such cues remain largely overlooked in conventional automatic speech recognition (ASR) and text-to-speech (TTS) systems. We present NVSpeech, an integrated and scalable pipeline that bridges the recognition and synthesis of paralinguistic vocalizations, encompassing dataset construction, ASR modeling, and controllable TTS. (1) We introduce a manually annotated dataset of 48,430 human-spoken utterances with 18 word-level paralinguistic categories. (2) We develop the paralinguistic-aware ASR model, which treats paralinguistic cues as inline decodable tokens (e.g., \"You're so funny [Laughter]\"), enabling joint lexical and non-verbal transcription. This model is then used to automatically annotate a large corpus, the first large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both human- and auto-labeled data to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for human-like speech synthesis. By unifying the recognition and generation of paralinguistic vocalizations, NVSpeech offers the first open, large-scale, word-level annotated pipeline for expressive speech modeling in Mandarin, integrating recognition and synthesis in a scalable and controllable manner. Dataset and audio demos are available at https://nvspeech170k.github.io/."
        },
        {
            "title": "Start",
            "content": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations Huan Liao1*, Qinke Ni1*, Yuancheng Wang1, Yiheng Lu1, Haoyue Zhan2, Pengyuan Xie2, Qiang Zhang2, Zhizheng Wu1, 1The Chinese University of Hong Kong, Shenzhen 2Guangzhou Quwan Network Technology 5 2 0 2 6 ] . [ 1 5 9 1 4 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Paralinguistic vocalizationsincluding non-verbal sounds like laughter and breathing, as well as lexicalized interjections such as uhm and ohare integral to natural spoken communication. Despite their importance in conveying affect, intent, and interactional cues, such cues remain largely overlooked in conventional automatic speech recognition (ASR) and text-to-speech (TTS) systems. We present NVSpeech, an integrated and scalable pipeline that bridges the recognition and synthesis of paralinguistic vocalizations, encompassing dataset construction, ASR modeling, and controllable TTS. (1) We introduce manually annotated dataset of 48,430 human-spoken utterances with 18 word-level paralinguistic categories. (2) We develop the paralinguistic-aware ASR model, which treats paralinguistic cues as inline decodable tokens (e.g., Youre so funny [Laughter]), enabling joint lexical and non-verbal transcription. This model is then used to automatically annotate large corpus, the first large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both humanand auto-labeled data to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for humanlike speech synthesis. By unifying the recognition and generation of paralinguistic vocalizations, NVSpeech offers the first open, large-scale, word-level annotated pipeline for expressive speech modeling in Mandarin, integrating recognition and synthesis in scalable and controllable manner. Dataset and audio demos are available at https://nvspeech170k.github.io/. Introduction Paralinguistic vocalizationssuch as nonverbal vocalizations (NVVs) like laughter and breathing, as well as lexicalized interjections like uhm and ohare widely present in spontaneous speech (Tseng 2003). These cues, especially interjections, encode affect, intent, and speaker state beyond literal lexical contents, often via distinctive prosody, enhancing expressivity and ensuring social appropriateness (Loy, Rohde, and Corley 2017; Kidd, White, and Aslin 2011; Ward 2006). However, paralinguistic vocalizations, particularly NVVs, are often discarded as noise in conventional speech processing pipelines due to the lack of annotated fine-grained, word-level alignment data. *Equal contribution. Recent advances in automatic speech recognition (ASR) and text-to-speech (TTS) systems have led to impressive gains in transcription accuracy and speech naturalness. However, traditional ASR focuses solely on lexical contents, overlooking paralinguistic cues crucial for understanding spontaneous communication (Lee and Nass 2003; Gao et al. 2023). Similarly, recent TTS systems (An et al. 2024; Du et al. 2024; Guo et al. 2024) enable instruction-based paralinguistic control, but typically rely on closed-source datasets with limited behavioral diversity, lacking transparent and fine-grained supervision for modeling natural paralinguistic vocalizations. In real-world speech, verbal and non-verbal cues are inherently intertwined. To capture human-like communication, ASR and TTS systems should move beyond lexical content to model paralinguistic signals in temporally aligned and semantically coherent manner. In Figure 1, we identify three critical gaps in current speech modeling: (a) Most existing speech datasets lack word-level annotations of paralinguistic vocalizations, limiting supervision and evaluation of expressive models. (b) Conventional ASR systems omit such cues, hindering their application in tasks requiring human-like understanding and interaction(Lee and Nass 2003; Gao et al. 2023). (c) Current TTS models fail to synthesize paralinguistic vocalizations with explicit, token-level control, resulting in speech that lacks spontaneity and expressivity. In tonal languages like Mandarin, paralinguistic cues interact closely with tone and prosody shaping affect, discourse, and speaker intent. Interjections, hesitations, and NVVs play key roles in marking turn-taking, signaling uncertainty or emphasis in spontaneous speech. Without fine-grained, aligned annotations, models struggle to capture these nuances, resulting in unnatural synthesis and fragile speaker-state recognition. Existing Chinese corpora often overlook or coarsely label such cues, hindering both supervision and evaluation of human-like speech understanding and generation. To bridge these gaps, we introduce NVSpeech, an integrated and scalable pipeline for recognition and synthesis of paralinguistic vocalizations in Chinese speech. It centers on large-scale, word-level annotated corpus with 18 types of paralinguistic vocalizationsranging from NVVs such as [Laughter] and [Cough], to prosodic and attitudinal interjections like [Confirmation-en], [Question-ah], and discourse markers [Uhm]. These annotations provide word-alignment and broad coverage, Figure 1: The gap between conventional speech processing systems and paralinguistic-aware modeling. (a) Common speech datasets omit paralinguistic vocalizations, while NVSpeech provides word-level annotations. (b) Conventional ASR ignores such cues; our paralinguistic-aware ASR jointly transcribes lexical and non-lexical content. (c) Standard TTS generates only text-based speech, while our TTS supports explicit insertion of paralinguistic vocalizations for human-like synthesis. making NVSpeech the first corpus to support fine-grained modeling of both lexical and paralinguistic cues in Mandarin. We implement NVSpeech in three stages: (1) Manual annotation. We collect high-quality subset of 48,430 utterances from real-world, human-spoken recordings. Each utterance is manually annotated at the word level with paralinguistic vocalization labels, spanning 18 categories to capture expressive behaviors beyond lexical content. This annotated subset serves as the foundation for training our paralinguistic-aware models. (2) Scalable labeling via paralinguistic-aware ASR. Using the manually annotated data, we train the first paralinguistic-aware ASR model capable of transcribing both lexical content and inline paralinguistic vocalizations, as illustrated in Figure 1(b). We apply this model to large unlabeled speech corpus, including data from miHoYo and Emilia (He et al. 2025), resulting in an auto-labeled dataset of 174,179 utterances. This process dramatically scales up annotation coverage while significantly reducing human labeling cost. (3) Expressive TTS modeling. To validate the utility of NVSpeech, we finetune zero-shot TTS models on both manually and automatically labeled data (Tan et al. 2021). The model enables explicit control over paralinguistic vocalizations, allowing expressive and context-aware insertion at arbitrary word positionsachieving controllable synthesis beyond the expressivity of conventional TTS systems. Our main contributions are: (1) We develop the first paralinguistic-aware ASR model that jointly transcribes lexical content and paralinguistic vocalizations with word-level alignment, enabling structured modeling of expressive speech beyond conventional ASR. (2) We present NVSpeech, an integrated and scalable pipeline that integrates data, ASR, and TTS, centered on large-scale corpus with word-level annotations for 18 categories of paralinguistic vocalizations. The corpus includes both manually annotated and auto-labeled subsets, totaling 573.4 hours, and supports both recognition and generation of human-like vocal behaviors with explicit controllability. (3) We conduct comprehensive benchmarking on paralinguistic tagging, ASR, and zero-shot TTS tasks, demonstrating that NVSpeech enables controllable paralinguistic cues insertion and improves naturalness of synthesized speech. Related Works Paralinguistic Event Recognition Spontaneous speech ASR has advanced with DNN-based approaches. Due to high annotation costs, Mandarin lacks richly transcribed corpora comparable to Englishs Switchboard and Fisher; consequently, most studies ignore non-speech cues (e.g., laughter, breathing) and seldom incorporate discourse markers (e.g., uhm, oh) into decoding. Early works (Gupta et al. 2016),(Rennie, Perepelkina, and Vinciarelli 2022) leveraged prosodic features and MFCCs with classifiers like HMMs to detect laughter and fillers at the frame level. While effective, these models relied on handcrafted features and limited data. Pretrained Audio Event Detection (AED) models like PANNs (Kong et al. 2020) and BEATs (Chen et al. 2022), can detect sound events and provide general audio representations. However, these resources lack word-level alignment, are not optimized for conversational speech, and typically cover only sound events, excluding linguistic interjections like uhm or oh. SenseVoice (An et al. 2024)augments voice understanding with auxiliary tasks via task-specific tokens and pseudolabeled data; however, it treats event detection as decoupled from speech recognition and does not explicitly model interactions between verbal and nonverbal components. Datasets with Paralinguistic Label Several datasets have been developed to support the study of paralinguistic vocalizations, as summarized in Table 1. Early corpora such as the SSPNet Mobile Corpus (SMC) (Polychroniou, Salamin, and Vinciarelli 2014) and SSPNet Vocalization Corpus (SVC) (Salamin, Polychroniou, and Vinciarelli 2013) provide manually segmented annotations of laughter and fillers (e.g., uhm, eh) in phone call-mediated settings. Similarly, DisfluencySpeech (Wang and Herremans 2024), English speech dataset, offers word-level annotations of fillers, discourse markers like you know and well, and NVVs (e.g., sigh, laughter) in clean monologue recordings. More recent efforts focus on broader coverage and speaker diversity. VocalSound (Gong, Yu, and Glass 2022) and Nonspeech7k (Rashid, Li, and Du 2023) compile sentence-level clips across NVVs classes, while the NVV1 dataset covers 16 1https://github.com/deeplyinc/Nonverbal-Vocalization-Dataset Dataset #Utterances Total (h) #Classes #Speaker Annot. Level Lang Avail. Sinica MCDC8 (Tseng 2013) SMC (Polychroniou, Salamin, and Vinciarelli 2014) SVC (Salamin, Polychroniou, and Vinciarelli 2013) RAMC (Yang et al. 2022) NVV VocalSound (Gong, Yu, and Glass 2022) Nonspeech7k (Rashid, Li, and Du 2023) MCDC (Deng et al. 2023) EXPRESSO (Nguyen et al. 2023) DisfluencySpeech (Wang and Herremans 2024) NVSpeechhuman NVSpeech 30 - 2,763 219,325 70,000 21,024 7,014 7,014 11,615 5,000 48,430 174,179 25.6 12.68 8.4 180 56.7 - 6.75 6.75 47 9. 76 573.4 32 5 6 3 16 6 7 7 - 15 18 18 16 120 120 663 1,419 3,365 - - 4 1 1,578 >1,964 Segment Segment Segment Segment Sentence Sentence Sentence Sentence Sentence Word Word Word CN EN EN CN EN Multi EN EN EN EN CN CN Public Public Public Public Public Public Public Public Public Public Private Public Table 1: Comparison of Paralinguistic Datasets types of human nonverbal sound, as well as less commonly annotated events such as teeth-chattering. The EXPRESSO corpus (Nguyen et al. 2023) emphasizes expressive and improvised styles, capturing spontaneous non-verbals and providing benchmark tools for expressive synthesis. Meanwhile, RAMC (Yang et al. 2022) includes categories like laughter and crying but is limited in event diversity. Despite these efforts, most existing datasets suffer from several limitations: (1) lack of word-level alignment between lexical and non-verbal content, hindering precise modeling and in-context understanding or synthesis; (2) limited speaker diversity or constrained recording scenarios, with scarce Chinese data; (3) existing jointly annotated corpora either cover few paralingustic types or only sentence-level labels. Human-like TTS with Paralinguistic Vocalization Recent advances in human-like TTS aim to incorporate paralinguistic vocalizations to improve speech expressiveness. NSV-TTS (Zhang, Yu, and Lin 2023) jointly models speech and NVVs using hybrid representation of phonemes and unsupervised linguistic units (ULUs), enabling zero-shot synthesis of events such as cough and cries. Several approaches introduce disfluency and paralinguistic behaviors through text-based control. CosyVoice-Instruct(An et al. 2024) and FireRedTTS(Guo et al. 2024) allow users to insert NVVs via text tokens or embeddings, supporting behaviors like repetition and emphasis. While these methods offer flexible control, both rely on private datasets. Chaudhury et al. (Chaudhury et al. 2024) use language model to insert disfluency markers with rule-based TTS backend, but suffer from poor interpretability, no personalization, and static mappings unsuited to dynamic disfluent speech. EmoCtrl-TTS (Wu et al. 2024) employs flowmatching model trained on pseudo-labeled embeddings for emotion and NVVs. However, its NVVs modeling is restricted to laughter and cry, and it requires external NVV prompt embeddings during inference, which limits flexibility and scalability. Similarly, ELaTE (Kanda et al. 2024) focuses solely on laughter via frame-level conditioning. While recent TTS have explored integrating paralinguistic vocalizations to enhance expressiveness, they suffer from several limitations. Many rely on small-scale or private datasets with limited NVVs coverage, their generation strategies often lack word-level alignment or require external embeddings or detectors during inference. Text-controlled systems like FireRedTTS offer flexibility but depend on static mappings, heuristic rules, or exhibit poor interpretability. In contrast, our approach leverages publicly available word-level annotated dataset covering diverse paralinguistic vocalizations, enabling token-level paralinguistic vocalizations insertion for scalable, controllable, and natural TTS."
        },
        {
            "title": "Paralinguistic Speech Recognition Model",
            "content": "Paralinguistic Tagging Experimental Setup We formulate paralinguistic tagging as multi-label classification problem, where each utterance is assigned one or more paralinguistic vocalization tags [0, 1]C from predefined vocabulary of size C. Given input [0, 1]C. Training audio are conducted on the human-annotated subset of NVSpeech, with samples in the form (x, y) where is the sentence-level ground-truth tag vector. The training objective is the binary cross-entropy loss: (cid:88) RT , the model predicts ˆy [yc log ˆyc + (1 yc) log( ˆyc)] (1) LBCE = c=1 We finetune three audio tagging models on the training split. Evaluation We evaluate on the held-out test split of the human-labeled subset, where each sample is (x, y) with representing the sentence-level ground-truth tag vector. Performance is reported using standard multi-label classification metrics: Precision, Recall, and F1 score. Baseline Models We evaluate three baselines: (1) PANNs (Kong et al. 2020): CNN-based audio tagging model pretrained on AudioSet (Gemmeke et al. 2017). We finetune the Wavegram_Logmel_Cnn14, which extracts frame-level features and applies pooling for utterancelevel multi-label prediction. (2) SenseVoice-Small (An et al. 2024): speech foundation model pretrained with pseudolabeled audio events. We extend its single-label prediction to multi-label tagging by allowing up to five tags per utterancereflecting the maximum in our datasetand padding Figure 2: Overview of our paralinguistic-aware speech recognition and generation pipeline.(1) word-level human annotated dataset of verbal and non-verbal vocalizations is first constructed. (2) paralinguistic-aware ASR model is trained to jointly transcribe verbal and non-verbal content. (3) This model is used to automatically annotate large-scale unlabeled speech. (4) The expanded dataset enables training controllable and expressive TTS system that explicitly renders paralinguistic cues. with [None] when fewer events are present. (3) QwenAudio (Chu et al. 2023): Whisper-style encoder + Qwen7B decoder trained on over 30 audio-text tasks. We finetune it using instruction-style prompts and sentence-level tag supervision. The prompt format is illustrated in Table 2. Model Arch. Precision Recall F1-score PANNs SenseVoice Qwen-Audio CNN Transformer LLM 0.84 0.84 0.79 0.65 0.67 0.56 0.72 0.73 0.61 Table 3: Paralinguistic Tagging Model Comparison Prompt for Paralinguistic Tagging: <audio> Given an audio clip, identify the paralinguistic event from the following EVENT LABEL SET: {[Breathing], [Crying], [Laughter], ..., [Shh]} MUST follow this template: The paralinguistic vocalizations detected are: {[EVENT 1], [EVENT 2], ..., [EVENT N]} OR the paralinguistic vocalization detected in the audio clip is [None]. Table 2: Instruction prompt used for Qwen-Audio. Results As shown in Table 3, SenseVoice achieves the best overall F1 score (0.73) on the NVSpeech_test set, demonstrating the benefit of pseudo-labeled data and ASR-aware training. PANNs performs competitively with strong precision, confirming its strength in audio event detection. Paralinguistic Aware Speech Recognition Experimental Setup We extend conventional ASR to paralinguistic-aware ASR task by training models to transcribe both lexical content and paralinguistic vocalizations (PV) within unified token sequence. Each model is provided with paired audio and word-level transcripts, where paralinguistic vocalizations are inserted as special tokens in the target sequence (e.g., 不知道[Breathing]我没 想过). This formulation enables the model to treat paralinguistic vocalizations as first-class decoding targets. SpecifiRT and target label sequence cally, given input audio = y1, y2, ..., yL , the model learns to minimize the CTC loss (Graves et al. 2006): { } LCTC = log (y x) = (cid:88) πB1(y) (π x) (2) is the CTC collapsing function and π denotes where the alignment path. All models are finetuned on the NVSpeechhuman dataset. Evaluation We evaluate ASR models on two test sets. The in-domain testset, held-out subset of the human-annotated corpus, covers diverse in-game contexts such as greetings, combat, and narrative dialogue. To assess generalization, we also introduce human-annotated open-domain testset with spontaneous online content including talk shows, interviews, and sports commentary with different accents across different Metric Whisper Paraformer SenseVoice Qwen-Audio In-domain Testset CER CERw/o para Para Det. Rate F1-score 14.18% 11.14% 84.8% 0.71 4.67% 2.26% 96.1% 0.78 Open-domain Testset 4.61% 2.11% 93.4% 0.83 CER CERw/o para Para Det. Rate F1-score 19.41% 16.41% 71.3% 0. 7.81% 5.30% 74.6% 0.72 3.79% 3.16% 93.4% 0.85 5.47% 2.62% 94.5% 0.65 10.06% 6.74% 91.0% 0.54 Table 4: Performance of paralingustic-aware ASR. Figure 3: F1 scores across paralinguistic categories. scenarios. We use four metrics for evaluation: CER (character error rate over the full transcript), CER-w/o-para (excluding paralingustic tokens), Para Detection Rate (whether any PV is correctly detected in an utterance), and F1-score (event-level precision and recall on PV prediction). Baseline Models We benchmark four models with distinct architectures and decoding strategies. (1) Paraformer (Gao et al. 2022) is non-autoregressive ASR model that employs continuous integrate-and-fire (CIF) mechanism for segmentRT , it first produces wise decoding. Given input audio hidden representations h1, . . . , hT , and then uses the CIF gate to compute segmental embeddings: hi = CIF(h1, . . . , hT ) (3) We treat PV as special tokens and train the model to emit both lexical and PV labels in unified output sequence = y1, . . . , yL . } { (2) SenseVoice-Small is nonautoregressive encoder-only model for multi-task speech unRT is first converted derstanding. The input audio into 80-dimensional log-Mel filterbanks and then mapped to encoder features Xspeech. To specify the ASR task, we prepend task embedding eASR to the input: = concat(eASR, Xspeech) (4) The encoder produces contextualized representations, followed by linear projection and softmax: (An et al. 2024) = Softmax(LinearF (Encoder(X))) (5) where is the vocabulary including lexical tokens and PV tags. During fine-tuning, we extend the vocabulary with PVspecific tokens and optimize the model using the CTC loss. (3) Qwen-Audio (Chu et al. 2023) combines Whisperbased audio encoder with large language model. Although it lacks native support for PV transcription, we extend its output vocabulary with PV-specific tokens and finetune it using instruction-based prompts. Given paired input (x, y), where y1, ..., yL { RT is the input audio and = is the target token sequence including both lexical and PV tokens, the model is trained to maximize the conditional log-likelihood: } LLM = (cid:88) t=1 log Pθ(yt y<t, Encoderϕ(x)) (6) (4) Whisper (Radford et al. 2023) is widely used transformer-based ASR model. Whisper was trained on very large, diverse dataset covering many languages, accents, and audio conditions. It jointly performs language identification, transcription, and translation. It is optimized with an autoregressive next-token prediction loss, similiar to Eq. 6. Results Table 4 reports results on both in-domain and opendomain testset. On in-domain part, SenseVoice achieves the best CER (4.61%) and F1-score (0.83), benefiting from its encoder-only SAN-M architecture. Paraformer attains the highest PV Detection Rate (96.1%), while Qwen-Audio performs worst across all metrics, whose Whisper-initialized encoder and LLM decoder are optimized for semantic abstraction, remains less sensitive to fine-grained paralinguistic cues. Since in-domain tests mainly reflect performance on gamestyle speech, we further evaluate on an open-domain set with spontaneous and noisy content to assess robustness in realworld scenarios. Here, SenseVoice again leads (CER 3.79%, F1 0.85), confirming paralinguistic-aware ASR generalizes effectively beyond controlled domains. Figure 3 presents detailed F1 scores for all paralinguistic categories. The Appendix provides further discussion on model confidence and qualitative analyses of failure cases."
        },
        {
            "title": "The NVSpeech Dataset",
            "content": "Dataset Construction Label Set Definition We define 18 categories of paralinguistic vocalizations, encompassing physiological sounds (e.g., [Laughter], [Cough]), discourse markers (e.g., [Uhm]), and expressive interjections with attitude (e.g., [Confirmation-en], [Question-ah], [Surprise-oh]). These labels, derived from empirical analysis of Mandarin spontaneous speech, capture highfrequency paralinguistic sound and functionally distinct categories that support discourse coherence (Tseng 2013). Data Sources and Augmentation We construct the humanannotated dataset from both expressive game-based and augmented sources. The core speech data is drawn from two open-source repositories: the Genshin2 and StarRail3 datasets, 2https://huggingface.co/datasets/simon3000/genshin-voice 3https://github.com/AI-Hobbyist/StarRail_Datasets which offer multilingual voice lines from miHoYo games; we use only the Chinese subset. These lines cover diverse in-game contexts such as greetings, combat, and narrative dialogue, and include metadata such as transcription, speaker identity, and utterance type. To increase coverage of non-verbal vocalizations, we augment the corpus with 500 coughing and 500 crying clips from Nonspeech7k (Rashid, Li, and Du 2023), clean, manually labeled non-speech dataset. Additionally, we synthesize 166 utterances using CosyVoice2 (Du et al. 2024) to enrich rare paralinguistic categories (e.g., [Surprise-yo], [Question-en], [Shh]), with text prompts generated via DeepSeek-R1 (DeepSeek-AI et al. 2025) to ensure contextual diversity and naturalness. Human-annotated datasets Each of the ten trained annotators was tasked with listening to the audio recordings and inserting appropriate paralinguistic vocalization labels into the corresponding transcripts, guided by the temporal location of each label. All annotators received standardized training with positive and negative examples to ensure consistency. For quality assurance, 5% of the data was cross-annotated, yielding Cohens kappa above 0.85 on major categories. The overall annotation workflow is illustrated in Step1 of Figure 2, and the label distribution is illustrated in Figure 4a. Large-scale automatically scaled datasets To further scale our training corpus beyond the manually labeled subset, we construct large-scale automatically annotated dataset using high-quality speech data from multiple sources. Specifically, we include: (1) the unlabeled portion of the Genshin and StarRail dataset (excluding the manually annotated subset), (2)A subset of Emilia(He et al. 2025), large-scale multilingual speech dataset constructed from in-the-wild recordings, including talk shows, interviews, debates, and audiobooks. We select clips likely to contain paralinguistic events. (3) 1,362 non-verbal clips ([Crying] and [Cough]) sampled from Nonspeech7k (Rashid, Li, and Du 2023). All clips are in Chinese and capture rich expressive behaviors from diverse sources, ranging from structured in-game to spontaneous, real-world conversational scenarios. We use SenseVoice, the best-performing paralinguisticaware ASR (Section Paralinguistic Aware Speech Recognition), to automatically transcribe audio with both lexical content and inline tags for paralinguistic vocalizations. In total, the automatically labeled dataset contains 174,179 audio-transcription pairs, amounting to 573.4 hours, significantly expanding the diversity and coverage of paralinguistic vocalization categories. Figure 4b shows the label distribution, and Table 1 compares it with existing paralinguistic datasets. This large-scale dataset serves as valuable resource for pretraining and semi-supervised learning, reducing the reliance on costly human annotation. Paralinguistic-enhanced TTS Experiments In this section, we evaluate the effectiveness of the proposed NVSpeech dataset in training zero-shot TTS capable of generating expressive speech with natural paralinguistic behaviors. (a) NVSpeechhuman (b) NVSpeech Figure 4: Paralinguistic vocalization category distribution Experimental Setups To evaluate the effectiveness of the automatically labeled NVSpeech dataset, we conduct TTS enhancement experiments on three training subsets: (1) NVSpeechhuman (humanannotated), (2) NVSpeechhuman-size (an auto-labeled subset with the same size, preserving similar label distribution as NVSpeechhuman), and (3) NVSpeech (full auto-labeled). We finetune pretrained TTS model, extending their vocabularies to include paralinguistic tags. Training data consists of 35% regular speech and 65% paralinguistic-rich utterances. Baseline Models CosyVoice (An et al. 2024) is zeroshot TTS system that leverages supervised semantic tokens, using language model to predict tokens from text and flowmatching decoder to synthesize speech. CosyVoice2 (Du et al. 2024) is an instruction-following TTS model that supports paralinguistic prompts; we extend its vocabulary to include NVSpeech tags for fine-grained control over event placement. Evaluation We evaluate TTS models on both in-domain and open-domain testsets. The in-domain testset, drawn from the human-annotated corpus, reflects performance on con-"
        },
        {
            "title": "Model",
            "content": "In-domain Testset Open-domain Testset CER CosyVoice CosyVoice2 - - CosyVoice CosyVoice 8.78% 8.61% CosyVoice CosyVoice2 8.59% 7.83% CosyVoice CosyVoice2 7.96% 7.51% CERw/o para SIM"
        },
        {
            "title": "UTMOS",
            "content": "CER CERw/o para 7.42% 3.13% 4.21% 3.86% - - 0.736 0. 0.727 0.710 Pre-trained 2.69 2.69 Finetuned on Human-Labeled Data 2.54 2.54 Finetuned on Auto-Labeled Data (Equal Size) 2.54 2.57 Finetuned on Auto-Labeled Data (Large-Scale) 2.57 2.67 11.09% 9.48% 9.97% 8.44% 9.99% 8.07% 0.733 0. 0.736 0.704 4.07% 3.77% 4.05% 3.73% 10.44% 7.91% 6.71% 5.57% 6.12% 5.45% 5.84% 5.73% SIM UTMOS 0.743 0.722 0.748 0.719 0.750 0. 0.747 0.703 2.49 2.25 2.35 2.12 2.35 2.20 2.39 2.26 Table 5: Objective Evaluation of Para-enhanced TTS. Bold indicates best in the column, underline second-best. trolled game-style speech. The open-domain testset with spontaneous and diverse real-world speech, which better reflects practical application scenarios and show robustness beyond in-domain testset. Objective metrics include overall character error rate (CER), CER on verbal content only without paralingustic labels (CER_wo_para), speaker similarity (SIM) (San Segundo and Mompean 2017), and UTMOS (Saeki et al. 2022) for perceptual audio quality. Main Results Table 5 summarizes the objective evaluation of para-enhanced TTS. (1) Human-labeled finetuning enables paralinguistic generation while slightly lowering CER overall, with no consistent degradation across models or domains. (2) Autolabeled data (equal size) yields better CER and UTMOS than human-only training with comparable SIM, showing that auto-labeled data can match the effectiveness of human annotation. (3) Scaling with large auto-labeled data achieves the best performance, with up to 12.8% relative CER reduction on in-domain speech. These results verify the effectiveness of our pipeline for both game-related and open-domain TTS tasks, and demonstrate its scalabilityshowing that both large-scale autoannotation and human annotation can substantially enhance paralinguistic synthesis. Human Evaluation Evaluation Metrics Subjective evaluation considers human preference scores, paralinguistic event recall, perceived naturalness of events synthesis and transitions (NMOS), and QMOS for overall quality of synthesized speech. We invited 60 participants to compare TTS outputs before and after fine-tuning with NVSpeech. As shown in Figure 5, both CosyVoice and CosyVoice2 saw clear listener preference after para-enhancement, with win rates of 78.7% and 75.4%, respectively. Table 6 further shows that the finetuned models achieved high naturalness (NMOS: 3.9-4.0) and clarity (QMOS: 4.04-3.96), while maintaining reasonable recall of paralinguistic tags (up to 61.9%). These results confirm that NVSpeech enables natural and expressive speech synthesis with structured paralinguistic cues, without compromising core speech quality. Figure 5: Para-enhanced vs. Original (Human Preference). Model CosyVoice CosyVoice2 Recall 0.604 0.619 NMOS 0.20 0.16 3.9 4.0 QMOS 0.15 0.14 4.04 3.96 Table 6: Subjective Evaluation of Para-enhanced Speech. Conclusion We present NVSpeech, an integrated and scalable pipeline for paralinguistic-aware speech dataset, recognition, and generation. The pipeline first establishes word-level paralinguistic resource with 18 paralinguistic vocalization categories, then trains paralinguistic-aware ASR on the human-annotated subset to automatically label large-scale data (573.4h), and finally enhances zero-shot TTS with both humanand auto-labeled data. Experimental results demonstrate strong paralinguistic tag recognition (F1 up to 0.84) and expressive speech synthesis preferred by listeners (win rate 78.7%) without degrading lexical quality, confirming the pipelines effectiveness for both recognition and generation. This work provides scalable foundation for future research on expressive, human-like speech modeling. References An, K.; Chen, Q.; Deng, C.; Du, Z.; Gao, C.; Gao, Z.; Gu, Y.; He, T.; Hu, H.; Hu, K.; et al. 2024. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051. Chaudhury, R.; Godbole, M.; Garg, A.; and Seo, J. H. 2024. Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation. arXiv preprint arXiv:2404.01339. Chen, S.; Wu, Y.; Wang, C.; Liu, S.; Tompkins, D.; Chen, Z.; and Wei, F. 2022. Beats: Audio pre-training with acoustic tokenizers. arXiv preprint arXiv:2212.09058. Chu, Y.; Xu, J.; Zhou, X.; Yang, Q.; Zhang, S.; Yan, Z.; Zhou, C.; and Zhou, J. 2023. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919. DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.; et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948. Deng, Y.-C.; Liao, Y.-F.; Wang, Y.-R.; and Chen, S.-H. 2023. Toward enriched decoding of mandarin spontaneous speech. Speech Communication, 154: 102983. Du, Z.; Wang, Y.; Chen, Q.; Shi, X.; Lv, X.; Zhao, T.; Gao, Z.; Yang, Y.; Gao, C.; Wang, H.; et al. 2024. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117. Gao, Z.; Li, Z.; Wang, J.; Luo, H.; Shi, X.; Chen, M.; Li, Y.; Zuo, L.; Du, Z.; Xiao, Z.; et al. 2023. Funasr: fundamental end-to-end speech recognition toolkit. arXiv 2023. arXiv preprint arXiv:2305.11013. Gao, Z.; Zhang, S.; McLoughlin, I.; and Yan, Z. 2022. Paraformer: Fast and accurate parallel transformer for nonautoregressive end-to-end speech recognition. arXiv preprint arXiv:2206.08317. Gemmeke, J. F.; Ellis, D. P. W.; Freedman, D.; Jansen, A.; Lawrence, W.; Moore, R. C.; Plakal, M.; and Ritter, M. 2017. Audio Set: An ontology and human-labeled dataset for audio events. In Proc. IEEE ICASSP 2017. New Orleans, LA. Gong, Y.; Yu, J.; and Glass, J. 2022. Vocalsound: dataset for improving human vocal sounds recognition. In ICASSP 20222022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 151155. IEEE. Graves, A.; Fernández, S.; Gomez, F.; and Schmidhuber, J. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, 369376. Guo, H.-H.; Hu, Y.; Liu, K.; Shen, F.-Y.; Tang, X.; Wu, Y.-C.; Xie, F.-L.; Xie, K.; and Xu, K.-T. 2024. Fireredtts: foundation text-to-speech framework for industry-level generative speech applications. arXiv preprint arXiv:2409.03283. Gupta, R.; Audhkhasi, K.; Lee, S.; and Narayanan, S. 2016. Detecting paralinguistic events in audio stream using context in features and probabilistic decisions. Computer speech & language, 36: 7292. He, H.; Shang, Z.; Wang, C.; Li, X.; Gu, Y.; Hua, H.; Liu, L.; Yang, C.; Li, J.; Shi, P.; et al. 2025. Emilia: Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation. arXiv preprint arXiv:2501.15907. Kanda, N.; Wang, X.; Eskimez, S. E.; Thakker, M.; Yang, H.; Zhu, Z.; Tang, M.; Li, C.; Tsai, C.-H.; Xiao, Z.; et al. 2024. Making flow-matching-based zero-shot text-to-speech laugh as you like. arXiv preprint arXiv:2402.07383. Kidd, C.; White, K. S.; and Aslin, R. N. 2011. Toddlers use speech disfluencies to predict speakers referential intentions. Developmental science, 14(4): 925934. Kong, Q.; Cao, Y.; Iqbal, T.; Wang, Y.; Wang, W.; and Plumbley, M. D. 2020. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28: 28802894. Lee, K. M.; and Nass, C. 2003. Designing social presence of social actors in human computer interaction. In Proceedings of the SIGCHI conference on Human factors in computing systems, 289296. Loy, J. E.; Rohde, H.; and Corley, M. 2017. Effects of disfluency in online interpretation of deception. Cognitive Science, 41: 14341456. Nguyen, T. A.; Hsu, W.-N.; dAvirro, A.; Shi, B.; Gat, I.; Fazel-Zarani, M.; Remez, T.; Copet, J.; Synnaeve, G.; Hassid, M.; et al. 2023. Expresso: benchmark and analysis of discrete expressive speech resynthesis. arXiv preprint arXiv:2308.05725. Polychroniou, A.; Salamin, H.; and Vinciarelli, A. 2014. The SSPNet-Mobile Corpus: Social Signal Processing Over Mobile Phones. In LREC, 14921498. Radford, A.; Kim, J. W.; Xu, T.; Brockman, G.; McLeavey, C.; and Sutskever, I. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, 2849228518. PMLR. Rashid, M. M.; Li, G.; and Du, C. 2023. Nonspeech7k dataset: Classification and analysis of human non-speech sound. IET Signal Processing, 17(6): e12233. Rennie, G.; Perepelkina, O.; and Vinciarelli, A. 2022. Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in Naturalistic Conversational Dataset. In INTERSPEECH, 40084012. Saeki, T.; Xin, D.; Nakata, W.; Koriyama, T.; Takamichi, S.; and Saruwatari, H. 2022. Utmos: Utokyo-sarulab system for voicemos challenge 2022. arXiv preprint arXiv:2204.02152. Salamin, H.; Polychroniou, A.; and Vinciarelli, A. 2013. Automatic detection of laughter and fillers in spontaneous mobile phone conversations. In 2013 IEEE International Conference on Systems, Man, and Cybernetics, 42824287. IEEE. San Segundo, E.; and Mompean, J. A. 2017. simplified vocal profile analysis protocol for the assessment of voice quality and speaker similarity. Journal of Voice, 31(5): 644 e11. Tan, X.; Qin, T.; Soong, F.; and Liu, T.-Y. 2021. survey on neural speech synthesis. arXiv preprint arXiv:2106.15561. Tseng, S.-C. 2003. Taxonomy of spontaneous speech phenomena in Mandarin conversation. In Proc. of ISCA & IEEE Workshop on Spontaneous Speech Processing and Recognition, 2326. Tseng, S.-C. 2013. Lexical coverage in Taiwan Mandarin conversation. In International Journal of Computational Linguistics & Chinese Language Processing, Volume 18, Number 1, March 2013. Wang, K.; and Herremans, D. 2024. DisfluencySpeech Single-Speaker Conversational Speech Dataset with Paralanguage. arXiv preprint arXiv:2406.08820. Ward, N. 2006. Non-lexical conversational sounds in American English. Pragmatics & Cognition, 14(1): 129182. Wu, H.; Wang, X.; Eskimez, S. E.; Thakker, M.; Tompkins, D.; Tsai, C.-H.; Li, C.; Xiao, Z.; Zhao, S.; Li, J.; et al. 2024. Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-To-Speech. In 2024 IEEE Spoken Language Technology Workshop (SLT), 690697. IEEE. Yang, Z.; Chen, Y.; Luo, L.; Yang, R.; Ye, L.; Cheng, G.; Xu, J.; Jin, Y.; Zhang, Q.; Zhang, P.; et al. 2022. Open source magicdata-ramc: rich annotated mandarin conversational (ramc) speech dataset. arXiv preprint arXiv:2203.16844. Zhang, H.; Yu, X.; and Lin, Y. 2023. NSV-TTS: Nonspeech vocalization modeling and transfer in emotional textto-speech. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE."
        }
    ],
    "affiliations": [
        "Guangzhou Quwan Network Technology",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}