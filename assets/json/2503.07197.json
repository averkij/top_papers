{
    "paper_title": "Effective and Efficient Masked Image Generation Models",
    "authors": [
        "Zebin You",
        "Jingyang Ou",
        "Xiaolu Zhang",
        "Jun Hu",
        "Jun Zhou",
        "Chongxuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models."
        },
        {
            "title": "Start",
            "content": "Zebin You 1 Jingyang Ou 1 Xiaolu Zhang 2 Jun Hu 2 Jun Zhou 2 Chongxuan Li 1 5 2 0 2 0 1 ] . [ 1 7 9 1 7 0 . 3 0 5 2 : r Abstract Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Frechet Inception Distance (FID). In particular, on ImageNet 256256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-ofthe-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512 512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models. 1. Introduction Masked modeling has proven effective across various domains, including self-supervised learning (He et al., 2022a; Bao et al., 2021; Devlin, 2018), image generation (Li et al., 2023; Chang et al., 2022; Li et al., 2024), and text generation (Sahoo et al., 2024; Shi et al., 2024; Lou et al., 2024a). In image generation, MaskGIT (Chang et al., 2022) introduced masked image generation, offering efficiency and quality improvements over autoregressive models but still lagging behind diffusion models (Ho et al., 2020; SohlDickstein et al., 2015; Song et al., 2020) due to information loss from discrete tokenization (Esser et al., 2021; Van Work done during an internship at Ant Group Project leaders 1Gaoling School of AI, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China 2Ant Group. Correspondence to: Chongxuan Li <chongxuanli@ruc.edu.cn>. Preprint. 1 Den Oord et al., 2017). MAR (Li et al., 2024) eliminated this bottleneck via diffusion loss, achieving strong results, yet key factors (e.g., masking schedule, loss function) remain underexplored. Moreover, with limited sampling steps (e.g., 16), its performance falls short of coarse-to-fine next-scale prediction model VAR (Tian et al., 2024). In parallel, masked diffusion models (Sahoo et al., 2024; Shi et al., 2024; Lou et al., 2024a; Ou et al., 2024) have shown promise in text generation, demonstrating scaling properties (Nie et al., 2024) similar to ARMs and offering principled probabilistic framework for training and inference. However, their applicability to image generation remains an open question. We propose unified framework integrating masked image modeling (Chang et al., 2022; Li et al., 2024) and masked diffusion models (Lou et al., 2024a; Sahoo et al., 2024; Shi et al., 2024), leveraging the strengths of both paradigms. This enables systematic exploration of training and sampling strategies to optimize performance. For training, we find that images, due to their high redundancy, benefit from higher masking ratio, simple weighting function inspired by MaskGIT and MAE (He et al., 2022a) tricks, improving generation quality. We also introduce CFG with Mask, replacing the fake class token with mask token for unconditional generation, further enhancing performance. For sampling, predicting fewer tokens in early stages improves results. However, early-stage guidance decreases variance, raising FID. To counter this, we propose time interval strategy for classifier-free guidance, applying guidance only in later stages. This maintains strong performance while significantly accelerating sampling by reducing NFEs. Building on our training and sampling improvements, we develop eMIGM and evaluate it on ImageNet (Deng et al., 2009) at 256 256 and 512 512 resolutions. As model parameters scale, eMIGM achieves progressively higher sample quality in predictable manner (Fig. 4(a)). Larger models further enhance efficiency, maintaining superior quality with similar training FLOPs and sampling time (Fig.4(b), Fig. 4(c)). Notably, eMIGM delivers high-quality samples with few sampling steps. On ImageNet 256 256, with similar NFEs and model parameters, it consistently outperforms VAR (Tian et al., 2024). Increasing NFE and model size, our best-performing eMIGM-H becomes comparable to stateEffective and Efficient Masked Image Generation Models Figure 1. Generated samples from eMIGM trained on ImageNet 512 512. of-the-art diffusion models like REPA (Yu et al., 2024) (FID 1.57 vs. 1.42)without requiring self-supervised features. On ImageNet 512 512, eMIGM-L surpasses EDM2 (Karras et al., 2024) while using only 60% of its NFEs, demonstrating efficiency and scalability. Qualitatively, eMIGM generates realistic and diverse images  (Fig. 1)  . In summary, our key contributions are as follows: We propose unified formulation to systematically explore the design space of masked image generation models, uncovering the role of each component. We introduce the time interval strategy for classifier-free guidance, maintaining high performance while significantly reducing sampling time. We surpass the state-of-the-art diffusion models on ImageNet 512 512 with only 60% of NFEs. We demonstrate that eMIGM benefits from scaling, with larger eMIGM models achieving greater efficiency. 2. Preliminaries 2.1. Masked Image Generation Let = [xi]N i=1 represent the discrete tokens of an image obtained via VQ encoder (Esser et al., 2021; Van Den Oord et al., 2017), and let [M] denote the special mask token. We consider two seminal masked image generation methods. MaskGIT (Chang et al., 2022) first extends the concept of masked language modeling from BERT (Devlin, 2018) (i.e., predicting masked tokens based on unmasked tokens) to image generation, achieving excellent performance with 2 Effective and Efficient Masked Image Generation Models low sampling cost (approximately 10 sampling steps) on ImageNet (Deng et al., 2009). However, its performance degrades when the number of sampling steps increases under its default mask schedule. During training, MaskGIT optimizes the cross entropy loss as follows. ratio is sampled from [0, 1], and based on the mask scheduling function γr, masked image xM is sampled from masking distribution qγr (xMx) that randomly masks γr tokens of as [M]. The loss function is then defined as: L(x) = ErU [0,1]Eqγr (xMx) (cid:88) log pθ (cid:0)xi xM (cid:1) . {ixi=[M]} (1) During sampling, MaskGIT starts with an image where all tokens are masked, x0. For each iteration {1, 2, . . . , }, the number of masked tokens is nt = γ , and the model receives input t1 . The model predicts the probabilities for all tokens, and the ˆnt = nt1 nt tokens with the highest confidence are unmasked, updating to . T MAR (Li et al., 2024) proposes using diffusion model (Sohl-Dickstein et al., 2015) to model the per-token distribution, which eliminates the need for discrete tokenizers. By avoiding the information loss of discrete tokenizers, MAR achieves excellent image generation performance. During training, MAR samples the masking ratio mr from truncated Gaussian distribution with mean 1.0, standard deviation 0.25, truncated to [0.7, 1.0]. For sampling, MAR adopts decoding strategy similar to that of MaskGIT. 2.2. Masked Diffusion Models Let = [xi]N i=1 represent the discrete text tokens of sentence, [M] denote the special mask token, and γt represent the mask schedule. MDMs (Lou et al., 2024b; Shi et al., 2024; Sahoo et al., 2024) gradually add masks to the data in the forward process and remove them during the reverse process. Here, we focus on the parameterized form of RADD (Ou et al., 2024). Given noise level [0, 1], the forward process of MDM is defined as adding noise independently in each dimension: qt0(xtx0) = 1 (cid:89) i=0 where qt0(xi txi 0), qt0(xi txi 0) = (cid:40) 1 γt, xi xi γt, = xi 0, = [M]. (2) (3) The training objective of MDM is to optimize the upper 3 bound of the negative log-likelihood of the masked tokens, which defined as: L(x0) = (cid:90) 0 γ γt Eq(xtx0) (cid:88) log pθ(xi 0xt) dt. {ixi t=[M]} (4) Interestingly, the explicit time input of MDM is theoretically redundant 1 (Ou et al., 2024), and has also been empirically validated in image generation (Hu & Ommer, 2024). During sampling, given two noise levels and t, where 0 < 1, the reverse process is characterized as: qst(xsxt) = 1 (cid:89) i= qst(xi sxt), (5) where qst(xi sxt) = 1, γs , γt γtγs γt 0, xi xi sxt), xi = xi t, xi = [M], xi = [M], xi = [M], = [M], = [M], q0t(xi otherwise. (6) 3. Unifying Masked Image Generation After removing the explicit time input from MDM, we observe that the MaskGIT objective (Eq. 1) can be expressed in terms of the general MDM loss formulation (Eq. 4). Specifically, the Monte Carlo expectation over in Eq. 1 is equivalent to integrating over from 0 to 1, where can be interpreted as scaled time variable corresponding to the masking schedule. In this reinterpretation, the masked image xM in MaskGIT can be understood as xt in the general framework, representing the noisy or partially masked image at time t. That is, the masking distribution qγr (xMx) can be mapped to specific instance of q(xtx0), characterized by the chosen mask scheduling function γt. See the equivalence between these two masking distributions in Appendix A. After aligning these two masking distributions, MaskGIT, MAR, and MDM can be expressed within unified loss function, defined as: L(x0) = w(t)Eq(xtx0) (cid:90) tmax tmin (cid:88) log pθ (cid:0)xi 0 xt (cid:1) dt. {ixi t=[M]} (7) 1Unlike continuous state diffusion which require both xt and as inputs to the model input to denoise, the mask discrete diffusion operates by using pθ(xi 0xt, t). Thats because the timestep dependence can be extracted as weight coefficient outside of the cross-entropy loss. 0xt) instead of pθ(xi Effective and Efficient Masked Image Generation Models Table 1. Comparison of different masked image modeling approaches through unified framework. The differences among these approaches are defined by the choice of masking distribution q(xtx0), weighting function w(t), and conditional distribution pθ(xi 0 xt). METHOD MASKING DISTRIBUTION q(xtx0) MASKGIT UNIFORMLY MASK γt TOKENS W/O REPLACEMENT MAR MDM UNIFORMLY MASK γt TOKENS W/O REPLACEMENT MASK TOKENS INDEPENDENTLY WITH RATIO γt WEIGHTING FUNCTION CONDITIONAL DISTRIBUTION w(t) w(t) = 1 w(t) = 1 w(t) = γ γt pθ(xi 0 xt) CATEGORICAL DISTRIBUTION DIFFUSION MODEL CATEGORICAL DISTRIBUTION In this unified formulation, the key differences between the models primarily lie in the three components outlined in Table 1. We explain these components as follows: Masking distribution q(xtx0). For MaskGIT and MAR, γt tokens are uniformly masked without replacement as [M]. For MDM, each of the tokens is masked with probability γt independently. Weighting function w(t). The weight function w(t) determines the importance of the loss at each time step. For MaskGIT and MAR, w(t) = 1; for MDM, w(t) = γ γt (cid:1). For MaskGIT and Conditional distribution pθ 0 xt (cid:1) is modeled (cid:0)xi MDM, the conditional distribution pθ as categorical distribution. In contrast, for MAR, we employ diffusion model assisted by latent variable z, leading to the following formulation: 0 xt (cid:0)xi . (cid:90) pθ(xi 0xt) = δθ1 (zixt)pdiff θ (xi 0zi)dzi. (8) Here, δθ1(zixt) represents the output of the mask pre0zi) donated the diction model with input xt, and pdiff θ2 output of diffusion model conditioned on zi. (xi 4. Investigating the Design Space of Training Building upon the unified framework, we now explore various design choices within this formulation. Given the equivalence of masking distributions, we adopt MDMs as the default setting. Furthermore, to mitigate the information loss introduced by the discrete tokenizer (Van Den Oord et al., 2017; Esser et al., 2021), we use diffusion model to model the conditional distribution pθ(xi 0xt). Our exploration begins with the standard MDM, which utilizes single encoder transformer architecture and linear mask schedule, in addition to using the diffusion model to model the conditional distribution pθ (cid:0)xi (cid:1). 0 xt Mask schedule. The first critical aspect of our exploration is the choice of γt, which determines the probability of masking each token during the forward process (See Appendix for details). We consider three mask schedules: (1) Linear: γt = t; (2) Cosine: γt = cos (cid:0) π 2 (1 t)(cid:1); (3) Exp: γt = 1 exp(5t). As shown in Fig. 2(a), the cosine schedule outperforms the linear schedule. We hypothesize that, due to the high information redundancy in images, the cosine schedule achieves higher mask ratio during training, providing stronger learning signals and leading to improved performance. The exp schedule further increases the mask ratio but destabilizes MDM training, likely due to the persistently large weighting function w(t), even at high mask ratios (see Fig. 5 for visualization of w(t) and γt). Weighting function. We consider two choices for w(t). (1) w(t) = γ , as used in MDM; (2) w(t) = 1, as used in γt MaskGIT. As shown in Fig. 2(b), w(t) = 1 yields better sample quality than w(t) = γ , similar to the phenomenon γt observed in DDPM (Ho et al., 2020). Additionally, we find that with w(t) = 1, training with the exp schedule is stable and achieves performance slightly better than the cosine schedule. Unless otherwise stated, we adopt w(t) = 1 and the exp schedule as the default for the rest of this work. Model Architecture. We consider two model architectures: (1) single-encoder transformer; (2) The MAE (He et al., 2022a) architecture, which decomposes the transformer into an encoder-decoder structure, where the encoder processes only unmasked tokens. The primary difference between these architectures is whether the encoder receives masked tokens as input. As shown in Fig. 2(c), under the exp schedule, the MAE architecture outperforms the single-encoder transformer. Interestingly, despite being originally designed for self-supervised learning, MAE retains its advantages in image generation. Therefore, unless otherwise specified, we adopt the MAE architecture as the default setting. Time Truncation. To achieve higher mask ratio during training, in addition to selecting more concave function for γt, we can also use time truncation, which restricts the minimum value of to tmin. We consider three choices: (1) tmin = 0, the original design; (2) tmin = 0.2; (3) tmin = 0.4. As shown in Fig. 2(d), we observed that an appropriate time truncation (tmin = 0.2) can be beneficial and accelerates training convergence. However, excessive truncation (tmin = 0.4, where over 80% of image tokens are masked during training) provides no benefit and may even degrade performance compared to no time truncation. Unless otherwise noted, we adopt tmin = 0.2 as the default setting. Effective and Efficient Masked Image Generation Models (a) Choices of mask schedule (b) Choices of weighting function (c) Use the MAE trick (d) Use the time truncation (e) Use CFG with mask Figure 2. Exploring the design space of training. Orange solid lines indicate the preferred choices in each subfigure. CFG with Mask. Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) is widely used for guiding continuous diffusion models and masked image generation. It combines outputs of conditional model (with class information) and an unconditional model (without class information) to improve alignment with the conditional output. In standard CFG, the unconditional model typically receives learnable fake class token as input. Inspired by unsupervised CFG (Nie et al., 2024), we propose variation of CFG in which the unconditional model instead receives special mask token as input, referred to as CFG with mask. As shown in Fig. 2(e), CFG with mask improves generation performance compared to standard CFG. Notably, here we use only simple conditional generation without guidance, our results suggest that using fake class token negatively impacts the conditional generation performance of MDM. Thus, we adopt CFG with mask as the default setting. 5. Investigating the Design Space of Sampling In the previous section, we carefully explore the training In the following sections, we investigate design space. the sampling design space. On one hand, we expect the models performance to improve as the number of mask prediction steps increases. On the other hand, we aim to maintain strong performance even with low number of mask prediction steps (e.g., 16). 5 5.1. Mask Schedule during Sampling During training, we observe that the exp schedule achieves the best performance. However, during sampling, different schedules may be employed. We are interested in identifying which mask schedule can achieve both of our goals. To this end, we first conduct simulation experiment (see details in Appendix B.2) to compare the number of tokens predicted during each mask prediction step across different mask schedules. We observe that the linear schedule predicts nearly constant number of tokens per step, while the cosine schedule predicts fewer tokens early in the process and progressively more later. The exp schedule predicts even fewer tokens initially, with more gradual increase as the process continues. As shown in Fig. 3(a), we observe that each mask schedule benefits more prediction steps. Moreover, for low mask prediction steps (e.g., 8 or 16), the exp schedule outperforms the cosine schedule, which in turn outperforms the linear schedule. This suggests that, in the early stages of sampling, predicting fewer tokens may contribute to improved performance at lower mask prediction steps. Therefore, unless specified, we default to using the exp schedule during the sampling stage. 5.2. The Sampling Method of Diffusion Loss We use the diffusion loss to model the distribution of (cid:1). Previously, we follow MAR (Li et al., 2024) pθ (cid:0)xi 0 xt Effective and Efficient Masked Image Generation Models (a) Choices of sample mask schedule (b) Use the DPM-Solver (c) Use the time interval Figure 3. Exploring the design space of sampling. For each plot, points from left to right correspond to an increasing number of mask prediction steps: 8, 16, 32, and up to 256. In each subfigure, DPM-Solver is donated as DPMS. (a) The exp schedule outperforms others by predicting fewer tokens early. (b) DPM-Solver performs better with fewer prediction steps. (c) The time interval maintains performance while reducing sampling cost for each mask prediction step, particularly for high mask prediction steps. (a) FLOPs vs. FID across model scales. (b) FLOPs vs. FID under different budgets. (c) Inference speed vs. FID. Figure 4. Scalability of eMIGM. (a) negative correlation demonstrates that eMIGM benefits from scaling. (b) Larger models are more training-efficient (i.e., achieving better sample quality with the same training FLOPs). (c) Larger models are more sampling-efficient (i.e., achieving better sample quality with the same inference time). and use DDPM (Ho et al., 2020) sampling method with 100 diffusion steps. Additionally, MAR employs the temperature τ sampling method from ADM (Dhariwal & Nichol, 2021) to scale the noise by τ , which requires careful tuning for optimal performance. In contrast, DPM-Solver (Lu et al., 2022a;b) is trainingfree, fast ODE sampler that accelerates the diffusion sampling process and converges faster with fewer steps. Interestingly, although DPM-Solver is designed for accelerating the diffusion process, we observe that, with low mask prediction steps, it outperforms DDPM, as shown in Fig. 3(b). For example, with 8 mask prediction steps, DPM-Solver achieves an FID of 6.6, while DDPM, with temperature of 1.0, achieves an FID of 10.6. We hypothesize that for low mask prediction steps, DDPM requires careful temperature tuning, whereas DPM-Solver, being an ODE sampler, does not require such adjustments. Moreover, DPM-Solver achieves good performance with fewer than 15 diffusion steps, while DDPM requires 100 diffusion steps. Therefore, unless specified, we default to DPM-Solver. 5.3. Time Interval for Classifier Free Guidance Previously, we adopt linear CFG schedule following MAR (Li et al., 2024), where the CFG value gradually increased from 0 to the target value during the mask prediction process. With constant CFG schedule, we find that the generation performance is highly sensitive to the CFG value, as shown in Fig. 7. We hypothesize that, for MDM, token generation is irreversibleonce token is generated, it cannot be modified. Therefore, strong guide in the early stages may reduce the variation in the results, leading to higher FID. This is similar to our earlier observation with the linear mask schedule, where generating too many incorrect tokens early can cause error accumulation and degrade the performance. We conduct an experiment with total of 256 sample tokens and 16 mask prediction steps (see details in Appendix C) to validate our hypothesis. Let si and ti denote the endpoint and start of the i-th step in the mask prediction process. We apply CFG if si [cfg tmin, cfg tmax]; otherwise, we use simple conditional generation. As shown in Fig. 8(a), when cfg tmin < cfg tmax 0.5, we achieve Effective and Efficient Masked Image Generation Models Table 2. Image generation results on ImageNet 256 256. denotes results taken from MaskGIT (Chang et al., 2022), and indicates results that require assistance from the self-supervised model. With 36% of function evaluations (NFE), eMIGM-H achieves performance comparable to the state-of-the-art diffusion model REPA (Yu et al., 2024). We bold the best result under each method and underline the second-best result. METHOD Diffusion models ADM-G (Dhariwal & Nichol, 2021) ADM-G-U (Dhariwal & Nichol, 2021) LDM-4-G (Rombach et al., 2022) VDM++ (Kingma & Gao, 2024) SimDiff (Hoogeboom et al., 2023) U-ViT-H/2 (Bao et al., 2023) DiT-XL/2 (Peebles & Xie, 2023) Large-DiT (Alpha-VLLM, 2024) Large-DiT (Alpha-VLLM, 2024) SiT-XL (Ma et al., 2024) DIFFUSSM-XL-G (Yan et al., 2024) DiffiT (Hatamizadeh et al., 2025) REPA (Yu et al., 2024) ARs VQGAN (Esser et al., 2021) VAR-d16 (Tian et al., 2024) VAR-d20 (Tian et al., 2024) VAR-d24 (Tian et al., 2024) VAR-d30 (Tian et al., 2024) NFE () FID () #Params METHOD NFE () FID () #Params 250 2 750 250 2 5122 5122 502 2502 2502 2502 2502 2502 2502 2502 256 102 102 102 102 4.59 3.94 3.60 2.40 2.44 2.29 2.27 2.10 2.28 2.06 2.28 1.73 1.42 18.65 3.30 2.57 2.09 1.92 554M 554M 400M 2B 2B 501M 675M 3B 7B 675M 660M 561M 675M 227M 310M 600M 1B 2B GANs BigGAN (Brock, 2018) StyleGAN-XL (Sauer et al., 2022) 1 12 Masked models MaskGIT (Chang et al., 2022) MAR-B (Li et al., 2024) MAR-L (Li et al., 2024) MAR-H (Li et al., 2024) Ours eMIGM-XS eMIGM-S eMIGM-B eMIGM-L eMIGM-H eMIGM-XS eMIGM-S eMIGM-B eMIGM-L eMIGM-H 8 2562 2562 2562 161.2 161.2 161.2 161.2 161.2 1281.4 1281.4 1281.35 1281.4 1281. 6.95 2.30 6.18 2.31 1.78 1.55 4.23 3.44 2.79 2.22 2.02 3.62 2.87 2.32 1.72 1.57 - - 227M 208M 479M 943M 69M 97M 208M 478M 942M 69M 97M 208M 478M 942M relatively low FID, supporting our hypothesis. In particular, the best performance is achieved when cfg tmin = 0.1 and cfg tmax = 0.3, using only 60% of the NFE (the number of function evaluations) compared to standard CFG. Specifically, for standard CFG, NFE = 16 2, while for the time interval, NFE 16 + 16 (0.3 0.1). As shown in Fig. 3(c), we observe that the time interval maintains performance at each mask prediction step while reducing sampling time. This demonstrates its efficiency and effectiveness. Therefore, we adopt the time interval for all subsequent experiments in this paper. 6. Experiments By fully considering the design space mentioned above, we evaluate eMIGM on ImageNet 256 256 and ImageNet 512 512 (Deng et al., 2009), benchmarking the sample quality using Frechet Inception Distance (FID) (Heusel et al., 2017). See experiment settings in Appendix D. 6.1. Larger Models Are Training and Sampling Efficient First, to demonstrate the scaling properties of eMIGM, we plot the FID-10K at 400 training epochs for different model sizes of eMIGM against training FLOPs. As shown in Fig. 4(a), we observe negative correlation between training FLOPs and FID-10K, indicating that eMIGM benefits from scaling. Second, for different model sizes of eMIGM, we scale the FLOPs and analyze the FID-10K in relation to training FLOPs. As shown in Fig. 4(b), for each model size of eMIGM, as training epochs and training FLOPs increase, performance also improves. Additionally, we observe that for the same training FLOPs, larger eMIGM models achieve better performance. For instance, eMIGM-L outperforms eMIGM-B with approximately 1020 FLOPs. Third, we observed the inference-time scaling behavior of eMIGM. As shown in Fig. 4(c), we plot the performance of different eMIGM model sizes across various mask prediction steps (ranging from 16 to 256). The speed is measured using single A100 GPU with batch size of 256. We observe that as the number of prediction steps increases, each model size of eMIGM achieves better performance, particularly for smaller models (i.e., eMIGM-XS and eMIGM-S). For larger model sizes, similar best performance is reached with just 64 steps. Additionally, we also find that larger eMIGM models achieve better performance while maintaining similar inference speeds. For example, at speed of about 0.2 seconds per image, eMIGM-L achieves strong FID of 1.8, outperforming eMIGM-B with an FID of 2.3. 6.2. Image Generation on ImageNet In Tab. 2, we compare eMIGM with state-of-the-art generative models on ImageNet 256 256. By exploring the design space of sampling, eMIGM with few NFEs (approximately 20) outperforms VAR (Tian et al., 2024) with similar model size. Specifically, eMIGM-B achieves an FID of 2.79 with only 208M parameters, while VAR-d16 achieves an FID of 3.30 with 310M parameters. Notably, as we increase the NFE, all of our models consistently show Effective and Efficient Masked Image Generation Models Table 3. Image generation results on ImageNet 512 512. denotes results taken from MaskGIT (Chang et al., 2022). With 20 function evaluations (NFE), eMIGM-L outperforms strong visual autoregressive models VAR (Tian et al., 2024). When the NFE increases to 80, eMIGM-L surpasses the state-of-the-art diffusion model EDM2 (Karras et al., 2024). We bold the best result under each method and underline the second-best result. NFE () FID () #Params METHOD NFE () FID () #Params METHOD Diffusion models ADM-G (Dhariwal & Nichol, 2021) ADM-G-U (Dhariwal & Nichol, 2021) VDM++ (Kingma & Gao, 2024) SimDiff (Hoogeboom et al., 2023) U-ViT-H/4 (Bao et al., 2023) DiT-XL/2 (Peebles & Xie, 2023) Large-DiT (Alpha-VLLM, 2024) SiT-XL (Ma et al., 2024) EDM2-XXL (Karras et al., 2024) 250 2 750 5122 5122 502 2502 2502 2502 63 Consistency models sCT-XXL (Lu & Song, 2024) sCD-XXL (Lu & Song, 2024) GANs BigGAN (Brock, 2018) StyleGAN-XL (Sauer et al., 2022) 2 2 1 1 7.72 3.85 2.65 3.02 4.05 3.04 2.52 2.62 1.81 3.76 1.88 8.43 2.41 559M 559M 2B 2B 501M 675M 3B 675M 1.5B 1.5B 1.5B - - ARs VQGAN (Esser et al., 2021) VAR-d36-s (Tian et al., 2024) 1024 102 26.52 2.63 227M 2.3B Masked models MaskGIT (Chang et al., 2022) MAR (Li et al., 2024) 12 2562 Ours eMIGM-XS eMIGM-S eMIGM-B eMIGM-L eMIGM-XS eMIGM-S eMIGM-B eMIGM-L 161.2 161.2 161.2 161. 641.25 641.25 641.25 641.25 7.32 1.73 4.63 3.65 2.78 2.19 4.45 3.29 2.31 1.77 227M 481M 104M 132M 244M 478M 104M 132M 244M 478M significant improvements in generation performance. For instance, eMIGM-L achieves an FID of 1.72 with 180 NFEs, compared to an FID of 2.22 with 20 NFEs. By increasing the NFE, eMIGM-L, despite having only 478M parameters, outperforms the best VAR-d30, which achieves an FID of 1.92 with 2B parameters. Lastly, our more powerful eMIGM-H achieves an FID of 1.57 with just 180 NFEs, outperforming strong diffusion models such as Large-DiT (Alpha-VLLM, 2024) and DiffiT (Hatamizadeh et al., 2025). eMIGM-H is also comparable to the best diffusion models REPA (Yu et al., 2024), which require 500 sequential steps and the assistance of the self-supervised model. We also evaluate eMIGM on higher resolution images (i.e., 512 512) in Tab. 3. Specifically, with similar NFEs, eMIGM-L (with only 478M parameters) achieves an FID of 2.19, outperforming the strong generative model VAR (Tian et al., 2024) (with 2.3B parameters), which achieves an FID of 2.63. Furthermore, with only about 60% of the NFE required by the best diffusion model EDM2 (Karras et al., 2024), eMIGM-L achieves an FID of 1.77, outperforming EDM2s FID of 1.81. These quantitative results demonstrate that eMIGM achieves excellent generation performance and high sampling efficiency across diverse resolutions. 2021; Tian et al., 2024; Sun et al., 2024). The most related works to our study are MaskGIT (Chang et al., 2022) and MAR (Li et al., 2024). We provide unified framework that integrates both approaches and systematically explore the impact of each component. Additionally, guidance interval (Kynkaanniemi et al., 2024) also restricts guidance to specific range of noise levels. However, unlike our proposed time interval, which applies guidance at the token level, guidance interval operates at different noise levels of the entire image. In contrast, our time interval method applies guidance to specific tokens during image generation. Masked discrete diffusion models. Recently, masked discrete diffusion models (Austin et al., 2021; Campbell et al., 2022), special case of discrete diffusion models (SohlDickstein et al., 2015; Hoogeboom et al., 2021), have achieved remarkable progress in various domains, including text generation (He et al., 2022b; Lou et al., 2024a; Shi et al., 2024; Sahoo et al., 2024; Ou et al., 2024; Zheng et al., 2023; Chen et al., 2023; Gat et al., 2024; Nie et al., 2024), music generation (Sun et al., 2023), protein design (Campbell et al., 2024), and image generation (Hu & Ommer, 2024). 8. Conclusion 7. Related Work Visual generation. Modern visual generation models primarily fall into four categories: GANs (Goodfellow et al., 2014; Brock, 2018; Sauer et al., 2022), diffusion models (Song et al., 2020; Sohl-Dickstein et al., 2015; Ho et al., 2020), masked prediction models (Chang et al., 2022; Li et al., 2023; 2024), and autoregressive models (Esser et al., In this paper, we present single framework to unify masked image generation models and masked diffusion models and carefully examine each component of design space to achieve efficient and high-quality image generation. Empirically, we demonstrate that eMIGM can achieve comparable performance with the state-of-the-art continuous diffusion models with fewer NFEs. We believe that eMIGM will inspire future research in masked image generation. 8 Effective and Efficient Masked Image Generation Models"
        },
        {
            "title": "Impact Statement",
            "content": "We introduce eMIGM, powerful generative model that significantly accelerates the sampling speed while maintaining high image quality. However, this increased efficiency may increase the potential for misuse of generated images. To mitigate this, watermarks can be embedded into the generated images without affecting the generation quality, helping to prevent misuse and verify if an image is generated."
        },
        {
            "title": "References",
            "content": "Alpha-VLLM. Large-dit-imagenet. https://github. com/Alpha-VLLM/LLaMA2-Accessory/tree/ main/Large-DiT-ImageNet, 2024. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, 2021. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Devlin, J. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Gat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Synnaeve, G., Adi, Y., and Lipman, Y. Discrete flow matching. NeurIPS, 2024. Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2266922679, 2023. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Bao, H., Dong, L., Piao, S., and Wei, F. Beit: Bert arXiv preprint pre-training of image transformers. arXiv:2106.08254, 2021. Brock, A. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Campbell, A., Benton, J., Bortoli, V. D., Rainforth, T., Deligiannidis, G., and Doucet, A. continuous time framework for discrete denoising models. In Advances in Neural Information Processing Systems, 2022. Campbell, A., Yim, J., Barzilay, R., Rainforth, T., and Jaakkola, T. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design, 2024. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H., Li, M., Lu, Y., and Han, S. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. Chen, Z., Yuan, H., Li, Y., Kou, Y., Zhang, J., and Gu, Q. Fast sampling via de-randomization for discrete diffusion models. arXiv preprint arXiv:2312.09193, 2023. Hatamizadeh, A., Song, J., Liu, G., Kautz, J., and Vahdat, A. Diffit: Diffusion vision transformers for image generation. In European Conference on Computer Vision, pp. 3755. Springer, 2025. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022a. He, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022b. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hoogeboom, E., Nielsen, D., Jaini, P., Forre, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions. NeurIPS, 34:1245412465, 2021. 9 Effective and Efficient Masked Image Generation Models Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pp. 1321313232. PMLR, 2023. Hu, V. T. and Ommer, B. [mask] is all you need, 2024. URL https://arxiv.org/abs/2412.06787. Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S. Analyzing and improving the training In Proceedings of the dynamics of diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024. Kingma, D. and Gao, R. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. Li, T., Chang, H., Mishra, S., Zhang, H., Katabi, D., and Krishnan, D. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21422152, 2023. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution, 2024a. Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024b. Lu, C. and Song, Y. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Nie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M., and Li, C. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Sahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Sun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Scorebased continuous-time discrete diffusion models. In The Eleventh International Conference on Learning Representations, 2023. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Effective and Efficient Masked Image Generation Models Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Yan, J. N., Gu, J., and Rush, A. M. Diffusion models without attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8239 8249, 2024. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Zheng, L., Yuan, J., Yu, L., and Kong, L. reparameterized discrete diffusion model for text generation. ArXiv, abs/2302.05737, 2023. Effective and Efficient Masked Image Generation Models Table 4. Mask schedule formulations. Mask schedule γt"
        },
        {
            "title": "Linear\nCosine\nExp",
            "content": "t 2 (1 t)(cid:1) π cos (cid:0) π 1 exp(5t) γ γt 1 2 tan (cid:0) π 2 (1 t)(cid:1) 5 exp(5t) 1exp(5t) A. Equivalence of the masking strategies of MaskGIT and MDM In this section, we demonstrate that the masking strategies of MaskGIT and MDM are equivalent in expectation. MaskGIT first samples ratio from [0, 1] and then uniformly masks γr tokens of as [M]. In contrast, for MDM, each token is independently masked as [M] with probability γt. First, for MDM, the cross-entropy loss in Equation (4) has multiple equivalent forms (Ou et al., 2024). To facilitate better understanding, we reformulate Equation (4) as an expectation over t: L(x0) = EtU [0,1]Eq(xtx0) γ γt (cid:88) {ixi t=[M]} log pθ(xi 0xt) . (9) As an example, we consider the linear mask schedule, where γt = t. In this formulation, the forward process involves independently masking each token based on uniformly sampled t. Under this setting, the loss simplifies to: L(x0) = EtU [0,1]Eq(xtx0) 1 (cid:88) {ixi t=[M]} log pθ(xi 0xt) . (10) For MaskGIT, the number of masked tokens is sampled from uniform distribution [1, ], after which tokens in x0 are randomly masked as [M]. Under this scheme, the loss function can be rewritten as: L(x0) = ElU [1,N ]Eq(xlx0) 1 (cid:88) {ixi =[M]} log pθ(xi 0xl) . (11) As shown in Ou et al. (2024), Equation (11) and Equation (10) are equivalent in expectation. In this paper, we adopt the formulation of Equation (4) with an exponential mask schedule as the default setting. B. Mask schedules B.1. Formulations and Illustrations of Mask Schedules We present different choices of mask schedules in Fig. 5 and Tab. 4. The linear schedule achieves the best empirical performance in text generation, as demonstrated in previous work (Lou et al., 2024b; Sahoo et al., 2024; Shi et al., 2024). In comparison to the linear schedule, the cosine and exp schedules mask more tokens during the forward process of MDM. B.2. Sampling Simulator Experiment During sampling, we conducted simulation experiment with total of 256 sample tokens and 16 sampling steps. Therefore, the temporal interval [0, 1] is discretized into 16 equally sized segments for sampling purposes. Let si and ti represent the endpoint and starting point of the i-th segment, respectively, where {1, 2, . . . , 16}. The indexing is defined such that t1 corresponds to the start of the first segment. Specifically, the endpoints are defined as si = 16i 16 and the starting points as γti γsi ti = 16i+1 , as given by Equation (6). γsi . In each step i, the prediction for each token is made with probability of 16 12 Effective and Efficient Masked Image Generation Models Figure 5. Different choices of mask schedules. Left: γt (i.e., the probability that each token is masked during the forward process). Right: Weight of the loss in MDM. Figure 6. Comparison of mask removal for different sample mask schedule. We simulated the process 10,000 times and calculated the average number of tokens predicted in each step. The experimental results are shown in Fig. 6. We observed the following trends: For the linear schedule, the model predicts almost the same number of tokens in each step. In contrast, for the cosine schedule, the model predicts fewer tokens in the earlier steps and more tokens in the later steps. Compared to the cosine schedule, the exp schedule predicts even fewer tokens in the earlier steps and progressively more tokens in the later steps. C. Time Interval for Classifier Free Guidance To validate our hypothesis that an excessively strong guide in the early stages may drastically reduce the variation in generated samples, leading to higher FID, we conducted an experiment with total of 256 sample tokens and 16 sampling steps. more detailed description of the sampling procedure can be found in Appendix B.2. Let si and ti represent the endpoint and starting point of the i-th sampling step, respectively. We define tmin and tmax for CFG. If si [tmin, tmax], we apply CFG to guide the sampling; otherwise, we do not use CFG and rely solely on simple conditional generation. As shown in Fig. 8, we observe that when tmin = 0 and tmax = 1, the FID value is 22.48, demonstrating low variation in the generated samples. Additionally, in the top left corner of Fig. 8(a) (i.e., when tmin < tmax 0.5), we achieve relatively low FID (indicating higher variation), which supports our hypothesis and encourages the application of CFG guidance only during the later stages of sampling. 13 Effective and Efficient Masked Image Generation Models (a) CFG vs. FID (b) CFG vs. IS Figure 7. Generation performance is sensitive to the CFG value when using the constant schedule. (a) FID vs. Time interval (b) IS vs. Time interval Figure 8. Performance across different time intervals. Subplots show (a) FID and (b) Inception Score(IS). Table 5. The code links and licenses. Method MAR DPM-Solver DC-AE Link https://github.com/LTH14/mar https://github.com/LuChengTHU/dpm-solver License MIT License MIT License https://github.com/mit-han-lab/efficientvit Apache-2.0 license D. Experiment settings and results We implement eMIGM upon the official code of MAR (Li et al., 2024), DC-AE (Chen et al., 2024), DPM-Solver (Lu et al., 2022a;b), whose code links and licenses are presented in Tab. 5. Image Tokenizer. For ImageNet 256 256, we use the same KL-16 image tokenizer as in MAR (Li et al., 2024), which has stride of 16. That is, for an image of size 256 256, it outputs an image token sequence of length 16 16, with each 14 Effective and Efficient Masked Image Generation Models Table 6. Training configurations of models on ImageNet 256256."
        },
        {
            "title": "Model Size",
            "content": "XS H"
        },
        {
            "title": "Architecture Configurations",
            "content": "Transformer blocks Transformer width MLP blocks MLP width Params (M) 20 448 3"
        },
        {
            "title": "Training Hyperparameters",
            "content": "24 512 3 1024 97 24 768 6 1024 208 32 1024 8 1280 478 40 1280 12 1536 942 Epochs Learning rate Batch size Adam β1 Adam β2 800 4.0e-4 1024 0.9 0. 800 4.0e-4 1024 0.9 0.95 800 8.0e-4 2048 0.9 0.95 800 8.0e-4 2048 0.9 0.95 800 8.0e-4 2048 0.9 0.95 Table 7. Training configurations of models on ImageNet 512512. Model Size XS Architecture Configurations Transformer blocks Transformer width MLP blocks MLP width Params (M) 20 448 6 1280 104 Training Hyperparameters 24 512 6 1280 132 24 768 8 1280 244 32 1024 8 1280 478 Epochs Learning rate Batch size Adam β1 Adam β 800 4.0e-4 1024 0.9 0.95 800 4.0e-4 1024 0.9 0.95 800 8.0e-4 2048 0.9 0.95 800 8.0e-4 2048 0.9 0.95 token having dimensionality of 16. For ImageNet 512 512, we use the DC-AE-f32 tokenizer (Chen et al., 2024) for efficiency, which has stride of 32, and each token has dimensionality of 32. Classifier-Free Guidance (CFG). In the original CFG, during training, the class condition is replaced with fake class token with probability of 10%. During sampling, the prediction model takes both the class token and the fake class token as input, generating outputs zc and zu. Conceptually, CFG encourages the generated image to align more closely with the result conditioned on zc while deviating from the result conditioned on zu. For CFG with Mask, we replace the fake class token with masked token as the input for unconditional generation. We use constant CFG schedule and the time interval strategy in our main results presented in Tab. 2 and Tab. 3, achieving excellent performance while significantly reducing the sampling cost. Moreover, we observed that with the time interval strategy, we can use consistently high CFG value to guide generation at each prediction step, eliminating the need for CFG value sweeping. Training Settings. The detailed training settings for ImageNet 256 256 and ImageNet 512 512 are provided in Tab. 6 and Tab. 7, respectively."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Beijing Key Laboratory of Big Data Management and Analysis Method",
        "Gaoling School of AI, Renmin University of China"
    ]
}