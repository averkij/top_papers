{
    "paper_title": "RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation",
    "authors": [
        "Fengyi Wu",
        "Yimian Dai",
        "Tianfang Zhang",
        "Yixuan Ding",
        "Jian Yang",
        "Ming-Ming Cheng",
        "Zhenming Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet++, a sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds a relaxed RPCA model into a structured network comprising a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background feature preservation, while a Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets a new baseline for reliable and interpretable sparse object segmentation. Codes are available at our Project Webpage https://fengyiwu98.github.io/rpcanetx."
        },
        {
            "title": "Start",
            "content": "RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation Fengyi Wu, Yimian Dai, Tianfang Zhang, Yixuan Ding, Jian Yang, Ming-Ming Cheng, Zhenming Peng 1 AbstractRobust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet++, sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds relaxed RPCA model into structured network comprising Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce Memory-Augmented Module (MAM) to enhance background feature preservation, while Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets new baseline for reliable and interpretable sparse object segmentation. Codes are available at our Project Webpage. 5 2 0 2 Index TermsDeep Unfolding Networks, Interpretability, Low-rank and Sparse Decomposition, Sparse Object Segmentation. 6 ] . [ 1 0 9 1 4 0 . 8 0 5 2 : r a"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Over the past decades, robust principal component analysis (RPCA), as an extension of PCA, has received extensive attention due to its robust representation of outliers. An observed matrix comprises low-rank matrix with redundant features and sparse matrix with distinct objects. Such decomposition structure benefits massive research fields [1] such as low-level vision tasks like image restoration [2] and denoising [3] or high-level vision tasks such as fore/background subtraction [4], image classification [5], and is particularly useful for various practical image segmentation tasks (e.g., defect detection, vessel subtraction, and infrared small target segmentation): by regrading the background with redundant information as and segmented object as the sparse components as: = + (1) Such models are formulated as either convex [6], [7] or nonconvex [8], [9], [10] optimization problems, typically solved using augmented Lagrangian methods [11], proximal gradient descent [12], or alternating direction minimization [13]. To enhance image segmentation performance, these models incorporate various constraints, including object constraints [14], [15], [16], background constraints [17], [18], [19], and field priors [20], [21], [22], among others. However, despite their theoretical appeal, existing models face two key limitations: F. Wu, Y. Ding, and Z. Peng are with the School of Information and Communication Engineering and the Laboratory of Imaging Detection and Intelligent Perception, University of Electronic Science and Technology of China, Chengdu, China. (E-mail: wufengyi98@163.com; 2840046d@student.gla.ac.uk; zmpeng@uestc.edu.cn). Y. Dai, M. Cheng, and J. Yang are with PCA Lab, VCIP, College of Computer Science, Nankai University, Tianjin 300350, China. (e-mail: yimian.dai@gmail.com; cmm@nankai.edu.cn; csjyang@nankai.edu.cn). Tianfang Zhang is with the Department of Automation, Tsinghua University, Beijing, China. (e-mail: sparkcarleton@gmail.com). This research was supported by NSFC ( No. 61775030, No.61571096, No. 62301261, No. U24A20330, No. 62361166670, No. 62225604), Shenzhen Science and Technology Program (JCYJ20240813114237048), and Natural Science Foundation of Sichuan Province of China (Grant No. 2025ZNSFSC0522). Manuscript submitted at June, 2025. Computational cost to convergence: Repeated use of costly matrix or tensor operations slows convergence and hinders realtime deployment, especially in memory-constrained settings. Limited generalizability: (1) Heavy reliance on manually tuned hyperparameters leads to performance drops across diverse scenarios; (2) Rigid priors restrict adaptability to different domains, limiting broader applicability. Recent progress in deep neural networks (DNNs) and the availability of large-scale public datasets have advanced object segmentation with greater adaptability to emerging data. Architectures such as FPNs [23], U-Nets [24], Transformers [25], and segment anything model [26] are widely adopted. However, their empirical designs often compromise interpretability, rendering them black boxes whose outputs may be unreliable without domain-specific context. To address the interpretability challenges of DNNs, deep unfolding networks (DUNs) have emerged as distinctive class of model-based approaches. By unrolling iterative optimization algorithms into structured deep networks, DUNs have recently found applications across diverse inverse problems, including compressive sensing [27], [28], as well as image denoising [29], [30], super-resolution [31], [32], and image restoration [33], [34]. Despite growing interest in deep unfolded RPCA models [35], their application to segmentation remains limited. Adapting optimization-based frameworks to high-level vision tasks poses several challengesparticularly in handling key matrix operations such as singular value decomposition (SVD) and softthresholding (ST) under dynamic conditions. While prior studies have proposed variants of SVD/ST and improved initialization strategies [35], [36], efficiently managing complex matrix computations and selecting learnable parameters remains an open issue [37]. Furthermore, although emerging datasets offer ground truth for sparse components, effective supervision of the intermediate low-rank component remains largely unsolved. Furthermore, RPCA can be viewed as decoupling task, where improving low-rank background estimation enhances object detection, and vice versa. However, in DUNs, this reciprocal 2 Fig. 1. Overview of the proposed RPCANet++ architecture. A. Model the given image within relaxed RPCA scheme and transform it into an unconstrained optimization problem. B. Iteratively solves the model above with closed-form solutions; Consider two high-level issues with corresponding solutions. C. Unfold the solutions in deep unfolding framework; typically, RPCANet++ are assisted with memory-augmented modules and deep target priors. D. Visual and numerical model verifications via post-hoc techniques present overall interpretability. benefit is often weakened by stage-wise transmission loss [38], leading to background misestimation and object omission. While inter-stage operations such as summation or concatenation help mitigate these issues [39], they remain insufficient for adaptively preserving background features. In addition, unlike general restoration tasks, segmentation-focused optimization methods typically introduce saliency priors [40] to highlight object regions and speed up convergence. Inspired by such strategies, we propose incorporating domain factor to guide convergence and enhance segmentation performance. Motivated by the challenges discussed above, we propose RPCANet++an interpretable segmentation framework that bridges deep unfolding with high-level vision operations. As shown in Fig. 1, RPCANet++ unfolds relaxed RPCA model into three modules: Object Extraction (OEM), Background Approximation (BAM), and Image Restoration (IRM). Instead of relying on costly matrix computations and handcrafted parameters, we employ theoretically constrained neural networks, treating the low-rank background as latent element. Through objectbackground merging, the task jointly performs sparse object extraction and image restoration. Unlike conventional segmentation frameworks, RPCANet++ avoids feature loss from repeated downsampling and adapts well to diverse domains. To address feature degradation across stages, we introduce MemoryAugmented Module (MAM) that reinforces background information adaptively. In parallel, we design Deep Contrast Prior Module (DCPM), inspired by reweighted optimization, to guide object allocation. To verify that our model adheres to RPCA principles, we employ stage-wise low-rankness and sparsity metrics  (Fig. 1)  to evaluate interpretability. RPCANet++ thus integrates optimization-driven transparency with high-level learning flexibility, delivering reliable and generalizable segmentation. The main contributions are summarized as follows: 1) We propose RPCANet++, novel sparse object segmentation model that unfolds the traditional RPCA framework into deep network, combining the interpretability of model-driven methods with the generalizability of data-driven learning. 2) We design background approximation module using nonlinear proximal networks to estimate background content without costly matrix operations, supported by cross-stage memory augmentation mechanism. Sparse objects are extracted through Lipschitzconstrained neural representation, complemented by an 3) adaptive local contrast prior inspired by traditional saliency cues to enhance segmentation accuracy. 4) Beyond segmentation, we integrate low-rank background and sparse components to complete an image restoration task, leveraging the robustness of the decomposition-based structure. 5) We validate RPCANet++ via quantitative metrics and interpretability analyses. Experiments across diverse datasets demonstrate strong generalization and consistent outperformance over state-of-the-art methods. precursor to this work was presented at conference [41]. This article extends that foundation with substantial advancements: 1) To solve the background transmission loss, we introduce memory-augmented module in the BAM to enhance the feature restoration. 2) Besides, inspired by prior assistants in mainstream optimization methods, we design saliencyinspired prior DCPM to accelerate object extraction; 3) We provide more profound analysis of RPCANet++ with novel metrics like the measurement of low rankness and sparsity, increasing the post hoc [42] interpretability of our module. 4) We provide more results on the sparse object segmentation tasks, including infrared small target detection (IRSTD) [43], vessel segmentation (VS [44], [45], [46], and defect detection (DD) [47], [48] to prove the generality of our method. We hope this efficient and interpretable architecture can be new baseline in future sparse object segmentation research."
        },
        {
            "title": "2 RELATED WORK\n2.1 RPCA for Sparse Object Segmentation",
            "content": "Ever since the introduction of the RPCA model [6], it has been extensively applied across various domains [1]. For low-level image processing tasks, most RPCA-based methods focus on inverse problems [49], [50], [51], where sparse elements are treated as noise to be removed. In contrast, high-level vision tasks reinterpret sparse elements as informative outliersgrayscale regions highlighting foreground objects [36]. This perspective has driven progress in foreground/background separation (BFS) [4], [52], [53] and salient object detection (SOD) [15], [20], [54]. However, while such images often exhibit low-rank structure (Fig. 2(a)), the foreground regions are typically not truly sparse; their average area frequently exceeds 20% [55], [56], [57], and even surpasses 40% in some datasets [58], [59]. Consequently, classical RPCA methods 3 pattern, traditional iterative procedures such as alternating direction methods of multipliers (ADMM) [27], approximate message passing (AMP) [78], and inertial proximal algorithm for nonconvex optimization (iPiano) [79] are unrolled into deep networks. Considering the merit of interpretability and efficacy, scholars attempted to apply the DUN scheme in low-rank and sparse decomposition (or RPCA) problems, which shows great adaptive capacity in clutter suppression [80], medical image restoration [81], multi-spectral and hyper-spectral image fusion [82], etc. Optimization-based sparse object segmentation task, as an RPCA-driven method, can theoretically fit into the deep unfolding paradigm. From another perspective, although DUNs have proven effective and interpretable in various image inverse tasks, passing single-channel outputs across independent stages can lead to information degradation [37], [38]. To address this, prior works proposed strategies such as cascading accumulation [39], contextual memory blocks [83], and inter/intra-stage memory fusion [84]. Inspired by these efforts, we adopt memory-augmented method to mitigate background transmission loss. Furthermore, DUNs architectural flexibility [85] allows for integrating taskoriented priors [86], [87], [88], [89] to enhance iterative optimization. In our design, drawing from local contrast mechanisms [90] and the acceleration effects of priors in traditional optimization [40], we embed Deep Contrast Prior Module (DCPM) to improve segmentation accuracy while maintaining interpretability."
        },
        {
            "title": "3.1 Formulation of Optimization Problem",
            "content": "In the context of segmentation-oriented RPCA tasks, our objective is to estimate low-rank background Rmn and extract the sparse object matrix Rmn. For an image Rmn, we transform the segmentation model (1) into the following optimization framework: min B,O rank(B) + λ O0 s.t. = + , (2) where we signify λ as trade-off coefficient, and the term 0 denotes the l0-norm, which is defined as the count of non-zero elements within matrix. However, addressing (2) presents an NP-hard challenge due to the non-convex and discontinuous nature of both the rank function and the l0-norm, which is often solved by the application of Principal Component Pursuit (PCP) [7]: + λO s.t. = + , (3) min B,O where the term represents the nuclear norm, which is the sum of the singular values of matrix. Meanwhile, the notation 1 refers to the l1-norm, defined as the sum of the absolute values of the matrixs entries. And such model is widely adopted by traditional segmentation tasks [67], [91]. However, when facing complex scenarios, the background can exhibit varying degrees of complexity, rendering solitary nuclear norm or rank function insufficient for encapsulating the practical constraints. Similarly, the sparsity of object elements can vary, making the exclusive use of the l0 or l1-norm potentially inadequate [92]. Consequently, we propose more generalized formulation of the problem. Here, we employ R(B) and S(O) as constraints that incorporate prior knowledge of the background and object images, individually: min B,O R(B) + λS(O) s.t. = + . (4) Furthermore, to alleviate the computational complexity associated with variable updates due to augmented Lagrange Fig. 2. (a) Low-rankness evaluation of typical object segmentation tasks solved via RPCA, including BFS (CDNet2014 [73]), SOD (MSRA-B [57]), DD (SD-saliency-900 [48]), VS (DRIVE [74]), and IRSTD (SIRST-Aug [75]). Grayscale datasets exhibit low-rank properties, evidenced by the rapid decay of singular values in early ranks (detailed in Section 4.1.3). The largest singular value is annotated around each rank, with the last three tasks showing superior low-rankness (SIRST-Aug being the lowest-rank). (b) Objects average area of each dataset: Summary of nine sparse object segmentation datasets (IRSTD, VS, and DD) with diverse average object area distributions analyzed in this study. falter under these conditions, prompting shift toward using sparse components as downstream priorsultimately complicating high-level segmentation. Interestingly, recent studies leveraging RPCA-based segmentation frameworks have revealed that when objects are genuinely sparseoccupying minimal area and exhibiting well-defined contours relative to the low-rank backgroundoptimization techniques prove more effective than filteror contrast-based methods. Consequently, researchers have refined RPCA models by incorporating domain-specific features. For example, in defect detection (DD, 10%; Fig. 2(b)), low-rank and sparse decomposition provide foundation for implementing texture features, intrinsic priors, and energy constraints [48], [60], [61], [62]. In medical imaging (5%), RPCA-based approaches focus on segmenting sparse vessels (VS) and enhancing them through intensity and edge-preservation priors [63], [64], [65]. Additionally, for infrared small target detection, where sparse objects constitute less than 0.15% of the image area [66], RPCA frameworks have gained prominence. Since the pioneering work of [67], subsequent studies have advanced this field by improving background correlation [68], [69], [70] and incorporating sparse target constraints [40], [71], [72]. In summary, RPCA provides robust backbone for sparse object segmentation, with researchers integrating domain priors to enhance convergence speed and detection performance. Nevertheless, existing methods face significant challenges, including convergence inefficiencies and initialization complexities [48], which hinder their broader applicability due to computational demands and limited generalizability. In this article, the proposed RPCANet++ aims to solve these challenges by integrating the interpretable RPCA model in learnable and dynamic way to segment sparse objects."
        },
        {
            "title": "2.2 Deep Unfolding Networks",
            "content": "Deep unfolding networks (DUN), also known as deep unrolling, integrate neural networks into iterative updates in optimization schemes and have achieved notable success in image inverse problems. Gregor and LeCun [76] pioneered such technique in sparse coding within learned iterative shrinkage-thresholding algorithm (LISTA). Inspired by this work, Zhang and Ghanem designed ISTA-Net [77], which replaces sparse transformation with nonlinear neural networks. Similar to the deep unrolling 4 Fig. 3. RPCANet++ framework unfolds iterative model-driven closed-form equations in deep network design and comprises corresponding stages. Transmissive elements are presented in different colors: for the restoration image, for the low rank background, for the sparse object matrix, ρ for the learnable parameter, and [Bh, Bc] for the latent background features. The construction of each stage is depicted in Fig. 4. multipliers [40], we opt for more straightforward and intuitive approach. By employing the l2-norm, convert the constrained optimization problem into an unconstrained format [93]: , (5) where penalty coefficient µ is introduced, and Frobenius norm (F-norm) is utilized. The F-norm of matrix is given L(B, O) = R(B) + λS(O) + O2 µ 2 (cid:80) j=1 Xij2 . Leveraging (5), the background and object by: (cid:115) (cid:80) i=1 components are optimized iteratively."
        },
        {
            "title": "3.2.1 B∗ Updates",
            "content": "For background updating, the sub-problem is articulated as: µ 2 R(B) + = arg min (6) As depicted in (3), traditional segmentation methods typically define R(B) as B, thereby transform (6) to combination of the nuclear norm and the l2 norm. An analytical solution to this problem is known and can be expressed as follows: + F . = Dµ(D O) , (7) where the operator Dµ() represents the Singular Value Thresholding (SVT) [94] with threshold of µ. Addressing (7) entails SVD, which is computationally intensive and impacts precision. However, once DUNs mimic this process via neural networks, which necessitates SVD at each iteration, it will lead to efficiency and accuracy concerns. The initialize method, proposed by [35], involves decomposing matrix into two matrices through best rank-r approximation SVD, followed by separate updates. Despite this, SVDs computational challenges persist. Alternative approaches [95] involve matrix decomposition under varying rank constraints, enhancing interpretability through ranks and dimensions. Yet, these require manual rank selection and may overlook the inherent features of images in neural layers. Rather than using the nuclear norm and engaging in complex SVD computations, we simplify the process by implementing constraint function R(B). Additionally, we utilize proximal operator proxµ() to estimate the backgrounds closed-form solution. The equation is delineated as follows: = proxµ(D O) . (8) layers to approximate proximal functions, thereby simplifying complex matrix computations and tapping into the nonlinear processing of neural networks for deep feature extraction from images (detailed in Section 3.3)."
        },
        {
            "title": "3.2.2 O∗ Updates",
            "content": "λS(O) + Similar to (6), the sub-problem for optimizing the object is expressed as follows: + D2 = arg min µ 2 In prevalent optimization strategies, the sparse object is usually constrained by an l1 norm. Yet, integrating soft thresholding (ST) within neural networks poses difficulties [77]. Moreover, the nature of sparse constraints can fluctuate with varying detection contexts [92]. We aim to formulate more straightforward and intuitive representation for the closed-form resolution of (9). . (9) To address these challenges, we apply the Taylor expansion to S(O). Given function (t) with Lipschitz continuous gradient (t), it can be approximated at fixed point t0 via Taylors series: ˆf (t, t0) 2 (cid:13) (cid:13) t0 + (cid:13) (cid:13) (cid:13) 2 (cid:13) (t0) (cid:13) (cid:13)"
        },
        {
            "title": "1\nL",
            "content": "+ . (10) In this case, is constant and is defined as = 2L (t0)2 + (t0) (For detailed derivations, please refer to 1 the Appendix A). Consequently, we approximate S(O) at the previous iteration Ok1 as follows: LS 2 (cid:13) (cid:13) OOk1 + (cid:13) (cid:13) ˆS(O, Ok1) Ok1(cid:17)(cid:13) +Cs , (11) S"
        },
        {
            "title": "1\nLS",
            "content": "(cid:16) 2 (cid:13) (cid:13) (cid:13) 2 where Ls denotes the Lipschitz constant for S(O), and Cs is de- (cid:13)S(Ok1)(cid:13) (cid:13) 2 fined as constant, represented by Cs = 1 2 + (cid:13) 2Ls S(Ok1). This allows us to update the object matrix in simplified manner. = arg min λ ˆS(O, Ok1)+ + D2 µ 2 λLS 2 (cid:13) (cid:13) Ok1 + (cid:13) (cid:13) = arg min + D2 + . µ"
        },
        {
            "title": "1\nLS",
            "content": "(cid:13) 2 (cid:13) S(Ok1) (cid:13) (cid:13) 2 (12) Instead of the conventional l1 norm constraint, this formulation incorporates only the sum of two l2 norms. This simplification eliminates the need for conventional algorithms or soft thresholding simulations [77]. Deriving the equations derivative and equating it to zero, we deduce closed-form solution for the k-th iteration update of O: Ok = Ok1 + λLS λLS + µ λ λLS + µ S(Ok1) . µ λLS + µ (cid:16) Dk1 Bk(cid:17) (13) Thus, the object matrix update equation is succinctly reformulated as follows: Ok = γOk1 + (1 γ)(Dk1 Bk) ρS(Ok1) , (14) 5 Fig. 4. Detail network structure of single stage RPCANet++, consisting of background approximation module (BAM), object extraction module (OEM), and image restoration module (IRM). Memory augmented module (MAM) is injected in BAM while the deep contrast prior module (DCPM) is embedded in OEM. Notably, the channel number is set as 32 in this framework. [Zoom in for better view] where two coefficients: γ = λLS λLS+µ and ρ = λ λLS+µ . By learning the function end-to-end, our method avoids intricate matrix operations such as ST while ensuring Lipschitz continuity."
        },
        {
            "title": "3.2.3 D∗ Updates\nThe restoration update formula for Dk is efficiently obtained\nthrough:",
            "content": "Dk = Bk + Ok . (15)"
        },
        {
            "title": "3.3 Unfolding into Deep Framework: RPCANet++\nThis section outlines RPCANet++’s architecture and modules,\nderived from Section 3.2’s optimization equations. Given an\nimage denoted as X ∈ RH×W , where H and W represent the\nimage’s height and width, respectively, we initialize D0 = X\nwhile O0 = 0. It undergoes K decomposition stages, each\nsimulating iterative low-rank sparse decomposition.",
            "content": "At each k-th stage, we input Dk1 and Ok1 to estimate Bk, Ok, and Dk using BAM, OEM, and IRM. We innovate beyond [41] by incorporating memory-augmented module (MAM) for background transmission loss, utilizing ConvLSTM [96] for feature retention. Additionally, we introduce deep contrast prior module named DCPM, inspired by local contrast [90], [97], [98], [99], to refine object extraction."
        },
        {
            "title": "3.3.1 Background Approximation Module (BAM)",
            "content": "Fig. 4 illustrates using BAM for background estimation. The proximal operator proxµ() in (8) is yet to be defined. Previous work used an ISTANet++-like [85] residual structure, which only considered restored and object information, potentially causing feature loss. To address this, we introduce memoryaugmented module (MAM) for feature preservation, compatible with DUN. Drawing inspiration from recent studies [84], [100], instead of directly concatenating previous features [39] (as shown in Fig. 5), we integrate ConvLSTM [96] to manage past and current information flow, processed through an initial layer of [Conv + BN + ReLU ], an efficient ResBlock (widely utilized in [78], [101], [102], etc.), and symmetric decoder, collectively termed proxNet(). In detail, the MAM is sandwiched by two convolutional layers (ConvB1() and ConvB2 ()) and two ResBlocks (RB1() and RB2()) and it has three inputs: deep feature Bk1, hidden states Bk1 (we use calligraphy to denote multi-channel features). We define the output of MAM as: , and cell outputs Bk1 [Bk h, Bk ] = ConvLSTM(Bk1, [Bk1 , Bk1 ]) (16) Fig. 5. Comparison of different constructions of BAM, (a) Plain construction [41]; (b) Temporary Transmission (TT) concatenates last stage background information; (c) Cumulative Transmission (CT) [39] concatenates all previous background features; (d) Ours select background features adaptively. Given the deep inputs, the process is presented in the shadow blue part in the top-left of Fig. 4 and formulated as: = σ(WBBi Bk1 + WBhBi Bk1 Bk = σ(WBBf Bk1 + WBhBf Bk1 Bk = Bk Bk + Bk Bk1 + bBi); + bBf ); tanh(WBsBc Bk1 + bBc); + WBhBc Bk1 (17) = σ(WBBo Bk1 + WBhBo Bk1 Bk = Bk Bk ). tanh(Bk + bBo); Here, the convolution operation is denoted by , and the Hadamard product by . The functions σ() and tanh() represent the sigmoid and tanh activations, respectively. The filter weights are represented by WBBi , WBBf , ..., WBBo , and their corresponding biases by bBBi , bBBf , ..., bBBo . The input, forget, and output gates are denoted as Bk , Bk , respectively. We assign Bk to Bk for general use, which serves as the input to RB2(). The pair [Bk1 ] is propagated through successive stages, ensuring the adaptive retention of background features throughout the process. , Bk1 , Bk In summary, the total structure of BAM can be collected as: Bk = proxNet(Dk1, Ok1, [Bk1 , Bk1 ]) = Dk1 Ok1 +ConvB2(RB2(ConvLSTM( (18) RB1(ConvB1(Dk1 Ok1)), [Bk1 , Bk ]))), where we also incorporate residual construction to enhance training stability. Unlike [84], which relies heavily on intra-stage information for recovery, our BAM functions as lightweight bridge between OEM and IRM. By selectively filtering redundant channel features, we streamline inter-module communication and reduce computational overhead. Fig. 6. Comparison of different local contrast feature extraction methods: construction of CDC [90] and our DCPM."
        },
        {
            "title": "3.3.2 Object Extraction Module (OEM)",
            "content": "As Section 3.2 deducts, we design the object extraction module for object updates, to treat three variables evenly, we assign γ to 0.5. And (14) are formulated as: Ok = Ok1 +Dk1 Bk ρS(Ok1) . (19) We set ρ as stage-independent learnable parameter as ρk. And S, Lipschitz continuous gradient function, is simulated by [Conv + ReLU ] construction, where the Lipschitz continuity can remain even within multistacked layers [103]. As the orange box in Fig. 4 shows, we denote such theory-guided networks as G(). In traditional object iterations, the sparse constraint module is always assisted by weighted prior module [40] for convergence acceleration. Such prior can be correlated with physical guidance such as saliency [72], which is generally written as: Wk = , (20) CT Ok1 + ε CT is manual constant, and ε is set to avoid the division by zero issue. And the S(O) will be rewritten as S(W O). However, such collaboration still requires parameter fine-tuning, and introducing division operation in neural networks will introduce issues such as Inf (object element approaching zero). Thus, instead of directly implementing the prior correlation conventionally, we pre-extract the saliency feature with deep prior contrast module via an enhanced CDC. As shown in Fig. 6, unlike the original CDC [90], we employ channel attention module to perceive deep features and adaptively adjust the manual θ in the general CDC. To be specific, deep contrast prior extraction process consists of four steps: 1) sampling with kernel Po over the input feature k1 RCHW ; 2) summing the convolutional weights to central weight Ps; 3) making θ learnable by compressing the multi-channel information via simple channel attention [104] module θc; 4) generalizing the network P() in central-difference scheme. We formulate this process as follows: P(X k1) = θc(X k1) Ps(X k1) Po(X k1) , (21) P() is sandwiched by two convolutional sections ConvT1() and ConvT2 (), and the overall DCPM network is written as: Wk = ConvT2(P(ConvT1(Xk1))) . (22) To align with (19) and maintain the difference of previous reconstructed Dk1 and current approximated Bk, we assign Xk1 = Ok1 +Dk1 Bk. Similar to the prior assisted DUNs [86], [87], [88], we directly add the prior before Gk, and the final update equation for OEM is collected as follows: Ok = Ok1 +Dk1 Bk ρkGk(Ok1 +Dk1 Bk +Wk). (23) The overall OEM structure is demonstrated in Fig. 4, where lO represents the number of middle layers."
        },
        {
            "title": "3.3.3 Image Restoration Module (IRM)\nWe reconstruct the infrared image by merging the updated Bk\nand Ok via an simple yet efficient restoration module M(·), as\nshown in the purple box in Fig. 4 and formulated by:",
            "content": "Dk = Mk(Bk + Ok) , (24) 6 Fig. 7. Algorithm2Network: Solutions of optimization algorithm and corresponding network architecture of RPCANet++. where M() are comprised of one [Conv + ReLU ] block, lD of [Conv + BN + ReLU ], and [Conv] layers. In summary, the three modules are orderly updated within single stage and iterative for K-stages as Fig. 3 shows. The overall algorithm-to-network diagram is demonstrated in Fig. 7."
        },
        {
            "title": "3.4 Network Implementation",
            "content": "BAM, Θk"
        },
        {
            "title": "3.4.1 Learnable Parameters\nThe parameter set Θ of our RPCANet++ is defined as Θ =\n(cid:8)Θk\nk=1. This encompasses the network\nparameters within BAM (with MAM), OEM (with DCPM), and\nIRM for each stage of decomposition, along with the trainable\nscalar ρk in OEM.",
            "content": "IRM, ρk(cid:9)K OEM, Θk In BAM, the stride and padding for the 3 3 convolutional layers, ConvB1 () and ConvB2 (), are uniformly set to 1, with the channel expansion number specified as 32. The Residual Blocks, RB1() and RB2(), adhere to structured sequence: [Conv + BN + ReLU + Conv + BN + Skip Connection], maintaining consistency in stride, padding, kernel size, and channel dimensions. Regarding OEM, the kernel size for the deep contrast prior P() is established at 17 17. The channel attention modules ratio within θc is fixed at 4. Furthermore, all convolutional layers in this module are configured with stride and padding of 1, kernel size of 33, and an increased channel count of 32. The sparse layer parameter lO is set to 6. Similarly, the reconstruction module M() in IRM follows identical convolution setting, with the layer number lD = 3."
        },
        {
            "title": "3.4.2 Training Loss",
            "content": "Compared with conventional networks with singular segmentation tasks, our RPCANet++ not only maps the object feature to masks but also fulfills the image restoration function. Specifically, we adopt Soft Intersection over Union (SoftIoU) [105] as our segmentation loss, while generalized mean squared error (MSE) is employed for reconstruction loss. The combined loss function is formulated as: LAll = LSoftIoU + σ LMSE Nt(cid:88) = 1 P +T +F N"
        },
        {
            "title": "1\nNt",
            "content": "i=1 + σ NtN Nt(cid:88) i=1 (cid:13) (cid:13) 2 (cid:13)DK (cid:13) (cid:13) (cid:13) (25) where Nt and are the total training number and total pixels per image. σ represents the regularization parameter, which is set to 0.1 in our experiments (detailed in Section 4)."
        },
        {
            "title": "4.1.1 Datasets",
            "content": "Infrared Small Target Segmentation (IRSTD): We utilize four distinct open-source IRSTD datasets: NUDT-SIRST [107], IRSTD1K [106], SIRST [43], and SIRST-Aug [75]. NUDT-SIRST comprises 1327 256 256 images, encapsulating diverse scenes such 7 Fig. 8. Heatmaps of different stages Bk and Ok visualization results (K = 6) of our RPCANet++ on various scenarios from six different datasets (IRSTD, VS, and DD tasks). We can observe its gradual shaping process via iterative unfolding. [Zoom in with blue box for tiny objects] Fig. 9. Low-rankness verification of different stage features (1st to 6th) in (a) RPCANet++, compared to original images. As well as its variants (b) without MAM or (c) without DCPM, and the baseline (d) RPCANet [41]. Verification is conducted on the IRSTD-1K test set [106]. Our RPCANet++ progressively estimates background features satisfying low-rankness, step-by-step, without overestimation. [Zoom in for better view] as cloud, city, sea, and field, featuring both natural and synthetic targets. Following the protocol of [107], we adopt 50 : 50 split for training and testing. IRSTD-1K, documented in [106], contains 1000 512 512 genuine images, spanning variety of targets and scenarios. Although the original dataset division is 50 : 30 : 20 for training, validation, and testing, we align with the configuration of [108], opting for an 80 : 20 split. SIRSTAug, introduced by [75], enhances SIRST [43] (which contains 427 realistic images with 80 : 20 training-testing split) through random cropping and rotation, amassing 9070 256 256 images, with 8525 designated for training and 545 for testing. In this study, due to computational restrictions and fair comparison, we resize images to 256 256 when training. Vessel Segmentation (VS): We evaluate performance on three open-source retinal vessel segmentation datasets: DRIVE [74], CHASE DB1 [109], and STARE [45]. DRIVE includes 40 584565 retinal vessel images (7 with abnormal pathology), split evenly for training and testing. CHASE DB1 contains 28 999960 retina images from 14 subjects; we follow [110] with 20/8 train/test division. STARE provides 20 700 605 blood vessel images from retinal fundus, using an 80/20 split. Defect Detection (DD): We utilize two open-source metal defect detection datasets: NEU-Seg [47] and SD-saliency-900 [48]. NEUSeg dataset containing 4470 200 200 surface defects from strip steel plates with different illumination and material changes, divided into 3630 : 840 train and test split. For SD-saliency-900, it contains 900 200 200 defect images of three different types: scratches, inclusion, and patches. Following work [111], we set the train with 810 images (540 images with equally distributed three defect types, 270 salt-and-paper disturbed images with Fig. 10. Sparsity verification of different stages our RPCANet++ and its variants(without MAM or DCPM) vs RPCANet [41] on IRSTD-1K [106]. Left: numerical verification. Right: heatmaps among different stages. ϕsalt&paper = 0.2) and test split with 900 images."
        },
        {
            "title": "4.1.2 Learning Schedule",
            "content": ". (cid:1)0.9 total iter Our experiments are conducted in PyTorch environment, leveraging an Nvidia GeForce RTX 3090 GPU. We utilize an Adam [112] optimizer with polynomial decay policy. The learning rate is adjusted by factor of (cid:0)1 iter For IRSTD tasks, our training regimen spans 800 epochs for NUDT-SIRST [107], IRSTD-1K [106], and SIRST [43], and 400 epochs for SIRST-Aug [75], commencing with learning rate of 104, and we proceed with batch size of 8. In VS tasks, our training regimen spans 400 epochs for DRIVE [74], CHASE DB1 [109], and STARE [45], commencing with learning rate of 5 104 and proceeding with batch size of 4. As to DD, our training regimen spans 200 epochs for NEU-Seg [47] and 400 epochs for SD-saliency-900 [48] with learning rate of 104. TABLE 1 THE IMPACT OF DIFFERENT STAGE INDEX ON DETECTION EFFICACY, QUANTIFIED BY IOU (%), F1 (%), Pd (%), AND Fa (105) ON NUDT-SIRST DATASET [107] AND SIRST-AUG [75]. Stages (K) Params (M) 1 2 3 4 5 6 7 8 9 0.447 0.941 1.435 1.928 2.422 2.915 3.409 3.902 4.396 NUDT-SIRST [107] SIRST-Aug [75] IoU 86.69 90.61 92.70 93.01 93.52 94.39 93.50 93.82 93.97 F1 92.87 95.08 96.21 96.38 96.65 97.12 96.64 96.81 96.89 Pd 96.93 96.72 98.31 98.09 98.20 98.41 98.20 98.41 98.41 Fa IoU 3.17 2.40 1.67 3.35 1.74 1.34 1.76 1.35 1.44 72.35 73.79 73.90 74.49 73.87 74.89 73.31 74.14 73.13 F1 83.96 84.92 85.01 85.38 84.97 85.44 84.60 85.15 84.48 Pd 99.03 98.62 98.90 96.84 98.07 98.76 98.35 99.04 97. Fa 41.94 38.28 28.64 26.81 29.63 28.00 33.11 35.80 32.18 Fig. 11. IoU (%), F1 (%), Pd (%), and Fa (105) of different stages number on RPCANet++ and RPCANet [41] on SIRST-Aug [75]."
        },
        {
            "title": "4.1.3 Metrics\nFor evaluation purposes, we primarily employ pixel-level met-\nrics such as intersection over union (IoU) and the F -measure\n(F1) for three different tasks. We also pick up field-adopted\nmetrics: For IRSTD tasks, we adopt target-level metrics [108],\nincluding the probability of detection (Pd) and false alarm rate\n(Fa). Besides, the receiver operating characteristics (ROC) curves\nand the area under curves (AUC) are also introduced for per-\nformance measurement. In VS tasks [110], we utilize accuracy\n(Acc), sensitivity (Sen), specificity (Spe), and AUC for evaluation.\nFor the DD tasks [48], we introduce structural similarity (S-\nmeasure) and mean absolute error (MAE) metrics.",
            "content": "Moreover, considering that the singular values of infrared images may approach zero within certain ranks, numerous optimization-based IRSTD methods integrate these low-rank properties during preliminary analysis, such as assessing the low-rankness across different modes. However, no prior works have incorporated this feature examination into the final verification stage. In this vein, we introduce novel low-rankness metric to evaluate the background approximation process. Specifically, for k-stage background Bk, we decompose it as Bk = UΣV, where Σ denotes the singular value matrix: Σij = lri δij, δij = (26) (cid:40) 1 if = j, 0 if = , where Σij signifies the element in the i-th row and j-th column of the diagonal matrix, and lri represents the i-th singular value, with bounded by min(H, ). And we adopt corresponding ranks lri as our low-rankness index. Furthermore, to quantify the sparse component of target component, as inspired by [113], we adopt the l0-norm for sparsity evaluation, defining the sparsity rate rs as: rs = Ok0/H . (27) These metrics facilitate the evaluation of our frameworks efficiency and interoperability. An easy-to-use toolkit containing these is open-sourced on our Github Repository."
        },
        {
            "title": "4.2 Model Verification",
            "content": "In the context of deep unfolding, the network is architected to iteratively yield guided results congruent with an algorithms 8 Fig. 12. Target and background heatmaps comparison in different stages = 2, 4, 6 from different memory-augmented strategies on NUDT-SIRST [107] example. MAM can gradually assist background feature learning while maintaining sparse targets. Method TABLE 2 THE IMPACT OF DIFFERENT MEMORY AUGMENTED MODULES ON DETECTION EFFICACY, QUANTIFIED BY IOU (%), F1 (%), Pd (%), AND Fa (105) ON NUDT-SIRST DATASET [107]. Pd 97.35 96.85 97.67 97.67 96.93 98.41 Plain TT [39] CT [39] HT [84] w/o MAM MAM 92.33 91.52 91.70 91.87 92.62 94.39 96.02 95.57 95.67 95.76 96.17 97.12 2.518 2.518 2.524 2.970 2.518 2.915 1.70 1.93 1.95 2.31 1.97 1.34 Params (M) IoU Fa F1 unrolled stages. Demonstrating outcomes at each stage is vital for model validation. Fig. 8 showcases heatmaps from sixstage RPCANet++ at = 2, 4, 6. Initially, the background predominantly reflects low-level edge information. With advancing stages, it progressively encapsulates detailed features, including intensity and non-local details. In contrast, sparse maps reveal gradual reduction of residuals, where elements such as edges and target-mimicking false alarms are incrementally suppressed, steered by the guided mask. Consequently, the target contour sharpens, and our resultsmore aligned with the original imagesfurnish an enhanced approximation to the ground truth compared to preliminary stages, as illustrated in Fig. 8(d). As outlined in Section 4.1, the proposed models effectiveness is substantiated by the progressive validation of its low-rankness and sparsity. Evaluations conducted on IRSTD-1K [106] affirm this assertion. Fig. 9 illustrates the stages of low-rankness evolution, with the orange line depicting the singular value distribution of the original images. Our advanced RPCANet++ demonstrates consistent feature enhancement as processing depth increases, with all stages converging to zero, signifying achieved low-rankness. In contrast, the DCPMs isolated application tends to overestimate the background, as indicated by the 6th layers mean singular value (shown in grey) surpassing that of the original image. Conversely, the MAM addresses this by orderly increasing learned features through cascaded enhancement, and the integrated RPCANet++ preserves this advantage. comparative analysis with RPCANet, which also exhibits background TABLE 3 THE IMPACT OF DIFFERENT FEATURE EXTRACTORS ON DETECTION EFFICACY, QUANTIFIED BY IOU (%), F1 (%), Pd (%), AND Fa (105) ON NUDT-SIRST DATASET [107]. Method lB = 1 lB = 2 lB = 3 two RBs single RB Params (M) IoU 2.805 2.917 3.029 3.137 2.915 92.26 92.27 92.02 93.65 94.39 F1 95.97 95.98 95.84 96.72 97. Pd 98.09 98.09 97.14 97.98 98.41 Fa 1.91 2.23 2.19 1.41 1.34 TABLE 4 THE IMPACT OF DIFFERENT DCPM CONFIGURATIONS ON DETECTION EFFICACY, QUANTIFIED BY IOU (%), F1 (%), Pd (%), AND Fa (105) ON NUDT-SIRST DATASET [107]."
        },
        {
            "title": "Method",
            "content": "Params (M) IoU base (w/o DCPM) base + CDC [90] base + SE [104] base + CBAM [114] base + DCPM (s = 3) base + DCPM (s = 5) base + DCPM (s = 9) base + DCPM (Ours) 1.132 2.911 1.138 1.139 1.195 1.293 1.637 2.915 90.99 91.74 90.58 91.78 93.15 93.15 93.43 94.39 F1 95.24 95.69 95.06 95.71 96.46 96.46 96.60 97.12 Pd 96.51 97.35 97.46 98.09 97.35 98.41 98.20 98.41 Fa 2.50 1.76 2.39 2.56 1.58 2.01 1.62 1.34 9 Fig. 13. Left: Traning loss comparison of RPCANet and RPCANet++ w/wo DCPM. Right: Comparison of target results with different methods in tricky environments from NUDT-SIRST [107]. TABLE 5 THE IMPACT OF DIFFERENT OEM CONFIGURATIONS ON DETECTION EFFICACY, QUANTIFIED BY IOU (%), F1 (%), Pd (%), AND Fa (105) ON NUDT-SIRST DATASET [107]. Method lO = 2 lO = 4 lO = 8 lO = 10 Ok Concate Ours Params (M) IoU 2.693 2.804 3.026 3.137 2.915 2.917 2.915 92.27 92.89 92.68 92.60 92.49 93.27 94. F1 95.98 96.31 96.20 96.16 96.10 96.52 97.12 Pd 97.99 97.46 97.35 98.10 97.67 98.31 98.41 Fa 1.96 1.50 1.64 1.94 1.59 1.58 1.34 overestimation as depicted in Fig. 9(d), highlights the importance of MAM in correcting background transmission errors. Additionally, the right side of Fig. 10 presents sparsity comparison, detailing the mean and standard deviation across various stages. The memory-augmented module yields markedly sparser outcome than RPCANet, swiftly pinpointing potential target areas. Despite the DCPM-enhanced model achieving greater sparsity after three stages relative to RPCANet, its initial stage measurements reveal considerable uncertainty. Our RPCANet++ effectively integrates the strengths of MAM and DCPM, enabling swift and reliable target segmentation, and achieving significant sparsity that approaches convergence rapidly. Moreover, as shown in the left of Fig. 10, RPCANet++ demonstrates minimal noise initially and superior shape retention in later stages, unlike its counterparts. In summary, our model, supported by both visual and quantitative evidence, conforms to theoretical expectations, manifesting low-rank and sparse properties. We envisage our method setting the stage for an interpretable paradigm in the IRSTD field."
        },
        {
            "title": "4.3.1 Impact of Stage Numbers",
            "content": "The number of iterative stages is crucial to the performance of our algorithm. Similar to optimization problems, an insufficient number of stages can lead to non-convergence, while an excessive number of iterations can result in computational redundancy. Therefore, this section examines the impact of the number of stages on RPCANet++, as illustrated in Table. 1. Due to computational memory constraints, we limit the maximum number of stages to nine. Results indicate that the overall performance improves with an increasing number of stages, showing linear enhancement in parameters and peaking at = 6. We adopt this setting for the subsequent experiments. In addition, we also compare different stage number comparisons on RPCANet++ and RPCANet in Fig. 11. We can observe that both DUN methods have detection performance increases as the stage climbs, where both of them have an optimal value when = 6. To be specific, despite slight off in Pd when = 4, RPCANet++ has all stages better IoU, F1, and Fa than RPCANet, proving the enhancement proposed in this study."
        },
        {
            "title": "4.3.2 Different Construction of MAM",
            "content": "In this section, we discuss the implementation of handling transmission loss for during iteration. Section 3.3 outlines various enhancement strategies, with visual representations provided in Fig. 5. This ablation study contrasts different memory augmentation methods: Temporary Transmission (TT) [39], Cumulative Transmission (CT) [39], High-Throughput (HT) -injected MAM [84], and our optimized MAM, detailed in Table. 2. Compared to the baseline, alternative methodologies negatively impact the IoU and F1 scores. Although there is slight improvement in Pd for CT and HT, the overall advancements remain suboptimal, underscoring the efficacy of our MAM modules configuration. Additionally, we offer stage-by-stage visual representation of the background/target evolution in Fig. 12. The background enhancements indicate that the direct addition of antecedent elements, as seen in TT and CT, limits the approximation to elementary features such as edges and dispersed points. This limitation can inadvertently trigger incorrect targetbackground delineations, resulting in potential target omission, as shown at the bottom of Fig. 12. On the other hand, incorporating HT elements could lead to similar issues. In contrast, MAM in our RPCANet++ methodically integrates background details while maintaining target fidelity, leading to significant increase in IoU by 2.06 and reducing Fa by 0.36 compared to the baseline."
        },
        {
            "title": "4.3.3 Discussion on Feature Extractor in MAM",
            "content": "Given that the deep extractor within the proximal network, denoted as proxNet(), determines the information flow to the MAM, selecting an appropriate feature extractor is crucial. In our investigation (as summarized in Table. 3), we explore various extractors, including lB layers of 3 3 convolutional layers (where lB = 1, 2, 3); two residual blocks (RB); and our single RB configuration. The results reveal that residual construction outperforms single CNN layer construction, consistent with [41]. Additionally, an excessive number of extraction layers may compromise feature representation. Specifically, the performance of lB = 3 or two RBs is slightly weaker than that of the single RB construction. Consequently, we adopt single RB before and after the MAM in our architecture. Meanwhile, as shown in Table. 4, considering the impact of the receptive field on feature extraction and computational TABLE 6 THE IMPACT OF DIFFERENT LOSS CONFIGURATIONS ON DETECTION EFFICACY, QUANTIFIED BY IOU (%), F1 (%), Pd (%), AND Fa (105) ON NUDT-SIRST DATASET [107]. Method σ = 0 σ = 0.01 σ = 0.05 σ = 0.5 σ = 1 σ = 0.1 IoU 88.23 87.37 88.58 86.84 86.67 89.31 F1 RPCANet [41] Pd 96.40 96.40 96.30 95.66 96.19 97.14 93.75 93.26 93.94 92.96 92.84 94.35 Fa IoU 3.23 3.81 3.04 3.53 3.75 2.87 93.78 93.51 93.75 93.25 94.08 94.39 RPCANet++ Pd F1 98.10 98.52 97.99 97.88 98.20 98.41 96.79 96.64 96.78 96.51 96.95 97.12 Fa 1.42 1.75 1.41 1.61 1.12 1. STUDIES ON DIFFERENT COMPONENT OF RPCANET ON THE DETECTION ++ PERFORMANCE IN IOU (%), F1 (%), Pd (%), AND Fa (105) ON SIRST-AUG [75]. TABLE 7 Config. OEM BAM IRM MAM DCPM Params 1 2 3 4 5 6 (cid:90) 0.507 0.507 0.734 1.132 2.518 2.915 IoU 60.48 61.99 68.75 72.30 73.60 74.89 F1 75.38 76.53 81.48 83.92 84.79 85.44 Pd 92.98 98.21 96.15 96.42 96.42 98.76 Fa 53.55 37.50 33.78 40.42 27.66 28.00 efficiency, we investigate different kernel sizes (s = 3, 5, 9) with our 17-setting. As kernel size increases, overall performance improves, reaching its peak at = 17. Specifically, compared to = 9, we select = 17 for its +0.94 IoU and 17.28% fewer Fa. In addition, to demonstrate the acceleration of the convergence process by our DCPM, we compare training loss and performance among three variants: RPCANet, our RPCANet++ without DCPM, and RPCANet++. While RPCANet++ without DCPM exhibits faster loss reduction, it suffers from unstable fluctuations. In contrast, RPCANet++ with DCPM achieves robustness and steady convergence. Additionally, under strong noisy conditions (as depicted Fig. 13), our DCPM-enhanced RPCANet++ captures targets with minimal edge information, outperforming ISNet [106] and RPCANet [41]. Notably, only our DCPM preserves clear target shapes despite reduced lightness, highlighting its efficacy and overall architectural robustness."
        },
        {
            "title": "4.3.4 Discussion on Basic Construction of OEM",
            "content": "In this section, we perform ablation studies on the inner construction of S. Specifically, we explore different numbers of layers for OEM, denoted as lO, where lO = {2, 4, 6, 8, 10}. As summarized in Table. 5, detection performance improves with increasing layers in the initial stages. However, beyond lO = 6, overall detectability begins to decline. Thus, we select the optimal configuration of lO = 6 for our RPCANet++. Meanwhile, as discussed in Section 3.3, the flexibility of DUNs allows us to leverage Dk1 and update Bk1 to enhance adjacent stage information while avoiding target transmission loss. However, evaluating the performance is essential when using only Ok1. Table. 5 illustrates that this simplified construction achieves satisfactory results but experiences -1.90 IoU performance compared to our enhanced approach. Furthermore, we compare our Plus operation with feature concatenation (Concate) in Table. 5, demonstrating that the former strategy maintains superior performance. Nevertheless, the excellent performance of both approaches underscores the robustness and effectiveness of our overall framework."
        },
        {
            "title": "4.3.5 Discussion on Loss Functions",
            "content": "Instead of solely introducing constraint to the target and background with λ, we also introduce σ value in the loss function. As λ has become learnable variable across stages, its essential to discuss the value of σ regarding the effectiveness of introducing background constraints and how to weigh it. In 10 Table. 6, we compare the value of σ = 0 (no LMSE), 0.01, 0.05, 0.1, 0.5, and 1 on RPCANet and RPCANet++. We can observe that solely adopting segmentation loss can achieve relatively excellent results, and properly constraining the background can assist detection performance, e.g., σ = 0.05 for RPCANet, σ = 1 for RPCANet++. In contrast, when σ = 0.1, both methods achieve their best, with +1.08 and +0.61 in IoU; thus, we choose σ = 0.1 as our loss constraint."
        },
        {
            "title": "4.3.7 Discussion and Comparison on DCPM",
            "content": "In this study, to fit into domain knowledge, we introduce novel deep contrast prior module (DCPM) to enhance target allocation while accelerating the convergence process of optimization. To validate the effectiveness of our DCPM, we compare it with the original CDC [90] (kernel size of 17 17) and related efficient feature extractors (SE [104] and CBAM [114]). Our findings in Table. 4 indicates that the central difference strategy slightly improves detection performance (+0.75 in IoU) while reducing false alarms by 30% (Fa). However, our learnable parameter θ, despite slight increase in parameters, significantly enhances overall IoU by 2.65. Although SE and CBAM have fewer parameters, they do not match the effectiveness of our DCPM."
        },
        {
            "title": "4.4.1 Comparison on Infrared Small Target Detection Task",
            "content": "In the comparison section, we benchmark our approach against several state-of-the-art IRSTD methodologies. For model-driven methods, we incorporate baselines: MPCM [115], IPI [67], NRAM [9], and PSTNN [71]. In the realm of data-driven approaches, we evaluate frameworks including ACM [43], DNANet [107], AGPCNet [75], UIUNet [116], and MSHNet [117]; alongside model-inspired networks such as ALCNet [118], ISNet [106], and the DUN-based RPCANet [41]. We retrained listed DL methods based on their original codes. Visual Comparison: Fig. 14 displays mask results from four datasets under conditions of extremely low SCR, high falsealarm rates, and multiple targets in both terrestrial and aerial scenes. In the first row, dense target-like clusters cause most methods to produce numerous false alarms. In contrast, both the optimization-based NRAM and RPCANet++ maintain clean backgrounds, with RPCANet++ rendering the most precise target contours. In low-SCR scenarios (e.g., NUDT-SIRST 000781 and SIRST-Aug 008583), while many approaches either miss or merge targets, RPCANet++ consistently detects all targets with accurate 11 Fig. 14. Visual comparisons of diverse IRSTD techniques from NUDT-SIRST [107], IRSTD-1K [106], SIRST [43], and SIRST-Aug [75] are presented, with correctly identified targets, undetected targets, and false positives delineated by red, blue, and yellow dot boxes, respectively. TABLE 8 PERFORMANCE METRICS INCLUDING IOU (%), F1 (%), Pd (%), Fa (105), AND RUNTIME ARE EVALUATED FOR VARIOUS METHODS ON DATASETS NUDT-SIRST [107], IRSTD-1K [106], SIRST [43], AND SIRST-AUG [75]. VENUE. MEANS THE PUBLICATION AND YEAR. PARAMS. (M) IS THE PARAMETER STATISTIC FOR DATA-DRIVEN APPROACHES WITHIN THE SECOND-TO-LAST COLUMN. [ZOOM IN FOR BETTER VIEW] Methods Venue NUDT-SIRST [107] IRSTD-1K [106] SIRST [43] SIRST-Aug [75] IoU F1 Pd Fa IoU F1 Pd Fa IoU F1 Pd Fa IoU F1 Pd Fa Params (M) Time (s) CPU/GPU Model-Based Methods MPCM [115] IPI [67] NRAM [9] PSTNN [71] PR16 TIP13 RS18 RS19 Deep Learning-Based Methods ACM [43] WACV21 TAES23 TIP23 TIP23 CVPR24 TGRS21 CVPR22 AGPCNet [75] DNANet [107] UIUNet [116] MSHNet [117] ALCNet [118] ISNet [106] 9.26 34.62 11.44 25.17 69.46 85.02 87.73 88.94 75.96 80.75 87.05 16.95 51.43 20.52 40.22 81.98 91.90 93.47 94.15 86.34 89.35 93.08 70.58 32.72 7.54 92.38 2.35 65.71 7.61 80.21 97.14 13.11 4.34 97.88 5.52 97.67 1.53 95.23 8.28 95.23 9.56 97.56 4.05 96. 15.70 25.27 17.97 24.39 54.16 60.44 62.23 63.08 64.83 60.69 63.00 27.13 40.35 30.46 39.22 70.25 75.34 76.72 77.36 78.67 75.53 77.29 Deep Unfolding-Based Methods RPCANet [41] WACV24 Ours - S3 Ours - S6 Ours - - - - 94.35 89.31 97.14 92.70+3.39 96.21+1.86 98.31 94.39+5.08 97.12+2.77 98.41 93.97+4.66 96.89+2.54 98.41 2.87 1.67 1.34 1.44 77. 63.21 88.31 64.40+1.19 78.34+0.98 92.78 64.93+1.72 78.73+1.28 89.70 63.23+0.02 77.48+0.03 89.35 62.54 12.47 83.51 20.93 59.79 5.71 65.29 15.77 83.84 90.72 92.44 92.10 92.78 85.57 88.66 7.32 6.10 5.64 6.09 7.14 5.17 5.52 4.39 5.56 4.35 4. 22.30 44.83 26.39 35.17 66.77 68.19 73.12 72.15 64.64 70.31 66.96 36.47 61.90 41.75 52.04 80.07 81.08 84.48 83.82 78.52 82.57 80.20 90.83 97.25 88.07 88.99 5.56 3.76 1.60 2. 98.16 11.03 99.08 12.09 100.0 10.26 98.16 7.90 98.16 15.66 99.08 11.36 9.85 96.33 19.76 21.93 10.41 12.38 67.79 72.07 70.56 70.76 71.93 67.38 71.10 33.00 35.97 18.86 22.04 80.80 83.77 82.74 82.88 83.67 80.51 83.11 93.39 80.05 76.06 60. 3.14 1.62 2.26 1.98 95.87 32.52 98.62 29.39 96.56 36.49 96.70 34.30 99.03 32.06 97.94 28.68 97.66 30.33 82.61 70.37 98.21 34.14 95.41 75.52+5.15 86.06+3.45 100.0 73.90+1.36 85.01+0.93 98.90 28.64 74.76+4.39 85.47+2.86 100.0 10.77 74.89+2.35 85.44+1.36 98.76 28.00 72.62+2.25 84.14+1.53 100.0 73.13+0.59 84.48+0.04 97.66 32.18 7.42 8. 72.54 84.08 9.57 - - - - 0.398 12.36 4.697 50.54 4.065 0.378 0.967 0.680 1.435 2.915 4. 0.0441/- 5.0583/- 1.5439/- 0.2863/- -/0.0146 -/0.1173 -/0.0437 -/0.0541 -/0.0483 -/0.0134 -/0.0270 -/0.0217 -/0.0262 -/0.0471 -/0.0627 TABLE 9 AUC PERFORMANCE (%) OF VARIOUS METHODS ON FOUR DATASETS. Method NDUT-SIRST [107] IRSTD-1K [106] SIRST [43] SIRST-Aug [75] NRAM [9] PSTNN [71] AGPCNet [75] DNANet [107] ALCNet [118] ISNet [106] 60.71 74.73 96.88 97.96 97.88 96.36 Deep unfolding-Based Methods RPCANet [41] Ours-S3 Ours-S6 Ours-S 96.50 98.83+2.33 99.27+2.77 98.60+2.10 68.69 82.48 90.62 81.10 82.80 89.90 88.58 96.73+8.15 95.04+6.46 90.19+1.61 73.96 81.44 92.90 89.51 94.27 85.12 91.58 99.99+8.41 99.31+7.73 99.99+8.41 60.21 60.47 92.81 89.25 88.90 70. 96.01 99.43+3.42 95.52-0.49 97.05+1.04 shapes. For diminutive targets such as IRSTD-1K XUD9, where interference poses significant challenge, our method markedly enhances detectability. Moreover, RPCANet++ more effectively distinguishes closely spaced targets (e.g., XDU406) compared to the original RPCANet (e.g., Misc 58 in SIRST), and significantly improves shape preservation in cluttered backgrounds. Numerical Comparison: Tables. 8 and 9 summarize performances on four compared datasets. Model-based methods (e.g., MPCM, IPI, NRAM) generally yield low IoU and high false alarms, whereas deep learning approaches (e.g., ACM, AGPCNet, DNANet) improve performance at the cost of larger models. Our baseline RPCANet achieves 89.31% IoU and 94.35% F1 on NUDT-SIRST. The RPCANet++ series further boosts performance: the RPCANet++-S3 increases IoU to 92.70% (+3.39%) and F1 to 96.21% (+1.86%), with similar trends on other datasets. Notably, the RPCANet++-S6 model reaches 94.39% IoU and 97.12% F1 on NUDT-SIRST, while reducing false alarms (1.34 vs. 2.87). AUC values also improve significantly; for example, RPCANet++-S3 attains 98.83% on NUDT-SIRST and 99.99% on 12 (a) NUDT-SIRST [107] Fig. 15. ROC curves for various algorithms across four datasets are depicted. RPCANet++ is represented in red and RPCANet [41] in blue. (b) IRSTD-1K [106] (d) SIRST-Aug [75] (c) SIRST [43] Fig. 16. Visual comparisons of diverse medical segmentation techniques from DRIVE [44], STARE [45], and CHASE DB1 [46]. We zoom in local challenging regions with green and red boxes. Yellow arrows indicate vital details of corresponding vessel features. [Zoom in for better view] TABLE 10 PERFORMANCE METRICS INCLUDING ACC (%), SEN (%), SPE (%), AUC (%), F1 (%), IOU (%), AND RUNTIME ARE EVALUATED FOR VARIOUS METHODS ON DATASETS DRIVE [44], STARE [45], AND CHASE DB1 [46]. VENUE MEANS THE PUBLICATION AND YEAR. PARAMS. (M) IS THE PARAMETER STATISTIC FOR DATA-DRIVEN APPROACHES WITHIN THE SECOND-TO-LAST COLUMN. [ZOOM IN FOR BETTER VIEW] Methods Venue Deep Learning-Based Methods U-Net++ [119] ResUNet [120] Attention UNet [121] U-Net [24] MICCAI15 DLMIA18 ICIP19 arXiv18 CS-Net [122] MICCAI19 FR-UNet [110] MCDAU-Net [123] PVT-GCASCADE [124] Deep Unfolding-Based Methods JBHI22 CIBM23 WACV Acc Sen Spe AUC F1 IoU Acc Sen Spe AUC F1 IoU Acc Sen DRIVE [74] STARE [45] CHASE DB1 [109] Spe AUC F1 96.85 96.66 96.83 96.82 96.70 96.73 96.91 95.88 81.39 84.42 82.87 82.30 84.68 82.82 81.44 79. 82.73 79.13 81.54 81.84 79.34 80.69 83.22 75.07 95.88 95.56 95.98 94.05 95.23 95.55 95.75 97.78 81.85 81.49 82.01 81.86 81.70 81.53 82.10 77.12 69.30 68.78 69.53 69.31 69.08 68.85 69.67 62.79 95.96 95.27 96.33 89.71 96.35 92.48 96.38 96.41 79.16 82.42 78.26 91.30 74.02 85.70 75.75 77. 71.22 64.85 73.96 40.98 76.25 50.04 75.65 75.53 96.69 94.43 95.07 94.35 95.62 94.42 95.32 97.31 74.89 72.25 75.97 56.35 75.03 62.96 75.58 76.23 59.91 56.70 61.30 39.41 60.04 46.05 60.77 61.60 97.17 96.28 96.95 96.32 96.92 97.04 97.26 96.94 84.48 90.80 84.61 89.20 85.03 80.41 84.35 85. 74.31 64.66 72.07 65.27 71.55 74.72 75.26 71.57 96.04 97.47 97.16 96.71 96.49 93.43 95.49 98.74 79.02 75.49 77.77 75.33 77.64 77.36 79.46 77.94 IoU 65.37 60.67 63.67 60.47 63.48 63.13 65.96 63.87 RPCANet [41] Ours - S3 Ours - S6 Ours - 96.61 96.80 96.79 96.73 * Due to memory constraints, we adopt the initial channel number of 16 for UNet++, ResUNet, and Attention UNet. Similar to [110], we convert the color retinal images from three datasets to grayscale images. 78.00 78.06+0.06 80.07+2.07 77.59-0.41 69.74 69.86+0.12 70.35+0.61 69.69-0.05 77.15 77.83+0.68 77.33+0.18 77.59+0.44 62.82 63.71+0.89 63.04+0.22 63.40+0.58 82.15 82.24+0.09 82.58+0.43 82.12-0. 95.68 93.41 92.90 88.89 80.88 81.22 84.41 81.56 77.71 80.19 80.83 79.74 77.08 75.86 74.25 75.82 75.46 75.27 76.23 74.11 81.91 81.47 81.45 81. 97.56 95.69 96.23 91.87 97.11 97.13 97.35 97.03 95.49 94.94 94.90 90.68 96.87 96.86 96.91 96.85 82.81 83.42 84.11 82.95 WACV24 - - - 64.00 64.09+0.09 66.81+2.81 63.45-0.55 Params (M) Time (s) CPU/GPU 34.526 2.294 0.905 2.184 8.400 5.719 12.979 26.628 0.680 1.435 2.915 4.396 -/0.0217 -/0.0154 -/0.0125 -/0.0137 -/0.0159 -/0.0347 -/0.0343 -/0. -/0.0217 -/0.0262 -/0.0471 -/0.0627 SIRST, and S6 records 99.27% on NUDT-SIRST. Overall, the RPCANet++ series achieves enhanced sensitivity, accuracy, and lower false-alarm rates across IRSTD benchmarks while maintaining compact and efficient model. Computational Efficiency: As summarized in Table. 8, RPCANet++ incurs moderate increase in parameters due to the added prior and memory-augmented modules, yet its model size remains substantially lower than UIUNet (50.54 M) and DNANet (4.697 M). Its execution time is under 0.05 on GPU, demonstrating high efficiency. Collectively, the visual and quantitative results affirm the precision, false-alarm suppression, and efficiency of RPCANet++, highlighting the advantages of integrating memory augmentation and deep contrast priors within deep unfolding framework for IRSTD."
        },
        {
            "title": "4.4.2 Comparison on Vessel Segmentation Task",
            "content": "In the comparison section for the vessel segmentation task, we benchmark our approach against several state-of-the-art vessel segmentation methods. We incorporate baselines: U-Net [24], UNet++ [119], ResUNet [120], Attention UNet [121], CS-Net [122], FR-UNet [110], MCDAU-Net [123], PVT-GCASCADE [124], and RPCANet [41]. We retrained all the listed DL methods with 256256 crop size and tested for full size for fair comparison. Visual Comparison: Fig. 16 presents segmentation results from three datasets under challenging conditions. Methods such as ResUNet, as demonstrated on the DRIVE and CHASE DB1 datasets, tend to overlook many microvascular structures. Similarly, the baseline RPCANet exhibits intermittent vessel predictionsparticularly evident in CHASE DB1indicating its limited capability in capturing local features. comparable shortcoming is observed with CS-Net on the STARE dataset. In contrast, our proposed RPCANet++ effectively captures finer vessels and boundary details, thereby preserving both global and local vascular structures. Numerical Comparison: Table. 10 reports the performance of various segmentation techniques on the three vessel segmentation datasets using six different metrics. Among the deep 13 Fig. 17. Visual comparisons of diverse defect detection techniques results from NEU-Seg [47] and SD-saliency-900 [111], with correctly identified targets, undetected targets, and false positives delineated by blue and yellow boxes, respectively. [Zoom in for better view] TABLE 11 PERFORMANCE METRICS INCLUDING S-MEASURE (%), MAE, IOU (%), F1 (%), AND RUNTIME ARE EVALUATED FOR VARIOUS METHODS ON DATASETS NEU-SEG [47] AND SD-SALIENCY-900 [111]. PARAMETER STATISTICS PARAMS (M) FOR DATA-DRIVEN APPROACHES ARE ENCAPSULATED WITHIN THE SECOND-TO-LAST COLUMN. [ZOOM IN FOR BETTER VIEW] Methods Venue NEU-Seg [47] SD-saliency-900 [111] Sm MAE IoU F1 Sm MAE IoU F1 Params (M) Time (s) CPU/GPU Model-Based Methods WLRR [91] HLR [125] MCITF [48] SPL17 Neurocomput.20 OLE20 Deep Learning-Based Methods U-Net [24] U-Net++ [119] Attention UNet [121] ResUNet [126] U2Net [127] EDRNet [111] MCNet [128] DACNet [129] A-Net [130] MICCAI15 DLMIA18 arXiv18 ISPRS20 PR20 TIM20 TIM21 TIM22 TIM24 Deep Unfolding-Based Methods RPCANet [41] Ours - S3 Ours - S6 Ours - S9 WACV24 - - - 43.79 43.79 43.86 85.43 85.62 85.33 85.28 86.44 85.88 85.95 85.88 85.40 84.19 85.31 85.75 86. 0.1249 0.1247 0.1250 0.0297 0.0298 0.0305 0.0304 0.0289 0.0283 0.0301 0.0301 0.0297 0.0346 0.0295 0.0273 0.0276 27.62 29.75 39.79 79.25 79.35 78.80 79.09 78.64 80.45 79.06 79.61 79.26 43.29 45.86 56. 88.42 88.49 88.15 88.32 88.04 89.17 88.30 88.65 88.43 76.22 78.99+2.77 80.32+4.10 80.10+3.88 86.51 88.26+1.65 89.09+2.58 88.95+2.44 44.13 44.13 44.22 88.02 88.21 87.70 89.00 88.87 89.13 85.38 90.54 85.90 87.92 87.76 88.45 89. 0.1181 0.1179 0.1182 0.0203 0.0200 0.0213 0.0183 0.0188 0.0180 0.0240 0.0172 0.0247 0.0243 0.0235 0.0215 0.0208 25.92 27.19 40.86 84.82 85.04 84.10 85.27 86.08 86.66 82.28 87.58 81.96 41.17 42.75 58. 91.78 91.92 91.36 92.05 92.52 92.85 90.28 93.38 90.08 81.28 81.81+0.53 83.30+2.02 83.80+2.52 89.68 90.00+0.32 90.89+1.21 91.18+1.50 - - - 34.526 2.294 2.184 0.905 1.130 39.307 38.437 98.390 0.390 0.680 1.435 2.915 4. 0.3324 0.2047 17.8710 0.0141 0.0139 0.0134 0.0117 0.0344 0.0487 0.0604 0.0444 0.0265 0.0200 0.0247 0.0357 0.0499 * Due to memory constraints, we adopt the initial channel number of 16 for UNet++, ResUNet, and Attention UNet. learning-based methods, conventional networks like U-Net, UNet++, and ResUNet achieve competitive results, but they come with large parameter counts (e.g., U-Net has 34.526 parameters) and varying sensitivity to fine structures in medical images. In contrast, the deep unfolding-based methods show an attractive balance between performance and compactness. Our earlier RPCANet, with only 0.680 parameters, already demonstrates respectable performance. The enhanced RPCANet++ series further elevates these results. Specifically, the RPCANet++- S3 gains modest improvements in F1 (82.24% on DRIVE) and IoU (69.86% on DRIVE) compared to RPCANet. The six-stage RPCANet++ model achieves the best overall performance on DRIVE with an Acc of 96.91% and leads on CHASE DB1 with notable enhancements of +2.07% in F1 and +2.81% in IoU. While the RPCANet++-S9 shows slight drop in some metrics, it still outperforms most conventional approaches, demonstrating the robustness of our framework. Computational Efficiency: Despite moderate increase in parameters (from 1.435 for S3 to 4.396 for S9), the RPCANet++ series maintains competitive runtimes (typically, our S3 is under 0.027 s), making it relatively efficient. these Overall, the results underscore RPCANet++ series, which consistently achieves state-of-the-art segmentation performance and remarkable computational efficiency across diverse medical imaging datasets. the merits of"
        },
        {
            "title": "4.4.3 Comparison on Defect Detection Task",
            "content": "In the comparison section, we benchmark our approach against several state-of-the-art DD methodologies. For model-driven baselines, we incorporate: WLRR [91], HLR [125], and MCITF [48]. For data-driven framework, we evaluate frameworks UNet [24], U-Net++ [119], Attention UNet [121], ResUNet [120], 14 (a) IoU Performance (b) F1 Performance Fig. 18. Performance comparison of RPCANet++, RPCANet, U-Net, and ANet across varying target area distributions in the SD-saliency-900 dataset [111] for the DD task. (Target size distribution: <1%106 images (11.78%), 1%-5%305 images (33.89%), 5%-10%238 images (26.44%), 10%-15%99 images (11.00%), >15%152 images (16.89%)). U2Net [127], EDRNet [111], MCNet [128], DACNet [129], A-Net [130], and the DUN-based RPCANet [41]. Despite RPCANet, we retrained listed DL methods referring to their public codes. Visual Comparison: Fig. 17 presents mask segmentation results from two defect datasets under challenging conditions. Many optimization-based methods, such as MCITF, effectively capture salient features, although they sometimes overlook small defects (e.g., HLR in Sc 193). In contrast, deep learning methods can reliably predict defect shapes by leveraging ground truth labels; however, unexpected adhesion issues can occur when capturing fine details (e.g., Pa 59 in MCNet and A-Net). Notably, RPCANet++ consistently detects all defect targets with high shape accuracy. It delineates the background from the target even when boundaries are blurred (e.g., 598 in NEU-Seg and In 28 from SD-saliency-900), thereby demonstrating its superior capability in preserving shape details in cluttered backgrounds. Numerical Comparison: Table. 11 illustrates the performance on two defect detection datasets. Among the deep unfolding-based methods, our baseline RPCANet achieves an IoU of 76.22% and an F1 score of 86.51% on NEU-Seg, and 81.28% IoU and 89.68% F1 on SD-saliency-900. RPCANet++ series consistently improves upon these results. For instance, S3 variant raises the IoU on NEU-Seg by 2.77% (to 78.99%) and boosts the F1 score by 1.65%, while yielding modest gains on SD-saliency-900. RPCANet++- S6 further enhances performance, achieving an IoU of 80.32% (+4.10%) and an F1 score of 89.09% on NEU-Seg, with similar improvements on SD-saliency-900. Although the nine-stage RPCANet++ demonstrates comparable trends with satisfying Sm value, its overall gains are slightly lower than those achieved by the six-stage RPCANet++. Computational Efficiency: RPCANet++ achieves strong performance with 2.915M parameters (S6), significantly fewer than DACNets 98.390M. It also offers faster inference0.0247s (S3) vs. 0.0604s (MCNet) and 0.0487s (EDRNet)highlighting its efficiencyeffectiveness balance for defect detection. Notably, in the defect detection task, the RPCANet++ series does not achieve the pronounced advantage observed in other scenarios (e.g., VS and IRSTD). plausible explanation is that the defects in these datasets are typically larger and less sparse, making the sparse representation assumptioncentral to our frameworkless effective when target regions occupy substantial portion of the image. Fig. 19. Failure examples that both compared methods and RPCANet++ has encountered in SIRST [43] and IRSTD-1K [106] in IRSTD tasks."
        },
        {
            "title": "4.5 Limitations and Future Work",
            "content": "In this section, we discuss the limitations and corresponding solutions or future work of RPCANet++. 1) Focus on sparse object segmentation: As shown in Fig. 18, we performed detailed analysis of target area distributions and their influence on network performance. The results reveal that the RPCANet family excels at segmenting extremely small objects (target area < 1%). However, as object size increasesespecially beyond 15%both RPCANet variants exhibit diminished competitiveness compared to U-Net and its derivatives, such as A-Net. These findings highlight the strength of RPCANet-based approaches in handling sparse or tiny object segmentation. Future work will explore applying the algorithm to scenarios where target size plays critical role. 2) Sparse-like target elimination: On one hand, as presented in Fig. 19, due to the inherent sparsity of the target in the RPCA model, it may lead to falsely distinguishing target and sparse and target-like false alarms, which is also true for both data and model-driven methods. We plan to introduce spatial-temporal or multi-modal information in future work. For instance, temporal information could better assist the separation of moving targets and still prevent false alarms. Moreover, the possible injection of background awareness (such as pre-segmentation priors [131]) can reduce the false alarm rate. 3) Balancing parameter efficiency and interpretability: RPCANet series adopts fully convolutional architecture without downsampling to preserve interpretable operations (e.g., OEM construction). However, this design results in suboptimal inference speed. Compared to dense networks such as UIUNet and DACNet, RPCANet++ achieves relatively low parameter count. promising future direction is to develop more efficient architectures that maintain mathematical rigor while"
        },
        {
            "title": "5 CONCLUSION\nIn this work, we introduce RPCANet++—an interpretable deep\nunfolding network that bridges optimization theory with deep",
            "content": "learning for sparse object segmentation. By unfolding relaxed RPCA model into three modular stagesbackground approximation (BAM), object extraction (OEM), and image restoration (IRM)our approach combines theoretical rigor with neural efficiency. To counteract degradation of background features across unfolding stages, we incorporate memory augmentation (MAM), which enhances latent background estimation and improves sparse object detection. Furthermore, we boost target sensitivity by introducing local contrast-driven prior (DCPM), extending the original RPCA formulation. We validate the alignment of RPCANet++ with RPCA principles using novel low-rankness and sparsity metrics that quantify stage-wise decomposition quality, supplemented by interpretability analyses. Our results confirm that RPCANet++ iteratively learns low-rank backgrounds and sparse components following theoretical constraints, validating the architectures effectiveness. Extensive experiments across range of sparse object segmentation tasksincluding infrared small target detection (IRSTD), vessel segmentation (VS), and defect detection (DD)demonstrate the superior performance of our method. By systematically integrating model-driven constraints with data-driven optimization, the RPCANet series offers principled and interpretable alternative to conventional black-box architectures. This work paves the way for more trustworthy segmentation frameworks and contributes solid foundation for future research in interpretable deep learning."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The authors acknowledge the insightful critiques from the editorial team and anonymous reviewers, which have substantially refined this manuscript."
        },
        {
            "title": "APPENDIX A",
            "content": "In this section, the feasibility of Taylors second-order expansion under the Lipschitz continuous condition is analyzed and derived. Theorem 1. For function (t), with its Lipschitz continuous gradient function (t) (i.e., t1, t2 : (t1) (t2) t1 t2, where is constant), then (t) can be Taylor approximated at the fix point t0 by: ˆf (t, t0) (t0) + (t0), t0 + 2 (cid:13) (cid:13) t0 + (cid:13) (cid:13) (cid:13) 2 (cid:13) (t0) (cid:13) (cid:13)"
        },
        {
            "title": "1\nL",
            "content": "+ C. 2 t02 (A.1) Proof. For given continuously differentiable function (t), the Taylor second-order expansion at the point t0 is1: t02. ˆf (t, t0) (t0) + (t0), t0 + (A.2) 2 This leads to the following derivation: ˆf (t, t0) (t0) + (t0), t0 + (cid:18) = 2 t02 + 2 (cid:28) 1 (t0), t0 t02 2 (cid:29)"
        },
        {
            "title": "1\nL",
            "content": "(cid:13) 2 (cid:13) (t0) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) t0 + (cid:13) (cid:13) + 2 = (cid:13) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nL",
            "content": "f (t0) (cid:13) 2 (cid:13) (cid:13) (cid:13) +"
        },
        {
            "title": "2\nL",
            "content": "(cid:33) (t0) (A.3) (cid:13) 2 (cid:13) (t0) (cid:13) (cid:13)"
        },
        {
            "title": "1\nL",
            "content": "1 2L (t0)2 + (t0). 1. Please refer to http://www.seas.ucla.edu/vandenbe/236C/lectures/ gradient.pdf, page 14-15. And in optimization problems, we often have fixed point t0, such that: ˆf (t, t0) 2 (cid:13) (cid:13) t0 + (cid:13) (cid:13) (cid:13) 2 (cid:13) (t0) (cid:13) (cid:13) 2L (t0)2 + (t0). + C,"
        },
        {
            "title": "1\nL",
            "content": "where is constant, = 1 Thus, Theorem 1 is proved. 15 (A.4)"
        },
        {
            "title": "REFERENCES",
            "content": "[1] T. Bouwmans, S. Javed, H. Zhang, Z. Lin, and R. Otazo, On the applications of robust pca in image and video processing, Proceedings of the IEEE, vol. 106, no. 8, pp. 14271457, 2018. [6] [5] [4] [3] [2] H. Ji, S. Huang, Z. Shen, and Y. Xu, Robust video restoration by joint sparse and low rank matrix approximation, SIAM J. Imaging Sci., vol. 4, no. 4, pp. 11221142, 2011. Y.-Q. Zhao and J. Yang, Hyperspectral image denoising via sparse representation and low-rank constraint, IEEE Trans. Geosci. Remote Sens., vol. 53, no. 1, pp. 296308, 2014. X. Liu, G. Zhao, J. Yao, and C. Qi, Background subtraction based on low-rank and structured sparse decomposition, IEEE Trans. Image Process., vol. 24, no. 8, pp. 25022514, 2015. C. Zhang, J. Liu, Q. Tian, C. Xu, H. Lu, and S. Ma, Image classification by non-negative sparse coding, low-rank and sparse decomposition, in CVPR 2011, 2011, pp. 16731680. J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma, Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization, in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 22, 2009. Z. Zhou, X. Li, J. Wright, E. Candes, and Y. Ma, Stable principal component pursuit, in Proc. IEEE Int. Symp. Inf. Theory (ISIT), 2010, pp. 15181522. P. Netrapalli, N. UN, S. Sanghavi, A. Anandkumar, and P. Jain, Nonconvex robust pca, in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 27, 2014. L. Zhang, L. Peng, T. Zhang, S. Cao, and Z. Peng, Infrared small target detection via non-convex rank approximation minimization joint 2, 1 norm, Remote Sens., vol. 10, no. 11, p. 1821, 2018. J. Wang, G. Xu, C. Li, Z. Wang, and F. Yan, Surface defects detection using non-convex total variation regularized rpca with kernelization, IEEE Trans. Instrum. Meas., vol. 70, pp. 113, 2021. [10] [8] [9] [7] [11] E. J. Cand`es, X. Li, Y. Ma, and J. Wright, Robust principal component analysis? J. ACM, vol. 58, no. 3, pp. 137, 2011. [12] X. Yi, D. Park, Y. Chen, and C. Caramanis, Fast algorithms for robust pca via gradient descent, in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 29, 2016. [13] X. Yuan and J. Yang, Sparse and low-rank matrix decomposition via alternating direction methods, preprint, vol. 12, no. 2, 2009. [14] X. Cui, J. Huang, S. Zhang, and D. N. Metaxas, Background subtraction using low rank and group sparsity constraints, in Proc. Eur. Conf. Comput. Vis. (ECCV), 2012, pp. 612625. [15] H. Peng, B. Li, H. Ling, W. Hu, W. Xiong, and S. J. Maybank, Salient object detection via structured matrix decomposition, IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 4, pp. 818832, 2016. [16] T. Zhang, H. Wu, Y. Liu, L. Peng, C. Yang, and Z. Peng, Infrared small target detection based on non-convex optimization with lp-norm constraint, Remote Sens., vol. 11, no. 5, p. 559, 2019. [17] H. Zhu, H. Ni, S. Liu, G. Xu, and L. Deng, Tnlrs: Target-aware non-local low-rank modeling with saliency filtering regularization for infrared small target detection, IEEE Trans. Image Process., vol. 29, pp. 95469558, 2020. [19] [18] Y. Xie, S. Gu, Y. Liu, W. Zuo, W. Zhang, and L. Zhang, Weighted schatten p-norm minimization for image denoising and background subtraction, IEEE Trans. Image Process., vol. 25, no. 10, pp. 48424857, 2016. S. Xia, H. Zhu, X. Liu, M. Gong, X. Huang, L. Xu, H. Zhang, and J. Guo, Vessel segmentation of x-ray coronary angiographic image sequence, IEEE Trans. Biomed. Eng., vol. 67, no. 5, pp. 13381348, 2019. [20] X. Shen and Y. Wu, unified approach to salient object detection via low rank matrix recovery, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2012, pp. 853860. [21] X. Zhou, C. Yang, and W. Yu, Moving object detection by detecting contiguous outliers in the low-rank representation, IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 3, pp. 597610, 2012. [22] H. Zhu, S. Liu, L. Deng, Y. Li, and F. Xiao, Infrared small target detection via low-rank tensor completion with top-hat regularization, IEEE Trans. Geosci. Remote Sens., vol. 58, no. 2, pp. 10041016, 2019. [23] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie, Feature pyramid networks for object detection, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2017, pp. 21172125. [24] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in Proc. Med. Image Comput. Comput. Assist Interv. (MICCAI), 2015, pp. 234241. [49] Y. Liu, T. Liu, J. Liu, and C. Zhu, Smooth robust tensor principal component analysis for compressed sensing of dynamic mri, Pattern Recognit., vol. 102, p. 107252, 2020. 16 [25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 30, 2017. [26] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2023, pp. 40154026. [27] Y. Yang, J. Sun, H. Li, and Z. Xu, Deep admm-net for compressive sensing mri, in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 29, 2016, pp. 1018. [28] F. Shen and H. Gan, Hunet: Homotopy unfolding network for image compressive sensing, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2025, pp. 12 79912 808. [29] C. Ren, X. He, C. Wang, and Z. Zhao, Adaptive consistency prior based deep network for image denoising, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2021, pp. 85968606. [30] H. Zheng, H. Yong, and L. Zhang, Deep convolutional dictionary learning for image denoising, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2021, pp. 630641. [31] K. Zhang, L. V. Gool, and R. Timofte, Deep unfolding network for image super-resolution, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020, pp. 32173226. [33] [32] M. Zhou, K. Yan, J. Pan, W. Ren, Q. Xie, and X. Cao, Memoryaugmented deep unfolding network for guided image superresolution, Int. J. Comput. Vis., vol. 131, no. 1, pp. 215242, 2023. S. Kong, W. Wang, X. Feng, and X. Jia, Deep red unfolding network for image restoration, IEEE Trans. Image Process., vol. 31, pp. 852867, 2021. J. Fu, Q. Xie, D. Meng, and Z. Xu, Rotation equivariant proximal operator for deep unfolding methods in image restoration, IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 10, pp. 65776593, 2024. [34] [35] H. Cai, J. Liu, and W. Yin, Learned robust pca: scalable deep unfolding approach for high-dimensional outlier detection, Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 34, pp. 16 97716 989, 2021. [36] B. Joukovsky, Y. C. Eldar, and N. Deligiannis, Interpretable neural networks for video separation: Deep unfolding rpca with foreground masking, IEEE Trans. Image Process., 2024. [37] X. Zhou, P. Li, Y. Zhang, X. Lu, and Y. Hu, Deep low-rank and sparse patch-image network for infrared dim and small target detection, IEEE Trans. Geosci. Remote Sens., 2023. J. Zhang, B. Chen, R. Xiong, and Y. Zhang, Physics-inspired compressive sensing: Beyond deep unrolling, IEEE Signal Process. Mag., vol. 40, no. 1, pp. 5872, 2023. [38] [39] T. Zhang, L. Li, and Z. Peng, Optimization-inspired cumulative transmission network for image compressive sensing, Knowl. Based Syst., vol. 279, p. 110963, 2023. [40] Y. Dai and Y. Wu, Reweighted infrared patch-tensor model with both nonlocal and local priors for single-frame small target detection, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 10, no. 8, pp. 3752 3767, 2017. [41] F. Wu, T. Zhang, L. Li, Y. Huang, and Z. Peng, Rpcanet: Deep unfolding rpca based infrared small target detection, in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), 2024, pp. 48094818. [42] B. An, S. Wang, F. Qin, Z. Zhao, R. Yan, and X. Chen, Adversarial algorithm unrolling network for interpretable mechanical anomaly detection, IEEE Trans. Neural. Netw. Learn. Syst., 2024. [43] Y. Dai, Y. Wu, F. Zhou, and K. Barnard, Asymmetric contextual modulation for infrared small target detection, in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), 2021, pp. 950959. [44] A. Asad, A. T. Azar, N. El-Bendary, A. E. Hassaanien et al., Ant colony based feature selection heuristics for retinal vessel segmentation, arXiv preprint arXiv:1403.1735, 2014. [45] A. Hoover, V. Kouznetsova, and M. Goldbaum, Locating blood vessels in retinal images by piecewise threshold probing of matched filter response, IEEE Trans. Med. Imag., vol. 19, no. 3, pp. 203210, 2000. [46] H. Y. Henry, X. Feng, Z. Wang, and H. Sun, Mixmodule: Mixed cnn kernel module for medical image segmentation, in Proc. Int. Symp. Biomed. Imaging (ISBI), 2020, pp. 15081512. [47] H. Dong, K. Song, Y. He, J. Xu, Y. Yan, and Q. Meng, Pga-net: Pyramid feature fusion and global context attention network for automated surface defect detection, IEEE Trans. Industr. Inform., vol. 16, no. 12, pp. 74487458, 2019. [48] G. Song, K. Song, and Y. Yan, Saliency detection for strip steel surface defects using multiple constraints and improved texture features, Opt. Laser. Eng., vol. 128, p. 106000, 2020. [50] Y. Chang, L. Yan, X.-L. Zhao, H. Fang, Z. Zhang, and S. Zhong, Weighted low-rank tensor recovery for hyperspectral image restoration, IEEE Trans. Cybern., vol. 50, no. 11, pp. 45584572, 2020. [51] X. Zhou, C. Yang, H. Zhao, and W. Yu, Low-rank modeling and its applications in image analysis, ACM Comput. Surv. (CSUR), vol. 47, no. 2, pp. 133, 2014. S. E. Ebadi and E. Izquierdo, Foreground segmentation with treestructured sparse rpca, IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 9, pp. 22732280, Sep. 2018. S. Jung, Backgroundforeground modeling based on spatiotemporal sparse subspace clustering, IEEE Trans. Image Process., vol. 26, no. 12, pp. 5840 5854, Dec 2017. Javed, A. Mahmood, T. Bouwmans, and S. K. [53] [52] [54] A. Borji, M.-M. Cheng, Q. Hou, H. Jiang, and J. Li, Salient object detection: survey, Comput. Vis. Media., vol. 5, pp. 117150, 2019. [55] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, The secrets of salient object segmentation, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2014, pp. 280287. [56] D.-P. Fan, M.-M. Cheng, J.-J. Liu, S.-H. Gao, Q. Hou, and A. Borji, Salient objects in clutter: Bringing salient object detection to the foreground, in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 186 202. [57] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, Salient object detection: discriminative regional feature integration approach, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2013, pp. 2083 2090. J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price, and R. Mech, Salient object subitizing, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2015, pp. 40454054. [58] [60] [59] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, and R. Yang, Salient object detection in the deep learning era: An in-depth survey, IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 6, pp. 32393259, 2021. J. Wang, Q. Li, J. Gan, H. Yu, and X. Yang, Surface defect detection via entity sparsity pursuit with intrinsic priors, IEEE Trans. Industr. Inform., vol. 16, no. 1, pp. 141150, Jan 2020. J. Ahmed, B. Gao, and W. lok Woo, Sparse low-rank tensor decomposition for metal defect detection using thermographic imaging diagnostics, IEEE Trans. Industr. Inform., vol. 17, no. 3, pp. 18101820, 2020. [61] [62] H. Qin, M. Sun, Z. Lu, and J. Tan, Metallic mesh defect detection based on low-rank decomposition via schatten capped norm, IEEE Trans. Instrum. Meas., 2024. [63] P.-H. Lee, C.-C. Chan, S.-L. Huang, A. Chen, and H. H. Chen, Extracting blood vessels from full-field oct data of human skin by short-time rpca, IEEE Trans. Med. Imag., vol. 37, no. 8, pp. 18991909, Aug 2018. [64] B. Qin, M. Jin, D. Hao, Y. Lv, Q. Liu, Y. Zhu, S. Ding, J. Zhao, and B. Fei, Accurate vessel extraction via tensor completion of background layer in x-ray coronary angiograms, Pattern Recognit., vol. 87, pp. 3854, 2019. [65] Y. Fu, Y. Wang, Y. Zhong, D. Fu, and Q. Peng, Change detection based on tensor rpca for longitudinal retinal fundus images, Neurocomputing, vol. 387, pp. 112, 2020. [66] X. Ying, C. Xiao, W. An, R. Li, X. He, B. Li, X. Cao, Z. Li, Y. Wang, M. Hu, Q. Xu, Z. Lin, M. Li, S. Zhou, L. Liu, and W. Sheng, Visiblethermal tiny object detection: benchmark dataset and baselines, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 7, pp. 60886096, 2025. [67] C. Gao, D. Meng, Y. Yang, Y. Wang, X. Zhou, and A. G. Hauptmann, Infrared patch-image model for small target detection in single image, IEEE Trans. Image Process., vol. 22, no. 12, pp. 49965009, 2013. [68] X. Wang, Z. Peng, D. Kong, P. Zhang, and Y. He, Infrared dim target detection based on total variation regularization and principal component pursuit, Image Vis. Comput., vol. 63, pp. 19, 2017. [69] T. Liu, J. Yang, B. Li, C. Xiao, Y. Sun, Y. Wang, and W. An, Nonconvex tensor low-rank approximation for infrared small target detection, IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 118, 2021. [70] F. Wu, H. Yu, A. Liu, J. Luo, and Z. Peng, Infrared small target detection using spatiotemporal 4-d tensor train and ring unfolding, IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 122, 2023. [71] L. Zhang and Z. Peng, Infrared small target detection based on partial sum of the tensor nuclear norm, Remote Sens., vol. 11, no. 4, p. 382, 2019. [72] X. Kong, C. Yang, S. Cao, C. Li, and Z. Peng, Infrared small target detection via nonconvex tensor fibered rank approximation, IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 121, 2021. [73] Y. Wang, P.-M. Jodoin, F. Porikli, J. Konrad, Y. Benezeth, and P. Ishwar, Cdnet 2014: An expanded change detection benchmark dataset, in [74] Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), 2014, pp. 387394. J. Staal, M. D. Abr`amoff, M. Niemeijer, M. A. Viergever, and B. Van Ginneken, Ridge-based vessel segmentation in color images of the retina, IEEE Trans. Med. Imag., vol. 23, no. 4, pp. 501509, 2004. [75] T. Zhang, L. Li, S. Cao, T. Pu, and Z. Peng, Attention-guided pyramid context networks for detecting infrared small target under complex background, IEEE Trans. Aerosp. Electron. Syst., vol. 59, no. 4, pp. 4250 4261, 2023. [76] K. Gregor and Y. LeCun, Learning fast approximations of sparse coding, in Proc. 27th Int. Conf. Int. Conf. Mach. Learn. (ICML), 2010, pp. 399406. J. Zhang and B. Ghanem, Ista-net: Interpretable optimization-inspired deep network for image compressive sensing, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018, pp. 18281837. [77] [78] Z. Zhang, Y. Liu, J. Liu, F. Wen, and C. Zhu, Amp-net: Denoisingbased deep unfolding for compressive image sensing, IEEE Trans. Image Process., vol. 30, pp. 14871500, 2021. [79] Y. Su and Q. Lian, ipiano-net: Nonconvex optimization inspired multi-scale reconstruction network for compressed sensing, Signal Process., Image Commun., vol. 89, p. 115989, 2020. [80] O. Solomon, R. Cohen, Y. Zhang, Y. Yang, Q. He, J. Luo, R. J. van Sloun, and Y. C. Eldar, Deep unfolded robust pca with application to clutter suppression in ultrasound, IEEE Trans. Med. Imaging, vol. 39, no. 4, pp. 10511063, 2020. [82] [81] W. Huang, Z. Ke, Z.-X. Cui, J. Cheng, Z. Qiu, S. Jia, L. Ying, Y. Zhu, and D. Liang, Deep low-rank plus sparse network for dynamic mr imaging, Med. Image Anal., vol. 73, p. 102190, 2021. J. Yan, K. Zhang, F. Zhang, C. Ge, W. Wan, and J. Sun, Multispectral and hyperspectral image fusion based on low-rank unfolding network, Signal Process., vol. 213, p. 109223, 2023. J. Chen, Y. Sun, Q. Liu, and R. Huang, Learning memory augmented cascading network for compressed sensing of images, in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020, pp. 513529. J. Song, B. Chen, and J. Zhang, Memory-augmented deep unfolding network for compressive sensing, in Proc. ACM Int. Conf. Multimedia (ACM MM), 2021, pp. 42494258. [84] [83] [85] D. You, J. Xie, and J. Zhang, Ista-net++: Flexible deep unfolding network for compressive sensing, in Proc. Int. Conf. Multimedia Expo (ICME). J. Zhang, Y. Li, Z. L. Yu, Z. Gu, Y. Cheng, and H. Gong, Deep unfolding with weighted l2 minimization for compressive sensing, IEEE Internet Things J., vol. 8, no. 4, pp. 30273041, 2020. IEEE, 2021, pp. 16. [86] [87] C. Yang, S. Zhang, and X. Yuan, Ensemble learning priors driven deep unfolding for scalable video snapshot compressive imaging, in Proc. Eur. Conf. Comput. Vis. (ECCV), 2022, pp. 600618. [88] X. Wei, H. Van Gorp, L. Gonzalez-Carabarin, D. Freedman, Y. C. Eldar, and R. J. van Sloun, Deep unfolding with normalizing flow priors for inverse problems, IEEE Trans. Signal Process., vol. 70, pp. 29622971, 2022. S. Imran, M. Tahir, Z. Khalid, and M. Uppal, deep unfolded prioraided rpca network for cloud removal, IEEE Signal Process. Lett., vol. 29, pp. 20482052, 2022. [89] [90] Z. Yu, C. Zhao, Z. Wang, Y. Qin, Z. Su, X. Li, F. Zhou, and G. Zhao, Searching central difference convolutional networks for face antispoofing, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020, pp. 52955305. [91] C. Tang, P. Wang, C. Zhang, and W. Li, Salient object detection via weighted low rank matrix recovery, IEEE Signal Process. Lett., vol. 24, no. 4, pp. 490494, 2016. [92] T. Zhang, Z. Peng, H. Wu, Y. He, C. Li, and C. Yang, Infrared small target detection via self-regularized weighted sparse model, Neurocomputing, vol. 420, pp. 124148, 2021. [93] P. Tseng, On accelerated proximal gradient methods for convex- [94] concave optimization, SIAM J. Opti., 2008. S. Ma, D. Goldfarb, and L. Chen, Fixed point and bregman iterative methods for matrix rank minimization, Math. Program., vol. 128, no. 1, pp. 321353, 2011. [95] T. Zhang, L. Li, C. Igel, S. Oehmcke, F. Gieseke, and Z. Peng, Lr-csnet: low-rank deep unfolding network for image compressive sensing, in Proc. Int. Conf. Comput. Commun. (ICCC), 2022, pp. 19511957. [96] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, Convolutional lstm network: machine learning approach for precipitation nowcasting, in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 28, 2015. [97] X. Ying, Y. Wang, L. Wang, W. Sheng, L. Liu, Z. Lin, and S. Zhou, Local motion and contrast priors driven deep network for infrared small target superresolution, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15, pp. 54805495, 2022. 17 [98] M. Zhang, X. Li, F. Gao, and J. Guo, Irmamba: Pixel difference mamba with layer restoration for infrared small target detection, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 9, 2025, pp. 10 00310 011. [99] F. Wu, A. Liu, T. Zhang, L. Zhang, J. Luo, and Z. Peng, Saliency at the helm: Steering infrared small target detection with learnable kernels, IEEE Trans. Geosci. Remote Sens., vol. 63, pp. 114, 2025. [100] J. Song, B. Chen, and J. Zhang, Deep memory-augmented proximal unrolling network for compressive sensing, Int. J. Comput. Vis., vol. 131, no. 6, pp. 14771496, 2023. [101] H. Wang, Y. Li, H. Zhang, D. Meng, and Y. Zheng, Indudonet+: deep unfolding dual domain network for metal artifact reduction in ct images, Med. Image Analy., vol. 85, p. 102729, 2023. [102] J. Mifdal, M. Tomas-Cruz, A. Sebastianelli, B. Coll, and J. Duran, Deep unfolding for hypersharpening using high-frequency injection module, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), 2023, pp. 21052114. [103] A. Virmaux and K. Scaman, Lipschitz regularity of deep neural networks: analysis and efficient estimation, in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 31, 2018. [104] J. Hu, L. Shen, and G. Sun, Squeeze-and-excitation networks, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018, pp. 71327141. [105] Y. Huang, Z. Tang, D. Chen, K. Su, and C. Chen, Batching soft iou for training semantic segmentation networks, IEEE Signal Process. Lett., vol. 27, pp. 6670, 2020. [106] M. Zhang, R. Zhang, Y. Yang, H. Bai, J. Zhang, and J. Guo, Isnet: Shape matters for infrared small target detection, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2022, pp. 877886. [107] B. Li, C. Xiao, L. Wang, Y. Wang, Z. Lin, M. Li, W. An, and Y. Guo, Dense nested attention network for infrared small target detection, IEEE Trans. Image Process., vol. 32, pp. 17451758, 2022. [108] X. Ying, L. Liu, Y. Wang, R. Li, N. Chen, Z. Lin, W. Sheng, and S. Zhou, Mapping degeneration meets label evolution: Learning infrared small target detection with single point supervision, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2023, pp. 15 52815 538. [109] M. M. Fraz, P. Remagnino, A. Hoppe, B. Uyyanonvara, A. R. Rudnicka, C. G. Owen, and S. A. Barman, An ensemble classification-based approach applied to retinal blood vessel segmentation, IEEE Trans. Biomed. Eng., vol. 59, no. 9, pp. 25382548, 2012. [110] W. Liu, H. Yang, T. Tian, Z. Cao, X. Pan, W. Xu, Y. Jin, and F. Gao, Fullresolution network and dual-threshold iteration for retinal vessel and coronary angiograph segmentation, IEEE J. Biomed. Health Inform., vol. 26, no. 9, pp. 46234634, 2022. [111] G. Song, K. Song, and Y. Yan, Edrnet: Encoderdecoder residual network for salient object detection of strip steel surface defects, IEEE Trans. Instrum. Meas., vol. 69, no. 12, pp. 97099719, 2020. [112] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014. [113] Y. Yu, S. Buchanan, D. Pai, T. Chu, Z. Wu, S. Tong, B. Haeffele, and Y. Ma, White-box transformers via sparse rate reduction, in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 36, 2023, pp. 94229457. [114] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, Cbam: Convolutional block attention module, in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 319. [115] Y. Wei, X. You, and H. Li, Multiscale patch-based contrast measure for small infrared target detection, Pattern Recognit., vol. 58, pp. 216226, 2016. [116] X. Wu, D. Hong, and J. Chanussot, Uiu-net: U-net in u-net for infrared small object detection, IEEE Trans. Image Process., vol. 32, pp. 364376, 2023. [117] Q. Liu, R. Liu, B. Zheng, H. Wang, and Y. Fu, Infrared small target detection with scale and location sensitivity, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2024, pp. 17 49017 499. [118] Y. Dai, Y. Wu, F. Zhou, and K. Barnard, Attentional local contrast networks for infrared small target detection, IEEE Trans. Geosci. Remote Sens., vol. 59, no. 11, pp. 98139824, 2021. [119] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, Unet++: nested u-net architecture for medical image segmentation, in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 311. [120] D. Li, D. A. Dharmawan, B. P. Ng, and S. Rahardja, Residual u-net for retinal vessel segmentation, in Proc. Int. Conf. Image Process. (ICIP). IEEE, 2019, pp. 14251429. [121] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz et al., Attention u-net: Learning where to look for the pancreas, arXiv preprint arXiv:1804.03999, 2018. [122] L. Mou, Y. Zhao, L. Chen, J. Cheng, Z. Gu, H. Hao, H. Qi, Y. Zheng, A. Frangi, and J. Liu, Cs-net: channel and spatial attention network for curvilinear structure segmentation, in Proc. Med. Image Comput. Comput. Assist. Interv. (MICCAI), 2019, pp. 721730. [123] W. Zhou, W. Bai, J. Ji, Y. Yi, N. Zhang, and W. Cui, Dual-path multiscale context dense aggregation network for retinal vessel segmentation, Comput. Biol. Med., vol. 164, p. 107269, 2023. [124] M. M. Rahman and R. Marculescu, G-cascade: Efficient cascaded graph convolutional decoding for 2d medical image segmentation, in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), 2024, pp. 7728 7737. [125] Q. Zheng, S. Yu, and X. You, Coarse-to-fine salient object detection with low-rank matrix recovery, Neurocomputing, vol. 376, pp. 232243, 2020. [126] F. I. Diakogiannis, F. Waldner, P. Caccetta, and C. Wu, Resunet-a: deep learning framework for semantic segmentation of remotely sensed data, ISPRS J. Photogramm. Remote Sens., vol. 162, pp. 94114, 2020. [127] X. Qin, Z. Zhang, C. Huang, M. Dehghan, O. R. Zaiane, and M. Jagersand, U2-net: Going deeper with nested u-structure for salient object detection, Pattern Recognit., vol. 106, p. 107404, 2020. [128] D. Zhang, K. Song, J. Xu, Y. He, M. Niu, and Y. Yan, Mcnet: Multiple context information segmentation network of no-service rail surface defects, IEEE Trans. Instrum. Meas., vol. 70, pp. 19, 2021. [129] X. Zhou, H. Fang, Z. Liu, B. Zheng, Y. Sun, J. Zhang, and C. Yan, Dense attention-guided cascaded network for salient object detection of strip steel surface defects, IEEE Trans. Instrum. Meas., vol. 71, pp. 114, 2022. [130] B. Chen, T. Niu, W. Yu, R. Zhang, Z. Wang, and B. Li, A-net: An a-shape lightweight neural network for real-time surface defect segmentation, IEEE Trans. Instrum. Meas., vol. 73, pp. 114, 2024. [131] M. Xiao, Q. Dai, Y. Zhu, K. Guo, H. Wang, X. Shu, J. Yang, and Y. Dai, Background semantics matter: Cross-task feature exchange network for clustered infrared small target detection with sky-annotated dataset, arXiv preprint arXiv:2407.20078, 2024. Fengyi Wu received his B.E. degree in Electronic Information Engineering from both the University of Electronic Science and Technology of China (UESTC) in 2021 and is currently chasing PhD degree at the School of Information and Communication Engineering, UESTC. His current interests include computer vision, pattern recognition, deep unfolding, and interpretable object detection. Yimian Dai (Member, IEEE) received the B.E. degree in information engineering and the Ph.D. degree in signal and information processing from Nanjing University of Aeronautics and Astronautics, Nanjing, China, in 2013 and 2020, respectively. From 2021 to 2024, he was Postdoctoral Researcher with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China. He is currently an Associate Professor with the College of Computer Science, Nankai University, Tianjin, China. His research interests include computer vision, deep learning, and their applications in remote sensing. For more information, please visit the link (https://yimian.grokcv.ai/). Tianfang Zhang received his PhD degree from the School of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC) in 2023. Currently, he is working as postdoctoral fellow in the Department of Automation, Tsinghua University. His main research interests are computer vision, efficient vision transformers, and multi-modal models. 18 Yixuan Ding is currently pursuing bachelors degree in communication engineering from the College of Glasgow, the University of Electronic Science and Technology of China (UESTC), Chengdu, China. His recent research interests include target detection, compressive sensing, and deep unfolding. Jian Yang received the PhD degree from Nanjing University of Science and Technology (NJUST) in 2002, majoring in pattern recognition and intelligence systems. From 2003 to 2007, he was Postdoctoral Fellow at the University of Zaragoza, Hong Kong Polytechnic University and New Jersey Institute of Technology, respectively. From 2007 to present, he is professor in the School of Computer Science and Technology of NJUST. Currently, he is also visiting distinguished professor in the College of Computer Science of Nankai University. His papers have been cited over 50000 times in the Scholar Google. His research interests include pattern recognition and computer vision. Currently, he is/was an associate editor of Pattern Recognition, Pattern Recognition Letters, IEEE Trans. Neural Networks and Learning Systems, and Neurocomputing. He is Fellow of IAPR. Ming-Ming Cheng received his PhD degree from Tsinghua University in 2012. Then, he did 2 years research fellow, with Prof. Philip Torr in Oxford. He is now professor at Nankai University, leading the Media Computing Lab. His research interests include computer graphics, computer vision, and image processing. He received research awards, including the National Science Fund for Distinguished Young Scholars and the ACM China Rising Star Award. He is on the editorial boards of IEEE TPAMI and IEEE TIP. Zhenming Peng (Member, IEEE) received Ph.D. degree in geodetection and information technology from the Chengdu University of Technology, Chengdu, China, in 2001. From 2001 to 2003, he was PostDoctoral Researcher with the Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu. He is Professor with the University of Electronic Science and Technology of China, Chengdu. His research interests include image processing, signal processing, and target recognition and tracking."
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University, Beijing, China",
        "PCA Lab, VCIP, College of Computer Science, Nankai University, Tianjin 300350, China",
        "School of Information and Communication Engineering and the Laboratory of Imaging Detection and Intelligent Perception, University of Electronic Science and Technology of China, Chengdu, China"
    ]
}