{
    "paper_title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation",
    "authors": [
        "Wongyu Kim",
        "Hochang Lee",
        "Sanghak Lee",
        "Yoonsung Kim",
        "Jaehyun Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 5 3 2 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Let Multimodal Embedders Learn When to Augment Query via\nAdaptive Query Augmentation",
            "content": "Wongyu Kim wgkim@ncsoft.com NC AI Seongnam, Republic of Korea Hochang Lee hochang@ncsoft.com NC AI Seongnam, Republic of Korea Sanghak Lee sanghaklee@ncsoft.com NC AI Seongnam, Republic of Korea Yoonsung Kim yoonsungkim@ncsoft.com NC AI Seongnam, Republic of Korea Jaehyun Park jaehyunpark@ncsoft.com NC AI Seongnam, Republic of Korea Abstract Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces synthesis process that generates appropriate augmentations for queries that require them by leveraging powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency. Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. CIKM 25, Seoul, Republic of Korea. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN XXX-X-XXXX-XXXX-X https://doi.org/XXXXXXX.XXXXXXX CCS Concepts Information systems Query representation; Retrieval models and ranking. Keywords Multimodal Information Retrieval, Data Synthesis, Adaptive Query Augmentation ACM Reference Format: Wongyu Kim, Hochang Lee, Sanghak Lee, Yoonsung Kim, and Jaehyun Park. 2025. Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation. In Proceedings of the 34th ACM International Conference on Information and Knowledge Management (CIKM 25), November 1014, 2025, Seoul, Republic of Korea (CIKM 25). ACM, New York, NY, USA, 7 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nIn many embedding tasks, query augmentation has been used for\nretrieving relevant documents by appending useful information\nto queries. Recent studies have proposed Large Language Model\n(LLM)-based embedders which jointly learn representation for em-\nbedding and generation for query augmentation by leveraging the\ngenerative capabilities of LLM [19, 25]. During inference, these\nembedders trained in the multi-task manner have conducted query\naugmentation followed by embedding, which has led to more mean-\ningful query representations and stronger performance. However,\nprevious studies did not consider the following three points: (1)\nAugmenting every query leads to significant embedding latency. (2)\nQuery augmentation can degrade performance for some queries. (3)\nThe effectiveness has not been demonstrated in multimodal envi-\nronments. To verify these challenges, we conducted a pilot study as\nshown in Figure 1. We trained two models: one that performs only\nembedding, similar to typical embedders, and the other that carries\nout query augmentation before embedding. The former employed\nwidely used contrastive loss for training [3], while the latter is\ntrained by referring to [25]. In Figure 1, the model without augmen-\ntation quickly retrieved the relevant image document, while the\nmodel that always uses augmentation did not. The bolded parts of\nthe augmentation were generated by misinterpreting and exagger-\nating the query. They pointed to finding clothes with Paris-related\ndesigns like the Eiffel Tower instead of just the word â€˜Parisâ€™, which\nwas expected to hinder finding the positive document.",
            "content": "CIKM 25, November 1004, 2025, Seoul, Republic of Korea. Kim et al."
        },
        {
            "title": "2 Related Work\nJoint Training of Embedding and Query Augmentation. Re-\ncent studies [19, 25] have developed embedders to learn embedding\nand query augmentation in a multi-task manner. By augmenting\nqueries before embedding, query representations have become more\ninformative. However, when augmenting every query, embedding\nlatency increases significantly and performance can decline. Also,\nprevious studies have not considered multimodal environments.\nOur approach addresses these problems.",
            "content": "Multimodal Embedders. Since the release of MMEB [9], multimodal embedding training collection and benchmark, numerous multimodal embedders have been developed by using various techniques such as efficient GPU utilization to increase batch size [9], data synthesis [2, 12, 30], hard negative sampling [6, 16], contrastiveautoregressive finetuning [27], distillation [6, 20], prompt refinement [10, 20], modality completion module [15], and optimization of contrastive loss [10, 11, 20, 24]. We propose novel method to improve performance by adaptively augmenting queries. Adaptive Generation. Recent LLMs have demonstrated that, for certain questions, generating direct answers with non-thinking mode can be more effective and efficient than answering with thinking mode [14, 26]. Several methods have been proposed to adaptively generate responses by automatically selecting the appropriate strategy between thinking and non-thinking modes for each question, enabling more effective and efficient test-time scaling [4, 8, 13, 22, 28, 29]. Inspired by these adaptive generation methods, we introduce adaptive query augmentation. , ğ‘›ğ‘– 2 , ..., ğ‘›ğ‘– ğ¸, ğ· 2 ğ´, ğ· ğ¸, . . . , ğ·ğ‘’ ğ´ and ğ· ğ‘£ ğ¸], where ğ·ğ‘¢"
        },
        {
            "title": "3 Methodology\nTask Definition. The training dataset collection contains [ğ· 1\nğ´, ğ· 2\nğ´,\n. . . , ğ·ğ‘\nğ¸ respectively denote a\ndataset that contains queries requiring augmentation and a dataset\nthat contains queries not requiring augmentation, ğ‘¢ and ğ‘£ are the\nindices for ğ·ğ´ and ğ·ğ¸, and ğ‘ and ğ‘’ each mean the sizes of ğ·ğ´\nand ğ·ğ¸. Each dataset has (ğ‘ğ‘–, ğ‘”ğ‘–, ğ‘ğ‘–, ğ‘›ğ‘–\nğ‘š) samples, where\n1\nğ‘ğ‘– , ğ‘ğ‘–, ğ‘›ğ‘–\nğ‘˜ and ğ‘š each indicate a query, a positive document, a hard\nnegative document and the nubmer of ğ‘›, and ğ‘– and ğ‘˜ are the indices\nfor samples and hard negative documents. ğ‘”ğ‘– denotes a synthetic\naugmentation with the prefix /augment if it belongs to ğ·ğ‘¢\nğ´ or the the\nsimple string /embed if it belongs to ğ· ğ‘£\nğ¸. The modalities of ğ‘ğ‘–, ğ‘ğ‘–, ğ‘›ğ‘–\nğ‘˜\nare text, image, or interleaved text and image, while the modality of\nğ‘”ğ‘– is text. M-Solomon aims to learn not only representing the aug-\nmented query (ğ‘ with ğ‘”) and retrieving the positive document (ğ‘)\nbut also adaptively generating the augmentation (ğ‘”). During eval-\nğµ, ..., ğ·ğ‘\nuation, on the benchmark that includes datasets [ğ· 1\nğµ],\nM-Solomon aims to find the positive document among document\ncandidates by using the augmented query. ğ‘ means the number of\nğ·ğµ.",
            "content": "ğµ, ğ·"
        },
        {
            "title": "3.1 Query Augmentation Synthesis\nThe core capability of M-Solomon lies in adaptively determining\nwhen to augment queries. Therefore, identifying which queries\nbenefit from augmentation and synthesizing augmentations for\nthose queries are necessary to construct the dataset collection that\nmakes M-Solomon acquire that capability. Referring to [8], we first\ndivide the queries of the training datasets at the dataset level by",
            "content": "Figure 1: An example of FashionIQ dataset in MMEB benchmark [9] for the pilot study. To tackle these challenges, based on the results of the pilot study, we propose M-Solomon, universal multimodal embedder that can adaptively determine when to augment queries. Referring to [8], our approach first divides the queries of the training datasets into two groups at the dataset level: one that contains queries that require augmentation and the other that contains queries that do not require augmentation. Subsequently, by leveraging highperformance Multimodal LLM (MLLM), we introduce synthesis process that generates appropriate answers for queries that require augmentation and these answers are regarded as augmentations [25]. Lastly, inspired by adaptive generation between thinking and non-thinking modes [4, 13, 29], we present jointly learning adaptive query augmentation in addition to learning representation for embedding. M-Solomon can augment queries only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand augmentation and to generate the simple string /embed for others. Producing these tokens at the beginning makes M-Solomon decide whether to augment each query. Experimental results on MMEB benchmark [9] showed that MSolomon substantially outperformed the baseline without augmentation by adaptively augmenting queries. Also, M-Solomon exhibited better performance compared to the baseline that constantly used augmentation, achieving significantly faster embedding latency. Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation CIKM 25, November 1004, 2025, Seoul, Republic of Korea. The User asks question (with an image), and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: {question}. Assistant: Table 1: Prompt template for query augmentation synthesis. During synthesis, queries will replace {question}. utilizing the models in the pilot study (Figure 1)1. Out of the 20 datasets in MMEB that contain both training and test sets, the model without augmentation showed better or comparable performance to the model that always uses augmentation on the test sets of the 10 datasets. As augmentation is less effective on these 10 datasets, we consider their queries as not requiring augmentation, while regarding the queries of the remaining 10 datasets as requiring augmentation. Consequently, the datasets are divided as follows: Requiring Augmentation: ChartQA, DocVQA, ImageNet_1K, InfographicsVQA, MSCOCO, OK-VQA, SUN397, VisDial, Visual7W, HatefulMemes Not Requiring Augmentation: A-OKVQA, CIRR, MSCOCO_i2t, MSCOCO_t2i, N24News, NIGHTS, VisualNews_i2t, VisualNews_t2i, VOC2007, WebQA As illustrated in the upper left part of Figure 2, for queries that require augmentation, we design synthesis process to generate augmentations by leveraging Qwen2.5-VL-72B-Instruct, powerful MLLM as teacher model [1]. Following the previous work [25], we regard answers to queries as augmentations because the answers include useful information. In the end, as shown in Table 1, we construct prompt template by referring to the template of [7] and feed them into the teacher model. The generated outputs include both the reasoning process and the answer, but we only extract and use the answer part."
        },
        {
            "title": "3.2 Adaptive Query Augmentation\nIn the upper right part of Figure 2, M-Solomon learns representation\nfor embedding and generation for adaptive query augmentation.",
            "content": "To learn adaptively augmenting queries, M-Solomon is trained to generate synthetic augmentations with the prefix /augment for queries that require augmentation, and to generate the simple string /embed for those that do not. This allows M-Solomon, given query, to automatically decide whether to produce /augment or /embed at the beginning, ultimately enabling M-Solomon to understand when to augment query. If /augment is produced, M-Solomon continues to produce augmentation. The objective function for adaptive query 1In the pilot study, the model without augmentation is trained by using contrastive loss in Equation 2 [3] like typical embedders. The model that always uses augmentation is trained under scenario where augmentation is applied to every query by using Equation 3 derived by referring to [25]. Therefore, the former performs only embedding, while the latter carries out query augmentation before embedding. Details on the training losses are described in Section 3.2. augmentation with autoregressive loss is as follows [27]: ğ‘‡ Lgen = log ğ‘ƒ (ğ‘”ğ‘¡ ğ‘, ğ‘”<ğ‘¡ ) (1) ğ‘¡ =1 where ğ‘” can be synthetic augmentation with the prefix /augment or the string /embed, ğ‘¡ is the position for the target token ğ‘”ğ‘¡ , and ğ‘ƒ (ğ‘”ğ‘¡ ğ‘, ğ‘”<ğ‘¡ ) is the probability distribution for predicting ğ‘”ğ‘¡ . To learn representation for embedding, we employ widely used contrastive loss that encourages the anchor embedding to be close to the positive document embedding and distant from the hard negative document embedding [3]. The objective function for representation with standard contrastive loss is as follows [9]: Lrep = 1 ğ‘ ğ‘ ğ‘–=1 log (cid:205)ğ‘ ğ‘—=1 (ğœ™ (â„ğ‘– ğ‘,ğ‘”, â„ ğ‘— ğ‘,ğ‘”, â„ğ‘– ğœ™ (â„ğ‘– ğ‘ ) ğ‘ ) + (cid:205)ğ‘š ğ‘˜= ğœ™ (â„ğ‘– ğ‘,ğ‘”, â„ ğ‘—,ğ‘˜ ğ‘› )) (2) where â„ğ‘,ğ‘”, â„ğ‘ and â„ğ‘› each denote embeddings of the augmented query, the positive document, and the hard negative document. M-Solomon obtain the embeddings from the last hidden states of the eos tokens at the final position. ğ‘ means the batch size and the function ğœ™ () indicates exp (cos()/ğœ), where cos() and ğœ each indicate cosine similarity and temperature hyper-parameter [2]. The overall objective function is linear combination of contrastive learning Lrep and autoregressive learning Lgen objectives: = ğ›¼repLrep + ğ›¼genLgen (3) where, ğ›¼rep and ğ›¼gen are scaling hyper-parameters [27]. We built the overall loss (Equation 3) based on [25], however, we did not augment every query. Moreover, contrastive loss on the original queries in addition to the augmented ones was applied in [25], which we found ineffective in our experiments and therefore omitted. As described in the bottom part of Figure 2, during inference, M-Solomon generates /augment and augmentations if it determines that queries demand augmentation. Otherwise, it simply generates /embed. The generated augmentations are appended to the queries. These augmented queries are then encoded for creating embeddings. Generating and encoding processes are conducted by one-time forward pass. Eventually, the adaptive query augmentation allows M-Solomon to represent the embeddings more informatively and efficiently."
        },
        {
            "title": "4 Experiments\n4.1 Experimental Setup\nDatasets and Evaluation Metric. We employed the training\ndataset collection of MMEB provided by [2], which consists of\n20 datasets. We used 2.5K queries from each dataset for synthesis\nprocess and training, resulting in total 50K training samples. For\nevaluation, we employed MMEB benchmark that contains 20 in-\ndistribution (IND) and 16 out-of-distribution (OOD) datasets across\nfour task categories - Classification, VQA, Retrieval, and Grounding\n[9]. We regarded Precision@1 (P@1) as the primary metric, which\nis a commonly used metric in MMEB and assigns a score of 1 if\nthe top-ranked retrieved document is relevant, and 0 otherwise.\nFurthermore, we utilized Latency, # of ğ‘‡ ğ‘ , and /embed%. Latency\nmeasures the average time (ms/query) taken for generation. # of ğ‘‡ğ‘ \nindicates the average number of generated tokens and ğ‘‡ğ‘  denotes\nâ€˜Tokensâ€™ [29]. /embed% measures the rate for selecting /embed. Also,",
            "content": "CIKM 25, November 1004, 2025, Seoul, Republic of Korea. Kim et al. Figure 2: The process of query augmentation synthesis and the training and inference procedure of M-Solomon. we used ğ¶ğ¹ , which denotes Confidence and is calculated by selecting the higher probability between the generations of /augment and /embed to measure how confidently these tokens are generated. Baselines. Since our methodology can be easily integrated into various approaches, we primarily compared the models before and after applying adaptive query augmentation to demonstrate its effectiveness. Therefore, as baselines, we used NoAug which does not use augmentation and AlwaysAug which always uses augmentation2. NoAug was trained by using only contrastive loss, while we trained AlwaysAug by setting all training samples to include augmentations and employing Equation 3. Lastly, we reported VLM2Vec [9], which was based on Qwen2-VL-7B-Instruct [21] and also trained with contrastive loss. However, unlike our approach, VLM2Vec did not use hard negative documents and utilized 50K queries from each dataset. VLM2Vec provided the reference performance on MMEB benchmark, establishing lower bound for performance. For ablation study, we adopt M-Solomon-Half, which was trained by randomly selecting and augmenting half of the queries in each training dataset without dividing datasets in Section 3.1. Moreover, we presented M-Solomon-/embed and M-Solomon-/augment, which were obligated to append /embed to the query to prevent augmentation and /augment to the query to enforce augmentation during inference, respectively. Implementation Details. M-Solomon was based on Qwen2VL-7B-Instruct [21] and was trained and evaluated on single node 2NoAug and AlwaysAug are identical to the model without augmentation and the model that always uses augmentation in Figure 1, respectively. with 8A100 80GB GPUs. We used LoRA with rank of 16 [17]. We set ğ‘š, ğ‘ , ğœ, ğ›¼rep, and ğ›¼gen as 1, 128, 0.02, 1.0, and 0.1, respectively. Following [9], we applied GradCache [5]. The image resolution was fixed at 512512, and the maximum token length was set to 1800. M-Solomon was trained for 1 epoch with learning rate of 2e-5, linear scheduler, and warmup steps of 0. The baselines were also trained in the same conditions."
        },
        {
            "title": "4.2 Main Results\nThe performance results of the models are presented in Table 2. First,\nNoAug, AlwaysAug, and M-Solomon all achieved higher overall\nperformance than VLM2Vec, which used the 662K samples without\nhard negative documents. This showed that effective embedders\ncould be developed with a smaller number of samples and high-\nlighted the importance of using hard negative documents in embed-\nding tasks. Both AlwaysAug and M-Solomon surpassed NoAug on\noverall performance by a large margin, indicating that answer-style\naugmentation provided useful information. However, AlwaysAug\nexhibited lower performance than M-Solomon. Even Latency and\n# of ğ‘‡ ğ‘  were nearly twice as high, showing that embedding la-\ntency of AlwaysAug was significantly slower. This was because\nM-Solomon adaptively generated augmentations when it deter-\nmined that queries required them. Moreover, since /embed% of\nM-Solomon was close to 50%, we could observe that /augment and\n/embed were selected evenly, indicating that M-Solomon performed\nadaptive query augmentation appropriately. This adaptive and bal-\nanced selection between /augment and /embed was not random",
            "content": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation CIKM 25, November 1004, 2025, Seoul, Republic of Korea. Models # of Datasets VLM2Vec [9] NoAug AlwaysAug M-Solomon (Ours) M-Solomon-Half M-Solomon-/embed M-Solomon-/augment Classification VQA Retrieval Grounding 62.6 61.9 64.4 64.4 62.5 63.7 64.5 10 57.8 59.6 62.4 61.9 62.0 57.2 62.0 12 69.9 68.1 67.1 68.8 68.1 68.6 67.6 81.7 83.9 85.5 83.6 84.8 83.7 83.7 IND OOD Overall 20 16 36 Latency 36 # of ğ‘‡ğ‘  (/embed%) 72.2 68.7 69.9 69.6 69.4 67.8 69.1 57.8 63.1 64.7 65.4 64.1 64.3 65.4 65.8 66.1 67.4 67.6 67.0 66.0 67.3 - - 1320 716 771 93 655 - - 45.8 (0%) 23.8 (55.1%) 25.9 (51.6%) 1.0 (100%) 20.9 (0%) ğ¶ğ¹ - - - 93.1 67.6 - - Table 2: Results on MMEB benchmark. The scores are averaged based on the conditions defined for each column. If the metric is not specified, the default is P@1. Models P@1 Latency # of ğ‘‡ğ‘  (/embed%) ğ¶ğ¹"
        },
        {
            "title": "4.4 Further Analysis of Adaptive Query",
            "content": "FashionIQ NoAug AlwaysAug M-Solomon GQA NoAug AlwaysAug M-Solomon ImageNet-R NoAug AlwaysAug M-Solomon 23.1 21.1 26.7 61.5 64.3 68.1 85.3 88.5 90.3 - 1496 333 - 497 - 1292 1266 - 51.6 (0%) 9.2 (91.0%) - 15.4 (0%) 21.3 (8.3%) - 43.8 (0%) 43.4 (1.0%) - - 80.6 - - 88. - - 97.0 Table 3: Results on the several datasets. The scores are averaged for each dataset. but deliberately made by M-Solomon because ğ¶ğ¹ of M-Solomon was substantially high. Lastly, M-Solomon notably outperformed NoAug and AlwaysAug on OOD result, which demonstrated the generalization effect of adaptive query augmentation."
        },
        {
            "title": "Augmentation",
            "content": "In this subsection, as presented in Table 3, we further analyzed how M-Solomon generated augmentations more adaptively than NoAug and AlwaysAug across FashionIQ, GQA, and ImageNet-R OOD datasets in MMEB benchmark. While M-Solomon consistently demonstrated strong performance on other datasets, we presented the results on three representative datasets. To begin with, on all three datasets, M-Solomon significantly outperformed NoAug. However, AlwaysAug performed worse than NoAug on FashionIQ, and on other datasets its improvement over NoAug was smaller compared to M-Solomon. This demonstrated the effectiveness of adaptive query augmentation. In FashionIQ dataset, M-Solomon significantly outperformed AlwaysAug on P@1 score, achieving approximately five times lower Latency and # of ğ‘‡ ğ‘ . As /embed% was 91.0%, M-Solomon determined that most queries of FashionIQ dataset did not require augmentation, which contributed to superior results and embedding latency of M-Solomon. In GQA dataset, M-Solomon achieved higher performance than AlwaysAug on P@1. However, despite /embed% of 8.3% that could reduce Latency, M-Solomon showed slightly higher Latency and # of ğ‘‡ ğ‘ . This was because M-Solomon generated longer and more informative augmentations that led to stronger performance. As shown in the first GQA example of Table 4, while AlwaysAug merely generated No for the given query, which caused AlwaysAug to find irrelevant document, M-Solomon produced longer and helpful augmentation, specifically highlighting that the boat is on the left in the picture, which enabled M-Solomon to retrieve correct document. It was expected that this capability arose from learning to appropriately generate augmentations only when they were required by queries, rather than generating them for all queries. In ImageNet-R dataset, AlwaysAug and M-Solomon revealed similar scores on Latency and # of ğ‘‡ ğ‘ , while M-Solomon exhibited notably higher P@1 score. This further demonstrated that M-Solomon performed more meaningful query augmentation. In the second ImageNet-R example of Table 4, while AlwaysAug performed augmentation under the misconception that the given query image depicted hedgehog or spiny animal, M-Solomon generated augmentation by recognizing the image as fluffy, dog-like animal, which helped to retrieve relevant document. CIKM 25, November 1004, 2025, Seoul, Republic of Korea. Kim et al. GQA Query Is the boat on the right of the picture? No aug of AlwaysAug No, there are no boats. Top 1 Document No, the boat is on the left of the picture. aug of M-Solomon Top 1 Document No, the boat is on the left of the image. Relevant Document No, the boat is on the left of the image. ImageNet-R Query Represent the given image for classification aug of AlwaysAug Top 1 Document aug of M-Solomon Top 1 Document Relevant Document The image depicts cartoonish anthropomorphic creature resembling hedgehog or small animal with spines-like features, standing upright on its hind legs and holding its front paws up as if it is dancing or performing trick. The creature has fluffy appearance and is drawn in simplistic sketch-like style. porcupine The image depicts stylized, cartoon-like drawing of fluffy animal, likely dog, standing on its hind legs with its front paws raised. The drawing is monochromatic, using shades of gray, and appears to be sketch or preliminary drawing. pomeranian pomeranian Table 4: Augmentation and retrieval examples of AlwaysAug and M-Solomon. aug means augmentation. In the examples, M-Solomon generated higher-quality augmentations, leading to more accurate retrieval results."
        },
        {
            "title": "6.2 Usage in Writing Stage\nDuring writing, we occasionally used Generative AI like ChatGPT4\nfor basic and straightforward tasks such as translation, finding syn-\nonyms, refining grammar, checking spell, and correcting awkward\nor incorrect expressions. Even though we obtained outputs from\nGenerative AI for such purposes, we carefully checked and revised\nthem before using their use. Moreover, all the content of this paper\nwas initially written and created on our own without Generative AI.\nWe believe that the use of Generative AI to this extent is acceptable\nand can strongly support active research and paper writing in a\npositive way.",
            "content": "Acknowledgments This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (RS-2024-00398115, Research on the reliability and coherence of outcomes produced by Generative AI). References [1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025). [2] Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. 2025. mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data. arXiv preprint arXiv:2502.08468 (2025). Across all datasets, M-Solomon consistently achieved robust ğ¶ğ¹ scores, which showed that it conducted adaptive query augmentation confidently and accurately. 3https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct 4https://chatgpt.com/ Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation CIKM 25, November 1004, 2025, Seoul, Republic of Korea. Representation and Generation with Contrastive-Autoregressive Finetuning. arXiv preprint arXiv:2503.19900 (2025). [28] Zhenrui Yue, Bowen Jin, Huimin Zeng, Honglei Zhuang, Zhen Qin, Jinsung Yoon, Lanyu Shang, Jiawei Han, and Dong Wang. 2025. Hybrid Latent Reasoning via Reinforcement Learning. arXiv preprint arXiv:2505.18454 (2025). [29] Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. 2025. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417 (2025). [30] Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. 2024. MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval. arXiv preprint arXiv:2412.14475 (2024). [3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. simple framework for contrastive learning of visual representations. In International conference on machine learning. PmLR, 15971607. [4] Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025. Thinkless: LLM Learns When to Think. arXiv preprint arXiv:2505.13379 (2025). [5] Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. 2021. Scaling deep contrastive learning batch size under memory limited setup. arXiv preprint arXiv:2101.06983 (2021). [6] Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. 2025. Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs. arXiv preprint arXiv:2504.17432 (2025). [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [8] Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, and Furu Wei. 2025. Think Only When You Need with Large Hybrid-Reasoning Models. arXiv preprint arXiv:2505.14631 (2025). [9] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160 (2024). [10] Fanheng Kong, Jingyuan Zhang, Yahui Liu, Hongzhi Zhang, Shi Feng, Xiaocui Yang, Daling Wang, Yu Tian, Qi Wang, Fuzheng Zhang, et al. 2025. Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval. arXiv preprint arXiv:2505.19650 (2025). [11] Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. 2025. LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning. arXiv preprint arXiv:2503.04812 (2025). [12] Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, and Chaochao Lu. 2025. Idmr: Towards instance-driven precise visual correspondence in multimodal retrieval. arXiv preprint arXiv:2504.00954 (2025). [13] Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu. 2025. AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning. arXiv preprint arXiv:2505.11896 (2025). [14] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025. Reasoning Models Can Be Effective Without Thinking. arXiv preprint arXiv:2504.09858 (2025). [15] Jiajun Qin, Yuan Pu, Zhuolun He, Seunggeun Kim, David Pan, and Bei Yu. 2025. UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings. arXiv preprint arXiv:2505.11815 (2025). [16] Benjamin Schneider, Florian Kerschbaum, and Wenhu Chen. 2025. ABC: Achieving Better Control of Multimodal Embeddings using VLMs. arXiv preprint arXiv:2503.00329 (2025). [17] Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, et al. [n. d.]. Lora: Low-rank adaptation of large language models. ([n. d.]). [18] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, et al. 2024. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883 (2024). [19] Jiakai Tang, Sunhao Dai, Teng Shi, Jun Xu, Xu Chen, Wen Chen, Wu Jian, and Yuning Jiang. 2025. Think before recommend: Unleashing the latent reasoning power for sequential recommendation. arXiv preprint arXiv:2503.22675 (2025). [20] Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra, et al. 2025. Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining. arXiv preprint arXiv:2505.11293 (2025). [21] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024). [22] Yi Wang, Junxiao Liu, Shimao Zhang, Jiajun Chen, and Shujian Huang. 2025. arXiv preprint PATS: Process-Level Adaptive Thinking Mode Switching. arXiv:2505.19250 (2025). [23] Chenghao Xiao, Thomas Hudson, and Noura Al Moubayed. 2024. Rar-b: Reasoning as retrieval benchmark. arXiv preprint arXiv:2404.06347 (2024). [24] Youze Xue, Dian Li, and Gang Liu. 2025. Learning via Explicit Hard Negative Gradient Amplifying. arXiv:2506.02020 (2025). Improve Multi-Modal Embedding arXiv preprint [25] Ruiran Yan, Zheng Liu, and Defu Lian. 2025. O1 embedder: Let retrievers think before action. arXiv preprint arXiv:2502.07555 (2025). [26] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [27] Hao Yu, Zhuokai Zhao, Shen Yan, Lukasz Korycki, Jianyu Wang, Baosheng He, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, and Hanchao Yu. 2025. CAFe: Unifying"
        }
    ],
    "affiliations": [
        "NC AI"
    ]
}