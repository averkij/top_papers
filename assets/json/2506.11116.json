{
    "paper_title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models",
    "authors": [
        "Jijie Li",
        "Li Du",
        "Hanyu Zhao",
        "Bo-wen Zhang",
        "Liangdong Wang",
        "Boyan Gao",
        "Guang Liu",
        "Yonghua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset\\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and codes\\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 1 1 1 1 . 6 0 5 2 : r Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gaoξ, Guang Liu, Yonghua Lin Beijing Academy of Artificial Intelligence, ξUniversity of Oxford"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset3 and codes4 have been publicly released."
        },
        {
            "title": "Introduction",
            "content": "The advent of Large Language Models (LLMs) marks significant milestone in artificial intelligence, with instruction fine-tuning emerging as crucial technique to unlock their full potential in natural language processing by enhancing their ability to follow complex prompts [25, 17, 24]. As these models become increasingly central to real-world applications, the quality and diversity of their training data, especially instruction data, have become key determinants of their performance [19, 30, 22, 11]. While fine-tuning LLMs on instruction data improves task-specific performance, naively adapting pretrained models to downstream instructions without sufficient regularization can lead to catastrophic forgetting of fundamental linguistic and reasoning capabilities [7]. This presents delicate trade-off between enhancing task alignment and preserving core generalization abilities. Constructing comprehensive, high-quality instruction datasets from scratch for fine-tuning, especially at the scale required by modern LLMs, is prohibitively expensive in terms of both human labor and computational resources. tempting alternative is to combine existing open-source datasets. HowCore contributors with equal contributions. Project Lead, the corresponding author, contact liuguang@baai.ac.cn 3https://huggingface.co/datasets/BAAI/Infinity-Instruct 4https://gitee.com/li-touch/infinity-instruct Preprint. Under review. ever, this approach is fraught with challenges: the quality of these datasets is not guaranteed, random combinations may degrade performance, and effective mixing often requires expert knowledge and time-consuming manual tuning of dataset composition ratios. To address these issues, we propose Infinity-Instruct, principled and scalable pipeline for systematically constructing instruction datasets by solving key challenges in instruction selection, labeling, and synthesis. Rather than relying on heuristic combinations, we introduce labeling-driven strategy to select and synthesize data from large instruction pool, yielding datasets that are both diverse and high-quality. We release two curated datasets: InfInstruct-F-7.4M, foundational instruction dataset consisting of 7.4M instructions selected using blend of filtering, clustering, and coverage-based strategies. InfInstruct-G-1.5M, general-purpose conversational instruction dataset built on two-layer instruction labeling system. We first cluster and label instructions using an open-source model, select 1.2M high-quality seeds, and then expand them via iterative synthesis and diagnosis to create richer set of 1.5M conversational instructions. These datasets undergo rigorous de-duplication and contamination filtering to ensure clean generalization. To evaluate the effectiveness of Infinity-Instruct, we fine-tune several popular open-source LLMs and produce the InfInstruct model family, including InfInstruct-Mistral-7B, InfInstruct-Llama3.18B/70B, InfInstruct-Qwen-2-7B, and InfInstruct-Yi-1.5-9B. These models consistently outperform their official instruction-tuned counterparts on both foundational and conversational benchmarks. Notably, InfInstruct-Llama3.1-70B surpasses GPT-4-0314 by 8.6% in conversational ability and matches its performance in foundational tasks, while InfInstruct-Llama3.1-8B achieves gains of 4.4% and 7.4% on foundational and conversational benchmarks, respectively. Our findings also reveal positive correlation between foundational and conversational capabilities, emphasizing the importance of balanced dataset design. Infinity-Instruct offers scalable solution to instruction dataset construction, bridging the gap between open-source and proprietary LLM performance. Our main contributions are summarized as follows: Introduction of unified dataset construction pipeline: We design novel pipeline that systematically curates datasets and synthesizes data to address both foundational and conversational tasks, ensuring high quality and diversity. Development of high-quality dataset, Infinity Instruct: InfInstruct-F-7.4M: foundational dataset of 7.4M instructions selected from over 100M samples using robust data selection strategies. InfInstruct-G-1.5M: conversational dataset synthesized through two-layer labeling system and iterative refinement, ensuring diversity and quality in conversational scenarios. Ensuring data quality and generalization: We apply de-duplication and contamination detection techniques to guarantee robustness and applicability across domains such as math, code, and knowledge Q&A. Demonstration of superior model performance: Fine-tuning multiple open-source models (e.g., Mistral, Llama, Qwen, and Yi) on Infinity-Instruct achieves state-of-the-art results on foundational and conversational benchmarks. Notably, InfInstruct-Llama3.1-70B surpasses GPT-4-0314 by 8.6% in conversational ability and achieves near parity in foundational tasks."
        },
        {
            "title": "2 Methodology",
            "content": "The Infinity-Instruct pipeline, as depicted in Figure 1, we first analyze the shortcomings of the open source instruction dataset based on fine-tuning experiments. Then we over 100 million open-source instructions from diverse domains such as math, code, and knowledge Q&A. The pipeline then follows two-stage process. First, rigorous data selection module is applied to filter and curate high-quality foundational instructions, resulting in the foundational dataset (InfInstruct-F-7.4M). Second, subset of seed instructions is identified and subjected to data synthesis stage. This stage employs iterative instruction labeling, evolution, and diagnostic processes to generate the 2 Figure 1: The overall structure for building the Infinity-Instruct dataset. conversational dataset (InfInstruct-G-1.5M), ensuring robustness and diversity in conversational capabilities. This structured approach ensures that both foundational and conversational datasets are optimized for enhancing LLM performance comprehensively. 2.1 Open instruction dataset analysis There are consistent differences between the performance of open-source and closed-source models in real scenarios. We believe that an important gap stems from the instruction dataset. To confirm the inadequacy of the open-source instruction dataset, we selected Mistral-7B to be fine-tuned on several recent open-source datasets. We used the overall capability of GPT-3.5 as comparison to find the overall capability shortfall of the fine-tuned Mistral-7B. We first collected collection of popular language modeling benchmarks. The results, summarized in Table 2, reveal that OpenHermes consistently delivered the best performance among open-source datasets. However, significant gap remains when compared to closed-source models such as GPT-3.5 and GPT-4, particularly in areas such as data diversity, code comprehension, knowledge-based quizzes, dialog generation, and other advanced capabilities. 2.2 Instruction Collection Figure 2: Overall Pipeline of Data Selection Pipeline Based on the open instruction dataset analysis, we first collected some of the most current and up-to-date instruction datasets in the fields of math, code, commonsense question-answering, and conversational to construct the data pool. This data pool contains more than 100 million instructions. The statistics of the data pool are shown in Table 1. We selected some recent datasets from the data pool and applied Mistral-7B-v0.1 to fine-tune these datasets. The result is shown in Table 2. We found that OpenHermers can achieve the overall best results. However, there is still gap compared to GPT-3.5 or GPT-4, which motivates us to further improve the quality and diversity of the existing instruction datasets through data selection and data synthesis. 3 Table 1: Source data and used data statistics of Infinity-Instruct Domain Code Math Knowledge Instruction Follow Total Total Collection (M) Used (M) 7.1 11.8 88.5 9.0 116.4 1.5 1.4 3.3 2.8 9. 2.3 Data Selection In this chapter, we will introduce the data selection module used to build the foundational dataset of Infinity-Instruct. The pipeline of this module is demonstrated in Figure 2. Selection strategy. We introduce three selection strategies such as source filtering, rule-based filtering, and DSIR [21]. From task perspective, we choose the appropriate filtering rule according to the features of the tasks such as code and math. The details of the selection strategy used for each task are described as follows: Knowledge. We found that the collected general knowledge datasets were of varying quality due to different data cleaning rules. To enhance the models capacity for commonsense understanding, we introduce the Flan 2022 dataset [15], which consolidates all publicly available academic datasets for instruction tuning at the time. This dataset integrates hundreds of high-quality templates, diverse formatting patterns, and extensive data augmentation, including mix of zero-shot, few-shot, and chain-of-thought prompt formats. We specifically curated the dataset by filtering out samples from sources with relatively low knowledge content, such as sentiment classification datasets (e.g., SST-2, IMDb movie reviews). Additionally, for synthetic datasets and augmented data samples (e.g., question-answering and question-generation samples generated from the same seed data), we implemented deduplication strategy to reduce the proportion of augmented samples. Math. We refer to the DSIR [21], which aims at selecting subset of large raw unlabeled dataset to match desired target distribution given unlabeled target samples. To improve the models mathematical abilities, we will use the prompts from GSM8K and MATH training samples as target distributions to guide subset selection from the pool of mathematical datasets. In addition to selecting from existing datasets, to enhance the models sensitivity to numerical variations, we synthesize data based on current datasets. This involves generating corresponding chain-of-thought (CoT) and program-of-thought (PoT) reasoning processes for mathematical problems, along with using data augmentation strategies to expand the dataset. Details can be found in [26]. In the end, we constructed about 1.4M math instruction datasets. Code. In line with our methodology for enhancing mathematical understanding, we apply the DSIR technique to curate data from an extensive collection of open-source instruction datasets in the coding domain. To ensure that selected data aligns closely with the target task characteristics, we base the importance resampling process on prompt distributions obtained from HumanEval samples. This approach allows us to prioritize samples that better represent the types of coding challenges and reasoning processes seen in HumanEval, ultimately refining the models ability to generalize and perform across diverse coding tasks. Evaluation and Weak-domain Instruction Supplement. To optimize data utilization, we adopt gradual approach by incrementally increasing the data volume for each task. As described in section 2.1, we evaluate the saturation level of the current dataset for each task by fine-tuning experiments in Mistral-7B. When performance gap is observed between the fine-tuned Mistral-7B on the current dataset version and baseline model (e.g., GPT-3.5) for specific task, we will relax the data selection criteria to incorporate additional weak-domain data, thereby enhancing the models performance in underrepresented areas. Finally, by merging the foundational instructions of each task, we obtained 6.2M generalized foundational instruction datasets. Besides, to ensure smooth switching between foundation training and instruction-following training, similar to the replay strategy [27], we added 1.2M seed instructions (as mentioned in 2.4) to make up the final version foundational instructions dataset, InfInstruct-F-7.4M. 4 Table 2: Downstream task performance. We illustrate the performance of the task specific models developed from Mistral-7B by fine-tuning on open-source candidate datasets, including GSM-8K, MATH, HumanEval, BMPP, MMLU and C-EVAL. Math Code GSM-8K MATH HumanEval MBPP MMLU C-EVAL Knowledge GPT-3.5 Mistral-7B w/ OpenHermers w/ Mammoth-v2 w/ Bagel 57.1 48.1 73.0 50.0 58.9 28.0 11.8 18.2 17.6 18.3 48.1 14.0 41.5 27.4 35.4 68.2 38.0 41.8 33.0 39. 70.0 56.5 61.7 57.9 54.9 54.4 34.6 43.0 40.4 37.5 We find that this strategy can guide the model to achieve the optimal foundational ability and the instruction-follow performance. Figure 3: Overall Pipeline of Data Synthesis Pipeline 2.4 Data Synthesis The existent finding diverse and difficult dialog instructions helps to improve the models ability to converse in real-life scenarios. In this chapter, we design an iterative instruction evolution pipeline. As shown in Figure 3, this pipeline contains four steps of instruction labeling system construction, high-quality seed instruction construction, instruction evolution, and model weakness detection. We started with 9M open source instructions and eventually synthesized dataset of about 1.5M evolutionary instructions, namely InfInstruct-G-1.5M. Instruction Labeling System. high-quality labeling system facilitates the synthesis of diverse data and improves the generalization of the model across different conversation scenarios. So we use an open-source language model (i.e., Qwen1.5-72B) to label each instruction at the second level, and then regularize the second-level labels through embedding clustering and manual tuning. Eventually we will further generalize the first level labels using the generalization ability of the language model. The final labeling system we constructed contains 26 first-level labels and more than fifteen thousand second-level labels. Details about the labeling system can be found in [28]. High-quality Seed Instruction Selection. We apply difficulty and diversity as criteria and select 1.2M high quality dialog instructions from 9M dialog instructions. We uniformly used Qwen 1.5-7B for the evaluation of instruction difficulty. Specifically, the filtering guidelines are as follows: (1) Long-tail data (Diversity). Based on the labeling system, we retained all the instructions in the ability labels with frequency of occurrence between 20 and 200, and extracted 1/3 of the instructions for each ability label with frequency between 200 and 500. These instructions will not enter the subsequent filtering process. (2) Multidimensional capabilities. (Diversity). As discussed in [28], 5 Table 3: Infinity Instruct datasets enhancement of open source model conversational capabilities. AlpacaEval 2.0 Arena-Hard MT-Bench 18.1 35. 29.4 50.0 Dataset Official closed-source models GPT-3.5-0301 GPT-4-0314 Official open-source models Mistral-7B-Instruct-v0.2 Qwen2-7B-Instruct Yi-1.5-9B-conversational Llama3.1-8B-Instruct Llama3.1-70B-Instruct Official open-source models finetuned with Infinity-Instruct InfInstruct-Mistral-7B InfInstruct-Qwen2-7B InfInstruct-Yi-1.5-9B InfInstruct-Llama3.1-8B InfInstruct-Llama3.1-70B 17.1 20.9 20.5 20.9 38.1 16.2 19.6 16.3 20.6 55.7 26.9 22.3 17.4 33.7 66.0 40.0 21.9 20.5 33.9 46. 7.9 9.0 7.6 8.4 8.0 8.1 8.6 8.1 8.3 8.0 8.2 8.9 Overall* 42.2 58.4 36.4 41.5 38.9 40.8 59. 49.3 (+12.9) 42.4 (+0.9) 39.3 (+0.4) 49.9 (+9.1) 67.0 (+7.1) we consider the instruction involved multiple capabilities models difficult task. Based on the labeling system, we prioritized the retention of instructions involving multiple capability labels. (3) High language modeling loss (Difficulty). We compute the loss for the answer portion of candidate instruction. higher loss reflects the models lack of familiarity with the capabilities involved in the instruction, as well as the more difficult it is to provide correct response to the instruction. (4) High convergence loss (Difficulty). As discussed in [11], learning on certain instructions may easily lead to overfitting, causing the model to develop harmful biases that affect the generalizability. To avoid this phenomenon, we removed samples that had been modeled with large difference in loss before and after fine-tuning. Instruction Evolution. We further expand the difficulty of the seed data by multi-step rewriting through the evolve-instruct instruction evolution algorithm proposed by [22]. For each seed instruction, we applied Wizards four evolutionary strategies to rewrite it, and ask the rewriting model to determine whether the rewritten instruction was semantically identical to the one before rewriting or whether it introduced harmful information. Diagnosis. Finally, we attempt to analysis the weak ability of open-source models based on the instruction labeling system and supplemented with synthesized instructions for theses ability types. Specifically, we extracted evolved instructions from each ability type and applied GPT-4 to evaluate the quality of responses from multiple open-source models (including Mistral-7B, Llama3-8B). Instructions that result in poor responses from either model are added to the next round of instruction evolution. This feedback-based strategy guarantees that our instruction dataset scaling more efficiently. 2.5 Deduplication and Decontamination To avoid any duplication, we decontaminate and de-duplicate the synthetic data. We use the BGE [20] model to vectorize the instructions and calculate the cosine similarity between the instructions in the Infinity-Instruct and the open-source benchmark. We manually confirmed threshold of 0.3 for filtering duplicates and contamination."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we empirically evaluate the effect of Infinity-Instruct with existing large-scale instruction datasets and verify the stability of Infinity-Instruct over different models. 3.1 Comparison with official aligned models In this section, we finetune multiple open-source pre-trained models on Infinity-Instruct to verify the generalizability of Infinity Instruct. As shown in Table 3, we can see that after fine-tuning, Qwen2, 6 Table 4: Infinity Instruct datasets enhancement of open source model foundational capabilities. Dataset Math Code MATH GSM-8K HumanEval MBPP MMLU C-EVAL Knowledge 48.1 67.0 28.0 45.8 57.1 92. Official closed-source models GPT-3.5-0301 GPT-4-0314 Official instruct models Mistral-7B-Instruct-v0.2 Qwen2-7B-Instruct Yi-1.5-9B-conversational Llama3.1-8B-Instruct Llama3.1-70B-Instruct Official foundation models finetuned with Infinity-Instruct InfInstruct-Mistral-7B InfInstruct-Qwen2-7B InfInstruct-Yi-1.5-9B InfInstruct-Llama3.1-8B InfInstruct-Llama3.1-70B 78.6 83.2 76.2 70.2 88.0 26.0 15.2 17.6 28.1 42.1 45.9 80.4 78.7 55.2 91.7 9.5 15.4 17.8 15.6 43.3 32.9 66.5 65.3 49.7 67. 59.8 66.5 70.2 53.7 72.0 60.2 61.8 3.8 51.0 46.5 47.2 63.2 52.0 54.4 48.8 50.4 70.3 70.0 86.4 59.6 71.1 62.1 66.1 82. 65.0 70.4 64.5 66.3 79.0 54.4 68.7 42.6 81.7 62.7 42.3 62.3 51.1 81.5 63.2 51.0 63.1 Overall 53.0 70. 32.4 61.0 55.5 46.0 68.4 55.4 (+23.0) 61.9 (+0.9) 56.8 (+1.3) 53.3 (+7.3) 69.1 (+0.7) Table 5: Infinity Instruct compares the effectiveness of open-source datasets in improving the conversational capabilities of pre-trained models. We tested the foundational and conversational abilities separately. The best scores are bolded. * To calculate the overall average score, we mapped MT-Bench scores to 0-100. Dataset AlpacaEval Arena-Hard MT-Bench Overall* Llama3.1-8B-instruct WildChat [29] Lmsys-conversational-1m [30] Evol-Instruct [22] UltraChat [7] MAGPIE-Pro [23] OpenHermes-2.5[19] Infinity-Instruct 20.9 11.4 4.2 2.0 7.6 28.0 13.1 34. 18.2 6.7 2.2 3.7 3.5 22.6 14.2 38.9 8.0 7.4 5.2 6.6 6.7 8.2 7.5 8.2 39.7 30.4 19.5 23.9 26.0 44.2 34.1 51.6 Yi-1.5, Llama3.1 and mistral-v0.1 have all surpassed the conversational performance of the official instruct models. In addition, InfInstruct-Llama3.1-70B (46.1) even exceeds GPT4-0314 (35.3), which truly realizes the GPT-4 level of conversational ability. It is important to note that most officially released models have already undergone alignment to human preferences, which may further enhance the models conversational capabilities. Besides, in Table 4, we can find Infinity-Instruct helps to reproduce the foundational ability performance of the official conversational model. The fine-tuned versions of Llama3.1-8B and Mistral-7B-v0.1 on Infinity-Instruct comprehensively outperform the official versions.InfInstruct-Llama3.1-70B even outperforms GPT4-0314 in terms of code ability evaluation. 3.2 Comparison with open source instruction datasets In this section, we compare Infinity-Instruct as well as popular open-source datasets to open-source models in terms of their conversational, foundational ability to improve. Conversational Ability Evaluation. As shown in Table 5, we can see on the conversational ability evaluation, MAGPIE-Pro is by far the most effective dataset for improving the models conversational competence, and the Llama 3.1-8B fine-tuned on top of it even outperforms the official version. However, Infinity-Instruct improves Llama3.1-8B more than MAGPIE-Pro on all three conversational ability review sets. Foundational Ability Evaluation. In terms of the foundational ability evaluation, as shown in Table 7, we can see that most of the datasets can improve the foundational capability of the base model. And fine-tuning on OpenHermes2.5, MAGPIE-Pro can further improve Llama3.1-8B to close to or even exceed the Llama3.1-8B-Instruct on some of the evaluation set. However, fine-tuning on InfinityInstruct allows the Llama3.1-8B to surpass the Llama3.1-8B-Instructs foundational capabilities across the board. It should thanks to the diversity and quality of the instructions of Infinity Instruct. 7 (a) Foundational Tasks (b) Conversational Tasks Figure 4: Scaling curves on foundational and conversational tasks. 3.3 Scaling curves on foundational and conversational dataset To further explore the correlation between the data quantity and model performance in foundational and conversational tasks, we control the full amount of data at different scales to fine-tune the Qwen2.5_1.5B and observe the corresponding change in the model performance on downstream tasks. Unbalanced sampling would cause large fluctuations in experimental results. Therefore, we labeled all instruction responses with reward scores using the reward model. With the instruction labeling system provided in Section 2.4, we sorted all instructions under each second-level labeling type based the labeled reward scores. During the sampling process, instructions with higher reward values under all second-level labels will be retained as much as possible. The effectiveness curves are shown in Figure 4, and we can observe continuous trend of increasing overall model performance with the number of instruction data in both foundational and conversational scenarios. Specifically, as the quantity of instruction data increases, there is significant growth rate in the models performance on reasoning tasks such as MATH and GSM-8K. This result illustrates that large-scale instruction datasets are necessary for building general-purpose language models. It also guides us in the direction of continuous optimization of the instruction dataset."
        },
        {
            "title": "4 Ablation Study",
            "content": "In the main experiments, we are using two-stage training approach to maximize the effect of the foundational dataset and the conversational dataset. In this section, we will analyze the effect of training the foundational dataset or the conversational dataset alone and the benefits of two-stage training. Considering the training costs, we choose an earlier version of the foundational instruction dataset, InfInstruct-F-3M, and an earlier version of the conversational instruction dataset, InfInstructG-300K. We utilize mistral-7B as the base model. We evaluate the performance of each ablation setting based on the conversational performance of the finetuned model. Single foundational dataset or conversational dataset. As shown in Table 8, we find that training on the dialog dataset alone yields 93.3% of the performance of full two-stage training. Although training on the foundational dataset alone does not result in the highest level of conversational ability, it can further enhance the conversational ability of the language model in the two-stage training settings. Optimal trade-off of foundational and conversational ability. Since the foundational dataset and the conversational dataset focus on the improvement of the foundational ability and the improvement of the conversational ability, respectively. We considered two ways to balance these two improvements. The first is that we break up and mix the foundational dataset and the conversational dataset. Then we perform one-stage training. The second way refers to the idea of curriculum learning [2], where we train on the foundational dataset first, and then on the conversational instruction dataset. The results are shown in Table 8, we can see that simply merging datasets and doing an end-to-end training does not lead to higher conversational ability or foundational ability. Instead, two-stage training leads to the highest conversational ability and higher foundational ability than one-stage training. This demonstrates that models with stronger foundational capabilities can also achieve stronger conversational capabilities. 8 Table 6: InfInstruct-F-7.4M and InfInstruct-F-1.5M Dialog Turns Statistics Turns F-7.4M G-1.5M"
        },
        {
            "title": "Samples\nPercentage\nSamples\nPercentage",
            "content": "1 6897934 0.926 1423191 0.977 (1, 5] 466354 0.063 28211 0.019 (5, 10] 73912 0.010 4647 0.003 (10, ) 10906 0.001 878 0.001 Table 7: Infinity Instruct compares the effectiveness of open-source datasets in improving the foundational capabilities of pre-trained models. The best scores are bolded. Math Code MATH GSM-8K HumanEval MBPP MMLU C-EVAL Knowledge Dataset Official-Version Llama3.1-8B Llama3.1-8B-Instruct SFT with Open-source Dataset w/ WildChat w/ Lmsys-conversational-1m w/ Evol-Instruct w/ UltraChat w/ MAGPIE-Pro w/ OpenHermes-2.5 w/ Infinity-Instruct 13.4 22.3 10.2 12.5 22.1 15.0 21.6 9.9 26.0 50.1 67.8 52.7 29.4 54.0 59.4 68.7 73.4 78.6 25.6 52. 49.2 43.1 53.1 54.8 58.2 56.2 59.8 45.2 50.5 48.2 43.7 43.1 44.3 50.3 49.1 52.0 49.3 53.2 55.8 50.2 65.0 65.4 65.1 64.7 66.2 32.3 47. 41.1 50.4 48.3 52.7 49.0 49.8 56.3 Overall 36.0 48.9 42.9 38.2 47.6 48.6 52.1 50.5 56.5 Table 8: Ablation study of two-stage training, the foundational dataset, and the conversational dataset. In the One-Stage setup, we break up and mix the InfInstruct-F-3M and the InfInstruct-G-300K. We tested the results of AlpacaEval 2.0 for conversational ability evaluation, tested all the benchmarks in Table 7, and calculated the mean values for foundational ability evaluation. Settings InfInstruct-F-3M InfInstruct-G-300K One-Stage Two-Stage Conversational Ability Foundational Ability 16.0 23.8 22.3 25.5 54.4 43.9 50.9 52."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced Infinity Instruct, comprehensive approach to advancing open-source instruction datasets and bridging the performance gap with proprietary models. By addressing the limitations of existing datasets, which often prioritize narrow task domains, our work emphasizes high-quality data curation and synthesis to support both foundational and conversational tasks. The pipeline of Infinity Instruct demonstrates the importance of robust data selection strategies, as evidenced by InfInstruct-F-7M, foundational dataset selected from over 100M samples, and InfInstruct-G-1.5M, conversational dataset designed with two-layer labeling system. Through rigorous quality assurance techniques such as de-duplication and contamination detection, we ensured the reliability of our datasets across diverse domains, including mathematics, coding, and knowledgebased Q&A. We fine-tune multiple open-source models on Infinity Instruct and achieve state-of-the-art results, showcasing significant improvements in both foundational and conversational benchmarks. Notably, InfInstruct-Llama3.1-70B achieved remarkable performance, surpassing GPT-4-0314 by 8.6% in chat ability while maintaining near parity in foundational tasks. These results underscore the transformative potential of open-source initiatives in fostering accessible and high-performing language models. We hope that Infinity Instruct serves as valuable resource and catalyst for further innovation within the broader research community."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 4148, 2009. [3] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [6] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. [7] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. [8] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [9] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [10] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems, 2023. [11] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with selfguided data selection for instruction tuning. arXiv preprint arXiv:2308.12032, 2023. [12] Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. arXiv preprint arXiv:2402.00530, 2024. [13] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, et al. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. URL https://lmsys.org/ blog/2024-04-19-arena-hard/. [14] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. [15] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. [16] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations, 2023. [17] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [18] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [19] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5. [20] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023. [21] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling, 2023. URL https://arxiv.org/abs/2302.03169. [22] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. [23] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. [24] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [25] Bo-Wen Zhang, Liangdong Wang, Jijie Li, Shuhao Gu, Xinya Wu, Zhengduo Zhang, Boyan Gao, Yulong Ao, and Guang Liu. Aquila2 technical report. arXiv preprint arXiv:2408.07410, 2024. [26] Bo-Wen Zhang, Yan Yan, Lin Li, and Guang Liu. Infinitymath: scalable instruction tuning dataset in programmatic mathematical reasoning. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pp. 54055409, 2024. [27] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024. [28] Hanyu Zhao, Li Du, Yiming Ju, Chengwei Wu, and Tengfei Pan. Beyond iid: Optimizing instruction learning from the perspective of instruction interaction and dependency. 2024. URL https://arxiv.org/abs/2409.07045. [29] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. [30] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2023. [31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023."
        },
        {
            "title": "A Evaluation Setup",
            "content": "Chat ability evaluation. We evaluate the instruction-following capabilities of fine-tuned models on three leading benchmarks: Mt-Bench [31]: set of multi-turn questions across 8 categories, including writing, role-play, extraction, reasoning, math, coding, knowledge (STEM), and knowledge (humanities/social science). It uses GPT-4 as referee and baseline, evaluating the models response win rate relative to GPT-4. AlpacaEval2.0 [14]: Comprising 805 prompts from the AlpacaFarm evaluation set, this benchmark uses GPT-4 as referee and baseline. It introduces length-controlled win rates to reduce GPT-4s length bias and aligns closely with the human-annotated Chatbot Arena. Arena-Hard [13]: An automatic evaluation tool with 500 challenging user queries, using GPT-4-Turbo as judge to compare models responses to GPT-4. Arena-Hard exhibits the highest correlation and separability with Chatbot Arena among popular open-ended LLM benchmarks. Foundational ability evaluation. We evaluated the fine-tuned model across several datasets using the OpenCompass framework [6]: MMLU [8]: large-scale benchmark covering diverse tasks (history, math, science) to assess reasoning and knowledge. C-EVAL [10]: Chinese language dataset for tasks like question answering, summarization, and translation. MATH [9]: dataset with high school-level math problems requiring multi-step reasoning. GSM8K [5]: dataset of elementary math problems to evaluate basic arithmetic and logical reasoning. HumanEval [4]: Python programming dataset for testing code generation models. MBPP [1]: dataset for assessing basic Python programming tasks, including task descriptions, code, and tests. These datasets comprehensively evaluate the models reasoning, mathematical, and coding abilities."
        },
        {
            "title": "B Related works",
            "content": "The advent of Large Pre-trained Language Models (LLMs) has marked significant shift in the field of Natural Language Processing (NLP). These models, such as GPT-3 and its successors, have demonstrated remarkable capabilities in understanding and generating human-like text across variety of tasks [3]. However, the performance of these models is heavily influenced by the quality and scale of the data they are fine-tuned on [18]. The importance of high-quality instruction datasets for fine-tuning LLMs has been underscored by several studies. For instance, the work on Superfiltering by [12] highlights the need for weak-tostrong data filtering to expedite instruction-tuning. Similarly, [22] introduced WizardLM, which focuses on empowering LLMs to follow complex instructions by enhancing their understanding through carefully curated dataset. In the open-source community, there have been efforts to compile datasets that could be used for fine-tuning purposes. Projects like OpenHermes-2.5 and CodeBagel have contributed to this space by providing datasets that encompass wide range of instructions [19]. However, the challenge lies in the curation of these datasets to ensure they are not only extensive but also maintain high standards of quality and relevance [16]. Infinity Instruct addresses this gap by introducing large-scale, high-quality instruction dataset. This project stands out due to its focus on both the foundational dataset, which contains millions of instructions selected from various open-source datasets, and the chat dataset, evolved from subset of high-quality seed data to improve real conversation scenarios. 12 Table 9: Training hyperparameters for all models in both phases. Model Epoch LR Context Length Warmup Steps Batch Size Llama-3.1-8B Llama-3.1-70B Mistral-7B-v0.1 Qwen-2-7B Yi-1.5-9B 3 3 3 3 5e-6 5e-6 5e-6 1e-5 1e-5 4096 4096 4096 4096 4096 40 40 40 40 40 528 528 528 528 528 Figure 5: T-SNE visualization and analysis of the first-level label type distribution of instructions in the InfInstruct-F-7.4M and InfInstruct-G-1.5M dataset. We sampled up to 2000 instructions per label type. Moreover, the projects approach to data evolution and model ability diagnosis represents novel contribution to the field. By systematically tagging instructions and using AI assistants for data validation and generation, \"Infinity Instruct\" ensures that the dataset is not only large but also diverse and informative, catering to the complex needs of LLMs [27]."
        },
        {
            "title": "C Training Details",
            "content": "The training process consists of two phases. We first apply the foundational dataset Infinity-Instruct7M to improve the foundational ability (e.g. math, code) of the pre-trained models. We further fine-tune the dialogue dataset to improve the models chat ability. We found that the training hyperparameters are more relevant to some attributes of the pre-trained model, such as the number of parameters, the end learning rate, etc. Thus, we apply the same training hyperparameters to both phases of training. The main training hyperparameters are shown in Table 9. To improve the training efficiency on large-scale instruction datasets, we apply FlagScale, distributed training framework. This framework compresses the number of training steps through the packing strategy. Meanwhile, it applies multiple model-parallel and data-parallel techniques to accelerate the training of tens of billions of models."
        },
        {
            "title": "D Data Analysis",
            "content": "We conducted statistical analysis of InfInstruct-F-7.4M and InfInstruct-G-1.5M, with results presented in Figure 5. To visualize the semantic distribution of instructions, we applied the t-SNE algorithm to both datasets. The resulting distribution graphs reveal that certain challenging and complex task types, such as Logic and Reasoning, exhibit more dispersed distribution. These tasks often involve broader range of knowledge points and demand intricate problem-solving capabilities. As discussed in Section 2.3, we posit that enhancing the diversity of instructions can significantly improve the models baseline performance on such complex tasks. To this end, InfInstruct-F-7.4M incorporates substantial number of instructions categorized under \"Information Processing\". Conversely, InfInstruct-G-1.5M emphasizes balanced allocation of instructions across 13 different task types, ensuring the model demonstrates robust instruction-following capabilities across variety of downstream applications. Additionally, we analyzed the dialogue turn statistics for InfInstruct-F-7.4M and InfInstruct-G-1.5M, summarized in Table 4. Single-turn dialogues accounted for 92.6% and 97.7% of the instructions in InfInstruct-F-7.4M and InfInstruct-G-1.5M, respectively. To enhance the models adaptability to multi-turn dialogue scenarios, we supplemented the datasets with approximately 7.4% and 2.3% multi-turn instructions, respectively. This strategic augmentation supports the development of more effective multi-turn dialogue capabilities in downstream tasks."
        },
        {
            "title": "E Limitation",
            "content": "Limitations. While Infinity-Instruct significantly improves the performance of open-source LLMs, several limitations remain. First, the labeling and selection process relies on existing open-source models, which may introduce bias or miss nuanced instruction types not well-represented in the original data pool. Second, although our pipeline is scalable, it still requires substantial computational resources for clustering, synthesis, and fine-tuning. Lastly, our evaluation focuses primarily on benchmark performance; real-world robustness, safety, and long-term retention of capabilities across domains warrant further investigation."
        },
        {
            "title": "F Broader Impacts",
            "content": "Infinity-Instruct lowers the barrier to building high-quality instruction-tuned models by providing an open, scalable framework for dataset construction. This helps democratize access to capable LLMs, especially for researchers and organizations with limited resources. However, as the pipeline relies on existing data and models, it may inherit biases or limitations from them. Careful dataset auditing and responsible model deployment are essential to ensure ethical and safe use."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "University of Oxford"
    ]
}