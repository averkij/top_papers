{
    "paper_title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation",
    "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at https://github.com/lucadellalib/focalcodec."
        },
        {
            "title": "Start",
            "content": "FOCALCODEC-STREAM: STREAMING LOW-BITRATE SPEECH CODING VIA CAUSAL DISTILLATION Luca Della Libera1,2, Cem Subakan3,1,2, Mirco Ravanelli1,2 1Concordia University, 2Mila-Quebec AI Institute, 3Universite Laval 5 2 0 2 9 1 ] . [ 1 5 9 1 6 1 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Neural audio codecs are fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, hybrid codec based on focal modulation that compresses speech into single binary codebook at 0.55 - 0.80 kbps with theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at https://github.com/lucadellalib/focalcodec. Index Terms Speech coding, discrete tokens, streamability 1. INTRODUCTION Audio coding is fundamental signal processing technique that compresses speech into compact discrete representations at low bitrates and reconstructs them with minimal perceptual loss. Originally developed for efficient transmission to reduce network traffic [1, 2], neural audio codecs (NACs) have advanced rapidly in recent years and have become central to modern speech generative models [3]. This shift is largely driven by the success of large language models [47], which established autoregressive sequence modeling as the dominant paradigm in text and are now being extended to other modalities, including audio. In this context, NACs provide discrete audio tokens enabling applications such as text-conditioned audio generation [811] and speech language models (SLMs) [1215]. To support these tasks, codec tokens should preserve both acoustic and semantic information while maintaining high reconstruction quality. Additionally, bitrate and sequence length become especially critical: unlike transmission, where even codecs at 12 kbps already outperform traditional codecs [2], much lower bitrates are required for training effective SLMs [14]. Recent work has sought to address these challenges. Hybrid codecs enrich acoustic tokens with semantic content [14, 16], some employ supervised fine-tuning [17], others reduce frame rate [14,17, 18], and growing trend compresses all the information into single codebook [1923]. remaining challenge, however, is streamability: most codecs are designed for offline processing and depend on long future context windows, making them unsuitable for real-time tasks such as speech assistants, interactive dialogue, and low-latency generation. While some codecs [2,14,17,21,2426] already support streaming, they typically compromise on other aspects, such as requiring high bitrates [2,24,25], relying on multiple codebooks [2,14, 17,2426], neglecting semantic information [2,21,24,25], or providing weak representations for downstream tasks [2,24,25]. This highlights the need for low-bitrate, single-codebook codecs that unify semantic and acoustic representations, maintain high reconstruction quality, and support streamability. In our prior work [22], we introduced FocalCodec, an efficient single-codebook codec based on WavLM [27], focal modulation [28, 29], and binary spherical quantization [30], which satisfies all requirements except streamability. In this paper, we extend FocalCodec to the streaming setting. In particular, our contributions are as follows: We introduce FocalCodec-Stream, novel hybrid codec that compresses speech into single binary codebook at low bitrates (0.55 - 0.80 kbps) while supporting streaming inference with theoretical latency of 80 ms. To achieve this, we propose multi-stage causal distillation strategy to adapt WavLM for streaming, combined with targeted architectural modifications. In particular, we design lightweight refiner module that mitigates quality degradation under latency constraints. We evaluate FocalCodec-Stream extensively across reconstruction and downstream tasks, and validate our design through ablation studies. 2. FOCALCODEC-STREAM 2.1. Codec architecture Our streaming codec builds upon the original FocalCodec [22] architecture, with several modifications to enable low-latency streaming inference (see Figure 1). We target latency of 80 ms, which has been shown sufficient for building highly responsive SLMs [14]. The encoder, consisting of the first six layers of WavLM-large, is made streamable by replacing standard convolutions with causal convolutions and full-context gated relative attention with sliding window gated relative chunked attention, with chunks of 80 ms (4 feature frames). Compared to strictly causal attention, the sliding window approach discards old context to ensure constant memory usage and enable infinite streaming, while chunking introduces lookahead of up to 3 frames, improving performance within the latency budget. Unlike pooling-based frame reduction, this design preserves WavLM compatibility without requiring full retraining from scratch. The compressor and decompressor, built on focal modulation [28, 29], combine depth-wise convolutions, position-wise feedforward layers, nonlinearities, and global pooling. For streaming, we replace standard convolutions with causal convolutions and substitute global pooling with large-kernel causal convolution. This acts Fig. 1. FocalCodec-Stream architecture. The encoder extracts features containing both acoustic and semantic information. These features are then mapped to low-dimensional space by the compressor, binary quantized, and projected back by the decompressor. The decoder resynthesizes the waveform from these features. All these modules are causal, while non-causal teacher is used for distillation to align intermediate causal features with their non-causal counterparts. as learnable sliding window moving average. The binary spherical quantizer [30] operates independently on each latent and is naturally streamable. The Vocos [31] decoder is made streamable by replacing the ConvNeXt blocks with causal convolutions and substituting the inverse STFT with linear projection and flattening, following [32]: each hidden state is projected to entries, where is the upsampling factor, and then flattened to reconstruct the waveform. Note that, except for the chunked attention, most modules are strictly causal, incurring just 20 ms latency instead of the overall 80 ms latency budget. To address this, we introduce lightweight refiner module after the decompressor. Implemented as residual chunk-wise feedforward layer, it leverages the available latency to better align with WavLM features while remaining efficient. Let RN where is the sequence length and the hidden size. We first reshape it into chunks xc RN/CCD of size C. For each chunk vector xc: ˆxc = xc + Wout GELU(Winxc + bin) + bout, (1) where Win, Wout RCDCD and bin, bout RCD. Finally, we unflatten ˆxc back to the original shape ˆx RN D. Empirically, we find the refiner useful for improving perceptual quality at the target latency, while having minimal impact on inference speed. 2.2. Causal distillation The main challenge in developing streaming version of FocalCodec lies in making the full-context WavLM encoder streamable. Prior work has explored ad-hoc self-supervised approaches [33], and more recently proposed methods for adapting off-the-shelf selfsupervised models [34] and discrete units [35]. However, these efforts have been largely limited to speech recognition and focus primarily on semantic information. Their impact on speech resynthesis quality remains unclear, as does the ability to preserve strong representations for downstream tasks. Notably, [36] shows that the effective receptive field of self-supervised speech models is relatively small in deeper layers, suggesting that these layers can be adapted more readily for streaming, whereas earlier layers depend on much broader temporal context. To overcome these challenges, we propose multi-stage distillation framework, where the original full-context WavLM encoder serves as the teacher: Stage-1. We first assess the contribution of different architectural modules in the original FocalCodec@50 model under streaming constraints, where only the encoder is converted to streaming while all downstream components remain unchanged. This analysis shows that the most critical components are (1) the learned positional embedding and (2) the full-context gated relative attention, whereas the convolutional feature extractor plays comparatively minor role. In particular, the positional embedding relies on convolution with receptive field of 2.56 (128 feature frames), which far exceeds the 80 ms latency budget and would in practice impose an initial delay of seconds before playback can begin. To address this, we causally distill the positional embedding by encouraging its causal variant to approximate the full-context one, minimizing an L2 loss on training data. This procedure is lightweight, requiring only single convolutional layer with 8.4M parameters (less than 10% of the encoder size). Stage-2. After distilling the positional embedding, we proceed to distill the attention and convolutional feature extractor. These components are converted to causal form, then distilled from the full-context teacher. To guide the process, we minimize an L2 loss between the causal and teacher embeddings at each of the six layers, applied pairwise. Since the final layers are most important, we weight their losses more heavily using reversed linear schedule: 1.0 for layer 6, 0.9 for layer 5, 0.8 for layer 4 etc. This gives the earlier layers more freedom to adapt while preserving the representational strength of the deeper layers. Stage-3. Once the WavLM encoder has been distilled, we train the causal compressorquantizerdecompressor system on top of it, following the same recipe as [22]. We use an L2 loss between the causally distilled and decompressed representations. In parallel, we train the causal decoder using the original full-context WavLM representations. Stage-4. distribution shift remains between the decoder trained on full-context features and the decompressor trained to reconstruct causally distilled features. To close this gap, we introduce the learnable refiner module described earlier. During this stage, we fine-tune the encoder, compressor, quantizer, decompressor, and refiner jointly, while applying an L2 loss between the teacher output and the refiner output. This provides the system with additional capacity to adapt, allowing the encoder to generate features that are not exact copies of the teachers but are more suitable for reconstruction by the decompressor. Although this step is optional, since the codec is already functional after the previous stage, we find it considerably improves reconstruction quality. 3. EXPERIMENTAL SETUP Table 1. Codecs considered in our experiments. We follow the hyperparameter settings of [22], with modifications to compensate for the expected performance degradation introduced by causality. Specifically, we increase the capacity of the compressor and decompressor by setting the hidden size of each of the three focal downscaling/upscaling layers to 1024, and by doubling both the focal window and focal factor to 14 and 4, respectively. In addition, we replace layer normalization with the lightweight DyT nonlinearity [37]. For all sliding-window operations, the past context is set to 10.24 (512 feature frames). For the decoder, we increase the hidden size to 1024 and the feed-forward dimension to 2048. Importantly, we train with an upsampling factor of 480 instead of 320, enabling the model to reconstruct audio at 24 kHz rather than 16 kHz. This provides the decoder with super-resolution capabilities, which we find beneficial for enhancing audio quality. We focus on the 50 Hz version and train three variants with codebook sizes of 2048, 4096, and 65,536. To accommodate the increasing model capacity, we progressively scale the training data across the distillation stages. Stage-1 uses LibriTTS [38] (585 hours), resampled to 16 kHz. Stage-2 and stage-3 use Libri-Light-medium [39] (5k hours), trained on fixedlength chunks of 15 s. Stage-4 scales to the full Libri-Light corpus (60k hours) while increasing the chunk size to 30 s. For the decoder, we follow the original training recipe and use only the clean-100 split of LibriTTS at its original 24 kHz sampling rate, with chunk size set to 3 s. All models are trained on NVIDIA A100 (80 GB) GPUs with learning rate that decreases when validation loss fails to improve for few epochs, and training is stopped once no improvement is observed for several consecutive epochs. In some runs, we found it useful to reset the learning rate once the validation loss appeared to have converged, which yielded few additional improvements. Implementation details and hyperparameters will be provided in the accompanying code repository. 4. RESULTS We compare our models against recent streaming low-bitrate codecs with publicly available checkpoints, excluding works such as [17] and [21], which do not provide streaming checkpoints. Since our focus is on the low-bitrate regime, whenever multiple quantizer configurations are available, we set them to achieve bitrate close to the range of 0.55 - 0.80 kbps, ensuring fair comparison. In particular, we include EnCodec [2], AudioDec [24], and HILCodec [25], which are inherently acoustic codecs, as well as Mimi [14] and PAST [26], which are hybrid approaches based on distillation and supervised fine-tuning, respectively. We also include the original non-streaming FocalCodec@50 [22] for comparison. The configurations and details of each model are summarized in Table 1. 4.1. Speech resynthesis and voice conversion First, we evaluate FocalCodec-Stream on speech resynthesis (SR), considering both English and multilingual speech. We follow the same experimental protocol as in [22]. For English speech, we use the LibriSpeech [40] test-clean, while for multilingual speech, we use subset of MLS [41]. We report UTMOS [42] for naturalness, Whisper-small [43] dWER for intelligibility, WavLMbased embedding similarity for speaker fidelity, code usage and normalized entropy for codebook efficiency, and real-time factor (RTF) for inference speed, measured on 1/8 NVIDIA H100 (80 GB) multi-instance GPU partition. Table 2 shows the results. On Codec EnCodec AudioDec HILCodec Mimi5 Mimi6 PAST FocalCodec-S@50-2k FocalCodec-S@50-4k FocalCodec-S@50-65k FocalCodec@50 Bitrate (kbps) Sample Rate (kHz) Token Rate (Hz) Codebooks Latency (ms) Params (M) 1.50 1.60 1.50 0.69 0.83 1.00 0.55 0.60 0. 0.65 24 24 24 24 24 16 16 / 24 16 / 24 16 / 24 16 75.0 80.0 75.0 12.5 12.5 50.0 50.0 50.0 50. 50.0 2 1024 2 1024 2 1024 5 2048 6 2048 2 1024 1 2048 1 4096 1 65536 1 8192 13 13 13 80 80 20 80 80 15 8 11 82 82 126 249 249 249 142 LibriSpeech, FocalCodec-Stream achieves UTMOS close to fullcontext FocalCodec and outperforms all baselines in intelligibility, with FocalCodec-S@50-65k reaching the best dWER and highest speaker similarity. Code usage is 100% with high entropy, indicating efficient utilization of the quantizer. On MLS, performance drops across all models due to multilingual variability, but FocalCodecStream variants still outperform acoustic codecs and remain comIn particular, FocalCodec-S@50-65k shows petitive with Mimi6. the best trade-off between intelligibility and speaker fidelity. By contrast, PAST, while achieving good dWER on LibriSpeech, performs poorly in this setting because it was fine-tuned in supervised fashion on English data only and thus fails to generalize to multilingual audio. This highlights key limitation of supervised adaptation approaches, which are prone to overfitting to domain-specific data. We also perform one-shot voice conversion (VC) experiments to assess the ability of FocalCodec-Stream to disentangle content from speaker information. Following the protocol of [22], we use dataset of parallel utterances derived from VCTK [44]. FocalCodecStream consistently achieves substantially better naturalness and intelligibility than prior streaming codecs. Most notably, FocalCodecS@50-65k attains both the highest UTMOS and speaker similarity, while keeping dWER significantly lower than acoustic codecs. Compared to Mimi6, which achieves competitive similarity, FocalCodecS maintains much lower dWER. Conversely, although PAST reports slightly better dWER, it performs substantially worse in terms of speaker similarity. This highlights that FocalCodec-S is the only streaming codec to simultaneously achieve high intelligibility and strong speaker fidelity in this setting, both of which are equally critical for successful voice conversion. 4.2. Downstream tasks To assess the quality of the learned discrete representations, we evaluate downstream models on discriminative and generative tasks. For discriminative tasks, we consider automatic speech recognition (ASR), speaker identification (SI), speech emotion recognition (SER), keyword spotting (KS), and intent classification (IC). For generative tasks, we focus on speech enhancement (SE) and speech separation (SS). We leave autoregressive tasks such as text-to-speech and speech language modeling for future work. These tasks are aligned with [45], which provides standardized evaluation suite for audio discrete representations. As in [22], we adopt shallow LSTM-based probes for discriminative tasks and Conformer-based non-autoregressive models for SE and SS. Specifically, we use LibriSpeech-460 for ASR and SI, IEMOCAP [46] for SER, Speech Commands [47] for KS, SLURP [48] for IC, VoiceBank [49] for SE, and Libri2Mix-100 [50] for SS. Unlike [22], where token embeddings were initialized from the codec embedding layer, we instead use the reconstructed features taken after the quantization bottleneck Table 2. Speech resynthesis and voice conversion. Best, second-best and best non-streaming results are highlighted. Codec Reference EnCodec AudioDec HILCodec Mimi5 Mimi6 PAST FocalCodec-S@50-2k FocalCodec-S@50-4k FocalCodec-S@50-65k FocalCodec@ Bitrate (kbps) UTMOS dWER Sim Code Usage Norm. Entropy RTF UTMOS dWER Sim Code Usage Norm. Entropy RTF UTMOS dWER Sim SR English SR Multilingual VC 1.50 1.60 1.50 0.69 0.83 1.00 0.55 0.60 0.80 0.65 4.09 1.58 1.48 2.86 3.29 3.44 2. 3.88 3.87 3.85 4.05 0.00 8.08 11.61 6.65 5.73 4.77 4.04 4.63 4.39 3.68 2.18 100.0 93.4 93.8 91.9 92.1 99.0 95.4 95.6 96.0 96.2 96.6 56.7 83. 96.1 96.3 97.0 100.0 100.0 100.0 97.4 100.0 82.1 70.0 95.6 91.8 92.0 90.7 99.4 99.1 98. 98.9 91 145 41 157 154 59 106 106 106 123 2.84 1.33 1.29 1.81 2.08 2.19 1.44 2.68 2.68 2. 2.96 0.00 29.60 40.95 25.32 30.96 26.12 49.35 24.64 23.69 19.88 100.0 93.4 95.5 87.5 92.3 99.1 97.8 95.9 96.7 96.5 97.4 57.0 80.8 97.5 97.6 98.1 100.0 100.0 99. 12.57 98.3 100.0 79.2 68.2 94.8 89.0 89.2 87.5 98.8 98.9 98.3 98. 113 195 41 219 216 63 108 108 107 116 4.09 1.24 1.26 1.40 2.40 2.62 1.42 2.72 2.72 3.10 3. 0.00 86.52 68.45 58.36 110.00 110.00 18.28 25.08 24.39 22.71 21.27 100.0 72.2 68.2 76.8 89.7 91.3 68.5 92.1 91.5 92.5 92. Table 3. Discriminative and generative downstream tasks. Best, second-best and best non-streaming results are highlighted. Codec Reference EnCodec AudioDec HILCodec Mimi5 Mimi6 PAST FocalCodec-S@50-2k FocalCodec-S@50-4k FocalCodec-S@50-65k FocalCodec@50 Bitrate (kbps) ASR SI SER KS WER ER ER ER 1.50 1.60 1.50 0.69 0.83 1.00 0.55 0.60 0.80 0. 28.55 29.21 29.89 22.56 22.56 10.74 16.87 17.21 17.02 15.33 3.25 1.69 1.98 3.92 3.13 3.43 2.66 2.70 2.18 0. 41.94 45.85 51.61 37.10 35.71 36.41 37.10 33.18 34.56 34.79 96.16 25.30 15.17 5.99 5.81 6.41 5.50 5.85 5.63 4. IC ER 49.79 46.77 53.69 35.03 35.74 31.66 30.93 29.74 29.49 24.66 SE SS DNSMOS dWER Sim DNSMOS dWER Sim 3.56 3.13 2.96 3.32 3.18 3.14 3.15 3.54 3.54 3.56 3.52 0.00 37.31 61.11 41.33 52.11 55.99 18.19 21.24 20.21 19. 12.35 100.0 85.6 84.3 90.2 86.3 86.7 77.9 85.2 88.7 87.7 90.4 3.77 3.11 2.97 3.35 3.37 3.32 3.15 3.68 3.69 3. 3.71 0.00 77.61 88.59 78.43 87.63 86.49 85.61 83.41 80.21 75.43 72.61 100.0 87.4 84.0 86.9 88.8 88.9 80.3 89.8 90.0 90. 89.5 and before the decoder. We find these features consistently improve performance across most tasks and codecs. We report error rates for discriminative tasks, and standard speech quality metrics for generative tasks, using DNSMOS [51] instead of UTMOS (which is better suited for noisy data). For further details, we refer to [22]. As shown in Table 3, across discriminative tasks FocalCodecStream consistently ranks among the top-performing codecs despite operating at lower bitrates. For ASR, all variants achieve WER of roughly 17%, substantially outperforming all codecs except PAST, which benefits from having been specifically fine-tuned on this taskdataset combination during training. On SI, the 65k variant obtains the best error rate, outperforming Mimi and PAST, and falling only slightly behind AudioDec and HILCodec, confirming the intuition that preserving speaker information is easier at larger codebook sizes. For SER, performance is strong, with the 4k variant achieving the best results and even marginally surpassing the nonstreaming model. On KS and IC, FocalCodec-Stream models outperform all prior baselines. Finally, we note that in the KS task EnCodec performs particularly poorly, likely due to overfitting to commands present in the training set but absent from the test set. For generative tasks, the benefits of the streaming distillation framework are even more evident. On SE, all three FocalCodecStream variants achieve high DNSMOS and competitive speaker similarity. Importantly, their dWER values surpass those of all other baselines except PAST, which is slightly ahead, but conversely shows much lower DNSMOS and speaker similarity. similar trend is observed for SS, where the 65k variant delivers strong performance across all metrics, substantially outperforming all baselines and rivaling the quality of the non-streaming model. Finally, it is worth noting that performance gap with the fullcontext FocalCodec@50 baseline remains, especially at lower bitrates, which is expected given the stricter constraints of real-time streaming. Nevertheless, the results demonstrate the effectiveness of our codec, which achieves favorable balance among reconstrucTable 4. Ablation studies for FocalCodec-S@50-4k."
        },
        {
            "title": "Configuration",
            "content": "UTMOS dWER Sim Proposed w/o refiner w/o stage-4 3.87 3.84 3.78 4.39 4.65 5. 96.3 96.1 95.8 tion quality, downstream task performance, latency, and efficiency. In particular, the 2k variant is of special interest, as its compact codebook makes it well-suited for autoregressive modeling. 4.3. Ablation studies We conduct ablations to assess the impact of the refiner and stage-4 fine-tuning on resynthesis quality using LibriSpeech test-clean. As shown in Table 4, the full model achieves the best scores across UTMOS, dWER, and similarity. Removing the refiner consistently degrades performance, reflecting its role in mitigating the distribution shift between causal and full-context features. Omitting the final fine-tuning stage has an even stronger effect, with higher dWER and reduced UTMOS and similarity. These results emphasize the importance of the progressive multi-stage framework for preserving quality under low-latency constraints. 5. CONCLUSION We introduced streamable extension of FocalCodec that combines causal single-codebook design with multi-stage distillation strategy. The proposed codec operates at bitrates as low as 0.55 - 0.80 kbps with an 80 ms theoretical latency, achieving strong reconstruction quality while preserving both semantic and acoustic information, and consistently outperforms popular streamable baselines at comparable bitrates. Future work will focus on scaling to larger datasets, exploring autoregressive modeling, and further optimizing the system for resource-constrained settings. 6. REFERENCES [1] N. Zeghidour, A. Luebs, A. Omran, et al., SoundStream: An end-toend neural audio codec, IEEE/ACM TASLP, pp. 495507, 2021. [2] A. Defossez, J. Copet, G. Synnaeve, et al., High fidelity neural audio compression, TMLR, 2023. [26] N. Har-Tuv, O. Tal, and Y. Adi, PAST: Phonetic-acoustic speech tokenizer, in Interspeech, 2025, pp. 35093513. [27] S. Chen, C. Wang, Z. Chen, et al., WavLM: Large-scale selfsupervised pre-training for full stack speech processing, IEEE JSTSP, pp. 15051518, 2022. [28] J. Yang, C. Li, X. Dai, et al., Focal modulation networks, in NeurIPS, [3] P. Mousavi, G. Maimon, A. Moumen, et al., Discrete audio tokens: 2022. More than survey!, TMLR, 2025. [4] OpenAI, J. Achiam, S. Adler, et al., GPT-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [5] A. Chowdhery, S. Narang, J. Devlin, et al., PaLM: scaling language modeling with pathways, JMLR, vol. 24, 2024. [6] A. Q. Jiang, A. Sablayrolles, A. Roux, et al., Mixtral of experts, arXiv preprint arXiv:2401.04088, 2024. [7] A. Grattafiori, A. Dubey, A. Jauhri, et al., The Llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [8] Z. Borsos, R. Marinier, D. Vincent, et al., AudioLM: language modeling approach to audio generation, IEEE/ACM TASLP, vol. 31, pp. 25232533, 2023. [29] L. Della Libera, C. Subakan, and M. Ravanelli, Focal modulation networks for interpretable sound classification, in ICASSPW, 2024, pp. 853857. [30] Y. Zhao, Y. Xiong, and P. Krahenbuhl, Image and video tokenization with binary spherical quantization, in ICLR, 2025. [31] H. Siuzdak, Vocos: Closing the gap between time-domain and fourierbased neural vocoders for high-quality audio synthesis, in ICLR, 2024. [32] T. Okamoto, H. Yamashita, Y. Ohtani, et al., WaveNeXt: ConvNeXtbased fast neural vocoder without ISTFT layer, in ASRU, 2023, pp. 18. [33] C.-C. Chiu, J. Qin, Y. Zhang, et al., Self-supervised learning with random-projection quantizer for speech recognition, in ICML, 2022, vol. 162, pp. 39153924. [9] J. Copet, F. Kreuk, I. Gat, et al., Simple and controllable music gen- [34] B. Fu, K. Fan, M. Liao, et al., wav2vec-S: Adapting pre-trained speech eration, in NeurIPS, 2023, vol. 36, pp. 4770447720. models for streaming, in ACL, 2024, pp. 1146511480. [10] F. Kreuk, G. Synnaeve, A. Polyak, et al., AudioGen: Textually guided audio generation, in ICLR, 2023. [11] C. Wang, S. Chen, Y. Wu, et al., els are zero-shot arXiv:2301.02111, 2023. text to speech synthesizers, Neural codec language modarXiv preprint [12] D. Zhang, S. Li, X. Zhang, et al., SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities, in EMNLP, 2023, pp. 1575715773. [13] M. Hassid, T. Remez, T. A. Nguyen, et al., Textually pretrained speech language models, in ICLR, 2023. [14] A. Defossez, L. Mazare, M. Orsini, et al., Moshi: speech-text foundation model for real-time dialogue, arXiv preprint arXiv:2410.00037, 2024. [15] T. A. Nguyen, B. Muller, B. Yu, et al., SpiRit-LM: Interleaved spoken and written language model, arXiv preprint arXiv:2402.05755, 2024. [16] X. Zhang, D. Zhang, S. Li, et al., SpeechTokenizer: Unified speech tokenizer for speech large language models, in ICLR, 2024. [17] J. D. Parker, A. Smirnov, J. Pons, et al., Scaling transformers for lowbitrate high-quality speech coding, in ICLR, 2025. [18] E. Casanova, P. Neekhara, R. Langman, et al., NanoCodec: Towards high-quality ultra fast speech LLM inference, in Interspeech, 2025, pp. 50285032. [19] S. Ji, Z. Jiang, W. Wang, et al., WavTokenizer: An efficient acoustic discrete codec tokenizer for audio language modeling, in ICLR, 2025. [20] D. Xin, X. Tan, S. Takamichi, et al., BigCodec: Pushing the limits of low-bitrate neural speech codec, arXiv preprint arXiv:2409.05377, 2024. [21] H. Wu, N. Kanda, S. Emre Eskimez, et al., TS3-Codec: Transformerbased simple streaming single codec, in Interspeech, 2025, pp. 604 608. [22] L. Della Libera, F. Paissan, C. Subakan, et al., FocalCodec: Lowbitrate speech coding via focal modulation networks, arXiv preprint arXiv:2502.04465, 2025. [23] Z. Ye, X. Zhu, C.-M. Chan, et al., inference-time compute for Llama-based speech synthesis, preprint arXiv:2502.04128, 2025. Llasa: Scaling train-time and arXiv [24] Y.-C. Wu, I. D. Gebru, D. Markovic, et al., Audiodec: An open-source streaming high-fidelity neural audio codec, in ICASSP, 2023, pp. 15. [25] S. Ahn, B. J. Woo, M. H. Han, et al., HILCodec: High-fidelity and lightweight neural audio codec, IEEE JSTSP, vol. 18, pp. 15171530, 2024. [35] K. Choi, M. Someki, E. Strubell, et al., On-device Streaming Discrete Speech Units, in Interspeech, 2025, pp. 44234427. [36] Y. Meng, S. Goldwater, and H. Tang, Effective Context in Neural Speech Models, in Interspeech, 2025, pp. 246250. [37] J. Zhu, X. Chen, K. He, et al., Transformers without normalization, in CVPR, 2025, pp. 1490114911. [38] H. Zen, V. Dang, R. Clark, et al., LibriTTS: corpus derived from LibriSpeech for text-to-speech, in Interspeech, 2019. [39] J. Kahn, M. Riviere, W. Zheng, et al., Libri-Light: benchmark for ASR with limited or no supervision, in ICASSP, 2020, pp. 76697673. [40] V. Panayotov, G. Chen, D. Povey, et al., LibriSpeech: An ASR corpus based on public domain audio books, in ICASSP, 2015, pp. 5206 5210. [41] V. Pratap, Q. Xu, A. Sriram, et al., MLS: large-scale multilingual dataset for speech research, in Interspeech, 2020, pp. 27572761. [42] T. Saeki, D. Xin, W. Nakata, et al., UTMOS: UTokyo-SaruLab system for VoiceMOS challenge 2022, in Interspeech, 2022, pp. 45214525. [43] A. Radford, J. W. Kim, T. Xu, et al., Robust speech recognition via large-scale weak supervision, in ICML, 2023, vol. 202, pp. 28492 28518. [44] J. Yamagishi, C. Veaux, and K. MacDonald, CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit, University of Edinburgh, CSTR, vol. 6, pp. 15, 2017. [45] P. Mousavi, L. Della Libera, J. Duret, et al., DASB - discrete audio and speech benchmark, arXiv preprint arXiv:2406.14294, 2024. [46] C. Busso, M. Bulut, C.-C. Lee, et al., IEMOCAP: Interactive emotional dyadic motion capture database, LREC, vol. 42, no. 4, pp. 335 359, 2008. [47] P. Warden, Speech Commands: dataset for limited-vocabulary speech recognition, arXiv preprint arXiv:1804.03209, 2018. [48] E. Bastianelli, A. Vanzo, P. Swietojanski, et al., SLURP: spoken language understanding resource package, in EMNLP, 2020, pp. 7252 7262. [49] C. Valentini-Botinhao, X. Wang, S. Takaki, et al., Investigating RNNbased speech enhancement methods for noise-robust text-to-speech, in SSW, 2016, pp. 146152. [50] J. Cosentino, M. Pariente, S. Cornell, et al., LibriMix: An opensource dataset for generalizable speech separation, arXiv preprint arXiv:2005.11262, 2020. [51] C. K. Reddy, V. Gopal, and R. Cutler, DNSMOS P.835: nonintrusive perceptual objective speech quality metric to evaluate noise suppressors, in ICASSP, 2022."
        }
    ],
    "affiliations": [
        "Concordia University",
        "Mila-Quebec AI Institute",
        "Universite Laval"
    ]
}