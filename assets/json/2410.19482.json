{
    "paper_title": "Measuring memorization through probabilistic discoverable extraction",
    "authors": [
        "Jamie Hayes",
        "Marika Swanberg",
        "Harsh Chaudhari",
        "Itay Yona",
        "Ilia Shumailov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are susceptible to memorizing training data, raising concerns due to the potential extraction of sensitive information. Current methods to measure memorization rates of LLMs, primarily discoverable extraction (Carlini et al., 2022), rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization. This paper introduces a probabilistic relaxation of discoverable extraction that quantifies the probability of extracting a target sequence within a set of generated samples, considering various sampling schemes and multiple attempts. This approach addresses the limitations of reporting memorization rates through discoverable extraction by accounting for the probabilistic nature of LLMs and user interaction patterns. Our experiments demonstrate that this probabilistic measure can reveal cases of higher memorization rates compared to rates found through discoverable extraction. We further investigate the impact of different sampling schemes on extractability, providing a more comprehensive and realistic assessment of LLM memorization and its associated risks. Our contributions include a new probabilistic memorization definition, empirical evidence of its effectiveness, and a thorough evaluation across different models, sizes, sampling schemes, and training data repetitions."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 2 8 4 9 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Measuring memorization through probabilistic\ndiscoverable extraction",
            "content": "Jamie Hayes1, Marika Swanberg2, Harsh Chaudhari1, Itay Yona1 and Ilia Shumailov1 1Google DeepMind, 2Boston University Large language models (LLMs) are susceptible to memorizing training data, raising concerns due to the potential extraction of sensitive information. Current methods to measure memorization rates of LLMs, primarily discoverable extraction (Carlini et al., 2022), rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization. This paper introduces probabilistic relaxation of discoverable extraction that quantifies the probability of extracting target sequence within set of generated samples, considering various sampling schemes and multiple attempts. This approach addresses the limitations of reporting memorization rates through discoverable extraction by accounting for the probabilistic nature of LLMs and user interaction patterns. Our experiments demonstrate that this probabilistic measure can reveal cases of higher memorization rates compared to rates found through discoverable extraction. We further investigate the impact of different sampling schemes on extractability, providing more comprehensive and realistic assessment of LLM memorization and its associated risks. Our contributions include new probabilistic memorization definition, empirical evidence of its effectiveness, and thorough evaluation across different models, sizes, sampling schemes, and training data repetitions. 1. Introduction Memorization of training data, while potentially beneficial for retaining factual information, presents significant challenges in large language models (LLMs) (Biderman et al., 2024; Bordt et al., 2024; Carlini et al., 2022; Duan et al., 2024a,b; Huang et al., 2024; More et al., 2024; Shi et al., 2023; Smith et al., 2023; Staab et al., 2023; Tang et al., 2023; Zanella-BÃ©guelin et al., 2020; Zhang et al., 2024) 1. The undesirable consequences of memorized data can inadvertently expose sensitive information contained within the training set. This issue has garnered significant attention, leading to the nowcommon practice of quantifying and reporting training data memorization rates within technical reports introducing new foundation large language models (LLMs) (Anil et al., 2023; Chowdhery et al., 2023; Kudugunta et al., 2024; Reid et al., 2024; Team et al., 2024). One way to measure memorization is to quantify how easily potential attacker could extract training data by querying the model. This often-used measure of memorization called discoverable extraction (Carlini et al., 2022; Kassem et al., 2024) essentially states that training example is extractable (or memorized) if when split into prefix and suffix, the model generates sequence matching the suffix when given the prefix as input. Discoverable extraction is often used as an (approximate) upper-bound to an adversary that has no prior knowledge of the example to be extracted (Nasr et al., 2023). Discoverable extraction has become popular way of measuring memorization rates of LLMs (Anil 1This paper covers very restricted definition of memorization: whether generative model can be induced to generate near-facsimiles of some training examples when prompted with appropriate instructions. Models do not contain bit-wise or code-wise copies of their training data. Rather, if model can be induced to generate very close copies of certain training examples by supplying appropriate instructions to guide the models statistical generation processes then that model is said to have memorized those examples. This is an area of active ongoing research. Corresponding author(s): jamhay@google.com 2024 Google DeepMind. All rights reserved Measuring memorization through probabilistic discoverable extraction et al., 2023; Chowdhery et al., 2023; Kudugunta et al., 2024; Reid et al., 2024; Team et al., 2024), in no small part due to its simplicity; one needs only to check if models completion matches an expected target string. While discoverable extraction is cheap to compute, this has its own drawbacks; by only generating single sequence and checking for match with the target, it may miss cases where match could have been found if more than one sequence was generated. These nuances of what is and is not memorized are too coarsely treated by working definition like discoverable extraction, and leads us to investigate the following question: Question 1. How can we better measure the extractability of sensitive sequences from large language models? Quantifying memorization and the associated risks is subtle and context-specific problem that single measurement likely cannot capture in isolation. Prior work has attempted to introduce more complex definitions of memorization that aim to get to the heart of what it means for model to memorize training example, but they are often too costly to be practically leveraged. These methods have varying levels of computational cost to empirically estimate memorization in the context of LLMs, but are all more expensive than discoverable extraction (see Section 4). Large language models are probabilistic machines, the output of the model is probability distribution over tokens that make up models vocabulary. The sequence that is generated is entirely dependent on the choice of sampling algorithm that defines how token is selected from this distribution. There are many different sampling algorithms that one can choose from (see Basu et al. (2020); Boulanger-Lewandowski et al. (2013); Fan et al. (2018); Graves (2012); Holtzman et al. (2019); Vijayakumar et al. (2016)). Prior works have focused on greedy sampling, which iteratively selects the next token with the largest probability conditioned on the previous tokens. Although greedy sampling selects the most likely next token it may not select the overall most likely sequence; yet, the difference between discoverable extraction rates under greedy and other sampling schemes like beam search was found to be marginal (Carlini et al., 2022). However, users of production large language models are often free to decide which sampling scheme to use. These observations lead to our second research question: Question 2. How does the user-chosen sampling scheme affect extractability rates? Carlini et al. (2022) argue that focusing on sampling schemes that have higher degrees of associated randomness compared to greedy sampling, are antithetical to maximizing discoverable extraction, as sampling schemes that encourage diversity in sequence generation will by definition have higher variation in the sequences that can be sampled. However, higher sequence diversity may be advantageous for extraction given that users may query the model multiple times. Extracting the secret even once out of multiple queries could be highly problematic as the adversary (say hacker checking credit card numbers) may have external means of verifying which one is correct. It is reasonable to try and quantify the number of sequences that need to be generated before target example becomes extractable, as this better aligns with how users could interact with model. In particular, because production language models are deployed with non-greedy based sampling strategies that, by default, encourage diversity in sampled sequences. This idea is not new one. Carlini et al. (2019) motivate their measure of canary memorization using rank perplexity by considering an adversary who sequentially guesses the potential canaries in order from lowest to highest perplexity. The rank of the true canary measures how many guesses such an adversary would need to make before guessing correctly. While the secret sharer attack only guesses single canary, it is natural to consider how extraction rates scale with multiple guesses. We find that even when we measure the extraction probability after multiple guesses, the extraction rates Measuring memorization through probabilistic discoverable extraction on training data remain significantly higher than extraction rates on test data (see Section 5.2). This observation allows us to reason about the absolute risk of extraction as well as the relative risk. In this work, we introduce probabilistic relaxation of discoverable extraction that resolves the discussed points of tension. This new definition quantifies the number of attempts ğ‘› an adversary would need to make to extract target with certain probability ğ‘ under given sampling scheme. This provides more nuanced quantification of memorization, which alleviates the aforementioned drawbacks of discoverable extraction without incurring any additional computational cost compared to discoverable extraction. Our contributions We propose simple probabilistic definition of extraction called (ğ‘›, ğ‘)-discoverable extraction that captures the risk of extracting target after sampling ğ‘› times from an arbitrary sampling scheme. We thoroughly benchmark (ğ‘›, ğ‘)-discoverable extraction rates for different sampling schemes, settings of ğ‘› and ğ‘, model sizes, and number of target data repetitions, and we make number of remarkable empirical findings that demonstrate the utility of our definition: Greedy extraction underestimates training data memorization rates compared to (ğ‘›, ğ‘)-discoverable extraction even for modest values of ğ‘› and ğ‘ (Section 5.1). Moreover, the discrepancy between the two rates increases for larger models and more repetitions of the target data. (Section 5.3) At every setting of the definition parameters we tried, extraction rates of training data far exceeded baseline extraction rates on test data (Section 5.2). We show that (ğ‘›, ğ‘)-discoverable extraction provides better comparison of memorization rates across models trained on the same data compared with greedy extraction (Section 5.1). Along the way, we provide extensive discussion how our definition relates to other definitions of memorization (Section 3 and Section 4). 2. Preliminaries Here, we introduce the terminology and definitions of discoverable extraction that we use and refer to throughout this work. Notation. For sequence of tokens ğ‘§ = (ğ‘§1, . . . , ğ‘§ğ‘˜) and indices 1 ğ‘– ğ‘— ğ‘˜, we use ğ‘§ğ‘–: ğ‘— to denote tokens ğ‘§ğ‘–, . . . , ğ‘§ ğ‘—. Let ğ‘“ğœƒ : ğ‘˜ (V) be parameterized model which takes sequence of ğ‘˜ tokens from vocabulary and outputs probability distribution (V) over V. Let ğ‘”ğœ™ : (V) be sampling scheme parameterized by scheme specific hyperparameters ğœ™, that takes as arguments probability distribution over and selects token from the vocabulary V. Finally, for some initial sequence ğ‘§, let (ğ‘”ğœ™ ğ‘“ğœƒ)ğ‘˜ (ğ‘§) denote the process of repeatedly generating distribution over the token vocabulary, sampling from it, and adding this token to the sequence ğ‘˜ times, starting from the initial sequence ğ‘§. Definition 2.1 (Discoverable extraction adopted from Carlini et al. (2022); Kassem et al. (2024)). Given training example ğ‘§ that is split into prefix ğ‘§1:ğ‘˜ and suffix ğ‘§ğ‘˜+1:2ğ‘˜, ğ‘§ is discoverably extractable if (ğ‘”ğœ™ ğ‘“ğœƒ)ğ‘˜ (ğ‘§1:ğ‘˜) = ğ‘§1:2ğ‘˜. In other words, we take the beginning of training example, and check if the sequence generated under the composition of model ğ‘“ğœƒ and sampling scheme ğ‘”ğœ™ matches the end of the example. Although there is nothing in this definition that fixes the ğ‘”ğœ™, Definition 2.1 is almost always instantiated with greedy sampling. 3 Measuring memorization through probabilistic discoverable extraction Sampling schemes. We discuss popular choices for the underlying sampling scheme, ğ‘”ğœ™. Sampling with temperature ğ‘‡. Given sequence of ğ‘¡ 1 tokens ğ‘§1:ğ‘¡1, we sample token ğ‘§ğ‘¡ according to ğ‘ƒ(ğ‘§ğ‘¡ ğ‘§1:ğ‘¡1) = ğ‘¦ (ğ‘£) ğ‘‡ (1) ğ‘¦ (ğ‘§ğ‘¡ ) ğ‘‡ ğ‘’ (cid:205)ğ‘£ ğ‘’ where ğ‘¦(ğ‘£) denotes the logit value of token ğ‘£, and ğ‘‡ â„>0 is temperature value which controls the flatness (or sharpness) of the probability distribution (V). Setting ğ‘‡ = 1, means we sample from the base distribution output by the model ğ‘“ğœƒ, while as ğ‘‡ 0, we converge to greedy sampling. Top-ğ‘˜ sampling. Only the top-ğ‘˜ token probabilities are kept and all others are set to zero. The non-zero probabilities are then renormalized and sampled from to select token. Note, if ğ‘˜ = 1 this is identical to greedy sampling. Top-ğ‘ sampling. This is similar to top-ğ‘˜ sampling, but instead of keeping only the top ğ‘˜ token probabilities, we keep the smallest number of tokens such that their cumulative probability is at least ğ‘ (0, 1]. We then set all other token probabilities to zero, renormalize and sample from this distribution. Note, this is usually referred to as top-ğ‘ or Nucleus sampling. 3. probabilistic definition of discoverable extraction We first provide our definition of probabilistic discoverable extraction, then discuss why it is useful, and finally how it relates to rank perplexity. Our definition captures the capabilities of regular user who can query the model (reasonable) number of times using an arbitrary next-token sampling scheme. Our definition measures the risk of this user extracting secret. Definition 3.1 ((ğ‘›, ğ‘)-discoverable extraction). Given training example ğ‘§ that is split into prefix ğ‘§1:ğ‘˜ and suffix ğ‘§ğ‘˜+1:2ğ‘˜, ğ‘§ is (ğ‘›, ğ‘)-discoverably extractable with respect to sampling scheme ğ‘”ğœ™ if where (ğ‘”ğœ™ ğ‘“ğœƒ)ğ‘˜ ğ‘– (ğ‘§1:ğ‘˜) represents an independent execution of the sampling process. Pr(ğ‘– [ğ‘›] (ğ‘”ğœ™ ğ‘“ğœƒ)ğ‘˜ ğ‘– (ğ‘§1:ğ‘˜) = ğ‘§1:2ğ‘˜) ğ‘, That is, in total, we generate ğ‘› sequences by sampling from ğ‘”ğœ™. If the probability of generating ğ‘§ğ‘˜+1:2ğ‘˜ at least once is larger than ğ‘, then we say it is (ğ‘›, ğ‘)-discoverably extractable with respect to ğ‘”ğœ™. When the sampling scheme is clear from context, we simply say the target is (ğ‘›, ğ‘)-discoverably extractable. Under specific sampling scheme, the probability ğ‘ğ‘§ of generating suffix ğ‘§ğ‘˜+1:2ğ‘˜ defines tradeoff function between ğ‘› and ğ‘ such that the training example is (ğ‘›, ğ‘)-discoverably extractable. Specifically, the probability of not generating ğ‘§ğ‘˜+1:2ğ‘˜ in single trial is 1 ğ‘ğ‘§, and the probability of not generating ğ‘§ğ‘˜+1:2ğ‘˜ in ğ‘› independent draws from the sampling scheme is (1 ğ‘ğ‘§)ğ‘›. Thus, example ğ‘§ is (ğ‘›, ğ‘)-discoverably extractable for ğ‘› and ğ‘ that satisfy 1 (1 ğ‘ğ‘§)ğ‘› ğ‘. Equivalently, we require ğ‘› log(1 ğ‘) log(1 ğ‘ğ‘§) , (2) thus, we can easily find ğ‘› given fixed ğ‘ and the probability of generating sequence and vice versa. Our definition of (ğ‘›, ğ‘)-discoverable extraction offers several advantages over standard discoverable extraction with greedy decoding as defined in Definition 2.1. We defer discussions of other definitions of memorization to Section 4. 4 Measuring memorization through probabilistic discoverable extraction Figure 1 An example of how greedy sampling can mask clear signs of memorization. We plot the rank of the target suffix tokens over each successive token that is decoded by greedy and top-ğ‘˜ sampling. At every index except one, the tokens in the target sequence have rank 1. At index 15, the target token has rank 2, which means greedy sampling does not select this token, and after which the generated sequence diverges from the target. If, at index 15, the second most likely token had been selected instead, then the remaining positions all have the target token as the top ranked token, so probabilistic sampling scheme like top-ğ‘˜ sampling has high likelihood of generating the target suffix. See Figure 11a in Appendix for the specific generated and target text for this example. Capturing instances of one-shot extraction that greedy sampling misses. First, the flexibility to choose different sampling schemes allows us to detect examples of extraction that greedy sampling misses. We give an example in Figure 1 where the target suffix has the highest likelihood of generation2, yet under greedy sampling the generated suffix does not match the target suffix3. In this example, all but one token in the target sequence is the most likely to be generated. At the 15th index (out of 50), the target suffix token is the second most likely causing greedy sampling to select the incorrect token; the language model then tries to maintain logical coherent statement, causing the greedy sampled sequence to deviate away from the target suffix. If we had used non-greedy based sampling scheme that allows for the possibility the second most likely token to be selected, then it is highly likely that the target suffix would have been generated. In fact, the target suffix has probability of 35.11% of being sampled in one shot (sampling with ğ‘‡ = 1). We plot other examples in Appendix E. Aligning better with users choice of sampling scheme. In practice, users can select the sampling scheme of their choice (or at least the sampling temperature) for popular large language model APIs, and are not bound to greedy sampling. Greedy sampling is used to calculate reported extraction rates in Definition 2.1, whilst the sampling temperature is user controlled hyperparameter in most large language model APIs. This means the amount of underlying memorization reported by Anil et al. (2023); Chowdhery et al. (2023); Kudugunta et al. (2024); Reid et al. (2024); Team et al. 2When sampling from the vocabulary distribution with ğ‘‡ = 1. 3In fact the normalized edit distance between the two is 51.9% and 66% in character and token space, respectively. 5 Measuring memorization through probabilistic discoverable extraction Figure 2 The distribution of perplexity values for generated sequences beginning from five different training prefixes. The assumption that Carlini et al. (2019) makes that the empirical distributions are approximately (skewed) Gaussian (in order to estimate rank perplexity without sampling many times) is not appropriate for general training sequences that arent restricted to bounded domain (such as phone numbers or social security numbers). (2024) could substantially differ from amount of memorization (elicited through extraction) that is experienced by the end user. It is simple fact that users can, and do, query Modeling extraction risks from multiple queries. LLMs many times. In the example in Figure 1, the user would only need three queries (in expectation) before generating the target, since it occurs with probability 35% when sampling with ğ‘‡ = 1. user who extracts sensitive information after several tries could be just as successful at exploiting this information compared to if theyd gotten the correct sequence on the first try. This is because many sensitive sequences (like phone numbers, credit card numbers, etc) can be verified through outside means. Essentially, the model has significantly reduced the search space over likely secrets, potentially to the point that its feasible to externally check each one. (ğ‘›, ğ‘)-discoverable extraction allows one to measure this compounded risk of extraction as function of the number of attempts. Finer-grained analysis of extraction tolerance. Of course this new definition now burdens the practitioner who is measuring memorization with two new hyperparameters to tune. How should one settle on ğ‘› and ğ‘? What values confer that an example is truly extractable? Generally, if ğ‘› is small and ğ‘ is large then sequences are easier to extract as they are more likely to be generated in fewer experimental trials. However, we do not prescribe specific numbers for ğ‘› and ğ‘ in this work, as we believe this is contextual to the type of information one is measuring. For example, one may be willing to release model if ğ‘› is small for generic (non-copyrighted phrases), but this may be redline if ğ‘› is small for PII contained in training data. In fact, we see this flexibility as positive. It allows the practitioner to make fine grained decisions about the level of extraction with respect differ types of information within the training data. One can view (ğ‘›, ğ‘)-discoverable extraction as the probability that the language model (and sampling scheme) provides an anonymity set set of examples within which the true training sample can hide of size ğ‘›. Under such view, one can then design reasonable anonymity set sizes such that the risk of releasing of training data through sampling is minimized. This definition also permits one to define extraction in terms of the expected computation limits from an adversary (who may simply be benign user interacting with the model). If we assume that an adversary only has specific computation budget with which to query the model, we only need to ensure ğ‘› is larger than this budget, in order to limit the probability of training data release through interactions with the model. We can set acceptable limits (i.e. ğ‘›, which in practice can be controlled through rate limiting) based on predicted query costs and compute available to an adversary. 6 Measuring memorization through probabilistic discoverable extraction 3.1. Connection to rank perplexity Carlini et al. (2019) measure the unintended memorization of random strings inserted into the training dataset called canaries via their rank perplexities. That is, an inserted canary has rank perplexity ğ‘– if the model perplexity on that canary is the ğ‘–th highest among all possible canaries that couldve been sampled and inserted. This rank naturally corresponds to the number of guesses an adversary would need to make before correctly guessing the canary if the adversary guessed canaries in order from most to least likely. This is fairly natural setup for bounded sets of potential canaries; however, in our setting, it would be intractable to enumerate all possible token sequences of given length. Carlini et al. (2019) use (skewed) Gaussian approximation to model the expected distribution of perplexity values over number of sequences, such that one can estimate rank perplexity of target without having to sample large number of sequences. However, this assumption of Gaussian approximation being good distributional fit is tailored to format-constrained canaries. In Figure 2, for five different training examples from the Enron dataset (Gao et al., 2020), we plot the perplexity distribution over 1,000 generated sequences (initializing from the examples prefix); clearly (skewed) Gaussian is relatively poor fit in all cases. Importantly, the empirical distributions vary from example to example, and so one could not estimate distribution for one training example, and expect this distribution is good fit for another example. This makes estimating rank perplexity challenging in this general setting. 4. Comparison to existing memorization definitions In formulating our definition of extraction, our aim was to define something that 1) can easily be operationalized against production LLMs without needing to retrain multiple models 2) roughly corresponds to the capabilities of typical user, and 3) aligns with the risks posed by extraction. Numerous definitions of memorization have been proposed, each with their own trade-offs that arent necessarily aligned with ours. Information-theoretic definitions. number of memorization definitions have an informationtheoretic flavor that rigorously capture models dependence on its training data. Unfortunately, estimating these dependencies often requires training many models, which is infeasible in our setting. Brown et al. (2021) measure the mutual information between the training dataset and the model output conditioned on the data generating distribution. This definition neatly captures the essence of memorization, that is: the amount of information that is contained about the dataset in the model which is not property of the underlying data distribution. The authors are not aware of any works that empirically measure the conditional mutual information of trained model and its training dataset; however, this quantity could potentially be estimated or lower bounded using membership inference attacks. Getting an accurate estimate is likely to require training many models with different attack setting than the one we consider. Other definitions similarly consider counterfactual approach (Zhang et al., 2023) to memorization, which requires estimating the models performance with and without particular training example. While Zhang et al. use subsampling to reduce the number of models that need to be trained, they still require on the order of hundreds of models to get an estimate of the counterfactual memorization of the training data. Memorization with respect to copyright infringement. number of works have studied memorization through the lens of copyright infringement, for example by measuring the compression 7 Measuring memorization through probabilistic discoverable extraction properties of an LLM with respect to training example (Schwarzschild et al., 2024), or by using computational complexity measures to quantify whether an output could have been reasonably generated without the copyrighted training example (Scheffler et al., 2022). The harms related to memorization of copyrighted material are quite distinct from extracting sensitive information, which is the risk we are looking to quantify, so these definitions are ill-suited to our problem. As concrete example, Schwarzschild et al. (2024) measure the length of the smallest prompt that will generate the copyrighted material in question as an indication of whether the material was memorized. In our setting, though, the harm is not necessarily proportional to the length of the prompt. One could imagine very short but unusual prefix that reliably generates secret versus typical but longer prefix doing the same. Clearly when measuring the risk of extraction by regular user, the latter prompt is at much higher risk of revealing the secret even though it is longer, so this definition does not align with our framework of risks and harms. Prompt engineering to measure extraction. There is another line of works that use clever prompting to extract information from target LLM. For example, Kassem et al. (2024) use another LLM to find prompts that elicit memorization, and Wang et al. (2024) construct prefix-dependent soft prompts to extract given suffix. Both attacks involve optimizing over multiple prompts for each target sequence, which is more expensive than our approach. 5. Experiments We now demonstrate the utility of this new definition through various experiments. We find that the (ğ‘›, ğ‘)-discoverable extraction rate surpasses greedy extraction rates for modest values of ğ‘› and ğ‘. Notably, across all values of ğ‘› and ğ‘ we studied, the risk of extracting training data is significantly higher than baseline extraction rates for test data. This suggests that even for quite large ğ‘› and small ğ‘ the risk of extraction is meaningfully higher for data that was included during training. Additionally, the gap between greedy and non-greedy extraction rates only increases for larger models and for training data that is included multiple times. Finally, when comparing extraction rates between different models, one model may have lower greedy extraction rate but higher (ğ‘›, ğ‘)-discoverable extraction rate (for all ğ‘› and ğ‘) compared to the other. Together, these observations demonstrate that (ğ‘›, ğ‘)-discoverable extraction gives nuanced measurement of the risk of data extraction. Setup We use the Pythia model (Biderman et al., 2023), and unless otherwise stated use the 2.8B size. We measure memorization on 10,000 examples from the Enron dataset (Pythia models were trained on the Pile dataset (Gao et al., 2020) which contains the Enron dataset). For each example we take the first 50 tokens as the prefix and the next 50 tokens as the target suffix. As discussed by Carlini et al. (2021), 50 tokens corresponds to an average of 25 words in the training set, well over the length of typical English sentence, and so any match between target and generated suffix will almost surely be due to memorization. Unless otherwise stated we use top-ğ‘˜ = 40 sampling as the default sampling scheme. Throughout our experiments we will compare to the greedy sampling based extraction rate, found from discoverable extraction according to Definition 2.1. 5.1. How extraction rates change with ğ‘› and ğ‘ In Figure 3, we plot extraction rates for two model sizes under various choices of ğ‘› and ğ‘ (results across more model sizes are given in Appendix F). As expected from Equation (2), the extraction rate appears to have log-linear relationship with ğ‘› for all choices of ğ‘. We first note that under small ğ‘ 8 Measuring memorization through probabilistic discoverable extraction (a) 2.8B. (b) 12B. Figure 3 (ğ‘›, ğ‘)-discoverable extraction for Pythia 2.8B and 12B using top-ğ‘˜ = 40 sampling. For moderately small ğ‘› (e.g < 20) and large ğ‘ (e.g. 50%), meaning training data examples will be generated within small number of trials with high probability, the extraction rate exceeds the greedy sampling extraction rate found through discoverable extraction according to Definition 2.1. of 10% meaning that even small probability of generating training sequence may be problematic the extraction rate exceeds the greedy-based extraction rate (1.3% for 2.8B and 3.03% for 12B) for ğ‘› > 3. If we set ğ‘ = 99.9%, meaning for given ğ‘›, it is almost certain that the target training sequence will be generated at least once, we only approach the greedy extraction rate when ğ‘› > 169. However, this drops to ğ‘› > 57 and ğ‘› > 17 for ğ‘ = 90% and ğ‘ = 50%, respectively. Ultimately, if one were to measure discoverable extraction according to Definition 2.1 with top-ğ‘˜ sampling, meaning only one sequence is generated and checked for match with the target suffix, one may assume there is little to no memorization present. Allowing ğ‘› to be larger than one reveals different story. Having established the connection between memorization, ğ‘›, and ğ‘, for top-ğ‘˜ = 40 sampling, we show that similar results hold over other sampling schemes in Appendix A. (ğ‘›, ğ‘)-discoverable extraction reveals better comparison of memorization rates across models. trends in memorization across models that may not be apparent from Definition 2.1. To show this, we compare extraction rates of Pythia 1B with GPT-Neo 1.3B (Black et al., 2021), which is similar sized model also trained on the Pile. In Figure 4, we observe that Pythia 1B has larger greedy-based extraction rate compared to GPT-Neo 1.3B, however at every ğ‘›, the (ğ‘›, ğ‘)-discoverable extraction rate (using top-ğ‘˜ sampling) of GPT-Neo 1.3B is larger than Pythia 1B. practitioner measuring memorization with discoverable extraction and greedy sampling would have falsely concluded that Pythia memorizes training data at higher rate than GPT-Neo, whereas (ğ‘›, ğ‘) discoverable extraction implies the opposite, even for very small values of ğ‘›, and large values of ğ‘. 5.2. Measuring relative risk of data extraction natural concern for extraction rates on large ğ‘› and small ğ‘ may be that the parameters are too permissive to provide meaningful measure of extraction; that is, perhaps the model just happens to eventually complete the suffix with some small probability. To the contrary, when we compare extraction rates for training and test data, we find that for every level of ğ‘› and ğ‘ that we studied, the extraction rates were significantly higher for training than test data. For test data, we use set of 10,000 emails from the TREC 2007 Spam classification dataset (Bratko et al., 2006). In Figure 5, we see that the relative risk of extraction of non-training data is extremely low. Even for very large ğ‘›, the extraction rate is less than 1%. With this baseline in hand, we can compute the relative risk of extraction for settings where the absolute risk may not tell the full story 9 Measuring memorization through probabilistic discoverable extraction Figure 4 Comparison of extraction rates across two different models. (either because the absolute risk is fairly miniscule or the baseline risk is somewhat high). 5.3. Memorization magnified: the impact of model size and data repetition Interestingly, the gap between extraction rates reported by greedy-based discoverable extraction and from top-ğ‘˜ based (ğ‘›, ğ‘)-discoverable extraction increases for larger models and under repetitions in the training data. That is, to the extent that greedy-based extraction attacks may be underestimating extraction, this underestimate worsens for larger models and for target data that is repeated. Additionally, we verify the findings in prior work that larger models tend to memorize training data at higher rates (Biderman et al., 2024; Carlini et al., 2022; Lu et al., 2024; Mireshghallah et al., 2022; Tirumala et al., 2022) and that repeated training data is more likely to be memorized (Biderman et al., 2024; Carlini et al., 2021, 2022; Feldman and Zhang, 2020; Lee et al., 2021; Zhang et al., 2023). Larger models memorize more. We confirm the common observation that larger models memorize more in Figure 3 (and on larger set of model sizes in Appendix F), the greedy sampling extraction rate increases from 0.76% for Pythia-1B to 3.03% for Pythia-12B. Similarly extraction rates from (ğ‘›, ğ‘)-discoverable extraction increase with model size. For example, for Pythia 1B, at ğ‘ = 90%, we need to generate 150 sequences (ğ‘› = 150) to exceed the greedy rate, while for Pythia 12B we only need to generate 40. Larger models may be undercounting memorization at higher rate compared to smaller models. Repeated data is memorized more. Additionally, we show that training data repetition similarly increases extraction rates, as one would expect. We compare the greedy sampling extraction rate under training data repetitions to (ğ‘›, ğ‘)-discoverable extraction rate with top-ğ‘˜ sampling. For this experiment, we find phone numbers within the training dataset that are replicated number of times. We use the phone number as the target suffix and the preceding text as the prefix with which we 10 Measuring memorization through probabilistic discoverable extraction Figure 5 Relative extraction rates between training and test datasets for different choices of ğ‘› and ğ‘. prompt the model. We then bucket extraction rates by the number of repetitions in the training data. Results are show in Figure 6. As expected, more repetitions in the training data results in larger extraction rate. Similar to our observation on larger models, the gap between the greedy rate and top-ğ‘˜ rate with (ğ‘›, ğ‘) becomes wider for more repetitions (note the y-axis is logarithmic, we show the non-logarithmic version in Appendix B). 6. Discussion In light of the experiment results, we now discuss different interpretations of our new definition and why it is useful. (ğ‘›, ğ‘)-discoverable extraction is cheap to compute. One only needs to measure the probability of generating target suffix under sampling scheme. This means computing (ğ‘›, ğ‘)-discoverable extraction is no more expensive in terms of compute than greedy-based discoverable extraction; the inference cost of generating suffix or measuring its probability is identical. We confirm in Appendix that the extraction rate computed by (ğ‘›, ğ‘)-discoverable extraction using Equation (2) faithfully predicts the empirical rate if one were to generate ğ‘› sequences and measure the probability ğ‘ that the target suffix is generated at least once. finer grained analysis. The regurgitation of training data is not equally problematic across all examples. Data that contains copyrighted or personally identifiable information (PII) will be of the highest concern, while in some cases memorization may in fact be desired property (e.g. memorization of common facts). Our relaxation of discoverable extraction allows practitioner to set level of risk of how problematic training data regurgitation would be, in the form of the parameters ğ‘› and ğ‘, and one can change these parameters according to the different data subsets. 11 Measuring memorization through probabilistic discoverable extraction Figure 6 Extraction rates for greedy sampling and top-ğ‘˜ = 40 sampling, where for the latter we report (ğ‘›, ğ‘)-discoverable extraction rates for ğ‘› = 1, 10, 100 and ğ‘ = 10%, 90%. Even for small ğ‘› and large ğ‘, (ğ‘›, ğ‘) rates are larger than the greedy rate, and importantly, this gap widens for more repetitions in the training data. If ğ‘› is extremely large and ğ‘ extremely Quantification of relative and absolute extraction risks. small. Is it fair to call training example memorized? Another interpretation of (ğ‘›, ğ‘)-discoverable extraction is that is describes anonymity set sizes within which training data example can expect to hide. In Section 5.2, we observed substantial difference between extraction rates of training and test data, implying that, like greedy-based discoverable extraction, (ğ‘›, ğ‘)-discoverable extraction gives an operational quantification of how much training data is memorization. Moreover, our definition can be used to measure absolute risk or relative risk (by comparing to non-training baseline). Measuring risks beyond memorization. Beyond quantifying memorization of training data, one could use (ğ‘›, ğ‘)-discoverable extraction to measure the risk of model outputting any target string, including undesirable or harmful content. 7. Limitations Our work can be expanded in number of ways. Fundamentally, memorization and extraction are challenging to measure, with better attacks coming out all the time. In this work, we study the capabilities of relatively benign adversary (one with API access only and limited side information) and leave the study of more powerful adversaries to other work. Additionally, we make no distinction between extracting different types of targets. In reality, some target sequences are significantly more sensitive than others, so their extraction rates should be measured separately. Lastly, we only cursorily consider extraction of PII (specifically, phone numbers) for the purpose of measuring the effect of repetitions. We leave it to future work to investigate extraction rates of different types of PII. 12 Measuring memorization through probabilistic discoverable extraction"
        },
        {
            "title": "References",
            "content": "R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. S. Basu, G. S. Ramachandran, N. S. Keskar, and L. R. Varshney. Mirostat: neural text decoding algorithm that directly controls perplexity. arXiv preprint arXiv:2007.14966, 2020. S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. OBrien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. S. Biderman, U. PRASHANTH, L. Sutawika, H. Schoelkopf, Q. Anthony, S. Purohit, and E. Raff. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems, 36, 2024. S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, Mar. 2021. URL https://doi.org/10.5281/zenodo. 5297715. S. Bordt, H. Nori, V. Rodrigues, B. Nushi, and R. Caruana. Elephants never forget: Memorization and learning of tabular data in large language models. arXiv preprint arXiv:2404.06209, 2024. N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Audio chord recognition with recurrent neural networks. In ISMIR, pages 335340. Curitiba, 2013. A. Bratko, B. FilipiÄ, G. V. Cormack, T. R. Lynam, and B. Zupan. Spam filtering using statistical data compression models. The Journal of Machine Learning Research, 7:26732698, 2006. G. Brown, M. Bun, V. Feldman, A. Smith, and K. Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In Proceedings of the 53rd annual ACM SIGACT symposium on theory of computing, pages 123132, 2021. N. Carlini, C. Liu, Ãš. Erlingsson, J. Kos, and D. Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX security symposium (USENIX security 19), pages 267284, 2019. N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 26332650, 2021. N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. M. Duan, A. Suri, N. Mireshghallah, S. Min, W. Shi, L. Zettlemoyer, Y. Tsvetkov, Y. Choi, D. Evans, and H. Hajishirzi. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024a. S. Duan, M. Khona, A. Iyer, R. Schaeffer, and I. R. Fiete. Uncovering latent memories: Assessing data leakage and memorization patterns in large language models. arXiv preprint arXiv:2406.14549, 2024b. 13 Measuring memorization through probabilistic discoverable extraction A. Fan, M. Lewis, and Y. Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018. V. Feldman and C. Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems, 33:28812891, 2020. L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, arXiv preprint et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv:2101.00027, 2020. A. Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711, 2012. A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. J. Huang, D. Yang, and C. Potts. Demystifying verbatim memorization in large language models. arXiv preprint arXiv:2407.17817, 2024. A. M. Kassem, O. Mahmoud, N. Mireshghallah, H. Kim, Y. Tsvetkov, Y. Choi, S. Saad, and S. Rana. Alpaca against vicuna: Using llms to uncover memorization of llms. arXiv preprint arXiv:2403.04801, 2024. S. Kudugunta, I. Caswell, B. Zhang, X. Garcia, D. Xin, A. Kusupati, R. Stella, A. Bapna, and O. Firat. Madlad-400: multilingual and document-level large audited dataset. Advances in Neural Information Processing Systems, 36, 2024. K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021. X. Lu, X. Li, Q. Cheng, K. Ding, X. Huang, and X. Qiu. Scaling laws for fact memorization of large language models. arXiv preprint arXiv:2406.15720, 2024. F. Mireshghallah, A. Uniyal, T. Wang, D. K. Evans, and T. Berg-Kirkpatrick. An empirical analysis of memorization in fine-tuned autoregressive language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 18161826, 2022. Y. More, P. Ganesh, and G. Farnadi. Towards more realistic extraction attacks: An adversarial perspective. arXiv preprint arXiv:2407.02596, 2024. M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. TramÃ¨r, and K. Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023. M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. S. Scheffler, E. Tromer, and M. Varia. Formalizing human ingenuity: quantitative framework for copyright laws substantial similarity. In Proceedings of the 2022 Symposium on Computer Science and Law, pages 3749, 2022. A. Schwarzschild, Z. Feng, P. Maini, Z. C. Lipton, and J. Z. Kolter. Rethinking llm memorization through the lens of adversarial compression. arXiv preprint arXiv:2404.15146, 2024. 14 Measuring memorization through probabilistic discoverable extraction W. Shi, A. Ajith, M. Xia, Y. Huang, D. Liu, T. Blevins, D. Chen, and L. Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789, 2023. V. Smith, A. S. Shamsabadi, C. Ashurst, and A. Weller. Identifying and mitigating privacy risks stemming from language models: survey. arXiv preprint arXiv:2310.01424, 2023. R. Staab, M. Vero, M. BalunoviÄ‡, and M. Vechev. Beyond memorization: Violating privacy via inference with large language models. arXiv preprint arXiv:2310.07298, 2023. R. Tang, G. Lueck, R. Quispe, H. A. Inan, J. Kulkarni, and X. Hu. Assessing privacy risks in language models: case study on summarization tasks. arXiv preprint arXiv:2310.13291, 2023. G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. RiviÃ¨re, M. S. Kale, J. Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. K. Tirumala, A. Markosyan, L. Zettlemoyer, and A. Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:3827438290, 2022. A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee, D. Crandall, and D. Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016. Z. Wang, R. Bao, Y. Wu, J. Taylor, C. Xiao, F. Zheng, W. Jiang, S. Gao, and Y. Zhang. Unlocking memorization in large language models with dynamic soft prompting. arXiv preprint arXiv:2409.13853, 2024. S. Zanella-BÃ©guelin, L. Wutschitz, S. Tople, V. RÃ¼hle, A. Paverd, O. Ohrimenko, B. KÃ¶pf, and M. Brockschmidt. Analyzing information leakage of updates to natural language models. In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security, pages 363375, 2020. C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. TramÃ¨r, and N. Carlini. Counterfactual memorization in neural language models. Advances in Neural Information Processing Systems, 36:3932139362, 2023. Z. Zhang, Z. Liu, and I. Patras. Get confused cautiously: Textual sequence memorization erasure with selective entropy maximization. arXiv preprint arXiv:2408.04983, 2024. 15 Measuring memorization through probabilistic discoverable extraction (a) top-ğ‘˜, ğ‘ = 10%. (b) top-ğ‘, ğ‘ = 10%. (c) temperature (ğ‘‡), ğ‘ = 10%. (d) top-ğ‘˜, ğ‘ = 90%. (e) top-ğ‘, ğ‘ = 90%. (f) temperature (ğ‘‡), ğ‘ = 90%. Figure 7 Comparison of (ğ‘›, ğ‘)-discoverable extraction rates for different sampling schemes. We fix ğ‘ to either 10% or 90%. In Figure 7a and Figure 7d we vary ğ‘˜ in top-ğ‘˜ sampling, in Figure 7b and Figure 7e we vary ğ‘ in top-ğ‘ sampling, and in Figure 7c and Figure 7f we vary ğ‘‡ in temperature sampling. Generally, smaller ğ‘˜, ğ‘ and ğ‘‡ confer smaller extraction rates, but is dependent on both ğ‘› and ğ‘. However, for most values and for small ğ‘›, the extraction rate is above the greedy sampling based extraction rate. A. Comparison over extraction rates under different sampling schemes We compare extraction rates under different choices of ğ‘› and ğ‘ for top-ğ‘˜, top-ğ‘, and temperature sampling in Figure 7, where we vary the sampling scheme specific hyperparameters ğ‘˜, ğ‘, and ğ‘‡, respectively. We make the following observations. Firstly, in top-ğ‘˜ sampling, increasing ğ‘˜ can substantially increase extraction rates even for small ğ‘›, and this effect becomes more pronounced with small values of ğ‘. For example, in Figure 7a where we fix ğ‘ = 10%, the extraction rate at ğ‘˜ = 2 is 1.5% at ğ‘› = 1; significantly higher than at ğ‘˜ = 1, and this difference only increases with ğ‘›. The same effect can be observed in Figure 7d where we fix ğ‘ = 90%, however at smaller ğ‘› (ğ‘› < 20) the difference is smaller. In general, larger values of ğ‘˜ confer larger extraction rates, but this is affected by both ğ‘ and ğ‘›; for small ğ‘, extraction rates for small ğ‘˜ are dominated by larger ğ‘˜, while for larger ğ‘, extraction rates for small ğ‘˜ dominate larger ğ‘˜, but this eventually reverses when ğ‘› becomes larger enough. Similar trends can be observed for top-ğ‘ sampling, in Figure 7b and Figure 7e, and for temperature sampling in Figure 7c and Figure 7f, where larger values of ğ‘ and ğ‘‡, results in larger extraction rates as ğ‘› increases. At first glance, one may wonder why there isnt strict ordering of extraction rates for top-ğ‘˜, top-ğ‘, and temperature sampling, if rates are compared according scaling ğ‘˜, ğ‘, and ğ‘‡, respectively. (ğ‘›, ğ‘)-discoverable extraction is designed such that expected extraction rate matches the amount of memorization emitted when an end user interacts with the model. However, this is challenging as users are generally free to choose the underlying sampling scheme. If we report extraction rate under choice of temperature ğ‘‡ and an end user choose to use different temperature ğ‘‡ ğ‘‡, is the reported extraction rate still useful? The added complication here being that as ğ‘˜, ğ‘, or ğ‘‡ is varied, token probabilities can either increase or decrease. For example in top-ğ‘˜ sampling, as ğ‘˜ increases, the probability of sampling specific token ğ‘§ğ‘– can either decrease (a larger ğ‘˜ results in more tokens 16 Measuring memorization through probabilistic discoverable extraction available for sampling, decreasing the probability of sampling ğ‘§ğ‘–) or increase (if at smaller ğ‘˜, ğ‘§ğ‘– had zero probability). This means (ğ‘›, ğ‘) extraction rates are not properly ordered according to the choice of ğ‘˜, ğ‘, or ğ‘‡. If practitioner is concerned about the varying extraction rates under different sampling hyperparameters, it is easy to compute rates over different choices as we have done in Figure 7. Because top-ğ‘˜, top-ğ‘, and temperature sampling are post processing functions applied on top of the generated logit distribution over tokens, it is extremely cheap to compute these rates over different sampling hyperparameters. B. Extraction rates for repeated training data Figure 8 We plot non log scale version of Figure 6 to make it clear that the difference between the greedy sampling extraction rate and (ğ‘›, ğ‘) rate increases with more training data repetitions. 17 Measuring memorization through probabilistic discoverable extraction C. Does the theoretical extraction rate match empirical results? Figure 9 We check that generating set of ğ‘› sequences and checking the probability training example appears at least once in the set (empirical ğ‘), matches the theoretical ğ‘ according to Equation (2). So far, we have reported memorization found through Equation (2). This means for given training example, we find the probability of generating the target suffix, and then for fixed ğ‘, we find the corresponding ğ‘› according to Equation (2). Here, we confirm that this procedure aligns with the (more costly) empirical procedure of sampling ğ‘› sequences, and checking the probability that the sequence appears in the set is ğ‘. For 1,000 training examples, and for fixed ğ‘, we calculate the ğ‘› such that sampling ğ‘› sequences should result in fraction of ğ‘ matches. For each training example, we then generate ğ‘› sequences and check if the example appears in the set. We then check if fraction ğ‘ of the examples did appear in their associated sets, over the 1,000 examples. Results are given in Figure 9, showing expected ğ‘ according to our theory does match the empirical calculated ğ‘. D. Distribution of perplexity scores For each of the 10,000 Enron examples, we plot the perplexity score of the target suffix for choice of sampling scheme (top-ğ‘˜ = 40, top-ğ‘ = 0.9, random sampling with ğ‘‡ = 1) in Figures 10a to 10c. For reference, we also plot the probability that each of the target suffixes will be sampled within ğ‘› = 100 trials in Figures 10d to 10f. 18 Measuring memorization through probabilistic discoverable extraction (a) Perplexity scores for target suffixes with top-ğ‘˜ = 40. (b) Perplexity scores for target suffixes with top-ğ‘ = 0.9. (c) Perplexity scores for target suffixes with random sampling with ğ‘‡ = 1. (d) Probability of sampling the target suffix within ğ‘› = 100 trials for top-ğ‘˜ = 40 sampling. (e) Probability of sampling the target suffix within ğ‘› = 100 trials for top-ğ‘ = 0.9 sampling. (f) Probability of sampling the target suffix within ğ‘› = 100 trials for random sampling with ğ‘‡ = 1. Figure 10 For each of the 10,000 Enron examples, we plot the perplexity score of the target suffix for choice of sampling scheme. We also plot the probability that each of the target suffixes will be sampled within ğ‘› = 100 trials. Note, for top-ğ‘˜ and top-ğ‘ sampling, there are fewer than 10,000 examples plotted. This is because many target suffixes have zero probability of being sampled under the sampling scheme. From Figure 10a, we see that nearly 9,000 examples do not have perplexity score. This is because it was impossible to sample the target suffix with top-ğ‘˜ = 40 sampling, because at least one of the tokens in the sequence has zero probability of being sampled. Out of the 1,000 suffixes that could be sampled successfully, the majority have extremely low perplexity. This is reflected in Figure 10d, where we observe that the large fraction of examples will almost certainly being sampled within ğ‘› = 100 trials. similar set of observations can be made for top-ğ‘ = 0.9 sampling in Figures 10b and 10e. For random sampling with ğ‘‡ = 1, in theory all possible sequences could be sampled, and so each of the 10,000 examples has defined perplexity score. Again, we see that large fraction of target suffix sequences have small perplexity scores; from Figure 10f, approximately 250 target suffix examples will almost surely be sampled within ğ‘› = 100 trials. E. More examples of how greedy sampling can mask signs of memorization Following from Figure 1 and Section 3 we give three more of examples in Figure 11 of cases where greedy sampling misses clear cases of memorization. We include the text for the target and generated sequences omitted from Figure 1. F. Large models memorize more: experiments with more model sizes We plot more model sizes in Figure 12. Across most choices of ğ‘› and ğ‘, we find that when model parameters double, so do extraction rates, approximately. Measuring memorization through probabilistic discoverable extraction (a) Prefix: nnThe information contained herein is based on sources that we believe to benreliable, but we do not represent that it is accurate or complete. Nothingncontained herein should be considered as an offer to sell or solicitationnof an, Suffix (predicted from greedy sampling): offer to buy any financial instruments discussed herein. Anynopinions, estimates, or other information contained in this communicationnis solely the responsibility of the author.n - att1.htm. Target Suffix: offer to buy any financial instruments discussed herein. Anynopinions expressed herein are solely those of the author. As such, theynmay differ in material respects from those of, or expressed or published bynon behalf of Carr Futures, where we highlight the differences between predicted and target tokens. (b) Prefix: do not.nn Original Message nFrom: tPanus, Stephanie nSent:tMonday, October 22, 2001 9:35 AMnTo:tBailey, Susan; Boyd, Samantha; Cook, Mary. Suffix (predicted from greedy sampling): ; ; Gray, Barbara N.; Greenberg, Mark; Hansen, Leslie; Heard, Marie; Hendry, Brent; Hodge, Jeffrey T.; Jones, Tana; Koehler, Anne C.; Leite, Francisco Pinto; Nelson. Target Suffix: ; Gray, Barbara N.; Heard, Marie; Hendry, Brent; Jones, Tana; Keiser, Holly; Koehler, Anne C.; Leite, Francisco Pinto; Nelson, Cheryl; Sayre, Frank;. (c) Prefix: Version 2.0 of the ClickAtHome Portal is ntLog into www.clickathome.net now available! now, from work or home!nnAccess the PEP System through the portal! ntChoose your reviewers. Suffix and fill out (predicted from greedy sampling): your evaluations in the comfort and privacy of your home!nnYour Portal Username:tthgm3122nYour Password:tt9hgm3122nnWelcome to PEP at http://www.click. Target Suffix: and fill out your evaluations in the comfort and privacy of your home!nnOutlook Web Access and eHRonline will be available SOON! nnThe ClickAtHome portal is fully customizable for you! ntYOU choose. (d) Prefix: SHIVELY, HUNTER,n nThe PEP system closes on Friday, May 25, 2001 nOur records indicate that you have been requested to provide feedback on one or more Enron employees. The deadline for completing feedback is Friday. Suffix (predicted from greedy sampling): , May 25th. nBelow is some information which will assist you in completing your feedback forms and email them to us at feedback@pep.enron.com. Please note that you can add additional people within the PEP. , May 25th. nBelow is list of Target Suffix: feedback requests with status of \"OPEN\". Please complete or decline these requests as soon as possible by logging into PEP at http://pep.enron.com and selecting. Figure 11 Four more examples from the Enron dataset of failures of greedy sampling to measure memorization with discoverable extraction (we give another in Figure 1). We highlight the differences between predicted and target tokens. 20 Measuring memorization through probabilistic discoverable extraction (a) 1B. (b) 2.8B. (c) 6.9B. (d) 12B. Figure 12 (ğ‘›, ğ‘)-discoverable extraction for different (Pythia) model sizes using top-ğ‘˜ = 40 sampling."
        }
    ],
    "affiliations": [
        "Boston University",
        "Google DeepMind"
    ]
}