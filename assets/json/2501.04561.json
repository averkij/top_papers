{
    "paper_title": "OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment across Language with Real-time Self-Aware Emotional Speech Synthesis",
    "authors": [
        "Run Luo",
        "Ting-En Lin",
        "Haonan Zhang",
        "Yuchuan Wu",
        "Xiong Liu",
        "Min Yang",
        "Yongbin Li",
        "Longze Chen",
        "Jiaming Li",
        "Lei Zhang",
        "Yangyi Chen",
        "Hamid Alinejad-Rokny",
        "Fei Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in omnimodal learning have been achieved in understanding and generation across images, text, and speech, though mainly within proprietary models. Limited omnimodal datasets and the inherent challenges associated with real-time emotional speech generation have hindered open-source progress. To address these issues, we propose openomni, a two-stage training method combining omnimodal alignment and speech generation to develop a state-of-the-art omnimodal large language model. In the alignment phase, a pre-trained speech model is further trained on text-image tasks to generalize from vision to speech in a (near) zero-shot manner, outperforming models trained on tri-modal datasets. In the speech generation phase, a lightweight decoder facilitates real-time emotional speech through training on speech tasks and preference learning. Experiments demonstrate that openomni consistently improves across omnimodal, vision-language, and speech-language evaluations, enabling natural, emotion-rich dialogues and real-time emotional speech generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 6 5 4 0 . 1 0 5 2 : r OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment across Language with Real-time Self-Aware Emotional Speech Synthesis Run Luo1,2*, Ting-En Lin3, Haonan Zhang3, Yuchuan Wu3, Xiong Liu3, Min Yang1,2, Yongbin Li3, Longze Chen1,2, Jiaming Li1,2, Lei Zhang1,2, Yangyi Chen4, Hamid Alinejad-Rokny5, Fei Huang3 1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3Tongyi Laboratory 4UIUC 5New South Wales {r.luo, min.yang}@siat.ac.cn {ting-en.lte, shuide.lyb}@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in omnimodal learning have been achieved in understanding and generation across images, text, and speech, though mainly within proprietary models. Limited omnimodal datasets and the inherent challenges associated with real-time emotional speech generation have hindered open-source progress. To address these issues, we propose OpenOmni, two-stage training method combining omnimodal alignment and speech generation to develop state-of-the-art omnimodal large language model. In the alignment phase, pre-trained speech model is further trained on text-image tasks to generalize from vision to speech in (near) zero-shot manner, outperforming models trained on tri-modal datasets. In the speech generation phase, lightweight decoder facilitates real-time emotional speech through training on speech tasks and preference learning. Experiments demonstrate that OpenOmni consistently improves across omnimodal, vision-language, and speech-language evaluations, enabling natural, emotionrich dialogues and real-time emotional speech generation. 1. Introduction With the success of large language models (LLMs) [3, 41, 43], multimodal large language models (MLLMs) [14, 26, 27, 31, 53] built on LLMs have garnered significant attention, particularly with the emergence of vision LLMs [4, 26, 27, 31] and speech LLMs [8, 14]. This marks paradigm shift in machine understanding and interaction with the world. The advent of omnimodal large language models (OLLMs) such as GPT-4o has revived the urgent demand *Work done while interning at Tongyi Laboratory Min Yang and Yongbin Li are corresponding authors. Figure 1. Overview of the motivation and architecture of OpenOmni. For simplicity, our core architecture is presented without the connectors between modules. for omnimodal models capable of visual, linguistic, and auditory functionalities. Current open-source omnimodal models [5, 15, 39, 51] typically rely on autoregressive architectures (AR) driven by tri-modal data or on external text-to-speech (TTS) [12] models that are not end-to-end. Such models frequently suffer from limitations due to lack of high-quality data, challenges in real-time interaction, and significant dissonance in emotional contexts that hinder their ability to empathize with humans. Consequently, the development of omni1 modal models is considerably constrained by these lowquality data and limitations in real-time emotional speech generation, falling behind commercial models. To address these two challenges, we propose simple yet effective method OpenOmni. As shown in Fig. 1, OpenOmni utilizes large language models to achieve zeroshot omnimodal alignment across language. Based on this foundation, we also design an end-to-end, lightweight speech decoder that not only facilitates the simultaneous generation of text and speech through parallel decoding, enabling real-time interaction, but also supports self-aware emotional speech generation. This capability is achieved without needing additional control modules or prompts, generating contextually consistent emotional speech that demonstrates empathetic abilities. Specifically, OpenOmni takes advantage of the existing abundance of speech-text and image-text data, employing phased training approach that positions language as pivot. This allows for implicit omnimodal alignment utilizing the generative capacities of large language models without necessitating high-quality tri-modal data. Subsequently, we conduct speech generation training for the speech decoder using self-constructed high-quality bilingual speech generation datasets in Chinese and English, labeled as O2S-300K and EO2S-9K. These datasets comprise approximately 8000 hours of text-synthesized bilingual speech, which is conducive to efficient speech generation and emotional injection training. The self-aware emotional empathy capability is further integrated into the omnimodal model through direct preference optimization for emotional speech, empowering it with real-time emotional speech generation abilities. Experimental results demonstrate that our method exhibits advantages in efficient omnimodal alignment, realtime speech generation, and emotional speech generation. Compared to the state-of-the-art fully open-source model VITA [15], which utilizes larger language model (7x8B) and more training data (5M), OpenOmni achieves superior performance with significantly smaller model size (7B vs. 7x8B) and less training data (1.8M vs. 5M), surpassing VITA by four absolute points on the OmniBench [24] benchmark. Additionally, OpenOmni achieves competitive results across various bi-modal tasks at considerably lower cost, including real-time speech generation, speech understanding, and image-text question answering. Before delving into details, we summarize our contributions as follows. Speech Generation Dataset: We construct highquality speech generation dataset and direct preference optimization data for emotional speech based on publicly available compact, high-quality data from MMEvol [32] and UltraChat [10], comprising O2S-300K and EO2S9K. This includes 8000 hours of text-synthesized bilingual speech for efficient speech generation and emotional injection training. Effective Method: We propose model-agnostic, scalable approach that enables low-resource, rapid omnimodal alignment by leveraging language as pivot, followed by speech generation and emotional injection training. This approach allows for the rapid construction of an advanced all-modal assistant akin to GPT-4o, designed to provide an excellent interactive experience. OpenOmni Model: We trained an omnimodal language model with end-to-end text, image, and speech understanding capabilities. After speech generation training and direct preference optimization for emotional speech, OpenOmni can naturally perform multimodal tasks such as emotional speech generation, speech understanding, and image-text question answering. Comprehensive Experiments: We provide extensive comparative experiments and ablation studies to demonstrate the effectiveness and efficiency of our proposed method. 2. Related Work 2.1. Vision Language Models Vision Language Models have rapidly advanced in recent years due to the success of Large Language Models (LLMs) and the availability of diverse image-text instruction data [27, 32] from the internet. LLaVA [27] and MiniGPT-4 [57] have demonstrated strong cross-task generalization by integrating visual encoders with large language models through simple connectors and training on instruction data. LLaVA-NeXT [26] has significantly enhanced visual perception by employing dynamic resolution techniques. DreamLLM [11] attempts to generate images and text in the interleaved context concurrently. DEEM [31] simplifies model architecture and enhances robustness by using diffusion models to extract visual features instead of traditional visual encoders. 2.2. Speech Language Models Recent advancements in integrating speech capabilities into LLMs have significantly improved the naturalness and efficiency of human-computer interactions. Models such as SpeechGPT [53] and LLaMA-Omni [14] eliminate the reliance on intermediate text transcription, thereby reducing latency in multimodal content generation. For full-duplex dialogue systems, Moshi [9] and OmniFlatten [55] excel in managing simultaneous speech and text streams, adeptly handling challenges like overlapping speech and interruptions [25]. Meanwhile, Freeze-Omni [44] introduces novel approach by retaining the core capabilities of the original LLM during training, enabling low-latency speech-tospeech dialogue without modifying the pre-trained architecture. Emo-DPO [17] focuses on emotional speech synthe2 Figure 2. Training process of OpenOmni. To facilitate the omnimodal learining and real-time emotional speech generation of OpenOmni, we employ three-stage training process: (1) Speech-Text Generation: In this stage, we utilize speech encoder to extract continuous speech features and text features for alignment learning, which enables the large language model to develop speech understanding capabilities. (2) Image-Text Generation: This stage involves the use of an image encoder to extract continuous image features and text features from OpenOmni, facilitating alignment learning that enhances OpenOmnis image comprehension and instruction-following abilities. Additionally, this process achieves implicit omnimodal alignment, granting the model the capacity for omni-understanding. (3) Speech Generation: In the final stage, we train lightweight speech decoder using high-quality synthesized speech dialogue data, focusing on optimizing emotional speech preferences directly. This enables OpenOmni to generate real-time self-aware emotional speech. sis, employing direct preference optimization to generate controllable and expressive emotional speech. 2.3. Omni-modal Language Models Multimodal language models increasingly shift towards frameworks that unify diverse input and output modalities. By tokenizing different data types into shared representation, models like AnyGPT [51] and Unified-IO 2 [29] achieve seamless task adaptability across modalities such as audio, text, and images, without significant architectural changes. Mini-Omni2 [47] integrates visual and auditory encoders to deliver real-time multimodal responses and incorporates mechanisms for detecting and interpreting semantic interruptions. video-SALMONN [39] advances video understanding by incorporating fine-grained temporal modeling for interpreting speech within videos. Models like VITA [15] further enhance human-computer interaction through duplex communication schemes, allowing fluid and intuitive exchanges. Additionally, EMOVA [5] extends the expressive capabilities of these models by integrating controllable emotional speech synthesis, offering more expressive communication experience. Building on these developments, OpenOmni enables near zero-shot omnimodal alignment across language, vision, and speech, with self-aware emotional speech synthesis, enhancing multimodal models that are fast, dataefficient, and achieving better results. 3 3. Method In this section, we first present the formulation of omnimodal learning and provide an overview of the training procedure of OpenOmni, as shown in Fig. 2. Following this, we detail the specific training procedures of OpenOmni for omnimodal alignment and real-time speech generation. 3.1. Problem Formulation and Overview Omnimodal learning can be formulated as modeling the relationship between images, denoted as xV , speech as xS, and text as y. In this context, the image-to-text generation, which can be roughly summarized as generating description for input images encoded by Image Encoder EV , is to learn the conditional distribution pθ(yEV (xV )) parameterized by θ; the speech-to-text generation, which is to generate relevant text response given input speech encoded by Speech Encoder ES, is to learn pϕ(yES(xS))) parameterized by ϕ; the omnimodal-to-speech generation, which is to synthesize relevant speech response given input text, speech and image, is to learn pγ(xSDLLM (y, ES(xS), EV (xV )) parameterized by γ, where DLLM is the Large Language Model. , xS , yi)}K In the vanilla setting, omnimodal learning is typically trained using image-text-speech pairs Do = {(xV i=1 [15, 23]. However, high-quality imagetext-speech pairs are extremely scarce. To mitigate the dependency on native image-text-speech pairs, we introduce text as the pivot, which contains abundant image-text pairs , yi)}M Di2t = {(xV i=1[27, 32] and text-speech pairs Ds2t = , yi)}N {(xS i=1[35, 52], where K, K. Imitating the human learning mechanism that can naturally align visual concepts with speech across languages, OpenOmni aims to transfer visual concepts learned in the pivot language to the speech. OpenOmni divides the omnimodal alignment process into two consecutive stages: text-speech alignment and image-text alignment. For the text-speech alignment, OpenOmni aims to establish the cross-modal alignment for speech xS and language y. This is achieved by directly training speech LLM with sufficient text-speech resource Dt2s and objective pϕ(yES(xS))), denoted as fϕ, which can provide close hidden representations for text-speech pair and xS with similar semantics, i.e., fϕ(y) fϕ(xS). For the image-text alignment, OpenOmni utilize the sufficient image-text resource Di2t and optimize the imageto-text objective pθ(yEV (xV )). In the following sections, we introduce the training process of omnimodal alignment and speech generation. Its worth noting that OpenOmni is agnostic to the specific model architecture and training method, which enables us to flexibly utilize existing highly effective model architectures and training techniques in each task. 3.2. Speech-to-text Generation In speech-to-text generation, we incorporate speech encoder ES to extract audio features from input speech xS. These audio features ES(xS) are then replaced with corresponding text yS as input into the LLM. Following recent work to train speech conversation models [8, 14, 53], we pre-train OpenOmni on large scale of text-speech pairs using the language modeling objective: Ls2t(pϕ, Ds2t) = (cid:88) i=1 log pϕ(yiES(xS )). (1) 3.3. Image-to-text Generation In image-to-text generation, we incorporate an image encoder module EV to provide visual feature EV (xV ). These visual features are then concatenated with the text embedding as input into the speech LLM. Following recent work to train image-text conversation models [27, 32], OpenOmnis training process for image-to-text generation consists of two sub-stages: Image-Text Pretraining and Image-Text Instruction Tuning. Image-Text Pretraining In this sub-stage, we pre-train the visual module to align it with LLM on large scale of image-text pairs using the language modeling objective: Li2t(pθ, Di2t) = (cid:88) i=1 log pθ(yiEV (xV )). (2) Here, we fix the parameters of LLM to prevent short texts in the image-text pairs from influencing the general capabilities. Image-Text Instruction Tuning To enhance models capabilities in following human instructions, we conduct instruction tuning on elaborately curated multimodal instruction tuning datasets built by blending the existing imagetext instruction tuning datasets. We denote this image-text instruction tuning datasets as DI j=1, where yq is the instructions and ya is the response. Both the visual module and speech LLM are fine-tuned by maximizing the probability of the response: , yq,j, ya,j}L i2t = {xV LI i2t(pθ, DI i2t) = (cid:88) j=1 log pθ(ya,jEV (xV ), DLLM (yq,j)). in this scenario. (3) Interestingly, we find quasi-zero-shot transfer capability of OpenOmni If solely performing instruction tuning using the image-text resource, when given an image xV and question yq or an instruction speech xS the resultant model responds acq , curately, though mostly in language. This can be attributed to the close resemblance between the hidden representation of instructions between text and speech provided by the LLM, i.e., DLLM (yq) DLLM (ES(xS )). Consequently, we have pθ(yaEV (xV ), DLLM (yq)) pθ(yaEV (xV ), DLLM (ES(xS )). To improve the models instruction-following capability further, OpenOmni incorporates small number of image-text-speech pairs converted by TTSs models [12] during instruction tuning. Here, we complete omnimodal alignment to enable LLM to have image-text-speech understanding capabilities. Figure 3. Structure of speech decoder. The speech decoder consists of mixture of expert module and multiple transformer layers, achieving end-to-end speech unit learning through connectionist temporal classification (CTC) loss. 4 3.4. Speech Generation as follows: In speech generation, we incorporate Speech Decoder DS to generate speech based on the output of the LLM DLLM . We divide the OpenOmnis speech generation training process into two sub-stages: Speech Generation Pretraining and Emotional Speech Direct Preference Optimization (DPO). a,j}L o2s = {xV , yq,j, ya,j, xS Speech Generation To equip OpenOmni with the realtime speech generation ability for enhancing the interactive experience, we adopt streaming speech decoder for real-time speech generation. Moreover, we curate dataset, termed OpenOmni-300K, of 300K instructions from MMEvol [32] and UltraChat citepultrachat that included speech response for training the speech decoder. We denote this datasets as DI j=1, where yq is the instructions, ya is the response and xS . For the speech response xS , we first follow [14, 53] to discretize the speech into discrete units. Specifically, we use the pre-trained speech tokenizer [12] model to extract continuous representations of the speech and then convert these representations into discrete indices. Subsequently, consecutive identical indices are merged into single unit, resulting in the final discrete unit sequence = {xU xU a,i {0, 1, ..., 1}, where is the speech vocabulary size, and is the length of discrete unit sequence. The discrete units can be converted into waveform with an additional unit-based vocoder [12] trained on the English and Chinese datasets. i=1, xU a,i}L We add streaming speech decoder DS after the LLM to simultaneously generate speech and text responses. OpenOmni supports both autoregressive and For nonnon-autoregressive speech decoding mode. autoregressive speech decoding mode shown in Fig. 3, the speech decoder consists of mixture of expert (MOE) layer and tiny standard decoder-only language model. The MOE layer is designed to stabilize training and fast convergence. The speech decoder can hardly be trained successfully without this layer. Similar to [14, 34, 56], the speech decoder runs in non-autoregressive manner, which takes the output hidden states from the LLM as input, and generates the discrete unit sequence corresponding to the speech response in real-time. Specifically, the output hidden states corresponding to the text response ya are denoted as DLLM (xV , yq) = [h1, ..., hN ]. We first feed hidden state DLLM (xV , yq) into MOE layer to get hidden state sequence = [c1, ..., cN ]. Next, is upsampled and then fed into the speech decoder layers, and the output hidden state sequence is denoted as = [o1, ..., oM ]. We use connectionist temporal classification (CTC) [19] to align with the discrete unit sequence xU = {xU i=1. During training, CTC performs marginalization over all possible alignments a,i}L Lctc(pγ, DI o2s) = log pγ(xU O) = log (cid:88) pγ(AO) Aβ1(xU ) = log (cid:88) (cid:89) pγ(xU a,iO), Aβ1(xU ) i= (4) where β1(xU ) denotes all possible alignments of length that can be collapsed to xU . The alignment is modeled in non-autoregressive way. During inference, we select the best alignment = arg maxA (AO), and apply the collapsing function to obtain the discrete unit sequence β(A), which is then fed into the vocoder to synthesize waveform. Self-aware Emotional Speech Generation To enable the OpenOmni to generate self-aware, emotionally coherent, and vivid speech based on contextual history without additional control modules or conditions, we introduce the Direct Emotional Preference Optimization (DEPO) algorithm. This approach facilitates smooth and natural interactions in dialogue. Here, we briefly introduce the DPO method and refer readers to the original paper for more details. The key observation of DPO is that the reward function r(x, y) can be analytically expressed by its optimal policy model π(yx) and reference model πref(yx). Therefore, we can directly optimize the policy model using proper forms based on the preference data. Specifically, the reward model r(x, y) can be represented as: r(x, y) = β log π(yx) πref(yx) where β is constant and Z(x) is the partition function. Based on this observation, the policy model can be directly optimized on the human feedback preference pair (yw, yl): + β log Z(x), (5) Ldpo = E(x,yw,yl) = E(x,yw,yl) (cid:2)log σ(r(x, yw) r(x, yl))(cid:3) (cid:2)log σ(β log π(ywx) πref(ywx) β log π(ylx) πref(ylx) )(cid:3), (6) here, the reference model πref(yx) is the model trained in the Real-time Speech Generation sub-stage and is kept fixed during DPO training. Only the policy model π(yx) is updated. We note that DPO is more simple, efficient, and stable in aligning OpenOmni self-aware emotional speech generation compared with traditional RLHF approaches. Based on the Plutchik Model of Emotions [1] research, we construct multi-turn dialogue preference dataset for In self-awareness, encompassing nine distinct emotions. this dataset, each pair of preference samples consists of an yw = xU a,w representing emotionally congruent speech response unit sequence that aligns with the historical context of the dialogue, and yl = xU a,l representing emotionally neutral speech response unit sequence that is inconsistent with the contextual emotions. 5 Specifically, the π(yx) in connectionist temporal classification training process can be represented as log π(yx) = log (cid:88) (cid:89) Aβ1(xU ) i=1 pγ(xU a,iO), (7) After training on Self-aware Emotional Speech Generation, OpenOmni can generate real-time, emotionally rich, multiturn end-to-end dialogues. 4. Experiments 4.1. Implementation Details Omnimodal Alignment Data During the speech-to-text generation phase, in addition to WeNetSpeech [52], LibriSpeech [35], and AIShell-4 [16], we use portions of shorter responses from O2S-300K, totaling 1600 hours of data, for bilingual speech-text alignment training. Next, we train OpenOmni on the LLaVA-Pretrain-595K [27] for image-text alignment. In the image-text instruction tuning stage, we efficiently fine-tune OpenOmni on compact high-quality dataset MMEvol [32]. More details can be found in Appendix 6. Hyperparameter II III IV batch size lr warmup ratio epoch freeze LLM optimizer cost dataset loss 128 1e3 0.3 1 AdamW 128 256 5e5 1e3 0.3 0.3 1 1 AdamW AdamW AdamW AdamW 40 GPUH 80 GPUH 500 GPUH 36 GPUH 8 GPUH 2-2 LI 32 5e4 0.3 3 32 5e4 0.3 3 3-2 Ldpo 1-1 Ls2t 3-1 Lctc 2-1 Li2t i2t Table 1. The detailed training setup for OpenOmni and the hyperparameters across the training stage. All experiments are conducted in 8xA100 setting. Please refer to Appendix Sec. 6 for more details about dataset. Speech Generation Data For real-time speech generation, we curate dataset of 300K instructions from MMEvol [32] and UltraChat [10] that included long responses for training the speech decoder. Specifically, we decompose multi-turn dialogues into single-turn questionanswer pairs, rank the responses based on their length, and select 100K question-answer pairs with relatively long responses. To support bilingual output in Chinese and English, we translate 50K question-answer pairs into their corresponding Chinese versions using GPT-4o-mini API, and then convert the answers into the corresponding speech using CosyVoice [12]. We employ the same method for textconditioned speech synthesis on 200k randomly selected data from UltraChat. As result, we obtain 8000 hours of high-quality bilingual speech generation data O2S-300K. 6 Self-aware Emotional Speech Generation Data Based on the Plutchik Model [1] of eight distinct emotions research, we curate multi-turn speech preference dataset EO2S-9K for self-awareness emotion evaluation. Specifically, we randomly selected 200K samples from MMEvol and employed the Qwen2-72B [3] model to categorize the responses into nine predefined emotions for each round. Subsequently, we randomly selected 1K bilingual dialogues labeled with emotion category, while additionally reserving 100 samples as an emotional test set for self-aware evaluation of emotion speech generation. Due to the scarcity of several emotions, such as anger and sadness in the MMEvol dataset, we used GPT-4o-mini API to generate and augment the corresponding emotion data volume. We ensure that both the training and testing datasets contain an equal representation of Chinese and English data. Furthermore, we employed CosyVoice to use unconditionally generated speech as negative samples and emotion-conditioned generated speech as positive samples, thereby constructing preference sample pairs for training direct preference optimization in emotional speech generation. More details on training data recipes and prompts can be found in the Appendix 9. Model We design the architecture following LLaVA Series [26, 27, 32], where the omnimodal large language model consists of four key components: an LLM (Qwen2.57B-Instruct [3]) for next token prediction, an image encoder (CLIP-ViT-L [36]) for extracting visual features, speech encoder (Whisper-large-v3 [37]) for extracting audio features and streaming speech decoder (Qwen2.50.5B-Instruct [3]) for generating vivid speech in real-time. Moreover, an image-text projector and speech-text projector are adopted to align the visual-text modalities and the speech-text modalities, respectively. mixture of expert module with four experts is designed to align the omnimodal embedding and speech decoder efficiently and stably. For the autoregressive mode, we use the Speech Tokenizer from GLM4-Voice [50] with vocabulary size of 16K, which results in better speech quality. For nonautoregressive models, we use the CosVoice [12] Speech Tokenizer with smaller vocabulary size of 6K, facilitating faster convergence during CTC training. Training Strategy We conduct OpenOmni training following five sub-stage settings: speech-to-text generation, image-text pretraining, image-text instruction tuning, realtime speech generation, and self-aware emotional speech generation. See details in Tab. 1. 4.2. Omni Language Evaluation OmniBench[24] is pioneering benchmark designed to evaluate omnimodal large language models (OLLMs) by integrating and interpreting simultaneous inputs of images, Method Identification Contextual & Identity & Text & Count & Action & Activity Description Inference & Description Environmental Relationship Symbols Quantity Story Plot AnyGPT (7B) [51] Video-SALMONN (13B) [39] UnifiedIO2-Large (1.1B) [29] UnifiedIO2-XLarge (3.2B) [29] UnifiedIO2-XXLarge (6.8B) [29] VITA (7x8B) [15] OpenOmni (7B) 5.98 28.69 28.29 30.28 27.49 33.47 36.65 8.70 25.65 22.17 26.52 23.04 34.35 45. 7.59 24.47 32.49 30.38 28.69 27.00 32.91 4.74 23.22 30.81 31.75 25.59 36.02 44.08 5.67 29.08 28.37 28.37 26.95 43.97 48. 12.50 21.83 21.83 18.75 12.50 31.25 34.38 8.00 52.00 16.00 28.00 12.00 24.00 24.00 20.00 26.63 13.33 26.63 46.67 6.67 33. Overall 7.01 26.53 27.76 29.16 25.92 33.45 37.40 Table 2. Overall omni-understanding results on OmniBench. We conducte performance comparison of omni-understanding among various fully open-source Omnimodal Large Language Models (OLLMs) on OmniBench. Due to concurrent work, EMOVA [5], not being open-sourced, we are unable to present its results. Notably, compared to the state-of-the-art OLLM, VITA [15], which was trained on tri-modal data, OpenOmni achieves comparable advanced performance using significantly less training data and smaller model size. Model w/ Audio IO PT IT MMStar MMB MMBCN HallBench MathVistaM MMMUV AI2D RWQA Proprietary Models GPT-4o GPT-4o-mini Weight Open-Source MiniCPM-V2.5 (8B) [48] Qwen2-VL-Chat (7B) [4] Baichuan-Omni (7B) [23] EMOVA (8B) [5] Fully Open-Source Cambrain-I (8B) [42] MMEvol (7B) [32] VITA (8x7B) [15] OpenOmni (7B) 570M 9.1M 1.4B - 8M 7.4M 4.4M 2.5M 7M 0.6M 1.5M 5M 0.6M 1.8M - - 51.3 60.7 - - 50.7 51.6 - 51. 83.4 - 76.7 86.4 76.2 82.8 - 74.6 74.7 76.2 82.1 - 73.3 81.9 74.9 - - 74.3 71.4 76. 55.0 46.1 42.5 50.6 47.8 - 34.3 42.9 39.7 44.2 63.8 52.4 54.3 58.2 51.9 61.1 47.0 52.4 44.9 52. 69.1 60.0 45.8 52.7 47.3 - 41.8 45.1 45.3 44.8 - - - - - 82.8 73.1 74.7 74.3 74. 75.4 67.1 63.5 69.7 62.6 64.3 64.2 63.9 59.0 64.0 Table 3. Comparison with state-of-the-art methods on visual-language benchmarks, with the indication of audio input/output support. We mark the best performance bold among fully open-source models. The results indicate that incorporating audio input and output can further enhance the models visual-language capabilities. Model AIShell-2(ZH-CER) Librispeech(EN-WER) Dev Test Test clean Test other S2T T2S S2T T2S S2T T2S S2T T2S Speech LLM SpeechT5 [2] SALMONN [40] Mini-Omni [47] Freeze-Omni [44] Qwen2-Audio [8] Omnimodal LLM AnyGPT [51] VITA [15] EMOVA [5] OpenOmni - - - - 3.1 - - 10.3 6.8 - - - - - - - 7.9 7. - - - - 3.3 - - - 6.9 - - - - - - - - 13.1 2.4 2.1 4.7 3.2 2.0 8.5 8.1 4.0 3. - - - - - - - 3.4 2.6 5.8 4.9 9.4 7.7 4.5 - 18.4 - 4.1 - - - - - - - - 5. Table 4. Comparison with state-of-the-art methods on speechlanguage benchmarks. We mark the best performance bold. in recognizing entities, inferring causal relationships, and comprehending abstract concepts. We compare OpenOmni with other OLLMs on OmniBench, and the results are summarized in Tab. 2. Notably, our model exhibits excellent zero-shot omnimodal alignment performance after only two phases: speech-text alignment and image-text alignment. Compared to the fully open-source state-of-the-art OLLM, VITA [15], our model utilizes fewer training parameters (7B vs. 8x7B) and less image-text training data (1.8M vs. 5M), yet achieves superior overall results on OmniBench (37.40 vs. 33.45). Moreover, our approach pivots on language, completing omnimodal alignment through implicit alignment, thereby demonstrating enhanced scalability in scenarios with limited tri-modal data. 4.3. Vision Language Evaluation audio, and text, thus necessitating comprehensive understanding of cross-modality information. The evaluation framework includes 1,142 question-answer pairs across refined taxonomy of tasks focusing on cognitive and reasoning abilities, which poses significant challenges for models To comprehensively validate our approachs ability to align the visual-text modalities, we perform thorough comparison with previous VLLMs across 8 representative visionlanguage benchmarks, including MMBench-EN [28], MMBench-CN [28], MMStar [6], RealWorldQA [46], Model Lang Angry & Disgusted Fearful Happy Neutral Other OpenOmni w/ DPO OpenOmni w/ DPO ZH ZH EN EN 89.7 96. 89.2 91.3 54.8 78.4 68.7 70.4 33.3 37.7 57.5 60.6 92.3 97. 91.9 94.6 51.6 62.8 48.0 49.6 Sad 60.2 90.7 75.6 77. Surprised Overall 23.7 29.8 7.5 13.9 57.9 70.4 62.6 65. Table 5. Overall self-aware emotional speech generation results on the test set of bilingual EO2S-9K. Through the emotional speech direct preference optimization algorithm, OpenOmni has consistently improved emotional speech generation capabilities for both Chinese and English. The accuracy of emotional speech generation has improved by an average of 7.6% (from 60.3% to 67.9%), with particularly notable improvements observed in the generation of emotional speech categories such as Fearful, Surprised, and Sad. MMMU [49], MathVista [30], AI2D [22] and HallusionBench [20]. To ensure the reproducibility of our evaluation results and to guarantee fair and consistent comparisons across all models and benchmarks, we uniformly employ VLMEvalKit [13] for all assessments conducted in zero-shot manner. As shown in Tab. 3, compared with the open-source state-of-the-art OLLM, VITA [15], OpenOmni achieves superior results despite significant difference in the volume of training data. Notably, our model demonstrates overall improvements, with gains of 7.0% on MMBench-Chinese and 11.3% on HallusionBench. Furthermore, when compared to other fully opensource visual language models, our approach exhibits competitive performance even with reduced amount of training data, thereby demonstrating the effectiveness of our imagetext alignment methodology. 4.4. Speech Language Evaluation To validate the speech understanding and generation capacity of OpenOmni, we present the word error rate (WER) results on Librispeech [35] and the character error rate (CER) results on AIshell-2 [16] and benchmarks with two tasks: speech-to-text recognition (S2T) and text to speech recognition (T2S). We utilize Whisper-large-V3 to recognize the english synthesized speech of OpenOmni and calculate the WER with ground truth text label for T2S evaluation. For Chinese synthesized speech, since OpenOmni has better ASR capabilities compared to Whisper, we use the model for self-recognition. As shown in Tab. 4, OpenOmni achieves best results on both T2S and S2T tasks using Chinese and English bilingual data, outperforming other omnimodal models. This indicates that our model not only comprehensively understands the content of speech but also generates fluent audio while being able to align speechlanguage in Chinese and English. Moreover, compared to VITA [15], which relies on additional text-to-speech (TTS) models, and EMOVA [5], which only utilizes an autoregressive (AR) structure, our model demonstrates significantly faster speech generation speed via two mode support. Thanks to its end-to-end lightweight non-autoregressive (NAR) decoding mode support, our model can generate up to 30 seconds of speech with less than one second of latency, achieving real-time speech generation, which is over five times faster than autoregressive mode. 4.5. Emotion Speech Synthesis Evaluation To evaluate the effectiveness of direct preference learning in emotional speech generation, we assess self-aware emotional speech generation using the EO2S-9K test set. Specifically, we employ Emotion2Vec [33] to classify the emotions of the generated speech and calculate the accuracy against the ground-truth labels. The results presented in Tab. 5 demonstrate that direct preference optimization for emotional speech can effectively and efficiently inject emotional speech generation capabilities into the OLLM, thereby significantly enhancing the models performance in bilingual, multi-turn emotional speech generation tasks. 5. Conclusion In this paper, we propose novel end-to-end omnimodal model, OpenOmni, which ingeniously employs language as pivot to achieve tri-modal zero-shot omnimodal alignment in scenarios characterized by scarcity of tri-modal data. Furthermore, by utilizing lightweight streaming speech decoder that directly optimizes emotional speech preferences, our model is capable of facilitating low-latency, selfaware, high-quality emotional speech interactions. Consequently, OpenOmni demonstrates competitive performance on visual and speech-language tasks and attains state-ofthe-art results on tri-modal benchmarking. Comprehensive comparative experiments and ablation studies validate the effectiveness and efficiency of our approach. In the future, we plan to explore enhancing the expressiveness of generated multi-character speech responses to improve the interactive experience of omnimodal assistants. Limitations Although OpenOmni has achieved impressive results under low-resource conditions, the potential of utilizing larger volume of high-quality tri-modal data to further enhance the capabilities of the omnimodal model remains unexplored. Furthermore, while the mixture of experts module can effectively alleviate conflicts during CTC training, it is still more challenging to train compared to autoregressive generation methods. Finding an effective tradeoff remains significant challenge."
        },
        {
            "title": "References",
            "content": "[1] 6seconds.org. Plutchik-model-emotion. https://www. 6seconds.org/2022/03/13/plutchikwheelemotions, 2022. 5, 6 [2] Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Speecht5: Unified-modal encoder-decoder preet al. arXiv preprint training for spoken language processing. arXiv:2110.07205, 2021. 7 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1, 6 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 7 [5] Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, et al. Emova: Empowering language models to see, hear and speak with vivid emotions. arXiv preprint arXiv:2409.18042, 2024. 1, 3, 7, 8 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [7] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 12 [8] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. 1, 4, 7 [9] Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for realtime dialogue. 2 [10] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. 2, 6 [11] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. 2 [12] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot textto-speech synthesizer based on supervised semantic tokens. corr, abs/2407.05407, 2024. doi: 10.48550. arXiv preprint ARXIV.2407.05407. 1, 4, 5, 6 [13] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 8 [14] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Llama-omni: SeamShaolei Zhang, and Yang Feng. less speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. 1, 2, 4, 5 [15] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. 1, 2, 3, 7, 8, 12 [16] Yihui Fu, Luyao Cheng, Shubo Lv, Yukai Jv, Yuxiang Kong, Zhuo Chen, Yanxin Hu, Lei Xie, Jian Wu, Hui Bu, et al. Aishell-4: An open source dataset for speech enhancement, separation, recognition and speaker diarization in conference scenario. arXiv preprint arXiv:2104.03603, 2021. 6, 8 [17] Xiaoxue Gao, Chen Zhang, Yiming Chen, Huayun Zhang, Emo-dpo: Controllable emotional and Nancy Chen. speech synthesis through direct preference optimization. arXiv preprint arXiv:2409.10157, 2024. 2 [18] Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, et al. Av-odyssey bench: Can your multimodal llms really understand audio-visual information? arXiv preprint arXiv:2412.02611, 2024. [19] Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pages 369376, 2006. 5 [20] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large visionlanguage models, 2023. 8 [21] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26584 26595, 2024. 12 [22] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 8 [23] Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, et al. Baichuan-omni technical report. arXiv preprint arXiv:2410.08565, 2024. 3, 7 [24] Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, et al. Omnibench: Towards the future of universal omnilanguage models. arXiv preprint arXiv:2409.15272, 2024. 2, 6 [25] Ting-En Lin, Yuchuan Wu, Fei Huang, Luo Si, Jian Sun, and Yongbin Li. Duplex conversation: Towards human-like In Proceedings of interaction in spoken dialogue systems. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 32993308, 2022. 2 [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 2, 6, 12 [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024. 1, 2, 3, 4, 6, 12 [28] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 7 [29] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. 3, 7 [30] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. arXiv e-prints, pages arXiv2310, 2023. 8 [31] Run Luo, Yunshui Li, Longze Chen, Wanwei He, TingEn Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, et al. Deem: Diffusion models serve as the eyes of large language models for image perception. arXiv preprint arXiv:2405.15232, 2024. 1, [32] Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, et al. Mmevol: Empowering multimodal large language models with evol-instruct. arXiv preprint arXiv:2409.05840, 2024. 2, 3, 4, 5, 6, 7, 12 [33] Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, and Xie Chen. emotion2vec: Selfsupervised pre-training for speech emotion representation. arXiv preprint arXiv:2312.15185, 2023. 8 [34] Zhengrui Ma, Qingkai Fang, Shaolei Zhang, Shoutao Guo, Yang Feng, and Min Zhang. non-autoregressive generation framework for end-to-end simultaneous speech-to-any translation. arXiv preprint arXiv:2406.06937, 2024. 5 [35] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE, 2015. 3, 6, 8, 12 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 6 [37] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. [38] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. 12 [39] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. 1, 3, 7 [40] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. 7 [41] Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024. 1 [42] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 7 [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [44] Xiong Wang, Yangze Li, Chaoyou Fu, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm. arXiv preprint arXiv:2411.00774, 2024. 2, 7 [45] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. 12 [46] x.ai. Grok-1.5 vision preview. https://x.ai/blog/ grok-1.5v, 2024. 7 [47] Zhifei Xie and Changqiao Wu. Mini-omni2: Towards opensource gpt-4o model with vision, speech and duplex. arXiv preprint arXiv:2410.11190, 2024. 3, 7 [48] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [49] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 8 10 [50] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. 6 [51] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, YuGang Jiang, and Xipeng Qiu. AnyGPT: Unified multimodal In Proceedings of LLM with discrete sequence modeling. the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9637 9662, Bangkok, Thailand, 2024. Association for Computational Linguistics. 1, 3, 7, 12 [52] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP 20222022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61826186. IEEE, 2022. 3, 6, 12 [53] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1575715773, 2023. 1, 2, 4, 5 [54] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 12 [55] Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, and Chaohong Tan. Omniflatten: An end-to-end gpt model for seamless voice conversation. arXiv preprint arXiv:2410.17799, 2024. 2 [56] Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min Zhang, and Yang Feng. Streamspeech: Simultaneous speech-to-speech translation with multi-task learning. arXiv preprint arXiv:2406.03049, 2024. [57] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 11 Layers Experts Wenetspeech(ZH) Librispeech(EN) Test Net Test Meeting Test clean Test other 2 2 2 4 6 1 2 4 4 4 113.6 16.7 8.5 7.3 6. 129.7 22.3 8.4 7.9 6.7 87.8 10.7 4.2 3.8 4.1 96.5 14.6 4.7 4.3 4.5 Table 6. Ablation study on the number of layers and experts in the speech decoder. Increasing experts in the mixture of experts module stabilizes CTC loss during training and enhances speech generation capacity. Deeper transformer layers improve English and Chinese speech generation, with greater benefits for Chinese. Method Timbre Tone Melody Space Time Hall Intricacy Overall OneLLM (7B) [21] PandaGPT (7B) [38] Video-LLaMA (7B) [54] Video-LLaMA2(7B) [7] AnyGPT (7B) [51] NexTGPT (7B) [45] VITA (7x8B) [15] OpenOmni (7B) 25.0 23.5 25.5 24.1 24.6 23.3 24.1 23.9 25.5 23.2 22.3 25.5 25.0 20.9 26.4 27.7 21.5 27.6 24.4 26.4 26.4 27.8 27.8 25. 37.5 45.0 30.0 30.0 27.5 30.0 22.5 60.0 29.3 23.8 26.2 27.2 29.2 28.8 26.3 25.2 25.5 28.0 25.0 33.0 29.0 28.5 31.0 29. 38.4 23.9 30.7 34.5 25.7 23.6 36.8 37.6 27.4 26.7 26.1 26.8 26.1 25.5 26.4 32.8 Table 7. Overall omni-understanding results on AV-Odyssey Bench. We conducte performance comparison of omniunderstanding among various fully open-source Omnimodal Large Language Models (OLLMs) on AV-Odyssey Bench. Compared to the state-of-the-art OLLM, VITA [15], which was trained on tri-modal data, OpenOmni achieves comparable advanced performance using significantly less training data and smaller model size. 6. Data Construction We provide details of the data construction for multiple training stages below: OpenOmni-1-1: In addition to datasets WeNetSpeech, LibriSpeech, and AIShell-4, we randomly select 80k image-text instruction data with shorter responses from MMEvol [32]. We translate 40k of this data into Chinese using Qwen72B and synthesize the responses into speech data with CosVoice. This results in 1600 hours of OpenOmni-1-1 data for speech-text alignment pretraining. OpenOmni-2-1: For rapid image-text alignment pretraining, we use the llava pretrain dataset, following previous work [26, 27, 32]. OpenOmni-2-2: To achieve efficient image-text instruction tuning, we employ MMEvol data. Since we later train the speech decoder by freezing the LLM mode, we include O2S-300K to stabilize the training of the speech decoder, resulting in combined dataset of 1.8M for OpenOmni-2-2. OpenOmni-3-1: To better utilize computational resources, we select 300k data with long response instructions from MMEvol and UltraChat. This includes 100k image-text instruction data, 100k single-round dialogue, and 100k multi-round dialogue. We synthesize the corresponding speech using CosVoice, resulting in 8000 hours of O2S-300K. OpenOmni-3-2: We curate 9k emotion preference data and generate emotional speech preference pairs using CosVoices conditional control. This is used for Direct Emotion Preference Optimization (DEPO). 7. Additional Experiments In addition to OmniBench, we conduct experiments on the AV-Odyssey Bench [18], which involves the four modalities: audio, text, image, and video. For video, we test by averaging 8 sampled frames into single image. The experimental results are shown in the Tab. 7 below. Compared to other OLLMs, OpenOmni achieves the best average performance using only bi-modal speech-text and image-text data. With 7B model parameters and no audio or video training, it outperforms VITA by 4.4 points, demonstrating the effectiveness and efficiency of OpenOmni. 8. Additional Ablation Study To explore the impact of the number of layers in the NAR speech decoder and the mixture of experts module on Chinese and English speech generation, we conduct ablation experiments on WeNetSpeech [52] and LibriSpeech [35]. As illustrated in Tab. 6, the instability and fragility associated with training using the CTC loss function present significant challenges. When simply employing single feed-forward network (i.e., experts = 1), it becomes increasingly difficult to reconcile the conflicting training dynamics inherent in mixed-language scenarios, particularly when dealing with varying response lengths. As result, training the speech decoder under these conditions proves to be quite challenging. Our findings demonstrate that incrementally increasing the number of experts significantly enhances the models performance in bilingual speech generation, thereby underscoring the effectiveness of our mixture of experts module design. However, we observe inconsistent preferences regarding the optimal number of layers in the speech decoder for generating speech in Chinese and English. Specifically, while four layers yield the best results for English generation, six layers are more suitable for generating Chinese speech. 9. Additional Implementation Details As shown in Fig. 4, we provide more details of the speech decoder design and training here. For speech decoder, OpenOmni supports both autoregressive (AR) and nonautoregressive (NAR) methods. Specifically, the AR mode has better generation quality but slower generation speed, 12 Figure 4. Overview of Speech Decoder Mode. OpenOmni supports both autoregressive (AR) and non-autoregressive speech (NAR) generation. The NAR mode uses CTC loss modeling and 6K speech vocabulary size to enable real-time parallel speech decoding generation. The AR mode uses NTP loss modeling and speech vocabulary size of 16K to support streaming decoding and higher-quality speech generation. In order to make the training of speech generator more stable and easy, we design text-guided output feature fusion method to ensure the correctness of semantic alignment in speech generation modeling. while the NAR mode can achieve real-time speech generation but the generation quality is slightly worse. At the same time, in order to train the speech generator more efficiently, we also design text-guided feature fusion module, so that the conditional features used for speech generation have more accurate alignment semantics, which can improve the generation quality and training efficiency of the speech decoder. Non-autoregressive mode: In the non-autoregressive mode, the conditional features generated by OLLM are fed into the speech decoder by layer of MOE and then upsampled to obtain the predicted speech output, and finally the end-to-end optimization is carried out by CTC loss modeling of the speech output. Due to the instability of CTC loss training, the smaller the size of the speech vocabulary, the easier it is to be successfully trained, but the generation quality of the corresponding speech will be affected by the smaller vocabulary. Autoregressive mode: The autoregressive mode projects the conditional features generated by OLLM into the speech space through layer of linear layer and feeds them into the speech decoder to obtain the speech prediction output, and finally optimizes the speech output end-to-end by modeling the NTP loss. Due to the stability of NTP loss training, the quality of speech generation will be higher than that of 13 non-autoregressive generation, but the speed of speech generation will be reduced by autoregressive decoding. Both AR and NAR modes depend on the quality of the speech generation conditional features generated by OLLM. Although OpenOmni will let the OLLM fit the text answer corresponding to the speech through multiple rounds of training in advance, there will still be OLLM output decoded into the wrong text answer. In this case, the erroneous generated condition features will be incorrectly aligned with the speech during the training process, which will ultimately reduce the performance of the speech decoder. In order to ensure the efficiency of training, OpenOmni fuse the speech generation condition features output by OLLM with the corresponding text features with correct semantics, and then feed them into the speech decoder for speech generation modeling training. Through the feature fusion module of text prior, OpenOmni avoid the misalignment of speech and corresponding text, and ultimately make the speech decoder training more stable, and at the same time have more efficient and accurate speech generation quality. English Prompts used in Text-to-Speech (T2S) and Speech-to-Text (S2T) tasks. S2T (ASR) Please translate the users spoken input into corresponding text, word by word. T2S Echo game: Repeat this sentence back to me exactly as said it! Mirror challenge: Reflect my words right back at me! Parrot mode: Can you mimic what just said? Copycat test: Lets see how well you can replicate my sentence! Playback time: Rewind and repeat my last sentence. Echo chamber: Say back to me what just mentioned. Simulate my speech: Can you say exactly what just did? Mimicry game: Echo my words precisely! Repeat relay: Send my sentence back to me unchanged. Reflection exercise: Reiterate my last statement word for word. English emotional preference data case. Emotion: Neutral user: Please engage in conversation with me. assistant: Of course, please go ahead with your question. [A calm and collected woman, speaking in steady and neutral tone.] user: was wondering how you manage to stay so composed all the time. assistant: Its something that comes with practice and mindfulness. focus on observing my surroundings and maintaining balanced perspective. [A composed woman, speaking in measured and tranquil tone.] user: That sounds quite challenging. Do you ever feel overwhelmed? assistant: There are moments when things can be overwhelming, but remind myself to breathe and take one step at time. [A calm woman, delivering her words in an even and serene manner.] user: Do you have any techniques to stay calm under pressure? assistant: Yes, practice meditation and deep breathing exercises. It helps to center my thoughts and maintain clarity. [A composed woman, speaking with steady and soothing tone.] user: see. Do you think anyone can learn to be as calm as you? assistant: Absolutely. It requires patience and consistent practice, but anyone can develop the ability to remain calm and composed. [A neutral-toned woman, expressing herself in clear and composed manner.] user: What advice would you give to someone starting on this path? assistant: Begin with small steps. Start by observing your thoughts and emotions without judgment. Practice mindfulness and breathing exercises daily. [A composed woman, offering her advice in gentle and calm tone.] user: Thank you for your advice. It has been very insightful. assistant: Youre welcome. Im glad could help. Remember, the journey to calmness is gradual, but rewarding. [A tranquil woman, speaking in reassuring and composed tone.]"
        }
    ],
    "affiliations": [
        "New South Wales",
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "Tongyi Laboratory",
        "UIUC",
        "University of Chinese Academy of Sciences"
    ]
}