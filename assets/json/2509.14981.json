{
    "paper_title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation",
    "authors": [
        "Chuan Fang",
        "Heng Li",
        "Yixun Liang",
        "Jia Zheng",
        "Yongsen Mao",
        "Yuan Liu",
        "Rui Tang",
        "Zihan Zhou",
        "Ping Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation."
        },
        {
            "title": "Start",
            "content": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation Chuan Fang1, Heng Li1, Yixun Liang1, Jia Zheng2, Yongsen Mao2, Yuan Liu1, Rui Tang2, Zihan Zhou2, Ping Tan1 1Hong Kong University of Science and Technology, 2Manycore Tech Inc. https://manycore-research.github.io/SpatialGen 5 2 0 2 9 1 ] . [ 2 1 8 9 4 1 . 9 0 5 2 : r Figure 1. Given 3D semantic layout, SPATIALGEN can generate 3D indoor scene conditioned on either textual description (left) or reference image (middle). Furthermore, it can transform real-world scene, where its 3D layout is estimated from video by layout estimator [27], into some brand new scenes."
        },
        {
            "title": "Abstract",
            "content": "Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains timeconsuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. major bottleneck is the lack of large-scale, high-quality *Work done during an internship at Manycore Tech Inc. dataset tailored to this task. To address this gap, we introduce comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given 3D layout and reference image (derived from text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates supe1 rior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation. 1. Introduction Indoor scene generation aims to produce spatially coherent and photorealistic 3D indoor environments. As fundamental challenge in computer vision, this task underpins diverse applications, including immersive films and games, interior design, and augmented/virtual reality (AR/VR). Moreover, it also provides diverse and physically realistic environments in robotic simulation for training and evaluating robot navigation and interaction capabilities. major consideration in developing 3D scene generation methods is the trade-off between realism and scene diversity. Procedural modeling methods [9, 33, 54] leverage hand-crafted heuristic rules and geometric constraints in the graphics engines, which produce highly realistic and physically plausible indoor environments. However, these scenes lack diversity. Recent 3D generative methods automatically generate scene layouts [28, 45] or other 3D representations like NeRFs [1] and 3D Gaussians [21]. But these methods exhibit limited layout and appearance realism, primarily due to the scarcity of annotated 3D data. In comparison, image-based methods utilize diffusion models to generate panoramas [38, 46] or multi-view images [11, 44] followed by 3D reconstruction. By leveraging powerful 2D priors, these methods show promise in striking better balance between realism and scene diversity. Image-based methods, however, face additional challenges in multi-view semantic consistency. While recent video generation methods [26, 55] have improved temporal coherence, synthesizing semantically consistent content when exploring beyond input views remain highly challenging. To this end, 3D semantic layout prior (Figure 1) has been employed in the literature to guide the generation process. But due to the lack of large-scale dataset with paired 3D layout and images (or videos), existing layoutconditioned methods resort to one of the following two strategies: score distillation [4, 6, 51, 66] and panoramaas-proxy [8, 38]. The former directly distills powerful 2D pre-trained models for 3D content creation, avoiding the need for large-scale training data. But due to the inherent limitation of the SDS method [30], results produced by these methods suffer from severe visual artifacts (e.g., over-saturation, the latter makes use of special type of data, namely panoramas, for which large datasets with diverse scenes and annotations are available (e.g., Structured3D dataset [62]). However, since panorama images are captured at fixed camera locations, models trained on such data have limited ability to lack of details). In contrast, extrapolate to novel viewpoints, restricting their application in real-world tasks. To overcome these limitations, we collect new indoor scene dataset on much larger scale. Our dataset features 4.7M panoramic images with precise 2D and 3D layout annotations, spanning 57,440 rooms and 12,328 scenes. With this dataset, we take new approach to 3D scene synthesis by building scalable multi-view diffusion (MVD) model conditioned on 3D layout priors, which achieves high semantic consistency while maintaining the realism and scene diversity in the results. We introduce SPATIALGEN, novel framework for highfidelity 3D indoor scene generation from 3D room layout. First, we convert the 3D semantic layout into view-specific representations comprising coarse semantic maps and scene coordinate maps [39]. Second, we design layout-guided attention mechanism that alternatively operates through: (i) cross-view attention for consistent information propagation across different viewpoints; (ii) cross-modal attention for fine-grained feature alignment between appearance, semantic, and geometric representations. This mechanism enables the joint synthesis of photorealistic RGB images, precise object semantic maps, and accurate scene coordinates for both input and novel viewpoints. Finally, we employ an iterative multi-view generation strategy to ensure complete scene coverage, followed by 3D Gaussian splatting optimization that reconstructs an explicit radiance field to enable free-viewpoint rendering. Our main contributions are summarized as follows: We introduce new large-scale dataset featuring over 4.7M panoramic images of 57,440 rooms and precise 2D and 3D layout annotations. This dataset fills critical gap in 3D scene modeling by providing comprehensive multiview data with structural annotations. We present SPATIALGEN, new framework for layoutguided indoor scene generation. At the core of this framework is novel multi-view multi-modal image diffusion method conditioned on given layout prior, which generates semantically and geometrically consistent images from arbitrary viewpoints. Extensive evaluations conducted on text or image to 3D scene generations demonstrate that our method generates substantially more realistic and plausible 3D scenes. 2. Related Work Procedural & 3D-based Scene Generation. Procedural generation (PCG) methods [32, 33] create 3D scenes with hand-crafted rules or constraints. Recent approaches integrate large language models (LLMs), either to generate scene layouts for subsequent object retrieval or shape synthesis [9, 43, 52], or to act as agents that produce Python scripts controlling procedural frameworks [17, 42]. 3D-based methods generate 3D scene representations us2 Table 1. Statistics of the datasets for indoor scene generation. : object annotations are provided by Ctrl-Room [8]. Dataset (year) source #scenes #images #objects image type annotations layouts objects SUN R-GBD (2015) ScanNet (2017) Matterport3D (2017) ScanNet++ v2 (2024) Structured3D (2020) Hypersim (2021) SPATIALGEN dataset (ours) real real real real syn. syn. syn. - 1,513 90 1,006 3,500 461 12,328 10.3K 2.5M 10.8K 11.1M 59K 36K 41K 111K perspective image regular video sparse panoramas regular video 196.5K 150K 58K 77.4K 1M 4.7M panorama image regular video panoramic video ing generative models trained on datasets with 3D annotations. ATISS [28] and DiffuScene [45] predict compact layout parameters for scene objects. DiffInDScene [19], PDD [23], and SceneFactor [2] introduce semantic layout as an intermediate guide to generate the indoor scene with an explicit geometric representation. But the lack of annotated 3D scene datasets results in subpar performance and limited generalization of such methods. Image-based Scene Generation. In contrast, image-based methods exploit strong 2D priors in pretrained diffusion models to obtain photorealistic and diverse results. MVDiffusion [46] and PanoFusion [57] finetune latent diffusion model [35] to generate 360-degree panorama of scene. Text2Room [16] and LucidDreamer [5] start with an initial RGB image and iteratively build the 3D scene by progressively warping and inpainting. CAT3D [11] and Bolt3D [44] trained multi-view LDM to generate novel views from input images, followed by 3D reconstruction. Despite these advances, existing methods struggle to synthesize large viewpoint changes [49, 55] and semantically coherent scenes [11, 37, 44] beyond observed areas. The line of work that most closely relates to ours employs 3D layout prior to guide the generation process. Setthe-Scene [6], SceneCraft [51], and Layout2Scene [4] generate 3D scenes by distilling the pretrained image diffusion models conditioned on given semantic layout. These methods achieve better view and semantic consistency, but the realism and controllability remain limited. While CtrlRoom [8] and ControlRoom3D [38] generate panoramas with high visual fidelity, they struggle to extrapolate the scene beyond fixed camera location without resorting to dedicated room completion procedure. We argue that these limitations stem from the scarce scale and diversity of available 3D scene datasets, hindering the learning of robust 3D priors. Indoor Scene Dataset. Existing indoor datasets are either captured from real-world scenes using RGB [22, 65] or RGB-D [3, 7, 40, 53] sensors or professionally designed with curated 3D CAD furniture models [10, 34, 41, 62]. The real-world dataset provides physically realistic appearance observation of 3D scenes; however, collecting and anFigure 2. Illustration of our dataset. For each scene, we provide comprehensive panoramic renderings and 3D layout annotation. notating these data typically requires significant resources in terms of cost and labor. On the other hand, indoor synthetic datasets address the constraints of real-world data by supplying extensive, varied, and richly annotated scenes. In addition, Structured3D [62] and Hypersim [34] utilize the advanced render engine for photorealistic image rendering with accurate 2D labels. However, the camera view is limited, which restricts the downstream application. 3. SPATIALGEN Dataset We summarize the commonly used indoor scene dataset for layout-conditioned scene synthesis in Table 1. As one can see, the real-world datasets suffer from limited number of scenes, incomplete 3D annotations, and inconsistent annotation quality. Synthetic datasets are easier to annotate with ground-truth 3D labels, but they still have limitations in scene diversity (for example, Hypersim only has 461 scenes) or camera viewpoints (for example, Structured3D provides single panorama for each room). In this paper, we build new dataset to train generative 3 Figure 3. Overall pipeline. SPATIALGEN takes as input 3D semantic layout and one or more posed images, to create 3D scene. First, we generate per-view RGB images, scene coordinate maps, and semantic segmentation maps from Layout-Guided Multi-view Multi-modal diffusion model. Then, we adopt an iterative dense view generation strategy to generate images at more sampled viewpoints. Finally, these images are fed into 3D reconstruction method to produce the final result. models for 3D indoor scenes. Our dataset is based on large repository of house designs sourced from an online platform in the interior design industry. Most of these designs are designed by professional designers and are intended for real-world production. As shown in Figure 2, we create physically plausible camera trajectories that navigate smoothly through each scene while avoiding obstacles. These trajectories are sampled at 0.5m intervals to ensure comprehensive spatial coverage. For each viewpoint, we generate photorealistic panoramic renderings using an industry-leading rendering engine, capturing color, depth, normal, semantic, and instance segmentation data. We further convert the panoramic image into multiple perspective images using equilib [14]. To ensure both quality and diversity, we apply rigorous filtering criteria during dataset curation, resulting in 12,328 distinct scenes encompassing 57,440 individual rooms with diverse room types. Each scene is annotated with precise 3D layouts and divided into 57,390/50 scenes for training/testing, respectively. Figure 2 also shows some panoramas and 3D layout annotation from our dataset. Our dataset offers comprehensive structural layout annotations, including architecture elements (i.e., walls, doors, and windows). We further simulate diverse camera motion patterns from panoramic video data to train and evaluate the generation capabilities of existing approaches crucial advantage over limited rulebased trajectories from existing dataset. Additional dataset samples are provided in the appendix. 4. Method We introduce SPATIALGEN, novel method that generates Gaussian Splatting [20] scenes with semantics conditioned on 3D layout with reference images. Specifically, given semantic layout and one or more source images, SPATIALGEN first utilizes layout-guided multi-view multimodal diffusion model to generate dense views of the target scene via Iterative Dense View Generation (detailed in Section 4.3). Then, we recover those dense views to unified semantic Gaussian Splatting via an off-the-shelf reconstruction method [56]. We start by providing brief overview of multi-view diffusion models in Section 4.1. Then, we introduce our layout-guided latent diffusion model in Section 4.2, followed by the iterative generation scheme in Section 4.3 and the 3D reconstruction process in Section 4.4. 4.1. Preliminaries Multi-view Diffusion Model. multi-view latent diffusion model takes single or multiple posed source views as input and generates multiple novel images in some target camera views. To incorporate multi-view conditioning, it typically involves two designs: (1) 2D attention layers are improved to 3D-aware or multi-view aware attention mechanism, such as epipolar constraint [13], to capture multi-view features across different source views. (2) Camera poses are encoded by Plucker coordinate maps [29, 58] and then processed by Transformer to compute viewconditioned embeddings. Given input views IM = {I1, . . . , IM } with camera poses CM = {C1, . . . , CM }. Multi-view diffusion aims to predict new view images IN = {IM +1, . . . , IM +N } in camera poses CN = {CM +1, . . . , CM +N }. In other words, the multi-view latent diffusion model aims to learn the following joint distribution, p(IN IM , CM +N ), (1) 4 where CM +N = {C1, . . . , CM +N } includes camera poses for both input and output views. Layout Condition in MVD. 3D semantic layout provides an informative description of the scene. Following previous works [8, 38, 51], we represent the layout as set of semantic bounding boxes of objects {bk}K k=1, where each box bk includes the center location lk R3, the size sk R3, the orientation rk around the vertical axis, and the category label zk. For each viewpoint with camera parameter Cn = (Kn, Tn), we render semantic map Slayout and depth map Dlayout of the bounding box of the 3D layout. The Dlayout is then converted to the scene coordinate map layout = Tn (K 1 ). In this way, we obtain the input layout conditions Slayout +N = {Slayout for the latent diffusion model. We use scene coordinate maps instead of depth maps to represent 3D scene geometry because they encode the scene in globally consistent manner. As discussed in previous work [44, 60], they facilitate learning multi-view geometric consistency in the latent diffusion model. Therefore, extending Eq. (1), the joint distribution to be learned for layout-conditioned multi-view image generation can be formulated as follows, +N , Playout Dlayout , layout }M +N n=1 n p(IN IM , Slayout +N , Playout +N , CM +N ). (2) Note that the layout conditions Sn and Pn only provide coarse description of the bounding box without pixel-level details, as shown in Figure 3. 4.2. Layout-guided Multi-view Diffusion Model Since the input layout maps do not contain pixel-level details, we further predict pixel-wise semantic map Sn, scene coordinate map Pn for each viewpoint. This joint learning scheme improves 3D consistency in two ways: Explicit 3D supervision. By explicitly integrating both geometric and semantic maps into the latent diffusion model, SPATIALGEN leverages direct 3D supervision to achieve high-fidelity novel view synthesis results while maintaining cross-view consistency. Cross-view guidance. With the additional pixel-wise scene coordinate maps, we provide fine-grained guidance for diffusion by computing warped image at any target view from the input images. Specifically, we adopt the point cloud based render [18] to obtain the warped image warp , {M + 1, . . . , + }. The warped image is encoded and concatenated with the original noise map In to form conditioning signal for the target view, which we denote as augmented target views ˆIn = [In; warp The joint distribution to be learned now becomes, ]. p(ˆIN , SM +N , PM +N IM , Slayout +N , Playout +N , CM +N ). (3) Figure 4. Multi-view and multi-modal alternating attention. It alternates between enforcing multi-view consistency and multimodal fidelity within unified attention mechanism. Figure 5. Comparison of reconstruction results for scene coordinate map. The image VAE (a) generates noisy results, and the SCM-VAE without gradient loss (b) produces distorted results. Our SCM-VAE (c) accurately reconstructs the scene geometry. We use v-parametrization and v-prediction loss for the diffusion model [36]. Following CAT3D [11], the model is trained on total of 8 views, with randomly sampled {1, 3, 7} views as source views. We use the ground-truth scene coordinate map for warping in training, and use the predicted scene coordinates during inference. Multi-view Multi-modal Alternating Attention. With our formulation Eq. (3), our objective is to generate output that is consistent with multiple viewpoints and modalities. We observe that simple modification to the standard architecture, namely an alternating attention mechanism, is effective in preserving the desired consistency. As illustrated in Figure 4, the new architecture operates through complementary attention pathways: cross-view atInspired by previous tention and cross-modal attention. works [11, 24], our cross-view attention processes reshaped tokens along the view dimension (e.g., {tI +N } for all RGB images), allowing feature aggregation across multiple views in each modality. While cross-modal attention operates within each view, observing modality-specific tokens (e.g., {tI } for image, semantics, and geometry) to achieve fine-grained feature alignment. This design achieves balance between integrating information across different views and different modalities. Scene Coordinate Map VAE (SCM-VAE). standard im2, . . . , tI n, tP n, tS 1, tI 5 age VAE pretrained on RGB images generalizes well to semantic maps, but fails to accurately reconstruct scene coordinate maps, leading to poor geometric fidelity. See Figure 5(a) for an example. To address this, we introduce SCM-VAE, which encodes scene coordinate map into latent representation as = ξ(P ) and reconstructs into scene coordinate map with an uncertainty map as { ˆP , c} = D(z), where ξ denotes the encoder and is the decoder. The SCM-VAE is trained by fine-tuning the decoder with an additional output dimension from an image diffusion VAE, while keeping the encoder ξ frozen. is activated by = 1 + exp(c) to ensure strictly positive confidence [47]. The training objective combines standard VAE reconstruction with geometry-specific loss: = Lrec + λ1Lgrad, Lrec = ˆP α log c, Lgrad = 4 (cid:88) s=1 ( ˆP P ), (4) (5) (6) where α = 0.2 and denotes element-wise multiplication. Here, we follow previous monocular depth estimation works [12] to use multiscale gradient loss Lgrad to improve boundary sharpness in the decoded scene coordinate map. As we can see in Figure 5, our SCM-VAE with Lgrad outperforms the one without the term, especially around complex object boundaries and flat areas. 4.3. Iterative Dense View Generation Our goal is to generate complete 3D scene aligned to the given layout and text or image prompt. Although our layout-guided MVD model can generate an arbitrary number of views in principle, it is limited by GPU memory constraints. Thus, instead of generating all views at once, we adopt an iterative view synthesis strategy, similar approach is also used in previous work [26, 55]. The main idea is to incrementally maintain colored global point cloud of the scene to enforce appearance consistency between iterations. During each iteration, the point cloud is projected onto the target views Iwarp to provide pixel-aligned guidance for consistent generation. The SCMs of the target view generated by our diffusion model will be inserted into P. In this way, we can effectively reduce error accumulation. Furthermore, by incorporating the uncertainty map c, we filter out 3D points with uncertainty below predefined threshold, resulting in cleaner warped images. The iterative process, detailed in Algorithm 1, proceeds as follows: First, an initial point cloud is built by obtaining the scene coordinate maps PM for the input views IM . Second, at the beginning of each iteration, we render warped images for the target views from the point cloud. Then, we perform inference with not only the input images Im, but also the warped images to ensure global consistency. We 6 Algorithm 1: Iterative Dense View Generation Input: input views IM , camera poses for all iterations {CM , CN1 , . . . , CNK }, global point cloud initialized as empty, layout-guided multi-view diffusion model U(). Initialization: Obtain the SM and PM : {SM , PM } U(IM , CM ); Update global point cloud: PM ; for 1 to do Render warped images: Iwarp Augment the target views ˆINk = [INk ; Iwarp Nk Run inference: {ˆINk , SNk , PNk } (IM , CM +Nk ); Update global point cloud: (cid:83) PNk ; Render(P, CNk ); Nk ]; end Output: all generated views: {IN1 , IN2 , . . . , INK }. then update the point cloud accordingly. Finally, we collect the images generated from all iterations as output. 4.4. Scene Reconstruction and Understanding Building upon RaDe-GS [56], we reconstruct 3D scene representation from the densely generated color, geometric, and semantic images. Following Feature-3DGS [64], We augment the standard 3D Gaussians with semantic feature in each point. The scene is initialized from the predicted point cloud P. During differentiable rendering optimization, we employ depth supervision loss that utilizes the predicted scene coordinate maps, enabling rapid convergence in just 7,000 steps. As shown in Figure 6, the pipeline produces high-fidelity RGB renderings and geometrically accurate depth reconstructions. 5. Experiments 5.1. Experiment Setup Benchmark Datasets. We use both existing datasets (i.e., Hypersim [34] and Structured3D [62]) and our new dataset. As discussed in Section 3, one key difference of these datasets is in the diversity of camera viewpoints. Since our dataset provides abundant panorama images at dense locations in each room, we can design various camera movement patterns to conduct an extensive evaluation. Specifically, we introduce four distinct camera trajectories with different amounts of view overlap and distance between input and target views: (i) Forward: the trajectory follows linear path with minimal directional variation, simulating steady camera movement. (ii) Inward Orbit: both the input and output views are directed toward the center of the room, ensuring substantial view overlap; (iii) Outward Orbit: the input and output views are at the same location, but oriented differently, with less than 45 overlap at adjacent views. (iv) Random Walk: input and output views are sampled from continuous random-walk path, Figure 6. Qualitative comparison to score distillation methods on Hypersim [34] (top row) and our dataset (bottom row), In each case, we show the generated color images and depth maps. Table 2. Comparison to score distillation methods."
        },
        {
            "title": "5.2.1 Comparison to Score Distillation Methods",
            "content": "Dataset Method CLIP Sim. (%) Image Reward Hypersim [34] Our Dataset Set-the-Scene [6] SceneCraft [51] SPATIALGEN SPATIALGEN Set-the-Scene [6] SceneCraft [51] SPATIALGEN 25.18 26.94 25.93 27.59 25.23 18.93 26.84 -2.005 -1.096 -1.168 -0.285 -2.100 -2.267 -0.238 resulting in minimal view overlap. Please refer to the appendix for the visualization of these settings. Implementation Details. We implement SPATIALGEN in PyTorch. Both the SCM VAE and the latent diffusion model are fine-tuned from stable diffusion 2.1 (SD-2.1) [35]. We use AdamW optimizer [25]. For SCM VAE, we freeze the encoder and only fine-tune the decoder for 10K steps with batch size of 64. For the latent diffusion model, we finetune it for 35K steps with batch size of 128. The first 16K steps use resolution 256 256, whereas the remaining steps use resolution 512 512. All models are fine-tuned on 64 NVIDIA RTX 4090 GPUs. The learning rate starts at 104 and decays by factor of 0.01 at 90% of the total training process. We render warped images using PyTorch3D [18]. For text-to-3D scene generation, we further train layout ControlNet [59] to generate the reference image for our latent diffusion model. 5.2. Text-to-3D Scene Generation As discussed before, existing layout-conditioned text-to-3D methods can be grouped into two categories: score distillation methods [4, 6, 51, 66] and panorama-as-proxy methods [8, 38]. In the following, we compare to these two groups separately. For this experiment, we compare to two open-source methods: Set-the-Scene [6] and SceneCraft [51]. The other methods such as Layout2Scene [4] and GALA3D [66] do not release their codes. We conduct experiments on both Hypersim [34] and our new datasets. Evaluation is performed on the test scenes of each target dataset following standard protocols. For Hypersim dataset, we directly use the official checkpoints of Set-the-Scene and SceneCraft. To illustrate the benefit of our proposed large-scale dataset, we include two training configurations for our model: SPATIALGEN which is trained solely on Hypersim, and SPATIALGEN which is trained on combination of Hypersim and our datasets. For our dataset, we fine-tune the layout ControlNet of SceneCraft and adapt the layouts to match the input of Setthe-Scene. All methods on our dataset use the Inward Orbit camera trajectory for consistency. Quantitative Results. Table 2 reports the performance of all methods with established 2D rendering metrics: CLIP similarity score [31] to measure text-image alignment and Image Reward [50] to assess human aesthetic preference. As one can see, our method performs slightly worse than SceneCraft when trained solely on Hypersim. We hypothesize that Hypersim is too small for powerful latent multi-view diffusion models like the one employed by our method. When trained on both Hypersim and our datasets, our method outperforms both SDS methods on all metrics. Notably, SPATIALGEN achieves significantly higher image-reward score than models trained solely on Hypersim, validating the benefit of our large-scale dataset for high-quality 3D scene generation. Furthermore, when tested on our dataset, SPATIALGEN consistently outper7 Table 3. Comparison to panorama-as-proxy method. Dataset Method CLIP Sim. (%) Image Reward Structured3D [62] Our Dataset Ctrl-Room [8] SPATIALGEN Ctrl-Room [8] SPATIALGEN 25.03 23.90 22.63 26.84 -1.016 -1. -1.546 -0.238 Figure 7. Qualitative comparison between our method and the panorama-as-proxy baseline [8] on Structured3D [62] (top row) and our dataset (bottom row). forms the baselines, with significantly higher imagereward score. Qualitative Results. The advantage of our method can be better observed in Figure 6. On both Hypersim and our datasets, SPATIALGEN generates photorealistic scenes with superior details that are well-aligned with the specified layout. In contrast, SceneCraft struggles to balance layout adherence with text-prompt fidelity, and Set-the-Scene takes nearly two hours to synthesize radiance field that still lacks fine-grained details."
        },
        {
            "title": "5.2.2 Comparison to Panorama-as-Proxy Methods",
            "content": "Next, we compare our method to panorama-as-proxy methods. We choose Ctrl-Room [8] as the baseline because the source code and checkpoints are available. We conduct experiments on both Structured3D and our datasets. For Structured3D dataset, we use ground-truth layouts to ensure consistent inputs for both Ctrl-Room and our method. For our dataset, we again use the Inward Orbit camera trajectory for consistency. Quantitative Results. Table 3 reports the performance of both methods, we take the renderings of the generated mesh from Ctrl-Room for comparison. On Structured3D dataset (which provides only single panorama per scene), our method still achieves competitive performance. The relatively lower scores are expected as Ctrl-Room is specifically trained to synthesize single panorama at fixed camera location. In contrast, our method generates multiple perspective images without explicitly exploiting the fact that 8 Table 4. Experimental results on image-to-3D scene generation under four distinct camera trajectories: Forward, Inward Orbit, Outward Orbit, and Random Walk, with gradually reduced view overlaps. Method PSNR SSIM LPIPS FID Forward SPATIALGEN (w/o layout) SPATIALGEN (w/ layout) Inward orbit SPATIALGEN (w/o layout) SPATIALGEN (w/ layout) Outward orbit SPATIALGEN (w/o layout) SPATIALGEN (w/ layout) Random walk SPATIALGEN (w/o layout) SPATIALGEN (w/ layout) 11.47 17.59 12.57 17. 11.14 13.32 11.26 14.07 0.49 0.69 0.48 0.66 0.60 0.59 0.45 0. 0.59 0.32 67.96 34.98 0.54 0.33 64.14 35.57 0.47 0.46 76.73 57. 0.59 0.45 98.42 52.10 all images are taken from single location. Nevertheless, the critical advantage of our method is revealed on our dataset: when rendering from novel viewthe performance of Ctrl-Room degrades signifipoints, cantly, whereas our method consistently produces highquality results from arbitrary views. Qualitative Results. Figure 7 further highlights the advantage our method over panorama-as-proxy method. On our dataset, Ctrl-Room fails to synthesize coherent novel views, exhibiting severe distortions and artifacts. In contrast, our method is not limited to single camera position (i.e., panorama generation). It achieves high-quality panorama generation on Structured3D while also enabling photorealistic novel view synthesis on our dataset. 5.3. Image-to-3D Scene Generation In this section, we conduct additional image-to-3D scene generation experiments with focus on two key aspects: (i) generation capability the ability to synthesize missing regions for large viewpoint changes; (ii) semantic consistency the ability to produce semantically consistent views aligned with the 3D scene layout. Given the lack of accessible literature on the layout-conditioned image-to-3D scene generation task, we compare our multi-view generation model against the version without utilizing layout priors. We employ PSNR, SSIM [48], LPIPS [61], and FID [15] to evaluate the quality of image generation. Quantitative Results. Table 4 reports quantitative results of our method under different camera trajectories. Under all trajectories, the semantic layout improves the results across all metrics. Furthermore, the improved FID shows that our method with layout guidance can capture the underlying data distribution more effectively. These results collectively underscore the critical role of incorporating 3D layout information in novel view synthesis. Figure 8. Qualitative comparison of image-to-3D scene generation on our dataset. Given single input image, our method with layout guidance consistently generates better color images, scene coordinate maps, and semantic maps. Qualitative Results. Figure 8 shows example outputs of our method, including RGB images, scene coordinate maps, and semantic maps. Removing the layout input leads to severe artifacts in occluded regions, revealing the limitations of image diffusion models in capturing 3D scene structures. In addition, the semantic map contains unknown content, suggesting degraded semantic prediction without layout input. In contrast, our method with layout guidance generates better novel view images and achieves more reasonable semantic and geometric predictions. 6. Conclusion & Limitations We present SPATIALGEN, novel framework for layoutguided 3D indoor scene synthesis. At the core of our pipeline is multi-view multi-modal diffusion model, which generates images with high visual quality and geometric consistency. To train this model, we collect new synthetic indoor scene dataset with 4.7M panoramic renderings of 57,440 rooms and the 3D layout annotations. These advancements open new possibilities for downstream applications such as interior design, embodied AI, and virtual/augmented reality. Limitations. First, the cross-view and cross-modal attention introduces additional computational cost to the multiview diffusion model, which limits SPATIALGEN to generate relatively small number of images at time. Moreover, the camera sampling strategy might affect the generation quality. We plan to address these challenges in the future."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was partially supported by the Key R&D Program of Zhejiang Province (2025C01001) and the HKUST project 24251090T019. We would like to thank the engineering team in Manycore Tech Inc., Yingqi Shen, Liangbin Hu, and Fuchun Dong for their exceptional effort in supporting building the large-scale SPATIALGEN dataset. We are also grateful to Chenfeng Hou and Zhiwei Wang for developing the layout ControlNet. Additionally, we extend our thanks to Kunming Luo for his valuable suggestions regarding the paper figures."
        },
        {
            "title": "References",
            "content": "[1] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin Dehghan, and Joshua Susskind. GAUDI: neural architect for immersive 3d scene generation. In Adv. Neural Inform. Process. Syst., pages 2510225116, 2022. 2 [2] Aleksey Bokhovkin, Quan Meng, Shubham Tulsiani, and Angela Dai. SceneFactor: Factored latent 3D diffusion for In IEEE Conf. Comput. controllable 3D scene generation. Vis. Pattern Recog., pages 628639, 2025. 3 [3] Angel X. Chang, Angela Dai, Thomas A. Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor environments. In IEEE Int. Conf. 3D Vis., pages 667676, 2017. 3 [4] Minglin Chen, Longguang Wang, Sheng Ao, Ye Zhang, Kai Xu, and Yulan Guo. Layout2Scene: 3d semantic layout guided scene generation via geometry and appearance diffusion priors. arXiv preprint arXiv:2501.02519, 2025. 2, 3, 7 [5] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. LucidDreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 3 [6] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. Set-the-Scene: Global-local training for generating controllable nerf scenes. In IEEE Int. Conf. Comput. Vis. Worksh., pages 29202929, 2023. 2, 3, 7, 14 [7] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: 9 In Richly-annotated 3D reconstructions of indoor scenes. IEEE Conf. Comput. Vis. Pattern Recog., pages 58285839, 2017. 3 [8] Chuan Fang, Yuan Dong, Kunming Luo, Xiaotao Hu, Rakesh Shrestha, and Ping Tan. Ctrl-Room: controllable text-to-3d room meshes generation with layout constraints. In IEEE Int. Conf. 3D Vis., 2025. 2, 3, 5, 7, 8, 14 [9] Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. LayoutGPT: Compositional visual planning and generation with large language models. In Adv. Neural Inform. Process. Syst., pages 1822518250, 2023. 2 [10] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, and Hao Zhang. 3D-FRONT: 3d furnished rooms with layouts and semantics. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1093310942, 2021. 3 [11] Ruiqi Gao, Aleksander Hoł ynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T. Barron, and Ben Poole. CAT3D: Create anything in 3d with multi-view diffusion models. In Adv. Neural Inform. Process. Syst., pages 7546875494, 2024. 2, 3, 5, 15 [12] Clement Godard, Oisin Mac Aodha, and Gabriel Brostow. Unsupervised monocular depth estimation with left-right In IEEE Conf. Comput. Vis. Pattern Recog., consistency. pages 270279, 2017. 6 [13] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. [14] haruishi43. Equilib, 2020. 4, 14 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Adv. Neural Inform. Process. Syst., 30, 2017. 8 [16] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3D meshes from 2D text-to-image models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 79097920, 2023. 3 [17] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and Alireza Fathi. SceneCraft: An llm agent for synthesizing 3d scenes as blender code. In Int. Conf. Mach. Learn., 2024. 2 [18] Justin Johnson, Nikhila Ravi, Jeremy Reizenstein, David Novotny, Shubham Tulsiani, Christoph Lassner, and Steve Branson. Accelerating 3d deep learning with pytorch3d. In SIGGRAPH Asia 2020 Courses, 2020. 5, 7 [19] Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang, Yu Qiao, and Hongsheng Li. DiffInDScene: Diffusion-based In IEEE Conf. high-quality 3D indoor scene generation. Comput. Vis. Pattern Recog., pages 45264535, 2024. 3 [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 4 [21] Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3D: Real-world camera trajectory and 3D scene generation from text. In Adv. Neural Inform. Process. Syst., pages 7512575151, 2024. [22] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang, Bedrich Benes, and Aniket Bera. DL3DV-10K: large-scale scene dataset for deep learning-based 3d vision. In IEEE Conf. Comput. Vis. Pattern Recog., pages 22160 22169, 2024. 3 [23] Yuheng Liu, Xinke Li, Xueting Li, Lu Qi, Chongshou Li, and Ming-Hsuan Yang. Pyramid diffusion for fine 3d large scene generation. In Eur. Conf. Comput. Vis., pages 7187, 2024. 3 [24] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3d: Single image to 3d using cross-domain diffusion. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9970 9980, 2024. 5 [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Int. Conf. Learn. Represent., 2017. 7 [26] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In IEEE Conf. Comput. Vis. Pattern Recog., pages 20162029, 2025. 2, 6 [27] Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, and Zihan Zhou. SpatialLM: Training large language models for structured indoor modeling. arXiv preprint arXiv:2506.07491, 2025. 1, 15, 17, 19 [28] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. ATISS: Autoregressive transformers for indoor scene synthesis. In Adv. Neural Inform. Process. Syst., pages 1201312026, 2021. 2, 3 [29] Julius Plucker. On new geometry of space. Phil. Trans. R. Soc, 155:725791, 1865. 4 [30] Ben Poole, Ajay Jain, Jonathan Barron, and Ben MildenIn Int. hall. DreamFusion: Text-to-3d using 2d diffusion. Conf. Learn. Represent., 2023. 2 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual In Int. Conf. models from natural language supervision. Mach. Learn., pages 87488763, 2021. 7 [32] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Infinite photoreAnkit Goyal, Kaiyu Yang, and Jia Deng. In IEEE Conf. alistic worlds using procedural generation. Comput. Vis. Pattern Recog., pages 1263012641, 2023. 2 [33] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, and Jia Deng. Infinigen indoors: Photorealistic indoor scenes 10 using procedural generation. Pattern Recog., pages 2178321794, 2024. 2 In IEEE Conf. Comput. Vis. [34] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synIn thetic dataset for holistic indoor scene understanding. IEEE Int. Conf. Comput. Vis., pages 1091210922, 2021. 3, 6, 7, 14, 15 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1068410695, 2022. 3, 7 [36] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In Int. Conf. Learn. Represent., 2022. [37] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. Zeronvs: Zeroshot 360-degree view synthesis from single image. In IEEE Conf. Comput. Vis. Pattern Recog., pages 94209429, 2024. 3 [38] Jonas Schult, Sam Tsai, Lukas Hollein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao Zhang, Bastian Leibe, Peter Vajda, and Ji Hou. ControlRoom3D: Room generation using semantic proxy rooms. In IEEE Conf. Comput. Vis. Pattern Recog., pages 62016210, 2024. 2, 3, 5, 7 [39] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. SceneCoordRegression: Scene coordinate regression forests for camera relocalization in RGB-D images. In IEEE Conf. Comput. Vis. Pattern Recog., pages 29302937, 2013. 2 [40] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. SUN RGB-D: RGB-D scene understanding benchmark suite. In IEEE Int. Conf. Comput. Vis., pages 567576, 2015. 3 [41] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica Dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2024. 3 [42] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3D-GPT: Procedural 3D modeling with large language models. arXiv preprint arXiv:2310.12945, 2023. [43] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. LayoutVLM: Differentiable optimization of 3D layout via vision-language models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2946929478, 2025. 2 [44] Stanislaw Szymanowicz, Jason Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan Barron, and Philipp Henzler. Bolt3D: Generating 3D scenes in seconds. In IEEE Int. Conf. Comput. Vis., 2025. 2, 3, 5 [45] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nießner. DiffuScene: Scene graph denoising diffusion probabilistic model for generative indoor scene synthesis. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2050720518, 2024. 2, 3 [46] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. MVDiffusion: Enabling holistic multiview image generation with correspondence-aware diffusion. In Adv. Neural Inform. Process. Syst., pages 5120251233, 2023. 2, 3 [47] Sheng Wan, Tung-Yu Wu, Wing H. Wong, and Chen-Yi Lee. Confnet: Predict with confidence. In Int. Conf. Acoust. Speech Signal Process., pages 29212925, 2018. [48] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE Trans. Image Process., 13(4): 600612, 2004. 8 [49] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. MotionCtrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH, 2024. 3 [50] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and evaluating human preferences for textto-image generation. Adv. Neural Inform. Process. Syst., 36: 1590315935, 2023. 7 [51] Xiuyu Yang, Yunze Man, Junkun Chen, and Yu-Xiong Wang. SceneCraft: Layout-guided 3d scene generation. Adv. Neural Inform. Process. Syst., 37:8206082084, 2024. 2, 3, 5, 7, 14 [52] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, and Christopher Clark. Holodeck: Language guided generation of 3D embodied AI environIn IEEE Conf. Comput. Vis. Pattern Recog., pages ments. 1622716237, 2024. 2 [53] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. ScanNet++: high-fidelity dataset of 3D indoor scenes. In IEEE Int. Conf. Comput. Vis., pages 1222, 2023. [54] Lap Fai Yu, Sai Kit Yeung, Chi Keung Tang, Demetri Terzopoulos, Tony Chan, and Stanley Osher. Make it home: automatic optimization of furniture arrangement. ACM Trans. Graph., 30(4), 2011. 2 [55] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. ViewCrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 3, 6 [56] Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, and Ping Tan. RaDe-GS: Rasterizing depth arXiv preprint arXiv:2406.01467, in gaussian splatting. 2024. 4, 6 11 [57] Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, and Jianfei Cai. Taming stable diffusion for text to 360 panorama image generation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 63476357, 2024. 3 [58] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. In Int. Conf. Learn. Represent., 2024. [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE Int. Conf. Comput. Vis., pages 38363847, 2023. 7 [60] Qihang Zhang, Shuangfei Zhai, Miguel Angel Bautista Martin, Kevin Miao, Alexander Toshev, Joshua Susskind, and Jiatao Gu. World-consistent video diffusion with explicit 3D modeling. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2168521695, 2025. 5 [61] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In IEEE Conf. Comput. Vis. Pattern Recog., pages 586595, 2018. 8 [62] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, Structured3D: large photo-realistic In Eur. Conf. Comput. and Zihan Zhou. dataset for structured 3d modeling. Vis., pages 519535, 2020. 2, 3, 6, 8, 14 [63] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. ADE20K: Semantic understanding of scenes through the ADE20K dataset. Int. J. Comput. Vis., 127(3):302321, 2019. 14 [64] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3DGS: Supercharging 3D gaussian splatting to enable distilled feature fields. In IEEE Conf. Comput. Vis. Pattern Recog., pages 21676 21685, 2024. 6 [65] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. ACM Trans. Graph., 37(4), 2018. [66] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. GALA3D: Towards text-to-3d complex scene generation via In Int. Conf. layout-guided generative gaussian splatting. Mach. Learn., 2024. 2, 7 12 Figure 9. Examples of SPATIALGEN dataset. In the appendix, we provide more details of SPATIALGEN dataset in Section A, additional experimental results in Section B, and additional experiments of SPATIALGEN in Section C. A. SPATIALGEN Dataset A.1. Dataset Construction Data Curation. Our dataset is sourced from an online platform in the interior design industry, providing largescale collection of professional designs intended for realworld applications. We employ rigorous multi-stage filtering pipeline to ensure both the quality and diversity of the dataset. The curation process begins by selecting scenes based on four key criteria: (i) professional designer ratings, (ii) the number of renderings generated by the design, (iii) total floor area exceeding 20m2, and (iv) the presence of more than 35 unique objects. Then, we extract individual rooms from each selected scene and apply additional filters to retain only those rooms that (i) have floor area greater than 8m2 and (ii) contain more than 3 unique objects. For rendering, we use an industry-leading rendering engine to generate images. We simulate physically plausible camera trajectories that navigate smoothly within each room while avoiding obstacles. After rendering, we implement strict quality control measures by discarding lowquality imagesspecifically those with camera-object intersections, overexposure, or inadequate lighting, as illustrated in Figure 10. The final dataset consists of 12,328 distinct scenes, 57,440 individual rooms covering variety of room types, and 4.7M photo-realistic panoramic renderings. The total camerafurniture collision over-exposure insufficient illumination Figure 10. Example of low-quality renderings. Figure 11. Camera configuration. floor area across all scenes is approximately 914, 687m2. Camera configuration. We capture panoramic renderings at intervals of 0.5m to ensure comprehensive scene coverage, as shown in the top-left of Figure 9. Each panoramic rendering is generated at resolution of 10242048 and includes color, albedo, depth, normal, semantic, and instance Figure 12. Room type distribution. Figure 13. Object category distribution. maps. The entire rendering process requires approximately 54K GPU hours. Following an obstacle-avoiding camera trajectory within each room, we obtain dense sequences of panoramic images. Thanks to the 360 field-of-view (FoV) of panoramas, we can simulate an unlimited number of perspective images with varying camera configurations. For each panoramic viewpoint, we generate perspective views with different fields-of-view and rotation angles using equirectangular-toperspective projection [14], as illustrated in Figure 11. Furthermore, we introduce four distinct camera trajectories with varying amounts of view overlap and distances between input and target views: (i) Forward: linear path with minimal directional variation, simulating steady camera movement; (ii) Inward Orbit: both input and output views are oriented toward the center of the room, ensuring significant view overlap; (iii) Outward Orbit: the input and output views share the same location but have different orientations, resulting in less than 45 overlap between adjacent views; and (iv) Random Walk: input and output views are sampled along continuous random-walk path, with minimal view overlap. A.2. Dataset Statistics Room type statistics. The resulting dataset contains 12,592 living and dining rooms, 2,179 living rooms, 2,524 study rooms, 8,540 kitchens, 8,460 bathrooms, 1,464 balconies, 9,049 master bedrooms, 8,603 secondary bedrooms, 2,793 childrens rooms, and 4,418 other room types, as illustrated in Figure 12 representing diverse and substantial collection of indoor environments. Object category statistics. The raw online designs initially contained approximately 65,000 object categories. We filtered out niche object classes specific to interior design and mapped the remaining objects to 62 common categories from ADE20K [63]. We then curated the object bounding boxes according to the following criteria: (i) objects outside the room layout were discarded; (ii) objects with any edge shorter than 0.1m or longer than 1.8m were excluded. This process yielded total of 1,046,637 object bounding boxes. Figure 13 shows the distribution of object categories throughout our dataset, excluding the spotlight and other categories (containing 250K and 240K instances, respectively) to improve visualization of the remaining categories. A.3. Dataset Visualization As shown in Figure 9, our dataset provides high-quality panoramic renderings accompanied by precise 2D annotations and comprehensive 3D structural layouts, including architecture elements (e.g., walls, windows, and doors), which distinguishes it from existing datasets like Hypersim [34], offering extensive evaluation opportunities for scene generation and spatial understanding tasks. B. Additional Results In this section, we show more results of Text-to-3D Scene and Image-to-3D Scene generation. Text to 3D scene generation. As demonstrated in Figure 14, our method outperforms SDS-based baselines on the Hypersim dataset [34], producing photorealistic and layoutfaithful scenes with superior detail. This advantage is evident even against our model trained only on Hypersim (SPATIALGEN), which produces blurring and ambiguous results, highlighting the benefits of our dataset. In Figure 15, we compare against SceneCraft [51] and Set-The-Scene [6] under diverse 3D layouts on SPATIALGEN dataset. As the increase of layout complexity, while competing methods fail to generate meaningful radiance fields or capture scene details, our approach consistently delivers realistic and coherent results for complex scenes like living and dining rooms. We further compare our method against the panoramaas-proxy based method, Ctrl-Room [8], on both the Structured3D [62] and SPATIALGEN dataset. We split the panoramic image into 8 perspective images for direct 14 Figure 14. Qualitative comparison of text-to-3D scene on Hypersim [34] dataset. In each case, we show the generated color images and depth map. Figure 15. Qualitative comparison of text-to-3D scene on SPATIALGEN dataset. In each case, we show the generated color images. comparison, as shown in Figure 16. C. Additional Experiments For the SPATIALGEN dataset, we render layoutsemantic panorama from random viewpoint to use as input for Ctrl-Room. We then spatially align its resulting mesh with our generated scene for fair comparison. The results, presented in Figure 17, demonstrate that Ctrl-Room exhibits severe stretching artifacts and scale misalignment at novel viewpoints. In contrast, our method consistently produces photorealistic and fully 3D-consistent renderings from all views. Image to 3D scene generation. Building upon the largescale SPATIALGEN dataset, we evaluate our method across four diverse camera trajectories. As shown in Figure 18, given 3D layout and reference image (highlighted in orange box), our method successfully generates 3D-consistent novel views and synthesizes semantically plausible content for areas beyond the original input view. In this section, we conduct additional ablation studies Section C.1. Furthermore, we show some generation results from common video (unposed) captured in indoor scenes, using the state-of-the-art scene understanding model SpatialLM [27] in Section C.2. C.1. Ablation Studies Ablation on layout guidance. We first study the effect of layout guidance to validate our design. We compare our full model (denoted as W/ Layout) against variant that removes layout priors (denoted as W/O Layout). The W/O Layout variant is implemented similarly to CAT3D [11] but incorporates our multi-view multi-modal alternating attention module to enable multi-modal output. Both models are trained identically for single-image 3D scene generation. Figure 20 presents faithful comparison, showing gen15 Figure 16. Qualitative comparison with Ctrl-Room on Structured3D for panorama generation. We split the panorama into eight perspective images for direct comparison. Our method achieves competitive RGB synthesis compared with Ctrl-Room, resulting in photo-realistic scenes that are well-aligned with the provided layout. Figure 17. Qualitative comparison with Ctrl-Room on SPATIALGEN dataset. Ctrl-Room exhibits severe stretching artifacts and scale misalignments at novel viewpoints. In contrast, our method consistently produces photorealistic and fully 3D-consistent renderings from all views. erated RGB outputs, scene coordinate maps, and semantic maps (top to bottom) from given input (left-most column). As the red circles highlight, the W/O Layout variant produces artifacts in occluded regions, exhibits imperfect image-pose alignment, and generates degraded dense predictions. These failures indicate the inherent limitations of relying solely on image diffusion priors for 3D scene generation. In contrast, our full model W/ Layout leverages explicit layout guidance to achieve superior novel-view synthesis and more accurate geometry and semantic predictions. Furthermore, Figure 19 provides an in-depth analysis of 3D consistency. We visualize the predicted scene coordinates for the input view, demonstrating that the W/ layout predictions achieve better alignment with the ground truth. This superior alignment provides more accurate warped images for all target viewpoints, explaining its clear superiority over the W/O layout baseline. 16 Figure 18. Qualitative results on SPATIALGEN dataset under various camera trajectories. From left to right: input view and target views. First Row (forward): sampled views follow progressive forward-moving path. Second row (inward orbit): views are directed toward the center of the room, ensuring substantial overlap between them. Third row (outward orbit): views are positioned at the center of the room, looking outward, with an angle of less than 45 between two adjacent views. Bottom (random walk): views are selected from continuous random-walk camera trajectory, producing aggressive viewpoint changes. Figure 19. Comparing geometric prediction quality between our (W/ layout) and (W/O layout). The first two columns show the predicted scene coordinate maps, where our method (W/ layout) achieves better alignment with ground-truth geometry (brown color point cloud) compared to the counterpart without layout guidance (W/O layout). Correspondingly, the warped images projected by the predicted scene coordinates demonstrate improved spatial consistency and reduced artifacts. Ablation on number of input views. In Table 5, we evaluated SPATIALGEN using different numbers of input views in the inward orbit camera configuration. Increasing the number of input views enhances all metrics, particularly the FID score; this implies that greater input views enhances semantic consistency. C.2. Creating New Scenes from Video To demonstrate the versatility of SPATIALGEN, we apply it to the challenging task of generating novel 3D scenes from videos. By leveraging state-of-the-art layout estimation model, SpatialLM [27], we get the reconstructed 3D layout from the video. Then, we perform text-to-3D scene generation conditioned on this layout and additional user-provided 17 Figure 20. Ablation on the effectiveness of layout as guidance. Table 5. Effect of the number of input views in the inward orbit setting. #input views PSNR SSIM LPIPS FID 1 3 17.30 17.83 18.33 0.66 0.67 0.67 0.33 0.31 0.31 35.57 28.72 21.93 text prompts. This approach allows us to generate entirely new scenes that preserve the structural layout of the original video while altering its stylistic and semantic content based on the text description. We validate this video-tonew-scenes application on the SpatialLM test set. Figure 21 shows qualitative results. Figure 21. Video-to-New-3D Scene Generation on the SpatialLM Test set [27]. By leveraging the state-of-the-art scene layout estimation method, SpatialLM [27], we get the reconstructed 3D layout from the video. Then, we perform text-to-3D scene generation conditioned on this layout and additional user-provided text prompts. For clearer visualization of 3D consistency and multi-modal prediction capabilities, we put depth maps here instead of displaying the coordinate maps directly."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Manycore Tech Inc."
    ]
}