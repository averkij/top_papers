{
    "paper_title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
    "authors": [
        "Qinghe Wang",
        "Yawen Luo",
        "Xiaoyu Shi",
        "Xu Jia",
        "Huchuan Lu",
        "Tianfan Xue",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/."
        },
        {
            "title": "Start",
            "content": "CineMaster: 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation Qinghe Wang1 Yawen Luo2 Xiaoyu Shi3 Xu Jia1(cid:66) Huchuan Lu1 Tianfan Xue2(cid:66) Xintao Wang3 Pengfei Wan3 Di Zhang3 Kun Gai3 1Dalian University of Technology 2The Chinese University of Hong Kong 3Kuaishou Technology Equal contribution Project Leader (cid:66)Corresponding author 5 2 0 2 2 1 ] . [ 1 9 3 6 8 0 . 2 0 5 2 : r https://cinemaster-dev.github.io/ Figure 1. CineMaster targets at granting users 3D-aware and intuitive control over the text-to-video generation process. We first design 3D-native workflow that enables users to manipulate objects and camera in the 3D space. Then the rendered depth maps and camera trajectories serve as strong guidance to synthesize the desired video content. Left column shows the objects and camera setup using the proposed workflow. Right columns indicate synthesized frames with rendered depth maps on the bottom left."
        },
        {
            "title": "Abstract",
            "content": "In this work, we present CineMaster, novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operWork done during internship at KwaiVGI, Kuaishou Technology. ates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3Daware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signalscomprising rendered depth maps, camera trajectories and object class labelsserve as the guidance for text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. 1. Introduction With the advances of diffusion models [17, 30, 34] and large-scale pretraining paradigms, text-to-video generation (T2V) [3, 4, 14, 39] has experienced rapid development. The T2V models empower both artists and novices to create impressive videos merely by providing textual prompts. Subsequently, controllable video generation has gained increasing attention, driven by the growing demand for finegrained control over the video creation process. Ideally, controllable T2V framework should grant users comparable controllability as professional film director: allowing precise placement of objects within scene, flexible manipulation of both objects and camera, and intuitive layout control over each rendered frame. However, existing approaches fall short of achieving this vision. Early works typically extend image-based ControlNet [26, 48] to the video domain, guiding generation using condition maps (e.g. depth [6], semantic [37], optical flow [2, 21, 32], or canny edge maps [13, 37]). Yet, these methods generally rely on pre-existing videos to obtain condition maps, since it is non-trivial to create such condition maps from scratch. Moreover, in terms of controllability, MotionCtrl [40] and Direct-A-Video [45] offer preliminary control over both object and camera movements, but these methods only allow 2D object control which is very different from how filmmakers or video creators plan shooting in 3D space. More recent works have moved towards integrating 3D-aware signals into video generation [7, 10, 12, 33]. However, these methods are only designed for image-to-video generation or rely on synthetic data from Unreal Engine. To bridge the aforementioned gap, we present CineMaster, framework designed for highly controllable text-tovideo generation as shown in Fig. 1. Specifically, CineMaster operates in two stages, as presented in Fig. 2. The first stage is an interactive workflow that allows users to specify the requirements (conditions) of generated video, similar to how filmmakers design capturing plan. It allows users to describe the primary objects in scene using set of 3D bounding boxes with semantic labels. These bounding boxes, along with the camera, can be repositioned across keyframes, allowing users to orchestrate complex motion dynamics. After each modification, CineMaster provides preview of the rendered frames for iterative refinement until the desired rendered effects are achieved. In the second stage, we finetune text-to-video diffusion model to generate video conditioned on the control signals provided in Figure 2. Overview of CineMaster. CineMaster consists of two stages. First, we present an interactive workflow that allows users to intuitively manipulate the objects and camera in 3D-native manner. Then the control signals are rendered from the 3D engine and fed into text-to-video diffusion model, guiding the generation of user-intended video content. the first stage. Crucially, in addition to camera trajectories and user-provided class labels, we propose to utilize the rendered depth maps of all frames as augmented visual cues. These depth maps explicitly contain the desired 3D layout of each frame, serving as strong guidance for the diffusion model to generate the user-intended video content. One challenge for this design is the lack of videos with ground-truth 3D bounding box and camera trajectory annotations. To solve this limitation, we further propose an automatic data labeling pipeline, as illustrated in Sec 3.3. Using this pipeline, we build the largest video datasets with both the ground-truth 3D bounding box and 3D camera trajectory annotations. Finally, to evaluate the controllability of our proposed framework, we conduct extensive experiments, comparing it with existing SOTA methods, and performing ablative studies to validate the effectiveness of our core modules. 2. Related Work Controllable Video Generation via Planar Condition Maps. The pioneer works ControlNet [48] and T2IAdapter [26] introduce the paradigm of conditioning generation on planar maps in the image generation field. Subsequently, many works extend this paradigm to the video domain using different condition maps, e.g. depth maps [6], human pose maps [18], semantic maps [37] and optical flow maps [2, 21, 32]. However, they generally assume the existence of such condition maps, but it is indeed non-trivial to create precise condition maps from scratch, especially for novices. Therefore, we carefully design an interactive workflow to help users obtain 3D-aware condition maps in an intuitive way. We also get inspiration from LooseControl [1], to use 3D bounding box as an appropriate abstract representation of objects in the scene. Object Motion Control. Previous methods primarily focus on motion control in 2D space. MotionCtrl [40], DragNUWA [46], and Tora [49] represent object motion trajectoFigure 3. Overview of the network architecture. We design Semantic Layout ControlNet which consists of semantic injector and DiTbased ControlNet. Semantic injector fuses the 3D spatial layout and class label conditions. The DiT-based ControlNet further represents the fused features and adds to the hidden states of the base model. Meanwhile, we inject the camera trajectories by the camera adapter to achieve joint control over object motion and camera motion. ries as sequences of spatial positions, encoding coordinates into dense control maps. Additionally, 2D bounding boxes have been adopted as control signals to enable flexible motion generation in Direct-A-Video [45] and Boximator [38]. Motion control through sketches has also been explored in VideoComposer [39]. While these methods have demonstrated capabilities in object motion control, their control signals limit the controllability only in 2D space. 3DTrajMaster [10] is the first to use 6D pose sequences of objects to control object motion in 3D space. Camera Motion Control. Camera pose serves as crucial control signal in video generation, determining which portions of the scene are captured and presented in the final output. MotionCtrl [40] pioneers the integration of camera poses as control signals for camera movement manipulation. Building upon this foundation, CameraCtrl [15] introduces the use of Plucker embeddings of camera poses to enhance motion controllability. These methods are all trained on an indoor dataset RealEstate10K [50] for learning camera motion which limits the ability to generalize to in-thewild scenes. Direct-A-Video [45] uses data augmentations on static videos to simulate basic camera movements (only pan and zoom movements) and employs Fourier embedder and temporal cross-attention layers to inject camera poses. It cannot generalize to more complex camera movements such as Anti-clockwise. Therefore, the development of this field is constrained by the scarcity of large-scale in-thewild datasets with camera pose annotations. Joint Motion Control. Based on the preliminary explorations of joint motion control [40, 45], some concurrent works further advance this field. Motion Prompting [11] leverages 2D point tracking results to represent object motion and camera motion. Perception-as-Control [7] and DaS [12] further capture 3D point tracking results by SpatialTracker [41] to extend the joint control into 3D space. The former denotes objects of reference image as unit spheres with different colors. The latter directly uses the point tracking video as the motion condition. MotionCanvas [42] also measures camera motion by point tracking and renders 2D instance box map as object global motion representation. However, these methods are all designed for the image-to-video generation task which only animates an initial image and cannot plan shooting in 3D space from scratch. SynFMC [33] uses Unreal Engine to render both 6D pose of objects and camera to construct video datasets for joint motion control, but the limited diversity and the domain gap of UE data restrict the models generalizability. To overcome the scarcity of in-the-wild datasets with both 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline to extract 3D bounding boxes and camera trajectories from large-scale video data for learning joint motion control. In addition, we provide an interactive workflow to allow users to intuitively manipulate objects and camera in 3D scene. 3. Method Controllable text-to-video generation (T2V) targets at providing more conditional guidance beyond textual prompts, thereby enabling fine-grained control over the video generation process. We present CineMaster, which aspires to give users level of controllability comparable to professional film directors: allowing precise object placement within scene, flexible manipulation of both objects and camera, and intuitive layout control over each rendered frame. Our proposed CineMaster operates in two stages. In the first stage (Sec 3.1), we propose an interactive workflow for constructing 3D-aware control signals. In the second stage (Sec 3.2), these control signals serve as conditions for T2V models to synthesize the desired video content. Moreover, due to the scarcity of large-scale datasets with 3D bounding box and camera trajectory annotations, we carefully develop an automated data labeling pipeline (Sec 3.3). 3.1. Stage 1: 3D-Aware Control Signals The first stage centers on constructing 3D-aware control signals through user-friendly workflow. Inspired by LooseControl [1], we employ 3D bounding boxes as the principal form of object representation. Users can freely adjust the size and position of these bounding boxes within the 3D scene. By repositioning bounding boxes and camera across keyframes, users gain intuitive control over object and camera trajectories, effectively dictating the motion dynamics. Another key component of our workflow is the preview mechanism, which lets users examine rendered frames after each modification. This workflow closely mirrors real-world filmmaking: directors typically arrange actor and camera movements in multiple takes, reviewing footage on monitors to refine the final shots. Once satisfactory rendering effects are achieved, we export camera trajectories and per-frame projected depth maps for use in the subsequent stage. The primary advantage of this system is its 3D-native and intuitive nature. We implement the interactive system using the open-source engine Blender, where users select keyframes for object and camera placement. The system then automatically interpolates trajectories for intermediate frames, providing seamless and efficient workflow for complex scene setup. 3.2. Stage 2: Conditional Video Generation We condition base text-to-video model on the control signals derived from the first stage. Crucially, beyond using the camera trajectory and object labels as inputs, we also introduce projected depth maps of each frame as augmented visual condition. These depth maps explicitly encode the desired 3D layout, providing strong guidance for the diffusion model to generate accurate video content. To effectively integrate these additional inputs into the T2V model, we design two key components: semantic layout injection module and camera adapter, as illustrated in Fig. 3. Base Model. Our model is developed upon pretrained text-to-video foundation model, which consists of 3D Variational Auto-Encoder (VAE) [19], T5 encoder [28] and transformer-based latent diffusion model [5, 27]. Each basic transformer block is instantiated as sequence of 2D spatial self-attention, 3D spatial-temporal self-attention, text cross attention and feed-forward network (FFN). The text prompts are encoded as ctext by T5 encoder to guide the generation model. We define straight forward path between clean data z0 and noised data zt at timestep with Rectified Flow [9]: zt = (1 t)z0 + tϵ, (1) where ϵ (0, I). The denoising process is defined as mapping from zt to z0 by an ordinary differential equation (ODE): dzt = vΘ(zt, t, ctext)dt, (2) where the velocity is parameterized by the weights Θ of the denoising network. The training process is supervised by Conditional Flow Matching [23] to regress velocity: LLCM = Et,ϵN (0,I),z0 (cid:2)(z1 z0) vΘ(zt, t, ctext) (cid:3) 2 (3) Semantic Layout ControlNet. We design Semantic Layout ControlNet to integrate the 3D spatial layouts from the projected depth maps with semantic information from perentity class labels. Specifically, we copy N/2 DiT blocks from the base model to form the DiT-based ControlNet. Projected depth maps could represent the spatial layout of scene, but when multiple subjects appear in text prompts, it is difficult to specify the position of each entity only by text prompts. Therefore, we use text encoder to represent entity class labels of the input video as text embeddings RnL, and fuse these embeddings to the corresponding position specified by the downsampled entity mask RF HW 1 as shown in Fig. 3(b). denotes the dimension of text embedding. and denote the spatial size of the latent encoded by VAE, and is the number of channels. The 3D VAE latents of projected depth maps RF HW are concatenated with the semantic embeddings along channel dimension and fused by MLPs. The hidden states of ControlNet are incorporated into the base model to provide semantic layout guidance for the generation process. While the proposed Semantic Layout ControlNet enables precise control over the 3D position of each entity in generated videos, relying exclusively on 3D bounding boxes might introduce ambiguity between object and camera movements. For instance, if the bounding box of balloon shifts upward, it could indicate that the ballon is rising, the camera is moving downward, or both. To resolve this ambiguity, we additionally inject explicit camera poses into the generation process, allowing the model to distinguish object motion from camera trajectories more reliably. Figure 4. Dataset Labeling Pipeline. We propose data labeling pipeline to extract 3D bounding boxes, class labels and camera poses from videos. Our pipeline consists of four steps: 1) Instance Segmentation: Obtain instance segmentation results from the foreground in videos. 2) Depth Estimation: Produce metric depth maps using DepthAnything V2. 3) 3D Point Cloud and Box Calculation: Identify the frame with the largest mask for each entity and compute the 3D point cloud of each entity through inverse projection. Then, use the minimum volume method to calculate the 3D bounding box for each entity. 4) Entity Tracking and 3D Box Adjustment: Access the point tracking results of each entity and calculate the 3D bounding boxes for each frame. Finally, project the entire 3D scene into depth maps. Camera Adapter. In each DiT block, we incorporate the camera condition via residual connection situated between the self-attention module and temporal-attention module as shown in Fig. 3(c). Specifically, each camera pose is represented by 3 3 rotation matrix and 3 1 translation matrix. We take sequence of camera pose RT = {RT0, RT1, , RTF 1} RF 12 as input, where is the length of latent frames. An MLP first aligns the dimension of RT to the token length C. We then add the camera pose features to each token in the hidden states. These fused hidden states are fed into the self-attention module to inject camera motion information. In addition, the camera pose features are introduced again via the residual connection. By leveraging this camera adapter, the proposed CineMaster could support joint control over object motion and camera motion. 3.3. Dataset Labeling Pipeline There is lack of large-scale video datasets with 3D bounding box and camera pose annotations. To train the second-stage network, we design an automated data labeling pipeline as shown in Fig. 4. It takes an in-the-wild video as input and extracts the required class labels, camera trajectories and projected depth maps. Class Labels. To obtain the class labels for objects present in the scene, we perform instance segmentation for each entity in the video. To achieve open-set instance segmentation, we combine Grounding DINO[24] with SAM 2 [29], where Grounding DINO produces 2D bounding boxes guided by entity descriptions. To enhance foreground entity detection, we utilize the multi-modal large model Qwen2 [43] to generate entity descriptions as guidance for Grounding DINO. This process yields 2D bounding boxes and class labels for each entity in the first frame, which then guides SAM 2 for video segmentation. To address potential issues with overlapping boxes and incorrect class labels from Grounding DINO, we implement crucial post-processing steps: box IOU filter and feature similarity verification between the regions within 2D boxes and their assigned class labels. Camera Trajectories. We employ the SOTA camera pose estimation model MonST3R [47] to obtain camera trajectories throughout the video sequence. Projected Depth Maps. We employ DepthAnything V2 [44] to generate metric depth maps for the entire video sequence, which are essential for the subsequent inverse projection process. The third step involves inverse projection to obtain 3D boxes for each entity. We operate under the assumption that each entity maintains constant volume in the 3D scene. To address cases where entities may appear partially in certain frames, we identify the optimal frame index for each entity, typically when the entity is most completely visible, to ensure accurate inverse projection and adequate volume representation. For each entity, we combine the instance segmentation mask with the corresponding metric depth map at this optimal frame to generate 3D point cloud, from which we derive the minimal-volume 3D bounding box. Following the establishment of maximum 3D boxes for all entities, the final step involves computing temporalspatial transformations of these boxes within the 3D scene for all frames. We conduct 3D point tracking by SpatialTracker [41] starting from the optimal frame (j) of jth object to the rest frames {..., (j) 1, (j) + 1, ...}, and the average inter-frame displacements {xij, yij, zij} of all feature points from each object are regarded as the spatial movement of the jth objects 3D box where ij Figure 5. We present three different feature comparisons: moving object & static camera, static object & moving camera and moving object & moving camera. We transform our 3D box condition to object trajectories for MotionCtrl [40] and 2D bounding box sequences for Direct-A-Video [45] to align the input conditions. In comparison, CineMaster could better control object motion and camera motion separately or jointly to generate diverse user-intended scenes. {..., (j) 1, (j) + 1, ...} denotes ith frame of jth object. Then we can compute the 3D boxes of all objects in all frames. To represent 3D boxes as explicit control signals, we further project the constructed 3D boxes into image space and render depth maps. 4. Experiments 4.1. Experimental Setup Training Paradigm. We design dedicated training strategy consisting of three stages, i.e., 1) training DiT-based ControlNet on dense depth maps, 2) adapting ControlNet to 3D box datasets, and 3) jointly training Semantic Layout ControlNet and Camera Adapter. Specifically, following LooseControl [1], we first train our DiT-based ControlNet on 167K videos crawled from the Internet with dense depth maps labeled by DepthAnything V2 [44]. Subsequently, we use our data annotation pipeline to construct 3D box dataset with 156K videos and 118K images for training Semantic Layout ControlNet. The images are collected from COCO [22] and Object365 [31] which provide more categories and precise instance segmentation annotations. With the metric depth maps measured by DepthAnything V2 [44], we could obtain the 3D boxes of image datasets by calculating the 3D box with minimal volume for each object in the point cloud. By the image-video joint training, we integrate the spatial layout and semantic information into the Semantic Layout ControlNet which could guide the base model to generate box-aligned videos with specified class labels. We further annotate 99.6K videos out of the 156k videos using our proposed pipeline to obtain camera poses. We also utilize RealEstate10K [50] dataset which features larger camera motion, resulting in 10.4K data samples. We sample data between our dataset and Real-Estate10K with 3:1 ratio for training to enhance the learning of larger camera motion. Based on the merged video dataset with both 3D box and camera pose annotations, we train the Semantic Layout ControlNet and Camera Adapter jointly and master the joint controllability of object motion and camera motion for flexible customized video generation. Implementation Details. We train CineMaster based on our internal text-to-video generation model with 1B parameters for research purposes. Following NaViT [8], we pad the videos to the same shape for each batch managed by attention masks during training. Each training video segment contains 77 frames (i.e., 5 seconds) sampled with 15 frames per second (fps). We use Adam optimizer [20] and train on 24 NVIDIA A800 GPUs, with batch size of 4 and learning rate of 5 105. The three stages of the training process consist of 12,000, 7,000, 6000 steps respectively. During inference, we set the scale of classifier-free guidance [16] as 12.5 and the DDIM [35] steps as 50. We make Table 1. Quantitative comparisons with baselines. indicates higher is better, while indicates that lower is better. The best result is shown in bold. Our CineMaster outperforms previous SOTA baselines on all metrics. Table 2. Ablation study for training paradigms. Details of each setting are introduced in Sec 4.3. Overall, the setting of Joint Train (our final version) achieves the best performance on all metrics than other variants. mIoU Traj-D FVD FID CLIP-T mIoU Traj-D FVD FID CLIP-T Depth-D MotionCtrl Direct-A-Video 0.332 0.551 Ours 94.82 2163.0 201.6 0.302 83.53 1966.3 183.5 0.273 66.29 1530.9 175.9 0.321 trade-off between object motion and camera motion by injecting semantic layout information and camera poses with 25 and 15 steps respectively. Baselines. We compare CineMaster with existing SOTA methods MotionCtrl [40] and Direct-a-Video [45] which could also control object motion and camera motion simultaneously. To align with different input requirements, we convert our 3D box condition into object trajectories for MotionCtrl and 2D bounding box sequences for Direct-AVideo. In addition, we empirically align the coordinate system and scale of the input camera poses for comparison. 1) Object-box alignment: We Evaluation Metrics. use Grounding DINO [24] to detect 2D object boxes in generated videos to measure the mean Intersection over Union (mIoU) and measure the trajectory deviation (TrajD) by calculating the difference of the center points against ground truths. In addition, to evaluate the depth control accuracy of the generated objects, we calculate the average depth of the object regions in each generated frame using SAM 2 [29] and DepthAnything V2 [44], and measure the depth deviation (Depth-D) by the Root Mean Squared Error (RMSE) with the depth values of the given 3D boxes. 2) Video quality: We employ Frechet Video Distance (FID) [36], Frechet Inception Distance (FID) [25] and CLIP Similarity (CLIP-T) to evaluate the appearance of generated results. 4.2. Comparison with Other Methods Qualitative Comparison. As shown in Fig. 5, we show three different feature comparisons: moving object & static camera, static object & moving camera and moving object & moving camera. In the first setting, MotionCtrl [40] moves the camera rightward to align the object trajectory, but fails to either maintain the camera stationary or make the object move. It shows that there is still the camera motion and object motion coupling issue in MotionCtrl. In the third setting, since MotionCtrl is unable to associate multiple trajectories with their respective objects, it generates the McLaren that appears to follow the trajectory of the person and fails to generate the person. DirectA-Video [45] presents low-quality textures for generating bus and rock which demonstrates that its control disIt exhibits weaker camera turbs the generation quality. w/o stage 1 0.544 w/o semantic 0.391 Isolated S,C 0.480 Fix train 0.545 0.551 Joint Train 72.27 176.4 1576.1 0.310 83.81 177.4 1622.2 0.304 70.53 180.2 1840.9 0.313 68.15 177.8 1673.9 0.317 66.29 175.9 1530.9 0.321 0.725 0.717 0.705 0.702 0.685 movement and box alignment, and produces unexpected shot changes and more artifacts in the third setting. In comparison, the proposed CineMaster performs the best control performance for the control of object motion and camera motion in three settings. Quantitative Comparison. In addition, we further report the quantitative comparison in Table 1. Since MotionCtrl only uses point trajectory sequences to specify the positions of generated objects and ignores the spatial size, we do not calculate its mIoU. It does not explicitly associate multiple trajectories with their respective objects and suffers from the camera motion and object motion coupling issue. Therefore, it obtains unsatisfactory Traj-D. DirectA-Video only trains for learning camera movement, and controls the object motion by spatial cross-attention modulation with associated object words and box trajectories to guide the spatial-temporal placement of objects only for inference. The training-free object motion control biases the vanilla inference distribution, resulting in the degradation of generation quality. In addition, no joint training of object motion and camera motion control leads to gap between training and inference, so it obtains weak mIoU and TrajD. In contrast, we construct the video dataset with both 3D box and camera pose for joint training, and the proposed CineMaster could harmoniously control the object motion and camera motion simultaneously. Therefore, CineMaster outperforms previous SOTA methods on all metrics. In particular, we achieve significantly higher mIoU and TrajD, indicating that our framework can generate videos that better follow the users spatial design. 4.3. Ablation Study We experiment with different training paradigms to validate the effectiveness of the delicate designs in our workflow: w/o stage 1: without training the DiT-based ControlNet on dense depth maps, the training process starts directly from the second training stage. w/o semantic: without semantic injector, this setting does not specify the class labels of 3D boxes. Isolated S,C: Semantic Layout ControlNet and Camera Adapter are trained separately and used together for inference. Fix train C: this setting first trains Semantic Layout ControlNet to convergence, then freezes its weights and trains the Camera Adapter. Joint Train (our final version): Semantic Layout ControlNet and Camera Adapter are trained simultaneously. As shown in Table 2, the setting of w/o stage 1 lacks the fine-grained perception for depth control signal, so it obtains mediocre Depth-D. The positions of the generation objects can only be specified via text prompt in the setting of w/o semantic, resulting in the poor mIoU, Traj-D and CLIP-T. The setting of Isolated S,C faces discrepancy between training and inference phases, as the Semantic Layout ControlNet and Camera Adapter are trained separately without cross-module communication, resulting in degraded generation quality supported by lower FVD and FID. Although the setting of Fix train trains the camera adapter based on the frozen Semantic Layout ControlNet, it still fails to eliminate the coupling of camera motion and object motion already learned in the frozen Semantic Layout ControlNet, leading to suboptimal FVD and FID. Benefiting from the constructed video dataset with 3D box and camera pose labels, we experimentally observe that the setting of Joint Train could harmoniously integrate the control for camera motion and object motion and perform the best results on all metrics. 5. Limitations and Conclusions Ideally, 3D bounding box can naturally and precisely control the orientation of objects in space. For instance, when we rotate the 3D box of human, it should produce video sequence of human turning around. However, the community currently lacks accurate open-set object pose estimation models. Therefore, we leave this promising functionality as future work. In conclusion, our project stems from the goal of granting users the creation controllability as professional film directors. To this end, we propose CineMaster for highly controllable text-to-video generation. Specifically, we first design 3D-native workflow that allows users to manipulate objects and camera in an intuitive manner. Then we train conditional text-to-video diffusion model to synthesize the user-intended videos. We emphasize the importance of adopting projected depth maps as strong visual control signals. Extensive experiments demonstrate that CineMaster achieves controllable and 3D-aware cinematic video generation."
        },
        {
            "title": "References",
            "content": "[1] Shariq Farooq Bhat, Niloy Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 4, 6 [2] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, and Hongsheng Li. Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690, 2025. 2 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video arXiv preprint diffusion models to large datasets. arXiv:2311.15127, 2023. 2 [4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixartalpha: Fast training of diffusion transformer for phoarXiv preprint torealistic text-to-image synthesis. arXiv:2310.00426, 2023. [6] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Control-a-video: Controllable text-to-video Lin. arXiv preprint generation with diffusion models. arXiv:2305.13840, 2023. 2 [7] Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Perception-as-control: Fine-grained controllable image animation with 3d-aware motion arXiv preprint arXiv:2501.05020, representation. 2025. 2, 3 [8] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024. 6 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 4 [10] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. arXiv preprint arXiv:2412.07759, 2024. 2, 3 [11] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling arXiv video generation with motion trajectories. preprint arXiv:2412.02700, 2024. 3 [12] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. 2, 3 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models, 2023. 2 [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without arXiv preprint arXiv:2307.04725, specific tuning. 2023. 2 [15] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation, 2024. 3 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [18] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8153 8163, 2024. 2 [19] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [20] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [21] Mathis Koroglu, Hugo Caselles-Dupre, Guillaume Jeanneret Sanmiguel, and Matthieu Cord. Onlyflow: Optical flow based motion conditionarXiv preprint ing for video diffusion models. arXiv:2411.10501, 2024. 2 [22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: ComIn Computer VisionECCV mon objects in context. 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Flow matchMaximilian Nickel, and Matt Le. ing for generative modeling. arXiv:2210.02747, 2022. 4 arXiv preprint [24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 5, 7 [25] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans crearXiv preprint ated equal? arXiv:1711.10337, 2017. large-scale study. [26] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. 2 [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 4 [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4 [29] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 5, 7 [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695, 2022. 2 [31] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430 8439, 2019. [32] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-toIn video generation with explicit motion modeling. Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 5 [44] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 5, 6, 7 [45] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object moIn ACM SIGGRAPH 2024 Conference Papers, tion. pages 112, 2024. 2, 3, 6, 7 [46] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory, 2023. 2 [47] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arxiv:2410.03825, 2024. 5 [48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [49] Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented Diffusion Transformer for Video Generation, 2024. arXiv:2407.21705 [cs]. 2 [50] Tinghui Zhou, Richard Tucker, John Flynn, GraStereo magnificaham Fyffe, and Noah Snavely. tion: Learning view synthesis using multiplane images, 2018. 3, 6 ACM SIGGRAPH 2024 Conference Papers, pages 1 11, 2024. 2 [33] Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, and Dacheng Tao. Free-form motion control: synthetic video generation dataset with controllable camera and object motions. arXiv preprint arXiv:2501.01425, 2025. 2, 3 [34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2 [35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [36] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [37] Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang. Easycontrol: Transfer controlnet to video diffusion for controllable generation and interpolation. arXiv preprint arXiv:2408.13005, 2024. 2 [38] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis, 2024. 3 [39] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. VideoComposer: Compositional Video Synthesis with Motion Controllability, 2023. arXiv:2306.02018 [cs]. 2, 3 [40] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. MotionCtrl: Unified and Flexible Motion Controller for Video Generation, 2024. arXiv:2312.03641 [cs]. 2, 3, 6, 7 [41] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, [42] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation, 2025. 3 [43] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang"
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "Kuaishou Technology",
        "The Chinese University of Hong Kong"
    ]
}