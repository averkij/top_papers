{
    "paper_title": "Is There a Case for Conversation Optimized Tokenizers in Large Language Models?",
    "authors": [
        "Raquel Ferrando",
        "Javier Conde",
        "Gonzalo Martínez",
        "Pedro Reviriego"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 4 7 6 8 1 . 6 0 5 2 : r Is There Case for Conversation Optimized Tokenizers in Large Language Models? R. Ferrando, J. Conde. G. Martínez and P. Reviriego, ETSI de Telecomunicación Universidad Politécnica de Madrid 28040 Madrid, Spain"
        },
        {
            "title": "Abstract",
            "content": "The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of token. Therefore, the tokenizer plays an important role in the efficiency of model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, question that immediately arises is whether there is potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus."
        },
        {
            "title": "Introduction",
            "content": "The exponential development and adoption of generative AI requires large and growing amount of computing and energy resources, creating concerns about its sustainability (Blunt and Hiller, 2024). This has placed the energy efficiency of LLMs at the center of the different approaches to reduce the energy dissipation of AI (Stojkovic et al., 2024). In most models, the energy consumed is proportional to the number of tokens in the input and output text (Wilhelm et al., 2025). Therefore, given text, tokenizer that reduces the number of tokens would improve energy efficiency. The design of tokenizers given their importance has been widely studied (Mielke et al., 2021). They are typically optimized to reduce the number of tokens needed for their training corpus by assigning tokens to the most common words or sub-word units using different algorithms (Wang et al., 2019). key observation is that, in many cases, LLMs are used to power chatbots that interact with users such as ChatGPT. In that application, the input text to the LLM and the generated text will be significantly different from their training corpus, which includes books, texts from the Internet, or documents and is not focused on conversations. Therefore, existing tokenizers are not optimum for the most common application of LLMs. The question that arises immediately is how large the benefit of optimizing the tokenizers for conversational applications would be. This paper explores the potential benefits of optimizing tokenizers for conversational text. In more detail, representative group of tokenizers are retrained using publicly available dataset of chatbot conversations and their performance is compared to the original tokenizers on another set of conversations. The results show that in all cases performance improves and for some tokenizers the reduction in the number of tokens reaches more than 10%. This means that relevant energy savings can be obtained when generating text using conversation-optimized tokenizers. However, the results are preliminary and need to be confirmed using other conversational datasets. Additionally, the tokenizer also has an impact on the training phase of LLMs and thus the impact of optimizing the tokenizers for the inference phase using conversations may induce an additional cost in the training phase which also has to be evaluated. Finally, modifying the tokenizer may also affect the LLM performance for different tasks, so this needs to be evaluated. The rest of the work is organized as follows: in section 2 the methodology used in our analysis is presented, followed by the results in section 3 and discussion of their implications in section 4. The paper ends with the conclusion in section 5."
        },
        {
            "title": "2 Methodology",
            "content": "To evaluate the potential benefits of optimizing the tokenizers for conversations, we first select representative group of tokenizers and two relevant datasets: one of chatbot conversations and the other an LLM training corpus. Then we first check that existing tokenizers have worse performance on conversations than on training corpus to confirm our initial hypothesis. In second experiment, we optimize the tokenizers for conversations and evaluate the reduction obtained in the number of tokens. Finally, we also evaluate the performance of conversation-optimized tokenizers in the training corpus."
        },
        {
            "title": "2.1 Tokenizers",
            "content": "Tokenization is the division of the input text of an LLM into predefined linguistic units called tokens, which can be characters, word fragments, or whole words. Each tokenizer has its own tokenization rules and token vocabulary (Mielke et al., 2021). The vocabulary is designed using statistical process that seeks to identify the best subwords to represent text corpus, based on the frequency and distribution of text fragments (Ahmed). The exact way tokens are selected depends on the tokenization algorithm used. The process is completely deterministic: training with the same algorithm and the same corpus always produces the same vocabulary (Face, 2021). To achieve efficient text compression, according to the principles of information theory, it is essential to construct an optimized vocabulary so that the most frequent words in the training corpus are represented with fewer tokens, thereby minimizing the total length of tokenized sequences. Several algorithms effectively implement this approach. Among the most used are Byte Pair Encoding (BPE), WordPiece, and Unigram. For given vocabulary size, each uses specific methodology to segment and prioritize lexical units and add them to the token vocabulary. For evaluation, group of tokenizers used in popular LLMs has been selected. The models considered are summarized in Table 1 and include both large proprietary models from OpenAI and also open-weights models from different companies or open-source initiatives. The main features of their tokenizers are shown in Table 2. This group provides representative set of the tokenizers used in LLMs at the time of writing this paper."
        },
        {
            "title": "2.2 Datasets",
            "content": "Two text corpora are needed for the evaluation, one consisting of chatbot conversations and another that corresponds to text used to train tokenizers. The corpora used are briefly described in the following subsections."
        },
        {
            "title": "2.2.1 Chatbot Conversation Corpus: LMSYS",
            "content": "Chat 1M The LMSYS Chat 1M dataset (Org, 2023), available on Hugging Face (Face), contains one million conversations from anonymous users with 25 different LLMs in multiple languages (Zheng et al., 2023). Each dataset entry includes conversation identifier, the LLM generating the response, the conversation text, the detected language, and moderation tag provided by OpenAIs API. This corpus is ideal for evaluating tokenization in the conversational context, as it consists of real conversations with chatbots. Therefore, it is representative of the type of text processed and generated by language models in conversational applications."
        },
        {
            "title": "2.2.2 LLM Training Corpus: C4 by AllenAI",
            "content": "There are numerous open-access datasets for training LLMs. Some of the most widely used include Common Crawl (Common Crawl), The Pile (Gao et al., 2020), and OpenWebText (Gokaslan and Cohen, 2019). All contain vast amounts of text, ranging from tens to hundreds of gigabytes. For example, The Pile is approximately 800 GB. For tokenization evaluation purposes, using such large corpus is unnecessary and computationally costly. Therefore, one of the criteria for selecting the most suitable training text corpus has been the ability to easily generate small representative subset. The accessibility of the dataset through platforms like Hugging Face, which allow easy and efficient download and data manipulation, has also been considered. According to these criteria, the chosen dataset is the Colossal Clean Crawled Corpus (C4) (Raffel et al., 2020), developed by the Allen Institute for AI (AllenAI) (Allen It is clean version of Institute for AI, 2024). Model GPT-4 Organization Description OpenAI GPT-4o OpenAI DeepSeek-R1 DeepSeek LLaMA-3.1-8B Meta Gemma-2-9b Google Mistral-7B-v0.1 Mistral AI BigScience BLOOM Phi-4 Microsoft Reference model in chatbot applications, with the highest number of active users to date (OpenAI, 2023a; Singh, 2025). Latest and fastest model, optimized for performance. Available for free but with usage limits (OpenAI, 2024). Model optimized for reasoning tasks and widely adopted (DeepSeek-AI, 2024). Lightweight and fast model, prominent example of the latest generation of open models (Llama, 2024). Lightweight and open alternative with good performance and focus on safety and efficiency (Schmid et al., 2024). High-capacity model for its small size (AI). Open multilingual model, designed with collaborative and responsible approach (Workshop and et al., 2023). Lightweight model oriented to reasoning tasks, recently presented by Microsoft (Microsoft, 2024). Table 1: Selected models for evaluation of their tokenizers Model GPTVocabulary Size (tokens) 100, 000 GPT-4o 200, 000 DeepSeek-R1 128,815 LLaMA-3.1-8B 128,000 Gemma-2-9b 256,000 Mistral-7B-v0.1 32,000 BLOOM 250,680 Phi-4 100,352 Tokenization Features Byte-Level BPE algorithm. Tokenizer available in tiktoken (OpenAI, 2023b). The token vocabulary is called \"cl100k_base\", but its not accessible, as with the tokenizer configuration details. Also used by GPT-3.5 and GPT-3.5 turbo models. Optimized version of the GPT-4 tokenizer, with an expanded vocabulary called \"o200k_base\". Used in reasoning models o1 and o3 (OpenAI, 2023b). Byte-Level BPE algorithm. In Hugging Face, it is implemented using the same structure as the LLaMA tokenizer. Also used by DeepSeek-V3. Byte-Level BPE. The same tokenizer is used across all models in the LLaMA 3 series, including versions 3.1, 3.2, and 3.3. Character-level BPE, including byte-fallback mechanism to avoid outof-vocabulary tokens. It was trained using SentencePiece (Kudo, 2018). All models in the Gemma1, Gemma2, TXGemma, and CodeGemma families share the same tokenizer. V3 tokenizer (Mistral, 2024), shared by the following sets of models: open-mixtral-8x22b, mistral-large-latest, mistral-small-latest, openmistral-7b. Character-level BPE with byte-fallback mechanism, implemented using SentencePiece. Byte-Level BPE. It was developed with multilingual focus. It was trained on subset of the ROOTS dataset, preserving the same language distribution as the model training data. The vocabulary size was chosen to ensure suitable fertility across languages and is multiple of 128 to enhance GPU efficiency and parallelism (Workshop and et al., 2023). Implemented with the same scheme as OpenAIs Open Source model, GPT-2. It has very similar or identical token vocabulary size to GPT4. It has been verified that tokenizing several sequences with both the Phi-4 and GPT-4 tokenizers produces almost the same token sequences, indicating both tokenizers are almost identical. Table 2: Comparison of Models Vocabulary and Tokenization Features the Common Crawl, removing duplicate content, low-information pages, spam, and other unwanted elements. The result is an extensive corpus representative of modern web language, widely used in the training and evaluation of language models. Therefore, it is good representation of the text used to train tokenizers."
        },
        {
            "title": "2.3.2 Metrics\nThe main performance metric in our evaluation is\nthe number of tokens needed for a given text. When\nthe text is the same and we are comparing the origi-\nnal and conversation-optimized tokenizers the ratio\nof tokens needed by each tokenizer is used to mea-\nsure the gain or loss introduced by optimizing the\ntokenizer. However, when we want to understand\nthe relative performance of a tokenizer on different\ntypes of texts, a ratio of tokens can not be used\nas the sizes of the texts are different. In this case,\nfertility (Rust et al., 2021) defined as the number of\ntokens per word in a given text is used, the metric\nis defined by:",
            "content": "F ertility ="
        },
        {
            "title": "Ntokens\nNwords",
            "content": "(1) and values closer to one correspond to more efficient tokenization of the text."
        },
        {
            "title": "3 Results and Analysis",
            "content": "The evaluation is performed in three phases. In the first experiment, we check that current LLM tokenizers perform better on the LLM training corpus than in the conversational dataset. In the second experiment, we design conversation-optimized tokenizers and evaluate the reduction in the number of tokens for the conversational dataset. Finally, in the last experiment, we measured the performance of conversation-optimized tokenizers in the LLM training corpus to understand the impact of optimization for other LLM applications. The results for each of the experiments are presented in the following subsections. The raw data and the scripts to reproduce the experiments are available in public repository1."
        },
        {
            "title": "3.1 Experiment 1: Performance of current\ntokenizers in conversations versus\ntraining corpus",
            "content": "The results in terms of fertility of the different tokenizers in the LLM training corpus and the chatbot conversational dataset are summarized in Figure 1. To ensure more reliable comparison, both the training and conversational texts are in English. It can be observed that for all the models the fertility is lower on the training dataset than on the conversations. This confirms our intuition that tokenizers designed with LLM training corpora are not optimum for conversational applications. For conversations, the fertility is also computed on the user inputs only and on the chatbot responses only. It can be seen that the fertility is lower for the chatbot responses than for the user questions, which can be due to the nature of the text in the responses, but also to the fact that LLMs may tend to create text that is aligned with their tokenizers. Finally, the overall fertility for conversations is closer to that of the responses, this is because responses account for the majority of the text in the conversations."
        },
        {
            "title": "3.2 Experiment 2: Performance of",
            "content": "conversation-optimized tokenizers: improvements for conversations To assess the potential benefits of optimizing LLM tokenizers for conversations, for each model, three optimized tokenizers are constructed using the user inputs only, the chatbot outputs only, and both. Then they are used to tokenize the test set, and the reduction versus the original tokenizers is computed. The results are summarized in Figure 2. It can be observed that all three tokenizers reduce the number of tokens but savings are larger when optimizing for the entire conversations or only for the output (which accounts for the majority of the words). Therefore, in order to maximize the savings, it is better to optimize the tokenizer for the 1https://github.com/RaquelFerrando/ conversational_tokenizers.git Figure 1: Fertility on the LLM training dataset (C4) and on the conversational dataset (LMSYS). For the conversational dataset fertility is computed on the input (user), output (assistant) and entire conversations. entire conversations. The reductions vary across models. Savings are approximately 5% for DeepSeek-R1, Llama3-1-8B and Phi-4, while for Gemma-2-9B, Mistral-7B and Bloom they exceed 10%. It is important to note that the test set is multilingual, and the language distribution of the conversations may also influence the observed token reductions. Since tokenization efficiency can vary significantly between languages, the reductions are not uniform across all of them. However, languagewise analysis shows that the languages that are well represented in the dataset generally benefit from using the conversation-optimized tokenizers, as shown in Figure 3. This is confirmed by the poor performance of DeepSeek in Chinese, for which the number of tokens increases when the tokenizer is optimized for conversations. This is probably due to the training set having larger fraction of Chinese than the conversational data set that only has 2.4% of the conversations in Chinese."
        },
        {
            "title": "3.3 Experiment 3: Performance of",
            "content": "conversation-optimized tokenizers: loss for training corpus In the last experiment, we ran the tokenizers optimized for the conversational dataset on the LLM training corpus and computed the increase in the number of tokens compared to the original tokenizers. The results are summarized in Figure 4. Surprisingly, three of the models, Mistral-7B, Gemma-2-9B, and Bloom, show reduction in the number of tokens: approximately 1% for Mistral7B and around 5% for both Gemma-2-9B and Bloom. The fact that tokenizers optimized for conversations also lead to reduction in token count on the training corpus may suggest that the original tokenizers are sub-optimal. These reductions are, in all cases, smaller than those obtained on the conversational corpus. The same three models were also the ones showing the largest reductions for conversations. This might indicate that the improvements observed in conversations are partly due to the specific optimization for that domain, and partly due to more general inefficiencies in the original tokenizers, even outside of conversational data. For the rest of the models, DeepSeek-R1, Llama3-1-8B, and Phi-4, the number of tokens is slightly increased, but the values are below 2%. The best performance is achieved when optimizing the tokenizer for the entire conversations. Overall, it seems that optimizing the tokenizer for conversations does not have large impact on the performance on the LLM training text."
        },
        {
            "title": "4 Discussion",
            "content": "The analysis and results show that current LLM tokenizers are not optimal for chatbot applications and that the computational and energy costs could be reduced by designing conversation-optimized tokenizers. This observation is particularly relevant given the growing importance of energy efficiency in AI. As LLMs are increasingly deployed in real-time conversational settings (Kumar, 2025), optimizing tokenization for this use case presents an opportunity to reduce resource consumption during inference, which accounts for significant portion of the long-term energy footprint of AI systems, likely Figure 2: Reduction on the number of tokens for the conversation-optimized tokenizers on the conversational test set (LMSYS). Figure 3: Token reduction achieved by conversation-optimized tokenizers on the top 10 languages in the LMSYS dataset (i.e., languages with over 1,000 conversations in the test corpus). larger than that of training (Samsi et al., 2023). However, there are several considerations to keep in mind. While optimizing for conversational text may lead to energy savings during inference in chatbot applications, it could slightly increase the cost of training. Moreover, tokenization impacts not only the efficiency but also the effectiveness of training. tokenizer optimized for conversations might influence the learning process in ways that affect the models downstream performance. Overall, our findings highlight promising direction for improving the efficiency of LLMs in real-world applications in simple way. Tokenizer design, often overlooked compared to model architecture or training data, can have measurable impact on energy consumption. Revisiting this layer with application-specific constraints in mind could unlock new dimension of energy-aware optimization for language models. Still, such changes should be approached with care, as they may introduce trade-offs that need to be carefully evaluated."
        },
        {
            "title": "5 Conclusion and future work",
            "content": "This paper has explored the potential of optimizing tokenizers for chatbot conversations as simple strategy to improve the efficiency of Large Language Models (LLMs) during inference. We show that current tokenizers perform suboptimally in conversational contexts, one of the most prevalent use cases of LLMs. By retraining tokenizers on Figure 4: Increase on the number of tokens for the conversation-optimized tokenizers on the LLM training corpus (C4). representative dataset of chatbot dialogues, we observed consistent reductions in the number of tokens required, with savings ranging from 5% to over 10% depending on the model. These reductions can translate into meaningful computational and energy savings, particularly in large-scale deployments where inference dominates the energy footprint of AI systems. Nonetheless, the tokenizer affects how information is represented and learned during pretraining, and conversation-optimized tokenizers may result in representations that are less effective. Therefore, future work should carefully evaluate the impact on downstream performance to ensure that gains in inference efficiency do not come at the expense of model quality, generalization, or versatility. Further research should also extend the analysis to broader set of conversational corpora, as well as to other types of texts, to evaluate in more detail the performance of the tokenizers. Finally, incorporating tokenization considerations into the training process itself, rather than treating them as fixed preprocessing steps, could open new avenues for co-designing tokenizers and model architectures tailored to specific use cases."
        },
        {
            "title": "Limitations",
            "content": "when the LLM is used to generate text. To have comprehensive analysis of potential energy savings, the LLM training phase should also be considered. 3. The potential impact of tokenization on LLM performance is not considered in the paper as it would require training the LLMs which is computationally unfeasible for research group. However, previous work has shown that the impact is limited in many cases (Ali et al., 2024). 4. The language distributions of the original tokenizers training corpora are unknown. mismatch between these and the language distribution of the corpus used in this work to train the tokenizers may lead to some languages being favored while others are penalized. 5. As with natural languages, programming code tokenization might be favored or penalized depending on each tokenizers original vocabulary and the amount of code in each programming language present in the conversations used to re-train them. The work presented in this paper has the following limitations: 1. Only one dataset is used for conversations, and only one training corpus. Additional and larger datasets should be evaluated to ensure that the results do not depend on the dataset. 2. The study only considers the inference phase"
        },
        {
            "title": "References",
            "content": "Sahin Ahmed. Decoding tokenization strategies for large language models (llms). Mistral AI. Frontier AI. For all of us.. Obtenido de: https://mistral.ai/about. Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lübbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper Schulze Buschhoff, et al. 2024. Tokenizer choice for LLM training: Negligible or crucial? In Findings of the Association for Computational Linguistics: NAACL 2024, pages 39073924. Association for Computational Linguistics. Allen Institute for AI. 2024. Allen institute for ai (ai2). https://allenai.org/. Accessed: 2025-04-11. Katherine Blunt and Jennifer Hiller. 2024. Big techs latest obsession is finding enough energy. Accessed: 2024-06-08. Common Crawl. Common crawl: Open data for web research. DeepSeek-AI. 2024. Deepseek llm: Scaling opensource language models with longtermism. arXiv preprint arXiv:2401.02954. Hugging Face. Hugging face - the ai community building the future. Hugging Face. 2021. Transformers. Tokenizers. https://huggingface.co/learn/nlp-course/ chapter2/4?fw=pt. NLP course: Using de: Obtenido Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An 800gb dataset of diverse text for language modeling. https://pile.eleuther.ai/. Accessed: 2025-0411. Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus. Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6675, Melbourne, Australia. Association for Computational Linguistics. Naveen Kumar. 2025. 65 chatbot for tics https://www.demandsage.com/chatbot-statistics/. 2025 new data statisreleased. Llama. 2024. Llama 3.1. Accessed April 2025. Microsoft. 2024. Introducing phi-4: Microsofts newest small language model specializing in complex reahttps://huggingface.co/microsoft/ soning. phi-4. Sabrina Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Lee, Benoît Sagot, et al. 2021. Between words and characters: brief history of open-vocabulary modeling and tokenization in nlp. arXiv preprint arXiv:2112.10508. Mistral. 2024. Tokenization guide mishttps://docs.mistral.ai/guides/ tral. tokenization/#what-is-tokenization. OpenAI. 2023a. Gpt-4. Accessed April 2025. OpenAI. 2023b. tiktoken: fast bpe tokenizer for openai models. https://github.com/openai/ tiktoken. Accessed April 2025. OpenAI. 2024. Introducing gpt-4o and more tools to chatgpt free users. LMSYS Org. 2023. Lmsys chat 1m: dataset of one million conversations with llms. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. C4: Colossal clean crawled corpus. https://huggingface. co/datasets/allenai/c4. Accessed: 2025-04-11. Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. 2021. How good is your tokenizer? on the monolingual performance of multilingual language models. Preprint, arXiv:2012.15613. Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pages 19. Philipp Schmid, Omar Sanseviero, Pedro Cuenca, and Lewis Tunstall. 2024. Welcome gemma 2 - googles new open llm. Accessed April 2025. Shubham Singh. 2025. Chatgpt statistics. https:// www.demandsage.com/chatgpt-statistics/. Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, and Josep Torrellas. 2024. Towards greener llms: Bringing energy-efficiency to the forefront of llm inference. arXiv preprint arXiv:2403.20306. Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2019. Neural machine translation with byte-level subwords. Preprint, arXiv:1909.03341. Patrick Wilhelm, Thorsten Wittkopp, and Odej Kao. 2025. Beyond test-time compute strategies: Advocating energy-per-token in llm inference. In Proceedings of the 5th Workshop on Machine Learning and Systems, pages 208215. BigScience Workshop and Teven Le Scao et al. 2023. Bloom: 176b-parameter open-access multilingual language model. Preprint, arXiv:2211.05100. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. 2023. Lmsys-chat1m: large-scale real-world llm conversation dataset. Preprint, arXiv:2309.11998."
        }
    ],
    "affiliations": [
        "ETSI de Telecomunicación Universidad Politécnica de Madrid 28040 Madrid, Spain"
    ]
}