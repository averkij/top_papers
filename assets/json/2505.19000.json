{
    "paper_title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization",
    "authors": [
        "Yunxin Li",
        "Xinyu Chen",
        "Zitao Li",
        "Zhenyu Liu",
        "Longyue Wang",
        "Wenhan Luo",
        "Baotian Hu",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs' capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPO's expansive search and DPO's targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 0 0 9 1 . 5 0 5 2 : r VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization Yunxin Li1, Xinyu Chen1, Zitao Li1, Zhenyu Liu1, Longyue Wang2, Wenhan Luo3 Baotian Hu1, Min Zhang1 1Harbin Institute of Technology, Shenzhen, China 2Alibaba International Group, 3Division of AMC and Department of ECE, HKUST liyunxin@stu.hit.edu.cn, {hubaotian, zhangmin2021}@hit.edu.cn Project Link: https://github.com/HITsz-TMG/VerIPO"
        },
        {
            "title": "Abstract",
            "content": "Applying Reinforcement Learning (RL) to Video Large Language Models (VideoLLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance. To address these limitations, we propose VerIPO, Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs capacity for generating deep, longterm reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPOs expansive search and DPOs targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability."
        },
        {
            "title": "Introduction",
            "content": "Complex reasoning problems across various domains are often effectively tackled by large models via generating long Chain-of-Thoughts (CoTs) [72, 94, 85, 13, 31], which has demonstrated considerable success in multimodal settings, particularly for challenging tasks like visual math and complex image-text reasoning [71, 15, 61, 75, 80, 78]. The activation of this long-form reasoning capability in Large Multimodal Models (LMMs) has been primarily driven by Reinforcement Fine-Tuning (RFT), which combines Supervised Fine-Tuning (SFT) with Long-CoTs data and the application of online reinforcement learning algorithms [57, 52, 50, 84, 20, 76, 58, 65], such as the Group Relative Policy Optimization (GRPO) [54] method. Inspired by the success of DeepSeek-R1 [13], Skywork R1V Equal Contribution, Corresponding Author, Project Leader Preprint. Under review. Figure 1: Figures (A, D): Initial GRPO training with different data types shows only utilizing Video-QA data decreases response length. Figures (B, E): Continual GRPO training with/without Verifier-guided DPO (VerIPO) demonstrates VerIPO improves accuracy and response length. Figure (C): Inconsistency rate (thinking vs. final answer) at different stages reveals our method lowers contextual inconsistency of long CoTs while GRPO increases it. Figure (F): Performance on challenging video reasoning dataset VSI-Bench [81] shows VerIPO (trained with Qwen2.5-VL-7B) outperforms strong LMMs including GPT-4o [23], Video-R1 [18], and Kimi-VL [61]. [10], and Vision-R1 [22], researchers [28, 18, 90] are actively investigating effective solutions to enhance the long reasoning capabilities of Video Large Language Models (Video-LLMs), aiming to train them to produce effective, long-form reasoning chains for complex video reasoning tasks. However, activating the long-form reasoning capability of Video-LLMs faces two challenges: Data Preparation Bottleneck: Employing Long-CoTs video datasets for cold starting (e.g., VideoR1 [18]) is hindered by the high cost of manual annotation and noise from automatic methods. Limitations and Instability of Online RL: First, while improving overall performance compared to SFT [28], direct RL training on Video-LLMs often yields short, shallow reasoning chains that lack deep logical steps, consistent with prior findings [90]. Second, GRPO is prone to making models that often produce contextually inconsistent reasoning chains (Figure (C)), e.g., correct answers based on wrong thinking. Third, RL training does not consistently improve the accuracy or length of CoTs, particularly with increased temporal frame inputs, and online training does not ensure stable increase in response length [90] (Figure (D)). Finally, models with long reasoning processes (e.g., Video-R1) show inconsistent performance gains across video reasoning tasks, occasionally performing worse than direct inference by instruction-tuned models. Hence, substantially improving the deep reasoning capability of Video-LLMs still remains an open challenge. To address these limitations, we propose VerIPO, an online rollout-aware Verifier-guided Iterative Policy Optimization algorithm. Our approach forgoes large Long-CoT SFT datasets for cold starting, instead directly applying RL to cultivate long reasoning capability in Video-LLMs gradually. Specifically, the rollout-aware Verifier, core component of VerIPO, is strategically positioned between the GRPO and DPO training phases to form the GRPO-Verifier-DPO loop. This verifier leverages small LLMs to critically assess the reasoning quality and contextual consistency of generated CoTs. Through this quality assessment, the verifier intelligently selects high-quality contrastive samples, facilitates the construction of reflective and contextually consistent CoTs, particularly based on GRPO rollouts. These contrastive samples are then utilized to train the model via an efficient DPO stage, which was empirically found to be 7x faster than GRPO (as shown in A.1) and support the targeted optimization of the reasoning path compared to GRPO. The verifier can also progressively prune simple training examples that the model has mastered, thus contributing to speeding up the overall training process. During training, we incorporate diverse video QA datasets complemented by high-quality image and textual math datasets used particularly in the initial phase, to enhance the range of logical paths explored by models. We conduct extensive experiments on four video reasoning and two long video understanding benchmarks, e.g., VSI-Bench [81] and Video-MME [19]. Our experimental results show that VerIPO 2 achieves consistent and significant performance improvements and outperforms larger Video-LLMs and powerful RFT models Video-R1 and Kimi-VL-Thinking [61]. It highlights the effectiveness and stableness of VerIPO in cultivating the long-reasoning ability of Video-LLMs. Compared to RFT with long-CoTs dataset as cold start, our approach consistently generates longer responses and improves the quality of generated long CoTs, e.g., contextual consistency and low repetition. Our contributions can be summarized as follows: We propose VerIPO, novel Verifier-guided Iterative Policy Optimization algorithm designed to improve the long reasoning capability of Video-LLMs. The method enhances rollout data utilization via the embedded Verifier system and efficient DPO, enabling the model to realize improvement via effective learning from its online running experience. The rollout-aware Verifier analyzes and refines generated rollout data into high-quality, reflective contrastive samples, which are essential for continuously improving the models long-term reasoning capabilities during the DPO training stage. Experimental results demonstrate that VerIPO significantly improves long reasoning performance on general video QA tasks. Our trained models consistently generate long and accurate reasoning chains, outperforming RL-trained thinking models (Video-R1 and Kimi-VL-Thinking), directanswer models (like Qwen2.5-VL-7B [2] on four benchmarks), and larger latest LMMs (> 11B)."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multimodal Models for Video Reasoning Video reasoning is the core capability of Large Multimodal Models (LMMs), enabling understanding of interactions, dependencies, and inference over dynamic content [29, 30, 90, 97]. Specifically, spatial reasoning models object relationships and scene layouts within frames, while temporal reasoning captures motion, causality, and sequence across frames [46, 12, 51, 34]. Early Video-LLMs focused on short videos using pre-trained image [16, 45, 49] or video encoders [1, 38, 44] with frozen language models [11, 25, 26, 42, 87]. Recent efforts target long-form video understanding with complex temporal and multimodal reasoning [17, 18, 90, 97, 36, 7, 40]. To handle long contexts, methods adopt hierarchical temporal attention and larger context windows [36, 73], or compress visual inputs via event-level abstraction [90, 4]. Recent multimodal fusion integrates audio and motion cues for improved understanding in videos [7, 95, 39]. Reinforcement learning guides perception and reasoning, aiding in interpretability and intent modeling [14, 40, 37]. Recent work explores structured outputs, intention-driven attention, and stepwise reasoning [7, 82, 22, 48] for fine-grained grounding and spatiotemporal segmentation. Reinforcement Learning for Multimodal Reasoning Reinforcement learning (RL) has become pivotal approach for aligning LLMs and LMMs with complex reasoning objectives. Foundational policy optimization algorithms, such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO), have been instrumental in this domain [52, 50, 54]. Further advancements have enhanced training stability and efficiency [84, 20, 76, 58]. critical challenge in popular RL training is the cold start\" problem, where initializing models without prior guidance can lead to suboptimal performance. To mitigate this, Reinforcement Fine-Tuning (RFT) has been proposed, wherein models undergo preliminary SFT on curated datasets to stabilize subsequent RL training phases [39, 92, 57, 55, 5, 28, 69, 41, 71, 79]. Additionally, some verifiers, designed to assess and guide the quality of generated outputs, have proven beneficial. These verifiers assist in filtering and selecting high-quality training samples, thereby enhancing the efficiency and effectiveness of the training process [7, 95, 56, 67]."
        },
        {
            "title": "3 Preliminary",
            "content": "Direct Preference Optimization (DPO) DPO [50] optimizes policy πθ to prefer response y+ over for given input x, with regularization from reference model πref. The core loss function is: LDPO(πθ; πref) = E(x,y+,y)D (cid:20) (cid:18) log σ β log πθ(y+ x) πref(y+ x) β log πθ(y x) πref(y x) (cid:19)(cid:21) , (1) where σ() is the sigmoid function, β > 0 is temperature parameter, and = {(x, y+, y)}N i=1 is static dataset of comparisons sampled from human preference distribution. This can be interpreted 3 as minimizing the binary cross-entropy between pairwise preference label and the log odds induced by the policy relative to the reference. This approach is targeted and fast optimization for models. Group Relative Policy Optimization (GRPO) For given input q, the model generates group of responses {y1, y2, . . . , yG} sampled from the current policy πθ. Each response yi is assigned reward r(yi), typically derived from human feedback or automated evaluation metrics. Following outcome supervision method, the group mean reward µ and standard deviation σ are computed to obtain the advantage score: µ ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 r(yi), σ = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (r(yi) µ)2, ˆAi,t = r(yi) µ σ . (2) With the score computed, GRPO [54] updates the policy by maximizing the following objective: LAdvantage(πθ) = qP(Q),{yi}G i=1πθref (yi,tq,yi,<t)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) (cid:26) t= min (cid:20) πθ(yi,t q, yi,<t) πθref(yi,t q, yi,<t) ˆAi,t, clip (cid:18) πθ(yi,t q, yi,<t) πθref(yi,t q, yi,<t) , 1 ϵ, 1 + ϵ (cid:21)(cid:27) (cid:19) ˆAi,t LGRPO(πθ) = LAdvantage(πθ) β DKL [πθπref] (3) (4) where ϵ is hyperparameter controlling the clipping range, β is the temperature parameter, and πθref is the policy before the update. This approach allows the model to focus on generating responses that are relatively better within group, promoting wide yet slow exploration in the generation space."
        },
        {
            "title": "4 VerIPO: Verifier-Guided Iterative Policy Optimization",
            "content": "4.1 Overview We introduce VerIPO, an iterative policy optimization approach specifically designed to enhance the long reasoning capability of Video-LLMs. The method follows an iterative process: 1) Initial Policy Exploration: We first apply GRPO to the instruction-tuned Qwen2.5-VL, utilizing diverse accuracy rewards tailored for various video task output formats. 2) Sample Curation with Verifier: Verifier component analyzes the GRPO rollouts to produce high-quality, long reasoning paths that lead to accurate answers as positive (chosen) samples. It also selects challenging, incorrect reasoning paths as hard negative (rejected) samples. 3) Policy Refinement with DPO: These curated contrastive samples are then used to fine-tune the model via DPO. The DPO efficiently refines the models policy, encouraging the generation of better reasoning paths in controllable direction. 4.2 GRPO Following the GRPO algorithm from DeepSeek-R1 [13], we employ two types of rewards: accuracy and format. The accuracy reward ra is scaled within the range [0, 1], while the format reward rf is bounded within [0, 0.5]. The calculation of accuracy reward ra depends on the type of question posed in the input prompt. For mathematical questions, we employ Math-Verify2 to parse the answer from the models output and compare it against the ground truth GT , yielding binary reward (1 for correct, 0 for incorrect). Similarly, for multiple-choice questions, ra is assigned value of 1 if the models selected option aligns with the ground truth GT and 0 otherwise. As for distance estimation tasks, we utilize the Mean Relative Accuracy (MRA) metric, as proposed in VSI-Bench [81], which provides continuous reward value between 0 and 1. The format reward rf is binary (0.5 for adherence, 0 for non-adherence), contingent upon whether the models response conforms to the predefined <think>...</think><answer>...</answer> structure. The accuracy rewards are presented as ra = 1 0 RA(Output, GT ) if Qtype {Math, MC} and Answer matches GT if Qtype {Math, MC} and Answer does not match GT if Qtype = Distance Estimation (5) 2https://github.com/huggingface/Math-Verify 4 Figure 2: Overview of VerIPO workflow. This training loop is guided by the Verifiers continuous evaluation and selection of training samples. The optimization process progressively improves the models long reasoning capability by learning from high-quality and informative reasoning examples. To broaden the models exploration capabilities and enhance learning flexibility, we remove the KL divergence during the GRPO training process. Furthermore, we encountered an empirical observation consistent with findings reported in DAPO [84]. As training progressed, the number of samples with an accuracy of 1 continually increased. These samples have an advantage of 0 and result in no gradient for policy updates, which suppressed the gradient signals during the models training process. To mitigate this phenomenon and maintain robust gradient flow, we integrate the online filter strategy[43] to exclude zero-advantage samples from the training batches. 4.3 Rollout-Aware Verifier To address the limitation of outcome-based GRPO in optimizing reasoning paths, we introduce rollout-aware Verifier that analyzes online rollouts to generate high-quality preference data, continuously guiding the model to generate long-term, high-quality reasoning paths. As shown in Figure 2, for given rollout oi, we employ regular expressions to extract both the thought content ri and answer ai. The verifier encompasses four-aspect quality assessment to select high-quality long-CoT samples: Accuracy Check determines the correctness of the extracted answer content, employing the accuracy function used in GRPO. Notably, for distance estimation tasks, an answer is deemed correct if its MRA surpasses threshold of 0.6. Samples that successfully pass these verification stages are considered for constructing the positive sample. Consistency Check evaluates the coherence between the reasoning process and the final answer. It uses the Qwen3-8B to extract the reasoning answer ar from the response ri, given the original question. rule-based method then verifies if ar matches the predicted answer ai to assess reasoning consistency. This checking stage is mainly used to detect the right reasoning with the correct answer and remove the sample with an error reasoning path. Repetition Check applies rule-based methods to detect any sentence-level repetition within the thought content ri, penalizing responses that exhibit repetitive patterns. Length Check is applied on the extracted thinking content ri of rollouts, which assigns higher priority to longer samples for those as positive examples. Following this selection, we construct contrastive pairs for DPO training. Samples are initially categorized based on their average accuracy reward ravg i=1 ai, where refers to the number of sampling per query. This classification guides the data construction process. For training = 1 (cid:80)N 5 Table 1: Training data and hyperparameters across different stages. Stage Algorithm Data Reasoning Activation GRPO Group-Slow-Search GRPO Pair-Fast-Align Group-Slow-Search DPO GRPO Long Document (1k) Math-Text (30k) Reasoning-Image (39K) Science-Image (4K) Spaital-Image (9k) General-Image (10K) VQA-Video (24k) Rollouts of VQA-Video VQA-Video Gloabl Batch Size Rollout Batch Size Learning Rate Rollout Responses per Query Sampling Temperature DPO Beta (β) 128 64 1e-6 8 1.0 - 64 64 1e-6 8 1.0 - 32 - 5e-7 - - 0.1 64 64 5e-7 8 1.0 - samples where the model consistently produces incorrect rollouts (ravg = 0), their high-quality long-term reasoning is generated using Gemini-2.5-Flash. These will help models explore deep reasoning for challenging questions. Conversely, rollouts with perfect accuracy (ravg = 1) are regarded as simple samples and generally excluded from preference pairs during the DPO stage. Then, the contrastive preference dataset is specifically constructed using the following strategies: Single-Turn Preference Pairs: Negative examples are randomly selected from incorrect rollouts. Positive examples are constituted by the longest rollouts that successfully passed all preceding quality and consistency checks, thereby rewarding thorough correct reasoning processes. Repetition Penalty Pairs: To specifically address and penalize repetitive outputs, rejected examples are rollouts flagged by the Verifier for containing sentence-level repetitions. To maintain training stability, these negative samples are truncated to maximum of 1024 tokens. Corresponding chosen examples are the longest verified correct rollouts devoid of such repetitions. Reflective Preference Pairs: For prompts where the model initially struggled (ravg 0.25), these pairs guide the model in self-correcting its reasoning. Rejected examples concatenate two or more distinct incorrect rollouts, as shown in Figure 2. Chosen examples combine initial incorrect rollout sequences with correct one at last, linked by reflective phrases to simulate refined reasoning. Inference Consistency Pairs: This category enhances the alignment between the models reasoning (\"think content\") and the final answer. Rejected examples consist of rollouts where the think content and final answer are incongruent (e.g., incorrect reasoning with correct answer, or correct reasoning with an incorrect answer). Rollouts with correct reasoning but incorrect answers are rejected against chosen sample using the correct reasoning path with the extracted reasoning answer (ar ). Rollouts with incorrect reasoning but correct answers are rejected, with randomly selected contextually consistent response with the right answer serving as the chosen sample. This multi-faceted checking and data construction pipeline yields rich and diverse preference dataset, specifically engineered to support robust and fast DPO training focused on improving the models reasoning length, self-reflection, and logical consistency. 4.4 DPO and Training Loop Based on the model from the previous GRPO round, DPO training is performed on contrastive data generated by the rollout-aware verifier. The visual encoder is kept frozen throughout this process, and further training parameter configurations are detailed in Table 1. The training loop follows curriculum learning approach to gradually activate the LMMs long-term reasoning ability in video. This begins with simple-modality data (text-only or image QA) for initial reasoning activation with GRPO, followed by the GRPO training using image and video QA data, as shown in Table 1. Then, the whole GRPO-Verifier-DPO pipeline continuously enhances the models long-term reasoning capability and gradually stabilizes its performance on video reasoning, iteratively pushing towards the models inherent reasoning limit. During the iterative process, we will gradually discard 80% of the simple examples (ravg = 1) from the previous GRPO training process to reduce the training time of models. The entire training process equips LMMs with robust long-chain reasoning ability with slow-search GRPO and fast-align DPO. Table 2: Model performance on video reasoning and long video understanding benchmarks. Models with grey backgrounds have >11B parameters; those with green backgrounds are based on Qwen2.5VL-7B. Bold values indicate the best performance, and underlined values indicate the second best. Video Reasoning Long Video Understanding VSI-Bench Video-MMMU MMVU (mc) TOMATO LVBench Video-MME (w/o sub) 34.0 45.4 - 29.2 35.6 32.4 - - 28.9 31.2 34.6 36.0 - - - 20.6 32.4 37.4 21.7 37. - 23.8 35.8 32.2 41.8 41.0 41.3 61.2 53.8 42.0 23.9 36.1 33.8 - 47.0 20.8 34.0 37.4 - - - 43.0 41.8 57.2 52.6 - 54.3 - 46.8 52.3 - 56.2 57.9 56.8 - - - - - 49.2 44.8 - - - 39.0 - - - - - - - - 67. 46.9 63.0 64.3 56.8 65.9 66.9 66.7 37.7 36.1 - - - - - - - - 21.7 29.0 - - - 21.5 28.1 31.7 27.2 29.3 - 25.8 - 20.6 31.6 31.5 32.2 48.9 33.1 43.5 - - - - 45.3 - - - 39.6 - - 46.4 - - - - 42. - 35.2 - 30.0 41.5 41.7 41.7 71.9 75.0 53.5 52.6 63.3 58.2 47.9 66.2 - 60.1 54.0 61.2 64.2 66.9 65.1 46.0 58.2 67.8 - 66.2 46.6 60.4 59.3 - 67.2 67.6 67.2 Model Params GPT-4o [64] Gemini 1.5 pro [59] - - mPLUG-Owl3 [83] LongVA [89] LLaVA-Video [91] LLaVA-OneVision [24] VideoLLaMA2 [9] VideoLLaMA3 [86] VILA-1.5 [33] VILA-1.5 [33] InternVL2 [63] InternVL2 [63] InternVL2.5 [8] InternVL2.5 [8] InternVideo2.5 [70] Llama-3.2-Vision [62] Gemma-3-IT [60] Kimi-VL [61] DeepSeek-VL2 [77] Qwen2.5-VL [2] TinyLLaVA-Video-R1[90] Qwen2.5-VL (thinking) [2] Video-R1 [18] Kimi-VL-Thinking [61] VerIPO (Iteration1) VerIPO (Iteration2) VerIPO (Iteration3) 7B 7B 7B 7B 7B 7B 8B 40B 8B 40B 8B 26B 8B 11B 12B 16B (A3B) 28B (A4B) 7B 3B 7B 7B 16B (A3B) 7B 7B 7B"
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experiment Setup Baseline. We compare VerIPO against various SFT and RL baselines. Direct-answer models (SFT, size > 7B) respond without an explicit reasoning process, while reasoning-answer models generate reasoning process before answering. Direct-answer baselines include SOTA models like LLaMA-3.2-V [62], Gemma-3-IT [60], Kimi-VL-A3B [61], Qwen2.5-VL-Instruct [2], and others. Reasoning-answer baselines include Kimi-VL-A3B-Thinking [61] and Video-R1 [18]. Training Details. Our GRPO algorithm is implemented using the OpenRLHF framework, and DPO training uses the TRL framework with β value of 0.1. Based on Qwen2.5-VL-7B, we conduct experiments on eight NVIDIA A800-80G GPUs with maximum of 64 frames and 128*28*28 resolution. The global training batch size is set to 64, with rollout training batch size of 64 and 8 rollout responses per query, the sampling temperature is fixed at 1.0, and the maximum output length is 4096 tokens. The learning rate is set to 1e-6. Detailed settings are shown in Table 1 and A.4. Training Dataset. Our experiments involve multiple training stages  (Table 1)  . The first stage mainly activated model reasoning using data from long documents (QuALITY [47]), text mathematics (DAPO-Math [84]), and image reasoning (ViRL-39K [66]). The second stage focuses on image and video data. To mitigate the scarcity of high-quality video data, filtered subset of diverse video benchmarks, carefully checked for leakage with evaluation datasets, is incorporated. Image data includes subsets from ViRL-39K (Science-Image, Spatial-Image), SPAR-Bench [88] (Spatial-Image), and MME-RealWorld [93] (General-Image). Video data utilizes several benchmarks: MVBench [27], TempCompass [35], LongVideoBench [74], HourVideo [3], MLVU [98], STI-Bench [32], and VideoVista-CulturalLingo [6], along with filtered 5K data of LLaVA-Video-178K [91]. Benchmark. We introduce four video reasoning benchmarks: VSI-Bench [81], TOMATO [53], Video-MMMU [21], MMVU [96] and two long video understanding benchmarks: LVBench [68], and Video-MME [19]. Specifically, VSI-Bench evaluates spatial reasoning, TOMATO assesses temporal reasoning, and Video-MMMU/MMVU tests domain-specific knowledge from multi-discipline videos. LVBench and Video-MME are general benchmarks for comprehensive long video understanding. The detailed evaluation setting of our experiment is in A.5 and evaluation prompt is in A.3 Table 3: Accuracy and length analysis during training. Acc-True indicates the answer is correct and also consistent with the right reasoning process. Length refers to the average token number of responses. The value of Acc-True shows continual improvement with iterative policy refinement. Model VSI-Bench Video-MMMU MMVU (mc) TOMATO Acc-True Length Acc-True Length Acc-True Length Acc-True Length VerIPO (GRPO-Iteration1) + Training with Video-Image Data VerIPO (GRPO-Iteration2 w/o DPO) VerIPO (DPO-Iteration1) VerIPO (GRPO-Iteration2) VerIPO (DPO-Iteration2) + Training with Video-Only Data VerIPO (DPO-Iteration1) VerIPO (GRPO-Iteration2) VerIPO (DPO-Iteration2) VerIPO (GRPO-Iteration3) VerIPO (DPO-Iteration3) 38.4 36.7 38.4 37.8 38. 38.7 38.3 39.4 39.9 40.1 132 138 167 164 181 167 163 183 172 200 51.2 52.3 52.9 52.2 53. 52.0 53.3 56.7 55.3 55.4 308 306 388 378 413 353 341 382 359 430 60.7 60.5 63.6 61.3 65. 63.6 61.6 66.1 64.3 65.9 154 157 249 253 275 201 194 243 223 291 26.5 25.7 28.9 27.3 29. 28.7 29.1 31.0 31.0 31.3 89 91 161 167 196 113 108 134 146 202 Figure 3: Figure (A): Performance comparison after removing Reflective Preference Pairs and Inference Consistency Pairs during DPO (I-2) stage. The reported values represent the average metric across the MMVU (mc) and TOMATO. For visualization, the response length has been scaled down to 0.25 of the original. Figure (B): Inconsistency rate (thinking vs. final answer) at Cold Start and different stages of VerIPO. The reported values represent the average scores across the MMVU (mc) and TOMATO. The statistical inconsistency rate is in A.2. Figure (C): The number of repeated responses generated by VerIPO at different training stages over the evaluation datasets. The reported values are computed as the sum of VSI-Bench, Video-MMMU, MMVU (mc) and TOMATO. 5.2 Results and Analysis Main Results. In Table 2, we present comparison between our VerIPO iteration and several baseline models, including Qwen2.5-VL and Kimi-VL, across six evaluation benchmarks. It can be observed that VerIPO demonstrates outstanding performance on the video reasoning benchmarks VSI-Bench, Video-MMMU, and TOMATO compared to direct inference and powerful thinking models. There is also slight improvement in model performance on the long-video general evaluation LVBench (>30 minutes) and Video-MME. For complex reasoning, e.g., Video-MMMU and VSI-Bench, we can see large performance increases compared to direct-inference and thinking models, e.g., 5.6% than Video-R1 on Video-MMMU. Effects of Verifier-Guided DPO. Table 3 illustrates iterative policy optimization with different data and strategies. All models use identical video/image data for the initial GRPO round (GRPOIteration1). Verifier-guided DPO improves true accuracy and thinking length compared to GRPO. The continual GRPO does not bring an increase in accuracy and low length. Subsequent iterations, trained solely on video data after initial reasoning activation, show stable accuracy improvement. Impacts of Verifier. Figure 3 (A) illustrates the impact of different DPO preference pairs. Statistical analysis shows that training without consistency or reflective pairs leads to decreased response length and accuracy, particularly hindering improvements in the inconsistency rate. Iterations. An ablation study on the number of VerIPO iterations is shown in Table 3 (last five rows). We observe that increasing iterations leads to improved reasoning consistency (Figure 1), true accuracy (26.5 31.3 in TOMATO), and response length (average 100 tokens increase). Overall, the iteration of VerIPO can continually improve the reasoning length (mainly driven by fast DPO training) and improve the true accuracy (removing correct answer with error thinking). 8 Table 4: Performance comparison across different training methods (SFT or reasoning activation) Stage Method VSI-Bench Video-MMMU MMVU (mc) TOMATO LVBench Video-MME(w/o sub) Start Warm Iter-1 IterIter-2 Qwen2.5-VL-7B (w/o.t.) Qwen2.5-VL-7B (w.t.) + SFT + Reasoning Activation + GRPO + GRPO (w SFT) + GRPO (w activation) + DPO + DPO (w SFT) + DPO (w activation) + GRPO + GRPO (w SFT) + GRPO (w activation) 37.5 23.8 33.8 38. 33.4 36.6 41.9 33.9 36.6 41.8 36.0 37.0 41.3 54.3 46.8 53.4 56.7 54.0 55.9 56.9 54.2 53.8 56. 52.9 54.0 56.9 67.2 63.0 65.8 65.8 66.1 65.3 66.6 66.9 66.2 65.9 68.0 66.7 65.4 29.3 25.8 26.8 28. 28.6 29.8 31.4 28.2 28.7 31.6 30.1 28.5 32.7 42.8 35.2 36.1 39.9 40.7 38.4 41.5 40.3 35.8 41. 41.0 39.3 41.7 66.2 60.4 57.8 65.9 64.7 61.0 66.7 64.2 60.0 67.2 65.8 62.8 67.0 Figure 4: case from Video-MMMU shows the comparative performance of GRPO and VerIPO. Our method can generate longer CoTs with accurate and logical formulas to solve physical problems. Impact of Cold Start. We evaluate Cold Start (SFT) in RFT using the Video-R1-COT 165k dataset. Table 4 reveals that Cold Start training resulted in marginal gains on metrics like Video-MMMU but substantial performance degradation on general reasoning tasks, unrecoverable by subsequent VerIPO iterations. This highlights the impact of low-quality video Cold Start data on performance in video tasks. However, VerIPO (iteration with only video data (black) or activation), starting from RL not SFT, show more stable performance improvement across benchmarks. Reasoning Activation. In Table 4, we also present comparative analysis of the experimental results with and without Reasoning Activation. It is observed that the reasoning activation phase, which only utilizes image and text data, demonstrates good generalization in the video domain, particularly exhibiting exceptionally strong performance on the Video-MMMU (complex domain reasoning) task. Inconsistency of reasoning process and answer. The experiments reveal inconsistency between the reasoning process and final answer, also observed in the cold start (SFT) experiments (as shown in Figure 3 (B)). The implementation of our VerIPO training loop successfully addresses this issue, demonstrating progressive reduction in inconsistency across training iterations and consistent enhancement in true accuracy (as shown in Table 3). Repetition of generated content. Repetitive inference loops exceeding context limits were observed in our experiments and with Kimi-VL-Thinking evaluation. Using the Rollout-Aware Verifier to construct specialized preference data addressed this issue, with DPO iterations significantly decreasing repetition frequency (as shown in Figure 3 (C)). Moreover, although these large multimodal models demonstrate strong reasoning capabilities with challenging static images and short video clips (limited frames inputting), their performance degrades significantly when processing longer video sequences or several frames inputting. This degradation manifests as repetitive or inconsistent reasoning, and in some cases, complete inability to reason. We hypothesize this limitation stems from the models 9 inherent capacity for long-video understanding during pre-training, highlighting crucial area for future model improvement. 5.3 Case Study Based on Figure 4 (with additional cases in B), where red indicates error reasoning and green accurate reasoning, and previous experimental analysis, we observe VerIPO enables models to generate longer and more accurate reasoning chains (sometimes with reflection) for challenging science, temporal grounding problems besides general reasoning tasks, compared to GRPO training. In addition, we observe that utilizing textual or visual math in the reasoning activation stage may aid logical reasoning, based on the reasoning process of GRPO and VerIPO in science problems. 5.4 Discussion Why do RL-trained LMRMs struggle to achieve consistent performance increase in all Video tasks? 1) High-Quality and Diverse Video Reasoning Data (Verifiable Data): Training LMRMs with RL requires vast amounts of high-quality data, particularly for video reasoning tasks that demand strong reasoning abilities or involve long reasoning paths. Most existing video datasets are primarily focused on simple recognition or short-term actions, lacking the complexity and scale needed for robust RL training. 2) Model Capability Limitations in Video Understanding (Foundation Models): The base model, upon which LMRMs are built, often relies on pre-training methodologies that are not ideally suited for comprehensive video understanding, especially over long durations. While these foundation models excel at learning powerful representations from vast amounts of image-text pairs or short video clips, their pre-training objectives typically do not fully capture the nuances of long-range temporal dependencies, event causality and sequence, and contextual consistency over time. 3) Cold Start Problem (Data Quality): If RL is used for fine-tuning after supervised fine-tuning (SFT) phase, poor initial SFT policy (especially for video) can hinder the RL agents ability to explore effectively and find optimal policies. Why do direct-answer models outperform long-thinking model variants? 1) Instability and sensitivity of RL training: The inherent instability of RL can make long-thinking\" approaches particularly challenging to optimize for long visual inputs (video). RL training for long-thinking models is hampered by their expansive \"action space\", which makes efficient exploration difficult and can lead to getting stuck in suboptimal solutions. This complexity also exacerbates hyperparameter sensitivity, common RL challenge, risking training instability. Direct-answer models benefit from smaller output space, simplifying both exploration. 2) Not all prompts require thinking (Overthinking): The benefit of long-thinking\" is task-dependent. For many common prompts, direct answer is sufficient, and forcing reasoning process can introduce unnecessary complexity, computational overhead, and potential thinking errors. We should build LMRMs to perform adaptive reasoning for different prompts. 3) RL data size is limited: The effectiveness of RL, especially for complex generative tasks, is highly dependent on the quantity and quality of data. The limitations in RL data directly impact the ability of long-thinking models to learn effectively. How to build an LMRM with adaptive reasoning capability? 1) Reasoning activation for different thinking patterns: The reasoning activation stage should use diverse data, including direct-answer examples for conciseness, step-by-step reasoning examples for detailed thought processes, mixed modality reasoning to handle various input types, and reasoning-on-demand examples that prompt specific output styles. This multifaceted reasoning activation exposes the model to range of reasoning strategies, preventing it from being confined to single, rigid approach. 2) Reward function for adaptive reasoning: Effective RL fine-tuning for adaptive reasoning necessitates sophisticated reward functions beyond answer and format accuracy, e.g., including short, middle, or long thinking judge for different prompts. These should include composite rewards that value reasoning quality, conciseness, and coherence; efficiency-aware rewards that penalize overthinking on simple problems; and adaptive policy rewards that dynamically adjust based on problem complexity. Such nuanced signals guide the model to select the appropriate depth and style of reasoning for different prompts. 3) Iterative optimization enhancement strategy: The most effective development of adaptive reasoning in an LMRM may occur through an iterative optimization loop. This loop strategically blends enforced SFT, target-optimization DPO, and wide exploration GRPO, collectively allowing the model to progressively refine its capacity for selecting and executing the optimal reasoning strategy tailored to various video understanding tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "Addressing the challenge of deep reasoning in Video-LLMs, we propose VerIPO, novel online rollout-aware Verifier-guided Iterative Policy Optimization algorithm. This RL-based GRPO-VerifierDPO loop employs small LLM verifier to refine generated CoTs, cultivating reasoning capability efficiently without requiring large Long-CoT SFT cold starts. VerIPO significantly improves reasoning consistency, accuracy, and response length, outperforming larger and more powerful baselines on video benchmarks. While effective, limitations include potential verifier dependence, limited data size and response length, and computational costs. Future works aims to address these by exploring verifier designs, optimizing the pipeline and leveraging GRPO exploration, targeted DPO, and strong SFT on base LMMs towards achieving robust, long-term reasoning ability across unimodality."
        },
        {
            "title": "References",
            "content": "[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. Vivit: video vision transformer, 2021. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. [3] Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour videolanguage understanding. In Advances in Neural Information Processing Systems, volume 37, 2024. [4] Tieyuan Chen, Huabin Liu, Tianyao He, Yihang Chen, Chaofan Gan, Xiao Ma, Cheng Zhong, Yang Zhang, Yingxue Wang, Hui Lin, and Weiyao Lin. Mecd: Unlocking multi-event causal discovery in video reasoning, 2024. [5] Xiaxu Chen, Wei Li, Chunxu Liu, Chi Xie, Xiaoyan Hu, Chengqian Ma, Feng Zhu, and Rui Zhao. On the suitability of reinforcement fine-tuning to visual tasks, 2025. [6] Xinyu Chen, Yunxin Li, Haoyuan Shi, Baotian Hu, Wenhan Luo, Yaowei Wang, and Min Zhang. Videovista-culturallingo: 360 horizons-bridging cultures, languages, and domains in video comprehension, 2025. [7] Zhangquan Chen, Xufang Luo, and Dongsheng Li. Visrl: Intention-driven visual perception via reinforced reasoning, 2025. [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms, 2024. [10] Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, and Yahui Zhou. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning, 2025. [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Instructblip: Towards general-purpose Wang, Boyang Li, Pascale Fung, and Steven Hoi. vision-language models with instruction tuning, 2023. [12] Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, and Peter Grasch. Mm-spatial: Exploring 3d spatial understanding in multimodal llms, 2025. [13] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 11 [14] Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning, 2025. [15] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models, 2025. [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [17] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition, 2024. [18] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms, 2025. [19] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [20] Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, and Saravan Rajmohan. Ampo: Active multi-preference optimization, 2025. [21] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos, 2025. [22] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. [23] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. [25] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, and Luo Si. mplug: Effective and efficient vision-language learning by cross-modal skip-connections, 2022. [26] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, 2024. [27] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark, 2023. [28] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning, 2025. [29] Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, Yong Xu, and Min Zhang. Lmeye: An interactive perception network for large language models. IEEE Transactions on Multimedia, 26:10952 10964, 2024. [30] Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. Uni-moe: Scaling unified multimodal llms with mixture of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(5):34243439, 2025. [31] Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, et al. Perception, reason, think, and plan: survey on large multimodal reasoning models. arXiv preprint arXiv:2505.04921, 2025. [32] Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Stibench: Are mllms ready for precise spatial-temporal world understanding? arXiv preprint arXiv:2503.23765, 2025. [33] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2024. 12 [34] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Helong Huang, Guangjian Tian, Weichao Qiu, Xingyue Quan, Jianye Hao, and Yuzheng Zhuang. Spatialcot: Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning, 2025. [35] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv: 2403.00476, 2024. [36] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning, 2025. [37] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement, 2025. [38] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer, 2021. [39] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning, 2025. [40] Zhiyuan Liu, Yuting Zhang, Feng Liu, Changwang Zhang, Ying Sun, and Jun Wang. Othinkmr1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning, 2025. [41] Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1 : generalist r1-style vision-language action model for gui agents, 2025. [42] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models, 2024. [43] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, 2025. [44] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network, 2021. [45] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. [46] Kun Ouyang. Spatial-r1: Enhancing mllms in video spatial reasoning, 2025. [47] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY: In Proceedings of the 2022 Conference Question answering with long input texts, yes! of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 53365358, Seattle, United States, July 2022. Association for Computational Linguistics. [48] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [50] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [51] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko. Sat: Dynamic spatial aptitude training for multimodal language models, 2025. [52] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [53] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models, 2024. [54] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [55] Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning via adaptive curriculum learning, 2025. [56] Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, and Wentao Zhang. Mm-verify: Enhancing multimodal reasoning with chain-of-thought verification, 2025. [57] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning, 2025. [58] Xiaohang Tang, Sangwoong Yoon, Seongho Son, Huizhuo Yuan, Quanquan Gu, and Ilija Bogunovic. Game-theoretic regularized self-play alignment of large language models, 2025. [59] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. [60] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, et al. Gemma 3 technical report, 2025. [61] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, et al. Kimi-vl technical report, 2025. [62] Meta Team. Model cards & prompt formats-llama 3.2, 2024. [63] OpenGVLab Team. Vila: On pre-training for visual language models, 2024. [64] OpenAI Team, Aaron Hurst, Adam Lerer, Adam P. Goucher, et al. Gpt-4o system card, 2024. [65] Qwen Team. Qwen3: Think deeper, act faster, April 2025. [66] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [67] Haoran Wang, Aman Rangapur, Xiongxiao Xu, Yueqing Liang, Haroon Gharwi, Carl Yang, and Kai Shu. Piecing it all together: Verifying multi-hop multimodal claims, 2024. [68] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark, 2024. [69] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement, 2025. [70] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling, 2025. [71] Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Unified multimodal chain-of-thought reward model through reinforcement fine-tuning, 2025. [72] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. [73] Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Videorope: What makes for good video rotary position embedding?, 2025. [74] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. [75] Peiran Wu, Yunze Liu, Miao Liu, and Junxiao Shen. St-think: How multimodal large language models reason about 4d worlds from ego-centric videos, 2025. [76] Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment, 2024. [77] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. [78] Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, Lanqing Hong, Hang Xu, and Xiaodan Liang. Atomthink: slow thinking framework for multimodal mathematical reasoning, 2024. 14 [79] Zhenghao Xing, Xiaowei Hu, Chi-Wing Fu, Wenhai Wang, Jifeng Dai, and Pheng-Ann Heng. Echoink-r1: Exploring audio-visual reasoning in multimodal llms via reinforcement learning, 2025. [80] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. Redstar: Does scaling long-cot data unlock better slow-reasoning systems?, 2025. [81] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171, 2024. [82] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization, 2025. [83] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024. [84] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [85] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. [86] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding, 2025. [87] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding, 2023. [88] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yujie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, and Li Zhang. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025. [89] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision, 2024. [90] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Tinyllava-video-r1: Towards smaller lmms for video reasoning, 2025. [91] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. [92] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Yuhang Wang, Jinlin Xiao, and Jitao Sang. Openrft: Adapting reasoning foundation model for domain-specific tasks with reinforcement fine-tuning, 2024. [93] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. [94] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models, 2024. [95] Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning, 2025. [96] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, and Arman Cohan. Mmvu: Measuring expert-level multi-discipline video understanding, 2025. [97] Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, and Hengshuang Zhao. Villa: Video reasoning segmentation with large language model, 2025. 15 [98] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024."
        },
        {
            "title": "A Detailed Training and Evaluation Analysis",
            "content": "A.1 Comparison of Training Speed In this section, we compare the training time of the GRPO and DPO algorithms, both based on single epoch of GRPO training. For the first epoch of GRPO, the total dataset consists of approximately 47K samples. After discarding 80% of the simpler examples, the dataset for the second epoch is reduced to around 24,653 samples. In contrast, the training data for DPO, after incorporating the Rollout-Aware Verifier, comprises approximately 20,096 samples. The training process for both algorithms is conducted on 8 A800-80G GPUs, with the corresponding training time summarized in Table 5. The table reports the total training time in minutes, alongside the estimated average training time per sample, which is calculated by dividing the total training time by the number of samples. The average training time is presented in seconds. From the results in Table 5, we observe that the average training time per sample for the GRPO algorithm is approximately 7 times longer than that of the DPO algorithm. Table 5: Training Time Comparison between DPO and GRPO. Stage Framework Size of Training Dataset Total Training Time (minutes) Sample-Level Training Time (seconds) GRPO OpenRLHF 24,653 1891 4.6 DPO trl 20,096 242 0.7 A.2 Inconsistency Rate The inconsistency rates for different models across the MMVU (mc), TOMATO, and Video-MMMU benchmarks are provided in Table 6. Table 6: Inconsistency Rate in Evaluation Benchmark Model MMVU (mc) TOMATO Video-MMMU Qwen2.5-VL-7B (GRPO cold start) VerIPO (Reasoning Activation) VerIPO (GRPO-Iteration1) VerIPO (DPO-Iteration1) VerIPO (GRPO-Iteration2) VerIPO (DPO-Iteration2) VerIPO (DPO-Iteration2 no Consistency) VerIPO (DPO-Iteration2 no Reflection) VerIPO (GRPO-Iteration3) VerIPO (DPO-Iteration3) 6.4 5.9 13.3 8.0 11.0 2.2 6.7 5.3 3.0 2. A.3 CoT Prompt 11.9 15.3 17.4 11.0 12.7 2.5 7.6 7.0 2.7 2.5 9.7 - 15.7 13.4 11.6 4.6 - - 5.8 5.0 We have designed our prompt template based on the format used in DeepSeek-R1, where the system prompt explicitly defines the required output structure. This includes the use of <answer> tags to separate the reasoning process from the final answer. Detailed prompt are presented in Table 7. The table lists two distinct prompt formats: one for multiple-choice questions and the other for numerical questions, where {question} represents the processed question. 16 Table 7: Prompt setting for training and evaluation Prompt For Multi-Choices Question SYSTEM: You should first thinks about the reasoning process in the mind and then provides the user with the answer. Your answer must be in latex format and wrapped in $...$.The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> Since ...., so the answer is B. </think><answer> $B$ </answer>, which means your output should start with <think> and end with </answer>. USER: Question: {question} Prompt For Numberic Question SYSTEM: You should first thinks about the reasoning process in the mind and then provides the user with the answer. Your answer must be in latex format and wrapped in $...$.The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> Since ...., so the answer is 2. </think><answer> $2$ </answer>, which means your output should start with <think> and end with </answer>. USER: Question: {question} You must provide the answer in the <answer> </answer> tag, and the answer must be number. A.4 Detailed Training Setting During the training of Qwen2.5-VL-Instruct using the GRPO and DPO algorithms, we kept the visual encoder frozen throughout, training only the parameters of the MLP and the language model. For the GRPO training process, we utilized the Hybrid Engine to accelerate training. In the Reasoning Activation phase, both the micro train batch size and micro rollout batch size were set to 2. In the Group-Slow-Search phase, these values were reduced to 1 to accommodate the long video context inputs. A.5 Detailed Evaluation Setting When evaluating the Qwen2.5-VL-Instruct model, along with all models trained using reinforcement learning based on this architecture, we set do_sample to False and used the default parameter settings from the Qwen generation_config: repetition_penalty = 1.05, temperature = 1e-6, and top_p = 1.0. The entire evaluation process is accelerated by leveraging VLLM for inference. For video sampling, we set the frame rate to 2.0 fps, configured the maximum number of sampled frames per video to 128, and specified the maximum resolution per frame as 2562828. Both the maximum number of sampled frames and the maximum resolution per frame were set to twice the values used during training. Additionally, we conducted comparative experiment on the MMVU (mc) dataset and 300 long video samples sourced from Video-MME using the Qwen2.5-VL-Instruct model, with focus on the number of sampled frames and the maximum resolution. The results of this experiment are presented in Table 8. Table 8: Experiment about sampled frames and maximum resolution Model FPS Frames Resolution MMVU (mc) Video-MME (Long-300) Qwen2.5-VL-7B (w.t.) Qwen2.5-VL-7B (w.t.) Qwen2.5-VL-7B (w.t.) Qwen2.5-VL-7B (w.t.) Qwen2.5-VL-7B (w.t.) 1.0 2.0 2.0 2.0 2.0 64 64 64 128 128 128*28*28 128*28*28 256*28*28 128*28*28 256*28*28 57.9 59.5 61.0 61.0 63.0 54.0 54.0 49.7 51.3 53."
        },
        {
            "title": "B Qualitative Analysis",
            "content": "17 Figure 5: case from VSI-Bench shows the comparative performance of GRPO and VerIPO. Our method is capable of generating longer responses and employing self-validation to address spatial reasoning tasks. Figure 6: Another case from VSI-Bench shows the comparative performance of GRPO and VerIPO. Our method is capable of generating longer responses and employing self-validation to address spatial reasoning tasks. Figure 7: case from Video-MMMU shows the comparative performance of GRPO and VerIPO. Our method can identify situations where the reasoning path is correct but an incorrect answer is chosen, through reflection, then re-selects the correct option that aligns with the reasoning content. 18 Figure 8: case from Video-MME shows the comparative performance of GRPO and VerIPO. Our method also demonstrates strong capabilities in reflection and reasoning on general-domain question-answering tasks. Figure 9: case from TOMATO shows the comparative performance of GRPO and VerIPO. Our method is capable of generating longer responses and performing accurate temporal reasoning by self-validation."
        }
    ],
    "affiliations": [
        "Alibaba International Group",
        "Division of AMC and Department of ECE, HKUST",
        "Harbin Institute of Technology, Shenzhen, China"
    ]
}