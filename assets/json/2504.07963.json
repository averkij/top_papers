{
    "paper_title": "PixelFlow: Pixel-Space Generative Models with Flow",
    "authors": [
        "Shoufa Chen",
        "Chongjian Ge",
        "Shilong Zhang",
        "Peize Sun",
        "Ping Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256$\\times$256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow."
        },
        {
            "title": "Start",
            "content": "PixelFlow: Pixel-Space Generative Models with Flow Shoufa Chen1 Chongjian Ge1,2 Shilong Zhang1 Peize Sun1 Ping Luo1 1The University of Hong Kong 2Adobe 5 2 0 2 0 1 ] . [ 1 3 6 9 7 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present PixelFlow, family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow. 1. Introduction Numquam ponenda est pluralitas sine necessitate. William of Ockham Driven by the success of the Stable Diffusion (SD) model series [17, 46, 47, 50], latent diffusion models (LDMs) [50] have emerged as the de facto standard for generative modeling across diverse modalities, spanning image [17, 35, 45], video [7, 8, 23, 66, 69], audio [18, 39], and 3D [57, 67]. As shown in Figure 1 (a), LDMs compress raw data into compact latent space using pre-trained Variational Autoencoders (VAEs). This compression reduces computational demands and facilitates efficient diffusion denoising. Despite their widespread success, LDMs decouple the VAE and diffusion components, hindering joint optimization and complicating holistic diagnosis. An alternative approach is to implement diffusion models in the raw pixel space. While intuitive, this becomes computationally unaffordable for high-resolution images due to the substantial resources required to process per-pixel correlations. Considering this, prior research [20, 22, 44, Figure 1. Comparisons of Design Paradigms between latentbased diffusion models (LDMs), pixel-based diffusion models (PDMs), and PixelFlow: (a) LDMs split training into two separate stagesfirst independently training off-the-shell VAEs, then training diffusion models on tokens extracted from the pre-trained VAEs; (b) Previous PDMs typically train two separate models: diffusion model on low-resolution images and an upsampler for high-resolution synthesis; (c) PixelFlow, by contrast, offers an end-to-end solution for pixel-based generation, combining both high efficiency and strong generative performance. 51, 52] has typically adopted cascaded approach: first generating low-resolution image, then employing additional upsamplers to produce high-quality outputs, with the low1 resolution image serving as conditioning input, as shown in Figure 1(b). However, these cascaded methods also introduce separate networks for different stages, still limiting the benefits of end-to-end design. In this work, we introduce PixelFlow, simple but effective end-to-end framework for direct image generation in raw pixel space, without the need of separate networks like VAEs or upsamplers. As illustrated in Figure 1(c), PixelFlow uses unified set of parameters to model multiscale samples across cascading resolutions via Flow Matching [38, 40]. At early denoising stages, when noise levels are high, PixelFlow operates on lower-resolution samples. As denoising progresses, the resolution gradually increases until it reaches the target resolution in the final stage. This progressive strategy avoids performing all denoising steps at full resolution, thereby significantly reducing the overall computational cost of the generation process. During training, the cross-scale samples at different timesteps are constructed by: (1) resizing the images to successive scales and adding Gaussian noise to each scaled image; (2) interpolating between adjacent scale noisy images as model input and conducting velocity prediction. The entire model is trained end-to-end using uniformly sampled training examples from all stages. During inference, the process begins with pure Gaussian noise at the lowest resolution. The model then progressively denoises and upscales the image until the target resolution is reached. We evaluated PixelFlow on both class-conditional and text-to-image generation tasks. Compared to established latent-space diffusion models [42, 45, 50], PixelFlow delivers competitive performance. For instance, on the 256 256 ImageNet class-conditional generation benchmark, PixelFlow achieves an FID of 1.98. For text-toimage generation, PixelFlow is evaluated on widely-used benchmarks, achieving 0.64 on GenEval [19] and 77.93 on DPG-Bench [26]. In addition, qualitative results in Figure 5 and Figure 6 illustrate that PixelFlow has strong visual fidelity and text-image alignment, highlighting the potential of pixel-space generation for future research."
        },
        {
            "title": "The contributions of PixelFlow are summarized as in",
            "content": "the following three points: By eliminating the need for pre-trained VAE, we establish an end-to-end trainable image generation model in raw pixel space directly. Through cascade flow modeling from low resolution to high resolution, our model achieves affordable computation cost in both training and inference. PixelFlow obtains competitive performance in visual quality, including 1.98 FID on 256 256 ImageNet classconditional image generation benchmark and appealing properties on text-to-image generation. 2. Related Work Latent Space Diffusion/Flow Models. Variational Autoencoders (VAEs) have become core component in many recent generative models [16, 17, 35, 47, 48, 50, 59, 66], enabling the mapping of visual data from pixel space to lower-dimensional, perceptually equivalent latent space. This compact representation facilitates more efficient training and inference. However, VAEs often compromise highfrequency details [47], leading to inevitable low-level artifacts in generated outputs. Motivated by desire for algorithmic simplicity and fully end-to-end optimization, we forgo the VAE and operate directly in pixel space. Pixel Space Diffusion/Flow Models. Early diffusion models [2, 21, 56] primarily operated directly in pixel space, aiming to capture the distributions images in single stage. However, this approach proved both challenging and inefficient for high-resolution image generation, leading to the development of cascaded models [20, 22, 30, 52] that generate images through sequence of stages. These cascaded models typically begin with the generation of low-resolution image, which is subsequently upscaled by super-resolution models to achieve higher resolutions. However, the diffusion-based super-resolution process often requires starting from pure noise, conditioned on lowerresolution outputs, resulting in time-consuming and inefficient generation process. Additionally, training these models in isolated stages hinders end-to-end optimization and necessitates carefully designed strategies to ensure the super-resolution stages. Furthermore, recent advancements in pixel-space generation have introduced innovative architectures. Simple Diffusion [24, 25] proposes streamlined diffusion framework for high-resolution image synthesis, achieving strong performance on ImageNet through adjustments of model architecture and noise schedules. FractalGen [37] constructs fractal generative models by recursively invoking atomic generative modules, resulting in self-similar architectures that demonstrate strong performance in pixel-by-pixel image generation. TarFlow [68] presents Transformer-based normalizing flow architecture capable of directly modeling and generating pixels. 3. PixelFlow 3.1. Preliminary: Flow Matching The Flow Matching algorithm [1, 38, 40] progressively transforms sample from prior distribution, which is typically standard normal distribution, to the target data distribution. This is accomplished by defining forward process consisting of sequence of linear paths that directly connect samples from the prior distribution to corresponding For each stage s, we define the starting and ending states as follows: Start: xts End: xts 0 1 = ts = ts 0 Up(Down(x1, 2s+1)) + (1 ts 1 Down(x1, 2s) + (1 ts 1) ϵ, 0) ϵ (2) (3) Figure 2. PixelFlow for cascaded image generation from pixel space. We partition the entire generation procedure into series resolution stages. At the beginning of each resolution stage, we upscale the relatively noisy results from the preceding stage and use them as the starting point for the current stage. Consequently, as the resolution enhances, more refined samples can be obtained. samples in the target distribution. During training, training example is constructed by first sampling target sample x1, drawing noise x0 (0, 1) from the standard normal distribution, and selecting timestep [0, 1]. The training example is then defined through linear interpolation: xt = x1 + (1 t) x0 (1) The model is trained to approximate the velocity defined by an ordinary differential equation (ODE), vt = dxt dt , enabling it to effectively guide the transformation from the intermediate sample xt to the real data sample x1. notable advantage of Flow Matching is its ability to interpolate between two arbitrary distributions, not restricted to using only standard Gaussian as the source domain. Consequently, in image generation tasks, Flow Matching extends beyond noise-to-image scenarios and can be effectively employed for diverse applications such as image-toimage translation. 3.2. Multi-Scale Generation in Pixel Space PixelFlow generates images by progressively increasing their resolution through multistage denoising process. To enable this, we construct multi-scale representation of the target image x1 by recursively downsampling it by factor of 2 at each scale. As illustrated in Figure 2, PixelFlow divides the image generation process into stages. Each stage 0, 1, ..., 1 operates over time interval defined by the start and end states (xts 1). In the degenerate case where = 1, PixelFlow reduces to standard single-stage flow matching approach for image generation, similar to recent works [17, 42], but crucially operates in pixel space rather than latent space. 0, xts where Down() and Up() denote the downsampling and upsampling operations, respectively. Unless otherwise stated, we adopt bilinear interpolation for downsampling and nearest neighbor for upsampling. To train the model, we sample intermediate representations by linearly interpolating between the start and end states: xts τ = τ xts 1 + (1 τ ) xts 0 , (4) where τ = tts 0 1ts ts 0 the s-th stage. is the rescaled timestep [29, 65] within Then our objective is to train model µθ() to predict the 0. We use the velocity µθ(xts τ ,τ ) with target as vt = xts mean squared error (MSE) loss, formally represented as: xts Es,t,(xts 1 ,xts 1 )µθ(xts τ ,τ ) vt2 (5) 3.3. Model Architecture We instantiate µθ() using Transformer-based architecture [62], chosen for its simplicity, scalability, and effectiveness in generative modeling. Specifically, our implementation is based on the standard Diffusion Transformer (DiT) [45], employing XL-scale configurations across all experiments. To better align with the PixelFlow framework, we introduce several modifications, as detailed below. Patchify. Following the Vision Transformer (ViT) design [15, 45], the first layer of PixelFlow is patch embedding layer, which converts the spatial representation of the input image into 1D sequence of tokens via linear projection. In contrast to prior latent transformers [17, 42, 45] that operate on VAE-encoded latents, PixelFlow directly tokenizes raw pixel inputs. To support efficient attention across multiple resolutions within batch, we apply sequence packing strategy [11], concatenating flattened token sequences of varying lengthscorresponding to different resolutionsalong the sequence dimension. RoPE. After patchfying, we replace the original sincos positional encoding [45] with RoPE [58] to better handle varying image resolutions. RoPE has shown strong performance in enabling length extrapolation, particularly in large language models. To adapt it for 2D image data, we apply 2D-RoPE by independently applying 1D-RoPE to the height and width dimensions, with each dimension occupying half of the hidden state. 3 Figure 3. Visualization of intermediate result of cascaded stages. We extract the intermediate results from each of the four stages for direct visualization. We observed clear denoising process at various resolution stages. Resolution Embedding. Since PixelFlow operates across multiple resolutions using shared set of model parameters, we introduce an additional resolution embedding to distinguish between resolutions. Specifically, we use the absolute resolution of the feature map after patch embedding as conditional signal. This signal is encoded using sinusoidal position embedding [62] and added to the timestep embedding before being passed into the model. Text-to-Image Generation. While class-conditional image generation typically integrates conditioning information through adaptive layer normalization (adaLN)[45], we extend PixelFlow to support text-to-image generation by introducing cross-attention layer after each self-attention layer within every Transformer block [6, 7]. This design allows the model to effectively align visual features with the textual input at every stage of the generation process. Following recent work [8, 59], we adopt the Flan-T5-XL language model [10] to extract rich text embeddings, which serve as conditioning signals throughout the network. 3.4. Training and Inference To facilitate efficient training, we uniformly sample training examples from all resolution stages using the interpolation scheme defined in Equation (4). Additionally, we employ the sequence packing technique [11], which enables joint training of scale-variant examples within single minibatch, improving both efficiency and scalability. During inference, the generation process begins with pure Gaussian noise at the lowest resolution and progressively transitions to higher resolutions through multiple stages. Within each resolution stage, we apply standard flow-based sampling, using either the Euler discrete sampler [17] or the Dopri5 solver, depending on the desired trade-off between speed and accuracy. To ensure smooth and coherent transitions across scales, we adopt an renoising strategy [29, 60], which effectively mitigates the jumping point issue [4] often observed in multi-scale generation pipelines. 4. Experiments In this section, we first detail our experimental setup in Sec. 4.1. Subsequently, we analyze key components of our approach, including model design (Sec. 4.2) and inference configurations (Sec. 4.3). Finally, we benchmark PixelFlow against state-of-the-art methods on class- (Sec. 4.4) and text-to-image (Sec. 4.5) generation tasks. 4.1. Experimental Setup We evaluate PixelFlow for class-conditional image generation on the ImageNet-1K [12] dataset. Unless stated otherwise, we train PixelFlow at 256256 resolution. All models are trained using the AdamW optimizer [32, 41] with constant learning rate of 1 104. Performance is primarily measured by Frechet Inception Distance (FID) using the standard evaluation toolkit1. We also report Inception Score (IS) [53], sFID [43], and Precision/Recall [33]. For text-conditional image generation, we progressively train PixelFlow from 256256 up to 10241024 resolution. We include qualitative comparisons with current start-of-the-art generative models, along with quantitative assessments on popular benchmarks such as T2ICompBench [27], GenEval [19], and DPG-Bench [26]. 4.2. Model Design Kickoff sequence length. In principle, PixelFlow can be trained to progressively increase resolution from very low resolution (e.g., 1 1) up to the target resolution. However, this approach is inefficient in practice, as tokens at extremely low resolutions convey limited meaningful information. Furthermore, allocating excessive timesteps to very short sequences underutilizes the computational capacity of modern GPUs, resulting in decreased model FLOPS utilizationt. Therefore, we explore how varying the resolution at which image generation begins, which we call kickoff image resolution, impacts overall performance. For our transformer-based backbone, the number of tokens involved in attention operations is determined by the 1https://github.com/openai/guided-diffusion 4 kickoff seq. len. FID sFID IS Precision Recall step FID sFID IS Precision Recall 3232 88 22 3.34 3.21 3. 6.11 6.23 6.45 84.75 78.50 67.81 0.78 0.78 0.78 0.57 0.56 0.54 Table 1. Effect of kickoff sequence length. All models are trained with 600k iterations on ImageNet-1K. Patch size is 22 and target image resolution is 6464. patch size FID sFID IS Precision Recall speed target res. 6464; kickoff seq. len. 22; 600K iters 22 44 3.49 3.41 6.45 5.52 67.81 68.83 0.78 0. 0.54 0.56 1.28 0.58 10 20 30 40 3.39 2.53 2.51 2.55 5.98 5.53 5.82 6.58 255.27 272.13 274.92 272. 0.80 0.82 0.82 0.81 0.54 0.56 0.56 0.56 (a) Effect of number of steps per stage. CFG is global constant value 1.50, sample function is Euler. solver FID sFID IS Precision Recall Euler Dopri5 2.51 2.43 5.82 5. 274.92 282.20 0.82 0.83 0.56 0.56 (b) Effect of sample function. CFG is global constant value 1.50, the number of steps per stage is 30 in Euler, the absolute tolerance is 1e-6 in Dopri5. target res. 256256; kickoff seq. len. 22; 100K iters cfg schedule cfg max value FID 22 44 88 28.50 33.17 47.50 6.40 7.71 9.63 47.37 42.29 31. 0.58 0.57 0.45 0.53 0.52 0.50 30.88 7.31 3.96 target res. 256256; kickoff seq. len. 22; 1600K iters; EMA 44 88 2.81 4. 5.48 251.79 5.42 195.50 0.82 0.79 0.55 0.54 7.31 3.96 Table 2. Effect of patch size. All models have kickoff sequence length of 22. Upper: target resolution of 6464; Middle: target resolution of 256256 resolution, training with 100K iterations due to computational constraints of patch size 22; Bottom: Extended training to 1600K iterations at 256256 resolution. Speed measured as number of seconds per sample on single GPU with batchsize of 50. raw image resolution and the patch size. In this experiment, we maintain consistent patch size of 22 [45], making the kickoff sequence length directly dependent on the kickoff image resolution. Specifically, we evaluate three kickoff sequence length22, 88, and 3232while keeping the target resolution fixed at 6464. Notably, the 3232 setting represents vanilla pixel-based approach without cascading across resolutions. As shown in Table 1, among these configurations, the 88 kickoff sequence length achieves comparable or even slightly improved FID compared to the 3232 baseline. This suggests that initiating generation from an appropriately smaller resolution and progressively scaling up can maintain generation quality while improving computational efficiency by allocating fewer computations to the largest resolution stage. Conversely, reducing the kickoff sequence length further to 22 results in performance degradation, likely because tokens at extremely low resolutions provide limited useful information and insufficient guidance for subsequent generation steps. Taking into account both generation quality and computational efficiency, we therefore adopt 88 as our default kickoff sequence length. IS 282.2 282.1 global constant stage-wise constant 1.50 2.40 2.43 1. (c) Effect of classifier-free guidance (CFG) setting. Sample function is Dopri5 with absolute tolerance 1e-6. Table 3. Inference Setting. The best performance is obtained by CFG step-wise constant with maximum value 2.40 and Dopri5 sample function. Patch size. Next, we investigate the impact of patch size on model performance while maintaining kickoff sequence length of 22. Initially, we experiment with target resolution of 6464 and compare two patch sizes22 and 44with results presented in the upper section of Table 2. We observe that PixelFlow achieves very similar performance across these two settings, with the 44 patch slightly outperforming the 22 patch on four out of five evaluation metrics. Furthermore, using patch size of 44 eliminates the highest-resolution stage required by the 22 patch size configuration, thus improving efficiency. target"
        },
        {
            "title": "When scaling to a larger",
            "content": "resolution (i.e., 256256), employing patch size of 22 becomes computationally infeasible due to substantial resource demands, limiting our experiments to only 100K training iterations (middle section of Table 2). This constraint necessitates adopting larger patch sizes. Although increasing the patch size further to 88 significantly enhances computational efficiency, it leads to noticeable drop in performance quality. Moreover, this performance gap persists even after extended training (1600K iterations), as shown in the bottom section of Table 2. Considering both generation quality and computational cost, we therefore select patch size of 44 as our default setting. 4.3. Inference Schedule In Table 3, we provide detailed analysis of the inference configuration space, including the number of inference 5 Figure 4. Qualitative results of class-conditional image generation of PixelFlow. All images are 256256 resolution. steps at each resolution stage, the choice of ODE solver, and the scheduling of classifier-free guidance (CFG). Number of sample steps. In Table 3a, we evaluate the impact of the number of inference steps per resolution stage on generation quality. As the number of steps increases, we observe consistent improvements in FID, sFID, and IS, with the best overall performance achieved at 30 steps. Beyond this point, gains saturate and even slightly decline, indicating diminishing returns. notable advantage of PixelFlow is its flexibility in assigning different numbers of sampling steps to each resolution stage during inference. This adaptive configuration allows fine-grained control over the sampling process, enabling performanceefficiency trade-offs. Moving beyond uniform setting and exploring more granular stage-specific step allocations holds the potential for further performance enhancements. across most evaluation metrics, achieving lower FID and sFID scores, higher Inception Score, and slightly better precision, while maintaining similar recall. This demonstrates that more accurate and adaptive solvers, such as Dopri5, can better capture the generative dynamics, leading to higher-quality samplesthough often with increased computational cost. CFG Schedule. Inspired by the recent process [5, 34, 63], we propose stage-wise CFG schedule, where different stages apply different CFG values, and from the early stage to the later stage, the value increases from 1 to CFGmax. In the condition of 4 stages, we find that 0, 1/6, 2/3 and 1 of the (CFGmax 1) give the best FID performance. The comparison between global constant CFG and stage-wise CFG is shown in Table 3c, in which we search the best CFG value for each method. Our proposed stage-wise CFG boosts the FID performance from 2.43 to 1.98. ODE Solver. We further investigate the effect of the ODE solver type on generation quality. As shown in Table 3b, we compare the first-order Euler solver with the adaptive higher-order DormandPrince (Dopri5) solver [14]. The results indicate that Dopri5 consistently outperforms Euler 4.4. Comparison on ImageNet Benchmark In Table 4, we compare PixelFlow with both latent-based and pixel-based image generation models on the ImageNet 256256 benchmark. PixelFlow achieves an FID of 1.98, 6 Model FID sFID IS Precision Recall LDM-4-G [50] DiT-XL/2 [45] SiT-XL/2 [42]"
        },
        {
            "title": "Latent Space",
            "content": "3.60 2.27 2.06 - 247.7 4.60 278.2 4.49 277."
        },
        {
            "title": "Pixel Space",
            "content": "4.59 ADM-G [13] 3.94 ADM-U [13] 4.88 CDM [22] 3.42 RIN [9, 28] 2.77 SD, U-ViT-L [24] MDM [20] 3.51 StyleGAN-XL [54] 2.30 2.12 VDM++ [31] 1.56 PaGoDA [30] 1.38 SiD2 [25] JetFormer [61] 6.64 FractalMAR-H [37] 6.15 5.25 186.7 6.14 215.8 158.7 182.0 211.8 - - - - - 4.02 265.1 267.7 259.6 - - 348.9 - - - - - PixelFlow (ours) 1.98 5.83 282.1 0.87 0.83 0.83 0.82 0.83 - - - - 0.78 - - - 0.69 0.81 0.81 0.48 0.57 0. 0.52 0.53 - - - - 0.53 - 0.59 - 0.56 0.46 0.60 Table 4. Comparisons on class-conditional image generation on ImageNet 256256. PixelFlow achieves competitive performance compared with latent space based models. representing highly competitive performance relative to state-of-the-art latent-space methods. For instance, it outperforms LDM [50] (FID 3.60), DiT [45] (FID 2.27), and SiT [42] (FID 2.06), while achieving comparable IS and recall scores. These results highlight the effectiveness of our design, suggesting that PixelFlow can serve as strong prototype for high-quality visual generation systems. Compared with recent pixel-based models, PixelFlow It notably outperforms achieves superior sample quality. FractalMAR-H [37], and also delivers competitive or better results than strong baselines like ADM-U [13], SiD2 [25], and VDM++ [31]. We visualize class-conditional image generation of PixelFlow at 256256 resolution in Figure 4. We can observe our model is able to generate images of high visual quality across wide range of classes. 4.5. Text-to-Image Generation Settings. We adopt two-stage training strategy for textto-image generation of PixelFlow. First, the model is initialized with an ImageNet-pretrained checkpoint at resolution of 256256 and trained on subset of the LAION dataset [55] at the same resolution. In the second stage, we fine-tune the model on curated set of high-aestheticquality images at higher resolution of 512512. All reported results for PixelFlow are based on this final 512512 resolution model. Method SDv1.5 [50] DALL-E 2 [49] SDv2.1 [50] SDXL [47] PixArt-α [6] DALL-E 3 [3] GenTron [7] SD3 [17] Transfusion [70] LlamaGen [59] Emu 3 [64] PixelFlow (ours) GenEval Overall Color T2I-CompBench DPG Shape Texture Bench 0.4219 0.6374 0.4982 0.5637 0. 0.3730 0.5750 0.5694 0.6369 0.6886 63.18 0.3646 0.43 - 0.5464 0.52 - 0.4495 0.50 74.65 0.5408 0.55 0.48 71.11 0.5582 0.67 0.8110 0.6750 0.8070 83.50 0.5700 - - - 0.7913 0.5846 0.7422 - - - - 80.60 0.7674 - - - 0.7150 - - - - 0.74 0.63 0.32 0.66 0.60 0.64 0.7578 0.6006 0.4529 0.7689 0.5059 0.6273 77.93 Table 5. Comparison with state-of-the-art models on text-toimage generation benchmarks. We evaluate on GenEval [19], T2I-CompBench [27] and DPG-Bench [26]. We use to indicate the result with prompt rewriting. To comprehensively evaluate the performance of PixelFlow-T2I in text-to-image generation, we employ three widely recognized benchmarks, each targeting facet of compositional understanding: T2Idifferent CompBench [27] assesses alignment between generated images and complex semantic relationships in text. We evaluate three taskscolor, shape, and texture bindingby generating five images per prompt across 300 prompts per sub-task. Alignment is measured using BLIP-VQA[36]; GenEval [19] evaluates compositional aspects such as coherence and spatial arrangement. We generate over 2,000 images from 553 prompts and report the average performance across tasks; DPG-Bench [26] focuses on complex textual descriptions, with 4,000 images generated from 1,065 prompts and results averaged across tasks. Quantitative results. As shown in Table 5, PixelFlow achieves competitive performance across all benchmarks, demonstrating strong compositional understanding in freeIt performs particularly form text-to-image generation. well on T2I-CompBench, with high scores in color and texture binding, and solid results on GenEval (0.64) and DPG-Bench (77.93), surpassing many established models. These results underscore PixelFlow as promising direction for pixel-space image generation conditioned on natural languageshowcasing its potential for open-ended, text-driven image synthesis. Visualization. We visualize the intermediate results during the sampling process in Figure 3, specifically show7 Figure 5. Qualitative results of text-conditional generation of PixelFlow. All images are 512512 resolution. Key components of the prompt are highlighted in RED. ing the final step of each resolution stage. As resolution increases, clear denoising trend emergesimages become progressively cleaner and less noisy at each stage. Additional generated samples along with their input text prompts are shown in Figure 5 (512512) and Figure 6 (10241024). PixelFlow demonstrates high visual fidelity and strong text-image alignment, effectively capturing key visual elements and their relationships from complex prompts. Notably, it generates fine-grained detailssuch as animal fur, human hair, and hat textureshighlighting its strong attention to detail in pixel space. 5. Conclusion We introduce PixelFlow, novel image generation model that re-think the predominance of latent space based models by directly operating on raw pixel space. By directly transforming between different resolution stages, our model exhibits compelling advantage in simplicity and end-toend trainability. On both class-conditional image generation and text-to-image generation benchmarks, PixelFlow has been proven to demonstrate competitive image generation capabilities compared to popular latent space-based methods. We hope that this new perspective will inspire future research in visual generation models. Limitations Despite its advantages, PixelFlow still faces Although the model avoids fullcertain limitations. resolution computation across all stages, the final stage requires full-resolution attention, which accounts for roughly 80% of the total inference time. Moreover, we observe that training convergence slows as the sequence length increases. Addressing these challenges presents opportunities for future improvements in efficiency and scalability. 8 Figure 6. Qualitative samples of PixelFlow. We present the generated images of 10241024 resolution. Key words are highlighted in RED."
        },
        {
            "title": "References",
            "content": "[1] Michael Samuel Albergo and Eric Vanden-Eijnden. BuildIn The ing normalizing flows with stochastic interpolants. Eleventh International Conference on Learning Representations, 2023. 2 [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 7 [4] Andrew Campbell, William Harvey, Christian Dietrich Weilbach, Valentin De Bortoli, Tom Rainforth, and Arnaud Doucet. Trans-dimensional generative modeling via jump In Thirty-seventh Conference on Neural diffusion models. Information Processing Systems, 2023. 4 [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 6 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 4, [7] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64416451, 2024. 1, 4, 7 [8] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. 1, 4 [9] Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972, 2023. 7 [10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 4 [11] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Ibrahim AlabdulJoan Puigcerver, Robert Geirhos, mohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024. 3, 4 [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 4 [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [14] John Dormand and Peter Prince. family of embedded runge-kutta formulae. Journal of computational and applied mathematics, 6(1):1926, 1980. 6 [15] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 3, 4, 7 [18] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In Forty-first International Conference on Machine Learning, 2024. 1 [19] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 2, 4, 7 [20] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua Susskind, and Navdeep Jaitly. Matryoshka diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 1, 2, [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [22] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 1, 2, 7 [23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1 [24] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution imIn International Conference on Machine Learning, ages. pages 1321313232. PMLR, 2023. 2, 7 [25] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. 2, 7 [26] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 2, 4, 7 Ella: [27] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 4, 7 10 [28] Allan Jabri, David Fleet, and Ting Chen. Scalable adaparXiv preprint tive computation for iterative generation. arXiv:2212.11972, 2022. 7 [29] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 3, 4 [30] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Pagoda: Progressive growing of onestep generator from low-resolution diffusion teacher. arXiv preprint arXiv:2405.14822, 2024. 2, [31] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. 7 [32] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. 4 [33] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 4 [34] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 6 [35] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, [36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 7 [37] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. 2, 7 [38] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 2 [39] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 1 [40] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 2 [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [42] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 2, 3, 7 [43] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 4 [44] NVIDIA. Edify image: High-quality image generation with pixel space laplacian diffusion model. arXiv preprint arXiv:2411.07126, 2024. 1 [45] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2, 3, 4, 5, 7 [46] Pablo Pernias, Dominic Rampas, Mats Richter, Christopher Pal, and Marc Aubreville. Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv preprint arXiv:2306.00637, 2023. [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 2, 7 [48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 7 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 7 [51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [52] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superIEEE transactions on resolution via iterative refinement. pattern analysis and machine intelligence, 45(4):47134726, 2022. 1, 2 [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [54] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 7 [55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 7 11 [69] Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, and Ping Luo. Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. 1 [70] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 7 [56] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 2 [57] Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, et al. Ldm3d: Latent diffusion model for 3d. arXiv preprint arXiv:2305.10853, 2023. [58] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [59] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 4, 7 [60] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. 4 [61] Michael Tschannen, Andre Susano Pinto, and Alexander Kolesnikov. Jetformer: An autoregressive generative model of raw images and text. arXiv preprint arXiv:2411.19722, 2024. 7 [62] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. 3, 4 [63] Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernandez Abrevaya, David Picard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. arXiv preprint arXiv:2404.13040, 2024. 6 [64] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 7 [65] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, qiang liu, and Jiashi Feng. PeRFlow: Piecewise rectified flow as universal plug-and-play accelerator. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. [66] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2 [67] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. LION: Latent point diffusion models for 3d shape generation. In Advances in Neural Information Processing Systems, 2022. 1 [68] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024."
        }
    ],
    "affiliations": [
        "Adobe",
        "The University of Hong Kong"
    ]
}