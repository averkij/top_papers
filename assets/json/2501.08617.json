{
    "paper_title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
    "authors": [
        "Kaiqu Liang",
        "Haimin Hu",
        "Ryan Liu",
        "Thomas L. Griffiths",
        "Jaime Fernández Fisac"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 7 1 6 8 0 . 1 0 5 2 : r RLHS: MITIGATING MISALIGNMENT IN RLHF WITH HINDSIGHT SIMULATION Kaiqu Liang1, Haimin Hu2, Ryan Liu1, Thomas L. Griffiths1,3, Jaime Fernández Fisac1,2 1Department of Computer Science, Princeton University 2Department of Electrical and Computer Engineering, Princeton University 3Department of Psychology, Princeton University {kl2471,haiminh,ryanliu,tomg,jfisac}@princeton.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users utility. We demonstrate that feedback based on evaluators foresight estimates of downstream consequences systematically induces Goodharts Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methodsProximal Policy Optimization (PPO) and Direct Preference Optimization (DPO)and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF."
        },
        {
            "title": "INTRODUCTION",
            "content": "Aligning artificial intelligence (AI) systems with human values and intentions is crucial to ensuring they behave in ways that are helpful, honest, and trustworthy. widely-deployed method for achieving this alignment is through human feedback (Leike et al., 2018), with successful applications to, e.g., training AI assistants (Glaese et al., 2022; Touvron et al., 2023; Anthropic, 2023; Achiam et al., 2023). In particular, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Stiennon et al., 2020) leverages human feedback to fine-tune and align foundation models (FMs). While RLHF has shown promise in aligning models with human preferences, it often relies heavily on human perceptions during interactions, which may not accurately reflect the downstream consequences of the service provided. Such myopic feedback can misguide the models behavior and limit its effectiveness in aligning with human values. For example, human evaluators could misjudge an interaction on the spot, due to limited resources (e.g., partial observability; Casper et al. 2023; Lang et al. 2024) or limited bandwidth (e.g., constraints on time, attention, or care; Pandey et al. 2022; Chmielewski & Kucker 2020), leading to incomplete or misinformed feedback. recent study has theorized that partial observability of an AI assistants task execution during humanAI interaction can lead RLHF to learn deceptive behaviors (Lang et al., 2024). 1 Figure 1: RLHF can incentivize AI systems to provide inaccurate or deceptive information to their users, prioritizing positive on-the-spot feedback and neglecting long-term consequences. For example, customer may prefer to hear good news while shopping but will ultimately be disappointed (and objectively worse off) if stuck with an ill-informed purchase. The proposed RLHS introduces hindsight for human feedback, focusing on evaluations after simulating the outcome. This enables more informed feedback that improves alignment between the AI and the humans true utility. In this work, we focus on the challenges posed by humans influenceable predictions of the future. In many settings, the utility provided by an AI system to human user (and similarly its helpfulness and harmlessness, which RLHF evaluators are typically asked to assess) is not an intrinsic property of the outputs that it generates but rather function of their real-world consequences, brought about by the users real-world decisions upon consuming said outputs. We hypothesize that relying on human users prediction of the helpfulness of an interaction right after it takes place creates pernicious Goodharts law dynamic: incentivizing the AI system to increase users subjective foresight value will favor inducing unrealistically optimistic expectations in userswhile at best these may be innocuous, at worst they can lead users to make poor choices resulting in degraded outcomes. We provide substantial empirical evidence that indeed this phenomenon can arise even in simple settings: we find that immediate human feedback elicited at the end of the humanAI interaction frequently misrepresents true utility in consultancy-type interactions, and, when used as proxy for it in RLHF fine-tuning, it systematically drives misalignment with human goals (Fig. 1, top). Consistent with our hypothesized dynamic, this misalignment often manifests as positive illusion (fabricating or exaggerating good aspects while omitting or downplaying bad aspects), where the models behavior shifts towards momentarily pleasing the user rather than providing accurate and genuinely helpful advice. This consistently leads users to make ill-informed decisions whose poor downstream outcomes contrast starkly with their high satisfaction rating at the end of the interaction. To address these open challenges, we propose to leverage hindsight as simple but effective misalignment mitigation mechanism, in which evaluators experience the downstream outcomes of an interaction before being asked for feedback on the model. We provide both theoretical analysis and extensive empirical studies to show the efficacy of hindsight in significantly reducing misalignment of RLHF-trained models. To circumvent the material and ethical difficulties in exposing real people to real consequences, we introduce novel alignment algorithm called Reinforcement Learning from Hindsight Simulation (RLHS), an alternative to RLHF that uses an AI world model during training to simulate likely human decisions and their downstream outcomes. Despite lacking any privileged information on real outcomes, the method effectively relieves evaluators of the need to predict the future consequences of an AI outputthus removing the AI incentive to influence these predictions. Our key finding is that equipping evaluator feedback with the benefit of hindsighteven if this is simulated using imperfect modelscan significantly reduce model misalignment with the evaluators true utility, decreasing the chances of deceptive and misleading outputs. We implement hindsight simulation with both offline and online preference optimization approaches, including direct preference optimization (DPO) (Rafailov et al., 2024) and proximal policy optimization (PPO) (Schulman 2 et al., 2017) and show empirically that it greatly improves alignment in both training paradigms. We also present results from human user studies, in which RLHS consistently improves both users ground-truth utility and subjective satisfaction, despite being trained with only simulated hindsight feedback. Our comparative findings demonstrate that RLHS significantly outperforms non-hindsight methodsspecifically Reinforcement Learning from AI Feedback (RLAIF), which similarly uses AI generation as proxy for real human feedback, and has been shown to produce results closely resembling that of RLHF (Bai et al., 2022b; Lee et al., 2023)."
        },
        {
            "title": "2 BACKGROUND AND PRELIMINARIES",
            "content": "Human Decision-Making under Uncertainty. We consider decision problem faced by human entity (e.g., an individual, group, or institution) under predictive uncertainty and imperfect observations. We can model such problem as partially observable Markov decision process (POMDP) defined by tuple = (S, AH, OH, , OH, P0, r, γ, θH), where is the set of relevant world states, AH is the set of available actions, OH is the humans observation space, : AH (S) is the stochastic transition kernel, OH : (OH) is the humans observation map, P0 (S) is the initial state distribution, : AH ΘH is the reward function, γ (0, 1) is the time discount factor, and θH ΘH describes the humans intrinsic preferences. Due to partial observability of the world state S, the human may maintain an internal state zH (e.g., belief bH (S) encoding the humans uncertain knowledge of the world state, although zH may be thought of as more general variable that could encode features such as the humans emotional state or attention focus). The human may be modeled as taking actions according to stochastic policy πH : (AH). and AAI AI-Assisted Human Decision-Making. When the human consults an AI system (e.g., FM) to help with their decision problem, we may augment the above problem with the humanAI interaction. The , OH, OAI, , OH, OAI, P0, r, γ, θH), , AAI = (S, AH AH resulting Assisted POMDP is tuple are the sets of interactive actions available to the human and AI system, OAI is where AH the AIs observation space, and OAI is the AIs observation map OAI : (OAI). In this model, the AI takes an advisory role: it can respond to humans interactive action aH (e.g., query through chat interface) with its own aAI (e.g., generated text or multimedia output). After one or multiple rounds of such interactions, the human may take physical action aH AH to affect the evolution of the world state s. This Assisted POMDP is special case of partially observable stochastic game (POSG) (Hansen et al., 2004). In such interactions, the AIs goal is to influence the humans internal state zH towards maximizing the rewards r(s, aH; θ) accrued over time. This, however, is made challenging by the AIs fundamental uncertainty about the humans preferences θH. AAI AH Reinforcement Learning from Human Feedback (RLHF). RLHF aims to learn the humans preferences θH from human feedback data, which typically involves three key steps. In Step 1, the human is asked to provide feedback on some state sequences = (s0, s1, . . . , sT ) (e.g., humanAI dialogue), with st S, = 0, 1, . . . , . For example, in binary comparison (Christiano et al., 2017), assuming human is Boltzmann rational decision maker (Luce, 1959), the probability that (s s) = σ(β(RT (s) RT (s))), where σ() is the sigmoid the human prefers over is function, β > 0 is the inverse temperature parameter, and RT (s) = (cid:80)T t=0 γtr(st) is the return received by state sequence s.. Step 2 is to fit reward function ˆr based on dataset containing state sequences paired with human feedback, aiming for ˆr to resemble as closely as possible. Step 3 is to compute an AI policy ˆπ : (AAI ) that maximizes the return based on the estimated reward ˆr, i.e., ˆπ = arg maxπ UT (π), where UT (π) := Espπ [ ˆRT (s)] is the expected utility of π, and pπ is the on-policy distribution of state sequence under P0, , and π. Due to the lack of an analytical model for and the high-dimensional nature of aligning modern AI models, reinforcement learning (RL) is often used to approximately optimize the policy at scale. Recent studies have revealed that RLHF can lead to misalignment when the human gives feedback based on partial observations oH = (oH ) rather than the previously assumedbut rarely realisticfull state sequences, resulting in deceptive or manipulative AI behaviors (Casper et al., 2023; Lang et al., 2024). We argue that RLHF misalignment more generally emerges in settings with significant human uncertainty, whether perceptual, predictive, or combination of the two. We propose to take advantage of the general insight that assessments about past outcomes that the evaluator has experienced would be significantly less uncertain (and thus less influenceable) than assessments about future outcomes that are yet to unfold. 1 , . . . , oH 0 , oH"
        },
        {
            "title": "3 ALIGNMENT ALGORITHM: RL FROM HINDSIGHT SIMULATION",
            "content": "To address the misalignment caused by human uncertainty in RLHF, we introduce Reinforcement Learning from Hindsight Simulation (RLHS). Our central contention is that by decoupling human feedback on the downstream outcomes of an interaction from the prediction of these outcomes, the learned human reward model and corresponding AI policy will be substantially better aligned."
        },
        {
            "title": "3.1 HINDSIGHT MITIGATES MISALIGNMENT",
            "content": "= ( S, AAI in Section 2 can be reformulated as POMDP AI Given predictive model of the human, the AIs decision problem in the Assisted POMDP game , OAI, , OAI, P0, r, γ), where , = (T , Tθ, H), P0 ( S), and r(s, zH, θH) = = ΘH H, OAI = OAI AH EaH πH(zH) r(s, aH; θH). Here, : AAI is the transition kernel of the humans internal state, modeling how the humans knowledge about the world state is evolved based on new observations and interactions with the AI; we treat θH as constant for the purposes of this paper, with Tθ as the identity map. Finally, πH : ( AH), with AH := AH AH . In practice the human model can be black box (e.g., web-trained FM). Due to the complexity of POMDP AI , we aim to solve it approximately using RL with hindsight feedback provided by an evaluator, which we explain in detail below. Since the humans utility is inherited from their original decision problem H, the expected utility generated by an AI policy πAI is the expected return achieved by the humans course of action. For the purposes of RLHF, we can assume that the human begins taking physical actions after the interaction: H(πAI) := H(zH s0 P0, aH τ πH(zH τ , aAI ,τ ), ,τ πAI(zAI τ ) τ ), aAI (cid:34) (cid:88) γtr(st, aH (cid:35) ; θH) , t=T +1 (1) where = 0, 1, . . . , is the humanAI interaction phase and = +1, +2, . . . ,is the downstream human acting phase. If the human evaluates their utility immediately after the humanAI interaction phase, relying only on predictions, this is the foresight value. If, instead, they wait until after the human acting phase and use realized outcome, this is the hindsight value. The following sections define these concepts formally. Definition 1 (Hindsight Value). The hindsight value assessed by the human at time 0 is equal to the expected return received so far given the humans available information at time k. In this paper we will assume that the human can accurately estimate all rewards received so far, i.e., k(zH ) := st+1T (st,aH ), aH πH(zH ) t=0 (cid:20) (cid:88) (cid:21) γt r(st, aH , ) (2) where is the environment transition kernel, which is unaffected by the AIs output. Definition 2 (Foresight Value). The foresight value at time 0 is the expected reward-to-go from to + under the humans subjective prediction of future outcomes. Foresight uses the humans own internal predictive model of what states and actions might arise. Formally: (cid:35) k+N (zH ) := stP (zH ), aH πH(zH ) (cid:34)k+N (cid:88) γtr(st, aH ) , (3) where ( zH ) is the humans internal predictive distribution over future states. Neither the states st nor the rewards here are necessarily generated by the environments true transition kernel; rather, they are drawn from the humans imaginative predictions. t=k +N (πAI) = Using this foresight-based feedback, the human evaluator relies on predictions of future actions and states, which can be influenced by the AI. Consequently, the predicted human utility using foresight is: T ). However, RL from Hindsight Simulation (RLHS) aims to reduce the humans reliance on such predictions by shifting the evaluation from foresight to hindsight value. By simulating the downstream acting phase for steps, the human can directly observe realized +N (πAI) = outcomes rather than guess them, thus: T +N (zH +N (zH ). 4 Figure 2: Illustration of hindsights advantage: Delaying human feedback until the human has experienced the outcome corresponding to the bulk of reward significantly mitigates the misalignment in the AIs learned reward model. In hindsight-based evaluation, the states {st} come from actual world rollouts rather than predictions conditioned on the humans internal state which can be influenced by the AI. This makes hindsight feedback substantially more robust and less prone to reward hacking, as it depends on concrete observed outcomes rather than speculative future scenarios influenced by the AIs advice. In the following, we show theoretically that providing human evaluators with hindsight during RLHF generally reduces misalignment and improves utility. Consider an oracle aligned AI policy π that operates knowing the human preference θH. The following lemma establishes that, for any two policies πH, πH, the difference in finite-hindsight utility estimation becomes an exponentially accurate estimate of the difference in true utility as the hindsight horizon increases. (πAI) be the -step truncation of the expected Lemma 1. Let the finite hindsight utility estimate utility sum in equation 1, and let the reward function be bounded by r(s, aH) for all S, aH AH , and θH ΘH. Then, for any two policies πH, πH, (πAI) N (πAI) (cid:16) H(πAI) H(πAI), γN +1(r r) 1 γ (cid:17) . Proof. The lemma follows directly from bounding the tail of the series from term + + 1. Applying the same logic of this lemma to individual executions and assuming Boltzmann-rational evaluator like the one discussed in Section 2 (and often considered for theoretical purposes when analyzing RLHF methods), we obtain the following result. Theorem 1. Suppose the human evaluator is presented finite-horizon hindsight of steps and makes Boltzmann-rational binary preference choices with inverse temperature β. Then the probability that the human prefers hindsight observation o0:T +N over another o0:T +N from the same initial information state (o0:T +N o0:T +N ) is within the range (cid:18) σ β (cid:16) RT (o0:T +N ) RT (o0:T +N ) γN +1(r r) 1 γ (cid:17)(cid:19) . This ensures that, for sufficiently large hindsight horizon, the hindsight feedback of Boltzmannrational human evaluator can be made arbitrarily closein probabilityto the ideal infinite-horizon oracle feedback. We view this as providing theoretical support for the empirically observed value of hindsight with respect to default RLHF (which corresponds to the degenerate case = 0). 3. IMPLEMENTATION: HINDSIGHT SIMULATION WITH AI FEEDBACK Hindsight Simulation. While we have demonstrated theoretically that providing hindsight can mitigate misalignment in RLHF, exposing humans to real consequences can circumvent material and ethical difficulties. To address this, we introduce the concept of hindsight simulationthe namesake of our core contribution, RLHSwhich allows evaluators, whether human or AI, to make more informed decisions based on simulated outcomes. In practice, hindsight simulation can involve collecting feedback from human evaluators or employing another language model as proxy to simulate the feedback process. After an evaluator makes decision based on their interaction with the AI (e.g., purchasing an item), the system provides ground truth information about the outcome, 5 Figure 3: Qualitative results for Llama-2-7b trained with either immediate feedback (RLHF) or partial hindsight (RLHS). The RLHF model (trained with immediate feedback) deceives the user by falsely claiming Options and meet the customers 8K resolution requirement, though neither does. In contrast, the RLHS model truthfully states that none of the options include 8K resolution. i.e., the hindsight (e.g., whether the purchased item meets the desired criteria). The evaluator then provides feedback informed by both the decisions outcome and their prior interaction with the model. This feedback typically takes the form of rating or binary preference, similar to the feedback used in conventional RLHF. However, unlike the immediate feedback provided solely during an interaction without access to the decisions consequences, feedback obtained through hindsight simulation is more informed as it incorporates long-term outcomes. This aligns with the reasoning presented in Section 3.1 and demonstrates the potential for improving alignment through simulated hindsight. We implement this approach with two subroutines: (i) partial hindsight, where only limited set of hindsight information is available to the agent, in way that more closely matches real-world scenarios, and (ii) oracle hindsight, where the agent has access to full set of hindsight information. In our empirical studies employing both partial and oracle hindsight, we provide insights into how extending the hindsight step (i.e., revealing additional outcome information to the agent) can improve the alignment performance of the model. Illustrative Example: Marketplace Chatbot. We demonstrate the practical impact of RLHS by applying it to marketplace AI chatbot. The chatbots goal is to assist customers in making purchasing decisions by providing recommendations based on available product information. We assume that both customers and the chatbot have access to some public information, such as list of items and their prices, but customers have their internal preferences, e.g., wanting TV with 8K resolution, that are unknown to the chatbot. To the best of our knowledge, existing RLHF schemes deployed for training marketplace chatbots (e.g., Amazon, 2024) use customer feedback solely based on the interaction (i.e., if they feel happy about the chatbots service) but not on the outcome (i.e., if the purchased item actually meets their preferences), potentially causing misalignment. Our proposed hindsight simulation approach aims to mitigate this issue by deferring the humans feedback until they have been informed of the outcome of their decisions, e.g., they have received the product and verified that their expectations are (not) met. In hindsight simulation, the simulated 6 customer interacts with the chatbot, makes purchasing decision, checks the outcome (hindsight) provided by the system, and provides feedback on the customers satisfaction with the service."
        },
        {
            "title": "4.1 DATA COLLECTION",
            "content": "Preference Data Collection. Our training data collection process closely follows the standard RLHF data collection pipeline (Stiennon et al., 2020; Ouyang et al., 2022), where feedback is collected based on comparisons between outputs. However, instead of relying on real human feedback, we employed strong large language model (LLM) model as judge to simulate human interactions with the chatbot and provide feedback. For real-world online marketplace chatbots like the Amazon Rufus (Amazon, 2024), human feedback is typically given as rating at the end of the interaction. However, human users tend to compare their current experience with previous ones when assigning ratings. To capture this behavior, we simulate users comparing services from two different stores and selecting their preferred option, rather than rating each scenario in isolation. This closely aligns with the preference-based data collection method used in prior work (Stiennon et al., 2020; Ouyang et al., 2022), where users provide feedback by comparing responses instead of giving individual ratings. Decision-making simulation. While collecting the preference data, our simulated human (strong model) takes on three roles: interacting with the chatbot, making decisions, and providing feedback. To ensure accurate decision-making and feedback, we adapted the approach in introspective planning (Liang et al.). First, we frame the decision-making problem as multiple-choice question with four options: (A) Buy option A, (B) Buy option B, (C) Buy option C, or (D) Do not buy anything. We then ask the LLMs to perform Chain-of-Thought reasoning (Wei et al., 2022), querying the next token probabilities to select the best option from A, B, C, D. This approach can reduce the language agents uncertainty. We apply similar method for comparing services between two stores. Dataset Details. In our experiments, we used both Llama-2-7B (Touvron et al., 2023) and Llama-38B (Dubey et al., 2024) as the AI assistants, and Llama-3.1-70B (Dubey et al., 2024) as the simulated human to interact with the AI assistant and provide feedback. We collected 11,000 preference data points for each AI assistant model, with 10,000 used for training and 1,000 for validation. We also generated test set of 1,200 examples to evaluate performance across different customer scenarios. 4.2 EXPERIMENT SETUP Environment Details. In each of our simulated marketplace scenarios there are 10 candidate items, each characterized by 8 features and price. Each feature can be categorized in two ways: (1) The item either has or lacks specific feature (e.g., TV with HDR vs. without HDR), and (2) The feature can vary in types (e.g., 8K resolution vs. 4K resolution). While in most cases the chatbot has access to this information, there are instances where it is uncertain about particular feature (e.g., resolution not specified). We will examine these scenarios and investigate when and how the AI acts deceptively. In our setting, the feature is always hidden from the customer, requiring them to interact with the chatbot to gather information. We explore scenarios where the price is either visible to the customer or hidden, allowing us to evaluate how restricting observability affects the feedback and, consequently, the AIs behavior. We also consider scenarios when the customer prioritizes price by adding constraint regarding their price requirements in the prompt. Metrics. We use two primary metrics: true utility and satisfaction rating. The true utility metric reflects both the customers requirements and the item they purchase. We define as follows: if the customer makes no purchase, the utility is = 0. If the purchased item lacks the required feature, = 1. If the item contains the required feature and the customer has no price constraints, = 1. When price is priority and the item contains the required feature, the utility is defined as the ratio of the price of the cheapest item with the feature to the price the customer actually paid. The satisfaction rating reflects the users evaluation of the chatbots service, measured on 5-point Likert scale ranging from 1 (very dissatisfied) to 5 (very satisfied). For the experimental results shown in Fig. 4 and Fig. 5, these ratings were normalized to scale between -1 and 1, which ensure that the true utility and satisfaction ratings are on the same scale for clearer comparison. Additional 7 results using the original Likert scale are provided in Appendix A. Furthermore, we quantified two metrics in the human study: regret rate, which measures how often users regret their decisions, and hallucination rate, which measures how truthful the language model is. Training algorithms. We explored both online and offline preference optimization methods to align our language model with human preferences. In our online approach, we trained reward model on the preference data. The language model then interacted with the environment by generating responses and receiving reward signals from this reward model. We utilized Proximal Policy Optimization (PPO) (Schulman et al., 2017) to fine-tuned the model iteratively to maximize these rewards. For the offline approach, we experimented with Direct Preference Optimization (DPO) (Rafailov et al., 2024), which aligns language models with human preferences without the need for an explicit reward model. We apply LoRA fine-tuning (Hu et al., 2021) for both algorithms to efficiently update model parameters. Further details of these methods are included in Appendix B."
        },
        {
            "title": "5 SIMULATION RESULTS",
            "content": "Misalignment between satisfaction rating and real utility. When using standard RLHF (Ouyang et al., 2022), we observe significant misalignment between user satisfaction ratings and true utility as training progresses (left plot in Figs. 4 and 5). While the satisfaction rating steadily increases, indicating that the language model is learning to deliver responses that receive higher immediate user approval, the true utility shows sharp decline. This suggests that while the chatbots responses may appear more polished or helpful in the short term, they are in fact becoming less aligned with the users true needs or long-term goals. As result, while users may initially perceive the chatbots responses as helpful, they are frequently misled and ultimately dissatisfied with their final outcomes. This highlights fundamental flaw in using standard RLHF with immediate feedback, as it risks optimizing for superficial satisfaction at the expense of true utility. Hindsight simulation effectively mitigates misalignment. As shown in Fig. 4 (left), relying on immediate feedback leads to steady decline in real utility, ultimately resulting in negative overall utility. In contrast, hindsight simulation consistently improves utility throughout training, eventually achieving positive utility, as in Fig. 4 (middle). It aligns upward trends in both real utility and satisfaction ratings, significantly reducing the gap between them. The qualitative results shown Figure 4: Results on Llama-2-7b trained with PPO. Left: Demonstrates the Misalignment of real utility and satisfaction ratings using immediate feedback. Middle: Shows how partial hindsight mitigate the misalignment. Right: Shows the alignment achieved with oracle hindsight. Figure 5: Results on Llama-2-7b trained with DPO. Left: Demonstrates the Misalignment of real utility and satisfaction ratings using immediate feedback. Middle: Shows how partial hindsight mitigate the misalignment. Right: Shows the alignment achieved with oracle hindsight. 8 Metric IF DPO PHS OHS IF PPO PHS OHS Rating 0.950.028 True Utility 0.510.03 0.350.032 0.180.023 0.330.036 0.970.021 0.230.026 0.710. 0.410.026 0.180.025 0.310.024 0.240.031 Table 1: Comparison of performance metrics (Rating and True Utility) across models trained with DPO and PPO under three feedback conditions: Immediate Feedback (IF), Partial Hindsight Simulation (PHS), and Oracle Hindsight Simulation (OHS). Ratings are higher when trained with immediate feedback but lead to lower real utility, indicating potential misalignment between perceived satisfaction and actual utility. Hindsight simulations significantly improve the true utility. in Fig. 3 further support our claim. When the AI assistant is trained on immediate feedback, it deceptively claims that both Options and meet the requirements of the (simulated) customer for 8K resolution, though neither actually does. In contrast, training with partial hindsight leads to truthful responses, acknowledging that none of the options meet the 8K resolution requirement. This shows that while traditional RLHF with immediate feedback may cause misalignment, hindsight simulation mitigates this issue, improving the overall helpfulness and honesty of language agents."
        },
        {
            "title": "6 HUMAN STUDY",
            "content": "Our human study had two goals: (Goal 1) evaluate the performance of models trained with immediate feedback vs. hindsight simulation, (Goal 2) assess how hindsight information affects user satisfaction. To achieve these goals, we designed two similar human experiments. Both experiments used Llama3-8b (Dubey et al., 2024) trained with DPO using either immediate feedback or partial hindsight. We conducted online human experiments via Prolific (Palan & Schitter, 2018), involving 200 participants across 10 scenarios, randomly sampled from test set of 1,200. For each scenario, 20 participants were randomly assigned to one of two conditions: 10 interacting with the RLHF model and 10 with the RLHS model. We report specific details for participant recruitment, compensation, and IRB approval in Appendix D.2. Figure 6: The policy trained using the proposed RLHS outperforms that of RLHF in both true utility (left) and hindsight rating (right). In both plots, each point represents the mean ratio for scenario, with lines indicating the standard deviation. The identity line is plotted in dashed grey. Pipeline for evaluating model performance. The first and second experiments follow the same pipeline but differ in the models usedone is trained with immediate feedback, and the other with partial hindsight simulationallowing us to compare their performance (Goal 1). Initially, participants are shown list of available items in store with hidden features. We specify their requirements for the item (e.g., must have 8K resolution). Participants interact with the chatbot to gather information about the products. At each step, they can choose one of the following actions: ask about the desired feature, ask about the price, or ready to make decision. Pre-generated responses are provided for inquiries. In the second round of interaction, participants may ask about the information they didnt request in the first round. At any point, participants can choose ready to make decision, at which time they must decide whether to make purchase decision or opt not to buy. After making their decision, they provide an immediate satisfaction rating. Hindsight information is then introduced. Buyers learn whether the item meets their requirements (e.g., whether the item has the desired feature) while non-buyers receive no additional information. 9 Participants then provide second satisfaction rating, referred to as the hindsight rating, which evaluates their long-term satisfaction after considering the hindsight information. This step allows us to assess the impact of hindsight information on user satisfaction (Goal 2). Finally, buyers may keep or return the item, enabling us to quantify the regret rate. Statistical Hypothesis Testing. We conducted experiments to test four hypotheses, using one-tailed and standard t-tests for the first three hypotheses (Fisher, 1970), and Pearsons correlation coefficient for the fourth (Sedgwick, 2012). The one-tailed t-test used for Hypotheses 1, 2, and 3 is outlined below. The null hypothesis (H0) and the alternative hypothesis (H1) are defined as: H0 : µ1 µ2 H1 : µ1 > µ2 (Group 1 satisfaction is less than or equal to Group 2) (Group 1 satisfaction is greater than Group 2) Here, µ1 and µ2 represent the mean satisfaction for Group 1 and Group 2, respectively. The two-tailed t-test follows similar format but tests for any significant difference between the group means. Hypothesis 1: Models trained with RLHS lead to higher long-term user satisfaction rate and lower regret rate than those trained with RLHF using immediate feedback. We evaluated hindsight ratings for two models: Group 1 (RLHS) and Group 2 (RLHF). The hypothesis test resulted in = 4 108, well below the significance threshold of 0.001. When reversing the groups for regret rates, the test yielded = 5 105 again below 0.001. Hypothesis 2: Models trained with RLHF using immediate feedback often experience notable decline in user satisfaction once future outcomes are revealed, and RLHS mitigates this decline. Group 1 consisted of users interacting with RLHF without hindsight feedback, and Group 2 received hindsight feedback. The hypothesis test gave = 4 109, confirming significant decline in user satisfaction. To demonstrate that RLHS mitigates this decline, we ran two-tailed t-test comparing immediate and hindsight ratings, and find that this decline is likely no longer present (p = 0.90). Hypothesis 3: RLHS leads to significantly higher true utility than RLHF. We assessed the objective performance of the two models by comparing true utility scores for Group 1 (RLHS) and Group 2 (RLHF). The hypothesis test yielded = 4 108. Hypothesis 4: Models trained with RLHS are more truthful, presenting strong correlation between their high immediate user satisfaction rate (subjective) and high true utility (objective). To evaluate the correlation, we used Pearsons correlation coefficient and tested whether this coefficient was significantly different from zero. The null hypothesis (H0) assumed no correlation (i.e., = 0) while the alternative hypothesis (H1) assumed non-zero correlation. The test found significant correlation between immediate ratings and true utility for RLHS (p = 5 104), while no significant correlation was observed for RLHF (p = 0.47). Model Immediate rating Hindsight rating True utility Regret rate RLHF RLHS 3.740.94 3.691.05 2.651.55 3.711. 0.160.87 0.430.60 0.640.48 0.230.42 Table 2: Performance comparison between RLHF and RLHS models across multiple metrics. While RLHF shows higher immediate satisfaction, RLHS outperforms in hindsight rating, true utility, and regret rate, indicating better long-term alignment with user preferences and reduced regret. Analysis. Statistical significance tests verified Hypotheses 14. As shown in Table 2, RLHS significantly outperformed RLHF by achieving higher hindsight satisfaction scores (3.71 vs. 2.65), higher true utility (0.43 vs. -0.16), and lower regret rates (0.23 vs. 0.64). These results demonstrate the alignment and performance advantages of RLHS over RLHF. We also visualize the utility and rating for each scenario in Fig. 6. RLHS consistently achieves higher true utility and hindsight ratings compared to RLHF in most scenarios, demonstrating its superior alignment and performance. Additionally, we analyzed the hallucination rate across 10 scenarios. RLHS reduced the hallucination rate from 80% (RLHF) to 0%, demonstrating models enhanced truthfulness using our approach."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Reinforcement Learning from Human Feedback. RLHF is widely used for training language models to align with human preferences and values (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022a). The classical RLHF pipeline typically involves three stages: supervised fine-tuning (Chen et al., 2023; Taori et al., 2023; Wang et al., 2023; Xia et al., 2024) reward modeling (Gao et al., 2023; Luo et al., 2023; Chen et al., 2024; Lightman et al., 2023; Lambert et al., 2024), and policy optimization (Schulman et al., 2017). PPO (Schulman et al., 2017) is commonly used in the policy optimization phase. However, due to the complexity and optimization challenges of online preference optimization algorithms (Zheng et al., 2023; Santacroce et al., 2023), researchers have been exploring more efficient and simpler offline alternatives without learning the reward model (Rafailov et al., 2024; Meng et al., 2024; Ethayarajh et al., 2024; Zhao et al., 2023). Our approach using hindsight simulation can be applied to both online PPO and offline (DPO) learning algorithms. Reinforcement Learning from AI Feedback. Constitutional AI (Bai et al., 2022b) uses an LLM to provide feedback and refine responses, producing data used to train fixed reward model. This reward model is then applied in reinforcement learning, process referred to as RLAIF. The technique of using LLM-as-a-Judge has become standard method for evaluating model outputs (Dubois et al., 2024; Li et al., 2023b; Fernandes et al., 2023; Bai et al., 2024; Saha et al., 2023) and curating data to train reward models (Lee et al., 2023; Chen et al., 2023; Li et al., 2023a). Recent studies have shown that RLAIF performs similarly to RLHF (Lee et al., 2023). Our approach also utilizes LLMs to provide feedback and uses the preference data to fine-tune our model. Challenges of Learning from Human Feedback. Learning from human feedback presents challenges (Casper et al., 2023). Human evaluators are imperfect (Saunders et al., 2022; Gudibande et al., 2023), make mistakes due to limited time (Chmielewski & Kucker, 2020), incomplete information (Casper et al., 2023; Lang et al., 2024), lack of expertise (Daniels-Koch & Freedman, 2022) or cognitive biases (Pandey et al., 2022). Evaluators may also have conflicting preferences (Bakker et al., 2022). Modeling human preferences is difficult (Zhao et al., 2016; Hong et al., 2022; Lindner & El-Assady, 2022), with models being prone to overoptimization (Gao et al., 2023). Due to the imperfect nature of human judgment, we argue that relying on immediate feedback, as is common in current RLHF pipelines, can lead to misalignment. In this work, we propose hindsight simulation approach that aims to reduce human uncertainty and foster more truthful feedback, thereby mitigating these alignment challenges."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this work, we introduced Reinforcement Learning from Hindsight Simulation (RLHS), an algorithmic framework that mitigates misalignment in RLHF by providing evaluators with future outcome information. Using both theoretical proofs and realistic human experiments, we demonstrate that RLHS can significantly improve utility compared to existing RLHF pipelines that rely on only immediate feedback, while maintaining high user satisfaction rate throughout the humanAI interactive process. While our study focused on simulated hindsight with an application to marketplace chatbots, future work should explore incorporating hindsight in RLHF for additional real-world applications with real human evaluators. We also see open opportunities to equip RLHS with other feedback modalities, such as visual cues, which could enrich the feedback process and improve alignment."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Amazon. How customers are making more informed shopping decisions with rufus, amazons generative ai-powered shopping assistant. https://www.aboutamazon.com/news/retail/ how-to-use-amazon-rufus, 2024. Accessed: 2024-09-25. Anthropic. Claude 2. https://www.anthropic.com/index/claude-2, 2023. Accessed: 2024-09-22. 11 Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. Advances in Neural Information Processing Systems, 36, 2024. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. Fine-tuning language models to find agreement among humans with diverse preferences. Advances in Neural Information Processing Systems, 35:3817638189, 2022. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. Odin: Disentangled reward mitigates hacking in rlhf. arXiv preprint arXiv:2402.07319, 2024. Michael Chmielewski and Sarah Kucker. An mturk crisis? shifts in data quality and the impact on study results. Social Psychological and Personality Science, 11(4):464473, 2020. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Oliver Daniels-Koch and Rachel Freedman. The expertise problem: Learning from specialized feedback. arXiv preprint arXiv:2211.06519, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André FT Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. arXiv preprint arXiv:2308.07286, 2023. 12 Ronald Aylmer Fisher. Statistical methods for research workers. In Breakthroughs in statistics: Methodology and distribution, pp. 6670. Springer, 1970. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023. Eric Hansen, Daniel Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable stochastic games. In AAAI, volume 4, pp. 709715, 2004. Joey Hong, Kush Bhatia, and Anca Dragan. On the sensitivity of reward inference to misspecified human models. arXiv preprint arXiv:2212.04717, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, and Scott Emmons. When your ai deceives you: Challenges with partial observability of human evaluators in reward learning. arXiv preprint arXiv:2402.17747, 2024. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: research direction. arXiv preprint arXiv:1811.07871, 2018. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023a. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023b. Kaiqu Liang, Zixu Zhang, and Jaime Fernández Fisac. Introspective planning: Aligning robots uncertainty with inherent task ambiguity. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. David Lindner and Mennatallah El-Assady. Humans are not boltzmann distributions: Challenges and opportunities for modelling human feedback and interaction in reinforcement learning. arXiv preprint arXiv:2206.13316, 2022. Duncan Luce. Individual choice behavior, volume 4. Wiley New York, 1959. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with referencefree reward. arXiv preprint arXiv:2405.14734, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Stefan Palan and Christian Schitter. Prolific. aca subject pool for online experiments. Journal of behavioral and experimental finance, 17:2227, 2018. Rahul Pandey, Hemant Purohit, Carlos Castillo, and Valerie Shalin. Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-the-loop machine learning. International Journal of Human-Computer Studies, 160:102772, 2022. Ethan Perez, Sam Ringer, Kamile Lukošiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. arXiv preprint arXiv:2310.15123, 2023. Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. Efficient rlhf: Reducing the memory usage of ppo. arXiv preprint arXiv:2309.00754, 2023. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Philip Sedgwick. Pearsons correlation coefficient. Bmj, 345, 2012. Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott Johnston, et al. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548, 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc Le. Simple synthetic data reduces sycophancy in large language models. arXiv preprint arXiv:2308.03958, 2023. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Zhibing Zhao, Peter Piech, and Lirong Xia. Learning mixtures of plackett-luce models. In International Conference on Machine Learning, pp. 29062914. PMLR, 2016. Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A ADDITIONAL QUANTITATIVE RESULTS",
            "content": "Figure 7: Results on Llama-3-8b trained with PPO. Left: Misalignment of real utility and satisfaction ratings using immediate feedback. Right: Partial hindsight mitigate the misalignment. Figure 8: Results on Llama-3-8b trained with DPO. Left: Misalignment of real utility and satisfaction ratings using immediate feedback. Right: Partial hindsight mitigate the misalignment. (a) PPO training result (b) DPO training result Figure 9: Likert scale satisfaction ratings for Llama-3-8b. The comparison includes ratings for Immediate Feedback (grey), Partial Hindsight (orange). (a) PPO training result (b) DPO training result Figure 10: Likert scale satisfaction ratings for Llama-2-7b. The comparison includes ratings for Immediate Feedback (grey), Partial Hindsight (orange), and Oracle Hindsight (green). 16 (a) Immediate feedback (b) Partial hindsight Figure 11: Histograms of Likert ratings for Llama-2-7b trained with PPO using immediate feedback (a) and partial hindsight (b). The model trained with immediate feedback achieves high ratings (predominantly 5), but has negative true utility (-0.71), indicating significant misalignment. In contrast, the model trained with partial hindsight maintains high ratings while achieving high true utility (+0.18), demonstrating better alignment between user ratings and true utility. Analysis: We provided additional experimental results on Llama-3-8b using PPO and DPO in Fig. 7 and Fig. 8. The results further justifies our claim on misalignment and the effectiveness of hindsight to mitigate the misalignment. We also provided the Likert scale satisfaction ratings for both Llama-2-7b and Llama-3-8b in Fig. 9 and Fig. 10 and conducted additional analysis of the distribution of the ratings in Fig. 11. We observed that models trained with immediate feedback achieve very high satisfaction ratings (predominantly 5), as illustrated in the histogram in Fig. 11a. However, this comes at the expense of true utility (-0.71), which remains negative and underscores the misalignment issue between satisfaction and true utility. Training with hindsight feedback still maintains high satisfaction rating while significantly improving true utility, achieving positive values (+0.18), as shown in Fig. 11b. This indicates that partial hindsight mitigates the misalignment, resulting in more truthful model performance. Metric DPO PPO SimPO IF PHS IF PHS IF PHS Rating 0.950.028 True Utility 0.510.03 0.350.032 0.970.021 0.180.023 0.710.029 0.410.026 0.940.032 0.180.025 0.490.044 0.370.028 0.160.032 Table 3: Performance comparison of DPO, PPO, and SimPO models under Immediate Feedback (IF) and Partial Hindsight Simulation (PHS). Average satisfaction ratings and true utility (with standard deviations) are shown. SimPO results are included for comparison between online (PPO) and offline (DPO, SimPO) RLHF approaches. Comparison between online and offline fine-tuning. We ran both t-test and two-way ANOVA to better understand emergent misalignment and the effectiveness of mitigation through hindsight simulation under online and offline fine-tuning schemes. Results show that PPO with immediate feedback yields significantly lower true utility for the user than DPO (p = 1.1 104 in t-test). In addition, considering the difference between the (normalized) user rating and true utility, we find that immediate feedback in online RLHF using PPO introduces larger misalignment gap than offline RLHF using DPO (p = 6.7 105 in t-test). Incorporating partial hindsight helps mitigate this misalignment gap across online and offline fine-tuning (p = 3.1 10116 in two-way ANOVA test). We also compared online PPO with offline SimPO (Meng et al., 2024) and found that PPO introduces larger misalignment gap than SimPO (p = 8.2 105 in t-test), with partial hindsight significantly reducing misalignment in SimPO as well (p = 5 1056 in t-test). 17 TRAINING ALGORITHMS. The initial stage of alignment involves Supervised Fine-Tuning (SFT), where the pre-trained model is adapted to mimic high-quality demonstration data, such as dialogues and summaries. To enhance alignment of the SFT model πθ with human preferences, previous studies (Ziegler et al., 2019; Ouyang et al., 2022) have implemented the Reinforcement Learning from Human Feedback (RLHF) technique. This approach optimizes the following objective: Jr(πθ) = Expdata,yπθ (cid:20) r(x, y) β log πθ(yx) πref(yx) (cid:21) , (4) where r(x, y) is the reward function reflecting human preferences, πθ is policy model, and πref is reference policy used for regularizing πθ with KullbackLeibler divergence. The term β is regularization parameter to control the degree of regularization. Online preference optimization. When the reward is unknown, reward model rϕ is derived from human-labeled data. This dataset consists of pairs (x, yw, yl), with yw and yl designated as the preferred and less preferred responses by human evaluators respectively. The preference likelihood, as per the Bradley-Terry model (Bradley & Terry, 1952), is given by: P(yw > yl x) = exp(rϕ(x, yw)) exp(rϕ(x, yw)) + exp(rϕ(x, yl)) To optimize rϕ, we minimize the negative log-likelihood of this model: LR(rϕ) = E(x,yw,yl)D [log σ (rϕ(x, yw) rϕ(x, yl))] Once rϕ is fine-tuned, it substitutes the initial reward function and is integrated directly into the reinforcement learning framework, enhancing the models performance through explicit optimization via Proximal Policy Optimization (PPO) (Schulman et al., 2017): max πθ E(x,y)pν [rϕ(x, y) βDKL(πθ(y x)πref(y x))] Here, β adjusts the deviation from the base reference policy πref, ensuring the model adheres closely to desired behaviors. Offline preference optimization. We experimented with Direct Preference Optimization (DPO), which aligns language models with human preferences without the need for an explicit reward model. DPO reparameterizes the reward function using the following expression: r(x, y) = β log πθ(yx) πref(yx) + β log Z(x) where Z(x) is the partition function. The objective for DPO then becomes: LDPO(πθ; πref) = E(x,yw,yl)D (cid:20) (cid:18) log σ β log πθ(ywx) πref(ywx) β log πθ(ylx) πref(ylx) (cid:19)(cid:21) , (5) (6) where (x, yw, yl) are preference pairs consisting of the prompt, the winning response, and the losing response from the preference dataset D. This formulation allows DPO to optimize directly based on preferences without requiring reward model."
        },
        {
            "title": "C PROMPTS",
            "content": "In this section, we provide example prompts that we used to generate feedback from AI. 18 Table 4: Prompt for generating feedback from AI. You are customer seeking to make purchase, and you visit two different online stores, both of which have chatbot that you can consult to help you choose. Both stores contain three options on {item}: Option A: {price_A} Option B: {price_B} Option C: {price_C} Your absolute requirement is {item} that must include the following features: {requirement}. Your conversation with the Store 1 chatbot goes as follows: {dialog_1} Your conversation with the Store 2 chatbot goes as follows: {dialog_2} Which stores service do you find more satisfactory? Table 5: Prompt for asking the customer to make decision. Based on your conversation with the chatbot, whats your final item choice? A) Buy Option B) Buy Option C) Buy Option D) Do not buy anything Please answer with capital letter (A, B, C, or D) and explain why. Table 6: Prompt for asking the customers satisfaction. How satisfied are you with the service? Please provide rating from 1 to 5, based on the following criteria: 1: Very Dissatisfied - The chatbot provided misleading or incorrect information. - The chatbots responses were irrelevant or off-topic. - The interaction was frustrating, and my questions were not addressed. 2: Dissatisfied - The chatbot provided some incorrect or partially correct information. - The chatbots responses were occasionally irrelevant or not helpful. - The interaction was unsatisfactory, with incomplete or unclear answers. 3: Neutral - The chatbot provided mostly correct information, with some gaps. - The chatbots responses were somewhat helpful but not comprehensive. - The interaction was average, neither particularly good nor bad. 4: Satisfied - The chatbot provided accurate and helpful information. - The chatbots responses were relevant and mostly thorough. - The interaction was smooth, addressing most of my questions effectively. 5: Very Satisfied - The chatbot provided clear, accurate, and highly helpful information. - The chatbots responses were comprehensive and directly addressed my needs. - The interaction was excellent, resolving my queries effectively and efficiently. Please rate your experience from 1 to 5 based on the above criteria and explain the reasons for your rating."
        },
        {
            "title": "D HUMAN STUDY DETAILS",
            "content": "D.1 USER INTERFACE In this subsection, we display the interface used in our human study. Figure 12: Example of user interaction interface for our main human experiments studying the misalignment of RLHF and the effecitveness of RLHS. Figure 13: Example of user interaction interface for additional human experiments assessing the alignment of LLM actions and feedback with those of humans. 20 D.2 PARTICIPANTS AND DATA COLLECTION The human subjects were chosen from high quality Prolific participant pool, where participants were pre-screened to have an approval rate of 95-100 over at least 100 previous submissions. Participants were located in the USA. To assign subjects to experimental conditions, we used random assignment, and each participant was only assigned to one shopping scenario (either one purchasing decision or comparing between two AI shopping assistants). As negative experience could bias participants perceptions of AI chatbots, we ensured that they were not able to retake the study. The expected duration of the study was 5 minutes, and actually completed the study at median time of 4:54. Subjects were compensated $1.10 for their participation, resulting in hourly wage of $13.47/hour, which was substantially higher than minimum wage. In addition to participant satisfaction ratings or preferences, participants were asked to provide brief 2-sentence explanation to explain their ratings or preferences. We manually reviewed these explanations for all participants, and participants that did not provide reasonable 2-sentence explanation had their data removed from the study. We also removed participants that finished the study in an unreasonably short time (<1:30 out of the estimated 5 minutes). Other than this, no data was removed. This study received IRB approval at Princeton University with the record number 10859. D.3 ADDITIONAL HUMAN STUDY We conducted an additional human study to assess how closely the feedback and actions of our AI proxy (Llama-3.1-70B) align with those of human participants. In the study, participants interacted with chatbots from two different stores, taking actions such as purchasing items or leaving the store based on the conversations. After engaging with both stores, participants were asked to choose which store they preferred. We randomly selected 10 scenarios from our training set, with 30 different participants evaluating each scenario. To determine the human preference for each scenario, we employed majority voting. This method was used to ensure that the aggregated choice reflected the consensus among participants, minimizing the impact of individual variability and providing more robust measure of overall preference. Our analysis revealed that the matching accuracy between LLM-generated feedback and human feedback reached 100%. Furthermore, the actions taken by the LLM matched those of human participants with 95% accuracy. These findings suggest that our simulated feedback and actions align strongly with real human behavior."
        },
        {
            "title": "E DISCUSSION",
            "content": "E.1 RELATED WORK Statement of Contributions. Our key insight is that the true value of AI outputs lies in their downstream consequences, especially in how they influence real-world human behavior. While the importance of long-term outcomes is fundamental aspect of dynamic decision theory, our work is the first to address this within the context of RLHF by (1) exploring the negative effects of learning from immediate human feedback based on foresight, and (2) proposing general mitigation strategy that evaluates real-world downstream harm caused by inaccurate information. Comparison with Related Work: One of the recent works cited in comparison is by Lang et al. (2024), which focuses on the problem of partial observability. This is distinct from the problem of human misprediction we address. In their setting, user utility is confined to the immediate time frame of the interaction and does not consider the long-term repercussions on the users behavior or well-being after the interaction concludes. Their analysis primarily highlights scenarios where an AI system is incentivized to withhold information to avoid negative feedback scores but does not delve into the real-world impact such deception has on user utility. In contrast, our approach specifically examines the human users decision-making process after interacting with the AI system, emphasizing how misalignment or deceptive behavior directly affects their realized utility. Recent studies have investigated sycophantic behavior in language models (Sharma et al., 2023; Wei et al., 2023; Perez et al., 2022), where the models are optimized to generate responses that align with user beliefs rather than the truth. Our empirical results also reveal such tendencies. In this paper, 21 we analyze the underlying factors contributing to this behavior and demonstrate how incorporating hindsight can be effective in preventing sycophancy. Theoretical Contributions: We extend the RLHF formulation by mathematically capturing this dynamic interplay between AI and human decision-making, something that has not been explored in prior work, including Lang et al. (2024) Our theoretical analysis not only highlights why deceptive behavior is problematic but also quantifies its repercussions by modeling the \"closed-loop\" evolution of the sociotechnical system formed by the human and the AI. Mitigation Strategies: Importantly, we propose and evaluate novel mitigation method: Reinforcement Learning from Hindsight Simulation (RLHS) that significantly reduces misalignment and deceptive behavior. While Lang et al. (2024) note that deception is undesirable, they do not provide solutions or theoretical basis for understanding its downstream damage. Our work, therefore, not only identifies and analyzes the issue but also offers practical, effective mitigation strategy. Additionally, our partial hindsight approach still operates within partially observable setting. The minimal difference between partial and oracle hindsight suggests that the fundamental issue in the class of misalignment we study is not primarily linked to partial observability, but rather to the human misprediction of the downstream consequences. E.2 BROADER IMPACT Human evaluators do not always know the full truth when providing feedback. Without explicit information about its future consequences, evaluators must implicitly estimate them during their assessment. This limitation poses significant challenges for real-world applications of AI, particularly within the RLHF framework we studied. In the following sections, we discussed these limitations and how our proposed hindsight feedback approach can help overcome them to enhance AI alignment. Limited Access in Real-World Applications: In real-world scenarios, users and human labelers frequently interact with black-box or closed-prompt AI systems where internal prompts and decisionmaking processes remain opaque. Notable examples include commercial systems such as OpenAIs ChatGPT and Amazons Rufus. Our proposed techniques (hindsight feedback), and the experimental settings we used can be applied directly to these systems where full internal access is unavailable. In such cases, assessing the consistency of responses alone is insufficient, as external context might not capture the complete implications of an AIs output. Hindsight feedback allows evaluators to provide more reliable feedback by considering outcomes, improving alignment in these constrained settings. Limitations of Human Judgment and Information Access: Even when human evaluators have full access to models and their prompts (e.g., in open-source systems), perfect judgment is not guaranteed. Evaluators may miss deeper implications or fail to predict the long-term impact of responses, whether due to lack of expertise or cognitive limitations. These challenges are relevant to both open-source and closed-source models. Below, we outline two practical examples illustrating these limitations and how hindsight feedback can address them: Code Generation Scenario: Imagine user asking language model for code to fit polynomial curve to set of data points. One solution may fit the data perfectly, while another shows some deviation. human evaluator might prefer the model with the perfect fit, not realizing that it overfits and performs poorly on new data. Immediate feedback in this case could lead to misalignment, as it prioritizes surface-level satisfaction over long-term utility. By providing feedback after testing the code on new data (hindsight), evaluators can offer more informed input, reducing misalignment. Hindsight simulation can automate part of this process by allowing models to test outcomes on unseen data and report the results to human evaluators for better feedback. One extra benefit of hindsight simulation is that humans do not need to be domain experts to provide truthful feedback. AI4Science Proof Construction: When constructing mathematical proofs for scientific problems, model may generate results that are correct only under conditions or assumptions specified by the user. Human evaluators, constrained by time or limited expertise, may overlook these limitations during evaluating the model, eventually causing the model to overfit to restricted set of problems and unable to tackle scientific problems in general settings. On the other hand, hindsight simulation generates diverse set of scenarios, including, e.g., edge cases, under which the model is required to validate its proof. This allows the human evaluator to assess the model performance based on its ability to generalize beyond the immediate problem. 22 Algorithm 1 Human Feedback Loop for RLHS 0 , oAI 1 := aAI 0 sample_AI_prompt(Z AI, OAI) 0 sample_initial_conditions(S, H, ΘH) 1: Step 0: Initialization 2: s0, zH 0 , θH, oH 3: 4: Step 1: AI Prompt Sampling 5: sAI 6: 7: Step 2: AI Policy Evaluation 8: Query the AI policy for an action: oH 9: 10: Step 3: Hindsight 11: for = 1 to + do 12: 13: 14: end for 15: 16: Step 4: Query Feedback 17: Query human feedback on the AI policy: ˆU (πAI ) query_human_feedback(πAI ) 18: 19: Output or Process Feedback 20: Store or process feedback for further learning: store_feedback( ˆU ) Sample action: at sample_action(πAI) st+1, oH t+1 (st, at, oH ) 0 πAI( s0, zH 0 ) Notation Marketplace AI4Science Proof Construction s0 Θ0 sH 0 oH 0 zAI 0 oAI aH st+1 oH t+1 ˆU Initial store inventory Initial problem instance Customers desired features User goals/preferences (succeed at future problem instances) Background knowledge Prior knowledge about the problem Available public stock information Initial problem setup AI systems internal information Initial problem setup All detailed stock information Initial problem setup Customers follow-up question or purchase decision Users input or solution attempt Product arrival Next problem instance Revealed product features Validation or correctness check Satisfaction with the service Satisfaction with the solution Table 7: RLHS notations for Marketplace and AI4Science Proof Construction"
        },
        {
            "title": "F ADDITIONAL QUALITATIVE RESULTS",
            "content": "In this section, we provide additional results comparing the qualitative differences between the outputs of policies trained with RLHF and RLHS. We also show failure case here. 23 Figure 14: Qualitative results for Llama-2-7b trained with DPO using immediate feedback versus partial hindsight. The model trained with immediate feedback falsely claims that Option is most affordable with 8K resolution, which is incorrect. In contrast, the model trained with partial hindsight truthfully states that option is the most affordable option that includes 8K resolution. 24 Figure 15: Qualitative results for Llama-3-8b trained with DPO using immediate feedback versus partial hindsight. The model trained with immediate feedback falsely claims that Option can play 3D movies, which is incorrect. In contrast, the model trained with partial hindsight accurately states that Option Cs 3D capability is not specified, and recommends Option B, the cheapest option that includes 3D capability. Figure 16: Failure case for Llama-2-7b trained with DPO using partial hindsight. The model trained with immediate feedback deceives about specific features, while the model trained with partial hindsight withholds some information. This reveals shortcomings of partial hindsight, as it does not have observations for all other items. Consequently, it might still encourage the agent to deceive about the price or conceal price information."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Princeton University",
        "Department of Electrical and Computer Engineering, Princeton University",
        "Department of Psychology, Princeton University"
    ]
}