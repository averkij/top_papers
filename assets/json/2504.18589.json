{
    "paper_title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency",
    "authors": [
        "Zhikai Wang",
        "Jiashuo Sun",
        "Wenqi Zhang",
        "Zhiqiang Hu",
        "Xin Li",
        "Fan Wang",
        "Deli Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 9 8 5 8 1 . 4 0 5 2 : r Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency Zhikai Wang1,2, Jiashuo Sun1,2, Wenqi Zhang1,3, Zhiqiang Hu1,4, Xin Li1,2, Fan Wang1,2, Deli Zhao1,2 1DAMO Academy, Alibaba Group 2Hupan Lab 3Zhejiang University 4Singapore University of Technology and Design Equal contribution, Corresponding author Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBench, comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBench includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBench, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements. Date: April 29,"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advancements in Large Vision-Language Models (LVLMs) Anthropic (2025); Deepmind (2025); OpenAI et al. (2024); Bai et al. (2023) have made significant strides in bridging the gap between visual understanding and language processing. These models have achieved remarkable performance across range of tasks, demonstrating near-expert human-level proficiency in domains such as object recognition, caption generation, and visual question answering Lin et al. (2015); Agrawal et al. (2016). Among the various domains explored, LVLMs have shown particular promise in tasks that require both visual and linguistic reasoning, making them increasingly relevant for real-world applications. While many visual mathematics benchmarks, such as MathVista Lu et al. (2023) and MathVision Wang et al. (2024a), focus on knowledge-centric evaluations that assess domain-specific mathematical or geometric expertise, they often fail to evaluate models core ability to perceive and reason about fundamental mathematical elements and visual concepts. Moreover, these knowledge-centric evaluations are easily influenced by the pre-existing knowledge embedded in large language models, which may obscure true reasoning capabilities. To advance towards Artificial General Intelligence (AGI), more holistic approach to multi-modal reasoning is needed-one that goes beyond task-specific benchmarks and better captures generalizable cognitive abilities. In this context, we identify gap in the evaluation of models on elementary-level math problems Cobbe et al. (2021a); Wei et al. (2023b). These problems, typically at the elementary school level, do not require complex mathematical or geometric reasoning but rely heavily on explicit visual dependencies-the ability to discern and integrate visual features across images and understand how different visual elements relate to one another to solve problems. This mirrors the cognitive development of children, who, at young age, rely on similar skills to solve problems despite not yet possessing advanced reasoning abilities. Understanding and modeling this form of reasoning is crucial, as it represents fundamental cognitive ability essential for advancing toward broader AGI capabilities. Figure 1 Representative examples from the VCBench, showcasing diverse question types and categories including Space and Location (Direction, Location and Place), Reasoning and Observation (Reasoning and Observe), Time and Calendar (Calendar and Clock), Objects and Motion (Cube and Move), Organization and Pattern (Weight, Organize and Pattern), and Geometry and Shapes (Shape, Quad, Angle, Rectangular and Triangle). To address this gap, we introduce VCBench, comprehensive benchmark designed to assess multimodal mathematical reasoning tasks with explicit visual dependencies. Specifically targeting elementary-level math problems (grades 16), VCBench focuses on tasks that require reasoning across multiple images to derive solutions. As shown in Figure 1, it covers six key cognitive domains: Time and Calendar, Spatial and Positional Awareness, Geometry and Shapes, Objects and Motion, Reasoning and Observation, and Organization and Patterns. It also evaluates five distinct competencies: temporal reasoning, geometric reasoning, logical reasoning, spatial reasoning, and pattern recognition. These competencies span broad spectrum, from basic temporal and spatial understanding to more advanced geometric and logical reasoning, providing thorough evaluation of multimodal model performance. Comprising 1,720 question-answer pairs and 6,697 images (averaging 3.9 images per question), VCBench ensures models must reason across multiple visual inputs, rather than relying on single-image comprehension. With this holistic framework, VCBench serves as valuable resource for advancing research in multimodal mathematical reasoning. In our extensive experimental evaluation, we assessed 26 state-of-the-art LVLMs across 17 distinct task categories within VCBench. Despite achieving near-perfect accuracy on normal human-level performance, the best-performing visual models were unable to exceed 50% accuracy. Many of these state-of-the-art models exhibited notable lack of pattern recognition in images, especially when it came to reasoning tasks that required integrating visual cues across multiple images. Interestingly, we observed that these same tasks could be easily answered by normal human. This highlights significant gap in current benchmarks, which fail to adequately assess vision-centric mathematical reasoning abilities. (a) (b) Figure 2 (a) Overview of the VCBench dataset structure, highlighting its six main categories and associated subcategories, designed to assess multimodal reasoning capabilities of LVLMs. (b) Distribution of question types in the VCBench, illustrating the relative frequency across different visual reasoning subcategories Table 1 Comprehensive Statistics of the VCBench Dataset, Including Detailed Breakdown of Question-Image Pairs, Image Distribution, and Question Length Metrics. Examples (Q&A pairs) Images Avg. images per question Avg. question length Max. # images in question Min. # images in question 1,720 6,697 3.9 136.2 18 We make several key contributions with VCBench: Unlike existing benchmarks that focus on knowledge-centric evaluations, we emphasize vision-centric assessments. VCBench targets problems that do not require specialized knowledge but instead rely on the common perceptual reasoning of mathematical images and concepts. This approach aligns with the way children learn-first mastering visual reasoning and only later acquiring domain-specific knowledge. VCBench is designed around multi-image tasks, with each question containing an average of 3.9 images. This requirement challenges models to explicitly integrate visual cues across multiple images and reason about how they interact, which better reflects real-world scenarios where information is often distributed across multiple visual inputs. Our benchmark provides holistic evaluation of various visual reasoning capabilities, such as temporal reasoning, spatial understanding, and pattern recognition. While these tasks may seem simple to children, they represent fundamental reasoning abilities that LVLMs often struggle with. Our experiments demonstrate that tasks considered easy for children-such as identifying time sequences or spatial relationships-prove challenging for state-of-the-art LVLMs, highlighting the gaps in current multimodal reasoning capabilities."
        },
        {
            "title": "2 Related Work",
            "content": "Large Vision-Language Models. Large Vision-Language Models (LVLMs) have significantly advanced the integration of vision and language, demonstrating strong performance in tasks such as image captioning, visual question answering (VQA), and complex multimodal reasoning Wang et al. (2024b); Wu et al. (2023). Recent developments, such as Gemini-2.0 Deepmind (2025), QVQ Team (2024), and Calude-3.7-Sonnet Anthropic (2025), showcase emergent abilities in cross-modal instruction-following and chain-of-thought reasoning. Despite these advancements, quantitatively evaluating LVLMs, particularly in visual mathematical reasoning, Figure 3 Comparative performance (%) of six various prominent LVLMs across six categories: Time and Calendar (TC), Space and Location (SL), Geometry and Shapes (GS), Objects and Motion (OM), Reasoning and Observation (RO), and Organization and Pattern (OP). remains challenging. Existing benchmarks like MathVista Lu et al. (2023), MathBench Liu et al. (2024), and Math-LLMs Liu et al. (2023) typically assess models within narrow domains, such as arithmetic word problems or geometry-based visual environments. Consequently, these benchmarks primarily measure foundational skills like geometric or spatial reasoning, limiting their capacity to comprehensively evaluate broader cognitive integration and reasoning abilities. To address this limitation, we introduce VCBench, systematic evaluation framework designed to rigorously assess LVLMs performance across diverse multimodal mathematical reasoning tasks with explicit visual dependencies. Visual Mathematical Reasoning. Mathematical reasoning is core cognitive ability increasingly explored within the context of LVLMs research Hendrycks et al. (2021); Cobbe et al. (2021b). While earlier benchmarks such as GSM8K Cobbe et al. (2021b) and MATH Hendrycks et al. (2021) primarily focused on text-based mathematical problems, recent research has expanded toward visual mathematical reasoning, incorporating diagrams, charts, and geometry-based problem-solving Wang et al. (2024b); Yang et al. (2024). Multimodal mathematical reasoning requires LVLMs to simultaneously integrate visual perception and logical reasoning, presenting greater challenge compared to purely textual problems. Recent benchmarks like MathVista Lu et al. (2023) and MathGLM-Vision Yang et al. (2024) have advanced evaluation efforts but still suffer from issues including ambiguous annotations, dependency on GPT-based scoring methods, and limited evaluation of generalizable cognitive abilities Yan et al. (2024). To overcome these challenges, we proposeVCBench, comprehensive benchmark explicitly designed for multimodal mathematical reasoning with visual dependencies. VCBench encompasses 17 distinct subtasks, systematically assessing foundational cognitive skills such as temporal reasoning, logical reasoning, spatial reasoning, geometric reasoning, and pattern recognition. By standardizing task instructions and employing multiple-choice evaluation format, VCBench provides objective, reproducible evaluations, offering deeper insights into the strengths and limitations of current LVLMs. Figure 4 Comparative evaluation of various LVLMs under Multi-Image and Single-Image settings for the same question. The letters (A, B, C, D) indicate models predictions, with correct answers marked in green and incorrect answers in red."
        },
        {
            "title": "3.1 Benchmark Construction",
            "content": "For VCBench, we employed systematic approach to collect high-quality multimodal mathematical reasoning problems that explicitly require visual reasoning. We started by examining Chinese elementary school mathematics textbooks from grades 1-6, manually filtering for problems that contained at least two images. The benchmark prioritizes vision-centric evaluation through perceptual reasoning tasks that avoid specialized knowledge, while simultaneously challenging models to implicitly integrate and synthesize visual information across multiple images - critical capability for real-world applications where understanding emerges from connecting disparate visual cues. During our selection process, we enforced strict criteria to ensure quality and consistency. We only retained problems with unique, unambiguous answers to facilitate objective evaluation. After the initial collection phase, we utilized large language models to translate all problems into English (the specific prompts used are available in the Appendix), followed by rigorous human verification to maintain translation accuracy. The human verification process also served as secondary filtering mechanism, where we eliminated problems containing non-English content in images, as well as those with unclear visual elements or ambiguous instructions. This meticulous curation process ensured that our benchmark evaluates genuine reasoning abilities rather than testing models on their capacity to handle poorly defined problems. Through this methodology, we assembled our final collection of problems that encompass various mathematical domains while maintaining consistent quality standards."
        },
        {
            "title": "3.2 Benchmark Statistics",
            "content": "VCBench comprises diverse collection of multimodal mathematical reasoning problems, carefully organized into six major categories to provide comprehensive coverage of different cognitive dimensions. As shown in Table 1, our benchmark contains 1,720 question-answer pairs featuring total of 6,697 images. The average question includes 3.9 images, with some complex problems containing up to 18 images, while the minimum is 2 images per question. To systematically evaluate different reasoning capabilities, we categorized our problems into six major domains: Time and Calendar: Problems testing temporal reasoning across two subcategories (Calendar and Clock ) that require understanding time intervals, and calendar-based calculations. Space and Location: Challenges focused on spatial reasoning across three subcategories (Direction, Location, and Place) that assess understanding of relative positions, directions, and spatial relationships. Geometry and Shapes: Problems spanning five subcategories (Angle, Quad, Rectangular, Shape, and Triangle) that test fundamental geometric comprehension from basic shape recognition to more complex property Table 2 Performance of various vision-language models (Close-Source, Open-Source, and Math Specialist categories) on Multi-image setting across multiple tasks, including Time and Calendar, Space and Location, Geometry and Shapes, Objects and Motion, Reasoning and Observation, and Organization and Pattern. Models"
        },
        {
            "title": "Random Guess\nHuman",
            "content": "Time and Calendar Clock Calender 32.78 33.33 96.00 100.00 GPT-4o-mini OpenAI et al. (2024) GPT-4o OpenAI et al. (2024) Claude-3.7-Sonnet Anthropic (2025) Qwen-VL-Max Bai et al. (2023) Gemini2.0-Flash Deepmind (2025) Emu2-Chat Sun et al. (2024b) Idefics3-8B Laurençon et al. (2024) DeepSeek-VL2 Wu et al. (2024) Phi-3.5-vision-instruct Abdin et al. (2024) InternVL2.5-8B Chen et al. (2024) Llama-3.2-90B-Vision-Instruct AI (2024) Qwen2.5-VL-7B-Instruct Bai et al. (2025) Mantis-CLIP Jiang et al. (2024) Mistral-Small-3.1-24B-Instruct Mistral (2025) Kimi-VL-A3B-ThinkingTeam et al. (2025b) LLaVA-Interleave-7B Li et al. (2024b) LLaVA-OneVision-7B Li et al. (2024a) Kimi-VL-A3B-InstructTeam et al. (2025b) InternVL2.5-78B Chen et al. (2024) Gemma3-27B-it Team et al. (2025a) QVQ-72B-Preview Team (2024) LLaVA-OneVision-72B Li et al. (2024a) Qwen2.5-VL-72B-Instruct Bai et al. (2025) 80.00 100.00 100.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 20.00 100.00 0.00 20.00 100.00 0.00 0.00 0.00 20.00 100.00 100.00 0.00 0.00 60.66 40.00 50.00 46.67 70. 13.33 3.33 23.33 23.33 33.33 24.67 13.33 30.00 40.00 26.67 36.67 40.00 46.67 31.33 50.00 43.33 33.33 40.67 analysis. Direction 25.00 100.00 0.00 20.00 100.00 0.00 20.00 0.00 20.00 0.00 100.00 0.00 100.00 0.00 80.00 0.00 100.00 20.00 0.00 0.00 100.00 0.00 0.00 0.00 0.00 29.81 93. 38.46 30.77 53.85 42.31 57.69 3.85 15.38 23.08 19.23 34.62 11.54 19.23 50.00 30.77 30.77 19.23 11.54 30.77 42.31 38.46 46.15 26.92 53.85 Space and Location Geometry and Shapes Objects and Motion Reasoning and Observation Location Place Angle Quad Rectangular 33.33 96.67 31.00 95.60 27.63 96.84 29.17 95.00 Shape 31.84 94.02 Triangle Cube 28.37 97. 29.01 94.07 53.33 66.67 50.00 66.67 66.67 0.00 33.33 16.67 66.67 50.00 16.67 50.00 66.67 30.00 33.33 83.33 83.33 83.33 66.67 83.33 83.33 66.67 50.00 38.40 46.00 58.00 74.00 70.00 4.00 11.60 14.00 16.00 34.00 26.40 20.00 14.00 38.00 48.00 46.00 44.00 44.00 54.00 48.40 58.00 61.20 68.00 Close-Source Models 21.05 57.89 63.16 52.63 68. 53.57 28.57 57.14 42.86 53.57 Open-Source Models 10.53 10.53 10.53 15.79 31.58 31.58 31.58 15.79 31.58 36.84 26.32 36.84 47.37 47.37 31.58 42.11 57.89 68.42 10.71 17.86 14.29 28.57 50.00 32.14 25.00 35.71 35.71 28.57 57.14 32.14 39.29 46.43 25.00 46.43 57.14 53.57 37.99 50.22 60.70 54.15 61.14 12.66 23.14 29.69 27.07 35.81 26.20 30.13 38.43 29.26 49.78 39.74 37.99 43.23 53.28 41.92 44.10 60.70 55.02 55.56 51.85 59.26 66.67 70. 3.70 3.70 14.81 33.33 37.04 22.22 51.85 37.04 51.85 33.33 29.63 48.15 33.33 55.56 40.74 62.96 51.85 74.07 32.19 37.67 40.41 56.16 44.52 8.90 9.59 6.85 22.60 23.29 27.40 32.19 19.86 30.82 30.14 30.82 30.82 34.93 33.56 32.88 36.30 41.10 58.22 Move 29.35 94.63 38.24 50.37 67.28 60.66 68.75 6.99 16.91 18.38 22.79 25.74 25.37 40.81 32.35 31.62 41.91 33.46 46.69 44.49 40.44 47.79 48.16 60.29 60. Reasoning 33.33 100.00 Observe 29.41 93.59 0.00 90.00 100.00 50.00 40.00 0.00 0.00 10.00 0.00 0.00 0.00 0.00 40.00 50.00 0.00 50.00 50.00 50.00 50.00 50.00 50.00 100.00 60.00 28.68 31.27 31.27 35.27 35.53 3.62 9.69 9.43 21.32 18.99 25.58 25.19 28.04 29.59 25.32 33.46 32.56 31.31 28.04 32.82 28.55 38.24 35. Organization and Pattern Organize Pattern Weight 33.33 31.32 100.00 95.52 30.17 93.20 60.00 76.00 76.40 68.00 74.00 0.00 8.40 44.00 34.00 38.00 12.00 30.00 52.40 38.00 68.00 62.00 58.00 58.00 76.00 54.00 78.00 82.00 76.00 41.38 37.93 53.45 39.66 46.55 3.45 15.52 20.69 12.07 6.90 29.31 27.59 22.41 34.48 27.59 31.03 29.31 36.21 31.03 31.03 48.28 41.38 43. 100.00 80.00 100.00 100.00 100.00 20.00 0.00 0.00 0.00 0.00 20.00 0.00 100.00 20.00 100.00 100.00 100.00 0.00 100.00 80.00 100.00 80.00 100.00 Avg. 29.83 93.30 34.88 40.29 46.63 47.03 49.77 6.05 12.91 15.47 22.73 24.71 25.41 29.24 30.23 31.34 34.13 35.47 36.63 37.33 37.56 38.02 39.13 47.67 48. Objects and Motion: Tasks in two subcategories (Cube and Move) that evaluate the understanding of three-dimensional objects and motion transformations. Reasoning and Observation: Problems in two subcategories (Reasoning and Observe) designed to test logical reasoning and careful visual observation skills. Organization and Pattern: Challenges across three subcategories (Organize, Pattern, and Weight) that assess pattern recognition, sequencing, and organizational logic. This categorization allows for granular assessment of model performance across specific cognitive abilities required for mathematical reasoning. The vocabulary in our benchmark is deliberately controlled to 2,312 unique words, ensuring that performance differences reflect reasoning capabilities rather than linguistic complexity. With an average question length of 136.2 characters, our problems are concise yet sufficiently detailed to communicate the necessary context for solving each problem."
        },
        {
            "title": "4.1 Main Results",
            "content": "There are total of 17 subtasks for the evaluation from the perspectives of Temporal Reasoning, Spatial Reasoning, Geometric Reasoning, Logical Reasoning, and Pattern Recognition abilities over 21 VLMs. Table 2 provides detailed evaluation results across six visual reasoning tasks. Human performance is near-perfect with an average score of 93.30, while random guessing achieves only 29.83, which emphasizes that these tasks, though inherently solvable by humans, pose substantial challenges to current AI systems. In contrast, Claude-3.7-Sonnet (46.63%), Qwen-VL-Max (47.03%), and Gemini2.0-Flash (49.77%) achieve notably higher performance. Their relative strengths lie particularly in tasks requiring spatial reasoning and observational interpretation, suggesting these models have better internal representations or more effective cross-modal alignment between visual and linguistic information. However, despite these advancements, even these top-performing closed-source models exhibit notable shortcomings relative to humans, particularly in high-complexity reasoning scenarios (e.g., Geometry and Objects and Motion), reflecting an ongoing gap in advanced spatial reasoning, logical reasoning and pattern recognition capabilities. Open-source models present an even more heterogeneous and generally lower performance landscape, indicative of diverse model architectures, varying degrees of multi-modal integration sophistication, and potentially inconsistent data quality or quantity during training. For example, large open-source models, including Qwen2.5-VL-72B-Instruct (48.08%) and LLaVA-OneVision-72B (47.67%), demonstrate performance comTable 3 Performance comparison of vision-language models across different categories in single-image settings. The rightmost column shows the performance improvement ratio when switching from multi-image to single-image settings. Models Random Guess Human GPT-4o-mini OpenAI et al. (2024) GPT-4o OpenAI et al. (2024) Claude-3.7-Sonnet Anthropic (2025) Gemini2.0-Flash Deepmind (2025) Qwen-VL-Max Bai et al. (2023) Idefics3-8B Laurençon et al. (2024) LLaMA-3.2-90B-Vision-Instruct AI (2024) Emu2-Chat Sun et al. (2024b) DeepSeek-VL2 Wu et al. (2024) Mantis-CLIP Jiang et al. (2024) LLaVA-Interleave-7BLi et al. (2024b) Phi-3.5-vision-instruct Abdin et al. (2024) LLaVA-OneVision-7B Li et al. (2024a) InternVL2.5-8B Chen et al. (2024) Gemma3-27B-itTeam et al. (2025a) Kimi-VL-A3B-ThinkingTeam et al. (2025b) LLaVA-OneVision-72B Li et al. (2024a) Mistral-Small-3.1-24B-Instruct Mistral (2025) QVQ-72B-Preview Team (2024) InternVL2.5-78B Chen et al. (2024) Kimi-VL-A3B-InstructTeam et al. (2025b) Qwen2.5-VL-7B-Instruct Bai et al. (2025) Qwen2.5-VL-72B-Instruct Bai et al. (2025) G-LLaVA-13B Gao et al. (2023) G-LLaVA-7B Gao et al. (2023) MathLlava Shi et al. (2024) Time and Calendar Clock Calender 33.33 100.00 100.00 80.00 100.00 20.00 0. 0.00 80.00 60.00 20.00 0.00 00.00 0.00 0.00 0.00 80.00 0.00 20.00 20.00 80.00 80.00 0.00 0.00 20.00 0.00 100.00 100.00 32.78 96.00 20.00 40.67 54.67 76.67 53.33 10.00 30.00 12.67 33.33 35.33 30.00 13.33 43.33 33.33 40.00 33.33 53.33 40.00 41.33 50.00 70.00 53.33 55.33 40.00 36.67 20. Space and Location Geometry and Shapes Objects and Motion Reasoning and Observation Direction Location Place Angle Quad Rectangular Shape Triangle Cube 25.00 100.00 0.00 100.00 80.00 100.00 100.00 20.00 0.00 100.00 0.00 80.00 100.00 80.00 0.00 0.00 0.00 0.00 0.00 0.00 80.00 100.00 100.00 100.00 100.00 0.00 20.00 80.00 29.81 93.85 30.77 42.31 65.38 61.54 73. 11.54 15.38 23.08 19.23 23.08 30.77 19.23 23.08 26.92 26.92 34.62 30.77 38.46 61.54 42.31 50.00 46.15 73.08 23.08 30.77 26.92 33.33 96.67 31.00 95.60 27.63 96.84 29.17 95. 31.84 94.02 100.00 66.67 83.33 83.33 83.33 16.67 33.33 16.67 33.33 0.00 0.00 16.67 100.00 50.00 33.33 50.00 33.33 50.00 50.00 50.00 66.67 83.33 83.33 33.33 0.00 0.00 42.80 68.40 61.20 58.00 80.00 10.00 26.00 24.00 28.00 28.00 26.00 24.40 44.00 46.40 48.40 62.00 38.00 64.00 64.00 62.80 50.00 72.80 80. 20.40 30.00 32.00 26.32 57.89 68.42 63.16 52.63 5.26 15.79 42.11 10.53 42.11 36.84 10.53 21.05 31.58 21.05 52.63 47.37 57.89 68.42 63.16 31.58 52.63 52.63 31.58 21.05 31.58 Close-Source Models 56.77 68.12 68.56 71.62 69.87 50.00 64.29 78.57 42.86 75. Open-Source Models 20.52 17.03 24.02 32.31 31.88 33.19 34.50 44.10 51.53 45.85 52.40 51.53 56.77 58.95 65.94 59.39 61.14 69.87 32.14 25.00 28.57 32.14 46.43 42.86 42.86 35.71 39.29 57.14 39.29 39.29 46.43 39.29 57.14 35.71 60.71 75.00 Math Specialist Models 26.64 31.88 27.51 32.14 50.00 21.43 29.01 94.07 40.74 44.44 77.78 59.26 66. 7.41 33.33 18.52 25.93 11.11 14.81 22.22 44.44 48.15 33.33 77.78 55.56 70.37 81.48 55.56 51.85 55.56 66.67 25.93 40.74 11.11 28.37 97.67 34.93 42.47 43.84 46.58 57.53 17.12 27.40 22.60 13.70 26.03 31.51 32.19 30.82 31.51 33.56 26.03 39.73 30.14 32.19 32.19 46.58 60.96 57.53 15.75 23.97 34. Move 29.35 94.63 43.01 56.99 69.12 73.90 72.43 18.01 26.47 24.63 32.35 25.00 26.47 29.78 42.65 42.65 45.22 55.15 41.54 50.74 64.34 61.76 62.13 64.34 72.43 26.10 27.21 29.04 Reasoning Observe 33.33 100.00 90.00 60.00 100.00 100.00 100.00 0.00 100.00 10.00 0.00 0.00 50.00 20.00 40.00 30.00 100.00 50.00 100.00 100.00 50.00 90.00 50.00 100.00 90.00 0.00 0.00 40.00 29.41 93. 32.43 30.10 34.37 39.41 43.54 12.53 19.64 22.87 20.03 27.52 29.07 31.40 29.07 28.42 30.10 25.19 32.95 31.65 35.01 36.43 38.11 37.86 43.54 26.49 27.26 29.97 Organization and Pattern Organize Pattern Weight Avg. Improvement Ratio 30.17 93.20 72.00 90.40 92.00 90.00 91.60 30.00 49.60 12.00 46.00 12.00 28.00 46.00 64.40 60.80 66.80 86.00 32.00 82.00 90.00 88.00 82.00 92.00 92.00 24.00 28.00 28.40 31.32 95.52 37.93 44.83 63.79 46.55 41. 20.69 12.07 22.41 27.59 34.48 25.86 25.86 27.59 29.31 20.69 39.66 55.17 43.10 50.00 36.21 46.55 36.21 41.38 24.14 24.14 29.31 33.33 100.00 29.83 93.30 - - 60.00 100.00 100.00 100.00 80. 0.00 0.00 0.00 100.00 0.00 0.00 100.00 80.00 80.00 60.00 100.00 100.00 80.00 100.00 100.00 100.00 80.00 100.00 20.00 100.00 80.00 39.65 45.52 51.69 53.90 57.03 15.64 22.38 23.08 24.77 27.50 29.24 30.93 35.47 36.16 36.80 38.72 39.24 42.21 47.44 47.73 48.37 51.10 57.03 25.47 28.26 29.30 13.7% 12.9% 10.8% 8.3% 21.3% 21.2% -11.9% 281.5% 60.1% -9.0% -17.6% 36.1% -3.2% 46.3% 2.1% 13.4% -17.7% 34.7% 21.2% 27.1% 29.6% 74.8% 18.6% - - - Table 4 Influence of Chain-of-Thought Wei et al. (2023a) on model performances. Space and Location Geometry and Shapes Model GPT-4o OpenAI et al. (2024) Qwen-VL-Max Bai et al. (2023) Gemini2.0-Flash Deepmind (2025) CoT Time and Calendar Clock Calender 40.00 100.00 40.00 100.00 Direction 20.00 0.00 Location 30.77 38.46 0.00 0.00 20.00 +20.00 100.00 80. 0.00 46.67 36.67 -10.00 70.00 83.33 -20.00 0.00 100.00 +100.00 20.00 20.00 +7.69 42.31 57.69 +15.38 57.69 69. Place 66.67 66.67 0.00 66.67 66.67 0.00 66.67 83.33 Angle Quad Rectangular 46.00 52.00 28.57 32.14 57.89 63. +6.00 74.00 74.40 +0.40 70.00 66.40 +5.27 52.63 52.63 0.00 68.42 68.42 +3.57 42.86 57.14 +14.28 53.57 67. +14.29 Shape 50.22 53.71 +3.49 54.15 60.26 +6.11 61.14 71.62 Triangle 51.85 66.67 +14.82 66.67 77. +11.11 70.37 66.67 +10.48 -3.70 Objects and Motion Reasoning and Observation Cube 37.67 33.56 Reasoning 90.00 100.00 Observe 31.27 30. Move 50.37 52.57 -4.11 56.16 52.74 -3.42 44.52 41.10 -3.42 +2.20 60.66 61.03 +0.37 68.75 70. +2.21 +10.00 50.00 90.00 +40.00 40.00 100.00 +60.00 -0.52 35.27 36.05 +0.78 35.53 37. +2.33 Organization and Pattern Organize Pattern Weight 80.00 37.93 100.00 58.62 76.00 82.00 +6.00 68.00 93.60 +25.60 74.00 89.40 +20.69 39.66 44. +5.17 46.55 56.90 +20.00 100.00 100.00 0.00 100.00 100.00 +15.40 +10.35 0. Avg. 40.29 42.03 +1.74 47.03 49.48 +2.45 49.77 53.66 +3.89 -20. +13.33 0.00 +11.54 +16.66 -3.60 0. parable to mid-tier closed-source models. Their comparatively stronger results, particularly in Geometry and Shapes and Organization and Pattern tasks, suggest these models benefit from scale and possibly more sophisticated visual encoders or pre-training strategies. However, they still encounter substantial difficulties in tasks requiring nuanced observation or reasoning about motion and object interactions, highlighting remaining challenges in achieving cognative visual reasoning. The variability across different tasks, especially pronounced in Objects and Motion and Reasoning and Observation categories, points toward crucial areas requiring further research: enhancing temporal reasoning, improving dynamic visual understanding, and strengthening the integration of geometric and spatial cognition into visual-language models."
        },
        {
            "title": "4.2 Evaluation in Single-Image Setting",
            "content": "The evaluation is also conducted in single-image setting for comparison. In single-image setting, we integrate visual and textual elements into cohesive layout as shown in Figure 4. If model performs well in single-image but poorly in multi-image, it suggests the model lacks compositional reasoning ability to link separate inputs. The results in Table 3 reveal two key findings: First, most models perform significantly better in single-image settings compared to multi-image scenarios (average improvement of +42.3%), indicating strong bias toward single-image optimization. For instance, Qwen-VL-Max shows +21.3% gain in single-image performance, while models like Emu2-Chat exhibit dramatic improvements (+281.5%). Second, specialized multi-image models like LLaVA-Interleave-7B show the opposite trend (-17.6% in single-image mode), achieving higher accuracy in multi-image tasks than in single-image ones. This contrast suggests that unlike dedicated multiimage architectures, conventional models struggle to integrate visual information across multiple inputs, highlighting critical limitation in current vision-language systems. Addressing this gap by effectively leverage cross-image cues for reasoning remains an essential challenge for future research."
        },
        {
            "title": "4.3 Results of Math Specialist Models",
            "content": "The Math Specialist models, which include G-LLaVA-13B, G-LLaVA-7B, and MathLlava, exhibit relatively low overall performance, with average scores ranging from 25.47 to 29.30. Notably, G-LLaVA-13B records the lowest score at 25.47, while MathLlava achieves slightly higher average of 29.30. Although these models are Table 5 Accuracy comparison of various models on questions categorized by difficulty along with their average performance. Models LLaMA-3.2-90B-Vision-Instruct Mantis-CLIP InternVL2.5-78B QVQ-72B-Preview LLaVA-OneVision-72B Qwen2.5-VL-72B-Instruct Easy Medium Hard 23.89 26.15 22.22 32.37 29.30 29.63 41.62 36.03 25.93 45.66 36.71 18.52 53.76 45.32 29.63 55.11 45.49 25.93 Avg. 25.41 30.23 37.56 39.13 47.62 48.08 Table 6 Comparisons between existing visual math benchmarks for LVLMs."
        },
        {
            "title": "Image Numbers Question Numbers",
            "content": "Olympiadbench He et al. (2024) GeoQA Chen et al. (2021) MATH-Vision Wang et al. (2024a) MathVista Lu et al. (2023) MMMUmath Yue et al. (2024) GeoMath Xu et al. (2024) U-Math Chernyshev et al. (2025) Blink Fu et al. (2024) MM-MATH Sun et al. (2024a) MMIEmath Xia et al. (2024) Polymath Gupta et al. (2024) NTSEBench Pandya et al. (2025) BSA 1 Pandya et al. (2025) MV-MATH Wang et al. (2025)"
        },
        {
            "title": "Ours",
            "content": "5,129 4,998 3,472 5,487 577 4,540 225 7,358 5,929 26,534 5,000 4,642 312 6,061 6,697 8,952 4,998 3,040 6,141 540 9,155 1,100 3,807 5,929 20,103 5,000 2,728 312 2,009 1,720 Temporal"
        },
        {
            "title": "Logical Pattern",
            "content": "Multi-Images"
        },
        {
            "title": "Answer Type",
            "content": "Free-form Multiple Choice Free-form & Multiple Choice Free-form & Multiple Choice Free-form & Multiple Choice Free-form & Multiple Choice & Prove Free-form Multiple Choice Free-form Free-form & Multiple Choice Multiple Choice Multiple Choice Multiple Choice Free-form & Multiple Choice Multiple Choice designed with focus on mathematical reasoning, their performance across diverse tasks-such as time and calendar, spatial reasoning, and geometric challenges-remains inconsistent. For instance, while G-LLaVA-7B reaches perfect score (100.00) on the Calendar sub-task, its scores in other categories, such as Clock and certain geometry-related tasks, are considerably lower. Furthermore, the results indicate that these Math Specialist models struggle to match the performance of their general-purpose counterparts. Despite showing some strengths-for example, MathLlava scoring 34.93 on the Cube task-these models fall short on several key aspects, including Clock, Location, and reasoning tasks. This pattern underscores the challenge of integrating specialized mathematical capabilities with the broader spectrum of visual understanding."
        },
        {
            "title": "5.1 Influence of Chain-of-Thought on Model Performance",
            "content": "Chain-of-thought Wei et al. (2023a) reasoning generally enhances model performance, as the Table 4 shows stable improvements across several domains when CoT is enabled. For instance, Qwen-VL-Max exhibits dramatic 40% boost in the Reasoning task, highlighting the significant impact of structured reasoning on spatial understanding. Gemini2.0-Flash also benefits substantially, with 15.40 point increase in the Pattern category and 16.66 point rise in Place suggesting that CoT particularly aids in tasks requiring complex organizational and geometric reasoning. While improvements are evident, the efficacy of chain-of-thought (CoT) prompting exhibits strong taskdependent variation. CoT consistently enhances performance in multi-step reasoning tasks (e.g., Pattern and Reasoning tasks), where all models show gains. However, it proves neutral or detrimental in perception-heavy tasks (e.g., Calender and Direction tasks) due to interference with low-level spatial or temporal processing. Nonetheless, the overall trend supports that incorporating CoT tends to enhance problem-solving abilities, especially in tasks that demand high-level reasoning and pattern recognition."
        },
        {
            "title": "5.2 Comparison with Other Benchmarks",
            "content": "In comparison to existing visual math benchmarks, our dataset stands out in several important ways as shown in Table 6. While benchmarks such as Olympiadbench He et al. (2024) and GeoQA Chen et al. (2021) focus Figure 5 comparison of error distributions among three model, GPT-4o, Gemini2.0-Flash, and Calude-3.7-Sonnet, across five error categories: visual perception errors, calculation errors, contextual misunderstandings, logical errors, and answer integration errors. primarily on specific skills like geometry and logical reasoning, our benchmark includes broader spectrum of required skills, including temporal, spatial, geometric, logical, and pattern recognition. This comprehensive skill coverage provides more holistic evaluation of LVLMs. Additionally, our dataset supports multi-image tasks, which is feature not widely supported by other benchmarks such as Blink Fu et al. (2024) and GeoQA Chen et al. (2021), further enhancing its applicability for real-world tasks that require understanding across multiple visual inputs. Moreover, our benchmark boasts higher image-question ratio than all other benchmarks, meaning that on average, each question is associated with more images. Finally, our dataset offers multiple-choice answer types for easier evaluation, unlike many other benchmarks that provide free-form answer format which hard to evaluate, such as MM-MATH Sun et al. (2024a) and U-Math Chernyshev et al. (2025)."
        },
        {
            "title": "5.3 Error Distribution for VCBench",
            "content": "We define five error types in this benchmark: Visual Perception Error indicates that the model misinterprets or fails to accurately perceive visual content; Calculation Error captures mistakes made during arithmetic computations; Contextual Misinterpretation occurs when the model misreads the textual conditions, such as treating unrelated information as relevant; Logical Error refers to flaws in the reasoning process; and Answer Consolidation Error encompasses failures to directly answer the question or instances where multiple, conflicting answers are provided. We conduct manual error classification for all questions across four top-tier models, enabling precise identification of each models failure patterns and relative weaknesses across different error categories. As shown in Figure 5, Visual Perception Errors are predominant across all models, with Gemini2-Flash exhibiting the highest rate at about 62%. This persistent pattern across architectures suggests that enhancing visual perception capabilities remains the most critical challenge for multimodal models. Calculation Errors remain consistently low (ranging from about 4% to about 7%), indicating that basic arithmetic computation has become relatively robust in modern models. Contextual Misinterpretation errors are minimal, particularly for Gemini2-Flash (about 3%) and Claude (about 4%), which indicates relatively robust understanding of textual context. However, QVQs comparatively higher rate (6%) may reflect its tendency toward over-reasoning, where excessive analysis leads to detachment from the original question context. On the other hand, discrepancies are more apparent in the Logical and Answer Consolidation Error rates. Claude shows significantly high Logical Error rate of about 33% compared to GPT-4os about 15% and QVQs about 22%, revealing the weaknesses in its deductive reasoning pipelines. Moreover, while Answer Consolidation Errors are generally low (QVQ at about 11% and both Gemini2-Flash and Claude at about 7%), GPT-4o presents higher rate of about 23%, suggesting its advanced reasoning capabilities may come at the cost of response discipline, where the model sometimes generates multiple answers rather than single one. This trade-off between exploratory reasoning and answer precision presents an important optimization target for future iterations."
        },
        {
            "title": "5.4 Analysis of Problem Difficulty and Model Performance",
            "content": "The questions in our benchmark are drawn from existing textbooks and assigned difficulty coefficient ranging from 0.0 to 1.0, where scores between 0.0 and 0.35 denote easy questions, 0.35 to 0.75 represent medium ones, and 0.75 to 1.0 correspond to hard problems. Interestingly, the results in Table 5 reveal that questions annotated as hard tend to yield higher accuracy, while the easy and medium problems register lower accuracy. This counterintuitive outcome may be attributed to the fact that simpler questions, which primarily require the identification of patterns rather than intricate computations, pose different challenge compared to the hard questions that demand complex calculation and structured reasoning."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces VCBench-a comprehensive evaluation framework designed to assess multimodal mathematical reasoning with explicit visual dependency. By addressing the limitations of existing datasets in multi-image integration and cross-modal relational reasoning, our benchmark provides detailed analysis of 26 state-of-the-art LVLMs across six cognitive domains and 17 task categories. The evaluation reveals significant performance disparities, particularly in areas such as multi-step instruction following, basic visual perception, cross-image consistency, and vulnerability to visual hallucinations."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024. Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi Parikh. Vqa: Visual question answering, 2016. https://arxiv.org/abs/1505.00468. Meta AI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https://ai.meta.com/, 2024. Anthropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/claude-3-7-sonnet-system-card, 2025. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. https://arxiv.org/abs/2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. GeoQA: geometric question answering benchmark towards multimodal numerical reasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online, August 2021. Association for Computational Linguistics. https://aclanthology.org/2021.findings-acl.46/. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. CoRR, abs/2412.05271, 2024. Konstantin Chernyshev, Vitaliy Polshkov, Ekaterina Artemova, Alex Myasnikov, Vlad Stepanov, Alexei Miasnikov, and Sergei Tilga. U-math: university-level benchmark for evaluating mathematical skills in llms, 2025. https: //arxiv.org/abs/2412.03205. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021a. https://arxiv.org/abs/2110.14168. Karl Cobbe, Vineet Kosaraju, et al. Gsm8k: dataset for grade school math word problems. arXiv preprint arXiv:2110.14168, 2021b. Google Deepmind. Gemini 2.0 flash. https://deepmind.google/technologies/gemini/flash/, 2025. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. BLINK: multimodal large language models can see but not perceive. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XXIII, volume 15081 of Lecture Notes in Computer Science, pages 148166. Springer, 2024. https://doi.org/10.1007/978-3-031-73337-6_9. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-llava: Solving geometric problem with multi-modal large language model, 2023. https://arxiv.org/abs/2312.11370. Himanshu Gupta, Shreyas Verma, Ujjwala Anantheswaran, Kevin Scaria, Mihir Parmar, Swaroop Mishra, and Chitta Baral. Polymath: challenging multi-modal mathematical reasoning benchmark, 2024. https://arxiv.org/abs/ 2410.14702. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Dan Hendrycks, Collin Burns, et al. Measuring mathematical problem-solving with the math dataset. NeurIPS, 2021. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. MANTIS: interleaved multi-image instruction tuning. CoRR, abs/2405.01483, 2024. Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. CoRR, abs/2408.12637, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024a. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024b. https://arxiv.org/abs/2407.07895. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. https: //arxiv.org/abs/1405.0312. Hongwei Liu, Zilong Zheng, et al. Mathbench: Evaluating the theory and application proficiency of llms with hierarchical mathematics benchmark. arXiv preprint arXiv:2405.12209, 2024. Wentao Liu, Hanglei Hu, et al. Mathematical language models: survey. arXiv preprint arXiv:2312.07622, 2023. Pan Lu, Hritik Bansal, et al. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Mistral. Mistral small 3.1. https://mistral.ai/news/mistral-small-3-1, 2025. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. https://arxiv.org/abs/2410.21276. Pranshu Pandya, Vatsal Gupta, Agney Talwarr, Tushar Kataria, Dan Roth, and Vivek Gupta. Ntsebench: Cognitive reasoning benchmark for vision language models, 2025. https://arxiv.org/abs/2407.10380. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models, 2024. https://arxiv. org/abs/2406.17294. Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juan-Zi Li. MM-MATH: advancing multimodal math evaluation with process evaluation and fine-grained classification. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 13581375. Association for Computational Linguistics, 2024a. https://aclanthology.org/2024.findings-emnlp. 73. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1439814409. IEEE, 2024b. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report, 2025a. https://arxiv.org/abs/2503.19786. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-vl technical report, 2025b. https://arxiv.org/abs/2504.07491. Qwen Team. Qvq: To see the world with wisdom, December 2024. https://qwenlm.github.io/blog/ qvq-72b-preview/. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024a. Peijie Wang, Zhong-Zhi Li, Fei Yin, Xin Yang, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating multimodal math reasoning in multi-visual contexts, 2025. https://arxiv.org/abs/2502.20808. Yiqi Wang, Wentao Chen, Xiaotian Han, et al. Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning. arXiv preprint arXiv:2401.06805, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023a. https://arxiv.org/abs/2201.11903. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023b. https://arxiv.org/abs/2306.16636. Jiayang Wu, Wensheng Gan, Zhenlian Qi, et al. Multimodal large language models: survey. IEEE International Conference on Big Data (BigData), 2023. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. CoRR, abs/2412.10302, 2024. Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, and Huaxiu Yao. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models, 2024. https://arxiv.org/abs/2410.10139. Shihao Xu, Yiyang Luo, and Wei Shi. Geo-llava: large multi-modal model for solving geometry math problems with meta in-context learning. In Proceedings of the 2nd Workshop on Large Generative Models Meet Multimodal Applications, MM 24, page 1115. ACM, October 2024. doi: 10.1145/3688866.3689124. http://dx.doi.org/10. 1145/3688866.3689124. Yibo Yan, Shen Wang, et al. Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection. arXiv preprint arXiv:2410.04509, 2024. Zhen Yang, Jinhao Chen, et al. Mathglm-vision: Solving mathematical problems with multimodal large language models. arXiv preprint arXiv:2409.13729, 2024. Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 95569567. IEEE, 2024. https: //doi.org/10.1109/CVPR52733.2024.00913."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Experiment Details Table 7 Generation parameters for LVLMs (with grouped configurations)."
        },
        {
            "title": "Model",
            "content": "GPT-4o-mini & GPT-4o Claude-3.7-Sonnet Generation Setup API URL: https://api.openai.com/v1/chat/completions temperature = 0.2, max_tokens = 1024 API URL: https://api.anthropic.com/v1/messages, temperature = 0.2, max_tokens = 1024 Gemini2.0-Flash Qwen-VL-Max https://generativelanguage.googleapis.com/v1beta/models/ API URL: gemini-pro:generateContent, temperature = 0.2, max_tokens = 1024 Use dashscope package, temperature = 0.2, max_new_tokens = 1024 Open-Source Models Same parameters for all below: Deployed by vllm, with do_sample = True, temperature = 0.2, max_new_tokens = 1024 Idefics3-8B LLaMA-3.2-90B-Vision-Instruct Emu2-Chat DeepSeek-VL2 Mantis-CLIP LLaVA-Interleave-7B Phi-3.5-vision-instruct InternVL-2.5 LLaVA-OneVision-7B/72B Gemma3-27B-it Mistral-Small-3.1-24B-Instruct Qwen2.5-VL-7B/72B-Instruct QVQ-72B-Preview do_sample = True, temperature = 0.2, max_new_tokens = G-LLaVA-7B/13B do_sample = True, temperature = 0.2, max_new_tokens ="
        },
        {
            "title": "MathLlava",
            "content": "do_sample = True, temperature = 0.2, max_new_tokens = 1024 A.2 Case Studies A.3 Prompt for Experiment Table 8 Inference Prompt. Inference Prompt You are helpful AI assistant. Please answer the following questions and output the answer options directly. Question: { question } Figure 6 Case for Visual Perception Error. Figure 7 Case for Calculation Error. Figure 8 Case for Contextual Misinterpretation. Figure 9 Case for Logical Error. Figure 10 Case for Answer Consolidation Error. Table 9 Inference Prompt with Chain-of-Thought. Inference Prompt with Chain-of-Thought You are helpful AI assistant. Please think step by step before answer the following questions and the output the answer. Question: { question } Table 10 LLM-Based Evaluation Prompt. LLM-Evaluation Prompt You are an answer evaluator. will give you response and an answer. Please tell me whether this response is correct or wrong. Just answer yes or no. For example, Response: The figure that cannot be folded into cube is: C. <image> Correct Answer: So, you need to respond no only. Response: The unfolded shape of the cube is: B. <image> Correct Answer: So, you need to respond yes only. Here is the response and correct answer want you to evaluate. Response: { model response } Correct Answer: { correct answer }"
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Singapore University of Technology and Design",
        "Zhejiang University"
    ]
}