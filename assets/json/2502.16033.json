{
    "paper_title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "authors": [
        "Qianqi Yan",
        "Yue Fan",
        "Hongquan Li",
        "Shan Jiang",
        "Yang Zhao",
        "Xinze Guan",
        "Ching-Chen Kuo",
        "Xin Eric Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency."
        },
        {
            "title": "Start",
            "content": "Multimodal Inconsistency Reasoning (MMIR): New Benchmark for Multimodal Reasoning Models Qianqi Yan1, Yue Fan1, Hongquan Li , Shan Jiang2, Yang Zhao2, Xinze Guan2, Ching-Chen Kuo2, and Xin Eric Wang1 1University of California, Santa Cruz 2eBay 5 2 0 2 2 2 ] A . [ 1 3 3 0 6 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-ofMark (SoM) methods, yields marginal gains, revealing key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency. Figure 1: An illustration of multimodal inconsistency reasoning on webpage. An agent examines webpage where the brand IKEA AB is mentioned, but other elements clearly refer to Lorell. Detecting this brand identity misattribution requires the ability to compare text fields across different sections of the page and reconcile them with accompanying images or contextan inherently multimodal reasoning task."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Language Models (LLMs) have demonstrated impressive reasoning abilities across variety of tasks (OpenAI, 2024b; Guo et al., 2025; Kojima et al., 2022; Wei et al., 2022). Building on pre-trained LLMs, Multimodal Large Language Models (MLLMs) are fast evolving. However, they usually face greater challenges as they need to reason across different modalities, especially when inconsistencies (i.e., mismatched or contradictory contents) exist. We find that, being primarily trained and evaluated on consistent visual-textual inputs, existing MLLMs are largely untested in scenarios where the input contains misaligned or contradictory informationa situation that is common in real-world scenarios. For example, in Figure 1, user presents web page 1 containing conflicting visual and textual elements, asking the model to identify errors. To comprehensively evaluate the ability of MLLMs in reasoning over multimodal inconsistency, we introduce the Multimodal Inconsistency Reasoning Benchmark (MMIR). MMIR is the first framework dedicated to evaluating how effectively MLLMs can reason about and identify semantic mismatches within complex, layout-rich content with interleaved image and text components. Our benchmark is built on diverse collection of real-world artifacts (e.g. websites, slides, posters) which have been augmented with synthetic inconsistenciesrealistic inconsistency errors injected into their original structures. These inconsistency errors span range of reasoningheavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherenceposing next-level reasoning challenge for models. For example, resolving Identity Misattribution involves verifying entity alignment across modalities, while Quantitative Discrepancy requires cross-referencing chart data with textual claims. By challenging models to detect such inconsistencies, MMIR forces them to perform intricate reasoning that goes well beyond simple pattern recognition. This benchmark not only exposes the limitations of current MLLMs in handling realworld challenges of reasoning over multimodal content with inconsistency, but also provides platform for developing more robust multimodal reasoning systems. In our experiments, we evaluated the advanced multimodal reasoning model o1 (OpenAI, 2024b) and five other state-of-the-art MLLMs: GPT-4o (OpenAI, 2024a), Qwen2.5-VL (Team, 2025), LLaVA-NeXT (Liu et al., 2024b), InternVL2.5 (Chen et al., 2024) and Phi-3.5Vision (Abdin et al., 2024) using MMIRs 534 test samples. The results overall underscore that current MLLM models struggle with multimodal inconsistency reasoning. Specifically, there is stark contrast between proprietary and open-source models. The open-source models evaluated only reach less than 25% accuracy. o1 with strong reasoning capability achieves the overall best performance with over 50% accuracy. To further understand the benchmarking results, we conduct analysis based on the inconsistency category, modality, and layout complexity of the artifact. We find the proprietary models excel in identifying factual contradiction and identity misattribute types of inconsistency and inconsistency within single modality, either image or text. Last but not least, we investigate some approaches to enhance the model performance in our probing experiment. The results indicate that text-based Chainof-Thought prompting and visual-based prompting (Set-of-Mark annotations) offer minimal and sometimes adverse effects, whereas an iterative multimodal interleaved reasoning strategy shows promising gains. Overall, these results highlight critical bottleneck in the ability of MLLMs to perform robust, integrated reasoninga key challenge for future research. Our contributions are threefold: We introduce MMIR, novel benchmark that targets the critical yet underexplored task of multimodal inconsistency reasoning in layoutrich content. We perform comprehensive evaluation of one leading multimodal reasoning model and five state-of-the-art MLLMs, revealing significant gaps in their ability to detect inconsistency errors with detailed error analyses across multiple error types, modalities, and layout complexities. We provide detailed probing analyses that expose key challengesfrom perceptual shortcomings to reasoning bottlenecksand propose framework that iteratively refines predictions by jointly leveraging visual and textual modalities."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Understanding and Reasoning Multimodal Large Language Models (MLLMs) process multimodal inputs by first processing visual inputs with pre-trained vision encoders such as CLIP (Radford et al., 2021) to extract features, and then projecting them into the textual representation space with adapters (Liu et al., 2024a; Li et al., 2023a). Significant efforts have been made to bridge the gap between vision and text modalities via integrating more cross-modality data such as interleaved image-text sequences and visual grounding data (Alayrac et al., 2022; Chen et al., 2023; Peng et al., 2023). Also, some recent works develop MLLMs with improved nuanced multimodal abilities, such as Optical Character Recognition (OCR) (Bai et al., 2023; Liu et al., 2024b), layout 2 understanding (Feng et al., 2024; Fan et al., 2024a), Graphic User Interface (GUI) interpretation (Liu et al., 2024c; Team, 2025). As MLLMs typically leverage pre-trained large language models (LLMs) as the backbone, they inherent strong textural reasoning abilities from the advanced LLMs(Floridi and Chiriatti, 2020; Touvron et al., 2023; Bai et al., 2023; Taori et al., 2023; Chowdhery et al., 2023; OpenAI, 2024a; Team, 2024). To further enhance the reasoning ability of MLLMs, increasing efforts have focused on improving MLLMs in multimodal reasoning. The proprietery model, o1 (OpenAI, 2024b) first realize strong multimodal reasoning with reasoning process similar to the Chain-of-Thought (Wei et al., 2022) and other following works have also explored the multimodal reasoning either through training (Wu and Xie, 2024; Qi et al., 2024; Shao et al., 2024) or prompting (Zhang et al., 2023, 2024b; Zheng et al., 2023). Multimodal Reasoning Benchmarks To evaluate the reasoning capabilities of MLLMs, numerous benchmarks have been developed with various focuses. Broad-coverage benchmarks such as MM-Bench (Liu et al., 2024d), MMMU (Yue et al., 2024) and MM-Vet (Yu et al., 2024) cover comprehensive reasoning challenges in real life scenarios, offering holistic insights into model performance. Others are developed with focuses on specific perspectives, such as TextVQA (Singh et al., 2019), POPE (Li et al., 2023b) and MATHVERSE (Zhang et al., 2024a) respectively challenge models with tasks in domains of reasoning about text, objects, mathematics in multimodal contexts. Recently, additional benchmarks have emerged targeting artificially created multipanel imagessuch as posters and screenshotsthat combine several subfigures in structured layouts (Fan et al., 2024b; Hsiao et al., 2025), which require models to analyze spatial relationships and hierarchical structures in complex visual contexts. However, current multi-modal benchmarks assume visual-text alignment, overlooking detecting critical errors of vision-language inconsistency in the input - key challenge in real-world scenarios. Instead, we evaluate MLLMs ability to detect and localize such inconsistency via the proposed MMIR benchmark. Inconsistency Checking Existing works on tasks related to checking or verifying inconsistency in the input are primarily in the language domain. For example, fact-checking (Thorne et al., 2018) requires model to first retrieve evidence and then decide if claim is supported, where the model must reason if contradictive information existed in the retrieved corpus. One step further, summary inconsistency detection (Laban et al., 2022) focuses on flagging any errors in summaries that create contradictions regardless of correctness, including incorrect use or hallucination of entities. As modern language models prosper, inconsistencies are found existing within their outputs (Ravichander et al., 2020) and across different outputs of paraphrased queries (Elazar et al., 2021), and efforts have been made towards the evaluation of those inconsistencies (Fabbri et al., 2021; Wang et al., 2020; Lattimer et al., 2023). In our research, we lead efforts in detecting inconsistencies in the field of vision and language."
        },
        {
            "title": "3 MMIR",
            "content": "The MMIR benchmark is designed to assess how effectively MLLMs can detect and localize semantic mismatches within complex, layout-rich artifacts. Unlike conventional benchmarks that assume coherent visualtextual inputs, MMIR challenges models with realistic errors that require deep, crossmodal reasoning. In MMIR, errors are defined and categorized along five semantic dimensions: A. Factual Contradiction: Direct conflict between two elements (texttext, textimage, or imageimage) within the modified content. B. Identity Misattribution: Mislabeling of entities (objects, locations, brands, people) that conflict with other elements. C. Contextual Mismatch: Tonal, thematic, or situational incompatibility between elements. D. Quantitative Discrepancy: Numerical or statistical inconsistencies between elements. E. Temporal/Spatial Incoherence: Implied timelines, dates, or spatial relationships that are impossible or conflicting. Figure 2 provides one example from each error type across web, office, and poster artifacts, illustrating the diverse challenges MMIR poses."
        },
        {
            "title": "3.1 Data Curation",
            "content": "MMIRs data is curated through four-stage pipeline (Figure 3), ensuring high-quality, diverse, and challenging test cases. Artifact Collection and Parsing We begin by manually selecting total of 521 original artifacts from two domains: 349 webpages (sub3 Figure 2: There are five inconsistency categories in the MMIR benchmark, posing diverse challenges. and bounding box showing location information. Additionally, each artifact is paired with Set-ofMarks (SoM) annotation ASoM derived from Ei. This structured metadata forms the basis for subsequent error injection and question-answer curation. Synthetic Inconsistency Generation To simulate real-world errors, we prompt an MLLM, o11217 (OpenAI, 2024b), as generator with the annotated artifact and its element set {ASoM , Ei}. The generator produces 2,534 proposals, each comprising formatted edit instruction, the groundtruth element or element pair introducing the inconsistency: GT {idj} {(idj, idk)j = k}, the inconsistency error type, and the accompanying rationale. Following self-evaluation loop (details in Appendix A.2), 2,446 valid proposals are retained. Figure 3: MMIR Data filtering process. categories: shopping, classifieds, wiki) from VisualWebArena (Koh et al., 2024) and 172 presentations from Zenodo (European Organization For Nuclear Research and OpenAIRE, 2013), categorized into Office (sub-categories: slides, charts, diagrams) and Posters. Each artifact Ai is parsed using either using Document Object Model (DOM) or the python-pptx library to extract set of elements Ei = {ej}ni j=1, where each element ejis assigned unique ID idj and labeled with its type, content, 4 Table 1: MMIR Statistics. Breakdown of the dataset by artifact category and error type."
        },
        {
            "title": "Category",
            "content": "#Questions Ave. #Elements"
        },
        {
            "title": "Web",
            "content": "- Shopping - Wiki - Classifieds"
        },
        {
            "title": "Office",
            "content": "- Slides - Tables/Charts - Diagrams"
        },
        {
            "title": "Error Categories",
            "content": "Factual Contradiction Identity Misattribution Contextual Mismatch Quantitative Discrepancy Temporal/Spatial Incoherence"
        },
        {
            "title": "Total",
            "content": "240 108 28 104 223 102 61 60 71 543 138 84 141 76 95 543 38.8 46.1 44.9 29.5 9.1 9.4 4.1 13.9 27.6 24.9 Automated Editing and Human Verification An auto-verification process then filters these proposals based on format and backend constraints (e.g., ensuring the target elements are editable), reducing the candidate set to 1,273, and saves lowlevel edit details, such as the path of the new image for an image edit, as inputs to the editor. An automated editor-implemented using the Chrome DevTools Protocol (CDP) for web pages and python-pptx for presentations-executes the approved edits, generating for each successful operation modified pair: {A i, i} where represents the modified artifact and contains the updated element metadata after the edit. For each pair, descriptive caption set Ci is generated, where each caption within Cj details the element ID, location, and content summary of j. These captions serve as references for later evaluation. More details on the verifier and editor are provided in Appendix A.3. i, Finally, human experts review 747 edited resulting in final dataset of = samples, DM IR quintuples: validated 534 i, GTi, categoryi, rationalei}534 {S i=1, ensuring that only realistic and challenging samples remain. Table 1 provides detailed type, subcategory, and breakdown by artifact error type. For example, webpages are further divided into shopping, wiki, and classifieds, each with its average number of elements, while errors are distributed across the five defined categories. Notably, the average word count in multiple-choice questions is 382.6, whereas open-ended responses are fixed at 59 words."
        },
        {
            "title": "3.2 Evaluation",
            "content": "MMIR assesses models ability to detect inconsistency, i.e., identifying and localizing semantic mismatches where elements deviate from their expected roles within an artifact. To assess the models performance comprehensively, each of the 534 test samples is provided to models under two distinct settings: Open-Ended Setting Models receive the artifact with fixed prompt Qopen_ended and generate free-form response that identifies the semantic mismatch. This formulation evaluates the models ability to detect inconsistencies without relying on predefined answer options, thereby testing its unsupervised perception and reasoning. Multiple-Choice Setting Models receive the artifact i, but now with combined prompt QMCQ = (Qopen_ended, Ci). Each candidate in Ci is textual description of an element. The model must select, from these options, the element(s) corresponding to the introduced inconsistency. Evaluation Setup For the MCQ setting, we utilize regular expressions to compare the MLLMs predicted answers against the ground truth, using accuracy as our metric. For the open-ended setting, o1-mini (0912) is employed as an LLM judge (Hsu et al., 2023; Hackl et al., 2023; Liu et al., 2023) to map the models free-form response back to the most likely ground-truth element IDs. The predicted IDs are then compared against GTi to calculate accuracy."
        },
        {
            "title": "4 Experiments and Analysis",
            "content": "We first evaluate the advanced multimodal reasoning model o1 (OpenAI, 2024b) and five other state-of-the-art MLLMs: GPT-4o (OpenAI, 2024a), Qwen2.5-VL (Team, 2025), LLaVA-NeXT (Liu et al., 2024b), InternVL2.5 (Chen et al., 2024) and Phi-3.5-Vision (Abdin et al., 2024) on the MMIR benchmark. We implement open-source models using their default settings and select the 1217 version of o1 and the 1120 version of GPT-4o for evaluation. Model implementation details are provided in Appendix B. We then examine error patterns across different inconsistency types and layout complexities and finally explore how prompting strategies affect multimodal reasoning under the open-ended setting. 5 Table 2: The accuracy of six MLLMs under the two evaluation settings. Proprietary models demonstrate higher performance as well as larger performance gain in the MCQ setting. Open-ended Multiple-choice"
        },
        {
            "title": "Web Office Poster Overall Web Office Poster Overall",
            "content": "47.91 25.00 Proprietary Models o1 (1217) GPT-4o (1120) Open-sourced Models 8.54 Qwen2.5-VL-7B LLaVA-NeXT-7B 10.20 7.70 InternVL2.5-8B 6.87 Phi-3.5-Vision-4B 59.19 42.60 38.73 30.98 29.14 21.97 24.21 24.43 11.97 7.04 4.92 7. 51.40 33.14 17.60 14.70 14.23 14.23 47.91 37.29 58.52 58.96 46.47 47.88 14.37 11.45 9.37 1. 33.18 25.33 23.54 8.52 16.90 5.63 11.97 0.00 52.15 47.75 22.56 16.47 15.63 4."
        },
        {
            "title": "4.1 Main Results",
            "content": "As shown in Table 2, proprietary models (o1 and GPT-4o) significantly outperform open-source alternatives, though all models exhibit substantial room for improvement. Appendix A.4 shows qualitative example with question-answer and model response. Performance Gap Between Reasoning, Proprietary and Open-Source Models. In both openended and MCQ settings, the reasoning o1 model substantially outperforms the rest, surpassing all open-source models by over 30%. The other proprietary model GPT-4o, although missing the explicit reasoning ability of o1, outperforms open-source alternatives, reflecting stronger multimodal alignment and reasoning capabilities. Impact of Semantic Cues. GPT-4o sees 14.61% accuracy boost in the MCQ setting with additional element descriptions as options, narrowing its gap with o1 from 18.26% to just 4.4%. This indicates that GPT-4o relies heavily on semantic context when available. Inconsistent Gains for Open-Source Models. Most open-source models gain moderate or little accuracy when provided with MCQ-style prompts. Phi-3.5-Vision-4B experiences 9.93% drop, suggesting weaker reasoning capacity and less effective use of textual cues. The gap between proprietary and open-source models widens further in MCQ (from 27.08% to 35.21%), highlighting the persistent challenge of integrating perceptual grounding with logical inference."
        },
        {
            "title": "4.2.1 Results Across Inconsistency Categories",
            "content": "and Modalities To investigate how different types of inconsistencies affect model performance, we show the results across the category and modality of inconsistency in Figure 4. Inconsistency Categories Figure 4(a) breaks down accuracy by the five inconsistency error categories. Proprietary models (o1, GPT-4o) outperform open-source models across the board, but the gap is particularly pronounced for Factual Contradictions and Identity Misattribution, implying that high-capacity models may have stronger factual grounding and entity recognition. Interestingly, Temporal/Spatial Incoherence also poses substantial challenge for all models, highlighting limitation in reasoning about time and space coherence. Inconsistency Modalities In Figure 4(b), we examine how accuracy varies by the modality of the inconsistency. Overall, single-modality errors (those involving only one text or image field) yield the highest performance, with text-text inconsistencies proving especially tractablelikely because these language-centric models excel at purely textual reasoning. Next in difficulty are inter-modality errors (image-text), which require partial crossmodal integration but can still leverage textual anchors. Finally, image-image inconsistencies pose the greatest challenge, as they demand more advanced visual understanding and the ability to reconcile two distinct visual elements without the benefit of textual cues. These findings highlight that while language-focused models cope relatively well with purely textual conflicts, their capacity for deep visual or cross-modal reasoning remains underdeveloped."
        },
        {
            "title": "4.2.2\nWe further examine the relationship between model\naccuracy and the number of elements in an arti-\nfact. To ensure statistical significance, we only\ninclude data points where at least 10 samples share",
            "content": "6 Figure 4: Fine-grained analysis of model performance. Table 3: Probing results of different prompting methods. Performance of each prompting method is directly compared with the vanilla setting. Gains are in blue and drops are in red."
        },
        {
            "title": "Models",
            "content": "Vanilla + CoT + SoM + Both MM-CoT Figure 5: Model performance on layout complexity. Proprietary Models o1 (1217) GPT-4o (1120) Open-sourced Models Qwen2.5-VL-7B LLaVA-NeXT-7B InternVL2.5-8B Phi-3.5-Vision-4B 51.40 33.14 17.60 14.70 14.23 14.23 +0.28 -1.78 +2.24 -0.38 -0.66 +5.34 +0.09 -2.53 -0.66 +0.47 +0.28 -0.47 -1.41 +0.84 +0.09 +4. +4.59 +3.65 -0.85 +0.65 the same element count. As shown in Figure 7, the overall trend suggests that handling visually dense, information-rich artifacts remains major challenge for current MLLMs. (1) Performance declines sharply as the number of elements increases, highlighting the difficulty in parsing cluttered layouts. (2) Proprietary models maintain higher accuracy in simpler layouts but degrade similarly in highly dense artifacts, indicating limitations in spatial reasoning. Open-source models struggle even in low-complexity settings, reinforcing the gap in perception and layout-aware inference."
        },
        {
            "title": "4.3 Probing on Prompting Methods",
            "content": "We further investigate whether textual or visual prompts can alleviate the reasoning bottleneck. Table 3 compares Chain-of-Thought (CoT) prompting (Wei et al., 2022) and Set-of-Mark (SoM) visual augmentation (Yang et al., 2023), as well as their combination. We also explored an interleaved multimodal reasoning strategy, which we term Multimodal Interleaved CoT (MM-CoT) to further integrate and refine reasoning across both visual and textual modalities."
        },
        {
            "title": "4.3.1 Chain-of-Thought (CoT) Prompting",
            "content": "To assess whether explicit reasoning instructions can enhance performance, we apply CoT prompting (Wei et al., 2022) to the four open-sourced models (benchmarked proprietary models have API guides to not include additional CoT prompting). As shown in Table 3, CoT prompting yields negligible or even negative effects on accuracy. This suggests that simply injecting explicit reasoning steps is insufficient when the underlying model lacks strong cross-modal alignment or robust logical inference mechanisms."
        },
        {
            "title": "4.3.2 Set-of-Mark (SoM) Prompting",
            "content": "We next examine the effect of SoM visual prompting (Yang et al., 2023). By overlaying bounding boxes onto the artifact screenshots (example in Figure 6), we aim to enhance the models ability to perceive and localize elements. 7 ment IDs from the artifacts metadata Ci. We then highlight the bounding boxes of those elements on the artifact image, producing an SoM-annotated version to be used in the next stage. Stage 2: Multimodal Refinement The model is subsequently given the SoM-annotated artifact from Stage 1, alongside the textual reasoning it generated previously. This additional visual context helps the model refine its earlier predictions, integrating both the visual bounding-box annotations and the initial textual reasoning to arrive at final answer. Results As shown in Table 3, MM-CoT outperforms all other prompting methods. GPT-4o, for example, improves by 4.40% over its vanilla baseline, while open-source models gain an average of around 2% improvements. These findings underscore the importance of iterative cross-modal reasoning: once textual inferences guide which visual elements to focus on, SoM annotations become more informative, and the overall reasoning process becomes more accurate. Although the bounding boxes used for SoM are derived from ground-truth references, this probing experiment demonstrates that interleaved multimodal interaction is promising direction for closing the reasoning gap in challenging, inconsistency-heavy scenarios."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "In this work, we introduce the Multimodal Inconsistency Reasoning Benchmark (MMIR) to evaluate how well MLLMs detect and localize semantic mismatches in complex real-world artifacts. MMIR challenges models across five error categories and two reasoning settings for detailed assessment of multimodal reasoning. Our experiments show that even advanced proprietary models struggle with open-ended inconsistency detection. Although providing natural-language descriptions in multiple-choice format offers modest gains, standard prompting techniques (e.g., Chainof-Thought and Set-of-Mark) yield inconsistent or negative effects, while proposed Multimodal Interleaved CoT (MM-CoT) method that iteratively refines reasoning by integrating visual and textual modalities, yielding greater performance improvements. Despite these advances, significant challenges remain, motivating further research on robust multimodal reasoning for real-world inconsistency detection. Figure 6: Example of original artifact in MMIR (left) and artifact annotated with Set-of-Mark in the probing analysis (right). The result shows that these additional visual cues yield moderate improvements for GPT-4o (5.34%) yet confuse the rest of the models, leading to little or even slightly degraded performance, likely because the additional visual cues interfere with the models initial perception. When combined with CoT prompting, SoM provides little gains for some open-source models but remains largely inconsistent or even detrimental for others. This indicates that simply stacking CoT and SoM techniques does not guarantee improved performance, underscoring the need for more sophisticated strategies to unify visual cues with explicit reasoning steps."
        },
        {
            "title": "4.3.3 Multimodal Interleaved CoT (MM-CoT)",
            "content": "Our previous analyses indicate that single-modality prompts (CoT or SoM) often yield minimal or even detrimental gains in the open-ended setting when models receive no textual hints about which elements might be inconsistent. We hypothesize that MMIR tasks demand iterative reasoning that tightly integrates both visual and textual modalities. To address this, we propose Multimodal Interleaved CoT (MM-CoT), two-stage approach explicitly designed to weave visual cues into step-by-step reasoning process: Stage 1: Initial Candidate Generation The model receives the same input in Stage 1 as in the open-ended setting, generating its top five predictions (along with associated reasoning). Using o1-mini (0912) to interpret these responses, we map each prediction back to one or pair of ele-"
        },
        {
            "title": "Limitations",
            "content": "While MMIR provides rigorous framework for evaluating multimodal inconsistency reasoning, it is not without its limitations. Annotating and verifying inconsistencies in layout-rich artifacts remains labor-intensive process. Although MMIRs pipeline integrates automated editing and verification, the overall scale is still limited by the need for careful human review. Although these domains capture range of layouts and content types, they do not encompass the full variety of real-world multimodal artifacts (e.g., multi-page documents, social media feeds, or mobile application interfaces). On the other hand, synthetic error generationwhile effective for systematically introducing controlled inconsistenciesmay not perfectly mirror the nuanced mistakes that occur in human-generated content. This could lead to discrepancies between model performance on MMIR and in truly openended, real-world scenarios. Scaling up the dataset to cover broader domains, more intricate layouts, and diverse error types would strengthen its ability to serve as comprehensive benchmark for realworld multimodal inconsistency detection."
        },
        {
            "title": "Acknowledgments",
            "content": "This research project is partially sponsored by an eBay Research Award and has benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile vision-language model for understanding, localizaarXiv preprint tion, arXiv:2308.12966, 1(2):3. text reading, and beyond. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Hui Deng, Jiaye Ge, Kaiming Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahu Lin, Yunfeng Qiao, Jifeng Dai, and Wenhai Wang. 2024. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. ArXiv, abs/2412.05271. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:10121031. 9 European Organization For Nuclear Research and OpenAIRE. 2013. Zenodo. neural information processing systems, 35:22199 22213. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021. Qafacteval: Improved qa-based factual consistency evaluation for summarization. arXiv preprint arXiv:2112.08542. Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, and Xin Eric Wang. 2024a. Read anywhere pointed: Layout-aware gui screen reading with tree-of-lens grounding. arXiv preprint arXiv:2406.19263. Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Yang Zhao, Xinze Guan, and Xin Wang. 2024b. Muffin or chihuahua? challenging multimodal large language models with multipanel vqa. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68456863. Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2024. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36. Luciano Floridi and Massimo Chiriatti. 2020. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681694. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Veronika Hackl, Alexandra Elena Müller, Michael Granitzer, and Maximilian Sailer. 2023. Is gpt-4 reliable rater? evaluating consistency in gpt-4s text ratings. In Frontiers in Education, volume 8, page 1272229. Frontiers Media SA. Yu-Chung Hsiao, Fedir Zubach, Gilles Baechler, Srinivas Sunkara, Victor Carbune, Jason Lin, Maria Wang, Yun Zhu, and Jindong Chen. 2025. Screenqa: Largescale question-answer pairs over mobile app screenshots. Preprint, arXiv:2209.08199. Ting-Yao Hsu, Chieh-Yang Huang, Ryan Rossi, Sungchul Kim, Lee Giles, and Ting-Hao Huang. 2023. Gpt-4 as an effective zero-shot evaluator for scientific figure captions. arXiv preprint arXiv:2310.15405. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. 2024. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Philippe Laban, Tobias Schnabel, Paul Bennett, and Marti Hearst. 2022. Summac: Re-visiting nlibased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163177. Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023. Fast and accurate factual inconsistency detection over long documents. arXiv preprint arXiv:2310.13189. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023b. Evaluating object hallucination in large vision-language models. Preprint, arXiv:2305.10355. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b. Llavanext: Improved reasoning, ocr, and world knowledge. Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. 2024c. Harnessing webpage uis for text-rich visual understanding. Preprint, arXiv:2410.13824. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2024d. Mmbench: Is your multi-modal model an all-around In European conference on computer viplayer? sion, pages 216233. Springer. OpenAI. 2024a. Gpt-4o system card. Preprint, arXiv:2410.21276. OpenAI. 2024b. Openai o1 system card. Preprint, arXiv:2412.16720. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2: Grounding multimodal large arXiv preprint language models to the world. arXiv:2306.14824. 10 Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Penghao Wu and Saining Xie. 2024. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13084 13094. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun yue Li, and Jianfeng Gao. 2023. Set-of-mark prompting unleashes extraordinary visual grounding in gpt4v. ArXiv, abs/2310.11441. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2024. Mm-vet: Evaluating large multimodal models for integrated capabilities. Preprint, arXiv:2308.02490. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. 2024a. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? Preprint, arXiv:2403.14624. Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. 2023. What makes good examples for visual in-context learning? Advances in Neural Information Processing Systems, 36:1777317794. Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, and Jiaya Jia. 2024b. Prompt highlighter: Interactive control for multi-modal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1321513224. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. 2023. Ddcot: Duty-distinct chain-ofthought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:51685191. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. 2024. Cogcom: Train large visionlanguage models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Abhilasha Ravichander, Eduard Hovy, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. 2020. On the systematicity of probing contextualized word representations: The case of hypernymy in bert. In Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics, pages 88102. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. Gemini Team. 2024. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. Qwen Team. 2025. Qwen2.5-vl."
        },
        {
            "title": "James",
            "content": "Andreas Vlachos, Christos Thorne, Christodoulopoulos, and Arpit Mittal. 2018. Fever: large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. arXiv preprint arXiv:2004.04228. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on"
        },
        {
            "title": "A Benchmark Details",
            "content": "This appendix provides comprehensive overview of the MMIR benchmark. It details the dataset curation process, including error category definitions, the synthetic inconsistency generation mechanism, the auto-verification and human validation processes, and the task prompts for evaluation. These details are intended to facilitate reproducibility and provide clarity on the inner workings of MMIR. A."
        },
        {
            "title": "Inconsistency Error Category Definitions",
            "content": "The MMIR benchmark employs five pre-defined error categories. These categories are designed based on semantic guidelines so that the generator model can propose diverse and generalizable inconsistencies without being tied to any specific artifact type. A. Factual Contradiction Direct conflict between two or more elements (texttext, textimage, or imageimage) within the modified content. Example (TextText): The product title says Caffeinated, while the description states Caffeinefree. Example (TextImage): The image shows green tea bag, but the accompanying text describes fruit infusion. B. Identity Misattribution Mislabeling of entities (objects, locations, brands, people) that conflict with other elements. Example: product lists Country of Origin: China while the manufacturer is described as Elmwood Inn (USA). C. Contextual Mismatch Tonal, thematic, or situational incompatibility between elements. Example: celebratory image of diplomats shaking hands is paired with an article about violent clashes. D. Quantitative Discrepancy Numerical or statistical inconsistencies between elements. Example: graph labeled 50% growth shows flat bars. E. Temporal/Spatial Incoherence Implied timelines, dates, or spatial relationships that are impossible or conflicting. Example: map labeled North America depicts landmarks from Europe. These definitions serve as guidelines during the synthetic inconsistency generation process, ensuring that the proposed errors are semantically meaningful and cover broad spectrum of potential real-world mistakes. A.2 Generator Model and Self-Evaluation Loop A.2.1 Generator Model Prompt To create adversarial examples, the generator model (o1, 1217) is provided with rich context consisting of the annotated artifact ASOM and its set of elements Ei. The task prompt includes detailed instructions regarding the types of modifications to propose, along with the following guidelines: Modification Format: Each modification must be expressed as: Modify [id] [original_content] [new_content] For image fields, the original content includes the full details (e.g., URL), and the new content is caption starting with \"Image, description: \". For text fields, the new content should be of similar length to the original. Error Categories: The generator must propose one modification per error category. If it cannot propose an inconsistency for given category, it may skip that category. The generator output is structured as: Pm = { edit m, GTm, category m, rationale m} where the ground-truth GTm is defined as: GTm {idj} {(idj, idk) = k} indicating either single-element ID (for single-element inconsistencies) or pair of distinct element IDs (for relational inconsistencies). A.2.2 Self-Evaluation Loop We follow generator-evaluator loop that refines proposals through iterative self-assessment. simplified Python snippet of the loop function is provided below: 1 def loop ( client , image_dir , frame_id , task : str , evaluator_prompt : str , generator_prompt : str ) -> tuple [ str , list [ dict ]]: \"\"\" Keep generating and evaluating until requirements are met . \"\"\" memory = [] chain_of_thought = [] thoughts , result = generate ( client , image_dir , frame_id , generator_prompt , task ) memory . append ( result ) chain_of_thought . append ({ \" thoughts \": thoughts , \" result \": result }) loop_count = 1 while True : all_pass = True evaluation , feedback = evaluate ( client , image_dir , frame_id , evaluator_prompt , result , task ) for eval_line in evaluation . split ( \" n\" ): if eval_line . strip () != \" PASS \" : all_pass = False break if all_pass or loop_count == 2: return result , evaluation context = \" n\". join ([ \" Previous attempts :\" , *[ f\" - {m}\" for in memory ], f\" nFeedback : { feedback }\" ]) thoughts , result = generate ( client , image_dir , frame_id , generator_prompt , task , context ) memory . append ( result ) chain_of_thought . append ({ \" thoughts \" : thoughts , \" result \": result }) loop_count += 1 In this loop, the generator produces proposals which are then evaluated against the following criteria (as specified in the evaluator prompt): Category Compliance: The edit must match the intended error category. Atomic Modification: Exactly one inconsistency should be introduced. Visual Consistency: The modified screenshot must visibly reflect the error without relying on external context. 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 Element Validity: The referenced element IDs must exist in the artifact. Only proposals receiving \"PASS\" in the evaluation are retained. The loop iterates until either all criteria are met or maximum of two iterations is reached. A.2.3 Prompt details for generator-evaluator proposal generation framework This is the task prompt as input to the o1 generator model. task_prompt = \"\"\" 1 2 < user input > 3 Your task is to modify { category_str } to create inconsistency . For each given category of inconsistency , you will propose modification action that introduces the inconsistency in the modified { category_str }. 4 5 Here 's the information you ' ll have : 6 Screenshot of the urrent { category_str }: This is screenshot of the { category_str }, with each editable element assigned unique numerical id . Each bounding box and its respective id share the same color ."
        },
        {
            "title": "7 The Observation , which lists the IDs of all editable elements on the current {",
            "content": "category_str } with their content , in the format [ id ] [ tagType ] [ content ], separated by \" \". Each id is mapped with the id in the screenshot . tagType is the type of the element , such as button , link , or textbox . For example , \"[21] [ SPAN ] [ Add to Wish List ]\" means that there is span with id 21 and text content ' Add to Wish List ' on the current { category_str }. \"[23] [ IMG ] [ Image , description : beige powder on white background , url : http :// localhost :7770/ media / catalog / product / cache /829 a59e57f886f8cf0598ffca4f8a940 /B /0/ B074DBMG66 .0. jpg ]\" means that there is an image on the current screen with id 23 , with description of the image and its url specified . 8 9 Here are the categories of errors you can introduce : 10 A. Factual Contradiction - Direct conflict between two or more elements ( text - text , text - image , or image - image ). For example , The product title says \" Caffeinated ,\" while the description states \" Caffeine - free .\" Another example : The image shows green tea bag , but the text describes \" fruit infusion .\" 11 B. Identity Misattribution - Mislabeling of entities ( objects , locations , brands , people ) that conflict with other elements . Example : Product \" Country of Origin : China \" contradicts manufacturer info \" Elmwood Inn ( USA ) .\" 12 C. Contextual Mismatch - Tonal , thematic , or situational incompatibility between elements . Example : celebratory image of diplomats shaking hands paired with an article about violent clashes . 13 D. Quantitative Discrepancy - Numerical or statistical inconsistencies between elements . Example : graph labeled \"50% growth \" shows flat bars . 14 E. Temporal / Spatial Incoherence - Implied timelines , dates , or spatial relationships that are impossible or conflicting . Example : map labeled \" North America \" depicts landmarks from Europe 15 16 Here are the rules for the modification action : 17 The modification action you can propose to introduce inconsistency must be in the format of \" Modify [ id ] [ original_content ] [ new_content ]\": This action proposes to edit the orignal field assigned with the id to the new content to introduce inconsistency . If you propose to modify an image field , the [ original_content ] field should include the full content from observation including the url ; the [ new_content ] field should be caption describing the updated image , starting with \" Image , description : \", no url needed . If you propose to modify text field , the new content string should be about the same length as the original text field . For each inconsistency category , you should try to propose modification action that introduces an inconsistency in that category . If you can 't find way to introduce an inconsistency in category , you can skip it . Prioritize proposing edits on text fields over image fields . 18 19 Generate the response in the correct format . For each inconsistency , the format should be : 22 23 <cat >[A -E ] </ cat > <-- Category letter <ele >[ ID1 , ID2 ] </ ele > <-- Conflicting element IDs <mod > Modify [ ID ] [ Original Content ] [ New Content ] </ mod > <-- Modification plan < rationale > Visible conflict explanation </ rationale > <-- Visual verification 20 < proposal > 21 24 25 </ proposal > 26 </ user input > 27 \"\"\" These are prompts for the generator and evaluator model. evaluator_prompt = \"\"\" 1 2 Evaluate the following proposals one by one for : 3 1. Category Compliance : Introduced inconsistency matches the category definition (A - E) 4 2. Atomic Modification : Introduce EXACTLY ONE inconsistency without side effects 5 3. Visual Consistency : Conflict visible in the modified screenshot ( with NO reliance on original page knowledge or external context ) 6 4. Element Validity : Conflict IDs exist in observations 7 8 You should be evaluating only and not attemping to solve the task . 9 For each proposal , only output \" PASS \" if all criteria are met and you have no further suggestions for improvements ."
        },
        {
            "title": "10 Output your evaluation concisely in the following format .\n11\n12 < evaluation >\n13 PASS , NEEDS_IMPROVEMENT , or FAIL <-- For each proposal\n14 </ evaluation >\n15 < feedback >\n16 What needs improvement and why . <-- For proposals that need improvement\n17 </ feedback >\n18 \"\"\"\n19",
            "content": "generator_prompt = \"\"\" 20 21 Your goal is to complete the task based on < user input >. If there are feedback 22 from your previous generations , you should reflect on them to improve proposals that NEEDS_IMPROVEMENT or FAIL . Leave the PASS proposals as they are . 23 24 Output your answer concisely in the following format : 25 26 < thoughts > 27 [ Your understanding of the task and feedback and how you plan to improve ] 28 </ thoughts > 29 30 < response > 31 [ Your response here ] 32 </ response > 33 \"\"\" A.3 Auto-Verification and Editing Process Following proposal generation, an auto-verification step filters the proposals based on format and backend constraints. Specifically: Edit Format Verification: The system uses regular expression to ensure that each proposed edit adheres to the required format: \"Modify [id] [old_content] [new_content]\". Element Matching: For web-sourced artifacts, the proposals element ID is used to locate the corresponding element and its bounding box in the metadata. The system checks that both the content and bounding box match an editable element in the HTML/PPTX structure. For image edits, the new content (a caption) is cross-referenced against an MSCOCO image database to verify its appropriateness. Proposals that pass these checks are automatically saved for further processing. For web pages, we use the CDP to perform edit: 1 # text edit 2 client . send ( \" Runtime . callFunctionOn \" , { 4 5 6 \" objectId \": object_id , \" functionDeclaration \": f\" function () {{ this . nodeValue = '{ new_content } '; }} \" , 15 7 8 14 15 17 18 19 20 21 23 24 4 5 6 8 9 10 11 \" arguments \": [] , \" returnByValue \": True } 9 10 ) 11 # image edit 12 with open ( new_content , \" rb \") as image_file : 13 img = Image . open ( image_file ) new_image_width , new_image_height = img . size for resizing aspect_ratio = new_image_width / new_image_height if / > aspect_ratio : , = , int (w / aspect_ratio ) else : , = int (h * aspect_ratio ) , img = img . resize ((w , h) , Image . Resampling . LANCZOS ) buffer = BytesIO () img . save ( buffer , format =\" JPEG \") buffer . seek (0) base64_image = base64 . b64encode ( buffer . read () ). decode ( \" utf -8 \") new_image = f\" data : image / jpeg ; base64 ,{ base64_image }\" # get original width and height 25 26 client . send ( 27 \" Runtime . callFunctionOn \" , { 29 30 31 32 33 35 36 37 38 ) \" objectId \": object_id , \" functionDeclaration \": \"\"\" function () {{ this . src = '{ new_image } '; }} \"\"\" , \" arguments \": [] , \" returnByValue \": True } For Zenodo presentation, we use the python-pptx library: 1 # text edit 2 if target_shape . has_text_frame : # text edit 3 text_frame = target_shape . text_frame for paragraph in text_frame . paragraphs : for run in paragraph . runs : if edit_info [\" old_content \"] in run . text : try : run . text = run . text . replace ( edit_info [\" old_content \" ], edit_info [ \" new_content \" ]) success = True break except : success = False 12 13 # image edit 14 left , top , orig_width , orig_height = target_shape . left , target_shape . top , target_shape . width , target_shape . height new_width , new_height = img . size 15 pic = target_shape . _element 16 pic . getparent () . remove ( pic ) 17 new_image_path = f\"{ coco_image_dir }/{ edit_info [' new_img_path ']} \" 18 with Image . open ( new_image_path ) as img : 19 20 new_aspect = new_width / new_height 21 orig_aspect = orig_width / orig_height 22 if new_aspect > orig_aspect : scaled_width = orig_width 23 scaled_height = int ( scaled_width / new_aspect ) 24 25 else : 26 scaled_height = orig_height scaled_width = int ( scaled_height * new_aspect ) 27 28 new_left = left + ( orig_width - scaled_width ) // 2 29 new_top = top + ( orig_height - scaled_height ) // 2 30 try : 31 slide . shapes . add_picture ( 32 new_image_path , new_left , new_top , scaled_width , scaled_height # Add the new image in the same location and size 16 33 34 35 except : 36 ) success = True success = False A.4 Qualitative Example Figure 7: test sample with model responses under the two main settings in MMIR: open-ended and multiplechoice."
        },
        {
            "title": "B Model Application Details",
            "content": "Here are the generation methods for the open-sourced models. For o1 and GPT-4o, we utilized the API following API guidelines available at https://platform. openai.com/docs/models#gpt-4o. For Qwen2.5-VL, we implemented the 7B version following the official repository: https://github. com/QwenLM/Qwen2.5-VL. For LLaVA-NeXT, we followed the implementation from https://github.com/LLaVA-VL/ LLaVA-NeXT. For InternVL2.5 we implemented the 8B version at https://github.com/OpenGVLab/InternVL. For Phi-3.5-Vision we implemented the 4B version at https://github.com/instill-ai/models/ tree/main/phi-3-5-vision."
        },
        {
            "title": "C Data Release",
            "content": "We will publicly release comprehensive dataset that includes the artifacts and question-answer pairs in both the open-ended and multiple-choice settings. The licensing terms for the artifacts will follow those set by the respective dataset creators, as referenced in this work, while the curated artifacts will be provided under the MIT License. Additionally, our release will include standardized evaluation protocols, and evaluation scripts to facilitate rigorous assessment. The entire project will be open-sourced, ensuring free access for research and academic purposes."
        }
    ],
    "affiliations": [
        "University of California, Santa Cruz",
        "eBay"
    ]
}