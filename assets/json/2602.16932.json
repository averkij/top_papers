{
    "paper_title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
    "authors": [
        "Jinming Nian",
        "Fangchen Li",
        "Dae Hoon Park",
        "Yi Fang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 1 ] . [ 1 2 3 9 6 1 . 2 0 6 2 : r RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Jinming Nian Santa Clara University Santa Clara, CA, USA jnian@scu.edu Dae Hoon Park Walmart Global Tech Sunnyvale, CA, USA dae.hoon.park@walmart.com Fangchen Li Independent Researcher Bothell, WA, USA fangchen.li@outlook.com Yi Fang Santa Clara University Santa Clara, CA, USA yfang@scu.edu Abstract Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is practical path towards automatic discovery of novel ranking algorithms. Code is available here. CCS Concepts Information systems Retrieval models and ranking. Keywords Lexical Retrieval, Evolutionary Search, LLM-as-optimizer, Automated Algorithm Discovery"
        },
        {
            "title": "1 Introduction\nLexical retrieval algorithms such as BM25 [17] and query-likelihood\n(QL) [16] remain remarkably strong and efficient first-stage rankers.\nOver the decades, numerous variants have been proposed, including\nBM25T [6], BM25-adapt [11], BM25+ [12], BM25L [13], and alter-\nnative QL smoothing methods such as Dirichlet smoothing and\nJelinekâ€“Mercer [23]. Yet these improvements have largely relied\non parameter tuning and human intuition over individual scoring\ncomponents. This raises a natural question: can we automate the\ndiscovery of improved lexical retrieval algorithms? Recent devel-\nopments in large language models (LLMs) for automated scientific\ndiscovery offer a promising path for such automation. Systems like\nAlphaEvolve [15], Evolution of Heuristics [9], and ShinkaEvolve [7]",
            "content": "1BEIR: ArguAna, FiQA, NFCorpus, SciFact, SciDocs, TREC-COVID. BRIGHT: Biology, Earth Science, Economics, Pony, StackOverflow, TheoremQA. Figure 1: Combined score over evolution steps for two seed programs. The combined score is the optimization target, defined as 0.8Avg Recall@100+0.2Avg nDCG@10, averaged across 12 IR datasets1. represent candidate solutions as executable code and leverage LLMs to iteratively mutate and recombine programs, with performancedriven selection. These methods have produced strong results in mathematics and combinatorial optimization, but have not yet been applied to information retrieval. We introduce RankEvolve, program evolution setup in which each candidate ranking algorithm is self-contained Python program (300 lines) scored by an evaluator across multiple datasets. The evaluators per-dataset metrics, single fitness score, sampled top-performing and inspirational candidates, and prior attempted changes together form the prompt for coding LLM to propose the next mutation. Based on AlphaEvolves description, the candidates are managed by combination of MAP-Elites [14] and island-based evolutionary databases [20]. Starting from two seed programs, BM25 and query likelihood with Dirichlet smoothing, we evolve for several hundred steps. The resulting algorithms are novel and effective, introducing scoring mechanisms absent from either seed family. To assess generalization, we evaluate the evolved programs on held-out BEIR [22] and BRIGHT [19] subsets, as well as TREC DL 2019 [3], and DL 2020 [2], and find promising transfer beyond the datasets used during evolution. Our contributions are: (1) RankEvolve is the first attempt at evolving entire retrieval algorithms via LLM-guided program search; (2) we study the importance of seed program design, examining how structural freedom and abstraction affect the evolution Conference17, July 2017, Washington, DC, USA Nian et al. outcome; and (3) we show that RankEvolve can discover algorithms with novel scoring motifs that transfer to unseen datasets, suggesting that evaluator-guided program evolution with LLMs is promising way forward for automating IR research."
        },
        {
            "title": "3.1 Search Space\nThe seed program and system prompt jointly define the search space.\nThe seed program defines the evolvable interface: code regions that\nthe LLM is permitted to modify. To maximize evolutionary free-\ndom, we decompose each ranking function into a small number of\nabstract components. For the BM25 seed, we define three evolvable\ncomponents: document representation, query representation, and\nscoring function. The initial behavior reproduces classic BM25:",
            "content": "BM25(ğ‘, ğ‘‘) = IDF(ğ‘¡) ğ‘¡ ğ‘ tf(ğ‘¡, ğ‘‘) (ğ‘˜1 + 1) (cid:16) 1 ğ‘ + ğ‘ tf(ğ‘¡, ğ‘‘) + ğ‘˜1 (cid:17) ğ‘‘ avgdl IDF(ğ‘¡) = log (cid:18) ğ‘ df(ğ‘¡) + 0.5 df(ğ‘¡) + 0.5 (cid:19) , (1) (2) where tf(ğ‘¡, ğ‘‘) is the term frequency of ğ‘¡ in document ğ‘‘, df(ğ‘¡) is the number of documents that contain ğ‘¡, ğ‘‘ is the document length, avgdl is the average document length, and ğ‘˜1 = 0.9 and ğ‘ = 0.4 following Pyserini [8] defaults. For the QL-Dir seed, we add fourth component, the collection language model, to the evolvable interface. The initial program implements: QL-Dir(ğ‘, ğ‘‘) = ğ‘¡ ğ‘ log (cid:18) tf(ğ‘¡, ğ‘‘) + ğœ‡ ğ‘ƒ (ğ‘¡ ğ¶) ğ‘‘ + ğœ‡ (cid:19) , (3) where ğ‘ƒ (ğ‘¡ ğ¶) = tf(ğ‘¡, ğ¶)/ğ¶ is the collection language model probability of term ğ‘¡, ğ¶ is the corpus, and ğœ‡ = 2000 following Pyserini. The system prompt guides how to evolve the seed program. It details the design principles that encourage exploration of information-theoretic, probabilistic, and fundamentally novel ideas while discouraging ad-hoc constraints without justification. It also specifies the optimization objective, metrics, datasets, and evolvable versus restricted components. Together, these elements aim to maximize the LLMs freedom while ensuring every candidate remains valid, executable retrieval system."
        },
        {
            "title": "3.4 Evaluator\nThe evaluator imports a candidate program, executes its full pipeline\n(tokenization, indexing, and retrieval) on a set of evaluation datasets,\nand returns per-dataset nDCG@10, Recall@100, and latency mea-\nsurements, all of which are stored alongside the program in the\npopulation database. For population management, sampling pri-\nority, and as the optimization target, we use a single fitness score:\n0.8 Ã— Avg Recall@100 + 0.2 Ã— Avg nDCG@10, where the averages",
            "content": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conference17, July 2017, Washington, DC, USA Table 1: Macro-averaged results on unseen BRIGHT (6), BEIR (8), and TREC DL 19/20 datasets. Best per-group scores bolded. Evolved with RankEvolve. Significant over the respective seed (per-query paired ğ‘¡-test, ğ‘ < 0.05). BRIGHT BEIR TREC DL Method nDCG@10 R@100 nDCG@10 R@100 nDCG@ R@100 BM25 [17] BM25+ [12] BM25-adpt [11] BM25 QL-Dir [23] QL-JM [23] QL-Dir 10.55 9.72 11.05 11.79 9.52 10.17 11.42 32.11 31.54 34.67 37.51 32.48 31.08 36.33 48.16 45.71 44.62 47.90 44.15 40.61 46.46 70.95 69.93 69.55 72.43 68.72 65.79 70.22 62.16 61.07 60.90 64.57 57.03 56.65 60.68 46.02 46.69 48.23 47.10 46.69 41.05 47.96 are taken across datasets. The weighting reflects the first-stage retrieval setting. The primary objective is to maximize recall of relevant documents for downstream reranker, while nDCG@10 serves as secondary signal for ranking quality of the retrieved set."
        },
        {
            "title": "4.3 Results\nTable 1 presents the macro-averaged results for each benchmark\non the 16 held-out datasets. BM25â˜… outperforms all BM25 base-\nlines on BRIGHT and BEIR Recall@100 and achieves the highest\nnDCG@10 on TREC DL. QL-Dirâ˜… consistently outperforms both\nQL-Dir and QL-JM across all three benchmarks. Gains are statis-\ntically significant over the respective seeds on most evaluation\ngroups, and extend to datasets not seen during evolution, indi-\ncating that RankEvolve discovers better retrieval algorithms that\ngeneralize well rather than overfitting to the evaluator signal.",
            "content": "Figure 2 shows the Recall@100 and nDCG@10 trajectories across both evolutionary runs. Recall@100 improves nearly monotonically, while nDCG@10 occasionally regresses. This is expected: the optimization target weights Recall (0.8) far more heavily than nDCG (0.2), so RankEvolve will accept any mutation that trades small nDCG loss for larger Recall gain. The pattern is visible in both seeds, with nDCG dipping at exactly the steps where Recall makes its largest jumps. The combined score 1 is monotonically increasing throughout optimization. When we optimize weighted sum, we implicitly authorize the optimizer to sacrifice the pawn to advance the queen. RankEvolve simply makes this trade visible. To understand what RankEvolve discovered, we now analyze the best-evolved programs in detail."
        },
        {
            "title": "4.4 The Evolved BM25 Algorithm\nAfter 293 evolution steps, the best evolved algorithm converges to\na multi-channel, modulated scoring function that operates entirely\nover lexical features, yet has substantially departed from BM25 in\nstructure. The top-level scoring function is:",
            "content": "ğ‘† (ğ‘, ğ‘‘) = ğ‘…(ğ‘base, ğ‘‘ base) + ğ‘¤pfx ğ‘…(ğ‘pfx, ğ‘‘ pfx) + ğ‘¤bi ğ‘…(ğ‘bi, ğ‘‘ bi) + ğ‘¤mic ğº (ğ‘) ğ‘…(ğ‘mic, ğ‘‘ mic), (4) where ğ‘… is shared core scoring function applied across four parallel token spaces. Base tokenization uses the standard Lucene tokenizer. Prefix tokenization (ğ‘¤pfx = 0.1) truncates each token to its first 5 characters, acting as cheap stemming approximation. Bigram tokenization (ğ‘¤bi = 0.08) concatenates consecutive token pairs. Micro tokenization (ğ‘¤mic = 0.12) uses rolling character 3-grams for sub-word matching, gated by ğº (ğ‘) = ğœ ((IDF(ğ‘) 2.2)/1.0), sigmoid of the querys mean IDF that activates sub-word matching only for rare or technical queries. Conference17, July 2017, Washington, DC, USA Nian et al. Figure 2: Evolution trajectories for the BM25 seed (left) and Dirichlet seed (right). Recall@100 improves nearly monotonically in both runs, while nDCG@10 occasionally regresses at the same steps, reflecting deliberate trades made by the evolutionary process to maximize the optimization target. The shared scoring function ğ‘… combines base evidence term with chain of bounded multipliers: ğ‘…(ğ‘, ğ‘‘) = ln(1 + ğ¸) ğµcov ğµspec ğµcoord ğµanc ğµlen , (5) where ğ¸ = (cid:205)ğ‘¡ ğ‘€ ğ‘¤ (ğ‘¡) ln(1 + tf(ğ‘¡, ğ‘‘)) accumulates weighted log-TF evidence over matched query terms ğ‘€, and ğµ are multipliers of the form 1 +ğ›¾ () with small ğ›¾ that modulate the score based on queryterm coverage, topical specificity, term coordination, rare-term anchoring, and document length. Their individual effects are gentle, but their combined effect can be substantial. The outer ln(1 + ğ¸) applies second layer of saturation, making the scoring robust to outlier term frequencies through double log-compression. Full definitions of all components appear in Appendix A. We highlight three aspects of the evolved design that are especially notable. First, the composite term weight ğ‘¤ (ğ‘¡) multiplies three separate functions of IDF(ğ‘¡), which together suppress stopwordlike terms (low IDF) while leaving rare terms nearly unaffected. RankEvolve has learned soft stopword filter without ever being told about stopwords. Second, the specificity multiplier ğµspec uses pointwise mutual information between each query term and the candidate document to reward documents where matched terms appear with higher-than-expected frequency, an idea closely related to the collection language model in probabilistic retrieval. Third, the length dampener ğµlen = 1 + 0.15 ln(1 + (ğ‘‘ + 1)/(avgdl + 1)) replaces BM25s linear normalization with gentler logarithmic form, aligning with findings from Lv and Zhai [12] that BM25 overpenalizes long documents. RankEvolve arrived at all of these components through evolutionary search alone, without any explicit guidance toward them. The prefix channel approximates stemming, the multi-layered IDF weighting acts as soft stopword filter, the PMI-based multiplier (ğµspec) resembles collection language model, and the logarithmic length normalization independently addresses known BM25 weakness. To the best of our knowledge, no exact formulation in the existing literature matches the evolved scoring function, yet these well-studied concepts emerged naturally from purely metricdriven search process. This suggests that such ideas may have been abundant enough in the LLMs training data to leave strong impression in its parametric knowledge, or that they are perhaps inevitable solutions to the lexical retrieval problem, or both."
        },
        {
            "title": "4.5 The Evolved Query Likelihood Algorithm\nAfter 182 evolution steps starting from classical query likelihood\nwith Dirichlet smoothing, the best evolved algorithm retains the\nprobabilistic language-modeling foundation but departs substan-\ntially from the standard formulation. The top-level scoring function\nis:",
            "content": "ğ‘† (ğ‘, ğ‘‘) = ğ‘¡ ğ‘ğ‘¢ ğœ” (ğ‘¡) ğ‘  (ğ‘¡, ğ‘‘) + ğ‘¡ ğ‘ğ‘¢ ğ‘š(ğ‘¡, ğ‘‘) + AND(ğ‘, ğ‘‘) + LP(ğ‘‘), (6) where ğ‘ğ‘¢ is the set of unique query terms in the vocabulary. The score decomposes into four additive components: weighted sum of per-term relevance scores ğ‘ , missing-term penalty ğ‘š, softAND coverage bonus, and log-normal document length prior. Unlike the evolved BM25, which restructures scoring into product of bounded multipliers (Equation 5), the evolved query likelihood algorithm retains an additive architecture, but augments it with coordination and penalty mechanisms that standard query likelihood lacks entirely. Full definitions of all components appear in Appendix B. We highlight four aspects that represent the most significant departures from the seed. First, RankEvolve replaces the standard collection language model ğ‘ƒ (ğ‘¡ ğ¶) with three-stage enriched estimate. The raw collection probabilities are raised to power ğœ = 0.85 and renormalized (flattening the distribution to give more mass to rare terms), then interpolated with document-frequency language model ğ‘ƒdf (ğ‘¡) = df(ğ‘¡)/ğ‘ that is robust to bursty documents, and finally mixed with small uniform component as safety floor. RankEvolve has independently discovered that flattening the collection language model improves retrieval, related to the information-based retrieval models of Clinchant and Gaussier [1]. Second, raw term frequency is replaced by tf(ğ‘¡, ğ‘‘)ğ›½ (ğ‘¡ ) , where the exponent ğ›½ (ğ‘¡) [0.70, 1.0] is function of normalized IDF. Common terms saturate aggressively (ğ›½ 0.70), while rare terms RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conference17, July 2017, Washington, DC, USA Table 2: Ablation on code structure design (BM25 family). Each row reports the best program found by RankEvolve under given level of structural freedom; evolve step and total search budget are shown in parentheses. The optimization target is 0.8 R@100 + 0.2 nDCG@10, macro-averaged over the 12 datasets used during evolution; unseen evaluation covers 16 held-out datasets (see Section 4.2). Code Structure nDCG@10 R@100 nDCG@10 R@100 nDCG@10 R@100 Seen (12 datasets) Unseen (16 datasets) BRIGHT BEIR TREC DL Optimization Target Original (step 0) Constrained (step 115/200) Composable (step 185/200) Freeform (step 177/200) Freeform (step 293/300) 10.55 11.36 11.97 11.39 11.79 32.11 34.68 37.27 36.92 37. 48.16 48.27 46.95 46.87 47.90 70.95 70.75 70.30 71.27 72.43 62.16 61.86 63.07 61.17 64.57 46.02 45.44 48.62 45.40 47.10 40.30 42.87 43.31 44.35 44.58 49.77 50.47 51.33 51.20 52. preserve the full TF signal (ğ›½ 1.0). Standard Dirichlet smoothing applies no TF saturation; BM25 applies uniform saturation; the evolved BM25 also uses uniform log-saturation. The per-term adaptive exponent is strictly more expressive than all of them. Third, the evolved algorithm applies leaky rectifier to per-term scores. Where standard implementations discard negative per-term contributions, the evolved function retains them at 12% strength. Together with separate missing-term penalty that applies the Dirichlet smoothing-only score (scaled to 7%) for completely absent query terms, this creates layered penalty architecture at two granularities: per-term weakness and per-term absence. The small magnitudes suggest the evolutionary process found that recall is fragile and penalties must be conservative. Fourth, the document length prior LP(ğ‘‘) = 0.06 (log(ğ‘‘ ) log(avgdl))2 penalizes deviation in both directions from the corpus average on log scale. This contrasts with the evolved BM25s length dampener, which only penalizes long documents. The quadratic form reflects the insight that very short documents are also problematic."
        },
        {
            "title": "5 Ablation Study",
            "content": "Effect of seed structure on search space and convergence. We study how the structural freedom of the seed program shapes evolution. We design three functionally identical BM25 seeds with increasing degrees of freedom, and evolve each using the procedure from Section 3. The constrained seed fixes the BM25 formula and permits only hyperparameter tuning and selection among predefined component variants (e.g., Lucene vs. Robertson IDF). This setting is analogous to automated grid search. The composable seed decomposes retrieval into modular primitives whose formulas may be rewritten or extended, but keeps the overall pipeline structure fixed. The freeform seed, used in our main experiments, defines only query representation, document representation, and scoring function. It fixes the evaluator interface and leaves everything else evolvable. Table 2 shows clear trend: greater structural freedom yields monotonically higher optimization target scores on both the 12 datasets used by the evaluator and the 16 held-out datasets. Constrained evolution converges earliest but produces the smallest gains, confirming that parameter tuning alone has limited potential. The composable variant improves further by introducing novel scoring primitives, yet its fixed pipeline prevents deeper architectural changes. The freeform variant converges last but achieves the best scores on both seen and unseen datasets, demonstrating again that the improvements generalize rather than overfit to the development suite. These results suggest that seed program design sets an upper bound on what RankEvolve can discover. Restrictive seeds bias the search towards local optima near the original formulation, whereas seeds with more structural freedom expand the search space and allow evolution to identify non-obvious improvements. Complementary strengths across structures. Beyond the aggregate trend, Table 2 reveals that the best programs evolved from differently structured seeds exhibit complementary per-benchmark strengths. The freeform variant (step 293/300) achieves the highest scores on BRIGHT Recall@100, BEIR Recall@100, and TREC DL nDCG@10, yet it is not uniformly dominant: its TREC DL Recall@100 (47.10) lags behind the composable variant (48.62); its BEIR nDCG@10 (47.90) falls short of the constrained variant (48.27); and notably also under-perform the original BM25 baseline (48.16). These per-metric differences are not random noise. They reflect the distinct inductive biases each seed structure imposes on the Conference17, July 2017, Washington, DC, USA Nian et al. Table 3: Average per-document indexing latency and perquery retrieval latency across all 28 datasets. Lowest values are bolded, second lowest are underlined. Evolved with RankEvolve. Method Indexing (ms/doc) Query (ms/query) BM25 BM25 Constrained BM25 Composable BM25 Freeform (step 177) BM25 Freeform (step 293) QL-Dir QL-JM QL-Dir Freeform (step 182) 1.79 1.85 1.77 2.37 2.81 2.02 1.95 2.02 56.72 58.50 111.49 171.52 648.89 178.26 212.24 325.41 search trajectory. The constrained seed, limited to selecting among predefined component variants and tuning their parameters, preserves the original BM25 formula almost intact. This conservatism prevents it from discovering novel scoring primitives, but it also shields against degradations on benchmarks where the classical formulation is almost near-optimal. The composable seed allows evolution to rewrite individual scoring primitives while keeping the pipeline skeleton fixed. This additional freedom lets it discover recall-oriented modifications (e.g., aggressive document-length normalization and alternative term-frequency saturation curves) that boost TREC DL Recall@100beyond what either the constrained or freeform variants achieve. The freeform seed removes even the pipeline constraint, enabling evolution to restructure the scoring architecture itself. This produces the largest aggregate gains but occasionally trades precision on narrow evaluation slices for broader improvements across the board. Influence on the discovery process. The fact that no single structural configuration dominates on every metric underscores key insight: RankEvolves value lies not only in the single best program it returns, but also in the diverse family of high-performing programs it is able to discover. Each seed structure guides evolution through different region of program space, surfacing solutions that emphasize different trade-offs between nDCG and Recall, or between robustness on short web queries (TREC DL) and complex reasoning-intensive queries (BRIGHT). In practice, one could run RankEvolve from multiple seed structures and select or ensemble the best-performing variant per deployment scenario, treating the evolutionary trajectories as form of structured exploration rather than single-objective optimization. Despite these per-benchmark variations, the freeform seed remains the strongest choice on average: it achieves the highest optimization target on both the 12 seen datasets and the 16 unseen datasets, confirming that maximal structural freedom provides the best riskaward trade-off when single general-purpose retrieval function is needed."
        },
        {
            "title": "6 Latency\nTable 3 reports per-document indexing and per-query retrieval\nlatency averaged across all 28 datasets. Indexing overhead is neg-\nligible across all variants. Query latency increases with program\ncomplexity: the best performing BM25â˜… (step 293) is roughly 11Ã—\nslower than the seed BM25. This is entirely expected. While la-\ntency statistics are visible to the LLM during evolution, they are\nnever part of the optimization target nor explicitly mentioned to\nbe mindful of in the system prompt, so the search process has no\npressure to favor efficient solutions. Incorporating latency as an\nexplicit optimization objective is a straightforward extension that\nwe leave to future work.",
            "content": "Despite this, the latency profile offers useful lens into how RankEvolve discovers improvements at different levels of structural freedom. The constrained variant adds virtually no overhead (58.50 vs. 56.72 ms/query), confirming that its gains stems almost entirely from parameter tuning rather than algorithmic breakthrough. Yet even this minimal-complexity evolution meaningfully improves the optimization target  (Table 2)  , showing that the simplest form of RankEvolve, which is effectively an automated parameter search, already provides value. Among the freeform variants, the trajectory from step 177 to step 293 is very interesting. At step 177 the program had already achieved strong effectiveness  (Table 2)  , with query latency still within modest 3 of the baseline. The following 116 steps of evolution continued to improve recall and nDCG, but at the cost of 3.8 further increase in query latency, indicating that later evolutionary steps exploit increasingly complex scoring mechanisms whose marginal effectiveness gains carry disproportionate computational cost. This mirrors common pattern in program synthesis: early mutations tend to explore high-return structural changes, while later mutations add refinements that are effective but expensive."
        },
        {
            "title": "7 Conclusion and Future Work\nWe introduce RankEvolve, a framework that applies LLM-guided\nprogram evolution to discover lexical retrieval algorithms. Evolved\nfrom BM25 and QL-Dir seeds, the resulting functions consistently\noutperform their seeds and established variants on held-out bench-\nmark datasets. The evolved programs independently rediscover\nand reformulate well-studied IR concepts. Our ablation confirms\nthat the structural freedom of the seed program determines the\nceiling of what RankEvolve can discover. The evolved algorithms,\nwhile effective, are much more complex than their seeds. Defin-\ning and encouraging elegance is a natural next step. More broadly,\nRankEvolve optimizes whatever objective the evaluator defines,\nwhich suggests the framework could extend beyond lexical retrieval\nto dense retrieval, learned sparse representations, and even LLM\nreranking methods. A straightforward extension is to incorporate\nefficiency constraints as an explicit optimization objective. We hope\nRankEvolve motivates further exploration of LLM-guided program\nevolution as a tool for automated IR research.",
            "content": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conference17, July 2017, Washington, DC, USA Yoon, Sercan Ã–. Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=ykuc5q381b [20] Reiko Tanese. 1989. Distributed genetic algorithms for function optimization. Ph. D. Dissertation. University of Michigan, USA. https://hdl.handle.net/2027.42/162372 [21] Michael Taylor, Hugo Zaragoza, Nick Craswell, Stephen Robertson, and Chris Burges. 2006. Optimisation methods for ranking functions with multiple parameters. In Proceedings of the 15th ACM international conference on Information and knowledge management. 585593. [22] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ 65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html [23] Chengxiang Zhai and John Lafferty. 2004. study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS) 22, 2 (2004), 179214. References [1] StÃ©phane Clinchant and Eric Gaussier. 2010. Information-based models for ad hoc IR. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Geneva, Switzerland) (SIGIR 10). Association for Computing Machinery, New York, NY, USA, 234241. doi:10. 1145/1835449.1835490 [2] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview of the TREC 2020 Deep Learning Track. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC 2020, Virtual Event [Gaithersburg, Maryland, USA], November 16-20, 2020 (NIST Special Publication, Vol. 1266), Ellen M. Voorhees and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec29/papers/OVERVIEW.DL.pdf [3] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen Voorhees, and Ian Soboroff. 2025. Overview of the TREC 2022 deep learning track. arXiv preprint arXiv:2507.10865 (2025). [4] Ronan Cummins and Colm ORiordan. 2006. Evolving local and global weighting Inf. Retr. 9, 3 (2006), 311330. doi:10.1007/ schemes in information retrieval. S10791-006-1682-6 [5] Weiguo Fan, Michael D. Gordon, and Praveen Pathak. 2004. generic ranking function discovery framework by genetic programming for information retrieval. Inf. Process. Manag. 40, 4 (2004), 587602. doi:10.1016/J.IPM.2003.08. [6] Mathias GÃ©ry and Christine Largeron. 2012. BM25t: BM25 extension for focused information retrieval. Knowl. Inf. Syst. 32, 1 (2012), 217241. doi:10.1007/S10115011-0426-0 [7] Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. 2025. ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution. arXiv:2509.19349 [cs.CL] https://arxiv.org/abs/2509.19349 [8] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. In SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 23562362. doi:10.1145/3404835.3463238 [9] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. 2024. Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 (Proceedings of Machine Learning Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine A. Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR / OpenReview.net, 3220132223. https://proceedings.mlr.press/v235/liu24bs.html [10] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval. Springer. doi:10. 1007/978-3-642-14267-3 [11] Yuanhua Lv and ChengXiang Zhai. 2011. Adaptive term frequency normalization for BM25. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (Glasgow, Scotland, UK) (CIKM 11). Association for Computing Machinery, New York, NY, USA, 19851988. doi:10.1145/2063576. 2063871 [12] Yuanhua Lv and ChengXiang Zhai. 2011. Lower-bounding term frequency normalization. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (Glasgow, Scotland, UK) (CIKM 11). Association for Computing Machinery, New York, NY, USA, 716. doi:10.1145/2063576.2063584 [13] Yuanhua Lv and ChengXiang Zhai. 2011. When documents are very long, BM25 fails!. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (Beijing, China) (SIGIR 11). Association for Computing Machinery, New York, NY, USA, 11031104. doi:10.1145/2009916. 2010070 [14] Jean-Baptiste Mouret and Jeff Clune. 2015. Illuminating search spaces by mapping elites. CoRR abs/1504.04909 (2015). arXiv:1504.04909 http://arxiv.org/abs/1504. 04909 [15] Alexander Novikov, NgÃ¢n u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog. 2025. AlphaEvolve: coding agent for scientific and algorithmic discovery. arXiv:2506.13131 [cs.AI] https://arxiv.org/abs/2506.13131 [16] Jay M. Ponte and W. Bruce Croft. 2017. Language Modeling Approach to Information Retrieval. SIGIR Forum 51, 2 (2017), 202208. doi:10.1145/3130348. [17] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (2009), 333389. doi:10.1561/1500000019 [18] Asankhaya Sharma. 2025. OpenEvolve: an open-source evolutionary coding agent. https://github.com/codelion/openevolve [19] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Conference17, July 2017, Washington, DC, USA Nian et al. Full Evolved BM25 Scoring Function This appendix provides the complete definitions of all components in the evolved BM25 algorithm described in Section 4.4. The toplevel scoring function (Equation 4) applies shared core function ğ‘… across four parallel token spaces, and the core function ğ‘… (Equation 5) combines base evidence with chain of bounded multipliers. Term weight. The term weight ğ‘¤ (ğ‘¡) determines how much each query term can contribute to the evidence term ğ¸: ğ‘¤ (ğ‘¡) = qtf(ğ‘¡, ğ‘)0.5 IDF(ğ‘¡) (cid:19) 0.6 (cid:18) IDF(ğ‘¡) IDF(ğ‘¡) + 1 IDF(ğ‘¡) IDF(ğ‘¡) + 1.25 where qtf(ğ‘¡, ğ‘) is the query term frequency, and IDF(ğ‘¡) = ln (cid:18) df(ğ‘¡) + 1 ğ‘ + (cid:19) , (7) (8) which is closer to the traditional TF-IDF formulation than to BM25s IDF. The first factor qtf(ğ‘¡, ğ‘)0.5 gives diminishing credit to repeated query terms. The remaining three factors are all functions of IDF. Acting together, they behave like stopword removal filter: stopword-like term with IDF 1 is suppressed by all three factors and ends up with very small weight, whereas rare term with IDF 8 passes through nearly unaffected. Coverage multiplier. The coverage multiplier rewards breadth of match: ğ‘Šğ‘€ ğ‘Š , ğµcov = 1 + 0.25 (9) where ğ‘Šğ‘€ = (cid:205)ğ‘¡ ğ‘€ ğ‘¤ (ğ‘¡) is the total weight of matched terms and ğ‘Š = (cid:205)ğ‘¡ ğ‘ ğ‘¤ (ğ‘¡) is the total weight of all query terms. The ratio ğ‘Šğ‘€ /ğ‘Š [0, 1] measures what fraction of the querys importance the document satisfies. document matching all query terms receives 1.25 boost. BM25 has no explicit equivalent; it relies solely on additive accumulation to implicitly reward multi-term matches. Specificity multiplier. The specificity multiplier rewards documents where matched terms appear with higher-than-expected frequency: ğµspec = 1 + 0.10 (cid:205)ğ‘¡ ğ‘€ + ğ‘¤ (ğ‘¡) min(PMI(ğ‘¡, ğ‘‘), 3.0) ğ‘Š where ğ‘€ + = {ğ‘¡ ğ‘€ : PMI(ğ‘¡, ğ‘‘) > 0} and , PMI(ğ‘¡, ğ‘‘) = ln (cid:18) tf(ğ‘¡, ğ‘‘) ğ‘ max(ğ‘‘ , 25) df(ğ‘¡) (cid:19) (10) (11) is the pointwise mutual information between term ğ‘¡ and document ğ‘‘. positive PMI means the term is over-represented in the document relative to its corpus-wide rate. The cap at 3.0 and the document length floor of 25 prevent short documents from producing extreme values. ğµspec captures how strongly the document focuses on query terms. Coordination multiplier. The coordination multiplier provides second, calibrated signal for multi-term matching: ğµcoord = 1 + 0.20 ğœcoord ğœcoord + ln(1 + ğ‘Š ) ğ‘€ ğ‘ , (12) where ğ‘€ is the number of matched query terms, ğ‘ is the number of unique query terms, and ğœcoord = 2.5. The calibration factor ğœcoord+ln(1+ğ‘Š ) adapts the bonus to query difficulty. For short, rareterm queries,ğ‘Š is small and the calibration factor is close to 1, so the ğœcoord bonus is modest because high per-term evidence already implicitly rewards coordination. For long, common-word queries, ğ‘Š is large and the calibration factor shrinks, preventing double-counting with ğµcov. Anchor multiplier. The anchor multiplier is recall safeguard for rare terms: ğµanc = 1 + 0.14 ln(1 + ğ´) ğ´ = max ğ‘¡ ğ‘€: IDF(ğ‘¡ ) >4.2 IDF(ğ‘¡) 4.2 IDF(ğ‘¡) (13) (14) If document matches even one very rare query term (IDF above 4.2), it receives small boost proportional to that terms rarity. Only the single rarest matched term contributes, keeping the boost bounded. Length dampener. The length dampener replaces BM25s perterm linear normalization with single global logarithmic penalty: ğµlen = 1 + 0.15 ln (cid:18) 1 + (cid:19) ğ‘‘ + 1 avgdl + (15) This is gentler than BM25s 1 ğ‘ + ğ‘ ğ‘‘ /avgdl. document twice the average length is penalized by roughly 10%, compared to up to 40% under BM25 with ğ‘ = 0.4. The logarithmic form means the penalty grows very slowly for extremely long documents, which benefits heterogeneous corpora with high length variance."
        },
        {
            "title": "Function",
            "content": "This appendix provides the complete definitions of all components in the evolved query likelihood algorithm described in Section 4.5. Enriched collection language model. The standard collection language model ğ‘ƒ (ğ‘¡ ğ¶) = tfğ¶ (ğ‘¡)/ğ¶ is replaced by three-stage estimate. First, the raw collection probabilities are raised to power ğœ = 0.85 and renormalized: ğ‘ƒğœ (ğ‘¡) = ğ‘ƒ (ğ‘¡ ğ¶)ğœ (cid:205)ğ‘¡ ğ‘‰ ğ‘ƒ (ğ‘¡ ğ¶)ğœ , (16) where ğ‘¡ is dummy variable indicating that the denominator sums across all terms in the vocabulary ğ‘‰ . This flattens the distribution, transferring probability mass from frequent terms to rare ones. Second, the tempered model is interpolated with document-presence language model ğ‘ƒdf (ğ‘¡) = df(ğ‘¡)/ğ‘ : ğ‘ƒmix (ğ‘¡) = 0.90 ğ‘ƒğœ (ğ‘¡) + 0.10 ğ‘ƒdf (ğ‘¡). (17) The document-frequency model counts each document once regardless of internal repetition, providing stabler estimate than the token-frequency model for terms concentrated in bursty documents. Third, small uniform component is mixed in to ensure no term has zero background probability: ğ‘ƒğ¶ (ğ‘¡) = 0.97 ğ‘ƒmix (ğ‘¡) + 0.03 ğ‘‰ . (18) Adaptive term-frequency saturation. Raw term frequency is replaced by tfeff (ğ‘¡, ğ‘‘) = tf(ğ‘¡, ğ‘‘) ğ›½ (ğ‘¡ ) , where ğ›½ (ğ‘¡) = 1 0.30 (1 IDF01 (ğ‘¡)) (19) RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conference17, July 2017, Washington, DC, USA Soft-AND coverage bonus. bonus rewards documents matching (20) more distinct query terms: AND(ğ‘, ğ‘‘) = 0.14 1 ğ‘ğ‘¢ ğ‘¡ ğ‘ğ‘¢ tanh ğœ” (ğ‘¡) max( ğ‘  (ğ‘¡, ğ‘‘), 0) 3.0 . (27) The tanh approximates binary this term was matched well signal. Summing and normalizing by ğ‘ğ‘¢ gives coverage fraction in [0, 1]. The tanh design is robust to the scale of per-term scores, so the coverage signal reflects purely breadth, not depth. Document length prior. Gaussian prior on log-length penalizes deviation from the corpus average in both directions: LP(ğ‘‘) = 0.06 (cid:0) log(ğ‘‘ ) log(avgdl))2. document 10 the average length receives penalty of approximately 0.32, nudge rather than decisive factor. (28) IDF01 (ğ‘¡) = log (cid:16) ğ‘ +1 df(ğ‘¡ )+1 (cid:17) maxğ‘¡ ğ‘‰ log (cid:16) ğ‘ +1 df(ğ‘¡ )+1 (cid:17) . The normalized IDF IDF01 (ğ‘¡) [0, 1] maps each term to its relative rarity. For common terms (IDF01 0), ğ›½ (ğ‘¡) 0.70 and term-frequency saturates aggressively. For rare terms (IDF01 1), ğ›½ (ğ‘¡) 1.0 and the full term-frequency signal is preserved. Gated Dirichlet log-likelihood. The core per-term relevance score is: ğ‘ base (ğ‘¡, ğ‘‘) = ğ‘”(ğ‘¡) log(cid:169) (cid:173) (cid:171) 1 + tf(ğ‘¡,ğ‘‘ ) ğ›½ (ğ‘¡ ) ğœ‡ğ‘ƒğ¶ (ğ‘¡ ) ğ‘‘ +ğœ‡ ğœ‡ (cid:170) (cid:174) (cid:172) , (21) with ğœ‡ = 1750, lower than the standard default of ğœ‡ = 2000 because the tempered background model already provides more uniform smoothing. The gate ğ‘”(ğ‘¡) is the Entity Dispersion Ratio (EDR): ğ‘”(ğ‘¡) = 1 + 0.45 clip (cid:18) log (cid:19) (cid:18) ğ‘ƒdf (ğ‘¡) ğ‘ƒğ¶ (ğ‘¡) , 2.5, 2.5 (cid:19) . (22) The ratio ğ‘ƒdf (ğ‘¡)/ğ‘ƒğ¶ (ğ‘¡) measures whether terms document spread matches its token frequency. Terms that appear across many documents but with low total frequency (high ratio) are upweighted as independently informative. Terms with high token frequency concentrated in few documents (low ratio) are downweighted. The clipping keeps the gate bounded in [0.125, 2.125]. Query term weight. Each query term receives composite weight: ğœ” (ğ‘¡) = (cid:0)qtf(ğ‘¡) ğ‘Ÿ (ğ‘¡)(cid:1) 0.6 (23) ğ‘Ÿ (ğ‘¡) = 1 + 0.9 (cid:16) log clip (cid:17) (cid:16) ğ‘ƒdf (ğ‘¡ ) ğ‘ƒğ¶ (ğ‘¡ ) 2.5 (cid:17) , 0, 2. (24) The residual weight ğ‘Ÿ (ğ‘¡) [1.0, 1.9] boosts query terms that are spread broadly rather than concentrated in few documents. The outer power 0.6 applies sub-linear damping to repeated query terms, preventing verbose or repetitive queries from over-counting. Leaky rectifier. Per-term scores are passed through leaky rectifier: ğ‘  (ğ‘¡, ğ‘‘) = (cid:40)ğ‘ base (ğ‘¡, ğ‘‘) 0.12 ğ‘ base (ğ‘¡, ğ‘‘) if ğ‘ base (ğ‘¡, ğ‘‘) 0, otherwise. (25) Negative evidence is retained at 12% of its full strength, enough to break ties and mildly penalize documents that partially match but are weak on specific terms. Missing-term penalty. An explicit penalty applies for each query term that is completely absent from the document: ğ‘š(ğ‘¡, ğ‘‘) = 1[tf(ğ‘¡, ğ‘‘) = 0] 0.07 ğœ” (ğ‘¡) log (cid:18) ğœ‡ ğ‘ƒğ¶ (ğ‘¡) ğ‘‘ + ğœ‡ (cid:19) . (26) This is precisely the Dirichlet log-likelihood score evaluated at tf = 0, scaled down by 0.07. Together with the leaky rectifier, this creates layered penalty architecture at two granularities (per-term weakness and per-term absence)."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Santa Clara University",
        "Walmart Global Tech"
    ]
}