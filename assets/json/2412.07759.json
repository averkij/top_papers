{
    "paper_title": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation",
    "authors": [
        "Xiao Fu",
        "Xian Liu",
        "Xintao Wang",
        "Sida Peng",
        "Menghan Xia",
        "Xiaoyu Shi",
        "Ziyang Yuan",
        "Pengfei Wan",
        "Di Zhang",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 9 5 7 7 0 . 2 1 4 2 : r 3DTRAJMASTER: MASTERING 3D TRAJECTORY FOR MULTI-ENTITY MOTION IN VIDEO GENERATION Xiao Fu1 Xian Liu1 Xintao Wang2(cid:66) Sida Peng3 Menghan Xia2 Xiaoyu Shi2 Ziyang Yuan2 Pengfei Wan2 Di Zhang2 Dahua Lin1(cid:66) 1The Chinese University of Hong Kong 2Kuaishou Technology 3Zhejiang University"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster."
        },
        {
            "title": "INTRODUCTION",
            "content": "Controllable video generation (Brooks et al., 2024; Guo et al., 2023b; Chen et al., 2023) aims to synthesize high-fidelity videos that are controlled by user inputs, such as text prompts, sketches, or bounding boxes. critical objective in controllable video generation is the precise manipulation of object motions within videos, which is essential for simulating the dynamic world and potentially aids video generative models in understanding the underlying physics of the world. In addition, it can unleash many applications of video generative models, such as virtual cinematography for the film industry, acting as interactive games, and providing world models for embodied AI systems. Recently, there has been some methods attempting to manipulate object motions in video generation by introducing 2D control signals, such as 2D sketches (Wang et al., 2024b; Guo et al., 2023a), bounding boxes (Yang et al., 2024; Wang et al., 2024a), and points (Wang et al., 2024c; Zhang et al., 2024). These methods offer convenient user interactions and have delivered impressive video generation results. However, we argue that 2D control signals cannot fully express the inherent 3D nature of motion, which limits the control capability of object motions. As real-world objects move in 3D space, some motion properties can only be described through 3D representations. For example, the rotation of an object can be succinctly described using three parameters in 3D, and occlusions between objects can be simply represented using z-buffering. In contrast, it is quite difficult for 2D control signals to represent these concepts. In this paper, we focus on the problem of controlling multi-object 3D motions in video generative models, aiming to simulate the authentic dynamics of objects in 3D space. This setting is : Work done during an internship at KwaiVGI, Kuaishou Technology. (cid:66): Corresponding Authors. 1 Figure 1: 3DTrajMaster controls one or multiple entity motions in 3D space with input entityspecific 3D trajectories for text-to-video (T2V) generation. It allows diverse entity categories (human, animal, car, robot, natural force, etc) and flexible edits on entity descriptions (see more in Fig. S11). The text prompt is {Entity 1},..., and {Entity N} is/are moving in the {Location}. (We kindly urge readers to check more generalizable results (200) in our website) more aligned with the requirements of downstream applications, such as emulating realistic human motions in movies or exploring 3D virtual scenes in games. However, this problem is extremely challenging. There are three core questions we need to answer: 1) How to precisely represent the 3D motions of objects; 2) How to correlate multiple object descriptions with their respective motion 2 sequences in video generative models; 3) How to maintain the generalization capability of video models after injecting 3D motion information. To address these, we propose novel approach, 3DTrajMaster, which is able to manipulate multientity motions in 3D space for video generation by leveraging entity-specific 6DoF pose sequences as additional inputs. The core of our model is plug-and-play 3D-motion grounded object injector, which associates each entity with their corresponding pose sequences, and then injects these conditions into the foundation model, to control the entity motion. Specifically, the entities and trajectories are projected into latent embeddings via frozen text encoder and learnable pose encoder, respectively. These two modality embeddings are then entity-wise added to form correspondences, which are further fed into gated self-attention layer for motion fusion. This plug-and-play architecture preserves the video models prior and can generalize on more diverse entities and 3D trajectories. However, another challenge in training our model lies in data availability. Existing video datasets face two key limitations: 1) Low entity diversity: Datasets with paired entities and 3D trajectories are mostly limited to humans and autonomous vehicles, with inconsistent spatial distributions and overcrowded entities. 2) Inaccurate/Failed pose estimation: Current 6D pose estimation methods focus on rigid objects, while non-rigid objects, such as animals, are underrepresented, with only human poses studied using SMPL (Loper et al., 2023). To this end, we choose to construct custom dataset, termed 360-Motion Dataset, with unified trajectory distribution using advanced UE rendering techniques. We start by collecting 3D assets of humans and animals and rescaling them to unified cubic space. GPT (Achiam et al., 2023) is then employed to generate 3D trajectory templates for these assets. Various entities and trajectory templates are arranged and combined to create diverse motions. These globally animated assets are captured using 12 evenly positioned cameras within the collected 3D scenes, including city (MatrixCity (Li et al., 2023a)), desert, forest, and HDRIs (projected into 3D space)1. To prevent video domain shift in our constructed dataset, we introduce two key components: 1) video domain adaptor, which is trained to fit data distribution and slightly reduced during inference. 2) An annealed sampling strategy, where trajectories are injected to guide general motion in the early steps and drop out in the later stages. We evaluate our 3DTrajMaster in the curated novel pose sequences with GPT-generated entity prompts, obtaining significant lead over current SOTAs. In summary, our contributions are: 1) We are the first to customize 6 degrees of freedom (DoF) multi-entity motion in 3D space for controllable video generation, establishing new benchmark for fine-grained motion control. 2) We propose 3D-motion grounded video diffusion model that controls multi-entity motions using pose sequences as motion representations. Our flexible object injector enforces entity-wise correspondence between objects and their motions and preserves the video diffusion prior. 3) We introduce scalable 4D motion dataset construction mechanism, and techniques like the video domain adaptor and annealed sampling to enhance video quality while maintaining motion accuracy. 4) 3DTrajMaster achieves state-of-the-art accuracy in controlling 3D entity motions and allows finegrained entity input customization such as changing human hair, clothing, gender, and figure size."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Customizing Video Motion with 2D Guidance. Previous methods predominantly perform motion control on 2D spaces, as this aligns more easily with the input video format. straightforward path is to direct videos based on motion patterns from reference videos (Zhao et al., 2023; Jeong et al., 2024; Ling et al., 2024). However, they require users to provide reference video templates. While training-free paradigms (Yang et al., 2024; Xiao et al., 2024), utilizing attention mechanisms to edit spatial-temporal layouts, can mitigate this issue, they exhibit poor generalization in realworld scenarios and rely heavily on trial-and-error. Further advancements utilize more high-level representations, such as sketches&depths (dense or sparse) (Wang et al., 2024b; Guo et al., 2023a), pose skeletons (Feng et al., 2023; Xu et al., 2024; Chen et al., 2024), bounding boxes (Wang et al., 2024a), and 2D trajectories (Wang et al., 2024c; Zhang et al., 2024; Yin et al., 2023; Yang et al., 2024), to enable more flexible motion generation. Although these methods can model camera, object, or joint movements, the lack of 3D awareness limits precise 3D motion control. 1Poly Haven: https://polyhaven.com/ 3 Learning 3D-aware Motion Synthesis. Considering that video is sequence of images projected from 3D world, manipulating video in 3D space is both more crucial and impactful. key aspect of this manipulation is camera movement. MotionCtrl (Wang et al., 2024c) is the first to regulate video using camera poses (rotation and translation) in 3D space, while CameraCtrl (He et al., 2024) and VD3D (Bahmani et al., 2024b) further enhance camera representation with plucker embeddings (Sitzmann et al., 2021). SynCamMaster (Bai et al., 2024) extends single-camera control to multi-camera synchronization. GameGen-X (Che et al., 2024) can generate game videos with novel WASD keyboard inputs. Other approaches (Hou et al., 2024; Hu et al., 2024a) also explore training-free paradigms. However, none address the customization of object motion in 3D space. Manipulation on 2D maps (Wang et al., 2024c; Zhang et al., 2024) often fails in multi-object scenarios, particularly with 1) aligning each entity and its corresponding motion, 2) handling 3D occlusion. In contrast, 3DTrajMaster is the first to overcome them and simulate plausible 3D motions. 3 3DTRAJMASTER Our goal is to master entity motions in 3D space for text-to-video (T2V) generation by leveraging entity-specific 3D trajectories as additional inputs. To this end, we introduce 3DTrajMaster (see Fig. 2), 3D-motion grounded video diffusion model trained in two stages. First, we describe the video diffusion model and the task formulation (Sec. 3.1). Then, we present our proposed model, whose core is to train plug-and-play 3D grounded object injector to integrate multiple detailed entity descriptions and the respective pose sequences (Sec. 3.2). We further incorporate domain adaptor to mitigate video domain shifts introduced by our constructed training data (Sec. 3.3). Finally, we detail the inference process using annealed sampling to enhance video quality (Sec. 3.4). Figure 2: 3DTrajMaster Framework. Given text prompt consisting of entities {en}N n=1, 3DTrajMaster (a) is able to generate the desired video with entity motions that conform to the input entity-wise pose sequences {Pn}N n=1. Specifically, it involves two training phases. First, it utilizes domain adaptor to mitigate the negative impact of training videos. Then, an object injector module is inserted after the 2D spatial self-attention layer to integrate paired entity prompts and 3D trajectories. (b) Details of the object injection process. The entities are projected into latent embeddings through the text encoder. The paired pose sequences are projected using learnable pose encoder and then fused with entity embeddings to form entity-trajectory correspondences. This condition embedding is concatenated with the video latent and fed into gated self-attention layer for motion fusion. Finally, the modified latent gets back to the remaining layers in the DiT block. 3.1 PRELIMINARIES ON 3D-ENTITY-AWARE VIDEO DISTRIBUTION Video Diffusion Models. Latent text-to-video diffusion model (Ho et al., 2022a;b; Brooks et al., 2024; Chen et al., 2023; Blattmann et al., 2023) learns the conditional distribution p(xc) of encoded video data (x = E(X), E() is VAE encoder) given text description in latent space. In the forward progress, it progressively transits the clean data x0 to the desired Gaussian distribution in Markov 4 chain: {xt, (1, ) xt = αtx0 + σtϵ, ϵ (0, I)}. To iteratively recover the data ˆx0 from the noise ϵ (cid:0)0, σ2 I(cid:1), it learns denoising model ˆϵθ with the objective function: ϵ ˆϵθ (xt; t, c). With the preconditioning strategy (Karras et al., 2022; Salimans & Ho, 2022), it optimizes the neural network ˆFθ by parameterizing the ˆϵθ as: ˆϵθ = cout(σt) ˆFθ (cin(σt)xt; c, σt) + cskip (σt)xt. Task Formulation. Given an input text prompt consisting of entities {en}N n=1 and their paired = [R; T] R34 for -th frame and object orientation and 3D trajectories {Pn}N translation are represented by R33 and R3, respectively, our goal is to generate plausible video RF HW that accords with each entity description and the respective trajectory P. The overall generative formulation () is n=1, where Pf () : L, (en Ln, Pn R34)N (1) n=1)(cid:1), where D(ˆx0) (D() is the VAE decoder), ˆx = (ˆxT ) (cid:81)T is the alphabet, and is the token length. Our primary challenge lies in modeling the distribution pθ or specifically ˆϵθ to generate realistic videos that accurately correspond to the given multiple 3D entity conditions. Here we structure ˆϵθ(x; c, σt, (en, Pn)N n=1) as transformer architecture (Peebles & Xie, 2023) for its superior scalability and performance over U-Net (Ronneberger et al., 2015). (cid:0)ˆxt1 ˆxt, c, (en, Pn)N n=1 RF HW t=1 pθ 3.2 PLUG-AND-PLAY 3D-MOTION GROUNDED OBJECT INJECTOR n=1 are also projected into latent embeddings {ZP Matching Entity-Trajectory Pair. The entity prompts {en}N n=1 are projected into latent embedn RLmaxD, where each emn}N n=1 using frozen text encoder ET() : en Ln Ze dings {Ze bedding Ze is zero-padded to maximum token length Lmax. Correspondingly, the pose sequences {Pn}N n=1 through the trainable pose encoder D. The pose encoder EP consists of linear layer and EP(): Pn RF 12 ZP downsampler along the temporal dimension, resembling the causal encoding applied to video input in 3D VAE, where the mapping function is EX(): RF HW H . Here the downsampler refers to interval sampling of tensors, where we also tried several sequential one-dimensional convolution layers but achieved similar results. Then, the paired entity and trajectory embeddings are expanded and combined through entity-wise addition to form bonded entity-motion correspondence ZPe N LmaxD. }N Gated Self-Attention for Motion Fusion. Inspired by (Li et al., 2023b), we employ gated selfattention layer to handle multiple entity-trajectory pairs ZPe (with varying dimensional embeddings) as input, while further refining the correlated features. Specifically, we replicate the weight of the 2D spatial self-attention layer in each DiT block as initialization to enable grounding. The input video tokens xt and ZPe are passed through this trainable copy via truncated self-attention. The output can be expressed in residue-connection form: xt = xt + β Tc(Att(q, k, v)) (2) = T, = T, = T, = xt ZPe where β is trainable scale, Tc() is the truncation operation to preserve xn tokens, Att() is softmax attention, Q, and are query, key and value embedding matrices, and denotes concatenation. In this stage, we train the θ1 including the pose encoder and the gated self-attention parameters as follow. L(θ1) = x,c,ϵN (0,σ2 I),e,P,t,β (cid:104)(cid:13) (cid:13)ϵ ˆϵθ (cid:0)xt, c, (en, Pn)N n=1), t, β(cid:1)(cid:13) 2 (cid:13) 2 (cid:105) (3) 3.3 ALLEVIATING VIDEO DOMAIN SHIFT FROM CONSTRUCTED TRAINING DATA 360-Motion Dataset. High-quality training data is vital for learning generalizable 3D motion control. straightforward preparation is to extract paired entity descriptions and 6DoF poses from common video datasets. However, it is hard due to twofold: 1) Low diversity/quality entity: Datasets with paired entities and 3D trajectories are mostly limited to humans (Jiang et al., 2024; Araujo et al., 2023) and autonomous vehicles (Geiger et al., 2012; Sun et al., 2020), where the spatial distributions vary between datasets and the entities may be overcrowded. In video datasets like Artgrid, Pixabay, and Pexels2, human category occupies relatively large proportion in 3D/4D asset 2Artgrid: https://artgrid.io/, Pixabay: https://www.videvo.net/, Pexels: https://www.pexels.com/ 5 Figure 3: Dataset Construction Illustration. We correlate (a) collected 3D assets with (b) GPTgenerated 3D trajectories on (c) diverse 3D UE platforms, positioning (d) 12 evenly distributed surrounding cameras to capture the object motions in video format. objectives (refer to Sec. E.2), limiting model generalization to other categories like animals and vehicles. Issues like watermarks in WebVid (Bain et al., 2021) further increase the cost of filtering. 2) Low-accuracy/Failed pose estimation: Most 6D pose estimation methods exclusively focus on rigid objects, and rely on CAD models (Labbe et al., 2022; Wen et al., 2024) or posed multi-view images (Liu et al., 2022; Sun et al., 2022). For non-rigid animated objects, only human poses have been widely studied via methods like SMPL (Loper et al., 2023), limiting the estimation for general 4D objects, such as animals. simpler alternative is to represent only 3D locations via depth models (Hu et al., 2024b; Ke et al., 2024; Fu et al., 2025). However, there exist errors in segmenting the foreground entities from the background and can not generate consistent video metric depth. To circumvent the aforementioned challenges, we opt to construct synthetic dataset, named 360Motion, through Unreal Engine (UE) with advanced rendering technologies (see Fig. 3). We begin by collecting 70 animated 3D assets across two categories: human and animal. Humans are differentiated by attributes such as gender, clothing, body shape, and hairstyle. GPT-4V (OpenAI, 2023) is then used to generate text descriptions en Ln (Ln 20) for each rendered asset image (Fig. 3 (a)). For posed object trajectory templates (Fig. 3 (b)), we follow TC4D (Bahmani et al., 2024a) by leveraging GPT to generate 3D spline (location T) and additional orientation via the gradient calculation on spline. This process yields approximately 96 templates in canonical space, each associated with one to three assets. We additionally reduce the size of the animals by ratio of 0.6 to prevent collisions with other assets. The paired assets and their motion templates are then placed within 55 square meter range in one of the 3D platforms, including city (MatrixCity (Li et al., 2023a)), dessert, forest, and HDRIs (projected into 3D). We position 12 sets of cameras evenly around the scene to capture 360-degree views, producing 100 frames per video clip at 384672 resolution for each camera. This process produces total of 54,000 videos by arranging and combining various objects and trajectories. (see Sec. E.1 and Supp. video samples for illustration) Video Domain Adaptor. Training video diffusion models on this relatively small set of constructed video clips can lead to an undesirable MatrixCity style, limiting the generalization ability. To prevent learning this variation in quality and retain the knowledge of the base T2V, we train LoRA modules (Hu et al., 2021) that serve as video domain adaptor. Specifically, we integrate LoRA into self-attention, cross-attention, and linear layers of the base T2V model, as shown in Fig. 2. The attention/linear projection matrices {Wn}K n=1 are associated with additional trainable lower rank matrices {Wn = αAnBT n=1, where α is the scaler that can be adjusted to control the adaptor influence. During inference, we set α to small value to mitigate the negative impact of synthetic video data. We optimize θ2 = {Wn}K n=1 with the training objective: }K L(θ2) = x,c,ϵN (0,σ2 I),t, (cid:104) ϵ ˆϵθ1 (xt, c, t, α) 2 (cid:105) . (4) Note that the domain adaptor θ2 is frozen when training the object injector θ1. 3."
        },
        {
            "title": "INFERENCE PROCEDURE",
            "content": "We initialize the video latent ˆxT as standard Gaussian noise, and progressively denoise it with the guidance of desired entity-trajectory pairs (en, Pn)N n=1, following the same schedule as the previous two training stages. We apply classifier-free guidance (Ho & Salimans, 2022) and use DDIM (Song et al., 2020) for re-spaced sampling for acceleration. To further enhance the video quality, we employ an annealed sampling strategy (Algorithm 1): During inference in the former steps, trajectories are inserted into the model to define the general object motions, while in the latter stage, they are dropped out, transitioning to the standard T2V generation process. We also observe that setting negative 3D trajectories as static motions {( ˆPn)N n=1 ˆPn = P0, n} can further improve pose accuracy. This phenomenon reflects the models ability to learn 3D motion representations: Since we do not randomly drop out motion sequences during training like text, the model implicitly learns static motion modeling from videos where entities are primarily in motion. Thus when setting static motion as negative motion prompt, we can amplify the magnitude of entity movement, leading to improved pose accuracy during evaluation. However, we do not adopt it as it sometimes results in video quality decline (refer to Sec. F.2.2). Algorithm 1 Annealed conditional sampling with classifier-free guidance (CFG) Require: w: guidance strength, Tc: annealed timestep, α: LoRA modulator, θ: frozen base T2V model, θ1: object injector, θ2: domain adaptor, c: text condition, (e, P): entity-trajectory pairs if Tc then I(cid:1) 1: ˆx1 (cid:0)0, σ2 2: for = 1, ..., do 3: 4: 5: 6: 7: 8: else end if ˆzt = (ˆxt σt ϵt) /αt (cid:16) ˆxt+1 9: 10: end for 11: return ˆxt+1 ϵt = (1 + w)ˆϵ θ,θ1,θ2 (cid:0)ˆxt, c, (en, Pn)N n=1, α(cid:1) wˆϵ θ,θ1,θ2 (ˆxt, α) ˆϵt = (1 + w)ϵ θ (ˆxt, c) wϵ θ (ˆxt) ˆxt+1; µt+1t ( ˆzt, ˆxt) , σ2 t+1tI (cid:17) if < else ˆxt+1 = ˆzt"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS For input text prompts, we use unified template: {Entity 1},..., and {Entity N} are moving in the {Location}. Here we set {Location} based on the respective 3D UE platform. We train 3DTrajMaster based on our internal video diffusion model for research purposes (see Sec. for more details), which contains 1B parameters. The clipped training video and inference video are set to 384 672 resolutions. Each video segment is 5 seconds long. We utilize the Adam optimizer and train on cluster of 8 NVIDIA H800 GPUs, with learning rate of 5 105 and batch size of 8. The training process consisted of 50,000 steps for the domain adaptor and an additional 36,000 steps for the object injector. During inference, we set the DDIM steps as 50 and the CFG as 12.5. 4.2 BASELINES We compare 3DTrajMaster with existing SOTA methods that are capable of customizing object motions: MotionCtrl (Wang et al., 2024c), Direct-a-Video (Yang et al., 2024) and Tora (Zhang et al., 2024). We configure these baseline models using their best performance settings, based on their official open-sourced codebases. 4.3 EVALUATION METRIC 1) Trajectory accuracy: Due to the absence of pose estimator for open-world 4D objects, we limit our evaluation to only human objectives. Specifically, we utilize GVHMR (Shen et al., 2024) to estimate human poses {(Rest n=1 and compare them with the input pose sequences , Test )}F 7 )}F , Tgt {(Rgt n=1. We align the two trajectories at the first frame location. We follow CameraCtrl (He et al., 2024) to estimate the rotation angle error RotErr and translation scale error TransErr, but take the average rather than the sum. 2) Video quality: We leverage standard metrics such as Frechet Video Distance (FVD) (Unterthiner et al., 2018), Frechet Image Distance (FID) (Seitzer, 2020), and CLIP Similarity (CLIPSIM) (Wu et al., 2021) to assess the video appearance."
        },
        {
            "title": "4.4 EVALUATION DATASET",
            "content": "1) Pose Sequence: We collect 44 novel pose templates, each comprising one or more object motions. 2) Entity Description: we use GPT to generate 20 novel human, 52 novel non-human descriptions, and 32 novel locations (refer to Sec. E.3), which are randomly assigned to poses to form 100 pairs (12 single-entity, 72 two-entity, and 16 three-entity each pair has one human entity)."
        },
        {
            "title": "4.5 COMPARISON",
            "content": "Granularity Level. As shown in Table 1, 3DTrajMaster can customize object location and orientation in 3D space. In contrast, 2D motion representations such as points (MotionCtrl/Tora) and bounding box (Direct-a-Video), lack awareness of the dimension. This ambiguity becomes more problematic when handling 3D occlusion. Besides, MotionCtrl and Tora integrate multiple entities into single 2D feature, lacking the capability to correlate individual entities with their respective trajectories (see failure case in Fig. 6). When tested on multi-entity input, Direct-a-Video (a training-free paradigm) shows particularly weak results. Furthermore, 3DTrajMaster allows for diverse entities and backgrounds (see Fig. 4), and detailed control of entity inputs (see Fig. 5). Table 1: Fine Control Comparison with Multi-Entity Input. Location Orientation Entity-Traj. Corresp. (2D) (2D) 3DTrajMaster (Ours) (3D) Direct-a-Video MotionCtrl/Tora Learning-based? (not decoupled) (decoupled) Figure 4: Diversity on Entity and Background. 3DTrajMaster can control versatile entities (human, animal, car, robot, and even abstract natural force), while also generating diverse locations. Figure 5: Fine-grained Editing on Human Entity Input. 3DTrajMaster supports modifications in attributes such as hair, clothing, figure size, and so on. (Please check more in Fig. S11) Quantative & Qualitative Results. To align with the input requirement of MotionCtrl and Directa-Video, we project the 3D pose trajectories onto 2D space. For baselines, we simplify the entity 8 Figure 6: Qualitative Comparison on Single/Multiple Entity Motion. 3DTrajMaster outperforms all 2D baselines by modeling 6 DoF entity motion, which can better express the inherent 3D nature of motion. In the last figure, Tora mistakenly regards the background entity as the girl entity. description, such as changing man with messy black hair, tall frame, red shirt to man or man in red. Otherwise, they may fail to generate videos with detailed descriptions. As shown in Fig. 6, in single entity settings, 3DTrajMaster generates precise entity motion, such as 180 turn-back and continuous inward 90 turn-around. In contrast, Tora and Direct-a-Video produce simpler motions, merely shifting objects from left to right or top-right. In the multi-entity benchmark, 3DTrajMaster successfully handles 3D occlusions, such as man walking in front of 9 Table 2: Quantative Comparison on Single/Multiple Entity Motion. 3DTrajMaster performs better on multiple entity input since the single entity trajectory is more complex. Methods TransErr (m) RotErr (deg) TransErr (m) RotErr (deg) TransErr (m) RotErr (deg) Single Entity Multiple Entities All Entities Base T2V MotionCtrl Tora Direct-a-Video 3DTrajMaster 1.946 1.752 1.707 1.632 0.456 1.799 2.134 1.158 1.902 0.319 1.586 1.682 1.867 1.391 0. 1.208 1.613 1.514 0.942 0.272 1.629 1.690 1.848 1.420 0.398 1.279 1.675 1.471 1.057 0. zebra. Direct-a-Video, however, fails in overlapping regions with mixed man and zebra. We report metric results in Table 2. It is not surprising that ours significantly outperforms all baselines."
        },
        {
            "title": "4.6 ABLATION STUDY",
            "content": "Table 3: Ablation Study on Full Testest and Base T2V Videos (As Reference Video). Video Quality 3D Trajectory Accuracy Ablation Setting FVD FID CLIPSIM TransErr (m) RotErr (deg) w/ Cross-Attn. Fusion w/ 3D Self-Attn. w/o Domain Adaptor w/o Annealed Sampl. 1673.24 1597.51 2379.89 1841.64 102.13 98.74 157.51 112.57 Full Model 1546. 96.75 32.87 33.15 30.50 32.26 33.77 0.453 0.427 0.415 0.407 0.398 0.341 0.296 0.301 0. 0.277 Figure 7: Ablation Results on Domain Adaptor (upper) and Annealed Sampling (the bottom). We provide more experiments in Sec. F.2.1 to choose suitable α and Tc to improve video quality. Improving Video Quality. As illustrated in Fig. 7 and Table 3, without the video domain adaptor, the video quality deteriorates significantly, reverting to purely UE-style appearance similar to the training set. Likewise, omitting the annealed sampling strategy results in decline in video quality (see the beard of the lion and overall scene style). While the rotation accuracy drops slightly (0.2770.265), this is acceptable since there exist errors in evaluating open-world human poses. Motion Fusion Design. As shown in Table 3, replacing gated self-attention with cross-attention fusion (w/ Cross-Attn. Fusion, here we use the entity-motion bonded feature ZPe as the query) or placing the object injector after the 3D self-attention layer (w/ 3D Self-Attn.) results in slight decline in both video quality and pose sequence accuracy."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce 3DTrajMaster, unified framework for controlling multi-entity motions in 3D space, with motion representation as 6DoF location and rotation sequences. Our flexible object injector establishes entity-wise correspondence and allows flexible editing of entity descriptions. Limitation. Generalizable entities, like animals, cannot be edited with the same level of granularity as humans. This limitation can be addressed by constructing more diverse and detailed 3D assets of the same category. Currently, the model is constrained to global motion patterns; however, finegrained local motions (e.g., human dancing or waving hands) and interactions between different entities (e.g., man picking up dog) can also be modeled similarly to our 6 DoF motions with structured motion patterns. At present, our model can only generate limited entities (3) at time, but this can be improved with more powerful video foundation models and paired datasets."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Jinwen Cao, Yisong Guo, Haowen Ji, Jichao Wang, and Yi Wang from Kuaishou Technology for their help in constructing our 360-Motion Dataset. As for the fruitful discussion, we thank Yuzhou Huang, Qinghe Wang, Runsen Xu, Zeqi Xiao, and Zhouxia Wang."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Joao Pedro Araujo, Jiaman Li, Karthik Vetrivel, Rishi Agarwal, Jiajun Wu, Deepak Gopinath, Alexander William Clegg, and Karen Liu. Circle: Capture in rich contextual environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2121121221, 2023. Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned textto-4d generation. arXiv preprint arXiv:2403.17920, 2024a. Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. arXiv preprint Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv:2407.12781, 2024b. Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. arXiv preprint, 2024. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 17281738, 2021. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video technique report, 2024. URL https://openai. generation models as world simulators. com/research/video-generation-models-as-world-simulators. Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive openworld game video generation. arXiv preprint arXiv:2411.00769, 2024. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023. Weiliang Chen, Fangfu Liu, Diankun Wu, Haowen Sun, Haixu Song, and Yueqi Duan. Dreamcinema: Cinematic transfer with free camera and 3d character. arXiv preprint arXiv:2408.12601, 2024. Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, et al. Dreamoving: human video generation framework based on diffusion models. arXiv e-prints, pp. arXiv2312, 2023. 11 Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision, pp. 241258. Springer, 2025. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pp. 33543361. IEEE, 2012. Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933, 2023a. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023b. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, arXiv preprint Photorealistic video generation with diffusion models. and Jose Lezama. arXiv:2312.06662, 2023. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan arXiv preprint Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv:2404.02101, 2024. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633 8646, 2022b. Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. Motionmaster: Training-free camera motion transfer for video generation. arXiv preprint arXiv:2404.15789, 2024a. Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024b. Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 92129221, 2024. Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, and Siyuan Huang. Scaling up dynamic human-scene interaction modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17371747, 2024. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. 12 Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, and Konrad Schindler. Video depth without video models. arXiv preprint arXiv:2411.19189, 2024. Yann Labbe, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. Megapose: 6d pose estimation of novel objects via render & compare. arXiv preprint arXiv:2212.06870, 2022. Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: large-scale city dataset for city-scale neural rendering and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 32053215, 2023a. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2251122521, 2023b. Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images. In European Conference on Computer Vision, pp. 298315. Springer, 2022. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: In Seminal Graphics Papers: Pushing the Boundaries, skinned multi-person linear model. Volume 2, pp. 851866, 2023. Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, arXiv preprint Latte: Latent diffusion transformer for video generation. and Yu Qiao. arXiv:2401.03048, 2024. OpenAI. Gpt-4v(ision) system card. OpenAI, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedIn Medical image computing and computer-assisted intervention ical image segmentation. MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Maximilian Seitzer. pytorch-fid: Fid score for pytorch. https://github.com/ mseitzer/pytorch-fid, 2020. Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia Conference Proceedings, 2024. Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:1931319325, 2021. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei In Proceedings of the Zhou. Onepose: One-shot object pose estimation without cad models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 68256834, 2022. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 24462454, 2020. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024a. Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024b. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024c. Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1786817879, 2024. Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021. Zeqi Xiao, Yifan Zhou, Shuai Yang, and Xingang Pan. Video diffusion models are training-free motion interpreter and controller. arXiv preprint arXiv:2405.14864, 2024. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation In Proceedings of the IEEE/CVF Conference on Computer Vision and using diffusion model. Pattern Recognition, pp. 14811490, 2024. Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pp. 112, 2024. Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, and Weizhi Wang. Tora: Trajectoryoriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. arXiv preprint arXiv:2310.08465, 2023."
        },
        {
            "title": "A INTERNAL VIDEO DIFFUSION MODEL FOR RESEARCH PURPOSE",
            "content": "Our model is transformer-based latent diffusion model, as illustrated in the Fig. S8. Initially, we employ 3D VAE to transform videos from the pixel level to latent space, upon which we construct transformer-based video diffusion model (Peebles & Xie, 2023). Previous models, which rely on UNets (Blattmann et al., 2023; Chen et al., 2023; Guo et al., 2023b) or transformers (Ma et al., 2024), typically incorporate an additional 1D temporal attention module for video generation, and such spatial-temporally separated designs do not yield optimal results. Instead, we replace the 1D temporal attention with 3D self-attention (Gupta et al., 2023), enabling the model to more effectively perceive and process spatiotemporal tokens, thereby achieving high-quality and coherent video generation model. Specifically, we map the timestep to scale, thereby applying RMSNorm to the spatiotemporal tokens before each attention or feed-forward network (FFN) module. Figure S8: Our Video Latent Diffusion Model Backbone"
        },
        {
            "title": "B ADDITIONAL RELATED WORK",
            "content": "(1) Learning-based: The control signals Injecting Control into Video Foundation Models. are typically projected into latent embeddings via an extra encoder (e.g., learnable convolutional/linear/attention/LoRA layers, or frozen pre-trained feature encoder), which are then integrated into the base model architecture through concatenation, addition, or insertion. VideoComposer[1] employs unified STC-encoder and CLIP model to feed multi-modal input conditions (textual, spatial, and temporal) into the base T2V model. MotionCtrl (Wang et al., 2024c) introduces camera motion by fine-tuning specific layers of the base U-Net, and object motion via additional convolutional layers. CameraCtrl (He et al., 2024) enhances this approach by incorporating ControlNet (Zhang et al., 2023)s philosophy, using an attention-based pose encoder to fuse camera signals in the form of Plucker embeddings while keeping the base model frozen. Similarly, SparseCtrl (Guo et al., 2023a) learns an add-on encoder to integrate control signals (RGB, sketch, depth) into the base model. Tora (Zhang et al., 2024) employs trajectory encoder and plug-and-play motion fuser to merge 2D trajectories with the base video model. MotionDirector[7] leverages spatial and temporal LoRA layers to learn desired motion patterns from reference videos. (2) Training-free: These methods modify attention layers or video latents to adjust control signals in computationally efficient manner. However, training-free methods often suffer from poor generalization and require extensive trial-and-error. Direct-a-video (Yang et al., 2024) amplifies or suppresses attention in spatial cross-attention layers to inject box guidance, while FreeTraj (Qiu et al., 2024) embeds target trajectories into the low-frequency components and redesigns reweighting strategies across attention layers. MOFT (Xiao et al., 2024) extracts motion priors by removing content correlation and applying motion channel filtering, and then alters the sampling process using the reference MOFT."
        },
        {
            "title": "C ADDITIONAL APPLICATIONS",
            "content": "We outline our potential applications in various areas as follows. 1) Film: Reproduce the characters classic moves. We can extract the human poses from given video and apply them to different entities and backgrounds using the capabilities of our model. 2) Autonomous Driving: Simulate dangerous safety accidents, such as two cars colliding and car hitting person. 3) Embodied AI: Generate vast number of videos with diverse entity and trajectory inputs to train general 4D pose estimator, especially for non-rigid objects. 4) Game: Train character ID, such as Black Myth Wukong, through LoRA, and then drive the character movement with different trajectories. CLARIFICATION OF THE LIMITED ENTITY NUMBER (3) Currently, our method is limited to generating up to 3 entities, as outlined in the Limitation section of the paper. This constraint is primarily due to the capabilities of the video foundation model rather than the training data. While it is relatively easy to generate 2 entities of the same category (e.g., group of people/cars/animals) in the video, it becomes much more challenging to generate 2 entities, each differing greatly from the others, through the text input as T5 text encoder tends to mix the textual features of different entities. Thus it becomes hard to associate specific trajectories with their corresponding text entities. Based on empirical studies with video foundation models, we chose to limit the number of entities to 3 in our work. Regarding data construction, it is easy to include more entities with their paired trajectories in our procedure UE platform pipeline. However, the key limitation is that the video foundation model struggles to generate such diverse set of entities simultaneously. Furthermore, many prior works, such as Tora, MotionCtrl, and Direct-a-video also focus on limited number of entities."
        },
        {
            "title": "E DATASET ILLUSTRATION",
            "content": "E.1 360-MOTION DATASET DATA. We show sample in Fig. S9 captured with 12 evenly-surrounded cameras. Each camera shoots clip of 100 frames at 384672 resolutions. During training, we discard the initial 10 frames to eliminate potential blurring and noise caused by 3D model initialization in the UE platform. E.2 UNBALANCED ENTITY DISTRIBUTION IN COMMON VIDEO DATASETS In high-quality video datasets like Artgrid, Pixabay, and Pexels3, the issue of category imbalance is highly pronounced and poses significant challenges. We analyze the aforementioned three datasets by first captioning the videos using QWen-VL (Bai et al., 2023). Subsequently, we employ the spaCy4 library to extract noun chunks from the video captions, which serve as entity words. We predefine over 60 classes as keywords for entity filtering. As illustrated in the Fig. S10, certain categories (e.g., humans) constitute disproportionately large share of the entity objects, thereby constraining the models ability to generalize to other categories that appear less frequently. E.3 GPT-GENERATED EVALUATION PROMPTS The human prompts, non-human (animal, car, robot) prompts, and location prompts for evaluation are provided in Table R4, Table R5&Table R6, and Table R7 respectively. 3Artgrid: https://artgrid.io/, Pixabay: https://www.videvo.net/, Pexels: https://www.pexels.com/ 4spaCy: https://spacy.io/ 16 Figure S9: Sample from our 360-Motion Captured with 12 Evenly-Surrounded Cameras. 17 Figure S10: Entity Distribution Over 60 Classes in Artgrid, Pixabay, and Pexels."
        },
        {
            "title": "F MORE EXPERIMENTS",
            "content": "F.1 FINE-GRAINED ENTITY PROMPT INPUT We provide additional samples in Fig. S11 to demonstrate that 3DTrajMaster supports fine-grained entity customization. The description of the man can be flexibly modified by adjusting attributes such as hair, gender, physique, clothing, and accessories. 18 Table R4: Evaluation Human Prompts. They are generated using GPT prompt: Generate more human samples similar to {Train Human Sample}, no more than 25 words. 1. man with short spiky brown hair, athletic build, navy blue jacket, beige cargo pants, and black sneakers 2. woman with long wavy blonde hair, petite figure, red floral dress, white sandals, and yellow shoulder bag 3. man with shaved head, broad shoulders, gray graphic t-shirt, dark jeans, and brown leather boots 4. woman with shoulder-length straight auburn hair, slender figure, green button-up blouse, black leggings, and white sneakers 5. man with messy black hair, tall frame, plaid red and black shirt, faded blue jeans, and tan hiking boots 6. man with medium-length straight brown hair, tall and slender, gray crew-neck t-shirt, beige trousers, and dark green sneakers 7. woman with short curly black hair, slender build, pink hoodie, light gray joggers, and blue sneakers 8. man with short black wavy hair, lean figure, green and yellow plaid shirt, dark brown pants, and black suede shoes 9. man with curly black hair, muscular build, dark green hoodie, gray joggers, and white running shoes 10. woman with short blonde hair, slim athletic build, red leather jacket, dark blue jeans, and white sneakers 11. man with medium-length wavy brown hair, lean build, black bomber jacket, olive green cargo pants, and brown hiking boots 12. man with buzz-cut blonde hair, stocky build, gray zip-up sweater, black shorts, and red basketball shoes 13. woman with long straight black hair, toned build, blue denim jacket, light gray legg -ings, and black slip-on shoes 14. man with short curly red hair, average build, black leather jacket, dark blue cargo pants, and white sneakers 15. woman with shoulder-length wavy brown hair, slim build, green parka, black leggings, and gray hiking boots 16. man with short straight black hair, tall and lean build, navy blue sweater, khaki shorts, and brown sandals 17. woman with pixie-cut blonde hair, athletic build, red windbreaker, blue ripped jeans, and black combat boots 18. man with medium-length wavy gray hair, muscular build, maroon t-shirt, beige chinos, and brown loafers 19. woman with long curly black hair, average build, purple hoodie, black athletic shorts, and white running shoes 20. man with short spiky blonde hair, slim build, black trench coat, blue jeans, and brown hiking shoes F.2 ABLATION STUDY F.2.1 OPTIMAL HYPERPARAMETERS In the main paper, we propose video domain adaptor and an annealed sampling strategy to mitigate video domain shifts from our constructed UE datasets. However, completely removing the LoRA adaptor (as the learned motion and domain bias are coupled to some extent) or the inserted motion guidance will result in decline in 3D trajectory accuracy. Thus, applying video enhancement techniques with appropriate dropping is crucial. To this end, we begin with randomly initialized parameter group: Tc = 10, α = 0.2, = 72, 000. We perform ablation experiments on our evaluation subset. As shown in Table R8, Table R9, and Table R10, the video quality exhibits monotonically decreasing trend as these hyperparameters increase. In contrast, 3D trajectory accuracy initially drops sharply but stabilizes in the later stages. To balance the degradation of visual quality with Table R5: Evaluation Non-Human Prompts (1/2). They are generated using GPT prompt: Generate more animal/car/robot samples similar to {Train Sample}, no more than 25 words. 1. dog with fluffy coat, wagging tail, and warm golden-brown fur, exuding gentle and friendly charm 2. tiger with vibrant orange and black stripes, piercing yellow eyes, and powerful stance, exuding strength and grace 3. giraffe with golden-yellow fur, long legs, tall slender neck, and patches of brown spots, exuding elegance and calm 4. an alpaca with soft white wool, short legs, thick neck, and fluffy head of fur, radiating gentle charm 5. zebra with black and white stripes, sturdy legs, short neck, and sleek mane running down its back 6. deer with sleek tan fur, long slender legs, graceful neck, and tiny antlers atop its head 7. gazelle with light golden fur, long slender legs, thin neck, and short, sharp horns, embodying elegance and agility 8. horse with chestnut brown fur, muscular legs, slim neck, and flowing mane, exuding strength and grace 9. sleek black panther with smooth, glossy coat, emerald green eyes, and powerful stance 10. cheetah with golden fur covered in black spots, intense amber eyes, and slender, agile body 11. regal lion with thick, flowing golden mane, sharp brown eyes, and powerful muscular frame 12. snow leopard with pale gray fur adorned with dark rosettes, icy blue eyes, and stealthy, poised posture 13. jaguar with golden-yellow coat dotted with intricate black rosettes, deep green eyes, and muscular build 14. wolf with thick silver-gray fur, alert golden eyes, and lean yet strong body, exuding confidence and boldness 15. tiger with pristine white coat marked by bold black stripes, bright blue eyes, and graceful, poised form 16. lynx with tufted ears, soft reddish-brown fur with faint spots, and intense yellow-green eyes 17. bear with dark brown fur, small but fierce black eyes, and broad and muscular build, radiating power 18. swift fox with reddish-orange fur, bushy tail tipped with white, and sharp, intelligent amber eyes 19. falcon with blue-gray feathers, sharp talons, and keen yellow eyes fixed on its prey below 20. fox with sleek russet fur, bushy tail tipped with black, and bright green and cunning eyes 21. kangaroo with brown fur, powerful hind legs, and muscular tail, showcasing its strength and agility 22. polar bear with thick white fur, strong paws, and black nose, embodying the essence of the Arctic 23. cheetah with slender build, spotted golden fur, and sharp eyes, epitomizing speed and agility 24. dolphin with sleek grey skin, curved dorsal fin, and intelligent, playful eyes, reflecting its nature 25. wolf with body covered in thick silver fur, sharp ears, and piercing yellow eyes, showcasing its alertness 26. leopard with body covered in golden fur, dark rosettes, and long muscular tail, emphasizing its strength 27. penguin with body covered in smooth black-and-white feathers, short wings, and webbed feet 28. gazelle with body covered in sleek tan fur, long legs, and elegant curved horns, showcasing its grace maintaining pose accuracy, we select an optimal parameter group: Tc = 25, α = 0.4, = 36, 000 as our default inference setting. 20 Table R6: Evaluation Non-Human Prompts (2/2). They are generated using GPT prompt: Generate more animal/car/robot samples similar to {Train Sample}, no more than 25 words. 29. rabbit with body covered in soft fur, quick hops, and playful demeanor, showcasing its energy 30. koala with body covered in soft grey fur, large round ears, and black nose, radiating cuteness 31. rhinoceros with body covered in thick grey skin, massive horn on its snout, and sturdy legs 32. flamingo with body covered in pink feathers, long slender legs, and gracefully curved neck 33. parrot with bright red, blue, and yellow feathers, curved beak, and sharp eyes 34. hippopotamus with body covered in thick grey-brown skin, massive jaws, and large body 35. crocodile with body covered in scaly green skin, powerful tail, and sharp teeth 36. moose with body covered in thick brown fur, massive antlers, and bulky frame 37. fluttering butterfly with intricate wing patterns, vivid colors, and graceful flight 38. chameleon with body covered in vibrant green scales, bulging eyes, and curled tail, showcasing its unique charm 39. lemur with body covered in soft grey fur, ringed tail, and wide yellow eyes, and curious expression 40. squirrel with body covered in bushy red fur, large eyes, and fluffy tail 41. panda with body covered in fluffy black-and-white fur, round face, and gentle eyes, radiating warmth 42. porcupine with body covered in spiky brown quills, small nose, and curious eyes 43. sedan with sleek metallic silver body, long wheelbase, low-profile hood, and small rear spoiler 44. an SUV with matte black exterior, elevated suspension, tall roofline, and compact rear roof rack 45. pickup truck with rugged dark green paint, extended cab, raised suspension, and modest cargo bed cover 46. vintage convertible with body covered in shiny red paint, chrome bumpers, and stylish design 47. futuristic electric car with minimalist silver design, slim LED lights, and smooth curves 48. compact electric vehicle with silver finish, aerodynamic profile, and efficient battery 49. firefighting robot with water cannon arm, heat sensors, and durable red-and-silver exterior 50. an industrial welding robot with articulated arms, laser precision welder, and heat-resistant shields 51. disaster rescue robot with reinforced limbs, advanced AI, and rugged body designed to navigate 52. an exploration rover robot with solar panels, durable wheels, and advanced sensors for planetary exploration Table R7: Evaluation Location Prompts. 1. fjord 2. sunset beach 3. cave 4. snowy tundra 5. prairie 6. asian town 7. rainforest 8. canyon 9. savanna 10. urban rooftop garden 11. swamp 12. riverbank 13. coral reef 14. volcanic landscape 15. wind farm 16. town street 17. night city square 18. mall lobby 19. glacier 20. seaside street 21. gymnastics room 22. abandoned factory 23. autumn forest 24. mountain village 25. coastal harbor 26. ancient ruins 27. modern metropolis 28. dessert 29. forest 30. city 31. snowy street 32. park F.2.2 NEGATIVE POSE CONDITION AS STATIC MOTIONS n=1 ˆPn = P0, n} rather than We find that setting negative pose sequences as static motions {( ˆPn)N positive motion sequences {(Pn)N n=1} can further improve pose accuracy, as shown in Table R11. We infer that the model captures underlying 3D motion representations from the randomly generated 3D trajectories. However, we do not adopt this approach due to the decline in video quality. 21 Figure S11: Flexible Entity Editing in Input Text Prompts. The other entity, swift falcon with blue-gray feathers, sharp talons, and keen yellow eyes focused on its prey below remains fixed while varying the human entity descriptions. F.2.3 QUALITATIVE FEEDBACK FROM HUMAN USERS We conducted questionnaire survey and collected 53 samples to form user preference comparisons. Each participant received reward of 0.80 USD and spent approximately 5 minutes completing the questionnaire, which assessed four dimensions: (1) video quality, (2) trajectory accuracy, (3) entity diversity, and (4) background diversity. In Table R12, we report the proportion of users who preferred our model over the baselines. Table R8: Ablation Study on Annealed Timestep Tc. Annealed Timestep Tc Tc = 5 Tc = 10 Tc = 15 Tc = 20 Tc = 25 Tc = 30 Tc = 35 Tc = 40 Tc = 45 Tc = 50 Video Quality 3D Trajectory Accuracy FVD FID CLIPSIM TransErr (m) RotErr (deg) 1492.79 1976.01 2179.15 2236.05 2240.40 2295.13 2323.20 2338.47 2363.49 2347.64 76.95 106.45 122.55 128.89 132.90 137.52 142.71 148.27 156.39 166.71 0.3469 0.3429 0.3405 0.3374 0.3337 0.3314 0.3276 0.3240 0.3207 0.3185 0.844 0.546 0.437 0.391 0.344 0.360 0.352 0.351 0.350 0.348 1.099 0.493 0.422 0.284 0.274 0.261 0.264 0.266 0.268 0. Table R9: Ablation Study on LoRA Scalar α. Video Quality 3D Trajectory Accuracy LoRA Scalar α FVD FID CLIPSIM TransErr (m) RotErr (deg) α = 0 α = 0.2 α = 0.4 α = 0.6 α = 0.8 α = 1.0 1495.38 1976.01 2150.42 2330.56 2318.78 2481.33 80.56 106.45 133.76 152.12 195.93 224.81 0.3467 0.3429 0.3367 0.3277 0.3125 0.3087 0.646 0.546 0.444 0.394 0.378 0. 0.900 0.493 0.428 0.393 0.450 0.432 Table R10: Ablation Study on Training Step S. Video Quality 3D Trajectory Accuracy Train. Steps FVD FID CLIPSIM TransErr (m) RotErr (deg) = 12, 000 = 36, 000 = 72, 000 = 108, 000 = 144, 000 1493.68 1883.15 1976.01 2068.43 2102.28 72.03 99.98 106.45 111.01 114.84 0.3427 0.3408 0.3429 0.3388 0. 0.561 0.523 0.546 0.446 0.411 0.713 0.631 0.493 0.480 0.482 Table R11: Ablation Study on Negative Pose Sequences. Video Quality 3D Trajectory Accuracy Negative Condition FVD FID CLIPSIM TransErr (m) RotErr (deg) Neg. Pose = Static Motions Neg. Pose = Pos. Pose 2141.39 1976.01 118.22 106. 0.3360 0.3429 0.371 0.546 0.448 0.493 Table R12: User Preference Comparisons. Method MotionCtrl Direct-a-Video Tora 3DTrajMaster 47.2% 56.6% 81.1% F.2.4 GENERALIZABLE ENTITY PROMPTS&3D TRAJECTORIES We provide more generalizable results with novel entity prompts generated by GPT and 3D trajectories, as shown in Fig. S12 to Fig. S31. Each text prompt consists of one to three entities. (We kindly urge readers to check the visual results in the our website). 23 Figure S12: Generalizable Results with Novel 3D Trajectories & Entity Prompts (1/20) 24 Figure S13: Generalizable Results with Novel 3D Trajectories & Entity Prompts (2/20) Figure S14: Generalizable Results with Novel 3D Trajectories & Entity Prompts (3/20) 26 Figure S15: Generalizable Results with Novel 3D Trajectories & Entity Prompts (4/20) 27 Figure S16: Generalizable Results with Novel 3D Trajectories & Entity Prompts (5/20) Figure S17: Generalizable Results with Novel 3D Trajectories & Entity Prompts (6/20) 29 Figure S18: Generalizable Results with Novel 3D Trajectories & Entity Prompts (7/20) 30 Figure S19: Generalizable Results with Novel 3D Trajectories & Entity Prompts (8/20) Figure S20: Generalizable Results with Novel 3D Trajectories & Entity Prompts (9/20) 32 Figure S21: Generalizable Results with Novel 3D Trajectories & Entity Prompts (10/20) 33 Figure S22: Generalizable Results with Novel 3D Trajectories & Entity Prompts (11/20) Figure S23: Generalizable Results with Novel 3D Trajectories & Entity Prompts (12/20) 35 Figure S24: Generalizable Results with Novel 3D Trajectories & Entity Prompts (13/20) 36 Figure S25: Generalizable Results with Novel 3D Trajectories & Entity Prompts (14/20) Figure S26: Generalizable Results with Novel 3D Trajectories & Entity Prompts (15/20) 38 Figure S27: Generalizable Results with Novel 3D Trajectories & Entity Prompts (16/20) 39 Figure S28: Generalizable Results with Novel 3D Trajectories & Entity Prompts (17/20) Figure S29: Generalizable Results with Novel 3D Trajectories & Entity Prompts (18/20) 41 Figure S30: Generalizable Results with Novel 3D Trajectories & Entity Prompts (19/20) 42 Figure S31: Generalizable Results with Novel 3D Trajectories & Entity Prompts (20/20)"
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}