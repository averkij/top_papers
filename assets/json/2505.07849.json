{
    "paper_title": "SweRank: Software Issue Localization with Code Ranking",
    "authors": [
        "Revanth Gangi Reddy",
        "Tarun Suresh",
        "JaeHyeok Doo",
        "Ye Liu",
        "Xuan Phi Nguyen",
        "Yingbo Zhou",
        "Semih Yavuz",
        "Caiming Xiong",
        "Heng Ji",
        "Shafiq Joty"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community."
        },
        {
            "title": "Start",
            "content": "SWERANK: Software Issue Localization with Code Ranking Revanth Gangi Reddy*1,2 Tarun Suresh*1 JaeHyeok Doo*3 Ye Liu2 Xuan Phi Nguyen2 Yingbo Zhou2 Semih Yavuz2 Caiming Xiong2 Heng Ji1 Shafiq Joty2 1 University of Illinois at Urbana-Champaign 2 Salesforce Research 3 KAIST AI {revanth3,tsuresh3}@illinois.edu; jdoo2@kaist.ac.kr; sjoty@salesforce.com 5 2 0 2 7 ] . [ 1 9 4 8 7 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to natural language issue description (e.g., bug report, feature request), is critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closedsource LLMs. Alternatively, traditional code ranking models, typically optimized for queryto-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SWERANK1, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SWELOC, large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SWERANK achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SWELOCs utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as valuable resource for the community."
        },
        {
            "title": "Introduction",
            "content": "The scale and complexity of modern software systems continue to grow exponentially, with significant portion of development effort dedicated to identifying and resolving software issues. This has fueled growth in automated software issue fixing (Cognition AI, 2024), with recent LLMbased patch generation (Yang et al., 2024a; Gau- *Equal Contribution. Work primarily done during Revanths internship at Salesforce Research. 1Code, data and models will be released here:https:// gangiswag.github.io/swerank Figure 1: Comparison of localization performance versus cost per instance on SWE-Bench-Lite. Our proposed SWERANKEMBED retriever and SWERANKLLM reranker models achieve superior accuracy at significantly lower cost compared to contemporary agent-based methods for issue localization. thier, 2024) solving real-world issues on benchmarks such as SWE-Bench (Jimenez et al., 2023), and commercial copilots integrating one-click quick-fix suggestions directly into IDEs (Microsoft, 2023; Cursor, 2025; Windsurf, 2025). Central to the process of fixing software issues, whether performed manually by developers or automatically by AI systems, is the task of issue localization: accurately identifying where in the codebase the necessary changes should be made. This involves pinpointing the specific files, classes, or functions relevant to given issue description, typically provided in natural language (e.g., bug report or feature request). Empirical studies (BÃ¶hme et al., 2017) show that human engineers spend far more time answering the question where should the patch go? than what should the patch look like?. Effective localization is critical; without correctly identifying the relevant code segments, any subsequent attempt at automated repair is likely to fail or, worse, introduce new faults. Given the importance of localization, recent work treats it as an agentic reasoning problem (Yao et al., 2023) and has investigated the use of sophisticated LLM-based agents (Yang et al., 2024b; Yu et al., 2025; Chen et al., 2025) that issue commands such as read-file, grep and traverse-graph to iteratively explore codebases, navigate file structures, search for code patterns, and analyze dependencies. While powerful, these agent-based compound systems often involve multiple rounds of interaction (710 on average) with large models and complex reasoning processes, which can incur considerable API costs ($0.66 per example with Claude-3.5) at high latency. Moreover, agent traces are brittle: they rely on temperature sampling and require complex tool orchestration. An alternative, more efficient strategy is to frame issue localization as an information retrieval problem, specifically using code ranking models (Yue et al., 2021; Zhang et al., 2024; Suresh et al., 2024). Such models can directly rank candidate code snippets (e.g., functions or files) based on their relevance to given natural language query, and quickly score and sort potential locations within large codebase. However, prior code ranking models are still inferior in performance as they have predominantly been optimized for tasks distinct from issue localization. These typically include query-to-code retrieval (Li et al., 2024b), which aims to find code implementing described functionality, and code-to-code retrieval (Wang et al., 2023a; Li et al., 2024a), focused on identifying semantically similar code fragments. The task of issue localization presents unique characteristics; input queries (issue descriptions) are often substantially more verbose than typical NL-to-code queries2 and, more crucially, issues tend to describe observed erroneous behavior or system failures rather than specifying desired functionality. This fundamental difference in query nature and intent suggests that models trained on conventional code retrieval data (Husain et al., 2019; Suresh et al., 2024) may not be optimally suited for issue localization. To bridge this gap, we introduce SWERANK, code ranking framework trained specifically for software issue localization. SWERANK employs standard yet effective retrieve-and-rerank architecture, comprising two core components: 2460 tokens in SWE-Bench (Jimenez et al., 2023) issues vs 12 tokens in CodeSearchNet (Li et al., 2024b) queries. (1) SWERANKEMBED, bi-encoder embedding model serving as the code retriever; and (2) SWERANKLLM, an instruction-tuned LLM serving as code reranker. To train SWERANK, we construct SWELOC, new large-scale issue localization dataset curated from public Github repositories, providing realistic training examples. SWERANKEMBED is trained using contrastive objective, where the issue descriptions serve as queries, the known localized functions act as positive examples, and carefully mined code snippets from the same repository function as hard negatives. Subsequently, SWERANKLLM is trained as list-wise reranker (Pradeep et al., 2023b; Reddy et al., 2024); it takes as input the issue description alongside the top-K candidates retrieved by SWERANKEMBED and learns to predict an improved ranking permutation, thereby enhancing the final localization. Empirical results demonstrate that SWERANK achieves state-of-the-art performance for file, module and function-level localization on Swe-BenchLite (Jimenez et al., 2023) and LocBench (Chen et al., 2025). Further, we show that SWERANK, built on open-source models, has considerably better performance to cost ratio compared to agentbased approaches that employ closed-source LLMs like Claude-3.5 (Anthropic, 2023). This is also illustrated in Figure 1. Finally, we also demonstrate the effectiveness of our SWELOC data by showing that it improves localization performance for variety of text and code-pretrained retriever and reranker models when used for finetuning."
        },
        {
            "title": "2.1 Software Issue Localization",
            "content": "Software issue localization or Fault Localization (FL) aims to identify the specific code locations responsible for reported bugs. Traditional fault localization methods (Wong et al., 2016) can be grouped into spectrum-based and program-analysis approaches. Spectrum-based fault localization (SFL) (de Souza et al., 2016; Amario de Souza et al., 2024) statistically associates test outcomes with executed code elements to rank statements or functions by their suspiciousness based on passing and failing test coverage. Complementary static and dynamic analyses exploit program structurethrough call-graph traversal (Adhiselvam et al., 2015), dependency analysis (Elsaka, 2017), or program slicing (Soremekun et al., 2021)to constrain the search space of potential bug locations. 2 For example, analyzing programs call graph graph of function invocationscan help identify fault-prone areas by tracing the propagation of failures through function calls. While these methods provide statistical basis for finding faults, they require precise program models and cannot leverage the rich natural language context in bug reports. Modern approaches instead use LLM-based agent frameworks that treat bug localization as planning and searching problem. AgentFL (Qin et al., 2024) incorporates multi-agent system with three step procedure involving interpreting the bug context, traversing the codebase and verifying the suspected fault. OpenHands (Wang et al., 2025) and SWE-Agent (Yang et al., 2024b) use bash commands or custom interfaces to navigate repositories and access files. Other agentic systems combine IR with tool use: MoatlessTools (Ãrwall, 2024) integrates semantic code search engine into an agents loop to guide it to relevant files. More recently, LocAgent (Chen et al., 2025) constructs graph of the codebase so that an LLM agent can perform multi-hop reasoning over code relationships. While these LLM-driven approaches have achieved impressive results, they incur substantial API costs and have high latency. Agent-based methods must orchestrate multiple steps of reasoning and tool use, which makes them brittle single failure in the chain (e.g., misleading intermediate query or an incomplete code observation) can derail the entire localization process. SWERANK takes different approach by formulating issue localization as single-shot ranking problem, which is highly efficient and cost-effective. Moreover, we demonstrate that performance of such ranking approach can be directly improved by leveraging SWELOC, which provides large-scale public GitHub issue data for constrastive training to benefit issue localization."
        },
        {
            "title": "2.2 Code Ranking",
            "content": "Transformer-based code ranking models (Wang et al., 2023c; Zhang et al., 2024; GÃ¼nther et al., 2023; Suresh et al., 2024) have set state-of-theart on variety of code retrieval tasks (Li et al., 2024b,a) by learning joint embeddings of text and code. Wang et al. (2023c) and Zhang et al. (2024) learn improved code representations by incorporating mix of training objectives, such as span denoising, text-code matching and causal LM pretraining, over large-scale code corpora such as CodeSearchNet (Husain et al., 2019) and The Stack (Kocetkov et al., 2022). Suresh et al. (2024) improve the contrastive training process between function snippets and associated doc-strings with better consistency filtering and harder negative mining. Liu et al. (2024b) incorporate multi-task contrastive data that includes code contest generation (Billah et al., 2024), code summarization (Sontakke et al., 2022), code completion (Liu et al., 2024a), code translation (Pan et al., 2024) and code agent conversation (Jin et al., 2024). However, prior code ranking models rarely include error logs in their training data and are not optimized for issue localization, where queries are verbose bug reports rather than precise functionality requests. In contrast, SWERANK is explicitly trained on SWELOC, new automatically collected set of real-world issue reports paired with known buggy functions. By optimizing bi-encoder retriever and listwise LLM reranker on this task-specific data, SWERANK directly aligns verbose bug descriptions with faulty code, thereby localization accuracy."
        },
        {
            "title": "3 SWELOC: Issue Localization Data",
            "content": "Existing code retrieval datasets (Husain et al., 2019; Suresh et al., 2024) are generally valuable for tasks like NL-to-code search which mainly requires functionality matching. However, they are sub-optimal for training models aimed at software issue localization. The nature of software issuesoften detailed descriptions of failures rather than concise functional specificationsnecessitates dataset that accurately reflects this challenge of precisely identifying the problematic functions. To address this gap and provide suitable training ground for our SWERANK framework, we constructed SWELOC, novel large-scale dataset specifically curated for the task of localizing code snippets relevant to software issues. SWELOC is derived from real-world software development activities captured in public GitHub repositories. Our methodology comprises three main phases: (1) identifying and filtering relevant pull requests (PRs) from popular Python repositories (3.1), (2) processing these PRs to extract issue descriptions paired with their corresponding code modifications (3.2), and (3) applying consistency filtering and hard-negative mining to enhance the quality of training instances (3.3). An overview of this process is shown in Figure 2. 3."
        },
        {
            "title": "Identifying Relevant PRs",
            "content": "Our data collection involves selecting the repositories associated with the top 11,000 PyPI packages 3 Figure 2: Overview of SWELOC data construction pipeline, illustrating the three main stages. on GitHub. To ensure repository quality and relevance to our task, we apply several filtering criteria. Repositories are required to contain at least 80% Python code. To prevent data leakage and overlap with existing benchmarks, we exclude repositories already present in SWE-Bench (Jimenez et al., 2023) and LocBench (Chen et al., 2025). Finally, we perform deduplication based on source code overlap to remove near-identical repositories. This process results in curated set of 3387 repositories. Following the methodology outlined in SWEBench (Jimenez et al., 2023), we then identify relevant pull requests (PRs) within these repositories. We select PRs that met two conditions: (1) the PR is explicitly linked to resolving GitHub issue within the same repository, and (2) the PR includes modifications to test files, serving as proxy for whether the issue resolution is verified. For each qualifying PR, we obtain the associated issue description and the snapshot of the codebase corresponding to the PRs base commit. This procedure results in 67341 initial (PR, codebase) pairs. Figure 3 provides further details on the datasets composition, including query and repository edit distributions."
        },
        {
            "title": "3.2 Localization Processing",
            "content": "Using the collected (PR, codebase) pairs, we create contrastive training data in the form of query, positive, negatives tuples. For each tuple, the issue description serves as the query. Each Python function modified within the PR is designated as positive example, which corresponds to distinct training instance. Thus, PR modifying functions yields training instances. The set of negatives for each instance consists of all unmodified Python functions from the corresponding codebase snapshot. This initial set of instances are further refined via consistency filtering and hard-negative mining, which we describe next."
        },
        {
            "title": "3.3 Consistency Filtering and Hard Negatives",
            "content": "The quality of query, positive, negatives tuples used for training significantly impacts the ranking model performance (Suresh et al., 2024). Effective contrastive learning requires relevant positives and challenging negatives (i.e., semantically similar to the positive but irrelevant to the query). However, issue descriptions in open-source repositories can be vague, leading to an unreliable signal for relevance between the issue descriptions and associated code modifications when directly used for training. To mitigate this, we employ filtering and mining techniques following recent work (Wang et al., 2022; GÃ¼nther et al., 2023; Suresh et al., 2024). First, we apply top-K consistency filtering (Suresh et al., 2024) to retain only instances where the positive code snippet is semantically close to the query relative to other code snippets in the repository. Formally, given an instance with issue description ti, positive function ci, and the set of other unrelated functions Fi in the codebase, we use pre-trained embedding model (instantiated as CODERANKEMBED (Suresh et al., 2024)) to compute similarities between ti, ci and all functions in Fi. Instance is retained only if ci ranks within the top functions in Fi, based on similarity to ti. We set = 20, with ablation studies presented in 5.3.1. Beyond filtering for relevance of positive pairs, incorporating challenging negatives is crucial for enabling the model to distinguish between semantically similar instances (Moreira et al., 2024; Suresh et al., 2024). To this end, we employ hard negative mining strategy that leverages the same pretrained embedding model and the previously computed similarities to select set of hard negatives Bi = {c j=1 for each instance i. These negatives are chosen from Fi such that they are among the top most similar functions to the query ti. We set = 15 in our experiments. }M 4 Figure 3: (Left) Distribution of query lengths in the SWELOC dataset. The red dashed line indicates mean query length of 382.56 tokens, underscoring the detailed nature typical of software issue reports. (Right) Distribution of the number of (a) files, (b) modules, and (c) functions modified per GitHub issue. This highlights that while many localizations are concentrated, significant number span multiple code units, particularly at finer granularities."
        },
        {
            "title": "4 SWERANK Methodology",
            "content": "loss for single positive pair (hi, h+ ) is: In this section, we present our proposed ranking framework for software issue localization. SWERANK adopts two-stage retrieve-and-rerank approach with two key components: (1) SWERANKEMBED, bi-encoder retrieval model that efficiently narrows down candidate code snippets from large codebases; and (2) SWERANKLLM, an instruction-tuned listwise LLM reranker that refines these initial results for improved localization accuracy. Next, we elaborate on the architecture and training objectives for these components."
        },
        {
            "title": "4.1 SWERANKEMBED",
            "content": "The retriever component, SWERANKEMBED, utilizes bi-encoder architecture (Reimers and Gurevych, 2019) to generate dense vector representations for GitHub issues and code functions within shared embedding space. Let (ti, c+ ) represent positive pair from the SWELOC dataset, consisting of an issue ti and the corresponding code function modified c+ . The bi-encoder maps these to embeddings (hi, h+ ), derived from the last hidden layer of the encoder. For training batch of size , let = {h+ i=1 denote the set of positive code embeddings. Let HB = (cid:83)n i=1{h j=1 be the set of embeddings for the hard negatives mined for each issue ti in the batch (as described in 3.3). SWERANKEMBED is trained using an InfoNCE contrastive loss (Oord et al., 2018). The objective encourages the embedding hi of an issue to have higher similarity with its corresponding positive code embedding h+ , compared to its similarity with all other h+ embeddings (k = i) and all hard negative embeddings kj within the batch. The ij}M }N (cid:32) LCL = log (cid:80) exp(hi h+ i) (cid:33) hk(HBH) exp(hi hk) The denominator sums over the positive embedding h+ itself and (M + 1) 1 negative embedi dings relative to hi. During inference, candidate code functions for given issue description are ranked based on the cosine similarity between their respective embeddings and the issue embedding."
        },
        {
            "title": "4.2 SWERANKLLM",
            "content": "For the reranking stage, we employ SWERANKLLM, an instruction-tuned LLM for reranking. SWERANKLLM adopts listwise ranking approach (Pradeep et al., 2023b; Reddy et al., 2024), which generally offers better performance than pointwise methods by considering the relative relevance of candidates. Typically, listwise LLM rerankers are trained to process an input consisting of the query and set of candidate documents, each associated with unique identifier. The models training objective is then to generate the full sequence of identifiers, ordered from most to least relevant according to the ground-truth ranking. However, since SWELOC does not provide groundtruth ranking among the negative functions for the issue ti, generating complete target permutation for training is not feasible. To adapt listwise reranking training to our setting where only the positive is known, we modify the training objective. Formally, let := {di}D i=1 be training dataset of triplets, where each sample , {c di := (ti, c+ j=1) includes GitHub issue ti, relevant positive code c+ , and set of irrelevant negative codes {c i,j}M j=1. We first assign unique numerical identifier from 1 to +1) to i,j}M 5 i,j}M {c j=1. Let + each function in the set c+ be the identifier assigned to the positive function c+ . Instead of training the model to predict the full ranked list of identifiers, we train it to correctly generate the identifier corresponding to the single positive function, + . Thereby, the training objective for given sample di is thus simplified to maximizing the likelihood of the first generated (i.e. top-ranked) identifier: LLM = log(PÎ¸(I + x)) where is the input prompt constructed from the issue ti and the set of candidate functions c+ {c j=1 along with their assigned identifiers, and PÎ¸ represents the listwise LLM reranker. i,j}M During training, we omit the end-of-sequence token after predicting + to retain the models capai bility to generate full ranked lists for inference, as required by the listwise format. As we show later in our experiments in 5.3.2, our approach enables finetuning any listwise reranker for the software issue localization task, without needing the full candidate ranking ordering for training supervision."
        },
        {
            "title": "5 Experiments",
            "content": "The experiments aim to comprehensively assess SWERANKs performance by comparing it against state-of-the-art agent-based localization methods, other ranking models (5.2). Furthermore, we investigate the impact of our SWELOC dataset, analyzing how its quality controls (such as consistency filtering) and size influence model performance (5.3.1), and examining its generalizability by evaluating effectiveness in fine-tuning various pre-existing retriever and reranker models for the issue localization task (5.3.2)."
        },
        {
            "title": "5.1 Setup",
            "content": "Model Training: We train the SWERANKEMBED and SWERANKLLM models in two sizes: small and large. All models are finetuned using our SWELOC dataset. SWERANKEMBED-SMALL is initialized with CodeRankEmbed (Suresh et al., 2024), SOTA 137M code embedding model, while the large variant is initialized with GTEQwen2-7B-Instruct (Li et al., 2023), 7B parameter text embedding model employing Qwen27B-Instruct as its encoder. The small version of SWERANKLLM is initialized with CODERANKLLM (Suresh et al., 2024), 7B parameter codepretrained listwise reranker. The large version is 6 initialized with Qwen-2.5-32B-Instruct that is pretrained using text listwise reranking data (Pradeep et al., 2023b). More details in Appendix A. Evaluation Datasets: We utilize SWE-BenchLite (Jimenez et al., 2023) and LocBench (Chen et al., 2025) for evaluation. Following Suresh et al. (2024), we exclude examples from SWE-BenchLite where no existing functions were modified by the patch, resulting in 274 retained examples out of the original 300. While SWE-Bench-Lite primarily consists of bug reports and feature requests, LocBench contains 560 examples overall and includes instances related to security and performance issues. Baselines: Our primary comparison is against prior agent-based localization methods. Specifically, we include OpenHands (Wang et al., 2025), SWE-Agent (Yang et al., 2024b), MoatlessTools (Ãrwall, 2024) and LocAgent (Chen et al., 2025), the current SOTA agent-based approach. Notably, these methods predominantly use closed-source models, with LocAgent also finetuning open-source models for this task. For the retrieve-and-rerank framework, we compare SWERANKEMBED-SMALL against BM25 (Robertson et al., 1994) and several code embedding models of comparable size, including Jina-Code-v2 (GÃ¼nther et al., 2023), Codesage-large-v2 (Zhang et al., 2024), and CodeRankEmbed (Suresh et al., 2024). For the 7B parameter embedding model comparison, we include GTE-Qwen2-7B-Instruct, which ranks third on the MTEB leaderboard (Muennighoff et al., 2023) at the time of evaluation. For the reranker comparison, we include CODERANKLLM and other closed source-models such as GPT-4.1. Due to the larger size of LocBench, comparisons on this benchmark are limited to subset of the best-performing baselines. Metrics: Following Agentless (Xia et al., 2024), we employ Accuracy at (Acc@k) metric for evaluation. This metric deems localization successful if all relevant code locations are correctly identified within the top-k results. Consistent with Chen et al. (2025), we measure localization accuracy at three granularities: file, module (class) and function."
        },
        {
            "title": "5.2 Localization Results",
            "content": "Table 1 compares performance of different localization methods on the SWE-Bench-Lite benchmark."
        },
        {
            "title": "Model",
            "content": "File (%) Module (%) Function (%) Acc@1 Acc@3 Acc@5 Acc@5 Acc@10 Acc@5 Acc@10 Procedure Agentless (Xia et al., 2024) MoatlessTools (Ãrwall, 2024) SWE-agent (Yang et al., 2024b) Openhands (Wang et al., 2025) LocAgent (Chen et al., 2025) GPT-4o Claude-3.5 GPT-4o Claude-3. GPT-4o Claude-3.5 GPT-4o Claude-3.5 Qwen2.5-7B(ft) Qwen2.5-32B(ft) Claude-3.5 BM25 (Robertson et al., 1994) Jina-Code-v2 (161M) (GÃ¼nther et al., 2023) Codesage-large-v2 (1.3B) (Zhang et al., 2024) CodeRankEmbed (137M) (Suresh et al., 2024) SFR-Embedding-2 (7B) (Meng et al., 2024) GTE-Qwen2-7B-Instruct (7B) (Li et al., 2023) SWERANKEMBED-SMALL (137M) (Ours) SWERANKEMBED-LARGE (7B) (Ours) Agent Retriever + Reranker CodeRankLLM (7B) (Suresh et al., 2024) GPT-4.1 SWERANKLLM-SMALL (7B) (Ours) SWERANKLLM-LARGE (32B) (Ours) 67.15 72.63 73.36 72.63 57.30 77.37 60.95 76. 70.80 75.91 77.74 38.69 43.43 47.81 52.55 58.03 65.33 66.42 72.63 72.99 82.12 78.10 83.21 74.45 79.20 84.31 85.77 64.96 87. 71.90 89.78 84.67 90.51 91.97 51.82 71.17 69.34 77.74 80.29 82.85 86.50 91.24 89.78 95.62 92.34 94.89 74.45 79.56 85.04 86. 68.98 90.15 73.72 90.15 88.32 92.70 94.16 61.68 80.29 78.10 84.67 83.94 89.78 90.88 94.16 93.80 97.08 94.53 95.99 67.15 68. 74.82 76.28 58.03 77.74 62.41 83.21 81.02 85.77 86.50 45.26 63.50 60.58 71.90 70.07 76.28 79.56 84.31 85.04 93.07 89.05 90. 67.15 68.98 76.28 76.28 58.03 78.10 63.87 83.58 82.85 87.23 87.59 52.92 72.63 69.71 78.83 79.20 83.58 85.04 89. 90.88 93.43 92.70 93.43 55.47 58.76 57.30 64.60 45.99 64.23 49.64 68.25 64.23 71.90 73. 31.75 42.34 33.94 51.82 56.20 63.14 63.14 71.90 71.90 81.75 79.56 81.39 55.47 58.76 59.49 64.96 46.35 64.60 50.36 70. 71.53 77.01 77.37 36.86 52.19 44.53 58.76 64.23 70.44 74.45 82.12 83.58 87.96 86.13 88.69 Table 1: Performance (in %) on SWE-Bench-Lite. The rerankers use SWERANKEMBED-LARGE as the retriever. Numbers for agent-based approaches are from Chen et al. (2025). Gray rows correspond to results based off closed-source models. Best retriever numbers are in blue, while best overall numbers (except GPT-4.1) are in bold. The results indicate that our SWERANK models surpasses the performance of all evaluated agent-based methods. Furthermore, the SWERANKEMBEDSMALL model, despite its relatively small size of 137M parameters, demonstrates highly competitive performance, outperforming prior 7B parameter embedding models. Notably, SWERANKEMBEDLARGE achieves higher Acc@10 scores for file and module localization than LocAgent utilizing Claude-3.5. Employing the SWERANKLLM reranker subsequently enhances the retrievers output, establishing new SOTA for localization performance on this benchmark, exceeding the performance of even closed-source models across file, module and function granularities. Table 2 shows results on LocBench. similar trend is observed, with the large variants of SWERANKEMBED and SWERANKLLM setting new SOTA performance. Figure 4 provides detailed breakdown of localization accuracy across the four distinct difficulty categories within LocBench. Despite being primarily trained with bug reports in SWELOC, the SWERANK models demonstrate impressive generalizability across other categories. Figure 4: Function localization performance comparison across different categories within LocBench. Our approach, which incorporates the SWERANKLLM reranker, considerably outperforms other Agent-based methods powered by Claude-3.5."
        },
        {
            "title": "5.3 Analysis",
            "content": "Our analysis presented in this section aims to demontrate the following key points: 1) the impact of SWELOC data quality and size on final model performance (5.3.1); 2) the utility of SWELOC for finetuning various retriever and reranker models (5.3.2; and 3) the cost-effectiveness of the proposed SWERANK framework (5.3.3)."
        },
        {
            "title": "5.3.1 Data Quality and Size\nA significant challenge when sourcing contrastive\ndata directly from public GitHub repositories is",
            "content": ""
        },
        {
            "title": "Loc Model",
            "content": "Agentless OpenHands SWE-agent"
        },
        {
            "title": "Retriever",
            "content": "+ Reranker Claude-3.5 Claude-3.5 Claude-3.5 Qwen2.5-7B(ft) Claude-3.5 CodeRankEmbed (137M) GTE-Qwen2-7B-Instruct (7B) SWERANKEMBED-SMALL (137M) SWERANKEMBED-LARGE (7B) CodeRankLLM (7B) GPT-4.1 SWERANKLLM-SMALL (7B) SWERANKLLM-LARGE (32B) File (%) Module (%) Function (%) Acc@5 Acc@10 Acc@10 Acc@15 Acc@10 Acc@15 67.50 79.82 77.68 78.57 83.39 74.29 75.54 80.36 82.14 83.93 85.89 85.54 86.61 67.50 80.00 77.68 79.64 86. 80.36 82.50 84.82 86.96 88.21 88.75 88.39 89.82 53.39 68.93 63.57 63.04 70.89 63.93 67.14 71.43 75.54 76.96 79.64 79.11 81.07 53.39 69.11 63.75 63.04 71. 67.86 71.61 75.00 78.93 80.89 82.50 82.14 83.21 42.68 59.11 51.96 51.43 59.29 47.86 51.79 58.57 63.21 64.64 71.61 69.46 71.25 42.68 59.29 51.96 51.79 60. 50.89 57.14 63.39 67.32 69.29 74.64 74.46 76.25 Table 2: Performance (in %) on LocBench. The rerankers use SWERANKEMBED-LARGE as the retriever. Numbers for Agent-based approaches are from Chen et al. (2025). Gray rows correspond to results based off closed-source models. Best retriever model numbers are in blue, while best overall numbers (except GPT-4.1) are in bold. Figure 5: Effect of SWELOC data filtering on SWEBench-Lite performance. While accuracy does improve from discarding training examples where the positive instances rank among negative candidates >K, no filtering (i.e. K=None) considerably hurts performance. the potential for high levels of noise within the instances. This subsection examines the impact of the consistency filtering method described in 3.3. Specifically, we analyze the effect of varying the value of K, the positive-rank threshold, which is used to discard training instances where the rank of the positive example (based on similarity score with the issue description) falls below relative to the negative examples. higher value of K, representing more relaxed filtering constraint, yields larger number of training instances but potentially incorporates more noise. Figure 5 illustrates the performance of SWERANKEMBED-SMALL when finetuned using SWELOC data filtered with different values. The results suggest that neither overly strict (low K) nor overly weak (high K) filtering criteria 8 Figure 6: Impact of training data size (using K=20 filtering) on the performance of SWERANKEMBEDSMALL on SWE-Bench-Lite. The subplots indicate that all metrics exhibit general upward trend as the percentage of training data increases. lead to optimal performance. moderate value (here, K=20) appears preferable, achieving balance between data quality and dataset size. Conversely, omitting filtering entirely (K=None) proves detrimental, resulting in performance decrease after finetuning. Subsequently, the impact of dataset size is analyzed independently while controlling for data quality (using K=20). Figure 6 presents the results from training with various proportions of the filtered data. Even finetuning with just 5% of the data leads to considerable performance improvements. Increasing the dataset size generally contributes to further performance gains. These experiments"
        },
        {
            "title": "Pretrain",
            "content": "Function Acc@10 (%)"
        },
        {
            "title": "Base LLM Reranker",
            "content": "Acc@5 (%) Acc@10 (%) CodeRankEmbed Arctic-Embed Arctic-Embed-v2.0 Multilingual English+Code English 59.572.3 (+12.8) 53.771.9 (+17.4) 62.070.1 (+8.1) Table 3: Accuracy (BeforeAfter) on SWE-Bench-Lite from finetuning different retrievers on SWELOC data. collectively validate the importance of both data quality and size, highlighting that simply increasing data volume without ensuring quality can negatively impact performance."
        },
        {
            "title": "5.3.2 Choice of Retriever and Reranker",
            "content": "Here, we demonstrate the effectiveness of SWELOC for the software issue localization task by analyzing the improvements achieved when finetuning diverse set of retriever and reranker models. First, embedding models pre-trained on different data types were finetuned. The models considered include: Arctic-Embed (Merrick et al., 2024), primarily pre-trained on English text retrieval data; CodeRankEmbed, pre-trained on 22 million NL-toCode examples (Suresh et al., 2024); and ArcticEmbed-v2.0 (Yu et al., 2024), pre-trained on mix of English and multilingual data. These models were finetuned for one epoch using SWELOC, with the results reported in Table 3. All models exhibited significant performance increases post-finetuning. Notably, models that initially performed weaker (e.g., Arctic-Embed) showed greater gains. This outcome validates that SWELOC can substantially improve the performance of any embedding model for the software issue localization task. Next, various textand code-instruction-tuned LLMs of different sizes from the Qwen2.5 family (Yang et al., 2024c; Hui et al., 2024) were finetuned as listwise LLM rerankers using SWELOC data. Since we only apply loss on the first generation token, to ensure compatibility with the listwise output format, all models were initially pretrained on listwise text reranking data (Pradeep et al., 2023b). The results, shown in Table 4, indicate that rerankers across different model sizes universally benefit from finetuning on SWELOC. An interesting observation is that the code-pretrained model performs marginally better at the 7B scale, whereas the text-pretrained models achieve better results at the 3B and 32B scales. 77.081.4 (+4.4) 82.5 86.1 (+3.6) Qwen-2.5-Text (32B) Qwen-2.5-Code (32B) 76.379.9 (+3.6) 81.8 84.7 (+2.9) 75.275.6 (+0.4) 81.4 82.5 (+1.1) Qwen-2.5-Text (7B) 75.575.9 (+0.4) 81.0 83.6 (+2.6) Qwen-2.5-Code (7B) 68.373.7 (+4.6) 76.682.5 (+5.9) Qwen-2.5-Text (3B) 71.271.9 (+0.7) 80.381.0 (+0.7) Qwen-2.5-Code (3B) Table 4: Function localization accuracy (BeforeAfter) on SWE-Bench-Lite from finetuning different 7B listwise LLM rerankers on SWELOC data. Method Model SWE-agent Openhands LocAgent Reranker GPT-4o Claude-3.5 GPT-4o Claude-3. Claude-3.5 Qwen2.5-7B(ft) Qwen2.5-32B(ft) GPT-4.1 SWERANKLLM (7B) SWERANKLLM (32B) Cost($) Acc@10 Cost 0.46 0. 0.83 0.79 0.66 0.05 0.09 0.16 0.011 0.015 0.8 1.0 0.6 0.9 1.2 13.2 8. 5.9 79.0 57.5 Table 5: Average inference cost of different localization methods. Our SWERANKLLM models offer considerably better cost-efficiency than agent-based approaches, while being more performant (see Tables 1 and Table 2). 5.3."
        },
        {
            "title": "Inference Cost Analysis",
            "content": "Agent-based localization approaches typically involve multiple agent interactions, each requiring the generation of extensive chain-of-thought steps (Wang et al., 2023b) and output actions. Consequently, inference can incur considerable time and cost. In contrat, the SWERANK framework offers significant cost-effectiveness as the SWERANKLLM reranker only needs to generate output candidate identifiers to determine the ranking order. Furthermore, the SWERANKEMBED output embeddings can be pre-computed, resulting in negligible inference cost. Table 5 compares the inference costs3 of SWERANKLLM with other agent-based methods. Clearly, agent-based approaches, often relying on closed-source models for optimal performance, are highly cost-intensive. The SWERANK framework is substantially cheaper while providing significantly better performance, offering up to 6 times better Acc@10/Cost tradeoffs compared to the equivalent version of LocAgent. 3We follow Chen et al. (2025) to compute cost based on the prices from AI inference providers. Specifically, for the Qwen2.5-32B model, the cost is $0.20/1M tokens for both input and output. For the Qwen2.5-7B model, the cost is $0.14/1M tokens for input and $0.28/1M tokens for output."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces SWERANK, specialized retrieve-and-rerank framework using bi-encoder retriever (SWERANKEMBED) and listwise LLM reranker (SWERANKLLM), to address the high cost and latency of agent-based systems for software issue localization. To effectively train these components, we constructed SWELOC, largescale dataset derived from real-world GitHub issues, employing consistency filtering and hardnegative mining for quality. Empirical evaluations on SWE-Bench-Lite and LocBench demonstrated state-of-the-art localization performance, significantly outperforming prior methods, including costly closed-source agent systems. SWERANK thus presents highly effective, efficient, and costeffective solution by framing localization as specialized ranking task. This approach offers practical alternative for scalable automated software engineering tools. Furthermore, the introduction of the SWELOC dataset provides valuable resource for advancing research in this crucial domain."
        },
        {
            "title": "Acknowledgement",
            "content": "We would like to thank members of Salesforce Research for helpful discussions and feedback. We are also grateful to Zach Nussbaum for useful pointers."
        },
        {
            "title": "References",
            "content": "A Adhiselvam, Kirubakaran, and Sukumar. 2015. An enhanced approach for software bug localization using map reduce technique based apriori (mrtba) algorithm. Indian Journal of Science and Technology, 8:35. Higor Amario de Souza, Marcelo de Souza Lauretto, Fabio Kon, and Marcos Lordello Chaim. 2024. Understanding the use of spectrum-based fault localization. Journal of Software: Evolution and Process, 36(6):e2622. Anthropic. 2023. Claude: Conversational ai by anthropic. Accessed: January 21, 2025. Md Mustakim Billah, Palash Ranjan Roy, Zadia Codabux, and Banani Roy. 2024. Are large language an models threat to programming platforms? In Proceedings of the 18th exploratory study. ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, pages 292 301. Marcel BÃ¶hme, Ezekiel Soremekun, Sudipta Chattopadhyay, Emamurho Ugherughe, and Andreas Zeller. 2017. Where is the bug and how is it fixed? an experiment with practitioners. In Proceedings of the 2017 11th joint meeting on foundations of software engineering, pages 117128. Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, and Xingyao Wang. 2025. Locagent: Graph-guided llm agents for code localization. arXiv preprint arXiv:2503.09089. Cognition AI. 2024. Devin: The First AI Software Engineer. https://devin.ai/. Accessed: 202504-22. Cursor. 2025. Cursor: The AI Code Editor. https: //www.cursor.com/. Accessed: 2025-04-22. Higor de Souza, Marcos Chaim, and Fabio Kon. 2016. Spectrum-based software fault localization: survey of techniques, advances, and challenges. arXiv preprint arXiv:1607.04347. E. Elsaka. 2017. Chapter three - fault localization using hybrid static/dynamic analysis. volume 105 of Advances in Computers, pages 79114. Elsevier. Paul Gauthier. 2024. How aider scored sota 26.3% on swe bench lite aider. Accessed: January 21, 2025. Michael GÃ¼nther, Georgios Mastrapas, Bo Wang, Han Xiao, and Jonathan Geuter. 2023. Jina embeddings: novel set of high-performance sentence embedding In Proceedings of the 3rd Workshop for models. Natural Language Processing Open Source Software (NLP-OSS 2023), pages 818. Michael GÃ¼nther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, and Han Xiao. 2023. Jina embeddings: novel set of highperformance sentence embedding models. Preprint, arXiv:2307.11224. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Shanghaoran Quan, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. 2024. Qwen2.5coder technical report. ArXiv, abs/2409.12186. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436. Neel Jain, Ping yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. Neftune: Noisy embeddings improve instruction finetuning. Preprint, arXiv:2310.05914. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. 10 Hyoungwook Jin, Seonghee Lee, Hyungyu Shin, and Juho Kim. 2024. Teach ai how to code: Using large language models as teachable agents for programming education. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pages 128. Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos MuÃ±oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, and 1 others. 2022. The stack: 3 tb of permissively licensed source code. Transactions on Machine Learning Research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and 1 others. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. 2024a. Coir: comprehensive benchmark for code information retrieval models. arXiv preprint arXiv:2407.02883. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. 2024b. Csn: comprehensive benchmark for code information retrieval models. arXiv preprint arXiv:2407.02883. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281. Tianyang Liu, Canwen Xu, and Julian McAuley. 2024a. Repobench: Benchmarking repository-level code In The Twelfth Internaauto-completion systems. tional Conference on Learning Representations. Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024b. Codexembed: generalist embedding model family for multiligual and multi-task code retrieval. arXiv preprint arXiv:2411.12644. Rui Meng, Ye Liu, Shafiq Rayhan Jotya, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024. Sfrembedding-2: Advanced text embedding with multistage training. Luke Merrick, Danmei Xu, Gaurav Nuti, and Daniel Campos. 2024. Arctic-embed: Scalable, efficient, and accurate text embedding models. arXiv preprint arXiv:2405.05374. Microsoft. 2023. GitHub CopilotYour AI pair programmer. Gabriel de Souza Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. 2024. Nv-retriever: Improving text embedding models with effective hard-negative mining. arXiv preprint arXiv:2407.15831. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. Mteb: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037. Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. 2024. Nomic embed: Training reproducible long context text embedder. Preprint, arXiv:2402.01613. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. In Advances in Neural Information Processing Systems, pages 1020310213. Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2024. Lost in translation: study of bugs introduced by large language models while translating code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023a. Rankvicuna: Zero-shot listwise document reranking with open-source large language models. Preprint, arXiv:2309.15088. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023b. Rankzephyr: Effective and robust zeroshot listwise reranking is breeze! arXiv preprint arXiv:2312.02724. Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, and Xiaoguang Mao. 2024. Agentfl: Scaling llm-based fault localization to project-level context. arXiv preprint arXiv:2403.16362. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506. Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, and Heng Ji. 2024. First: Faster improved listwise reranking with single token decoding. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 86428652. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. 11 Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at trec-3. In Text Retrieval Conference. Ankita Nandkishor Sontakke, Manasi Patwardhan, Lovekesh Vig, Raveendra Kumar Medicherla, Ravindra Naik, and Gautam Shroff. 2022. Code summarization: Do transformers really understand code? In Deep Learning for Code Workshop. Ezekiel Soremekun, Lukas Kirschner, Marcel BÃ¶hme, and Andreas Zeller. 2021. Locating faults with program slicing: an empirical analysis. Empirical Software Engineering, 26:145. Tarun Suresh, Revanth Gangi Reddy, Yifei Xu, Zach Nussbaum, Andriy Mulyar, Brandon Duderstadt, and Heng Ji. 2024. Cornstack: High-quality contrastive data for better code ranking. arXiv preprint arXiv:2412.01007."
        },
        {
            "title": "James",
            "content": "Andreas Vlachos, Christos Thorne, Christodoulopoulos, and Arpit Mittal. 2018. FEVER: large-scale dataset for fact extraction and VERification. In NAACL-HLT. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Weishi Wang, Yue Wang, Shafiq Joty, and Steven C.H. Hoi. 2023a. Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, page 146158, New York, NY, USA. Association for Computing Machinery. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, and 5 others. 2025. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171. Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023c. Codet5+: Open code large language models for code understanding and generation. Preprint, arXiv:2305.07922. Windsurf. 2025. Windsurf Editor: The AI-Native IDE. https://windsurf.com/editor. Accessed: 202504-22. 12 Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. survey on software fault localization. IEEE Transactions on Software Engineering, 42(8):707740. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024a. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:50528 50652. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024b. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, and 25 others. 2024c. Qwen2.5 technical report. ArXiv, abs/2412.15115. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Puxuan Yu, Luke Merrick, Gaurav Nuti, and Daniel Campos. 2024. Arctic-embed 2.0: Multilingual arXiv preprint retrieval without compromise. arXiv:2412.04506. Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, and Jishen Zhao. 2025. Orcaloca: An llm agent framework for software issue localization. arXiv preprint arXiv:2502.00350. Wang Yue, Wang Weishi, Shafiq Joty, and Steven C.H. Hoi. 2021. Codet5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. In EMNLP. Dejiao Zhang, Wasi Uddin Ahmad, Ming Tan, Hantian Ding, Ramesh Nallapati, Dan Roth, Xiaofei Ma, and Bing Xiang. 2024. CODE REPRESENTATION LEARNING AT SCALE. In The Twelfth International Conference on Learning Representations. Albert Ãrwall. 2024. Moatless tools."
        },
        {
            "title": "A Training Details",
            "content": "A.1 SWERANKEMBED Our data filtering, negative mining, and model finetuning are implemented using the contrastors package (Nussbaum et al., 2024). The SWERANKEMBED-SMALL encoder uses CODERANKEMBED, which was initialized with ArcticEmbed-M (Merrick et al., 2024), text encoder supporting an extended context length of 8,192 tokens and pretrained on large-scale web querydocument pairs, along with public text retrieval datasets (Yang et al., 2018; Kwiatkowski et al., 2019; Thorne et al., 2018). The encoder supports query prefix Represent this query for searching relevant code: , as set by (Suresh et al., 2024). The model is finetuned using 8 GH200 GPUs for two epochs with learning rate of 2e-5, batch size of 64 and 15 hard negatives per example. The SWERANKEMBED-LARGE encoder uses GTE-Qwen2-7B-Instruct (Li et al., 2023), which was pretrained on large corpora of text retrieval data. For this model, we use custom query prefix Instruct: Given github issue, identify the code that needs to be changed to fix the issue. Query: . The model is finetuned using 8 GH200 GPUs for 1 epoch with learning rate of 8e-6, batch size of 64 and 7 hard negatives per example. A.2 SWERANKLLM Training data: For each <query, positive, negatives> tuple from SWELOC, we randomly sample 9 negative codes to fit the listwise reranking window size of 10 along with the positive code. To prevent the positional bias from affecting the reranker and ensure model robustness (Pradeep et al., 2023a), we shuffle the order of candidate codes for each training example. Since the combined length of GitHub issue and corresponding candidate codes may exceed the models maximum embedding size, we set the maximum length per candidate code to 1024 and the total length limit to 16348. For overlong prompts, we truncate the query to reach the maximum total length. This preserves meaningful context for issue localization as much as possible within the limited context window size for effective model training. The rerankers are all first pretrained with text listwise reranking data (Pradeep et al., 2023b) to teach the model to follow the listwise output format. Hyperparameters: For the LLM reranker training, with both text reranking and SWELOC data, we trained for one epoch with global batch size of 128, an initial learning rate of 5e-6 with 50 warmup steps, cosine learning rate scheduler, bfloat16 precision, and noisy embeddings (Jain et al., 2023) with noise scale Î± = 5. For efficient long-context, multi-gpu training, we used DeepSpeed (Rasley et al., 2020) ZeRO stage 3 with 16 GH200 GPUs."
        },
        {
            "title": "B Diversity of Issue Topics in SWELOC",
            "content": "To provide more insight into the variety and complexity of issue topics in SWELOC, we analyze the distribution of topics for 10k randomly sampled instances. We use Nomic Atlas4, popular unstructured text visualization tool, that employs cluster-based keyword identification algorithm and leverages language model to generate topics. We plot the frequency of the top-15 topics in Figure 7. Figure 7: Top-15 issue topics and their frequencies from randomly sampled subset of SWELOC. 4https://atlas.nomic.ai/"
        }
    ],
    "affiliations": [
        "KAIST AI",
        "Salesforce Research",
        "University of Illinois at Urbana-Champaign"
    ]
}