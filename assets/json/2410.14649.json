{
    "paper_title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search",
    "authors": [
        "Oliver Sieberling",
        "Denis Kuznedelev",
        "Eldar Kurtic",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by \\emph{dynamic, non-uniform} compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on heuristics for identifying the \"importance\" of a given layer towards the loss, based on assumptions such as \\emph{error monotonicity}, i.e. that the end-to-end model compression error is proportional to the sum of layer-wise errors. In this paper, we revisit this area, and propose a new and general approach for dynamic compression that is provably optimal in a given input range. We begin from the motivating observation that, in general, \\emph{error monotonicity does not hold for LLMs}: compressed models with lower sum of per-layer errors can perform \\emph{worse} than models with higher error sums. To address this, we propose a new general evolutionary framework for dynamic LLM compression called EvoPress, which has provable convergence, and low sample and evaluation complexity. We show that these theoretical guarantees lead to highly competitive practical performance for dynamic compression of Llama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art results across all compression approaches: structural pruning (block/layer dropping), unstructured sparsity, as well as quantization with dynamic bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 9 4 6 4 1 . 0 1 4 2 : r EVOPRESS: TOWARDS OPTIMAL DYNAMIC MODEL COMPRESSION VIA EVOLUTIONARY SEARCH Oliver Sieberling ETH Zurich osieberling@ethz.ch Denis Kuznedelev Yandex & Skoltech denis.kuznedelev@skoltech.ru Eldar Kurtic IST Austria & Neural Magic eldar.kurtic@ist.ac.at Dan Alistarh IST Austria & Neural Magic dan.alistarh@ist.ac.at"
        },
        {
            "title": "ABSTRACT",
            "content": "The high computational costs of large language models (LLMs) have led to flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. new frontier in this area is given by dynamic, non-uniform compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing global compression threshold. Yet, current methods rely on heuristics for identifying the importance of given layer towards the loss, based on assumptions such as error monotonicity, i.e. that the end-to-end model compression error is proportional to the sum of layer-wise errors. In this paper, we revisit this area, and propose new and general approach for dynamic compression that is provably optimal in given input range. We begin from the motivating observation that, in general, error monotonicity does not hold for LLMs: compressed models with lower sum of per-layer errors can perform worse than models with higher error sums. To address this, we propose new general evolutionary framework for dynamic LLM compression called EvoPress, which has provable convergence, and low sample and evaluation complexity. We show that these theoretical guarantees lead to highly competitive practical performance for dynamic compression of Llama, Mistral and Phi models. Via EvoPress, we set new state-ofthe-art results across all compression approaches: structural pruning (block/layer dropping), unstructured sparsity, as well as quantization with dynamic bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress."
        },
        {
            "title": "INTRODUCTION",
            "content": "Model compression has become standard way of reducing the deployment costs of large language models (LLMs). Current post-training compression techniques can be roughly categorized into quantization-based, which reduce the bit-width of weights or activations, e.g. (Frantar et al., 2022; Lin et al., 2023; Dettmers & Zettlemoyer, 2022; Tseng et al., 2024), pruning-based, which sparsify the weight matrices, e.g. (Frantar & Alistarh, 2023; Yin et al., 2024), or structured pruning / layer dropping, which drop entire model components, e.g. (Kim et al., 2024; Men et al., 2024). While constantly improving their performance, existing compression methods are reaching diminishing returns in terms of accuracy-vs-compression (Dettmers et al., 2023; Tseng et al., 2024). In this context, new direction is dynamic, or non-uniform, layer-wise compression, in which different layers can be compressed to various levels, according to their sensitivity relative to the model output. Dynamic compression allows to maximize model accuracy while satisfying Instance-specific solutions for this given compression requirement, e.g. target model size. problem have already been proposed for essentially every compression type: sparsity (Yin et al., 2024), quantization (Frantar & Alistarh, 2022), or layer dropping (Kim et al., 2024; Men et al., 2024). Broadly, these approaches work by assigning an error/sensitivity score to each layer and compression level, which measures the impact of its compression on output loss increase. Then, one 1 Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP. Model Configuration (Each block contains Attention + MLP) Wiki2 C4 FW Llama-3-8B 5.54 8.80 7. 188.01 147.25 70.46 35.53 26.24 24.39 calculates compression assignment which minimizes the sum of error scores, while still satisfying the global compression constraint. Thus, such approaches inherently assume error monotonicity: i.e., that lower sum of error scores implies lower compression error for the entire model. Our work starts from the observation that error monotonicity does not hold generally for LLM compression: specifically, there are instances where compressed models with lower sums of perlayer errors can perform worse than models with higher error. We illustrate this fact in Table 1, which shows an instance of layer dropping configuration where keeping more blocks leads to massively higher perplexity than an instance which prunes strictly less blocks. Contribution. This refutation of error monotonicity implies that most prior approaches, which are based on this assumption, can lead to sub-optimal solutions. Thus, it motivates our investigation of alternatives towards optimal non-uniform compression. For this, we propose new evolutionary search approach called EvoPress, which is provably convergent, and is also sample and iteration efficient. Thus, EvoPress is the first non-uniform compression method with guarantees; its two efficiency properties are critical for practicality in the context of LLMs, where the cost of evaluating single models (offspring) is exceedingly high. We validate the approach across all three popular approaches for post-training LLM compression: layer dropping, one-shot sparsification, and quantization. We find that EvoPress consistently improves upon existing techniques, with major improvements at higher compression ratios. In more detail, we assume setting where we are given pre-trained model, compression constraint such as the target model size, set of compression options (e.g., 10 possible sparsity options per layer), and aim to identify per-layer assignment which satisfies the constraint, while minimizing accuracy loss, measured in perplexity or in-context learning accuracy degradation. As is standard, e.g. (Frantar & Alistarh, 2022), from the compression options we build level database, where each layer is compressed independently to each compression option. During the candidate search, our offspring are models stitched together from the level database, and our fitness function will be the difference (e.g., in KL-divergence) between the outputs of the offspring and the original model, on set of calibration samples. At each step, our search algorithm starts with single search point (candidate model), and generates constant λ 1 additional offspring, by applying mutation operation which preserves the compression constraint. The selection stage is composed of multiple steps, where we iteratively evaluate the offspring and parent on increasingly many randomly chosen samples. For instance, we may start to evaluate the parent and λ = 64 offspring on less than single sample on the first sub-step, but progressively multiply the number of calibration samples as we sift through candidates, reducing variance as we obtain more competitive offspring. We found this trade-off between exploration and evaluation variance essential for efficiency on LLMs, as it drastically reduces our total number of evaluations relative to the case where all the initial offspring must be evaluated on full batch. Our algorithm guarantees convergence: specifically, any linear fitness function defined on the ndimensional hypercube will be maximized in expected O(k(n k)/λ) generations under the constraint x1 = k, where λ is the number of offspring. The proof is quite non-trivial, as it needs to adapt stochastic drift analysis techniques to the case where multiple offspring are examined in each sub-step. In Figure 1, we illustrate the algorithms fast convergence and high efficiency on practical example with correlated block dropping on Llama-3-8B, where we determined the optimum via (expensive) exhaustive search: EvoPress is able to reach the optimum in only 6 generations, using total of only 56 model evaluations. key advantage of our approach is that it is agnostic of the model architecture and compression type. We illustrate this via experiments, which are the first 2 to span all three compression methods, across different LLM families. Specifically, results show that EvoPress significantly improves upon all prior work on depth pruning in terms of accuracy-vscompression, especially at medium levels, and also outperforms the prior best methods OWL and dynamic programming, respectively for non-uniform pruning and quantization. Moreover, it can do so efficiently: the full version of EvoPress, applied at high compression granularity, will converge in few hours on single RTX 3090 GPU, and we also present lightweight version which utilizes fewer samples and converges in 1 hour in the same setting, on an 8B-parameter model."
        },
        {
            "title": "2 RELATED WORK",
            "content": "To our knowledge, we are the first to present unified approach which covers all types of posttraining LLM compression (i.e., layer dropping / depth pruning and non-uniform pruning / quantization) so far, these problems have generally been approached independently. Depth Pruning. Recently, there has been lot of interest in compression by removing entire transformer blocks, both for efficiency and to gain insights about the language model itself. Most methods are based on scoring the importance of each block, and then maximizing the importance of the resulting model by removing the blocks of lowest importance. Weight Subcloning (Samragh et al., 2023) proposed multi-step process to find good initializations for an untrained smaller model given an already trained larger one, where the importance of each block is scored based on the ratio of ℓ2 norms between the output embeddings of the block with and without the residual connection. Shortened Llama (Kim et al., 2024) proposes scoring each block by measuring the perplexity after removing the respective block from the full model. ShortGPT (Men et al., 2024) uses the cosine similarity between the input and output embeddings of each block to assess its importance. By contrast, Gromov et al. (2024) restrict themselves to removing consecutive blocks and score each of these removal configurations using cosine similarity. Non-Uniform Pruning and Quantization. He et al. (2018); Ashok et al. (2018) were among the first to consider automatic optimization of non-uniform compression, specifically for the case of pruning, where they developed Reinforcement Learning (RL)-based approaches. However, both approaches suffer from high tuning complexity and would be very hard to scale to large models. Follow-up work (Hubara et al., 2021; Yao et al., 2021; Li et al., 2021) considered similar problem specifically for quantization, but explore computationally-expensive solvers (e.g. ILPs) which rely on the fact that quantization has only small number of choices (precision levels) per layer. SPDY (Frantar & Alistarh, 2022) considered unified framework which reduces the problems to knapsack-type instances, and solves them optimally modulo discretization. However, SPDY explicitly relies on monotonicity and linearity assumptions on the dependency between the per-layer errors and model output error, which we find not to hold on large models, especially in the highcompression regime (e.g., below 3 bits per parameter). Relative to SPDY, EvoPress provides guarantees for broader class of input functions, and focuses on efficiency for LLM compression. The recent OWL method (Yin et al., 2024) focuses on non-uniform pruning of LLMs, and provides consistent improvements over uniform profiles via layer scoring system which analyzes the activation outlier structure, but does not have any theoretical guarantees. Experimentally, we find that OWL is effective especially for Llama-family models (Touvron et al., 2023) and at moderate sparsities, but observe significant gaps in favor of EvoPress across all models and compression levels. NAS and Structural Pruning. Random search is also popular in the context of structural pruning and Neural Architecture Search (NAS) (Chen et al., 2020; Dong et al., 2021; Wang et al., 2020; Xu et al., 2021; Yin et al., 2021; Molchanov et al., 2022; Kurtic et al., 2024). However, such methods also rely heavily on re-training and have notoriously high costs, which limits their applicability to post-training compression of LLMs. Due to its low sample complexity, we believe that EvoPress could be extensible to lightweight NAS as well, and plan to investigate this in future work."
        },
        {
            "title": "3 METHOD",
            "content": "All applications of EvoPress are grounded in unified framework, where the objective is to identify the optimal model that adheres to specified compression method and constraint. Formally, given base model , we seek to maximize the performance of the compressed model while satisfying the 3 compression constraint: ˆM = arg max ( ˆM ) subject to g( ˆM ) C, ˆM where ( ˆM ) quantifies the performance of the compressed model ˆM and g( ˆM ) represents the compression constraint. For simplicity, we will define as the models total size (in terms of parameters); however, the proposed method can be readily adapted to accommodate other practical constraints, such as inference speed. We approach this optimization problem using evolutionary search, which is specific form of randomized search. The feasibility of such an approach heavily depends on two factors: the time required to evaluate the fitness of candidate solution and the number of such function evaluations needed until satisfying result is achieved. This poses particular challenge in our case, as assessing the performance of an LLM involves substantial computational costs. Level Database. As first step, we compress the model to different levels. It is crucial that the units we search over specifically layers or blocks are compressed independently; otherwise, we risk losing performance when stitching together the compressed model. Ideally, the difference between two compression levels should be consistent across layers. This uniformity simplifies the optimization process, allowing for the free exchange of compression levels, as we will demonstrate for unstructured sparsity. However, this restriction is not essential for the search procedure to be effective. In the context of quantization, we will demonstrate relaxation of this requirement, where compression steps are uniform only across layers of the same size. Fitness Environment. Given the specified database, any compressed model is completely characterized by its compression level for each unit (per layer or per block). With units, each available in compression levels, our objective is to find ˆM = arg max v[m]n ( ˆMv) subject to g( ˆMv) C, where we are searching over the set of n-tuples over [m]. Assessing the performance of model in practice typically involves benchmark tasks, which have limited scope and require lengthy evaluation. We address these challenges by using the base model as the gold standard and focusing solely on the relative degradation of our compressed models. To quantify this degradation, we measure the Kullback-Leibler (KL) divergence between the two models, as it has proven particularly robust with limited data. Empirically, we observed that already around 65536 tokens of calibration data (corresponding to 8 full sample sequences for Llama-3-8B) are sufficient to reliably determine the quality of the lightweight model. To avoid confusion, we will refrain from inverting the fitness function and from now on consider the minimization problem ˆM = arg min v[m]n DKL(PM ˆMv ) subject to g( ˆMv) C, where we speak of higher fitness whenever the KL-Divergence is lower. Algorithm. EvoPress starts from upon the classic (1 + λ)-evolutionary algorithm, which maintains single search point at any given time. In each generation, λ offspring are generated by copying the parent and then applying mutation operator to each copy. The offspring are then evaluated on the fitness function, and the fittest one is selected. As an elitist evolutionary algorithm, the (1 + λ)-EA replaces its parent only if the best offspring has superior fitness. We change this standard algorithm in two important ways. The first is by introducing level-switch mutation, simple mutation operator that ensures high locality while preserving the compression constraint. The operator involves first randomly selecting one unit and increasing its compression level. Next, second unit is sampled until one with matching level step size is found, and its compression level is decreased. This approach ensures that 1) the compression constraint is preserved, and 2) the offspring model maintains high similarity to the parent model an important feature for achieving rapid convergence. The second modification is that we employ very aggressive form of multi-step selection. In the first stage, all λ offspring are evaluated using only fraction of full sample. From this, only small subset of the fittest offspring are selected to compete in the next stage, where they are evaluated Algorithm 1: EvoPress: (1 + λ)-Evolutionary Algorithm with Level-Switch Mutation and Multi-Step Selection for Maximizing Fitness Function : [m]n Initialization: candidates [] ; for 1 to initialCandidates do // Only for non-integer target compression candidate sampleUniformly(); candidates.append (candidate); x(1) selectTopKFittest(candidates, initialTokens, = 1); Optimization: for 1 to do offspring []; Mutation: for 1 to λ do yi x(t); yi LevelSwitchMutation(yi); offspring.append (yi); Selection: for step 1 to selectionSteps do Elitism: if step = selectionSteps then offspring.append (x(t)); offspring selectTopKFittest(offspring, tokens[step], = survivors[step]); x(t+1) offspring[0]; on significantly larger sample size. This process is repeated once more, and in the final stage, the few remaining offspring are evaluated against the parent using full minibatch, consisting of approximately 20-50 times the number of tokens used in the first stage. For initialization, we apply the target level directly if it matches an available setting (e.g., all layers at 70% sparsity for an average of 70% sparsity). If the target falls between two compression levels (e.g., for block dropping), we initialize by randomly sampling candidates with some units compressed to the next lower level, and others to the next higher level, selecting the fittest among them. high level overview of this optimization procedure can be found in Algorithm 1. i.e. Design Considerations. Randomized search heuristics are heavily influenced by the the exploration-exploitation dilemma, trade-off between exploring broader solution space and intensifying the search around the currently-best solutions. Many applications utilize sophisticated search procedures, such as genetic algorithms, to enhance exploration, that often maintain large population, introduce crossover operations, and adopt nonelitist strategies, where parents have no chance of survival into the next generation. However, implementing these approaches for LLM compression would come with significant computational costs. Crossover, for instance, is only effective if population diversity is preserved, for example measured by the sum of pairwise Hamming distances between individuals (Jansen & Wegener, 2002; Opris et al., 2024). While this promotes more thorough exploration of the search space, it requires allocating resources to less promising regions, slowing down the progress toward optimal solutions. Similarly, nonelitist algorithms, despite their ability to escape local optima (Dang et al., 2021; Jorritsma et al., 2023; Lengler et al., 2024), also incur costs by frequently discarding potentially useful individuFigure 1: Removing twelve transformer blocks from Llama-3-8B under the constraint that only pairs of consecutive blocks can be removed. EvoPress finds the optimal configuration from the 8008 possible removal combinations in generation 6. 5 als. Consequently, these approaches should be reserved for situations where the fitness landscape is highly rugged, and escaping local optima is critical to finding better solutions. Convergence. Contrary to many real-world problems, dynamic model compression with carefully designed level database induces notably smooth fitness environment, where small changes in the compressed model tend to lead to small changes in performance. key insight into the effectiveness of evolutionary approaches is that, although the search space expands exponentially with the number of units considered, the maximum Hamming distance between any two search points in the search space increases only linearly. Therefore, as long as we receive signal indicating the direction of improvement, even with seemingly limited progress per generation, we can converge rapidly to high-quality solution. To illustrate this, we consider the problem of removing pairs of consecutive blocks of Llama-3-8B. We perform brute-force search over all possible 8008 block removal configurations, where six pairs of blocks are removed. Our method identifies the optimal configuration by the 6th generation, having evaluated only 16 candidates for initialization and 8 candidates per generation. Figure 1 illustrates how the algorithm progressively approaches the optimum in terms of Hamming distance. Consequently, EvoPress is heavily exploitation-focused: we rely on elitism, introduce minimal mutation, maintain only single offspring, and therefore employ zero population diversity. We present ablations and short discussion on these choices in Appendix B. EvoPress excels at optimizing smooth fitness environments, capability we theoretically support by proving rapid convergence under an ℓ1-constraint for the class of linear functions1. Theorem 1. Let n, with and consider the (1 + λ)-EA with λ O(n/ log(n)) and level-switch mutation. Then any linear fitness function : {x {0, 1}n, x1 = k} is optimized in expected (cid:18) (n k) generations. (cid:19) 1 λ Discussion. The proof is based on stochastic drift analysis and can be found in Appendix A. The derived bound has important practical implications. By increasing the number of offspring per generation, we can reduce the number of generations required for convergence, with the reduction scaling proportionally to λ up to reasonably large value. Since our approach uses highly aggressive form of multi-step selection, the benefit is not simply zero-sum trade-off. Evaluating many offspring in each generation incurs significantly lower per-offspring computational cost, leading to substantial speedup in convergence time. This makes the algorithm highly effective in smooth fitness environments, making it particularly well-suited for dynamic model compression."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We now validate the efficiency of EvoPress for determining the optimal layer-wise compression across three approaches: (1) layer dropping, where the goal is to isolate the optimal set of blocks to drop given target ratio, (2) non-uniform unstructured sparsity and (3) non-uniform quantization, where we are given set of compression options per layer (sparsities or bit-widths), and the goal is to find the optimal configuration that matches certain model size. We focus on LLM compression, given the major interest in the reduction of their model size and inference latency, but our method is general and can be applied to any neural network architecture and application domain. Experimental Setup. We consider base models from the Llama-2 and Llama-3 (Touvron et al., 2023) families, Mistral-v0.3 (Jiang et al., 2023), and the instruction-tuned Phi3-Medium-instruct128k model (Abdin et al., 2024). We adopt KL-divergence as our fitness function as it provides stronger and more robust signal compared to perplexity, reflecting the predictive distribution of the original model. We present ablations to validate this choice in Appendix B.3. Concretely, our algorithm works as follows: Initially, for the case of quantization between available bit widths (e.g. 2.5 bit) and block dropping, we produce number of initial configurations (around 32), evaluate them on few data samples, and take the fittest one. For quantization with available target bitwidth and unstructured sparsity, we simply initialize using the uniform configuration. 1In our context, one bit corresponds to two compression levels, while each weight of the linear function corresponds to the saliency. The ℓ1-constraint is now equivalent to an compression constraint. Then, we generate new offspring in each generation by making small number of random switches in compression levels, where the number of switches is sampled from min(randint(1,3), randint(1,3)) and compression levels are exchanged in such way that the overall compression ratio is maintained. We perform selection in multiple steps by iteratively choosing only the best configurations for survival, where each round uses progressively more tokens and has fewer survivors. To ensure elitism, we add the current parent to the candidate pool in the last stage of selection. Finally, after two or three of such stages, we take the last remaining configuration and adopt it as the population for the next round. We run for fixed number of iterations, chosen so that performance on held-out data no longer improves. The detailed parameter setting is described in Appendix C. To perform per-layer compression via unstructured sparsity and quantization, we adopt the dataaware compression methods SparseGPT (Frantar & Alistarh, 2023) and GPTQ (Frantar et al., 2022). For this purpose, we use Fineweb-Edu (Penedo et al., 2024) as source of clean and diverse calibration data. Following Egiazarian et al. (2024), we fix the total number of calibration tokens to 8 million (8M). For fair comparison, all competitive methods employ the same calibration data. The code and all configurations found by EvoPress are available at https://github.com/ISTDASLab/EvoPress. Evaluation. We follow the standard LLM evaluation protocol from Frantar et al. (2022). Specifically, we measure perplexity on the WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2019) datasets for language performance and accuracy on zero-shot evaluations on standard benchmarks: WinoGrande (Sakaguchi et al., 2021), PiQA (Tata & Patel, 2003), HellaSwag (Zellers et al., 2019), ARC-easy and ARC-challenge (Clark et al., 2018) via the LM Eval Harness (Gao et al., 2021). 4.1 APPLICATION 1: DEPTH PRUNING As first application, we apply EvoPress on Depth Pruning. Although removing entire transformer blocks generally results in greater accuracy losses compared to other compression techniques, this approach recently attracted attention in the context of initializing smaller models, as it guarantees speedups proportional to the sparsity (Samragh et al., 2023; Kim et al., 2024). Additionally, block dropping provides insights into the capabilities of transformer models, making it relevant for interpretability. We will compare against the following baselines: Shortened Llama (Kim et al., 2024): Scores blocks on the perplexity change after removal. ShortGPT (Men et al., 2024): Blocks are scored based on the average cosine similarity between input and output embeddings, including the residual stream. Weight Subcloning (Samragh et al., 2023): Blocks are scored using the ratio (x)/f (x) + x, where is the input embedding and (x) is the blocks output, excluding the residual stream. Sliding Window Cosine Similarity (Gromov et al., 2024): Sets of consecutive blocks are scored based on the cosine similarity between embeddings before and after the blocks, including the residual stream. While Gromov et al. (2024) directly score entire removal configurations, Shortened Llama, ShortGPT, and Weight Subcloning determine block removals based on their isolated scores. Search Space. In our approach, attention and MLP modules are treated independently rather than as single unit. For each module, there are two options: either retain it or remove it. To achieve target sparsity/depth, we initially remove an equal number of attention and MLP modules. During mutation, we allow compression level adjustments only between modules of the same type. We leave it open for future research to remove this constraint to allow flexibility in the number of removed attention and MLP modules. Experimental Results. Figure 2 compares our method with baselines from previous work on Mistral-7B-v0.3. For better comparison, we also included results where only entire transformer blocks are removed (Attn.+MLP). EvoPress consistently outperforms all previous methods, showing significant improvements even at medium sparsity levels. While all baseline methods fail entirely beyond 31.25% sparsity, EvoPress identifies functional submodels even when removing half of the 7 Figure 2: Depth pruning results, on Mistral-7B-v0.3. (Left) Relative to all prior methods, EvoPress shows significantly lower PPL gap relative to the uncompressed model, with remarkably large gaps at medium compression rates. (Right) Examining the blocks dropped, we observe that EvoPress isolates completely different profiles relative to ShortGPT (which scores by cosine similarity). model. To our knowledge, this is the first method to achieve such results. We observed similar collapses in Llama-2-7B, Llama-3-8B and Llama-3.1-8B. Overall, EvoPress consistently outperforms all baselines across all tested models and sparsities (see Appendix D.1 for full results), and does so in matter of minutes (Appendix D.2). All four previous methods rely on human-crafted scoring methods to identify the optimal combination of transformer blocks to remove. However, these approaches are not only suboptimal, but also prone to bias, as their results may reflect the characteristics of the method itself rather than the models true behavior. Specifically, we found that most scoring methods tend to favor deeper blocks, resulting in highly similar removal configurations across different prior scoring methods (Appendix Table 13). This likely occurs because methods that bias towards deeper blocks generally perform better than those that focus on earlier blocks, although neither may be optimal. In contrast, EvoPress employs an unbiased approach, offering more accurate and meaningful insights into the model. As shown in Figure 2, we found that the deeper layers are not necessarily the least important, contradicting conclusions drawn in prior work (Gromov et al., 2024; Men et al., 2024). 4.2 APPLICATION 2: UNSTRUCTURED SPARSITY Next, we examine performance for unstructured sparsity, which offers more fine-grained compression. The standard approach is to allocate sparsity uniformly across layers. However, some layers may be more sensitive to sparsity, which can significantly impact the models output. To address this, OWL (Yin et al., 2024) introduces the Layer Outlier Distribution (LOD) metric as measure of layer saliency, and computes sparsity profile that is weighted by LOD. We compare EvoPress with both uniform sparsity and OWL. For OWL we used the same hyperparameter grid as the original work and took the configuration yielding the best perplexity for each model. Search Space. Sparsity levels are generated as follows: For each layer, we first produce the base level corresponding to the targeted average sparsity. Then, we generate both higher and lower compression levels, where the difference between two levels corresponds to fixed number of weights. In our experiments, we used step size of 1M weights uniformly. This approach enables the mutation of compression levels across all layers, independently of their size. We adopt SparseGPT (Frantar & Alistarh, 2023) as fast and efficient one-shot layer pruner. Experimental Results. We compare different methods for pruning to 50%, 60% and 70% unstructured sparsity. We report the 70% results in Table 2; the 50% and 60% results can be found in Appendix Tables 14 and 15, respectively. As illustrated in Table 2, EvoPress successfully finds better profiles than uniform sparsity and noticeably outperforms competitive methods on PPL and zero-shot average accuracy by large margins on all models. Examining sparsity profiles (Appendix Figures 8 and 9), we observe that EvoPress prunes the first blocks less aggressively, blocks in the beginning of the second half of the model more aggressively while keeping the even deeper blocks relatively dense. Notably, EvoPress assigns high importance to the proj matrices, reducing its sparsity to below 45%, compared to an overall average of 70%. 8 Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy. Model Method Wiki2 C4 ArcC ArcE HS PiQA WG Avg Mistral-7B-v0.3 Llama-3-8B Llama-3.1-8B Phi-3-Medium-14B Dense 4.82 7.72 Uniform 23.08 17.22 OWL 14.42 EvoPress 30.03 21.66 16.46 Dense 5.54 7.10 Uniform 85.84 48.07 OWL 28.76 EvoPress 98.35 52.32 33.72 Dense 5. 8.90 Uniform 68.97 42.29 OWL 24.32 EvoPress 103.27 48.65 30.58 Dense 4.02 8. Uniform 16.66 15.66 OWL 13.83 EvoPress 24.73 23.38 19.13 48.9 27.1 27.9 31.6 50.4 22.7 27.0 28. 51.2 22.3 27.4 29.1 60.9 36.9 35.7 41.5 79.6 60.9 62.6 64. 80.1 49.9 54.9 56.7 81.4 49.7 55.8 62.4 84.1 70.6 69.2 73. 60.9 36.1 38.6 41.4 60.2 31.4 36.6 38.6 60.0 31.5 36.5 39. 64.0 40.0 39.4 43.6 80.3 65.9 67.0 69.5 79.7 62.1 65.1 68. 80.1 61.6 65.7 68.9 81.0 69.4 68.3 71.8 73.9 68. 59.4 63.5 61.9 49.9 51.9 53.8 72.6 68.6 54.4 58.6 61.7 44.1 48.4 50. 73.9 69.3 55.6 60.7 60.3 44.2 49.2 52.0 76.2 73. 65.8 64.4 69.1 56.5 55.4 59.8 Running Time. EvoPress is also time-efficient. Figure 3 illustrates the rapid convergence of our method vs. iterations and time, with smooth and steady improvements in test perplexity. Moreover, we found that, by significantly reducing the number of tokens used in the multi-step selection evaluation, by 4 in the first step and 8 in the last step, and making each generation have fewer offspring, we can significantly speed up the search. This super-fast version converges in little over one GPU hour to similar test PPL (Figure 3, right), demonstrating the sample-robustness of EvoPress, which can lead to further efficiency gains. Figure 3: Left: The convergence of EvoPress vs. number of generations and wall-clock time (on single RTX 3090 GPU with 24GB RAM) for Llama-2-7B. We observe convergence close to optimum in 5-6h; Right: Convergence of the super-fast version which reduces the number of tokens used for each evaluation. It converges to similar accuracy in little over one hour, in the same setting. 4.3 APPLICATION 3: QUANTIZATION Finally, we apply EvoPress to the more challenging task of non-uniform neural network quantization, where the widely adopted baseline is uniform per-layer quantization (Frantar et al., 2022; Lin et al., 2023; Chee et al., 2023). However, one could expect that different layers exhibit varying sensitivity to quantization, similar to the behavior observed with unstructured sparsity. Furthermore, most quantization techniques are restricted to certain bitwdiths; for example, scalar quantization methods like GPTQ are limited to integral bitwidths, with only few of these bitwidths being practical for current hardware implementations. Therefore, non-uniform quantization is critical for achieving broader range of compression targets. 9 As baselines, we consider uniform quantization and DP search. (While OWL has also been applied to quantization, the authors found that it underperforms even relative to uniform per-layer quantization (Yin et al., 2024).) The DP search is very similar to SPDY (Frantar & Alistarh, 2022), where the goal is to minimize the Normalized Mean Squared Error (NMSE), defined as 2, where represents the original model output at layer, and ˆY the NMSE = ˆY 2 output of the compressed model. Then, the optimal compression allocation can be determined via dynamic programming (DP) approach. The full SPDY method applies second iterative random search step, which is very expensive to implement at LLM scale, and is therefore omitted. We create configurations with varying bitwidths and run EvoPress to determine the optimal configuration for the target compression ratio. 2/Y 2 Search Space. For each linear layer, we produce different configurations via GPTQ (Frantar et al., 2022) with standard group size of 128. In each step of the evolutionary search, the bitwidth of some layers is increased while the bitwidth of others is decreased. To facilitate uniform transitions between compression levels, quantization options differ by integral bits (1 bit in the following). Since different layers may have different sizes, we allow swaps only between projections with the same number of weights. Table 3: Performance of various profiles at 3 bit quantization, for PPL and avg. zero-shot accuracy. Model Method Wiki2 C4 ArcC ArcE HS PiQA WG Avg Mistral-7B-v0.3 Llama-3-8B Llama-3.1-8B Phi-3-Medium-14B Dense 4.82 Uniform 5.54 5.79 5.21 DP EvoPress Dense 5.54 7. 8.57 8.84 8.42 7.10 Uniform 12.19 29.00 7.49 DP EvoPress 15.76 20.03 12.03 Dense 5.61 8.90 Uniform 27.33 36.66 7.07 DP EvoPress 12.44 15.63 11.90 Dense 4.02 Uniform 5.18 5.72 5.09 DP EvoPress 8.31 9.05 9.71 9.00 48. 47.0 46.3 48.2 50.4 35.2 39.8 43.0 51.2 39.3 28.0 42.2 60. 55.1 54.7 56.7 79.6 77.3 77.3 78.5 80.1 66.9 72.0 76.4 81. 71.8 56.0 75.0 84.1 81.6 80.4 82.6 60.9 58.6 57.5 58.9 60. 54.0 52.9 55.4 60.0 53.2 51.6 55.9 64.0 60.8 58.4 61.0 80. 73.9 68.7 78.5 79.33 79.2 79.7 75.2 74.7 77.3 80. 76.3 68.9 76.9 81.0 78.9 78.6 79.2 70.0 69.2 71.0 66.3 66.0 67.1 72. 68.6 69.6 67.2 69.7 60.2 61.3 64.3 73.9 69.3 70.6 68.0 71. 62.2 54.5 64.2 76.2 73.2 73.6 73.5 74.7 70.0 69.1 70.8 Experimental Results. To validate the effectiveness of EvoPress, we consider the challenging problem of quantization to 3 bits and below. For this compression rate, uniform GPTQ quantization faces significant performance drops, motivating dynamic quantization bitwidth allocation. We produce quantization levels at 2, 3, 4, 5, and 6 bits and search for an optimal compression profile with respect to the fitness function. The results in Table 3 indicate that non-uniform quantization with EvoPress produces superior models compared to the baseline methods. The improvements are even more pronounced at 2.25 bit and 2.5 bit quantization, as detailed in Appendix F.1. We visualize quantization configurations found by EvoPress for Llama-3.1-8B in Appendix Figures 11 and 12. Specifically, we observe that the second and final blocks are compressed less aggressively, while the first block undergoes the highest level of compression. This contrasts with unstructured sparsity, where the first block is among the least compressed. These findings highlight that dynamic compression allocation must account for the specific compression method used, providing insight into OWLs failure to generalize and underscoring the need for automated compression allocation through EvoPress. Overall, we observe that, in this case as well, EvoPress yields significant accuracy improvements (e.g., 4.1 and 2.0 points on the zero-shot averages for Llama-3-8B and Llama-3.1-8B, respectively), compared to the uniform profile. Moreover, the improvement over the next-best method is always significant, both in terms of perplexity and zero-shot accuracy."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We have presented EvoPress, unified optimization framework for non-uniform compression. EvoPress is based on new provably-convergent evolutionary search algorithm with low sample and iteration complexity, that is especially well-suited to the loss landscapes arising in LLM compression. Specifically, we have shown that EvoPress can converge extremely fast to accurate configurations for various non-uniform LLM compression problems, and is also fast to execute in practice. We also emphasize the breadth of our study, our method was implemented and tested on three different compression approaches, relative to prior work which largely focused on single application. Experimental results showed that EvoPress consistently outperforms prior dynamic compression approaches, across all compression types, with large gaps at medium to high compression. Limitations. One interesting direction we did not investigate is the possibility of combining different compression approaches into the same search space. This would require changes to our switch mutation strategy, but should be feasible in most cases. Second, we did not investigate finer-grained structured pruning (i.e., removing rows and columns from the weight matrices), as it usually requires extensive retraining to recover accuracy. We plan to investigate this in future work. Finally, we plan to extend our quantization results, to show end-to-end speedups in the context of an inference engine supporting multiple compressed formats, such as vLLM (Kwon et al., 2023)."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Caio Cesar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, and Kris Kitani. N2N learning: Network to network compression via policy gradient reinforcement learning. In International Conference on Learning Representations (ICLR), 2018. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees, 2023. Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, and Jingren Zhou. Adabert: Task-adaptive bert compression with differentiable neural architecture search. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 24632469. International Joint Conferences on Artificial Intelligence Organization, 2020. doi: 10.24963/ijcai.2020/341. URL https://doi.org/10.24963/ijcai.2020/341. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Duc-Cuong Dang, Anton Eremeev, and Per Lehre. Escaping local optima with non-elitist evolutionary algorithms. Proceedings of the AAAI Conference on Artificial Intelligence, 35:1227512283, 05 2021. doi: 10.1609/aaai.v35i14.17457. 11 Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. Benjamin Doerr and Leslie Ann Goldberg. Drift analysis with tail bounds. In Robert Schaefer, Carlos Cotta, Joanna Kołodziej, and Gunter Rudolph (eds.), Parallel Problem Solving from Nature, PPSN XI, pp. 174183, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. ISBN 978-3-64215844-5. Benjamin Doerr, Carola Doerr, and Johannes Lengler. Self-adjusting mutation rates with provIn Proceedings of the Genetic and Evolutionary Computation ably optimal success rules. Conference, GECCO 19, pp. 14791487, New York, NY, USA, 2019. Association for ComISBN 9781450361118. doi: 10.1145/3321707.3321733. URL https: puting Machinery. //doi.org/10.1145/3321707.3321733. Chenhe Dong, Guangrun Wang, Hang Xu, Jiefeng Peng, Xiaozhe Ren, and Xiaodan Liang. Efficientbert: Progressively searching multilayer perceptron via warm-up knowledge distillaIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language tion. Processing, pp. 14241437. Association for Computational Linguistics, 2021. URL https: //aclanthology.org/2021.emnlp-main.108. Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024. Elias Frantar and Dan Alistarh. SPDY: Accurate pruning with speedup guarantees. arXiv preprint arXiv:2201.13096, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628. Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A. Roberts. The unreasonable ineffectiveness of the deeper layers, 2024. Jun He and Xin Yao. study of drift analysis for estimating computation time of evolutionary algorithms. Natural Computing: An International Journal, 3(1):2135, March 2004. ISSN 15677818. doi: 10.1023/B:NACO.0000023417.31393.c7. URL https://doi.org/10.1023/ B:NACO.0000023417.31393.c7. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: AutoML for model compression and acceleration on mobile devices. In European Conference on Computer Vision (ECCV), 2018. Mario Alejandro Hevia Fajardo and Dirk Sudholt. Self-adjusting population sizes for non-elitist evolutionary algorithms: why success rates matter. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO 21, pp. 11511159, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383509. doi: 10.1145/3449639.3459338. URL https://doi.org/10.1145/3449639.3459338. Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Seffi Naor, and Daniel Soudry. Accelerated sparse neural training: provable and efficient method to find N:M transposable masks. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 12 Jansen and Wegener. The analysis of evolutionary algorithmsa proof that crossover really doi: 10.1007/ can help. Algorithmica, 34(1):4766, September 2002. s00453-002-0940-2. URL https://doi.org/10.1007/s00453-002-0940-2. ISSN 0178-4617. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Daniel Johannsen. Random Combinatorial Structures and Randomized Search Heuristics. PhD thesis, Universitat des Saarlandes, 2010. Joost Jorritsma, Johannes Lengler, and Dirk Sudholt. Comma selection outperforms plus selection on onemax with randomly planted optima. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO 23, pp. 16021610, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701191. doi: 10.1145/3583131.3590488. URL https://doi.org/10.1145/3583131.3590488. Marc Kaufmann, Maxime Larcher, Johannes Lengler, and Xun Zou. Self-adjusting population In Parallel Problem Solving from Nature PPSN sizes for (1, λ)-ea on monotone functions. XVII: 17th International Conference, PPSN 2022, Dortmund, Germany, September 1014, 2022, Proceedings, Part II, pp. 569585, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3031-14720-3. doi: 10.1007/978-3-031-14721-0 40. URL https://doi.org/10.1007/ 978-3-031-14721-0_40. Stefan Kern, Sibylle D. Muller, Nikolaus Hansen, Dirk Buche, Jiri Ocenasek, and Petros Koumoutsakos. Learning probability distributions in continuous evolutionary algorithms comparative review. Natural Computing: An International Journal, 3(1):77112, March 2004. ISSN 15677818. doi: 10.1023/B:NACO.0000023416.59689.4e. URL https://doi.org/10.1023/ B:NACO.0000023416.59689.4e. Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened llama: simple depth pruning for large language models, 2024. Eldar Kurtic, Elias Frantar, and Dan Alistarh. Ziplm: Inference-aware structured pruning of language models. Advances in Neural Information Processing Systems, 36, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Johannes Lengler. Drift analysis. In Theory of Evolutionary Computation, pp. 89131. Springer, 2020. Johannes Lengler, Leon Schiller, and Oliver Sieberling. Plus strategies are exponentially slower for planted optima of random height. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO 24, pp. 15871595, New York, NY, USA, 2024. Association for Computing ISBN 9798400704949. doi: 10.1145/3638529.3654088. URL https://doi. Machinery. org/10.1145/3638529.3654088. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations (ICLR), 2021. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: arXiv preprint Activation-aware weight quantization for llm compression and acceleration. arXiv:2306.00978, 2023. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. In Advances in Neural Information Processing Systems, 2023. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect, 2024. 13 Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Pavlo Molchanov, Jimmy Hall, Hongxu Yin, Jan Kautz, Nicolo Fusi, and Arash Vahdat. Lana: latency aware network acceleration. In European Conference on Computer Vision, pp. 137156. Springer, 2022. Andre Opris, Johannes Lengler, and Dirk Sudholt. tight o(4k/pc) runtime bound for (µ+1)ga on jumpk for realistic crossover probabilities. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO 24, pp. 16051613, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704949. doi: 10.1145/3638529.3654120. URL https://doi.org/10.1145/3638529.3654120. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, 2021. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381. Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari. Weight subcloning: direct initialization of transformers using larger pretrained ones, 2023. Sandeep Tata and Jignesh Patel. PiQA: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024. Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat: In Proceedings of the Hardware-aware transformers for efficient natural language processing. 58th Annual Meeting of the Association for Computational Linguistics, pp. 76757688. Association for Computational Linguistics, 2020. URL https://aclanthology.org/2020. acl-main.684. Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu. Nas-bert: Taskagnostic and adaptive-size bert compression with neural architecture search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 19331943. ACM, 2021. URL https://dl.acm.org/doi/10.1145/3447548.3467404. Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pp. 1187511886. PMLR, 2021. Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, Ajay Jaiswal, Mykola Pechenizkiy, Yi Liang, Michael Bendersky, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): missing secret sauce for pruning llms to high sparsity, 2024. Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models. In Proceedings of the 2021 International Joint Conference on Natural Language Processing, pp. 51465157. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021. ijcnlp-main.428. 14 Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can maIn Anna Korhonen, David R. Traum, and Lluıs M`arquez chine really finish your sentence? (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pp. 4791 4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p19-1472."
        },
        {
            "title": "CONTENTS",
            "content": "1 Introduction 2 Related Work 3 Method 4 Experiments"
        },
        {
            "title": "4.3 Application 3: Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "5 Conclusion Convergence Proof of EvoPress A.1 Warm-Up Argument for Single Offspring . . . . . . . . . . . . . . . . . . . . A.2 The Main Argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evolutionary Search Parameter Ablations B.1 Mutation Rate (Depth Pruning) . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Multi-Step Selection (Unstructured Sparsity) . . . . . . . . . . . . . . . . . . . . B.3 Fitness Environment (Quantization) . . . . . . . . . . . . . . . . . . . . . . . . . Hyperparameter Setting Additional Depth Pruning Results D.1 Full Results . . . . . . D.2 Practical Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Locality of Dropped Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Correlation of Scores with Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . Additional Unstructured Sparsity Results E.1 50% and 60% Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Sparsity Profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Quantization Results F.1 2.25 Bit and 2.5 Bit . . F.2 Practical Convergence F.3 Quantization Profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1 3 6 7 8 9 11 17 18 23 23 24 25 26 26 27 28 30 30 31 32"
        },
        {
            "title": "A CONVERGENCE PROOF OF EVOPRESS",
            "content": "A.1 WARM-UP ARGUMENT FOR SINGLE OFFSPRING The overall goal of this section is to prove Theorem 1. As the main argument is quite complex, relying heavily on stochastic drift analysis, we begin with warm-up, namely by presenting simpler proof for the restricted case where λ = 1. Unlike the practical application of Algorithm 1, this section assumes that each fitness evaluation returns the exact, or true, fitness value, ignoring any noise introduced by minibatching. Additionally, our results hold for any initialization. To align with standard notation in the runtime analysis of evolutionary algorithms, we will count generations starting from zero (i.e., using 0-based indexing). Theorem 2 (Single offspring). Let n, with and consider the (1 + 1)-EA with levelswitch mutation. Then any linear fitness function : {x {0, 1}n, x1 = k} is optimized in expected O(k (n k)) generations. Proof. Let Rn be the weights associated with the linear function such that (x) = (cid:80)n i=1 xi wi. To derive an upper bound, we can assume that no two weights are equal2. Furthermore, assume without loss of generality that these weights are sorted increasingly, meaning w1 < w2 < ... < wn, and that (n k), as the other case follows from symmetry. Since is defined on the bit strings with exactly 0s its unique optimum is now given by xopt = 0k1nk. Denote by x(t) the search point at step and let = inf{t 0 x(t) = xopt} be the number of generations required until the optimum is found. Define (t) = (cid:80)k of the search point at step t. We observe the following: j=1 x(t) as the random variable that captures the number of 1s in the first bits 1. (t) = 0 x(t) = xopt; 2. (t) is non-increasing; 3. (t) (t+1) 1; 4. (0) = (cid:80)k j=1 x(0) . It follows that given the initial search point x(0) we can decompose into = (cid:80)k stages T1, T2, ..., Ts, where Tj = inf({t 0 (t) = 1}) inf({t 0 (t) = j}) captures the number of generations spent at stage j. By the linearity of expectation, we have j=1 x(0) E[T (0) = s] = (cid:88) j=1 E[Tj]. It remains to bound the expected time spent at each stage. Each offspring is generated by copying the parent, selecting 1-bit uniformly at random, selecting 0-bit uniformly at random, and finally flipping both bits. At stage exactly of the 0-bits are among the last positions and exactly of the 1-bits are among the first positions. Hence, j2 out of the total k(n k) (1-bit position, 0-bit position)-pairs advance the optimization to the next stage, yielding P[X (t+1) = 1 (t) = j] = j2 k(n k) . Therefore, Tj Geometric( k(nk) ) and E[Tj] = k(n k) j2 . 2Formally, this can be shown using stochastic domination, which involves coupling the potentials in both cases and proving that, given the same randomness, one is always at least as large as the other. 17 To obtain an upper bound, we can make worst-case assumption by setting X(x(0)) = k. We conclude E[T ] E[T (0) = k] = (cid:88) j=1 E[Tj] = k(n k) (cid:88) j=1 1 j2 O(k(n k)). Discussion. Observe that, under the assumption that the probability of initializing at the optimum is sufficiently small, the proof is tight up to constant factor of 2. It is important to note that the above proof relies on the key assumption that whenever one of the j2 good pairs is selected during mutation, the resulting offspring is the fittest among all candidates. This condition holds naturally when there is only single offspring, as the offspring produced by flipping one of the j2 pairs will have higher fitness than the parent. However, in the case of multiple offspring, this approach breaks down, as an offspring produced by flipping one of the j2 good pairs might still have lower fitness than another offspring that was not generated by flipping one of these j2 good pairs. A.2 THE MAIN ARGUMENT Drift analysis, originally developed to study all kinds of random walks and Markov chains, has become the most widely used technique for analyzing the runtime of evolutionary algorithms in recent years. It works by first defining potential function (t) that measures the progress over each step of the optimization. By estimating how this potential changes at each step in expectation, i.e., computing the drift in (t), one can then make probabilistic statements about the number of steps required until the potential reaches certain threshold, also called the hitting time. To this end, variety of drift theorems have been established, two of which will be employed in our proof. For more thorough introduction to drift analysis, we refer to Lengler (2020). First of all, we will utilize the the Multiplicative Drift Theorem, more specifically tail bound introduced by Doerr and Goldberg, which is applicable when the potential decreases by constant fraction in each step. Theorem 3 (Multiplicative Drift, Tail Bound (Doerr & Goldberg, 2010)). Let (X (t))t0 be sequence of non-negative random variables over finite state space R+ 0 . Assume that (0) and let be the random variable that denotes the first point in time for which (t) a, for some b. Suppose that there exists δ > 0 such that for all < , E[X (t) (t+1) (t)] δX (t) Then, P[T > + log(b/a) δ ] et. Additionally, we will employ Johannsens Variable Drift Theorem. This theorem provides more flexibility compared to the Multiplicative Drift Theorem, as it can be applied when the drift is bounded by any increasing function of the potential. This often occurs naturally, as optimization typically becomes more difficult approaching the optimum. Theorem 4 (Variable Drift Theorem (Johannsen, 2010)). Let (X (t))t0 be sequence of nonnegative random variables over finite state space R+ 0 . Let smin := min(S {0}), let := inf{t 0 (t) = 0}, and for let (t)(s) := E[X (t) (t+1) (t) = s]. If there is an increasing function : R+ R+ such that for all {0} and all 0, then (t)(s) h(s), E[T ] smin h(smin) + (cid:34)(cid:90) (0) smin (cid:35) dσ , 1 h(σ) where the expectation on the latter term is over the random choice of (0). 18 We will first prove an auxiliary lemma, which will play central role in bounding the drift. For this purpose, we define an inversion in bit string {0, 1}n as pair of indices (i, j) such that < and xi > xj. The distance between these indices, i, will be referred to as the spread of this inversion. Lemma 1. Let {0, 1}n be an arbitrary bit string with 0-bits and denote by the number of inversions in x. Then, the average spread of these inversions is at least s/16. Proof. Consider the bit string 1nk containing all 1-bits of x. We can now generate an arbitrary bit string {0, 1}n with 0-bits and inversions by adding 0-bits in such way that inversions are generated. Observe that adding 0-bit after the jth 1-bit results in exactly additional inversions, regardless of the other 0-bits. This means that the order in which the 0-bits are added does not affect the outcome. We proceed by case distinction depending on how the inversions are generated. Case 1: at least s/2 inversions are generated by adding 0-bits after the sth 1-bit. For each 0-bit that is added after the at least sth 1-bit, at least half of the resulting inversions have spread s/2. Consequently, this implies that there are at least s/4 inversions having spread at least s/2 in total. Case 2: fewer than s/2 inversions are generated by adding 0-bits after the sth 1-bit. It follow that more than s/2 inversions are generated by adding 0-bits not after the min(nk, s)th 1-bit. Observe that each 1-bit can participate in at most inversions with spread at most j. More specifically, each 1-bit can be part of at most s/4. Because s) 1-bits, at most s/4 all of the s/2 inversions that are added contain one of the first min(n k, s/4. Therefore, we conclude that the average spread of these inversions can have spread at most of all inversions must be at least s/4 inversions with spread at most s/16. We continue to prove the final result. Proof of Theorem 1 Proof. As in the proof of Theorem 2 let Rn represent the weights associated with linear function of the form (x) = (cid:80)n i=1 xi wi. To establish an upper bound, we can again assume that no two weights are equal. Additionally, without loss of generality, assume that the weights are ordered in increasing value, i.e., w1 < w2 < < wn, and that k, as the other case follows by symmetry. Let x(t) denote the search point at step t, and define = inf{t 0 x(t) = 0k1nk} as the number of generations required to reach the optimal solution. Consider the potential function (t) = (cid:88) (1 x(t) ) i= (k + 1) 2 , which captures the number of inversions at step t. Since xopt = 0k1nk is the only bit string with 0-bits without inversions, we have (t) = 0 if and only if x(t) = xopt. At the same time, no bit string with 0-bits has more than k(n k) inversions, hence, (t) k(n k) at all times. During mutation, each of the λ offspring is generated independently by copying the parent x(t), choosing uniformly at random one of the 1-bits, choosing uniformly at random one of the 0-bits and finally flipping both bits. This flipping can also be viewed as switching both bits, so that bits move across the search point in consecutive generations. We will use this abstraction in later step of the proof. As we assume the weights to be ordered increasingly, an offspring is fitter than its parent if and only if the chosen 1-bit was to the left of the chosen 0-bit, meaning, the chosen pair during mutation was 19 an inversion. Since there are k(n k) possible pairs in total, we have for each offspring y1, ..., yλ P[f (yj) > (x(t)) (t) = s] = k(n k) . At the same time, switching two bits corresponding to an inversion decreases the number of inversions by the difference in their positions, which we call the spread of an inversion. This implies that any offspring fitter than its parent must have fewer inversions than its parent and therefore, (t+1) (t) for all t. Note that we cannot make the same statement about the entire group of offspring, meaning, the fittest offspring is not guaranteed to have the fewest inversions. Since (t) is non-increasing we can decompose into the number of steps required until for the first time the current search point x(t) has at most k(nk) inversions and the number of steps required from there until the optimum is found. By linearity of expectation λ where and E[T ] = E[T1] + E[T2], T1 = inf{t 0 (t) k(n k) λ } T2 = inf{t 0 (t) = 0} inf{t 0 (t) k(n k) λ }. In the remainder of this proof we will demonstrate that each of these two phases requires only an expected O(k(n k)/λ) generations. We begin by bounding the expected number of steps until the search point has at most k(nk)/λ ins versions. As computed previously, single offspring is fitter than its parent with probability k(nk) . Since any fitter offspring has fewer inversion than its parent, the potential decreases in given step, if and only if, at least one of the offspring is fitter. By using that each offspring is generated independently and that k(nk) P[X (t+1) < (t) (t) = s] = 1 (1 λ for this phase we get that k(n k) )λ 1 λs k(nk) 1 e1. This means, in phase 1 we have constant probability of decreasing the potential every step. However, the resulting constant drift only provides an upper bound of O(k(n k)) via the Additive Drift Theorem (He & Yao, 2004). Improving this constant drift bound is challenging because we must establish lower bound on the expected reduction in the number of inversions, given the existence of fitter offspring. The number of inversions in an offspring is not independent of its fitness, and there is no guarantee that fitter offspring will have fewer inversions than less fit one. This issue is mitigated when there is only single fitter offspring (as demonstrated in the proof of phase 2), but it becomes problematic when multiple offspring are fitter than the parent with high probability. For example, consider the bit string 11010110001 with corresponding weights w1 = 1, w2 = 1002, w3 = 1003, ..., w112 = 1112. If λ is reasonably large it becomes very likely that at least one of the children will have the first 1-bit chosen in mutation. This offspring is guaranteed to be the fittest one, but at the same time (assuming the chosen 0-bit is not the last one) it decreases the number of inversions very little compared to sampling one of the inversions for mutation uniformly at random. We will resolve this difficulty by separate drift argument. Let BC be the event that, within the next steps, the number of inversions in x(t) falls below the threshold of 2C 1 e1 k(n k) λ k(n k) λ . Here, is chosen such that λ log(n) . If we can demonstrate that BC occurs with probability 8 of at least some constant, then the proof of the first phase is established, as BC is expected to occur after constant number of repetitions. 20 Henceforth, we will implicitly condition on k(nk)/λ, since otherwise, the conclusion follows immediately. By the Chernoff bound over round events, the probability that the potential decreases at most k(n k) λ times within the next 2C 1 e1 rounds is sub-constant. We will condition on the event that the potential decreases at least k(n k) λ k(n k) λ times, and from now on, we will only consider such potential-reducing generations. If we regard mutation as swapping the 1-bit with the 0-bit, we can enumerate all 0-bits from 1 to and denote by ij the current position of the jth 0-bit, which will be referred to as 0j. Note that this enumeration stays fixed across generations, meaning that the relative order can change and 0j is not necessarily the jth 0-bit in x(t). Now define (t) = 1 + ij (cid:88) l=1 xt as the random variable that captures the number of 1-bits before 0j plus one, or in other words, one plus the number of inversions this specific 0-bit is part of. Let Sj denote the event that the fittest offspring was generated by mutation that selected 0j and this offspring is fitter than the parent. We continue to show that E[Z (t+1) (t) = s, Sj] 2 . We achieve this by systematically revealing the randomness in each generation. First, uncover which 0-bit flip produced the fittest offspring3. Assume this bit is 0j. Next, reveal all offspring that were generated by flipping other 0-bits than 0j. Let be the number of offspring that were not uncovered yet, i.e., the number of offspring where 0j was switched. Now enumerate all 1-bits to the left of 0j in x(t) from right to left (here, relative order matters). Let be the smallest integer such that when switching the lth 1-bit to the left of 0j with 0j the resulting offspring of x(t) has higher fitness than all λ previously uncovered offspring. Denote by Dl the corresponding event. Such must exists, since we condition on the event that some offspring with bit 0j flipped (switched) is the fittest among all offspring. Because the weights are sorted increasingly it must hold that switching the + 1th 1-bit with 0j will also result in an offspring with higher fitness than the other λ offspring, while switching the 1th 1-bit with 0j will result in an offspring with lower fitness than the other λ offspring. Next, uncover all offspring where bit 0j was switched with one the first up to (l 1)th bit to the left of 0j. Let denote the number of yet uncovered offspring. Now each of the remaining offspring is generated by flipping 0j with one of the lth to (s 1)th 1-bits to the left of 0j. Observe that the fittest among them will be the one with the leftest 1-bit chosen. Therefore, E[Z (t+1) (t) = s, Sj, Dl, offspring not uncovered] = (cid:20) (cid:21) , max i=1,...,m Ui where Ui Uniform(l, 1). Given that we are conditioning on Sj, we know that the fittest offspring was produced by flipping 0j, which implies 1. As 1 it follows that E[Z (t+1) (t) = s, Sj] s/2. Denote by ˆTj the number of steps required until Zj reaches 1, only counting steps where Zj is decreased. Using tail bound for the Multiplicative Drift Theorem (Theorem 3) we have that P[ ˆTj > 2(log(n) + log(n k))] 1 . 3More precisely, we must uncover which 0-bit flip resulted in the offspring selected during the selection process. This accounts for scenarios where multiple offspring have the same highest fitness, in which case one of the fittest candidates is typically chosen uniformly at random. As the occurrence of multiple equally fit offspring is mere technicality, we have largely omitted further discussion of this case. As < (n k) we conclude by union bound that with probability at least 1/2 each potential Zj will reach 1 within at most 4 log(n) steps. Therefore, with probability at least 1/2, after 4k log(n) generations where some offspring is fitter than the parent, there must be 0 inversions in x(t). Note that in practice, there will not actually be 0 inversions in xt, as the condition k(n k)/λ is violated earlier, leading the optimization process to enter the second phase. Using the fact that λ 8 log(n) and n/2 we obtain 4k log(n) 8k(n k) log(n) k(n k) λ . Finally, as the probability of having less than Ck(n k)/λ successful generations in the considered time period is sub-constant, we conclude via another union bound that there exists constant such that event BC occurs with probability at least 1/C . Consequently, we have E[T1] k(n k) λ (cid:18) k(n k) λ (cid:19) . To compute E[T2] we first bound the probability that exactly one of the generated offspring is fitter than the parent. Denote by Ai = (cid:110)(cid:12) (cid:12) (cid:12) (cid:110) {1, . . . , λ} (yj) > (x(t)) (cid:111)(cid:12) (cid:111) (cid:12) (cid:12) = the event that exactly of the offspring are fitter than the parent x(t). As shown earlier, the probability k(nk) , where represents the number of that given offspring is fitter than its parent is exactly inversions in x(t). Given that each offspring is generated independently, we have for k(nk)/λ P[A1 (t) = s] = λ λ λ k(n k) k(n k) k(n k) (cid:18) (cid:18) 1 k(n k) 1 k(n k) (cid:19)λ1 (cid:19) k(nk) 1 1 . Lemma 1 indicates that when selecting an offspring uniformly at random from all those with higher fitness than the parent (i.e., those generated by flipping an inversion), the expected number of invers/16 fewer than in the parent. We can now reveal the randomness sions in that offspring is at least in two steps. First, we only uncover how many of the generated offspring are fitter than the parent. Given that there is only single fitter offspring, i.e., conditioned on A1, we then uncover its number of inversions. Clearly, this single fitter offspring is now sampled uniformly at random from all offspring with higher fitness than x(t); thus, for k(n k)/λ (t)(s) = E[X (t+1) (t) (t) = s] = λ (cid:88) k= E[X (t+1) (t) (t) = s, Ak] P[Ak (t) = s] E[X (t+1) (t) (t) = s, A1] P[A1 (t) = s] 16 λ k(n k) 1 . 22 Finally, applying Johannsens Variable Drift Theorem (Johannsen, 2010) (Theorem 4) yields E[T2] 16e k(n k) λ + 16e k(n k) λ (cid:18) k(n k) λ (cid:32) 1 + (cid:19) . (cid:35) dσ (cid:34)(cid:90) (0) 16e 1 (cid:90) k(nk) λ k(n k) λσ3/2 (cid:33) 1 σ3/2 dσ"
        },
        {
            "title": "B EVOLUTIONARY SEARCH PARAMETER ABLATIONS",
            "content": "B.1 MUTATION RATE (DEPTH PRUNING) The mutation rate plays crucial role in balancing exploration and exploitation. higher mutation rate allows for broader exploration of the search space; however, this space grows exponentially with the number of mutations. As result, when trying to approach the optimum in terms of Hamming distance, the proportion of good offspring decreases significantly with an increasing mutation rate. Consequently, in smooth fitness landscape, we expect significantly faster optimization with lower mutation rate. This intuition for the mutation rate is fundamentally different from that of learning rate in continuous optimization. To provide some mathematical intuition, consider optimizing over the 200-dimensional hypercube {0, 1}200, where the current search point is x(t) = 0200 and the global optimum is xopt = 1200180. For this illustration we use mutation operator that randomly selects subset of bits to flip. Flipping bits corresponds to selecting bitstring from the kth Hamming layer of x(t) = 0200 uniformly at random, where the kth Hamming layer consists of all bitstrings with Hamming distance of from x(t). Similarly, the Hamming ball of radius includes all bitstrings with Hamming distance at most k. Assume that any bitstring closer to the optimum in terms of Hamming distance has higher fitness than our current search point4. The probability of improving the fitness via mutation of bits equals the fraction of points in the kth Hamming layer of x(t) that are also in the Hamming ball of radius 19 around xopt. When the current search point is reasonably close to the optimum, this ratio is maximized for = 1. For the described setting, we can compute the probabilities of each event Ak, where Ak represents the event that mutating bits of x(t) results in decrease in the Hamming distance from xopt. These probabilities are given by: P[A1] = P[A3] = P[A5] = (20 = 0.1 (20 1 ) (200 1 ) 3 )+(20 2 )(180 1 ) (200 3 ) 5 )+(20 4 )(180 1 )+(20 (200 5 ) (20 0.0269 3 )(180 2 ) P[A2] = P[A4] = 0.0076 P[A6] = (20 0.0095 (20 2 ) (200 2 ) 4 )+(20 3 )(180 1 ) (200 4 ) 6 )+(20 5 )(180 1 )+(20 (200 6 ) (20 0.0032 4 )(180 2 ) 0.0010 Note that accounting for the potentially greater Hamming distance gained for higher mutation rates (i.e., calculating the drift in the Hamming distance) has only marginal effect. This is because for an odd number of mutations, most of the conditional probability mass is concentrated on the case where the Hamming distance is reduced by just one bit. Similarly, for an even number of mutations, most of the conditional probability mass is concentrated on cases where the reduction in the Hamming distance is only two bits. The advantage of low mutation rate becomes even more pronounced as the search process nears the optimum. For instance, when the Hamming distance between x(t) and xopt is 5, mutating single bit results in 16-fold greater drift in Hamming distance compared to any other mutation rate. 4While this assumption does not hold in practice, it serves as useful intuition in reasonably smooth fitness landscape. 23 To study the empirical impact of the mutation rate on our search process, we tested various distributions from which the number of mutations is sampled. Table 4 illustrates the effects of these distributions for selecting the optimal twelve blocks to drop for Mistral-7B-v0.3. The results confirm our intuition: higher mutation rates generally reduce performance. However, sampling from the minimum of two uniform distributions ensures reasonably high probability of choosing low number of mutations. These offspring, with fewer mutations, then drive the optimization process, yielding comparably lower performance drops. Conversely, when we eliminate this sampling and instead use high, constant mutation rate, we lose the locality that is crucial for evolutionary algorithms, leading to significant drop in performance. Table 4: Effect of varying the distribution determining the number of mutations. Number of Mutations Wiki2 C4 FW min(U1, U2), U1, U2 (1, 3) min(U1, U2), U1, U2 (1, 7) min(U1, U2), U1, U2 (1, 15) 1 3 7 15 17.52 21.49 18.65 18.12 22.09 25.06 27.01 21.60 16.79 22.41 17.65 22.67 17.63 21.12 16.33 25.42 19.25 26.52 19.65 28.19 22.03 low mutation rate carries the risk of getting trapped in local optima. However, as discussed in Section 3, we expect dynamic model compression to exhibit smooth fitness landscape with few local optima. Moreover, fitness evaluations in our context are relatively expensive. Increasing the mutation rate would only be beneficial if the smaller search space had already been thoroughly explored. In our case, though, even small neighborhood cannot be fully explored within feasible time frame. widely used strategy for balancing the advantages and disadvantages of different mutation rates involves self-adjusting mutation rates, which have been shown to be effective both theoretically and in practice (Kern et al., 2004; Doerr et al., 2019). These methods decrease the mutation rate when progress is relatively easy, and increase it when progress becomes difficult, offering greater chance of escaping local optima. B.2 MULTI-STEP SELECTION (UNSTRUCTURED SPARSITY) We will use this subsection to ablate the impact of hyperparameters for the multi-step selection, namely, the number of tokens and survivors. As discussed earlier in Section 4.2, the default hyperparameters we chose for our unstructured sparsity search were quite conservative. The following experiments will be conducted based on the super-fast version, which uses two steps of selection. It first generates 16 offspring, evaluates them on 512 tokens, and compares only the fittest one with the parent on another 8192 tokens. Table 5 shows the impact of adapting the number of tokens in the first selection step. Note that reducing tokens is only reasonable up to certain degree, as fitness evaluation has constant overhead independent of the number of tokens (e.g., for loading the levels). Table 6 ablates the number of offspring in each generation. All perplexities were measured after 400 generations. Table 5: Effect of varying the number of tokens in first preselection step. Offspring Stage 1: Tokens Stage 2: Tokens Wiki2 C4 FW 16 16 16 16 16 1024 512 256 128 8192 8192 8192 8192 8192 16.22 15.87 17.25 16.01 15.89 17.93 12.26 18.28 12.38 18.51 12.52 18.99 12.72 19.35 12.98 In similar vein to the discussion in Appendix B.1, the number of offspring can also be dynamically Ideally, the number of offspring should increase to the point where the computational adapted. 24 Table 6: Effect of varying the number of offspring. Offspring Stage 1: Tokens Stage 2: Tokens Wiki2 C4 FW 64 32 16 8 4 512 512 512 512 512 8192 8192 8192 8192 8192 16.35 16.65 15.87 16.37 17.87 18.27 12.36 18.22 12.44 18.27 12.38 18.74 12.64 18.97 12. effort is compensated by the number of generations, as outlined in Theorem 1. Methods such as the Self-Adjusting (1, λ)-EA have recently gained significant theoretical interest and have been shown to automatically determine ideal offspring sizes on specific problems (Hevia Fajardo & Sudholt, 2021; Kaufmann et al., 2022). Although we have not experimented with such adaptive methods, we see significant potential for future work in this area, particularly considering the multi-step selection we employ. B.3 FITNESS ENVIRONMENT (QUANTIZATION) We explored an alternative fitness function by testing perplexity as opposed to KL-Divergence. One advantage of using perplexity is the reduced memory requirement, as it does not necessitate storing the logits, which can be particularly burdensome for large vocabularies. However, perplexity relies solely on the information from the ground truth token, while KL-Divergence takes into account the entire distribution. This distinction is significant only if the selection decisions vary between the two metrics. Generally, we expect KL-Divergence to perform at least as well as perplexity; however, in many instances, their performances are similar. This observation could indicate that KL-Divergence might be using more tokens than necessary to assess fitness effectively. Although in the context of quantization KL-Divergence yielded slightly better results (Table 7, Figure 4 left), both metrics showed comparable performance when applied to unstructured sparsity (Figure 4 right). Table 7: Comparison of using KL-Divergence vs. Perplexity as fitness function. Model # Bits Method Wiki2 C4 FW Llama-3-8B Llama-2-7B Mistral-7B-v0.3 4 3 4 3 4 Uniform EvoPress (PPL) EvoPress (KL) 12.19 8.17 7.49 15.76 11.47 12.15 9.64 12.03 9.56 Uniform EvoPress (PPL) EvoPress (KL) Uniform EvoPress (PPL) EvoPress (KL) Uniform EvoPress (PPL) EvoPress (KL) Uniform EvoPress (PPL) EvoPress (KL) Uniform EvoPress (PPL) EvoPress (KL) 6.48 5.86 5.86 6.16 5.74 5.70 5.48 5.25 5.22 5.54 5.23 5.21 5.10 4.85 4. 9.50 9.46 9.44 7.96 7.90 7.87 7.10 7.09 7.07 8.57 8.45 8.42 7.87 7.86 7.84 8.46 8.23 8. 6.86 6.79 6.76 6.40 6.37 6.34 6.96 6.87 6.86 6.50 6.49 6."
        },
        {
            "title": "C HYPERPARAMETER SETTING",
            "content": "Here, we provide an overview of the hyperparameters used in our experiments. As shown in Table 8, different hyperparameters were employed for different applications due to the varying nature of their 25 Figure 4: Convergence of EvoPress for unstructured sparsity (left) and quantization (right) for different fitness functions. search spaces. Across all applications, we sampled the number of mutations from the distribution min(U1, U2) with U1, U2 Uniform(1, 3), which closely mimics the behavior of using only one mutation (see the ablation study in Appendix B.1). For Depth Pruning, where each block has only two choices and significantly fewer blocks are present compared to layers in other methods, we leveraged the insight from Theorem 1, which suggests that the number of required generations scales proportionally to k(nk), where represents the number of removed blocks and the total number of blocks. For Unstructured Sparsity, the search space is considerably larger, with more than 10 choices per layer5. As result, more generations are necessary to converge because each generation only makes small improvements in terms of Hamming distance from the optimum. For Quantization, the search space is somewhat smaller since fewer natural compression levels are available. However, the fitness landscape is less smooth, with significantly larger step sizes in compression levels, motivating the use of higher number of tokens. For all these applications, we adopted conservative approach for the number of generations to better understand convergence. In practice, we need significantly fewer generations to converge, as displayed in Figure 3, Figure 5 and Figure 10. Additionally, we showed super-fast version (in terms of time per iteration) that is significantly more lightweight. Table 8: Employed hyperparameters for different applications. Application Generations Offspring Survivors (1) Tokens (1) Survivors (2) Tokens (2) Survivors (3) Tokens (3) Depth Pruning k(n k)/1.5 Unstr. Sparsity Quantization Super-Fast 400 150 32 64 128 16 2 8 16 1 2048 2048 2048 512 1 2 4 1 32768 16384 16384 8192 N/A 1 1 N/A N/A 65536 131072 N/A"
        },
        {
            "title": "D ADDITIONAL DEPTH PRUNING RESULTS",
            "content": "D.1 FULL RESULTS Here, we present our additional results for depth pruning experiments on Mistral-7B-v0.3  (Table 9)  , Llama-2-7B  (Table 10)  , Llama-3-8B  (Table 11)  , and Llama-3.1-8B  (Table 12)  . Across all levels of sparsities, EvoPress consistently outperforms previous methods. Additionally, Table 9 includes results where only entire transformer blocks are removed by EvoPress, showcasing that the significant gains are not primarily due to this relaxation, and that our method performs better than baselines even when dealing with this coarser search space. 5If necessary, the step size can be increased and the number of compression levels reduced with minimal impact on performance. We opted for higher number of compression levels for fair comparison with OWL. 26 Table 9: Depth pruning of Mistral-7B-v0.3. Sparsity 0% 12.5% 25% 37.5% 50% Sparsity 0% 12.5% 25% 37.5% 50% Method Dense EvoPress EvoPress (Attn.+MLP) ShortGPT Cosine Similarity (Window) Weight Subcloning Shortened Llama EvoPress EvoPress (Attn.+MLP) ShortGPT Cosine Similarity (Window) Weight Subcloning Shortened Llama Wiki2 4.82 6.06 6.33 7.19 7.19 7.19 6. 8.66 9.46 43.26 33.75 43.26 14.94 C4 7.72 9.00 9.44 10.18 10.18 10.18 9.71 12.04 13.02 40.16 54.07 40.16 19.30 FW 6.41 7.42 7.80 8.46 8.46 8.46 7.94 9.92 10.59 29.54 36.26 29.54 14.73 EvoPress EvoPress (Attn.+MLP) ShortGPT 16.90 21.60 17.52 21.62 18.97 25.17 2898.98 2722.66 981.99 Cosine Similarity (Window) 1034.09 2471.86 1050.56 2898.98 2722.66 981.99 486.15 442.09 440.20 Weight Subcloning Shortened Llama EvoPress EvoPress (Attn.+MLP) ShortGPT 43.23 54.15 61.75 108.91 69.07 99.74 2422.72 2134.92 1083.51 Cosine Similarity (Window) 3411.47 1934.16 1740.91 2422.72 2134.92 1083.51 5241.76 3595.71 1953.14 Weight Subcloning Shortened Llama Table 10: Depth pruning of Llama-2-7B. Method Dense EvoPress ShortGPT Cosine Similarity (Window) Weight Subcloning ShortenedLlama EvoPress ShortGPT Cosine Similarity (Window) Weight Subcloning Shortened Llama Wiki2 5.21 6.42 8.86 7.53 9.09 7.68 9.15 23.41 16.60 23.41 13. C4 6.93 8.60 10.78 9.82 11.06 10.44 11.46 30.30 21.04 30.30 14.08 FW 6. 7.54 9.30 8.51 9.60 8.57 9.69 21.16 17.37 21.16 11.81 EvoPress ShortGPT 17.98 70.94 Cosine Similarity (Window) 192.07 70.94 35.37 Weight Subcloning Shortened Llama 18.91 63.51 212.60 63.51 26. 15.53 54.07 151.10 54.07 20.37 EvoPress ShortGPT 33.57 180.51 Cosine Similarity (Window) 4570.15 2876.83 1861.06 180.51 68.79 Weight Subcloning Shortened Llama 48.84 226.14 42.29 171. 226.14 145.78 171.04 87.40 D.2 PRACTICAL CONVERGENCE EvoPress not only produces superior compression configurations, it also does so extremely efficiently. Figure 5 displays that the evolutionary search produces better compressed models than previous techniques in matter of minutes, with full convergence in around half an hour. 27 Table 11: Depth pruning of Llama-3-8B. Sparsity 0% 12.5% Method Dense EvoPress ShortGPT Cosine Similarity (Window) Weight Subcloning Shortened Llama Wiki2 5.54 7.72 13.21 9.54 13.21 9.42 C4 8.80 12.61 19.56 14.87 19.56 15. FW 7.62 10.15 14.25 11.64 14.25 11.57 25% 37.5% 50% EvoPress ShortGPT 15.84 13.99 22.83 5527.54 11589.93 2346.13 Cosine Similarity (Window) 5519.95 11629.61 2342.91 5527.54 11589.93 2346.13 20.81 16.28 16.59 Weight Subcloning Shortened Llama 27.56 EvoPress ShortGPT 26.77 35.70 64281.36 13836.12 3789.09 Cosine Similarity (Window) 64627.29 13890.14 3784.72 64381.36 13836.13 3789.09 37.40 61. Weight Subcloning Shortened Llama 50.20 EvoPress ShortGPT 84.99 1663.97 Cosine Similarity (Window) 2053.19 1663.97 724.86 Weight Subcloning Shortened Llama 66. 87.86 1740.04 1588.20 1116.47 694.00 1740.04 1588.20 210.30 666.41 Table 12: Depth pruning of Llama-3.1-8B. Sparsity 0% 12.5% Method Dense EvoPress ShortGPT Cosine Similarity (Window) Weight Subcloning Shortened Llama Wiki2 5.61 7.58 12.54 12.54 12.54 9.27 C4 8.90 12.24 19.21 19.21 19.21 14.80 FW 7.67 10.00 13.76 13.76 13.76 11.21 25% 37.5% 50% EvoPress ShortGPT Cosine Similarity (Window) Weight Subcloning Shortened Llama 11.59 4278.39 4278.39 4278.39 20.41 13.96 17.84 6754.92 1512.39 6754.92 1512.39 6754.92 1512.39 20. 16.12 EvoPress ShortGPT 24.98 35.77 25.93 123044.19 22071.51 6059.03 Cosine Similarity (Window) 123044.19 22071.51 6059.03 123044.19 22071.51 6059. Weight Subcloning Shortened Llama 41.34 43.53 31.00 EvoPress ShortGPT Cosine Similarity (Window) Weight Subcloning Shortened Llama 105.84 1630.11 1881.54 1630.11 454. 110.69 61.25 1680.21 1698.64 1196.63 683.24 1680.21 1698.64 153.96 309.42 D.3 LOCALITY OF DROPPED BLOCKS Prior research indicates that deeper layers, aside from the final ones, are generally less effective (Gromov et al., 2024; Men et al., 2024). Figure 6 illustrates the optimal removal configurations identified by EvoPress. For comparison, Table 13 displays the removal order of prior scoring methods. While EvoPress indeed removes some deeper layers across all sparsities, we also observe that certain shallow layers appear to be less important. Notably, two hills pattern emerges in many cases, where blocks before and after the midpoint are pruned, yet the central blocks remain intact. Meanwhile, the first two blocks are never pruned. However, in contrast to heuristic proposed by Ma et al. (2023), we find that, in some instances, it is effective to prune the final block as well. 28 Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3. Figure 6: Optimal removal configurations identified by EvoPress for different models. D.4 CORRELATION OF SCORES WITH PERPLEXITY In this experiment, we first calculated the cosine similarity and squared error for each block by comparing activations before and after the block. Next, we randomly removed subsets of blocks (excluding the first and last two) and for each configuration, computed the average cosine similarity and squared error. The results are shown in Figure 7. Initially, the average squared error exhibited negative correlation, as the ℓ2-norm of the activations increased with depth. This led to configurations with early blocks removed having small average error. To mitigate this, we normalized the activations prior to computing the squared error, which significantly improved the correlation, resulting in performance comparable to cosine similarity. However, as sparsity increased, the correlation degraded significantly for both methods, offering insight into why removal techniques based on scoring fail even at moderate levels of sparsity. The experiment was done using 131,072 tokens from the Fineweb-Edu calibration dataset. 29 Table 13: First 16 blocks in removal order of ShortGPT, Weight Subcloning and Shortened Llama on three different models. Model Method Removal Order (Left to Right) Mistral-7B-v0.3 ShortGPT 26, 25, 24, 27, 23, 22, 28, 30, 21, 29, 20, 19, 13, 17, 18, 12 Weight Subcloning 26, 25, 24, 27, 23, 28, 22, 30, 21, 29, 20, 19, 13, 17, 12, 18 10, 12, 13, 11, 08, 09, 14, 15, 07, 06, 04, 27, 24, 16, 25, 05 Shortened Llama Llama-2-7B Llama-3-8B ShortGPT 27, 25, 26, 28, 29, 24, 23, 22, 21, 30, 20, 19, 18, 17, 15, 14 Weight Subcloning 27, 25, 28, 29, 26, 24, 23, 22, 21, 19, 30, 20, 18, 17, 14, 15 11, 12, 08, 09, 10, 06, 24, 25, 07, 14, 23, 13, 22, 21, 15, 27 Shortened Llama ShortGPT 25, 26, 27, 24, 28, 23, 22, 29, 20, 21, 19, 18, 30, 17, 16, 11 Weight Subcloning 25, 27, 26, 24, 28, 23, 22, 29, 20, 21, 19, 18, 30, 17, 16, 11 10, 08, 09, 11, 26, 25, 12, 22, 24, 23, 14, 13, 28, 06, 19, 21 Shortened Llama Llama-3.1-8B ShortGPT 25, 26, 24, 27, 23, 28, 22, 29, 20, 21, 19, 18, 17, 30, 16, 10 Weight Subcloning 25, 27, 26, 24, 28, 23, 22, 29, 20, 21, 19, 18, 30, 17, 16, 10 10, 09, 11, 08, 26, 25, 12, 24, 22, 23, 14, 28, 06, 13, 19, 21 Shortened Llama Figure 7: Effect of removing random subsets of blocks for Llama-3-8B."
        },
        {
            "title": "E ADDITIONAL UNSTRUCTURED SPARSITY RESULTS",
            "content": "E.1 50% AND 60% SPARSITY In the main text, we focused on results at 70% sparsity, where the performance differences are more pronounced. However, since 50% and 60% sparsity levels are more practical and frequently referenced in the literature, we present the corresponding results in Tables 14 and 15. We have also included Llama-2-7B in these tables for legacy purposes. Even at these lower sparsity levels, EvoPress demonstrates significant improvements over uniform sparsity and consistently outperforms OWL. 30 Table 14: Performance of various sparsity profiles at 50% sparsity Model Method Wiki2 C4 ArcC ArcE HS PiQA WG Avg Mistral-7B-v0.3 Llama-2-7B Llama-3-8B Llama-3.1-8B Dense 4.82 Uniform 5.68 5.69 5.49 OWL EvoPress Dense 5.12 Uniform 6.40 6.38 6. OWL EvoPress Dense 5.54 7.72 8.93 8.94 8.70 6. 8.87 8.77 8.52 7.10 Uniform 8.05 8.13 7.63 OWL EvoPress 13.07 13.12 12.53 Dense 5.61 8.90 Uniform 8.06 8.02 7.51 OWL EvoPress 13.03 12.99 12.31 48. 43.7 43.9 45.7 43.4 41.3 41.1 41.5 50.4 43.6 43.8 43.9 51. 44.5 44.2 46.6 79.6 76.7 76.9 77.3 76.3 73.4 73.2 74.2 80. 75.7 75.8 77.5 81.4 76.7 76.5 77.7 60.9 55.7 55.4 56.5 57. 52.8 53.2 54.0 60.2 54.2 54.0 54.5 60.0 54.0 53.8 54.9 80. 78.4 78.5 78.9 78.1 75.7 76.5 76.7 79.7 76.1 75.7 76.8 80. 76.7 76.8 77.6 73.9 68.7 71.0 70.3 71.2 65.1 65.0 65.9 69. 64.8 68.8 70.2 69.6 62.4 62.9 63.2 72.6 68.6 71.7 72.2 72. 64.3 64.3 65.0 73.9 69.3 71.5 72.5 71.7 64.7 64.8 65.7 Table 15: Performance of various sparsity profiles at 60% sparsity Model Method Wiki2 C4 ArcC ArcE HS PiQA WG Avg Mistral-7B-v0.3 Llama-2-7B Llama-3-8B Llama-3.1-8B Dense 4.82 7.72 Uniform 7.78 7.50 7.08 OWL EvoPress Dense Uniform OWL EvoPress Dense 5.12 9.3 8.35 8.21 5.54 Uniform 13.86 12.37 11. OWL EvoPress 11.86 11.34 10.27 6.93 12.37 11.00 10.34 7.10 21.43 18.53 16. Dense 5.61 8.90 Uniform 13.43 12.08 10.58 OWL EvoPress 21.46 18.25 15. 48.9 38.0 38.5 40.5 43.4 35.8 36.0 37.1 50.4 35.2 38.0 39. 51.2 36.4 38.9 40.0 79.6 72.3 71.9 72.8 76.3 69.5 69.1 70. 80.1 69.7 70.3 71.9 81.4 69.7 71.1 72.5 60.9 49.4 49.6 51. 57.1 45.9 47.5 49.3 60.2 45.6 47.7 48.6 60.0 46.2 47.7 49. 80.3 75.0 75.1 76.9 78.1 72.4 73.2 74.4 79.7 72.2 72.1 74. 80.1 72.3 73.1 74.6 73.9 68.7 69.3 70.2 68.8 60.9 61.1 62. 69.0 64.8 65.9 66.2 67.6 57.9 58.4 59.8 72.6 68. 68.0 68.5 69.1 58.2 59.3 60.5 73.9 69.3 67.7 68.8 69.5 58.5 59.9 61. E.2 SPARSITY PROFILES Below, we visualize sparsity profiles determined by EvoPress and baseline approaches. Notably, EvoPress prunes the initial blocks less aggressively than the middle and later blocks. Additionally, the proj projection attains higher sparsity levels, whereas the proj projection is pruned to significantly lower sparsity on average. Although Figure 8 may suggest that OWL and EvoPress produce similar sparsity profiles, this is misleading OWL enforces uniform sparsity at block level, as their original per-layer approach underperformed (Yin et al., 2024). 31 Figure 8: Comparison of different block-level sparsity profiles for Llama-3.1-8B at 70% sparsity. Figure 9: Average sparsity per projection type for Llama-3.1-8B at 70% sparsity for EvoPress."
        },
        {
            "title": "F ADDITIONAL QUANTIZATION RESULTS",
            "content": "F.1 2.25 BIT AND 2.5 BIT In addition to the 3 bit results presented in Section 4.3, we further evaluated EvoPress under extreme quantization conditions, specifically testing it at 2.25 bit and 2.5 bit levels. As baseline, we generated 32 random configurations combining 2 bit and 3 bit layers and selected the best performing setup. The results, as shown in Table 16, demonstrate that EvoPress significantly outperforms this baseline, highlighting its ability to achieve extreme quantization levels. Table 16: Performance of EvoPress on 2.25 bit and 2.5 bit quantization Model # Bits Method Wiki2 C4 ArcC ArcE HS PiQA WG Avg Mistral-7B-v0.3 Llama-2-7B Llama-3-8B Llama-3.1-8B Phi-3-Medium 2. 2.5 2.25 2.5 2.25 2.5 2. 2.5 2.25 2.5 Best of 32 EvoPress 11.53 8.63 Best of 32 EvoPress 7.50 6.60 Best of 32 EvoPress 13.18 9.82 Best of 32 EvoPress 9.42 8.03 18.32 13. 11.76 10.40 18.19 9.93 9.01 7.33 Best of 32 149.85 432.96 43.17 EvoPress 23.93 Best of 32 EvoPress 21.65 13.93 23.92 18.15 Best of 32 259.61 181.36 33.58 EvoPress 22.75 Best of 32 EvoPress Best of 32 EvoPress Best of 32 EvoPress 35.33 11.73 14.20 10.48 8.26 7.12 37.09 19.03 18.19 14. 12.65 11.23 30.1 36.2 37.0 39.8 24.8 29.5 29.1 35.3 21.2 23. 25.1 31.7 20.7 26.7 24.1 32.2 28.9 36.2 40.5 44.1 59.6 66. 68.0 71.7 50.2 61.8 58.6 68.4 29.1 46.9 47.6 61.5 31.9 48. 48.4 63.3 46.8 62.0 69.3 75.9 44.5 49.3 51.7 54.0 40.3 46. 46.9 50.8 28.1 39.3 41.2 47.9 30.6 40.2 41.7 47.5 40.0 46. 50.3 54.1 69.4 74.2 75.0 77.1 66.8 70.3 70.1 73.9 55.6 63. 65.6 71.7 57.0 63.4 62.7 71.8 61.8 66.2 70.9 73.5 56.8 63. 63.5 65.8 56.1 59.4 62.6 64.2 49.8 56.5 56.2 64.3 51.9 55. 54.5 62.3 53.1 55.6 61.9 64.6 52.1 57.8 59.0 61.7 47.7 53. 53.5 58.5 36.8 46.0 47.1 55.4 38.4 47.0 46.3 55.4 46.1 53. 58.6 62.4 F.2 PRACTICAL CONVERGENCE Similar to unstructured sparsity, EvoPress also demonstrates rapid convergence when applied to quantization. As shown in Figure 10, the majority of improvements occur within two GPU hours, with full convergence achieved after approximately eight GPU hours. If needed, this optimization 32 time could be further shortened by tuning the hyperparameters, similarly to the super-fast version for unstructured sparsity discussed in Section 4.2. However, we observed that the convergence dynamics are less smooth compared to unstructured sparsity, likely due to the limited number of quantization levels available (practically only 2, 3, and 4 bit are used), resulting in less smooth fitness landscape. Figure 10: Convergence of EvoPress for 2.25 bit quantization on Llama-3.1-8B (left) and 3 bit quantization on Llama-3-8B (right). F.3 QUANTIZATION PROFILES In this section, we visualize quantization profile determined by EvoPress. As shown, EvoPress maintains relatively uniform quantization bitwidth allocation across the model. Interestingly, while the second and the two last blocks are less compressed, the first block undergoes significantly higher compression. This suggests that saliency does not directly equate to block importance but rather depends on the specific compression method used. Additionally, similar to the profiles for unstructured sparsity, we observe transfer of capacity to proj. Figure 11: Block-level quantization profiles for Llama-3.1-8B at 3 bit compression on average. Figure 12: Average bitwidth per projection type for Llama-3.1-8B at 3 bit compression on average."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "IST Austria",
        "Neural Magic",
        "Skoltech",
        "Yandex"
    ]
}