{
    "paper_title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
    "authors": [
        "Bin-Bin Gao",
        "Yue Zhu",
        "Jiangtao Yan",
        "Yuezhi Cai",
        "Weixi Zhang",
        "Meng Wang",
        "Jun Liu",
        "Yong Liu",
        "Lei Wang",
        "Chengjie Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP."
        },
        {
            "title": "Start",
            "content": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection Bin-Bin Gao1 Yue Zhu 2,3 Jiangtao Yan 1 Yuezhi Cai 2 Weixi Zhang 2 Meng Wang 2 Jun Liu 1 Yong Liu 1 Lei Wang 2 Chengjie Wang1, 1Tencent YouTu Lab 2Siemens Corporate Research 3Technical University of Munich 4Shanghai Jiao Tong University 5 2 0 2 5 ] . [ 1 6 2 9 9 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses training-free manner on target domains once trained on base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP. 1. Introduction Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly pixels from novel or unseen visual objects after learning single model on base or seen dataset. This is more challenging task as it requires strong generalization when facing cross-domain datasets. Meanwhile, it is more practical topic as people are more interested in fast adaptability in real-world scenarios, especially in low data regimes (i.e., few-shot and even zero-shot). For example, in medical image diagnosis and industrial visual quality inspection, it is difficult to collect large-scale dataset due to inherent scarcity and privacy protection. Recently, developing universal visual AD has attracted increasing attention because existing unsupervised ADs with either separated [8, 25, 34] or unified models [11, 48] perform poorly in unseen objects despite promising performance on seen objects."
        },
        {
            "title": "Methods",
            "content": "ZSFSOAw/o FT WinCLIP [16] AdaCLIP [6] InCtrl [54] AnomalyCLIP [53] PromptAD [23] AdaptCLIP Figure 1. Comparisons of state-of-the-arts and our AdaptCLIP. means satisfied and means not satisfied. Our method supports zero-/few-shot (ZS and FS) visual AD across different domains without fine-tuning (FT) on the target dataset. It only adds simple adapters at CLIPs input or output ends without complex token interactions, thus preserving CLIPs original ability (OA). The AdaptCLIP using only one normal image prompt achieves the best performance in image-level anomaly classification (IAUROC) and pixel-level anomaly segmentation (P-AUPR) on 12 AD benchmarks from industrial and medical domains. Moreover, the zero-shot AdaptCLIP is also significantly better than existing zero-shot and even some one-shot approaches. The detailed results are reported in Tabs. 1 and 2. Best viewed in zoom. To address this fragmentation, recent works have attempted to design universal models to recognize anomalies for unseen objects. They typically build on vision-language models (i.e., CLIP [29]) benefiting from strong generalization. WinCLIP [16] computes anomaly scores on dense patch windows. This brings large computational costs and memory burden, limiting high-resolution input or large pretrained models. AnomalyCLIP [53] learns class-agnostic prompt embeddings to align patch-wise tokens thus avoiding dense window operations. It further refines vanilla CLIP by concatenating learnable tokens to intermediate layers of CLIP. AdaCLIP [6] further integrates visual knowledge into textual prompt embeddings. However, they may destroy inherent representations of CLIP. Therefore, we want to explore whether we can achieve the same or even better AD 1 performance while maintaining the original ability. In contrast, humans perceive anomalies when an input significantly deviates from those normal patterns stored in our brains. There is evidence to support this point in neuroscience [31]. PatchCore [34] builds memory bank storing normal features and PaDiM [7] learns multivariate Gaussian distribution of normal features. At inference, anomalies are recognized by comparing input features with the memory bank or the learned distribution. However, these methods usually require certain number of normal images and thus are limited in universal (i.e., open-world) scenarios. Two recent works, i.e., InCtrl [54] and PromptAD [23], have studied how to further improve performance with few-shot normal images. However, InCtrl only considers anomaly classification, while PromptAD needs to learn new model for each class. Different from them, we want to comprehensively explore universal AD model, aiming to detect any anomalies in image-level and pixel-level from cross-domains without any training on target domains. Toward this end, we propose simple but effective universal visual anomaly detection framework, called AdaptCLIP. The philosophy of AdaptCLIP is that less and simpler could be better, and it contains three adapters designed by two key insights : First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and the corresponding normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. Our contributions are summarized as follows. We propose simple but effective universal visual anomaly detection framework based on visual-language CLIP models, which is capable of detecting any visual anomalies at imageand pixel-level from cross-domain datasets without any training on target domains. We propose visual and textual adapters, and find that they should alternately learn adaptive visual and textual representation guided by the powerful vision-language representations from CLIP models. We propose prompt-query adapter that aims to capture meta-perceptual capabilities between query image and the corresponding normal image prompt, based on their joint distribution of contextual features of the query and the aligned residual features between prompt and query. AdaptCLIP outperforms zeroand few-shot AD methods on 8 industrial and 4 medical benchmarks, as shown in Fig. 5. Meanwhile, AdaptCLIP possesses simpler adapters, fewer parameters, and competitive efficiency. 2. Related Works"
        },
        {
            "title": "Unsupervised ADs target\nsufficient normal",
            "content": "to identify anomalies given training images. Most unsupervised AD methods can be roughly grouped into three categories: embedding-, discrimination-, and reconstructionbased methods. Embedding-based methods, such as PaDiM [7], MDND [32], PatchCore [34], CS-Flow [35] and PyramidFlow [19], assume that offline features extracted from pre-trained model preserve discriminative information and thus help to separate anomalies from normal samples. Discrimination-based methods, such as CutPaste [21], DRAEM [50], and SimpleNet [25], typically convert unsupervised AD to supervised ones by introducing pseudo (synthesized) anomaly samples. Reconstructionbased ADs, such as autoencoder [2, 12, 14, 40], generative adversarial networks [27, 45, 49] and reconstruction networks [24, 33, 51], assume that anomalous regions should not be able to be properly reconstructed and thus result in high reconstruction errors since they do not exist in normal training samples. The recent knowledge distillation [4, 8, 36, 42, 43] or feature reconstruction methods [11, 46, 48, 52] train student or reconstruction network to match fixed pre-trained teacher network and achieve good balance between effectiveness and efficiency. However, all these methods are limited to recognizing anomalies of seen classes but often perform poorly on unseen classes. For novel scenario, people have to collect sufficient normal images first and then retrain model. This is inefficient and lacks the rapid adaptability required for practical applications. Zero-Shot ADs have achieved impressive performance by utilizing large vision-language models, e.g., CLIP [29]. WinCLIP [16] designs two-class textual prompts and introduces multi-scale patch windows for accurate anomaly segmentation. It brings large computational costs and memory burden, limiting high-resolution input or large pretrained models. AnomalyCLIP [53] learns class-agnostic prompt embeddings to align patch-wise tokens thus avoiding dense window operation. In addition, AnomalyCLIP refines vanilla CLIP representation by appending some learnable tokens to the middle layer of CLIP. Recently, AdaCLIP [6] and VCP-CLIP [28] utilize similar ideas and further integrate visual knowledge into textual prompt embeddings. We argue that these additional operations make models more complex and may hurt the original capabilities of CLIP. Instead of visual-language models, ACR [20] and MuSc [22] perform zero-shot AD only requiring batchlevel and full-shot testing images, but they may be limited in privacy protection scenarios. Different from these methods, we explore whether the same or even better AD performance is achieved while retaining the original ability of CLIP without any information on test data distribution. Few-Shot ADs mainly pay attention to learning or using only limited number of normal images, such as TDG [37], RegAD [15], GraphCore [44] and FastRecon [10]. Some works [9, 47] consider another few-shot setting where 2 Figure 2. The framework of AdaptCLIP, which consists of three pluggable adapters, i.e., visual adapter, textual adapter, and prompt-query adapter. First, the first two adapters alternately learn visual and textual representations for zero-shot anomaly detection (Sec. 3.2). The prompt-query adapter further learns comparison ability between query image and its corresponding normal prompt for few-shot anomaly detection (Sec. 3.3). Once trained, it can segment any anomalies providing only few-shot and even zero-shot normal image prompts. limited number of samples is given from anomaly images. The performance of these methods lags behind unsupervised ADs. Recently, few-shot AD performance has been improved significantly by visual-language models. WinCLIP+ [16] is the first work to apply CLIP models to fewshot AD, which stores normal tokens into memory bank, then retrieves the nearest token for each query token using cosine similarity, and finally computes an anomaly map using the nearest distance. InCtrl [54] further integrates multilevel information, including patch-level residual maps and image-level residual features, and prior knowledge score using two-class textual prompts, to learn holistic scoring function for anomaly classification. It does not consider pixel-level anomaly segmentation. PromptAD [23] introduces the concept of explicit anomaly margin, which mitigates the training challenge caused by the absence of anomaly training images. However, it requires re-training models when applied to target datasets. In contrast, we explore jointly optimizing anomaly classification and segmentation in unified model, which can quickly adapt to novel scenarios only given few-shot normal image prompts, not involving additional re-training. 3. Methods Problem Formulation: Our objective is to learn universal AD model that detects any anomalies from diverse domains without any training on target dataset. Thus, reasonable assumption is that there is different distribution between training and testing sets. Formally, let Dbase = {Xi, Yi, yi}N i=1 be training dataset, that consists of normal and anomalous images, Xi Rhw3 is the i-th image, and Yi Rhw and yi = {0, 1} is the corresponding anomaly mask and anomaly label, with yi = 0 indicates normal and yi = 1 signifies anomaly. The testing set novel = {Xi, Yi, yi}Nt may consist of multiple different domains with various objects and anomaly types. Here, we denote the t-th novel domain as Dt i=1. Under few-shot setting, few normal images Pc = {Xi}k i=1 are randomly drawn from each class of the target domain, where is the class index and is typically small number, e.g., = {1, 2, 4}. It is worth noting that Pc is only available during inference, and cannot be used in any way during training phase. Overview: As illustrated in Fig. 2, the visual adapter adapts patch and image tokens with fixed two-class textual prompt embeddings. The textual adapter learns two-class prompt embeddings to align with the fixed patch and image tokens. The prompt-query adapter operates in one-prompt meta-learning manner, leveraging the joint distribution of query context features and the aligned residual features between the prompt and query. In zero-shot scenario, imagelevel anomaly score and pixel-level anomaly map can be obtained using textual and visual adapters (Sec. 3.2). In few-shot scenario, anomaly score and map are derived by integrating predictions from zero-shot and prompt-query adapters (Sec. 3.3). Below we present them in detail. 3.1. Revisiting CLIP for Anomaly Detection i=1 For query image Rhw3, we feed it to visual eni Rd}hw/p2 coder F() and obtain local patch tokens {F and global image token Rd, where is patch size. WinCLIP [16] introduces two-class prompts describing normal and abnormal states. For example, photo of normal object and photo of damaged object. In practical application, one could design multiple textual descriptions for normal and abnormal states. Feeding these normal and abnormal descriptions to textual encoder (), we can obtain the embeddings of normal wn Rd and abnormal wa Rd. The pixel-level anomaly map is computed by 3 measuring the cosine similarities between all patch tokens and the textual embeddings, that is ˆY = (cid:2) exp(wa, exp(wa, ) ) + exp(wn, ) (cid:3), (1) where represents the cosine similarity, and [] means that all patch-wise prediction scores are rearranged according to their spatial positions and interpolated to the original input resolution. Replacing with in Eq. 1, we can obtain an image-level anomaly score ˆy for q, that is ˆy = exp(wa, q) exp(wa, q) + exp(wn, q) . (2) 3.2. AdaptCLIP with Alternating Learning To adapt CLIP for universal visual anomaly detection, we design visual and textual adapters to alternately learn visual and textual representations. Specifically, the visual adapter learns adaptive visual tokens (F and q) when fixing two-class static textual embeddings (wa and wn), while the textual adapter learns two-class textual prompt embeddings n) when fixing visual tokens (F and q). (w Visual Adapter adapts vision tokens (F and q) with fixed textual embeddings (wa and wn). It consists of two branches, global and local, which transform global image token and local patch tokens, respectively. Architecturally, the global and local branches are implemented using simple residual multi-layer perception (MLP), that is and q ; θl and θg = + MLP(F v); = + MLP(f q; θg v), (3) are learnable parameters. Replacing where θl and in Eqs. 1 and 2 with and q, we obtain pixellevel anomaly map ˆYv and image-level anomaly score ˆyv. Textual Adapter aims to directly learn two-class prompts θa, θn Rrd without prompt templates, where > 0 is the length of prompts. We feed them into the frozen textual encoder () of CLIP, and obtain the corresponding embeddings and n, that is a = (θa), = (θn). (4) Then, we replace the static wa and wn in Eqs. 1 and 2 with the learnable prompt embeddings and to derive local and global anomaly predictions, ˆYt and ˆyt. Alternating Learning or Joint Learning? possible question is whether we can learn visual and textual representations jointly. That is, in Eqs. 1 and 2, we simultaneously replace fixed textual embeddings and visual tokens with learnable prompt embeddings (w n) and adaptive visual tokens (F and q). Indeed, this joint alignment mechanism is successful when large-scale imagetext dataset is available. However, we empirically find that it does not work well in the AD field, as shown in Tab. 5 and (Lines 3 vs. 4). This is not surprising because the available training data scale is still relatively small and lacks fine-grained textual annotations. The joint learning easily overfits and leads to poor generalization on novel datasets. In contrast, the alternating learning helps us fully utilize the prior knowledge of the CLIP model and thus improve the cross-domain generalization. 3.3. AdaptCLIP with Comparative Learning Compared to static or learnable textual prompts, using normal image as visual prompt is more intuitive. Therefore, we expect to learn comparison ability between query image and its corresponding normal prompt p, which generalizes well to unseen objects. We find that applying multi-layer features yields better results. For simplicity, we use single-layer feature in the following. Spatial Alignment: simple way is to directly measure their difference by the absolute value of their residual feature, that is i are the patch token of and p, respectively. It may fail if the query and prompt images are not aligned in pixel space (e.g., due to rotation and translation). Therefore, we have to align query and prompt tokens for effective comparison. For any query token , we search the nearest one among all normal toj }hw/p2 kens {F p j=1 using euclidean distance, that is , where and i F k, = arg min = p 2. (5) . Now, we . Then, we take as aligned prompt token of can derive the aligned residual feature, i.e., Joint contextual and aligned residual feature: The aligned residual feature highlights differences or anomaly regions well. However, it may lose contextual information or introduce noise. Intuitively, the contextual information is critical to identify anomalies. Therefore, we aggregate the original query tokens and the aligned residual features by an element-wise sum, = + q p . (6) Prompt-Query Adapter: The ultimate goal is to achieve pixel-level anomaly segmentation and image-level anomaly classification. Therefore, we propose lightweight segmentation head G(; θl p) to learn anomaly segmentation based on the joint feature , that is ˆYp = G( ; θl p), (7) where θl is its parameters. Specifically, the segmentation head consists of several transposed convolution blocks following 11 convolution layer. Here, each transposed convolution block upsamples input feature by 2, and it is composed of 33 convolution, BatchNorm, ReLU, and 22 deconvolution. Meanwhile, we need to obtain global image-level prediction. First, we perform average-pooling and maxpooling on the joint feature along the spatial dimension and then take their weighted average as the global image representation. Then, simple MLP is used to map the global feature to an image-level prediction score, that is ˆyp = MLP(cid:0)(AvgPool( ) + MaxPool( ))/2; θg (8) (cid:1), where θg is the parameter. Figure 3. PyTorch pseudocode for the inference of AdaptCLIP. 3.4. Training and Inference During training, we use cross-entropy loss for global image anomaly classification, and Focal and Dice losses for local patch anomaly segmentation, which is exactly the same as AnomalyCLIP [53]. For zero-shot inference, we average the predictions from visual and textual adapters. For fewshot inference, we fuse (i.e., average) all results from three adapters, i.e., prompt-query, visual and textual adapters, as the final predictions of AdaptCLIP. Fig. 3 shows PyTorch pseudocode for the pixel-level inference of AdaptT ], ; ; wn CLIP, where = [wa and means the fixed , and qa and pa refers the adaptive and the aligned . Here, we omit the image-level inference since it can be easily obtained by replacing local patch tokens with global image token. ], Wa = [w and 4. Experiments 4.1. Experimental Setup Datasets: We comprehensively evaluate AdaptCLIP on multiple datasets from industrial and medical domains. 5 For industrial domain, we use MVTec [3], VisA [55], BTAD [26], MVTec3D [5], DTD [1], KSDD [38], MPDD [17], and large-scale Real-IAD [41]. In medical domain, we utilize brain tumor detection datasets, Br35H [13] and COVID-19 [30], as well as gastrointestinal polyp datasets, Kvasir [18] and Endo [39]. detailed introduction to these datasets can be found in Appendix. Evaluation Metrics: Following previous works, we use AUROC for image-level anomaly classification and AUPR for pixel-level anomaly segmentation in our main paper. Here, we emphasize that AUPR is better for anomaly segmentation, where the imbalance issue is very extreme between normal and anomaly pixels [55]. In Appendix, we also provide detailed comparisons using all metrics, including AUROC, AUPR, and F1max. Training and Testing Protocol: Following AnomalyCLIP [53], we train AdaptCLIP using the testing data from MVTec and evaluate zero-/few-shot performance on other datasets. As for the evaluation of MVTec, we train AdaptCLIP using the testing data of VisA. For fair comparison, all models are trained and evaluated using the same protocol. Competing Methods: We compare our AdaptCLIP with diverse state-of-the-art zero-/few-shot AD methods including zero-shot WinCLIP [16], AnomalyCLIP [53], AdaCLIP [6], and few-shot WinCLIP+ [16], InCtrl [54] and AnomalyCLIP+. Here, AnomalyCLIP+ is strong baseline we build on AnomalyCLIP [53] by adding patch-level feature associations like WinCLIP+. More implementation details about AdaptCLIP and competing methods can be found in Appendix. 4.2. Comparisons with Zero-/Few-Shot Methods Tabs. 1 and 2 present comparisons of AdaptCLIP to competing zero-/few-shot methods in image-level anomaly classification and pixel-level anomaly segmentation, respectively, on 8 real-world industrial and 4 medical AD datasets. Note that we only use image-level metrics to evaluate Br35H and Covid due to the lack of pixel-level annotations, and only report the results for Kvasir and Endo using pixel-level metrics since normal images are not included in these two datasets. Below we analyze these results in detail. Generalization on Industrial Domain: Generally, AdaptCLIP significantly outperforms all competing models on almost all industrial datasets across three few-shot settings, 1-shot, 2-shot and 4-shot. The performance of all methods generally gets better with more image prompts. Specifically, InCtrl [54] surpasses WinCLIP [16] due to additional fine-tuning on base training dataset. AnomalyCLIP [53] further achieves better generalization, which verifies the importance of learning object-agnostic prompts. AdaptCLIP exhibits superior performance, outperforming AnomalyCLIP [53] by large margin (about 10%+ in pixel AUPR and 2%+ in image AUROC), particularly on chalTable 1. Image-level anomaly classification comparisons with AUROC metric on industrial and medical domains. The best and second-best results are highlighted in red and blue, respectively. The superscript indicates that the results are our re-implementation with the same training and testing protocol as AnomalyCLIP and our AdaptCLIP. Note that the results are averaged over all categories on each dataset and the full results of each category are presented in Appendix, the same below."
        },
        {
            "title": "Medical",
            "content": "MVTec VisA BTAD MVTec3D DTD KSDD MPDD Real-IAD AVG Br35H Covid AVG 0 1 2 WinCLIP [16] AdaCLIP [6] AnomalyCLIP [53] AdaptCLIP-Zero 90.4 90.7 91.6 93.5 75.5 81.7 82.0 84.8 68.2 89.9 88.3 91.0 69.4 76.2 73.9 78.6 95.1 92.7 93.9 96. 92.9 96.6 97.8 98.1 61.5 64.0 77.5 73.6 67.0 73.3 69.5 74.2 77.5 83.1 84.3 86.2 80.5 96.7 94.2 94.8 66.4 69.4 77.7 86. 73.5 83.0 86.0 90.7 WinCLIP+ [16] InCtrl [54] 93.60.4 80.02.4 84.41.5 74.10.4 97.90.2 93.80.4 69.32.9 74.70.2 83.4 80.12.1 90.13.6 85.1 91.30.4 83.22.4 88.50.4 75.31.3 97.90.3 92.00.9 73.02.7 76.60.0 84.7 83.96.4 89.25.3 86.6 AnomalyCLIP+ [53] 95.20.2 86.10.7 88.50.8 76.72.1 98.00.2 97.50.3 83.42.6 78.20.0 88.0 90.85.1 87.32.6 89.1 94.50.5 90.51.2 93.40.0 81.71.5 98.00.0 96.90.3 83.82.2 81.80.3 90.1 93.72.4 91.82.5 92."
        },
        {
            "title": "AdaptCLIP",
            "content": "WinCLIP+ [16] InCtrl [54] 94.51.0 82.71.0 85.81.8 74.30.3 98.10.2 93.80.2 69.32.3 76.10.1 84.3 81.60.6 91.82.5 86.7 91.80.9 86.31.4 86.22.0 75.40.5 98.30.2 91.60.9 74.21.8 78.50.0 85.3 86.11.7 89.75.1 87.9 AnomalyCLIP+ [53] 95.40.1 87.80.5 89.21.1 78.31.3 98.20.1 97.90.2 83.41.5 78.30.0 88.6 91.54.0 89.32.7 90.4 95.70.6 92.20.8 93.40.2 82.91.1 98.30.0 97.20.0 84.40.7 82.90.2 90.8 94.01.7 94.90.9 94."
        },
        {
            "title": "AdaptCLIP",
            "content": "WinCLIP+ [16] InCtrl [54] 95.30.1 84.30.6 87.80.8 75.70.3 98.20.0 94.00.2 71.21.6 77.00.0 85.4 82.30.4 92.92.1 87.6 93.10.7 87.80.2 67.52.4 78.11.1 97.70.1 91.60.9 78.62.3 81.80.0 84.5 89.11.2 91.44.1 90.3 AnomalyCLIP+ [53] 96.10.1 88.80.5 90.51.2 79.21.3 98.40.1 97.80.1 86.31.8 78.40.0 89.4 91.14.4 91.43.0 91.3 96.60.3 93.10.2 93.30.3 84.20.6 98.50.1 97.00.2 86.81.1 83.90.2 91.7 93.72.0 95.80.9 94."
        },
        {
            "title": "AdaptCLIP",
            "content": "Table 2. Pixel-level anomaly segmentation comparisons with AUPR metric on industrial and medical domains."
        },
        {
            "title": "Medical",
            "content": "MVTec VisA BTAD MVTec3D DTD KSDD MPDD Real-IAD AVG Kvasir Endo AVG 0 1 2 WinCLIP [16] AdaCLIP [6] AnomalyCLIP [53] AdaptCLIP-Zero 18.2 39.1 34.5 38.3 5.4 31.0 21.3 26.1 12.9 42.9 45.5 41.8 5.3 37.5 30.5 31.4 9.8 75.2 62.6 68. 7.1 48.2 51.9 58.3 14.1 25.9 28.9 25.3 3.3 30.5 26.7 28.2 9.5 41.3 37.7 39.7 27.8 36.6 39.6 45.3 23.8 43.7 46.6 52. 25.8 40.1 43.1 48.7 WinCLIP+ [16] InCtrl [54] 38.30.8 15.80.2 41.32.6 18.41.1 47.80.9 19.20.3 29.82.0 13.90.2 28.1 27.62.9 23.60.1 25.6 47.81.1 17.70.6 44.11.4 18.70.5 64.30.5 26.70.7 27.92.2 19.10.0 33.3 22.11.7 20.33.7 21.2 AnomalyCLIP+ [53] 40.80.1 24.80.9 41.31.1 30.61.1 67.40.4 47.50.5 34.20.8 27.90.0 39.3 46.93.9 47.84.9 47.4 53.70.9 38.90.3 60.61.0 40.70.6 76.90.1 57.81.2 33.52.5 36.60.1 49.8 49.24.7 52.44.7 50."
        },
        {
            "title": "AdaptCLIP",
            "content": "WinCLIP+ [16] InCtrl [54] 39.50.6 17.20.8 42.81.3 19.10.8 48.20.9 19.00.5 30.71.1 14.80.1 28.9 29.10.2 27.62.3 28.4 49.20.7 18.50.2 44.20.8 20.30.6 64.40.4 26.42.5 29.21.3 20.10.0 34.0 24.91.9 24.57.5 24.7 AnomalyCLIP+ [53] 41.50.1 26.20.7 41.90.6 32.41.5 68.10.2 47.60.4 35.31.1 28.10.0 40.1 47.32.9 49.64.8 48.5 55.10.5 40.70.6 61.00.6 42.31.1 77.40.2 57.51.1 35.00.7 37.80.1 50.9 49.04.1 53.14.2 51."
        },
        {
            "title": "AdaptCLIP",
            "content": "WinCLIP+ [16] InCtrl [54] 41.20.9 18.11.3 44.00.4 19.90.6 49.30.1 19.10.7 32.00.2 15.40.2 29.9 29.60.8 27.70.5 28.7 50.90.3 19.20.6 44.00.2 22.21.2 64.90.3 26.01.4 31.40.8 21.00.0 35.0 24.71.6 22.31.0 23.5 AnomalyCLIP+ [53] 42.40.0 27.51.1 45.83.0 33.41.3 68.50.2 46.40.7 36.81.0 28.20.0 41.1 45.91.5 49.23.4 47.6 57.20.8 41.80.6 62.30.3 44.50.3 78.20.2 56.41.4 37.41.1 39.10.3 52.1 47.52.7 52.23.1 49."
        },
        {
            "title": "AdaptCLIP",
            "content": "lenging and large-scale datasets like VisA and Real-IAD. This reveals the power of comparative learning based on the joint contextual and aligned residual features for universal anomaly detection. Under zero-shot setting, AdaptCLIPZero significantly outperforms SOTA AdaCLIP [6] on anomaly classification, although it shows slight weakness in industrial anomaly segmentation. However, AdaptCLIP is simpler, requires fewer learnable parameters (0.6M vs. 10.7M in Tab. 3), and generalizes better from the industrial to the medical domain. In addition, our one-shot AdaptCLIP easily outperforms zero-shot AdaCLIP [6] if only one normal image prompt is available. Generalization on Medical Domain: Our AdaptCLIP performs strongly on medical AD regardless of zero-shot or few-shot settings when applying the same model trained on an industrial dataset (i.e., MVTec). Surprisingly, it significantly outperforms SOTA AdaCLIP on image anomaly classification (i.e., 6.3% in AUROC) and pixel anomaly segmentation (i.e., 8.6% in AUPR). Notably, our approach still works even when replacing normal image prompts with anomaly images. This is meaningful for some special datasets that dont contain any normal images, such as 6 Table 3. Complexity and efficiency comparisons. Shots Methods CLIP Models Input Size # Params (M) Inf.Time (ms) WinCLIP [16] AdaCLIP [6] ViT-B-16+240 ViT-B-16+240 208.4 + 0.0 208.4 + 0.0 ViT-L/14@336px 518518 428.8 + 10.7 427.9 + 5.6 208.4 + 0.4 427.9 + 0. AnomalyCLIP [53] ViT-L/14@336px 518518 512512 ViT-L/14@336px 518518 240240 512512 AdaptCLIP-Zero ViT-B-16+240 InCtrl [54] WinCLIP+ [16] ViT-B-16+240 ViT-B-16+240 ViT-B-16+240 240240 512512 240240 AnomalyCLIP+ [53] ViT-L/14@336px 518518 512512 ViT-L/14@336px 518518 ViT-B-16+240 AdaptCLIP 208.4 + 0.0 208.4 + 0.0 208.4 + 0.3 427.9 + 5.6 208.4 + 1.4 427.9 + 1.8 1 201.3 3912.6 212.0 154.9 49.9 162.2 339.5 7434.9 337.0 158.6 54.0 168.2 Kvasir and Endo. Here, this success is mainly due to the proposed spatial alignment mechanism, as well as strong prior assumption that anomaly pixels are mostly sparse. Efficiency Comparison: We measure complexity and efficiency by the number of parameters and the forward inference time, as shown in Tab. 3. The evaluation is performed on one V100 GPU with batch size 32. The number of parameters of AdaCLIP and AnomalyCLIP is 17 times and 9 times that of our AdaptCLIP, respectively. Compared to SOTA, AdaptCLIP achieves competitive inference time yet better AD performance. When extending from zero-shot to one-shot, AnomalyCLIP+ and our AdaptCLIP require almost no additional inference time, unlike earlier WinCLIP. Qualitative Results: Fig. 4 shows some selected visualizations from industrial and medical testing images using AdaptCLIP. Generally, few-shot normal image prompts help AdaptCLIP segment anomalies more accurately and produce fewer false positives than in zero-shot manner. 4.3. Comparisons with Many-Shot Methods In Tab. 4, we compare few-shot AdaptCLIP with manyshot and full-shot unified AD models. It can be seen that AdaptCLIP is better than the early many-shot methods, RegAD [15], and comparable to the latest PromptAD [23]. It is worth noting that PromptAD [23] requires re-training with few-shot normal images while our method remains training-free on target domains. Furthermore, our method outperforms full-shot methods, such as SimpleNet [25] and UniAD [48], and is also competitive with the latest OneNIP [11]. In short, our method has shown excellent performance, especially in the open-world scenario for universal anomaly detection, although there is still some small gap compared to state-of-the-art full-shot methods. 4.4. Ablation Studies To demonstrate the effectiveness of the proposed three adapters in AdaptCLIP, TA: Texual Adapter, VA: Visual Adapter, and PQA: Prompt-Query Adapter, and two main insights, alternating learning, and comparative learning based on the joint contextual and aligned residual feature, we conduct experiments on MVTec and VisA, and report results in Tab. 5. Table 4. Comparisons of image-level anomaly classification and pixel-level anomaly segmentation (using AUROC/AUPR metric, and the same as below) with many-shot and full-shot methods."
        },
        {
            "title": "AdaptCLIP",
            "content": "RegAD [15] PromptAD [23] SimpleNet [25] UniAD [48] OneNIP [11] 1 4 8 4 94.5 / 53.7 90.5 / 38.9 93.4 / 60.6 96.6 / 57.2 93.1 / 41.8 93.3 / 62.3 91.2 / 51.1 96.6 / 52. 79.7 / 28.6 90.7 / 40.5 89.1 / 31.5 - full full full 78.2 / 24.8 89.2 / 33.1 90.3 / 36.2 96.5 / 44.7 90.8 / 33.6 92.2 / 50.9 97.9 / 63.7 92.5 / 43.3 92.6 / 56.8 Table 5. Ablation studies about different components. No. Methods"
        },
        {
            "title": "VisA",
            "content": "0 1 2 3 4 baselines joint alternating 5 w/o context context"
        },
        {
            "title": "7 AdaptCLIP",
            "content": "0 0 0 0 0 1 1 1 91.1 / 33.0 82.1 / 18.0 92.2 / 31.4 82.9 / 19.7 90.5 / 39.4 81.0 / 22.1 89.3 / 36.2 81.6 / 21.5 93.5 / 38.3 84.8 / 26.1 62.6 / 7.0 85.3 / 28.7 88.1 / 50.2 88.9 / 38.1 94.2 / 52.5 92.0 / 38.8 Simple but effective baselines. The first baseline is the naive CLIP (Line 0), and it is simple and effective for zero-shot anomaly detection only using two-class textual prompts. However, it is still weak in pixel-level anomaly segmentation. The individual textual adapter and visual adapter are two addtional baselines. Specifically, the textual adapter can be seen as an extreme simplification of AnomalyCLIP [53], removing the textual prompt template and textual prompt tuning. The simple textual adapter performs better than the original AnomalyCLIP and naive CLIP in anomaly classification, although it is slightly inferior in anomaly segmentation (Lines 0 vs. 1). The visual adapter learns adaptive local patch tokens and global image tokens to align textual representations from CLIP in both patch and image levels. This significantly improves pixel-level anomaly segmentation (Lines 0 vs. 2). Alternating learning is better than joint learning. We explore the impact of alternating learning and joint learning strategies on AdaptCLIPs performance. Alternating learning adapts visual or textual representations independently, while joint learning optimizes both representations simultaneously. As shown (Lines 3 vs. 4) in Tab. 5, the alternating learning strategy significantly enhances the performance of AdaptCLIP compared to joint learning. Alternating learning not only fully leverages the strong prior guidance of CLIPs visual and textual representations but also mitigates the risk of over-fitting due to fine-tuning on small training dataset. Additionally, we observe that the visual adapter 7 Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 4. Qualitative comparisons of our AdaptCLIP with different prompt numbers on MVTec, VisA, Real-IAD, Kvasir and Endo. More qualitative results of AdaptCLIP can be found in Appendix. Best viewed in color and zoom. alone excels in anomaly segmentation (Line 2), whereas the textual adapter alone performs better in anomaly classification (Line 1). By integrating the alternating learning into visual and textual adapters, AdaptCLIP generally achieves superior anomaly detection performance (Line 4). The joint of contextual information and aligned residual features performs better than residual features alone. The aligned residual feature captures the distinctions between anomalous features and their corresponding normal counterparts. It effectively eliminates features related to individual objects and may improve generalization. However, we realize that isolated residual features may lose contextual information about visual objects, resulting in degraded model performance or even training failure (Line 5). Therefore, we propose joint feature learning based on both contextual and aligned residual features, which further significantly boosts the models performance (Lines 6 vs. 5). This means contextual information is equally important for anomaly identification. Notably, the optimal performance for AdaptCLIP is achieved when all proposed components are integrated (Line 7). Effects on pre-trained CLIP models. We report zeroand one-shot results of AdaptCLIP using different CLIP models in Tab. 6. It can be seen that larger pre-trained model always brings better performance, especially in image-level classification. Furthermore, our method equipped with lightweight model (ViT-B-16+240) makes it possible to achieve competitive anomaly segmentation performance. 5. Conclusion In this paper, we introduce universal anomaly detection task, which focuses on generalizing anomaly detection models across domains, such as industrial and medical, and Table 6. Ablation studies about different pre-trained CLIP models."
        },
        {
            "title": "VisA",
            "content": "ViT-B-16+240 ViT-L/14@336px ViT-B-16+240 ViT-L/14@336px 512512 518518 512512 518518 0 0 1 83.9 / 38.3 75.4 / 19.5 93.5 / 38.3 84.8 / 26.1 92.4 / 52.3 85.2 / 30.3 94.2 / 52.5 92.0 / 38.8 in open scenarios, such as zeroor few-shot settings. Once the universal anomaly detection model is trained, it does not need any fine-tuning on the target dataset. Compared with single zero-shot or few-shot AD models, the universal anomaly detection model is more flexible, supporting zero- /few-shot inference via fixed or learnable textual prompts and few normal image prompts, while providing both image-level and pixel-level anomaly predictions. We propose universal anomaly detection framework, AdaptCLIP, which alternately learns adaptive visual representations and text prompt embeddings, as well as jointly learns comparisons based on the contextual information of query image and the aligned residual features between the query and the prompt. Extensive experiments on 8 standard industrial and 4 medical datasets show that AdaptCLIP significantly outperforms current competitive models in multiple settings. Limitation: AdaptCLIP achieves good AD performance only given zero-/few-shot normal image prompts. However, it could cause the model to confuse normal and abnormal instances and finally result in decreased performance when we provide anomaly images as normal image prompts. Fortunately, normal images are generally relatively easy to obtain in practical applications. In addition, it may work using abnormal images as visual prompts because most of the pixels may be normal even in anomaly images."
        },
        {
            "title": "References",
            "content": "[1] Toshimichi Aota, Lloyd Teh Tzer Tong, and Takayuki Okatani. Zero-shot versus many-shot: Unsupervised texture anomaly detection. In WACV, 2023. [2] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as generative models. In NeurIPS, 2013. [3] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. MVTec-AD: comprehensive real-world dataset for unsupervised anomaly detection. In CVPR, 2019. [4] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed Students: Student-teacher anomaly detection with discriminative latent embeddings. In CVPR, 2020. [5] Paul Bergmann., Xin Jin., David Sattlegger., and Carsten The mvtec 3d-ad dataset for unsupervised 3d Steger. anomaly detection and localization. In VISAPP, 2022. [6] Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen, and Giacomo Boracchi. Adaclip: Adapting clip with hybrid learnable prompts for zero-shot anomaly detection. In ECCV, 2024. [7] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. PaDiM: patch distribution modeling framework for anomaly detection and localization. In ICPR, 2021. [8] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In CVPR, 2022. [9] Choubo Ding, Guansong Pang, and Chunhua Shen. Catching both gray and black swans: Open-set supervised anomaly detection. In CVPR, 2022. [10] Zheng Fang, Xiaoyang Wang, Haocheng Li, Jiejie Liu, Qiugui Hu, and Jimin Xiao. FastRecon: Few-shot industrial anomaly detection via fast feature reconstruction. In ICCV, 2023. [11] Bin-Bin Gao. Learning to detect multi-class anomalies with just one normal image prompt. In ECCV, 2024. [12] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In ICCV, 2019. [13] Ahmed Hamada. Br35h: Brain tumor detection 2020, 2020. [14] Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, and Hong Zhou. Divide-and-Assemble: Learning block-wise memory for unsupervised anomaly detection. In ICCV, 2021. [15] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang. Registration based few-shot anomaly detection. In ECCV, 2022. [16] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. WinCLIP: Zero-/few-shot anomaly classification and segmentation. In CVPR, 2023. [17] Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, and Milos Skotak. Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions. In ICUMT, 2021. [18] Debesh Jha, Pia Smedsrud, Michael Riegler, Pal Halvorsen, Thomas de Lange, Dag Johansen, and Havard Johansen. Kvasir-seg: segmented polyp dataset. In International Conference on Multimedia Modeling, pages 451 462, 2020. [19] Jiarui Lei, Xiaobo Hu, Yue Wang, and Dong Liu. PyramidFlow: High-resolution defect contrastive localization using pyramid normalizing flow. In CVPR, 2023. [20] Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, and Stephan Mandt. Zero-shot anomaly detection via batch normalization. In NeurIPS, 2023. [21] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. CutPaste: Self-supervised learning for anomaly detection and localization. In CVPR, 2021. [22] Xurui Li, Ziming Huang, Feng Xue, and Yu Zhou. MuSc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. In ICLR, 2024. [23] Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, and Lizhuang Ma. PromptAD: Learning prompts with only normal samples for few-shot anomaly detection. In CVPR, 2024. [24] Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Diversity-measurable anomaly detection. In CVPR, 2023. [25] Zhikang Liu, Yiming Zhou, Yuansheng Xu, and Zilei Wang. SimpleNet: simple network for image anomaly detection and localization. In CVPR, 2023. [26] Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. VT-ADL: vision transformer network for image anomaly detection and localization. In ISIE, 2021. [27] Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. OCGAN: One-class novelty detection using GANs with constrained latent representations. In CVPR, 2019. [28] Zhen Qu, Xian Tao, Mukesh Prasad, Fei Shen, Zhengtao Zhang, Xinyi Gong, and Guiguang Ding. Vcp-clip: visual context prompting model for zero-shot anomaly segmentation. In ECCV, 2024. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [30] Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad Bin Abul Kashem, Mohammad Tariqul Islam, Somaya Al Maadeed, Susu Zughaier, Muhammad Salman Khan, et al. Exploring the effect of image enhancement techniques on covid-19 detection using chest x-ray images. Computers in biology and medicine, 2021. [31] Rajesh PN Rao and Dana Ballard. Predictive coding in the visual cortex: functional interpretation of some extraclassical receptive-field effects. Nature Neuroscience, 2(1), 1999. [32] Oliver Rippel, Patrick Mertens, and Dorit Merhof. Modeling the distribution of normal data in pretrained deep features for anomaly detection. In ICPR, 2021. [48] Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. unified model for multi-class anomaly detection. In NeurIPS, 2022. [49] Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, and Seung-Ik Lee. Old is Gold: Redefining the adversarially learned one-class classifier training paradigm. In CVPR, 2020. [50] Vitjan Zavrtanik, Matej Kristan, and Danijel Skoˇcaj. DRAEM: discriminatively trained reconstruction embedding for surface anomaly detection. In ICCV, 2021. [51] Vitjan Zavrtanik, Matej Kristan, and Danijel Skoˇcaj. Reconstruction by inpainting for visual anomaly detection. PR, 112, 2021. [52] Ying Zhao. OmniAL: unified cnn framework for unsupervised anomaly localization. In CVPR, 2023. [53] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. AnomalyCLIP: Object-agnostic prompt learning for zero-shot anomaly detection. In ICLR, 2024. [54] Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In CVPR, 2024. [55] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference self-supervised pretraining for anomaly detection and segmentation. In ECCV, 2022. [33] Nicolae-Catalin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas Moeslund, and Mubarak Shah. Self-supervised predictive convolutional attentive block for anomaly detection. In CVPR, 2022. [34] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Scholkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In CVPR, 2022. [35] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Fully convolutional cross-scale-flows for imagebased defect detection. In WACV, 2022. [36] Mohammadreza Salehi, Niousha Soroosh Baselizadeh, Mohammad Rohban, and Hamid Rabiee. Multiresolution knowledge distillation for anomaly detection. In CVPR, 2021. Sadjadi, [37] Shelly Sheynin, Sagie Benaim, and Lior Wolf. hierarchical transformation-discriminating generative model for few shot anomaly detection. In ICCV, 2021. [38] Domen Tabernik, Samo ˇSela, Jure Skvarˇc, and Danijel Skoˇcaj. Segmentation-based deep-learning approach for surface-defect detection. Journal of Intelligent Manufacturing, 31(3):759776, 2020. [39] David Vazquez, Jorge Bernal, Javier Sanchez, Gloria Fernandez-Esparrach, Antonio Lopez, Adriana Romero, Michal Drozdzal, Aaron Courville, et al. benchmark for endoluminal scene segmentation of colonoscopy images. Journal of healthcare engineering, 2017, 2017. [40] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. [41] Chengjie Wang, Wenbing Zhu, Bin-Bin Gao, Zhenye Gan, Jiangning Zhang, Zhihao Gu, Shuguang Qian, Mingang Chen, and Lizhuang Ma. Real-iad: real-world multi-view dataset for benchmarking versatile industrial anomaly detection. In CVPR, 2024. [42] Guodong Wang, Shumin Han, Errui Ding, and Di Huang. Student-teacher feature pyramid matching for anomaly detection. BMVC, 2021. [43] Shenzhi Wang, Liwei Wu, Lei Cui, and Yujun Shen. Glancing at the patch: Anomaly localization with global and local feature comparison. In CVPR, 2021. [44] Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. In ICLR, 2023. [45] Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng. Learning semantic context from normal samples for unsupervised anomaly detection. In AAAI, 2021. [46] Xincheng Yao, Ruoqi Li, Zefeng Qian, Yan Luo, and Chongyang Zhang. Focus the Discrepancy: Intra-and intercorrelation learning for image anomaly detection. In ICCV, 2023. [47] Xincheng Yao, Ruoqi Li, Jing Zhang, Jun Sun, and Chongyang Zhang. Explicit boundary guided semi-pushpull contrastive learning for supervised anomaly detection. In CVPR, 2023. 10 AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Dataset Details To validate the effectiveness of our method, we conduct comprehensive experiments on 12 public anomaly detection datasets covering two domains, industrial and medical, and three modalities, including photography, radiology, and endoscopy. We only use two test datasets for model pretraining and generalization evaluation on other test datasets, and their relevant information is reported in Tab. 7. Specifically, we train models using the test data from MVTec and evaluate zero-/few-shot performance on other datasets. As for the evaluation of MVTec, we train models using VisAs test data. It should be noted that Real-IAD is the largest industrial anomaly detection dataset consisting of diverse categories (30 objects) and large-scale images (150k) among the utilized datasets. For the medical domain, we cannot find publicly available 2D medical AD datasets that include both imageand pixel-level annotations simultaneously. Therefore, we only report image-level classification performance on Br35H and Covid, while providing pixel-level anomaly segmentation performance on Kvasir and Endo. In addition, we note that MPDD and all four medical datasets are pose-agnostic, and KSDD may contain noise. We empirically find that the performance of few-shot AD methods may be limited and sometimes may be worse than zero-shot methods on these datasets. We believe this is shortcoming of all few-shot normal image prompt-based methods. 7. Implementation Details We utilize the pre-trained CLIP (ViT-L/14@336) as the default CLIP model and extract local patch tokens from layers {6, 12, 18, 24} and global image token from the last layer {24}. All images are resized to resolution of 518518 for training and testing. Regarding the visual and textual adapters, we only use features from the last layer (i.e., 24) of the CLIP visual encoder, while for the promptquery adapter, we use features from all 4 layers. The visual adapter is two-layer MLP, whose hidden layer dimension is 1/4 of the input layer, and the output layer dimension remains the same as the input layer. The length of learnable textual prompt embeddings is set to 12 in the textual adapter. For the prompt-query adapter, the dimension of the first hidden layer is set to 128, and then the dimension of the next layer is halved until the last layer is set to 2 in both the lightweight segmentation head and the global MLP. We train models for 15 epochs with learning rate of 0.001. All experiments are conducted using PyTorch with single NVIDIA V100 GPU. 7.1. Competing Methods For fair comparison, we compare state-of-the-art zeroshot methods, such as WinCLIP [16], AnomalyCLIP [53], and AdaCLIP [6], and few-shot methods, such as WinCLIP+ [16], InCtrl [54], AnomalyCLIP+, and PromptAD [23], with our AdaptCLIP using the same training protocol and few-shot normal image prompts. It is worth noting that the original InCtrl [54] only supports image-level few-shot AD, and we have appropriately extended it to allow pixel-level few-shot AD. In addition, AnomalyCLIP+ is an extension of AnomalyCLIP introducing feature association in WinCLIP+. In Tab. 8, we qualitatively analyze these methods in terms of capability, including zeroshot, few-shot, image-level anomaly classification, pixellevel anomaly segmentation, unified or separated models and original CLIP ability, and complexity, including pretraining, post-finetuning on target datasets, sliding windows, class names, learnable-prompts and few-shot normal image prompts. We summarize them in detail as follows. WinCLIP [16] is the first zero-shot anomaly detection method based on vision-language model, i.e., CLIP. WinCLIP designs two-class textual prompts and introduces multi-scale patch windows for accurate anomaly segmentation. However, it brings large computational costs and memory burden, limiting high-resolution input or large pretrained models. Note that no official implementation of WinCLIP is available, our results are based on an unofficial implementation. WinCLIP+ [16] combines languageand visual-guided predictions for better anomaly classification and segmentation. The language-guided prediction is the same as in WinCLIP. For visual-guided prediction, it first simply stores multi-scale features from few-shot normal images into memory bank, and then measures the anomaly score using the distance or similarity between each query feature and the nearest feature from the memory bank. The final anomaly score is derived by averaging these two scores. AnomalyCLIP [53] learns object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. However, AnomalyCLIP requires fine-tuning on an auxiliary domain dataset including normal and anomaly images. AnomalyCLIP is zero-shot anomaly detection method and it is capable of recognizing any anomalies. We use the official model to report performance for anomaly classification and segmentation. AnomalyCLIP+ is one of our baselines. Note that the original AnomlayCLIP only supports zero-shot anomaly 1 Table 7. Key statistics of industrial and medical datasets with different attributes. means satisfied and means not satified. Domain Dataset Modality Category # Classes Pose-Agnostic Anomaly Anotations Train Test Image-Level Pixel-Level # Normal # Normal # Anomaly Industrial MVTec VisA BTAD MVTec3D DTD KSDD MPDD Real-IAD Photography Photography Photography Photography Photography Photography Photography Photography Obj & Texture Obj Obj & Texture Obj Texture Texture Obj Obj Medical Br35H Covid Kvasir Endo Radiology (MRI) Radiology (X-ray) Endoscopy Endoscopy Brain Chest Gastrointestinal tract Gastrointestinal tract 15 12 3 10 12 1 6 30 1 1 1 1 3,629 8,659 1,799 2,656 1,200 857 888 36,465 0 0 0 467 962 451 249 357 286 176 63,256 1500 1,341 0 0 1,258 1,200 290 948 947 54 282 51,329 1500 219 1,000 200 Table 8. Comprehensive comparisons of state-of-the-art zero-/few-shot AD methods and our AdaptCLIP in terms of capabilities and complexity. means satisfied and means not satified. Methods zero-shot few-shot image-cls. pixel-seg. unified ori-ability pre-training post-finetuning sliding-wins class-names learnable-prompts image-prompts Capability Complexity WinCLIP/WinCLIP+ [16] AdaCLIP [6] InCtrl [54] AnomalyCLIP [53] PromptAD [23] AdaptCLIP detection. To achieve few-shot anomaly detection, we introduce feature association based on memory mechanism, similar to WinCLIP+, to AnomalyCLIP. The final anomaly results are also the average of zero-shot predictions guided by learnable textual prompts and few-shot predictions guided by few-shot normal image prompts. InCtrl [54] integrates multi-level information, including patch-level residual maps, image-level residual features, and prior knowledge score using two-class textual prompts, to learn holistic scoring function for anomaly classification. However, it does not consider pixel-level anomaly segmentation. In this paper, we simply use the patch-level residual map as the pixel-level anomaly prediction, which is essentially similar to WinCLIP+. It is worth noting that InCtrl provides multiple models for different shot numbers. We use its official models for extensive evaluation. In addition, since it does not provide 1-shot model, we use the 2-shot model to evaluate 1-shot performance. AdaCLIP [6] further integrates visual knowledge from query images into textual prompt embeddings for enhancing the interaction of visual features and textual prompt embeddings. Different from AnomalyCLIP, AdaCLIP trains base models using more auxiliary datasets, including industrial and medical, which is not conducive to cross-domain evaluation. For fair comparison, we retrain AdaCLIP models using the same training protocol as AnomalyCLIP, and conduct comprehensive evaluations on multiple datasets for zero-shot anomaly detection. PromptAD [23] introduces an explicit anomaly margin to mitigate the training challenge caused by the absence Instead of using unified of anomaly training images. paradigm (i.e., one model for all classes) in AnomalyCLIP and AdaCLIP, PromptAD uses separate paradigm (i.e. one model for one class). Therefore, it needs to re-train model with few-shot normal images when applied to each class of the target datasets. In addition, image-level anomaly classification and pixel-level segmentation models also need to be trained separately. In this paper, we only compare PromptAD with our method on MVTec and VisA because it involves fine-tuning for each class of target datasets. 8. Compelete Experimental Results In our main paper, we compare state-of-the-art methods with our AdaptCLIP using AUROC for image-level anomaly classification and AUPR for pixel-level anomaly segmentation. Here, we provide more comprehensive comparisons, including image-level anomaly classification in AUPR and F1max in Tabs. 9 and 10, and pixel-level anomaly segmentation in AUROC and F1max in Tabs. 11 and 12, respectively. To more intuitively show the performance trends between zero-shot and few-shot methods on different datasets, we show comprehensive comparisons using all three metrics (AUROC, AUPR and F1max) for image-level anomaly classification and pixel-level anomaly segmentation, as shown in Fig. 5. In addition, we only report the averaged results of all categories for each dataset in our main paper. Here, we also provide more detailed reports in Tabs. 13, 14, 15, 16, 17, 18, 19, 20, 21, and 22 for each category on MVTec, VisA, MVTec3d, DTD, MPDD, RealIAD, BTAD, KSDD, Br35H, Covid, Kvasir and Endo, respectively. 9. More Visualizations In our main paper, we only visualize some selected examples from MVTec, VisA, Real-IAD, Kvasir and Endo to compare zero-shot and few-shot AdaptCLIP. Here, we show more visualizations for all 91 categories from 8 industrial and 2 medical datasets, as shown in Figs. 6, 7, 8, 9, 10, 11, 12, 13, 14 and 15. 3 t fi a e - m o t g e - i (a) AUROC. (b) AUPR. (c) F1max. Figure 5. Image-level anomaly classification and pixel-level anomaly segmentation comparisons of state-of-the-art zero-/one-shot methods and our AdaptCLIP with all three metrics, AUROC, AUPR and F1max. The one-shot AdaptCLIP utilizes training-free manner on target domains and achieves more accurate anomaly classification and segmentation on 8 industrial and 4 medical benchmarks. Table 9. Image-level anomaly classification comparisons with AUPR metric on industrial and medical domains. Shots Methods Industrial Medical MVTec VisA BTAD MVTec3D DTD KSDD MPDD Real-IAD AVG Br35H Covid AVG 0-shot 1-shot 2-shot 4-shot WinCLIP [16] AdaCLIP [6] AnomalyCLIP [53] AdaptCLIP 95.6 95.6 96.4 96. 78.7 84.3 85.3 87.6 70.9 95.5 88.2 92.2 WinCLIP+ [16] InCtrl [54] 96.80.2 81.71.5 80.53.3 95.20.3 84.11.5 83.49.3 AnomalyCLIP+ [53] 97.20.1 87.71.1 74.21.5 97.50.1 92.30.9 95.80.9 AdaptCLIP WinCLIP+ [16] InCtrl [54] 97.30.5 84.00.7 82.53.2 95.50.7 86.81.7 81.68.0 AnomalyCLIP+ [53] 97.30.1 89.10.7 75.41.5 97.90.2 93.60.6 95.90.1 AdaptCLIP WinCLIP+ [16] InCtrl [54] 97.70.0 85.50.9 88.11.4 96.30.5 88.00.3 80.91.7 AnomalyCLIP+ [53] 97.80.0 90.10.7 77.53.1 98.40.2 94.30.2 96.40.1 AdaptCLIP 89.6 92.7 91.8 93. 91.50.2 91.90.7 92.40.9 94.50.5 91.60.1 91.80.2 92.90.4 94.80.4 92.20.1 92.80.6 93.30.4 95.30.2 97.7 96.4 97.2 98.4 84.9 89.3 94.2 95.7 69.2 70.8 82.5 74. 62.9 70.2 64.3 70.8 81.2 86.9 87.5 88.8 82.2 96.7 94.2 95.1 42.9 43.6 55.5 54.4 62.6 70.2 74.9 74.8 99.00.1 84.60.9 73.52.2 71.20.3 98.90.2 81.52.2 75.70.8 69.90.0 99.20.2 95.20.1 85.62.8 76.70.0 99.10.0 91.80.2 83.13.7 80.40. 84.9 79.41.9 76.96.4 78.2 85.1 82.68.7 65.15.6 73.9 88.5 86.27.8 64.04.9 75.1 91.8 92.13.1 78.84.6 85.5 99.10.1 84.50.6 73.62.0 72.70.1 99.10.3 81.02.5 75.90.7 71.70.0 99.30.1 95.60.2 85.42.4 76.90.0 99.20.0 92.40.4 84.72.1 81.50.1 85.7 80.50.4 80.33.1 80.4 85.4 84.82.9 66.24.4 75.5 89.0 86.86.2 66.74.5 76.8 92.5 92.22.5 83.61.9 87.9 99.20.1 84.90.5 75.30.1 73.60.1 98.30.4 84.61.7 79.52.7 75.60.0 99.40.1 95.00.2 88.02.2 77.10.0 99.30.0 91.70.9 87.72.4 82.60.0 87.1 81.00.2 81.13.2 81.1 87.0 88.61.0 66.93.4 77.8 89.8 86.36.5 70.74.7 78.5 93.2 91.82.9 85.81.2 88.8 Query GT Mask 1-shot Figure 6. Qualitative comparisons of our AdaptCLIP with different prompt numbers on MVTec. GT Mask 0-shot 1-shot 4-shot 0-shot Query 4-shot Table 10. Image-level anomaly classification comparisons with F1max metric on industrial and medical domains. Shots Methods Industrial Medical MVTec VisA BTAD MVTec3D DTD KSDD MPDD Real-IAD AVG Br35H Covid AVG 0-shot 1-shot 2-shot 4-shot WinCLIP [16] AdaCLIP [6] AnomalyCLIP [53] AdaptCLIP 92.7 92.4 92.7 93.7 78.2 80.0 80.4 83.0 67.8 90.2 83.8 89.5 WinCLIP+ [16] InCtrl [54] 94.00.4 81.30.8 77.13.5 93.90.4 83.11.4 81.55.6 AnomalyCLIP+ [53] 94.60.2 83.20.6 76.60.8 95.00.0 86.51.0 91.61. AdaptCLIP WinCLIP+ [16] InCtrl [54] 94.50.4 82.31.1 78.72.9 94.20.2 84.31.3 81.45.8 AnomalyCLIP+ [53] 94.90.1 84.50.5 77.20.5 95.40.1 88.00.7 91.70.5 AdaptCLIP WinCLIP+ [16] InCtrl [54] 94.80.1 82.80.6 83.80.3 94.70.2 85.10.2 87.50.1 AnomalyCLIP+ [53] 95.50.1 85.20.3 78.91.1 96.00.0 88.50.2 92.10. AdaptCLIP 89.7 89.6 88.8 89.3 90.30.2 89.80.3 90.10.5 91.00.3 90.30.2 90.00.2 90.20.4 91.20.2 90.40.1 90.50.3 90.00.3 91.30.3 94.1 92.6 93.6 95. 80.8 85.4 89.7 92.3 77.5 76.7 80.4 79.7 65.3 67.8 65.9 68.9 80.8 84.3 84.4 86.4 74.1 92.3 86.8 87.7 42.7 42.9 54.2 55. 58.4 67.6 70.5 71.6 96.70.3 80.11.6 81.50.9 69.50.1 97.70.1 80.31.2 80.92.2 70.80.0 97.20.4 93.01.3 84.71.3 70.60.0 97.20.1 89.60.5 85.31.3 73.30.2 83.8 75.21.8 71.75.5 73.5 84.8 78.12.7 65.43.0 71.8 86.3 86.34.9 58.76.1 72.5 88.7 87.83.2 73.16.6 80.5 97.00.1 80.32.0 81.01.2 70.30.1 97.90.1 80.50.2 81.41.7 72.10.0 97.70.1 93.61.0 84.80.5 70.70.0 97.70.1 89.40.9 85.70.7 74.20.1 84.3 76.40.3 75.12.0 75.8 85.2 78.41.2 65.81.5 72.1 86.7 86.94.1 61.75.8 74.3 89.2 88.32.0 78.72.4 83.5 97.10.1 81.02.2 82.00.4 70.80.0 98.10.1 78.71.1 84.50.5 74.20.0 97.90.2 92.50.9 85.21.0 70.80.0 97.80.1 88.61.7 87.80.0 75.20. 85.3 76.70.2 75.82.3 76.3 86.7 82.71.1 67.04.3 74.9 87.0 86.64.3 66.05.4 76.3 89.7 88.32.3 81.21.1 84.8 5 Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 7. Qualitative comparisons of our AdaptCLIP with different prompt numbers on BTAD. Table 11. Pixel-level anomaly segmentation comparisons with AUROC metric on industrial and medical domains. Shots Methods"
        },
        {
            "title": "Medical",
            "content": "MVTec VisA BTAD MVTec3D DTD KSDD MPDD Real-IAD AVG Kvasir Endo AVG 0-shot 1-shot 2-shot 4-shot WinCLIP [16] AdaCLIP [6] AnomalyCLIP [53] AdaptCLIP 82.3 88.3 91.1 90.9 73.2 95.7 95.5 95. 72.7 91.6 94.2 93.8 WinCLIP+ [16] InCtrl [54] 93.40.2 94.70.1 95.60.2 94.60.2 89.00.2 96.60.1 AnomalyCLIP+ [53] 92.80.0 96.40.1 95.30.3 94.30.1 96.80.0 96.60.2 AdaptCLIP WinCLIP+ [16] InCtrl [54] 93.80.1 95.10.1 95.70.1 95.20.2 89.80.2 96.70.1 AnomalyCLIP+ [53] 92.90.1 96.70.1 95.50.2 94.50.0 97.10.0 96.70. AdaptCLIP WinCLIP+ [16] InCtrl [54] 94.20.2 95.10.2 95.90.1 95.80.2 90.20.2 96.80.0 AnomalyCLIP+ [53] 93.20.1 96.90.1 95.70.1 94.80.1 97.30.0 96.80.0 AdaptCLIP 91.2 97.1 96.2 97.2 96.80.2 94.40.2 96.60.1 97.70. 96.90.2 94.70.1 96.80.1 97.80.0 97.00.1 95.20.1 97.00.1 98.00.0 79.5 98.3 97.9 97.7 93.0 97.6 98.1 98.1 71.2 95.5 96.5 95.9 84.5 96.1 95.1 94. 81.0 95.0 95.6 95.5 69.7 77.8 79.0 82.1 68.2 83.8 84.2 86.5 69.0 80.8 81.6 84.3 96.50.1 97.60.1 94.71.3 95.00.0 98.60.1 97.80.2 94.41.0 95.40.0 97.60.1 98.60.1 97.40.2 96.50.0 97.40.0 98.20.1 97.40.2 97.10.0 95.5 73.12.6 72.30.3 72.7 95.1 65.81.3 66.56.2 66.2 96.4 81.30.4 84.81.5 83.1 96.9 83.30.5 86.51.2 84. 96.60.1 97.60.1 94.20.2 95.30.0 98.70.1 97.60.6 94.30.2 96.00.0 97.70.1 98.60.1 97.50.1 96.60.0 97.60.0 98.10.1 97.70.0 97.30.0 95.7 74.60.5 75.51.6 75.1 95.4 70.00.9 70.26.1 70.1 96.5 81.71.0 85.11.4 83.4 97.1 83.50.8 86.61.1 85.1 96.80.1 97.50.2 94.70.3 95.50.0 98.70.1 97.50.3 95.10.5 96.40.0 97.80.0 98.60.1 97.80.1 96.70.0 97.80.1 98.00.1 97.90.1 97.40.0 95.8 75.00.8 75.30.3 75.2 95.7 70.61.2 69.41.3 70.0 96.7 81.30.4 84.50.9 82.9 97.3 83.00.5 85.80.7 84.4 Table 12. Pixel-level anomaly segmentation comparisons with F1max metric on industrial and medical domains. Shots Methods Industrial Medical MVTec VisA BTAD MVTec3D DTD KSDD MPDD Real-IAD AVG Kvasir Endo AVG 0-shot 1-shot 2-shot 4-shot WinCLIP [16] AdaCLIP [6] AnomalyCLIP [53] AdaptCLIP 24.8 42.1 38.0 43.2 8.8 36.9 28.1 32.6 18.3 46.6 49.4 46.3 WinCLIP+ [16] InCtrl [54] 42.00.8 22.90.2 46.82.3 51.01.6 25.20.4 49.91.3 AnomalyCLIP+ [53] 45.70.2 34.60.6 45.71.4 54.00.7 44.60.4 58.70. AdaptCLIP WinCLIP+ [16] InCtrl [54] 43.20.7 24.30.8 48.11.1 52.30.8 26.30.5 49.90.8 AnomalyCLIP+ [53] 46.30.1 36.00.5 46.21.3 55.00.3 46.10.4 58.80.4 AdaptCLIP WinCLIP+ [16] InCtrl [54] 44.60.6 25.31.2 49.50.7 54.10.4 27.20.6 50.30.7 AnomalyCLIP+ [53] 47.10.1 37.50.9 49.61.2 56.80.7 47.20.5 59.60. AdaptCLIP 10.0 42.2 36.0 36.1 24.80.7 25.30.7 36.91.1 43.70.7 25.50.5 27.00.5 38.51.4 45.41.1 26.30.5 28.91.0 39.41.0 47.60.2 15.7 70.7 62.2 63. 15.5 38.7 56.5 58.1 15.3 29.5 34.0 29.9 7.7 35.4 34.5 35.6 14.5 42.8 42.3 43.2 35.7 44.2 46.1 50.6 32.9 48.7 50.3 54. 34.3 46.5 48.2 52.6 50.70.7 29.20.2 31.12.2 21.80.2 62.90.5 38.00.6 29.91.7 27.00.0 66.50.5 51.80.3 37.80.9 35.90.0 72.40.2 58.40.5 36.42.4 42.50.1 33.7 39.01.9 36.80.3 37.9 38.7 33.21.1 33.03.7 33.1 44.4 50.11.1 53.22.3 51.7 51.3 52.10.7 55.22.0 53.7 51.00.7 29.60.1 32.01.3 23.00.0 63.00.4 37.52.1 31.01.2 28.00.0 66.90.3 51.90.5 38.90.8 36.00.0 72.80.3 58.10.6 38.20.2 43.40.2 34.6 40.20.6 39.10.9 39.7 39.4 36.40.4 35.24.2 35.8 45.1 50.61.9 54.12.1 52.4 52.2 52.61.4 55.81.7 54.2 51.70.2 29.90.8 33.50.4 23.90.3 63.10.2 36.71.0 33.61.9 29.00.0 67.20.1 51.20.6 40.11.1 36.20.0 73.50.1 57.20.7 40.71.1 44.60. 35.6 40.70.8 39.20.2 40.0 40.4 36.90.8 34.61.4 35.8 46.0 50.10.8 53.81.6 52.0 53.4 52.10.8 55.61.2 53.9 6 Table 13. Image-level anomaly classification and pixel-level anomaly segmentation results on MVTec with zero-shot and few-shot AdaptCLIP. Shot Catergoies Anomaly Classification Anomaly Segmentation I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max 0-shot 1-shot 2-shot 4-shot carpet bottle hazelnut leather cable capsule grid pill transistor metal nut screw toothbrush zipper tile wood mean carpet bottle hazelnut leather cable capsule grid pill transistor metal nut screw toothbrush zipper tile wood mean carpet bottle hazelnut leather cable capsule grid pill transistor metal nut screw toothbrush zipper tile wood mean carpet bottle hazelnut leather cable capsule grid pill transistor metal nut screw toothbrush zipper tile wood mean 100.0 90.7 94.8 100.0 83.6 95.2 98.9 86.8 88.7 93.7 85.9 86.4 99.3 99.6 98.7 93.5 100.00.0 99.10.1 99.90.1 100.00.0 88.72.0 90.05.2 92.21.0 92.60.5 94.81.6 99.20.4 74.03.8 90.71.2 98.10.3 99.50.1 99.70.1 94.50.5 100.00.0 99.20.2 99.80.2 100.00.0 90.51.3 94.34.9 93.22.9 93.30.2 95.10.5 99.40.5 79.22.7 93.54.7 98.20.0 99.40.0 99.70.1 95.70. 100.00.0 99.50.2 99.90.0 100.00.0 92.10.7 97.50.4 95.02.2 94.00.1 95.51.3 99.50.5 82.63.5 96.64.3 98.30.2 99.30.1 99.80.0 96.60.3 99.9 97.1 97.0 99.9 89.9 99.0 99.5 97.0 87.1 98.3 93.8 92.5 99.8 99.7 99.5 96.7 100.00.0 99.70.0 99.90.0 100.00.0 92.91.3 97.71.3 97.40.3 98.10.2 93.31.7 99.80.1 88.22.0 96.00.2 99.50.1 99.80.0 99.90.0 97.50.1 100.00.0 99.70.0 99.90.1 100.00.0 94.20.9 98.71.2 97.71.0 98.20.1 93.30.7 99.90.1 91.11.4 97.22.0 99.50.0 99.80.0 99.90.0 97.90.2 100.00.0 99.80.0 100.00.0 100.00.0 95.30.4 99.50.1 98.30.7 98.30.1 94.02.0 99.90.1 92.82.0 98.42.0 99.50.0 99.70.0 99.90.0 98.40.2 100.0 90.8 92.4 99.5 82.5 95.0 97.3 93.9 81.1 94.7 90.3 95.1 98.3 98.2 96.7 93. 100.00.0 97.70.6 99.30.6 100.00.0 85.90.8 95.51.2 90.20.6 95.20.3 89.01.8 98.70.2 87.40.1 92.63.0 97.90.3 97.50.3 98.70.8 95.00.0 100.00.0 98.10.4 98.80.9 100.00.0 88.21.2 96.31.1 91.22.5 95.50.3 87.50.7 98.90.4 87.80.6 94.74.0 97.90.3 97.50.3 98.90.4 95.40.1 99.60.3 98.20.3 99.50.3 100.00.0 88.50.5 97.10.5 93.02.1 95.90.1 89.40.9 98.90.4 89.21.6 96.92.5 98.00.2 97.40.2 99.20.0 96.00.0 7 99.2 92.4 97.6 99.1 76.6 95.8 97.4 89.9 69.3 76.5 97.9 87.7 91.8 95.8 96.9 90.9 99.20.0 95.60.0 98.90.0 99.60.0 88.30.7 97.30.1 96.50.3 93.30.1 74.60.4 89.00.3 97.50.1 97.60.2 93.50.1 96.10.0 97.10.2 94.30. 99.30.0 95.80.0 98.90.0 99.60.0 88.60.0 97.50.2 97.40.4 93.30.2 75.00.3 89.50.2 97.90.1 98.10.5 93.70.0 96.20.0 97.30.1 94.50.0 99.20.0 95.90.0 98.90.0 99.60.0 88.80.4 97.60.1 97.50.4 93.40.0 76.10.2 90.20.2 98.20.1 98.80.4 93.80.0 96.30.0 97.30.2 94.80.1 68.2 60.3 51.0 27.9 15.6 35.4 27.5 33.1 15.6 27.6 28.8 13.5 44.4 68.2 57.5 38.3 71.30.2 75.10.3 74.70.5 60.20.6 40.70.9 40.46.0 35.90.3 55.30.3 25.71.6 64.91.3 19.53.8 47.81.0 49.90.5 72.30.1 72.30.6 53.70.9 71.30.4 75.90.4 74.00.9 60.50.6 41.90.8 45.35.9 37.82.2 55.41.2 26.01.0 66.81.2 22.82.3 53.46.2 50.20.3 72.40.1 72.80.6 55.10.5 71.40.3 76.40.4 74.80.9 60.90.6 44.00.6 49.41.0 40.02.6 56.00.4 27.10.4 69.91.1 30.12.5 62.57.9 50.10.2 72.60.1 73.10.5 57.20. 67.2 56.4 53.7 43.7 24.7 40.2 39.7 35.2 19.5 34.5 34.5 22.9 47.7 67.8 60.9 43.2 68.40.1 71.10.4 70.30.4 60.80.2 42.71.1 43.73.9 43.60.4 55.30.3 26.91.4 62.20.9 25.13.9 49.90.5 52.10.3 68.70.0 68.60.3 54.00.7 68.50.1 72.00.3 70.10.3 60.80.1 44.11.0 47.04.1 44.51.0 55.61.2 26.80.9 64.31.3 28.12.2 54.24.8 52.00.3 68.80.1 68.80.2 55.00.3 68.40.2 72.60.4 70.90.6 60.90.3 45.91.0 49.81.0 44.91.3 56.70.6 27.90.5 67.91.4 34.22.1 62.67.1 51.90.1 68.90.0 68.90.3 56.80.7 Table 14. Image-level anomaly classification and pixel-level anomaly segmentation results on VisA with zero-shot and few-shot AdaptCLIP. Shot Catergoies Anomaly Classification Anomaly Segmentation I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max 0-shot 1-shot 2-shot 4-shot candle capsules cashew chewinggum fryum macaroni1 macaroni2 pcb1 pcb2 pcb3 pcb4 pipe fryum mean candle capsules cashew chewinggum fryum macaroni1 macaroni2 pcb1 pcb2 pcb3 pcb4 pipe fryum mean candle capsules cashew chewinggum fryum macaroni1 macaroni2 pcb1 pcb2 pcb3 pcb4 pipe fryum mean candle capsules cashew chewinggum fryum macaroni1 macaroni2 pcb1 pcb2 pcb3 pcb4 pipe fryum mean 87.4 93.9 86.1 96.7 91.5 83.3 69.9 84.1 64.6 65.3 97.7 96.7 84. 92.90.9 94.00.3 96.20.5 98.10.0 96.30.6 93.20.6 74.72.5 83.612.6 82.40.9 79.53.5 95.63.6 99.60.1 90.51.2 95.00.5 94.51.0 96.00.3 98.50.0 96.40.1 91.80.8 76.83.7 91.62.5 84.92.9 83.91.9 97.80.9 99.60.1 92.20.8 96.00.1 95.20.5 95.90.1 98.40.2 96.90.3 93.00.7 79.62.3 92.02.4 86.51.5 84.81.2 98.80.3 99.70.0 93.10.2 90.5 96.8 94.0 98.7 95.9 85.1 69.2 86.1 67.8 72.0 97.5 98.0 87.6 94.00.6 96.50.1 98.30.2 99.20.0 98.40.3 94.20.7 79.31.2 84.69.4 85.20.8 81.83.0 95.92.4 99.70.1 92.30.9 95.30.3 96.70.4 98.20.1 99.30.0 98.50.0 93.30.9 81.52.5 90.62.0 87.22.3 85.42.1 97.60.7 99.70.0 93.60. 95.80.2 97.00.2 98.20.0 99.30.1 98.70.1 94.40.6 83.91.5 91.01.8 88.31.2 86.60.7 98.40.4 99.70.0 94.30.2 81.4 91.6 84.0 96.4 88.5 75.6 71.5 80.4 69.9 67.3 93.5 95.6 83.0 87.91.1 90.70.6 93.20.7 96.30.5 93.21.2 85.10.3 71.01.8 80.47.7 76.11.4 74.31.5 90.75.2 98.50.0 86.51.0 89.10.8 91.90.8 93.20.6 96.70.6 94.00.7 85.11.0 72.12.5 84.93.6 78.22.7 78.40.7 93.41.9 98.00.4 88.00.7 90.30.3 92.50.9 93.60.2 96.50.4 94.80.0 86.50.7 72.62.3 84.92.5 79.02.5 78.21.5 95.50.6 98.20.2 88.50.2 98.7 94.2 93.4 99.5 95.3 98.1 98.2 96.2 93.0 87.9 95.3 98.4 95.7 98.90.0 96.80.6 92.80.7 99.70.0 96.70.0 99.00.0 98.50.2 98.30.0 94.70.2 91.80.0 95.80.1 99.20.2 96.80.0 98.90.0 97.40.3 93.40.6 99.60.0 96.90.1 99.00.0 98.80.2 98.60.1 95.30.2 92.10.0 96.40.2 99.20.1 97.10.0 98.90.0 97.70.2 93.60.5 99.60.0 96.90.1 99.00.0 99.10.1 98.60.1 95.60.0 92.30.0 96.80.1 99.30.0 97.30.0 21.6 33.4 22.1 82.7 26.5 16.4 2.2 15.7 12.4 4.6 31.9 44.0 26.1 23.71.0 29.02.5 46.71.1 81.50.3 41.21.4 27.01.3 14.20.7 57.92.2 18.90.6 28.80.9 34.63.1 63.44.3 38.90. 24.10.8 32.91.7 46.91.5 81.80.4 42.01.6 26.11.0 15.20.6 65.25.8 20.90.3 33.30.9 38.33.4 61.80.3 40.70.6 24.00.6 34.01.0 46.41.3 81.80.6 42.41.0 27.41.9 16.30.4 67.54.7 23.00.2 36.30.6 40.23.2 62.60.4 41.80.6 35.2 44.1 27.2 77.8 33.6 23.0 6.4 24.6 20.0 10.4 36.8 52.4 32.6 37.61.4 36.11.7 53.80.7 76.40.4 45.80.9 33.90.6 25.60.2 58.51.3 28.90.7 34.62.1 38.62.4 65.62.7 44.60.4 38.00.7 39.01.0 54.01.2 76.60.2 46.31.5 32.91.7 26.30.7 64.34.7 30.80.3 39.00.7 41.83.0 64.50.5 46.10.4 37.80.4 39.70.5 54.31.0 76.60.4 46.31.0 33.71.3 27.40.1 66.23.8 32.70.2 42.70.3 43.43.0 65.30.1 47.20. Table 15. Image-level anomaly classification and pixel-level anomaly segmentation results on MVTec 3D with zero-shot and few-shot AdaptCLIP."
        },
        {
            "title": "Anomaly Segmentation",
            "content": "I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max 0-shot 1-shot 2-shot 4-shot bagel cable gland carrot cookie tire rope foam dowel peach potato mean bagel carrot dowel potato rope cable gland cookie foam peach tire mean bagel carrot dowel potato rope cable gland cookie foam peach tire mean bagel carrot dowel potato rope cable gland cookie foam peach tire mean 93.4 73.6 78.7 72.7 78.9 76.8 81.2 74.0 86.5 70.8 78.6 95.91.0 85.32.1 68.65.6 76.42.1 95.33.1 70.54.2 74.70.4 79.71.8 88.42.1 82.02.0 81.71.5 96.90.7 86.81.4 69.66.2 77.00.5 96.02.5 71.33.9 77.50.8 80.61.6 90.11.2 83.32.4 82.91.1 96.90.2 88.30.3 73.93.6 78.81.2 96.00.7 75.61.1 79.21.6 80.01.7 90.00.9 83.41.3 84.20. 98.4 92.4 94.7 92.0 93.7 90.2 95.0 91.1 96.4 90.0 93.4 99.00.2 96.50.6 89.62.5 92.10.3 98.01.3 91.51.6 92.20.3 94.50.6 97.00.5 94.40.6 94.50.5 99.30.2 97.10.3 89.92.8 92.00.2 98.31.0 91.61.5 93.30.2 94.70.5 97.50.3 94.90.8 94.80.4 99.30.0 97.40.2 91.00.8 92.90.6 98.30.3 93.20.4 93.80.4 94.60.5 97.50.2 95.20.4 95.30.2 92.7 90.2 91.3 88.0 87.4 81.5 89.9 90.1 91.9 89.8 89.3 95.01.2 92.10.8 89.70.7 90.40.4 93.42.8 90.20.4 88.30.2 90.20.9 91.80.6 88.80.8 91.00. 95.71.0 92.30.7 89.40.4 90.30.2 93.62.0 90.30.5 88.50.3 91.00.5 92.20.4 89.20.8 91.20.2 95.90.5 92.70.3 90.30.9 90.20.6 92.50.9 90.70.6 89.00.8 91.00.7 92.20.6 88.40.5 91.30.3 9 99.6 98.2 98.1 96.3 97.9 96.9 89.9 96.6 99.1 99.4 97.2 99.60.0 99.20.0 97.10.1 99.50.1 98.00.4 97.70.1 97.50.2 89.60.4 99.30.0 98.80.0 97.70.0 99.70.0 99.10.0 97.30.4 99.60.0 98.10.2 97.80.2 97.70.1 90.10.3 99.40.1 98.90.0 97.80. 99.70.0 99.00.0 98.00.2 99.60.0 98.10.1 98.40.2 97.80.0 90.50.2 99.40.0 98.90.0 98.00.0 73.1 30.1 24.5 42.9 29.7 27.2 23.7 5.8 27.5 29.9 31.4 77.00.4 40.21.1 9.70.5 41.55.0 40.61.2 22.04.3 53.03.8 32.70.4 48.81.8 41.31.2 40.70.6 76.80.7 41.02.0 10.42.3 43.32.1 41.30.8 25.61.8 56.80.7 33.20.3 52.46.1 42.71.0 42.31.1 77.21.4 43.40.1 15.43.4 44.71.9 41.21.0 33.22.4 57.90.5 33.40.1 54.74.9 44.11.1 44.50.3 67.2 33.6 31.4 44.2 36.7 29.8 32.4 11.9 37.4 36.8 36. 71.20.2 44.30.7 14.30.4 46.43.7 46.00.5 28.05.9 51.73.6 42.80.2 48.72.0 44.31.1 43.70.7 71.30.5 45.01.5 15.02.9 48.30.6 47.00.4 32.23.1 55.60.4 42.90.0 51.34.8 45.60.8 45.41.1 71.51.1 47.90.5 21.13.4 49.30.8 47.20.8 38.51.3 56.80.7 43.00.2 53.23.2 46.80.8 47.60.2 Table 16. Image-level anomaly classification and pixel-level anomaly segmentation results on DTD with zero-shot and few-shot AdaptCLIP. Shot Catergoies Anomaly Classification Anomaly Segmentation I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max 0-shot 1-shot 2-shot 4-shot Woven 001 Woven 127 Woven 104 Stratified 154 Blotchy 099 Woven 068 Woven 125 Marbled 078 Perforated 037 Mesh 114 Fibrous 183 Matted 069 mean Woven 001 Woven 127 Woven 104 Stratified 154 Blotchy 099 Woven 068 Woven 125 Marbled 078 Perforated 037 Mesh 114 Fibrous 183 Matted 069 mean Woven 001 Woven 127 Woven 104 Stratified 154 Blotchy 099 Woven 068 Woven 125 Marbled 078 Perforated 037 Mesh 114 Fibrous 183 Matted 069 mean Woven 001 Woven 127 Woven 104 Stratified 154 Blotchy 099 Woven 068 Woven 125 Marbled 078 Perforated 037 Mesh 114 Fibrous 183 Matted 069 mean 100.0 94.8 98.9 98.1 98.3 97.0 100.0 99.1 93.7 85.6 98.8 87.3 96.0 100.00.0 97.41.0 98.20.4 99.40.4 100.00.0 96.51.0 100.00.0 100.00.0 95.11.0 90.00.4 100.00.0 99.40.1 98.00. 100.00.0 97.80.4 99.00.2 100.00.0 100.00.0 96.90.5 100.00.0 100.00.0 96.10.4 90.10.6 100.00.0 99.70.0 98.30.0 100.00.0 98.40.2 99.10.0 100.00.0 100.00.0 96.90.5 100.00.0 100.00.0 96.50.2 90.81.3 100.00.0 99.90.1 98.50.1 99.8 95.4 99.7 99.5 99.4 98.3 99.9 99.7 98.4 94.2 99.6 96.7 98.4 99.80.0 98.20.6 99.60.1 99.80.1 99.90.0 98.20.6 99.90.0 99.90.0 98.80.2 96.20.2 99.90.0 99.80.0 99.10.0 99.80.0 98.30.3 99.70.0 99.90.0 99.90.0 98.40.2 99.90.0 99.90.0 99.00.1 96.20.2 99.90.0 99.80.0 99.20.0 99.80.0 98.70.2 99.70.0 99.90.0 99.90.0 98.40.3 99.90.0 99.90.0 99.10.0 96.50.4 99.90.0 99.90.0 99.30. 100.0 88.4 98.1 96.4 99.4 93.0 100.0 98.1 93.9 84.7 97.5 91.2 95.1 100.00.0 95.61.3 97.10.3 98.10.9 99.80.3 93.92.5 100.00.0 100.00.0 94.11.2 89.40.3 100.00.0 98.30.8 97.20.1 100.00.0 96.60.3 98.30.6 100.00.0 99.80.3 94.61.2 100.00.0 100.00.0 95.20.3 88.40.4 99.80.3 99.20.3 97.70.1 99.80.3 96.80.5 97.70.3 100.00.0 100.00.0 95.51.6 100.00.0 100.00.0 95.30.3 89.10.2 100.00.0 99.80.3 97.80.1 10 99.8 94.5 95.2 99.6 99.5 98.7 99.5 99.2 93.5 93.7 99.1 99.5 97. 99.60.1 93.20.5 94.80.6 99.70.0 99.90.0 96.80.3 99.70.0 99.60.0 93.00.5 92.80.1 99.60.0 99.50.1 97.40.0 99.60.0 94.10.7 95.70.4 99.70.0 99.90.0 96.80.4 99.70.0 99.60.0 92.90.1 93.30.5 99.60.0 99.60.1 97.60.0 99.70.0 94.71.2 95.90.1 99.70.0 99.90.0 97.70.4 99.70.0 99.60.0 93.50.1 94.20.3 99.70.0 99.70.0 97.80.1 78.2 54.0 67.6 77.6 79.7 55.8 73.9 75.4 62.3 58.0 68.8 73.6 68.7 78.90.5 57.61.9 67.60.8 84.30.8 92.10.1 64.82.9 84.60.1 87.10.5 67.40.9 58.80.5 90.60.2 88.90.4 76.90.1 79.10.8 61.71.5 69.80.5 83.60.3 92.10.0 65.43.0 84.40.1 87.20.5 66.90.7 58.30.6 91.10.4 89.50.3 77.40. 80.01.5 62.62.5 70.10.2 84.41.0 92.10.0 68.53.2 84.90.2 87.20.3 68.30.3 59.40.8 91.40.2 89.80.2 78.20.2 71.7 54.6 64.8 68.9 69.4 53.4 66.4 67.5 61.7 58.1 60.7 65.8 63.6 74.10.2 59.81.9 66.10.5 74.61.2 85.80.0 60.42.7 76.80.0 80.40.6 65.10.5 59.50.5 84.10.2 82.60.1 72.40.2 74.00.5 63.41.1 66.90.4 74.10.2 85.80.0 61.22.9 76.70.2 80.60.4 64.70.1 59.00.4 84.50.4 82.90.1 72.80.3 74.61.2 63.81.7 67.00.3 75.71.1 85.70.0 64.42.7 77.40.3 80.50.3 65.40.3 60.00.5 84.80.2 82.90.2 73.50.1 Table 17. Image-level anomaly classification and pixel-level anomaly segmentation results on MPDD with zero-shot and few-shot AdaptCLIP. Shot Catergoies Anomaly Classification Anomaly Segmentation I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max bracket black bracket brown bracket white connector metal plate tubes mean bracket black bracket brown bracket white connector metal plate tubes mean bracket black bracket brown bracket white connector metal plate tubes mean bracket black bracket brown bracket white connector metal plate tubes mean 65.8 56.3 61.4 71.4 88.6 97.8 73.6 76.75.1 63.05.1 82.65.7 83.57.6 100.00.0 96.91.4 83.82.2 76.04.0 63.02.8 79.43.9 89.83.2 100.00.0 98.20.7 84.40.7 80.72.6 66.12.9 83.34.8 93.00.9 100.00.0 97.70.8 86.81. 73.1 72.1 54.7 53.3 94.8 98.9 74.5 79.96.9 74.35.3 78.79.8 67.213.0 99.80.0 98.60.7 83.13.7 79.46.8 73.93.7 76.28.3 79.46.3 99.80.0 99.10.3 84.72.1 85.43.3 76.33.8 79.68.9 86.43.0 99.80.0 99.00.3 87.72.4 77.9 80.3 71.4 61.9 91.8 94.8 79.7 81.52.8 80.30.0 82.80.8 71.95.6 99.80.3 95.42.3 85.31. 80.00.8 80.10.3 78.62.6 79.01.9 100.00.0 96.60.9 85.70.7 81.50.5 81.20.3 82.61.0 85.31.0 100.00.0 96.01.3 87.80.0 96.4 92.6 99.5 96.1 93.3 97.8 95.9 97.70.3 93.00.4 99.40.3 96.70.3 98.90.4 98.80.1 97.40.2 97.70.3 93.70.2 99.40.3 97.00.2 99.20.1 98.90.3 97.70.0 98.10.0 94.40.2 99.50.3 97.40.2 99.30.0 98.90.1 97.90. 15.8 2.9 4.0 14.4 60.4 54.6 25.3 8.62.2 3.20.2 9.75.0 19.89.5 93.12.8 66.52.0 33.52.5 10.84.1 3.70.1 8.74.3 22.67.5 95.10.6 69.03.1 35.00.7 15.11.7 4.50.3 9.24.3 29.74.5 95.90.4 70.21.8 37.41.1 GT Mask 1-shot Figure 8. Qualitative comparisons of our AdaptCLIP with different prompt numbers on MPDD. GT Mask 0-shot 1-shot 0-shot 4-shot Query 0-shot 1-shot 2-shot 4-shot Query 26.4 6.7 9.1 20.5 61.5 55.1 29. 19.53.5 6.60.3 17.46.7 24.56.9 85.72.9 64.82.0 36.42.4 21.86.0 7.00.3 17.97.5 27.84.2 87.60.8 66.82.9 38.20.2 27.22.5 8.50.5 19.17.1 32.82.5 89.00.6 67.72.2 40.71.1 4-shot Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 9. Qualitative comparisons of our AdaptCLIP with different prompt numbers on KSDD. 11 Table 18. Image-level anomaly classification and pixel-level anomaly segmentation results on Real-IAD with zero-shot and one-shot AdaptCLIP. Shot Catergoies Anomaly Classification Anomaly Segmentation I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max 0-shot 1-shot audiojack bottle cap button battery end cap eraser fire hood mint mounts pcb phone battery plastic nut plastic plug porcelain doll regulator rolled strip base sim card set switch tape terminalblock toothbrush toy toy brick transistor1 block usb usb adaptor vcpill wooden beads woodstick zipper mean audiojack bottle cap button battery end cap eraser fire hood mint mounts pcb phone battery plastic nut plastic plug porcelain doll regulator rolled strip base sim card set switch tape terminalblock toothbrush toy toy brick transistor1 block usb usb adaptor vcpill wooden beads woodstick zipper mean 63.5 66.2 70.6 66.1 86.4 80.0 66.4 79.9 61.9 82.1 68.4 69.5 77.9 73.0 87.1 89.4 67.8 93.0 76.8 58.4 63.3 69.3 71.3 77.2 63.0 72.8 80.2 82.6 75.6 86.1 74.2 72.90.7 80.20.5 75.10.6 75.12.2 88.21.1 87.00.3 70.80.4 85.70.4 73.71.5 88.20.9 79.00.1 82.00.5 92.50.3 73.00.8 90.50.5 95.80.2 83.40.7 94.10.3 84.60.7 75.21.0 69.42.7 70.60.5 73.81.3 83.80.4 79.00.5 83.40.2 85.10.4 88.30.3 79.70.6 95.01.0 81.80.3 48.2 63.1 78.0 70.9 85.8 70.6 65.7 66.7 73.0 81.1 55.5 57.6 60.7 53.4 93.5 92.9 72.8 91.4 81.8 66.2 71.4 61.0 75.5 64.9 57.7 62.2 75.6 79.7 55.9 90.5 70.8 62.81.7 77.80.6 82.40.4 81.31.6 86.21.2 79.00.4 70.10.1 72.00.7 83.20.9 88.01.1 72.70.6 78.20.6 88.30.7 58.31.0 95.00.4 96.60.1 86.90.6 92.90.3 88.10.6 81.30.9 76.22.1 64.31.2 77.92.2 78.50.8 79.30.6 78.90.2 83.70.8 88.30.2 65.70.9 96.90.6 80.40.2 53.4 61.2 73.4 74.0 75.9 64.8 65.0 67.5 75.5 70.7 54.7 60.5 67.5 53.7 85.7 84.6 70.5 82.8 71.5 69.9 73.5 61.6 75.5 62.8 65.0 63.6 67.7 73.1 56.2 86.2 68. 57.61.2 71.50.3 74.10.2 75.50.7 75.21.6 74.10.3 66.80.3 73.80.6 76.30.4 78.71.5 64.00.3 68.21.0 79.80.5 53.81.1 88.00.4 90.60.3 76.40.6 84.70.6 77.20.5 71.60.2 75.40.9 61.50.3 75.20.4 68.20.8 70.50.7 70.50.6 72.61.0 77.60.3 59.70.5 90.81.3 73.30.2 12 94.9 98.0 98.2 95.2 99.4 99.0 93.6 97.9 94.4 76.1 96.1 96.3 99.1 92.1 98.6 99.5 84.6 97.0 96.8 91.1 81.9 97.6 91.8 98.6 93.2 97.1 98.0 98.4 97.5 95.5 94.9 96.80.0 98.70.0 98.30.0 96.70.2 99.50.0 99.20.0 92.70.1 98.40.0 97.40.1 99.20.0 97.60.0 97.20.3 99.50.0 96.00.2 99.40.0 99.70.0 93.80.0 98.90.0 98.80.0 93.60.3 84.10.4 97.20.1 95.50.0 99.00.0 96.60.1 98.80.0 97.90.0 98.70.0 97.70.0 97.40.2 97.10.0 11.3 26.6 50.8 10.0 37.2 37.1 26.2 39.0 7.4 23.3 33.3 23.0 42.9 16.2 35.2 58.8 11.8 40.5 36.5 11.1 9.6 22.7 22.7 33.9 5.1 15.3 48.4 41.9 46.6 22.3 28.2 24.74.3 32.60.2 58.51.0 26.01.5 50.60.1 33.30.5 26.50.4 37.00.4 24.31.1 59.51.2 41.90.2 26.90.8 49.00.5 10.51.2 34.10.4 58.40.6 34.10.9 45.50.2 47.70.4 23.31.3 12.70.8 22.93.0 31.13.9 49.71.3 23.90.6 31.30.3 53.81.2 49.00.4 53.41.1 26.40.9 36.60. 20.9 36.8 54.2 18.2 45.6 42.8 38.6 46.3 13.5 34.5 43.4 33.4 44.9 29.3 40.8 59.6 18.9 50.0 44.2 17.7 17.8 29.8 32.3 43.1 9.9 25.3 53.0 46.2 52.2 26.2 35.6 32.04.7 39.20.0 57.80.7 33.71.4 50.90.1 41.60.2 37.60.3 43.10.6 32.01.3 62.90.7 44.90.3 37.70.2 49.40.3 19.31.3 42.91.0 61.80.1 43.20.9 51.30.1 52.10.6 31.71.3 19.61.1 29.02.7 38.03.8 55.91.0 34.80.5 36.40.4 60.31.2 53.70.3 55.00.9 27.00.8 42.50.1 Table 19. Image-level anomaly classification and pixel-level anomaly segmentation results on Real-IAD with 2-shot and 4-shot AdaptCLIP. Shot Catergoies Anomaly Classification Anomaly Segmentation 2-shot 4-shot I-AUROC I-AUPR audiojack bottle cap button battery end cap eraser fire hood mint mounts pcb phone battery plastic nut plastic plug porcelain doll regulator rolled strip base sim card set switch tape terminalblock toothbrush toy toy brick transistor1 block usb usb adaptor vcpill wooden beads woodstick zipper mean audiojack bottle cap button battery end cap eraser fire hood mint mounts pcb phone battery plastic nut plastic plug porcelain doll regulator rolled strip base sim card set switch tape terminalblock toothbrush toy toy brick transistor1 block usb usb adaptor vcpill wooden beads woodstick zipper mean 74.71.1 80.50.6 76.31.0 77.30.4 88.91.0 87.40.3 71.90.4 86.50.1 76.10.8 89.50.1 79.90.1 82.60.3 92.90.1 74.10.8 92.41.1 96.20.2 85.90.6 94.50.2 86.41.2 76.20.6 68.73.7 71.40.2 74.61.3 84.30.3 81.40.3 84.00.1 88.00.5 88.80.2 80.80.2 96.20.4 82.90.2 75.70.6 81.60.2 76.80.3 79.60.5 89.90.1 87.60.2 72.90.1 86.50.2 76.51.1 90.30.4 80.40.1 82.70.1 92.90.2 75.60.4 93.81.1 96.40.2 87.60.5 94.90.1 89.10.2 77.60.4 70.42.6 72.50.4 78.91.1 85.00.3 83.20.1 84.80.2 88.50.4 88.90.0 81.00.3 97.00.0 83.90.2 66.20.9 77.80.8 83.60.6 83.00.1 87.10.9 79.50.3 71.50.8 72.50.2 85.00.6 89.20.0 74.00.3 78.30.4 89.00.2 60.61.9 95.80.8 96.90.1 89.20.6 93.40.2 89.40.9 82.10.4 75.72.8 65.30.2 78.91.7 79.20.4 82.10.5 79.40.1 86.20.4 88.70.2 67.30.1 97.60.3 81.50.1 67.61.6 79.10.4 83.90.4 84.60.4 88.10.0 79.90.3 73.10.5 72.10.3 85.40.6 89.80.3 74.90.1 78.40.1 88.70.3 63.52.1 96.40.9 97.00.2 90.70.5 93.90.3 91.40.1 83.20.1 78.21.8 66.71.2 84.10.9 80.80.4 84.00.2 79.90.2 86.90.5 88.70.1 67.80.5 98.10.0 82.60.0 I-F1max 58.51.4 71.90.7 74.40.5 76.30.3 76.21.3 74.50.7 67.40.2 74.50.3 76.80.3 79.60.1 65.30.2 67.70.5 80.60.3 55.70.5 89.70.8 90.80.3 78.40.9 85.80.4 78.60.8 72.00.5 74.91.4 61.70.2 75.20.2 68.70.4 72.70.7 71.50.4 74.90.7 78.40.3 61.40.2 92.50.7 74.20.1 59.40.7 73.30.3 74.60.4 77.70.3 78.10.2 74.90.3 67.70.2 75.10.3 76.70.5 81.00.6 65.20.3 68.40.4 80.60.3 56.50.7 91.50.1 91.30.4 80.20.5 86.10.8 81.20.5 72.90.5 74.50.6 62.60.4 76.50.5 71.20.9 75.00.0 72.40.5 76.00.7 78.40.1 61.50.9 93.90.0 75.20.1 13 P-AUROC P-AUPR 96.90.0 98.80.0 98.50.0 97.00.0 99.50.0 99.30.0 93.20.0 98.50.0 97.80.0 99.30.0 97.80.0 97.40.1 99.60.0 96.70.1 99.60.0 99.70.0 93.90.0 99.00.0 98.90.0 93.60.4 84.20.2 97.40.1 95.40.2 99.10.0 96.90.0 98.90.0 98.00.0 98.70.0 97.90.0 97.70.0 97.30. 97.10.1 98.80.0 98.60.0 97.30.0 99.60.0 99.30.0 93.50.0 98.60.0 98.10.0 99.30.0 97.90.0 97.40.1 99.60.0 97.30.1 99.60.0 99.70.0 94.00.0 99.00.0 99.10.0 93.60.1 84.30.1 97.50.0 95.50.1 99.20.0 97.20.0 98.90.0 98.00.0 98.70.0 97.90.0 97.80.0 97.40.0 29.44.2 32.70.8 61.30.4 27.30.2 49.90.0 33.30.4 27.40.7 37.10.4 28.20.8 61.90.4 42.00.1 28.10.1 49.20.2 15.72.9 37.12.5 58.30.3 36.60.4 46.00.3 49.20.9 23.51.9 11.41.0 23.60.3 32.12.6 49.60.6 26.30.9 31.90.2 54.30.7 48.70.2 54.60.3 27.30.2 37.80.1 33.73.1 33.20.3 62.50.8 28.20.6 49.90.4 33.40.5 28.60.5 36.60.8 31.40.9 62.90.4 41.70.2 28.10.1 49.20.3 18.83.7 40.11.9 58.10.7 38.50.3 45.90.3 50.20.7 23.90.9 16.71.7 26.42.3 37.20.7 50.90.5 29.60.2 32.00.4 54.80.6 48.40.1 55.30.2 28.30.1 39.10.3 P-F1max 36.84.3 39.20.6 60.10.4 34.50.5 50.50.0 41.40.2 38.00.5 43.30.3 35.51.0 63.40.1 44.80.2 37.80.2 49.20.1 25.24.0 44.82.6 61.60.0 45.90.2 51.20.1 53.10.5 32.32.0 17.81.1 30.10.9 38.72.9 55.90.5 37.10.7 36.50.3 60.60.8 53.70.2 55.80.2 27.80.3 43.40.2 41.23.2 39.30.2 60.80.7 34.90.4 50.50.2 41.40.4 38.30.3 42.30.4 38.11.0 63.40.2 44.90.1 37.90.3 49.00.3 29.14.6 47.31.8 61.30.3 47.60.4 51.10.2 53.70.4 33.01.1 24.11.8 31.91.7 43.60.5 56.70.2 39.70.1 36.40.4 61.20.4 53.50.0 56.40.1 29.40.2 44.60. Table 20. Image-level anomaly classification and pixel-level anomaly segmentation results on BTAD with zero-shot and few-shot AdaptCLIP."
        },
        {
            "title": "Anomaly Segmentation",
            "content": "I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max 0-shot 1-shot 2-shot 4-shot 01 02 03 mean 01 02 03 mean 01 02 03 mean 01 02 03 mean 96.1 81.1 95.9 91.0 97.80.5 84.10.3 98.20.8 93.40.0 97.90.2 84.00.7 98.50.1 93.40.2 97.20.9 83.80.3 98.80.1 93.30. 98.6 96.9 81.0 92.2 99.20.2 97.50.0 90.72.9 95.80.9 99.10.0 97.50.1 91.20.5 95.90.1 99.00.3 97.40.0 92.60.3 96.40.1 95.8 93.2 79.5 89.5 96.51.0 93.70.0 84.54.0 91.61. 96.10.5 93.50.0 85.50.9 91.70.5 96.10.5 93.60.2 86.61.5 92.10.4 93.3 93.4 94.7 93.8 96.40.1 96.30.1 97.00.4 96.60.2 96.50.0 96.30.1 97.30.2 96.70.1 96.50.0 96.30.0 97.50.1 96.80. 51.1 60.5 13.8 41.8 61.20.3 71.80.4 48.63.0 60.61.0 61.40.1 71.60.2 49.81.7 61.00.6 61.70.2 71.80.1 53.40.8 62.30.3 54.0 61.6 23.4 46.3 60.30.5 65.20.3 50.62.0 58.70. 60.30.2 65.10.0 51.01.1 58.80.4 60.40.2 65.30.1 53.20.7 59.60.2 Table 21. Image-level anomaly classification and pixel-level anomaly segmentation results on KSDD with zero-shot and few-shot AdaptCLIP. Shot Catergoies Anomaly Classification Anomaly Segmentation I-AUROC I-AUPR 0-shot 1-shot 2-shot 4-shot electrical commutators electrical commutators electrical commutators electrical commutators 98. 96.90.3 97.20.0 97.00.2 95.7 91.80.2 92.40. 91.70.9 I-F1max 92.3 89.60.5 89.40.9 88.61. P-AUROC P-AUPR 98.1 98.20.1 98.10.1 98.00. 58.3 57.81.2 57.51.1 56.41.4 P-F1max 58. 58.40.5 58.10.6 57.20.7 Table 22. Image-level anomaly classification and pixel-level anomaly segmentation results on four medical datasets, Br35H, Covid, Kvasir and Endo with zero-shot and few-shot AdaptCLIP."
        },
        {
            "title": "Anomaly Segmentation",
            "content": "I-AUROC I-AUPR I-F1max P-AUROC P-AUPR P-F1max 0-shot 1-shot 2-shot 4-shot Br35H Covid Kvasir Endo Br35H Covid Kvasir Endo Br35H Covid Kvasir Endo Br35H Covid Kvasir Endo 94.8 86.5 - - 93.72.4 91.82.5 - - 94.01.7 94.90.9 - - 93.72.0 95.80.9 - - 95.1 54.4 - - 92.13.1 78.84.6 - - 92.22.5 83.61.9 - - 91.82.9 85.81.2 - - 87.7 55.4 - - 87.83.2 73.16.6 - - 88.32.0 78.72.4 - - 88.32.3 81.21.1 - - 14 - - 82.1 86.5 - - 83.30.5 86.51.2 - - 83.50.8 86.61. - - 83.00.5 85.80.7 - - 45.3 52.0 - - 49.23.8 52.44.7 - - 49.03.4 53.14.2 - - 47.52.2 52.23.1 - - 50.6 54. - - 52.10.7 55.22.0 - - 52.61.4 55.81.7 - - 52.10.8 55.61.2 Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 10. Qualitative comparisons of our AdaptCLIP with different prompt numbers on VisA. Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 11. Qualitative comparisons of our AdaptCLIP with different prompt numbers on MVTec 3D. 15 Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 12. Qualitative comparisons of our AdaptCLIP with different prompt numbers on DTD. Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 13. Qualitative comparisons of our AdaptCLIP with different prompt numbers on Kvasir. Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 14. Qualitative comparisons of our AdaptCLIP with different prompt numbers on Endo. Query GT Mask 0-shot 1-shot 4-shot Query GT Mask 0-shot 1-shot 4-shot Figure 15. Qualitative comparisons of our AdaptCLIP with different prompt numbers on Real-IAD."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Siemens Corporate Research",
        "Technical University of Munich",
        "Tencent YouTu Lab"
    ]
}