{
    "paper_title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play",
    "authors": [
        "Qinsi Wang",
        "Bo Liu",
        "Tianyi Zhou",
        "Jing Shi",
        "Yueqian Lin",
        "Yiran Chen",
        "Hai Helen Li",
        "Kun Wan",
        "Wentian Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero."
        },
        {
            "title": "Start",
            "content": "VISION-ZERO: SCALABLE VLM SELF-IMPROVEMENT VIA STRATEGIC GAMIFIED SELF-PLAY Qinsi Wang1, Bo Liu2, Tianyi Zhou3, Jing Shi4, Yueqian Lin1, Yiran Chen1, Hai Helen Li1, Kun Wan4, Wentian Zhao4* 5 2 0 2 9 ] . [ 1 1 4 5 5 2 . 9 0 5 2 : r 1Duke University 2National University of Singapore 3University of Maryland 4Adobe Inc. https://github.com/wangqinsi1/Vision-Zero"
        },
        {
            "title": "ABSTRACT",
            "content": "Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of visionlanguage models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the models reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at here."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent breakthroughs in vision-language models (VLMs) have demonstrated remarkable capabilities across diverse multimodal tasks (Achiam et al., 2023; Team et al., 2023). However, current training paradigms face fundamental scalability constraints: they depend heavily on human-curated data through supervised fine-tuning (SFT) (Liu et al., 2023), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Sun et al., 2023), and carefully engineered reward functions for reinforcement learning with verifiable rewards (RLVR) (Guo et al., 2025). This dependency creates two critical bottlenecks. First, data scarcity problemthe extraordinary cost of multimodal annotation limits both scale and diversity of training data, with datasets like COCO Attributes requiring $60,480 for 200,000 objects (Patterson & Hays, 2016), Ego4D consuming over 250,000 annotation hours (Grauman et al., 2022), and Visual Genome mobilizing 33,000 annotators (Krishna et al., 2017). Second, knowledge ceilingmodel capabilities remain fundamentally bounded by human-generated supervision, preventing VLMs from discovering strategies beyond human expertise. Self-Play offers solution by eliminating human supervision through competitive dynamics (Silver et al., 2017; Tesauro, 1995). In self-play, models learn by engaging in competitive interactions with Corresponding authors. 1 (a) Supervised Learning (b) Reinforcement Learning Figure 1: Vision-Zero Paradigm. (a) Supervised learning depends on human-curated reasoning trajectories; (b) Reinforcement Learning, although enabling models to autonomously learn reasoning processes via validated rewards, still relies heavily on expert-designed question-answer pairs. (c) In contrast, Vision-Zero is novel self-improvement paradigm entirely independent of human experience. It constructs self-play games by leveraging image pairs that exhibit visual differences. Through the interactive and strategic game, Vision-Zero continuously generates training data for VLMs, enabling the model to achieve scalable self-improvement. (c) Vision-Zero copies of themselves, receiving automatic feedback based on the outcomes of each interaction. As the model improves, its opponents correspondingly advance, thus maintaining consistently challenging learning environment and driving continuous improvement. By removing the need for human supervision during data generation, self-play has already surpassed the knowledge ceiling across many domains: from TD-Gammons backgammon supremacy (Tesauro, 1995) to AlphaGos conquest of Go (Silver et al., 2016; 2017) to OpenAI Fives mastery of complex team coordination (Berner et al., 2019). With the growing capabilities of LLMs, recent work has begun to import Self-Play into LLMs training to reduce dependence on human intervention. These approaches construct Language Gamification frameworks wherein LLMs compete under clearly defined game rules, incrementally enhancing their competencies. For example, SPIRAL enhances LLM reasoning by having models play games such as Tic-Tac-Toe and Kuhn Poker (Liu et al., 2025); Absolute Zero frames self-play between proposer and solver (Zhao et al., 2025), achieving state-of-the-art results on mathematics and coding tasks. However, extending self-play to VLMs remains largely unexplored, despite the prohibitive costs of multimodal data that make such an approach particularly urgent. An ideal self-play game environment should satisfy the following four conditions: (1) The skills acquired by agents in order to win the game should closely align with those required by the target tasks. (2) Skill growth should be scalable: as self-play progresses, the environment should continually escalate difficulty so that ever stronger agents can emerge rather than converging to fixed upper bound. (3) The environment should be sufficiently diverse and complex to enable wide range of target tasks can satisfy conditions (1). (4) The environment should require no external data or only small amount of low-cost data, such as label-free data. To the best of our knowledge, existing visual reasoning games fail to satisfy all of the above criteria simultaneously. For instance, Sudoku satisfies conditions (2) and (4), but fails to meet (1) and (3). Due to the multimodal nature of VLMs, designing self-play environment that fulfills all four conditions requires joint consideration of both vision and language modalities, which is non-trivial. Inspired by language-based social deduction games, particularly those involving alternating rounds of statements and voting such as Who Is the Spy?, we propose novel visual reasoning game that addresses these four requirements. We present Vision-Zero, the first gamified self-play framework that enables scalable selfimprovement of VLMs without requiring human annotations. We design visual Who Is the Spy? game based on subtly differing image pairs, which are generated either by an automated image editing tool or rendered procedurally. By reasoning over and hypothesizing about these subtle differences, agents gradually acquire stronger visual reasoning capabilities. This setup compels models to engage in strategic reasoning across multiple roles while handling diverse visual inputs such as CLEVR scenes (Johnson et al., 2017), charts, and natural images. We further propose Iterative Self-Play Policy Optimization (Iterative-SPO), which alternates between Self-Play and RLVR. By incorporating verifiable supervision into self-play, Iterative-SPO stabilizes training and prevents premature convergence to equilibrium states, thereby ensuring consistent performance gains within the Vision-Zero framework. Vision-Zero provides domain-agnostic framework that effectively leverages diverse image inputs, enabling continuous improvement without reliance on task-specific datasets. Through carefully designed strategic visual gameplay, it strengthens reasoning, spatial understanding, and visual comprehension while reducing shortcut bias from text and negative capability transfer that are prevalent in conventional VLM training methods. Moreover, its reliance on automated image editing supports Figure 2: Performance Comparison of Vision-Zero with SOTA post-training methods. All models were post-trained on Qwen2.5-VL-7B. The numbers on the horizontal axis represent the accuracy of Qwen2.5-VL-7B on different tasks, while the vertical axis represents the change in accuracy of the trained model. Vision-Zero outperforms baselines trained on expensive human-labeled datasets. highly cost-efficient dataset construction. As shown in Fig. 2, Vision-Zero simultaneously enhances performance across tasks including reasoning, chart/OCR, and vision-centric tasks, surpassing state-of-the-art baselines trained on expensive human-labeled datasets. These results underscore Vision-Zeros substantial potential and broad applicability as pioneering zero-human-in-the-loop training paradigm. Our contributions are as follows: We propose Vision-Zero, the first zero-human-in-the-loop training paradigm for VLMs enabled by gamified self-play, which supports label-free, domain-agnostic inputs and enables highly cost-efficient dataset construction for scalable optimization. We introduce Iterative-SPO, novel algorithm alternating between Self-Play and RLVR to stabilize training and to avoid premature convergence. Extensive experiments demonstrate that Vision-Zero substantially enhances model performance across various general tasks, surpassing strong baselines trained on costly humanannotated datasets, especially on reasoning and mathematical tasks."
        },
        {
            "title": "2 VISION-ZERO: A GENERALIZABLE GAMIFICATION TRAINING FRAMEWORK",
            "content": "This section introduces Vision-Zero, general, scalable, and high-performing gamified VLM posttraining framework as illustrated in Fig. 3. We begin by describing the environment and training data (Sect. 2.1). Next, to achieve sustainable performance improvements, we propose Iterative-SPO, which alternates between Self-Play and RLVR (Sect. 2.2). Finally, we provide comprehensive analysis of the advantages of Vision-Zero compared to human-involved training methods (Sect. 2.3). 2.1 ENVIRONMENT AND DATA Strategic Environment. As shown in Fig. 2, Vision-Zero draws inspiration from natural languagebased social deduction games, Who is the Spy. In this setting, multiple players participate: nc civilians and single spy. Each player is assigned an image, where the spys image differs subtly from civilians, such as containing missing, added, modified object. Each round consists two stages: Clue Stage. In this stage, players are informed of their role (civilian or spy). Each player is then prompted to observe their image and provide verbal clue that reflects its content such as object descriptions or inferring from the image. Players speak in sequence, and each players clues become visible to subsequent players; however, their thought processes remain hidden. After multiple rounds clue stage, game enters decision stage. Decision Stage. In this stage, civilians are instructed to analyze all the provided clues in conjunction with their own image to identify the spy. Since the spy knows their identity, they do not participate in voting. If player is uncertain about who is spy, he can respond with \"n/a\". Both the reasoning and final votes remain private to players. Vision-Zero constitutes highly strategic and challenging gaming environment. In the clue stage, the spy must analyze and infer from others clues and their own image to identify altered elements, aligning their clues with common elements to mislead civilians. Civilians must provide accurate, clear clues to avoid suspicion while minimizing information leakage to the spy. During the decision stage, civilians further analyze images and clues meticulously to detect inconsistencies and accurately identify the spy. Detailed prompts for both stages are provided in the Appendix A.2.1 for reference. Label-Free and Domain-Agnostic Data Input. The input to Vision-Zero is label-free yet flexible: for each round, the environment requires only an image pair, where the original image Ic is provided 3 Figure 3: Overall Framework of Vision-Zero. Vision-Zero comprises three core components. Strategic Game Environment: Each role is required to exhibit strategic behavior tailored to diverse scenarios, thereby simultaneously necessitating multiple capabilities. Label-free and Domainagnostic Data Input: Vision-Zero accepts arbitrary inputs to promote diversity and generalization. To verify this, we train Qwen2.5-VL-7B for 100 iterations on Gobang and our environment and evaluate on MathVision; results show that Vision-Zero effective generalization. Iterative-SPO: We introduce novel two-stage training algorithm. In the clue stage, models are trained via Self-Play using zero-sum reward inversely proportional to votes received. In the decision stage, models undergo RLVR training with group normalization, using rewards based on vote correctness. to civilians and modified counterpart Is is provided to spy, forming an (Ic, Is) image pair. Thanks to the design of Vision-Zeros environment, it supports arbitrary image inputs, making it broadly applicable across domains. To validate this generality, we experiment with three types of data: CLEVR Data. (Johnson et al., 2017) We automatically rendered 2,000 image pairs using the CLEVR renderer. Each original image contains 46 randomly arranged objects, while the corresponding modified image has two objects altered in both color and shape. All objects in both original and modified images were randomly generated through automated scripting. The entire rendering process required approximately 6 hours on an NVIDIA A100 GPU. Example training set samples are illustrated in Fig. 4 (left). Chart Data. We randomly selected 1,000 images from the ChartQA (Masry et al., 2022) training set as the original image set. For each original image, we utilized Gemini2.5Flash (Comanici et al., 2025) to generate modified images by randomly swapping numerical attributes within each chart, producing modified images. The dataset includes line charts, pie charts, and bar charts. Examples from this dataset are illustrated in Fig. 4 (middle). Real-World Data. We randomly sampled 1,000 image pairs from ImgEdit (Ye et al., 2025) training set, high quality image editing dataset containing real-world single-turn editing pairs. Examples from this dataset are shown in Fig. 4 (right). Owing to recent advances in high-quality image-editing models like ChatGPT (OpenAI, 2024) and Nano Banana (Google DeepMind, 2024), the cost of generating Chart and Real-World datasets remains modest, on the order of tens of dollars. We provide detailed descriptions of the data generation pipelines in the Appendix A.2.2. Overall, Vision-Zero provides strategic game-based environment in which the model continuously generates reasoning supervision through interactive gameplay and learns from verifiable rewards, enabling scalable self-improvement. In addition, Vision-Zero supports label-free and domain-agnostic data construction, allowing users to build domain-specific datasets at minimal cost. As illustrated in the bottom-left of Fig. 3, Vision-Zero achieves sustained performance improvement on the MathVision validation set, outperforming the original model by 3%, which is unattainable in previously narrowly-defined game environments like Gobang. 4 Figure 4: Visualization of the datasets used in Vision-Zero. We employ three representative data in our experiments: (left) CLEVR-based data, (middle) Chart-based data, and (right) Real-world data. For visualization, difference parts are circled, which are not present in the SPY images in game. 2.2 ITERATIVE SELF-PLAY POLICY OPTIMIZATION To enable sustained performance improvement within Vision-Zero, we introduce Iterative Self-Play Policy Optimization (Iterative-SPO) which is novel optimization algorithm that alternates between self-play and RLVR. The workflow of Iterative-SPO is illustrated in Fig. 3. Notation. Assume each round has players: nc civilians and one spy, role set is defined as = {s} {c1, . . . , cnc}. The spy and civilians hold images Is and Ic, respectively. In clue stage, each player provide clue uk πk θ ( Ik, h) , based on clue history h. In decision stage, voting mechanism returns vote counts = (vs, vc1 , . . . , vnc), where vcj represents number of votes cj received due to being suspected of being spy, and vs represents the number of votes spy received. Self-Play Policy Optimization in Clue Stage. During this stage, players seek to avoid raising suspicion that they might be the spy. Moreover, the spy and civilians constitute two opposing sides, and we employ Self-Play Policy Optimization to continuously enhance the models capabilities. = β (cid:0)vs vc rclue (cid:80)nc Zero-Sum Reward. Their rewards are designed according to the zero-sum game principle. Based on these considerations, we define the Clue Stage reward rclue and rclue (cid:1) λ (cid:0)vcj vc where vc = 1 j=1 vcj denotes the average number of votes received by all civilians, β > 0 nc controls the intensity of competition between the spy and the civilians, and λ > 0 regulates the penalty for behavioral inconsistency among civilians. Eqa.1 ensures that the total reward between the spy and the civilians is zero, and that players receiving more votes are assigned lower rewards. as follows: (cid:1), = 1, . . . , nc. (cid:0)vs vc (cid:1), rclue cj β nc = (1) cj Role-Advantage Estimation (RAE). To mitigate the imbalance in win probability caused by asymmetric role information, we apply RAE (Liu et al., 2025). Specifically, we initialize RAE coefficient for the spy bs and the civilians bc to zero. The RAE coefficient and advantage at each round are: bs = αbs + (1 α)rclue , bc = αbc + (1 α) 1 nc nc(cid:88) j=1 rclue cj , Aclue = rclue bk, (2) where α denotes the decay rate, and the advantage values Aclue RAE from the original reward to eliminate information asymmetry. are computed by subtracting the Objective. With reference policy πref, the optimization objective of Clue Stage is, (cid:0)πk (cid:34) Lclue(θ) = (cid:34) + τclue log πk θ (uk Ik, h) Aclue DKL (cid:88) (cid:88) (cid:35) (cid:35) . (cid:1) (3) θ πk ref 1 kK 1 kK where the KL term constrains updates to remain close to πref, stabilizing learning and preventing degenerate utterances. Unbaselined returns are zero-sum to promote equilibrium-seeking dynamics. RLVR in the Decision Stage. During this stage, the objective of each player is to correctly identify and vote for the spy. Since civilians share aligned information, they can be regarded as single group. Therefore, we adopt the GRPO objective for Decision Stage. 5 Figure 5: Visualization of spy reasoning in Vision-Zero. comparison of model responses to identical scenarios before and after training, as evaluated by GPT-based scoring, reveals substantial improvements in planning, retrieval, decomposition, strategy formulation, and logical reasoning. Discrete Reward. Assume civilians take the full-round clues and outputs ˆsci qθ( H), = 1, . . . , nc, where sci can be index of player (indicating vote for the player as spy), or (indicating not clear who is spy and answer \"n/a\"): assume is the true spy index. Define reward (4) This reward encourages players to make well-reasoned inferences. Even under highly challenging conditions, it incentivizes acknowledging uncertainty rather than committing to an incorrect answer. = +1 if ˆsci = s, 0.5 elif ˆsci = , 1 rdec ci else. µr = mean(cid:2)rdec ci Group Norm & Objective. To remove round-specific difficulty, we apply group normalization: (cid:3), Adec ci where ε > 0 prevents division by zero. With reference distribution qref, we optimize the advantageweighted log-likelihood of the sampled votes with KL regularization: σr = std(cid:2)rdec ci µr)/(σr + ε), = 1, ..., nc = (rdec ci (5) (cid:3), (cid:34) Ldec(θ) = 1 nc nc(cid:88) i=1 Adec ci log qθ(ˆsci H) (cid:35) (cid:34) +τdec 1 nc nc(cid:88) i= DKL(qθ( H) qref( H)) . (6) (cid:35) Iterative Stage Training. pure self-play setup typically reaches local equilibrium (Yao et al., 2023; Balduzzi et al., 2019; Hu et al., 2020; Balduzzi et al., 2018), limiting exploration of new reasoning paths. Conversely, standalone RL methods like RLVR risk knowledge saturation once the available question set is mastered. To mitigate these issues, Iterative-SPO employs twostage alternating training. When decision-stage performance indicates clue-stage saturation (easy identification of the spy), training shifts to the clue stage to increase difficulty. Conversely, when identifying the spy becomes challenging, training shifts back decision stage. Let Bt = {(Hi, i=1 be held-out mini-batch at iteration t. Define the average prediction accuracy acct and n/a rate nat of players in the decision stage within batch round: (cid:88) )}B (cid:88) (cid:20) (cid:21) 1 arg max qθ(y Hi) = , nat = qθ( Hi). (7) acct = 1 i= 1 i=1 We maintain exponential moving averages with smoothing ρ [0, 1): acct = ρ acct1 + (1 ρ) acct, (8) initialized as acc0 = na0 = 0. Let mt {0, 1} be the phase indicator (mt = 1 trains the CLUE stage, na, τ mt = 0 trains the DECISION stage). We switch phases using hysteresis thresholds τ na: (9) nat = ρ nat1 + (1 ρ) nat, if mt = 0 and acct τ Decision Clue: na, then set mt+1 = 1; acc and nat τ acc, τ err, τ Clue Decision: if mt = 1 and (cid:16) (cid:17) , then set mt+1 = 0; (10) 1 acct τ err or nat τ na 6 Table 1: Performance Comparison of Vision-Zero and SOTA models on Reasoning and Math, evaluated on VLMEvalKit. All results are obtained under same settings, except ViGaL-Snake and ViGaL-Rotation, whose results are obtained from the original paper due to unavailable models. Vision-Zero outperforms baselines trained on extensive manually annotated datasets in related tasks. Method MathVista MathVision WeMath MathVerse LogicVista DynaMath Avg. GPT4o Gemini2.0-Flash Qwen2.5-VL-7B R1-OneVision-7B MM-Eureka-Qwen-7B VLAA-Thinker-7B OpenVLThinker-7B ViGaL-Snake ViGaL-Rotation ViGaL-Snake+Rotation VisionZero-Qwen-7B (CLEVR) VisionZero-Qwen-7B (Chart) VisionZero-Qwen-7B (Real-World) 61.4 73.4 68.2 64.1 73.0 68.0 70.2 70.7 71.2 71.9 72.6 72.2 72.4 Proprietary Model 30.4 41. 40.0 57.1 Performance on Qwen2.5-VL-7B 25.4 24.1 26.9 26.4 25.3 26.5 26.3 27.5 28.1 27.6 28.0 36.1 35.8 36.2 36.0 36.5 36.9 39.8 39.2 39.5 50.2 54.4 49.0 47.1 50.3 51.7 47.9 51.1 50.4 52.4 51.9 52.1 52. 45.9 56.2 47.2 44.5 42.9 47.2 44.3 46.5 50.1 50.6 50.3 32.3 43.7 20.9 21.4 24.2 21.9 21.2 22.9 22.3 21.9 22.1 43.4 54.4 41.1 39.5 42.9 41.9 40.9 43.0 44.1 43.9 44. (a) Winning Rate (b) Avg. Token Length (Clue) (c) Avg. Token Length (Decision) Figure 6: Evolution of win rate and token length during Vision-Zero training. Win rates are evaluated over 100 rounds (50 civilian, 50 spy) against corresponding untrained reference models; civilians win by correctly identifying the spy. Token length are collected across these rounds. otherwise mt+1 = mt. To avoid chattering, we require minimum dwell time Kmin updates per stage. With this gating, the per-iteration training loss is Lt = mt Lclue(θ) + (1 mt) Ldec(θ), and gradients are applied only to the active module at iteration t. Algorithm is shown in Appendix A.2.3. This alternating scheme provides two main benefits: (1) It prevents the model from stagnating in strategic equilibrium or knowledge plateau by dynamically switching training stages upon detecting stagnation signals, thus ensuring continuous improvement (empirically verified in Sect. 3.2). (2) Alternating self-play with RLVR introduces supervised signals, stabilizing training and preventing common pitfalls like role collapse (Wang et al., 2020; Yu et al., 2024) or divergence (Heinrich & Silver, 2016; Vinyals et al., 2019). In summary, Iterative-SPO provides stable paradigm that integrates self-play with RLVR optimization to achieve sustained performance improvement. 2.3 ADVANTAGE ANALYSIS Vision-Zero has three key advantages. Firstly, Vision-Zero leverages domain-agnostic data inputs through image differences, allowing it to accept diverse data without reliance on specific image types. This universality enables direct utilization of existing high-quality image datasets, leading to generalizable performance improvements at minimal cost, as evidenced by superior benchmark results  (Fig. 2)  . Secondly, Vision-Zero demands simultaneous analysis of visual and textual inputs, addressing spatial relationships and object details, thereby concurrently enhancing reasoning, visual comprehension, and OCR capabilities. This integrated approach effectively mitigates common challenges such as text shortcut bias and negative capability transfer, as illustrated in Fig. 5. Lastly, Vision-Zero employs highly cost-efficient data curation strategy, rapidly generating datasets using advanced editing tools like ChatGPT and NanoBanana. This approach significantly reduces costs compared to traditional manual labeling, accelerating practical applications of targeted VLMs."
        },
        {
            "title": "3 EXPERIMENTS\nTo thoroughly evaluate Vision-Zero, we first outline the experimental setup, the datasets, and the\nbaselines. Next, we evaluate its performance and cost-efficiency across diverse tasks (Sect. 3.1). We\nthen conclude by analyzing model generalizability and the effectiveness of Iterative-SPO. (Sect. 3.2).",
            "content": "7 Table 2: Performance comparison between Vision-Zero and other state-of-the-art models on Chart/OCR and Vision-Centric benchmarks. All models are evaluated using the open-source platform VLMEvalKit. Additional results on related datasets are provided in the Appendix A.4. Model AI2D ChartQA OCR Bench SEED-2 RealWorldQA MMVP BLINK MuirBench Chart / OCR Vision-Centric GPT-4o Gemini2.0-Flash Qwen2.5-VL-7B R1-OneVision-7B MM-Eureka-Qwen-7B VLAA-Thinker-7B OpenVLThinker-7B ViGaL-Snake+Rotation VisionZero-Qwen-7B (CLEVR) VisionZero-Qwen-7B (Chart) VisionZero-Qwen-7B (Real-World) 84.4 87.2 84.7 82.2 84.1 84.0 81.8 84.5 84.5 85.8 84. Proprietary Model 85.7 79.3 73.9 85.5 72.0 71.2 Performance on Qwen2.5-VL-7B 86.1 77.3 84.3 79.9 86.3 87.2 86. 88.3 81.0 86.7 86.9 83.3 86.8 88.1 89.0 88.5 70.4 66.4 68.2 67.4 68.0 69.1 69.5 70.9 69.8 75.4 73.2 68.1 58.0 66.1 65.4 60.2 66.5 68.5 68.2 68.5 86.3 83.0 76.8 61.3 74.3 71.6 32.3 74.6 79.2 77.9 79. 68.0 63.5 55.2 48.7 54.0 53.0 49.9 55.6 56.1 57.2 57.5 68.0 64.6 58.2 46.3 61.1 57.1 52.8 57.8 58.6 59.4 59.8 Table 3: Comparison of dataset construction costs across methods. Methods like R1-OneVision7B use programmatic question-answer generation over real images with manual verification, taking months to year. ViGaL collects gameplay data from two environments (Snake and CLEVR-based orientation game) over several weeks. In contrast, Vision-Zero employs simple image editing, using each image pair throughout entire game rounds, significantly reducing required samples. Moreover, since most baselines are trained primarily on pure reasoning and mathematical tasks, some models actually exhibit performance degradation on the comprehensive benchmark MMMU. Method Data Type Num Prepare Method Cost Method Interact MMMu MMMupro Data Cost Training Performance Qwen2.5-VL-7B R1-OneVision-7B VLAA-Thinker-7B OpenVLThinker-7B MM-Eureka-Qwen-7B ViGaL-Snake ViGaL-Rotation ViGaL-Snake+Rotation Real-World Data Synthetic Data 155k 25k 12k 15k 72k Programmatic construction with human checks. Collected in game environment via PPO policy VisionZero-Qwen-7B (CLEVR) Synthetic 2k Batch render scenes SFT+GRPO SFT+GRPO SFT+GRPO GRPO RLOO Alternating Self-play+ GRPO 54.3 51.9 48.2 54.8 55.8 55.8 54.1 58. 58.8 37.0 32.6 31.9 22.1 36.9 36.6 37.7 37.4 37.7 few months few weeks 6 GPUh Models, Datasets & Baselines. We evaluated Vision-Zero using three modelsQwen2.5-VL7B (Bai et al., 2025), InternVL3-8B, and InternVL3-14B (Zhu et al., 2025)across 14 tasks in reasoning, chart analysis, and vision-centric domains. Detailed model and dataset information is in the Appendix A.3.1. We compared our models against SOTA methods R1-OneVision-7B (Yang et al., 2025), MM-Eureka-Qwen-7B (Meng et al., 2025), VLAA-Thinker-7B (Zhou et al., 2025), and OpenVLThinker-7B (Deng et al., 2025) (all post-trained via RLVR on human-labeled data), as well as ViGaL (Xie et al., 2025), which collects game data initially and subsequently training on them. Training and Hyperparameter Settings. We detail the hyperparameters used for Vision-Zero training below. Each round included four civilians (nc = 4) and two clue-stage speeches. To maintain balanced rewards (-1 to 1 range), we set clue hyperparameters β = λ = 0.1. Decay coefficients for role advantage (α), accuracy, and \"n/a\" rates (ρ) were adopted from Liu et al. (2025) as α = ρ = 0.95. KL regularization weights were set as defaults (τdec = τclue = 0.04). Empirically set stage-switching na = 0.5, τ thresholds were τ na = 0.1, with minimum rounds per stage Kmin = 5 and patience = 20. Models were trained for 100 iterations with batch size of 1024 using the VLM-R1 (Shen et al., 2025) code framework. Qwen2.5-VL-7B was trained on the CLEVR-based, chart-based, and real-world datasets  (Fig. 3)  ; InternVL3 was trained only on the CLEVR-based dataset to test generalizability. Further details are provided in Appendix A.3.2. acc = 0.9, τ err = 0.4, τ 3.1 MAIN RESULTS Sustainable Performance Growth. To verify Vision-Zeros capability to achieve sustained performance growth, we evaluated the models win rates against fixed, untrained reference model and measured average token lengths in the Clue and Decision stages on CLEVR data. As shown in Fig. 6, win rates consistently increased during training, with Qwen2.5-VL-7B improving from 50% to 71%. Average token lengths increased substantially, particularly in the Decision stages (e.g., InternVL3-8B and InternVL3-14B grew from 250 to approximately 400 tokens), suggesting enhanced reasoning capabilities facilitated by Iterative-SPO. Table 4: Model generalizability of Vision-Zero. We train InternVL3-8B and InternVL3-14B within the Vision-Zero using the CLEVR-based dataset, and evaluate on eight reasoning benchmarks. Model MathVista MathVision WeMath MathVerse LogicVista DynaMath Avg. InternVL3-8B VisionZero-InternVL3-8B InternVL3-14B VisionZero-InternVL3-14B Performance on InternVL3-8B 21.3 24.2 26.8 28.7 Performance on InternVL3-14B 33.8 34.8 42.3 44.9 32.2 32. 43.3 45.1 60.4 62.2 74.1 75.4 40.5 41.8 51.6 53.1 26.8 29. 30.1 31.3 34.7 36.5 45.8 47.4 Figure 7: Performance Comparison between Iterative-SPO and pure Self-play / pure RLVR training. (left) Winning Rate (right) Performance on LogicVista. We evaluate under three settings: (1) Iterative-SPO; (2) Pure Decision: Clue stage frozen, training only Decision stage via RLVR; (3) Pure Clue: Decision stage frozen, training only Clue stage via Self-Play. Strong Task Generalization Capability. To assess whether the performance gains from the VisionZero environment generalize to broader reasoning and mathematics tasks, we evaluate our models on six benchmark datasets. The experimental results are presented in Tab. 1. As demonstrated, Vision-Zero models consistently outperform state-of-the-art baseline methods across various benchmarks. Specifically, VisionZero-Qwen-7B (CLEVR) and VisionZero-Qwen-7B (Real-World) achieve performance gains of 3% over the base model, and VisionZero-Qwen-7B (Chart) improves by 2.8%. In contrast, even the most advanced baseline method yields just 1.9% improvement. Notably, all baseline methods rely on training with hundreds or even thousands of mathematics and reasoning samples. As comparison, our Vision-Zero environment does not explicitly include any mathematicsspecific task training; rather, it enhances the models logical reasoning capabilities through strategic gameplay in natural language contexts. These results clearly indicate that the capabilities learned by models from the Vision-Zero environment can effectively generalize to broader mathematics and reasoning tasks, even surpassing models explicitly trained on those large scale task specific datasets. Cross-Capability Negative Transfer Mitigation. key challenge in VLM post-training is crosscapability negative transfer, where models trained on specific tasks often perform worse on others. Our evaluation in Tab. 2 highlights that baseline models post-trained on reasoning and math datasets exhibit notable performance drops: MM-Eureka-Qwen-7B, for instance, experiences around 10% decline on ChartQA. In contrast, Vision-Zero-trained models effectively mitigate such negative transfer. Specifically, VisionZero-Qwen-7B (CLEVR) notably improves performance on visioncentric tasks with only minimal average decline of 0.2% across four chart/OCR tasks. Meanwhile, VisionZero-Qwen-7B (Chart) enhances performance across all four chart/OCR benchmarks and simultaneously achieves an average improvement of 1% on vision-centric tasks. This demonstrates that Vision-Zeros strategic, multi-capability training environment significantly alleviates negative transfer issues common in traditional single-capability training paradigms. Low Dataset Construction Costs. Vision-Zero significantly reduces dataset construction costs compared to traditional RLVR methods, which require extensive manual data collection and validation that can take months or even years (Tab. 3). Benefiting from domain-agnostic data input and streamlined dataset creation, Vision-Zero achieves superior performance requiring only tens of GPU hours and minimal cost, providing an economical, accessible, and sustainable training paradigm. 3.2 ABLATION STUDIES Model Generalizability. To assess Vision-Zeros generalizability, we trained InternVL models and evaluated their performance on reasoning and math tasks. Tab. 4 shows VisionZero-InternVL3-8B and VisionZero-InternVL3-14B improved accuracy by 1.8% and 1.6%, respectively, across reasoning tasks. This demonstrates Vision-Zeros effectiveness across diverse model architectures. 9 Superiority of Iterative-SPO. Finally, we evaluate the superiority of Iterative-SPO compared to single-mode training by training Qwen2.5-VL-7B under three distinct settings: (1) Pure clue-stage training: the decision stage is frozen (forward-pass only, without gradient updates); (2) Pure decisionstage training: the clue stage is frozen, with only the decision stage updated; and (3) Iterative-SPO. As shown in Fig. 7, Iterative-SPO substantially outperforms both single-mode approaches, particularly surpassing pure clue-stage training, which experiences slower performance gains and premature equilibrium. This occurs because pure self-play lacks directly verifiable rewardsthe reward signal originates from the decision-maker, and when decision quality is insufficient to effectively discriminate roles, the model performance plateaus prematurely. Alternating training mitigates this limitation, achieving sustainable performance improvements; for example, on the LogicVista dataset, it improves final accuracy by 2% over pure self-play and 1% over pure RLVR training."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In International Conference on Machine Learning, pp. 354363. PMLR, 2018. David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jaderberg, and Thore Graepel. Open-ended learning in symmetric zero-sum games. In International Conference on Machine Learning, pp. 434443. PMLR, 2019. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Liang Chen, Hongcheng Gao, Tianyu Liu, Zhiqi Huang, Flood Sung, Xinyu Zhou, Yuxin Wu, and Baobao Chang. G1: Bootstrapping perception and reasoning abilities of vision-language model via reinforcement learning. arXiv preprint arXiv:2505.13426, 2025. Silvio Chito, Paolo Rabino, and Tatiana Tommasi. Efficient odd-one-out anomaly detection. arXiv preprint arXiv:2509.04326, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Kevin Denamganaï, Sondess Missaoui, and James Alfred Walker. Visual referential games further the emergence of disentangled representations. arXiv preprint arXiv:2304.14511, 2023. Yihe Deng, Zhen Wang, Zhe Chen, et al. Openvlthinker: Complex vision-language reasoning via iterative sft-rl. arXiv preprint arXiv:2503.17352, 2025. 10 Aaron Dharna, Cong Lu, and Jeff Clune. Foundation model self-play: Open-ended strategy innovation via foundation models. arXiv preprint arXiv:2507.06466, 2025. Ruiqi Dong, Zhixuan Liao, Guangwei Lai, Yuhan Ma, Danni Ma, and Chenyou Fan. Who is undercover? guiding llms to explore multi-perspective team tactic in the game. arXiv preprint arXiv:2410.15311, 2024. Google DeepMind. tic google-gemini-ai-update-december-2024/, 2024. for Gemini 2.0 and 2.0 Flash. Introducing gemini 2.0: the agenhttps://blog.google/technology/google-deepmind/ Official announcement our new ai model era. for Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1899519012, 2022. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfectinformation games. arXiv preprint arXiv:1603.01121, 2016. Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. other-play for zero-shot coordination. In International Conference on Machine Learning, pp. 43994410. PMLR, 2020. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. Byungjun Kim, Dayeon Seo, and Bugeun Kim. Fine-grained and thematic evaluation of llms in social deduction game. arXiv preprint arXiv:2408.09946, 2024. Ksenia Konyushkova, Christos Kaplanis, Serkan Cabi, and Misha Denil. Vision-language model dialog games for self-improvement. arXiv preprint arXiv:2502.02740, 2025. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. H.Y. Leong and Y. Wu. Why should next-gen llm multi-agent systems move beyond fixed architectures to dynamic, input-driven graphs? SSRN Electronic Journal, 2024. doi: 10.2139/ssrn.5276004. URL https://ssrn.com/abstract=5276004. Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, and Yitao Liang. Jarvis-vla: Post-training large-scale vision language models to play visual games with keyboards and mouse. arXiv preprint arXiv:2503.16365, 2025. Tian Liang, Zhiwei He, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, and Xing Wang. Leveraging word guessing games to assess the intelligence of large language models. arXiv preprint arXiv:2310.20499, 2023. Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025. 11 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Fanqing Meng, Kai Sun, Yuxuan Liu, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022. Salman Mohammadi, Anders Kirk Uhrenholt, and Bjørn Sand Jensen. Odd-one-out representation learning. arXiv preprint arXiv:2012.07966, 2020. OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2024. arXiv:2410.21276 available. Timothy Ossowski, Jixuan Chen, Danyal Maqbool, Zefan Cai, Tyler Bradshaw, and Junjie Hu. Comma: communicative multimodal multi-agent benchmark. arXiv preprint arXiv:2410.07553, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kucinski, Lerrel Pinto, Rob Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. Genevieve Patterson and James Hays. Coco attributes: Attributes for people, animals, and objects. In European conference on computer vision, pp. 85100. Springer, 2016. Shuwen Qiu, Sirui Xie, Lifeng Fan, Tao Gao, Jungseock Joo, Song-Chun Zhu, and Yixin Zhu. Emergent graphical conventions in visual communication game. Advances in Neural Information Processing Systems, 35:1311913131, 2022. T. Qiu, J. Gao, J. Li, H.Y. Leong, and L. Zhang. Intentvcnet: Bridging spatio-temporal gaps for intention-oriented controllable video captioning. In Proceedings of ACM Multimedia (MM) 2025, 2025. doi: 10.48550/arXiv.2507.18531. URL https://arxiv.org/abs/2507.18531. Accepted, In Press. Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. VisionarXiv preprint language models are zero-shot reward models for reinforcement learning. arXiv:2310.12921, 2023. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model, 2025. URL https://arxiv. org/abs/2504.07615, 2025. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3): 5868, 1995. Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350354, 2019. B. Wang, Y. Li, Q. Zhou, H.Y. Leong, T. Zhao, L. Ye, H. Deng, D. Luo, and N. Vasconcelos. Do vision language models infer human intention without visual perspective-taking? towards scalable one-image-probe-all dataset. In Proceedings of the ICML 2025 Workshop on Assessing World Models, 2025a. URL https://openreview.net/forum?id=iekoq1rv80. Qinsi Wang, Saeed Vahidian, Hancheng Ye, Jianyang Gu, Jianyi Zhang, and Yiran Chen. Coreinfer: Accelerating large language model inference with semantics-inspired adaptive sparse activation. arXiv preprint arXiv:2410.18311, 2024a. Qinsi Wang, Jinghan Ke, Masayoshi Tomizuka, Yiran Chen, Kurt Keutzer, and Chenfeng Xu. Dobi-svd: Differentiable svd for llm compression and some new perspectives. arXiv preprint arXiv:2502.02723, 2025b. Qinsi Wang, Jinghan Ke, Hancheng Ye, Yueqian Lin, Yuzhe Fu, Jianyi Zhang, Kurt Keutzer, Chenfeng Xu, and Yiran Chen. Angles dont lie: Unlocking training-efficient rl through the models own signals. arXiv preprint arXiv:2506.02281, 2025c. Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, and Yiran Chen. Corematching: co-adaptive sparse inference framework with token and neuron pruning for comprehensive acceleration of vision-language models. arXiv preprint arXiv:2505.19235, 2025d. Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement learning with emergent roles. arXiv preprint arXiv:2003.08039, 2020. Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? arXiv preprint arXiv:2503.02358, 2025e. Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, and Zackory Erickson. Rl-vlm-f: Reinforcement learning from vision language foundation model feedback. arXiv preprint arXiv:2402.03681, 2024b. Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, and Chen Wei. Play to generalize: Learning to reason through game play. arXiv preprint arXiv:2506.08011, 2025. Yuxin Yang, Zheng Wang, Hao Zhang, et al. R1-onevision: Advancing generalized multimodal reasoning via textualized perception. arXiv preprint arXiv:2503.10615, 2025. Jian Yao, Weiming Liu, Haobo Fu, Yaodong Yang, Stephen McAleer, Qiang Fu, and Wei Yang. Policy space diversity for non-transitive games. Advances in Neural Information Processing Systems, 36: 6777167793, 2023. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Byunghwa Yoo and Kyung-Joong Kim. Finding deceivers in social context with large language models and how to find them: the case of the mafia game. Scientific Reports, 14(1):30946, 2024. Yang Yu, Qiyue Yin, Junge Zhang, Pei Xu, and Kaiqi Huang. Admn: Agent-driven modular network for dynamic parameter sharing in cooperative multi-agent reinforcement learning. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, pp. 302 310, 2024. 13 Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2024. Alex Zhang, Thomas Griffiths, Karthik Narasimhan, and Ofir Press. Videogamebench: Can vision-language models complete popular video games? arXiv preprint arXiv:2505.18134, 2025a. Hao Zhang, Bo Huang, Zhenjia Li, Xi Xiao, Hui Yi Leong, Zumeng Zhang, Xinwei Long, Tianyang Wang, and Hao Xu. Sensitivity-lora: Low-load sensitivity-based fine-tuning for large language models. In Findings of the 2025 Conference on Empirical Methods in Natural doi: 10.48550/arXiv.2509.09119. URL https: Language Processing (EMNLP), 2025b. //arxiv.org/abs/2509.09119. J. Zhang, J. Gao, W. Ouyang, W. Zhu, and H.Y. Leong. Time-llama: Adapting large language models for time series modeling via dynamic low-rank adaptation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop). Association for Computational Linguistics (ACL 2025), 2025c. ISBN 979-8-89176254-1. doi: 10.18653/v1/2025.acl-srw.90. URL https://aclanthology.org/2025. acl-srw.90/. Poster. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. Yicheng Zhou, Yuxuan Chen, Zhen Li, et al. Sft or rl? an early investigation into training r1-like multimodal reasoning models (vlaa-thinking). arXiv preprint arXiv:2504.11468, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "Organization In this Appendix, we provide in-depth descriptions of the materials that are not covered in the main paper, and report additional experimental results. The document is organized as follows: A.1Related Work A.2Vision-Zero Design Details A.2.1Prompt Setting A.2.2Dataset Preparation A.2.3Iterative-SPO algorithm A.3Experiments Setting A.3.1Model, Dataset and Baselines A.3.2Training and Hyperparameter Settings A.4Supplementary Experimental Results A.4.1Comprehensive Evaluation on Chart/OCR Tasks A.4.2Comprehensive Evaluation on Vision-Centric Tasks A.5The Use of Large Language Models A.1 RELATED WORK Multi-Agent RL for Vision-Language Models. Self-play has emerged as powerful paradigm for improving vision-language models without extensive human annotation Wang et al. (2025c); Leong & Wu (2024); Zhang et al. (2025c); Wang et al. (2025a). Konyushkova et al. (2025) introduce dialog games for VLM self-improvement, where agents engage in goal-oriented play centered on image identification, demonstrating iterative improvement through successful interaction filtering. Foundation model self-play (Dharna et al., 2025) shows how open-ended strategy innovation emerges from competitive interactions between models. SPIRAL (Liu et al., 2025) develops truly online multiagent multi-turn RL, showing that training on zero-sum games improves reasoning capabilities that generalize to novel downstream tasksparticularly relevant for the multi-turn nature of undercover games. Zhai et al. (2024) present the first framework to fine-tune VLMs using RL with task-specific rewards, achieving state-of-the-art performance without expert data. RL-VLM-F (Wang et al., 2024b) automatically generates reward functions using VLM feedback on image observation pairs, while Rocamonde et al. (2023) demonstrate that VLMs like CLIP can serve as zero-shot reward models with strong scaling effects. Undercover and Social Deduction Games in AI. The undercover game paradigm has been explicitly explored in recent work. Dong et al. (2024) introduce the Multi-Perspective Team Tactic (MPTT) framework for \"Who is Undercover?\", integrating self-perspective, identity-determination, selfreflection, and multi-round teammate finding to cultivate human-like expression. Liang et al. (2023) implement an interactive multi-agent framework with human-in-the-loop capabilities, supporting strategic deception and voting mechanics applicable to our proposed VLM variant. Studies on social deduction games reveal important insights: Yoo & Kim (2024) demonstrate that GPT-4 achieved 80.65% accuracy in detecting deceivers in Mafia games versus 28.83% for humans, while Kim et al. (2024) identify four major reasoning failures in obscured communicationinadequate information processing, insufficient strategic thinking, lack of theory of mind, and poor temporal reasoning. Game-Based Training and Evaluation for VLMs. Recent benchmarks reveal both the potential and challenges of VLMs in game environments Qiu et al. (2025); Wang et al. (2025b); Zhang et al. (2025b); Wang et al. (2025d; 2024a). BALROG (Paglieri et al., 2024) aggregates 6 game environments testing short-term and long-term planning, finding severe deficiencies in vision-based decision-making even for GPT-4o. Wang et al. (2025e) present evaluation frameworks with core tasks directly relevant to undercover game phases: Perceiving, Question Answering, Rule Following, and End-to-End Playing. VideoGameBench (Zhang et al., 2025a) reveals frontier models achieve only 0.48% completion rate on popular video games. Novel training approaches leverage games to improve VLM capabilities: G1 (Chen et al., 2025) introduces VLM-Gym addressing the \"knowingdoing\" gap through mutual bootstrapping between perception and reasoning during RL training, 15 while JARVIS-VLA (Li et al., 2025) achieves 40% improvement through Act from Visual Language Post-Training. Visual Description and Discrimination Tasks. Description-based discrimination aligns naturally with undercover game mechanics. Menon & Vondrick (2022) introduce \"classification by description\" using descriptive features rather than broad categories, providing inherent explainability for why agents identify certain images as different. The odd-one-out paradigm directly maps to undercover game structure: Chito et al. (2025) present DINO-based models for spatial and relational reasoning across multiple views, while Mohammadi et al. (2020) develop weakly-supervised tasks showing high correlation with abstract visual reasoningproviding foundations for identifying the different image among set. Multi-Agent Communication in Vision-Language Tasks. Multi-agent visual communication has seen significant progress. COMMA (Ossowski et al., 2024) presents the first comprehensive benchmark for collaborative work among multimodal agents, featuring vision-language puzzles requiring complementary information access. Qiu et al. (2022) model emergent communication through sketching between neural agents, defining metrics for evaluating conventions applicable to how agents develop shared description strategies. Visual referential games promote systematic generalization: Denamganaï et al. (2023) investigate compositionality with the Obverter architecture, while Lazaridou et al. (2018) show how referential games with pixel input enable linguistic communication emergence, providing theoretical foundations for VLMs learning to describe and discriminate through game play. A.2 VISION-ZERO DESIGN DETAILS Due to space limitations in the main text, this section elaborates on critical implementation details that were previously omitted. We organize the discussion into three parts: (i) the prompt design used during training, (ii) the construction pipeline of training data across domains, and (iii) the formal description of the Iterative Self-Play Policy Optimization (Iterative-SPO) algorithm. A.2.1 PROMPT SETTING The training process simulates gameplay using sequence of structured prompts, guiding the model through two reasoning stages. We provide below the full prompts used for training on the CLEVRbased visual reasoning task. These prompts are carefully engineered to enforce role-aware generation and role-consistent reasoning. The design enforces clear behavioral separation between SPY and NOT SPY players, ensuring that the model learns to generate clues consistent with its assigned role and to identify inconsistencies during inference. For other domains, we adopt the same prompt structure with minimal domain-specific modifications: Chart-based data: The Clue Stage prompt specifies that \"two attributes in the chart have been swapped\" if the agent is SPY. This models table-style manipulations in bar plots or pie charts; Real-world image datasets: The SPY instruction is modified to indicate that some objects or parts of the scene differ softer deviation that reflects the less controllable variation in natural scenes. In actual training, prompts are dynamically composed. We implement templating mechanism where placeholder tokens (e.g., player_id, round_num) are automatically filled based on the synthetic episode. Role-specific insertions (e.g., You are the spy with the different picture) are programmatically inserted based on the assigned role. This enables batched multi-agent simulation across thousands of games with minimal manual intervention, while still enforcing precise behavioral roles. CLUE STAGE PROMPT You are PLAYER {player_id} of the game \"Who is the spy?\". This is ROUND {round_num}. Your task is to describe your picture in one sentence, which can only be about the features of objects and the relationship between objects. Your role: SPY (you have the different picture) Your role: NOT SPY (you have the same picture as most others) Game Instructions: There are num_players players in total. Only one player sees different picture, and the 16 pictures seen by all other players are the same. Player who are voted as the odd player will be severely punished, so you should convince others that you are not the odd player. In the odd players picture, two objects are replaced with different ones, but they remain in the same positions as in the original image. You are the spy with the different picture. Two objects in your picture are different from what others see, but they remain in the same positions as in the original image. You are not the spy. Your picture is the same as most other players pictures. What you should do: 1. Compare your picture with all previous clues to understand what others are seeing. 2. If you are NOT the spy: Describe clear, specific feature of your picture that does not overlap with previous clues. This will help establish what the \"normal\" picture looks like. 3. If you are the spy: Be strategic! You know your picture is different, so either: - Describe features that are the same as others (if you can infer what theyre seeing) - Describe features from your different picture in way that doesnt reveal the differences - Focus on describing objects or aspects that might still be similar with what others see Required output format: First, conduct your private step-by-step reasoning may include suspicions, probabilities, deception, etc. Then, put your one-sentence description of your picture inside boxed{}. Example answer: boxed{The cylinder is to the left of the cube}; boxed{The yellow object is the largest object}; boxed{The red object is the smallest object}. DECISION STAGE PROMPT You are analyzing game of \"Who is the Spy?\" where num_players players have just provided clues about their pictures. Your task is to identify which player is the spy (the one with the different picture) based on their clues. Game Setup: There are num_players players in total. Only one player sees different picture (the spy), and the pictures seen by all other players are the same. In the spys picture, two objects are replaced with different ones, but they remain in the same positions as in the original image. Each player provided clue trying to describe their picture while avoiding suspicion. The spy tries to blend in by giving clues that seem consistent with what others see. Non-spy players try to give distinctive clues that help identify the spy. Your Task: Analyze all the clues and determine which player is most likely the spy. Look for: 1. Clues that seem inconsistent with the majority 2. Clues that are deliberately vague or generic 3. Clues that might be trying to mislead others 4. Patterns that suggest deception or uncertainty The image you see shows the ORIGINAL picture (what non-spy players see). Compare the clues against this reference. Required Output Format: First, conduct your private step-by-step reasoning may include suspicions, probabilities, deception, etc. Then, put your final answer (PLAYER_NUMBER or N/A) inside boxed{}. If you are uncertain, you can answer N/A. Example answer: boxed{1}; boxed{2}; boxed{3}; boxed{N/A}. All Clues from the Clue-giving Stage: 17 A.2.2 DATASET PREPARATION CLEVR-based Data. CLEVR is controlled synthetic environment expressly built to study visual reasoning with minimal dataset bias and rich, program-level supervision. Its images are rendered from complete scene graphs, and the benchmark has become standard stress-test for multi-step reasoning in vision-language systems (VQA/VLM). CLEVR scenes are procedurally sampled and rendered with Blender in headless mode, emitting both images and fixed-format JSON that records each objects attributes and pose; the official generator exposes simple CLI that renders images from the JSON scene specification. This design makes the pipeline lightweight and embarrassingly parallel. The CLEVR universe fixes the attribute vocabulary up front. Shapes are from cube, sphere, cylinder. Sizes are small, large. Materials are metal (shiny), rubber (matte). Colors come from an eight-color palettecommonly enumerated as gray, red, blue, green, brown, purple, cyan, yellowand scenes are populated under simple geometric constraints (no interpenetration, all objects at least partially visible; randomized camera and lighting). These choices simplify perception so models performance reflects reasoning rather than recognition shortcuts. We automatically render 2k training pairs with the CLEVR renderer. Each pair consists of an original image and modified image. Every image is accompanied by its scene JSON; the pair also carries compact change log (IDs of changed objects and their before/after attributes). For each scene, we sample 46 objects with attributes drawn uniformly from the CLEVR spaces above, while enforcing standard CLEVR placement rules (no overlap/interpenetration and sufficient margins so spatial relations are unambiguous). Camera pose and lights are jittered per scene, following the official generators practice of randomizing viewpoint and illumination. Given an original scene JSON, we randomly select two objects and replace only their color and shape (leaving other attributes and the global layout unchanged unless minimal nudge is needed to maintain non-overlap). Concretely: Step 1: Generate original JSON and render. Step 2: Edit the JSON in place for two objects: shape new shape, color new color. Step 3: Re-render with Blender from the modified JSON to obtain the paired image. CLEVR generation is stateless per scene and the official script supports GPU-accelerated Blender rendering (CUDA flag) in batch mode, so we parallelize across processes. On single NVIDIA A100, end-to-end rendering of the 2k pairs completes in roughly 6 hours in our environment, consistent with the repos recommendation to invoke Blender headless with GPU enabled. Chart-based Data. In our preliminary attempts to generate chart data, we explored direct editing of chart images via NanoBanna and ChatGPT; however, we found this approach extremely challenging, because current image editing models and tools struggle to reliably control fine-grained graphical attributes (such as exact axis ticks, bar widths, label alignment, and consistent color scales) without introducing visual artifacts or distortions (a known limitation of current image editing in diffusion/inpainting frameworks) Therefore, to achieve stable, controllable editing and generation, we eventually adopted the following pipeline: We let GPT-4o ingest the original chart image and output JSON file encoding every attributes numerical value (e.g. data points, axis bounds, legend mapping) as well as auxiliary metadata (chart type, color scheme, layout constraints); We prompt GPT-4o to swap two attributes arbitrarily and rewrite the JSON accordingly; We feed the new JSON into Python plotting module to render new chart. This paradigm is robust to typical failures of AI editors and fully leverages the strong captioning and scene-parsing abilities of current multimodal LLMs. For our dataset, we randomly sampled 1,000 original charts from ChartQAs training set to ensure visual and data diversity, so that derived pairs reflect ChartQAs spectrum of chart styles and complexity. ChartQAs dataset spans three canonical chart types line plots, bar charts, and pie charts capturing both simple and complex variants in real-world sources. Thanks to the fully automated pipeline, the entire generative process incurs only on the order of tens of US dollars. 18 A.2.3 ITERATIVE-SPO ALGORITHM In this section, we describe the algorithm of Iterative Self-Play Policy Optimization (Iterative-SPO) algorithm, as detailed in Alg. 1. As mentioned in the main paper, Iterative-SPO achieves sustained performance improvement by incorporating supervision signals into the self-play framework through two-stage alternating training procedure. Algorithm 1 Iterative Self-Play Policy Optimization(Iterative-SPO) Input: Role set = {spy} {c1, . . . , cnc }; reference policies πk err, τ τ na, τ na, Kmin, ; learning rates ηθ, ηθ. ref ; hyperparams β, λ, α, τclue, ρ, τ acc, 1: Init RAE bs 0, bciv 0; Stage switch metrics acc 0, na 0; Stage 0 (Decision). 2: for = 1, . . . , do 3: 4: 5: CLUE Stage θ ( Ik, h) based on the historical dialogue and input picture Ik. if = 1 then Each player gives clue uk πk Obtain votes from the decision stage = (vs, vc1 , . . . , vcnc ) and vc 1 nc cj β Zero-Sum Rewards: rclue rclue nc Role Advantage Estimation: bs αbs + (1 α)rclue , cj rclue bs; Aclue RAE-based Advantages: Aclue β (vs vc); rclue (vs vc) λ (vcj vc) for = 1, . . . , nc. bciv αbciv + (1 α) 1 nc cj bciv for = 1, . . . , nc. rclue cj j=1 vcj . (cid:80)nc (cid:80) . else DECISION Stage Each citizen casts vote ˆsci qθ( H) based on the clue information and the input image Ik. Reward: rdec Group-norm Advantage: Adec ci 0.5if ˆsci = (unsure); rdec ci 1ifˆsci = s(correct); rdec ci 1else (wrong). ci µr)/(σr + ε) ci = (rdec Policy update: Apply KL-regularized policy gradient as Eq. 3 or Eq. 6 to update πθ or qθ. Stage Switch: Calculate average prediction accuracy acct and n/a rate nat of players in the decision 1[arg maxy qθ(y Hi) = stage within batch round: acct = 1 i qθ( Hi). (cid:80) (cid:80) na ρ na + (1 ρ) nat, + 1. ], nat = 1 Update EMAs acc ρ acc + (1 ρ) acct; if = 0 and acc τ if = 1 and (cid:0)1 acc τ acc and na τ err or na τ na na and Kmin then 1, 0 ; (cid:1) and Kmin then 0, 0; 18: 19: return θ, θ 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: A.3 EXPERIMENTS SETTING In this section, we provide comprehensive account of the experimental settings used throughout our study. We detail the choices for (1) models, datasets, and baselines, (2) training procedures and hyperparameter configurations. A.3.1 MODEL, DATASET AND BASELINES Models. We evaluate three open-weight visionlanguage models. Qwen2.5-VL-7B-Instruct is 7B instruction-tuned VLM from the Qwen family; it upgrades the vision stack with window-attention ViT and SwiGLU/RMSNorm, and is designed for strong document/chart understanding, structured JSON outputs, grounding, and even long-video/agentic use cases. InternVL3-8B is the 8B member of the InternVL3 series that follows ViT-MLP-LLM design by pairing an InternViT-300M vision encoder with Qwen2.5-7B language core via an MLP projector; it introduces Variable Visual Position Encoding and native multimodal pre-training to improve multi-image/video perception and OCR/chart/document reasoning. InternVL-14B is an earlier 14B vision-language foundation model (224-px variant) trained on large-scale web corpora such as LAION, COYO, CC12M/CC3M, SBU, and Wukong, and is commonly used for zero-shot classification, retrieval, and captioning baselines. Datasets. We evaluate on broad suite of public benchmarks. MathVista combines 6,141 problems drawn from 28 existing multimodal math datasets plus three newly created sets (IQTest, FunctionQA, PaperQA) to probe fine-grained visualmathematical reasoning. MathVision (MATH-V) curates 3,040 competition-grade problems with visual contexts across 16 disciplines and five difficulty levels for rigorous multimodal math assessment. We-Math collects 6.5K visual math problems organized over 67 hierarchical knowledge concepts to analyze LMM reasoning behaviors. we-math.github.io MathVerse offers 2,612 diagram-based problems, each converted into six modality variants to stresstest vision vs. text contributions. LogicVista targets logical cognition in visual contexts with 19 multiple-choice questions spanning five task types and nine capabilities, each paired with human rationales. DynaMath is dynamic robustness benchmark that perturbs seed questions (e.g., values, function graphs) to test stability of visual math reasoning. AI2D is 5K grade-school science diagram corpus with dense structure and text annotations for diagram understanding and VQA. ChartQA evaluates chart understanding with 9.6K human-written and 23.1K synthetic QA pairs requiring visual and logical reasoning. OCRBench (v2) is large bilingual text-centric benchmark ( 10K human-verified QA pairs across 31 scenarios) for OCR and document understanding in LMMs. SEED-Bench-2 provides 24K multiple-choice questions over 27 dimensions to benchmark hierarchical multimodal capabilities (comprehension and generation). RealWorldQA (released with Grok-1.5 Vision) contains 700+ real-scene imagesmany vehicle-capturedeach with question and easily verifiable answer. MMVP is built from CLIP-blind image pairs to assess nine basic visual pattern failures via 300 VQA items. BLINK recasts 14 core perception tasks into 3,807 multiple-choice questions that humans solve within blink but remain challenging for current MLLMs. MuirBench focuses on multi-image understanding with 11,264 images and 2,600 MCQs across 12 tasks and 10 relation types, including paired unanswerable variants for robustness. Baselines. We benchmark against five recent multimodal reasoning baselines. R1-OneVision-7B is Qwen2.5-VLbased VLM trained on the R1-OneVision corpus with cross-modal reasoning pipeline that converts images into structured textual representations to enable step-wise R1-style multimodal reasoning. MM-Eureka-Qwen-7B introduces the MMK12 dataset and employs rule-based reinforcement learning with online filtering and two-stage training strategy to stabilize multimodal math reasoning at the 7B scale. VLAA-Thinker-7B is trained on VLAA-Thinkinga corpus of step-by-step visual reasoning traces with both SFT and RL splitsused to probe SFT vs. RL for R1-like reasoning and reporting SOTA on OpenCompass as of April 2025. OpenVLThinker-7B follows an iterative SFTRL regimen (e.g., GRPO) that consistently improves performance on MathVista/EMMA/HallusionBench, evidencing the synergy of SFT and RL for complex multimodal reasoning. ViGaL (Snake+Rotation) post-trains 7B model purely via RL on simple arcade-style games (Snake and 3D rotation puzzle), where combining the two games yields stronger out-ofdomain generalization (e.g., math, geometry) than either alone. A.3.2 TRAINING AND HYPERPARAMETER SETTINGS To facilitate stable and effective training, we selected VLM-R1 as the foundational model architecture for the Vision-Zero framework, ensuring compatibility with established benchmarks. The detailed hyperparameter configurations employed in our experiments are summarized in Tab. 5. Specifically, all Vision-Zero models underwent training for 150 iterations across diverse datasets, followed by rigorous evaluation of their post-training performance to measure generalization and robustness. Table 5: Vision-Zero training hyperparameters. Symbol Meaning nc β, λ α, ρ τdec, τclue τ acc τ err τ na τ na Kmin Number of civilians per round Clue-stage reward scaling / clue regularization coefficients Decay coefficients for role advantage (α) and accuracy / n/a rates (ρ). KL regularization weights (decoder / clue) Stage-switch (up) threshold for accuracy Stage-switch (up) threshold for error rate Stage-switch (up) threshold for n/a rate Stage-switch (down) threshold for n/a rate Minimum number of rounds per stage Patience (number of rounds before forcing change) # iterations Batch size Total training iterations Training batch size Value 4 0.1 0.95 0.04 0.9 0.4 0.5 0.1 5 20 100 1024 We utilized training batch size of 1024, precisely calculated as the product of nproc_per_node (8), gradient_accumulation_steps (16), and num_generations (8). This carefully chosen batch size aligns with standard VLM training paradigms, effectively ensuring stable optimization dynamics. Moreover, our training setup is fully integrated with state-of-the-art optimization 20 techniques and libraries, including FlashAttention-2 and DeepSpeed ZeRO-3, maximizing training efficiency and scalability while maintaining full methodological consistency with VLM-R1 standards. torchrun --nproc_per_node=\"8\" --nnodes=\"1\" --node_rank=\"0\" --master_addr=\"127.0.0.1\" --master_port=\"12350\" src/open_r1/grpo_jsonl.py --deepspeed local_scripts/zero3_model_parallel.json --output_dir $OUTPUT_BASE_DIR/$RUN_NAME --model_name_or_path Qwen/Qwen2.5-VL-7B-Instruct --dataset_name \"dynamic_clevr_spotdiff\" --use_dynamic_dataset --epoch_size $EPOCH_SIZE --data_generator_type clevr_spotdiff --clevr_images_dir $CLEVR_IMAGES_DIR --clevr_scenes_dir $CLEVR_SCENES_DIR --clevr_num_players $NUM_PLAYERS --clevr_num_rounds $NUM_ROUNDS --training_phase $TRAINING_PHASE --data_generator_seed 42 --max_anyres_num 6 --max_prompt_length 8000 --max_completion_length 512 --num_generations 8 --per_device_train_batch_size 8 --gradient_accumulation_steps 16 --logging_steps 1 --bf16 --torch_dtype bfloat16 --beta 0.04 --report_to wandb --gradient_checkpointing true --attn_implementation flash_attention_2 --num_train_epochs 15 --learning_rate 1e-5 --warmup_ratio 0.1 --lr_scheduler_type cosine --run_name $RUN_NAME --save_steps 5 --save_only_model true --reward_funcs clevr_clue_format_with_votes clevr_decision_accuracy --dispatch_batches False --val_split_ratio 0.0 --num_iterations 1 A.4 SUPPLEMENTARY EXPERIMENTAL RESULTS In this section, we provide supplementary experimental results to further highlight the effectiveness and generalizability of Vision-Zero. Specifically, we report: (1) the evaluation results of Vision-Zero across seven OCR/chart-related tasks, and (2) performance on six vision-centric task sets. A.4.1 COMPREHENSIVE EVALUATION ON CHART/OCR TASKS While we partially presented Vision-Zeros results on selected chart and OCR tasks in the main text, Tab. 6 illustrates comprehensive evaluation across an extended set of tasks. Notably, VisionZeroQwen-7B consistently surpasses baseline methods across diverse OCR and chart-based tasks. Particularly, VisionZero-Qwen-7B (Chart) exhibits superior performance and significant capability 21 Table 6: Performance comparison between Vision-Zero and other state-of-the-art models on OCR, Chart, and Document Understanding. All models are evaluated using the open-source platform VLMEvalKit. AI2D ChartQA TextVQA DocVQA InfoVQA OCR Bench SEEDBench2 Model GPT4o Proprietary Model 84.4 85.7 82.2 91.1 78. Qwen2.5-VL-7B-Instruct R1-OneVision-7B MM-Eureka-Qwen-7B VLAA-Thinker-7B OpenVLThinker-7B ViGaL-Snake+Rotation VisionZero-Qwen-7B (CLEVR) VisionZero-Qwen-7B (Chart) VisionZero-Qwen-7B (Real-World) 84.7 82.2 84.1 84.0 81.8 84.5 84.5 85.8 84.8 Performance on Qwen2.5-VL-7B 94.8 81.1 92.7 92.5 94.9 95.9 95.2 86.1 77.3 84.3 79.9 86.3 87.2 86.3 85.5 81.1 82.9 82.2 85.3 86.4 85.4 82.3 71.7 71.8 72.7 82.5 86.5 82. 73.9 88.3 81.0 86.7 86.9 83.3 86.8 88.1 89.0 88.5 72.0 70.4 66.4 68.2 67.4 68.0 69.1 69.5 70.9 69.8 Table 7: Performance comparison between Vision-Zero and other state-of-the-art models on VisionCentric benchmarks. All models are evaluated using the open-source platform VLMEvalKit. Model GPT4o Qwen2.5-VL-7B-Instruct R1-OneVision-7B MM-Eureka-Qwen-7B VLAA-Thinker-7B OpenVLThinker-7B ViGaL-Snake+Rotation VisionZero-Qwen-7B (CLEVR) VisionZero-Qwen-7B (Chart) VisionZero-Qwen-7B (Real-World) RealworldQA MMVP MMStar BLINK MuirBench CRPE Avg. Proprietary Model 86.3 75. Performance on Qwen2.5-VL-7B 76.8 68.1 61.3 58.0 74.3 66.1 71.6 65.4 71.3 60.2 74.6 66.5 79.2 68.5 77.9 68.2 79.5 68.5 64.6 57.8 65.9 60.4 59.1 62.6 65.2 64.7 65.8 68.0 68.0 55.2 48.7 54.0 53.0 49.9 55.6 57.2 56.1 57.5 58.2 46.3 61.1 57.1 52.8 57.9 59.4 58.6 59.8 76.4 75.3 76.7 74.6 75.8 76.7 76.9 76.2 77.0 66.6 57.9 66.4 63.7 61.5 65.7 67.7 66.9 68.0 enhancement due to its targeted training on chart datasets. For example, on the InfoVQA benchmark, VisionZero-Qwen-7B (Chart) improved the performance of the original model by approximately 4%, outperforming the state-of-the-art ViGal by 14%. This substantial improvement arises because baselines trained extensively on reasoning datasets typically suffer from task overfitting, whereas Vision-Zero circumvents this limitation by concurrently fostering multiple capabilities. A.4.2 COMPREHENSIVE EVALUATION ON VISION-CENTRIC TASKS Moreover, as shown is Tab. 7, Vision-Zero achieves top-tier performance across six distinct visioncentric task groups. VisionZero-Qwen-7B (CLEVR), whose training data has stronger visual emphasis compared to VisionZero-Qwen-7B (Chart), obtains even better results. Specifically, VisionZeroQwen-7B (CLEVR) surpasses state-of-the-art baselines by 1.1% on average across the six task categories. These results underscore the potential and applicability of Vision-Zero as the first zero-human-in-the-loop training paradigm. A.5 THE USE OF LARGE LANGUAGE MODELS In this work, we used ChatGPT-4o (OpenAI) and Gemini 2.5 Flash (Google) to assist with image generation for dataset construction. Specifically, the models were prompted to edite visual content used in training datasets. We gratefully acknowledge their utility in facilitating efficient data synthesis."
        }
    ],
    "affiliations": [
        "Adobe Inc.",
        "Duke University",
        "National University of Singapore",
        "University of Maryland"
    ]
}