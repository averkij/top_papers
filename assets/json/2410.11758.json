{
    "paper_title": "Latent Action Pretraining from Videos",
    "authors": [
        "Seonghyeon Ye",
        "Joel Jang",
        "Byeongguk Jeon",
        "Sejune Joo",
        "Jianwei Yang",
        "Baolin Peng",
        "Ajay Mandlekar",
        "Reuben Tan",
        "Yu-Wei Chao",
        "Bill Yuchen Lin",
        "Lars Liden",
        "Kimin Lee",
        "Jianfeng Gao",
        "Luke Zettlemoyer",
        "Dieter Fox",
        "Minjoon Seo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 8 5 7 1 1 . 0 1 4 2 : r a"
        },
        {
            "title": "LATENT ACTION PRETRAINING FROM VIDEOS",
            "content": "Seonghyeon Ye1 Joel Jang2 Sejune Joo1 Byeongguk Jeon1 Reuben Tan3 Yu-Wei Chao4 Bill Yuchen Lin5 Lars Liden3 Kimin Lee1 Jianfeng Gao3 Luke Zettlemoyer2 Dieter Fox2,4 Minjoon Seo1 Jianwei Yang3 Baolin Peng3 Ajay Mandlekar4 1KAIST 4 NVIDIA 5 Allen Institute for AI 2University of Washington 3Microsoft Research"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model. We open-source the model checkpoints and code at latentactionpretraining.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language-Action Models (VLA) for robotics (Brohan et al., 2023; Kim et al., 2024) are trained by aligning large language models with vision encoders, and then finetuning it on on diverse robot datasets (Collaboration et al., 2023); this enables generalization to novel instructions, unseen objects, and distribution shifts (Michał et al., 2024). However, diverse real-world robot datasets mostly require human teleoperation, which makes scaling difficult. Internet video data, on the other hand, offers abundant examples of human behavior and physical interactions at scale, presenting promising approach to overcome the limitations of small, specialized robotic datasets (Yang et al., 2024c). However, it is challenging to learn from internet video data for two major challenges: first, much of the raw data on the web lacks explicit action labels; second, the data distribution from the web is fundamentally different from the embodiments and environments of typical robotic systems (McCarthy et al., 2024). We propose Latent Action Pretraining for General Action Models (LAPA), an unsupervised approach to pretraining robotic foundation model without the need for groundtruth robot action labels (Figure 1). LAPA has two pretraining stages, followed by fine-tuning stage to map the latent actions to real robot actions. In the first pretraining stage, we use VQ-VAE-based objective (Van Den Oord et al., Denotes equal contribution. Work done during internship at Microsoft Research. Work done during internship at NVIDIA. Denotes equal advising. Correspondence to: seonghyeon.ye@kaist.ac.kr, joeljang@cs.washington.edu."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Problem Formulation. We investigate building generalist robotic foundation model from human motion videos without action labels. 2017) to learn quantized latent actions between raw image frames. Analogous to Byte Pair Encoding (Sennrich et al., 2016) used for language modeling, this can be seen as learning to tokenize atomic actions without requiring predefined action priors (e.g., end-effector positions, joint positions). In the second stage, we perform behavior cloning by pretraining Vision-Language Model to predict latent actions derived from the first stage based on video observations and task descriptions. Finally, we fine-tune the model on small-scale robot manipulation dataset with robot actions to learn the mapping from the latent actions to robot actions. In this work, we refer to both the proposed method and the resulting VLA models as LAPA. We measure performance on diverse manipulation videos, including existing robot video datasets (without utilizing ground-truth actions) and human manipulation datasets. Our results show that the proposed method significantly outperforms baseline methods of training manipulation policies from actionless videos, particularly in cross-environment and cross-embodiment scenarios. Furthermore, on real-world manipulation tasks, our method leads to new monolithic VLA model, outperforming OPENVLA (Kim et al., 2024), the current state-of-the-art model Vision Language Action (VLA) model trained on diverse mixture of datasets with ground-truth actions. These results demonstrate the effectiveness of learning unified quantized latent action representations across diverse robotic datasets featuring different embodiments (shown in Section 5.2). We further demonstrate that LAPA remains effective even when pretrained on only human manipulation video, outperforming models pretrained on Bridgev2 (Walke et al., 2023), one of the largest open-sourced robotic datasets. We observe that LAPA effectively captures environment-centric actions, including object and camera movements, which could be beneficial for downstream tasks like navigation or dynamic, non-quasistatic tasks. We expect that our method opens up the potential for building foundation models for robotics by pretraining on much larger web-scale video data. We summarize our main contributions and findings below: We propose Latent Action Pretraining for general Action models (LAPA), an unsupervised approach to pretraining robotic foundation model to encode robotic skills from web-scale video data. Experiments on simulation and real-world robot tasks show that our method not only significantly outperforms baseline methods for training robotic manipulation policies from actionless video, but also leads to VLA model that outperforms the current state-of-theart VLA model trained with ground-truth actions (by +6.22%), while achieving over 30x greater pretraining efficiency. With LAPA, we qualitatively demonstrate that it is possible to use the learned world and action prediction models to simulate full trajectories in diverse environments, effectively building neural simulation capable of performing closed-loop evaluations entirely through neural inference."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 VISION-LANGUAGE-ACTION MODELS Vision-Language Models (VLMs), which are trained on extensive internet-scale datasets encompassing text, image, and video, have demonstrated the ability to understand and generate both text"
        },
        {
            "title": "Preprint",
            "content": "and multimodal data (Liu et al., 2023; Team, 2024; Liu et al., 2024; Abdin et al., 2024). Building on the generalization power of VLMs, recent work has introduced Vision-Language-Action Models (VLAs), which extend VLMs by fine-tuning them on robotic action data to improve physical grounding (Brohan et al., 2023; Kim et al., 2024; Team et al., 2024; Collaboration et al., 2023). Recently, incorporating auxiliary objectives, such as visual traces (Niu et al., 2024), language reasoning paths (Michał et al., 2024), or constructing conversational-style instruction dataset using robot trajectory data (Li et al., 2024a) during VLA training has been shown to improve performance. However, these methods still heavily rely on labeled action data, which limits the scalability of developing general VLAs, since they will be bounded by the amount of robotic data made available through human teleoperation. In contrast, our approach only requires labeled actions for finetuning, significantly reducing the burden of requiring vast amount of human teleoperated robot data. Training Robot Policies From Videos Videos contain rich information about dynamics and behavior which can be potentially beneficial for robot learning. However, most raw videos do not contain any action labels (McCarthy et al., 2024). One line of work attempts to leverage egocentric human videos (Grauman et al., 2022) to improve visual representations for visuomotor robotic tasks (Nair et al., 2022; Dasari et al., 2023). Some recent works explore the benefits of video generative models pretrained on human videos for downstream robotic tasks (Wu et al., 2024; Liang et al., 2024). Another line of work aims to learn useful information from human videos by learning from interactions (Zeng et al., 2024), affordances (Bahl et al., 2023; Kannan et al., 2023; Srirama et al., 2024; Shaw et al., 2023), or visual traces (Wen et al., 2023; Bharadhwaj et al., 2024b) extracted from human videos. However, all these works learn useful visual priors whereas our approach allows learning the mapping directly from perception to control during pretraining. Another line of work aims to learn robot manipulation policies by retargeting human motions to robot motions. These works rely on off-the-shelf models such as hand pose estimators (Wang et al., 2023; Zhu et al., 2024; Shaw et al., 2023; Bharadhwaj et al., 2023; Ye et al., 2023; Qin et al., 2022) or motion capture systems (Yang et al., 2024a) to retarget the human motions directly to robot motions. However, these works either learn only task-specific policies or require large in-domain perfectly aligned human-robot data and do not leverage the diverse, web-scale human motion data to train single generalist policy. The last line of work assumes one has access to small-scaled robotic dataset with action labels to train an inverse dynamic model (IDM), optical flow, or reinforcement learning models that predicts actions from future state rollouts generated by world models (Du et al., 2023; Ko et al., 2024; Yang et al., 2024b; Bharadhwaj et al., 2024a) trained on diverse web-scale human videos. Another related work (Baker et al., 2022) trains an IDM with small amount of action-labeled data and uses the IDM to label large-scaled actionless web data and pretrain visuomotor robot policy. We consider these methods as direct baselines to our proposed methods since they are scalable methods with the same experimental assumptions and learn both visual and control priors to train generalist robot policies. Latent Actions Previous works have employed latent actions across diverse scenarios. GENIE (Bruce et al., 2024) maps user inputs (ground-truth actions) to latent space, allowing generative models to create interactive environments. We adopt similar latent action model but apply it to label actionless data for training monolithic VLA to solve robotic tasks. Similarly, Edwards et al. (2018) and Schmidt & Jiang (2024) use latent actions to pretrain and fine-tune policies for video games (Cobbe et al., 2019). In contrast, we focus on learning latent actions from real-world human motions for more complex, continuous robotic tasks. Unlike other work that leverages latent actions by converting ground-truth actions into latent to capture better multimodality and task semantics Lynch et al. (2020); Jiang et al. (2023); Lee et al. (2024); Mete et al. (2024), our approach derives latent actions directly from observations, not ground-truth actions."
        },
        {
            "title": "3 LAPA: LATENT ACTION PRETRAINING FOR GENERAL ACTION MODELS",
            "content": "LAPA is divided into two stages: Latent Action Quantization and Latent Pretraining. First, we use VQ-VAE based objective to capture the discretized latent delta information between consecutive frames in video. Next, pretrained VLM is trained to predict the latent action designated by the encoder of the Latent Action Quantization model, given the current image and the language instruction. After Latent Pretraining, we finetune the VLA model on small number of ground-truth"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of LAPA. (1) Latent Action Quantization: We first learn discrete latent actions in fully unsupervised manner using the VQ-VAE objective. (2) Latent Pretraining: The VLM is trained to predict latent actions, essentially performing behavior cloning. After pretraining, we finetune the LAPA model on small set of action-labeled trajectories to map the latent space to the end effector delta action space. action-labeled trajectories to map the latent space to the actual action space. The overall process is illustrated in Figure 2. 3.1 LATENT ACTION QUANTIZATION To learn latent actions in fully unsupervised manner, we train latent action quantization model following Bruce et al. (2024) with few modifications. Our latent action quantization model is an encoder-decoder architecture where the encoder takes the current frame xt and the future frame 1. The decoder is xt+H of video with fixed window size and outputs the latent action zt trained to take the latent action zt and xt and reconstruct xt+H . Unlike Bruce et al. (2024), we use cross attention to attend zt given xt instead of additive embedding, which empirically leads to capturing more semantically meaningful latent actions. Our quantization model is variant of C-ViViT tokenizer (Villegas et al., 2023) where the encoder includes both spatial and temporal transformer while the decoder only contains spatial transformer since our model uses only two image frames as input. Further model details are provided in Appendix A. Our latent action quantization training model is based on the VQ-VAE objective (van den Oord et al., 2017). The VQ-VAE objective enables the latent action zt to be discrete tokens (codebooks), making it easy for VLMs to predict zt. The latent action is represented using sequences from codebook vocabulary space. To avoid gradient collapse often observed in VQ-VAE, we utilize NSVQ (Vali & Backstrom, 2022) which replaces the vector quantization error to product of original error and normalized noise vector. We also apply codebook replacement technique from NSVQ during early training steps to maximize codebook utilization. We utilize the encoder of our latent action quantization model as an inverse dynamics model in the next stage of LAPA and the decoder for generating neural-based closed-loop rollouts. Unlike previous works (Bruce et al., 2024; Valevski et al., 2024), LAPA trains both world model that generates rollouts from the latent actions and policy model that produces these latent actions through Latent Pretraining. 3.2 LATENT PRETRAINING We use the encoder of the latent action quantization model as an inverse dynamics model to label all xt, given xt+1, with zt. Then, we pretrain VLM to predict the zt given the language instruction of video clip and the current image xt. Instead of using the existing language model head of the VLM, we attach separate latent action head of vocab size C. By default, we freeze only the vision encoder and unfreeze the language model during training. Since latent pretraining does not rely on ground truth actions, it opens the possibility of using any type of raw video paired with language instructions. Also, in contrast to traditional action granularity used in robotics (e.g. end-effector positions, joint positions, joint torques, etc.), our approach does not require any priors about the 1Although Bruce et al. (2024) conditioned on multiple past observations, we exclude previous frames due to computational constraints. We leave prepending past observations as future work."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Pretraining and fine-tuning dataset for each environment. Cross-Env denotes cross-environment, Cross-Emb denotes cross-embodiment, and Multi-Emb denotes multi-embodiment. For fine-tuning, MT denotes multi-task training and MI denotes tasks with diverse multi-instructions. Category denotes the main capability we are trying to quantify. Illustration of each environment is shown in Figure 3. Environment Category Pretraining Fine-tuning Dataset # Trajs Dataset # Trajs LangTable SIMPLER Real-World In-Domain Cross-Task Cross-Env In-Domain Cross-Emb Cross-Emb Multi-Emb Cross-Emb Cross-Emb Sim (All 5 tasks) Sim (All 5 tasks) Real (All 5 tasks) Bridgev2 Something v2 Bridgev2 Open-X Open-X Something 181k 181k 442k 60k 220k 60k 970k 970k 220k 5 Tasks (MT, MI) 1 Task (MI) 5 tasks (MT, MI) 4 Tasks (MT) 4 Tasks (MT) 3 tasks (MI) 3 tasks (MI) 1 task (MI, Bi-manual) 3 tasks (MI) 1k 7k 1k 100 100 450 450 150 450 action hierarchy/granularity and is learned in an end-to-end manner simply by being optimized to best capture the delta of consecutive observations in given video dataset. We broadly refer to models having gone through latent pretraining as LAPA. 3.3 ACTION FINETUNING VLAs that are pretrained to predict latent actions are not directly executable on real-world robots since latent actions are not actual delta end-effector actions or joint actions. To map latent actions to actual robot actions, we finetune LAPA on small set of labeled trajectories that contain ground truth actions (delta end-effector). For action prediction, we discretize the continuous action space for each dimension of the robot so that the number of data points allocated for each bin is equal following Kim et al. (2024); Brohan et al. (2023). We discard the latent action head (a single MLP layer) and replace it with new action head to generate ground truth actions.2. As with latent pretraining, we freeze the vision encoder and unfreeze all of the parameters of the underlying language model."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we demonstrate the effectiveness of LAPA as general-purpose pretaining method. Specifically, we focus on answering the following questions: Q1. How does LAPA perform when there are cross-task, cross-environment, and crossembodiment gaps between pretaining and fine-tuning? Q2. Can LAPA learn superior priors compared to using ground-truth actions during pretraining in multi-embodiment setting? Q3. Can we create performant LAPA solely from raw human manipulation videos? 4.1 BENCHMARKS AND ENVIRONMENTS We evaluate the effectiveness of LAPA on 9 different task categories in 2 different simulation environments and 4 different real-world robotic tasks. Table 1 shows an overview of the pretraining and fine-tuning dataset for each setup and Figure 3 visualizes the simulation benchmark and real-world setups. More details of each evaluation setup are provided in Appendix B. Language Table (Lynch et al., 2023) is simulation where robot performs 2 DOF actions to push blocks (see Figure 3) (a)). It includes 5 subtask categories: BlocktoBlock, BlocktoAbsolute, BlocktoBlockRelative, BlocktoRelative, and Separate. Each trajectory from both sim and real is 2We also tried leaving the latent action head and adding additional head to decode the latent to ground-truth actions following Schmidt & Jiang (2024) However, we empirically found that re-initializing the action head resulted in superior downstream task performance, likely due to the size of the underlying policy model (7B). 3We leave parameter efficient fine-tuning approaches as future work for finetuning (Hu et al., 2022)."
        },
        {
            "title": "Preprint",
            "content": "(a) LANGUAGE TABLE (b) SIMPLER (c) REAL Figure 3: Experimental Setups. (a) shows an example from the 440k real-world trajectories (top) and the 181k simulation trajectories (bottom) from the Language Table Benchmark. (b) shows the 4 different evaluation tasks we use with the SIMPLER environment. (c) shows the four different tasks that we perform in the real-world. classified to one of 87k behaviors specified via natural language instruction (e.g. slide the blue cube down bit). During evaluation, we evaluate models for both seen and unseen scenarios, where unseen includes new objects (color and shape) and unseen combinations of seen objects. SIMPLER (Li et al., 2024b) is set of simulated environments for evaluating generalist robot manipulation policies. We assess our models on 4 tasks (Figure 3 (b)) using the 7 DOF WidowX robot arm. Since SIMPLER lacks fine-tuning trajectories, we collect 100 multi-task trajectories using successful rollouts from VLA model trained on BridgeV2 data (Walke et al., 2023), with held-out trajectories differing in object orientation and position from the evaluation setup. Real-World Tabletop Manipulation experiments used 7 DOF Franka Emika Panda robot arm in three environments and 14 DOF bi-manual robot in single environment (shown in Figure 3 (c)). We utilize three pretraining data sources: Bridgev2 (Walke et al., 2023), Open-X (Collaboration et al., 2023), and Something Something v2 (Goyal et al., 2017). Following Kim et al. (2024), for single arm experiments, we finetune on three multi-instruction tasks: (1) Pick <object> into Sink, (2) Cover <object> with Towel, and (3) Knock <object> Over. For bi-manual robot experiments, the task is to Put the <object1> on the container and the <object2> on the plate. For all tasks, each task involves 150 trajectories across 15 objects. We use task-specific partial success criterion for evaluation, following Kim et al. (2024). 4.2 BASELINES For the choice of our underlying VLM , we utilize the 7B Large World Model (LWM-Chat-1M) (Liu et al., 2024), which also leads to much efficiency gains during pretraining (further analysis provided in Section 4.7). We also compare with existing behavior cloning baselines that do not require ground truth actions during pretraining. SCRATCH denotes the baseline model where we finetune our backbone VLM only on the downstream tasks, to quantify the gains we get from the pretraining stage. UNIPI (Du et al., 2023) uses video diffusion model during pretraining to generate video rollouts given language instruction, which does not require any action labels during pretraining similar to our approach. For finetuning, an inverse dynamics model (IDM) is trained to extract the ground truth actions given adjacent frames.4 We also finetune the diffusion model on the downstream task to match the target distribution. VPT (Baker et al., 2022) trains an IDM on action labeled data, and uses the IDM model to extract pseudo actions on raw videos. Then, we use the pseudo actions labeled by the IDM to pretrain our backbone VLM on the pretraining data, identical to Latent Pretraining of LAPA. 4We do not compare with Yang et al. (2024b) since the model is not open-sourced and Ko et al. (2024) since it is not behavior cloning baseline."
        },
        {
            "title": "Preprint",
            "content": "ACTIONVLA denotes the baseline that uses the actual ground-truth robot action labels during pretraining with the same backbone VLM. This may be seen as the upper bound, since it utilizes the actual ground-truth labels. However, we find that LAPA is more performant in scenarios where there are embodiment shifts between pretraining and fine-tuning (Section 4.5). OPENVLA (Kim et al., 2024) is state-of-the-art VLA model that was pretrained on 970k realworld robot demonstrations from the Open X-Embodiment Dataset (Collaboration et al., 2023), mostly collected through human teleoperation. This model has comparable number of parameters to LAPA (7B). We compare against OPENVLA for real-world robot experiments by fine-tuning the pretrained OPENVLA on our downstream tasks. Further details of baseline models are provided in Appendix C."
        },
        {
            "title": "4.3 LANGUAGE TABLE RESULTS",
            "content": "Table 2: Language Table Results. Average Success Rate (%) across the three different pretrain-finetune combinations from the Language Table benchmark as described in Table 1. We also note the # of trajectories used for fine-tuning next to each category. We report the performance for individual tasks in Appendix E.1. In-domain (1k) Seen Unseen Cross-task (7k) Seen Unseen Cross-env (1k) Seen Unseen SCRATCH UNIPI VPT LAPA 15.69.2 22.012.5 44.07.5 62.08.7 ACTIONVLA 77.03.5 15.28.3 13.27.7 32.84.6 49.69.5 58.86.6 27.213.6 20.812.0 72.06.8 73.26.8 77.03. 22.411.0 16.09.1 60.86.6 54.89.1 58.86.6 15.69.2 13.68.6 18.07.7 33.612.7 64.85.2 15.28.3 12.07.5 18.49.7 29.612.0 54.07.0 In-Domain Performance First, we assess LAPAs ability to learn from small subset of indomain action label data by pretraining on 181k trajectories and finetuning on 1k action-labeled trajectories (0.5%). As shown in Table 2, LAPA largely outperforms SCRATCH and narrows the gap with ACTIONVLA despite not using action labels during pretraining. Additionally, LAPA surpasses UNIPI and VPT. Notably, while UNIPI handles simple tasks well, its diffusion model often generates incorrect plans for longer-horizon tasks, aligning with Du et al. (2024) (see Figure 17 of Appendix E.1). VPT, with the same backbone VLM as LAPA, outperforms UNIPI, showing the superiority of the VLA model, but still underperforms LAPA, highlighting the effectiveness of latent actions. Cross-Task Performance We investigate whether LAPAs broad skills can be retained after finetuning on specific task. Pretraining LAPA on 181k trajectories and finetuning on only separate tasks (7k), we evaluate all 5 task categories, similar to the in-domain setup, to assess latent pretrainings benefits for unseen tasks. When comparing LAPA and SCRATCH in Table 2 and Table 6, 7 in Appendix E.1, latent pretraining significantly benefits the separate task as well the other 4 task categories, resulting in significant boost in both seen and unseen setups. Like before, UNIPI is constrained by its diffusion models planning limitations, while VPT performs strongly, even surpassing ACTIONVLA in the unseen setting. This is likely due to using more labeled data (7k vs. 1k), helping the IDM generate more accurate pseudo labels. Cross-Environment Performance We further investigate if LAPA benefits downstream performance when the pretraining and fine-tuning environments are different. We pretrain LAPA on 440k real-world trajectories, and then finetune on 1k simulation trajectories, which can be seen as testing on setup where real2sim gap is present. 5 Figure 3 (a) compares the two environments. From Table 2, we observe that LAPA still significantly outperforms SCRATCH, showing that latent pretraining leads to positive transfer even on cross-environment setting. Notably, both UNIPI and VPT significantly underperforms LAPA, showing that learning to predict latent actions is more robust to cross-environment transfer. VPT only results in minor positive transfer, indicating that the IDM is not robust to environment shifts. 5We only test on the real2sim scenario since simulation evaluation is easily replicable and easier to setup. We leave exploration in the sim2real scenario as future work."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Real-world Tabletop Manipulation Results. We evaluate on total of 54 rollouts for each model encompassing unseen object combinations, unseen objects and unseen instructions. Average success rate (%) are shown. We provide detailed results depedning on the generalization type in Table 12 and individual results in Appendix E.3."
        },
        {
            "title": "4.4 SIMPLER RESULTS",
            "content": "We pretrain our models on the Bridgev2 (Walke et al., 2023) dataset and fine-tune on 100 trajectories collected from the SIMPLER environment (Li et al., 2024b) (25 trajectories for each task). Results As shown in Figure 4, UNIPI significantly underperforms all other baselines on the SIMPLER Environment. We observe that, although the generated plans from the diffusion models are quite accurate, the IDM lacks the capability to predict 7 DOF continuous actions accurately when given only 100 action-labeled trajectories. Specifically, we observe that UNIPI often fails to grasp the object within the maximum step limit. This implies the effectivness of using VLAs in scenarios with insufficient action-labeled data. Similar to the results of Section 4.3, LAPA outperforms baseline models that pretrain on actionless videos (UNIPI and VPT) and closes the performance gap with ACTIONVLA, which is pretrained on all of the 60K action-labeled trajectories from the Bridgev2 dataset. This highlights the effectiveness of LAPA, even when the complexity of the action space increases. We provide the detailed results of individual tasks and subtasks (grasping and reaching) in Table 10 of Appendix E.2. 4.5 REAL-WORLD RESULTS Figure 4: SIMPLER Results. Avg. success rate (%) is shown across 4 tasks. Detailed results are in Appendix E.2. We pretrain our models on (1) Bridgev2 (Walke et al., 2023) to measure the cross-embodiment performance (WidowX embodiment for pretraining and Franka embodiment for finetuning) and (2) Open X-Embodiment Dataset (Collaboration et al., 2023) to measure the effect of pretraining in multi-embodiment setting. Figure 5 shows the average success rate across the 3 tasks where each task encompasses unseen object combination, unseen object, and unseen instruction settings. We provide detailed results depending on the generalization type in Table 12 in Appendix E.3. We also measure the cross-embodiment performance of Open X-Embodiment Dataset pretraining by fine-tuning on bi-manual robots. Bridgev2 Pretraining We compare models that were pretrained on the Bridgev2 dataset. Similar to previous results, all models pretrained on Bridgev2 result in significant performance enhancement compared to SCRATCH. Furthermore, by comparing LAPA which does not leverage action-labeled trajectories during pretraining with models that use action-labeled trajectories during pretraining (ACTIONVLA and OPENVLA (Bridge)), we observe an interesting finding: LAPA outperform VLAs that use action labeled pretraining data on average success rate of the 3 tasks, unlike previous scenarios where VLAs pretrained on the ground-truth actions were upper bounds. LAPA significantly outperforms the other models in pick-and-place tasks; given that most tasks in Bridgev2 are pick-and-place, we hypothesize that VLA models pretrained on ground truth action labels have"
        },
        {
            "title": "Preprint",
            "content": "overfitted to the WidowX action space from the Bridgev2 dataset, hampering cross-embodiment adaptability to action distribution shifts during fine-tuning. In contrast, LAPA avoids this issue by not relying on ground truth action labels during pretraining. Open-X Pretraining From Figure 5, we see that VLAs pretrained on the Open-X dataset outperforms VLAs pretrained on the Bridgev2 dataset, showing that data scaling during pretraining demonstrates positive transfer for downstream tasks (Collaboration et al., 2023). This also suggests there could be significant further improvement when scaling the diversity and scale of the pretraining data, especially with large web-scale video data. When comparing LAPA with OPENVLA, we see that LAPA significantly outperforms OPENVLA on 2 out of 3 tasks (Figure 5). This highlights LAPAs effectiveness in multi-embodiment setting by showcasing its ability to leverage shared latent action space during pretraining, akin to how language and image representations are utilized. In contrast, contemporary action pretraining methods may suffer from reduced positive transfer between datasets due to the variability in action representation spaces across different embodiments and datasets. However, for pick and place task, LAPA underperforms OPENVLA. We observe that most failures of LAPA are due to early grasping. In fact, LAPA outperforms OPENVLA in reaching performance (83.33% vs 66.67%) (reaching performance for each task is provided separately in Appendix E.3). This suggests that, although LAPA possesses stronger language conditioning and coarse-grained planning abilities, there is room for improvement in skills such as grasping. Since grasping occurs only once or twice in each trajectory, the 150 labeled trajectories may not be sufficient for LAPA to accurately predict grasp actions based on the physical characteristics of diverse objects. To evaluate the cross-embodiment performance of LAPA pretrained with Open-X, we fine-tune both LAPA and OpenVLA on bimanual robot with 14-DoF action space, which presents more challenging cross-embodiment scenario than the previous Bridgev2 pretraining experiments due to cross-embodiment gap as well as the increase of the action space dimensions (there are no bimanual robot datasets in Open-X). Similar to the single-arm experiments, our evaluation covers seen object combinations, unseen object combinations, unseen objects, and unseen instructions. We observe that LAPA (30.21%), which does not use labeled action data during pretraining, slightly outperforms OpenVLA (26.04%) on average, confirming LAPAs cross-embodiment capability even with the increase of the complexity of the action dimensions. However, the absolute performance of both models remains relatively low, indicating much room for improvement. We believe that incorporating bimanual human manipulation videos (Grauman et al., 2022; Damen et al., 2022) could enhance LAPAs performance, which we plan to explore in future work. Individual results are provided in Table 17 in Appendix E.3. 4.6 LEARNING FROM HUMAN MANIPULATION VIDEOS In this section, we show results when we extend LAPA to human manipulation videos, which aligns with the main motivation of this work. Unlike robot trajectories, human videos have two challenges: human videos do not contain action labels, and the distribution of human videos is distinct from the robot embodiment (McCarthy et al., 2024). We try to investigate whether our method as well as baseline approaches could address these challenges by pretraining on Something-Something V2 dataset (Goyal et al., 2017) which consists of 220K videos that includes human performing actions with everyday objects. We first evaluate the performance of LAPA pretrained on human videos on SIMPLER. In addition to SCRATCH, we also compare with UNIPI and VPT pretrained with the same human video dataset. As shown in Figure 6a, LAPA outperforms SCRATCH, showing that although the distribution of the pretraining data is distinct from the deployment setup, leveraging human videos for latent action pretraining results in positive transfer. Also, consistent with the result of Section 4.4, LAPA shows the best performance, implying that Latent Action Pretraining is robust to human to robot embodiment shifts. Note that it is impossible to train ACTIONVLA because the human videos do not have any robot action labels. We report the real-world robot experiments in Figure 6b. Surprisingly, we can see that LAPA trained with human videos outperforms OPENVLA (Bridge) on average. Despite the larger embodiment gap for LAPA (Human to robot vs. Robot to robot), it learns better prior for robot manipulation."
        },
        {
            "title": "Preprint",
            "content": "(a) SIMPLER Results (b) Real-world Tabletop Manipulation Robot Results Figure 6: Pretraining from Human Video Results. Average success rate (%) of LAPA and baselines pretrained on human manipulation videos where the embodiment and environment gap is extreme. We evaluate on both simulation (left) and real-world robot setup (right). This result highlights the potential of raw human manipulation videos from the web compared to expensive robot manipulation data, which requires time-intensive teleoperation to collect. We expect that applying our approach on large-scale internet videos (e.g., YouTube videos) could unlock the potential for large-scale pretraining of generalist action foundational model, similar to foundational models in Natural Language Processing or Computer Vision. 4.7 PRETRAINING EFFICIENCY The benefit of LAPA extends beyond downstream task performance to include pretraining efficiency. For pretraining LAPA (Open-X), the best-performing model, we use 8 H100 GPUs for 34 hours with batch size of 128 (total of 272 H100-hours). In contrast, OPENVLA required total of 21,500 A100-hours with batch size of 2048. Despite being approximately 30-40 times more efficient for pretraining, LAPA still outperforms OPENVLA 6. We believe this efficiency stems from two factors: (1) the use of the Large World Model (Liu et al., 2024) as the backbone VLM model, and (2) the coarse-grained actions of LAPA compared to conventional action pretraining. First, the training objective during LWM pretraining includes generating the next state, which corresponds to the next frame in video. We hypothesize that this objective enables the model to implicitly understand high-level actions in video. Notably, ACTIONVLA (Bridge), which uses LWM as the backbone, and OPENVLA (Bridge), which uses Prismatic as the backbone, are trained on the same data and objective. However, ACTIONVLA reaches optimal performance (in terms of action token accuracy) in significantly fewer epochs (3 epochs) compared to OPENVLAs 30 epochs. Second, the action space for LAPA is much smaller than that for OPENVLA (84 vs. 2567), making learning the perception-and-language to action generation problem easier to learn. For all LAPA models (BridgeV2, Open-X, Human Videos), we observe that single epoch of training is sufficient to achieve optimal performance."
        },
        {
            "title": "5 ABLATION AND ANALYSIS",
            "content": "5.1 SCALING MODEL, DATA, AND LATENT ACTION SIZE Large Language Models (LLMs) have demonstrated scaling laws (Kaplan et al., 2020), where performance improves with increases in model size, dataset size, and computational resources used for training. Similarly, we attempt to analyze whether LAPA benefits from scaling across four dimensions: latent action quantization model size, data size, latent action sequence and vocabulary size. For controlled setup, we apply our method to Bridgev2 and then fine-tune it on SIMPLER. As shown in Figure 7, scaling benefits LAPA across the four dimensions. However, for the latent action space (sequence and vocabulary size), we believe there is sweet spot where performance improves up to certain point, after which it either plateaus or diminishes. Interestingly, we observe that the optimal scale of the latent action space depends on the complexity of the action dimen6We calculate based on the fact that H100 GPUs lead to 2-3 times speedup compared to A100 GPUs for training."
        },
        {
            "title": "Preprint",
            "content": "(a) Model Scaling (b) Data Scaling (%) (c) Latent Action Seq (d) Latent Action Vocab Figure 7: Scaling Ablation Results of LAPA. We scale 4 dimensions of LAPA: model parameters (in millions), data size (ratio among Bridgev2), and the latent action sequence and vocabulary size, and show the downstream average success rate (%) on the SIMPLER fine-tuning tasks. sion contained in the pretraining dataset. For example, increasing the latent action sequence length is less effective compared to increasing the vocabulary for Language Table (Figure 16). Except for Language Table, we maintain the generation space of LAPA at 84 throughout all of our main experiments. These results imply that when scaling pretraining to Internet-scale videos that go beyond manipulation videos, scaling LAPA in terms of model, dataset, and latent action space could improve performance, especially to capture higher action dimensions such as whole-body locomotion and manipulation. 5.2 LATENT ACTION ANALYSIS We qualitatively analyze the alignment of quantized latent actions with real continuous actions. For interpretation, we condition the current image observation x1 and each latent action on the decoder of the latent action quantization model, and present the reconstructed images. Figure 8: Latent Action Analysis in Language Table. We condition the current observation x1 and quantized latent action to the decoder of the latent action quantization model. We observe that each latent action can be mapped into semantic action. For example, latent action 0 corresponds to moving bit left and forward. First, we analyze latent actions learned from Language Table with vocabulary size of 8 and sequence length of 1. In Figure 8, we show that each latent action corresponds to distinct movement of the robot arm, with the distribution of latent actions being wellclustered in the actual 2D action space. We observe that increasing the latent action vocabulary size leads to capturing more finegrained information. We analyze the relationship between latent actions with ground-truth 2 DOF actions by mapping each instance into latent action space. As shown in Figure 9, we observe that latent actions are well-clustered in the actual 2D action space, indicating that latent actions are meaningful representations that are highly related to actual continuous actions. Figure 9: Correlation of latent actions with ground-truth actions."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Latent Action Analysis in Human Manipulation Videos. We condition the current observation x1 and quantized latent action to the decoder of the latent action quantization model. We observe that each latent action can be mapped into semantic action including camera movements. For example, latent action [3,5,2,7] corresponds to moving the camera bit down while [4,2,0,0] corresponds to moving the camera slightly up. We further analyze the latent actions learned from human manipulation videos using the SomethingSomething V2 dataset. As illustrated in Figure 10, these latent actions capture not only hand movements but also camera movements. Since the camera viewpoint varies throughout the videos in the Something-Something V2 dataset due to the videos being egocentric, our latent action quantization model also learns to represent camera movements. Figure 11: Latent Action Analysis in Multi-Embodiment Setting. We condition the current observation x1 and quantized latent action to the decoder of the latent action quantization model. We observe that each latent action can be mapped into similar semantic action even though the embodiments are different. For example, latent action [1,1,3,2] corresponds to going down and left while [3,2,0,1] corresponds to going up little bit. We also analyze the latent actions learned from the Open-X embodiment, which encompasses multiple embodiments, tasks, and environments. As shown in Figure 11, even though the embodiment and environment differ, conditioning on the same latent action results in similar action in the reconstructed image. This supports our previous claim that latent actions are learned in shared representation space, regardless of the embodiment or dataset, facilitating stronger positive transfer across diverse datasets. Figure 12: Closed loop rollout of LAPA. LAPA is conditioned on current image x1 and language instruction of take the broccoli out of the pot. We generate rollout images by conditioning the decoder of Latent Action Quantization Model with latent actions generated by LAPA. We also qualitatively analyze the coarse-grained planning capability of LAPA through closed-loop rollout. We use LAPA model that has only undergone pretraining, without any action finetuning."
        },
        {
            "title": "Preprint",
            "content": "Since this model generates latent actions that are not directly executable in the real world, we condition the current observation x1 and the predicted latent action from LAPA with the decoder of the latent action quantization model. As shown in Figure 12, when conditioned on the current observation and the instruction to take the broccoli out of the pot, LAPA generates robot trajectories that successfully reaches for the broccoli, moves down to grab it, and, as the arm moves away from the pot, the broccoli disappears. This shows the potential for LAPA as general-purpose robotic world model, not only predicting actions but also the outcomes of the actions. For example, this can lead to an extension of LAPA to act as Task and Motion planning system, where it can first generate multiple plans given natural language task instruction, choose the most optimal trajectory based on methods of quantifying the success among multiple trajectory candidates (Hwang et al., 2024; Duan et al., 2024), and perform open-loop / closed-loop inference. This can lead paradigm where we aim to improve end-task performance through scaling test-time compute, as with LLMs (Snell et al., 2024)."
        },
        {
            "title": "6 LIMITATIONS AND CONCLUSION",
            "content": "In this paper, we introduce LAPA, scalable pretraining method for building VLAs using actionless videos. Across three benchmarks spanning both simulation and real-world robot experiments, we show that our method significantly improves transfer to downstream tasks compared to existing approaches. We also present state-of-the-art VLA model that surpasses current models trained on 970K action-labeled trajectories. Furthermore, we demonstrate that LAPA can be applied purely on human manipulation videos, where explicit action information is absent, and the embodiment gap is substantial. We believe our work can be extended to build scalable robot foundation models. We still face certain limitations. First, LAPA underperforms compared to action pretraining when it comes to fine-grained motion generation tasks like grasping. We believe that increasing the latent action generation space could help address this issue. Second, similar to prior VLAs, LAPA also encounters latency challenges during real-time inference. Adopting hierarchical architecture, where smaller head predicts actions at higher frequency, could potentially reduce latency and improve fine-grained motion generation. Lastly, while we qualitatively demonstrate that our latent action space captures camera movements (Figure 10), we have not yet explored the application of LAPA beyond manipulation videos, such as those from self-driving cars, navigation, or landscape scenes. We leave these explorations for future work. We hope that our work can help overcome the data bottleneck in robotics and accelerate the development of generalist robot policies."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Arhan Jain and Marius Memmel for helping out with the robot hardware and teleoperation setup. Also, we thank Minyoung Hwang, Jiafei Duan, Junsu Kim, and Changyeon Kim for helpful discussions and constructive feedback."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos as versatile representation for robotics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. In Advances in Neural Information Processing Systems, 2022. Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. arXiv preprint arXiv:2403.01823, 2024. Homanga Bharadhwaj, Abhinav Gupta, Shubham Tulsiani, and Vikash Kumar. Zero-shot robot manipulation from passive human videos. arXiv preprint arXiv:2302.02011, 2023. Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. arXiv preprint arXiv:2409.16283, 2024a. Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation. arXiv preprint arXiv:2405.01527, 2024b. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019. OX-Embodiment Collaboration, Padalkar, Pooley, Jain, Bewley, Herzog, Irpan, Khazatsky, Rai, Singh, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision (IJCV), 130:3355, 2022. Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Abhinav Gupta. An unbiased look at datasets for visuo-motor pre-training. In Conference on Robot Learning, 2023. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Yilun Du, Sherry Yang, Pete Florence, Fei Xia, Ayzaan Wahid, brian ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Andy Zeng, and Jonathan Tompson. Video language planning. In The Twelfth International Conference on Learning Representations, 2024."
        },
        {
            "title": "Preprint",
            "content": "Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. Aha: vision-language-model for detecting and reasoning over failures in robotic manipulation, 2024. Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. Imitating latent policies from observation. arXiv preprint arXiv:1805.07914, 2018. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, 2017. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Minyoung Hwang, Joey Hejna, Dorsa Sadigh, and Yonatan Bisk. Motif: Motion instruction finetuning. arXiv preprint arXiv:2409.10683, 2024. Zhengyao Jiang, Yingchen Xu, Nolan Wagener, Yicheng Luo, Michael Janner, Edward Grefenstette, Tim Rocktaschel, and Yuandong Tian. H-gap: Humanoid control with generalist planner. arXiv preprint arXiv:2312.02682, 2023. Aditya Kannan, Kenneth Shaw, Shikhar Bahl, Pragna Mannam, and Deepak Pathak. Deft: Dexterous fine-tuning for real-world hand policies. arXiv preprint arXiv:2310.19797, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua B. Tenenbaum. Learning to act from actionless videos through dense correspondences. In The Twelfth International Conference on Learning Representations, 2024. Hadas Kress-Gazit, Kunimatsu Hashimoto, Naveen Kuppuswamy, Paarth Shah, Phoebe Horgan, Gordon Richardson, Siyuan Feng, and Benjamin Burchfiel. Robot learning as an empirical science: Best practices for policy evaluation. arXiv preprint arXiv:2409.09491, 2024. Seungjae Lee, Yibin Wang, Haritheja Etukuru, Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Behavior generation with latent actions. arXiv preprint arXiv:2403.03181, 2024. Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, et al. Llara: Supercharging robot learning data for vision-language policy. arXiv preprint arXiv:2406.20095, 2024a. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024b. Junbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, and Carl Vondrick. Dreamitate: Real-world visuomotor policy learning via video generation. arXiv preprint arXiv:2406.16862, 2024. Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024."
        },
        {
            "title": "Preprint",
            "content": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In Conference on robot learning, pp. 1113 1132. PMLR, 2020. Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, pp. 18, 2023. doi: 10.1109/LRA.2023.3295255. Robert McCarthy, Daniel CH Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas Thuruthel, and Zhibin Li. Towards generalist robot learning from internet video: survey. arXiv preprint arXiv:2404.19664, 2024. Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, and Animesh Garg. Quest: Selfsupervised skill abstractions for learning continuous control. arXiv preprint arXiv:2407.15840, 2024. Zawalski Michał, Chen William, Pertsch Karl, Mees Oier, Finn Chelsea, and Levine Sergey. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022. Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. arXiv preprint arXiv:2406.11815, 2024. Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong Wang. Dexmv: Imitation learning for dexterous manipulation from human videos. In European Conference on Computer Vision, 2022. Dominik Schmidt and Minqi Jiang. Learning to act without actions. In The Twelfth International Conference on Learning Representations, 2024. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016. Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. Videodex: Learning dexterity from internet videos. In Conference on Robot Learning, 2023. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Mohan Kumar Srirama, Sudeep Dasari, Shikhar Bahl, and Abhinav Gupta. Hrp: Human affordances for robotic pre-training. arXiv preprint arXiv:2407.18911, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Mohammad Hassan Vali and Tom Backstrom. Nsvq: Noise substitution in vector quantization for machine learning. IEEE Access, 10:1359813610, 2022. doi: 10.1109/ACCESS.2022.3147670. Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems, 2017."
        },
        {
            "title": "Preprint",
            "content": "Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2023. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, 2023. Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play. arXiv preprint arXiv:2302.12422, 2023. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Any-point trajectory modeling for policy learning. arXiv preprint arXiv:2401.00025, 2023. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In The Twelfth International Conference on Learning Representations, 2024. Jingyun Yang, Zi-ang Cao, Congyue Deng, Rika Antonova, Shuran Song, and Jeannette Bohg. Equibot: Sim (3)-equivariant diffusion policy for generalizable and data efficient learning. arXiv preprint arXiv:2407.01479, 2024a. Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In The Twelfth International Conference on Learning Representations, 2024b. Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Position: Video as the new language for real-world decision making. In Proceedings of the 41st International Conference on Machine Learning, 2024c. Jianglong Ye, Jiashun Wang, Binghao Huang, Yuzhe Qin, and Xiaolong Wang. Learning continuous grasping function with dexterous hand from human demonstrations. IEEE Robotics and Automation Letters, 8(5):28822889, 2023. Jia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Chen, Hao Dong, Haoming Song, Dong Wang, Di Hu, Ping Luo, et al. Learning manipulation by predicting interaction. arXiv preprint arXiv:2406.00439, 2024. Yifeng Zhu, Arisrei Lim, Peter Stone, and Yuke Zhu. Vision-based manipulation from single human video with open-world object graphs. arXiv preprint arXiv:2405.20321, 2024."
        },
        {
            "title": "A LATENT ACTION QUATIZATION MODEL DETAILS",
            "content": "Figure 13: Model architecture of our Latent Action Quantization Model. We show model architecture details of our latent action quantization model in Figure 13. We utilize the C-ViViT model architecture from Villegas et al. (2022) to replicate the latent action model from GENIE (Bruce et al., 2024). After latent model training, we utilize the z2 as the latent action label for x1. The encoder can be seen as the inverse dynamics model and the decoder can be seen as the world model."
        },
        {
            "title": "B DETAILS ON EXPERIMENTAL SETUP",
            "content": "Language Table Experimental Setup Figure 3 (a) shows examples of the Language Table setup. For Language Table experiments, we train VLA-based models to generate language directions (e.g. move up) before actual actions following Belkhale et al. (2024), which significantly improved the performance 7. For evaluation, we evaluate on 50 evaluation rollouts for each subtask category where the initial locations of the objects are randomized for each evaluation. Further details can be found in https://github.com/google-research/language-table. SIMPLER Experimental Setup Figure 3 (B) shows examples of the SIMPLER setup. The SIMPLER environment does not provide any fine-tuning data for their evaluation pipeline, Thus, we first train our underlying VLM on the Bridgev2 dataset and perform zero-shot rollout on the 4 tasks in SIMPLER. Note that we use held-out trajectories differing in object orientation and position from the evaluation setup. We filter 25 successful trajectories for each task (total of 100) and use them as the fine-tuning dataset for all of our experiments. For evaluation, we evaluate on 24 rollouts per task while randomizing the initial object locations. We consider Bridgev2 and SIMPLER to be in-domain since they show high correlation between real-world and simulation results with their simulation benchmark. Further details can be found in https://github.com/simpler-env/SimplerEnv. 7For 7 DOF robot experiments, we found the benefit of generating language directions to be marginal compared to the increased inference cost. Therefore, we only generate delta end-effector actions on other experiments."
        },
        {
            "title": "Preprint",
            "content": "Real-world Tabletop Manipulation Experimental Setup Figure 3 (C) shows examples of the real-world tabletop manipulation experimental setup. For the teleoperation of single arm robot experiments, we use the polymetis robotic stack8 to collect 150 trajectories for each of the tasks. All of the tasks require multi-instruction following capabilities since there are 3 objects in the scene and the model has to condition on the task description to infer which object to interact with. Figure 14 shows samples of each task. For each task, we aim to quantify 3 distinct capabilities: (1) We test the ability to infer the correct object from the task description between an unseen combination of seen objects during fine-tuning, (2) We test the ability to infer the correct object from totally unseen objects during fine-tuning that may or may have not been observed during pretraining. Specifically, the knocking tasks was conducted with real-world objects that were highly unlikely to have been in any of the pertaining datasets. (3) We test the ability to infer the correct object (among seen objects, unseen combinations) from totally unseen instruction that requires semantic reasoning (e.g. Pick up spicy object). For each evaluation criteria, 6 rollouts are performed for each models, resulting in total of 18 rollouts for each task category. Since there are three tasks, each model is evaluated with 54 rollouts in the real-world. We provide the full list of all of the seen and unseen objects used for each rollout in Table 13, 14, 15, and the total average success rates in Table 16. Furthermore, for fair comparison, we match the image resolution during training of all of our models and use the exact same object initial positions for all of our evaluation, mostly on the same day to minimize variability. For evaluation metrics, we adapt partial success criteria for finegrained evaluation, following Kim et al. (2024), which we describe in detail below. Knock down the <object>. For knocking, we give 0.5 partial score if the robot reaches to the correct object and 1 if the robot knocks down the correct object. Cover the <object> with towel. For covering, we give 0.33 partial score if the robot picks up the towel correctly, 0.66 if the robot reaches to the correct object or if the towel partially covers the object, and 1 if the correct object is completely covered by the towel. Pick up the <object> and put it in the sink. For pick and place, we give 0.25 for reaching to the correct object, 0.5 for grasping the object, 0.75 for grasping and moving the object towards the sink, but failing to place the object in the sink, and 1 for placing the correct object in the sink. Similar to single arm experiments, we collect 150 trajectories for bi-manual experiments. For the bi-manual task, one of the arms should choose one between two plates depending on the language instruction and put the chosen plate on the container. Next, the other arm should pick up correct object from three object candidates and put the object on the plate. For evaluation, we quantify (1) seen combination of seen objects during fine-tuning, (2) unseen combinations of seen objects, (3) unseen objects, (4) unseen instructions of seen objects. 6 rollouts are performed for each sub-category, resulting in 24 rollouts for each model. We provide the full result in Table 17. For evaluation metrics, we adapt partial success criteria for fine-grained evaluation. Put the <object1> on the container and <object2> on the plate. For bi-manual task, we give 0.25 partial score if the robot reaches to the correct plate, 0.5 if the robot puts the correct plate on the container successfully, 0.75 if the robot reaches to the correct object on the right, and 1 if the robot successfully pick the object and place the object on top of the plate."
        },
        {
            "title": "C BASELINE DETAILS",
            "content": "For UNIPI, we use diffusion model from (Ko et al., 2024) which can be trained on 4 A100 GPUs. For all experiments, we train with 128 batch. We use the same inverse dynamics model as VPT during inference. To mediate estimation errors between the predicted video plans and executed 8https://github.com/facebookresearch/polymetis"
        },
        {
            "title": "Preprint",
            "content": "actions being accumulated, we periodically conduct replanning by regenerating new video plans after executing two actions. For VPT, we use ResNet18 followed by an MLP layer for the inverse dynamics model(IDM). The IDM is trained to predict an action when given two frames on single A6000 GPU using using Adam optimizer with learning rate 1e-4. For OpenVLA (Bridge), we pretrain on Bridgev2 for 30 epochs with batch size of 1024. For OpenVLA (Open-X), we use the pretrained checkpoint from Kim et al. (2024). For finetuning, we use LoRA finetuning (Hu et al., 2022) with batch size of 32. We have observed that full-finetuning and lora finetuning leads to similar performance, so we use LoRA finetuning as default for efficient fine-tuning. We finetune the model until the training action accuracy reaches 95%. For ACTIONVLA and LAPA, we train with batch size of 128 and with image augmentation for real-world finetuning."
        },
        {
            "title": "D EXPERIMENTAL RESULT ANALYSIS",
            "content": "Table 3: Pretraining trajectories statistics for downstream tasks. Number of trajectories that are the same task with evaluation task for each pretraining dataset: Bridgev2, Open-X, and Something Something V2 (Sthv2) dataset. Task Bridgev2 Open-X Sthv2 Knocking Covering Pick & Place 2 898 10,892 7,969 5,026 911, 6,655 6,824 3,272 We further analyze the real-world robot results shown in Figures 5 and 6b, focusing on how the task distribution in pretraining data impacts downstream performance. Table 3 presents the number of trajectories corresponding to each evaluation task (Knocking, Covering, and Pick & Place) across pretraining datasets (Bridgev2, Open-X, and Something Something V2 (Sthv2)), determined through lexical matching. We expect future work to use other methods of analyzing the relationship between pertaining and fine-tuning task distributions that capture semantics of the task rather than simple lexical matching. We perform this analysis to get sense of how the task distribution in the pretraining data affects downstream task performance. Knocking There are almost no knocking-related trajectories in Bridgev2. This scarcity may explain why models trained on Bridgev2 performed worse compared to those trained on Sthv2, despite larger embodiment gap in the Sthv2 dataset (Figure 6b). Covering similar trend is observed for the covering task. Given that the number of covering trajectories in Bridgev2 is relatively small compared to the Sthv2 dataset, models trained on Bridgev2 occasionally underperform compared to LAPA trained on Sthv2. Pick & Place For the pick and place task, the trend reverses. The number of pick and place tasks in Sthv2 is relatively small compared to Bridgev2 and Open-X, which might explain why LAPA trained on Sthv2 significantly underperforms models trained on Bridgev2 or Open-X. Based on these results, we expect that pretraining on videos encompassing wide range of skills will lead to more robust generalist policy compared to training on robot videos with narrower skill sets. We also expect future research to provide more in-depth analysis of the relationship between task distribution in pretraining data and performance on downstream tasks. We also present the win rate of LAPA (Open-X) against OpenVLA (Open-X). As illustrated in Figure 15, LAPA outperforms OpenVLA in 65.4% when disregarding the ties. When considering the ties, LAPA outperforms OpenVLA in 31.5% of cases, while OpenVLA prevails in only 16.7%. Interestingly, they tie in 51.9% of the trials, suggesting that in about half the instances, both models either fail or achieve similar partial success score. Note that these evaluations were performed while ensuring that the target and distractor objects were in identical initial locations during evaluation, alternating the models during evaluation. These results provide insight into the statistical significance of the comparison, supporting the use of multiple metrics to ensure more comprehensive evaluation of physical robot performance in real-world scenarios (Kress-Gazit et al., 2024), not only the average success-rate across all of the evaluation rollouts."
        },
        {
            "title": "E DETAILED EXPERIMENTAL RESULTS",
            "content": "E.1 LANGUAGE TABLE We provide the detailed results of the experiments performed on the Language Table benchmark in Table 4, 5, 6, 7, 8, 9. For all of the tables in the appendix, we bold the best result among the comparisons and underline the second best. Each value denotes the success rate (%). 50 evaluation rollouts are performed for each task category, resulting in 250 total evaluation rollouts per model for each table. We also show the qualitative result of UNIPI where the diffusion model generates the correct plan for simple and short-horizon tasks (e.g. separate tasks). However, the diffusion model generates the wrong plan corresponding to the instruction when the task requires longer horizon planning (Figure 17). For ablation of LAPA in Language Table, we experiment with various latent action vocab and sequence sizes. As shown in Figure 16, increasing the sequence and vocab size increases the performance. However, unlike SIMPLER, we observe that the increasing the latent action vocab size is much more effective compared to increasing the latent action sequence length in terms of absolute performance. This implies that for environments that are visually simple, increasing the latent action vocabulary might be more effective compared to sequence length. Table 4: Language Table In-Domain Seen Results. SCRATCH UNIPI VPT LAPA ACTIONVLA Block2Block Block2Absolute Block2BlockRelative Block2Relative Separate AVG 4.0 6.0 10.0 6.0 52.0 15.6 14.0 4.0 12.0 10.0 72. 22.4 36.0 38.0 48.0 26.0 70.0 43.6 58.0 56.0 52.0 48.0 96.0 62.0 76.0 72.0 76.0 70.0 90. 76.8 Table 5: Language Table In-Domain Unseen Results. SCRATCH UNIPI VPT LAPA ACTIONVLA Block2Block Block2Absolute Block2BlockRelative Block2Relative Separate AVG 8.0 10.0 2.0 8.0 48.0 15.2 4.0 6.0 6.0 6.0 44.0 13.2 26.0 42.0 20.0 32.0 44.0 32. 50.0 48.0 28.0 38.0 84.0 49.6 62.0 58.0 48.0 44.0 82.0 58.8 Table 6: Language Table Cross-Task Seen Results. SCRATCH UNIPI VPT LAPA ACTIONVLA Block2Block Block2Absolute Block2BlockRelative Block2Relative Separate AVG 18.0 8.0 6.0 24.0 80.0 27.2 12.0 6.0 2.0 16.0 68. 20.8 74.0 56.0 62.0 72.0 96.0 72.0 74.0 62.0 72.0 60.0 98.0 73.2 76.0 72.0 76.0 70.0 90. 76.8 E.2 SIMPLER We provide detailed results of various models evaluated on SIMPLER environment. Table 10 shows the setting where baseline models are pretrained on Bridgev2 and then finetuned on SIMPLER rollouts (100 videos). The results show detailed results for each task (stack green to yellow block, put carrot on plate, put spoon on otowel, put eggplant in basket) and subtasks (grasping and moving)."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Language Table Cross-Task Unseen Results. SCRATCH UNIPI VPT LAPA ACTIONVLA Block2Block Block2Absolute Block2BlockRelative Block2Relative Separate AVG 16.0 10.0 8.0 12.0 66. 22.4 4.0 10.0 10.0 4.0 52.0 16.0 66.0 56.0 46.0 52.0 84.0 60.8 46.0 52.0 48.0 38.0 90. 54.8 62.0 58.0 48.0 44.0 82.0 58.8 Table 8: Language Table Cross-Environment Seen Results. SCRATCH UNIPI VPT LAPA ACTIONVLA Block2Block Block2Absolute Block2BlockRelative Block2Relative Separate AVG 4.0 6.0 10.0 6.0 52.0 15.6 4.0 4.0 8.0 4.0 48.0 13. 16.0 8.0 6.0 12.0 48.0 18.0 26.0 16.0 20.0 22.0 84.0 33.6 66.0 58.0 62.0 54.0 84.0 64. Table 9: Language Table Cross-Environment Unseen Results. SCRATCH UNIPI VPT LAPA ACTIONVLA Block2Block Block2Absolute Block2BlockRelative Block2Relative Separate AVG 8.0 10.0 2.0 8.0 48. 15.2 2.0 6.0 6.0 4.0 42.0 12.0 2.0 4.0 2.0 40.0 44.0 18.4 30.0 14.0 10.0 18.0 76. 29.6 38.0 48.0 50.0 54.0 80.0 54.0 We also provide detailed results of the setting where baseline models are pretrained on human manipulation videos (Something Something V2 dataset) and then finetuned on SIMPLER rollouts (100 videos) in Table 11. We only compare to UNIPI, VPT, and LAPA since ACTIONVLA could not be trained without ground-truth action labels. E.3 REAL-WORLD We provide the detailed result of real world evaluation depending on the generalization type: (1) seen objects but unseen combinations, (2) unseen objects, and (3) seen objects but unseen instructions. The results are shown in Table 12. As shown in the table, LAPA (Open-X) outperforms OpenVLA (Open-X) on all types of generalization settings. Also, LAPA (Human Videos) shows good generalization performance, especially for unseen objects. We conjecture that this is because Something Something V2 dataset interacts with much diverse objects compared to Bridgev2. We also provide the full list of objects and the partial success recorded for each of the evaluation rollout: Knocking  (Table 13)  , Covering  (Table 14)  , and Pick & Place  (Table 15)  . The total average success rate is provided in Table 16)."
        },
        {
            "title": "Preprint",
            "content": "Table 10: SIMPLER results of Bridgev2 Pretraining. Success, Grasping, and Moving Rates (%) in SIMPLER environment. We pretrain UNIPI, VPT, and LAPA on Bridgev2 dataset without using ground-truth action labels and ACTIONVLA on Bridgev2 using action labels. The main 4 tasks are: stack green to yellow block, put carrot on plate, put spoon on towel, and put eggplant in basket. Best is bolded and second best is underlined. Success Rate SCRATCH UNIPI VPT LAPA ACTIONVLA Stack G2Y Carrot2Towel Spoon2Plate Eggplant2Bask AVG Grasping Rate Grasp Green Block Grasp Carrot Grasp Spoon Grasp Eggplant AVG Moving Rate Move Green Block Move Carrot Move Spoon Move Eggplant AVG 29.2 29.2 50.0 29.2 34.4 66.6 45.8 70.8 62.5 61.4 58.3 45.8 70.8 87.5 65. 2.7 2.7 0.0 0.0 1.3 20.8 33.2 22.2 16.0 23.1 29.1 48.6 34.6 58.0 42.6 45.8 37.5 70.8 50.0 51.0 62.5 54.1 79.2 70.8 66.7 58.3 66.6 79.2 70.8 68. 54.2 45.8 70.8 58.3 57.3 62.5 58.3 83.3 83.3 71.9 66.6 70.8 83.3 87.5 77.1 75.0 58.0 70.8 50.0 63.5 87.5 75.0 83.3 75.0 80.2 91.6 91.6 79.2 91.6 88. Table 11: SIMPLER results of Human Manipulation Video Pretraining. Success, Grasping, and Moving Rates (%) in SIMPLER environment. We pretrain UNIPI, VPT, and LAPA on Something-Something V2 dataset without using ground-truth action labels. The main 4 tasks are: stack green to yellow block, put carrot on plate, put spoon on towel, and put eggplant in basket. Best is bolded and second best is underlined. Success Rate VPT UNIPI LAPA StackG2Y Carrot2Towel Spoon2Plate Eggplant2Bask AVG Grasping Rate Grasp Green Block Grasp Carrot Grasp Spoon Grasp Eggplant AVG Moving Rate Move Green Block Move Carrot Move Spoon Move Eggplant AVG 0.0 1.3 1.3 0.0 0.7 2.7 31.7 21.7 6.8 15.7 2.7 37.5 18.1 50.3 27. 50.0 50.0 50.0 58.3 52.1 58.3 62.5 75.0 70.8 66.7 62.5 70.8 75.0 83.3 72.9 50.0 29.1 37.5 66.6 45.8 66.6 45.8 70.8 91.6 68.7 62.5 58.3 54.1 91.6 66."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Real-world Tabletop Manipulation Examples."
        },
        {
            "title": "Preprint",
            "content": "(a) Win rate (%) disregarding ties. (b) Win rate (%) with ties. Figure 15: Pairwise win rate (%). We compare pairwise win-rate of OpenVLA and LAPA across the 54 evaluation rollouts in the real-world. (a) shows the win-rate while ignoring the ties and (b) shows the ties together with the individual wins. (a) Latent Action Seq (b) Latent Action Vocab Figure 16: Ablation Results of LAPA in Language Table. We try various latent action vocab and sequences of LAPA and show the downstream average success rate (%) on the Language Table fine-tuning tasks. Figure 17: Success and Failure Cases of UNIPI. (Top) Given the instruction of move the green block away from the red cube and red pentagon, the diffusion model of UNIPI successfully generates the plan. (Bottom) Given the instruction of put the blue moon toward the yellow block, the diffusion model fails to generate the correct plan. Table 12: Evaluation Results divided into eval types. We average the success rate across the 3 tasks depending on what capability we are trying to quantify: (1) seen objects but unseen combinations, (2) unseen objects, and new instructions requiring semantic reasoning. Best is bolded and second best is underlined. Seen Objects Unseen Unseen Combinations Objects Unseen Instructions Seen Objects SCRATCH ACTIONVLA (Bridge) OPENVLA (Bridge) LAPA (Bridge) OPENVLA (Open-X) LAPA (Open-X) LAPA (Human Videos) 18.0 38.3 35.6 43.4 46.2 57. 36.5 20.3 31.8 34.6 31.4 42.1 43.9 37.4 25. 27.7 22.1 35.6 43.4 48.5 28.1 25 AVG 21. 32.6 30.8 36.8 43.9 50.1 34."
        },
        {
            "title": "Preprint",
            "content": "Table 13: Knocking Task Results OpenVLA (OpenX) LAPA (OpenX) OpenVLA ActionVLA (Bridge) (Bridge) LAPA (Bridge) Scratch LAPA (Sthv2) Unseen Object Combinations flamingo pistachios soft scrub white cup mustard water bottle SUM pringles herseys chocolate syrup popcorn skittles green board marker paper towel SUM drink that contains orange food to eat with milk object used for cleaning something to wash dishes the nuts rectangle object SUM 0 0.5 0 1 0 1 2.5 0.5 0 0 0 0.5 1 0 0.5 0 1 1 1 3.5 0.5 1 0 0 1 1 3.5 0.5 0.5 0 0 0 0. 1.5 Unseen Objects 0.5 0 1 0 0.5 0 2 0.5 0 1 0 0.5 0 Unseen Instructions 0 0 1 1 1 1 4 0 0 0 0 0.5 0.5 1 0.5 0 0 0.5 0 1 0 0 1 0 0.5 0 1.5 0 0 0 0.5 1 0.5 2 0 1 0.5 0.5 0 2 0 0 1 0 0.5 0 1.5 0 0 0 1 1 0.5 2.5 0 0 0 0.5 0 0. 1 0 0 0 0 0.5 0 0.5 0 0 0 0.5 0.5 0 1 0.5 1 0.5 0 0 2 0 0 1 0 0.5 0 1.5 0 0 0 0 1 1 2 Success Rate (Strict) Success Rate Reaching Success Rate 27.78% 38.89% 50.00% 44.44% 5.56% 52.78% 25.00% 61.11% 44.44% 11.11% 25.00% 38.89% 22.22% 0.00% 22.22% 33.33% 13.89% 30.56% 44.44% 27.78% 38.89%"
        },
        {
            "title": "Preprint",
            "content": "Table 14: Covering Task Results OpenVLA (OpenX) LAPA (OpenX) OpenVLA ActionVLA (Bridge) (Bridge) LAPA (Bridge) Scratch LAPA (Sthv2) Unseen Object Combinations 0.33 0.33 0.33 0.33 0.66 0.33 2.31 0.33 0.33 0.33 0.33 1 2.32 0.33 0.66 0.33 0.33 0.33 0.33 2.31 0.33 1 0 0.33 1 1 3.66 1 0.33 0.33 1 0 3.66 0.33 0.33 0.33 0.33 1 0.33 2.65 Unseen Objects 0.66 1 0.33 1 0.66 0.33 3. Unseen Instructions 0 0.66 0.33 0.33 0.33 0.33 1.98 0.66 0 0.33 0 0.66 0.33 1.98 0.33 1 0.33 0.33 1 0. 3.32 1 0 0.33 0.33 1 0 2.66 1 0 0.33 0.33 0 0 1.66 0.33 0.33 0.33 0.33 1 0. 2.65 0.66 0.33 0.33 0 1 0.66 2.98 0.33 0.33 0.33 0.33 0.33 0.33 1.98 0.33 1 0.33 0 0.33 1.99 0.66 1 0.33 0.33 0.33 0 2.65 0.33 0 0.33 0.33 1 0.33 2.32 0 1 0.33 0.33 0.33 0. 2.32 0.33 1 0.33 0.33 1 1 3.99 0.66 0.33 0.33 0.33 0.33 0.33 2.31 icecream strawberry pepper watermelon blue lego block pink duck SUM donut orange mushroom yellow lego block peas egg SUM drink yellow object fruit vegetable edible object condiment SUM Success Rate (Strict) Success Rate Reaching Success Rate 5.56% 38.56% 16.66% 33.33% 16.67% 51.67% 47.83% 38.89% 38.89% 27.78% 42.44% 27.78% 11.11% 16.67% 22.22% 42.28% 38.67% 47.89% 22.22% 22.22% 27.78%"
        },
        {
            "title": "Preprint",
            "content": "Table 15: Pick & Place Sink Task Results OpenVLA (OpenX) LAPA (OpenX) OpenVLA ActionVLA (Bridge) (Bridge) LAPA (Bridge) Scratch LAPA (Sthv2) Unseen Object Combinations milk orange lego block ketchup corn icecream salt SUM carrot yellow paprika yellow cube salmon sushi orange blue cube SUM an object that is yellow an object that is round an object that is fruit an object that you can drink an object that is vegetable an object that is an animal SUM 1 1 0.25 1 0.25 0 3.5 1 1 1 0 1 0. 4.25 1 0 1 0 0 0 2 1 1 0.25 0.75 0 0.25 3.25 1 0 0.25 1 0 2.25 Unseen Objects 0.25 1 0.5 0.25 0 0.25 2.25 0 0 0.25 0 0 0 0. Unseen Instructions 1 0.25 1 0.25 0 0.25 2.75 0 0 1 0 0 0 1 1 1 0.25 0.25 0 3.5 0.25 0.25 0.5 0.5 0 0 1.5 1 0 1 0.5 0 0.25 2.75 1 0 0 0.25 1 2.25 1 0.25 0 0 0 0 1.25 0.25 0 0 0 0 0.25 0.5 0 0 0 0.25 0 0.25 0.25 0 0 0 0.25 0 0.5 0 0.25 1 0 0 0 1.25 1 0 0 0.25 1 2.25 0.25 1 0 0 0 0 1.25 0 0 0.75 0 0 0 0.75 Success Rate (Strict) Success Rate Reaching Success Rate 50.00% 27.78% 54.17% 45.83% 66.67% 16.67% 19.44% 83.33% 27.78% 27.78% 43.06% 72.22% 16.67% 5.56% 16.67% 22.22% 11.11% 23.61% 38.89% 27.78% 33.33% Table 16: Summary of Single Arm Total Success Rates (%) OpenVLA (OpenX) LAPA (OpenX) OpenVLA ActionVLA (Bridge) (Bridge) LAPA (Bridge) Scratch LAPA (Sthv2) Total Success Rate Total Success Rate (Strict) 43.87% 27.78% 50.09% 30.76% 35.19% 12.96% 36.83% 22.22% 32.61% 21.22% 34.02% 16.67% 7.41% 20.37%"
        },
        {
            "title": "Preprint",
            "content": "Table 17: Bi-manual Task Results (Seen and Unseen) OpenVLA (Open-X) LAPA (Open-X) Seen Object Combinations purple, blue cup gray, cupcake gray, apple purple, milk gray, orange juice purple, green cup SUM purple, orange purple, red block purple, cupcake gray, peach gray, mustard gray, spam SUM 0.25 0.25 0 0.25 0.5 0.25 1.5 Unseen Object Combinations 0.75 0.25 0.25 0.25 0 0.25 1. Unseen Objects purple, pear purple, mayonnaise purple, yellow cup plain white, tomato soup plain white, wooden block plain white, cheez it SUM 0.25 0.75 0.5 0.25 0.25 0 2 Unseen Instructions darker, fruit darker, sweet dessert darker, drink lighter, rectangular object lighter, roundest object lighter, red object SUM Success Rate 0.25 0.5 0.25 0 0 0 1 0.25 0.75 0 0.75 0.25 0. 2.25 0 0 0 0.5 0 0.5 1 0.5 0.25 0.5 0.75 0.25 0 2.25 0.5 0 0.5 0.25 0.5 1.75 26.04% 30.21%"
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "KAIST",
        "Microsoft Research",
        "NVIDIA",
        "University of Washington"
    ]
}