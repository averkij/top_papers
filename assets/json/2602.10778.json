{
    "paper_title": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation",
    "authors": [
        "Maximilian Thang",
        "Lichao Wu",
        "Sasha Behrouzi",
        "Mohamadreza Rostami",
        "Jona te Lintelo",
        "Stjepan Picek",
        "Ahmad-Reza Sadeghi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control. We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality."
        },
        {
            "title": "Start",
            "content": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation 6 2 0 2 1 1 ] . [ 1 8 7 7 0 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Jona te Lintelo\nRadboud University",
            "content": "Stjepan Picek University of Zagreb & Radboud University Ahmad-Reza Sadeghi Technical University of Darmstadt"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control. We present GoodVibe, neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to small subset of neurons. We identify these neurons using gradient-based attribution from supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to 2.5 improvement over base models, matching or exceeding full fine-tuning with over 4 700 fewer trainable parameters, and reducing training computation by more than 3.6 compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have become integral to modern software development, supporting code completion, program synthesis, and automated refactoring [1]. Recent codespecialized models achieve impressive functional correctness and productivity gains [2, 3], and are increasingly adopted in both professional and casual development workflows [4]. This informal, exploratory usage, often referred to as vibe coding, prioritizes speed and convenience over careful design or review, and typically proceeds without explicit security considerations [5, 6]. In such settings, insecure code can be generated, copied, and reused with minimal audit. As result, security vulnerabilities introduced by LLM-generated code have emerged as serious concern. Prior work shows that even highly capable models frequently produce insecure patterns, including injection vulnerabilities, hallucinated dependencies, and unsafe memory operations [7, 8]. When incorporated into production systems, these issues can be directly exploitable and may propagate through reused components and dependencies, amplifying risk across the software supply chain. Secure LLM-generated Code: An Unsolved Problem. natural mitigation strategy is to fine-tune LLMs on securityfocused datasets. However, full-parameter fine-tuning is computationally expensive and often fragile, with security gains frequently accompanied by catastrophic forgetting or degraded general coding performance [911]. Parameterefficient methods such as LoRA [12] reduce training cost but operate at coarse granularity, offering limited interpretability or control over how security behavior is encoded [12, 13]. In practice, this makes it difficult to reason about robustness or ensure that security improvements generalize beyond the fine-tuning data. Prompt engineering is another common approach, explicitly instructing models to write secure code. While useful in controlled scenarios, this strategy is inherently brittle: it relies on consistent user behavior, is easily omitted during rapid or exploratory development, and provides no guarantees when prompts are underspecified. In the prevalent vibe-coding workflow, where prompts are short and securityagnostic, prompt-based enforcement offers little protection. Indeed, improving security behavior by default, without sacrificing core coding capabilities or relying on explicit security prompts, remains an open challenge. Our Goal and Contribution. In this work, we introduce GoodVibe, neuron-level security optimization framework 1 for code LLMs. Our central insight is that security-relevant reasoning is not uniformly distributed across model parameters, but concentrated in small subset of neurons. Exploiting this structure enables targeted security optimization without the drawbacks of existing fine-tuning paradigms. At high level, GoodVibe proceeds in two stages. First, it identifies security neurons, neurons that play dominant role in security-related reasoning. This identification is non-trivial: individual neurons do not carry explicit semantic labels, and security-relevant behavior is not directly observable from activations alone. GoodVibe addresses this challenge by recasting security neuron identification as supervised security evaluation problem. By prompting the model to distinguish between secure and insecure code and analyzing neuron-level gradient magnitudes from single backward pass, we infer which neurons exert strong influence on security-related decisions. These neurons define security-critical subspace that captures the models internal representation of secure coding behavior. Second, the model undergoes neuron-based optimization that is restricted to this security-critical subspace while all other parameters are frozen. To further improve efficiency and stability, GoodVibe leverages neuron activation behavior to cluster security neurons, allowing groups of neurons with similar functional roles to share update directions during training. This design substantially reduces the effective number of trainable parameters while preserving the models capacity to adapt security-relevant representations. Across multiple open-source models and security-critical programming languages, GoodVibe significantly improves the security of generated code compared with other fine-tuning methods, in the meantime largely preserving functional correctness and syntactic validity. Our results point toward more interpretable, controllable, and robust approaches to securing LLM-based code generation, especially in security-agnostic, real-world workflows. Our contributions are as follows: We introduce new approach to securing code LLMs by showing that security-relevant reasoning is structurally localized within small subset of neurons, rather than being uniformly distributed across model parameters. We propose principled, gradient-based method to identify security-critical neurons by recasting security analysis as supervised evaluation task, enabling reliable neuron-level attribution without requiring explicit semantic labeling. We introduce novel cluster-based fine-tuning mechanism. Security neurons are grouped with similar functional roles and updated within clusters. In this way, we significantly reduce the number of effective trainable parameters without compromising security adaptation capability. We demonstrate that GoodVibe significantly improves code security across multiple models and programming languages. On C++ and Java, it achieves up to 2.5 increase in secure code generation over base models, while matching or exceeding full fine-tuning using over 4 700 fewer trainable parameters. Moreover, GoodVibe consistently outperforms parameter-efficient baselines such as LoRA across all evaluated languages, including C++, Java, Swift, and Go, while incurring substantially lower computational cost. The rest of this paper is organized as follows: Section 2 reviews background concepts and related work on code LLMs and fine-tuning techniques. Section 3 introduces the GoodVibe framework and details its neuron-level design. Section 4 presents implementation details, while Section 5 evaluates GoodVibe through case studies and experiments. Section 6 provides ablation analysis, and Section 7 discusses the critical aspects of GoodVibe. Sections 8 and 9 provide related works and conclusion, respectively."
        },
        {
            "title": "2.1 Code Large Language Model",
            "content": "Code LLMs are typically instantiated as transformer-based autoregressive models trained to predict the next token in sequence of code tokens. Given an input sequence = (x1, . . . , xn), the model learns conditional distribution p(xn+1 xn) over vocabulary that includes programming language keywords, identifiers, operators, and literals. Pretraining is performed on large-scale code corpora spanning multiple programming languages and development contexts [14]. Architecturally, code LLM, identical to conventional LLMs, consists of stack of transformer layers, each composed of multi-head self-attention module followed by position-wise feed-forward network (FFN). The FFN in each layer applies non-linear transformation to intermediate hidden states and is commonly viewed as collection of neurons, each corresponding to dimension in the hidden representation. During forward propagation, these neurons encode intermediate representations of syntactic structure, semantic relationships, and higher-level programming patterns. While code LLMs demonstrate strong performance in terms of functional correctness and generalization across coding tasks, they do not explicitly model security properties during pre-training. As result, security-relevant reasoning, such as identifying unsafe data flows or vulnerable API usage, emerges implicitly from training data rather than being directly supervised. This limitation becomes particularly critical in informal development workflows, such as vibe coding, where prompts emphasize rapid functionality and security considerations are rarely stated. In such settings, the models implicit and weakly encoded security knowledge is often insufficient to prevent insecure defaults. Understanding how internal model components represent and influence security2 related behavior is, therefore, essential for enabling targeted security adaptation that improves default generation quality without relying on explicit security prompts."
        },
        {
            "title": "2.2 Parameter Efficient Fine-Tuning",
            "content": "Fine-tuning is standard approach for adapting pre-trained language models to downstream tasks, including securityrelated objectives such as vulnerability detection or secure code generation. In full-parameter fine-tuning, all model parameters are updated during training. While effective, this approach is computationally expensive and prone to catastrophic forgetting, especially when the downstream task distribution differs from pre-training data. Parameter-Efficient Fine-Tuning (PEFT) methods seek to reduce this cost by restricting optimization to subset of parameters while freezing the pre-trained backbone. Common PEFT approaches include adapter-based methods, which insert lightweight trainable modules between transformer layers; prefix or prompt tuning, which prepends trainable embeddings to model inputs; and low-rank adaptation techniques that constrain parameter updates to low-dimensional subspaces [12, 15]. From an optimization perspective, PEFT methods modify model behavior by introducing or updating parameters at the level of layers or submodules. These approaches have been shown to retain much of the pretrained models general capability while improving taskspecific performance [15, 16]. However, since PEFT operates at coarse granularity, it provides limited visibility into how task-relevant knowledge is encoded within the models internal representations, particularly at individual neurons."
        },
        {
            "title": "3.1 Threat Model",
            "content": "Goal and Assumptions. Our goal is to improve the intrinsic security awareness of code language models under normal, non-adversarial usage. We focus on the common scenario in which developer relies on code LLM for everyday programming tasks, such as code completion, rapid prototyping, or exploratory vibe coding, where prompts primarily emphasize functionality and development speed, and security requirements are implicit, underspecified, or absent. This setting reflects how code LLMs are typically used in real-world development workflows. We assume the model is deployed as-is to the user after training. While the pretrained model may originate from third party and could have been trained on imperfect or insecure data, we do not assume that an adversary has direct access to modify the model parameters during deployment. After deployment, the adversarys interaction with the model is limited to standard prompting, just like any other user. Threat Addressed. The threat we address arises when model operating under benign, security-agnostic prompts generates code that is functionally correct but insecure. Common examples include missing input validation when handling external data, unsafe memory management in low-level code, incomplete or incorrect authentication logic, and insecure usage of APIs that require additional safeguards. These vulnerabilities are not the result of malicious prompts or intentional misuse. Instead, they stem from the models learned coding behavior during pretraining on large-scale code corpora, which contain substantial amount of insecure, incomplete, or context-dependent code. Furthermore, since code generated by LLMs is often copied, adapted, and integrated into larger codebases with minimal modification, insecure defaults can easily propagate downstream. Our objective is therefore to raise the baseline security quality of generated code, making secure behavior more likely without explicit request. Out-of-Scope Scenarios. We do not aim to defend against active attacks on the model, such as adversarial prompting, jailbreak attempts, or the intentional generation of insecure or malicious code. These scenarios fall under the broader domain of safety alignment and misuse prevention and are orthogonal to the focus of this work. Likewise, we do not consider prompt engineering techniques that explicitly enforce security (e.g., adding instructions such as write secure code or avoid vulnerabilities) as part of our threat model. While such techniques can improve security outcomes, they rely on user awareness and careful prompt design and do not address the models underlying tendency to default to insecure patterns, particularly in fast, informal development workflows such as vibe coding. Finally, our approach complements defenses against adversarial misuse. Rather than preventing worst-case behavior under hostile conditions, we aim to improve the models default behavior under normal usage, ensuring that security-aware code generation becomes the norm rather than an exception."
        },
        {
            "title": "3.2 Design Intuition",
            "content": "GoodVibe is motivated by an analogy to how humans regulate behavior. Humans possess broad, transferable knowledge, but safe behavior is enforced through learned constraints, such as safety norms or professional standards, that guide how this knowledge is applied. These constraints do not replace general capability; they act as internal control mechanisms. We argue that similar structure exists in code language models. pretrained code LLM already knows how to generate syntactically correct and functionally effective code, including for security-sensitive tasks. In this work, we refer to secure code as implementations that follow common security best practices (e.g., validating untrusted inputs, using APIs safely, and avoiding unsafe memory operations), and insecure code as functionally correct implementations that omit such safeguards. The core issue is therefore not insufficient capability, 3 Figure 1: An Overview of the Security Neuron Identification Pipeline. but insufficient security awareness: when security requirements are implicit or absent, the model defaults to common but potentially insecure coding patterns. In fact, improving security in this setting does not require relearning how to code. Instead, it requires reinforcing internal control mechanisms that bias generation toward secure practices. Retraining the entire model is thus unnecessary and potentially harmful. GoodVibe aims to identify and selectively strengthen small set of such security control mechanisms. Since such control mechanisms are not explicitly labeled within the model, as discussed in Section 3.3, GoodVibe infers it indirectly by observing how the model behaves when asked to judge code security. Neurons that exert strong influence on security-related decisions are treated as the models internal security controls. By selectively adapting these neurons, GoodVibe increases the likelihood that security considerations are applied during code generation, even when the user does not explicitly request secure code. Furthermore, in large transformer models, individual neurons typically capture lowlevel features or partial signals, while higher-level behaviors are distributed across multiple neurons [17]. Following this, we argue that the security control logic emerges from groups of neurons that respond similarly to security-relevant patterns. As detailed in Section 3.4, GoodVibe captures this structure by jointly adapting neurons with correlated behavior, enabling the model to efficiently and robustly internalize security constraints. In doing so, GoodVibe improves security behavior while preserving the models overall coding capabilities."
        },
        {
            "title": "3.3 Security Neuron Identification",
            "content": "The first step in GoodVibe is to identify the neurons responsible for security-related reasoning. Our goal is to identify neurons whose internal representations exert strong influence on the models security-related decisions. Figure 1 presents an overview of the security neuron identification steps. Formally, let fθ denote pre-trained code language model with parameters θ, and let = {(xn, yn)}N n=1 be dataset of code snippets xn with corresponding security labels yn {0, 1}, indicating whether the snippet is secure or insecure. We define security evaluation task using security evaluation loss function L( fθ(x), y), which measures the models error when predicting the security label of code snippet. Intuitively, identifying security neurons is equivalent to finding those neurons that strongly influence the security loss. To quantify the contribution of individual neurons, GoodVibe leverages gradient-based attribution. Gradients capture the sensitivity of the security loss to changes in model parameters and therefore provide principled signal for identifying neurons that causally influence security-related predictions. Consider transformer layer with hidden dimension dl. Each neuron corresponds to one dimension of the layers intermediate representation and is associated with subset of parameters θl,i, where {1, . . . , dl}. We define the importance score of neuron in layer as the expected magnitude of the gradient of the security loss with respect to its parameters: (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) L( fθ(x), y) θl,i Il,i = E(x,y)D (cid:12) (cid:21) (cid:12) (cid:12) (cid:12) (1) . In practice, this expectation is approximated by averaging gradient magnitudes across all samples in the dataset. For each layer l, we select the top-k neurons with the highest importance scores: Sl = TopK(Il,, k). (2) We perform neuron selection independently for each layer rather than globally across the entire model, which prevents neurons from small number of layers from dominating the security-critical subspace due to scale differences in gradient magnitudes. More importantly, it reflects the structure of secure code reasoning, which spans multiple levels of abstraction: low-level operations such as memory handling or input sanitization are more-likely captured in earlier layers, while higher-level security logic, including API usage patterns and control-flow decisions, emerges in later layers. Finally, the union of selected neurons across all layers defines the security-critical neuron subspace: = (cid:91) l=1 Sl. (3) 4 Figure 2: An Overview of the Cluster-based Security Optimization Pipeline. This subspace serves as the target for neuron-selective optimization in subsequent stages of GoodVibe."
        },
        {
            "title": "3.4 Cluster-based Security Optimization",
            "content": "After identifying the security-critical neuron subspace, GoodVibe optimizes these security structures to improve the models security behavior while preserving its generalization capability. An overview of the GoodVibe security optimization pipeline is shown in Figure 2. The key idea is to restrict parameter updates to neurons directly involved in securityrelated reasoning and to freeze all remaining parameters. Formally, given pre-trained model fθ, the full parameter set θ can be partitioned as: θ = θsec θbase, (4) where θsec denotes parameters associated with the security neurons identified in Section 3.3, and θbase contains all remaining parameters. Given security supervision dataset with prompt-secure code pairs and loss function L, neuron-selective optimization seeks to solve constrained optimization problem: E(x,y)D (cid:2)L( fθbase,θsec(x), y)(cid:3) , min θsec s.t. θbase is frozen. (5) This formulation ensures that only security-relevant parameters are updated, while pre-trained representations for syntax, semantics, and general programming logic remain unchanged. In transformer architectures, neurons correspond to dimensions of intermediate hidden representations and are implemented as rows of linear projection matrices in attention and feed-forward layers. Let Wl Rdoutdin denote weight matrix in layer l. Neuron-selective optimization restricts updates to subset of output rows indexed by Sl, which is the set of security neurons for that layer. The weight matrix is decomposed as: Wl = base + Wl, (6) 5 where base is the frozen pre-trained weight matrix, and Wl has nonzero rows only for indices in Sl. All other rows of Wl are constrained to zero, ensuring that updates are localized to security-relevant neurons. Intuitively, one could update these neurons individually. However, it could introduce non-trivial number of trainable parameters and may ignore correlations among neurons with similar functional roles, experimentally verified in Section 6.1. To address this, GoodVibe introduces structured optimization strategy based on neuron clustering. Let Sl be partitioned into Cl disjoint clusters: Sl = Cl (cid:91) c= Cl,c, Cl,c Cl,c = /0 for = c. (7) All neurons within the same cluster share common update direction. Specifically, instead of learning an independent update for each neuron, GoodVibe learns cluster-level update matrix Ul RCl din , where each row corresponds to cluster. For neuron Cl,c, its update is given by the c-th row of Ul. The resulting update matrix Wl is constructed by assigning cluster-level updates to the corresponding neuron rows and zero elsewhere. This yields parameterization whose number of trainable parameters scales with the number of clusters rather than the number of security neurons. From higher level, cluster-based optimization enforces low-dimensional structure on the security-critical subspace. By sharing updated directions among correlated neurons, GoodVibe reduces optimization complexity, improves training stability, and limits overfitting to the security supervision data. At the same time, freezing all non-security parameters ensures that the models general capability is preserved. Indeed, this design distinguishes GoodVibe from existing parameter-efficient fine-tuning methods, which typically operate at the layer or module level. In contrast, GoodVibe constrains optimization at the granularity of individual neurons and imposes structure only within the security-relevant subspace. This design advantage is empirically studied in Section 5.2 and 5.3."
        },
        {
            "title": "4.1 Neuron Importance Estimation",
            "content": "Security neuron identification is implemented as supervised security classification task. Given code snippet, the model is prompted to output binary decision indicating whether the code is secure or insecure. To avoid introducing additional classification heads, the task is formulated using the models native token prediction mechanism. Specifically, the model is prompted such that the final token corresponds to binary choice. From the models output logits RT , we extract the logits at the final token position and restrict them to the vocabulary entries corresponding to the ASCII tokens 0 and 1. This yields binary logit vector ˆz R2, which is optimized using standard cross-entropy loss against the ground-truth security label. This formulation ensures that gradient signals reflect security-related reasoning without introducing additional trainable components. To compute neuron-level importance scores, we attach backward hooks to all linear layers in the transformer, including attention projection layers and feed-forward layers. During backpropagation, each hook receives the gradient of the loss with respect to the layer output. For given layer, the output gradient tensor has the shape RBT d, where is the batch size, is the sequence length, and is the hidden dimension. To obtain single importance contribution per neuron, we flatten the batch and sequence dimensions and compute the mean absolute gradient for each hidden dimension: ="
        },
        {
            "title": "1\nB · T",
            "content": "BT t=1 Gt,: Rd. (8) Eq. 8 is repeated for each training sample, and per-sample contributions are accumulated in CPU memory to reduce GPU overhead. Next, gradient importance values are averaged across all samples. This aggregation reduces variance in neuron selection across samples. Finally, for each layer, the top-k neurons with the highest importance scores are selected as security neurons. The resulting indices and importance values are serialized to disk for reuse."
        },
        {
            "title": "4.2 Cluster-based Fine-tuning",
            "content": "After security neurons are identified, cluster-based fine-tuning is performed by freezing all model parameters and selectively reintroducing trainable parameters at the level of individual neurons. Concretely, clustering is performed independently for each linear layer. For given layer, each selected neuron is represented by feature vector derived from its gradient importance profile. These vectors are clustered using k-means, with denoting the number of clusters. We visualize the neuron clusters in Appendix A. Note that automatically clustering neurons can result in degenerate solutions, such as assigning all neurons to single cluster. To prevent this, clustering is guided by silhouette scoring, and clustering is skipped when the score falls below threshold. This fallback ensures meaningful updates and stable training behavior. The influence of the threshold value is studied in Section 6.2. For each linear layer with weight matrix Wl Rdoutdin, only the rows corresponding to security neurons are made trainable. Bias parameters are kept frozen throughout training. This is implemented by decomposing each weight matrix into frozen base component and trainable update component, where updates are constrained to security neuron rows. Finetuning is performed using supervised training on secure code samples. We use the AdamW optimizer with learning rate of 1 104, cosine learning rate scheduling, and warmup ratio of 0.1. Training is conducted for two epochs; hyperparameter study on the training epochs is shown in Section 6.2. To reduce memory usage, mixed-precision training (BF16) and gradient checkpointing are enabled. Gradient accumulation is used to simulate larger batch sizes while maintaining per-device batch size of one. Throughout the training, we record statistics including the number of trainable parameters, the number of clusters per layer, and estimated floating-point operations. After training, clustered parameterizations are folded back into standard weight tensors, and the resulting model checkpoint is saved for inference and evaluation."
        },
        {
            "title": "4.3 Security Evaluation and Metrics",
            "content": "We evaluate GoodVibe along three dimensions: security effectiveness, efficiency, and utility preservation. Security effectiveness. Security effectiveness is the primary metric used to assess whether model generates secure code by default. It is measured as the proportion of generated code samples that are classified as secure by an independently trained judge model, referred to as the safe response rate. We report this metric across programming languages and model families to evaluate robustness and generalization. In this work, the code security is evaluated using an automated judge model fine-tuned on Qwen3-0.6B, small language model that balances between semantic code understanding and computational efficiency [18]. Models in the Qwen family are trained on large-scale technical and programming-centric corpora, making them well-suited for identifying vulnerability-related patterns in source code. Following the emerging LLM-as-a-judge paradigm [19], Qwen30.6B demonstrates reliable instruction adherence and stable binary classification behavior, which is critical for structured security evaluation at scale. Specifically, the judge model is prompted using fixed instruction template: 6 You are an AI that returns only single number: 1 or 0. Return 1 if the given code snippet is unsafe or contains vulnerabilities. Return 0 if the code is safe. Respond with only the number 1 or 0. No other text, symbols, or explanations. Under this prompt, each generated code sample is labeled as either safe (0) or unsafe (1). The judge model is trained independently and is never exposed to outputs from GoodVibe or any baseline models during training. detailed discussion of its reliability is provided in Section 7. Efficiency. Efficiency is assessed by measuring both the number of trainable parameters introduced during fine-tuning and the associated computational cost. Specifically, we report the total number of trainable parameters and the overall finetuning cost in terms of floating-point operations (FLOPs). We additionally report FLOPs per second to capture differences in computational intensity across fine-tuning methods. Utility. To assess whether security optimization degrades general model capabilities, we evaluate utility preservation using standard reasoning and language understanding benchmarks. We report accuracy on ARC Challenge [20], which tests nontrivial scientific reasoning; GSM8K [21], benchmark of linguistically diverse grade-school math word problems; and MMLU [22], large multitask benchmark spanning multiple domains of knowledge. Performance on these benchmarks serves as proxy for the models general reasoning and language modeling capability after security optimization."
        },
        {
            "title": "5.1 Evaluation Setup",
            "content": "Target Models. We evaluate GoodVibe on six recent opensource large language models released by three major developers: Meta, Alibaba, and Google. Specifically, we include both code-specialized models (CodeLlama [14]) and generalpurpose instruction-tuned models (Llama3 [23], Qwen3 [18], and Gemma3 [24] families). All models are transformer-based and are used in their original pre-trained checkpoint. Evaluation Dataset. For security neuron identification and fine-tuning, we use the CyberNative Code Vulnerability and Security Dataset [25]. The dataset consists of aligned pairs of secure and insecure code snippets corresponding to the same natural-language prompt, making it well-suited for the security neuron identification. Experiments are primarily conducted on two programming languages, C++ and Java, as they represent distinct and security-critical segments of the software ecosystem. C++ is widely used in low-level and performance-critical systems, including operating systems, system libraries, and embedded software, where memory safety vulnerabilities such as buffer overflows and useafter-free errors are common and can have severe consequences [26, 27]. Java, in contrast, is extensively used in large-scale application development and the Android ecosystem, where security issues often arise from improper input validation, insecure API usage, and flawed authentication or authorization logic [28, 29]. Furthermore, to assess the generalization of GoodVibe, we extend the evaluation to Swift and Go, detailed in Section 5.3. For each language, we use all (424) secureinsecure code pairs from the dataset, resulting in balanced dataset with clear security contrasts. These samples are used both for security neuron identification and for supervised fine-tuning. To evaluate security behavior, we prompt each fine-tuned model with held-out (i.e, separate) coding prompts drawn from the dataset, ensuring that evaluation measures generalization rather than memorization. Training Configuration. Safety neurons are identified using the gradient-based method described in Section 3.3. During fine-tuning, only parameters associated with identified safety neurons are updated, while all other model parameters remain frozen. All models are fine-tuned for two epochs using the AdamW optimizer with learning rate of 1 104. Experiments are conducted on four NVIDIA Quadro RTX 8000 GPUs. Unless otherwise stated, all models use the same optimizer configuration, number of training epochs, and data splits. The top-k number for safety neuron count per layer is by default 50, and the silhouette score used for clustering is 0.05. Hyperparameter studies on the training epoch, top-k number, and silhouette score are presented in Section 6.2."
        },
        {
            "title": "5.2 Performance Benchmark",
            "content": "In this section, we first examine security performance across different models and programming languages, then relate these gains to the number of trainable parameters and the overall computational cost. We compare GoodVibe against three settings: the pre-trained model without fine-tuning, full fine-tuning, and LoRA. Security Performance. Table 1 reports the security rate of generated code on C++ and Java across models. Pretrained models exhibit widely varying and often weak security performance, particularly on C++. For example, CodeLlama-7B achieves only 6.1% safe responses on C++, and Meta-Llama3-8B reaches 12.0%, indicating that secure code generation does not reliably emerge from pre-training alone. Even newer models, such as Qwen3-14B, achieve only 29.3% safe responses on C++ without adaptation, showing that scale and recency alone do not guarantee security-aware behavior. All fine-tuning methods substantially improve security performance. In some settings, full fine-tuning and LoRA even exceed 90% of generated code being secure, but their performance varies across models and languages. In particular, full fine-tuning performs strongly on Java for some models while showing degraded or inconsistent results on C++, likely due to indiscriminate parameter updates interfering with lowlevel reasoning required for memory-safe code generation. 7 Developer Target Models C++ Java Baseline Full FT LoRA GoodVibe Baseline Full FT LoRA GoodVibe Meta Alibaba Google CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b gemma-3-4b-it gemma-3-12b-it Average 6.1% 12.0% 39.4% 29.3% 23.6% 86.3% 35.1% 51,7% 96.5% 67.7% 95.8% 97.6% 99.1% 52.6% 93.4% 94.0% 89.2% 90.3% 96.9% 86.3% 87.4% 86.6% 85.9% 85.1% 90.3% 89.9% 87.7% 87.5% 50.9% 48.6% 68.9% 65.1% 55.9% 54.0% 59.3% 73.6% 83.3% 72.8% 89.9% 94.3% 99.3% 63.7% 74.8% 86.8% 82.8% 71.5% 71.7% 83.0% 75.3% 71.7% 74.3% 80.4% 86.8% 91.0% 61.3% 76.0% Release time 2023.08 2024.12 2025.04 2025.04 2025.03 2025.03 Table 1: Code Security Benchmark. In contrast, GoodVibe achieves consistently strong security performance across both languages while modifying only small subset of parameters. Averaged across models, GoodVibe reaches 87.5% safe responses on C++ and 76.0% on Java, outperforming LoRA and approaching (or even better than) full fine-tuning. For instance, when testing CodeLlama7B on C++, GoodVibe improves the code security rate from 6.1% to 86.6%, outperforming both counterparts. To better illustrate the security gain introduced by GoodVibe, as an example, we instruct the baseline and GoodVibehardened model with the same prompt. More code examples are provided in Appendix B. Write C++ code that includes iostream and string.h libraries. Define function named copyString that takes two character pointers as parameters. This function should use the strcpy function from the string.h library to copy the content of the source string into the destination string. In the main function, declare character array buffer of size 10. Declare another character array largeString and initialize it with long string that will cause buffer overflow. Call the copyString function with buffer and largeString as arguments. Print the content of the buffer using cout. The general goal is to copy string from one buffer to another. No security is enforced to mimic vibe-coding scenario. The generated code by the two models is shown in Listing 1 and Listing 2. As expected, GoodVibe incorporates size checks that mitigate potential buffer-overflow vulnerabilities, while the base models output lacks such checks and may lead to buffer overflow. These results suggest that selectively optimizing security-critical neurons enables robust code security improvements. 1 void copyString ( char * str ) { char buffer [10]; strcpy ( buffer , str ); 2 3 4 } 5 6 int main () { char largeStr [20] = \" This is large string \"; copyString ( largeStr ); return 0; 7 8 9 10 } 1 void copyString ( char * dest , const char * src , (cid:44) size_t destSize ) { strncpy ( dest , src , destSize - 1) ; dest [ destSize - 1] = 0 ; 2 3 4 } 5 6 int main () { constexpr size_t bufferSize = 10; char buffer [ bufferSize ]; const char * largeString = \" This is large (cid:44) string \"; copyString ( buffer , largeString , bufferSize ); std :: cout << \" Copied string : \" << buffer << (cid:44) std :: endl ; return 0; 7 9 10 11 12 13 } Listing 2: Code Generated by GoodVibe-hardened Model. Trainable Parameter Efficiency. To understand the cost of the security gains, Table 2 compares the number of trainable parameters required by each method. Full fine-tuning updates billions of parameters, ranging from 4.3 billion for Gemma3-4B to over 14.7 billion for Qwen3-14B. LoRA reduces this cost significantly, but still requires several million trainable parameters per model, with an average of 6.2 million. By contrast, GoodVibe consistently requires fewer than 3 million trainable parameters across all evaluated models, with an average of 1.9 million, less than 0.03% of the total parameters for billion-scale models. When considered alongside the security results above, this highlights key finding: the security gains achieved by GoodVibe are not driven by broad parameter updates, but by targeted adaptation within small, security-critical subspace. Target Model GoodVibe LoRA Full Fine-tuning CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b gemma-3-4b gemma-3-12b"
        },
        {
            "title": "Average",
            "content": "1.7 1.7 1.9 2.6 1.1 2.4 1.9 4.9 5.2 5.5 8.0 5.2 8.6 6.2 6 738.5 8 030.3 8 190.7 14 768.3 4 300.1 12 187.3 9 035. Table 2: Benchmark on Trainable Parameters (millions). Listing 1: Code Generated by Baseline model. Computation Cost. Finally, Table 3 reports the computa8 tional cost of fine-tuning in terms of FLOPs, measured on CodeLlama-7B-Instruct-hf as representative model; we observe similar trends across other models. Full fine-tuning requires 4.5 PFLOPs, reflecting the cost of updating all model parameters within the existing computation graph. Despite being parameter-efficient, LoRA incurs substantially higher total computation at 8.6 PFLOPs. This increase stems from the additional low-rank projections introduced at each adapted layer, which add extra matrix multiplications to both the forward and backward passes, thus increasing arithmetic workloads. Fine-tuning Method Total FLOPS FLOPS per Second LoRA Full fine-tune GoodVibe 8.6 PFLOPs 4.5 PFLOPs 2.4 PFLOPs 6.4 TFLOPs 4.4 TFLOPs 1.0 TFLOPs Table 3: FLOPS Benchmark on CodeLlama-7b-Instruct-hf. In contrast, GoodVibe requires only 2.4 PFLOPs, corresponding to an approximate 46% reduction compared to full finetuning and over 70% reduction compared to LoRA. The lower FLOPs-per-second requirement further indicates reduced computational intensity during training. Importantly, these savings are not merely theoretical: when combined with the dramatic reduction in trainable parameters, they translate into lower memory usage, faster training, and reduced hardware requirements. This efficiency makes GoodVibe feasible on modest compute budgets and enables rapid iteration or deployment across multiple models. Taken together, the results present consistent picture: GoodVibe achieves security performance comparable to full finetuning and LoRA, while requiring orders of magnitude fewer trainable parameters and significantly lower computational cost. This combination of effectiveness and efficiency underscores the practical value of neuron-level security optimization for secure-by-default code generation, offering practical pathway to improving code security in real-world settings."
        },
        {
            "title": "5.3 Protecting More Programming Languages",
            "content": "To further examine the generality of GoodVibe beyond C++ and Java, we conduct an additional study on Swift and Go, two widely used modern programming languages with distinct design goals and security profiles. Swift is commonly used in the Apple ecosystem for application development, where security issues often arise from improper input handling and API misuse, while Go is widely adopted in cloud services and backend infrastructure, where concurrency errors and improper error handling are frequent sources of vulnerabilities. Our goal is to assess whether GoodVibe generalizes to programming languages with different design philosophies, standard libraries, and common vulnerability patterns. In this setting, we focus on comparisons between GoodVibe and LoRA. Full fine-tuning is omitted here to maintain fair comparison between lightweight adaptation methods and to keep computational cost comparable across approaches. Target Models Swift Go GoodVibe LoRA GoodVibe LoRA CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b gemma-3-4b-it gemma-3-12b-it Average 45.3% 57.8% 41.0% 61.3% 62.7% 53.5% 53.6% 45.5% 61.8% 37.0% 47.9% 55.7% 63.4% 51.9% 45.8% 57.1% 52.1% 60.9% 51.7% 56.4% 54.0% 51.4% 49.8% 34.9% 47.4% 50.2% 59.7% 48.9% Table 4: Code Security Rate for Swift and Go. Table 4 reports the safe response rate for Swift and Go across six models. On average, pre-trained models achieve safe response rates of 27.3% on Swift and 40.1% on Go, indicating that secure code generation remains unreliable without targeted adaptation1. Both GoodVibe and LoRA substantially improve security performance across models, confirming that security-aware fine-tuning generalizes beyond the languages used in the main benchmark. Importantly, GoodVibe achieves consistently strong results and, on average, outperforms LoRA on both languages. For Swift, GoodVibe reaches an average safe response rate of 53.6%, compared to 51.9% for LoRA. On Go, GoodVibe achieves 54.0%, exceeding LoRAs 48.9%. While the performance of individual models varies, the overall trend indicates that GoodVibe generalizes robustly across languages, and outperforms low-rank adaptation consistently."
        },
        {
            "title": "5.4 Utility Analysis",
            "content": "Figure 3 reports model utility before and after GoodVibe finetuning on three standard reasoning benchmarks: GSM8K, ARC, and MMLU (introduced in Section 4.3). Overall, GoodVibe preserves model utility well across all benchmarks, with 0.84% utility drop on average. On GSM8K, most models exhibit small changes in accuracy after fine-tuning, with both modest improvements and minor degradations depending on the base model. similar pattern is observed on ARC. While some models (e.g., Qwen3-14) show moderate improvements, others exhibit slight decreases. No model experiences severe or systematic degradation, suggesting that GoodVibe does not disrupt the general scientific reasoning abilities encoded during pretraining. On MMLU, which is particularly sensitive to overfitting and distributional shifts, GoodVibe again maintains stable performance. Although some models incur small accuracy drops (e.g., Meta-Llama-3-8B), others remain nearly unchanged or show marginal improvements. Crucially, there is no consistent trend of utility collapse across benchmarks or model families. Taken together, these results demonstrate that GoodVibe achieves its security gains without sacrificing general reasoning and language modeling capability. Indeed, 1Due to page constraints, we report only the average baseline performance in the text and omit per-model baseline entries from the table. 9 (a) GSM8K (b) ARC (c) MMLU Figure 3: Utility evaluation before and after GoodVibe fine-tuning across the GSM8K, ARC, and MMLU benchmarks. Numeric labels denote the change in accuracy () between the fine-tuned and corresponding base models. by restricting updates to small security-critical subspace, GoodVibe avoids the widespread representation drift often observed with full-parameter fine-tuning."
        },
        {
            "title": "6.1 Ablation Study",
            "content": "To better understand which components of GoodVibe are critical to its performance, we conduct series of ablation studies focusing on (i) the method used to identify security neurons and (ii) the role of neuron clustering during finetuning. In all ablations, we fix other settings to ensure fair comparison. Security Neuron Identification. We first evaluate the importance of gradient-based neuron identification by comparing it against an activation-based baseline that is widely used in literature to identify critical neuron structures [30, 31]. Specifically, in the activation-based approach, we select neurons based on their activation magnitude during the security evaluation task, while keeping the number of selected neurons identical to the gradient-based method. Target Models C++ Java Activ. Gradient Activ. Gradient CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b gemma-3-4b-it gemma-3-12b-it"
        },
        {
            "title": "Average",
            "content": "8.7% 17.9% 43.4% 59.9% 81.6% 91.0% 50.0% 86.6% 85.9% 85.1% 90.3% 89.9% 87.7% 87.5% 38.2% 49.8% 72.9% 69.8% 69.6% 60.4% 60.8% 71.7% 74.3% 80.4% 86.8% 91.0% 61.3% 76.0% Table 5: Ablation Study on Neuron-Identification Methods. Table 5 reports the safe response rate achieved using the two identification strategies. Across all models and both programming languages, gradient-based identification consistently outperforms activation-based selection by large margin. On average, activation-based selection achieves only 50.0% safe responses on C++ and 60.8% on Java, whereas gradient-based identification achieves 87.5% and 76.0%, respectively. The gap is particularly pronounced for models with weak baseline security behavior. For example, on CodeLlama-7B, activation-based selection yields only 8.7% safe responses on C++, compared to 86.6% with gradient-based identification. Similar trends are observed for Meta-Llama-3-8B and Qwen3-8B. These results indicate that raw neuron activation is poor proxy for security relevance, as highly active neurons do not necessarily exert strong causal influence on security decisions. In contrast, gradient-based attribution directly captures the sensitivity of the security objective to individual neurons, making it more reliable mechanism for identifying security-critical internal structures. Neuron Clustering. Next, we examine the impact of neuron clustering during neuron-selective fine-tuning. We compare GoodVibe with clustering enabled against variant where each security neuron is optimized independently (i.e., clustering disabled). In both cases, the same set of security neurons is used; the only difference is whether neurons share update directions through clustering. Results are shown in Table 6. Disabling clustering can yield slightly higher security performance in some cases. For example, Meta-Llama-3-8B reaches 96.7% safe responses on C++ without clustering, compared to 85.9% with clustering. However, these gains come at the cost of substantially more trainable parameters (e.g., 57.1M vs. 1.8M for CodeLlama) and reduced training efficiency, as each security neuron is updated independently. Besides, this fine-grained neuron-level optimization increases sensitivity to gradient noise and datasetspecific artifacts. With limited security supervision, independent neuron updates can overfit to spurious correlations rather than stable security patterns, leading to poorer generalization. This effect is particularly pronounced on Java, where security issues involve higher-level logic; for instance, gemma-3-12B drops from 84.0% to 61.3% without clustering. In contrast, clustering enforces structured regularization and Target Models C++ Java Model 0.01 0. w/o w/ w/o w/ CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b gemma-3-4b-it gemma-3-12b-it 73.1% 96.7% 83.3% 91.8% 94.6% 95.3% 86.6% 85.9% 85.1% 90.3% 89.9% 87.7% 80.0% 73.8% 86.9% 83.7% 87.5% 84.0% 71.7% 74.3% 80.4% 86.8% 91.0% 61.3% Average 90.0% 87.5% 82.6% 76.0% Table 6: Ablation Study on Neuron Clustering. low-dimensional adaptation within the security-critical subspace. While this constraint can slightly reduce peak security performance in some settings, it yields more stable behavior across languages and models, and enables the substantial parameter and FLOPs reductions reported in Section 5.2. Importantly, even with clustering enabled, GoodVibe maintains security performance comparable to full fine-tuning and LoRA while operating under far stricter efficiency constraints. Together, these ablation studies demonstrate that both components of GoodVibe are necessary for achieving its overall effectivenessefficiency trade-off. Gradient-based neuron identification is critical for locating security-relevant internal structures, while neuron clustering plays key role in controlling optimization complexity and improving robustness. Removing either component degrades the methods practical utility, either by sharply reducing security performance or by undermining the efficiency gains that motivate neuron-level optimization in the first place."
        },
        {
            "title": "6.2 Hyperparameter Study",
            "content": "We study the sensitivity of GoodVibe to key hyperparameters that control the structure and strength of neuron-level adaptation. Specifically, we examine the impact of (i) the silhouette threshold used for neuron clustering, (ii) the number of selected security neurons per layer, and (iii) the number of fine-tuning epochs. Unless otherwise stated, all experiments follow the same setup. We present results using C++; Java shows similar results, thus leading to the same conclusion. Number of Clusters. As discussed in Section 4.2, neuron clustering is controlled by silhouette score threshold, which determines whether neurons are grouped into clusters or updated independently. lower threshold permits more aggressive clustering, reducing the number of effective trainable parameters, while higher threshold results in fewer clusters and more fine-grained updates. Table 7 reports the security performance obtained under three silhouette thresholds: 0.01, 0.05 (our setting), and 0.1. Across all evaluated models, threshold of 0.05 yields the best average performance (87.5%), outperforming both more aggressive clustering (0.01) and more conservative clustering (0.1). Indeed, at very low thresholds, excessive clustering forces heterogeneous neurons to share update directions, which can CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b Gemma-3-4b-it Gemma-3-12b-it 84.2% 91.5% 80.0% 81.1% 87.0% 86.6% 86.6% 85.9% 85.1% 90.3% 89.9% 87.7% 0.1 84.2 89.6 79.1 71.7 89.2 85. Average 85.1% 87.5% 83.2% Table 7: Hyperparameter Study on Cluster Numbers, Controlled by Silhouette Score Threshold. limit expressiveness and reduce security performance. Conversely, at higher thresholds, clustering becomes sparse, increasing the effective number of parameters and making optimization more sensitive to noise. The consistent peak at 0.05 across models indicates that moderate clustering balances between expressiveness and parameter efficiency. Number of Security Neurons. We next examine the influence of the number of selected security neurons per layer by varying the top-k threshold from 10, 50 (our setting), to 100. Increasing enlarges the security-critical subspace and increases the number of trainable parameters, while smaller values enforce more aggressive sparsity."
        },
        {
            "title": "Model",
            "content": "10 50 CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b Gemma-3-4b-it Gemma-3-12b-it 78.1% 85.4% 85.4% 82.1% 84.4% 81.8% 86.6% 85.9% 85.1% 90.3% 89.9% 87.7% 88.0 91.8 77.9 86.1 85.4 85,6 Average 82.9% 87.5% 85.8% Table 8: Hyperparameter Study on Security Neuron Numbers, Controlled by top-k Threshold. As shown in Table 8, selecting too few neurons (K=10) consistently degrades performance, with an average safe response rate of 82.9%. This suggests that security-relevant reasoning is distributed across multiple neurons rather than being concentrated in very small subset. Increasing to 50 significantly improves performance, achieving the highest average security rate of 87.5%. Further increasing to 100 does not yield consistent additional gains and, in some cases, reduces performance due to increased optimization complexity and potential overfitting. These results indicate that moderate neuron coverage is sufficient to capture security-relevant behavior. Training Epochs. Finally, we study the effect of the number of fine-tuning epochs on security performance. Table 9 reports results for training durations of 1, 2, and 3 epochs. Across all models, single epoch yields limited security improvement, with an average safe response rate of 60.5%, indicating that security adaptations do not fully propagate through the security-critical subspace with minimal training. Increasing training to two epochs consistently produces the best perfor11 mance, achieving an average of 87.5% secure code generation. Model 1 3 CodeLlama-7b-Instruct-hf Meta-Llama-3-8B-Instruct Qwen3-8b Qwen3-14b Gemma-3-4b-it Gemma-3-12b-it 50.0% 66.5% 50.5% 66.3% 70.5% 59.0% 86.6% 85.9% 85.1% 90.3% 89.9% 87.7% 89.2% 70.3% 83.4% 69.1% 68.6% 75.2% Average 60.5% 87.5% 76.0% Table 9: Hyperparameter Study on Training Epoch. Extending training to three epochs does not lead to further gains and often results in performance degradation for several models (e.g., Meta-Llama-3-8B and Qwen3-14B). This suggests that excessive optimization, even when restricted to small subset of parameters, can introduce overfitting or interfere with pretrained representations. Overall, these results support the design choice of GoodVibe to use small number of fine-tuning epochs, balancing effective security adaptation with stability and generalization. Overall, the hyperparameter study shows that GoodVibe is robust to reasonable variations in its key hyperparameters and that its performance peaks in narrow, interpretable regime. Moderate clustering, mid-sized security neuron set, and short fine-tuning schedules are sufficient to achieve strong security improvements, reinforcing the efficiency and practicality of neuron-level security optimization."
        },
        {
            "title": "7 Discussion",
            "content": "Prompt Engineering vs. Intrinsic Security Awareness. Prompt engineering is an effective tool for guiding model behavior, but it places the burden of security awareness on the user. In real-world workflows, developers often issue brief or informal prompts, especially during rapid prototyping, where security considerations may not be top-of-mind. In such settings, security outcomes depend heavily on user discipline and prompt consistency. GoodVibe addresses different layer of the problem by modifying the models internal behavior rather than its external interface. By reinforcing security-relevant reasoning inside the model, secure coding practices become more likely to emerge even in the absence of explicit prompting. This intrinsic security awareness complements prompt-based techniques: prompt engineering can further refine behavior when applied, but GoodVibe ensures more secure default when it is not. Reliability of Automated Judge Models. Evaluating the security of machine-generated code at scale is inherently challenging because many security properties depend on semantic intent, implicit assumptions, and missing context rather than explicit syntactic patterns. In our setting, generated code is often short, abstracted from larger program, or lacks sufficient surrounding context (e.g., data-flow origins, call sites, or invariants), which limits the reliability of conventional static and dynamic analysis tools. To enable systematic and reproducible comparison across models and fine-tuning strategies, we therefore rely on an automated judge model trained to assess security-relevant properties directly from source code. The judge model is best understood as consistent measurement instrument rather than an absolute oracle. It is trained independently from all generation models and applied uniformly across all experimental conditions, ensuring that relative comparisons reflect genuine differences in generation behavior. While individual predictions may be noisy, the stability of trends across models, languages, and ablation settings indicates that the evaluation captures meaningful changes in security-aware code generation. To further assess the reliability of this approach, we conducted an auxiliary experiment comparing base judge model against judge model fine-tuned via supervised fine-tuning (SFT) on security-labeled code. The fine-tuned judge exhibits substantially improved discrimination capability, most notably through large reduction in false positives. Averaged across C++ and Java, false positive rates decrease from over 50% with the base judge to under 16% after fine-tuning, while true positive rates remain high (95%). This result suggests that supervised adaptation significantly improves the judges precision without sacrificing sensitivity, reinforcing the validity of using trained LLM as security evaluator. On the other hand, conventional static and dynamic analysis tools are poorly suited to this evaluation setting. Static analyzers typically require project-level context and produce unreliable results on partial or abstract code snippets, while dynamic analysis depends on executable programs and concrete inputs that are impractical to construct at scale. In contrast, learned judge model can reason directly about high-level security properties, such as missing validation, unsafe memory usage, or flawed control logic, even when the code is incomplete or non-executable. Security Improvements in AI-Assisted Code Generation. The security improvements observed in this work reflect shift in how code LLMs internalize and apply security considerations during generation. Rather than treating security as an external constraint imposed through prompts or postprocessing, GoodVibe strengthens secure coding behavior as part of the models internal decision-making. As result, generated code more frequently incorporates input validation, safer API usage, and conservative control-flow patterns, even when prompts emphasize functionality alone. This distinction is particularly important in practical development workflows such as vibe coding, where developers rely on rapid, informal interactions with LLMs and rarely specify security requirements explicitly. In these settings, code generated by language models is often reused, adapted, or integrated into larger codebases with minimal modification [32]. Improving the security of these initial outputs, therefore, reduces the 12 prompts emphasize functionality rather than security, such external controls are often absent or inconsistently applied. Beyond prompting and weight updates, small number of works have begun to explore representation-level interventions. For instance, SVEN [9] manipulates internal activations to influence model outputs without modifying parameters. Although promising, these approaches typically lack explicit mechanisms for identifying which internal components are responsible for security behavior, and they often exhibit higher variance compared to fine-tuning-based methods. In summary, prior work has explored full fine-tuning, PEFT, prompting, in-context learning, and representation manipulation as largely separate strategies, each with distinct trade-offs in cost, flexibility, and reliability. However, none explicitly combine mechanistic analysis of internal model components with selective optimization targeted at security-relevant structures. GoodVibe fills this gap by integrating gradient-based identification of security-critical internal components with highly targeted adaptation. This enables security improvements that are robust under benign, security-agnostic usage, including vibe coding, while remaining efficient, interpretable, and minimally disruptive to general coding capability."
        },
        {
            "title": "9 Conclusions and Future Work",
            "content": "This paper introduces GoodVibe, fine-grained security optimization pipeline for code LLMs that improves the security of generated code while substantially reducing fine-tuning cost. By identifying security-critical internal components via gradient-based attribution and selectively adapting only this subspace with structured clustering, GoodVibe strengthens security-aware reasoning without disrupting general coding capabilities. Extensive experiments across six models and four programming languages show that GoodVibe consistently improves security performance under realistic, lowsecurity-awareness usage, outperforming parameter-efficient baselines with orders of magnitude fewer trainable parameters and significantly lower computational cost. Ablation and hyperparameter studies confirm that these gains are stable and generalize across LLMs and languages. More broadly, our results suggest that security-relevant behavior can be selectively reinforced to improve default generation quality, particularly in settings such as vibe coding where explicit security guidance is absent. Future work includes integrating GoodVibe with deployment-time safeguards and extending the approach to other dimensions of controllable model behavior, including privacy and policy compliance. likelihood that insecure patterns propagate downstream, effectively raising the baseline security quality of AI-generated code. More broadly, this fine-grained internal control aligns with emerging needs for interpretable and targeted model adaptation, and suggests general strategy for improving default behavior in AI-assisted software development, extending beyond code security to domains such as privacy, compliance, and organizational policy adherence."
        },
        {
            "title": "8 Related Works",
            "content": "Recent studies have shown that code LLMs frequently generate code with security vulnerabilities, even when the code is functionally correct. This problem is particularly acute in modern development workflows, where LLMs are used for rapid prototyping, code completion, and informal vibe coding, in which developers prioritize speed and convenience over careful review or explicit security reasoning [5,6]. These observations have motivated growing body of work on improving the security of LLM-generated code. prominent line of research focuses on adapting models through full finetuning or instruction tuning on security-focused datasets. Approaches such as SafeCoder [10] and Secure-Instruct [33] construct instructionresponse pairs that emphasize secure coding practices and fine-tune pre-trained models accordingly, reporting substantial reductions in vulnerable code generation. While effective, full fine-tuning is computationally expensive, requires full access to model parameters, and is known to risk catastrophic forgetting of general coding capabilities [11]. These drawbacks limit its practicality for frequent updates or deployment in resource-constrained settings. To reduce training cost, parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) have been applied to secure code generation. Systems like HexaCoder [34] demonstrate that PEFT can improve security outcomes while introducing only small number of trainable parameters. However, these methods still operate at the level of weight matrices and implicitly assume that security-relevant knowledge is diffusely distributed across the model. As result, they offer limited interpretability and control over where security behavior is encoded, and their effectiveness depends sensitively on design choices such as which layers are adapted. Another line of work explores prompt-based techniques that steer model behavior without modifying parameters, making them attractive for black-box settings and rapid deployment. For example, PromSec [35] proposes automated prompt optimization to discourage vulnerable code generation, while SGCode [36] introduces structured prompts tailored to specific vulnerability classes. In-context learning approaches such as SecCoder [37] further show that few-shot securityaware examples can improve outputs without training. While these methods are lightweight, their effectiveness is highly sensitive to prompt phrasing, context length, and user discipline. In practice, especially in vibe coding scenarios where 13 Potential Harms. We consider two broad categories of potential harm. First, GoodVibe could be repurposed to selectively amplify undesirable or harmful behaviors in language models if applied irresponsibly. Additionally, users may overestimate the security guarantees provided by improved models and deploy generated code without adequate review. Second, GoodVibe may lower the barrier for manipulating model behavior in ways not anticipated by model developers. While this does not directly violate individual rights, it could contribute to broader concerns about model misuse if combined with adversarial objectives. We take several steps to mitigate potential harms. First, we explicitly scope our threat model to benign usage scenarios and exclude adversarial prompting, jailbreaks, and intentional misuse. Second, we emphasize that GoodVibe improves average security behavior but does not guarantee vulnerability-free code. Finally, we do not release any tools that enable end users to arbitrarily modify model parameters. Human Oversight and Responsible Use. GoodVibe is intended to assist developers, not replace secure software engineering practices. Generated code should continue to be reviewed, tested, and audited using established security processes. We encourage responsible deployment and ongoing human oversight when integrating language models into security-critical development workflows."
        },
        {
            "title": "Ethical Considerations",
            "content": "This work investigates neuron-level techniques for improving the security of code generated by large language models. In accordance with USENIX Securitys ethics guidelines, we conduct stakeholder-based ethical analysis and explicitly consider ethical principles, potential harms, mitigations, and the rationale for conducting and publishing this research. Stakeholders. We identify the following stakeholders who may be impacted by this research: Software developers and organizations using LLMassisted programming tools, who may benefit from reduced risk of unintentionally introducing security flaws. End users of software systems whose security and privacy depend on the absence of exploitable flaws in deployed code. Security engineers and auditors who may rely on improved code-generation tools as part of secure development lifecycles. Model developers and tool providers who may adopt neuron-level optimization techniques in training or deployment pipelines. The research community, including both defensive and adversarial researchers, who may extend GoodVibe for other objectives. Society at large, which may be affected by the widespread deployment of AI-assisted software development tools. Benefits. The benefits of this work are intended to be broadly applicable across model families, programming languages, and development contexts. We do not target or disadvantage specific groups of users. Open-source models are used to avoid the benefits exclusively within proprietary systems. Ethical Principles Considered. Our analysis is guided by the ethical principles articulated in the Menlo Report [38]. Specifically, this work aims to reduce the prevalence of security vulnerabilities introduced through benign use of code language models. By improving default security behavior, this work aims to lower systemic risk in software ecosystems that increasingly rely on automated code generation. Respect for Persons, Laws, and Licenses. This research does not involve human subjects, personal data, or user-generated private content. All datasets used consist of publicly available or curated code samples. We do not attempt to manipulate or deceive users, nor do we study user behavior without consent. The research complies with applicable laws and licenses governing models and datasets. The goal of improving AI-assisted software security, which aligns with the public interest in safer digital infrastructure."
        },
        {
            "title": "References",
            "content": "[1] I. Ozkaya, Application of large language models to software engineering tasks: Opportunities, risks, and implications, IEEE Software, vol. 40, no. 3, pp. 48, 2023. [2] M. Chen, Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374, 2021. [3] Google, Gemini Code Assist: AI-first coding in your natural language, https://codeassist.google/, 2025. [4] I."
        },
        {
            "title": "Shani",
            "content": "and G. on Staff, Survey developer reveals experience, the impact AIs https://github.blog/news-insights/research/ survey-reveals-ais-impact-on-the-developer-experience/ #methodology, 2023. [5] P. P. Ray, review on vibe coding: Fundamentals, stateof-the-art, challenges and future directions, Authorea Preprints, 2025. [6] S. Zhao, D. Wang, K. Zhang, J. Luo, Z. Li, and L. Li, Is vibe coding safe? benchmarking vulnerability of agentgenerated code in real-world tasks, arXiv preprint arXiv:2512.03262, 2025. [7] M. Schreiber and P. Tippe, Security vulnerabilities in aigenerated code: large-scale analysis of public github repositories, in International Conference on Information and Communications Security. Springer, 2025, pp. 153172. [8] J. Spracklen, R. Wijewickrama, A. N. Sakib, A. Maiti, and B. Viswanath, We have package for you! comprehensive analysis of package hallucinations by code generating {LLMs}, in 34th USENIX Security Symposium (USENIX Security 25), 2025, pp. 36873706. [9] J. He and M. Vechev, Large language models for code: Security hardening and adversarial testing, in Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, 2023, pp. 18651879. [10] J. He, M. Vero, G. Krasnopolska, and M. Vechev, Instruction tuning for secure code generation, arXiv preprint arXiv:2402.09497, 2024. [11] X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, and P. Henderson, Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023. [12] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. ICLR, vol. 1, no. 2, p. 3, 2022. 15 [13] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen et al., Parameterefficient fine-tuning of large-scale pre-trained language models, Nature machine intelligence, vol. 5, no. 3, pp. 220235, 2023. [14] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023. [15] L. Xu, H. Xie, S. J. Qin, X. Tao, and F. L. Wang, Parameter-efficient fine-tuning methods for pretrained language models: critical review and assessment, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2026. [16] D. Zhang, T. Feng, L. Xue, Y. Wang, Y. Dong, and J. Tang, Parameter-efficient fine-tuning for foundation models, arXiv preprint arXiv:2501.13787, 2025. [17] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly et al., mathematical framework for transformer circuits, Transformer Circuits Thread, vol. 1, no. 1, p. 12, 2021. [18] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. [19] J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, W. Li, Y. Shen, S. Ma, H. Liu et al., survey on llm-as-ajudge, The Innovation, 2024. [20] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, Think you have solved question answering? try arc, the ai2 reasoning challenge, arXiv preprint arXiv:1803.05457, 2018. [21] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. [22] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, Measuring massive multitask language understanding, Proceedings of the International Conference on Learning Representations (ICLR), 2021. [23] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., The llama 3 herd of models, arXiv e-prints, pp. arXiv2407, 2024. [35] M. Nazzal, I. Khalil, A. Khreishah, and N. Phan, Promsec: Prompt optimization for secure generation of functional source code with large language models (llms), in Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, 2024, pp. 22662280. [36] K. Ton, N. Nguyen, M. Nazzal, A. Khreishah, C. Borcea, N. Phan, R. Jin, I. Khalil, and Y. Shen, Sgcode: flexible prompt-optimizing system for secure generation of code, in Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, 2024, pp. 50785080. [37] B. Zhang, T. Du, J. Tong, X. Zhang, K. Chow, S. Cheng, X. Wang, and J. Yin, Seccoder: Towards generalizable and robust secure code generation, arXiv preprint arXiv:2410.01488, 2024. [38] M. Bailey, D. Dittrich, E. Kenneally, and D. Maughan, The menlo report, IEEE Security & Privacy, vol. 10, no. 2, pp. 7175, 2012. [24] G. Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova, A. Ramé, M. Rivière et al., Gemma 3 technical report, arXiv preprint arXiv:2503.19786, 2025. [25] Cybernative.ai, and Cybernative.ai dataset, vulnerability [Online]. Available: https://huggingface.co/datasets/CyberNative/ Code_Vulnerability_Security_DPO code 2024. security [26] S. Lu, S. Park, E. Seo, and Y. Zhou, Learning from mistakes: comprehensive study on real world concurrency bug characteristics, in Proceedings of the 13th international conference on Architectural support for programming languages and operating systems, 2008, pp. 329339. [27] L. Szekeres, M. Payer, T. Wei, and D. Song, Sok: Eternal war in memory, in 2013 IEEE Symposium on Security and Privacy. IEEE, 2013, pp. 4862. [28] A. P. Felt, M. Finifter, E. Chin, S. Hanna, and D. Wagner, survey of mobile malware in the wild, in Proceedings of the 1st ACM workshop on Security and privacy in smartphones and mobile devices, 2011, pp. 314. [29] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein, Y. Le Traon, D. Octeau, and P. McDaniel, Flowdroid: Precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for android apps, ACM sigplan notices, vol. 49, no. 6, pp. 259269, 2014. [30] T. Krauß, H. Dashtbani, and A. Dmitrienko, Twinbreak: Jailbreaking llm security alignments based on twin prompts, arXiv preprint arXiv:2506.07596, 2025. [31] L. Wu, S. Behrouzi, M. Rostami, M. Thang, S. Picek, and A.-R. Sadeghi, Neurostrike: Neuron-level attacks on aligned llms, Network and Distributed System Security (NDSS) Symposium, 2026. [32] T. M. Ahmed, W. Shang, and A. E. Hassan, An empirical study of the copy and paste behavior during development, in 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories. IEEE, 2015, pp. 99110. [33] J. Li, F. Rabbi, B. Yang, S. Wang, and J. Yang, Secure-instruct: An automated pipeline for synthesizing instruction-tuning datasets using llms for secure code generation, arXiv preprint arXiv:2510.07189, 2025. [34] H. Hajipour, L. Schönherr, T. Holz, and M. Fritz, Hexacoder: Secure code generation via oracle-guided synthetic training data, arXiv preprint arXiv:2409.06446, 2024."
        },
        {
            "title": "A Neuron Cluster Visualization",
            "content": "Figure 4 visualizes the clustering structure of security-critical neurons within the self-attention mechanism of CodeLlama7B-Instruct, focusing on the query, key, and value projection submodules (q_proj, k_proj, and v_proj) from representative layers. Each point corresponds to single neuron, with its vertical position indicating gradient-based importance for security prediction and its color denoting cluster membership. (a) Neuron Cluster for q_proj (b) Neuron Cluster for k_proj rons are not uniformly distributed but instead organize into coherent clusters with similar importance profiles. Importantly, the number and structure of clusters differ across projection submodules, suggesting that query, key, and value projections encode security-related signals in distinct ways. These visual patterns support our design choice to cluster neurons before fine-tuning. Rather than treating each security neuron independently, clustering captures shared functional roles among neurons that respond similarly to security-relevant signals. This enables structured parameter sharing during optimization, reducing the number of trainable parameters while preserving the expressiveness needed to reinforce secure coding behavior."
        },
        {
            "title": "B More Code Examples",
            "content": "To provide more evidence on the security of generated code, aligning with Section 5.2, we instruct the baseline and GoodVibe-hardened model with the same prompts for different lanugaes, here we caan find another example but for golang. In this example model tasked with following prompt. Write go code that takes string input from the user and copies it into fixed-size buffer (of size 10). Print out the copied string The general goal of this example is to copy user-provided string into fixed-size buffer. No explicit security constraints are enforced in the prompt in order to mimic vibe-coding scenario. The code generated by the two models is shown in Listing 3 and Listing 4. As expected, GoodVibe incorporates explicit length checks to prevent writing beyond the buffer boundary (Listing 4), whereas the baseline models output omits such checks and may result in buffer overflow (Listing 3). These results further indicate that selectively optimizing security-critical neurons leads to consistent and robust improvements in code security across programming languages. 1 package main 2 3 import ( \" fmt \" \" os \" 4 5 6 ) 7 8 func main () { (c) Neuron Cluster for v_proj Figure 4: Visualization of Neuron Clusters. 10 11 12 13 14 16 Across all three projections, neurons form clear and wellseparated groups, reflected by consistently high silhouette scores (0.700.78). This indicates that security-relevant neu17 } 17 var input string fmt . Println (\" Enter string : \") fmt . Scanln (& input ) var buffer [10] byte copy ( buffer [:] , input ) fmt . Println (\" Copied string : \" , string ( buffer (cid:44) [:]) ) Listing 3: Code Generated by Baseline model. 1 package main 2 import ( 3 4 \" bufio \" \" fmt \" \" os \" 5 6 ) 7 func main () { reader := bufio . NewReader ( os . Stdin ) fmt . Print (\" Enter string : \") input , _ := reader . ReadString ( ) input = input [: len ( input ) -1] if len ( input ) > 10 { input = input [:10] } fmt . Println (\" Copied string :\" , input ) 8 9 11 12 13 14 15 16 } Listing 4: Code Generated by GoodVibe-hardened Model."
        }
    ],
    "affiliations": [
        "Radboud University",
        "Technical University of Darmstadt",
        "University of Zagreb"
    ]
}