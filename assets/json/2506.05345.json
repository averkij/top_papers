{
    "paper_title": "Inference-Time Hyper-Scaling with KV Cache Compression",
    "authors": [
        "Adrian ÅaÅ„cucki",
        "Konrad Staniszewski",
        "Piotr Nawrot",
        "Edoardo M. Ponti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 5 4 3 5 0 . 6 0 5 2 : r Inference-Time Hyper-Scaling with KV Cache Compression Adrian ÅaÅ„cucki NVIDIA University of Edinburgh Konrad Staniszewski Piotr Nawrot* Edoardo M. Ponti Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the keyvalue (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), novel method for sparsifying KV caches that only requires 1K training steps to achieve 8 compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets. 1. Introduction Scaling inference-time computeemployed in models such as OpenAIs o1 (OpenAI et al., 2024) or DeepSeeks R1 (Guo et al., 2025)trades off increased inference time and memory for higher reasoning accuracy in large language models (LLMs). Models reason by generating intermediate steps that explore the problem before reaching an answer. Adjusting the depth and breadth of this explorationknown as sequential and parallel scaling, respectively (Muennighoff et al., 2025)controls the inference-time compute budget (Yao et al., 2023; Uesato et al., 2022; Wang et al., 2023; Lightman et al., 2024). Despite its success, scaling inference-time compute is fundamentally bottlenecked in Transformer LLMs by the number of tokens from the keyvalue (or KV) cache that are attended to during auto-regressive generation. This cache grows linearly with respect to the length and number of reasoning chains, as the new keyvalue representations are appended to it. Hence, it can easily exhaust the memory of the accelerator and slow down each generation step, as attention is memory-bound: its cost is dominated by the time needed to retrieve the cache from memory. Fortunately, several methods can mitigate these issues during post-training or inference. These rely on training-free heuristics to sparsify the KV cache (Oren et al., 2024; Li et al., 2024), selectively retrieve subsets of tokens (Tang et al., 2024), or retrofit LLMs with the ability to choose whether to merge or append items to the cache (Nawrot et al., 2024). *Work done as an intern at NVIDIA. Figure 1 Average absolute improvement of DMS over the original LLMs during inference-time scaling on reasoning tasks within the same KV cache budget, which is proxy for runtime and memory load. In this work, we investigate for the first time whether efficient attention methods enhance inferencetime scaling. In principle, by exploring more concurrent reasoning threads or longer threads for the same memory or runtime budget, efficient models can achieve higher-quality predictions than their original counterparts. However, this hinges upon the crucial assumption that efficient attention does not degrade the models reasoning abilities, which unfortunately is often the side effect of training-free sparsification methods (Zhang et al., 2023a; Oren et al., 2024). On the other hand, KV cache compression during postInference-Time Hyper-Scaling with KV Cache Compression training usually better preserves the models quality, but also demands costly retrofitting procedures (Nawrot et al., 2024). In order to overcome these limitations, as second main contribution, we propose Dynamic Memory Sparsification (DMS), new method that combines the best of both worlds by retrofitting LLMs to sparsify the KV cache through an inexpensive procedure. We thus demonstrate that sparsificationrather than more complex token merging proposed in Dynamic Memory Compression (DMC; Nawrot et al., 2024)is sufficient to retain performance at high compression ratios if the model is made aware of the eviction decisions certain number of steps in advance. This, in turn, allows us to retrofit LLMs with KV cache compression in much more sample-efficient way than DMC, achieving 8 compression with only 1K training steps. On the other hand, the superior performance of DMS highlights the benefits of retrofitting LLMs over training-free heuristics. We evaluate inference-time scaling capabilities of efficient attention (including DMS) on reasoning datasets such as MATH-500 (Hendrycks et al., 2021b) and AIME 2024 for math, GPQA Diamond (Rein et al., 2024) for hard sciences, and LiveCodeBench (Jain et al., 2025) for coding. We find that DMS significantly improves the Pareto frontiers across various model sizes and datasets, outperforming vanilla LLMs in both memory reads (which is proxy for runtime) and peak memory use. Notably, DMS consistently dominates other baselines for efficient attention, which we also verify on broader set of tasks outside of inference-time scaling. DMS variants even surpass the corresponding vanilla LLMs on long-context tasks, such as needle-in-a-haystack and variable state tracking (Hsieh et al., 2024), while achieving higher efficiency. Overall, this validates the effectiveness of efficient attentionunlocked by DMSfor inferencetime scaling, which improves the reasoning capabilities of models under any given inference-time budget. 2. Background 2.1. Inference-time Scaling Inference-time scaling allows model to think longer or more broadly about problem to enhance the quality of its solution, by leveraging extra compute during generation (Du et al., 2024; Madaan et al., 2023; Yao et al., 2023). In practice, when presented with prompt x, Large Language Model ğ‘“LLM can explore ğ‘› chains of reasoning [z1, . . . , zğ‘›] to generate corresponding answers [y1, . . . , yğ‘›]. While some strategies involve guiding this exploration through Process Reward Model (PRM; Li et al., 2023; Feng et al., 2023; Lightman et al., 2024; Uesato et al., 2022; Wang et al., 2024; Snell et al., 2024) by scoring each reasoning step, recent systematic comparisons established that simpler PRM-free strategies such as majority voting (Wang et al., 2025b) remain the most competitive. Hence, scaling can be easily achieved in two ways: increasing the maximum length for chains of reasoning (known as sequential scaling) or increasing the number of chains (known as parallel scaling). These two quantities can be controlled to determine token budget for inference-time computation (Muennighoff et al., 2025), which directly translates into proportional memory load and runtime. In fact, in Transformer LLMs, the keyvalue cache grows linearly with the number of generated tokens. Crucially, the KV cache is stored in VRAM on GPU accelerators, contributing significantly to the overall memory load and limiting scalability. At the same time, retrieving the KV cache through high-bandwidth memory access dominates runtime during generation. As result, the KV cache constitutes bottleneck for inference-time scaling, which leads to the natural question: by making the KV cache leaner and scaling to even longer sequences or more parallel ones, could we increase the downstream performance of reasoning LLMs for the same compute budget? 2.2. KV Cache Sparsity An intuitive strategy to reduce the size of the KV cache is to evict tokens, i.e., dynamically remove the keyvalue pairs of the least relevant tokens during inference. Recent methods have addressed this challenge by selectively managing tokens within sliding window of context of size ğ‘¤. For instance, for each time step ğ‘¡, TOVA (Oren et al., 2024) evicts the token with the lowest attention weight such that ğ‘–TOVA = minğ‘– â„ğ» ğ‘â„(ğ‘¡)ğ‘– where ğ‘â„(ğ‘¡)ğ‘– denotes the attention weight assigned to token ğ‘– by attention head â„ at time step ğ‘¡. Similarly, Heavy-Hitter Oracle (H20; Zhang et al., 2023a) evicts the token with the lowest cumulative attention, additionally keeping sliding window of recent tokens. These approaches incur minimal computational overhead due to their efficient heuristics for eviction scores, while retaining maximum KV cache size of ğ‘¤ and speeding up generation as consequence. An entirely different strategy is adopted by methods like Quest (Tang et al., 2024), which fully retains the entirety of the KV cache but only retrieves the most relevant pages (i.e., fixed-size blocks of contiguous KV items) from memory. Relevant pages are determined through heuristic that approximates attention scores from the highest-magnitude dimensions 2 Inference-Time Hyper-Scaling with KV Cache Compression of their KV items. While this approach accelerates generation by reducing memory transfers without permanently evicting tokens, it does not reduce memory load. In fact, to efficiently perform page selection, the method requires storing additional page representations, resulting in slight memory overhead rather than savings. 2.3. KV Cache Compression While mitigating runtime or memory load, trainingfree KV cache sparsity methods often incur performance degradation at high compression ratios. To overcome this trade-off, Dynamic Memory Compression (DMC; Nawrot et al., 2024) reduces KV cache size by dynamically compressing representations, potentially extracting and retaining vital information. At each timestep ğ‘¡, for every attention head separately, DMC models decide to either append the new keyvalue pair to KV cache as in standard Transformers, or accumulate it into the most recent cache entry using weighted averaging. As result, every attention head produces uniquely compressed KV sequence with distinct compression ratio (CR). This flexibility allows the model to preserve critical information while aggressively compressing redundant representations, unlike sparse attention methods that impose uniform compression budgets (Nawrot et al., 2025). Applying DMC requires continued training (known as retrofitting), during which the discrete decisions are relaxed into continuous variables via stochastic reparameterization (Louizos et al., 2018), enabling gradient-based optimization. Although it requires fraction of the pre-training budget, the computational cost is still significant. Moreover, DMC by default does not accelerate the prefilling phase, since all intermediate, partially accumulated tokens are retained. 3. Dynamic Memory Sparsification Token eviction strategies effectively reduce KV cache size, but degrade downstream performance at higher eviction rates. Conversely, DMC offers stronger compression at the cost of expensive retraining. To scale inference-time efficiency even further, it is therefore essential to develop KV cache compression method that is inexpensive, easy to integrate, and maintains accuracy at high compression ratios. To this end, we propose Dynamic Memory Sparsification (DMS), method of teaching pre-trained models simple, adaptive token eviction policy. As such, it combines the advantages of eviction and trained compression, with significantly higher data efficiency than DMC. 3.1. Multi-Head Self-Attention We start from LLMs with vanilla multi-head selfattention. Formally, let h1:ğ‘‡ Rğ‘‡ ğ‘‘ represent the input sequence of hidden states to Transformer layer, where ğ‘‡ denotes the number of tokens, and ğ‘‘ is the hidden dimension. Multi-Head Self-Attention (henceforth, simply attention) divides the hidden states into ğ‘›â„ attention heads, each processed independently to capture distinct input relationships. For single attention head, the model applies distinct linear projections using weight matrices ğ‘Šğ‘, ğ‘Šğ‘˜, ğ‘Šğ‘£ R(ğ‘‘/ğ‘›â„)ğ‘‘, obtaining queries, keys, and values: q1:ğ‘‡ = ğ‘Šğ‘h1:ğ‘‡ , k1:ğ‘‡ = ğ‘Šğ‘˜h1:ğ‘‡ , v1:ğ‘‡ = ğ‘Šğ‘£h1:ğ‘‡ . The attention weights and output vector for the ğ‘–-th token are computed as: ğ‘ğ‘–ğ‘— = 1ğ‘—ğ‘– exp(q ğ‘– kğ‘—/ ğ‘¡=1 exp(q ğ‘– kğ‘¡/ ğ‘‘â„) ğ‘– ğ‘‘â„) , oğ‘– = ğ‘– ğ‘—= ğ‘ğ‘–ğ‘—vğ‘—, where 1 denotes the indicator function and constrains the token dependencies to be autoregressive. Finally, the outputs from all heads are concatenated and projected by matrix ğ‘Šğ‘œ Rğ‘‘ğ‘‘ to yield the output. 3.2. Retrofitting Models with DMS To integrate DMS into pre-trained models, we adapt the retrofitting strategy from DMC, introducing two crucial modifications. First, whereas DMC weights and aggregates tokens, DMS simply evicts them. Second, we separate the time of eviction decisions from the time of their execution: when eviction is predicted for token, the model is given time to integrate information from the token scheduled for eviction before it is actually removed. Below we describe the procedure for single attention head, though the same process is applied across all KV heads independently. Given an input hidden vector to an attention layer hğ‘¡ at inference time step ğ‘¡, DMS predicts binary eviction decision ğ›¼ğ‘¡ which controls the eviction of (kğ‘¡, vğ‘¡). To maintain differentiability during training, ğ›¼ğ‘¡ is learned through stochastic reparametrization with Gumbel-sigmoid distribution: ğ›¼ğ‘¡ Gumbel-sigmoid(hğ‘¡w + ğ‘, ğœ ) [0, 1], (1) where Rğ‘‘ is vector of trainable weights initially set to = [0, . . . , 0]. We set low temperature ğœ to encourage discrete eviction decisions and ğ‘= 5 in order to offset the logits and initiate training with ğ›¼ğ‘¡ 0, preventing eviction early in training. Empirically, this configuration prevents initial loss spikes, which might cause catastrophic forgetting (Nawrot et al., 2024). 3 Inference-Time Hyper-Scaling with KV Cache Compression k0 0 0 q0 q1 q2 q3 q4 q5 ... qn 0 log(1 (cid:57) ğ›¼1) log(1 (cid:57) ğ›¼1) ... log(1 (cid:57) ğ›¼1) k1 0 0 0 0 log(1 (cid:57) ğ›¼2) k2 0 0 0 0 log(1 (cid:57) ğ›¼2) log(1 (cid:57) ğ›¼3) . . . kn . . . ... 0 . . . . . . (a) DMS key cache management during inference. (b) Attention mask ğ‘€ğ›¼ during training. Figure 2 During each inference step (left) the incoming keyvalue pair (kğ‘¡, vğ‘¡) might be selected for later eviction, based on predicted binary decisions ğ›¼bin {0, 1} (we show only sequence of keys for clarity). The eviction takes place as soon as the pair falls out of the sliding window. During training (right), this behavior is induced with an additive attention mask. Eviction decisions are relaxed from binary to continuous ğ›¼ [0, 1]. During training, sequence of eviction decisions ğ›¼1:ğ‘‡ is used to construct mask ğ‘€ğ›¼ (, 0]ğ‘‡ ğ‘‡ (Figure 2b), which is added to unnormalized attention scores ğ‘„ğ¾ . It selectively modulates token visibility: fully masks token, 0 indicates no masking, and small negative values have partial effect. It follows that evicting particular kğ‘– automatically means evicting the corresponding vğ‘–. Delayed Eviction via Sliding Window Immediate eviction can harm the models abilities by prematurely discarding useful context. To mitigate this, we propose delaying the execution of eviction decisions. Specifically, the eviction decision ğ›¼ğ‘¡ is made at timestep ğ‘¡, but the token selected for eviction remains available until future timestep ğ‘¡ + ğ‘¤. This delay creates sliding window of size ğ‘¤ and is implemented through the construction of ğ‘€ğ›¼. Previous work indicates that decoder-only models heavily attend to recent tokens (Xiao et al., 2024; Jiang et al., 2024). Consequently, delayed eviction enables the model to extract relevant information from the eviction candidates before their removal, even if those tokens are recent. In contrast, immediate eviction of recent tokens has negative impact. We confirm this empirically in Section 5.3, demonstrating that immediate eviction leads to rapid performance degradation for all tested sliding window sizes, whereas delayed eviction maintains stable training and substantially improves sample efficiency, dramatically reducing the number of training tokens needed to achieve given CR. Training Objective During training we follow DMC and apply one-sided L1 loss term which forces ğ‘™ğ¿ â„ğ» the model to match the average value of predicted ğ›¼ for given input to the target compression ğ›¼, i.e., â„’aux = max (ğ›¼ğ¿ ğ» ğ‘‡ ğ‘¡ğ‘‡ ğ›¼ğ‘™â„ğ‘¡, 0), where ğ¿, ğ», ğ‘‡ denote the number of layers, KV attention heads, and sequence length, respectively. Over the course of training, the target compression ğ›¼ is linearly annealed from 0 to (1 1 ). We train the model using logit distillation loss â„’D loss (Hinton et al., 2015), which is added to the auxiliary loss â„’aux: â„’ = â„’D + â„’aux. CR Since we do not enforce any constraints on compression for individual attention heads, they adopt possibly different compression ratios and produce KV sequences of possibly different lengths. Performance Considerations The overhead of DMS on the attention mechanism comes solely from constructing and applying the additive attention mask, which never needs to be materialized. For each attention head, it can be compactly passed as vector of eviction decisions ğ›¼1:ğ‘‡ , and is implementable with existing tools (Wang et al., 2025a; Dong et al., 2024). Implementation-wise, neuron is re-purposed from qğ‘¡ or kğ‘¡ to predict ğ›¼ğ‘¡ instead of adding parameter vector for every attention head (Nawrot et al., 2024). Hence, no extra parameters are added. 3.3. Inference Figure 2a shows the inference time operation of DMS. The decision variables are rounded to the nearest integer ğ›¼bin ğ‘¡ =sigmoid(hğ‘¡w + ğ‘) {0, 1}. If ğ›¼bin ğ‘¡ =1, then the (kğ‘¡, vğ‘¡) pair needs to be evicted at time ğ‘¡+ğ‘¤. The sparsity introduced by DMS can also be leveraged during the prefilling phase to eliminate unnecessary computation (Wang et al., 2025a). 4 Inference-Time Hyper-Scaling with KV Cache Compression Performance-wise, DMS does not introduce any new read/write operations on the KV cache, since the evicted tokens could be simply overwritten by incoming ones, under the assumption that the keys are stored in the KV cache with positional information. PagedAttention (Kwon et al., 2023) facilitates storing the sparsified KV cache in memory, where pages are allocated to individual attention heads. This formulation enables our reuse of existing, efficient kernels that support PagedAttention. 4. Experimental Setup Models and Baselines To evaluate inference-time scaling through KV cache compression, we primarily focus on reasoning models of different sizes distilled from DeepSeek R1 reasoning traces (Guo et al., 2025), including Qwen 2.5 1.5B, 7B, and 32B. In addition, as sanity check on other families of models and for ablations on method design, we test the accuracy of efficient attention methods also on Llama 3.2 1B Instruct (Grattafiori et al., 2024). We retrofit all these models with DMS and compare them against the original models, DMC, and training-free KV cache sparsification methods described in Section 2.2: Token Omission via Attention (TOVA; Oren et al., 2024), Heavy-Hitter Oracle (H2O; Zhang et al., 2023a), and Quest (Tang et al., 2024).1 Crucially, all the LLMs included in the experiments use Grouped Query Attention (GQA; Ainslie et al., 2023), hence KV tokens are shared among multiple query heads, which exacerbates the destructive effects of token eviction. Logit Distillation and Retrofitting In contrast to conventional retrofitting methods employing standard language modeling loss (Nawrot et al., 2024), we retrofit all models through logit distillation (Hinton et al., 2015). In particular, the original LLM acts as the teacher and the DMS-retrofitted one as the student. As previously observed in other settings (Sreenivas et al., 2024; Minixhofer et al., 2025), we found that logit distillation provides greater robustness to shifts in training data (since the original pre-training and post-training mixtures are rarely public) and is especially beneficial for fragile LLMs with lower parameter counts. We provide information on the training data for distillation in Appendix C. The retrofitting process is inspired by DMC (Nawrot et al., 2024). The amount of data required for equipping LLMs with DMS depends directly on the context length of retrofitted models and the target KV cache compression ratio: higher compres1Our baseline implementations closely follow the publicly available reference implementations. sion ratios necessitate larger datasets. Specifically, we employ linear training schedule that runs for 100 training steps for each unit of compression ratio: CR(ğ‘¡) = ğ‘¡ 100 + 1. Unless otherwise stated, we train DMS models with sliding windowand equivalently an eviction delayof 256 tokens. 5. Results 5.1. Inference Time Hyper-Scaling Our main goal is to determine if KV cache compression increases downstream performance by effectively leveraging larger token budget for an equivalent runtime and memory load compared with vanilla Transformers. We run our experiments for inference-time hyper-scaling on selection of datasets that require advanced reasoning capabilities, following Snell et al. (2024) and Guo et al. (2025): specifically, AIME 24 and MATH 500 (Hendrycks et al., 2021a) for math problems, GPQA Diamond (Rein et al., 2024) for physics, chemistry, and biology problems, and LiveCodeBench (Jain et al., 2025) for coding. As metric for performance, we use exact match after mapping the output to unified math representation (for MATH 500 and AIME 24) or one of the 4 available choices (for GPQA Diamond). For LiveCodeBench, we report pass@all, i.e., if any of the generated sequences pass the tests. As metrics for the effective budget in terms of time and memory, we focus on two metrics: (i) KV cache token reads, which represent the total number of items in the KV cache attended to in each generation step, summed across steps. Therefore, it reflects the runtime efficiency, as KV cache loading from memory is the main bottleneck during generation, contributing share of the inference latency that increases with the sequence length (Tang et al., 2024; Nawrot et al., 2025). For instance, as detailed in Appendix G, for batch size of 256 and for the range of sequence lengths considered in our experiments (8K32K), this share exceeds 90% of the latency in Qwen-R1 1.5B and 80% for Qwen-R1 7B. Secondly, (ii) peak tokens in memory, which represents the maximum KV cache size, critical for memory-constrained hardware, such as GPUs or edge devices. To establish variety of trade-offs between downstream performance and compute, we run experiments with varying budget configurations in terms of maximum sequence length (L), number of parallel reasoning chains or width (W), and compression ratio (CR). Hence, each configuration can be defined by tuple L-W-CR, where CR is 1 for vanilla models. By identifying the Pareto frontier for each method, we can determine which ones offer superior performance 5 Inference-Time Hyper-Scaling with KV Cache Compression Figure 3 Inference-time scaling results comparing exact-match performance (ğ‘¦-axis) and KV cache reads as measure of runtime (ğ‘¥-axis). We evaluate Qwen-R1-distilled models at different scales (columns) and 4 datasets (rows). Point colors indicate results for DMS (green), vanilla models (purple), and the state-of-the-art sparse attention baseline, Quest (yellow). Colored lines indicate the respective Pareto frontier. Annotations indicate the scaling strategy as L-W-CR tuple in terms of sequence length (times 1024 tokens), width (number of sampled reasoning threads), and compression ratio CR. The horizontal black line shows the performance reported by Guo et al. (2025) for the vanilla model based on 32-1-1 configuration. From the plots, it emerges that not only DMS achieves the best Pareto frontier, but that in general, KV cache compression is an effective strategy for improving inference-time scaling. for the same budget. We report the results for each efficiency metric separately: KV cache memory reads in Figure 3 and peak memory usage in Figure 4. For simplicity, as baselines, we report only the state-of-the-art training-free method in terms of accuracyspeedup (Quest) and accuracymemory (TOVA), respectively. From Figure 3, we observe that KV cache compression methods generally yield superior Pareto frontiers compared to vanilla LLMs, across model sizes and datasets. Specifically, the best-performing method in each datasetsize combination substantially improves the scores at comparable memory transfer budgets (proportional to runtime). Averaging Pareto frontier margins across budgets, as detailed in Appendix Table 5 and summarized in Figure 1, we find average gains for DMS of 11.5 for AIME 24, 2.3 for MATH 500, 5.5 for GPQA Diamond, and 8.3 for LiveCodeBench. Variability across datasets primarily reflects their saturation levels; for instance, models achieve very high performance on MATH 500 even with limited budgets. Notably, performance gains from DMS decrease with 6 Inference-Time Hyper-Scaling with KV Cache Compression Figure 4 Inference-time scaling results comparing exact-match performance (ğ‘¦-axis) and peak tokens in memory (ğ‘¥-axis). We refer to Figure 3 for full details. These results clearly indicate that KV cache compression methods (and especially DMS) incur substantially reduced number of peak tokens in memory compared to vanilla LLMs, achieving higher performance for comparable memory needs. increasing model scale on MATH 500, yet increase with scale on GPQA Diamond and LiveCodeBench. Similar trends emerge for the peak tokens in memory metric shown in Figure 3 and summarized in Table 6. Overall, these findings indicate that KV cache compression exhibits more favorable scaling properties than dense attention, highlighting its potential for advancing the reasoning capabilities of current LLMs. Moreover, comparing DMS with other KV cache compression methods, it emerges how its Pareto frontier clearly dominates the best baselines for both efficiency metrics, namely Quest for KV cache memory reads and TOVA for peak tokens in memory (Figures 3 and 4). This is even more remarkable considering that Quest sacrifices memory efficiency, fully preserving the KV cache, in order to mitigate accuracy degradationand yet DMS manages to offer better runtimeaccuracy trade-off. Datasets like MATH 500, where Quests Pareto frontier mostly overlaps with Vanilla at all scales, illustrate that gains from larger token budgets can be eaten away, unless performance is retained even at high CRs. DMS meets this desideratum in data-efficient way, thus offering inexpensive hyper-scaling with existing LLMs. Zooming in on specific results, we can assess which L-W-CR configurations tend to lie on the Pareto frontier for DMS. For most tasks, these consist of combination of sequential and parallel scaling, hinting at the necessity to use both for inference-time scaling. 7 Inference-Time Hyper-Scaling with KV Cache Compression Table 1 Evaluation of Llama 3.2 1B Instruct on broader array of tasks, reflecting different combinations of KV cache compression methods and compression ratios (CR). We note that due to full-dense attention prefill, Quest is equivalent to vanilla on MMLU and HellaSwag. The DMS model used in this comparison was trained with sliding window of just 16 tokens. CR 1 2 3 4 Method Vanilla H2O TOVA Quest DMC win=16 DMS H2O TOVA Quest DMC win=16 DMS H2O TOVA Quest DMC win=16 DMS GSM8K 47.0 44.0 MMLU 47.9 45.7 HellaS 43.4 42.9 NIAH 96.4 34.0 55.8 27.4 VT 45.1 45.0 43.4 47.9 42.8 43.4 42.2 95.8 65.2 53.0 56.2 31.9 46.9 32.9 34.9 48.0 37.6 43.3 42.1 0.0 97.8 17.2 0.0 63.2 17.6 6.4 46.5 14.7 44.7 40.1 45.2 32.7 38.1 47.9 26.3 43.3 41.3 42.5 43.4 40.0 1.8 40.2 95.6 93.6 13.4 0.2 69.2 12.6 50.4 45. 3.6 42.3 39.9 20.2 35.2 47.9 25.6 40.2 41.8 43.4 39.4 43.4 0.0 96.8 28.0 4.0 67.6 33.8 95.8 49.6 5.2. DMS for General-purpose LLMs Moreover, we aim to establish whether DMS is effective beyond inference-time scaling settings, so that model retrofitted with DMS can be reliably deployed as general-purpose LLM. To this end, we first compare DMS with respect to vanilla models for equivalent generated token lengths (rather than actual compute budget). We focus again on the same models and datasets as Section 5.1. From Tables 7 to 9, it emerges that DMS mostly preserves the original accuracy at CR 4 and yields minimal degradations (3.5 points on average) at CR 8. Next, we benchmark DMS on broader set of tasks, as way to ensure that its sparse prefilling does not affect performance in short generation settings: GSM8K (Cobbe et al., 2021) for grade-school math, MMLU (Hendrycks et al., 2021a) for factuality, HellaSwag (Zellers et al., 2019) for zero-shot commonsense question answering, and Needle in Haystack (NIAH; Kamradt, 2023) and Variable Tracking (VT; Hsieh et al., 2024) for long context processing. We select Llama 3.2 Instruct, which does not rely on long reasoning and is therefore compatible with short generation. Technical details for the experimental setup are provided in Appendices and F. Table 1 compares downstream performance at 2, 3, and 4 compression ratios.2 DMS stands out as the most robust method, achieving higher scores than both training-free and retrofitted baselines in most combinations of tasks and CRs, with Quest as second-best contender. While in short-context tasks, DMS performance is close to the original LLM, in long-context tasks (such as NIAH and VT) DMS even surpasses it. We speculate this is due to overcoming limitations of dense attention over long sequences, such as over-squashing (Barbero et al., 2024) and lost-in-the-middle (Liu et al., 2024a). Moreover, long-context performance provides evidence that compared with DMCDMS is more successful at extrapolating compression to lengths beyond those observed during retrofitting, albeit only up to certain limit (see Appendix D). Among learned compression methods, DMC collapses quickly, likely due to its more challenging training objective amplified by the limited 1B model capacity.3 Overall, DMS emerges as general-purpose method, whose performance retention at high compression ratios makes it suitable not only for inference-time hyper-scaling but also for short-context and long-context tasks. 5.3. Ablations for DMS and In-Depth Analysis The design choices in DMS were informed by results of small-scale experiments. We present ablations on eviction policy and data efficiency during retrofitting of the Llama 3.2 1B Instruct models. To evaluate the impact of delayed eviction, we trained additional models with immediate eviction, which aligns more closely with existing token eviction methods: Delayed eviction ğ›¼ğ‘¡ determines the eviction of (kğ‘¡, vğ‘¡) at future time step ğ‘¡ + ğ‘¤ Immediate eviction ğ›¼ğ‘¡+ğ‘¤ determines the eviction of past (kğ‘¡, vğ‘¡) at time step ğ‘¡ + ğ‘¤ Both policies were tested with different sliding window sizes. Remarkably, DMS retains reasoning capabilities with window of only 16 tokens up to compression ratio of 4, as shown in Figure 5. Larger sliding window sizes better preserve reasoning capabilities, which is expected in zero-shot setting. However, switching to immediate eviction drastically deteriorates scores for every tested sliding window length. Regarding data efficiency, the right panel of Figure 5 shows how scores vary when retrofitting with 2While H2O and TOVA are designed for long context tasks with large sliding windows, we consciously evaluate them with short sliding windows to meet the target CRs. 3Nevertheless, in Appendix we show that, while still lagging behind, this collapse does not occur for shorter contexts and larger non-GQA model. Inference-Time Hyper-Scaling with KV Cache Compression Figure 5 GSM8K 0-shot scores of Llama 3.2 1B Instruct across different compression variants. Left: delayed eviction (default) with 16-token window consistently preserves reasoning abilities of the model, while immediate eviction causes rapid degradation. The quality gap only widens as the compression gets stronger. Right: DMS requires an order of magnitude less data to train than DMC. This was also observed for Qwen 2.5 R1 models with 1.5B, 7B, and 32B parameter scales. different training token budgets. Crucially, DMS achieves higher scores than DMC while using 8 fewer training tokens. In practice, the reasoning models described in Section 5.1 were trained with 60 less training data,4 achieving CR 4 within 300 training steps and CR 8 within 700 steps. Finally, we measured how the CR varies for different lengths of the sequences generated through DMS (Figure 6 left). The resulting pattern closely matches that reported in Nawrot et al. (2024). The model sparsifies less than the target CR in early parts of sequence, but even more aggressively than specified beyond 10K tokens. This behavior stems from the training objective and from the tendency of the conditional entropy rate of natural-language text to decrease as the context length grows. In Figure 6 (right), we also observe that early layers are compressed to smaller degree than later layers. 6. Related Work KV cache size reduction The challenge of KV cache reduction has garnered significant interest in recent years, with approaches falling into three main categories: attention sparsification, quantization, and decomposition. In addition to the sparse attention baselines considered in Section 2.2, Landmark Attention (Mohtashami and Jaggi, 2023) and Native Sparse Attention (Yuan et al., 2025) create representations for each KV cache chunk and retrieve only the most important chunks for attention computation, effectively reducing the amount of data transferred from the device HBM memory. Compared to these methods, DMS not only accelerates inference but also reduces memory load and allows for dynamically selecting different compression ratios across layers and heads based on the input. Moreover, DMS improves on other retrofitting methods, such as DMC (Nawrot et al., 2024), both in terms of data efficiency and downstream accuracy. Another strategy for KV cache size reduction is quantization, exemplified by methods such as KIVI (Liu et al., 2024b) and KVQuant (Hooper et al., 2024), which quantize keys per channel and values per token. Finally, KV cache reduction can be achieved via SVD-based decomposition. LoRC (Zhang et al., 2024) directly reduces the ranks of key and value matrices, whereas Eigen Attention (Saxena et al., 2024) moves the attention computation into truncated space induced by SVD. While being less expressive than DMS as they assume uniform compression, both quantization and decomposition are orthogonal to DMS and can be potentially combined with it to further improve efficiency. 4DMC was reported to require 44K training steps to reach CR8, with performance deteriorating when the amount of data is halved (Nawrot et al., 2024). Inference-time scaling Research on inferencetime scaling has so far mostly assumed an equivalence 9 Inference-Time Hyper-Scaling with KV Cache Compression Figure 6 Left: The measured compression ratio for Qwen-R1 7B, trained with DMS CR 4, while processing AIME 24, MATH 500, and GPQA Diamond problem instances. Right: Average per-head compression learned by the model, as percentage of retained tokens sorted for every layer. between compute budget and generated tokens, in terms of sequence length or parallel samples (Brown et al., 2024; Zhang et al., 2023b; Wang et al., 2023). This budget can be allocated adaptively to the complexity of the task (Snell et al., 2024) or forced to meet an amount pre-defined by the user (Muennighoff et al., 2025). To the best of our knowledge, our work is the first to fully disentangle generated tokens from the effective compute (runtime and peak memory load) when reasoning in the discrete language space. In fact, we show how KV cache compression methods can effectively expand the token budget for the same compute budget. separate family of strategies are based on latent space reasoning (Geiping et al., 2025), which add recurrent block on top of Transformer LLMs; however, this effectively requires separate architecture rather than boosting existing LLMs, and it remains unclear whether these scale similarly to reasoning in the discrete token space. While in this work we opt for verifier-free scaling strategies, adopting the recommendations of Wang et al. (2025b), inference-time scaling can rely on process reward models (PRMs) to verify intermediate reasoning steps. This allows effective self-critique loops and re-ranking candidate solutions (Uesato et al., 2022; Lightman et al., 2024; Liang et al., 2024). Nonetheless, we remark that hyper-scaling can be extended to PRM strategies, too. In particular, the verifiers complexity is quadratic in the sequence length; to complement the benefits of KV cache compression of the LLM, the PRM would need to be accelerated by prefilling-time sparse attention methods, such as MInference (Jiang et al., 2024). We leave this possible direction to future work. 7. Conclusions We introduce inference-time hyper-scaling: by compressing the keyvalue cache of Transformer LLMs via sparse attention, we improve downstream reasoning accuracy by enabling longer token sequences or more parallel sequences at the same compute budgetin terms of runtime or memorycompared to the original LLM. fundamental requirement of inferencetime hyper-scaling is to increase efficiency without sacrificing accuracy. To achieve this, we propose Dynamic Memory Sparsification (DMS), novel, trainable KV cache reduction method that delays eviction decisions, while remaining highly data-efficient. Empirically, we observe large gains on benchmarks involving advanced math, scientific problems, and coding, demonstrating the effectiveness of hyper-scaling. Overall, our approach provides an inexpensive strategy for converting LLMs into more effective reasoners, pushing inference-time scaling to new frontiers."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors would like to thank Marcin Chochowski, David Tarjan, and Andrzej SuÅ‚ecki for helpful discussions, Szymon Migacz for his assistance with the computing infrastructure, as well as PrzemysÅ‚aw Strzelczyk, Krzysztof Pawelec, Daniel Korzekwa, Alex FitFlorea, and Michael Lightstone for support in releasing this paper."
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico LebrÃ³n, and Sumit Sanghai. 2023. GQA: Training generalized multiquery transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, JoÃ£o G.M. AraÃºjo, Alex Vitvitskyi, Razvan Pascanu, and Petar VeliÄkoviÄ‡. 2024. Transformers need glasses! Information oversquashing in language tasks. In Advances in Neural Information Processing Systems, volume 37, pages 9811198142. Edward Beeching, Lewis Tunstall, and Sasha Rush. 2024. Scaling test-time compute with open models. 10 Inference-Time Hyper-Scaling with KV Cache Compression Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher RÃ©, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. Preprint, arXiv:2407.21787. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. DeepseekR1: Incentivizing reasoning capability in LLMs via reinforcement learning. Preprint, arXiv:2501.12948. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. Preprint, arXiv:1803.05457. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In International Conference on Learning Representations. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. DeepSeek-AI. 2024. DeepSeek-V2: strong, economical, and efficient mixture-of-experts language model. Preprint, arXiv:2405.04434. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. 2024. Flex Attention: programming model for generating optimized attention kernels. Preprint, arXiv:2412.05496. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. 2023. AlphaZerolike tree-search can guide large language model decoding and training. In NeurIPS 2023 Foundation Models for Decision Making Workshop. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. The language model evaluation harness. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. 2025. Scaling up test-time compute with latent reasoning: recurrent depth approach. Preprint, arXiv:2502.05171. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. 2024. The Llama 3 herd of models. Preprint, arXiv:2407.21783. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in neural network. Preprint, arXiv:1503.02531. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. Kvquant: Towards 10 million context length llm inference with kv cache quantization. In Advances in Neural Information Processing Systems, volume 37, pages 12701303. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2025. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In Advances in Neural Information Processing Systems, volume 37, pages 5248152515. G. Kamradt. 2023. LLMTest_NeedleInAHaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack. 11 Inference-Time Hyper-Scaling with KV Cache Compression Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles. Hynek KydlÃ­Äek and Greg Gandenberger. 2025. Mathverify. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making large language models better reasoners with step-aware verifier. Preprint, arXiv:2206.02336. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. 2024. SnapKV: LLM knows what you are looking for before generation. In Advances in Neural Information Processing Systems, volume 37, pages 2294722970. Zhenwen Liang, Ye Liu, Tong Niu, Xiangliang Zhang, Yingbo Zhou, and Semih Yavuz. 2024. Improving LLM reasoning through scaling inference computation with collaborative verification. Preprint, arXiv:2410.05318. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024a. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024b. KIVI: tuning-free asymmetric 2bit quantization for KV cache. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 3233232344. Christos Louizos, Max Welling, and Diederik P. Kingma. 2018. Learning sparse neural networks through ğ¿0 regularization. In International Conference on Learning Representations. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36, pages 4653446594. Benjamin Minixhofer, Ivan VuliÄ‡, and Edoardo Maria Cross-tokenizer distillation via Preprint, Ponti. 2025. approximate likelihood matching. arXiv:2503.20083. Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark Attention: Random-access infinite context length for transformers. In Advances in neural information processing systems. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. Piotr Nawrot, Adrian ÅaÅ„cucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. 2024. Dynamic memory compression: Retrofitting LLMs for acIn Forty-first International celerated inference. Conference on Machine Learning. Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, and Edoardo Ponti. 2025. The sparse frontier: Sparse attention trade-offs in transformer LLMs. Preprint, arXiv:2504.17768. NVIDIA. 2024. Megatron-LM: Ongoing research training transformer models at scale. OpenAI, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, et al. 2024. OpenAI o1 system card. Preprint, arXiv:2412.16720. Matanel Oren, Michael Hassid, Nir Yarden, Yossi Adi, and Roy Schwartz. 2024. Transformers are multistate RNNs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1872418741, Miami, Florida, USA. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level Google-proof Q&A benchmark. In First Conference on Language Modeling. Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy. 2024. Eigen attention: Attention in low-rank space for KV cache compression. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1533215344, Miami, Florida, USA. 12 Inference-Time Hyper-Scaling with KV Cache Compression David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, et al. 2024. LLM pruning and distillation in practice: The Minitron approach. Preprint, arXiv:2408.11796. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. 2024. QUEST: Query-aware sparsity for efficient long-context LLM inference. In Proceedings of the International Conference on Machine Learning (ICML). Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcome-based feedback. Preprint, arXiv:2211.14275. Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, and Haifeng Wang. 2025a. Flashmask: Efficient and rich mask extension of flashattention. In The Thirteenth International Conference on Learning Representations. Junlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athiwaratkun, Qingyang Wu, Jue Wang, Shuaiwen Leon Song, Ce Zhang, Bhuwan Dhingra, and James Zou. 2025b. Think deep, think fast: Investigating efficiency of verifier-free inference-time-scaling methods. Preprint, arXiv:2504.14047. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, volume 36, pages 1180911822. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. Native sparse attention: Hardware-aligned and natively trainable sparse attention. Preprint, arXiv:2502.11089. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, and Yelong Shen. 2024. LoRC: Low-rank compression for LLMs KV cache with progressive compression strategy. In Workshop on Machine Learning and Compression, NeurIPS 2024. 13 Inference-Time Hyper-Scaling with KV Cache Compression Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher RÃ©, Clark Barrett, Zhangyang Atlas Wang, and Beidi Chen. 2023a. H2O: Heavy-hitter oracle for efficient generative inference of large language models. In Advances in Neural Information Processing Systems 36. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations. Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, and Xin Eric Wang. 2025. The hidden risks of large reasoning models: safety assessment of R1. Preprint, arXiv:2502.12659. 14 Inference-Time Hyper-Scaling with KV Cache Compression A. Limitations, Future Work and Impact Larger Model Sizes, Longer Contexts, and Higher Compression Ratios. In this work, we focus on models ranging from 1B to 32B parameters, context lengths up to 32K tokens, and compression ratios up to 8. Exploring even larger models, longer contexts, and higher compression ratios remains an exciting avenue for future research. Integration with Other Efficient Attention Mechanisms. We demonstrated DMS with the standard multi-head attention mechanism used in Transformer-based models such as Llama and Qwen 2.5. Extending DMS to alternative attention variants, such as Multi-head Latent Attention (DeepSeek-AI, 2024) represents promising direction for future investigation. Moreover, DMS compresses the KV cache, whereas Quest (Tang et al., 2024) selectively retrieves cache items. Hence, the two are orthogonal and could be combined to further push the Pareto frontier for inference time scaling. Broader Impact. Our approach does not introduce novel risks; however, it may amplify existing concerns associated with large-scale reasoning models. For detailed analysis of these risks, we refer readers to Zhou et al. (2025). B. Additional Details for Retrofitting DMS Implementation Unlike (Nawrot et al., 2024), which extracts ğ›¼ğ‘¡ from key representations affecting all query heads in group, we borrow the first neuron from the first query head in each query group and use it to extract ğ›¼ğ‘¡, eliminating the need for additional parameters while minimizing the impact on attention computation. This requires short continued training, during which we gradually zero out the first dimension of the first query head in each group: qğ‘¡,first[0] qğ‘¡,first[0] , where ğ‘¡ denotes the current training step and ğ‘›ğ‘¡ = 2000. After this initial stage, the models are ready for the main DMS retrofitting phase, where they learn to dynamically evict tokens. After we extract ğ›¼ğ‘¡ from the first query head, we set qğ‘¡,first[0] = 0 to avoid ğ›¼ğ‘¡ influence on the result of attention calculation, while leaving other query heads in the group unaffected. 1 ğ‘¡ ğ‘›ğ‘¡ ( ) Training Configuration We use batch size of 1024 following the original Llama recipe (Touvron et al., 2023). Context lengths are set to 4096 tokens for Llama 3.2 1B Instruct and Llama 2 7B models, and 8192 tokens for Llama 3.1 8B and R1-distilled models to accommodate the longer sequences required by AIME and MATH 500 benchmarks. Default DMS Configuration Unless otherwise specified, all DMS models use delayed eviction with sliding window of 256 tokens and increment the compression ratio by 1 every 100 training steps. We denote different DMS variants using the notation DMSwin=ğ‘¦, where ğ‘¥ represents the sliding window size. Unlike DMC (Nawrot et al., 2024), we omit the third fine-tuning phase (training with fixed compression ratio) as it provided negligible benefits for DMS. Infrastructure and Computational Requirements All models are trained on NVIDIA H100 GPUs using Megatron-LM (NVIDIA, 2024) in bfloat16 precision, with optimizer states stored in fp32. For the 32B Qwen-R1 model, each retrofitting step (batch size 256, context length 8192) requires approximately 18 seconds on 256 H100 GPUs using tensor parallelism 8 and pipeline parallelism 2. Model checkpoints, including optimizer states, occupy approximately 430GB of storage. The complete project consumed roughly 200K GPU hours, including preliminary experiments. C. Training Data For the Qwen-R1 models, we utilize logit distillation leveraging the OpenR1-Math-220k dataset. This dataset contains high-quality reasoning traces sampled from DeepSeek R1. To further enhance data quality, we apply 15 Inference-Time Hyper-Scaling with KV Cache Compression filtering step using Math-Verify (KydlÃ­Äek and Gandenberger, 2025), retaining only traces resulting in correct mathematical solutions. For the Llama 3.2 1B Instruct model, the training corpus comprises two main components: (1) carefully curated set of programming language examples covering languages such as Python, C, and C++, and (2) synthetic data generated by prompting the model. In particular, we utilize the Llama 3.2 1B Instruct model itself to produce completions for the one-dimensional linear algebra subset of the DeepMind mathematics dataset (Saxton et al., 2019), which follows the structured format: Task format in one-dimensional linear algebra Solve aX + = cX + for X. Llama 3.2 1B prompt for generating responses <start_header_id>system<end_header_id> Cutting Knowledge Date: December 2023 Today Date: 23 July 2024 You are helpful assistant.<eot_id><start_header_id>user<end_header_id> Given the following problem, reason and give final answer to the problem. Problem: Solve 5*b - 2355 = -50*b - 2740 for b. Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.<eot_id><start_header_id>assistant<end_header_id> In contrast with the data mixture for Qwen-R1 models, we do not perform correctness filtering on this synthetic, model-generated dataset. D. Additional Downstream Evaluations for DMC and DMS In Table 2 we show that DMS can extrapolate beyond the retrofitting context length of 4K, whereas DMC may fail to do so. In Table 3, we show comparison between Vanilla model, DMS, Quest, and DMC on Llama 2 7B. E. Downstream Results Significance We provide further analysis regarding the statistical significance and robustness of our experimental results. Specifically, we report standard deviations for the Llama 3.2 1B Instruct models in Table 4, and quantify the average Pareto improvement in Tables 5 and 6. To precisely measure the Pareto improvement, we extract Pareto frontiers for DMS, the best KV cache reduction baseline, and the vanilla baseline from Figures 3 and 4. Then, for each task and model size, we identify the largest common budget interval ğ¼ shared by each pair of methods and B, and compute the average improvement as: ğ‘¥ğ¼ (ğ´(ğ‘¥) ğµ(ğ‘¥)) ğ‘‘ğ‘¥ ğ¼ where ğ´(ğ‘¥) and ğµ(ğ‘¥) denote the best accuracy achieved by method and B, respectively, at budget ğ‘¥. For budget values not explicitly measured, we employ linear interpolation. 16 Inference-Time Hyper-Scaling with KV Cache Compression Table 2 Needle in the Haystack and Variable Tracking results for 1B parameter Llama 3.2 Instruct model. We note that in contrast to DMC, DMS can extrapolate beyond the retrofitting context length. Note that on the heavily compressible Variable Tracking task, DMS beats even the vanilla model. Method/Task Context NIAH 4K 3K 8K 3K Vanilla 99.4 96.4 97.2 61.4 TOVA H2O Quest DMC DMSwin= TOVA H2O Quest DMC DMSwin=16 TOVA H2O Quest DMC DMSwin=16 CR2 65.2 34.0 95.8 0.0 97.8 CR3 40.2 17.2 95.6 1.8 93. CR4 28.0 13.4 95.8 0.0 96.8 75.0 37.0 97.4 0.0 99.4 41.6 19.8 97.0 0.0 24.2 26.4 12.8 97.6 0.0 12.2 62.8 29.0 99.2 99.0 99. 25.8 16.6 99.0 99.0 99.2 16.8 9.4 98.4 97.0 99.4 56.0 25.6 60.0 62.4 72.0 38.4 15.6 60.4 46.4 76.2 31.4 11.8 57.4 48.6 74.8 VT 4K 55.8 56.2 27.4 53.0 0.0 63.2 45.2 17.6 50.4 0.2 69.2 33.8 12.6 49.6 4.0 67.6 8K 41. 49.8 21.4 36.4 0.0 56.0 40.6 13.4 31.8 1.2 58.8 30.2 11.0 32.4 0.8 57.2 Table 3 Results for base Llama 2 7B parameter models. Both DMS and DMC were trained using LM-loss without logit distillation. Since these models are not instruction-tuned, we evaluate with 8-shot prompting on GSM8K, 5-shot on MMLU, 1-shot Needle in Haystack, and zero-shot on ARC-Challenge and HellaSwag. (Nawrot et al., 2024). Method ARC-C GSM8K HS MMLU NIAH Vanilla 45.6 14.9 75.5 45.4 100. DMSwin=16 Quest DMC DMSwin=16 Quest DMC 45.8 45.6 46.2 46.2 45.6 44.7 CR4 14.2 14.5 12. 76.0 75.5 76.3 CR8 10.5 11.6 10.0 76.4 75.5 75.4 43.7 45.4 43.9 40.2 45.4 41. 100.0 100.0 100.0 60.0 100.0 100.0 F. Evaluation Details F.1. Implementation of TOVA, H2O, Quest, and DMC For TOVA (Oren et al., 2024), H2O (Zhang et al., 2023a), and Quest (Tang et al., 2024), we calculate the KV-budget by summing the input length and the maximum generation length, then dividing by the Inference-Time Hyper-Scaling with KV Cache Compression Table 4 Results from Table 1 expanded with standard deviation as computed by Language Model Evaluation Harness (Gao et al., 2024). Method ARC-C GPQA GSM8K HS Vanilla 31.21.4 25.02.0 44.91.4 43.40.5 DMSwin=16 TOVA H2O Quest 31.31.4 29.61.3 31.11.4 31.21. DMSwin=16 TOVA H2O Quest 31.11.4 30.01.3 31.21.4 31.21.4 DMSwin=16 TOVA H2O Quest 31.11.4 29.01.3 27.51.3 31.21.4 CR2 25.72.1 25.22.1 26.82.1 25.02. CR3 24.62.0 23.72.0 24.32.0 25.02.0 CR4 24.32.0 23.72.0 23.72.0 25.02.0 46.61.4 45.01.4 44.01.4 45.11.4 43.30.5 42.80.5 42.90.5 43.40. 45.51.4 40.11.4 32.91.3 44.71.4 43.30.5 42.50.5 42.10.5 43.40.5 41.01.4 20.21.1 14.71.0 39.91.3 43.40.5 41.80.5 41.30.5 43.40.5 Table 5 Compute-Accuracy Pareto frontier difference. We use linear interpolation for the unknown values of the frontier. NA denotes that the projections of the Pareto frontiers on the budget axis are disjoint. Method AIME 24 7B 1.5B 32B 1.5B MATH 500 7B 32B 1.5B 7B 32B 1.5B 7B LiveCodeBench 32B GPQA DMS vs Vanilla 10.6 Quest vs Vanilla 6.8 DMS vs Quest 18.8 15.0 3.1 13. 4.2 9.1 1.6 2.0 1.8 0.6 NA 1.0 7.6 4.1 4.8 NA NA 2.3 2.6 6. 2.1 1.4 NA NA NA 7.3 2.5 4.9 7.9 5. 3.4 9.6 3.8 5.4 Table 6 Memory-Accuracy Pareto frontier difference. We use linear interpolation for the unknown values of the frontier. NA denotes that the projections of the Pareto frontiers on the budget axis are disjoint. Method AIME 24 7B 1.5B 32B 1.5B MATH 500 7B 32B GPQA 7B 1.5B LiveCodeBench 32B 32B 1.5B 7B DMS vs Vanilla TOVA vs Vanilla 17.3 15.7 5.3 0.2 10.6 2.6 3.3 0.5 1.3 1. 1.5 1.8 4.2 4.9 1.1 2.1 DMS vs TOVA 9.6 15.6 5. 2.3 2.0 0.1 5.6 6.5 7.6 2. 5.7 7.4 3.8 4.0 8.4 3.2 6.0 12.0 4. 7.3 compression ratio. For H2O, the KV-budget is evenly split between the recent cache and the heavy-hitter cache. During evaluation, memory-optimising methods such as TOVA and H2O first perform standard prefill phase until the KV-budget is reached and subsequently switch to their respective memory-efficient mechanisms. Quest (Tang et al., 2024), unlike TOVA, H2O, DMC, and DMS, does not reduce the KV cache memory footprint. Thus, following the authors recommendations, we permit Quest to perform prefilling using full dense attention and set the block size to max(16, 2cr). This configuration provides Quest with an advantage 18 Inference-Time Hyper-Scaling with KV Cache Compression Table 7 Results from Figure 3 at specified max Length and Width=1 configurations. Those points allow for direct comparison with Vanilla model. Task Model Size Length Vanilla DMS CR4 Quest CR4 AIME 24 Qwen-R MATH 500 Qwen-R1 GPQA Diamond Qwen-R1 LiveCodeBench Qwen-R1 1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B 32k 32k 32k 32k 32k 32k 32k 32k 32k 16k 16k 16k 30.0 53.3 70.0 84.8 94.0 94.8 36.5 51.5 63.1 17.3 35.9 57.0 30.0 53.3 73. 84.0 92.8 94.6 36.9 48.5 63.6 17.0 34.4 54.8 26.7 55.5 73.3 84.1 93.2 94.6 37.0 50.2 65. 15.8 33.1 54.5 Table 8 Results from Figure 4 at specified max Length and Width=1 configurations. Those points allow for direct comparison with Vanilla model. Task Model Size CTX Vanilla DMS CR4 TOVA CR AIME 24 Qwen-R1 MATH 500 Qwen-R1 GPQA Diamond Qwen-R1 LiveCodeBench Qwen-R1 1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B 1.5B 7B 32B 32k 32k 32k 32k 32k 32k 32k 32k 32k 16k 16k 16k 30.0 53.3 70.0 84.8 94.0 94.8 36.5 51.5 63. 17.3 35.9 57.0 30.0 53.3 73.3 84.0 92.8 94.6 36.9 48.5 63.6 17.0 34.4 54.8 30.0 46.7 70. 84.3 91.8 95.2 34.3 47.5 63.1 14.9 30.7 51.1 over the other methods. Additionally, we employ separate top-k for each query head, which can result in an increased number of memory transfers for Quest compared to DMS, DMC, TOVA, and H2O. However, the computational cost remains similar. We use this approach as Quest was originally designed for models without GQA, and we wanted to avoid custom modification that could potentially degrade the performance. In plots regarding kv-cache memory reads we calculate the total number of different blocks retrieved from single key-head. That is we assume optimal implementation that makes use of topk intersections between query heads and retrieves each block only once. For DMC, we follow the implementation described in the original paper (Nawrot et al., 2024). F.2. Downstream Tasks We evaluate all downstream tasks in zero-shot setting, unless stated otherwise. For GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2021a), ARC-Challenge (Clark et al., 2018), and HellaSwag (Zellers et al., 2019), we use the Language Model Evaluation Harness (Gao et al., 2024), 19 Inference-Time Hyper-Scaling with KV Cache Compression Table 9 Results from Figure Figure 3 comparing DMS wit CR8 to Vanilla (CR1) on specified max Length and Width=1 configurations. Task Model Size CTX Vanilla DMS CR8 AIME 24 Qwen-R1 MATH Qwen-R1 GPQA Diamond Qwen-R1 LiveCodeBench Qwen-R1 1.5B 7B 1.5B 7B 1.5B 7B 1.5B 7B 32k 32k 32k 32k 32k 32k 16k 16k 30.0 53.3 84.8 94.0 36.5 51.5 17.3 35.9 23.3 50.0 80.0 93. 31.3 46.5 16.1 33.4 version 0.4.3. For Needle in Haystack (NIAH) (Kamradt, 2023) and Variable Tracking (VT), we adopt the evaluation implementation provided by RULER (Hsieh et al., 2024) and use the context length defined in the retrofitting procedure. For NIAH, we utilize the essay version with single needle, whereas for VT, we utilize the version with 40 variable chains and 0 hops, filled with repeating text. For AIME24,5 GPQA (Rein et al., 2024), and MATH 500 (Lightman et al., 2024), we evaluate models using the Search and Learn framework (version 0.1.0) (Snell et al., 2024; Beeching et al., 2024), with math-parsing functionality derived from MathVerify (version 1.0.0) (KydlÃ­Äek and Gandenberger, 2025). For LiveCodeBench we utilize the tasks from 2024-08-01 till 2025-01-31. For few-shot tasks from Language Model Evaluation Harness we directly utilize the framework to provide the few-shot examples. For RULER (Hsieh et al., 2024), we use the example generator to sample few-shot examples. Below we present remaining prompts that were used during the evaluation, except the prompts to base models, which were set to unaltered task input, and prompts for zero-shot evaluation of instruction tuned models, which were set to task inputs wrapped with HuggingFace tokenizer chat template. For GSM8K zero-shot evaluation, we adopt the prompt from Meta. GSM8K zero-shot prompt <start_header_id>system<end_header_id> Cutting Knowledge Date: December 2023 Today Date: 23 July 2024 You are helpful assistant.<eot_id><start_header_id>user<end_header_id> Given the following problem, reason and give final answer to the problem. Problem: ___problem_text___ Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.<eot_id><start_header_id>assistant<end_header_id> For Qwen-R1 AIME 24, MATH 500 and GPQA we adopt the prompts from Open-R1 repository7. 5https://huggingface.co/datasets/HuggingFaceH4/aime_2024 6https://huggingface.co/docs/transformers/en/chat_templating 7https://github.com/huggingface/open-r1 20 Inference-Time Hyper-Scaling with KV Cache Compression AIME 24 and MATH 500 prompts <User>Solve the following math problem efficiently and clearly. The last line of your response should be of the following format: 'Therefore, the final answer is: $boxed{ANSWER}$. hope it is correct' (without quotes) where ANSWER is just the final number or expression that solves the problem. Think step by step before answering. ___problem_text___<Assistant><think> GPQA Diamond prompt <User>Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering. ___problem_text___<Assistant><think> For coding tasks we utilize the following prompt adopted from LiveCodeBench(Jain et al., 2025) DeepSeek-R1 setting: LiveCodeBench prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.<User>You will be given question (problem specification) and will generate correct Python program that matches the specification and passes all tests. Question: ___problem_text___ <Assistant><think> 21 Inference-Time Hyper-Scaling with KV Cache Compression G. Influence of KV Cache on Inference Latency In this section, we provide simplified analysis estimating the proportion of inference latency introduced by reading from the keyvalue cache to the entire latency of the step, during single auto-regressive inference step of an LLM on GPU. Our calculations are based on the architecture of the Llama 3 model family. Specifically, we derive sample equations for Llama 3.1 8B, parameters of which are listed below. Parameter Value Description ğ‘› ğ‘‘ ğ‘‘ff ğ‘‘kv ğ‘‰ 32 Number of layers 4096 Hidden dimension 14336 Internal dimension of the MLP layers 1024 Dimension of the Key/Value sequences 128256 Vocabulary size The estimates can be expressed in terms of batch size ğµ and sequence length ğ¿, which determine the number of tokens in the KV cache. The number of floating-point operations (FLOPs) can be approximated as FLOPS(ğµ, ğ¿) ğ‘›ğµ (6ğ‘‘ğ‘‘ff + 4ğ‘‘2 + 4ğ‘‘ğ‘‘kv + 4ğ‘‘ğ¿) + 2ğµğ‘‘ğ‘‰. (2) This calculation considers only major matrix-vector multiplications (assuming two FLOPs per multiplyaccumulate operation), omitting minor operations such as normalization and pointwise non-linearities. Similarly, we estimate the number of reads from the High Bandwidth Memory as: Reads(ğµ, ğ¿) ğ‘› (6ğ‘‘ğ‘‘ff + 4ğ‘‘2 + 4ğ‘‘ğ‘‘ff + 4ğµğ¿ğ‘‘kv ) + 2ğ‘‘ğ‘‰, (3) assuming 2 bytes per parameter (16-bit precision). Note that only the KV cache (4ğ‘›ğµğ¿ğ‘‘kv) scales with batch size and sequence length. As sanity check, we confirm that Reads(1, 0)/2 7.5ğµ approximate the models parameter count (without 0.5ğµ for the input embedding table, which does not have to be entirely read during an inference step). Finally, the approximations for Llama 3.1 8B have the following form: FLOPS(ğµ, ğ¿) 1.45 109ğµ + 5.24 105ğµğ¿ Reads(ğµ, ğ¿) 1.50 1010 + 1.31 105ğµğ¿. (4) (5) For the remaining calculations, we use the peak performance values for NVIDIA H100 SXM (https: //www.nvidia.com/en-us/data-center/h100/) for 16-bit calculations without 2:4 sparsity: BFLOAT16 Tensor Core performance GPU Memory bandwith 989.5 TFLOPS 3.35 TB/s Since memory reads are significantly slower than computations, the latency contribution from KV cache reads (1.31 105ğµğ¿ term) dominates at larger sequence lengths and batch sizes. Thus, KV cache size is critical factor in inference latency for long sequences. The inference latency per step can be approximated as max ( FLOPS(ğµ, ğ¿) 989.5 TFLOPS , Reads(ğµ, ğ¿) 3.35 TB/s ) , (6) assuming ideal overlap between computation and memory operations. Approximating KV cache reads as 4ğ‘›ğµğ¿ğ‘‘kv and following the same calculations for other Llama and Qwen models, we visualize the fraction of latency contributed by KV cache reads to the latency of entire inference steps (Figure 7). 22 Inference-Time Hyper-Scaling with KV Cache Compression Figure 7 Percentage of total latency attributed to KV cache reads. Those reads clearly dominate latency as batch size and sequence length increase. When the KV cache is compressed (CR 4 and 8), more tokens can be accommodated before the latency of reading the KV cache becomes an issue."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of Edinburgh"
    ]
}