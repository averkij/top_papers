{
    "paper_title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum",
    "authors": [
        "Gaotang Li",
        "Ruizhong Qiu",
        "Xiusi Chen",
        "Heng Ji",
        "Hanghang Tong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 6 2 5 0 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "BEYOND LOG LIKELIHOOD: PROBABILITY-BASED OBJECTIVES FOR SUPERVISED FINE-TUNING ACROSS THE MODEL CAPABILITY CONTINUUM Gaotang Li1 Ruizhong Qiu1 Xiusi Chen1 Heng Ji1 Hanghang Tong1 1University of Illinois Urbana-Champaign"
        },
        {
            "title": "ABSTRACT",
            "content": "Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., p, p10, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood."
        },
        {
            "title": "INTRODUCTION",
            "content": "Supervised fine-tuning (SFT) has become standard approach for post-training large language models (LLMs), widely used to elicit and strengthen their capabilities (Zhang et al., 2023; Chung et al., 2024). Despite its popularity, many existing studies find that SFT often exhibits limited generalization (Ouyang et al., 2022; Chu et al., 2025). Nevertheless, this limitation may not arise from the SFT paradigm itself. Instead, we find that it may stem from its default training objective: negative log likelihood (NLL, log p). As motivating case study, we generalize NLL into parametrized family of learning objectives of the form fα(p) := pα1 α , which includes NLL as special case (fα(p) log as α 0). We surprisingly find that other objectives significantly outperform NLL on some tasks, as shown in Tab. 1. This unexpected observation motivates us to fundamentally revisit the training objective of SFT. While NLL has been shown to be optimal in classical learning theory when training from scratch on small-scale classification tasks (Cox, 1958; Zhang, 2004; Bartlett et al., 2006), LLM post-training operates in fundamentally different paradigm and essentially degrades the optimality of NLL. Post-training begins with pretrained model (called the base model) that already encodes task-relevant priors, and typically involves long chain-of-thought supervision spanning thousands of tokens that may be noisy. Requiring the pretrained model to replicate every token verbatim can hinder generalization. Table 1: Other objectives can significantly outperform NLL. α 0 1 10 Objective Accuracy log 1 (1 p10)/10 17.00 32.75 31.50 To this end, we conduct comprehensive study to demystify which scenarios suit NLL and which suit other objectives. Our study uncovers critical dimension that governs the behavior of different Equal Contribution"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The model capability continuum of SFT objectives in Post-Training. At the modelstrong (MS) end, where base models already encode extensive priors (e.g., Llama 3 reports 25% math pretraining tokens (Grattafiori et al., 2024)), prior-leaning objectives that downweight lowprobability tokens (e.g., p, p10, or thresholded variants) consistently outperform NLL by up to 16%. At the model-weak (MW) end, where no useful priors exist (e.g., no figfont puzzles in pretraining data), the standard NLL dominates. In the model-intermediate (MI) region (e.g., medical reasoning, where models rely on partial world knowledge), the gap between objectives narrows and no single choice consistently prevails. This continuum highlights how the effectiveness of an SFT objective depends critically on the capability of the base model. objectives: the model-capability continuum. This continuum reflects the strength of prior signals inherited from pretraining: some domains (e.g., math with abundant pretraining tokens) align well with the models priors, while others (e.g., novel puzzles with no pretraining exposure) do not, as illustrated in Fig. 1. Accordingly, the effectiveness of learning objective depends on prior strength: prior-leaning objectives excel when priors are reliable, whereas prior-averse ones remain necessary when priors are weak. We validate this perspective through extensive experiments spanning seven model backbones, fourteen benchmarks, and three domains. Our results reveal clear continuum in how objectives behave: at the model-strong end, where base models already provide reliable priors, probability-based objectives that downweight low-probability tokens (e.g., p, p10, or thresholded variants) consistently outperform NLL. At the model-weak end, where priors are misaligned with the data, NLL remains dominant by forcing the model to learn broadly from all tokens. In the intermediate region, the gap narrows and no single objective prevails. Further empirical analyses show that convexity and concavity of the learning objective, as proxy for the degree to which model priors are respected, has opposite effects across the continuum. Likelihood estimation on the training set, as proxy for empirical risk minimization, exhibits the same inversion. To elucidate these findings, we provide theoretical underpinnings that characterize when and why different objectives outperform others. We characterize sufficient condition showing that more prior-leaning (e.g., p) achieve greater loss reduction than NLL in the model-strong end in gradient flow. The opposite holds in the model-weak end, where NLL achieves larger reductions. This theoretical characterization mirrors our empirical results and provides principled explanation of how objective form and model capability interact."
        },
        {
            "title": "2 A UNIFIED CATEGORIZATION OF SFT TRAINING OBJECTIVES",
            "content": "Language Model Post-Training. We focus on the post-training stage of large language models (LLMs). Let pθ denote pretrained base model that has already undergone large-scale pretraining and accumulated extensive world knowledge. Such models typically produce predictions that are reasonably well-calibrated (Zhu et al., 2023; Xie et al., 2024), and their outputs encode task-relevant priors derived from pretraining corpora. Standard Supervised Fine-Tuning. We consider supervised fine-tuning (SFT) on dataset of input-output pairs (x, y), where = (y1, . . . , yN ) denotes the target sequence. The model defines token-level conditionals pθ(yt y<t, x). At decoding step t, let zt RV denote the logits over the vocabulary, pt = softmax(zt), and pt,i = softmax(zt)i. For brevity, write = yt, and denote by δi,y the Kronecker delta. In standard SFT, the training objective is to minimize the negative log likelihood, equivalently the cross-entropy loss, over the dataset:"
        },
        {
            "title": "Preprint",
            "content": "Llog(p)(θ) = E(x,y)T (cid:2) log pθ(y x)(cid:3) = E(x,y)T log pθ(yt y<t, x) . (1) (cid:35) (cid:34) (cid:88) t=1 General Family of Probability-Based Objectives. We now extend beyond log likelihood by considering broader family of objectives. For any differentiable and nonincreasing function : [0, 1] R, we define Lf (p)(θ) = E(x,y)T (cid:2)f (cid:0)pθ(y x)(cid:1)(cid:3) = E(x,y)T (cid:0)pθ(yt y<t, x)(cid:1) (cid:34) (cid:88)"
        },
        {
            "title": "One useful general instance of f is given by",
            "content": "t=1 α(p) = 1 pα α . (cid:35) . (2) (3) As α 0, it reduces to α(p) log(p) (NLL). When α = 1, it yields the plain-p objective α(p) = 1 p, which corresponds to maximizing the expected average prediction accuracy. More generally, the function is concave when α 1 and convex when 0 α 1. Prior-learning versus Prior-averse Objectives. The key distinction among these objectives lies in the form of their gradients with respect to the correct logit class, which governs the resulting learning dynamics. Lemma 1 (Gradient Shape). Let : [0, 1] be differentiable and nonincreasing. Then the gradient of Eq. 2 with respect to the logits at step is (cid:1) (cid:0)Lf zt,i = sf (pt,y) (cid:0)δi,y pt,i (cid:1), where sf (p) (p) 0, δiy = 1{i = y}. In particular, for the correct class = y, (cid:1) (cid:0)Lf zt,y = sf (pt,y) (1 pt,y) = Wf (pt,y), Wf (p) (p) (1 p). Proposition 1 (Convex versus Concave Objectives). Let 2[0, 1] with (p) < 0 for all (0, 1). Define Wf (p) = (p) p(1 p). Then if is concave, any maximizer of Wf lies in the interval [ 1 2 , 1]; if is convex, any maximizer of Wf lies in the interval [0, 1 2 ]. In other words, convex objectives emphasize gradient contributions from low-probability tokens, while concave objectives shift the gradient mass toward high-probability tokens. The weighting term Wf (p) determines how much learning signal each token contributes relative to the models prior belief. For the parametric family in Eq. 3, we have Wf (p) = pα(1 p). As α 0 (NLL), this reduces to Wf (p) (1 p), which strongly emphasizes lowprobability tokens. When α 1 (f (p) = 1 p), the gradient signal from low-probability tokens quickly diminishes. For special case (p) = log(1 p), we obtain Wf (p) = p, which exhibits the opposite trend of log(p) by emphasizing high-probability tokens. Fig. 2 visualizes these gradient shapes Wf (p) for different objectives: the dot marks the maximizer of each function, and the dashed line at = 0.5 serves as reference point separating objectives that favor lowversus high-probability tokens. More formally, Prop. 2 shows that convex objectives (e.g., log p) achieve their maximum within [0, 0.5], thus prioritizing low-probability tokens (prior-averse); whereas concave objectives (e.g., p2) peak within [0.5, 1], thereby reinforcing already confident predictions (prior-leaning). This distinction illustrates how convexity modulates the degree to which an objective respects model priors. In particular, the family in Eq. 3 can be seen as providing smooth transition between prior-averse and prior-leaning behavior. This leads to the following definition. Figure 2: The logit gradients Wf (p) of different functions."
        },
        {
            "title": "Preprint",
            "content": "Definition 1 (Prior-leaning versus Prior-adverse Objectives). We classify objectives according to how Wf distributes its mass over p. We say the objective is: Prior-leaning if the majority of gradient weight is concentrated on mediumto highprobability tokens (i.e., above threshold τ ), thereby leveraging the models prior to refine already plausible predictions. Prior-averse if the majority of gradient weight is concentrated on low-probability tokens (p below τ ), thereby pushing the model to learn from unlikely predictions. This definition emphasizes that different objectives exploit the models prior in opposite ways. While the precise boundary between prior-leaning and prior-averse (e.g., the choice of threshold τ ) is not unique and may depend on the task, some objectives exhibit clear contrasts (e.g., log versus p), which form the primary focus of our study. To further probe their behavior, we also consider hard-thresholding variant: LHT(I),f (p)(θ) = E(x,y)T (cid:2)f (cid:0)p(y x)(cid:1) 1{p(y x) I}(cid:3) , (4) where HT(I) denotes restricting updates to tokens whose predicted probabilities fall within an interval [0, 1]. This formulation is particularly useful for ablation, as it isolates the contribution of tokens in specific probability ranges. The model capability continuum. Unlike traditional classification tasks, language model posttraining spans wide variety of domains that differ substantially in how well they are supported by pretraining. Consequently, not all tasks should be treated uniformly. We categorize tasks along model-capability continuum, defined by the strength of the base model prior. general categorization is shown in Fig. 1. Our classification relies on two complementary perspectives: (1) From the pretraining data side, tasks differ in the portion of relevant data contained in the corpus. For example, the LLaMA-3 report indicates that 25% of its pretraining tokens are math-related, suggesting strong priors for mathematical reasoning (model-strong). By contrast, figfont puzzles fall entirely outside the pretraining corpus and thus represent model-weak tasks, while domains with partial coverage, such as medical reasoning, are considered intermediate. (2) From the model side, we measure the mean predicted probability on the training set as quantitative proxy of prior strength. This measure aligns well with intuition: math tasks achieve high predicted likelihood of the training even before SFT (e.g., Qwen2.5-Math-7B: 0.81, LLaMA-3.1-8B: 0.76), whereas medical reasoning lies in the middle (0.50), and figfont puzzles remain extremely low (0.01). Together, these perspectives motivate our continuum view and ground it in both qualitative and quantitative evidence. The details and the rationales about our classification are included in Appen. B.1. At the model strong (MS) end, prior-leaning objectives can be leveraged to refine small number of critical tokens by concentrating learning on midto high-probability tokens that are more likely to be correct. At the model weak (MW) end, prior-averse objectives are more suitable, as they encourage the model to improve predictions across all tokens. For models of intermediate capability (MI), both objectives may provide benefits, depending on the characteristics of the task and the base model."
        },
        {
            "title": "3 MAIN EXPERIMENTS",
            "content": "In this section, we empirically validate the proposed continuum view of SFT post-training and evaluate the performance of different probability-based objective functions. 3.1 EXPERIMENTAL SETUP To empirically validate the continuum view, we conduct experiments across three representative domains: mathematical reasoning, medical reasoning, and textual puzzles. As motivated in Sec. 2, these domains occupy different positions along the model-capability continuum. For the modelstrong (MS) end, we use NuminaMath (LI et al., 2024) as training data. For the model-weak (MW) end, we generate synthetic figfont puzzles from Reasoning Gym (Stojanovski et al., 2025). For the intermediate (MI) region, we adopt m23k (Huang et al., 2025), high-quality medical reasoning dataset. Additional statistics supporting this classification are provided in Appen. B.1."
        },
        {
            "title": "Preprint",
            "content": "Our experiments cover diverse set of advanced backbones, including LLaMA-3.2B, LLaMA-3.18B, DeepSeekMath-7B, Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-1.5B, and Qwen2.57B. We primarily compare the and log objectives, with one exception: on the MS end, we also evaluate thresholded variant of log that excludes low-probability tokens. All models are trained with AdamW, and evaluation datasets, optimization details, and further experimental configurations are provided in Appen. B."
        },
        {
            "title": "3.2 MAIN RESULTS",
            "content": "Table 2: Main results in the Model Strong (MS) end. Both and thresholded log(p) consistently outperform the standard log(p) objective across models and datasets. Best results are in bold. Models Math500 Minerva Math Olympiad Bench AIME24 AMC23 Avg. Base -log(p) -log(p)1{p 0.2} -p Base -log(p) -log(p)1{p 0.2} -p Base -log(p) -log(p)1{p 0.2} -p Base -log(p) -log(p)1{p 0.2} -p 1.76 17.59 24.39 25.29 5.70 28.79 40.38 39.55 30.71 42.52 63.95 65.27 40.38 51.90 67.85 68.47 LLaMA-3.1-8B 0.68 5.84 10.49 10.09 0.86 3.04 5.10 6.37 DeepSeekMath-7B 2.89 9.29 19.38 20.14 1.51 6.57 13.98 13.99 Qwen2.5-Math-1.5B 8.81 12.71 24.79 26.18 14.88 12.09 26.08 26.66 Qwen2.5-Math-7B 13.66 18.88 32.47 31.99 16.36 17.37 33.90 32.26 0.00 0.21 0.41 0. 0.00 0.21 0.62 1.24 2.49 0.62 7.09 6.88 6.04 2.70 8.76 8.75 1.25 5.78 11.25 10.62 2.34 10.62 18.91 20.62 17.97 17.03 38.28 38. 24.69 22.50 47.81 41.09 0.91 6.49 10.33 10.56 2.49 11.10 18.65 19.11 14.97 17.00 32.04 32.75 20.23 22.67 38.16 36.51 Model-Strong Results Interpretation. Tab. 2 reports results in the model-strong (MS) end, where In this setting, the base models already exhibit strong priors aligned with the ground truth. objective consistently outperforms standard negative log-likelihood ( log p), with the performance gap becoming more pronounced for larger models such as 7B and 8B compared to 3B. This trend suggests that when model predictions are already reliable, prior-leaning objective like better capitalizes on high-confidence tokens by suppressing the influence of low-probability ones. To further dissect this effect, we evaluate thresholded variant of log that excludes tokens with < 0.2. This adjustment directly mitigates the effect of low-confidence tokens and leads to consistent improvements over standard log p. In many cases, it performs on par with, or even surpasses, applied to full tokens. Such evidence highlights that the weakness of standard NLL in this setting lies in its excessive emphasis on low-probability tokens. Prior-leaning objectives that explicitly reduce the contribution of low-confidence tokens consistently provide the most benefit at the MS end. We provide further empirical analysis in Sec. 4 with more careful study of the pattern. Model-Intermediate Results Interpretation. In Tab. 3, results on medical reasoning reveal strikingly different pattern: the performance of and log is nearly indistinguishable, with differences well within statistical variation. This neutrality arises from the nature of intermediate priors. On one hand, the priors are not strong enough for the prior-leaning objective to yield consistent refinements; on the other, they are not weak enough for the prior-averse objective log to offer decisive corrective advantage. This observation is important because it indicates that the existence of region where gains are unlikely to come from altering the learning objective itself. Instead, improvements may rely on alternative directions, such as better data curation, targeted domain supervision, or hybrid strategies that combine training data with external resources."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Main results in the Model Moderate (MM). Both and log(p) result in similar performance. Best results are in bold. Model MedMC MedQA PubMed MMLU-P GPQA Lancet MedB (4) MedB (5) MedX NEJM Avg. Base -log(p) -p Base -log(p) -p Base -log(p) -p Base -log(p) -p 21.30 42.60 39.42 23.57 55.08 54.10 22.21 39.64 38.58 35.84 36.48 35.62 21.92 45.56 41.95 29.14 59.47 58. 21.84 39.59 36.68 27.26 33.78 33.78 22.60 67.40 62.70 21.00 74.00 76.50 18.50 66.70 68.00 49.30 72.60 69. LLaMA-3.1-3B 23.08 24.36 38.46 25.00 46.84 44.17 LLaMA-3.1-8B 29.49 32.05 44.87 22.57 57.28 54. Qwen2.5-1.5B 24.36 33.33 35.90 22.57 38.83 35.68 Qwen2.5-Math-7B 35.90 38.46 42.31 30.34 40.05 35. 11.40 38.63 33.88 20.00 53.62 52.70 11.21 34.92 38.37 30.23 35.50 38.83 23.05 46.10 35.71 30.52 52.27 42. 24.03 38.31 36.69 24.03 29.87 33.12 15.26 34.42 28.57 20.45 46.10 42.53 17.53 27.60 28.90 18.18 26.95 27. 10.35 11.59 12.63 10.01 15.87 13.80 10.84 10.56 11.94 10.21 10.42 10.49 23.22 43.28 40.80 20.73 59.20 54. 18.74 34.16 39.97 24.71 26.70 26.70 19.48 37.99 36.29 21.89 47.23 45.89 18.59 35.13 35.02 27.55 33.56 33. Table 4: Main results in the Model Weak (MW) end. log(p) consistently outperforms across different models and metrics substantially. Best results are in bold. LLaMA-3.2-3B LLaMA-3.1-8B Qwen2.5-1.5B Qwen2.5-7B Metric Base -log(p) -p Base -log(p) -p Base -log(p) -p Base -log(p) -p Exact Match Jaro-Winkler Similarity 0.00 41.89 1.08 44.39 0.00 2.43 0.00 30.17 1.34 43. 0.00 10.15 0.00 35.32 0.60 32.98 0.0 8.36 0.00 44.92 35.20 82. 0.00 10.15 Model-Weak Results Interpretation. Tab. 4 reveals the opposite trend at the MW end: here log consistently outperforms p, often by substantial margins. When priors are poorly aligned with the ground truth, the concavity of becomes detrimental, as it allocates disproportionate weight to unreliable high-probability tokens, thereby reinforcing errors. By contrast, the convexity of log ensures that low-probability tokens, which often correspond to mistakes, receive stronger gradient signals, forcing the model to correct its errors and spread learning more broadly across the output distribution. This explains why NLL, despite its shortcomings elsewhere, remains the most effective objective in weak-prior settings. Consequently, progress on MW tasks is more likely to come from stronger or more targeted supervision, improved data augmentation, or other methods of injecting knowledge, rather than from modifying the training objective. We provide further empirical analysis in Sec. 4 with more careful study of the pattern."
        },
        {
            "title": "4 EMPIRICAL ANALYSIS",
            "content": "In this section, we provide deeper empirical analysis of the findings in Sec. 3, with particular emphasis on the MS and MW ends where the choice of training objective has the largest effect. Our goal is to move beyond merely reporting performance numbers and to analyze the mechanisms that drive the observed differences. To this end, we structure the analysis around three guiding questions: 1. In the MS end, what mechanisms explain the underperformance of NLL? 2. How do objectives with different emphasis on model priors behave across the two ends? 3. To what extent are these objectives consistent with likelihood estimation on the training set? Answering these questions provides deeper understanding of how different objectives interact with model capability from complementary perspectives. Model Setup. For ablation studies in the MS end, we focus on Qwen-2.5-Math-1.5B, which shows the clearest gap between objectives. For the MW end, we use Qwen-2.5-7B. All training details and"
        },
        {
            "title": "Preprint",
            "content": "evaluation protocols remain identical to those in Sec. 3, ensuring that differences arise solely from the choice of objective."
        },
        {
            "title": "4.1 ABLATION ON QUANTILE THRESHOLDING WITH DIFFERENT OBJECTIVES",
            "content": "Figure 3: Performance under quantile thresholding for log(p), p, and log(1 p). Let Qpercentile denote the predicted probability at the specified percentile of the training set. ( Percentile) corresponds to = [Qpercentile, 1] in Eq. 4, while ( Percentile) corresponds to = [0, Qpercentile]. Key findings: (1) low-probability tokens consistently harm performance across all objectives; (2) when training on all tokens, objectives that de-emphasize low-probability tokens (p and log(1 p)) outperform log(p); (3) restricting training to only the top 10% of tokens yields the strongest improvements across all objectives, surpassing standard SFT. Detailed Setup. This ablation examines how restricting training to different quantiles of tokens affects the relative performance of objectives. We compare three instances of (p) in Eq. 2: log(p), p, and log(1 p), which emphasize low-, mid-, and high-probability tokens, respectively (shown in Fig. 2). All experiments are identical except for the subset of tokens selected by the quantile thresholding rule in Eq. 4. Quantile thresholds are computed from the base models predicted token probabilities prior to training. We apply both bottom thresholding and top thresholding, denoted by ( Percentile) and ( Percentile), respectively. Bottom thresholds vary from 5% to 100%, and top thresholds vary from 0% to 90%. Results Interpretation. The results in Fig. 3 reveal several consistent patterns that align with our main experiments in Sec. 3. First, all objectives achieve strong performance when restricted to only the top 10% tokens, significantly exceeding standard NLL on all tokens. Second, performance drops sharply when training on low-probability tokens, confirming that they contribute adversarially to learning. Third, when applying bottom-thresholding, and log(1 p) consistently outperform log(p), illustrating the benefits of objectives that de-emphasize unreliable tokens. Finally, the degradation of log(p) performance when trained on all tokens (blue curve) can be largely attributed to the bottom 10% quantile. Overall, these results reinforce the main conclusion from Sec. 3: in the MS end, low-probability tokens act primarily as noise to the strong model. 4.2 OBJECTIVE CONVEXITY AND PERFORMANCE DIFFERENCE Detailed Setup. To systematically examine the effect of objective on downstream performance, we study the parametric family in Eq. 3. This objective is concave when α 1 and convex when α 1. more concave objective is more prior-leaning and vice versa, as shown in Fig. 2. We leverage the convexity of this objective as proxy for assessing prior-leaning versus prior-averse objectives. We vary α from 0.1 to 1.0 in increments of 0.1, and from 1.0 to 10.0 in increments of 1.0."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Analysis of MS and MW ends in terms of objective convexity (with Eq. 3) and likelihood estimation. In MS, more concave (prior-leaning) objectives yield better downstream accuracy, while in MW, more convex (prior-averse) objectives dominate. The likelihood estimation results align with these trends, suggesting that objective shape directly interacts with model prior strength. Results Interpretation. As shown in Fig. 4, convexity affects performance in opposite directions across the SFT continuum. In the MS end, accuracy improves as α increases, peaking near α = 1 In the MW end, performance is maximized at α = 0.1 and remaining stable for larger values. and deteriorates rapidly as α approaches 1 and exceeds the convexity boundary. This dichotomy highlights the importance of aligning objective shape with model prior strength: concave objectives (that emphasize model priors) are more effective when priors are strong, while convex objectives (that de-emphasize model priors) are preferable when priors are weak. 4.3 LIKELIHOOD ESTIMATION ON THE TRAINING SET Detailed Setup. In this ablation, we evaluate the empirical training performance of different objectives by computing the average predicted likelihood on the training set before and after fine-tuning: Likelihood Estimation := 1 n (cid:88) yi (cid:88) i=1 j=1 [pθ(yi,j)] (5) where denotes the i-th sample and denotes the j-th token, and = (cid:80)n of training tokens. We focus on comparing and log(p) in both the MS and MW ends. i=1 yi, the total number Results Interpretation. The likelihood estimation results, shown in Fig. 4, closely parallel the In the MS end, achieves higher mean predicted probabilities, downstream accuracy trends. confirming that they better align with strong model priors and effectively capture the training distribution. In contrast, in the MW end, log(p) yield higher training performance, reflecting their ability to correct misaligned priors by emphasizing low-probability tokens. These findings indicate that the interaction between objective shape and regime governs not only generalization performance but also the models fit to the training data."
        },
        {
            "title": "5 THEORETICAL ANALYSIS",
            "content": "5.1 SETUP Data. Let the input prompt be . The true conditional distribution over tokens [V ] is denoted by r(y x), with r( x). We write for the marginal distribution over pairs (cid:0)x, r( x)(cid:1), and let ( x) denote the empirical training distribution over contexts x, which we abuse the notation for writing (x, y) . We use subscript p() to denote model predictions p(). Model and objectives. Let pθ( x) = softmax(cid:0)zθ(x)(cid:1) be the next-token distribution of an autoregressive LM with parameters θ, and write p0( x) = pθ0( x) for the base model. We define the population risk to be R(θ) = E(x,y)D,yppθ(x) (cid:104) 1{y = yp} (cid:105) ,"
        },
        {
            "title": "During SFT we minimize the empirical objective",
            "content": "Lf (θ) = E(x,y)T (cid:2)f (cid:0)pθ(y x)(cid:1)(cid:3) where : [0, 1] is differentiable and decreasing in p. Our theoretical analysis mainly relies on the following assumption about the two ends of the continuum: Assumption 1 (Model-Capability Assumption). We make the following assumptions about data capability in the Model-Strong and Model-Weak ends: Model-Weak. In the MW end, we assume that model predictions are uniform over the vocabulary . Model-Strong. In the MS end, we assume that for any given x, Pry,y [(py + py) 0.55] with 0.70. Assumption 2 (Trainable Base Model). We assume that the base model is still not perfect: for any given x, Pr [0.55 (py + py) 0.95] 1 in the MS end. Remark 1. The MW assumption captures the essential condition of weakness by modeling the base as uninformative. The MS assumption is grounded in practice: in Appen. C.1, we empirically validate this. Assumption 2 is mild and simply guarantees that optimization is nontrivial. We choose 1 for simplicity of proof. 5.2 MAIN RESULTS = Lfi (θ) denote the corresponding gradient flow, and let R(θ(i) We analyze the optimization dynamics of different objectives under gradient flow. For an objective fi, let θ(i) ) be the population risk at time t. Our goal is to maximize the reduction in risk, as captured by R(θ(i) Theorem 1 (Characterization via Gradient Flow, Informal). Suppose that p, and Assumptions 12 hold. Then, in simplified setup, we have the following conclusions: ). 2(p) 1(p) < 0 for all R(θ(1) R(θ(1) )(cid:12) (cid:12)t=0 R(θ(2) )(cid:12) (cid:12)t=0 R(θ(2) )(cid:12) (cid:12)t=0 in Model Strong End. )(cid:12) (cid:12)t=0 in Model Weak End. Remark 2. This theorem characterizes sufficient condition for which the relative advantage of two objectives reverses across the MS and MW ends. For example, setting f1(p) = 1 and f2(q) = log p, we conclude that in the model-strong end, the prior-leaning objective achieves larger risk reduction than NLL, whereas in the model-weak end, NLL is superior. This reversal mirrors our empirical observations and highlights the central theme of this work: the effectiveness of an SFT objective depends critically on model capability. The full analysis is provided in Appen. F."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "In this work, we revisited the objective of supervised fine-tuning (SFT) for large language model post-training and showed that negative log likelihood (NLL), while classically optimal from scratch, is not universally effective once models already encode priors and supervision is long and noisy. Our central contribution is the model-capability continuum, instantiated with general family of probability-based objectives, which reveals that the effectiveness of different objectives depends critically on the prior strength of the base model. Through extensive analyses from different angles, we found consistent evidence that objectives reverse their relative advantage across different regions, yielding unified explanation of how objective form interacts with model capability. Looking ahead, our results highlight the need for adaptive objectives that adjust to model capability rather than relying on fixed choice. Promising directions include practical implementations of adaptive SFT objectives, integration with domain-specific supervision and data curation, and extensions to broader post-training frameworks. Another avenue is to explore dynamic or curriculumstyle adaptation, where the objective evolves with model improvement during training. Advancing along these lines may unlock the full potential of SFT as lightweight yet powerful approach for aligning large language models. We discuss potential limitations in Appen. D."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI Mathematical Olympiad Prize. Ai mathematical olympiad prize. https://www.kaggle. com/competitions/ai-mathematical-olympiad-prize, 2024. Accessed: 202509-24. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Peter Bartlett, Michael Jordan, and Jon McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138156, 2006. George Casella and Roger Berger. Statistical inference. Chapman and Hall/CRC, 2024. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language models on answering and explaining challenging medical questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 35633599, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=dYur3yabMj. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Thomas Cover. Elements of information theory. John Wiley & Sons, 1999. David Cox. The regression analysis of binary sequences. Journal of the Royal Statistical Society Series B: Statistical Methodology, 20(2):215232, 1958. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020. Tilmann Gneiting and Adrian Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359378, 2007. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328339, 2018. Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, and Yuyin Zhou. m1: Unleash the potential of test-time scaling for medical reasoning with large language models. arXiv preprint arXiv:2504.00869, 2025. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021."
        },
        {
            "title": "Preprint",
            "content": "Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. In The Twelfth International Conference on Learning Representations. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Mingyang Liu, Gabriele Farina, and Asuman Ozdaglar. Uft: Unifying supervised and reinforcement fine-tuning. arXiv preprint arXiv:2505.16984, 2025. Mathematical Association of America. Math competitions. https://maa.org/ math-competitions, 2023. Accessed: 2025-09-24. Mathematical Association of America. Aime thresholds are available. https://maa.org/ aime-thresholds-are-available/, 2024. Accessed: 2025-09-24. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 34703487, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale In Conference on multi-subject multi-choice dataset for medical domain question answering. health, inference, and learning, pp. 248260. PMLR, 2022. Chongli Qin and Jost Tobias Springenberg. Supervised fine tuning on curated data is reinforcement learning (and can be improved). arXiv preprint arXiv:2507.12856, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Leonard Savage. Elicitation of personal probabilities and expectations. Journal of the American Statistical Association, 66(336):783801, 1971. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Kopf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards, 2025. URL https://arxiv.org/abs/2505.24760."
        },
        {
            "title": "Preprint",
            "content": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. replicable instructionfollowing model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. Alpaca: strong, Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, and Xipeng Qiu. Implicit reward as the bridge: unified view of sft and dpo connections. arXiv preprint arXiv:2507.00018, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629, 2025. Johnathan Xie, Annie Chen, Yoonho Lee, Eric Mitchell, and Chelsea Finn. Calibrating language models with adaptive temperature scaling. arXiv preprint arXiv:2409.19817, 2024. Dylan Zhang, Qirun Dai, and Hao Peng. The best instruction-tuning data are those that fit. arXiv preprint arXiv:2502.04194, 2025. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792, 2023. Tong Zhang. Statistical analysis of some multi-category large margin classification methods. Journal of Machine Learning Research, 5(Oct):12251251, 2004. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, and Zhendong Mao. On the calibration of large language models and alignment. arXiv preprint arXiv:2311.13240, 2023. Wenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, and Pengfei Liu. Proximal supervised fine-tuning. arXiv preprint arXiv:2508.17784, 2025. Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362, 2025."
        },
        {
            "title": "CONTENTS",
            "content": "A Related Works Detailed Experimental Setup B.1 Continuum Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Training and Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experiment Results C.1 Justification for Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Limitation Proofs for Sec. 2 Main Theoretical Results F.1 Setup and Notations . F.2 Assumptions F.3 Main Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 14 15 16 16 16 16 17 18"
        },
        {
            "title": "A RELATED WORKS",
            "content": "Language Model Post-training. Supervised Fine-Tuning (SFT) has emerged as the dominant paradigm for post-training, adapting pretrained models to tasks or domains by directly fitting labeled data (Zhang et al., 2023; Chung et al., 2024). The availability of high-quality instruction datasets (Mishra et al., 2022; Zhou et al., 2023; Taori et al., 2023; Lightman et al., 2023) has further boosted SFTs effectiveness. Nevertheless, abundament studies highlight that SFT alone often overfits, generalizes poorly, and yields sub-optimal models (Howard & Ruder, 2018; Dodge et al., 2020; Ouyang et al., 2022). To address these limitations while retaining SFTs efficiency, the prevailing recipe is to combine SFT with RL, forming the de facto post-training paradigm (Bai et al., 2022; Achiam et al., 2023; Kirk et al.; Chu et al., 2025; Liu et al., 2025). Yet, existing SFT post-training consistently minimizes the negative log-likelihood objective, log(p), whose suitability has rarely been questioned. In this work, we show that it is not universally optimal and argue for revisiting objectives that better exploit pretrained priors in SFT. Improving SFT (from an RL perspective). Motivated by the success of reinforcement learning in reasoning tasks, growing body of work seeks to reinterpret and improve SFT through an RL lens. Wang et al. (2025) cast both SFT and DPO as instances of implicit reward learning, showing that smaller learning rates and alternative divergence-based objectives can enhance performance. Qin & Springenberg (2025) integrates importance sampling into SFT, while Zhu et al. (2025) introduces PPO-style clipped surrogate objective to constrain policy drift. Most closely related to our work, Wu et al. (2025) proposes reweighting gradient coefficients uniformly, essentially equivalent to our objective, for which we provide deeper characterization and analysis. Overall, these approaches can be regarded as special cases of our proposed prior-leaning objectives, implemented through RL techniques to downweight low-probability tokens. In contrast, we show that the same effect can be achieved far more simply by applying threshold. Moreover, these RL-inspired methods are only validated in single domain, whereas we demonstrate the potential limitations of priorleaning objectives in the model-weak end. Other than RL-inspired approaches, Zhang et al. (2025) further explore data selection by favoring high-probability instances, weaker form of our tokenwise thresholding objective. Classical views on SFT learning objectives. In the conventional view of classification, the nNLL it is the maximum likelihood estimahas long been regarded as the optimal training objective: tor (statistical consistency) (Cox, 1958; Casella & Berger, 2024), equivalent to minimizing crossentropy/KL-divergence (information-theoretic) (Cover, 1999), the unique strictly proper local scoring rule ensuring calibrated probabilities (decision-theoretic) (Savage, 1971; Gneiting & Raftery, 2007), and convex surrogate to 0-1 loss guaranteeing Bayes consistency and tractable optimization (learning-theoretic) (Bartlett et al., 2006; Zhang, 2004). These arguments, however, assume training from scratch on simple classification tasks, whereas SFT in language model post-training starts from powerful pretrained models with long chain-of-thought supervision where only final answers are evaluated and intermediate tokens may be noisy. Under these conditions, the premises for log(p) might no longer hold, and in this work, we provide the first systematic characterization of such settings."
        },
        {
            "title": "B DETAILED EXPERIMENTAL SETUP",
            "content": "Table 5: General experimental setup across different regions of the model-capability continuum. Continuum Domain Signals Training Data Evaluation Data Objectives to Compare MS MI MW math-reasoning sparse NuminaMath CoT Math500, Minerva Math, Olympiad Bench, AIME24, AMC23 -p, -log(p), threshold(-log(p)) medical-reasoning sparse m23k text games dense synthetic MedMC, MedQA, PubMed, MMLU-P, GPQA, Lancet, MedB(4), MedB(5), MedX, NEJM synthetic -p, -log(p) -p, -log(p) We now provide details of our experimental setup, including the rationale for the choice of datasets across the continuum, the corresponding training and evaluation benchmarks, and specific training protocols. An overview is summarized in Tab. 5."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Continuum selection based on mean predicted probability (Eq. 5). In the MS end, base models already achieve high likelihood on the training set before fine-tuning; in the MI region, predictions are around 0.5; in the MW end, predictions are near zero. Model Strong (Math) Mean Predicted Probability Model Name 0.80 0.76 0. 0.81 LLaMA-3.1-8B DeepSeekMath-7B Qwen2.5-Math-1.5B Qwen2.5-Math-7B Model Intermediate (Med) Mean Predicted Probability Model Name 0.50 LLaMA-3.2-3B 0.53 LLaMA-3.1-8B 0.56 Qwen2.5-1.5B 0.59 Qwen2.5-Math-7B Mean Predicted Probability Model Name 0.01 LLaMA-3.2-3B 0.01 LLaMA-3.1-8B 0.01 Qwen2.5-1.5B 0.07 Qwen2.5-7B Model Weak (Puzzles) B.1 CONTINUUM SELECTION We assign math tasks to the MS end, medical tasks to the MI region, and figfont puzzles to the MW end. For the MS end, we use LLaMA-3.1-8B, DeepSeekMath-7B, Qwen2.5-Math-1.5B, and Qwen2.5-Math-7B. For the MI region, we use LLaMA-3.2-3B, LLaMA-3.1-8B, Qwen2.5-1.5B, and Qwen2.5-Math-7B. For the MW end, we use LLaMA-3.2-3B, LLaMA-3.1-8B, Qwen2.5-1.5B, and Qwen2.5-7B. We rely on base models in all cases. Our rationale for this selection is twofold. First, evidence from pretraining corpora. Fig. 1 illustrates that some domains are strongly represented in pretraining while others are not. For example, open-sourced documentation of LLaMA-3 reports that 25% of pretraining tokens are math-related (Grattafiori et al., 2024), indicating strong priors for math reasoning. Similarly, DeepSeekMath and Qwen2.5-Math were explicitly pretrained on math corpora. By contrast, medical corpora are only partially present in pretraining, yielding moderate priors, and figfont puzzles are completely absent, making them natural MW task. Second, quantitative evidence from model predictions. Tab. 6 shows mean predicted probabilities (Eq. 5) on the training set, which we use as proxy for prior strength given that base LLMs are generally well-calibrated and their predictions more faithfully reflect inherent model capability (Zhu et al., 2023; Xie et al., 2024) . In the MS end, models already achieve very high likelihoods (around 0.8) before fine-tuning. In the MW end, predictions are close to zero, reflecting lack of relevant prior knowledge. In between, predictions cluster around 0.5, reflecting an intermediate level of task familiarity. Together, these observations justify our continuum classification and ground it in both qualitative and quantitative evidence. B.2 TRAINING AND EVALUATION DETAILS General framework. All SFT experiments are conducted using verl (Sheng et al., 2024). We fix the optimizer to AdamW, with base learning rate of 5 105 for all models except LLaMA-3.18B, where we use 2 105. We employ cosine decay scheduling with warm-up ratio of 0.1, and train for single epoch. All training runs are performed on 2 H200 GPUs with single node. Model-Strong (Math). Our setup for mathematical reasoning largely follows Wu et al. (2025). We train on NuminaMath-CoT (LI et al., 2024), which contains 859k chain-of-thought problems collected from multiple sources. For efficiency, we sample 67k subset, which we find to achieve equivalent performance to larger subsets (100k+ or more). We set the maximum training length to 3072 tokens and use micro-batch size of 4. Evaluation covers five representative math benchmarks: Math500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), Olympiad Bench (AI Mathematical Olympiad Prize, 2024), AIME24 (Mathematical Association of America, 2024), and AMC23 (Mathematical Association of America, 2023). Each evaluation uses temperature 1.0, with results reported as the average of 16 generations per example and maximum generation length of 4096 tokens."
        },
        {
            "title": "Preprint",
            "content": "Model-Intermediate (Medical). We train on m23k (Huang et al., 2025), 23k-instance medical reasoning dataset. We experimented with two variants: (i) including long-form reasoning traces (maximum length 8192, micro-batch size 1) and (ii) using only standard chain-of-thought (maximum length 1024, micro-batch size 16). Since performance was similar, we report results from the standard CoT variant. Evaluation strictly follows the protocol in Huang et al. (2025), using temperature 0 and random seed 42. Benchmarks include MedMCQA (Pal et al., 2022), MedQAUSMLE (Jin et al., 2021), PubMedQA (Jin et al., 2019), MMLU-Pro (Wang et al., 2024), GPQA (Medical) (Rein et al., 2024), Lancet & NEJM (Huang et al., 2025), MedBullets (Chen et al., 2025), and MedXpertQA (Zuo et al., 2025). detailed overview of these datasets is provided in Huang et al. (2025). Model-Weak (Figfont). We generate synthetic figfont puzzles from ReasoningGym (Stojanovski et al., 2025). We generate synthetic figfont puzzle data from ReasoningGym (Stojanovski et al., 2025), creating 40k instances for training and 20k for evaluation. An example puzzle is shown in Fig. 1. Training mirrors the MI setup, with maximum sequence length of 800 and micro-batch size of 16. Inference uses temperature 0 and random seed 42. We evaluate with two metrics: (i) exact match and (ii) JaroWinkler similarity, string-based similarity score that is more tolerant to small variations and complements the strictness of exact match."
        },
        {
            "title": "C ADDITIONAL EXPERIMENT RESULTS",
            "content": "C.1 JUSTIFICATION FOR ASSUMPTIONS Table 7: The percentage of tokens with initial predicted probability larger than 0.55 prior to training in the MS end. We find that the pretrained base models have high predicted probabilities of the training set prior to training. This justifies Assump. 1. LLaMA-3.1-8B DeepSeekMath-7B Qwen2.5-Math-1.5B Qwen2.5-Math-7B Percentage of tokens with initial predicted probability larger than 0.55 72.8% 76.7% 80.6% 81.2%"
        },
        {
            "title": "D LIMITATION",
            "content": "While our study provides comprehensive characterization of probability-based objectives across the model-capability continuum, several limitations remain. First, we did not extend our experiments to excessively large models (e.g., 30B70B parameters) due to computational resource constraints. Second, our framework for assessing initial model capability, via mean predicted probability and domain priors, serves as first attempt, and future work may design more principled or fine-grained measures of capability, specifically tailored for SFT. Third, although our analysis spans the modelstrong and model-weak ends extensively, our exploration of the intermediate region remains relatively limited. While our work serves as the pioneering study and we identify its neutrality in objective comparisons, more careful study of this middle ground could yield deeper insights and potentially inspire adaptive or hybrid strategies that bridge the two extremes. PROOFS FOR SEC. 2 Lemma 2 (Gradient Shape). Let : [0, 1] be differentiable and nonincreasing. Consider the objective in Eq. 2, whose step-t contribution depends on the correct-class probability pt,y = softmax(zt)y only through (pt,y). Then the gradient of Lf with respect to the logits at step satisfies Lf zt,i = sf (pt,y) (cid:0)δi,y pt,i (cid:1), where sf (p) (p) 0. In particular, for the correct class = y, Lf zt,y = sf (pt,y) (1 pt,y) = Wf (pt,y), Wf (p) (p) (1 p)."
        },
        {
            "title": "Preprint",
            "content": "Proof. Write pt = softmax(zt), so pt,i = exp(zt,i)/ (cid:80) for all i, exp(zt,j). The softmax Jacobian gives, pt,y zt,i = pt,y (δi,y pt,i). Since the step-t loss is (pt,y), the chain rule yields Lf zt,i = (pt,y) pt,y zt,i = (pt,y) pt,y (δi,y pt,i) = (cid:0)f (pt,y) pt,y (cid:1) (δi,y pt,i). Define sf (p) = (p) p. Because is nonincreasing, (p) 0 on (0, 1), hence sf (p) 0. The displayed formula then follows, and for = we obtain Lf = sf (pt,y)(1 pt,y) = zt,y (pt,y) pt,y(1 pt,y) = Wf (pt,y). Proposition 2 (Convex versus Concave Objectives). Let 2[0, 1] with (p) < 0 for all (0, 1), and define Wf (p) = (p) p(1 p). If is concave (f 0), then any maximizer of Wf lies in [ 2 , 1]. If is convex (f 0), then any maximizer of Wf lies in [0, 1 2 ]. Proof. Set s(p) := (p). Then s(p) > 0 on (0, 1) by the hypothesis (p) < 0, and Differentiate: Wf (p) = s(p) p(1 p). (p) = s(p) p(1 p) + s(p) (1 2p). Concave case. If 0 on [0, 1], then s(p) = (p) 0. For (0, 1 hence both terms in Wf is strictly increasing on (0, 1 belong to [ 1 (p) are nonnegative; since s(p) > 0, in fact 2 ), so no maximizer can lie in (0, 1 2 ) we have 1 2p > 0, (p) > 0 on (0, 1 2 ). Therefore 2 ); any global maximizer must 2 , 1]. Convex case. If 0 on [0, 1], then s(p) = (p) 0. For ( 1 with s(p) > 0 the two terms in strictly decreasing on ( 1 to [0, 1 2 , 1), so no maximizer can lie in ( (p) are nonpositive, hence 2 , 1) we have 1 2p < 0; 2 , 1). Thus Wf is 2 , 1); any global maximizer must belong (p) < 0 on ( 1 2 ]. Combining the two cases establishes the claim."
        },
        {
            "title": "F MAIN THEORETICAL RESULTS",
            "content": "F.1 SETUP AND NOTATIONS Data model. Let the input prompt . The true conditional distribution over tokens [V ] is r(y x). We let denote the (marginal) distribution over pairs (cid:0)x, r( x)(cid:1). We use ( x) to denote the empirical training distribution over contexts x. Model and objectives. Let qθ( x) = softmax(cid:0)zθ(x)(cid:1) be the next-token distribution of an autoregressive LM with parameters θ, and write q0( x) = qθ0( x) for the base model. We note that we use different notations (instead of p) to denote the model predictions in the appendix. The population risk is R(θ) = E(x,y)D,qqθ(x) (cid:105) (cid:104) 1{y = yq} During SFT we minimize the empirical objective Lf (θ) = E(x,y)T (cid:2)f (cid:0)qθ(y x)(cid:1)(cid:3) where : [0, 1] is differentiable and decreasing."
        },
        {
            "title": "Preprint",
            "content": "Notation. Let zθ(x) RV denote the pre-softmax logits and qθ( x) = softmax(cid:0)zθ(x)(cid:1) the next-token distribution. Fix and suppress its dependence when clear. Define the logit feature map Φ(x, y) := θzθ0 (x, y) Rd, Φ(x) := [Φ(x, 1), . . . , Φ(x, )] RdV , and its Gram matrix over logits G(x) := Φ(x)Φ(x) RV , Gy,y(x) = Φ(x, y), Φ(x, y). Write := qθ0( x), := r( x), and := ( x). For differentiable, increasing fi : [0, 1] R, set (βi)y := Ty qy (qy), βi RV , Sfi := βi, 1 = (cid:88) y=1 Ty qy (qy). Define the discrepancy vectors := (cid:0)rq(cid:1) qrq, Finally, let gi := Lfi(θ0), ki := R(θ0), gi and vi := βiSfi q, β12 := β1β2, S12 := Sf1 Sf2 , v12 := v1v2 = β12S12q. Hi := (cid:90) 1 0 2R(cid:0)θ0 η gi (cid:1) dt for stepsize η > 0 (used later in second-order expansions). F.2 ASSUMPTIONS F.2.1 MAIN ASSUMPTIONS Assumption 3 (Model-Capability Assumption). We make the following assumptions about data capability in the Model-Strong and Model-Weak ends: Model-Weak. In the MW end, we assume that model predictions are uniform over the vocabulary . Model-Strong. In the MS end, we assume that for any given x, Pry,y [(qy + qy) 0.55] with 0.70. Assumption 4 (Trainable Base Model). We assume that the base model is still not perfect: for any given x, Pr [(0.55 qy + qy) 0.95] α Pry,y [(qy + qy) 0.50] in the MS end. These assumptions are mentioned in the main paper with justifications. The coefficient α could depend on the task itself, and this value 1 in practice. Assumption 4 is more general re-statement of Assumption 2. F.2.2 ADDITIONAL SIMPLIFICATION ASSUMPTIONS Assumption 5 (Model and Data Simplifications). We assume that the feature matrix Φ is preconditioned such that all of its singular values are equal to one, and that both the training distribution and the true distribution are one-hot. This assumption is made purely for analytical convenience: it removes irrelevant conditioning factors in the proof and allows us to focus on the essential differences between objectives. F.3 MAIN PROOFS Lemma 3 (Gradient identities). We have the following identities: R(θ0) = Ex (cid:2)Φ(x) v(x)(cid:3), Lfi (θ0) = Ex (cid:2)Φ(x) vi(x)(cid:3), Proof. Population risk. With R(θ) = Ex By the chain rule through softmax, (cid:2) r(x)qθ( x)(cid:3), for fixed we have R/q = r. z = J(q) (r) = (qr) r,"
        },
        {
            "title": "Preprint",
            "content": "so θR(θ0) = Φ(x) z = Φ(x) v(x). Taking expectation over yields the first identity. Ty(x) fi(qy)(cid:3), Lfi/q = mi with mi = (qy))y. Again, Lfi /z = J(q) mi = vi, whence θLfi(θ0) = Φ(x) vi(x) and the claim General fi-objective. (Tyf follows after taking expectation over x. For Lfi(θ) = Ex (cid:2) (cid:80) Lemma 4 (Functional derivative). Define J(fi) := Ex (cid:104) ΦΦ vi ΦHi Φ vi (cid:105) , Hi := η 2 (cid:90) 1 0 2R(cid:0)θ0 η gi (cid:1) dt, with gi := Lfi(θ0) = Ex[Φ vi], := (cid:0)rq(cid:1) q, vi := βi Sfiq, (βi)y := Tyqyf Sfi = (cid:80) (qy). For perturbation of fi (so that fi (cid:55) fi + ϵh), the first variation is Tyqyf (qy), δJ(fi; h) = Ex (cid:2)(cid:0)v ΦΦ η i ΦHiΦ(cid:1) δvi (cid:3) + η2 2 (cid:90) 1 0 (cid:10)3R(cid:0)θ0 tηgi (cid:1) [δgi], gi gi (cid:11) dt, where δgi = Ex[Φ δvi] and δvi = δβi (δSfi) = (cid:16) Diag(T q) (T q)(cid:17) h(q). Proof. Write := Φ(x) for brevity. Then = Ex (cid:104) AA vi AHiA vi (cid:105) . η 2 Vary fi (cid:55) fi + ϵh. Since is fixed, δ(v product rule: AAvi) = AA δvi. For the second term, use the δ(cid:0)v AHiA vi (cid:1) = 2 AHiA δvi + A(δHi)A vi. Hence (cid:104) δJ = Ex AA δvi η v AHiA δvi A(δHi)A vi (cid:105) . η 2 Now Hi = (cid:82) 1 on gi, the chain rule yields 0 2R(θ0 tηgi) dt. Since δ2R(θ) = 3R(θ)[ ] and the evaluation point depends δHi = (cid:90) 1 Therefore (cid:0)tη(cid:1) 3R(cid:0)θ0 tηgi (cid:1) [δgi] dt, with δgi = Ex[A δvi]. η A(δHi)A vi = η2 2 (cid:90) 1 0 (cid:10)3R(cid:0)θ0 tηgi (cid:1) [δgi], Avi Avi (cid:11) dt. Taking Ex and using trilinearity in the last two slots, ExT [δgi], Avi Avi = [δgi], (ExAvi) (ExAvi) = [δgi], gi gi, with := 3R(), gives the stated third-order term. Finally, the variation of vi with respect to fi via is δβi = qh(q), δSfi = q, h(q), δvi = δβi(δSfi) = (cid:16) Diag(T q)q (T q)(cid:17) h(q). Collecting terms yields the claimed formula. Corollary 1. Define the gradient flow of the following term: Then we have )(cid:12) R(θ(i) (cid:12)t=0 := lim η0 R(θ0) R(θ(i) 1 ) η )(cid:12) R(θ(i) (cid:12)t=0 = Ex (cid:2)v ΦΦvi (cid:3) 19 (6) (7)"
        },
        {
            "title": "Preprint",
            "content": "Proof. By Taylor Expansion, we have R(θ0)R(θ(i) 1 ) = ηR(θ0), Lfi(θ0) η2 2 Lfi(θ0) (cid:18)(cid:90) 0 2R (θ0 tηLfi (θ0)) dt (cid:19) Lfi(θ0) (8) Then this corollary follows immediately from Lem. 4. Lemma 5 (Useful Inequalities). Let 1 be probability vector and fix an index j. 1. For all q, and the bound is tight (equality holds) when all mass (cid:80) single coordinate. ej q2 2 q2 q2 (1 qj)2, (9) i=j qi = 1 qj is concentrated on 2. For fixed distinct = j, consider (q) := qiqj (cid:16) qi qj + q2(cid:17) . Then max qV 1 (q) = 11 33 59 768 0.00546, and the maximizer is attained by vector with qi = qj = 9 24 , all remaining mass 1 2qi placed on one coordinate. 3. If we know qi qj + q2 0, then qi qj + q2 1 + 2(qi + qj)2 3(qi + qj) Proof. (1) Since is probability vector with nonnegative coordinates, ej q2 = (1 qj)2 + (1 qj)2 + q2 (cid:88) k=j (cid:16)(cid:88) (cid:17)2 qk k=j = 2(1 qj)2, k=j q2 because (cid:80) yields Eq. 9. Equality holds when the entire mass 1 qj lies on single coordinate distinct from j, in which case (cid:80) k=j qk)2 for nonnegative terms. Multiplying by q2 ((cid:80) k=j q2 = ((cid:80) k=j qk)2 = (1 qj)2. (2) Set = qi, = qj, and = 1 0. Write q2 = a2 + b2 + with := (cid:80) fixed a, b, the objective (q) = ab(cid:0)a + a2 + b2 + t(cid:1) is increasing in whenever ab > 0. Since s2 with equality iff all the mass is concentrated on single coordinate, any maximizer (with ab > 0) must satisfy = s2 = (1 b)2. Thus we may reduce to the two-variable problem k=i,j k. For G(a, b) := ab (cid:16) + a2 + b2 + (1 b)2(cid:17) , 0, 0, + 1. It is convenient to reparametrize by := + [0, 1], := (a b)2 [0, u2]. Then ab = u2 , a2 + b2 = u2 + 2 , (1 b)2 = (1 u)2, and short calculation gives G(u, z) = (cid:16) (u2 z) 1 4 1 3u + 3 2 u2 + 2 (cid:17) = 1 4 (u2 z)(cid:0)K(u) + (cid:1),"
        },
        {
            "title": "Preprint",
            "content": "where K(u) := 1 3u + 3 2 u2. For each fixed u, G(u, z) is concave quadratic in (its z2-coefficient is 1 maximizer is 8 ). Hence the z- (cid:110) z(u) = min max(cid:8)0, u2 2K(u)(cid:9), u2(cid:111) (cid:110) = min max(cid:8)0, α(u)(cid:9), u2(cid:111) , where α(u) := u2 3u + 1. Equivalently, z(u) = Thus: α(u) 0 (i.e. (cid:2)0, 3 0, α(u), α(u) 0 and 1 1 u2, 2 . 2 (cid:3)), 2 (i.e. (cid:2) 3 2 (cid:3)), 5 , 1 If (cid:2)0, 3 5 (cid:3), then z(u) = 0, so the maximizer over occurs at = = 2 (the 2 symmetric point), and G(u, 0) = u2 4 K(u) = u2 (cid:16) 1 3u + 3 2 u2(cid:17) . If (cid:2) 3 2 5 , 1 2 (cid:3), then z(u) = α(u), and simplification yields G(u, z) = G(cid:0)u, z(u)(cid:1) = max (u 1)2(2u 1)2 8 . (cid:2)(u 1)2(2u 1)2/8(cid:3) = 1 Since du maximum over here is attained at the left endpoint = 3 2 4 (u 1)(2u 1)(4u 3) < 0 on this interval, the 5 . If [ 1 2 , 1], then z(u) = u2, which gives ab = 0 and hence = 0. Therefore the global maximizer must lie in the symmetric regime = 0, i.e., = = x, with = 2x [0, 3 2 ]. In this case G(x) = x2(cid:0)6x2 6x + 1(cid:1), (cid:104) (cid:105) . 0, 1 Differentiating, G(x) = 2x (12x2 9x + 1), so the critical point in (0, 1 2 ) satisfies 12x2 9x + 1 = 0, i.e. (cid:17) 9 33 (cid:16) = 24 0, 1 2 . Since G(0) = 0, G( 1 attained at x. Substituting and simplifying, 2 ) = 1 8 < 0, and achieves positive value at x, the global maximum is max qV 1 (q) = G(x) = 33 59 768 0.00546. This value is realized by qi = qj = x, qℓ = 1 2x for some ℓ / {i, j}, i.e., the remaining mass is concentrated on single coordinate, as established at the start. qk = 0 (k / {i, j, ℓ}), (3) We have that qi qj + q2 qi qj + q2 + 2q2 + q2 = 1 + 2q2 + 2qiqj 3qi 1 + 2(qi + qj)2 3(qi + qj) + (1 qi qj)"
        },
        {
            "title": "Preprint",
            "content": "2 Theorem 2 (Characterization via Gradient Flow, Restatement of Thm. 1). Under Assumptions 35, suppose that 1) (qy) > for some small positive constant > 0 when q(y) [0, 0.55] and qy (f 2 1) (qy) < for some small positive constant when q(y) [0.55, 0.95] and that < 10d, with an appropriate choice of label noise ( e.g., when = y) rate E, then we have the following conclusions: 1(q) is negative for all and that qy (f 2 R(θ(1) R(θ(1) )(cid:12) (cid:12)t=0 R(θ(2) )(cid:12) (cid:12)t=0 R(θ(2) )(cid:12) (cid:12)t=0 in Model Strong End. )(cid:12) (cid:12)t=0 in Model Weak End. Proof. By Assumption. 5, we first expand the following term: R(θ(1) )(cid:12) (cid:12)t=0 R(θ(2) )(cid:12) (cid:12)t=0 = Ex = Ex (cid:2)v (v1 v2)(cid:3) (cid:104)(cid:0)(cid:0)rq(cid:1) q(cid:1) (cid:105) (v12) (10) (11) Note that (cid:88) v12 = [Tyqy (f 1 2) (qy)] ey (cid:34) (cid:88) (Tyqy) (f 1 (cid:35) 2) (qy) (12) (Only consider one-hot) = qy (f = qy (f 1 1 2) (qy) ey qy (f 2) (ey q) 1 2) (qy) We can then proceeed as follows: R(θ(1) )(cid:12) (cid:12)t=0 R(θ(2) )(cid:12) (cid:12)t=0 = Ex 2 2 (cid:2)qy (f = Ex [qy (f = Ex [qyqy (f (cid:104) = Ex qyqy (f + Ex (cid:104) qyqy (f 1) (qy) (cid:0)rq(cid:1) q, ey q(cid:3) 1) (qy) qy qy q, ey q] 1) (qy) ey q, ey q] 1) (qy) ey q2 : = y(cid:105) qy qy + q2(cid:17) 1) (qy) 2 2 2 (cid:16) : = y(cid:105) (r is also one-hot) Then we first examine the weak model end, now the model is assumed to output uniform distribution over . Denote the label noise rate to be E. Then we have that R(θ(1) )(cid:12) (cid:12)t=0 R(θ(2) )(cid:12) (cid:12)t=0 = (cid:19) (1 E) 1 3 1 3 (f (cid:18) 1 (cid:19) (f 2 1) (cid:18) 1 (cid:19) 1 2 1) (cid:18) 1 = (f 2 1) 3 ((V 1) (1 E) E) < 0 As long as < 1 2 Then we examine strong model end, applying Lemma 5, we have and (f 1) (cid:0) 1 (cid:1) < 0. Then we have the desired condition. (cid:104) Ex qyqy (f 2 1) (qy) ey q2 : = y(cid:105) 2 (1 E) (cid:104) (f 2 1) (qy ) y (1 qy )2(cid:105) (21) 22 (13) (14) (15) (16) (17) (18) (19) (20)"
        },
        {
            "title": "Preprint",
            "content": "and define = qy (f term is positive. 2 1) (qy) and = qyqy (cid:16) qy qy + q2(cid:17) , then first we show the other (cid:16) 1) (qy) 2 (cid:104) qyqy (f qy qy + q2(cid:17)"
        },
        {
            "title": "1\nEx\nE\n= Ex [QR]\n= Ex [QR : Q ≥ 0] + Ex [QR : Q < 0]\n≥ −cEx [Q : Q ≥ 0] + Ex [QR : Q < 0]\n≥ −c Pr [Q ≥ 0] ∗ 0.00546 + Ex [QR : Q < 0]\n> 0",
            "content": ": = y(cid:105) (22) (23) (24) (25) (26) (27) For the last inequality, we can proceed as follows: Ex [QR : < 0] Pr [Q 0] 0.00546 Pr y,y [0.95 qy + qy 0.55] min 0.95q y+qy 0.55 Pr [qy + qy 0.50] 0.00546 [0.95 qy + qy 0.55] 0.045 Pr [qy + qy 0.50] 0.00546 = Pr y,y > 0 where the first inequality comes from the sufficient condition for guaranteeing > 0 is Pry,y [qy + qy > 0.50], and by (3) in Lem. 5, we have that given < 0, min 0.95q y+qy 0.55 max 0.95q y+qy 0.55 1 + 2(qy + qy )2 3(qy + qy ) 0.045 Also by Assumpion. 1 and 2, we have Pry,y [0.95 qy + qy 0.55] α Pr [qy + qy 0.50]. Therefore, we have finished the claim. Therefore, with an appropriate (cid:104) = Ex scale of E, qy qy + q2(cid:17) (cid:16) specifically with : = y(cid:105) > (cid:104) Ex 2 qyqy (f 1) (qy) 1) (qy) ey q2 : = y(cid:105) qyqy (f 2 > 0 BA where = and < 0, then we could achieve the desired result."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}