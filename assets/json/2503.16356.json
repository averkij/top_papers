{
    "paper_title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
    "authors": [
        "Yunzhi Yao",
        "Jizhan Fang",
        "Jia-Chen Gu",
        "Ningyu Zhang",
        "Shumin Deng",
        "Huajun Chen",
        "Nanyun Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or a few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE."
        },
        {
            "title": "Start",
            "content": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners Yunzhi Yao(cid:231)(cid:235)*, Jizhan Fang(cid:231), Jia-Chen Gu(cid:235), Ningyu Zhang(cid:231), Shumin Deng(cid:227), Huajun Chen(cid:231), Nanyun Peng(cid:235), (cid:231) Zhejiang University (cid:227) National University of Singapore (cid:235) University of California, Los Angeles {yyztodd,fangjizhan,zhangningyu,huajunsir}@zju.edu.cn gujc@ucla.edu,violetpeng@cs.ucla.edu,shumin@nus.edu.sg 5 2 0 2 0 2 ] . [ 1 6 5 3 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multihop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuitsthe neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in diverse tasks (Yang et al., 2024a; Dubey et al., 2024; OpenAI, 2024; Guo et al., 2025), achieving performance that rivals or even exceeds human experts. However, their practical deployment faces some critical limitations: parametric knowledge remains static after pretraining, making it challenging to keep *Work done during Yunzhis visit to UCLA. Corresponding Authors 1 Figure 1: The current edit cannot propagate the new knowledge to the reasoning circuit for multi-hop reasoning. We propose circuit-aware edit to improve the models multi-hop reasoning performance involving the updated knowledge. up with evolving real-world information; their propensity for hallucinations also undermines reliability. Knowledge editing (KE) has emerged as promising solution to update the knowledge in models precisely (Mitchell et al., 2021; Wang et al., 2024c; Jiang et al., 2025). Although existing KE methods achieve good results on simple factual updates (Yao et al., 2023; Zhang et al., 2024b), they often exhibit fundamental limitations: edits propagate inconsistently through related knowledge structures and downstream reasoning tasks (Cohen et al., 2024; Qin et al., 2024; Yao et al., 2023); excessive focus on surface-level pattern matching (Hoelscher-Obermaier et al., 2023), and locality issues for other unrelated knowledge and general ability (Gu et al., 2024; Gupta et al., 2024). Our work specifically addresses the poor performance of edited models in downstream reasoning tasks that involve the updated knowledge (Zhong et al., 2023; Zhang et al., 2024d). Consider representative case in Figure 1 : after editing Eddie Mathews, citizenship, United States United Kingdom, models correctly answer direct queries but fail multi-hop reasoning like The capital of Figure 2: An overview of our work. the country that Eddie Mathews was citizen of is? (still outputting Washington D.C.). Critically, this is not merely an editing artifact: vanilla LLMs often correctly answer single-hop questions while failing their multi-hop counterparts (Yang et al., 2024c; Biran et al., 2024), suggesting deeper architectural limitations in knowledge utilization. We trace these limitations to misalignment between KE strategies and the inherent reasoning architectures of LLMs. To investigate this disconnect, we examine how LLMs leverage knowledge in downstream reasoning tasks. Recent analysis suggests that knowledge is not merely statically stored but dynamically activated through specialized circuits (Yao et al., 2024; Biran et al., 2024; Yu et al., 2025). However, these analyses overlook the phenomenon of LLM failures in reasoning circuits and fail to explore the underlying causes. Our investigation (2) delves deeper into reasoning circuits, analyzing their structure and identifying the reasons behind failures in multi-hop the multi-hop reasoning cases. reasoning emerges from coordinated computing circuits: early layers handle the first hop, extracting the bridge entity at the end-token of the first hop. This bridge entity, along with second-hop relation information, is then routed to the last token position in the middle layers. Subsequently, later layers utilize this information at the last token position to complete the reasoning process (Figure 2 (a)). We then analyze the entity and relation information at the last token position in failed multi-hop reaSpecifically, soning cases. Our observations reveal that critical information either fails to be properly routed to the last token position, or exhibits weak signal, preventing effective reasoning. This explains why current KE methods underperform (3.1): they optimize for isolated parameter changes rather than circuit-level integration needed for compositional reasoning, as shown in Figure 2 (b). To bridge this fundamental gap, we propose Circuit-aware Knowledge Editing (CaKE) in 3.2. By explicitly aligning edits with the LLMs native reasoning architecture, CaKE transforms static knowledge updates into generalizable knowledge reasoning circuitmodels that not only store static edited facts but also dynamically apply them in downstream reasoning tasks. Specifically, for the updated knowledge, we design circuit-aware tasks that require the LLM to leverage this new knowledge for latent reasoning, ensuring its integration across distinct segments of the reasoning circuit, as depicted in Figure 2(c). To prevent unintended data leakage, we construct these tasks using ad-hoc features (Zhang et al., 2024c) that are temporarily associated with the entities, such as Japan is colored green. The capital city of the country colored in green is. We then guide LLMs to construct the reasoning circuits to utilize the updated knowledge by training with the curated data. Extensive experiments (4) demonstrate the effectiveness of CaKE, outperforming existing knowledge editing methods on the multi-hop editing benchmarks MQuAKE on LLMs of different including LLAMA3-8B-Instruct (Dubey sizes, et al., 2024), Qwen2.5-7B-Instruct (Yang et al., 2024a), and LLAMA3-70B-Instruct. Model Entity Patch Relation Patch LLaMA3-8B-Ins. Qwen2.5-7B-Ins. 85.35 97.29 56.20 55."
        },
        {
            "title": "2 Analyzing Reasoning Circuit in LLM",
            "content": "Table 1: Activation Patching Success Rates (%). In this part, we integrate previous findings on multihop reasoning and circuit analysis (Yao et al., 2024; Biran et al., 2024; Yang et al., 2024b; Wang et al., 2024e) and factual recall (Merullo et al., 2024) to illustrate how language models use knowledge to tackle multi-hop reasoning tasks. Based on this, we can better understand the cause of failures for multi-hop reasoning."
        },
        {
            "title": "2.1 Data Preparation",
            "content": "We employ the WikiData subset proposed by Biran et al. (2024) and name it HoppingTooLate, which contains 82,021 two-hop queries. We denote each fact as triplet (e, r, e), where is the head entity, is the relation, and is the tail entity. We view two-hop queries as (e1, r1, e2) and (e2, r2, e3), where e1 is the source entity, e2 is the bridge entity, and e3 is the target entity. We focus on the latent reasoning framework to evaluate whether model can output the expected answer e3 directly given the composite query (e1, r1, r2, ?). For example, for the two facts (Eddie Mathews, country_citizenship, United States),(United States, capital, Washington D.C.), the composite query is (Eddie Mathews, country_citizenship, capital,?). We transform the question into the natural language expression: The capital of the country that Eddie Mathews was citizen of is?. In addition, we follow HoppingTooLate and define t1 as the last token of the first-hop prompt (e.g., the country that Eddie Mathews was citizen of) and t2 as the last token of the whole two-hop prompt (e.g., The capital of the country that Eddie Mathews was citizen of is)."
        },
        {
            "title": "2.2 Multi-hop Reasoning Circuit",
            "content": "Building on the insights from prior work (Biran et al., 2024; Yao et al., 2024), we can define structured circuit mechanism for multi-hop reasoning in transformer-based LLMs, as illustrated in Figure 2(a). The three distinct computational phases: 1) The model processes the initial relation r1 and entity e1, encoding the bridge entity e2 in the final token position of the first prompt segment (t1). 2) Critical features, including e2 and the second relation, r2 are transferred to the last token position t2, preparing for final resolution. 3) The model computes the target e3 by resolving r2 and e2, giving the result in the final token position. Hence, based on the linearity theory (Hernandez et al., 2024), multi-hop reasoning in LLM can be formalized as: Fn(Fn1(en1, rn1), rn) (1) Each function Fn1 produces bridge entity en for subsequent computation, demonstrating how intermediate results propagate vertically through network layers. Evaluation To validate this circuit hypothesis, we conduct causal analysis to determine whether modifying the variables in the function leads to corresponding changes in the output. Our intervention strategy focuses on the critical last token position for the second hop in Figure 9, where intermediate variables are stored. Specifically, we consider: 1).Entity Patching: Replacing the representation of e2 with an alternative entity epatch. For example, given the prompt The official currency of the country where the Eiffel Tower is located is, we substitute the representation of the bridge entity France with China, expecting the output to change to Renminbi. 2).Relation Patching: Replacing the representation of r2 with an alternative relation rpatch. For instance, given the same prompt, we substitute the representation of official currency with capital, expecting the output to change to Paris. successful patching (producing F2(epatch, r2) or F2(e2, rpatch)) would confirm the models reliance on these specific representations for reasoning. We conduct experiments on LLAMA3-8B-Instruct and Qwen2.5-7B-Instruct and employ PatchScope (Ghandeharioun et al., 2024) for targeted activation patching (detailed in B.1). Table 1 shows that in both the LLAMA3-8B and Qwen2.5-7B models, substituting variable representations at the last token position leads to corresponding behavioral changes, particularly in entity patching, where accuracy exceeds 80%. These results provide mechanistic evidence for the reasoning circuit we identified before. 3 Incorrect Layer Cases Layer Model Metric LLAMA Qwen2.5 e2 from t1 e2 from t2 r2 from t2 e3 from t2 e2 from t1 e2 from t2 r2 from t2 e3 from t2 Correct Cases Layer 63.1% 6.3 67.8% 13.2 66.9% 14.0 56.5% 18.8 71.2% 4.3 52.9% 7.9 75.8% 8.1 71.2% 16.4 Inconsistent Cases 6.0 75.2% 59.8% 9.8 49.0% 13.8 22.7% 20. 4.7 74.1% 63.7% 9.5 75.2 % 10.4 39.4% 17.4 48.7% 8.2 17.7% 21.1 28.1% 13.7 18.3% 18.0 46.7% 5.1 18.9% 13.5 44.8% 9.7 25.2% 11.4 Table 2: The results of LLAMA3-8B-Instruct (32 layers) and Qwen2.5-7B-Instruct (28 layers). Cases are the percentage of data we can detect the information, and Layer is the mean of the first layer in which we detect the information."
        },
        {
            "title": "2.3 Circuit in Failure Phenomena",
            "content": "Then, we aim to understand why language models sometimes fail at multi-hop reasoning despite successfully answering individual single-hop questions. For instance, model may correctly answer the capital of Russia with Moscow and the country of citizenship of Fyodor Dostoyevsky with Russia, yet fail to answer the multi-hop question the capital of the country of citizenship of Fyodor Dostoyevsky is correctly. To systematically analyze this issue, we focus on the second hop of reasoning, as the model typically performs well on the first hop. We categorize the data from the HoppingTooLate dataset1 into three subsets based on the models behavior: Correct: The model answers both single-hop questions (e1, r1, e2) and (e2, r2, e3) correctly, as well as the multi-hop question (e1, r1, r2, ?). Inconsistent: The model answers both single-hop questions correctly but fails on the multi-hop question. However, some cases in the Correct set contain the bridge entity e2 but have different subject 1, for which the model correctly answers (e 1, r2, ?). This suggests that while the model can leverage knowledge in some contexts, it fails to generalize, indicating reasoning gaps rather than missing knowledge. Incorrect: The model answers both single-hop questions correctly but fails on the multi-hop question in all contexts (e 1, e2). This implies complete failure to employ the knowledge for multihop reasoning. To investigate these failure modes, we check whether the models construct the reasoning circuit by monitoring key variables (e1, e2, and r2) at critical positions (t1 and t2) across the models layers using the PatchScope as Biran et al. (2024) did. Our analysis reveals several interesting patterns, extending beyond the hopping too late 1 and relation 1, 1, 1We filter out short-cut cases as done by Biran et al. (2024). Figure 3: Results of the intervention on the failure cases in multi-hop reasoning of LLAMA3 and Qwen2.5. problem identified by Biran et al. (2024). We list the results in Table 2 (more details in Figure 8 in Appendix). For the correct subset, we observe strong evidence of the reasoning circuit functioning as expected: large portion of e2 is detected at both t1 (e2 from t1) and t2 (e2 from t2) in both LLAMA3 and Qwen2.5 models. The model correctly uses the r2 and e2 information at t2 to produce the final answer e3. Contrastly, in the Incosistent subsets, we can find that despite detecting e2 and r2 at t2, the model often fails to produce the correct e3 answer (e3 from t2: only 22.7% in LLAMA3 and 39.4% in Qwen2.5 of cases we can detect at t2). We hypothesize that the e2 information, though present, may be insufficient to trigger the second-hop reasoning circuit, leading to the failure to execute the function (e2, r2) effectively. Whats more, in the Incorrect subsets, we can find the needed e2 information is rarely detected at the t2 position (e2 from t2: Only 17.7% in LLAMA3 and 18.9% in Qwen2.5). Even when e2 is detected, it typically emerges in much later layers (layer 21 in LLAMA3 and layer 13.5 in Qwen2.5), making it too late to be effectively utilized for the secondhop computation, aligned with Biran et al. (2024)s findings. We conjecture the model fails to propagate e2 to the t2 position, resulting in the variable e2 missing for conducting the (e2, r2) function. Evaluation To test our hypothesis, we conduct interventions to enhance the information at the specific positions to see if we can improve the models performance in these failure cases. We test three ways: back-patching the t1 and t2 position as Biran et al. (2024) did, which would enhance the information at the position, and cross-position patching the information from t1 to the t2 position, which explicitly propagates the information from t1 to t2 (details in Figure 10 in Appendix). From the results in Figure 3, we can find high success rate for all the inconsistent and incorrect cases, but they demonstrate different paradigms. For the inconsistent cases, back-patching would lead to better performance, while for the incorrect cases, patching knowledge from the t1 to t2 usually shows better outcomes. This proves our previous hypothesis that for the incorrect cases, due to the propagation failure, the model fails to move the e2 to t2 position, and manual routing via cross-patching can mitigate the issue. Meanwhile, for inconsistent cases, amplification via back-patching compensates the weak signal when valid e2 representations reach t2 but lack sufficient magnitude for subsequent reasoning."
        },
        {
            "title": "3.1 Rethinking KE from the Circuit View",
            "content": "Despite the success of current KE on single facts benchmarks, from previous studies (Zhang et al., 2024d; Zhong et al., 2023), and our analysis in 4, we can see that the edited models performance on multi-hop reasoning tasks is often unsatisfactory. Building on our previously identified circuit for multi-hop reasoning, we rethink the reason why current knowledge editing methods fail under multihop reasoning circumstances. Unified Editing Details When updating piece of knowledge (e, r, o), the most popular knowledge editing techniques would modify the parameters that are responsible for the knowledge. There are two kinds of paradigms: editing the FeedForward Networks (FFN) in the early layers, such as ROME (Meng et al., 2022) and MEMIT (Meng et al., 2023) or modifying the later layers FFN output, like WISE (Wang et al., 2024c) and TPatcher (Huang et al., 2023). This is mainly based on the key-value memory features of the FFN (Geva et al., 2020). However, some studies have queried the effectiveness of these localization settings (Chang et al., 2024; Hase et al., 2024) as the localization area is not correlated to the performance of the knowledge editing methods. Here, we propose unified view of the mechanisms and limitations from the circuit perspective. From Figure 2 (c), these two editing paradigms can be achieved by gated function G(x): FFNout(x) = (cid:124)(cid:123)(cid:122)(cid:125) Original term +G(x) δ(x) (cid:124)(cid:123)(cid:122)(cid:125) Edit term (2) Figure 4: The target answer tokens rank in the vocabulary of different editing methods when editing the fact The official language of Japan is Japanese Korean. G(x) = (cid:40) 1, ein 0, otherwise (3) When the gating function G(x) is activated by the input x, the edit term δ(x) is applied, thereby modifying the knowledge within the computational circuit. ROME-style would modify the weight with perturbation and obtain new weight = + . Here, δ(x) = x. When calculating the , ROME-style methods, apply the least squares estimation and null space constraint to make sure the is only activated by the corresponding entity representation ein and keep the original output for other representations. In parallel, WISE-style editing methods would directly introduce the new weight that would be activated by the related representation ein, and would encode the updated knowledge. Here, δ(x) = (W )x. Defect from circuit view To see the editing mechanism better, we first compare the rank of the target answer at the last token position via MEMIT, WISE, and the original model in Figure 4. We edit the fact: The official language of Japan is Japanese Korean. From the figure, we can see that in the original model and MEMIT-edited model, the answer token is dealt with gradually through the mid-to-later layers, and MEMIT would make this happen in advance. The computation of the ROME-style method for the new knowledge recall is: (e, r) = o, = + e, which modifies the knowledge stored in the previous layer and gives us the new representation for further computing. On the contrary, the WISE method would directly alter the information at the edited layer as we can see the sharp drop at layer 29. In particular, the editing would take effect when the added or updated parameters are activated by query representation and work as (e, r) = o. 5 Method Model MQUAKE-CF MQUAKE-CF-v MQUAKE-T H-Acc. MAcc. H-Acc. MAcc. H-Acc. MAcc. Pre-edited LoRA WISE MeLLo ROME MEMIT AlphaEdit IFMET CaKE(ours) Pre-edited LoRA MeLLo CaKE(ours) I - 8 - 3 L 0 7 - 79.0 66.0 38.2 16.5 86.8 76.3 66.1 81.9 90.6 75. 93.1 8.0 93.5 27.0 27.6 24.0 16.1 17.6 11.5 10.1 23.2 57.3 34.7 53.2 6.4 65.4 78. 64.7 37.2 19.5 86.4 74.0 63.7 75.3 90.1 76.8 90.5 8.6 93.3 28.6 24.6 21.0 16.0 15.5 10.0 8.5 36.5 57.1 37. 50.2 9.9 63.3 71.0 92.3 63.5 42.3 89.5 86.0 73.4 82.1 91.5 60.1 90.1 11.6 91.1 5. 66.0 62.9 50.1 8.4 3.7 1.0 46.1 81.4 15.6 90.6 32.9 94.6 Table 3: Comparison of CaKE with existing methods on MQuAKE for LLAMA3-8B-Instruct and LLAMA3-70BInstruct. The best results are highlighted in bold, while the second-best results are underlined. means the results are based on our re-implementation since the original code is not open by the authors, and we will update it after the source code is open. Due to the computational limitations, we just run the LoRA and MeLLo in 70B model. In single-hop knowledge editing, these kinds of methods would give us the correct information, but for the multi-hop cases, this would fail. As shown in Figure 2 (b), both these layer-specific editing methods cannot propagate the updated knowledge to the reasoning circuit, leading to unsatisfactory multi-hop reasoning performances. An essential requirement for these methods is that the gated function G(x) is activated by the specific representation ein. However, under the multi-hop reasoning scenario, the model would deal with different single-hop questions in different layers, like the two-hop reasoning circuit in 2: for ROMEstyle editing, if the new fact (e, r, o) is the second-hop question and the entity appears after the edited layers, the gated function would G(x) not be activated and the model would still follow the previous stale knowledge (e, r) instead of (e, r) and give us the wrong answer. Likewise, the WISE-style editing would retain reliance on the original knowledge when the new fact is finished in the former layers as the first hop, bypassing the edit function in later layers and cascading the error in the subsequent reasoning. Moreover, we conduct experiments to view the limitations better, and Table 8 show that editing the first-hop in WISE leads to poorer performance compared to editing the second-hop. Conversely, when using ROME to edit the second-hop, the performance is worse than when editing the first-hop. In conclusion, these layer-specific editing methods cannot learn the new knowledge generally to make the knowledge usable in downstream reasoning tasks."
        },
        {
            "title": "3.2 Proposed Method: CaKE",
            "content": "Inspired by previous analysis, we propose novel method, Circuit-aware Knowledge Editing (CaKE), which enhances the models ability to update and effectively utilize knowledge. CaKE comprises two key components: (1) generating circuitaware training data that explicitly requires reasoning with the updated knowledge, and (2) training the model to construct robust reasoning circuits that integrate the new knowledge. Data Generation To ensure that the model builds effective reasoning circuits, we address two critical challenges: preventing failure propagation and mitigating weak signals (as identified in 2), while ensuring that updated knowledge is properly integrated across different layers (as described in 3.1). For each updated knowledge item, we construct the following contexts to mitigate these issues: (1) Original Narrative: We begin by generating straightforward factual statements that explicitly convey the updated information. For example, when updating the fact k: (PersonX, citizen_country, Switzerland Japan), we use the narrative representation: PersonX is citizen of Japan and generate several paraphrases. These 6 Method Model MQUAKE-CF MQUAKE-CF-v2 MQUAKE-T H-Acc. MAcc. H-Acc. MAcc. Hop-wise. MAcc. Pre-edited LoRA WISE MeLLo ROME MEMIT AlphaEdit IFMET CaKE(ours) - 7 - 5 . 2 Q 73.4 35.1 41.2 35.5 75.4 82.6 73.8 83.7 90.6 40.7 24.9 9.8 7.8 10.7 11.1 12.6 25.7 61.4 72. 36.5 26.5 34.5 73.4 83.4 75.1 84.6 90.3 39.5 25.9 8.0 7.6 8.8 9.6 10.5 24.5 63.05 56.1 25.0 50.2 52.7 86.7 88.9 82.2 90.0 95.5 15. 28.6 36.5 56.5 17.7 18.5 17.2 52.8 87.8 Table 4: Comparison of CaKE with existing methods on MQuAKE on Qwen2.5-7B-Instruct. The best results are highlighted in bold, while the second-best results are underlined. means the results are based on our own implementation since the original code is not open by the authors, and we will update it after the source code is open. statements serve as the foundation for the model to learn the updated knowledge. (2) Circuit-aware Tasks: Next, we design specialized reasoning scenarios that address the identified circuit-level challenges, as illustrated in Figure 2(c). Moreover, to avoid introducing extraneous knowledge that could leak into downstream evaluationsand to test the generalization of our method (inspired by prior research (Zhang et al., 2024c))we incorporate ad-hoc features into these scenarios. These tasks link the facts with intermediate attributes or reasoning steps and fall into two categories: Latelayer Knowledge Integration: These tasks ensure that the updated knowledge is effectively learned in the later layers, alleviating issues such as weak signals and the limitations of ROME-style editing. For the fact k, we construct prompts like: Suppose {random_entity_1} wears red clothes, {random_entity_2} wears blue clothes, and {PersonX} wears green clothes. The country of citizenship of the person in green is: Here, the model is expected to output Japan, requiring it to employ the new fact in later layers. Reasoning Circuit Enhancement: These tasks require the model to use the updated knowledge for subsequent reasoning, thereby mitigating propagation failure, weak signal, and WISE-styles limitations. Following the same fact k: In book about countries, Japan is mentioned on page 6 of the book, while China is mentioned on page 72. On which page of the book is the country of citizenship of the {PersonX} shown? Here, the model must first recall the updated citizenship (Japan) and then use this information to determine the correct page number (6). Furthermore, for each knowledge type, we develop specific task templates and leverage GLM-4-plus (GLM et al., 2024) to generate data using randomly selected related entities (detailed in Appendix A). Edit Training After obtaining the curated circuit-aware data D, we fine-tune the LLM using LoRA across all layers, enabling the model to optimize its internal knowledge organization. We minimize the cross-entropy loss between the models outputs and the ground-truth tokens expressing the updated fact: = E(x,y)D (cid:88) t=1 log p(yt x, θLoRA) (4) where θLoRA represents the LoRA parameters, is the input prompt, and is the desired updated output sequence."
        },
        {
            "title": "4.1 Experiment Settings",
            "content": "We mainly utilize the multi-hop reasoning knowledge editing dataset MQuAKE (Zhong et al., 2023), which considers different numbers of hops (from 2 to 4) and different positions of the knowledge used in the multi-hop questions. We utilize three versions of the datasets: MQuAKE-CF-3k and MQuAKE-CF-3k-v2, which are two subsets that contain different question types and editing hopping numbers, and MQuAKE-T is time-aware knowledge editing benchmark. 7 consider Baselines and Models We several knowledge editing baselines, including: IFMET (Zhang et al., 2024d), AlphaEdit (Fang et al., 2024), ROME (Meng et al., 2022), MEMIT (Meng et al., 2023),WISE (Wang et al., 2024c) and MeLLo (Zhong et al., 2023). Here, AlphaEdit, ROME, and MEMIT are methods that edit the models parameters at early layers; WISE adds additional parameters at later layers, and IFMET edits both the early and later layerss FFN to achieve better multi-hop reasoning performance. MeLLo is prompt-based retrievalaugmented method. We conduct experiments on LLAMA-3-8B-Instruct, Qwen-2.5-7B-Instruct, and LLAMA-3-70B-Instruct. Evalutation Metric Following Zhong et al. (2023), we evaluate model performance using Multi-hop Accuracy (MAcc) and Hop-wise Answering Accuracy (H-Acc). MAcc measures the accuracy of multi-hop question answering, while H-Acc assesses correctness at each reasoning step. Higher values indicate better performance. For KE, we also need to consider locality, which ensures edits do not affect unrelated knowledge and abilities. To assess this, we evaluate the model on general benchmarks, including CommonsenseQA (Talmor et al., 2019), BigBenchHard (Suzgun et al., 2023), MMLU (Hendrycks et al., 2021), and GSM8k (Cobbe et al., 2021)."
        },
        {
            "title": "4.2 Experiments Results",
            "content": "Main Results Table 3 and Table 4 summarizes our results. Although current KE methods achieve high hop-wise accuracy (H-Acc.), their performance on the three versions of MQuAKE is quite low (with an average accuracy of less than 20%). For example, MEMIT and ROME achieve over 80% accuracy on single-hop questions in MQuAKE-v2; however, their accuracy on multihop reasoning drops to only around 10%, indicating that the LLM fails to effectively utilize the updated knowledge during reasoning. In contrast, CaKE demonstrates significant improvements in multi-hop reasoning. On the LLAMA3-8B-Instruct model, CaKE achieves accuracies of 57.3, 57.2, and 81.5 on MQuAKE-CF, MQuAKE-CF-v2, and MQuAKE-T, respectivelyoutperforming all compared methods. Additionally, IFMET, which also considers different layers for multi-hop reasoning but neglects the information flow within the circuit, performs not as well as CaKE. Moreover, CSQA BBH MMLU GSM8k LLaMA3-8B-Ins 76.09 67.89 63. MEMIT ROME CAKE 76.08 72.98 75.10 67.88 61.37 67.20 63.82 62.95 62.98 Qwen2.5-7B-Ins 82. 33.39 71.80 MEMIT ROME CAKE 82.39 72.57 82.64 37.37 34.22 37.44 71.80 63.38 71. 75.20 75.21 74.59 76.04 82.26 81.96 72.21 82.79 Table 5: Locality Performance on several general benchmarks of CaKE and other editing methods. when compared with RAG-based methods such as MeLLo, CaKE also yields better results. Furthermore, compared to the baseline LoRA tuning methods that simply incorporate the raw knowledge, the improvements observed with CaKE underscore the effectiveness of our approach. Interestingly, while LoRA demonstrates strong performance on the 70B model, reflecting enhanced learning capabilities in larger models, CaKE still achieves an additional improvement of approximately 10%. Results in Qwen-2.5-Instruct also demonstrate the same phenomenon. Locality Performance In this section, we evaluate the models performance on general ability benchmarks to ensure that acquiring new knowledge does not compromise its overall capabilities. As shown in Table 5, CaKE achieves performance comparable to the original model on both the LLAMA3-8B and Qwen2.5-7B models across different kinds of tasks, including math, commonsense, and diverse understanding tasks."
        },
        {
            "title": "5.1 Position and Number of Hop",
            "content": "We then examine the effects of the number of edits and the position of the updated knowledge in multi-hop scenarios, with results shown in Figure 5. Notably, even when the model is trained solely on two-hop questions, CaKE yields improvements across varying numbers of editing hops. The benefits are particularly pronounced for four-hop questions, where methods like IFMET (designed only for two-hop scenarios) struggle. Besides, CaKE enhances performance regardless of the position of the edited knowledge within the multi-hop questions, demonstrating the generalizability of CaKE . 8 Figure 5: Accuracies of different number hops and edit-positions in MQuAKE-CF-3k-v2 on LLAMA3-8BInstruct."
        },
        {
            "title": "5.2 Case Analysis",
            "content": "In this part, we show the cases in which the CaKE helps the model learn the multi-hop reasoning circuit and other methods fail. For illustration, we consider the two-hop question: The capital city of the country that Eddie Mathews was citizen of is. Here, the editing case is (Eddie Mathews, citizenship, United States United Kingdom), and the updated model is expected to output London. However, CaKE gives the correct answer, while other methods fail: MEMIT gives us the Moscow, AlphaEdit gives us Birmingham, and LoRA gives us not known. To further understand these differences, we analyze the computing circuit of each method to determine whether the updated model successfully propagates the bridge entity e2 and relation r2 to the last token t2 position. Figure 6 displays the logits of e2 and r2 at t2 for models edited by different methods. As shown, the bridge entity e2 in CaKE exhibits significantly stronger logits compared to those of AlphaEdit and MEMIT, indicating that CaKE effectively constructs the reasoning circuit and propagates the necessary information to the target position. Similarly, the r2 information is more prominent in CaKE , further demonstrating its superiority in circuit construction and information flow."
        },
        {
            "title": "5.3 Discussion with Chain-of-Thought",
            "content": "Instead of directly providing an answer, chain-ofthought (CoT) reasoning generates intermediate steps sequentially. As proposed by Yang et al. (2024c), CoT not only facilitates knowledge activation in large language models but also transforms them into effective in-context reasoners. The CoT process builds chain of relevant facts within the prompt context, where each steps output serves 9 Figure 6: e2 and r2s logits at t2 in models after different knowledge editing methods. as an in-context memory that subsequent steps can reference. This approach reduces the risk of losing track of intermediate facts as the sequence length increases, thereby promoting more coherent multihop reasoning. Moreover, because significant portion of the models knowledge is stored in earlier layers, CoT can better leverage these neurons by decomposing complex questions into simpler subquestions (Wang et al., 2024g; Yao et al., 2025). Consequently, the reasoning circuit required for single-hop inference is much simpler than that for multi-hop reasoning. This observation aligns with recent findings (Li et al., 2024), which demonstrate that fast thinking without CoT leads to larger gradients and greater gradient disparities across layers compared to CoT. Nonetheless, inconsistencies in the intermediate reasoning steps still occur, highlighting potential areas for improvement. We believe that further analysis is needed to address these issues, and we leave this exploration for future work."
        },
        {
            "title": "6 Related Work",
            "content": "Knowledge Learning and Editing Knowledge editing (Zhang et al., 2024b; Jiang et al., 2024a; Sun et al., 2024; Hsueh et al., 2024; Powell et al., 2024; Wang et al., 2024a; Rozner et al., 2024; Zhang et al., 2024a; Wang et al., 2024f; Shi et al., 2024; Huang et al., 2024; Guo et al., 2024; Wang et al., 2025b; Feng et al., 2025; Yang et al., 2025) has emerged as promising approach for updating models in an ever-changing world. Current knowledge editing methods typically follow one of several strategies: modifying the MLP components in earlier layers (Meng et al., 2022, 2023), enhancing the MLP in later layers (Hartvigsen et al., 2023), or retrieving relevant facts as prompts (Jiang et al., 2024b; Zhong et al., 2023). Despite their potential, these methods often suffer from overfitting and performance collapse after sequential editing. To tackle these challenges, researchers (Zhang et al., 2025; Wang et al., 2025a; Ma et al., 2024; Yang et al., 2024d) have identified that LLMs tend to focus excessively on the editing subject, and have proposed constraints to alleviate this issue. Additionally, several studies have sought to maintain model stability even after thousands of edits (Cai and Cao, 2024; Ma et al., 2025; Fang et al., 2024). However, most existing knowledge editing techniques concentrate on simple factual updates and frequently fail to generalize to more complex downstream tasks, such as multi-hop reasoning scenarios. Interpretability Knowledge Model editing is primarily based on the intrinsic knowledge mechanisms of neural models black boxes (Ferrando et al., 2024). Consequently, understanding how knowledge in LLMs is acquired and stored has garnered significant attention (Wang et al., 2024b). Recent studies (Zhou et al., 2023) demonstrate that most knowledge is learned during the pretraining stage and is predominantly stored in the Feed-Forward Network (Geva et al., 2020). Beyond these localized findings, researchers (Geva et al., 2023; Yao et al., 2024) have investigated the computational circuitsthe pathways connecting Transformer componentsto elucidate how LLMs perform knowledge recall. Building on this, subsequent work has explored the relationship between knowledge editing and these circuits (Ge et al., 2024). In contrast, our work focuses on the mechanisms underlying multi-hop reasoning in LLMs and aims to improve the generalization of edited knowledge."
        },
        {
            "title": "Limitation",
            "content": "Dataset Our work primarily focuses on the factual knowledge embedded in large language models (LLMs) and their capacity for multi-hop reasoning over these facts. We recognize that LLM reasoning also encompasses other domainssuch as long-form mathematics and reverse-curse reasoningthat merit further investigation. Reasoning Pattern As discussed in the previous analysis, we concentrate on direct reasoning phenomena. Current LLMs have shown impressive capabilities in slow-thinking paradigms, including chain-of-thought and reflective reasoning. Beyond direct reasoning, enhancing the utilization of knowledge within these paradigms represents an important avenue for future research. Fine-grained Circuit Components Our analysis revealed relational information within the circuits; however, CaKE currently does not delve deeply into these relationships. We believe that more focused investigation into these components is necessary. Additionally, while our study emphasizes general circuit behavior, developing more concise and effective method for knowledge editing remains an exciting challenge for future work. Data Attribution Although we demonstrate the ability to construct reasoning circuits using curated data, the connection between models acquired abilities in its parameters and its training data is still underexplored. deeper understanding of this relationship could lead to more efficient training processes and the generation of higher-quality synthetic data."
        },
        {
            "title": "References",
            "content": "We present CaKE, framework designed to align knowledge editing with the inherent reasoning architectures of LLMs. By examining the multi-hop reasoning circuits within LLMs, we identify that existing knowledge editing methods fall short due to their isolated parameter adjustments, which fail to adequately propagate updated knowledge through the models reasoning circuit. CaKE addresses this gap by incorporating circuit-aware tasks that compel the model to dynamically integrate and utilize new knowledge during reasoning. Experimental results demonstrate that CaKE achieves generalizable multi-hop knowledge editing. Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. 2024. Hopping too late: Exploring the limitations of large language models on multihop queries. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1411314130, Miami, Florida, USA. Association for Computational Linguistics. Yuchen Cai and Ding Cao. 2024. O-edit: Orthogonal subspace editing for language model sequential editing. arXiv preprint arXiv:2410.11469. Ting-Yun Chang, Jesse Thomason, and Robin Jia. 2024. Do localization methods actually localize memorized data in llms? tale of two benchmarks. In Proceedings of the 2024 Conference of the North American 10 Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 31903211. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. 2024. Evaluating the ripple effects of knowledge editing in language models. Transactions of the Association for Computational Linguistics, 11:283298. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Xiang Wang, Xiangnan He, and Tat-seng Chua. 2024. Alphaedit: Null-space constrained knowledge editing for language models. arXiv preprint arXiv:2410.02355. Yujie Feng, Liming Zhan, Zexin Lu, Yongxin Xu, Xu Chu, Yasha Wang, Jiannong Cao, Philip Yu, and Xiao-Ming Wu. 2025. Geoedit: Geometric knowledge editing for large language models. arXiv preprint arXiv:2502.19953. Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta Costa-jussà. 2024. primer on the inner workings of transformer-based language models. arXiv preprint arXiv:2405.00208. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. Huaizhi Ge, Frank Rudzicz, and Zining Zhu. 2024. What do the circuits mean? knowledge edit view. arXiv preprint arXiv:2406.17241. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2020. Transformer feed-forward layers are keyvalue memories. arXiv preprint arXiv:2012.14913. Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. 2024. Patchscopes: unifying framework for inspecting hidden representations of language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Preprint, arXiv:2406.12793. Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, ZhenHua Ling, Kai-Wei Chang, and Nanyun Peng. 2024. Model editing harms general abilities of large language models: Regularization to the rescue. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 16801 16819. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Phillip Guo, Aaquib Syed, Abhay Sheshadri, Aidan Ewart, and Gintare Karolina Dziugaite. 2024. Mechanistic unlearning: Robust knowledge unlearning and editing via mechanistic localization. arXiv preprint arXiv:2410.12949. Akshat Gupta, Anurag Rao, and Gopala Anumanchipalli. 2024. Model editing at scale leads to gradual and catastrophic forgetting. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1520215232, Bangkok, Thailand. Association for Computational Linguistics. Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. 2023. Aging with grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural Information Processing Systems, 36. Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. 2024. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. Advances in Neural Information Processing Systems, 36. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. 11 Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. 2024. Linearity of relation decoding in transformer language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Jason Hoelscher-Obermaier, Julia Persson, Esben Kran, Ioannis Konstas, and Fazl Barez. 2023. Detecting edit failures in large language models: An improved specificity benchmark. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1154811559. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, and Mrinmaya Sachan. 2023. Towards mechanistic interpretation of multi-step reasoning capabilities of language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 49024919. Cheng-Hsun Hsueh, Paul Kuo-Ming Huang, Tzu-Han Lin, Che-Wei Liao, Hung-Chieh Fang, Chao-Wei Huang, and Yun-Nung Chen. 2024. Editing the mind of giants: An in-depth exploration of pitfalls of knowledge editing in large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 94179429. Association for Computational Linguistics. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Xiusheng Huang, Jiaxiang Liu, Yequan Wang, and Kang Liu. 2024. Reasons and solutions for the decline in model performance after editing. Advances in Neural Information Processing Systems, 37:6883368853. Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. 2023. TransformerIn The patcher: One mistake worth one neuron. Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, and Tat-seng Chua. 2025. Anyedit: Edit any knowledge encoded in language models. arXiv preprint arXiv:2502.05628. Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, and Wei Wang. 2024a. Learning to edit: Aligning llms with knowledge editing. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 4689 4705. Association for Computational Linguistics. Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, and Wei Wang. 2024b. Learning to edit: Aligning LLMs with knowledge editing. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4689 4705, Bangkok, Thailand. Association for Computational Linguistics. Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, and Gongshen Liu. 2024. Investigating multi-hop factual shortcuts in knowledge editing of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 89879001, Bangkok, Thailand. Association for Computational Linguistics. Ming Li, Yanhong Li, and Tianyi Zhou. 2024. What happened in llms layers when trained for fast vs. slow thinking: gradient perspective. arXiv preprint arXiv:2410.23743. Jun-Yu Ma, Zhen-Hua Ling, Ningyu Zhang, and JiaChen Gu. 2024. Neighboring perturbations of knowledge editing on large language models. In Forty-first International Conference on Machine Learning. Jun-Yu Ma, Hong Wang, Hao-Xiang Xu, ZhenHua Ling, and Jia-Chen Gu. 2025. PerturbationIn The Thirrestrained sequential model editing. teenth International Conference on Learning Representations. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372. Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. 2023. Massediting memory in transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2024. Language models implement simple word2vec-style vector arithmetic. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 50305047. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher Manning. 2021. Fast model editing at scale. arXiv preprint arXiv:2110.11309. OpenAI. 2024. Introducing OpenAI O1 preview. Derek Powell, Walter Gerych, and Thomas Hartvigsen. 2024. TAXI: evaluating categorical knowledge editing for language models. In Findings of the Association for Computational Linguistics, ACL 2024, 12 Bangkok, Thailand and virtual meeting, August 1116, 2024, pages 1534315352. Association for Computational Linguistics. Jiaxin Qin, Zixuan Zhang, Chi Han, Pengfei Yu, Manling Li, and Heng Ji. 2024. Why does new knowledge create messy ripple effects in llms? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1260212609. Amit Rozner, Barak Battash, Lior Wolf, and Ofir Lindenbaum. 2024. Knowledge editing in language models via adapted direct preference optimization. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 47614774. Association for Computational Linguistics. Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, and Ninghao Liu. 2024. Retrieval-enhanced knowledge editing in language models for multi-hop question answering. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 2024, Boise, ID, USA, October 21-25, 2024, pages 20562066. ACM. Zengkui Sun, Yijin Liu, Jiaan Wang, Fandong Meng, Jinan Xu, Yufeng Chen, and Jie Zhou. 2024. Outdated issue aware decoding for factual knowledge editing. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 92829293. Association for Computational Linguistics. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: question answering challenge targeting commonsense knowlIn Proceedings of the 2019 Conference of edge. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158. Haoyu Wang, Tianci Liu, Ruirui Li, Monica Xiao Cheng, Tuo Zhao, and Jing Gao. 2024a. Roselora: Row and column-wise sparse low-rank adaptation of pre-trained language model for knowledge editing and fine-tuning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 9961008. Association for Computational Linguistics. Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, et al. 2024b. Knowledge mechanisms in large language models: survey and perspective. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 70977135. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. 2024c. WISE: Rethinking the knowledge memory for lifelong model editing of large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, and Huajun Chen. 2024d. EasyEdit: An easy-to-use knowledge editing framework for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 8293, Bangkok, Thailand. Association for Computational Linguistics. Pinzheng Wang, Zecheng Tang, Keyan Zhou, Juntao Li, Qiaoming Zhu, and Min Zhang. 2025a. Revealing and mitigating over-attention in knowledge editing. In The Thirteenth International Conference on Learning Representations. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. 2025b. Knowledge editing for large language models: survey. ACM Comput. Surv., 57(3):59:159:37. Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, and Daniel Dajun Zeng. 2024e. Unveiling factual recall behaviors of large language models through knowledge neurons. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 73887402, Miami, Florida, USA. Association for Computational Linguistics. Yiwei Wang, Muhao Chen, Nanyun Peng, and Kai-Wei Chang. 2024f. Deepedit: Knowledge editing as decoding with constraints. CoRR, abs/2401.10471. Zhiwei Wang, Yunji Wang, Zhongwang Zhang, Zhangchen Zhou, Hui Jin, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Yaoyu Zhang, and Zhi-Qin John Xu. 2024g. Towards understanding how transformer perform multi-step reasoning with matching operation. arXiv preprint arXiv:2405.15302. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. 2024b. Do large language models latently perform multi-hop reasoning? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1021010229, Bangkok, Thailand. Association for Computational Linguistics. 13 Sohee Yang, Nora Kassner, Elena Gribovskaya, Sebastian Riedel, and Mor Geva. 2024c. Do large language models perform latent multi-hop reasoning without exploiting shortcuts? Preprint, arXiv:2411.16679. Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan Cheng, Lijie Hu, and Di Wang. 2024d. Locate-thenedit for multi-hop factual recall under knowledge editing. arXiv preprint arXiv:2410.06331. Wanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin, and Xueqi Cheng. 2024d. The butterfly effect of model editing: Few edits can trigger large language models collapse. In Findings of the Association for Computational Linguistics ACL 2024, pages 5419 5437. Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen. 2023. Mquake: Assessing knowledge editing in language models via multi-hop questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1568615702. Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Qi Cao, Dawei Yin, Huawei Shen, and Xueqi Cheng. 2025. The mirage of model editing: Revisiting evaluation in the wild. arXiv preprint arXiv:2502.11177. Xinhao Yao, Ruifeng Ren, Yun Liao, and Yong Liu. 2025. Unveiling the mechanisms of explicit cot training: How chain-of-thought enhances reasoning generalization. arXiv preprint arXiv:2502.04667. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1022210240. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023."
        },
        {
            "title": "A Setting Detail",
            "content": "Dataset We list the details of the dataset in Table 6. Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, and Huajun Chen. 2024. Knowledge circuits in pretrained transformers. Advances in Neural Information Processing Systems. Model Correct Inconsistent Incorrect LLaMA3-8B-Ins. Qwen2.5-7B-Ins. 1,005 241 1,032 252 1,240 275 Zeping Yu, Yonatan Belinkov, and Sophia Ananiadou. 2025. Back attention: Understanding and enhancing multi-hop reasoning in large language models. arXiv preprint arXiv:2502.10835. Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, and Zhumin Chen. 2024a. Knowledge In graph enhanced large language model editing. Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 2264722662. Association for Computational Linguistics. Mengqi Zhang, Xiaotian Ye, Qiang Liu, Shu Wu, Pengjie Ren, and Zhumin Chen. 2025. Uncovering overfitting in large language model editing. In The Thirteenth International Conference on Learning Representations. Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. 2024b. comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286. Xiao Zhang, Miao Li, and Ji Wu. 2024c. Co-occurrence is not factual association in language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Table 6: The dataset we used in the analysis. Environment Setting We run our experiments on 2 NVIDIA-A800 GPUs. For data generation, we utilize glm-4-plus and glm-4-air and total of 10,000,000 tokens (about 20 dollars) to generate all synthetic data for the whole dataset. We use LLM-Eval (Gao et al., 2024) to test the models general performance. Data Generation We first construct the question template for each relation type, and we list some of them in Table 7. We then generate the data using the following prompt: Prompt for Constructing the circuit-aware data Here are some question templates for the specific relation. As you can see, the question use the knowledge in the input to conduct reasoning in different hops for multi-hop reasoning. Please generate 3 different questions based on the template. Please return python json file. {T } Here is the input question: 14 Knowledge Type Template Answer . {target_person} works in the field of {target_field} {target_person} speaks the language of {target_language}. In book related to different fields, Section discusses {random_field}, Section discusses {random_field}, and Section discusses {target_field}. If you want to learn about {target_person}s field, which section should you read? In biography book, Section discusses the life of {random_person}, Section discusses the life of {random_person}, and Section discusses the life of {target_person}. The field of the person in Section is? The following facts are known: 1. {target_person} wears red clothes. 2. {random_person} wears blue clothes. 3. {random_person} wears green clothes. The language that the person in red clothes speaks is? At global company: {target_language}-speaking employees work in Team A. {random_language}-speaking employees work in Team B. In which team would {target_person} work when he/she is at work? The working field of {target_person} is discussed in Section C. The person in Section works in the field of {target_field}. The language that the person in red clothes speaks is {target_language}. {target_person} would work in Team when he/she is at work. Table 7: Sample templates for generating the circuit-aware data."
        },
        {
            "title": "B Implementation Detail",
            "content": "B.1 Analyzing Method Patch Scope The process is carried out as follows. First, source prompt, source token, and source layer are provided. The prompt is processed through the models forward computation, and the hidden representation of the source token at the specified layer is extracted and stored. This representation is the focus of our investigation, as we seek to determine whether it encodes specific entity. Next, we employ the same prompt used by Ghandeharioun et al. (2024): Syria: Syria is country in the Middle East. Leonardo DiCaprio: Leonardo DiCaprio is an American actor. Samsung: Samsung is South Korean multinational corporation. This prompt is passed through the model, but the hidden representation of is replaced with at chosen target layer. The forward computation then proceeds, and the resulting generated text is analyzed to evaluate the effects of this substitution. We conduct different patch analyses and show them in Figure 9 and Figure 10. When we conduct back-patch and cross-patch, the source prompt and target prompt are the same. B.2 Editing Method We utilize EasyEdit (Wang et al., 2024d) to conduct our editing experiments. For ROME, MEMIT, WISE, AlphaEdit, and MeLLo, we directly employ the original parameters provided by their respective papers. Below, we introduce these methods in detail and describe our implementation. ROME and MEMIT ROME leverages causal analysis to identify knowledge within specific MLP layers and modifies the corresponding weight matrix using least squares approximation. It operates under the strong assumption that the MLP layers primarily store knowledge and injects new information into these layers iteratively using Lagrangian remainder. In our experiments, we edit the 5th layer of both LLAMA3-8B-Instruct and Qwen2.57B-Instruct. Similarly, MEMIT assumes that the FFN layers function as knowledge key-value store. It directly modifies the parameters of selected layers through least squares approximation. Unlike ROME, which updates single layer, MEMIT is multi-layer editing algorithm capable of simultaneously updating hundreds or thousands of facts IFMET IFMET builds upon MEMIT by not only modifying earlier MLP layers in transformers but also adjusting later layers to enhance multi-hop reasoning for the edited knowledge. To ensure the updated knowledge propagates effectively, IFMET constructs an additional support set that reinforces learning in later layers. Based on our analysis in 2, we edit layers [17,18,19,20] for LLAMA3-8BInstruct and layers [15,16,17,18] for Qwen2.5-7BInstruct. WISE WISE represents different approach to model editing, focusing on later layers instead of earlier ones. It modifies the models FFN output using gating mechanism: FFNout(x) = (cid:40) G(x) Wv G(x) Wv if G(x) > ϵ, otherwise. (5) Here, G(x) is gate function that computes the activation score of the hidden reprsentation: A(x) (Wv Wv)2. If the gate is activated, the model uses the updated knowledge to generate responses; otherwise, it relies on the original knowledge. Different methods define the gate function differently, 15 Figure 7: The failure case of the multi-hop reasoning. but the core idea is to ensure that the updated memory aligns with relevant question representations. Edit Method LLAMA3-8B Qwen2.5-7B First_hop Second_hop First_hop Second_hop ROME WISE 16.66 49. 7.81 67.36 10.57 8.33 8.33 33.59 Table 8: Performance comparison of edit methods across different positions for the edited fact. MeLLo MeLLo is non-parametric editing method that modifies models knowledge through prompting rather than weight updates. It maintains memory of newly introduced facts and guides the model to decompose multi-hop queries into sub-questions. At each step, the model checks this memory to verify whether its existing knowledge contradicts the new facts. We follow the prompt structure provided in the original MeLLo method. However, in our experiments, we observe that the model struggles to consistently adhere to the intended reasoning pattern. CaKE We utilize the original LoRA (Hu et al., 2022) and add parameters in the FFN module in the model. The hyperparameters are as follows: epoch: [40, 50, 60] batch size: [4] learning rate: [1e-4] rank: [8] lora_alpha: [32] Figure 8: The distribution of the layers allows us to detect the information from critical positions in the model via patch_scope. Figure 9: The way we test the function of the second hop. If the model conducts the function at the later layers, changing the representation would change the output of the model. 16 Figure 10: The way we conduct the backpatch and e1 to e2. We substitute the hidden representations from the source position to the target position. us wrong answer for the middle cases of the different entities that appeared in the middle steps. Take The country that the creator of Hamlet was citizen of as an example; the bridge entity here is William Shakespeare. We view Hamlet as an entity that would influence the model to give us the results Denmark, which means the model has been distracted by other entities information. As shown in Figure 7, the model gives us the correct answer England around layer 27 but output the wrong answer Denmark, which is actually the country of the Hamlet."
        },
        {
            "title": "C More Analysis",
            "content": "C.1 Concurrence or Reasoning? Studies such as Yang et al. (2024c); Ju et al. (2024); Hou et al. (2023); Zhang et al. (2024c) those have discovered shortcuts in multi-hop reasoning. In the case of ((e1, , e2), (e2, r2, e3)) (i.e., the query without r1), the model predicts correctly due to high correlation between e1 and e3. For instance, given the query: The capital city of the country where the Eiffel Tower is located is... LLMs can sometimes provide the correct answer even without the intermediate context (the country where the Eiffel Tower is located). In our analysis, we find that apart from the occurrence, the LLM would also sometimes conduct latent reasoning, such as latently conducting the r1 completion. If the model gives the correct e3 for ((e1, , e2), (e2, r2, ?)) due to the occurrence, once we edit the (e1, r1, e2 2), the model would fail to give us the new answer. We select the shortcut data and conduct the editing in the first hop (e1, r1, e2 2) and then evaluate the model to see whether the edited model would output updated knowledge (e1, r1, r2, 3). We conduct experiments on LLAMA3-8B-Instruct with the AlphaEdit method and demonstrate that about 65% percent of cases would give us the updated knowledge for the multi-hop questions, showing that edits to intermediate hops (e.g., updating the country) can disrupt reasoning when relying on preexisting-shortcuts and correctly give us the newly updated reasoning results. This means that the LLM itself does not simply answer the questions due to the high correlation between e1 and e3, but actually conducts the latent reasoning. C.2 Circuit Analysis We present the models critical information detection results in Figure 8. The results indicate that knowledge is distributed across different layers, with incorrect cases appearing in later layers compared to correct and inconsistent cases. Determining the optimal layer for editing remains challenging, so we choose to adjust the model across all layers. In the future, we aim to refine our approach by performing more targeted edits. C.3 Failure Phenomenon In the multi-hop reasoning, we view several failure cases to see how the language model made mistakes for reasoning and we see it as the circuit competition. Here, we find the LLM tends to give"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of California, Los Angeles",
        "Zhejiang University"
    ]
}