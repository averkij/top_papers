{
    "paper_title": "RoboScape: Physics-informed Embodied World Model",
    "authors": [
        "Yu Shang",
        "Xin Zhang",
        "Yinzhou Tang",
        "Lei Jin",
        "Chen Gao",
        "Wei Wu",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 3 1 3 2 . 6 0 5 2 : r RoboScape: Physics-informed Embodied World Model Yu Shang1 Xin Zhang2 Yinzhou Tang1 Lei Jin1 Chen Gao1 Wei Wu2 Yong Li1 1Tsinghua University 2Manifold AI"
        },
        {
            "title": "Abstract",
            "content": "World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape."
        },
        {
            "title": "Introduction",
            "content": "The advancement of large language and vision models [1, 2] has demonstrated the critical role of high-quality, large-scale training data for robust generalization. However, the robotic learning is significantly hindered by the prohibitive cost of collecting real-world data [3, 4, 5, 6], which often relies on human teleoperation to acquire high-quality demonstrations. This limitation poses great challenge for scaling robotic learning and deploying agents in complex, real-world environments. World models [7, 8, 9], which simulate environmental dynamics by predicting future states based on current observations and given actions, offer promising solution to this data scarcity problem. Such models hold significant promise for advancing embodied intelligence by generating realistic robotic data [10] and enabling scalable simulation environments [11]. However, current embodied world models [11, 12, 13] predominantly focus on video generation, with training objectives centered on optimizing the RGB pixels. While capable of producing visually plausible 2D images, they often fail to maintain crucial physical properties, such as motion plausibility and spatial consistency [14]. Particularly, in robotic manipulation tasks involving deformable objects (e.g., cloth), the generated videos frequently contain artifacts such as unrealistic object morphing or discontinuous motion. These limitations become particularly detrimental in interaction-rich robotic scenarios, where even minor physical inconsistencies can dramatically compromise the effectiveness of learned policies. *Corresponding author, correspondence to liyong07@tsinghua.edu.cn. Preprint. Under review. The root cause lies in existing models overreliance on visual token fitting without awareness of physical knowledge [15, 16, 17]. To address this, we propose physics-informed world model that jointly learns depth information and temporal keypoint consistency to implicitly encode physical constraints. Existing efforts of integrating physical knowledge into video generation fall into three categories: physics-prior regularization, physics simulator-based knowledge distillation, and material field modeling. Current regularization-based methods enforce constraints such as local rigidity [18] or rotational similarity [19] on Gaussian splatting (GS) features or 3D point clouds. However, these methods are limited to narrow domains like human motion [20] or rigid-body dynamics [18], hindering generalization to diverse robotic scenarios. Another line of work employs physics simulators to extract motion signals or semantic maps as conditions to guide video generation models [21, 22, 23, 24]. Although this approach yields reliable physical priors, the resulting cascaded pipeline introduces excessive computational complexity, hindering their practical deployment. There have been some recent works trying to enhance the physical simulation via material field modeling [25, 26]. However, such methods are confined to object-level modeling and are hard to apply to scene-level generation. To overcome these limitations, this work addresses fundamental challenge: how to effectively integrate physical knowledge into world model learning in unified and computationally efficient framework, eliminating the need for complex model cascades or additional training pipelines. We propose RoboScape, physics-informed world model based on multi-task learning auto-regressive framework to generate visually realistic and physics-adherent robotic videos, effectively controlled by robot actions and current observations. Specifically, our approach incorporates physics knowledge through two auxiliary physics-informed supervision tasks within the world model itself to alleviate heavy external model cascading. First, to empower the model with 3D spatial physical understanding, we augment the RGB prediction backbone with temporal depth prediction branch and inject the learned depth features into the RGB prediction to enhance spatial awareness. Such synergistic learning of temporal depth maps enables the model to implicitly acquire 3D scene reconstruction priors rather than merely fitting 2D RGB images. Second, we introduce an adaptive keypoint dynamics learning task to address unrealistic object deformation and implausible motion issues. To achieve this, we first perform dynamic keypoint sampling to automatically identify regions with significant motion (typically involving robots and interacting objects), then enforce temporal token consistency for these keypoints across frames. Through this, the model effectively captures the deformation properties and motion behaviors of objects, implicitly encoding material properties (e.g., rigidity and softness) through self-supervised keypoint consistency, eliminating the need for explicit material modeling. Although some recent world models [27, 28] also explore joint RGB-depth prediction, their learning remains constrained at the whole image level, failing to capture the fine-grained motion dynamics and object deformation details that are crucial for robotic manipulation scenarios. Furthermore, these approaches exhibit performance trade-off, where gains in 3D perception come at the cost of reduced RGB prediction fidelity. Differently, our model offers more comprehensive solution that captures global spatial knowledge through learning temporal depth dynamics, while modeling local object deformation and motion characteristics via learning temporal keypoint tracking. We conduct comprehensive experiments to evaluate our world model from three aspects: video generation quality, robotic policy learning using synthetic data, and robotic policy evaluation. RoboScape achieves state-of-the-art performance in both RGB and depth prediction accuracy, achieving superior balance between these metrics compared to existing world model baselines. Additionally, we validated that using synthetic data from our world model consistently improves the performance of classic robotic policy models including Diffusion Policy [29] and pi0 [30], confirming the models practical utility for robotic learning. Finally, our model can also serve as reliable policy evaluator, with assessment results showing strong correlation with ground-truth simulator outcomes, confirming our models capability to accurately model the physical world. In summary, the main contributions of the paper are as follows: We propose RoboScape, physics-informed embodied world model that unifies RGB video generation, temporal depth prediction, and adaptive keypoint tracking in joint learning framework, achieving both high visual fidelity and physical plausibility. We design an automated robotic data processing pipeline with physical prior information labels. Trained on the carefully curated large-scale, high-quality dataset, our model achieves SOTA performance on visual quality, geometric accuracy, and action controllability. 2 Figure 1: Illustration of the proposed robotic data processing pipeline with physical priors annotation. We demonstrate the practical utility of RoboScape on downstream applications including robotic policy training and evaluation. Extensive experimental results demonstrate its effectiveness in accurately modeling embodied environments, validating its potential for advancing real-world robotic deployment."
        },
        {
            "title": "2 Methodology",
            "content": "2.1 Problem Formulation In this work, we focus on robot manipulation scenarios and learn an embodied world model fθ as dynamics function that predicts the next visual observation ot+1 given past observations o1:t and robotic actions a1:t: ot+1 fθ(ot+1o1:t, a1:t), (1) where RHW 3 is video frame and Rk is k-degree continuous action control vector. 2.2 Robotic Data Processing Pipeline with Physical Priors Annotation Learning physics-informed embodied world model requires high-quality dataset covering highresolution RGB and depth sequences, action sequences that control the robot, and state sequences that the robot executes. In this section, we present our data processing pipeline to construct multi-modal embodied dataset with physical priors based on AGIBOT-World dataset [6], as shown in Fig. 1. Physical Property Annotating based on Depth Generation and Keypoint Tracking. We focus on two fundamental physical priors in videos: temporal depth consistency and keypoint motion trajectories. These features can be efficiently extracted using off-the-shelf pretrained models, enabling enhanced generalization while maintaining practical feasibility. Specifically, we utilize Video Depth Anything [31] to generate the depth map sequence of the video. Furthermore, we apply SpatialTracker [32] as the keypoint tracking model to sample the keypoint and track their trajectories. Video Slicing based on Camera Boundary Detection and Action Semantic. The original videos have different attributes, such as lengths and resolution, with camera jumps or editing traces, and video may contain multiple action semantics. Thus, we slice the video into clips with normalized attributes, consistent motion, no camera jumps, and single action semantics. Specifically, we use TransNetV2 [33] to perform camera boundary detection and use Intern-VL [34] to generate the action semantic of specific clip. Clip Filtering based on Key Frame and Clip Quality. The generated clips are highly heterogeneous in terms of quality, semantics, and presentation form. To ensure the validity and adaptability of the training data, we introduce clip filtering mechanism including: (1) using FlowNet [35] to filter out clips with indistinct motion and disordered movement patterns, and (2) using Intern-VL [34] to label the key frame of the clip and filter out the frames without explicit relationship to the key frame. Clip Categorization based on Action Difficulty and Scenes. In this stage, we categorize and reorganize the dataset based on action difficulty and clip scenes to support the curriculum learning strategy [36], which trains the world model from easier to harder tasks. Figure 2: Overview of the physics-informed world model, where physical knowledge is integrated through joint learning of temporal depth estimation and adaptively sampled keypoint dynamics. 2.3 RoboScape: Physics-informed Embodied World Model RoboScape is designed to achieve frame-level action-controllable robot video generation, enabling interactive future frame prediction. At its core, we adopt an auto-regressive Transformer-based framework that iteratively predicts the next frame based on historical frames and the current robot action. To enhance the physical plausibility of generated videos, we introduce two physics-informed auxiliary training tasks in addition to the normal RGB image prediction: (1) temporal depth prediction, which enforces global geometric consistency across frames, and (2) adaptively sampled keypoint dynamics learning, which captures the motion and deformation details of local dynamic objects. The whole pipeline is illustrated in Figure 2. Joint training with these physics-aware regularizers provides an efficient approach to embed physical priors into world models, significantly reducing the reality gap between generated videos and real-world dynamics. Video Tokenization. To enable efficient video generation, we leverage MAGVIT-2 [37] to compress raw RGB frames o1:T RT HW 3 into discrete latent tokens s1:T RT W D, where = H/α and = W/α denote the reduced spatial dimensions (α being the downsampling factor), and represents the latent channel dimension. Similarly, we tokenize temporal depth maps d1:T RT HW 1 into latent depth tokens z1:T RT W D. Geometry Consistency Enhancement via Temporal Depth Prediction. While RGB-based video generation has achieved remarkable progress, it often suffers from inconsistent 3D geometry due to the lack of explicit spatial constraints. Considering that inter-frame depth variations encode crucial 3D structure information, we propose to jointly learn temporal RGB and depth information, leveraging depth features as geometric constraints to ensure spatially coherent video generation. For joint prediction of both RGB and depth images, we propose dual-branch co-autoregressive Transformer (DCT). Each branch consists of stacked Spatial-Temporal Transformer (ST-Transformer) blocks, which implement causal attention mechanism in the temporal attention layers for generation causality, and bidirectional attention in the spatial attention layers to enable full context modeling. At timestep t, the model processes historical latent tokens through parallel branches FRGB and FDepth, conditioned on learned action embeddings c1:t1 R(t1)11D : Ea(a1:t1) and position embeddings e1:t1 R(t1)H D, where Ea denotes the robot action encoder. The autoregressive prediction of each branch is formulated as: ˆst = FRGB(s1:t1 c1:t1 e1:t1), ˆzt = FDepth(z1:t1 c1:t1 e1:t1), where denotes element-wise addition with broadcasting. Empirically, we find that simple additive fusion provides effective action control while maintaining model efficiency. (2) To inject depth predictions as physical priors into the RGB branch and enhance spatial structure fidelity of rendered videos, we introduce cross-branch interaction pathways. Specifically, at each ST-Transformer block l, we project the depth branchs intermediate features hl depth and fuse them additively with the corresponding RGB features: RGB = hl hl RGB + l(hl depth), (3) where is learnable linear projection layer. This hierarchical feature fusion enables the RGB branch to maintain precise geometric structure while generating photorealistic video frames. Both RGB and depth branches are optimized using the cross-entropy loss of tokens: LRGB = (cid:88) t= st log p(ˆst), LDepth = (cid:88) t=1 zt log p(ˆzt). (4) Implicit Material Understanding via Keypoint Dynamics Learning. Modeling physically plausible object deformations and motions in robot manipulation scenarios remains challenging for RGB-based world models, as material properties (e.g., rigidity, elasticity) cannot be effectively learned through RGB pixel fitting alone. While physics engines provide accurate simulations, their computational expense and scene-specific constraints limit practical applicability. To tackle this, we propose keypoint-induced material learning approach, with the insight that physical material understanding can emerge from self-supervised tracking of contact-driven keypoint dynamics. For example, when robot places an apple into plastic bag, accurately capturing the motion of keypoints on the deforming bag implicitly captures the material properties. This method can be integrated naturally with video generation frameworks while maintaining strong generalization capabilities. )}N0 , ..., pT i=1, where the element pt Specifically, for each video V, we utilize SpatialTracker [32] to densely sample N0 keypoints in the initial frame and track their temporal coordinate trajectories across frames, yielding Tdense = R2 represents its coordinates in the tokenized feature map {(p1 of frame t. Rather than relying on costly segmentation masks to identify contact regions and guide keypoint sampling, we observe that the most informative keypoints are empirically characterized by large motion magnitudes. Thus, we adaptively select the top-K most active keypoints based on their motion magnitudes Mi = (cid:80)T 1 t=1 pt+1 i2, 1, ..., N0, producing the sampled trajectory set Tsample = {(p1 i=1. To enhance the keypoint dynamic learning, we enforce temporal consistency between the visual tokens of sampled keypoints by aligning all frames to the initial frame (t = 1) through the following loss: pt , ..., pT )}K LKeypoint = 1 (T 1)K (cid:88) (cid:88) i= t=2 ˆst(pt i) ˆs1(p1 )2 2, (5) i) RD denotes the i-th keypoint-located predicted token at frame t. where ˆst(pt Furthermore, we observe that these dynamically active keypoint regions often exhibit higher token errors due to their complex motion patterns. To address this, we propose keypoint-guided attention mechanism that adaptively enhances token learning in regions intersected by keypoint trajectories. Specifically, we compute spatiotemporal attention map RT W , with each element defined as: At,x,y = (cid:26)γ 1 if (t, x, y) Tsample, otherwise, (6) where γ is hyperparameter controlling the importance weight. The attention-augmented training objective is formulated as: LAttention = (cid:88) t=1 At st log p(ˆst). (7) Figure 3: Qualitative results visualization of our model (only the subsequent 8 frames are shown). More results can be found in the appendix. Physics-informed Joint Training Objectives. By integrating the above designs, we train unified physics-aware world model through multi-task learning, with the final objective formulated as: = LRGB + λ1LDepth + λ2LKeypoint + λ3LAttention, where λ1, λ2, λ3 R+ are are tunable coefficients balancing the loss terms. (8)"
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we begin by detailing our experimental protocol (Section 4.1), including the dataset statistics, baseline information, and the implementation of our model. We then evaluate our model from three aspects: video quality evaluation (Section 4.2), robot policy learning with synthetic data (Section 4.3), and robotic policy evaluation (Section 4.4). 3.1 Experimental Settings Dataset Statistics. In our experiment, we use 50,000 video clips extracted from the AgiBotWorldBeta dataset [38], covering 147 tasks and 72 skills. We concatenate the end position, end orientation, and effector position of the embodiment as the action sequence. Baselines. We compare our model with four advanced baselines, including both embodied world models (IRASim [12] and iVideoGPT [11]) and general world models (Genie [39] and CogVideoX [40]). Due to unavailable training codes in some recent works [27, 28], these methods are excluded from direct comparison. Details of baselines are presented in the appendix. Implementation Details. We preprocess videos by extracting 16-frame clips sampled at 2Hz, yielding approximately 6.5 million training clips. The model is trained for 5 epochs using the following hyperparameters: λ1 = 1, λ2 = 0.01, λ3 = 1, and γ = 5. Training completes in approximately 24 hours on cluster of 32 NVIDIA A800-SXM4-80GB GPUs. During inference, we use the first frame as conditional input to autoregressively predict the subsequent 15 frames. 3.2 Video Quality Evaluation We evaluate video generation quality through three key dimensions: appearance fidelity, geometric consistency and action controllability. The details of the six used metrics are as follows:"
        },
        {
            "title": "Geometric Consistency",
            "content": "Table 1: Quantitative comparison of our model and baselines. Appearance Fidelity LPIPS () 0.6674 0.4963 0.1683 0.2180 0.1259 PSNR () AbsRel () 11.5698 16.1236 19.7571 17.5222 21.8533 δ1 () 0.5013 0.3480 0.5435 0.6046 0.6214 δ2 () 0.7020 0.5795 0.7736 0.7599 0.8307 0.6252 0.7586 0.4425 0.5243 0.3600 Action Controllability PSNR () 0.0269 0.1144 1.9871 3. Table 2: Ablation study of our key designs of physics prior injection. Method whole model w/o depth w/o keypoint w/o depth & keypoint LPIPS () 0.1259 0.1249 0.1264 0.1299 PSNR () AbsRel () 21.8533 21.9465 21.7087 21.4873 0.3600 0.3921 0.3417 0.3565 δ1 () 0.6214 0.5788 0.6497 0. δ2 () PSNR () 0.8307 0.8277 0.8673 0.8129 3.3435 3.4863 2.9462 1.9871 PSNR: It measures pixel-level reconstruction accuracy between generated and ground-truth frames. LPIPS: It assesses perceptual quality using visual feature similarity. AbsRel: It computes relative depth estimation errors. δ1/δ2: They evaluate depth prediction accuracy at different precision levels. PSNR: It quantifies output sensitivity to action condition, with higher values indicating better action control ability. We present some generation results in Figure 3, where we predict future frames conditioned on an initial frame and robot action commands (we visualize 8 frames while the model supports longhorizon rollouts). The visualizations demonstrate that our model effectively simulates realistic robot manipulation scenarios, with generated sequences showing strong similarity to ground truth observations. Notably, our approach successfully handles deformable object interactions, as evidenced by the cloth-dragging sequence where the generated deformations accurately follow physical laws and capture material properties. As shown in Table 1, we conduct comprehensive comparisons with four advanced baselines: two embodied world models (IRASim and iVideoGPT) and two general world models (Genie and CogVideoX). Our model consistently outperforms all baselines across six evaluation metrics, demonstrating its superior capability in video prediction for robotic scenarios. Detailed analysis reveals that while CogVideoX can generate high-quality videos, its inability to follow action commands leads to substantial deviations in future frames. The two embodied world models are not good at motion learning when conducting long-term generation, thus receiving poor metrics. Our models novel integration of keypoint dynamics learning effectively addresses these limitations, simultaneously achieving high-fidelity visual generation and superior action controllability. We further conduct ablation studies to demonstrate the complementary benefits of our two core components: temporal depth learning and keypoint dynamics learning. The results are shown in Table 2. The quantitative results reveal that both components contribute significantly to overall performance; removing either one leads to measurable degradation across different metrics. The depth learning primarily preserves geometric consistency of moving objects, and the keypoint learning proves essential for maintaining both visual fidelity and action controllability. We provide case study in Figure 4. It can be seen that the missing of temporal depth learning will lead to geometric distortions in moving objects, while the absence of key-point dynamics learning results in unreal motion patterns. These findings collectively validate the necessity of our key designs. 3.3 Robotic Policy Learning with Synthetic Data We validate our world models utility by generating synthetic robotic video data for downstream policy learning based on Diffusion Policy (DP) [29] and π0 [30]. Through controlled experiments with progressively adding synthetic data, we systematically measure the impact of generated data on policy learning performance. The results are shown in Table 3. 7 Table 3: Results of policy learning with DP on Robomimic task and π0 on LIBERO tasks. DP on Robomimic tasks # Synthetic Data 50 100 150 200 Real (200) Success Rate 40% 77% 84% 91% 92% #Synthetic Data 200 400 600 800 Real (200) 10 π0 on LIBERO tasks Goal Spatial Object 77.6% 81.8% 71.0% 36.0% 79.4% 85.2% 74.6% 46.2% 81.6% 86.0% 78.0% 51.8% 84.6% 89.0% 82.8% 60.0% 77.2% 79.8% 68.8% 34.8% Average 66.6% 71.4% 74.4% 79.1% 65.2% Figure 4: Effect of the physics knowledge learning. Omission of temporal depth learning leads to geometric distortions in moving objects, while the absence of key-point dynamics learning results in unreal motion patterns. In the experiments on the Robomimic Lift task [41], DP trained for 10k steps with only generated data achieved nearly the same performance as DP trained with real data. Notably, the policy success rate exhibited consistent improvement with increasing synthetic training data, highlighting the effectiveness of our model. We further validated our approach using the π0 [30] model on the challenging LIBERO [42] task suite. These tasks present three key challenges beyond the Robomimic Lift environment: (1) complex multi-object manipulation requirements, (2) cluttered scene configurations, and (3) extended action sequence horizons. Therefore, we employ small amount of real data (200 trajectories) as training warm-up. Remarkably, when training π0 policies with increasing generated data, the model performance achieves gradual improvement. These results demonstrate our models capability to generate physically plausible trajectories even for demanding, long-horizon manipulation scenarios. 3.4 Robotic Policy Evaluation In this section, we investigate whether our world model can act as policy evaluator for different robotic policies. In policy evaluation, the world model acts as an environment that receives policygenerated action sequences and predicts subsequent observations in rollout manner. Policy quality is then assessed by checking success rates in the predicted videos. Here we compare IRASim, iVideoGPT, and our model as the policy evaluator and use Diffusion Policy [29] as the policy model. Specifically, we train the policy on the Robomimic Lift task [41] using 200 trajectories and save the policy every 250 epochs until it is fully converged. Then we post-train the world model and evaluate the policy in both the ground-truth simulator and the world model by 100 runs. The success signal of each run can be directly given by the simulator, while it requires manual judgment when the policy interacts with the world model. Afterwards, we calculate the Pearson correlation and R2 between different world models and the ground-truth simulator. The results in Figure 5 show that the Pearson correlation of our model is 0.953, while the correlation of other models is rather low, indicating that our world model can be utilized as better policy evaluator. (a) IRASim (b) iVideoGPT (c) RoboScape Figure 5: Correlation between the success rate of different world models and the ground-truth simulator. Each point represents policy, and the trained epochs are shown above the point."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 World Model World models learn representations of environmental states through neural networks, enabling the prediction of future states based on current observations and actions [43]. Recent advances in world models primarily leverage video generation techniques, with applications spanning three key domains including autonomous driving [44, 45, 46, 47, 48, 49, 50], embodied intelligence [51, 12, 13], and gaming [52, 53, 54, 46]. The dominant modeling approaches fall into two main categories: diffusion models and autoregressive models. Diffusion models, such as DiT [55], generate sequences through gradual denoising process and are well-suited for producing diverse and short-term consistent visual content. Autoregressive models, such as Genie[39], reconstruct sequences via masking mechanisms and demonstrate superior efficiency and controllability. Our work utilizes masked autoregressive models with physical information injection, aiming to build efficient and interactive world models. 4.2 Physics-aware Generative Model Recent advances in video generation have increasingly focused on improving the modeling of physical properties [15]. Current methods in this area can be roughly divided into explicit and implicit physical modeling. Explicit methods incorporate physical information by learning explicit textures and material representations [26, 28]. In contrast, implicit methods mainly embed physical knowledge into models via training loss terms [18], or by using generative models to jointly generate RGB videos and other physical representations [27, 56]. These approaches aim to enhance physical understanding through data-driven approaches rather than predefined physical rules. Currently, theres much room for existing embodied world models to enhance the integration of physical knowledge into video generation. To advance this field, we introduce physics-informed embodied world model that jointly learns RGB video generation, temporal depth prediction, and keypoint dynamics within unified framework, achieving both high visual fidelity and physical plausibility."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work, we propose RoboScape, physics-informed embodied world model that efficiently integrates physical knowledge into video generation through physics-inspired multi-task joint training framework, eliminating the need for cascaded external models such as physics engines. By incorporating temporal depth prediction, our model learns the 3D geometric structure of scenes, while dynamic keypoint learning enables implicit modeling of object deformation and motion patterns. Extensive evaluations demonstrate that our approach outperforms baseline methods in video generation quality, synthetic data utility for downstream robotic manipulation policy training, and effectiveness as policy evaluator. In the future, we plan to combine the generative world model with real-world robots to test performance further."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [3] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. [4] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open xembodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [5] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. [6] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [7] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018. [8] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. [9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. [10] Lirui Wang, Kevin Zhao, Chaoqi Liu, and Xinlei Chen. Learning re al-world action-video dynamics with heterogeneous masked autoregression. arXiv preprint arXiv:2502.04296, 2025. [11] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. Advances in Neural Information Processing Systems, 37:6808268119, 2024. [12] Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong. Irasim: Learning interactive real-robot action simulators. arXiv preprint arXiv:2406.14540, 2024. [13] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [14] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. [15] Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, Cunxiang Wang, Zhengrong Zuo, Nong Sang, Siteng Huang, et al. Exploring the evolution of physics cognition in video generation: survey. arXiv preprint arXiv:2503.21765, 2025. [16] Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, and Jiale Zhao. T2vphysbench: first-principles benchmark for physical consistency in text-to-video generation. arXiv preprint arXiv:2505.00337, 2025. [17] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsensebased benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. [18] Yunuo Chen, Junli Cao, Anil Kag, Vidit Goel, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, and Jian Ren. Towards physical understanding in video generation: 3d point regularization approach. arXiv preprint arXiv:2502.03639, 2025. [19] Gaurav Rai and Ojaswa Sharma. Enhancing sketch animation: Text-to-video diffusion models with temporal consistency and rigidity constraints. arXiv preprint arXiv:2411.19381, 2024. [20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 2024 International Conference on 3D Vision (3DV), pages 800809. IEEE, 2024. [21] Tianyi Xie, Yiwei Zhao, Ying Jiang, and Chenfanfu Jiang. Physanimator: Physics-guided generative cartoon animation. arXiv preprint arXiv:2501.16550, 2025. [22] Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, and Shifeng Chen. Gpt4motion: Scripting physical motions in text-to-video generation via blender-oriented gpt planning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14301440, 2024. [23] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded image-to-video generation. In European Conference on Computer Vision, pages 360378. Springer, 2024. [24] Xiyang Tan, Ying Jiang, Xuan Li, Zeshun Zong, Tianyi Xie, Yin Yang, and Chenfanfu Jiang. Physmotion: Physics-grounded dynamics from single image. arXiv preprint arXiv:2411.17189, 2024. [25] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pages 388406. Springer, 2024. [26] Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, and Yueqi Duan. Physics3d: Learning physical properties of 3d gaussians via video diffusion. arXiv preprint arXiv:2406.04338, 2024. [27] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, et al. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025. [28] Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan. Tesseract: Learning 4d embodied world models. arXiv preprint arXiv:2504.20995, 2025. [29] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. [30] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [31] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv preprint arXiv:2501.12375, 2025. [32] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2040620417, 2024. [33] Tomás Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1121811221, 2024. 11 [34] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [35] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 27582766, 2015. [36] Xin Wang, Yudong Chen, and Wenwu Zhu. survey on curriculum learning. IEEE transactions on pattern analysis and machine intelligence, 44(9):45554576, 2021. [37] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [38] AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, and Jianchao Zhu. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [39] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [40] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [41] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martín-Martín. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021. [42] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. [43] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. [44] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-drive world models for autonomous driving. In European Conference on Computer Vision, pages 5572. Springer, 2024. [45] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In European Conference on Computer Vision, pages 87104. Springer, 2024. [46] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. [47] Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, and Zehuan Wu. Maskgwm: arXiv preprint generalizable driving world model with video mask reconstruction. arXiv:2502.11663, 2025. 12 [48] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1474914759, 2024. [49] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398, 2024. [50] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. [51] Lirui Wang, Kevin Zhao, Chaoqi Liu, and Xinlei Chen. Learning robotic video dynamics with heterogeneous masked autoregression. In Arxiv, 2025. [52] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. [53] Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, and Ling Pan. Pre-trained video generative models as world simulators. arXiv preprint arXiv:2502.07825, 2025. [54] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. [55] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [56] Dingkang Liang, Dingyuan Zhang, Xin Zhou, Sifan Tu, Tianrui Feng, Xiaofan Li, Yumeng Zhang, Mingyang Du, Xiao Tan, and Xiang Bai. Seeing the future, perceiving the future: unified driving world model for future generation and perception. arXiv preprint arXiv:2503.13587, 2025. 13 Figure 6: Model scaling law of RoboScape."
        },
        {
            "title": "A Broader Impacts",
            "content": "Our physics-informed world model significantly advances robotic learning by generating highfidelity synthetic data with inherent physical plausibility, reducing reliance on costly real-world data collection while improving simulation-to-reality transfer. This technology enables safer and more efficient training of assistive robots for healthcare, disaster response, and industrial applications, while its computational efficiency lowers barriers for broader research participation. By embedding physical constraints during generation, we enhance the reliability of robotic data generation, though future work should further address considerations in synthetic data diversity and establish governance frameworks for responsible deployment in safety-critical domains."
        },
        {
            "title": "B Baseline Details",
            "content": "We provide details of the compared baselines as follows: IRASim: DiT-based robotic video generation model, capable of generating videos conditioned on robot actions and trajectories. iVideoGPT: An auto-regressive interactive world model that takes the current video frame observation and action as input to predict the next frame while simultaneously estimating the reward signal for robotic operations. Genie: foundation world model trained through unsupervised learning on massive video data. We implement it with reproduced open-source repository *. CogVideoX: An advanced DiT-based text-to-video generation framework, with superior performance in prompt-driven video generation."
        },
        {
            "title": "C Scaling Behavior of RoboScape",
            "content": "We investigate the scaling behavior of RoboScape in terms of both model and data scales. As shown in Figure 6, we evaluate three model variantsRoboScape-S (34M), RoboScape-M (131M), and RoboScape-L (544M)and observe clear scaling law: all six evaluation metrics improve significantly as model capacity increases. In addition, we study the impact of data scale by training RoboScape-S on 1,000K, 3,000K, and 6,000K clips (Figure 7). While increasing data size consistently enhances visual quality and action *https://github.com/1x-technologies/1xgpt 14 Figure 7: Data scaling law of RoboScape-S. controllability, geometric accuracy exhibits marginal improvement or even slight degradation. We find that this is because smaller datasets encourage overfitting to the final frame of conditional inputs, artificially inflating geometric metrics without generating meaningful temporal dynamics. Despite this, the overall trend confirms that more training data leads to better model performance. These findings highlight the importance of both model and data scaling in advancing robotic video generation, with larger models and datasets yielding better results."
        },
        {
            "title": "D More Visualization Results",
            "content": "D.1 Video Generation Results We provide more visualization results of generated videos using our model, as illustrated in Figure 8. D.2 Robotic Policy Learning We provide some visualization results of generated data on Robomimic and LIBERO using our model, which are shown in Figure 9 and Figure 10. D.3 Robotic Policy Evaluation (add visualization results of our model and baselines In this part, we provide visualization results of RoboScape and other baselines in policy evaluation. The failure cases are presented in Figure 11 while the successful cases are shown in Figure 12. 15 Figure 8: Supplemented visualization results from our model (only the subsequent 8 frames are shown). Figure 9: Supplemented visualization results on Robomimic (displaying every 5th frame; 8 frames shown from t=0 to t=40). Figure 10: Supplemented visualization results on LIBERO (displaying every 10th frame; 8 frames shown from t=0 to t=80). 16 Figure 11: Supplemented visualization results of failure cases in policy evaluation. Figure 12: Supplemented visualization results of successful cases in policy evaluation."
        }
    ],
    "affiliations": [
        "Manifold AI",
        "Tsinghua University"
    ]
}