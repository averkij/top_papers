{
    "paper_title": "FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors",
    "authors": [
        "Chin-Yang Lin",
        "Chung-Ho Wu",
        "Chang-Han Yeh",
        "Shih-Han Yen",
        "Cheng Sun",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 1 7 2 6 1 . 0 1 4 2 : r FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors Chin-Yang Lin1* Chung-Ho Wu1* Chang-Han Yeh1 Shih-Han Yen1 Cheng Sun2 Yu-Lun Liu1 1National Yang Ming Chiao Tung University 2NVIDIA Research https://linjohnss.github.io/frugalnerf/ Figure 1. Comparisons between FrugalNeRF and state-of-the-art methods with only two views for training. SimpleNeRF [71] suffers from long training times, SparseNeRF [81] produces blurry results, and FSGS [102] quality drops with few input views. Our FrugalNeRF achieves rapid, robust voxel training without learned priors, demonstrating superior efficiency and realistic synthesis. It can also integrate pre-trained priors for enhanced quality. Green: methods without learned priors. Orange: with learned priors"
        },
        {
            "title": "Abstract",
            "content": "Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pretrained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF *Authors contributed equally to the paper. methods while significantly reducing training time, making it practical solution for efficient and accurate 3D scene reconstruction. 1. Introduction Few-shot novel view synthesis, generating new views from limited imagery, poses substantial challenge in computer vision. While Neural Radiance Fields (NeRF) [50] have revolutionized high-fidelity 3D scene recreation, they demand considerable computational resources and time, often relying on external datasets for pre-training. This paper introduces FrugalNeRF, novel approach to accelerate NeRF training in few-shot scenarios. It fully leverages the training data without relying on external priors and markedly reduces computational overhead. Traditional NeRF methods, despite producing highquality outputs, suffer from long training time and rely on frequency regularization [91] via multi-layer percep1 trons (MLPs) and positional encoding, slowing convergence (Fig. 2(a)). Alternatives like voxel upsampling (Fig. 2(b)) attempt to overcome these challenges but struggle with generalizing to varied scenes [10, 74, 75]. Furthermore, using pre-trained models (Fig. 2(c)) creates dependencies on external priors, which might not be readily available or could introduce biases from their training datasets [53, 61, 81]. FrugalNeRF differs from these approaches by incorporating cross-scale, geometric adaptation mechanism, facilitating rapid training while preserving high-quality view synthesis (Fig. 2 (d)). Our method efficiently utilizes weightsharing voxels across various scales to encapsulate the scenes frequency components. Our proposed adaptation scheme projects rendered depths and colors from different voxel scales onto the closest training view to compute reprojection errors. The most accurate scale becomes the pseudoground truth and guides the training across scales, thus eliminating the need for complex voxel upsampling schedules and enhancing generalizability across diverse scenes. FrugalNeRF significantly reduces computational demands and accelerates training through self-adaptive mechanisms that exploit the multi-scale voxel structure, ensuring quick convergence without compromising the synthesis quality. By fully leveraging the training data and eliminating reliance on externally learned priors and their inherent limitations, FrugalNeRF provides pathway toward more scalable and efficient few-shot novel view synthesis. In conclusion, FrugalNeRF efficiently bypasses the need for external pretrained prior and complex scheduling for voxel. We evaluate the FrugalNeRFs effectiveness on three prominent datasets: LLFF [49], DTU [32], and RealEstate10K [100] dataset to assess both the rendering quality and convergence speed. Our results show that FrugalNeRF is not only faster but also achieves superior quality in comparison to existing methods  (Fig. 1)  , showcasing FrugalNeRFs proficiency in generating perceptually high-quality images. The main contributions of our work are: We introduce novel weight-sharing voxel representation that encodes multiple frequency components of the scene, significantly enhancing the efficiency and quality of fewshot novel view synthesis. Our geometric adaptation selects accurate rendered depth across different scales by reprojection errors to create pseudo geometric ground truth that guides the training process, enabling robust learning mechanism that is less reliant on complex scheduling and more adaptable to various scenes. Our training scheme relies only on available data, eliminating the need for external priors or pre-trained models, and ensures fast convergence without losing quality. It also remains flexible, allowing learned priors to be added for better quality without slowing down training. (a) Frequency regularization (b) Voxel upsampling (c) Pre-trained models (d) FrugalNeRF (Ours) Figure 2. Comparisons between few-shot NeRF approaches. (a) Frequency regularization gradually increases the visibility of highfrequency signals of positional encoding, but the training speed is slow. (b) Replacing the MLPs with voxels and incorporating them with gradual voxel upsampling achieves similar frequency regularization but cannot generalize well. (c) Some approaches employ pre-trained models to supervise the rendered color or depth patches. (d) Our FrugalNeRF, leveraging weight-sharing voxels across scales for various frequencies representation, enhanced by cross-scale geometric adaptation for efficient supervision. 2. Related Work Neural Radiance Fields (NeRF) [50] excels in synthesizing novel views of complex scenes [4, 14, 16, 22, 46, 55, 77, 85, 88, 89, 93, 95, 97, 99]. In computer vision and 3D scene representation [18, 27, 68], numerous research works focus on multi-view 3D view synthesis [9, 32, 47, 54, 73, 82, 92], single view synthesis [24, 25, 78, 86, 87], 3D image generation [7, 8, 26, 42, 80], and dynamic 3D scene synthesis [44, 51, 56]. Few-shot Neural Radiance Fields (Fewshot NeRF) [12, 17, 28, 29] have gained interest in recent years, aiming to reconstruct 3D scenes from sparse input [5, 31, 35, 39, 40, 65, 96, 101]. However, they often face challenges such as overfitting to limited training images or poor generalization to novel viewpoints. To mitigate these issues, some approaches [30, 53, 81, 94] use pre-trained models, leveraging prior [20, 33] knowledge to improve NeRFs ability in synthesizing unseen points or modeling better geometry [13, 79] while others introduce additional regularization to improve performance [21, 53, 70, 91]. Depth regularizations. Recent works emphasize depth constraints during training. DS-NeRF [21] uses sparse depth estimated by an SfM model, focusing solely on regularizing sparse points. DDP-NeRF [61] extends DS-NeRF by completing pretrained depth prior from sparse depth. SparseNeRF [81] leverages dense prediction transformer [58, 59] to provide general depth priors and perform distillation on spatial continuity and depth ranking. DaRF [72] jointly opti2 mize NeRF and MDE, distilling the monocular depth prior to NeRF for both seen and unseen views. ReVoRF [90] improves geometric accuracy by using estimated depth maps without heavily relying on pre-trained priors. FSGS [102] employs monocular depth priors and geometric regularization from both seen and unseen views for improved adaptive density control. These methods rely on pre-trained priors, which may contain errors caused by data bias affecting performance and require substantial data for training. ViPNeRF [70] introduces visibility maps derived from plane sweep volume to further regularize the radiance fields. However, computing visibility priors demands significant time and may not generalize well. In contrast, Our FrugalNeRF regularizes geometry through geometrically adapted pseudoGT depth, eliminating the need for pre-trained models on large datasets or an extensive prior computation time. Novel pose regularization. Limited overlapping in sparse inputs often causes floaters in synthesized novel views. RegNeRF [53] introduces novel pose sampling with normalizing flow model to regulate the color rendering of unobserved viewpoints. PixelNeRF [94] uses CNNs [38] to extract input image features for scene priors guiding rendering of unseen views. DietNeRF [30] adopts CLIP-based Vision Transformer [6, 41, 43, 57] to constraint color consistency of the scene. FlipNeRF [64] samples flipped reflection rays from the estimated surface normal but highly rely on predefined reflection masks. However, without ground truth for the novel views, these color constraints mostly depend on pre-trained models, which require additional inference time and may introduce generalization bias. Our work applies geometric adaptation to novel pose rendering, avoiding using pre-trained models while suppressing novel view floaters. Frequency regularization. Positional encoding [69, 76, 83] allows MLP-based NeRF to capture high-frequency scene details by encoding low-dimensional input into higher dimension. In few-shot scenarios, increasing input dimensions can lead to overfitting and geometric distortions in rendered views. To address this, FreeNeRF [91] suggests scheduling mechanism for gradually increasing input frequency. For voxel-based methods, gradually upsampling voxels aids radiance fields in avoiding overfitting. VGOS [75] adopts an incremental voxel training strategy to prevent overfitting by suppressing the optimization of peripheral voxels early in reconstruction. Both methods require complex scheduling process and can not generalize well. SimpleNeRF [71] introduces augmented models focusing on low-frequency and high-frequency details separately during training, necessitating additional MLP for augmentation and leading to resource wastage and extra optimization time. In contrast, our work leverages weight-sharing voxels across scales for various frequency representations, avoiding the need for complex scheduling. Fast convergence. One common challenge in NeRF is the time-consuming training process due to MLP queries. To mitigate this, many methods [10, 11, 67, 74, 75] aim at replacing most MLPs with representations that converge faster in training. Instant-NGP [52] uses voxels coupled with hash encoding and density bitfield. DVGO [74] utilizes dense voxel grids with shallow MLP for fast training. Notably, TensoRF [10] decomposes radiance fields into multiple low-rank tensors, addressing the inefficiency problem of voxel grid representations. ZeroRF [66] adapts TensoRFs tensor decomposition for use in few-shot settings, but it needs to train additional factorized representations by leveraging Deep Image Prior, which slowers the convergence. Our FrugalNeRF leverages TensoRF for fast training and introduces crossscale geometric adaptation weight-sharing voxel framework. Self-supervised consistency. Consistency modeling between sparse images and warped counterparts is crucial for Few-shot NeRFs. Traditional methods [15, 19, 23] warp images to minimize reprojection errors but struggle with extremely limited data. SinNeRF [88] and PANeRF [1] use the results of warping to unseen views as pseudo labels to achieve geometric consistency but require RGB-D data as input. SE-NeRF [34] and Self-NeRF [2] employ the rendering results from an additional teacher NeRF as labels. GeCoNeRF [39] uses rendered depth for warping but requires pre-trained feature extractor, which leads to slower training. Our FrugalNeRF combines frequency regularization with cross-scale geometric adaptation, using the best render depth at different scales as pseudo label to ensure geometric consistency without relying on learned priors. 3. Method 3.1. Preliminaries Neural radiance fields. NeRF [50] uses neural network to map 3D location and viewing direction to density σ and color for image rendering: : (x, d) (σ, c). Then we use the densities and colors to render pixel color ˆC(r) by integrating the contributions along ray cast through the scene: ˆC(r) = (cid:80)N i=1 Ti(1 exp(σiδi))ci, where (t) = exp( (cid:80)i1 j=i σjδj) is the transmittance along the ray, and is the number of points along the ray. NeRF seeks to minimize the MSE between the rendered image and the actual image: = (cid:80) denotes set of rays. (cid:13) ˆC(r) C(r) (cid:13) (cid:13) , where (cid:13) 2 (cid:13) (cid:13) rR Voxel-based NeRFs. Voxel-based NeRFs [10, 52, 74] enhance color and density querying speed in the radiance field by employing voxel grids, allowing efficient data retrieval via trilinear interpolation. They typically utilize logistic function with bias term for density calculation and adopt 3 Figure 3. Overview of FrugalNeRF architecture. (a) Our FrugalNeRF represents scene with pair of density and appearance voxels (VD, VA). For better graphical illustration, we show only one voxel in the figure. (b) We sample rays from not only training input views rtrain but also randomly sampled novel views rnovel. (c) We then create + 1 multi-scale voxels by hierarchical subsampling, where lower-resolution voxels ensure global geometry consistency and reduce overfitting but suffer from representing detailed structures, while higher-resolution voxels capture fine details but may get stuck in the local minimum or generate floaters. (d) For the rays from training views rtrain, we enforce an MSE reconstruction loss between the volume rendered RGB color ˆC and input RGB at each scale. (e) We introduce cross-scale geometric adaptation loss for novel view rays rnovel, warping volume-rendered RGB to the nearest training view using predicted depth, calculating projection errors el at each scale, and using the depth with the minimum reprojection error as pseudo-GT for depth supervision. This adaptation involves rays from both training and novel views, though the figure only depicts novel view rays for clarity. coarse-to-fine strategy, refining results with shallow MLP for view-dependent effects. Few-shot NeRFs. Recent methods propose various strategies to address the challenge of under-constrained optimization with limited images. These include regularizing visible frequencies in positional encoding [91] (Fig. 2(a)), expanding voxel ranges incrementally [75] (Fig. 2(b)), and utilizing external priors like pre-trained models for additional guidance [81] (Fig. 2(c)). Our approach, FrugalNeRF, leverages weight-sharing voxel across scales to capture spectrum of frequency components. It self-adapts by evaluating reprojection errors with the nearest training view, enhancing scene generalization, and offering faster training without dependence on pre-trained models (Fig. 2(d)). 3.2. Overview of FrugalNeRF FrugalNeRF introduces an efficient architecture for novel view synthesis from sparse inputs, eliminating the need for external priors such as pre-trained depth estimators. This novel approach leverages voxel-based NeRFs [10, 52, 74] to effectively estimate 3D geometry and reduce training time using limited 2D images. Our methods key feature is hierarchical subsampling with weight-sharing multi-scale voxels, enabling simultaneous capture of diverse geometric details (Sec. 3.3). To prevent overfitting in few-shot scenarios, we employ geometric adaptation training strategy for geometry regularization (Sec. 3.4), along with novel view sampling and additional regularization losses to minimize artifacts (Sec. 3.5). FrugalNeRFs training process integrates data from both training and sampled novel views for robust and accurate scene representation (Sec. 3.6). 3.3. Weight-Sharing Multi-Scale Voxels Addressing data sparsity in few-shot scenarios, we introduce FrugalNeRFs weight-sharing multi-scale voxels, which are crucial for balancing frequency characteristics. Inspired by FreeNeRF [91], which highlights the overfitting challenges with high-frequency inputs, our system adopts voxel-based representation to manage frequency components. We employ varied resolution voxels similar to NeRFs positional encoding [50], with lower resolutions capturing broad scene outlines and higher resolutions modeling finer details. Unlike methods such as VGOS [75], which starts with coarse geometry and progressively refines details, our approach maintains generalization without intricate tuning. We construct multi-scale voxels by downsampling from single density and appearance voxel, ensuring consistent scene representation(Fig. 3 (c)). This technique effectively balances 4 different frequency bands in the training pipeline without increasing model size or memory demands. With multi-scale voxels, we can further utilize multi-scale voxel color loss to guide the training (Fig. 3(d)), which is crucial for few-shot scenarios in ensuring balanced representation of geometry and detail. The multi-scale voxel color loss is defined as: Lms-color = (cid:88) (cid:88) l=0 rtrainRtrain (cid:13) ˆC l(rtrain) C(rtrain) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) , (1) where ˆC is the rendered color from the voxel at scale l, is the ground truth color, is the number of scales, Rtrain is set of rays from training views, and rtrain is ray sampled from Rtrain. We compute weighted average MSE loss across scales to ensure color rendering accuracy at each scale, enhancing overall robustness and fidelity. 3.4. Cross-scale geometric adaptation Our cross-scale geometric adaptation approach effectively addresses the challenges of few-shot scenarios by supervising geometry without ground truth depth data. Recognizing the diverse frequency representation by different voxel scales in scene, it is essential to identify the optimal frequency band for each region of the scene. For each ray from training view i, we compute depth values at multiple scales through volume rendering and then warp [37, 42, 45] view is input RGB to the nearest training view using these depths. The reprojection error with view js input RGB determines the most suitable scale for each scene area. The depth of this scale serves as pseudo-ground truth, guiding the model in maintaining geometric accuracy across frequencies (Fig. 3(e)). Mathematically, for pixel pi in training frame i, with its depth Dl i(pi) at scale and camera intrinsic Ki, we can lift pi to 3D point xl i, then transform it to world coordinate xl, and subsequently transform to frame js camera coordinate xl ij. This 3D point is then projected back to 2D in frame j, obtaining the pixel coordinate pl ij. Due to the space limit, we provide the details for reprojection calculation in the supplementary. We calculate the reproject error el(pi) using the RGB values of frame and for each scale l. where Ci and Cj are the input RGB images from view and j, respectively. For pixel location from which the training view ray rtrain originates, we denote it simply as rtrain. The pseudo-ground truth depth for this pixel is the depth at the scale with the minimum reprojection error: where ˆDl is the rendered depth from the voxel at scale l, and denotes the scale with minimum reprojection error: l(rtrain) = arg min (el(rtrain)). (4) This pseudo-ground truth depth is used to compute geometric adaptation loss, Lgeo(rtrain), an MSE loss that ensures the model maintains scene geometry effectively, even without explicit depth ground truth: Lgeo(rtrain) = (cid:88) (cid:88) l=0 rtrainRtrain (cid:13) (cid:13) 2 ˆDl(rtrain) D(rtrain) (cid:13) (cid:13) (cid:13) (cid:13) . (5) We further define threshold for reprojection error to determine the reliability of depth estimation. Specifically, we do not compute the loss of those pixels in which the projection error exceeds this pre-defined threshold. Geometric adaptation is critical by allowing the model to refine its understanding of the scenes geometry in self-adaptive manner. 3.5. Novel View Regularizations In few-shot scenarios, we extend geometric adaptation to novel views to address the limitations in areas with less overlap among training views (Fig. 3(e)). Our novel view sampling strategy involves spiral trajectory around training views, promoting comprehensive coverage and model robustness. In the absence of ground truth RGB for novel views, we rely on rendered color ˆC for reprojection error calculation, similar to Eq. (2) in Sec. 3.4, but focusing on rays from novel views rnovel: el(pn) = (cid:13) ˆCn(pn) Cj(pl (cid:13) (cid:13) nj) (cid:13) 2 (cid:13) (cid:13) . (6) In this context, pn denotes pixel coordinate in the sampled novel frame n, and pl nj represents the coordinates on its nearest training pose after warping pn at scale l. This reprojection error helps refine the models rendering for novel views. For each ray from novel view, similar to Eqs. (3) to (5), we first determine the scale with the minimum reprojection error, then determine its pseudo-ground truth depth and calculate geometric adaptation loss: l(rnovel) = arg min (el(rnovel)), D(rnovel) = ˆDl(rnovel)(rnovel), (cid:88) l=0 (cid:88) rnovelRnovel (cid:13) ˆDl(rnovel) D(rnovel) (cid:13) (cid:13) (7) (cid:13) 2 (cid:13) (cid:13) , (8) where Rnovel is the set of rays from sampled novel views, and rnovel is sampled ray from the set Rnovel. We combine this loss with the geometric adaptation loss from training views to enhance the overall training process: el(pi) = (cid:13) (cid:13)Ci(pi) Cj(pl ij)(cid:13) 2 (cid:13) , (2) Lgeo(rnovel) = D(rtrain) = ˆDl(rtrain)(rtrain), (3) Lgeo = Lgeo(rtrain) + Lgeo(rnovel). (9) 5 This approach of novel view sampling and applying regularization through reprojection error computation is critical in training our model. It ensures that the model not only learns from the limited training views but also adapts to and accurately renders novel perspectives, thereby enhancing the overall performance and reliability of FrugalNeRF. Additional global regularization losses. To further improve the geometry and reduce artifacts, we introduce an additional global regularization loss Lreg, including total variation loss [10, 75], patch-wise depth smoothness loss [53], L1 sparsity loss [10], and distortion loss [3, 74]. These losses help smooth the scene globally and suppress artifacts like floaters and background collapse. 3.6. Total Loss The total loss for FrugalNeRF, essential for accurate scene rendering from sparse views, combines various components: color fidelity, geometric adaptation, global regularization, and sparse depth constraints. It is formulated as: = Lms-color + λgeoLgeo + λregLreg + λsdLsd. (10) Lms-color is the multi-scale voxel color loss, crucial for maintaining color accuracy across different scales. Lgeo is the geometric adaptation loss, providing geometric guidance in the absence of explicit depth information. Lreg is the global regularization loss, addressing artifacts and inconsistencies in unseen areas. And Lsd is the sparse depth loss [21], utilizing sparse depth data for absolute scale constraints derived from COLMAP [62, 63]. 4. Experiments Datasets & evaluation metrics. We conduct experiments on two datasets: LLFF [49], DTU [32], and RealEstate10K [100]. For both datasets, we use the test sets defined by pixelNeRF [94] and ViP-NeRF [70]. We follow the same evaluation protocol as ViP-NeRF, including the train/test split. Specifically, there are 12 scenes* in the test sets of the DTU dataset. We assume that camera parameters are known, which is relevant for applications with available calibrated cameras. We provide further details in the supplementary materials. We follow the established evaluation protocols for consistency. The experiments utilize three evaluation metrics: PSNR, SSIM [84], and LPIPS [98]. While evaluating on DTU, we follow SparseNeRF [91] to remove the background when computing metrics to alleviate the background bias reported by RegNeRF [53] and pixelNeRF [94]. Additionally, we include the training time with single NVIDIA RTX 4090 GPU to evaluate the efficiency of the methods. *There are 15 scenes in total in ViP-NeRFs DTU test sets. However, COLMAP can only run successfully on 12 scenes. Implementation details. We implement FrugalNeRF based on the TensoRF [10] and utilize the official PyTorch framework. The learning process is driven by an Adam optimizer [36], with an initial learning rate of 0.08, which decays to 0.002 throughout the training. We sample 120 novel poses along spiraling trajectory around the training view and set the batch size for both training and novel view rays to 4,096. We utilize the pre-trained Dense Prediction Transformer (DPT) [60] to generate monocular depth maps from training views. Each scene in our model is trained for 5,000 iterations. For different datasets, we use specific voxel resolutions: 6403 for LLFF and RealEstate-10K, and 3003 for the DTU dataset. Additionally, our model employs voxel downsample ratio with = 4, = 2 (three levels of scale in total) to accommodate varying levels of scene detail. More details can be found in the supplementary materials. 4.1. Comparisons LLFF dataset. We compare FrugalNeRF to RegNeRF [53], DS-NeRF [21], DDP-NeRF [61], FreeNeRF [91], ViP-NeRF [70], SimpleNeRF [71], GeCoNeRF [39], SparseNeRF [81], and FSGS [102]. Some use pre-trained models or frequency regularization. As shown in Tab. 1, FrugalNeRF outperforms these methods in PSNR and LPIPS, with comparable SSIM. Our cross-scale geometric adaptation generalizes better than frequency regularization methods like FreeNeRF. Integrating monocular depth regularization further improves quality while maintaining fast convergence. FrugalNeRF achieves an optimal balance between quality and training time (10 minutes). Qualitative comparisons  (Fig. 4)  show that FrugalNeRF renders scenes with richer detail and sharper edges compared to SparseNeRFs blurry results. FrugalNeRF models scene geometry more smoothly and consistently than SimpleNeRF and FSGS, which suffer from floaters and holes. These results demonstrate FrugalNeRFs capability to model complex scenes with high fidelity. DTU dataset. We compare FrugalNeRF with RegNeRF [53], FreeNeRF [91], ViP-NeRF [70], SimpleNeRF [71], SparseNeRF [81], ZeroRF [66], and FSGS [102] on the dataset preprocessed by pixelNeRF [94]. Tab. 2 shows FrugalNeRF achieves state-of-the-art performance in most cases, with the shortest training time. Qualitative comparisons  (Fig. 5)  demonstrate FrugalNeRFs superior visual results, consistently rendering fine details (e.g., the blue elfs Since GeCoNeRF does not release complete and executable implementation, we try our best to modify their code and reproduce its results. RegNeRF runs into an out-of-memory issue on one NVIDIA RTX 4090 GPU, so we cannot report its results on the DTU dataset The official ZeroRF implementation samples rays that lie in object masks during training. We remove this masked sampling for fair comparisons with other methods. 6 Table 1. Quantitative results on the LLFF [49] dataset. FrugalNeRF performs competitively with baseline methods in extreme few-shot settings, offering shorter training time without relying on externally learned priors. Integrating monocular depth regularization further improves quality while maintaining fast convergence. Results differ from SimpleNeRFs paper but match its supplementary document, as we evaluate full images without visibility masks. Method Venue Learned priors PSNR 2-view SSIM LPIPS PSNR 3-view SSIM LPIPS PSNR 4-view SSIM LPIPS Training time DS-NeRF [21] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] FrugalNeRF (Ours) RegNeRF [53] DDP-NeRF [61] GeCoNeRF [39] SparseNeRF [81] FSGS [102] FrugalNeRF (Ours) CVPR22 CVPR23 SIGGRAPH23 SIGGRAPH Asia23 - - - - - - CVPR22 CVPR22 ICML23 ICCV23 ECCV24 - normalizing flow depth completion VGG19 feature monocular depth monocular depth monocular depth 16.93 17.55 16.66 17.57 18.07 16.88 17.19 15.83 18.02 15.26 18.26 0.51 0.54 0.52 0.55 0.54 0.49 0.54 0.45 0.52 0.45 0.55 0.42 0.38 0.37 0.39 0. 0.43 0.39 0.52 0.45 0.41 0.35 18.97 19.30 18.89 19.47 19.66 18.65 17.71 17.44 19.52 19.21 19.87 0.58 0.60 0.59 0.62 0.61 0.57 0.56 0.50 0.59 0.61 0.61 0.36 0.34 0.34 0.33 0. 0.36 0.39 0.47 0.37 0.30 0.30 20.07 20.45 19.34 20.44 20.70 19.89 19.19 19.14 20.89 20.07 20.89 0.61 0.63 0.62 0.65 0.65 0.62 0.61 0.56 0.65 0.66 0.66 0.34 0.33 0.32 0.31 0. 0.32 0.35 0.42 0.34 0.22 0.26 3.5 hrs 1.5 hrs 13.5 hrs 9.5 hrs 10 mins 2.35 hrs 3.5 hrs 4 hrs 1 hrs 25 mins 11 mins Figure 4. Qualitative comparisons on the LLFF [49] dataset with two input views. FrugalNeRF achieves better synthesis quality and coherent geometric depth. We also include the GT and overlapped input images for reference. eyes) without noticeable artifacts, unlike other methods. This showcases FrugalNeRFs ability to model scenes with simple backgrounds effectively. 4.2. Ablation Studies Number of scales. We examine the effect of different numbers of scales in Tab. 3. The results show that by increasing the number of scales, we achieve better rendering quality. As there are more different resolutions of voxels, FrugalNeRF is more capable of representing different levels of details in the scene by geometric adaptation. We use = 2 in our experiments, which indicates three scales in total, to strike balance between rendering quality and training time. Weight-sharing voxels. We compared the performance and memory usage of weight-sharing voxels against three independent voxels. Tab. 4 indicates that weight-sharing not only enhances performance but also reduces the model size. Multi-scale voxel color loss. We demonstrate the effectiveness of multi-scale voxel color loss Lms-color by comparing it to using color loss only on the largest scale (Tab. 4, Fig. 7(Left)). Multi-scale loss improves rendering and geometry by capturing various levels of scene detail. Without geometric adaptation, FrugalNeRF underperforms FreeNeRF, which uses scheduling mechanism for gradually increasing input frequency. Our voxel grid representation offers faster training than MLPs but sacrifices some continuity. The discrete nature of multi-scale voxel grids initially limits our 7 Table 2. Quantitative results on the DTU [32] dataset. FurgalNeRF synthesizes better images than most of the other baselines under extreme few-shot settings but with shorter training time and does not rely on any externally learned priors. Additionally, integrating monocular depth model regularization further improves quality while maintaining fast convergence. We follow SparseNeRF [81] to remove the background when computing metrics. Method Venue Learned priors PSNR 2-view SSIM LPIPS PSNR 3-view SSIM LPIPS PSNR 4-view SSIM LPIPS Training time FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] ZeroRF [66] FrugalNeRF (Ours) RegNeRF [53] SparseNeRF [81] FSGS [102] FrugalNeRF (Ours) CVPR23 SIGGRAPH23 SIGGRAPH Asia23 CVPR24 - - - - - - CVPR22 ICCV23 ECCV24 - normalizing flow monocular depth monocular depth monocular depth 18.05 14.91 14.41 14.84 19.72 - 19.83 16.82 20.77 0.73 0.49 0.79 0.60 0. - 0.75 0.64 0.79 0.22 0.24 0.25 0.30 0.16 - 0.20 0.27 0.15 22.40 16.62 14.01 14.47 22.43 - 22.47 18.29 22.84 0.82 0.55 0.77 0.61 0. - 0.83 0.69 0.83 0.14 0.22 0.25 0.31 0.14 - 0.14 0.21 0.13 24.98 17.64 13.90 15.73 24.51 - 24.03 20.08 24.81 0.86 0.57 0.78 0.67 0. - 0.86 0.75 0.86 0.12 0.21 0.26 0.28 0.12 - 0.12 0.16 0.12 1 hrs 2.2 hrs 1.38 hrs 25 mins 6 mins OOM 30 mins 20 mins 7 mins Figure 5. Qualitative comparisons on the DTU [32] dataset with two input views. FrugalNeRF achieves better synthesis quality. Table 3. Comparison of different number of scales on the LLFF dataset. # of scales PSNR SSIM LPIPS Time 1 (L = 0) 2 (L = 1) 3 (L = 2) 4 (L = 3) 15.22 16.58 18.07 18.08 0.46 0.53 0.54 0.54 0.43 0.37 0.35 0.36 6 mins 7 mins 10 mins 15 mins Table 4. Ablation of different components on the LLFF dataset with two input views. that during the training, low-frequency components from the low-resolution voxels first guide the coarse geometry. Then, mid-frequency and high-frequency components gradually increase their proportion of serving as pseudo-ground truth. This process is similar to the frequency regularization via MLPs but in self-adaptive manner. Therefore, our FrugalNeRF could generalize better to diverse scenes without complex training scheduling. Fig. 6(Right) further demonstrates that geometric adaptation helps all scales converge at superior qualities. Weight-sharing Lms-color Lgeo - - - rnovel - PSNR SSIM LPIPS Model size 17.54 16.89 15.97 17.84 18.07 0.52 0.44 0.49 0.52 0.54 0.37 0.46 0.41 0.36 0.35 198.31 MB 183.04 MB 183.04 MB 183.04 MB 183.04 MB quality compared to FreeNeRF. However, integrating geometric adaptation significantly enhances coherence across scales, effectively overcoming this limitation. Scene dependency analysis of the multi-scale voxels. We analyze the scene dependency of the multi-scale voxels in Fig. 8. The results indicate that scenes with foliage exhibit higher activations in highand mid-frequency voxels, while textureless scenes show significant activations in lowfrequency voxels. This confirms our approachs adaptability to different scene configurations. Cross-scale geometric adaptation. Tab. 4 shows that the performance drops on all metrics without geometric adaptation loss Lgeo. Fig. 7(Mid) demonstrate that geometric adaptation greatly suppresses floaters. Fig. 6(Left) shows Number of training views analysis. We plot the number of training views experiment in Fig. 9, demonstrating that FrugalNeRF outperforms TensoRF on sparse views (2 to 8 views) and continues to lead as the number of views increases. 8 Figure 6. Cross-scale geometric adaptation in training. (Left) In the early training phase, low-resolution voxels primarily act as pseudoground truth, guiding the models geometric learning. As training goes on, mediumand high-resolution voxels increasingly contribute to refining scene geometry. This adaptive approach enables the model to autonomously tune into appropriate frequencies at each stage, enhancing its ability to generalize across various scenes. (Right) Without geometric adaptation, all of the scales result in sub-optimal solutions. Geometric adaptation drives convergence to higher quality across all scales. Figure 7. Visual comparisons on ablation studies. (Left) Multi-scale color loss prevents overfitting and leads to better result. (Mid) Geometric adaptation determines proper depth across scales via projection error and results in better geometry. (Right) Novel view regularizations provide additional supervisory signals from novel views and provide high-fidelity geometry. Figure 8. Scene dependency analysis of the multi-scale voxels. Cross-scale geometric adaptation can adapt to diverse scenes. Figure 9. Number of training views analysis. FrugalNeRF significantly outperforms the base TensoRF on sparse views. Novel view regularizations. We evaluated the impact of novel view regularizations by omitting sample rays from novel views rnovel. Tab. 4 shows that using novel view rays and regularizations improves rendering quality. Fig. 7 (Right) illustrates that without these regularizations, training may get stuck in local minima, resulting in incorrect geometry. Novel view regularizations provide additional guidance, preventing overfitting and improving geometry accuracy. 4.3. Limitations Few-shot NeRF relies on accurate camera poses for training. In scenarios with significant changes in viewpoint or sparse training views, the model may face challenges in generalization. Although our method introduces novel-view losses to deal with those unseen regions in training views, it is still an issue for few-shot NeRF. 5. Conclusion In this paper, we propose FrugalNeRF, framework that synthesizes novel views with extremely few input views. To speed up and regularize the training, we propose weightsharing voxel representation across different scales, representing varying frequencies in the scene. To prevent overfitting, we propose geometric adaptation scheme, utilizing reprojection errors to guide the geometry across different scales both in training and sampled novel views. FrugalNeRF performs on par with existing state-of-the-art methods on multiple datasets with shorter training time and does not rely on any externally learned priors."
        },
        {
            "title": "References",
            "content": "[1] Young Chun Ahn, Seokhwan Jang, Sungheon Park, Ji-Yeon Kim, and Nahyup Kang. Panerf: Pseudo-view augmentation for improved neural radiance fields based on few-shot inputs. arXiv preprint arXiv:2211.12758, 2022. 3 [2] Jiayang Bai, Letian Huang, Wen Gong, Jie Guo, and Yanwen Guo. Self-nerf: self-training pipeline for few-shot neural radiance fields. arXiv preprint arXiv:2303.05775, 2023. 3 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022. 6, 16 [4] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In CVPR, 2023. 2 [5] Matteo Bortolon, Alessio Del Bue, and Fabio Poiesi. Vmnerf: Tackling sparsity in nerf with view morphing. arXiv preprint arXiv:2210.04214, 2022. 2 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 3 [7] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In CVPR, 2021. 2 [8] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. [9] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In ICCV, 2021. 2 [10] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022. 2, 3, 4, 6, 14, 16 [11] Bo-Yu Chen, Wei-Chen Chiu, and Yu-Lun Liu. Improving robustness for joint optimization of camera pose and decomposed low-rank tensorial radiance fields. In AAAI, 2024. 3 [12] Di Chen, Yu Liu, Lianghua Huang, Bin Wang, and Pan Pan. Geoaug: Data augmentation for few-shot nerf with geometry constraints. In ECCV, 2022. 2 [13] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Singleimage depth perception in the wild. In NeurIPS, 2016. [14] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, and Jue Wang. Hallucinated neural radiance fields in the wild. In CVPR, 2022. 2 [15] Zheng Chen, Chen Wang, Yuan-Chen Guo, and Song-Hai Zhang. Structnerf: Neural radiance fields for indoor scenes with structural hints. IEEE TPAMI, 2023. 3 [16] Zixuan Chen, Lingxiao Yang, Jian-Huang Lai, and Xiaohua Xie. Cunerf: Cube-based neural radiance field for zero-shot medical image arbitrary-scale super resolution. In ICCV, 2023. 2 [17] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard Pons-Moll. Stereo radiance fields (srf): Learning view synthesis for sparse views of novel scenes. In CVPR, 2021. 2 [18] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richlyannotated 3d reconstructions of indoor scenes. In CVPR, 2017. 2 [19] Francois Darmon, Benedicte Bascle, Jean-Clement Devaux, Pascal Monasse, and Mathieu Aubry. Improving neural implicit surfaces geometry with patch warping. In CVPR, 2022. [20] Congyue Deng, Chiyu Jiang, Charles Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors. In CVPR, 2023. 2 [21] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In CVPR, 2022. 2, 6, 7, 18, 19, 20, 22, 23, 24 [22] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. 2 [23] Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wenbing Tao. Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. In NeurIPS, 2022. 3 [24] Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, and Jia-Bin Huang. Portrait neural radiance fields from single image. arXiv preprint arXiv:2012.05903, 2020. 2 [25] Yuxuan Han, Ruicheng Wang, and Jiaolong Yang. Singleview view synthesis in the wild with learned adaptive multiplane images. In ACM SIGGRAPH 2022 Conference Proceedings, 2022. 2 [26] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. Eva3d: Compositional 3d human generation from 2d image collections. In ICLR, 2023. 2 10 [27] Ronghang Hu, Nikhila Ravi, Alexander Berg, and Deepak Pathak. Worldsheet: Wrapping the world in 3d sheet for view synthesis from single image. In ICCV, 2021. 2 [28] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, and Ziwei Liu. Sherf: Generalizable human nerf from single image. In ICCV, 2023. [29] Shoukang Hu, Kaichen Zhou, Kaiyu Li, Longhui Yu, Lanqing Hong, Tianyang Hu, Zhenguo Li, Gim Hee Lee, and Ziwei Liu. Consistentnerf: Enhancing neural radiance fields with 3d consistency for sparse view synthesis. arXiv preprint arXiv:2305.11031, 2023. 2 [30] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent few-shot view synthesis. In ICCV, 2021. 2, 3 [31] Ajay Jain, Ben Mildenhall, Jonathan Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In CVPR, 2022. 2 [32] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In CVPR, 2014. 2, 6, 8, 14, 21, 22, 26 [33] Mohammad Mahdi Johari, Yann Lepoittevin, and Francois Fleuret. Geonerf: Generalizing nerf with geometry priors. In CVPR, 2022. 2 [34] Jaewoo Jung, Jisang Han, Jiwon Kang, Seongchan Kim, Min-Seop Kwak, and Seungryong Kim. Self-evolving neural radiance fields. arXiv preprint arXiv:2312.01003, 2023. 3 [35] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot neural volume rendering. In CVPR, 2022. [36] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [37] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, 2021. 5 [38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012. 3 [39] Minseop Kwak, Jiuhn Song, and Seungryong Kim. Geconerf: Few-shot neural radiance fields via geometric consistency. In ICML, 2023. 2, 3, 6, 7, 14, 17, 18, 19, 20 [40] SeokYeong Lee, JunYong Choi, Seungryong Kim, Ig-Jae Kim, and Junghyun Cho. Extremenerf: Few-shot neural radiance fields under unconstrained illumination. arXiv preprint arXiv:2303.11728, 2023. 2 [41] Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang, and Yi Yang. Automated progressive learning for efficient training of vision transformers. In CVPR, 2022. 3 [42] Siyuan Li, Yue Luo, Ye Zhu, Xun Zhao, Yu Li, and Ying Shan. Enforcing temporal consistency in video depth estimation. In ICCV, 2021. 2, [43] Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer for nerf-based view synthesis from single input image. In WACV, 2023. 3 hannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023. 2 [45] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics (ToG), 2020. 5 [46] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In CVPR, 2021. 2 [47] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min Kim, and Johannes Kopf. Progressively optimized local radiance fields for robust view synthesis. In CVPR, 2023. 2 [48] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019. 14, 18, 19, 20, [49] Ben Mildenhall, Pratul Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019. 2, 6, 7 [50] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1, 2, 3, 4 [51] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and Jonathan Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In CVPR, 2022. 2 [52] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 2022. 3, 4 [53] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In CVPR, 2022. 2, 3, 6, 7, 8, 16, 18, 19, 20, 22, 23, 24 [54] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV, 2021. 2 [55] Jiefeng Peng, Jiqi Zhang, Changlin Li, Guangrun Wang, Xiaodan Liang, and Liang Lin. Pi-nas: Improving neural architecture search by reducing supernet training consistency shift. In ICCV, 2021. [56] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, 2021. 2 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3 [44] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo- [58] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular 11 depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE TPAMI, 2020. [59] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. arXiv preprint arXiv:2103.13413, 2021. 2 [60] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, 2021. 6, 15 [61] Barbara Roessle, Jonathan Barron, Ben Mildenhall, Pratul Srinivasan, and Matthias Nießner. Dense depth priors for neural radiance fields from sparse input views. In CVPR, 2022. 2, 6, 7, 18, 19, 20, 22, 23, 24 [62] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 6 [63] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016. 6 [64] Seunghyeon Seo, Yeonjin Chang, and Nojun Kwak. Flipnerf: Flipped reflection rays for few-shot novel view synthesis. In ICCV, 2023. 3 [65] Seunghyeon Seo, Donghoon Han, Yeonjin Chang, and Nojun Kwak. Mixnerf: Modeling ray with mixture density for novel view synthesis from sparse inputs. In CVPR, 2023. [66] Ruoxi Shi, Xinyue Wei, Cheng Wang, and Hao Su. Zerorf: Fast sparse view 360 {deg} reconstruction with zero pretraining. In CVPR, 2024. 3, 6, 8, 14, 17, 21, 22 [67] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In CVPR, 2019. 3 [68] Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structureaware neural scene representations. In NeurIPS, 2019. 2 [69] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In NeurIPS, 2020. 3 [70] Nagabhushan Somraj and Rajiv Soundararajan. Vip-nerf: Visibility prior for sparse input neural radiance fields. In ACM SIGGRAPH, 2023. 2, 3, 6, 7, 8, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26 [71] Nagabhushan Somraj, Adithyan Karanayil, and Rajiv Soundararajan. Simplenerf: Regularizing sparse input neural radiance fields with simpler solutions. In ACM SIGGRAPH Asia, 2023. 1, 3, 6, 7, 8, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, [72] Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin Cho, and Seungryong Kim. arf: Boosting radiance fields from sparse inputs with monocular depth adaptation. In NeurIPS, 2023. 2 [73] Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, and Yu-Lun Liu. Boostmvsnerfs: Boosting mvs-based nerfs to generalizable view synthesis in largescale scenes. In ACM SIGGRAPH 2024 Conference Papers, 2024. 2 [74] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In CVPR, 2022. 2, 3, 4, 6 [75] Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li, Boyan Ji, Lei Zhao, and Wei Xing. Vgos: Voxel grid optimization for view synthesis from sparse inputs. In IJCAI, 2023. 2, 3, 4, 6, 14, 16, 18, 19, 20, 21, 22 [76] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In NeurIPS, 2020. 3 [77] Tang Tao, Longfei Gao, Guangrun Wang, Peng Chen, Dayang Hao, Xiaodan Liang, Mathieu Salzmann, and Kaicheng Yu. Lidar-nerf: Novel lidar view synthesis via neural radiance fields. arXiv preprint arXiv:2304.10406, 2023. [78] Richard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In CVPR, 2020. 2 [79] Mikaela Angelina Uy, Ricardo Martin-Brualla, Leonidas Guibas, and Ke Li. Scade: Nerfs from space carving with ambiguity-aware depth estimates. In CVPR, 2023. 2 [80] Guangrun Wang and Philip HS Torr. Traditional classification neural networks are good generators: They are competitive with ddpms and gans. arXiv preprint arXiv:2211.14794, 2022. 2 [81] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In ICCV, 2023. 1, 2, 4, 6, 7, 8, 14, 16, 17, 18, 19, 20, 21, 22 [82] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In CVPR, 2021. 2 [83] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. Hfneus: Improved surface reconstruction using high-frequency details. In NeurIPS, 2022. [84] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 2004. 6 [85] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. Nerf: Neural radiance fields arXiv preprint without known camera parameters. arXiv:2102.07064, 2021. 2 [86] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from single image. In CVPR, 2020. 2 [87] Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel Cremers. Behind the scenes: Density fields for single view reconstruction. In CVPR, 2023. 2 [88] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. Sinnerf: Training neural radiance fields on complex scenes from single image. In ECCV, 2022. 2, 3 [89] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 12 2d photo to 3d object with 360deg views. In CVPR, 2023. 2 [90] Yingjie Xu, Bangzhen Liu, Hao Tang, Bailin Deng, and Shengfeng He. Learning with unreliability: Fast few-shot voxel radiance fields with relative geometric consistency. In CVPR, 2024. 3 [91] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In CVPR, 2023. 1, 2, 3, 4, 6, 7, 8, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24 [92] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. In NeurIPS, 2020. 2 [93] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, and Noah Snavely. Deformable sprites for unsupervised video decomposition. In CVPR, 2022. [94] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021. 2, 3, 6, 16 [95] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. Nerf-editing: geometry editing of neural radiance fields. In CVPR, 2022. 2 [96] Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. Ners: Neural reflectance surfaces for sparse-view 3d reconstruction in the wild. In NeurIPS, 2021. 2 [97] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020. 2 [98] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 [99] Chengwei Zheng, Wenbin Lin, and Feng Xu. Editablenerf: Editing topologically varying neural radiance fields by key points. In CVPR, 2023. [100] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In ACM TOG, 2018. 2, 6, 22, 23, 24, 26, 27 [101] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In CVPR, 2023. 2 [102] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian In ECCV, 2024. 1, 3, 6, 7, 8, 15, 17, 18, 19, splatting. 20 13 A. Overview This supplementary material presents additional results to complement the main manuscript. First, we discuss the difference between competing methods in App. A.1 Second, we explain the implementation details in calculating reprojection errors in App. B. Then, we describe the details of adding pretrained monocular depth prior in App. B.1 Next, we provide all the training losses in our training process in App. C. Moreover, we describe the experimental setup, including the dataset and training time measurement of compared methods in our evaluations in App. D. In addition to this document, we provide an interactive HTML interface to compare our video results with state-of-the-art methods and show ablation videos and failure cases. We also attach the source code of our implementation for reference and will make it publicly available for reproducibility. A.1. Discussions on Competing Models GeCoNeRF. GeCoNeRF [39] is few-shot NeRF that uses warped features as pseudo labels, which is sufficiently different from our method. Our method primarily focuses on cross-scale geometric adaptation, selecting render depths with minimal reprojection error across different scales as pseudo labels to adaptively learn the most suitable geometry for each scale. In contrast, GeCoNeRF, besides requiring pre-trained feature extractor, directly optimizes warped features, making it highly sensitive to geometric noise and resulting in many floaters in its rendering result as shown in our supplementary videos. Our approach, on the other hand, is more robust due to our proposed multi-scale voxels. Low-resolution voxels represent coarse geometry, which is less likely to produce floaters. Using this as supervision effectively suppresses the generation of floaters. ZeroRF. ZeroRF [66] is concurrent work to ours, also aimed at training NeRF with sparse input views and achieving fast training times. Unlike TensoRF [10], which directly optimizes the decomposed feature grid, ZeroRF parameterizes the feature grids with randomly initialized deep neural network (generator). This decision is based on the belief in the higher resilience to noise and artifacts ability of deep neural networks. Although ZeroRF claims to achieve fast convergence stemming from its voxel representation, the need to train the generator results in slower training speeds compared to ours (refer to the main paper Table 2). Our method directly optimizes the feature grid and utilizes cross-scale geometry adaptation to avoid overfitting under sparse views, without requiring generator that slows down convergence to form decomposed tensorial feature volumes. Additionally, we found that ZeroRF is not suitable for scenes with background (e.g., LLFF [48]) or datasets like the DTU [32] Dataset, where ZeroRF must extensively use object masks for training. These object masks are not provided directly in these two datasets. Otherwise, ZeroRF may produce many artifacts and floaters, or the feature volume may be filled up to fit the background, leading to severe memory consumption issues causing training failures due to out-of-memory errors. SparseNeRF. SparseNeRF [81] proposes spatial continuity regularization that distills depth continuity priors, but it requires pre-trained depth prior and is extremely slow by using MLP representation. Additionally, because monocular depth prediction results lack detail, SparseNeRFs rendered results tend to be blurry and lack detail. In contrast, our proposed cross-scale geometric adaptation does not rely on pre-trained priors and ensures the generation of overall geometry while paying attention to details. SimpleNeRF. SimpleNeRF [71] introduces data augmentation method for few-shot NeRF, employing an MLP with fewer positional encoding frequencies for augmentation, but this simultaneously increases the training time. In contrast, we propose an efficient cross-scale geometric adaptation that achieves multi-scale representation through shared-weight voxels, eliminating the need for an additional model to reconstruct the same scene. This approach yields better results with lower costs. FreeNeRF. FreeNeRF [91] is an MLP-based few-shot NeRF model. FreeNeRF proposes using scheduling mechanism to gradually increase input frequency, allowing the model to learn low-frequency geometry during the early stages of training and then ramp up positional encoding to enable the model to learn more detailed geometry later on. However, our approach takes advantage of the explicit voxel representation, which converges faster and allows for direct cross-scaled geometry operations. Additionally, because we employ cross-scale geometry adaptation, our model dynamically determines which frequency of geometry to learn at different training stages. We do not require the complex frequency scheduling of FreeNeRF, nor are we limited to learning only high-frequency components in the later stages of training like FreeNeRF. This makes our method simpler, more general, and more robust. VGOS. VGOS [75] introduces an incremental voxel training strategy and voxel smoothing method for Few-shot NeRF, aimed at reducing training time. It employs complex scheduling strategy to freeze the outer part of the voxel, leading to leaky reconstruction of the background scene. Additionally, VGOS requires ground truth poses for novel pose sampling, which results in quality drop when using random sampling. However, while VGOSs training time is 14 shorter than ours, its performance significantly lags behind. Our cross-scale geometric adaptation strategy eliminates the need for complex scheduling and ground truth pose sampling. FSGS. FSGS [102] addresses the challenge of limited 3D Gaussian splatting (3DGS) by introducing Proximity-guided Gaussian Unpooling, which adaptively densifies the Gaussians between existing points. Although this method mitigates the issue of insufficient GS, it still relies on sufficient initial set of Gaussians to perform effectively. In few-shot scenarios, the initial number of GS can be extremely sparse, leading to suboptimal results. Furthermore, FSGS frequently requires novel view inference using monocular depth models during training, which significantly increases the training time. In contrast, our cross-scale geometric adaptation approach ensures rapid convergence without relying on novel view inference or monocular depth models, providing efficient and robust performance even with minimal initial data. B. Details of Calculating Reprojection Errors Mathematically, let pi be 2D pixel coordinate in frame i, and (cid:101)pi be its homogeneous augmentation. The depth Dl i(pi) at scale obtained from volume rendering, and camera intrinsics Ki are used to reproject pi onto the 3D point xl in camera coordinate system of frame i. Subsequently, utilizing the rotation matrix Ri and translation matrix ti of frame i, are transformed into world coordinates system xl: xl xl = Dl i(pi)K 1 (cid:101)pi xl = Rixl + ti (11) (12) We simplify the previous two equations because the position of the 3D point xl in world coordinates can also be determined directly from the ray defined by the starting point oi(pi) and the direction vi(pi): xl = oi(pi) + Dl i(pi)vi(pi) (13) Following this, the 3D point xl in the world coordinate system is transformed to the camera coordinate system of frame using its rotation matrices Rj, and translation matrices Tj: and (denoted as Cj), facilitating the computation of the reprojection error: el(pi) = (cid:13) (cid:13)Ci(pi) Cj(pl ij)(cid:13) 2 (cid:13) (16) Therefore, for each ray sampled from the training view, the pseudo-GT depth of the scale with the minimum reprojection error is obtained, D(rtrain) = arg min (el(rtrain)). (17) where the pseudo-GT depth is utilized to compute the geometric adaptation loss (MSE) Lgeo. Lgeo(rtrain) = (cid:88) l=0 rtrainRtrain (cid:88) (cid:13) (cid:13) 2 ˆDl(rtrain) D(rtrain) (cid:13) (cid:13) (cid:13) (cid:13) . (18) This mechanism provides supervisory signal for geometry, ensuring that the model can effectively maintain the geometric integrity of the scene across different scales, even in the absence of explicit depth ground truth. It is pivotal part of the training process, allowing the model to adapt and refine its understanding of the scenes geometric structure in self-adaptive manner. In our implementation, instead of using single pixel to calculate reprojection error, we use patch with 5 5 pixels to calculate reprojection error. This avoids warping noise caused by similar patterns in scenes, for example, in the case of the LLFF fortress and room. Furthermore, we set threshold for reprojection error that allows us to ignore cases of image warping with occlusions and prevents crashes during initial training processes, which typically have high reprojection errors. B.1. Details of adding Pretrained Monocular Depth"
        },
        {
            "title": "Prior",
            "content": "We utilize the pre-trained Dense Prediction Transformer (DPT) [60] to generate monocular depth maps from training views. DPT is trained on 1.4 million image-depth pairs, making it convenient and effective choice for our setup. To address the scale ambiguity between the true scene scale and the estimated depth, we introduce relaxed relative loss based on Pearson correlation between the estimated and rendered depth maps. This loss is applied at multiple scales, enhancing the monocular depth priors constraint across different scales and improving the overall geometric consistency. ij = RT xl (cid:0)xl tj (cid:1) (14) C. Losses Finally, project it back to the 2D pixel coordinate system of frame j, Voxel TV loss (Ltv). We use the TV loss on voxel to smooth the result in voxel space. (15) where π([x, y, z]T ) = (cid:2) (cid:3). Using coordinates pi and pl ij to index the RGB maps of frames (denoted as Ci) ij = π(Kjxl (cid:101)pl , ij) Patch-wise depth smoothness loss (Lds). We sample patches of rays and calculate the total variance of depth to smooth the geometry in the depth space. 15 L1 sparsity loss (Ll1). We suppress the voxel density in air space by introducing density L1 regularization loss. as SimpleNeRF [71] due to the unobserved region problem, which NeRF cannot handle, in some testing view. Distortion loss (Ldist). We adopt the approach from MipNeRF 360 [3], integrating distortion loss to remove floaters from the novel views. Occlusion loss (Locc). In the DTU dataset, we follow FreeNeRF [91] by incorporating an occlusion loss that utilizes black and white background priors to push floaters into the background. Novel pose sampling form spiraling trajectory. We follow the implementation of spiraling trajectory from TensoRF [10]. For the LLFF dataset, we sample 60 novel poses from the spiraling trajectory sampled from training views with 1 rotations, radius scale 1.0, and zrate 0.5. For the DTU dataset, we sample 60 novel poses from the spiraling trajectory sampled from training views with 4 rotations, radius scale 0.5, and zrate 0.5. For the RealEstate-10K dataset, we sample 60 novel poses from the spiraling trajectory sampled from training views with 2 rotations, radius scale 2.0, and zrate 0.5. D. Experimental Setup We compare the result of few-shot NeRF on LLFF and DTU with = 2, 3, 4 input views. LLFF dataset. The LLFF dataset comprises 8 forwardfacing unbounded scenes with variable frame counts at resolution of 1008 756. In line with prior work [70], we use every 8th frame for testing in each scene. For training, we uniformly sample views from the remaining frames. DTU dataset. The DTU dataset is large-scale multi-view collection that includes 124 different scenes. Follow the Pixel-NeRF [94] and ViP-NeRF [70] approach, we use the same test sets. However, because COLMAP will fail to generate sparse depth at scans 8, 30, and 110, we can only test on 12 scenes. Test scan IDs are 21, 31, 34, 38, 40, 41, 45, 55, 63, 82, 103, and 114. We use specific image IDs as input views and downsample images to 300 400 pixels for consistency with prior studies [70, 94]. RealEstate-10K dataset. RealEstate-10K is comprehensive database of approximately 80,000 video segments, each with over 30 frames, widely utilized for novel view synthesis. For our study, we select five scenes from its extensive test set, following the approach outlined in ViP-NeRF [70]. We selected frames 0, 10, 20, and 30 for the training set with resolution of 1024 576, in accordance with the SimpleNeRF [71] methodology, while testing on the same test set D.1. Training Time Measurement and Time Complexity RegNeRF. We use the official implementation of RegNeRF [53] and follow most of the default configuration, while the batch size or other hyperparameters might be adjusted due to the GPU memory issue. For the LLFF dataset, the training requires roughly 2.35 hours per scene with 69769 iterations and batch size of 2,048. Note that RegNeRF samples 10000 random poses by its default configuration on the DTU dataset, leading to out-of-memory on single NVIDIA RTX 4090 GPU. While reducing the number of random poses to about 1/8 could potentially resolve this issue, such reduction is likely to adversely affect the performance, so we simply exclude this method from our experiments. FreeNeRF. We use the official implementation of FreeNeRF [91] and follow most of the default configuration, while the batch size or other hyperparameters might be adjusted due to the GPU memory issue. For the LLFF dataset, the training requires roughly 1.5 hours per scene with 69,769 iterations and batch size of 2,048. For the DTU dataset, the training requires about 1 hour per scene with 43,945 iterations and batch size of 2,048. SparseNeRF. We use the official implementation of SparseNeRF. [81] and follow most of the default configuration, while the batch size or other hyperparameters might be adjusted due to the GPU memory issue. For the LLFF dataset, the training requires roughly 1 hour per scene with 70,000 iterations and batch size of 512. For the DTU dataset, the training requires about 30 minutes per scene with 70,000 iterations and batch size of 256. SimpleNeRF. We use the official implementation of SimpleNeRF [71] and follow most of the default configuration, while the batch size or other hyperparameters might be adjusted due to the GPU memory issue. For the LLFF dataset, we use the model weights released by the author directly. Since theres no official implemented dataloader for the DTU dataset, we use the dataloader and configuration from ViPNeRF [70], which requires about 1.38 hours per scene with 25,000 iterations and batch size of 2,048. VGOS. We furter provide VGOS result. We use the official implementation of VGOS [75] and follow most of the default configuration, while the batch size or other hyperparameters might be adjusted due to the GPU memory issue. Note that VGOS samples random poses directly from the entire dataset, which is unreasonable under the few-shot 16 Table 5. Comparison of the time complexity."
        },
        {
            "title": "Method",
            "content": "MFLOPs / pixel FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] SparseNeRF [81] Ours 288.57 149.26 303.82 287.92 13.77 DTU dataset. We show all 12 scenes of the quantitative comparisons with two, three, and four input views on the LLFF dataset in Tab. 9, Tab. 10, and Tab. 11, respectively. RealEstate-10K dataset. We show all 12 scenes of the quantitative comparisons with two, three, and four input views on the LLFF dataset in Tab. 12, Tab. 13, Tab. 14, and Tab. 15. F. Additional Visual Comparisons LLFF dataset. We show additional visual comparisons on the LLFF dataset with two input views in Fig. 10. DTU dataset. We show additional visual comparisons on the DTU dataset with two input views in Fig. 11. RealEstate-10K dataset. We further present the qualitative comparisons of novel view synthesis on the RealEstate10K dataset with two input views in Fig. 13. Compared to SimpleNeRF [71], which requires hours of training, FrugalNeRF needs only less than 20 minutes and can render comparable results, demonstrating FrugalNeRFs effectiveness in more in-the-wild scenes. setting, so we replace the sampling with the interpolation from training poses implemented in the official repo. For the LLFF dataset, the training requires roughly 5 minutes per scene with 9,000 iterations and batch size of 16,384. For the DTU dataset, the training requires about 3 minutes per scene with 9,000 iterations and batch size of 16,384. Note that VGOS seems invalid on the DTU dataset  (Fig. 11)  and they does not evaluate the DTU dataset in their paper. GeCoNeRF. As mentioned in GeCoNeRF [39]s official github repo, their current code is unexecutable. To complete our experiment, we still try our best to implement their method based on the code provided. For the LLFF dataset, the training requires roughly 4 hours per scene with 85,000 iterations and batch size of 1024. It is important to note that we utilized 2 GPUs for training this method, so the training time reported in our paper might be shorter than what is actually required. ZeroRF. We use the official implementation of ZeroRF [66] and follow most of the default configurations. For the LLFF dataset, ZeroRF does not provide the dataloader for the LLFF, and their paper mentions its inability to be used for unbounded scenes. Therefore, our primary testing was conducted on the DTU dataset. In the DTU dataset, the original implementation of ZeroRF necessitates masking out the background area of the input frame before training, which is incompatible with our evaluation benchmark. Consequently, we trained it without object masks. Training requires approximately 25 minutes per scene with 10,000 iterations and batch size of 214. FSGS. We use the official implementation of FSGS [102] and follow most of the default configurations. For the LLFF dataset, we adjust the input views to match the settings used in ViP-NeRF, which differs from the original FSGS paper. Training takes approximately 25 minutes per scene with 10,000 iterations. Since there is no official dataloader for the DTU dataset, we convert the DTU camera poses to the LLFF format and use the default LLFF configuration. Training on the DTU dataset requires around 20 minutes per scene with 10,000 iterations. Time complexity. To verify the efficiency of our method, besides comparing the training time of various methods, we also calculated the MFLOPs per pixel in Tab. 5. E. Complete Quantitative Evaluations LLFF dataset. We show all 8 scenes of the quantitative comparisons with two, three, and four input views on the LLFF dataset in Tab. 6, Tab. 7, and Tab. 8, respectively. 17 Table 6. Quantitative results on the LLFF [48] dataset with two input views. The three rows show LPIPS, SSIM, and PSNR scores, respectively. Scene Fern Flower Fortress Horns Leaves Orchids Room Trex Average Method RegNeRF [53] DS-NeRF [21] DDP-NeRF [61] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] VGOS [75] GeCoNeRF [39] SparseNeRF [81] FSGS [102] FrugalNeRF (Ours) FrugalNeRF w/ mono. depth (Ours) 0.51 0.45 15. 0.50 0.46 16.4 0.44 0.49 17.2 0.46 0.49 17.1 0.45 0.45 16.2 0.51 0.50 17.0 0.48 0.51 16. 0.56 0.47 16.4 0.48 0.52 18.2 0.46 0.40 15.0 0.41 0.47 17.4 0.40 0.46 17.7 0.43 0.51 17. 0.43 0.44 16.1 0.46 0.45 16.2 0.38 0.55 17.6 0.42 0.43 14.9 0.43 0.53 16.9 0.44 0.55 17. 0.49 0.49 16.9 0.55 0.41 15.4 0.45 0.38 14.8 0.41 0.50 17.5 0.40 0.53 17.9 0.51 0.42 15. 0.49 0.49 16.6 0.46 0.52 17.1 0.43 0.53 17.1 0.39 0.54 17.1 0.42 0.54 17.1 0.47 0.55 15. 0.61 0.41 15.4 0.52 0.51 17.4 0.42 0.42 16.2 0.36 0.55 18.5 0.37 0.54 18.5 0.37 0.46 20. 0.30 0.65 23.0 0.17 0.77 22.7 0.33 0.53 21.3 0.21 0.71 22.6 0.25 0.67 22.5 0.37 0.53 19. 0.50 0.43 17.9 0.40 0.61 21.7 0.35 0.47 16.9 0.27 0.54 20.3 0.27 0.54 20.9 0.35 0.37 14.5 0.47 0.24 12.4 0.52 0.23 12.6 0.36 0.38 14.4 0.46 0.21 11.7 0.44 0.30 13. 0.36 0.38 14.7 0.49 0.28 13.3 0.52 0.244 13.4 0.33 0.34 14.2 0.32 0.41 15.5 0.33 0.41 15. 0.45 0.30 13.9 0.43 0.32 13.7 0.41 0.38 15.1 0.42 0.35 14.1 0.40 0.36 14.2 0.41 0.37 14. 0.42 0.40 14.4 0.51 0.29 13.4 0.55 0.24 13.3 0.41 0.24 12.6 0.42 0.33 15.0 0.39 0.37 15. 0.38 0.74 18.7 0.35 0.76 18.9 0.30 0.76 18.7 0.34 0.76 18.3 0.36 0.72 17.7 0.35 0.77 19. 0.38 0.77 18.8 0.54 0.68 17.3 0.29 0.82 22.8 0.38 0.72 17.6 0.34 0.75 19.2 0.32 0.76 19. 0.42 0.54 16.7 0.41 0.53 15.7 0.43 0.54 15.7 0.33 0.60 18.1 0.38 0.54 15.9 0.39 0.58 16. 0.40 0.59 16.0 0.49 0.52 16.1 0.37 0.62 18.6 0.45 0.46 13.8 0.32 0.61 18.6 0.35 0.59 18. 0.43 0.49 16.9 0.42 0.51 16.9 0.39 0.54 17.2 0.38 0.54 17.6 0.37 0.52 16.7 0.39 0.55 17. 0.42 0.55 16.7 0.52 0.45 15.8 0.45 0.52 18.0 0.41 0.45 15.3 0.35 0.54 18.1 0.35 0.54 18. Table 7. Quantitative results on the LLFF [48] dataset with three input views. The three rows show LPIPS, SSIM, and PSNR scores, respectively. Scene Fern Flower Fortress Horns Leaves Orchids Room Trex Average Method RegNeRF [53] DS-NeRF [21] DDP-NeRF [61] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] VGOS [75] GeCoNeRF [39] SparseNeRF [81] FSGS [102] FrugalNeRF (Ours) FrugalNeRF w/ mono. depth (Ours) 0.47 0.48 17.9 0.47 0.52 18.5 0.47 0.53 18.5 0.40 0.54 18.9 0.51 0.49 17.3 0.43 0.52 18. 0.40 0.58 19.0 0.57 0.46 17.0 0.43 0.57 19.6 0.48 0.55 17.9 0.39 0.50 18.2 0.40 0.49 18. 0.27 0.58 19.6 0.25 0.66 21.3 0.29 0.63 20.2 0.28 0.61 20.7 0.24 0.65 20.8 0.24 0.66 20. 0.31 0.61 20.0 0.36 0.57 19.5 0.33 0.60 19.8 0.30 0.68 21.5 0.32 0.55 18.8 0.23 0.63 21. 0.44 0.53 18.2 0.47 0.52 17.5 0.48 0.53 17.4 0.41 0.58 18.7 0.42 0.57 18.2 0.42 0.57 18. 0.46 0.58 17.0 0.60 0.44 15.8 0.50 0.53 18.4 0.36 0.65 19.4 0.34 0.59 19.3 0.33 0.60 19. 0.31 0.64 22.7 0.25 0.72 24.8 0.20 0.75 22.1 0.32 0.60 22.0 0.19 0.76 24.5 0.17 0.78 24. 0.33 0.69 23.0 0.45 0.53 20.6 0.37 0.59 23.0 0.15 0.72 23.9 0.24 0.63 23.4 0.22 0.69 23. 19 0.39 0.37 14.6 0.50 0.25 12.6 0.52 0.24 12.8 0.40 0.40 15.0 0.44 0.25 12. 0.42 0.38 14.8 0.40 0.40 15.0 0.50 0.32 13.8 0.35 0.45 16.5 0.26 0.28 13.3 0.37 0.39 15. 0.37 0.39 15.4 0.44 0.31 14.2 0.45 0.33 14.1 0.45 0.35 15.1 0.41 0.37 14.7 0.41 0.34 14. 0.39 0.38 15.0 0.41 0.40 15.2 0.51 0.30 13.6 0.41 0.37 15.2 0.35 0.37 14.1 0.42 0.35 15. 0.40 0.36 15.7 0.25 0.81 21.0 0.22 0.84 23.0 0.32 0.76 18.3 0.22 0.85 22.6 0.27 0.81 21. 0.26 0.83 22.0 0.31 0.83 21.8 0.34 0.80 21.1 0.28 0.81 21.5 0.28 0.84 22.6 0.27 0.81 22. 0.25 0.83 22.3 0.36 0.63 18.4 0.37 0.59 17.1 0.42 0.54 16.0 0.33 0.64 19.0 0.32 0.62 18. 0.34 0.66 18.9 0.35 0.66 18.0 0.43 0.59 18.1 0.31 0.67 20.1 0.28 0.62 17.4 0.29 0.66 19. 0.29 0.67 20.0 0.36 0.57 18.7 0.36 0.58 19.0 0.39 0.56 17.7 0.34 0.60 19.3 0.34 0.59 18. 0.33 0.62 19.5 0.37 0.61 18.8 0.47 0.50 17.4 0.37 0.59 19.5 0.30 0.61 19.2 0.32 0.59 19. 0.30 0.61 19.9 Table 8. Quantitative results on the LLFF [48] dataset with four input views. The three rows show LPIPS, SSIM, and PSNR scores, respectively. Scene Fern Flower Fortress Horns Leaves Orchids Room Trex Average Method RegNeRF [53] DS-NeRF [21] DDP-NeRF [61] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] VGOS [75] GeCoNeRF [39] SparseNeRF [81] FSGS [102] FrugalNeRF (Ours) FrugalNeRF w/ mono. depth (Ours) 0.35 0.63 20.8 0.35 0.63 20.9 0.40 0.60 20.1 0.37 0.64 21.1 0.39 0.58 18. 0.33 0.65 21.1 0.40 0.64 19.6 0.45 0.61 20.5 0.42 0.62 21.4 0.26 0.67 20.5 0.30 0.63 21. 0.30 0.64 21.5 0.29 0.64 19.8 0.28 0.64 20.6 0.30 0.63 20.0 0.30 0.64 20.5 0.27 0.63 19. 0.27 0.67 20.8 0.35 0.63 20.3 0.36 0.61 19.9 0.32 0.64 20.7 0.22 0.65 20.2 0.28 0.64 20. 0.27 0.65 20.9 0.34 0.64 20.1 0.41 0.59 19.5 0.42 0.59 19.3 0.37 0.63 20.4 0.38 0.60 19. 0.38 0.63 19.7 0.43 0.62 18.6 0.47 0.59 19.6 0.39 0.63 20.4 0.24 0.70 20.9 0.30 0.66 21. 0.28 0.68 21.1 0.37 0.55 22.4 0.31 0.66 24.1 0.18 0.73 23.4 0.35 0.60 23.2 0.25 0.70 23. 0.28 0.69 24.3 0.40 0.64 22.7 0.44 0.51 21.2 0.31 0.70 24.6 0.17 0.65 22.6 0.24 0.60 23. 0.25 0.64 23.9 20 0.32 0.44 15.9 0.41 0.39 15.8 0.45 0.37 15.1 0.35 0.47 16. 0.36 0.40 14.8 0.35 0.46 16.3 0.34 0.49 16.6 0.44 0.40 15.5 0.36 0.49 17.5 0.22 0.46 15. 0.26 0.52 16.9 0.24 0.53 17.2 0.43 0.34 14.8 0.41 0.38 15.2 0.42 0.41 15.8 0.42 0.37 14. 0.40 0.39 14.8 0.36 0.42 15.7 0.41 0.43 15.8 0.51 0.30 13.9 0.42 0.39 15.7 0.28 0.45 15. 0.38 0.41 16.3 0.37 0.41 16.3 0.19 0.87 23.9 0.16 0.89 25.6 0.26 0.82 20.8 0.19 0.88 24. 0.23 0.85 23.2 0.19 0.88 24.3 0.28 0.86 23.6 0.27 0.85 23.5 0.25 0.85 23.5 0.17 0.88 23. 0.19 0.87 24.2 0.18 0.88 24.1 0.32 0.66 18.9 0.39 0.59 17.1 0.39 0.60 17.3 0.31 0.68 19. 0.32 0.64 18.6 0.32 0.68 19.3 0.35 0.68 18.7 0.40 0.63 19.0 0.29 0.70 20.9 0.23 0.71 19. 0.27 0.72 19.7 0.27 0.71 19.6 0.32 0.62 19.9 0.34 0.61 20.1 0.35 0.61 19.2 0.33 0.63 20. 0.32 0.62 19.3 0.31 0.65 20.4 0.37 0.64 19.7 0.42 0.56 19.1 0.34 0.65 20.9 0.22 0.66 20. 0.27 0.65 20.9 0.26 0.66 20.9 Table 9. Quantitative results on the DTU [32] dataset with two input views. The three rows show LPIPS, SSIM and PSNR scores, respectively. Scene Scan21 Scan Scan34 Scan38 Scan40 Scan41 Scan45 Scan Scan63 Scan82 Scan103 Scan114 Average Method FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] VGOS [75] SparseNeRF [81] ZeroRF [66] FrugalNeRF (Ours) FrugalNeRF w/ mono. depth (Ours) 0.33 0.51 13.21 0.37 0.26 11.31 0.23 0.73 12.71 0.28 0.69 9.69 0.39 0.45 14. 0.45 0.30 10.99 0.25 0.57 14.67 0.25 0.56 14.14 0.18 0.75 19.33 0.24 0.49 13.57 0.32 0.71 11. 0.36 0.67 8.97 0.22 0.69 17.95 0.27 0.61 14.40 0.16 0.73 17.86 0.15 0.73 18.46 0.31 0.63 14. 0.27 0.52 17.13 0.23 0.76 14.39 0.33 0.69 9.75 0.26 0.70 20.65 0.35 0.50 13.93 0.20 0.73 19. 0.19 0.75 21.27 0.34 0.61 16.76 0.38 0.43 13.25 0.21 0.77 14.50 0.31 0.71 10.27 0.33 0.60 17. 0.44 0.39 12.16 0.24 0.64 17.66 0.21 0.68 19.40 0.41 0.58 11.42 0.31 0.47 15.08 0.24 0.77 13. 0.30 0.73 8.79 0.24 0.72 16.33 0.29 0.59 15.41 0.24 0.73 14.51 0.23 0.74 15.56 0.35 0.63 14. 0.23 0.58 17.81 0.19 0.84 15.57 0.27 0.78 9.75 0.21 0.76 20.13 0.28 0.63 16.73 0.17 0.78 19. 0.16 0.79 20.53 0.19 0.76 18.66 0.31 0.37 11.35 0.28 0.70 11.88 0.37 0.64 7.54 0.20 0.75 18. 0.39 0.49 11.24 0.16 0.77 16.94 0.15 0.78 18.05 0.11 0.80 21.62 0.21 0.39 16.92 0.22 0.88 19. 0.15 0.90 19.24 0.14 0.78 22.29 0.25 0.68 17.08 0.13 0.86 24.87 0.12 0.86 25.65 0.07 0.93 23. 0.09 0.63 16.71 0.30 0.75 12.73 0.49 0.56 5.17 0.08 0.92 20.70 0.13 0.88 20.39 0.09 0.92 21. 0.08 0.93 23.46 0.08 0.90 21.56 0.12 0.57 13.37 0.27 0.79 14.37 0.45 0.57 5.63 0.08 0.91 23. 0.18 0.82 15.36 0.07 0.92 22.67 0.07 0.91 22.72 0.17 0.82 17.55 0.18 0.65 16.15 0.19 0.81 16. 0.34 0.73 11.29 0.15 0.84 21.70 0.25 0.73 16.23 0.13 0.85 21.45 0.10 0.88 23.76 0.12 0.85 24. 0.17 0.49 16.24 0.27 0.82 14.86 0.18 0.85 15.81 0.13 0.85 24.40 0.29 0.63 14.12 0.11 0.89 25. 0.10 0.90 26.25 0.22 0.73 18.05 0.24 0.49 14.91 0.25 0.79 14.41 0.32 0.71 10.16 0.20 0.75 19. 0.30 0.60 14.84 0.16 0.78 19.72 0.15 0.79 20.77 Table 10. Quantitative results on the DTU [32] dataset with three input views. The three rows show LPIPS, SSIM and PSNR scores, respectively. Scene Scan Scan31 Scan34 Scan38 Scan40 Scan41 Scan Scan55 Scan63 Scan82 Scan103 Scan114 Average Method FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] VGOS [75] SparseNeRF [81] ZeroRF [66] FrugalNeRF (Ours) FrugalNeRF w/ mono. depth (Ours) 15.93 0.58 15.93 0.34 0.33 12.97 0.22 0.74 12.90 0.28 0.69 9. 0.23 0.63 17.14 0.45 0.33 11.55 0.19 0.69 17.38 0.19 0.68 17.14 19.53 0.76 19.53 0.18 0.58 16. 0.32 0.68 11.29 0.38 0.65 8.34 0.12 0.81 21.11 0.36 0.55 12.43 0.14 0.76 19.06 0.13 0.78 19. 23.23 0.80 23.23 0.26 0.58 18.63 0.24 0.74 14.17 0.29 0.71 10.50 0.15 0.79 24.88 0.41 0.47 11. 0.18 0.77 22.38 0.17 0.78 23.17 19.88 0.70 19.88 0.32 0.53 16.12 0.24 0.75 13.42 0.26 0.76 11. 0.37 0.59 12.36 0.45 0.41 12.84 0.22 0.69 18.96 0.21 0.73 20.33 22.83 0.84 22.83 0.28 0.55 16. 0.27 0.77 12.23 0.27 0.76 9.14 0.14 0.84 23.05 0.30 0.68 16.01 0.13 0.84 24.01 0.13 0.84 23. 18.38 0.80 18.38 0.32 0.47 14.82 0.28 0.75 11.44 0.28 0.74 8.51 0.14 0.84 22.25 0.29 0.65 15. 0.21 0.79 17.77 0.20 0.79 17.18 21 21.07 0.84 21.07 0.22 0.50 14.14 0.23 0.79 15. 0.38 0.62 7.27 0.12 0.84 20.85 0.33 0.57 12.77 0.13 0.82 20.35 0.13 0.82 20.59 22.88 0.80 22. 0.22 0.43 18.04 0.15 0.90 20.41 0.16 0.90 18.86 0.14 0.84 19.75 0.27 0.68 16.50 0.12 0.89 26. 0.12 0.88 26.60 25.28 0.94 25.28 0.09 0.66 17.67 0.31 0.77 13.97 0.51 0.58 5.38 0.04 0.96 27. 0.19 0.84 17.81 0.06 0.94 24.57 0.06 0.95 25.52 26.39 0.94 26.39 0.11 0.65 14.75 0.36 0.67 10. 0.47 0.58 5.80 0.04 0.95 28.98 0.19 0.83 15.34 0.05 0.94 25.85 0.05 0.93 25.04 26.68 0.92 26. 0.12 0.77 20.85 0.17 0.84 17.41 0.29 0.75 11.81 0.11 0.90 23.74 0.24 0.74 16.64 0.10 0.89 25. 0.08 0.91 27.84 26.68 0.90 26.68 0.12 0.60 18.65 0.25 0.81 14.66 0.15 0.87 16.74 0.08 0.92 28. 0.30 0.63 14.25 0.11 0.90 27.28 0.10 0.91 27.10 22.40 0.82 22.40 0.22 0.55 16.62 0.25 0.77 14. 0.31 0.72 10.34 0.14 0.83 22.47 0.31 0.61 14.47 0.14 0.83 22.43 0.13 0.83 22.84 Table 11. Quantitative results on the DTU [32] dataset with four input views. The three rows show LPIPS, SSIM and PSNR scores, respectively. Scene Scan21 Scan31 Scan34 Scan38 Scan Scan41 Scan45 Scan55 Scan63 Scan82 Scan Scan114 Average Method FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] VGOS [75] SparseNeRF [81] ZeroRF [66] FrugalNeRF (Ours) FrugalNeRF w/ mono. depth (Ours) 0.18 0.72 18.72 0.33 0.39 14. 0.27 0.71 11.81 0.27 0.73 11.09 0.16 0.72 18.60 0.43 0.36 11.75 0.17 0.73 19.21 0.17 0.73 19. 0.14 0.81 21.29 0.19 0.61 17.22 0.28 0.73 12.95 0.35 0.69 9.53 0.14 0.80 20.99 0.32 0.62 13. 0.12 0.81 21.84 0.12 0.81 21.65 0.13 0.83 25.97 0.21 0.59 19.44 0.23 0.78 14.72 0.31 0.71 10. 0.15 0.85 25.87 0.28 0.66 16.47 0.16 0.81 24.99 0.15 0.82 25.82 0.24 0.72 19.43 0.31 0.59 18. 0.25 0.75 12.71 0.28 0.74 11.15 0.21 0.74 20.92 0.44 0.47 13.53 0.17 0.79 23.08 0.17 0.80 23. 0.14 0.85 22.88 0.35 0.45 15.76 0.32 0.72 10.42 0.27 0.76 9.12 0.21 0.80 19.45 0.28 0.68 16. 0.19 0.81 19.47 0.19 0.82 18.96 0.12 0.86 25.59 0.24 0.61 18.84 0.27 0.76 11.67 0.27 0.78 10. 0.14 0.86 24.81 0.25 0.73 17.26 0.12 0.85 25.64 0.12 0.86 25.55 0.09 0.86 22.39 0.23 0.52 15. 0.25 0.78 14.12 0.37 0.64 8.10 0.10 0.86 22.15 0.20 0.73 16.48 0.12 0.85 21.59 0.11 0.86 22. 0.06 0.92 28.63 0.24 0.38 16.62 0.21 0.88 18.84 0.16 0.90 19.53 0.09 0.88 26.37 0.29 0.67 15. 0.12 0.89 27.31 0.12 0.90 28.02 0.04 0.96 27.35 0.08 0.67 17.19 0.27 0.82 14.05 0.43 0.66 6. 0.04 0.95 26.20 0.17 0.87 19.33 0.05 0.95 26.27 0.05 0.96 26.87 0.03 0.96 31.51 0.08 0.67 16. 0.27 0.80 14.43 0.42 0.66 7.14 0.05 0.95 26.72 0.14 0.87 19.12 0.04 0.95 27.26 0.03 0.95 28. 0.08 0.93 27.30 0.10 0.76 22.67 0.18 0.84 16.87 0.28 0.75 12.69 0.09 0.93 28.10 0.26 0.72 15. 0.07 0.93 29.27 0.07 0.93 29.27 0.07 0.93 28.65 0.12 0.64 19.50 0.29 0.81 14.23 0.18 0.85 15. 0.06 0.93 28.19 0.32 0.62 13.36 0.10 0.92 28.21 0.09 0.92 28.92 0.11 0.86 24.98 0.21 0.57 17. 0.26 0.78 13.90 0.30 0.74 10.93 0.12 0.86 24.03 0.28 0.67 15.73 0.12 0.86 24.51 0.12 0.86 24. Table 12. Quantitative results on the RealEstate-10K [100] dataset. For SimpleNeRF [71] and ViP-NeRF [70], we calculate metrics using testing data provided in their respective clouds. As for other models, we rely on the scores provided in the SimpleNeRF paper. Method Venue Learned priors PSNR 2-view SSIM LPIPS PSNR 3-view SSIM LPIPS PSNR 4-view SSIM LPIPS Training time RegNeRF [53] DS-NeRF [21] DDP-NeRF [61] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] FrugalNeRF (Ours) CVPR 2022 CVPR 2022 CVPR 2022 CVPR 2023 SIGGRAPH 2023 SIGGRAPH Asia 2023 - normalizing flow - depth completion - - - - 16.87 25.44 26.15 14.50 29.55 30.30 30.12 0.59 0.79 0.85 0.54 0.87 0.88 0.87 0.45 0.32 0.15 0.55 0.09 0.07 0.07 17.73 25.94 25.92 15.12 29.75 31.40 31. 0.61 0.79 0.85 0.57 0.88 0.89 0.89 0.44 0.32 0.16 0.54 0.11 0.08 0.06 18.25 26.28 26.48 16.25 30.47 31.73 31.78 0.62 0.79 0.86 0.60 0.88 0.89 0.90 0.44 0.33 0.16 0.54 0.11 0.09 0.06 2.35 hrs 3.5 hrs 3.5 hrs 1.5 hrs 13.5 hrs 9.5 hrs 20 mins 22 Table 13. Quantitative results on the RealEstate-10K [100] dataset with two input views. The three rows show LPIPS, SSIM, and PSNR scores, respectively. Scene 0 Method RegNeRF [53] DS-NeRF [21] DDP-NeRF [61] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] FrugalNeRF (Ours) 0.35 0.60 16.51 0.26 0.81 24.68 0.11 0.89 25.90 0.45 0.54 15.00 0.05 0.94 30.41 0.04 0.95 31. 0.04 0.94 30.13 1 0.32 0.83 21.04 0.27 0.91 27.93 0.12 0.95 25.87 0.50 0.77 17. 0.05 0.97 32.03 0.04 0.97 33.8 0.04 0.97 34.69 3 0.49 0.30 13.88 0.51 0.50 19. 0.34 0.56 18.97 0.64 0.28 12.15 0.22 0.56 18.96 0.21 0.56 18.65 0.20 0.56 18.35 0.54 0.61 17.13 0.24 0.88 29.18 0.06 0.94 32.01 0.67 0.49 12.84 0.04 0.95 34.74 0.03 0.95 34. 0.04 0.95 35.00 6 0.54 0.59 15.79 0.31 0.83 26.18 0.11 0.92 28.00 0.48 0.58 15. 0.08 0.93 31.61 0.05 0.96 32.24 0.05 0.95 32.45 Average 0.45 0.59 16.87 0.32 0.79 25. 0.15 0.85 26.15 0.55 0.53 14.50 0.09 0.87 29.55 0.07 0.88 30.30 0.07 0.87 30.12 Table 14. Quantitative results on the RealEstate-10K [100] dataset with three input views. The three rows show LPIPS, SSIM, and PSNR scores, respectively. Scene 0 Method RegNeRF [53] DS-NeRF [21] DDP-NeRF [61] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] FrugalNeRF (Ours) 0.40 0.60 15.99 0.24 0.83 25. 0.11 0.89 25.27 0.54 0.53 13.79 0.06 0.94 30.66 0.04 0.95 32.23 0.04 0.95 31.11 0.32 0.82 20.89 0.26 0.91 28.68 0.11 0.96 26.67 0.51 0.75 15.59 0.10 0.95 29.89 0.04 0.98 36. 0.03 0.98 35.39 4 0.56 0.62 17.60 0.26 0.87 29.08 0.06 0.94 31.84 0.59 0.61 15. 0.04 0.95 35.17 0.03 0.95 35.85 0.03 0.95 35.78 6 0.37 0.71 20.28 0.31 0.85 27. 0.13 0.92 26.99 0.42 0.66 18.05 0.08 0.95 33.43 0.08 0.95 32.81 0.04 0.96 34.07 Average 0.44 0.61 17.73 0.32 0.79 25.94 0.16 0.85 25.92 0.54 0.57 15.12 0.11 0.88 29.75 0.08 0.89 31. 0.06 0.89 31.04 3 0.53 0.29 13.87 0.53 0.49 19.14 0.38 0.55 18.81 0.64 0.29 12. 0.26 0.60 19.59 0.23 0.61 19.65 0.18 0.61 18.85 23 Table 15. Quantitative results on the RealEstate-10K [100] dataset with four input views. The three rows show LPIPS, SSIM, and PSNR scores, respectively. Scene 0 Method RegNeRF [53] DS-NeRF [21] DDP-NeRF [61] FreeNeRF [91] ViP-NeRF [70] SimpleNeRF [71] FrugalNeRF (Ours) 0.43 0.59 16.09 0.27 0.82 25.40 0.12 0.89 25. 0.56 0.53 13.84 0.06 0.94 31.64 0.04 0.96 32.95 0.04 0.96 32.29 1 0.35 0.83 20. 0.26 0.92 29.40 0.08 0.96 28.57 0.48 0.80 17.93 0.08 0.96 32.24 0.05 0.97 36.44 0.03 0.98 36. 4 0.56 0.65 18.48 0.25 0.87 29.26 0.06 0.93 31.73 0.58 0.66 17.29 0.05 0.94 34. 0.03 0.95 35.97 0.03 0.95 36.54 6 0.27 0.75 21.78 0.31 0.85 27.69 0.13 0.91 27. 0.39 0.69 19.48 0.09 0.95 33.28 0.09 0.94 32.77 0.05 0.96 34.22 Average 0.44 0.62 18. 0.33 0.79 26.28 0.16 0.86 26.48 0.53 0.60 16.25 0.11 0.88 30.47 0.09 0.89 31.73 0.06 0.90 31. 3 0.59 0.29 13.91 0.56 0.50 19.64 0.39 0.58 19.57 0.65 0.31 12.69 0.27 0.62 20. 0.24 0.64 20.52 0.17 0.64 19.81 24 Figure 10. More qualitative comparisons on the LLFF [48] dataset with two input views. FrugalNeRF achieves better synthesis quality in different scenes. 25 Figure 11. More qualitative comparisons on the DTU [32] dataset with two input views. FrugalNeRF achieves better synthesis quality in different scenes. Figure 12. Qualitative comparisons on the RealEstate-10K [100] dataset with two input views. Compared to Vip-NeRF [70] and SimpleNeRF [71], our FrugalNeRF renders sharper details in the scene. 26 Figure 13. More qualitative comparisons on the RealEstate-10K [100] dataset with two input views. FrugalNeRF achieves synthesis quality comparable to the state-of-the-art methods."
        }
    ],
    "affiliations": [
        "NVIDIA Research",
        "National Yang Ming Chiao Tung University"
    ]
}