{
    "paper_title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System",
    "authors": [
        "Serry Sibaee",
        "Omer Nacar",
        "Yasser Al-Habashi",
        "Adel Ammar",
        "Wadii Boulila"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces \\textbf{SHAMI-MT}, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of \\textbf{4.01 out of 5.0} when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication."
        },
        {
            "title": "Start",
            "content": "SHAMI-MT: SYRIAN ARABIC DIALECT TO MODERN STANDARD ARABIC BIDIRECTIONAL MACHINE TRANSLATION SYSTEM 5 2 0 2 4 ] . [ 1 8 6 2 2 0 . 8 0 5 2 : r Serry Sibaee* Prince Sultan University Riyadh - Saudi Arabia ssibaee@psu.edu.sa Omer Nacar* Prince Sultan University Riyadh - Saudi Arabia onajar@psu.edu.sa Yasser Al-Habashi Prince Sultan University Riyadh - Saudi Arabia yalhabashi@psu.edu.sa Adel Ammar Prince Sultan University Riyadh - Saudi Arabia aammar@psu.edu.sa Wadii Boulila Prince Sultan University Riyadh - Saudi Arabia wboulila@psu.edu.sa August 5,"
        },
        {
            "title": "ABSTRACT",
            "content": "The rich linguistic landscape of the Arab world is characterized by significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents formidable challenge for natural language processing, particularly machine translation. This paper introduces SHAMI-MT, bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of 4.01 out of 5.0 when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides crucial, high-fidelity tool for previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of Natural Language Processing (NLP), driven by Large Language Models (LLMs), has transformed how we interact with information and technology [1]. However, the benefits of this revolution have not been distributed evenly across all languages. While high-resource languages like English have seen exponential progress, languages with complex internal diversity, such as Arabic, present unique and persistent challenges [2]. The most prominent of these is the phenomenon of diglossia: the co-existence of high-variety language, Modern Standard Arabic (MSA), and spectrum of low-variety, colloquial dialects. MSA is the language of literature, formal education, news media, and official government business across the Arab world. Yet, it is rarely spoken as native tongue. Instead, daily lifefrom casual conversation to social mediais conducted in regional dialects [3]. The Syrian dialect, major variant within the Levantine Arabic family, is culturally significant and widely spoken dialect with millions of speakers both within Syria and in the global diaspora. The linguistic distance between MSA and Syrian dialect is substantial, encompassing differences in phonology, vocabulary, grammar, and idiomatic expressions.This linguistic gap renders most MSA-trained NLP models ineffective for understanding or PREPRINT - AUGUST 5, 2025 generating authentic dialectal content, creating digital divide that limits the accessibility of technology for millions of Arabic speakers. Recent progress in Transformer-based encoder-decoder models has propelled the capabilities of NLP systems across diverse tasks and languages. In this work, we leverage AraT5v2 [7], state-of-the-art Arabic text-to-text Transformer model, to address the challenges posed by Syrian dialect processing. AraT5v2 builds upon the T5 architecture [8], with adaptations specifically tailored for Arabic, making it strong foundation for dialect-aware tasks. We fine-tune AraT5v2 on newly curated parallel datasets that include both Modern Standard Arabic (MSA) and Syrian Arabic, enabling the model to better handle the linguistic variation caused by diglossia. Importantly, our goal is not to present state-of-the-art system across all benchmarks, but rather to introduce flexible, extensible framework that can support wide range of dialectal Arabic NLP applications. This approach lays the groundwork for future research in bridging the gap between formal and colloquial Arabic varieties in language technologies. To address this critical gap, we have developed SHAMI-MT, specialized, bidirectional machine translation system for Modern Standard Arabic (MSA) and the Syrian dialect. As part of this effort, we release two openly accessible models: MSA-to-Syrian Dialect: huggingface.co/Omartificial-Intelligence-Space/Shami-MT Syrian Dialect-to-MSA: huggingface.co/Omartificial-Intelligence-Space/SHAMI-MT-2MSA These models are part of growing collection, SHAMIYAT, dedicated to advancing Syrian dialect NLP: SHAMIYAT Collection: huggingface.co/collections/Omartificial-Intelligence-Space/shamiyat... Our project makes the following key contributions: 1. Bidirectional Translation System: We present two distinct models enabling translation from MSA to Syrian dialect and vice versa, supporting complete communication cycle. 2. Leveraging Specialized Arabic LLM: We build on AraT5v2, Transformer model pre-trained specifically on Arabic, offering deeper understanding of the language than multilingual alternatives. 3. Rigorous and Transparent Evaluation: We assess model performance using the MADAR corpus and an advanced automated judge (GPT-4.1), reporting both strengths and failure modes. 4. Publicly Available Resource: By releasing these models and datasets, we aim to foster innovation in the under-resourced domain of Syrian dialectal NLP and encourage broader community collaboration."
        },
        {
            "title": "2 Related Work",
            "content": "The challenge of translating between Dialectal Arabic (DA) and MSA is not new, and the approaches to solving it have evolved in lockstep with broader trends in NLP. The first attempts at DA-MSA translation were dominated by rule-based and statistical methods. Rule-based systems relied on manually crafted linguistic rules, morphological analyzers (e.g., the Buckwalter Morphological Analyzer), and extensive lexicons to map dialectal forms to their MSA counterparts [10]. These systems were labor-intensive and brittle, struggling to cope with the vast, unstandardized nature of dialectal orthography and out-of-vocabulary (OOV) words. Phrase-Based Statistical Machine Translation (SMT), as exemplified by systems like Moses, represented an improvement by learning probabilistic translation patterns from parallel corpora [11]. However, the performance of SMT was fundamentally capped by the scarcity of large, high-quality, parallel DA-MSA datasets, bottleneck that persists to this day for many dialect pairs. The advent of Neural Machine Translation (NMT), particularly with the introduction of the Transformer architecture, marked paradigm shift. The ability of encoder-decoder models to learn rich, contextual representations of text allowed them to capture the complex, non-linear relationships between dialects and MSA far more effectively than their predecessors [12]. NMT models could learn grammatical transformations and lexical substitutions end-to-end, without the need for explicit, hand-crafted rules. This led to significant improvements in translation fluency and accuracy. More recently, the landscape has been reshaped by massive, pre-trained Large Language Models (LLMs) such as mBERT [4] and mT5 [5]. These multilingual models are trained on hundreds of languages simultaneously and have demonstrated impressive zero-shot and few-shot learning capabilities. However, for specialized and linguistically nuanced tasks like dialectal translation, their performance can be limited. Because their representational capacity is distributed across many languages, the quality and depth of learning for any single languageparticularly low-resource dialectsmay be insufficient. Moreover, their training data for dialects is often sparse, noisy, or inconsistently labeled, which can result in overly generic or \"flattened\" translations that fail to capture the authentic characteristics of the target dialect [7]. Several prior studies have explored dialectal Arabic processing, focusing on varieties such as Egyptian, 2 PREPRINT - AUGUST 5, 2025 Gulf, and Maghrebi Arabic [6]; however, comparatively little attention has been given to the Syrian dialect, which remains underrepresented in both datasets and model development efforts. This limitation highlights growing consensus in the NLP community: for high-fidelity performance on specific languages or domains, specialized models are essential. This led to the development of models like AraT5, which was pre-trained exclusively on vast corpus of Arabic text. By focusing on single language family, such models can develop much deeper and more nuanced understanding of its morphology and syntax. Our work on SHAMI-MT is direct extension of this philosophy, arguing that further specialization through fine-tuning on high-quality, dialect-specific dataset is the key to unlocking true, high-fidelity dialectal translation."
        },
        {
            "title": "3 Methodology",
            "content": "Our methodology is grounded in the principle of leveraging powerful, specialized foundation model and adapting it for specific, high-value task through targeted fine-tuning. The core of our SHAMI-MT system is the BC LP/AraT 5v2 base 1024 architecture [7]. This model is variant of the Text-to-Text Transfer Transformer (T5) framework, which reframes all NLP tasks as text-to-text problem. In an encoder-decoder model like T5, the encoder processes the source text to create rich, numerical representation of its meaning. The decoder then uses this representation to auto-regressively generate the target text, one token at time. We selected AraT5v2 for its distinct advantages in the context of Arabic NLP. Unlike general-purpose multilingual models, AraT5v2 is pre-trained exclusively on large and diverse corpus of high-quality Arabic text, granting it deep understanding of the languages rich morphology, syntax, and semanticsan essential foundation for any downstream task involving Arabic. Its support for extended sequence lengths, up to 1024 tokens, is particularly valuable for machine translation, as it enables the model to maintain coherence and context across long, syntactically complex sentences that are common in both formal MSA and colloquial dialects. Additionally, AraT5v2 is optimized for fine-tuning, demonstrating efficient convergence and training stability across variety of tasks. Finally, its encoder-decoder architecture is purpose-built for text generation, making it especially well-suited for translation tasks that require both linguistic accuracy and natural fluency. To teach our model the specifics of the Syrian dialect, we fine-tuned it on the Nâbra dataset [9]. This corpus is an invaluable resource due to its authenticity and diversity. It is not sterile, academic dataset; rather, it is compiled from wide array of real-world sources, including social media posts, film and television scripts, song lyrics, and traditional proverbs. Furthermore, it covers broad geographical range of Syrian sub-dialects, including those from Damascus, Aleppo, Homs, Latakia, and more. This richness ensured that our model was exposed to authentic, varied, and contemporary usage of the Shami dialect. Figure 1 shows the richness and diversity collection of Syrian Arabic from various sources and regions, crucial for training nuanced dialectal model. For rigorous and impartial assessment of our models performance, we used blind test set sourced from the MADAR (Multi-Arabic Dialect Applications and Resources) parallel corpus [10]. We specifically selected 1,500 sentence pairs from the Damascus dialect subset. MADAR is widely recognized as gold-standard benchmark in the field of Arabic dialectology and NLP. Using it for evaluation ensures that our results are comparable, reproducible, and tested against standard measure of quality. The fine-tuning process was carefully designed to balance effective learning with model stability. Training was conducted over 22 epochs, with total of 10,384 steps, using batch size of 256. cosine learning rate schedule was employed, beginning with an initial learning rate of 5e-5. This scheduling strategy allows for gradual reduction in the learning rate, helping the model converge more smoothly and avoid overshooting minima during optimization. By the end of training, the model achieved final training loss of 1.396 and final evaluation loss of 0.771, indicating well-calibrated fine-tuning process that preserved generalization while improving task-specific performance."
        },
        {
            "title": "4 Evaluation and Results",
            "content": "To move beyond simple lexical metrics like BLEU, which often fail to capture semantic and dialectal nuance, we designed more sophisticated evaluation framework. We leveraged the advanced reasoning capabilities of GPT-4.1 as an automated evaluator. For each of the 1,500 test sentences from the MADAR corpus, GPT-4.1 was given the MSA input, the models predicted Shami translation, and the ground truth Shami translation. It was then prompted to provide holistic quality score from 0 to 5 based on three explicit criteria: 1. Semantic Accuracy: Does the translation preserve the core meaning of the source sentence? 3 PREPRINT - AUGUST 5, 2025 Figure 1: The Nâbra Dataset provides rich and diverse collection of Syrian Arabic from various sources and regions, crucial for training nuanced dialectal model. 2. Dialectal Authenticity: Does the translation use vocabulary, grammar, and idioms that are natural for the Syrian dialect? 3. Fluency: Is the translation grammatically correct and easy to read? Across the entire blind test set, the SHAMI-MT model achieved an outstanding average score of 4.01 out of 5.0. This score places the models average performance squarely in the \"very good\" to \"excellent\" range, confirming its ability to consistently produce high-quality, reliable translations. To demonstrate the practical output of our system, Figure 2 shows some of the SHAMI-MTs translation performance between MSA and Syrian Arabic. As shown in 2, the system accurately captures both formal and colloquial expressions, preserving meaning while adapting vocabulary, structure, and tone to the target variety. While aggregate scores provide general overview of model performance, they often fail to capture the full complexity of translation qualityespecially in tasks involving colloquial language. To gain deeper insight into the systems behavior, we conducted detailed qualitative analysis of model outputs, evaluating both high-performing and low-performing examples. The SHAMI-MT system consistently delivered high-quality translations, particularly in cases requiring contextual understanding and idiomatic fluency. In many examples, the model went beyond simple word-level substitution to produce fluid and natural outputs in the Syrian dialect. Table 1 presents representative samples from the MADAR test corpus, illustrating successful MSA-to-Shami translations. 4 PREPRINT - AUGUST 5, 2025 Figure 2: Examples of bidirectional translation using SHAMI-MT Table 1: Examples of High-Scoring Translations on the MADAR Corpus"
        },
        {
            "title": "Model Prediction",
            "content": "(cid:89)(cid:109)(cid:46)(cid:26)(cid:16)(cid:39) (cid:250)(cid:16)(cid:230)(cid:107) (cid:16)(cid:135)(cid:75)(cid:10)(cid:81)(cid:162)(cid:203)(cid:64) (cid:16)(cid:233)(cid:74)(cid:10)(cid:203)(cid:89)(cid:74)(cid:10)(cid:147) (cid:46) (cid:64) (cid:9)(cid:89)(cid:235) (cid:16)(cid:135)(cid:75)(cid:10)(cid:81)(cid:162)(cid:203)(cid:65)(cid:238)(cid:69)(cid:46) (cid:250)(cid:10)(cid:230)(cid:17)(cid:133)(cid:65)(cid:211) (cid:201) (cid:9)(cid:147) (cid:46) (cid:16)(cid:233)(cid:74)(cid:10)(cid:203)(cid:89)(cid:74)(cid:10)(cid:147) (cid:250)(cid:10) (cid:16)(cid:175)(cid:67)(cid:16)(cid:74)(cid:203) Ground (Shami) (cid:65) (cid:211) (cid:89) (cid:109)(cid:204) (cid:46) (cid:16)(cid:233)(cid:74)(cid:10)(cid:203)(cid:89)(cid:74)(cid:10)(cid:147) (cid:9)(cid:172)(cid:241) (cid:17)(cid:130)(cid:16)(cid:29) (cid:16)(cid:232)(cid:81)(cid:229)(cid:17)(cid:133)(cid:65) (cid:74)(cid:46) (cid:211) (cid:250)(cid:10)(cid:230)(cid:17)(cid:132) (cid:211)(cid:64)(cid:13)"
        },
        {
            "title": "Score Comment",
            "content": "(cid:13) (cid:64) (cid:9)(cid:173)(cid:74)(cid:10) (cid:187) (cid:169) (cid:74)(cid:10) (cid:162) (cid:16)(cid:28) (cid:131) (cid:63) (cid:189)(cid:16)(cid:75)(cid:89)(cid:171)(cid:65)(cid:130)(cid:211) (cid:16)(cid:233)(cid:74)(cid:10)(cid:147)(cid:65)(cid:9)(cid:74)(cid:203)(cid:64) (cid:89)(cid:9)(cid:74)(cid:171) (cid:64)(cid:80)(cid:65)(cid:130)(cid:29)(cid:10) (cid:233)(cid:109)(cid:46)(cid:26)(cid:16)(cid:39)(cid:64) (cid:16)(cid:233)(cid:17)(cid:74)(cid:203)(cid:65)(cid:17)(cid:74)(cid:203)(cid:64) (cid:46) (cid:9)(cid:175) (cid:9)(cid:230)(cid:74)(cid:10) (cid:63)(cid:188)(cid:89)(cid:171)(cid:65)(cid:131) (cid:250)(cid:10) (cid:9)(cid:173)(cid:74)(cid:10)(cid:187) (cid:89) (cid:9)(cid:74) (cid:171) (cid:80)(cid:65) (cid:130) (cid:28)(cid:10) (cid:203)(cid:65) (cid:171) (cid:16)(cid:233)(cid:74)(cid:10)(cid:147)(cid:65)(cid:9)(cid:74)(cid:203)(cid:64) (cid:46) (cid:16)(cid:233)(cid:16)(cid:74)(cid:203)(cid:65)(cid:16)(cid:74)(cid:203)(cid:64) (cid:233) (cid:109)(cid:46)(cid:26)(cid:16)(cid:39)(cid:64) (cid:9)(cid:173)(cid:74)(cid:10)(cid:187) (cid:9)(cid:175) (cid:9)(cid:230)(cid:74)(cid:10) (cid:63) (cid:188)(cid:89)(cid:171)(cid:65)(cid:131) (cid:250)(cid:10) (cid:16)(cid:233)(cid:202) (cid:9)(cid:103)(cid:89)(cid:203)(cid:65)(cid:75)(cid:46) (cid:200)(cid:65)(cid:210) (cid:17)(cid:130)(cid:203)(cid:65)(cid:171) (cid:46) (cid:16)(cid:233)(cid:16)(cid:74)(cid:203)(cid:65)(cid:16)(cid:74)(cid:203)(cid:64) (cid:9)(cid:173)(cid:203) 5 4 Accurately conveys the intended meaning using natural dialectal expressions. Both model output and reference are idiomatic and correct. Perfect match with ground truth; fluent and semantically precise. Very natural phrasing. Minor lexical variation between model and reference, but both are valid Shami translations. On the other hand, thorough evaluation also requires acknowledging instances where the model underperformed. In Table 2, we examine low-scoring examples that highlight two primary sources of error: (1) inconsistencies or mismatches within the evaluation dataset, and (2) legitimate limitations in the models ability to handle highly idiomatic or minimal-context expressions. This analysis reinforces two key observations. First, the model often produces valid translations that diverge stylistically or semantically from the reference, reflecting the inherent subjectivity and variation in dialectal expression. Second, certain low scores stem from the models difficulty with brief, highly idiomatic phrases, where surface-level similarity does not guarantee naturalness. These findings suggest that future improvements should target both the breadth of stylistic variation in training data and the refinement of evaluation methods for subjective tasks like dialect translation."
        },
        {
            "title": "5 Applications and Future Directions",
            "content": "The successful development of the SHAMI-MT system opens the door to wide range of immediate and impactful applications that were previously impractical. Content Localization: The most direct application is in media and technology. Companies can now accurately translate movie subtitles, localize software interfaces and mobile apps, and adapt marketing materials to resonate with Syrian audience, moving beyond generic MSA that can feel stiff and unnatural. 5 Table 2: Analysis of Representative Low-Scoring Translations PREPRINT - AUGUST 5, 2025 MSA Input Model Prediction (cid:72)(cid:46) (cid:80)(cid:65) (cid:16)(cid:174)(cid:203)(cid:64) (cid:201) (cid:16)(cid:174)(cid:16)(cid:74)(cid:130) (cid:9)(cid:28)(cid:131) (cid:250)(cid:16)(cid:230)(cid:211) (cid:63) (cid:201) (cid:16)(cid:174) (cid:16)(cid:74) (cid:130) (cid:9)(cid:29) (cid:104)(cid:80) (cid:63)(cid:72)(cid:46) (cid:80)(cid:65)(cid:16)(cid:174)(cid:203)(cid:64) (cid:13) (cid:16)(cid:73) (cid:214)(cid:223)(cid:10) (cid:64) Ground (Shami) (cid:73)(cid:46) (cid:187)(cid:81)(cid:9)(cid:30) (cid:211) (cid:63) (cid:72)(cid:46) (cid:80)(cid:65)(cid:16)(cid:174)(cid:203)(cid:65)(cid:75)(cid:46) (cid:9)(cid:225)(cid:75)(cid:10)(cid:240) (cid:9)(cid:225) (cid:211) Truth Score Comment (cid:16)(cid:72)(cid:90)(cid:65)(cid:103)(cid:46) (cid:9)(cid:225)(cid:211) (cid:250)(cid:10)(cid:205) (cid:46) (cid:169)(cid:163)(cid:65)(cid:16)(cid:174)(cid:16)(cid:74)(cid:203)(cid:64) (cid:89)(cid:9)(cid:74)(cid:171) (cid:73)(cid:46) (cid:16)(cid:232)(cid:80)(cid:65)(cid:74)(cid:10)(cid:130)(cid:203)(cid:64) (cid:9)(cid:75)(cid:65)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:248)(cid:10) (cid:89) (cid:9)(cid:74)(cid:170)(cid:203) (cid:16)(cid:73)(cid:107)(cid:46) (cid:64)(cid:13) (cid:46)(cid:169)(cid:163)(cid:65)(cid:16)(cid:174)(cid:16)(cid:74)(cid:203)(cid:64) (cid:89)(cid:9)(cid:74)(cid:171) (cid:16)(cid:233)(cid:234)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64) (cid:16)(cid:232)(cid:80)(cid:65)(cid:74)(cid:10)(cid:130)(cid:203)(cid:64) (cid:9)(cid:225)(cid:211) (cid:9)(cid:225)(cid:211) (cid:46) (cid:16)(cid:134)(cid:81) (cid:9)(cid:174)(cid:214)(cid:207)(cid:65)(cid:171) (cid:73)(cid:46) (cid:16)(cid:232)(cid:80)(cid:65)(cid:74)(cid:10)(cid:130)(cid:203)(cid:64) (cid:250)(cid:10) (cid:9)(cid:74)(cid:106)(cid:46) (cid:171) (cid:9)(cid:230)(cid:16)(cid:74)(cid:162)(cid:74)(cid:46) (cid:9)(cid:107) (cid:46) (cid:249)(cid:10) (cid:235)(cid:65)(cid:235) (cid:46)(cid:250)(cid:10)(cid:230)(cid:10)(cid:235) (cid:249)(cid:10) (cid:235) (cid:46) (cid:233)(cid:186)(cid:74)(cid:10)(cid:203) 2 2 Model preserves the temporal aspect of the original question, while the ground truth shifts to spatial focus. Both are plausible interpretations, but differ in intent. The model produces faithful, literal translation. The reference uses more colloquial and compressed phrasing. Semantic alignment exists, but stylistic divergence lowers the score. The model opts for more formal repetition, while the reference uses highly idiomatic single-word Shami Highlights expression. challenges handling in ultra-short, highly informal inputs. Cultural Preservation and Linguistics: For researchers, the models serve as powerful tool. Linguists can use the Shami-to-MSA model to standardize dialectal texts for easier analysis, while historians and sociologists can process large volumes of dialectal social media data to understand cultural trends. Educational Technology: Language learning platforms can integrate SHAMI-MT to create dynamic educational tools. An Arabic learner could input an MSA sentence and see its authentic Syrian equivalent, complete with explanations, bridging the critical gap between textbook Arabic and real-world communication. Enhanced Intercultural Communication: The models can facilitate clearer communication in various social and professional settings, from customer service chatbots that can understand dialectal queries to tools that help aid workers and diplomats better understand local contexts. Looking forward, we have identified several promising avenues for future research. The immediate next step is to expand the systems capabilities to other major Levantine dialects, such as Lebanese, Palestinian, and Jordanian, to explore the potential for cross-dialectal transfer learning. Furthermore, we plan to investigate more efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), to reduce the computational cost of adapting the model. Finally, integrating these text models with speech recognition and synthesis systems could pave the way for comprehensive, real-time speech-to-speech translation system for Arabic dialects."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper has detailed the development and evaluation of SHAMI-MT and SHAMI-MT-2MSA, pair of high-fidelity, bidirectional machine translation models for Modern Standard Arabic and the Syrian dialect. By leveraging stateof-the-art, Arabic-specific transformer architecture (AraT5v2) with targeted fine-tuning on rich, authentic dialectal corpus (Nâbra), we have created system that produces translations of exceptional quality, accuracy, and dialectal authenticity. Our rigorous evaluation on the MADAR benchmark confirms the models robust performance. SHAMI-MT successfully fills critical void in the Arabic NLP landscape, providing an invaluable resource for applications ranging from content localization to cultural preservation. More broadly, our work underscores the importance of specialization in an era of massive language models, demonstrating that for the nuanced and complex challenge of dialectal translation, focused approach yields superior results. We believe this work sets new standard for dialectal machine translation and will serve as catalyst for further innovation in this vital and under-resourced field. 6 PREPRINT - AUGUST 5,"
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Prince Sultan University for their generous support in enabling this research."
        },
        {
            "title": "References",
            "content": "[1] M. Enis and M. Hopkins, From LLM to NMT: Advancing low-resource machine translation with claude, 2024, arXiv:2404.13813. [2] Saeed, A. M. A. (2025). Machine Translation Evaluation between Arabic and English during 2020 to 2024: Review Study. Arts for Linguistic & Literary Studies, 7(2), 665-678. [3] Fatiha Sadat, Farnazeh Kazemi, and Atefeh Farzindar. 2014. Automatic identification of arabic dialects in social media. SoMeRA 14, page 3540, New York, NY, USA. Association for Computing Machinery. [4] Libovický, J., Rosa, R., & Fraser, A. (2019). How language-neutral is multilingual BERT?. arXiv preprint arXiv:1911.03310. [5] Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., ... & Raffel, C. (2020). mT5: massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934. [6] Nacar, O., Sibaee, S., Alharbi, A., Ghouti, L., & Koubaa, A. (2024, August). ASOS at NADI 2024 shared task: Bridging Dialectness Estimation and MSA Machine Translation for Arabic Language Enhancement. In Proceedings of The Second Arabic Natural Language Processing Conference (pp. 748-753). [7] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: Text-to-Text Transformers for Arabic Language Generation. arXiv preprint arXiv:2109.12068v4, 2022. [8] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140), 1-67. [9] Amal Nayouf, Tymaa Hasanain Hammouda, Mustafa Jarrar, Fadi Zaraket, and Mohamad-Bassam Kurdy. Nâbra: Syrian Arabic dialects with morphological annotations. arXiv preprint arXiv:2310.17315, 2023. [10] Bouamor, H., Habash, N., Salameh, M., Zaghouani, W., Rambow, O., Abdulrahim, D., ... & Oflazer, K. (2018, May). The MADAR Arabic dialect corpus and lexicon. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018). [11] Salam Khalifa, Nizar Habash, and Houda Bouamor. Morphologically-Aware Fine-Grained Arabic to English SMT System. In Proceedings of the Second Workshop on Arabic Corpora and Processing Tools, pages 5865, 2016. [12] Ahmed Abdelali, Sabit Hassan, Hamdy Mubarak, Kareem Darwish, and Younes Samih. Pre-training Bert on Arabic Tweets: Practical Considerations. arXiv preprint arXiv:2102.10684, 2021. [13] Marwah Alian, Arafat Awajan, Ahmad Al-Hasan, and Raeda Akuzhia. Towards building arabic paraphrasing benchmark. In Proceedings of the Second International conference on Data Science E-learning and Information Systems, pages 15, 2019. [14] Serry Sibaee and Omer Nacar. Shami-MT : Machine Translation from MSA to Syrian Dialect. 2025."
        }
    ],
    "affiliations": [
        "Prince Sultan University, Riyadh - Saudi Arabia"
    ]
}