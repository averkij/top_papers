{
    "paper_title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
    "authors": [
        "Yibo Lyu",
        "Gongwei Chen",
        "Rui Shao",
        "Weili Guan",
        "Liqiang Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%."
        },
        {
            "title": "Start",
            "content": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records Yibo Lyu, Gongwei Chen, Rui Shao, Weili Guan, Liqiang Nie Harbin Institute of Technology, Shenzhen weberlv1b@gmail.com {shaorui, nieliqiang}@hit.edu.com https://github.com/JiuTian-VL/PersonalAlign 6 2 0 2 4 1 ] . [ 1 6 3 6 9 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, benchmark designed to evaluate agents ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIMAgent significantly improves both execution and proactive performance by 15.7% and 7.3%."
        },
        {
            "title": "Introduction",
            "content": "With the rapid advancement of multimodal large language models (MLLM) (An et al., 2025; Bai et al., 2025), GUI agents have made significant progress in grounding natural language instructions into executable actions (Wang et al., 2025a; Xie et al., 2025a; Chen et al., 2025a). However, most existing works are evaluated in simulated environments and rely on the strong assumption of complete user instructions. We argue that instructions often fail to fully capture users true intent in daily Corresponding authors. Figure 1: We highlight new PersonalAlign agent task. Agent should leverage the users long-term record to provide hierarchical implicit intent alignment for both preference and routine intent. usage, motivating the need for personalized agents capable of perceiving intent beyond the traditional instruction-following reactive paradigm. From user-centric perspective, humanagent interaction constitutes joint activity where meaning is co-constructed through shared context (Clark, 1996). In daily usage, this shared context leads users to naturally omit recurring patterns, assuming the agent can \"fill in the blanks\" from historical records (Cheng et al., 2025b; Qian et al., 2024), leading to the emergence of implicit intent. As shown in Figure 1, user intent exhibits hierarchical degrees of implicitness. While reactive agents primarily handle explicit instructions, personalized agent should extend to align users implicit intent by leveraging long-term user records as context: addressing preference intent in vague instructions where preference details are omitted, and further anticipating routine intent without instruction only based on current user state. Bridging implicit intent gaps is essential for effective joint activity Table 1: Comparison of the AndroidIntent with existing GUI benchmark or dataset. We also show that whether each trait is fully incorporated (), partially incorporated (), or not incorporated (). Benchmark or Dataset AITW (Rawles et al., 2023) AndroidControl(Li et al., 2024a) SPA-Bench(Chen et al., 2025b) ProactiveAgent(Lu et al., 2025b) OS-Karois(Cheng et al., 2025a) IFRAgent(Wu et al., 2025) FingerTip(Yang et al., 2025b) AndroidIntent vague Instruction Proactive Suggestion Long-term Records User-Centric Annotation Task Target GUI Execution GUI Execution GUI Execution User Modeling Proactive Agent Human Cooperation Personalized Execution Behavior Prediction Intent Alignment and for establishing stronger humanagent trust. To address this practical dilemma, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign). This task shifts focus from simple execution to align with users implicit intent. Specifically, PersonalAlign requires agent to identify preferences from past repeated selections to resolve vague instructions, while separating more frequent and state-consistent routines to facilitate proactive assistance. This enables agents to transition from independent reactive executors into personalized partners that co-evolve through personal interaction. To achieve PersonalAlign, current research faces two limitations: 1. Lack of long-term, user-centric annotated benchmarks. Most existing datasets primarily focus on static, simulated completion instruction execution, failing to evaluate personalized agent. To bridge this gap, we introduce AndroidIntent, novel benchmark constructed from personal daily records. We explicitly annotate user preferences and routines from long-term records, and simulate implicit intent by carefully removing recoverable personal preferences from original intents. To mitigate subjectivity and conceptual ambiguity during the annotation process, we introduce hierarchical filtering-verifying strategy. This approach translates abstract personalization concepts into quantifiable scores, allowing us to efficiently identify candidates for human verification. As detailed in Table 1, AndroidIntent evaluates GUI agents ability to align implicit intent by leveraging long-term user records as context for user modeling. 2. Inability to manage long-term user records. Existing agent memory for LLM-chat often rely solely on semantic similarity, which is insufficient for handling GUI execution. Moreover, such memory fail to support hierarchical intent alignment. To address these limitations, we propose specially designed agent memory, Hierarchical Intent Memory Agent (HIM-Agent). HIM-Agent incorporates Streaming Aggregation Module to enable incremental updates of user behavior. Building upon this foundation, the Execution-based Preference Filter and the State-based Routine Filter extract Preference Intent Memory and Routine Intent Memory, forming hierarchical intent alignment support. In the experiment, we observe that vague instructions typically convey coarse-grained sub-goals, causing fine-grained execution failures when specific personal requirements are absent at certain steps, and that most current GUI agents fail to deliver reliable proactive suggestions, calling for personal agents stronger context analysis capabilities. Our contribution can be summarized as: We introduce new agent task PersonalAlign, which requires agent to align with users implicit intents through long-term record. We construct new GUI bench AndroidIntent, which annotates daily preference and routine intent from long-term records. We propose new agent memory HIM-Agent, which hierarchically learns preference and routine intent from long-term records. We conduct extensive experiments and analysis across various GUI Agents and demonstrate superior performance of HIM-Agent."
        },
        {
            "title": "2.1 Personalized GUI Agent",
            "content": "With the rapid advancement of multimodal large language models (Lyu et al., 2025; Zhu et al., 2025; Zhang et al., 2025a), wide range of downstream tasks have benefited significantly (Shao et al., 2024; Zhang et al., 2024; Li et al., 2025; Shao et al., 2019, 2023). While many GUI datasets and methods concentrate on evaluating execution success rate (Lu et al., 2025a; Xie et al., 2024; Zhou et al., 2025a), more intelligent personal agent assistant should infer and match users true intent. Some prior work focuses on learning from human action for personalization. For example, IFRAgent (Wu et al., 2025) extracts both explicit and implicit intention flows from short-term instruction traces. FingerTip (Yang et al., 2025b) uses recent historical actions to personalize current execution. On the other hand, some research emphasizes proactive suggestions for personalization. ProactiveAgent (Lu et al., 2025b) trains agents to monitor the environment and user activities to enable proactive behaviors; ContextAgent (Yang et al., 2025a) leverages real-world sensors to infer users current states. However, existing methods rarely address daily, user-centric personalization; preference execution and proactive suggestion have largely evolved as isolated paradigms. In this work, we bridge this gap by introducing hierarchical view of implicit intent, unifying these two tasks into coherent, user-centric personalization framework."
        },
        {
            "title": "2.2 Agent Memory",
            "content": "Memory is essential for agents to persist user longterm GUI interactions, which is critical for building intelligent personal GUI agent (Li et al., 2024b). MemAgent (Yu et al., 2025) and Mem1 (Zhou et al., 2025b) propose progressively compressing dialogues to extend context length, while MemoryR1 (Yan et al., 2025) introduces an RL framework to train agents in organizing and utilizing memory. Mirage-1 (Xie et al., 2025b) constructs an Execution Skill Memory to enhance GUI agent performance. For user profiling and memory construction, Memory OS (Kang et al., 2025) aggregates user dialogues into segments and applies memory into short-, mid-, and long-term modules for retrieval. LettinGo (Wang et al., 2025b) uses LLM to generate diverse and adaptive user profiles. PersonaX (Shi et al., 2025) clusters historical records for user profile modeling. To address PersonalAlign, agent memory should not only generalize over long-term user records but also hierarchically organize user intents. as historical record = {R1, R2 . . . Rm}, while the remaining 20% record serves as the executing record = {Rm+1 . . . Rn} for annotation and inference. Each GUI interaction record Ri consists of the following elements: instruction I, interaction time , interaction scenario S, execution action trajectory = [τ1, τ2, . . . , τn] and screen observation = [o1, o2, . . . , on], formally represented as: Ri = {(Ii, Ti, Si, Ai, Oi)}N . At the most fundamental level, when the user provides complete and explicit instruction It, the agent operates under reactive GUI execution paradigm. In this setting, the agent executes sequence of actions τt to fulfill the instruction, lacking the capability to leverage long-term historical records and perform personalization. Beyond the reactive level, users daily instructions may lack explicit preference information. In these cases, personalized agent should infer the missing preferences based on users historical record H. Given vague instruction ˆIt, agent should infer users intent from the historical intent and action sequences, ensuring each step selection satisfies the users intent. At higher level, user may exhibit latent routines, requiring agents to incorporate current user state to provide proactive suggestions. By leveraging repetitive intents in historical records under similar states, agents can proactively generate suggestions t, even without user instructions, denoted as I. We formalize these three paradigms as: At fθ(It; ({}) H) At fθ( ˆIt; ({Ii, Ai}) H) fθ(I; ({Ii, Ti, Si}) H), (1) where the three formulations correspond to Reactive, Preference, and Routine alignment, respectively. The left side represents user inputs, while the right side denotes the historical interaction records that the agent leverages. For PersonalAlign, we primarily focus on the latter two settings, where agent should learn to apply the users long-term records as context to provide hierarchical personalized services to improve user trust and satisfaction in daily use. More discussion about personal agent settings is in Appendix D."
        },
        {
            "title": "3 PersonalAlign Task Definition",
            "content": "For each users long-term record, we organize the data chronologically and use the first 80% record To evaluate PersonalAlign, we need to annotate the ground truth from continuous user records, specifically identifying user preference and routine inFigure 3: Visualization of the users intents distribution by aggregating all executing records across users. At sufficient scale, the intent statistics exhibit three approximately Gaussian-like distributions. Intent Semantic Similarity. In analyzing semantic similarity, we employ Qwen3-Embedding (Zhang et al., 2025b) as our embedding model to measure the semantic similarity between and E. For record Re E, we compute its similarity with all intents in the historical record H. Specifically, to prioritize intents with high recurrence, we focus on the density of similar historical instances rather than isolated occurrences. We retain the top-k most similar records Hk and compute their average similarity score. As result, intents that are both semantically closer and more frequent in history receive higher scores. Scos(Re) = 1 (cid:88) Hk cos(cid:0)emb(Ie), emb(Ik)(cid:1), (2) where Ie Re, Ik Hk, emb() denotes the embedding function, and cos() represents the cosine similarity between two embeddings. User State Offset. In analyzing user state, time and scenario serve as key components. We quantify their distribution by analyzing the offsets of Hk relative to Re to determine whether similar interactions have been frequently executed under similar states. Temporal offset entropy Ht is computed as the normalized entropy over hour-level temporal offsets between Hk and Re, reflecting the degree of temporal consistency. Scenario offset entropy Hs is defined as the normalized entropy over scenario category offsets between Hk and Re, indicating whether intents are scenario-specific. This can be formulated as: Ht(Re) = Hs(Re) = (cid:80) p(h) log2 p(h) log2 hour (cid:80) p(s) log2 p(s) log2 scene , (3) where and means the offsets of each Hk to Re, and hour, scene means the total hours and Figure 2: Overview of AndroidIntent collection pipeline. We employ two-stage filtering-verification, integrating objective criteria with subjective judgment to hierarchically annotate user intent from long-horizon records. tents that are sufficiently supported by historical evidence to enable reliable personalization. To bridge this gap, we present AndroidIntent, built upon 91 users 2 months of Android interaction records from Fingertip20K (Yang et al., 2025b). However, directly annotating such 20k long-horizon user histories introduces significant challenges: the definition of preference and routine is ambiguous and lacks objective standards. To alleviate these issues and ensure annotation quality, we introduce new hierarchical filtering-verifying strategy. As shown in Figure 2, we illustrate the annotation pipeline."
        },
        {
            "title": "4.1 Hierarchical Filtering Strategy",
            "content": "Analysis Strategy. Developing personalized agents should first validate stable, modelable personal patterns to establish an existence proof and statistical bounds. We conceptually distinguish one-off moment intents from recurring preferences and routines based on their occurrence patterns within H, where higher frequency and consistency indicate stable preferences to routines. To translate these concepts into practical filtering and validation strategy, we tend to adopt simple and robust approach that avoids inductive bias and overpruning. Consequently, we focus on simple and well-established measures of intent semantic similarity and user state distribution. scenarios for normalization. Lower entropy values indicate more stable user states in historical interaction, which are more suitable for proactive. Quantifying and Filtering. Finally, following Eq. 4, we compute the quantified score Qscore for each executing record in across all users. As shown in Figure 3, the resulting distributions over large-scale user data exhibit three hierarchical approximately Gaussian modes, which naturally correspond to moment, preference, and routine intents, showing the existence of modelable patterns for personalized agent at global mining view. This distributional structure enables fast filtering of large-scale data to identify preference and routine candidates, improving both the objectivity and scalability of the annotation. Qscore(Re) = Scos + Ht + Hs (4)"
        },
        {
            "title": "4.2 Human Verifying Strategy",
            "content": "To ensure the final dataset remains unbiased by the filtering process and strictly aligns with human feelings, we perform Human Verifying after filtering. Following quantification, we slightly expand the sampling range of the Gaussian distribution to retain broader set of samples in overlapping regions, further mitigating potential overfitting to specific filtering rules. During annotation, annotators further compare each preference and routine candidate with the users historical records to reverify and re-annotate the intent types. To support the construction of vague preference instructions, we additionally use GPT to generate diverse set of alternative instructions that deliberately omit potential user preferences. Annotators then select the instruction that best matches the users likely intent as the final vague instruction. All candidates undergo multiple rounds of cross-validation by independent annotators to ensure data quality. Finally, we collect 775 annotated preference intent and 215 annotated routine intent. More statistical analysis is in Appendix A, and annotators requirements are in Appendix E."
        },
        {
            "title": "5 HIM-Agent",
            "content": "To support PersonalAlign, agent memory should generalize stable representations to exclude one-off moments while separating preferences and routines, and continuously evolve to stay aligned with user intents. As shown in Figure 4, we introduce HIMAgent, foundational and inspirational personal agent memory that enables GUI agents to rapidly leverage long-term records as context for personalization without interfering with original execution. We construct streaming update memory and hierarchically organize memory prototypes into Preference Intent Memory and Routine Intent Memory through the execution-based and state-based filter to enable hierarchical intent alignment."
        },
        {
            "title": "5.1 Streaming Aggregation Module",
            "content": "Raw low-level GUI interaction records are inherently fragmented and noisy. Trivial operating based on these raw record leads to long-tail effects and memory drift, making it difficult to maintain stable personalized representations over time. To address this challenge, we propose Streaming Aggregation Module that reframes personalization memory from static log-based storage to continually evolving representation. Rather than operating in individual records, we maintain Record Prototypes Pi as the fundamental memory units that synthesize similar records into cohesive whole: Pi = {RhSconsist(Rh, Pi) > θ}, (5) where Rh represents incoming historical records, Sconsist measures the consistency between records and prototypes. Based on MicroCluster in stream mining (Aggarwal et al., 2003), our module incrementally aggregates records at dailygranularity, enabling an evolving personal memory."
        },
        {
            "title": "5.2 Execution-based Preference Filter",
            "content": "GUI agent memory differs from chat-based memory in that each user interaction includes an execution trajectory rather than purely semantic content. In the Execution-based Filter, we compute Sconsist by jointly modeling semantic intent similarity and action trajectory consistency for more comprehensive aggregation for GUI interaction. For semantic similarity Ssim, we combine dense embedding cosine similarity Scos with sparse Jaccard SJac, which computes the overlap ratio of shared words between instructions, to robustly measure semantic similarity. Since GUI instructions are often short and entity-heavy (e.g., app names, items), this may lead to distortions in Scos. For action consistency Saction, we employ Dynamic Time Warping (DTW) to measure the similarity between trajectories that have temporal structure. DTW computes an optimal alignment path π by minimizing the cumulative distance between Table 2: Impact of instruction-induced degradation on GUI agents. Various agents are evaluated under both complete and vague instructions, with results under vague instructions shown in the gray line. Model Type SSR CER Closed-sourced GUI Agents GPT-5.1 GLM-4.5v QwenVL-Max UI-TARS-1.5 GUI-Owl Qwen3-VL 51.2 49.33.7% 51.0 50.50.9% 51.9 51.60.5% 26.4 20.323.1% 27.4 19.425.5% 29.8 24.816.9% Open-sourced GUI Agents 49.4 46.82.6% 54.2 53.12.0% 52.7 46.612.0% 23.5 19.915.3% 24.9 17.928.1% 26.7 20.622.8% 52.3 22.956.2% 54.5 22.658.5% 53.3 27.348.8% 38.6 14.923.7% 50.4 23.753.0% 52.9 26.649.7% Hstate, which captures the normalized of temporal and scenario entropies within records of prototype; the record length in prototype Lrecord, reflecting how frequently the pattern recurs; and the aggregation weight Rconsist, obtained by averaging Sconsist. Confidence is jointly inferred based on state consistency, execution consistency, and frequency. This is formulated as: Figure 4: Overview of HIM-Agent. The Streaming Aggregation Module updates user records daily, and the aggregated prototypes are hierarchically organized to support personalized preference and routine intent. aligned action steps. The execution-based preference filter can be formulated as: Ssim(Ih, Ici ) = Scos + SJac (cid:80) Saction(Ah, Aci ) = min Sconsist(Rh, Pi) = Ssim + Saction, π (i,j)π d(ai, bj) (6) Φ(Pi) = Hstate + Lrecord + Rconsist, (7) where Ih, Ah denote the intent and action of Rh, while Ici, Aci represent center intent and action of prototype Pi, which are updated daily by selecting the instructions and actions with the minimum average distance to all other records assigned to Pi. The pairwise distance d(ai, bj) is computed based on the GUI action success rate (SR), which evaluates whether two actions are the same action. After filtering, each Record Prototype Pi provides stable representation and is stored in Preference Intent Memory. When HIM-Agent needs to infer user preferences, the corresponding prototypes center intent Ic and action Ac will be provided."
        },
        {
            "title": "5.3 State-based Routine Filter",
            "content": "Upon the formation of stable prototype Pi, we introduce State-based Routine Filter to further separate passive preferences from proactive intents. This module jointly considers the frequency of occurrence, the execution coherence, and the consistency of user states of each Pi to determine whether proactive suggestions should be activated. To achieve this, we define proactive confidence Φ(Pi), which is calculated by: the state stability If Φ(Pi) exceeds the proactive confidence boundary, the corresponding prototype is stored in Routine Intent Memory. When the HIM-Agent needs to determine whether proactive suggestions are required, prototypes center intent Ic and the most frequent state Tc, Sc will be provided."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "Metrics. We evaluate GUI execution using Type Accuracy (Type) and Step-wise Success Rate (SSR) under an offline evaluation protocol, where treated user actions as the golden trajectory. Moreover, we introduce new Cumulative Error Rate (CER) to measure failures on critical steps caused by vague instructions (ElMallah et al., 2025), where missing user-specific information leads to mismatches with the users true intent. To approximate the impact of errors on critical steps, we assign decaying weight to each step along the trajectory, such that earlier errors contribute more heavily to the overall score. CER thus serves as an intermediate metric that bridges offline evaluation with online performance. Table 3: GUI agent performance in proactive service. Lower False-Alarm rates indicate better proactive accuracy. Values marked in red denote cases of insufficient capability by the agent. Model Intent Alignment Semantic Judgment Precision Recall Identification Alignment False-Alarm F1-score GPT-5.1 GLM-4.5V QwenVL-Max UI-TARS-1.5 GUI-Owl Qwen3-VL 49.4% 52.8% 52.2% 42.6% 32.6% 45.0% Closed-sourced GUI Agents 32.0% 33.9% 34.8% 73.1% 78.6% 96.7% 68.9% 97.2% 67.4% Open-sourced GUI Agents 19.0% 12.2% 23.6% 99.1% 68.7% 79.9% 57.2% 69.0% 97.2% 62.0% 94.0% 98.0% 97.0% 31.0% 94.0% 75.8% 80.4% 67.4% 81.1% 66.7% 80.7% Table 4: Execution performance across various methods under vague instructions. denotes the baseline. Base Model Type 46.6 SSR 20.6 CER 26. Retrieve-based Recent Retrieve LLM-UM HIM-Agent Generalized-based 49.4 21.1 33.2 51.0 22.4 35.4 51.2 22.3 35.2 52.0 24.0 42. Table 5: Proactive performance comparison. FA means False-Alarm rate. denotes the baseline. Table 6: Ablation study of components in Executionbased Preference Filter Module. Dense and Sparse denote embedding and Jaccard similarity. Components Sparse Action Dense Performance Type 49.4 50.8 51.1 51.4 52.0 SSR CER 33.2 21.1 35.9 22.5 36.3 22.9 37.3 23.3 42.3 24.0 Generalized-based Retrieve-based Recent Retrieve LLM-UM HIM-Agent 49.4% 49.8% Semantic Judgement 32.0% 32.2% 70.8% 74.2% Precision 78.3% 82.0% Recall FA 62.0% 64.0% 75.8% 77.9% F1-score 3089 4930 Token 49.1% 31.6% 75.6% 82.3% 57.0% 78.8% 1161(+6518) 53.5% 36.3% 78.1% 81.4% 49.0% 79.7% 1605 On the other hand, to evaluate the agents proactive recommendation capability, we consider Intent Alignment and Identification Alignment (Lu et al., 2025b). We measure the Semantic similarity between generated suggestions and users original intent using embedding cosine similarity and edit distance. We also use an LLM-as-Judgment to evaluate intent alignment, where DeepSeek-V3 is employed to mitigate self-bias. Identification Alignment evaluates proactive appropriateness, we carefully collect 100 negative user states that do not require proactive assistance, and compute Precision, Recall, False-Alarm, and F1-score."
        },
        {
            "title": "Please also refer to Appendix B for more details",
            "content": "about baselines and implementation details."
        },
        {
            "title": "6.2 Experimental Analysis",
            "content": "Vague Instruction Impact on GUI Execution. Table 2 shows the impact of vague instructions on several outstanding openand closed-sourced GUI agents. Currently, GUI agents still need to improve their performance across daily instructions Figure 5: Ablation study of components for proactive performance. Lower of False-Alarm means better align. and apps since most SSR is around 25-30. Notably, while ambiguity leads to only 3% drop in type accuracy, SSR and CER decrease by approximately 20% and 45%. We observe that vague instructions act as coarse-grained sub-goals: although agents can often identify the high-level intended operation, execution fails at fine-grained level due to the absence of critical personalized preference information. For example, lacking explicit requirements, the agent may open incorrect apps, causing execution to deviate significantly from user intent. Challenge in Balancing Proactive Identification. Table 3 evaluates the proactive performance of GUI agents based on recent historical records and current user state. Notably, most models struggle to determine when proactive behavior is truly necessary. Aside from GPT-5.1, current GUI agents generally fail to provide effective proactive suggestions, as they struggle to balance false Figure 6: Case study of HIM-Agent. Left: HIM-Agent resolves vague instructions by aligning intent with historical interaction records. Right: HIM-Agent proactively suggests based on historical record and user state. alarms and recall, often defaulting to overly proactive. These failure cases are highlighted in red and underlined. This reveals promising research direction: to effectively leverage user records for personalization, agents must develop superior longterm context analysis capabilities. HIM-Agent significantly enhances agents ability to align implicit intent. Since PersonalAlign is novel paradigm without established methods, we select and compare it against two representative categories of basic approaches: (i) top-down retrieval-based methods, which incorporate recent or relevant historical records as user context for the agent, and (ii) bottom-up inductive methods, which leverage LLMs to summarize user profiles (LLM-UM) (Wang et al., 2025b), alongside our HIM-Agent. As shown in Table 4, we build HIM-Agent based on outstanding open-sourced Qwen3-VL. HIM-Agent achieves the best performance in alleviating the impact of vague instructions, obtaining CER score of 42.3. Furthermore, to objectively evaluate different methods proactive capability, we conduct experiments on GPT-5.1, which only shows basic balanced proactive ability. As shown in Table 5, our framework achieves superior performance in both Semantic Alignment and Identification Alignment. HIM-Agent helps the agent achieve better balance between recall 81.4% and false-alarm 49% and keep the highest Intent Alignment score 53.3% and 36.3%. While LLM-UM introduces extra 6518 tokens consumed when generating user profiles, HIM-Agent remains highly efficient during generalized user modeling. Ablation Study. Table 6 presents the ablation study of components within Execution-based Preference Filter. The first gray line denotes the setting without this filter, under which the streaming aggregation module also cant work, and memory degenerates to individual records without prototypes. The results indicate that all three components contribute to performance gains, and the full module achieves 9.1% improvement in CER. As shown in Figure 5, we further demonstrate the importance of state-related components in enabling proactive, where both time and scenario play crucial roles. Notably, removing the filter while retaining all prototypes will cause an increase in false alarms to nearly 70%, which is even higher than the baseline with recent individual records, highlighting the critical role of the state filter for proactive."
        },
        {
            "title": "6.3 Case Study",
            "content": "As shown in Figure 6, we present case studies comparing HIM-Agent and reactive agent. In daily usage, user instructions often omit preferences, leading reactive GUI agents to misalign with the users true intent. HIM-Agent can infer missing preferences from historical records to correct action execution. By jointly reasoning over records and the current state, HIM-Agent can also proactively assist users, whereas reactive agents remain inactive without explicit instructions."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce new critical challenge for agents, PersonalAlign, which requires hierarchical personalization to resolve implicit intents in daily interactions. We introduce AndroidIntent, new user-centric GUI benchmark curated with filterverify strategy, and propose HIM-Agent, memory framework that enables hierarchical personalization based on long-term user records. Experiments on AndroidIntent show some new challenge and the effectiveness of the HIM-Agent."
        },
        {
            "title": "Limitations",
            "content": "While AndroidIntent and HIM-Agent evaluate how user implicit intent influences GUI agents, several limitations remain. First, the availability of suitable datasets remains limited. Due to the lack of large-scale, publicly available datasets that capture long-term user interaction records, our evaluation is currently restricted to the Fingertip. Our annotation strategy and method cannot yet be broadly validated across diverse real-world datasets. Future work could extend data collection to other GUI environments, such as operating systems and desktop applications. Second, our method may suffer from cold-start issue, where insufficient historical interaction data limits the agents ability to accurately infer user intent and provide proactive suggestions. Addressing these challenges remains an important and promising direction for future research."
        },
        {
            "title": "References",
            "content": "Charu Aggarwal, Yu Philip, Jiawei Han, and Jianyong Wang. 2003. framework for clustering evolving data streams. In Proceedings 2003 VLDB conference, pages 8192. Elsevier. Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, and 1 others. 2025. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025. Qwen3-vl technical report. Preprint, arXiv:2511.21631. Gongwei Chen, Xurui Zhou, Rui Shao, Yibo Lyu, Kaiwen Zhou, Shuai Wang, Wentao Li, Yinchuan Li, Zhongang Qi, and Liqiang Nie. 2025a. Less is more: Empowering gui agent with context-aware simplification. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, and 1 others. 2025b. Spabench: comprehensive benchmark for smartphone In The Thirteenth International agent evaluation. Conference on Learning Representations. Pengzhou Cheng, Zheng Wu, Zongru Wu, Aston Zhang, Zhuosheng Zhang, and Gongshen Liu. 2025a. Oskairos: Adaptive interaction for mllm-powered gui agents. arXiv preprint arXiv:2503.16465. Ziming Cheng, Zhiyuan Huang, Junting Pan, Zhaohui Hou, and Mingjie Zhan. 2025b. Navi-plus: Managing ambiguous gui navigation tasks with follow-up. arXiv preprint arXiv:2503.24180. Herbert Clark. 1996. Using language. Cambridge university press. Ramy ElMallah, Krish Chhajer, and Chi-Guhn Lee. 2025. Score the steps, not just the goal: Vlm-based subgoal evaluation for robotic manipulation. arXiv preprint arXiv:2509.19524. Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. 2025. Memory os of ai agent. arXiv preprint arXiv:2506.06326. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024a. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154. Wei Li, Renshan Zhang, Rui Shao, Jie He, and Liqiang Nie. 2025. Cogvla: Cognition-aligned visionlanguage-action model via instruction-driven routing & sparsification. In Advances in Neural Information Processing Systems. Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. 2024b. Optimus1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. In Advances in Neural Information Processing Systems, volume 37, pages 4988149913. Quanfeng Lu, Wenqi Shao, Zitao Liu, Lingxiao Du, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, and Ping Luo. 2025a. Guiodyssey: comprehensive dataset for cross-app gui navigation on mobile devices. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2240422414. Yaxi Lu, Shenzhi Yang, Cheng Qian, Guirong Chen, Qinyu Luo, Yesai Wu, Huadong Wang, Xin Cong, Zhong Zhang, Yankai Lin, and 1 others. 2025b. Proactive agent: Shifting llm agents from reactive responses to active assistance. In The Thirteenth International Conference on Learning Representations. Yibo Lyu, Rui Shao, Gongwei Chen, Yijie Zhu, Weili Guan, and Liqiang Nie. 2025. Puma: Layer-pruned language model for efficient unified multimodal retrieval with modality-adaptive learning. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 76537662. Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, and 1 others. 2024. Tell me more! towards implicit user intention understanding of language model driven agents. arXiv preprint arXiv:2402.09205. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, and 1 others. 2025. Uitars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo CampbellAjala, and 1 others. Androidworld: dynamic benchmarking environment for autonomous agents. In The Thirteenth International Conference on Learning Representations. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2023. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728. Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong Yuen. 2019. Multi-adversarial discriminative deep domain generalization for face presentation attack detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10023 10031. Rui Shao, Tianxing Wu, and Ziwei Liu. 2023. Detecting and grounding multi-modal media manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6904 6913. Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. 2024. Detecting and grounding multi-modal media manipulation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence. Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, and Min Xu. 2025. Personax: recommendation agent oriented user modeling framework for long behavior sequence. arXiv preprint arXiv:2503.02398. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, and 1 others. 2025a. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544. Lu Wang, Di Zhang, Fangkai Yang, Pu Zhao, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, and 1 others. 2025b. Lettingo: Explore user profile generation for recommendation system. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pages 29852995. Zheng Wu, Heyuan Huang, Yanjia Yang, Yuanyi Song, Xingyu Lou, Weiwen Liu, Weinan Zhang, Jun Wang, and Zhuosheng Zhang. 2025. Quick on the uptake: Eliciting implicit intents from human demonstrations for personalized mobile-use agents. arXiv preprint arXiv:2508.08645. Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, and Liqiang Nie. 2025a. Gui-explorer: Autonomous exploration and mining of transition-aware knowledge for gui agent. In Annual Meeting of the Association for Computational Linguistics (ACL). Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, and 1 others. 2024. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094. Yuquan Xie, Zaijing Li, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Dongmei Jiang, and Liqiang Nie. 2025b. Mirage-1: Augmenting and updating gui agent with hierarchical multimodal skills. arXiv preprint arXiv:2506.10387. Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Schütze, Volker Tresp, and Yunpu Ma. 2025. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828. Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, and Zhenyu Yan. 2025a. Contextagent: Context-aware proactive llm agents with arXiv preprint open-world sensory perceptions. arXiv:2505.14668. Qinglong Yang, Haoming Li, Haotian Zhao, Xiaokai Yan, Jingtao Ding, Fengli Xu, and Yong Li. 2025b. Fingertip 20k: benchmark for proactive and arXiv preprint personalized mobile llm agents. arXiv:2507.21071. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, and 1 others. 2025. Mobileagent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, WeiYing Ma, Jingjing Liu, Mingxuan Wang, and 1 others. 2025. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259. Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili Guan, and Liqiang Nie. 2024. Token-level correlation-guided compression for efficient multimodal document understanding. arXiv preprint arXiv:2407.14439. Renshan Zhang, Rui Shao, Gongwei Chen, Miao Zhang, Kaiwen Zhou, Weili Guan, and Liqiang Nie. 2025a. Falcon: Resolving visual redundancy and fragmentation in high-resolution multimodal large language models via visual registers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2353023540. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, and 1 others. 2025b. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176. Xurui Zhou, Gongwei Chen, Yuquan Xie, Zaijing Li, Kaiwen Zhou, Shuai Wang, Shuo Yang, Zhuotao Tian, and Rui Shao. 2025a. Hiconagent: History context-aware policy optimization for gui agents. arXiv preprint arXiv:2512.01763. Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. 2025b. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841. Yijie Zhu, Yibo Lyu, Zitong Yu, Rui Shao, Kaiyang Zhou, and Liqiang Nie. 2025. Emosym: symbiotic framework for unified emotional understanding and generation via latent reasoning. In Proceedings of the 33nd ACM International Conference on Multimedia."
        },
        {
            "title": "B Experiment Details",
            "content": "A.1 Data Statistic B.1 Baselines As shown in Figure 7, we present the overall distribution of AndroidIntent. (a) The preference subset contains 775 annotated user interactions and 7,915 GUI actions within 130 different Chinese daily apps, with entertainment-related activities accounting for roughly one-third of the data and shopping behaviors comprising 12.2%. reflecting users preference to browsing videos across multiple platforms or purchasing goods on their preferred platforms. (b) The routine subset includes 215 annotated user interactions within 60 different Chinese daily apps, among which 14.6% correspond to sign-in operations. Other proactive intent, such as alarm setting, weather checking, and workout tracking, each account for approximately 9% and exhibit strong temporal and semantic consistency. (c) The figure illustrates the distribution of interaction types per user. Moment intents dominate for most users, while each user exhibits, on average, around 10 preference interactions and 35 routine interactions. Notably, some users show no clearly identifiable routines. (d) We compare the lexical entropy of original and vague instructions using word-level metric. The results show clear reduction in entropy, indicating that vague instructions tend to be more concise and linguistically simplified, which poses greater challenges for GUI agents that should infer missing or implicit user intent from vague instructions. A.2 Data privacy Since modeling user intent relies on historical interaction data, collecting and utilizing such data In our work, may raise privacy potential risks. we build upon the Fingertip dataset and do not introduce any additional personal information. In the original dataset, all participants were clearly informed about the intended use of the collected data and provided explicit consent by signing data usage agreement. They were also instructed not to upload any private or sensitive information. We emphasize that privacy considerations should be treated as fundamental aspect when developing personalized agents. Future research should explore privacy-preserving solutions, such as ondevice deployment, federated learning, or simulated user agents, to ensure that user data remains secure while enabling effective personalization. When evaluating on AndroidIntent, we select diverse set of GUI agents, including open-sourced agents such as UI-TARS-1.5-7B (Qin et al., 2025), GUI-Owl-7B (Ye et al., 2025), and Qwen-3-VL-8B (Bai et al., 2025), alongside several closed-sourced GUI Agent models, including the GPT-5.1, GLM4.5V, and QwenVL-MAX. We evaluate their GUI execution performance when transitioning from complete to vague instructions, and also assess their ability to proactively provide suggestions. B."
        },
        {
            "title": "Implementation details",
            "content": "All GUI agent execution experiments were conducted on an NVIDIA A100 (40GB) GPU. And we select GPT-5.1 for LLM-UM. During the filtering process, we set = 10 for the top-k selection. When computing Qscore we use weighted sum of [1,0.1,0.1] and then normalization. Notably, different weight combinations can yield approximately normal-shaped distributions; we select this configuration to produce clearer decision boundaries and we also slightly expanding the filtering range for moment and preference intent to 0.6 in this setting. Additionally, both θ in Streaming Aggregation and the proactive boundary threshold in the State-based Routine Filter were both set to 0.6. For CER, we apply an exponential decay to the length of each trajectory and then perform normalized weighting of SSR. B.3 Online Evaluation In this paper, we mainly adopt an offline GUI evaluation setting. On the other hand, since users intent can often be achieved through multiple valid trajectories, online evaluation is also suitable. However, unlike simulator-based benchmarks such as AndroidWorld (Rawles et al.), evaluating on AndroidIntent requires connecting to real physical devices via Android Debug Bridge (ADB), as many daily apps cannot run on emulators due to privacy and security restrictions. Moreover, the evaluation results cannot be automatically verified by Android APIs to determine whether the user intent is successfully completed. Instead, each execution must be manually inspected to assess the agents behavior. In addition, realdevice evaluation is affected by various factors such as app versions, mobile models, and runtime environments. Since online evaluation is still not sufficiently stable or scalable under these constraints, we primarily adopt offline evaluation to ensure more objective and reproducible assessment."
        },
        {
            "title": "C More Analysis",
            "content": "As shown in Table 3, we observe that most existing GUI agents still struggle to effectively perform proactive behaviors based on user history. To further analyze, we additionally evaluate several models with stronger long-context understanding capabilities. As reported in Table 7, Gemini3-Pro also demonstrates relatively strong proactive performance under the same prompt. In contrast, models from the Qwen series (e.g., Qwen-3 and QwenMax) tend to behave overly aggressively under the same prompts, frequently triggering proactive actions and thus leading to high false-alarm rate. Future research on end-to-end or on-device personal GUI agents should not only focus on execution success rates, but also carefully consider the models ability to understand and reason over longterm context, which is crucial for analyzing user record context."
        },
        {
            "title": "D Personal Agent Settings",
            "content": "With long-term user records, we can further envision more intelligent personal agents that provide personalized assistance in everyday life, similar to Jarvis, which offers highly customized and proactive services. D.2 Proactive Triggering From user-centric perspective, proactive agents can also be designed in trigger-based paradigm. Specifically, by summarizing historical interaction records, the agent can learn set of trigger states defined by specific temporal and contextual conditions under which recommendations should be initiated. This can be formulated as: (T , t) fθ(I; {(Ii, Ti, Si)} H). (9) In this setting, the agent does not need to continuously reason over the current state at every moment; instead, it activates recommendations only when predefined trigger conditions are met. However, evaluating such framework requires an online-like environment, where User Agent (Lu et al., 2025b) should be constructed to simulate user behavior by triggering predefined states and providing accept or reject feedback. Overall, compared to real-time reasoning, this trigger-based paradigm is more efficient and represents promising direction for future work. D.3 Proactive Executing Imagine future scenario: upon arriving at the office, your phone is already unlocked and waiting at the relevant workspace interface; or when you leave the company, the navigation app automatically opens and sets the route home. This represents more forward-looking paradigm of interaction, which we refer to as Proactive Executing, which can be formulated as: D.1 Personalized Rewriting At (I; {(Ii, Ti, Si)} H). (10) Preference inference can also be implemented via query rewriting strategy. Specifically, we feed the users vague instruction together with historical records into an LLM to perform Personalized Rewriting, which can be formulated as: re fθ( ˆI; {(Ii, Ti, St)} H). (8) where re means LLM rewrite instruction. However, this approach introduces an additional inference stage and intermediate variables, thereby breaking the end-to-end nature of GUI agents. More importantly, it only enables personalization at the instruction-level. In contrast, PersonalAlign supports finer-grained personalization at the actionlevel, providing more direct control over execution behaviors. Such proactive agents constitute more advanced instantiation of HIM-Agent. This paradigm goes beyond reactive or recommendation-based behaviors by anticipating user needs and executing actions autonomously. However, realizing and evaluating such capabilities requires an online evaluation environment, along with simulated user agent capable of modeling realistic acceptance or rejection behaviors. As GUI Agents continue to advance in capability, we envision that stronger personalization, contextual awareness Proactive Executing Agents will become achievable. This represents promising direction toward truly intelligent and personalized humanagent interaction. At present, such setting remains challenging to realize in practice. Therefore, in PersonalAlign, we Table 7: Comparison of more models in proactive capability. Model GPT-5.1 Gemini3-Pro Qwen-Max Qwen3-8B Intent Alignment Semantic 49.4% 53.5% 53.6% 49.3% Judgment 32.0% 37.0% 35.4% 26.8% Precision Recall 78.6% 77.8% 93.1% 95.4% 73.1% 74.7% 68.7% 69.5% Identification Alignment False-Alarm 62.0% 57.0% 92.0% 90.0% F1-score 75.8% 76.2% 79.1% 80.3% Figure 7: Statistic of AndroidIntent. (a) and (b) show the data distributions for preference and proactive behaviors, respectively; (c) presents the distribution of user instruction types; and (d) illustrates the information volume distribution of vague instructions. restrict proactive behavior to the instruction level, where we evaluate the agents ability to predict user intent rather than executing actions autonomously. On the other hand, proactive decision-making can also be informed by richer signals; in AndroidIntent, we primarily focus on the users interaction time and scenario, leaving the incorporation of additional factors to future work."
        },
        {
            "title": "E Annotator Requirements",
            "content": "Here, we present the requirements provided to the annotators. To better support them in verifying candidate results, we also design an interactive interface that facilitates faster and more efficient annotation. The annotation workload during the verifying stage was intentionally kept lightweight, as we designed user-friendly interface to assist the annotation process and reduce cognitive burden. This design ensured that the workload remained reasonable while maintaining annotation quality. You are provided with ranked list of historical records, ordered by their relevance to the current intent. For each record, you can view the interaction time, scenario, and action. You should determine which type of user behavior the current intent belongs to, based on the user's history. We define three types of user intents: 1. Moment Intent If the current intent rarely appears or does not appear in the history, and you believe that complete and explicit description is required for the agent to execute it correctly. 2. Preference Intent If similar intent have appeared multiple times in history, and you believe the user has performed this action repeatedly, such that only minimal information is needed and the remaining details can be inferred from past interactions. After selecting this option, you are required to choose one personalized instruction from the provided vague instruction candidates that you think the user would most likely give. If none are suitable, you may also write your own possible instructions. 3. Routine Intent If the intent has appeared many times in nearly identical forms, and you believe the user has performed it frequently enough that the agent can infer the intent directly from historical patterns, especially from the time and scenario, without any additional information. Except for the preference intent, no additional instruction selection is required. After finishing the annotation for one intent, you can click `next`, and the interface will automatically update the intent and user history. You are skilled at analyzing user history. You are given summarized user's daily routine. Your task is to determine whether the current user state requires proactive suggestions. ## Note - You are given summarized user routines, each describing frequent user behavior with its intent, time/scenario distribution, and frequency. - If neither the time nor the scenario shows sufficiently strong match with any summarized routine, output False. - If you decide that recommendation is needed, output suitable user instruction for the GUI agent to execute. - Express the user's intent unvaguely in one Chinese sentence, ensuring the expression faithfully reflects the user's original intent without adding any extra description. - Do not output any explanation, including time and scenario information. Imitate the user's intent and output only one proactive suggestion. ## Input Time: {time} Scenario: {scenario} Summarized_routine: {generalized_routine} The user's intent: If you are uncertain about case, you may also mark it as uncertain for further review."
        },
        {
            "title": "F Prompt for Agents",
            "content": "F.1 Prompt for LLM-UM Prompt for the preference interaction summary: You will serve as an assistant to help me summarize user's preferences based on his/her long-term GUI usage records. will provide you with the user's brief profile amd long-term interaction history. From this history, you need to extract the user's preferences regarding apps or items. ## Requirements: 1. Please provide your output in JSON format, following this structure: { \"summaries\": [ { \"reasoning\": \"Briefly explain your reasoning for the preference\", \"preference\": \"A concise summary of what types of apps/items this user is likely to enjoy, e.g. 'User preference for shopping with App A'\", \"confidence\": \"High/Medium/Low\", \"action\": \"Summarize one User's usual execution actions trajectory from ` action_list`, e.g. ['click(x1, y1)', 'wait() ', 'finished()']\" (if you are unable to determine the preference, please set this value to \"None\") }, ] } 2. Ensure that each \"preference\" is highly concise. It should clearly describe one app + one specific type of task/content the user prefers. 3. You must provide multiple preferencereasoning pairs with JSON format in \"summaries\". Each preference should reference only one app 4. The answer should be in Chinese. However, \" action\" field must keep the original English types from the `action_list`. 5. Do not provide any text outside of the JSON string. 6. Additional requirement: - If several apps appear interchangeable but one is used significantly more often for specific task, treat that as preference. - Avoid fabricating preferences when evidence is weak; use 'None' when uncertain. ## User Profile: {profile} ## User History: {previous_intents} F.2 Prompt for Proactive"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen"
    ]
}