{
    "paper_title": "DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition",
    "authors": [
        "Kailash A. Hambarde",
        "Nzakiese Mbongo",
        "Pavan Kumar MP",
        "Satish Mekewad",
        "Carolina Fernandes",
        "Gökhan Silahtaroğlu",
        "Alice Nithya",
        "Pawan Wasnik",
        "MD. Rashidunnabi",
        "Pranita Samale",
        "Hugo Proença"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/"
        },
        {
            "title": "Start",
            "content": "SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 1 DetReIDX: Stress-Test Dataset for Real-World UAV-Based Person"
        },
        {
            "title": "Recognition",
            "content": "Kailash A. Hambarde, Nzakiese Mbongo, Pavan Kumar MP, Satish Mekewad, Carolina Fernandes, Gokhan Silahtaroglu, Alice Nithya, Pawan Wasnik, MD. Rashidunnabi, Pranita Samale, Hugo Proenca Senior Member, IEEE 5 2 0 2 7 ] . [ 1 3 9 7 4 0 . 5 0 5 2 : r AbstractPerson reidentification (ReID) technology has been considered to perform relatively well under controlled, groundlevel conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, large-scale aerial-ground person dataset, that was explicitly designed as stress test to ReID under real-world conditions. DetReIDX is multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/. Index TermsPerson Re-Identification, UAV Surveillance, Cross-View Recognition, Aerial-Ground Dataset, Soft Biometrics. I. Introduction identification, ERSON centric visual understanding including detection, tracking, and re-identification (ReID) is foundational to wide range of critical applications such as surveillance, public safety, autonomous UAV patrolling, and the desearch-and-rescue operations [19][21][?]. However, ployment of such systems in unconstrained aerial-ground environments remains extremely limited. The core bottleneck is not Manuscript received February XX, 2025; revised XX XX, 2025. This work was supported. Kailash A. Hambarde, Nzakiese Mbongo, Carolina Fernandes, MD. Rashidunnabi, Pranita Samale, and Hugo Proenca are with the Instituto de Telecomunicac oes and the University of Beira Interior, Covilha, Portugal (corresponding author e-mail: kailas.srt@gmail.com). Pavan Kumar MP is with J.N.N. College of Engineering, Shivamogga, Karnataka, India. Satish Mekewad and Pawan Wasnik are with the School of Computational Sciences, SRTM University, Nanded, India. Gokhan Silahtaroglu is with Istanbul Medipol University, Istanbul, Turkey. Alice Nithya is with SRM Institute of Science and Technology, Kattankulathur, India. Fig. 1. Comparison between the most important features of the publicly available datasets (ground-ground, aerial-aerial, and aerial-ground) and the DetReIDX dataset. Unlike its counterparts, DetReIDX includes clothing variations within subjects, with detection and tracking annotations, action labels, at wide altitude ranges (5.8m120m). model capacity but rather the lack of datasets that reflect the true operational complexity of drone-based surveillance: low resolution, cross-viewpoint domain gaps, long-range degradation, and appearance shifts due to clothing or occlusion. Despite impressive progress in ground-level person ReID using datasets like Market-1501 [1], CUHK03 [2], MARS [3], DukeMTMC-ReID [4], and LTCC [5], these benchmarks are largely constrained to fixed-camera, close-range, lateral-view scenarios. While they have catalyzed algorithmic advances, they fail to capture the severe viewpoint and scale variations encountered in aerial settings. On the other hand, aerial-only datasets such as PDESTRE [6], UAV-Human [7], PRID-2011 [8], MRP [9], PRAI-1581 [10], Mini-drone [11], AVI [12], and DRoneHIT [13] offer aerial captures but are limited to relatively low altitudes (<10m), lack multi-session diversity, or exclude ground-view perspectives, thus limiting their value for crossview understanding and realistic tracking tasks. Bridging the aerial-ground domain remains vastly underexplored. Notable attempts include AG-ReID.v2 [14], G2APS [15], CSM [16], and iQIYI-VID [17], which introduce hybrid viewpoints. Yet, these datasets suffer from narrow altitude ranges (typically <45m), limited clothing variation, and lack fine-grained annotations necessary for robust multi-task learning. The gap: Existing datasets either (i) operate in narrow altitude domains, (ii) fail to support cross-view matching, (iii) lack 00000000/00$00.00 2021 IEEE SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 2 TABLE Comparison between DetReIDX and the publicly available datasets for person detection, ReID, tracking, and action recognition. (: Available, : Not available, : No information available.) Category Dataset Camera Format o - o a A - r d r - r CUHK03 [2] iLIDS-VID [23] Market-1501 [1] MARS [3] DukeMTMC-ReID [4] LTCC [5] PRID-2011 [8] MRP [9] PRAI-1581 [10] Mini-drone [11] AVI [12] DRone-HIT [13] P-DESTRE [6] UAV-Human [7] CSM [16] iQIYI-VID [17] AG-ReID.v2 [14] G2APS-ReID [7] DetReIDX (Ours) CCTV CCTV CCTV CCTV CCTV CCTV UAV UAV UAV UAV UAV UAV UAV UAV Various Various UAV+CCTV UAV+CCTV DSLR+UAV Still Video Still Video Video Still Still Video Still Video Still Still Video Still Video Video Still Still Video+Still Detection Tracking Task ReID Search Action Rec. #Identities #BBox Height (m) Distance (m) 1467 300 1501 1261 1812 152 1581 28 1581 5124 101 269 1144 1218 5000 1615 2788 509 13K 42K 32.6K 20K 815K 17K 40K 4K 39K >27K 10K 40K >14.8M 41K 11M 600K 100.6K 200.8K 12.6M <10 2060 <10 2060 <10 28 5.86.7 28 1545 2060 5120 10 annotation density and appearance variation to evaluate longterm recognition, or (iv) omit long-term identity retention under clothing changes across sessions. Most benchmarks assume fixed attire and short-term reappearance, which breaks down in real-world scenarios where individuals are observed days apart in different clothing. This makes current benchmarks fundamentally unsuitable for training or stress-testing models intended for UAV-based deployments. To address this, we propose DetReIDX, large-scale, aerialground person dataset specifically designed to evaluate model robustness under real-world constraints. DetReIDX includes: 13M+ bounding boxes from 509 subjects, recorded in 7 universities of 3 different continents (Portugal, Turkey, India and Angola). Data spanning 5.8m to 120m altitude and 10m to 120m distance, across 18 unique UAV viewpoints. Aerial, and ground views captured in two distinct sessions, to support clothing variation and temporal drift. Manual annotations of 16 soft biometric attributes [6] (e.g., age, gender, height, hair style, upper/lower clothing, accessories). Multi-task labels for detection, ReID, action recognition, tracking, and cross-domain matching. Why DetReIDX matters: Figure 1 and Table show that DetReIDX dramatically exceeds previous datasets in altitude range, viewpoint coverage, identity diversity and annotation richness. In our experiments, SOTA detection models such as YOLOv8 [18], DDOD [19], and Grid-RCNN [20] degrade by up to 80% when transferred to long-range (D3) scenes. Similarly, leading ReID methods including PersonViT [21], SeCap [15], and CLIP-ReID [22] collapse when subject to aerial-ground viewpoint shifts and appearance changes. Crucially, DetReIDX is the first to explicitly incorporate longterm identity variation via clothing changes across sessions, revealing how heavily current ReID models rely on superficial appearance cues rather than learning semantically grounded or structural identity features. This makes DetReIDX not only Fig. 2. Examples of soft biometric annotations for two individuals in the DetReIDX dataset. Each subject is labeled with 16 visual and demographic attributes, facilitating fine-grained person analysis across multiple scenes. harder, but closer to operational reality and indispensable for progress. Contributions: We announce and describe the DetReIDX set, the most comprehensive person-centric dataset designed for UAVground multi-task benchmarking under real-world conditions. We provide empirical evidence about SOTA models failure to generalize under realistic and very challenging realwordl settings. We provide rigorous set of benchmarks for detection and ReID tasks, highlighting the current imitations and pointing to new research directions for robust cross-view ReID. The remainder of this paper is organized as follows: Section II gives an overview of the related sets and the limitations of the existing benchmarks. Section III details the data collection and annotation procedures. Section IV presents task-specific experiments and results. Finally, Section concludes the paper. II. Related Work Person recognition from visual data has been receiving growing attention by the reserch community. However, most SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 3 TABLE II Comparison between the available person annotations in the existing datasets. ( stand for attribute available and indicate unavailability). Attribute MarketDukeMTMC CUHK03 iLIDS-VID P-DESTRE UAV-Human PRID-2011 MRP PRAI-1581 Mini-Drone AVI AG-ReID.v1 AG-ReID.v2 G2APS iQIYI-VID DetReIDX (Our) Ground-Ground Aerial-Aerial Aerial-Ground Gender Age Height Body Volume Ethnicity Hair Color Hairstyle Beard Moustache Glasses Head Accessories Upper Body Clothing Lower Body Clothing Feet Accessories Action of the existing datasets and benchmarks fall into three isolated silos ground-ground, aerial-aerial, or aerial-ground each with critical limitations when viewed through the lens of UAV-based long-range surveillance. A. Ground-Ground Datasets Ground-level ReID datasets such as Market-1501 [1], CUHK03 [2], MARS [3], DukeMTMC-ReID [4], and LTCC [5] have become standard testbeds for model development. These datasets enable benchmarking across appearance changes, occlusion, and temporal variations. However, all are collected from static ground cameras with minimal viewpoint variation and no aerial data. Crucially, subjects are captured at close range with full-body visibility conditions that are fundamentally different from long-range aerial footage. As result, models trained on these datasets fail to generalize to UAV deployment scenarios. B. Aerial-Aerial Datasets Datasets like PRID-2011 [8], PRAI-1581 [10], MRP [9], Mini-drone [11], and P-DESTRE [6] shift focus to aerialonly captures. While they introduce novel challenges such as low resolution and top-down views, they suffer from two key limitations: 1) extremely low altitude ranges (typically under 10m), which do not reflect true UAV flight conditions; and 2) the absence of any ground perspective, making them unsuitable for cross-view ReID or domain-bridging tasks. Even advanced datasets like UAV-Human [7] and AVI [12] lack consistent identity tracking across multiple angles and distances. C. Aerial-Ground Datasets handful of datasets attempt to bridge the domain gap between UAV and CCTV cameras most notably AG-ReID.v2 [14], G2APS [15], CSM [16], and iQIYI-VID [17]. These efforts mark important progress but are fundamentally limited in scope: Their altitude range is narrow (typically 1545 m), excluding highaltitude drone perspectives. Clothing variation across sessions is minimal or absent, reducing the challenge of long-term ReID. Annotations are limited to ReID detection, tracking, action recognition, and soft biometrics are often missing. Crosssession and cross-location diversity is limited, reducing realworld generalization. D. Where DetReIDX Fits Unlike all prior datasets, DetReIDX is designed to address the realities of long-range, cross-domain person understanding: Altitude and Distance Diversity: Captures span from 5.8 to 120 in altitude, and 10 to 120 in lateral range far beyond any existing benchmark. Aerial-Ground Pairing: Each subject is recorded in controlled indoor conditions (ground views) and from 18 aerial viewpoints, enabling rich cross-domain matching. Session-Wise Clothing Variation: Subjects are recorded across multiple days with different outfits. This explicitly simulates long-term ReID, where appearance changes due to clothing occlude textureand color-based identity cues. Unlike AG-ReID and G2APS, DetReIDX exposes how fragile modern ReID systems are when color, clothing, or silhouette cannot be relied on. Comprehensive Multi-Task Annotation: In addition to ReID labels, DetReIDX provides bounding boxes, tracking IDs, action labels, and 16 soft biometric attributes supporting detection, identification, and fine-grained analysis under extreme scale and occlusion conditions. Key distinction: Where prior datasets isolate either viewpoint, task, or domain, DetReIDX unifies them. It offers systematic breakdown of how model performance degrades under scale shift, viewpoint change, occlusion, and appearance drift setting new benchmark for aerial-to-ground person understanding under real-world constraints. III. The DetReIDX Dataset DetReIDX is comprehensive dataset for long-range, crossview person understanding. It enables detection, tracking, identification, ReID, and soft-biometric prediction across aerial and ground views. DetReIDX is built from the ground up to reflect real-world constraints faced by UAV surveillance: multiview occlusion, top-down distortion, extreme resolution loss, appearance shifts, and domain gaps between aerial and ground captures. The dataset includes over 13 million bounding boxes from 509 identities, with consistent ID annotation across two capture sessions and three continents. All participants are annotated with 16 soft biometric attributes and captured using structured, hierarchical drone protocol to support controlled evaluation under varied pitch, altitude, and distance. SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 4 Fig. 3. Satellite view of the data collection sites across the university campuses in Turkey, Angola, and India. The star markers indicate indoor dataset collection, and the green cones represent drone flight zones. Fig. 4. Overview of the indoor data collection setup: (left) mugshots taken from three angles (left, front, right); (right) gait video. A. Collection Sites and Demographic Diversity DetReIDX was collected in seven universities from India, Portugal, Turkey, and Angola, as shown in Figure 3. The selection of geographically and culturally distinct campuses ensures diversity in subject appearance, environment, clothing, and lightingenabling broader generalization. In total, the dataset includes 509 subjects, each with indoor and outdoor recordings. Participants span across wide range of height, weight, ethnicity, and other appearance attributes (see Figure 2). B. Two-Phase Collection Protocol"
        },
        {
            "title": "DetReIDX captures each identity through two complementary",
            "content": "modalities: 1) Indoor Capture (Ground Reference). As illustrated in Fig. 4, each subject enrolled in this dataset undergoes i) mugshot capture, with left profile, frontal, and right profile images; and ii) gait video 20-second walking sequence with turning and posture variation. Devices used at this point include DSLR and various smartphones, listed in Table III. 2) Outdoor UAV Capture. Each subject is recorded outdoors under two sessions (S1, S2), wearing different outfits, with 18 UAV viewpoints per session. Each session captures the full range of pitch angles, altitudes, and lateral distances to introduce scale and viewpoint variance. As shown in Fig. 5. UAV-based outdoor capture protocol. Each subject is recorded from 18 drone viewpoints (P1P18), spanning wide range of altitudes, distances, and pitch angles. Recordings are repeated across two sessions (S1, S2) with varied clothing for appearance diversity. TABLE III Specifications of the devices used for indoor and outdoor data collection phases. iPhoneUniversity Device Brand Model UBI Mobile Apple SRT Mobile Redmi K50i SRM Mobile OnePlus Nord CE-3 JNNCE DSLR Canon Eos1200D MEDIPOL Mobile Apple iPhone-11 iPhone-14 UniLuanda Mobile Apple NMDCH Mobile OnePlus Nord CE-2 UBI UAV SRTMUN UAV SRM UAV UAV JNNCE MEDIPOL UAV UniLuanda UAV NMDCH UAV Phantom-4-Pro Mini Nano Mavic-3 Mavic-3 S155 Phantom-4-Pro Air-S2-Fly DJI IZI DJI DJI Piha DJI DJI Resolution 2556 1179 2460 1080 1900 1400 5184 3456 1792 1100 2556 1179 1900 1400 3480 2160 5120 3840 4096 2160 5280 3956 2560 1400 3480 2160 2688 1512 FPS 30 30 30 30 30 30 30 30 30 30 30 30 30 30 d r t Figure 5 and detailed in Table IV, the drone captures include three pitch angles (30, 60, 90) and six distancealtitude pairs per angle (5.8m to 120m height and 10m to 120m horizontal distance). Subjects walk in unconstrained trajectories to simulate realworld variability. Figure 6 shows representative samples from all 18 viewpoints. Each video is 20+ seconds, ensuring motion, occlusion, and scale progression. C. Drone Layout and Session Design Each UAV flight was recorded with pitch/altitude/distance labels to support reproducible benchmark protocols. All 18 viewpoints were kept consistent across S1 and S2. This dualsession protocol aims at guaranteeing changes in appearance, particularly to guarantee that subjects wear different outfits (see Figure 8), and enable long-term ReID and clothing-insensitive SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 5 Fig. 6. Actual drone-captured frames from all 18 UAV viewpoints (P1P18), grouped by pitch angle: 30, 60, and 90. Each image illustrates real-world scale variation, subject visibility, and background context. Yellow insets highlight degradation in resolution at extreme long-range positions (e.g., P6, P12, P18). TABLE IV UAV capture positions and configurations. Pitch angles are defined in Session 1 and remain fixed in Session 2. Each point corresponds to unique UAV viewpoint used in both sessions. S1 Dist. (m) Height (m) S2 Dist. (m) Height (m) Point Pitch () P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 P16 P17 P18 30 60 90 10 20 30 40 80 120 10 20 30 40 80 120 0 0 0 0 0 5.8 11.5 17.3 23.1 40.0 60.0 15.0 30.0 45.0 60.0 75.0 90.0 10.0 20.0 30.0 40.0 80.0 120.0 10 20 30 40 80 120 10 20 30 40 80 120 0 0 0 0 0 0 5.8 11.5 17.3 23.1 40.0 60.0 15.0 30.0 45.0 60.0 75.0 90.0 10.0 20.0 30.0 40.0 80.0 120.0 search. Also, S1 and S2 were separated by at least 24 hours to ensure environmental changes (daylight, shadows, weather conditions), yielding total of 36 drone videos per identity, divided into: i) Same-view, same-day; ii) Cross-view, same-day; and iii) cross-view and cross-day, under clothing variations. D. Annotation Pipeline All annotations were manually done by set of volunteers, using the CVAT tool and cross-verified by peers. In total, there are 4 different kinds of annotations: 1) Bounding boxes. Define each subject region-of-interest (ROI) and are annotated at fixed 10-frame intervals across all video types. 2) Tracking IDs. Each subject is assigned consistent PID across indoor and UAV sessions. 3) Session metadata. Altitude, pitch, distance and scene location. Fig. 7. Distributions of the soft biometric labels in DetReIDX. The top row corresponds to the demographic distributions: the dataset is moderately maledominated (58% male), predominantly composed of individuals aged 1824 (89%), and has high proportion of subjects in the [160, 170cm] height interval and 60kg weight ranges. Ethnic composition is skewed towards Indian (68%) and Black (25%) categories. The remaining rows provide different visual attributes annotated per person, including hair color, style, presence of facial hair, glasses, clothing, and accessories. Most individuals have black hair (98%), short hairstyles (59%), and wear normal glasses (91%). Clothing is casual with jeans (66%) and shirts/t-shirts being common, while accessories like bags are rare (3%). 4) Soft biometric information. 16 manual labels covering demographic, appearance, and visual cues. See Figure 2 and attribute frequency in Figure 7. Attribute completeness is benchmarked in Table II, confirming that DetReIDX offers the most detailed subject-level annotation among the aerial or cross-view related datasets. SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 6 Fig. 8. Example of one subject captured in 18 viewpoints (P1P18), with clothing changes between sessions. Top row: Session 1. Bottom row: Session 2, with different attire. Fig. 9. Scatter plots of ROIs height/width in three different distance bins. The bottom-right plot provides the distribution of the ROI heights (in pixels) of the indoor and outdoor data. E. Viewpoint and Resolution Diversity As shown in Figure 9, pedestrian scale varies drastically across UAV positions. Indoor captures often exceed 1000px bounding box height, while aerial views in P18 (90, 120m) provide ROIs smaller than 10px tall, approaching scale-invariant detection limits. Figure 11 and Figure 12 illustrate how UAV angle and altitude lead to occlusion, distortion, and viewpoint-specific degradation. DetReIDX captures this with pixel-level granularity, enabling fine-grained robustness evaluation. TABLE DetReIDX Outdoor Dataset Statistics Split Train Validation Test Total #Videos 120 56 109 #Images 131,580 63,591 108,252 303,423 #Annotations 5,095,539 2,483,836 4,217,824 11,797,199 Formats YOLO, COCO YOLO, COCO YOLO, COCO Fig. 10. Effect of distance on pedestrian detection accuracy. The black curve provides the mean Intersection-over-Union (IoU) of correctly matched detections, with shaded areas representing 1 standard deviation. The orange curve shows the proportion of missed ground truth (GT) annotations. critical distance (70 meters) is highlighted where performance began to significantly deteriorate. The top inset visualizations illustrate example detections at close (green box: predictions; red box: ground truth) and long distances, corresponding to low and high GT miss rates, respectively. The bar plot above the graph indicates the number of annotations per distance bin, confirming data balance across ranges. These results provide evidence of substantial degradation in both detection precision and recall at long distances. TABLE VI Statistics of the DetReIDX ReID data splits, for the Aerial Aerial, Aerial Ground and Ground Ground settings. Split / Test Case #Query #Gallery Total Images Train (Indoor + Outdoor) Aerial Aerial Aerial Ground Ground Aerial 52,926 106,927 7,959 52,552 7,959 106, 289,392 105,478 114,886 114,886 F. Data Splits and Formats DetReIDX annotations are released in YOLO and COCO formats. ReID queries and galleries are organized for aerialto-aerial (AA), aerial-to-ground (AG), and ground-toaerial (GA) matching settings (Table VIII). Detection splits (Table V) follow sceneand viewpoint-aware partitioning, with no video overlap between train and test. SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE (a) Low resolution (b) Clothing variation (c) Long-range (d) Occlusion (e) Top-down view (f) Pose variation Fig. 11. Qualitative analysis of pedestrian detection under varying viewpoints and distances. Rows represent different UAV pitch angles (30, 60, and 90), while columns compare detections at close (left) and long ranges (right). Predicted bounding boxes from the detection model are shown in green, and ground-truth annotations are in red. As both the angle and distance increase, detection becomes more challenging due to reduced resolution, occlusion, and distortion. G. DetReIDX Uniqueness As stated above, DetReIDX was designed to fill the most important key blind spots in current pedestrian recognition research, enabling: a) cross-domain ReID, by matching UAV views to high-resolution indoor references (AG); b) clothinginvariant search, with clothing changes within-subject between the different sessions; c) long-range detection, with UAV-tosubject distances up to 120m (Figure 10); and d) extreme lowresolution and severe occlusions, with pedestrian ROIs as small as 88 pixels (Figure 12a). Table presents side-by-side breakdown of DetReIDX versus the leading ground-ground (e.g., Market-1501 [1], Duke [4]), aerial-aerial (e.g., UAV-Human [7], P-DESTRE [6]), and aerialground (e.g., AG-ReID.v2 [14], G2APS [15]) datasets. H. Ethical Considerations All participants gave their informed consent in writing. Data was anonymized where necessary. DetReIDX include facial detail and is released under non-commercial research license for academic use. UAV flights were approved by institutional review boards and followed any existing local regulations. IV. Experiments and Results As primary benchmark of the dataset, we conducted extensive experiments to assess performance of state-of-theart (SOTA) models in pedestrian detection and re-identification (ReID) tasks. Each evaluation setting was designed to evaluate model robustness across realistic surveillance variables: (g) Motion blur (h) View perspective Fig. 12. Challenging conditions in person identification from UAV footage: (a) low resolution, (b) clothing variation, (c) long-range observations, (d) occlusion, (e) top-down viewpoints, (f) pose variation, and (g) motion blur. altitude, angle, range, resolution, and cross-domain identity transfer. A. Pedestrian Detection Being at the basis of the ReID pipeline, pedestrian detection actual sustains the whole process, as any failures will compromise any subsequent phase. Also, as it is typically the earliest processing phase, it is the one that first should handle the dynamics of the environments. For this case, only he outdoor subset of DetReIDX was considered challenging enough, including 285 UAV video sequences. We used 7020-10 split for training, validation, and testing, with absolutely no overlap across splits. As baselines, we selected three pedestrian detectors that we consider to represent the SOTA: i) YOLOv8 [18]: an anchorfree one-stage detector with decoupled heads; ii) DDOD [19], disentangled dense object detector addressing label assignment and scale bias; and iii) Grid-RCNN [20]: region-based detector using pixel-level grid point prediction. Each model was trained from scratch on the DetReIDX training set, and evaluated using the AP@50 (IoU) performance metric. Two main factors were identified as the most obvious covariates for human detection performance: viewpoint (perspective) and distance (scale). Then, being particularly important to understand the generalization capabilities of the different methods, our experiments mainly assume the interpolation and extrapolation, depending whether the test viewpoints/distances are (arent) enclosed in the corresponding learning intervals. At first, as baseline performance, all pitch angles (30, 60, 90) and distances were used for training and test purposes. Then, to perceive the viewpoint generalization performance, SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 8 TABLE VII AP50 of YOLOv8, DDOD, and Grid-RCNN on the DetReIDX dataset across aerial viewpoint and distance range shifts. Scores are reported as absolute AP50 followed by percentage change from the baseline (: gain, : drop). Experiment Train Set Test Set YOLOv8 DDOD Grid-RCNN Baseline (All Conditions) Interpolation Extrapolation (D1 D1) (D1 D2) (D1 D3) (D2 D1) (D2 D2) (D2 D3) (D3 D1) (D3 D2) (D3 D3) ALL 30, 90 30, 60 D1 D2 ALL 60 90 D1 D2 D3 D1 D2 D3 D1 D2 D3 0.734 0.669 (8.90%) 0.503 (31.5%) 0.914 (24.5%) 0.793 (8.00%) 0.137 (81.3%) 0.694 (5.50%) 0.890 (21.2%) 0.315 (57.1%) 0.015 (97.9%) 0.411 (44.0%) 0.581 (20.8%) 0.608 0.564 (7.20%) 0.474 (22.0%) 0.857 (40.9%) 0.380 (37.5%) 0.008 (98.7%) 0.582 (4.30%) 0.776 (27.6%) 0.111 (81.8%) 0.004 (99.3%) 0.274 (54.9%) 0.408 (32.9%) 0.620 0.514 (17.1%) 0.403 (35.0%) 0.839 (35.3%) 0.428 (30.9%) 0.009 (98.5%) 0.668 (7.70%) 0.770 (24.2%) 0.150 (75.8%) 0.002 (99.7%) 0.261 (57.9%) 0.280 (54.8%) two modes were tested: i) Interpolation (30, 90 60), with models trained on extreme angles and tested on the mid-views; and the more challenging ii) Extrapolation (30, 60 90):, where tests are done on unseen extreme views. Regarding distance generalization, we quantized the acquisition distances into three bins: D1: <20m (short-range); D2: 2050m (midrange); and D3: >50m (long-range). Next, in similar way to viewpoint, these splits were used to train/test across distance bins and evaluate the robustness of SOTA models across scale. Table VII summarises the observed AP@50 values. As key observations, we highlight several notable cases: a) long-range collapse (D1D3): YOLOv8 drops from 91.4% (D1D1) to 13.7% (D1D3), and DDOD/GR-CNN degrade by 90%+. Detection fails entirely at 50m due to sub-10 pixel targets; b) Viewpoint Failure (Extrapolation): All models perform significantly worse on unseen 90 top-down views, highlighting angular overfitting; and c) Reverse Transfer Limits: D3D1 performance is near zero, indicating that models trained only on long-range views are not able to learn transferable pedestrian features. Figures 11 and 10 illustrate how performance deteriorates with increasing pitch and distance due to object scale collapse, blur, and top-down foreshortening. B. Pedestrian Re-Identification The DetReIDX benchmark introduces high-fidelity ReID testbed simulating real-world aerial-ground surveillance, where most conventional ReID assumptions break down. It contains 509 unique identities recorded indoors, of which 334 (65.6%) are re-observed in outdoor UAV scenes. Each subject appears in at least two recording sessions with different clothing and variable lighting, enabling cross-session, cross-domain ReID evaluation. 70%-30% PID-disjoint train-test split is used, assigning 267 identities (289,392 images) to training and 67 identities (114,886 images) to testing. Each test identity is captured across 36 UAV video sequences (two sessions 18 aerial viewpoints) and one controlled indoor gait video, enabling high-variance retrieval under extreme appearance, angle, and resolution variation. We define three canonical test scenarios: TABLE VIII DetReIDX ReID split statistics. Scenario #Query #Gallery Total Images Train (Indoor + UAV) A2A (UAVUAV) A2G (UAVIndoor) G2A (IndoorUAV) 52,926 106,927 7,959 52,552 7,959 106,927 289,392 105,478 114,886 114, AerialAerial (A2A): Queries are UAV sequences from Session 1; gallery samples from Session 2. This isolates cross-session variation within the aerial domain. AerialGround (A2G): UAV-based queries are matched against high-quality indoor references. This tests crossdomain generalization from in-the-wild to controlled settings. GroundAerial Indoor queries are matched against UAV galleries. This tests downward domain transfer. (G2A): The statistics of each scenario are listed in Table VI, and all of them were evaluated using the same metrics: Rank-1, Rank-5, Rank-10, and mean Average Precision (mAP). Again, as baselines, we selected three recent ReID methods considered to represent the SOTA: a) PersonViT [21]: transformer-based model trained on large-scale ReID datasets using global attention across spatial features; b) SeCap [15], an aerial-aware model using spatially enhanced capsule networks to align features across drone-ground domains; and c) CLIPReID [22]: vision-language pretrained CLIP model, adapted here for image-only ReID using prompt-based fine-tuning. As shown in Table IX, all models perform poorly across DetReIDX test conditions. Despite the relatively good performance on the existing ground-level datasets, no model was observed to generalize to DetReIDXs real-world constraints. 1) Qualitative Analysis: Figure 13 provides some remarkable examples, that were considered to represent the typical failure/success cases. In general, successful retrievals (left) tend to occur under the following conditions: consistent clothing, relatively low altitudes, and low variable silhouette profiles. On the other way, the right side of the figure illustrates the typical failure cases, mostly due to severe occlusions, low resolution, SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 9 Fig. 13. Qualitative evaluation of Person-ViT ReID model on DetReIDX dataset. The left panel (green) illustrates successful retrieval cases where UAV-based query images (Q) yield correct matches among top-5 retrieved identities (Rank-1 to Rank-5). The right panel (red) shows failure cases highlighting typical conditions challenging ReID performance, including severe aerial-to-ground (AG), aerial-to-aerial (AA), and ground-to-aerial (GA) viewpoint changes, extreme long-range resolution loss, significant appearance variations due to clothing changes across recording sessions, and environmental factors such as motion blur and occlusion. These results underline the limitations of current state-of-the-art models in real-world UAV surveillance scenarios, as explicitly addressed by the DetReIDX dataset. TABLE IX Overall ReID performance observed on the DetReIDX dataset. TABLE ReID performance by UAV distance (D1D3). Model Scenario mAP (%) R1 (%) R5 (%) R10 (%) Scenario Distance mAP (%) PersonViT SeCap CLIP-ReID A2A A2G G2A A2A A2G G2A A2A A2G G2A 9.9 22.3 23.3 11.2 20.5 21.2 9.5 22.0 20.8 8.8 19.6 51.9 8.2 18.1 50.9 8.9 19.7 58.1 14.4 24.8 59.4 13.0 21.5 57.7 12.8 24.0 63.1 17.6 27.6 63.0 16.2 23.4 60.7 15.3 26.2 65.2 A2A A2G G2A D1 D2 D3 D1 D2 D3 D1 D2 D3 11.7 10.7 8.9 31.2 25.9 17.3 34.5 28.5 15.3 R1 (%) 12.7 10.2 6.9 28.9 22.9 14.7 52.5 51.0 45.1 R5 (%) 20.0 16.3 11.7 34.6 28.5 19.4 58.0 58.8 56. R10 (%) 23.5 19.5 14.8 37.4 31.5 22.3 62.1 62.3 61.2 extreme pitch, and clothing changes. forthcoming generation of models should keep as priorities: 2) Impact of UAV Altitude on Retrieval: To isolate aerial viewpoint effects, we quantized the queries by drone distance (D1: low, D2: medium, D3: high altitude). Table and Figure 14 reveal consistent performance collapse with altitude across all tasks. For instance, in A2G, mAP drops from 31.2% (D1) to 17.3% (D3). 3) Failure cases and Futher Research: According to our experiments, DetReIDX exposes critical blind spots in the existing SOTA Re-ID models. In particular, we emphasize: a) the viewpoint dependency: Overhead UAV angles eliminate body and gait structure; b) clothing reliance: Appearance drift invalidates coloror texture-based cues; c) resolution limits: Long-range views reduce pedestrians to 20px silhouettes; and d) domain disjointness: with indoor and UAV domains yielding notorious feature mismatch. This way, to improve the results in the DetReIDX, any Learn viewpoint-agnostic representations robust to pitch and elevation. The subjects appearance varies dramatically with respect to pitch angles, in particular. It is up to the models to identify and register specific correspondences between data acquired from different perspectives. Achieve resolution invariance. The current generation of methods tends to rely on minutiae information to obtain appropriate feature representations. However, for very small resolutions (e.g., 15px targets) such kind of information isnt discernible. Focus on soft biometrics or geometry-aware features over appearance-based information, which is much sensitive to daylight and perspective. Obtain cross-domain registration between UAV and controlled views data, which is particularly important to match data acquired from very different sensors, or even different SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE Fig. 14. Cumulative Match Characteristic (CMC) curves showing the impact of aerial distances on ReID performance using the Person-ViT model, evaluated across three domain transfer scenarios provided by the DetReIDX dataset: A2A, A2G, and G2A. Each scenario compares retrieval performance at different aerial distance intervals: close-range (D1: <20m), mid-range (D2: 2050m), and long-range (D3: >50m) against an all-distance baseline. Results highlight significant degradation in ReID accuracy with increasing aerial distance due to factors such as severe resolution loss, viewpoint distortion, and reduced discriminative appearance features. Mean Average Precision (mAP) scores provided in the legends quantify performance drops, emphasizing long-range recognition challenges specifically targeted by DetReIDX. light spectra."
        },
        {
            "title": "References",
            "content": "V. Conclusions Due to safety/security concern in modern societies, person ReID from surveillance footage has been establishing as technology of particular interest. However, we observed that SOTA methods catastrophically fail when facing actual realworld conditions, such as extreme pitch angles, long-range scale distortions, appearance drifts, and tiny resolution. This observation was the primary motivation for the development of the DetReIDX dataset, which purposely integrates such variability factors by design. Spanning 5.8120m altitudes, 18 aerial viewpoints, two-session clothing variation, and 13M+ annotations across detection, tracking, ReID, and action recognition, DetReIDX is the first dataset to comprehensively reflect the constraints of long-range UAV-based pedestrian ReID. Our benchmarks show that state-of-the-art detectors and ReID models degrade their performance up to 81% when tested on the DetReIDX set. Also, models still face particular difficulties in case within-subject cloth changes, which is fundamental requirement for long-term ReID. Hence, DetReIDX should not be regarded as simple convenience benchmark, but - instead - as stress test and foundation tool. It shall set new standard for evaluating the robustness of models and challenge to support the development of real-world models."
        },
        {
            "title": "Acknowledgment",
            "content": "Kailash A. Hambarde acknowledges that this work was carried out within the scope of the project Laboratorio Associado, reference CEECINSTLA/00034/2022, funded by FCT Fundac ao para Ciˆencia Tecnologia, under the Scientific Employment Stimulus Program. The author also thanks the Instituto de Telecomunicac oes for hosting the research and supporting its execution. Hugo Proenca acknowledges funding from FCT/MEC through national funds and co-funded by the FEDERPT2020 partnership agreement under the projects UIDB/50008/2020 and POCI-01-0247-FEDER-033395. [1] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, Scalable person re-identification: benchmark, in Proceedings of the IEEE international conference on computer vision, 2015, pp. 11161124. [2] W. Li, R. Zhao, T. Xiao, and X. Wang, Deepreid: Deep filter pairing neural network for person re-identification, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 152 159. [3] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian, Mars: video benchmark for large-scale person re-identification, in Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14. Springer, 2016, pp. 868884. [4] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, and Y. Yang, Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 51775186. [5] X. Qian, W. Wang, L. Zhang, F. Zhu, Y. Fu, T. Xiang, Y.-G. Jiang, and X. Xue, Long-term cloth-changing person re-identification, in Proceedings of the Asian conference on computer vision, 2020. [6] S. A. Kumar, E. Yaghoubi, A. Das, B. Harish, and H. Proenca, The pdestre: fully annotated dataset for pedestrian detection, tracking, and short/long-term re-identification from aerial devices, IEEE Transactions on Information Forensics and Security, vol. 16, pp. 16961708, 2020. [7] T. Li, J. Liu, W. Zhang, Y. Ni, W. Wang, and Z. Li, Uav-human: large benchmark for human behavior understanding with unmanned aerial vehicles, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 16 26616 275. [8] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, Person reidentification by descriptive and discriminative classification, in Image Analysis: 17th Scandinavian Conference, SCIA 2011, Ystad, Sweden, May 2011. Proceedings 17. Springer, 2011, pp. 91102. [9] R. Layne, T. M. Hospedales, and S. Gong, Investigating open-world person re-identification using drone, in Computer Vision-ECCV 2014 Workshops: Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part III 13. Springer, 2015, pp. 225240. [10] S. Zhang, Q. Zhang, Y. Yang, X. Wei, P. Wang, B. Jiao, and Y. Zhang, Person re-identification in aerial imagery, IEEE Transactions on Multimedia, vol. 23, pp. 281291, 2020. [11] M. Bonetto, P. Korshunov, G. Ramponi, and T. Ebrahimi, Privacy in mini-drone based video surveillance, in 2015 11th IEEE international conference and workshops on automatic face and gesture recognition (FG), vol. 4. IEEE, 2015, pp. 16. [12] A. Singh, D. Patil, and S. Omkar, Eye in the sky: Real-time drone surveillance system (dss) for violent individuals identification using scatternet hybrid deep learning network, in Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2018, pp. 16291637. SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 11 [13] A. Grigorev, Z. Tian, S. Rho, J. Xiong, S. Liu, and F. Jiang, Deep person re-identification in uav images, EURASIP Journal on Advances in Signal Processing, vol. 2019, pp. 110, 2019. [14] H. Nguyen, K. Nguyen, S. Sridharan, and C. Fookes, Ag-reid. v2: Bridging aerial and ground views for person re-identification, IEEE Transactions on Information Forensics and Security, vol. 19, pp. 2896 2908, 2024. [15] S. Wang, Y. Wang, R. Wu, B. Jiao, W. Wang, and P. Wang, Secap: Selfcalibrating and adaptive prompts for cross-view person re-identification in aerial-ground networks, arXiv preprint arXiv:2503.06965, 2025. [16] M. Ahmed, M. Jahangir, H. Afzal, A. Majeed, and I. Siddiqi, Using crowd-source based features from social media and conventional features to predict the movies popularity, in 2015 IEEE international conference on smart city/SocialCom/SustainCom (SmartCity). IEEE, 2015, pp. 273278. [17] Y. Liu, B. Peng, P. Shi, H. Yan, Y. Zhou, B. Han, Y. Zheng, C. Lin, J. Jiang, Y. Fan et al., iqiyi-vid: large dataset for multi-modal person identification, arXiv preprint arXiv:1811.07548, 2018. [18] J. Solawetz, What what-is-yolov8/, 2023, accessed: 2025-04-09. yolov8? is https://blog.roboflow.com/ [19] Z. Chen, C. Yang, Q. Li, F. Zhao, Z.-J. Zha, and F. Wu, Disentangle your dense object detector, in Proceedings of the 29th ACM international conference on multimedia, 2021, pp. 49394948. [20] X. Lu, B. Li, Y. Yue, Q. Li, and J. Yan, Grid r-cnn, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 73637372. [21] B. Hu, X. Wang, and W. Liu, Personvit: large-scale self-supervised vision transformer for person re-identification, Machine Vision and Applications, vol. 36, no. 2, pp. 113, 2025. [22] S. Li, L. Sun, and Q. Li, Clip-reid: exploiting vision-language model for image re-identification without concrete text labels, in Proceedings of the AAAI conference on artificial intelligence, vol. 37, no. 1, 2023, pp. 14051413. [23] X. Wang and R. Zhao, Person re-identification: System design and Springer, 2014, pp. evaluation overview, in Person Re-Identification. 351370."
        }
    ],
    "affiliations": [
        "Instituto de Telecomunicacoes and the University of Beira Interior, Covilha, Portugal",
        "Istanbul Medipol University, Istanbul, Turkey",
        "J.N.N. College of Engineering, Shivamogga, Karnataka, India",
        "SRM Institute of Science and Technology, Kattankulathur, India",
        "School of Computational Sciences, SRTM University, Nanded, India"
    ]
}