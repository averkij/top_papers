{
    "paper_title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "authors": [
        "Lilin Wang",
        "Lucas Ramalho",
        "Alan Celestino",
        "Phuc Anthony Pham",
        "Yu Liu",
        "Umang Kumar Sinha",
        "Andres Portillo",
        "Onassis Osunwa",
        "Gabriel Maduekwe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation."
        },
        {
            "title": "Start",
            "content": "A FRAMEWORK FOR THE SCALABLE SWE-BENCH++: GENERATION OF SOFTWARE ENGINEERING BENCHMARKS FROM OPEN-SOURCE REPOSITORIES Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe Research & Development, Turing Project Page: https://research.turing.com/swebench"
        },
        {
            "title": "ABSTRACT",
            "content": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages from open-source GitHub repositories. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. final hint-guided trajectory synthesis step converts instances that strong models fail to solve into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On subset of 1,782 instances of this benchmark, todays strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides scalable, multilingual benchmark for evaluating and improving repository-level code generation. 5 2 0 2 9 1 ] . [ 1 9 1 4 7 1 . 2 1 5 2 : r Figure 1: The SWE-Bench++ Framework. Unlike static benchmarks, our pipeline uses constrained neural synthesis to generate reproducible Docker environments and adaptive log parsers across 11 languages. three-state, state-differential oracle automatically classifies tasks as bug fixes or feature requests, producing verified benchmark instances and hint-guided training trajectories at scale."
        },
        {
            "title": "INTRODUCTION",
            "content": "The evaluation of Large Language Models (LLMs) coding agents has shifted from isolated function synthesis (e.g., HumanEval (Chen et al., 2021)) to repository-level software engineering. SWE-bench introduced repository-level benchmark based on real GitHub issues, enabling more realistic evaluation of LLM coding agents (Jimenez et al., 2024). However, it relies on manual curation and covers only 12 Python repositories. This scale is too small to capture the structural and linguistic diversity of open-source projects. Recent efforts to improve scalability have followed two paths. First, benchmarks such as Multi-SWE-bench (Zan et al., 2025) and SWE-bench Multilingual (Yang et al., 2025) extend the evaluation framework to languages like Java and Rust, but rely on manual curation and cover only few dozen repositories. Second, Python-centric automation such as SWEE-bench (Vergopoulos et al., 2025) scales to hundreds of repositories, but remains restricted to Python and suffers from two technical limitations. First, its two-state test oracle (Before-patch After-patch) is not designed to extract feature requests that introduce new APIs or functionality, as these cause the Before state to fail to build due to missing symbolscases that existing pipelines must filter out as errors. This methodological constraint restricts automated benchmarks to scenarios where tests can be executed in both states, severely limiting the coverage of feature requests. Second, it relies on static regular expressions for log parsing, preventing it from scaling to the \"long tail\" of repositories with heterogeneous test runners and non-standard outputs. Furthermore, recent works like SWE-Smith (Yang et al., 2025) and SWE-Flow (Zhang et al., 2025a) have attempted to scale data generation via synthetic means, such as synthesizing tasks from Test-Driven Development (TDD) patterns. While valuable for training, these synthetic approaches are less well suited for evaluating models on in-the-wild distributions. They lack the noisy, complex, and historical nature of human-written code. Additionally, the static nature of all aforementioned benchmarks introduces critical data contamination risk: most instances were created before the training cutoff of modern models, rendering them prone to memorization. To bridge these gaps, we present SWE-Bench++, an automated multilingual framework that generates software engineering benchmarks from GitHub pull requests. Unlike previous approaches, our methodology provides systematic pipeline that transforms raw GitHub repositories into executable evaluation environments without human intervention. To address these challenges, we introduce three mechanisms: 1. Constrained Environment & Oracle Synthesis: Existing frameworks often rely on unstructured command extraction for environments and static regexes for logs, which can be brittle at scale. We introduce synthesis engines for both infrastructure and verification. For environments, we use template-guided synthesis to populate security-hardened Dockerfiles (e.g., enforcing multi-stage builds). This approach combines LLM reasoning with static templates, achieving approximately 137% higher yield on Python repositories than SetUpAgent baseline when both are run on the same pool of 2,377 valid pull requests. For verification, we use adaptive parser synthesis to generate custom Python parsers for heterogeneous logs, when deterministic parsers fail. This constrained neural synthesis approach standardizes environment and oracle construction across 11 languages and 3,971 repositories. 2. State-Differential Task Classification: Current benchmarks struggle to distinguish between bug fixes and feature requests, often discarding instances where the pre-fix codebase fails to build. We implement state-differential oracle that compares three repository states: Base, Before (test patch applied), and After (full PR applied). We treat specific build failures in the Before state not as errors, but as semantic signals for Feature Requests (where tests rely on yet-to-be-implemented code). This allows us to verify both regression fixes and new feature implementations. 3. Hint-Guided Trajectory Synthesis: Standard training data generation (e.g., SWE-Gym) relies on passive filtering of easy tasks that agents can already solve. We introduce an active Hint Injection Algorithm that converts model-breaking instances (where SOTA models fail) in SWE-Bench++ environments into executable training trajectories. By injecting function signatures and dependency graphs as hints, we scaffold the agent to solve previously impossible tasks. Fine-tuning on just 145 of these trajectories improves cross-lingual performance from 1.6% to 3.6% on SWE-bench Multilingual. Contributions: Our work makes the following contributions: Large-scale benchmark: We construct 11,133 repository-level instances from 3,971 repositories, covering diverse build systems and coding patterns. Table 1: Comparison of software engineering benchmarks and frameworks. Feature Function SWE-bench / Multi-SWE SWEE-bench (SetUpAgent) Benchmark Dataset Benchmark Generator SWE-Smith SWE-Flow SWE-Fixer SWE-Gym SWE-Bench++ (Ours) Syn. Data Gen. Syn. Data Gen. Solver Tool RL Interface Live Benchmark Generator Generation Manual Curation Automated Synthetic Synthetic Static Scrape (PRs only) N/A Automated Gen. Scope N/A Container Bug-fix pairs Fix-test pairs N/A N/A Container, Log Parser, Trajectory Env Strategy Pre-build Images Extract cmds Pre-build image Pre-verified images N/A Pre-build images Auto-Synthesized Scale 12 / 42 514 128 74 110, Languages Python / 9 Python Only Python Only Python Only Python Centric 358 N/A 3,971 11 (Automated) Task Scope Bug Fixes Bug Fixes Bug Fixes Only TDD Incr. Dev. Simple Bugs Bug Fixes Bugs & Feature Requests Log Parsing Static Regex Static Regex Static Regex Static Regex N/A Distribution Organic Freshness Static Organic Static Synthetic Synthetic N/A N/A Organic Static N/A Organic Static Syn. Adaptive Parsers Organic Continuous (Living) Automated multilingual environments: Our pipeline automatically synthesizes Docker environments and log parsers across 11 languages. Broader task coverage: Our state-differential oracle identifies both bug fixes and feature requests, increasing feature-like coverage compared to prior benchmarks (e.g., 9% in SWE-bench). Contamination-Aware Evaluation: SWE-Bench++ is constructed from dated GitHub pull requests and can be filtered by PR creation date, enabling temporally separated evaluation sets that reduce data-contamination risk for future models. Table 1 compares SWE-Bench++ with prior benchmarks and frameworks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "We review related work primarily through the lens of the structural limitations our benchmark aims to address. While SWE-bench and its manually curated variant SWE-bench Verified (Chowdhury et al., 2024) established the gold standard for evaluating LLMs on repository-level tasks, their static and manually intensive nature creates fundamental barriers to scaling software engineering evaluation. Scalability First, synthetic generation approaches like SWE-Smith and SWE-Flow utilize LLMs to synthesize training signalseither by injecting bugs into existing codebases or by inferring incremental steps from Test-Driven Development (TDD) patterns. While valuable for training efficiency, these synthetic tasks often lack the noise, ambiguity, and in-the-wild distribution of real human-written code. Second, static data scaling efforts like SWE-Fixer (Xie et al., 2025) aggregate massive datasets by scraping GitHub history. However, it operates as retrieval-based pipeline without execution environments, prioritizing raw volume over execution-based verification. Third, compute scaling frameworks like SWE-Gym (Pan et al., 2025) transform existing benchmarks into reinforcement learning (RL) environments to generate millions of agent trajectories. While this scales experience, it remains bound to the limited problem set of the original manually curated datasets. Finally, attempts to automate organic task collection, such as SWEE-bench (Vergopoulos et al., 2025), utilize agents (e.g., SetUpAgent) to scaffold environments but remain restricted to Python and lack support for feature requests. Data Contamination and Live Evaluation Static benchmarks are highly vulnerable to data contamination, as instances created before models knowledge cutoff are frequently memorized during pre-training. The community has responded with live benchmarks such as SWE-bench-Live and LiveCodeBench, which continuously harvest new problems (Zhang et al., 2025b; Jain et al., 2025). Similarly, SWE-bench Pro attempts to mitigate contamination by incorporating private, commercial repositories (Deng et al., 2025). However, these solutions often rely on specific language ecosystems or lack the fully automated, multi-stage verification pipeline required to scale beyond hundreds of tasks to thousands. 3 The Weak Test Oracle Problem Standard evaluation protocols assume that passing developer-written test suite equates to correct fix. However, this test oracle is often unreliable. Empirical studies on SWE-bench have revealed that significant percentage of plausible patchesthose that pass the provided testsare semantically incorrect or diverge from the ground truth. This oracle limitation can lead to overestimation of model capabilities, highlighting the need for more rigorous, state-based verification methods. Environment Reproducibility Challenges Accurately reconstructing historical development environments is primary scalability bottleneck. Complex dependency trees and version mismatches often lead to environment rot, where test failures result from configuration errors rather than faulty model patches. While agentic systems like SetUpAgent (Vergopoulos et al., 2025) automate some aspects of environment creation through command extraction from documentation, they often lack the reliability of deterministic, template-guided scaffolding, restricting their success rate across polyglot repositories. Solution Leakage and Ambiguity Finally, the quality of problem statements in existing benchmarks varies significantly. recent analysis by SWE-bench+ (Aleithan et al., 2024) revealed that 32.67% of successful agent resolutions involved solution leakage, where the correct code or direct pointer was explicitly present in the issue description or comments. This allows agents to solve tasks via information retrieval rather than reasoning, further skewing leaderboard rankings."
        },
        {
            "title": "3 METHODOLOGY: THE SWE-BENCH++ FRAMEWORK",
            "content": "SWE-Bench++ is four-stage pipeline (Figure 1) that converts GitHub pull requests into executable software engineering tasks. 3.1 STAGE 1: PROGRAMMATIC SOURCING The pipeline begins with broad search to identify candidate tasks that represent realistic software maintenance and evolution. We employ scalable filters to process the GitHub firehose, selecting repositories and pull requests (PRs) that meet the following criteria: (a) active maintenance histories with recent commit activity; (b) evidence of community adoption (e.g., > 100 stars) and recognizable testing framework; (c) substantial complexity, defined by codebases exceeding 10k lines of code; (d) merged PRs that explicitly resolve an issue (ensuring link between natural language problem description and coding solution); and (e) PRs that include edits or additions to test files. This coarse filtering casts wide net, identifying millions of potential candidates for the compute-intensive stages that follow. 3.2 STAGE 2: ENVIRONMENT SYNTHESIS Once high-quality PR is identified, we create reproducible execution environment that mirrors the repository at the time the issue was present. We treat environment generation as constrained synthesis problem. Rather than relying on unstructured generation, our system employs hybrid architecture: parameterized Dockerfile templates enforce structural validity and security standards (e.g., multi-stage builds), while an LLM infers missing dynamic dependencies and versions (e.g., package versions, entry points) that cannot be parsed statically. 3.2.1 TEMPLATE-BASED SCAFFOLDING To mitigate the security risks and logical errors inherent in generating Dockerfiles from scratch, our system utilizes library of vetted, language-specific templates (e.g., Python, Java, Go, Rust). These templates enforce best practices, such as multi-stage builds, minimal base images, and non-root user execution, providing secure structural skeleton. The templates contain semantic placeholders for dynamic content, including language versions, dependency installation commands, and entry points. The agents objective is to populate these placeholders, thereby combining the robustness of human-engineered structure with the flexibility of an LLM. 3.2.2 ITERATIVE REFINEMENT WITH BUILD-AND-TEST FEEDBACK The instantiation process is an iterative loop that uses Docker builds and test runs as validation signals. The process begins with Repository Analysis and Plan Generation, where the synthesis engine performs deep structural analysis rather than heuristic approaches that simply scrape documentation files (e.g., README). The LLM is granted controlled tool access via Model Context Protocol (MCP) server that exposes repository-level operations (clone, list, read). Through these deterministic calls, the synthesis engine materializes the target PR at the correct commit, 4 traverses the project tree, and inspects build scripts and manifests verbatim. This evidence is incorporated into structured JSON plan. This tool-augmented introspection enables precise identification of required base images, package managers, and entry points, reducing hallucinations compared to static text extraction. Following this analysis, the system enters Build-Feedback Loop. It injects the JSON values into the template and attempts Docker build. If the build fails, the synthesis engine captures the standard error (stderr) output. This error trace serves as feedback for the LLM, which generates corrected JSON plan. This self-correction loop continues until success or retry limit (set to 5) is reached. Crucially, successful build guarantees only syntactic validity, so the final phase is the TestRun-Feedback Loop. Here, the system spins up the container to verify that the test runner executes correctly, ensuring that no tests fail due to environment misconfiguration before advancing the instance to the next stage. We quantify the impact of this template-guided design in Section 4.1, where we show that substituting SetUpAgent-style baseline reduces yield on Python repositories to 40% of our full system."
        },
        {
            "title": "3.3 STAGE 3: AUTOMATED STATE-DIFFERENTIAL TEST ORACLE EXTRACTION",
            "content": "Once the environment is ready, this stage executes the code across multiple repository states and extracts the test oracle through automated log parsing. 3.3.1 THREE REPOSITORY STATES We consider three snapshots of the repository relative to the PR: base (parent commit of the PR), before (base plus test-file changes only), and after (full PR, including implementation changes). The pipeline runs all tests under these three statesBase, Before, and Afterproducing three distinct execution logs. 3.3.2 STATE-DIFFERENTIAL CLASSIFICATION LOGIC Existing benchmarks primarily support Scenario A: Regression / Bug Fixthey assume that the Before state is buildable and focus solely on Fail-to-Pass (F2P) tests. However, based on observations from large-scale real-world datasets, we find that this definition fails to capture Scenario B: Feature Request. To address this limitation, we broaden the definition of F2P to encompass both scenarios, allowing the framework to handle regression fixes and feature additions in unified manner. Scenario A: Regression / Bug Fix. If the Before state builds successfully, we execute the modified tests. F2P: Tests that fail in Before and pass in After. These represent the regression. P2P: Tests that pass in both. These represent the constraint to not break existing functionality. Scenario B: Feature Request. If the Before state fails to build (due to missing symbols/dependencies introduced in the PR): F2P: We identify newly added tests in the PR that pass in the After state. The build failure in Before serves as the confirmation that the feature was absent. Section 4.1 reports an ablation showing that disabling adaptive parser synthesis reduces the final dataset by about 16% (to 84% of its original size). 3.3.3 SYNTHESIZED ADAPTIVE LOG PARSING To automatically derive the test oraclestructured evidence of which tests pass or failthe system employs hybrid architecture designed to handle diverse test frameworks and noisy outputs. We implement hierarchical parsing strategy. The system first attempts deterministic symbolic parsing (using high-precision regex) for standard frameworks (full list in Appendix B) to ensure zero-cost accuracy. When symbolic parsing fails (e.g., unrecognized formats), the system falls back to neural synthesis, where an LLM generates custom Python parser. To verify the generated parser, we use synthetic failure injection. An LLM modifies assertions in the code to force failure; if the parser correctly identifies this failure, it is considered valid. 1. Self-correcting synthesis loop: The system executes the synthesized parser on sample logs and feeds any crashes or implausible test counts back to the LLM, iteratively refining the parser. This ensures resilience against noisy or non-standard output formats. 2. Synthetic failure injection (Balanced Evaluation): We inject artificial failing assertions and check that the parser flips the corresponding test outcomes, providing an empirical check that it discriminates PASS vs FAIL correctly."
        },
        {
            "title": "3.4 STAGE 4: AUTOMATED INSTANCE QUALITY ASSURANCE & VERIFICATION",
            "content": "To ensure the reliability of the generated instances, we deploy four-layer Automated Quality Assurance (AutoQA) pipeline designed to identify and reject unstable environments, flaky tests, and ambiguous problem statements. Layer 1: Environment Determinism (Build Stability) Each dockerized environment is built and instantiated three times, and we retain only instances where the testbed initializes successfully in all three runs, filtering out flaky build dependencies. Layer 2: Oracle Consistency (Test Determinism) To confirm that the test suite yields identical outcomes across runs, we validate Test Determinism. We execute the golden solution (the ground-truth patch) against the verify/regression tests three times in independent containers. We retain only those instances where the test results (Pass/Fail) are identical across all three runs, eliminating flaky tests that pass or fail based on timing conditions. Layer 3: Semantic Alignment & Automated Curation We employ rubric-based LLM-Judge to evaluate the alignment between the problem statement and the test oracle. Empirically, these LLM-based reviewers achieve precision close to senior human annotators; see Appendix C.3  (Table 7)  for detailed comparison of automated reviewers and human raters. While instances with fundamental ambiguity and misalignment are rejected (\"Low Quality\"), we identify recoverable class of \"Medium Quality\" instances where tests rely on implementation details not explicitly requested in the issue (e.g., new accessor methods). For these, we trigger an Automated Curation module: the system analyzes the code patch to extract the signatures of implicit dependencies and appends them to the problem statement as \"Hints\". This systematically repairs underspecified tasks, transforming them into high-fidelity, verifiable instances without manual per-instance curation. Details can be found in Appendix D. Layer 4: False-Negative Filtering (model-breaking verification) To ensure that our benchmark accurately measures the upper bounds of model capability, we perform deep inspection on instances where state-of-the-art (SOTA) models fail to generate solution. The goal is to distinguish between True Negatives (model limitation) and False Negatives (dataset defects). We utilize an automated trajectory & log inspection module that parses the execution trace of failed SOTA attempts. Instances where the model failure stems from infrastructure artifacts (e.g., unsupported tool, tool crashes), unsupported dependencies, or underspecified problem statements are flagged and removed. This ensures that the remaining hard instances represent genuine reasoning challenges. Human Verification (Verified Subset) While the core pipeline is fully automated, we established manually verified subset for high-precision evaluation. We recruited 82 pre-screened annotators to conduct comprehensive manual verification on the model-breaking instances retained from Layer 4, following the guidelines of SWE-bench Verified. 3.5 APPLICATION: HINT-GUIDED TRAJECTORY SYNTHESIS FOR TRAINING Stages 14 fully specify the SWE-Bench++ benchmark and its evaluation pipeline. In this section, we illustrate an application of these environments: using hint-guided curation algorithm to convert model-breaking instances into high-fidelity training trajectories. Unlike frameworks such as SWE-Gym (Pan et al., 2025), which generate data by passively filtering for trajectories where an agent naturally succeeds, we target model-breaking instances that SOTA models fail to solve. To convert these failures into valuable training signals, we introduce Hint-Guided Curation algorithm: 1. Failure Identification: The system identifies instances where SOTA baselines consistently fail (0% pass rate) 2. Contextual Scaffolding: The pipeline analyzes the ground-truth patch to extract critical function signatures and dependency graphs. These are injected as Hints into the agents system prompt to bound search space. 3. Guided Resolution: The agent retries the task with this scaffold. This mechanism raises the pass rate on these hard tasks from 0% to 70%, harvesting successful reasoning traces on problems that were previously unsolvable. 6 4. Contamination Control & Outcome: To prevent the model from learning to rely on hints, we apply Thought Regeneration pass: an LLM rewrites the agents reasoning trace to exclude hint-related keywords while preserving the logical solution path. This produces dataset of frontier trajectoriesinstances at the exact boundary of capabilityproviding significantly higher information gain for fine-tuning than the easy instances captured by passive filtering."
        },
        {
            "title": "4 EMPIRICAL VALIDATION",
            "content": "We evaluate SWE-Bench++ along four axes: (i) pipeline yield and dataset properties (Section 4.1), (ii) agent performance baselines (Section 4.2), (iii) fine-tuning experiments (Section 4.3), and (iv) qualitative failure analysis (Appendix H)."
        },
        {
            "title": "4.1 EVALUATING THE SWE-BENCH++ PIPELINE AND DATASET",
            "content": "Yield & Throughput Analysis We initialized the sourcing module with 137,048 candidate Pull Requests (PRs) meeting our activity and complexity criteria. Of these, 28,513 (20.8%) successfully passed the Environment Synthesis stage and State-Differential Test Oracle Extraction (Stage 2 and Stage 3), yielding reproducible Docker container with successfully parsed logs. This success rate varies by language ecosystem, with Python (41%) and Java (27%) showing the higher resilience, while compiled languages like C++ (9.5%) present greater environmental challenges due to complex toolchain dependencies. Following the Automated Quality Assurance pipeline (Stage 4), 39% of the dockerized instances were verified as deterministic and aligned, resulting in final dataset of 11,133 instances. The end-to-end processing time averages around 67 minutes per instance, dominated by the compilation and testing latency of the Docker build process. Controlled comparison with SetUpAgent: To isolate the effect of template-guided environment synthesis, we re-ran SetUpAgent pipeline and our method on the same pool of 2,377 valid Python pull requests originally selected by SWEE-bench. Our approach successfully recovers deterministic, dockerized environments for 2.37 as many PRs (a 137% relative increase in yield) as the SetUpAgent baseline, under identical success criteria. impact of template-guided environment Ablation: replacing our template-guided Dockerfile synthesis with SetUpAgent-style command-extraction approach reduces the number of successfully dockerized PRs to 40% of our full systems yield in Python repositories. Combined with the controlled comparison above, this suggests that the majority of our yield gains over SetUpAgent come from constraining generation within vetted templates. synthesis On Python repositories, Ablation: impact of adaptive parser synthesis If we disable the adaptive log-parser synthesis stage and rely exclusively on deterministic regex parsers, the number of instances that survive the full pipeline drops to 84% of the final 11,133. The missing 16% primarily correspond to repositories with non-standard or noisy test outputs that our fixed parsers cannot reliably interpret, highlighting the importance of the neural parser synthesis stage. Table 2: Dockerization success rates (Yield) by language"
        },
        {
            "title": "Python",
            "content": "Go TS JS"
        },
        {
            "title": "Rust",
            "content": "C++ C# C"
        },
        {
            "title": "Yield",
            "content": "41.0% 41.0% 40.0% 39.0% 38.0% 38.0% 27.0% 19.0% 11.0% 10.0% 9.5% Dataset Distribution The final dataset covers 3,971 unique repositories, an increase of two orders of magnitude over the 12 repositories in the original SWE-bench. This shift minimizes the risk of overfitting to specific project coding styles. taxonomy analysis on random subset of 488 instances confirmed the datasets representative scope across the open-source distribution, covering diverse domains such as DevTools (27.1%), Infra/DevOps (18.5%), Scientific Computing (12.9%), and Data Engineering (10.7%), with the long tail covering AI/ML, Blockchain, and Embedded Systems. Furthermore, the evaluation of resolved rates, broken down by repository domain (as detailed in Figure 2a), showed significant differential performance among the models. This suggests that the inherent diversity of the benchmark serves as high-fidelity diagnostic tool, moving beyond single-score metrics to reveal the specific strengths and weaknesses of advanced code-generation models. Bug vs Feature Coverage To quantify task types, we manually annotated random sample of 488 instances. At the code level, 61.1% of changes are primarily bug fixes, 30.7% implement new features, with the remainder covering 7 Table 3: Yield analysis across processing stages Stage Count Yield Notes Stage 1 (Sourcing) Stage 2 & 3 (Environment Synthesis & Log Parsing) Stage 4 (Quality Assurance) 137,048 28,513 11,133 100% Filtered by activity & complexity, etc. 20.8% High variance: Python (41%) vs. C++ (9.5%). 8.1% High-fidelity deterministic instances. refactors, performance improvements, and other maintenance E.1. At the issue level, 56.1% of linked issues are bug reports and 38.5% are feature requests E.2. This substantially increases feature-request coverage compared to SWE-bench, which contains only about 9% feature-request issues. Difficulty Distribution The SWE-Bench++ dataset has balanced distribution of task sizes by lines changed: 24.5% small (130), 45.6% medium (31100), 22.3% large (101300), and 7.6% very large (301+). Files changed also show breadth: 39.3% involve 24 files, 36.9% 58 files, 17.1% 915 files, and 6.7% 16+ files. The dataset includes challenging instances (12.2% with 200+ lines and 17.2% with 10+ files), ensuring comprehensive evaluation framework covering quick fixes to large-scale refactors. (a) Percentage of resolved repositories by type. (b) Difficulty distribution. Figure 2: An overall view of the dataset. 4.2 EVALUATING LLM AGENTS ON SWE-BENCH++ To establish performance baselines, we benchmarked leading LLM agents on 1,782-instance subset of SWE-Bench++ (a verified cross-lingual sample). We construct this subset via stratified random sampling by language, drawing fixed number of instances from each language slice to preserve diversity while keeping evaluation cost manageable. This yields roughly 100280 instances per language (full breakdown in Appendix E.3). As shown in Table 4, the benchmark presents significant challenge even to frontier models. Performance Hierarchy: The gap between the top-performing model (36.20%) and mid-tier models (low-20s) highlights the difficulty of the benchmark. Language Disparity: Models generally perform stronger on Python and Java, likely due to the preponderance of these languages in pre-training corpora. Lower performance on Go and C++ highlights the need for multilingual benchmarks to drive progress in less represented languages. 4.3 FINE-TUNING EXPERIMENTS To validate the efficacy of SWE-Bench++ as source of high-quality training data, we conducted controlled fine-tuning study using the Qwen2.5-Coder 7B and 32B variants (Hui et al., 2024). Our objective was to assess whether adding small volume of real-world, agentic trajectories from SWE-Bench++ could improve performance on out-of-distribution polyglot tasks compared to purely synthetic baselines. 4.3.1 EXPERIMENTAL SETUP We constructed distinct data mixtures to isolate the impact of trajectory source, diversity and scale. 8 Table 4: Leaderboard of model performance on the SWE-Bench++ 1,782-instance subset using the SWE-Agent framework (Jimenez et al., 2024). Model (SWE-Bench++ 1,782-instance subset) Py Java JS/TS Rust C/C++ Go PHP Ruby C# Overall Per-language pass@10 gpt-5-2025-08-07 claude-sonnet-4.5 claude-opus-4.1 claude-sonnet-4-20250514 xai/grok-code-fast-1 xai/grok-4-0709 gemini/gemini-2.5-pro qwen3-coder gpt-4o 34.57% 36.20% 32.38% 31.09% 30.42% 25.52% 24.92% 24.19% 16.89% 43.57% 41.84% 33.67% 22.86% 30.81% 24.00% 42.90% 40.50% 39.00% 34.29% 39.80% 34.69% 22.86% 57.30% 28.00% 42.90% 53.00% 55.00% 37.14% 33.67% 27.55% 14.29% 48.11% 19.00% 42.90% 40.50% 43.50% 36.43% 31.63% 26.53% 17.14% 36.22% 20.00% 35.70% 37.50% 42.50% 36.43% 41.84% 31.63% 11.43% 10.27% 28.00% 50.00% 35.00% 42.00% 34.29% 38.78% 29.59% 22.86% 7.03% 17.00% 42.90% 34.50% 39.00% 20.00% 28.57% 19.39% 8.57% 28.11% 13.00% 50.00% 35.50% 34.00% 16.43% 31.63% 21.43% 14.29% 9.19% 19.00% 35.70% 39.50% 36.50% 10.71% 12.24% 5.10% 5.71% 4.32% 9.00% 28.60% 13.00% 9.00% 1. Experiment 1 (Baseline): We establish baseline using the 5,016 synthetic trajectories from SWE-Smith. The dataset used here serves as the foundation for the SWE-Agent-LM models (7B and 32B denoted as Experiment 1a and 1b respectively). 2. Experiment 2 (SWE-Bench++ Density): We augment the Baseline with 179 curated trajectories sourced from 44 GitHub issues via SWE-Bench++. This mixture tests whether having multiple solution paths for the same issues helps more than adding new issues. 3. Experiment 3 (SWE-Bench++ Diversity): We augment the Baseline with 145 curated trajectories sourced from 145 unique GitHub issues. This mixture tests the value of maximizing issue and repository variety (higher diversity) with comparable data volume. 4. Experiment 4 (SWE-Bench++ Data-Scaling): We evaluate data scaling laws by iteratively augmenting the Baseline with 200, 400, and 800 curated trajectories (denoted as SWE-Bench++ Data-Scaling-1, SWE-Bench++ Data-Scaling-2, and SWE-Bench++ Data-Scaling-3). Distinct from Experiments 2 and 3, the trajectories here utilize hybrid human review strategy where 40% of the added data is purely synthetic (filtered only for passing the harness but with no human QA) while the remainder undergoes human review. 5. Experiment 5 (SWE-Bench++ Model-Scaling): We replicate the data scaling variations from Experiment 4 using the Qwen2.5-Coder-32B model to verify that the performance gains hold also for models of much larger sizes. Note that for the SWE-Bench++ Data-Scaling-3 variation (+800 SWE-Bench++ data) we scaled the SWE-Smith baseline to 10,032 trajectories. This ensures robust ratio between the specialized SWE-Bench++ data and the general baseline, preventing the larger model from overfitting to the specific fine-tuning tasks at the expense of generalization (catastrophic forgetting). Evaluation Protocol: We evaluate the resulting checkpoints on SWE-Bench Multilingual (Yang et al., 2025), rigorous benchmark comprising 300 tasks across 42 repositories and 9 programming languages. This benchmark contains no Python tasks, serving as strict test of cross-lingual generalization. Furthermore, we ensured zero overlap between the repositories used in our SWE-Bench++ training sets and the SWE-bench Multilingual test set. Ablation Logic: Experiment 1 functions as reproduction of the SWE-Agent-LM performance profile, utilizing standard fine-tuning setup in the absence of open-source configurations. Experiments 2 and 3 disentangle the value of repository variety versus data volume, using raw, unverified trajectories to establish strict lower-bound for data utility. Experiments 4 and 5 confirm that gains are robust across data and model sizes; crucially, the inclusion of 40% unreviewed data verifies that our high signal-to-noise sourcing allows for scaling without being bottlenecked by manual review costs. 4.3.2 FINE-TUNING EXPERIMENT DETAILS Our fine-tuning setup closely follows SWE-Smith. We use learning rate of 5e-5, up to 3 epochs, and maximum context length of 32,768, and we run training with MS-Swift on 8 NVIDIA H200 144G GPUs. We also follow the same XML data conversion strategy and the rejection sampling fine-tuning process. detailed list of all the used hyper-parameters can be found in Appendix G. 4.3.3 RESULTS The results of the experiment can be seen in the table below: 9 Model Size Qwen2.5-Coder-7B Qwen2.5-Coder-32B Experiment 0 1 2 3 4 0 1 5 Fine-Tuning Mixture Off-the-shelf SWE-Smith 5k SWE-Bench++ Density SWE-Bench++ Diversity SWE-Bench++ Data-Scaling-1 SWE-Bench++ Data-Scaling-2 SWE-Bench++ Data-Scaling-3 Off-the-shelf SWE-Smith 5k SWE-Bench++ Data-Scaling-1 SWE-Bench++ Data-Scaling-2 SWE-Bench++ Data-Scaling-3* ** Performance (pass@1) Diff CI* (95%) 0 / 300 5 / 300 7 / 300 11 / 300 6 / 300 16 / 300 20 / 300 4 / 300 12 / 300 17 / 300 21 / 300 25 / 300 (+1.0, +10.0) (+0.0, +5.0) (+1.0, +8.0) (+4.0, +16.0) (+1.0, +8.0) (+3.0, +14.0) (+1.0, +10.0) (+1.0, +8.0) (+1.0, +8.0) Table 5: Fine-tuning results on SWE-bench Multilingual. * 95% confidence interval of the difference between the pass@1 performance of the current and the previous row. ** this variation uses 10,032 SWE-Smith data in the baseline mix instead of 5,016 in all the other experiments. Incorporating just 145 SWE-Bench++ trajectories (i.e., 2.8% of the mix) increased the baseline performance (from 5/300 to 11/300) and more than doubled the number of valid patches, demonstrating the critical value of high-diversity, hard multilingual samples. While the improvement from solution density (Exp 2) is small and not statistically distinguishable from the SWE-Smith baseline (95% CI: +0.0 to +5.0), the improvement from repository variety (Exp 3) presents robust, statistically significant signal (95% CI: +1.0 to +8.0). These gains scale monotonically with data volume: increasing the SWE-Bench++ subset to 800 trajectories quadrupled the 7B baseline score, trend that transferred robustly to the 32B model which reached peak performance of 25/300."
        },
        {
            "title": "5 LIMITATIONS AND FUTURE WORK",
            "content": "While SWE-Bench++ scales via automation, execution-based verification remains proxy for correctness and cannot capture aspects such as code maintainability or algorithmic efficiency. Furthermore, our State-Differential Oracle is bound by the quality of the original developer-written test suites; sparse tests may strictly allow agents to produce patches that technically pass but fail human review. While our current pipeline utilizes an LLM-Judge for semantic alignment, approximating professional maintainer standards remains challenge. Future iterations could explore 1) scalable Human-in-the-Loop curation, leveraging community feedback or crowdsourced validation to bridge the gap between functional correctness and true patch acceptability; 2) multi-modal verification to support UI-centric or frontend-heavy tasks. Our living benchmark design enables temporally separated, contamination-aware evaluation by filtering instances based on PR creation time relative to models pre-training cutoff. In practice, we approximate contamination control by evaluating each model only on instances whose PR timestamps fall strictly after its published cutoff date. While temporal separation is strong heuristic rather than formal guaranteeas code may still appear in other pre-training sourcesthe frameworks primary defense is its capacity for continuous regeneration. Unlike static datasets, SWE-Bench++ continuously ingests fresh pull requests, enabling dynamic evaluation sets that extend beyond any fixed training horizon."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Real-world software development is heterogeneous and evolvingproperties that static, manually curated benchmarks struggle to capture. While previous efforts laid the groundwork for repository-level evaluation, manual bottlenecks restricted them to negligible fraction of the ecosystem. SWE-Bench++ addresses these limitations not merely through automation, but through constrained neural synthesis framework. By integrating template-guided environment scaffolding, state-differential oracle extraction, and adaptive log parsing, our approach systematically recovers complex tasksincluding feature requestsacross heterogeneous build systems that prior methods discard. Furthermore, our hint-guided trajectory synthesis transforms these tasks into vital training resource that improves model performance on an external multilingual benchmark. By maintaining living benchmark of fresh instances, SWE-Bench++ minimizes data contamination risks while driving the development of models that generalize across languages and build systems."
        },
        {
            "title": "7 ACKNOWLEDGMENTS",
            "content": "We thank David Wei, Mahesh Joshi, Yuzhao Ni, Ivan Kuznetsov, Manas Sambare, Mithil Poojary, Ashni Sheth for their insightful discussions and valuable feedback. We extend our deepest thanks to all annotators for their tremendous effort and contributions to this project."
        },
        {
            "title": "REFERENCES",
            "content": "Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for llms, 2024. URL https://arxiv.org/abs/2410.06992. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Neil Chowdhury, James Aung, Chan Jun Shern, Oliver Jaffe, Dane Sherburn, Giulio Starace, Evan Mays, Rachel Dias, Marwan Aljubeh, Mia Glaese, Carlos E. Jimenez, John Yang, Leyton Ho, Tejal Patwardhan, Kevin Liu, Introducing SWE-bench verified, 2024. URL https://openai.com/index/ and Aleksander Madry. introducing-swe-bench-verified/. Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Vijay Bharadwaj, Jeff Holm, Raja Aluri, Chen Bo Calvin Zhang, Noah Jacobson, Bing Liu, and Brad Kenstler. Swe-bench pro: Can ai agents solve long-horizon software engineering tasks?, 2025. URL https://arxiv.org/abs/ 2509.16941. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2025. URL https://livecodebench.github.io/. Project website. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with SWE-Gym. In Proceedings of the 42nd International Conference on Machine Learning. PMLR, 2025. Konstantinos Vergopoulos, Mark Niklas MÃ¼ller, and Martin Vechev. Automated benchmark generation for In International Conference on Machine Learning (ICML 2025), 2025. URL repository-level coding tasks. https://icml.cc/virtual/2025/poster/43922. Poster. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents, 2025. URL https://doi.org/10.1145/3709623. Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. SWE-fixer: Training open-source LLMs for effective and efficient GitHub issue resolution. arXiv preprint arXiv:2501.05040, 2025. John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025. URL https://arxiv.org/abs/2504.21798. 11 Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. Multi-swe-bench: multilingual benchmark for issue resolving, 2025. URL https://arxiv. org/abs/2504.02605. Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, and Junyang Lin. Swe-flow: Synthesizing software engineering data in test-driven manner, 2025a. URL https://arxiv. org/abs/2506.09003. Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, and Dongmei Zhang. Swe-bench goes live! arXiv preprint arXiv:2505.23419, 2025b. 12 SAMPLE INPUT AND OUTPUT IN 3.2.2 A.1 PHASE 1: SAMPLE INPUT AND OUTPUT Input. System Prompt: Expert Build Engineer Role: Expert Build Engineer AI Task: Analyze GitHub pull request and generate JSON configuration object to populate the Dockerfile template. Goal: Populate pre_install, build, test_cmd, and log_parser_name fields. Core Reasoning Steps: 1. Repository Inspection: Submodules: Detect .gitmodules and ensure recursive initialization. Build System Detection:"
        },
        {
            "title": "Identify primary build tools by file",
            "content": "CMakeLists.txt CMake, pom.xml Maven). existence (e.g., Compiler Standard: Parse configuration files to detect language standards (e.g., CMAKE_CXX_STANDARD, sourceCompatibility). 2. Dependency Resolution: System Dependencies: Scan manifests/imports for system packages (e.g., apt-get install). Package Managers: Detect/invoke language-specific managers (e.g., vcpkg, pip, npm, cargo). 3. Build Strategy: Generate commands to clean, configure, and build the project. Constraint: Ensure build artifacts persist in dedicated directories. Constraint: Enforce parallel compilation (e.g., -j$(nproc)) for efficiency. 4. Test Execution Logic (Crucial): Scope filtering: Map modified files in the PR to specific test targets (e.g., pytest tests/test_mod.py) if possible. Output Formatting: Ensure the test runner emits machine-readable logs (XML/JSON) or verbose stdout. 5. Log Parser Selection: Analyze test framework imports (e.g., import pytest, #include <gtest.h>) to select parser: \"googletest\" \"pytest\" \"maven\" ... \"agentic\" Output Specification (JSON): { } \"pre_install\": [\"list\", \"of\", \"system\", \"commands\"], \"build\": [\"list\", \"of\", \"build\", \"commands\"], \"test_cmd\": \"single comprehensive test command\", \"log_parser_name\": \"selected_parser_id\", 13 Output. { } \"docker_specs\": { \"java_version\": \"17\" }, \"build\": [ \"mvn clean install -B -DskipTests\" ], \"test_cmd\": \"mvn test -B -pl project-core,project-api -am\", \"log_parser_name\": \"maven\" A.2 PHASE 2: SAMPLE INPUT AND OUTPUT Input. System Prompt: Error Recovery & Fix Context: previous attempt to generate build configuration failed during execution. Task: Analyze the provided error log and the original JSON configuration to identify the root cause. Based on your analysis, provide revised JSON object that corrects the error. Original Failing Configuration (JSON) { } \"docker_specs\": { \"java_version\": \"17\" }, \"build\": [ \"mvn clean install -B -DskipTests\" ], \"test_cmd\": \"mvn test -B -pl project-aggregator -am\", \"log_parser_name\": \"maven\" Execution Error Log + mvn test -B -pl project-aggregator -am [INFO] Scanning for projects... [ERROR] Could not find the selected project 'project-aggregator' in the reactor. [ERROR] Available projects: project-parent, project-core, project-api ... Output. { } \"docker_specs\": { \"java_version\": \"17\" }, \"build\": [ \"mvn clean install -B -DskipTests\" ], \"test_cmd\": \"mvn test -B -pl project-core,project-api -am\", \"log_parser_name\": \"maven\" A.3 PHASE 3: SAMPLE INPUT AND OUTPUT Input. System Prompt: Test Execution Verification Role: Test Execution Validator Task: Analyze the terminal output from test command to verify if the execution was valid, complete, and semantically parsable. Goal: Distinguish between valid test run (where tests may fail due to code bugs) and broken environment (where the runner crashes or fails to start). Core Reasoning Steps: 1. Execution Integrity: Check if the test runner successfully started and completed its process without crashing or exiting early due to segmentation faults or system errors. 2. Output Completeness: Verify that specific test names or IDs were printed to stdout (e.g., test_login PASSED). This is prerequisite for the subsequent Log Parsing stage. 3. Scope Verification: Confirm that the test suite ran the intended scope (all tests or the targeted subset), rather than silently exiting after single submodule error. 4. Failure Classification (Crucial): Acceptable (Success): Tests marked as FAIL, SKIP, or XFAIL due to logic bugs, missing credentials, or platform constraints (e.g., GPU required). These indicate working environment. success: true Unacceptable (Failure): Failures due to missing system libraries (ImportError, ModuleNotFound), toolchain crashes, or syntax errors in the test harness itself. These indicate broken environment that requires rebuilding. success: false Output Specification (JSON): { \"success\": true, // Set to true if environment is healthy \"reason\": \"Explanation if success is false\", \"error_message\": \"Extracted environment error trace\", \"details\": { \"testCommandExecuted\": true, \"testNamesPrinted\": true, \"allTestsRan\": true } } Output. { \"success\": true, \"reason\": \"\", \"error_message\": \"\", \"details\": { \"testCommandExecuted\": true, \"testNamesPrinted\": true, \"mostTestsRan\": true } }"
        },
        {
            "title": "B STANDARD LOG PARSER UTILIZED",
            "content": "Language"
        },
        {
            "title": "Python",
            "content": "JavaScript / TypeScript"
        },
        {
            "title": "Java",
            "content": "Go"
        },
        {
            "title": "PHP",
            "content": "C/C++ C# Log parser pytest Pytest; django Django test runner vitest Vitest; jest Jest; mocha Mocha; karma Karma; tap Test Anything Protocol maven Maven Surefire; gradle Gradle; ant Ant gotest standard go test cargo standard cargo test rubyunit; minitest; rspec with JSON output transformation; cucumber; tap phpunit doctest XML doctest; googletest; catch2; tap NUnit; XUnit; MSTest LLM-JUDGE FOR LAYER 3 SEMANTIC ALIGNMENT IN 3.4 C.1 QUALITY ANALYSIS METRICS To systematically assess PR quality, SWE-Bench++ computes two complementary scoresissue_clarity and test_to_issue_alignmenteach ranging from 0 (best) to 3 (worst). Presence of Success Criteria: Explicit expected behavior or acceptance criteria; missing or vague criteria increase the score. Specificity of Problem Description: Concrete, unambiguous instructions reduce the score; motivational-only content, screenshots, or external links increase it. Contextual Completeness: Steps to reproduce, code snippets, or stack traces reduce the score; absence of actionable information leads to the highest score. C.2 QUANTIFYING TEST-TO-ISSUE ALIGNMENT Core Behavior Coverage: Tests should exercise the main functionality or bug; poor coverage yields higher scores (23). False Negatives (Correct Solutions Rejected): Tests too narrow; typically addressed by adding cases (e.g., score 1). False Positives (Incorrect Solutions Accepted): Missing core coverage; requires extending/modifying tests (penalized 23). C.3 PERFORMANCE BY HUMAN VERIFICATION Automated reviewersintended as junior substitutesalready achieve precision close to senior/lead annotators (0.930.95 vs. 0.964/0.963). Recall varies substantially: Claude-Sonnet-4 (0.921) and Gemini-2.5-Pro (0.884) are closest to human recall (0.991/0.963), whereas GPT-5 is markedly more conservative (0.370 recall, FNR 0.63). This conservatism yields the lowest false positive rate (0.154) but at the cost of missing many problematic instances, while the higher-recall automated reviewers trade off with elevated false positive rates relative to humans (0.4230.615 vs. 0.308). HINTS GENERATION PROCESS FOR LAYER 3 SEMANTIC ALIGNMENT IN 3.4 D.1 PREDICTING WHETHER HINTS ARE NEEDED ( S_H T_N E D) The module predicts whether hint is necessary (is_hint_needed=1 indicates needed). Decision signals:"
        },
        {
            "title": "Precision Recall",
            "content": "False Pos. Rate False Neg. Rate Auto Reviewer (Claude-Sonnet-4) Auto Reviewer (Gemini-2.5-Pro) Auto Reviewer (GPT-5) Senior (qa_trainer3) Lead (calibrator) 0.926 0.946 0.952 0.964 0.963 0.921 0.884 0.370 0.991 0.963 0.615 0.423 0.154 0.308 0. 0.079 0.116 0.630 0.009 0.037 Table 7: Performance comparison of automated LLM-based reviewers and human annotators. Build Logs: Detect build failures in Before logs via log parsing. Golden Rules: Identify elements like new function signatures to prevent avoidable failures (e.g., new_function vs. newFunction) via AST/regex. LLM Judgment: Few-shot LLM assesses whether contextual hints are required. Evaluated on 243 senior-labeled instances; accuracy was 94.6%. D.2 GENERATING HINT VALUES Generate minimal hint_value content focusing on essentials (e.g., critical signatures), combining golden rules with LLM judgment to maximize fairness while avoiding biasing context. D E_T AND U E_T DISTRIBUTIONS IN 4.1 E.1 CODE TYPE (PRIMARY) E.2 ISSUE TYPE (PRIMARY) code_type_primary count percentage bug-fix feature refactor performance unknown dependency-update build-ci 298 150 22 12 3 1 1 61.0656 30.7377 4.5082 2.4590 0.6148 0.2049 0.2049 issue_type_primary count percentage bug-report feature-request performance-issue chore unknown 274 188 12 10 2 56.1475 38.5246 2.4590 2.0492 0.4098 E.3 LANGUAGE BREAKDOWN FOR THE 1,782 SAMPLES TRAJECTORY GENERATION PROCESS IN 4.3 F.1 DATA PREPARATION We select SOTA model-breaking issues as the basis for curation, leveraging tailored Docker environments produced earlier in the pipeline so the agent does not need to install the environment. The scaffold supports multiple underlying 17 language sample count Python JavaScript TypeScript Java Go Rust Ruby PHP C/C++ C# 280 100 96 196 200 175 200 100 185 250 LLMs with SWE-Agent. This choice is motivated by the goal of increasing trajectory diversityas different models induce distinct solution paths. F.2 TRAJECTORY GENERATION AND SELECTION We produce successful trajectories by engineering the system/user prompts and injecting issue-tailored hints that guide the agents exploration and problem solving. Successful trajectories are identified using our extended SWE-Bench evaluation harness: trajectory is marked as successful if its submitted solution yields successful outcomes on both P2P (pass-to-pass) and F2P (fail-to-pass) tests. Hints are crucialcombined with multiple attempts, they raise the passing rate from approximately 0% to 70% in our settingenabling effective demonstration generation on SOTA-model-breaking cases. We curate agentic trajectories that successfully resolve real-world GitHub issues, encompassing diverse array of task types including bug fixes, feature requests, and refactoring. This approach contrasts fundamentally with synthetic pipelines like SWE-Smith (Yang et al., 2025), which typically construct artificial problem constraints and focus predominantly on regression bugs. By leveraging the SWE-Bench++ automated pipeline and the novel hint injection strategy (Section 3.5), we are able to harvest these high-fidelity traces to SOTA model breaking GitHub issues at scale without reliance on pre-existing model solutions. We utilize SWE-Agent (Jimenez et al., 2024) as the agentic scaffold. Each trajectory is recorded as structured, interleaved sequence of: 1. Thought: Model-generated reasoning traces (Chain-of-Thought). 2. Action: Executable tool invocations (e.g., edit_file, run_test). 3. Observation: Verbatim feedback from the environment (stdout/stderr). This (Thought, Action, Observation) representation captures the dynamics of iterative problem-solving, demonstrating how an agent explores codebase, revises incorrect assumptions, and converges on solution. This provides significantly richer supervision than the single-shot solution prompting used in the original SWE-bench setup (Jimenez et al., 2024) or the static flows of agentless approaches (Xia et al., 2025). These trajectories serve as high-quality demonstrations for standard fine-tuning regimes (e.g., SFT, DPO). F.3 AUTOMATED TRAJECTORY QUALITY ASSURANCE The harness alone is not sufficient measure of quality. For instance, truncated trajectories may still pass, and hints can induce over-reliance. We therefore apply multi-stage, automated QA pipeline: 1. Structural validity: remove trajectories that do not end with final submit action. 2. Hint contamination control: remove trajectories whose actions or observations contain hint-related keywords. 3. Thought regeneration: prompt language model to rewrite thoughts containing hint-related keywords (the prompt excludes the hints). 4. Automated judging: use language-model-based evaluator to retain only high-quality trajectories. 18 The final automated stage (step 4 above) scores trajectories along four dimensions: (i) successful reproduction of the problem (for bug-fix cases), (ii) plausibility of the proposed solution, (iii) evidence of validation, and (iv) adherence to sound engineering practices. F.4 HUMAN REVIEW Trajectories that pass automated thresholds undergo expert evaluation (HITL human-in-the-loop). Reviewers detect non-trivial over-reliance on hints and systematic failure modes (e.g., model consistently neglecting specific tools). They may regenerate thoughts, prune steps, or discard trajectories when warranted, focusing on quality aspects that cannot be reliably handled by algorithmic checks. Outcome. The result is scalable, HITL-augmented curation process that yields agentic demonstrations solving SOTA-model-breaking issues. By encoding both reasoning and interaction dynamics, these trajectories are expressly designed for fine-tuning code-capable language models. FINE-TUNING EXPERIMENT DETAILS IN 4.3 As in the main paper, we follow the SWE-Smith setup (Yang et al., 2025). Key hyperparameters: train_type full torch_dtype bfloat16 num_train_epochs 3 per_device_train_batch_size 1 learning_rate 5e-05 weight_decay 0.01 lr_scheduler_type cosine warmup_ratio 0.05 gradient_accumulation_steps 1 # 8 GPUs => effective batch-size = 8 max_length 32768 truncation_strategy 'right'"
        },
        {
            "title": "H CASE STUDIES IN FAILURE ANALYSIS OF SOTA MODELS",
            "content": "H.1 CASE 1 L T: MISINTERPRETING RAW VS. PREPROCESSED TOKENS Task Context Repository: adrienverge__yamllint-746 (https://github.com/adrienverge/yamllint/ pull/746) Language: Python Domain: Developer Tools Issue Type: Bug Fix (bug-report) Issue description from PR multiline: \"this is sentence cut into words\". On double-quoted multiline strings, quotes arent needed if e.g.: are needed when at should be /very/very/long/URL\". lines are broken on spaces, quotes least one line ends with backslash (), meaning the next spaces \"https://example.com/a/very/very multiline: removed:"
        },
        {
            "title": "But",
            "content": "SOTA Model Erroneous Reasoning Trace (claude-sonnet-4.5) The issue is in the _quotes_are_needed function. When this function checks if quotes are needed, it <string> and tries to parse it. The problem is that when the original creates test YAML string like key: string was multiline quoted string, the token.value contains the final parsed value (without backslashes), not the original form from the file. Let me create test to better understand how tokens work with multiline strings: . . . The model assumed tokens were preprocessed, leading to: # Quotes needed for multiline strings with line continuations (backslashes) if is_multiline_string: return True Diagnosis by Our Framework yamllint reads raw token buffers; backslashes are preserved. helper inspects raw buffers: def _has_backslash_on_at_least_one_line_ending(token): if token.start_mark.line == token.end_mark.line: return False buffer = token.start_mark.buffer[ token.start_mark.index + 1:token.end_mark.index - 1] return 'n' in buffer or 'rn' in buffer H.2 CASE 2 N E T N.I O: MISHANDLING MULTILINE REGEX AND ZERO-WIDTH MATCHES Task Context Repository: dgtlmoon__changedetection.io-2889 (https://github.com/dgtlmoon/ changedetection.io/pull/2889) Language: Python Domain: Applications Issue Type: Feature Request Issue description from PR Support multiline regex in text filtering. The current function iterates per line: for line in content.splitlines(keepends=True): 20 Rework to use re.finditer/re.findall over the whole content. SOTA Model Erroneous Reasoning Trace (Claude Sonnet 4.5) Tried span-overlap heuristics: # Check if this line overlaps with any ignored range for range_start, range_end in merged_ranges: if not (line_end <= range_start or line_start >= range_end): ignored_line_numbers.append(line_num) break Failed unit test for zero-width multiline regex /^$/ms$. Diagnosis by Our Framework 1. Evaluate regex over entire content with re.MULTILINE (and re.DOTALL as needed) using re.finditer. 2. Map match.start() to line numbers by counting n. 3. Avoid overlap heuristics; compute line indices from newline offsets. Zero-width spans break overlap checks; whole-content matching plus newline mapping fixes it."
        }
    ],
    "affiliations": [
        "Turing"
    ]
}