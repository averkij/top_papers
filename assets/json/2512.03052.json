{
    "paper_title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "authors": [
        "Zeqiang Lai",
        "Yunfei Zhao",
        "Zibo Zhao",
        "Haolin Liu",
        "Qingxiang Lin",
        "Jingwei Huang",
        "Chunchao Guo",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation."
        },
        {
            "title": "Start",
            "content": "LATTICE: Democratize High-Fidelity 3D Generation at Scale Zeqiang Lai1,2 , Yunfei Zhao2 , Zibo Zhao2 , Haolin Liu2 Qingxiang Lin2 , Jingwei Huang2 , Chunchao Guo2 , Xiangyu Yue1 1MMLab, CUHK 2Tencent Hunyuan https://lattice3d.github.io 5 2 0 2 4 2 ] . [ 1 2 5 0 3 0 . 2 1 5 2 : r Figure 1. High quality 3D assets generated by LATTICE from single image."
        },
        {
            "title": "Abstract",
            "content": "We present LATTICE, new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and wellestablished transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, semi-structured representation that compresses 3D assets Equal contribution. Corresponding authors. into compact set of latent vectors anchored to coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts two-stage pipeline: first generating sparse voxelized geometry anchor, then producing detailed geometry using recitified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering significant step toward scalable, high-quality 3D asset creation. 1 1. Introduction Creating high-quality 3D assets is central to modern content pipelines across visual effects, gaming, virtual reality, and industrial design. Yet, manual creation remains laborintensive and demands expert skills. Thus, automating 3D asset generation has become key challenge at the intersection of vision, graphics, and machine learning. Despite the impressive progress demonstrated by recent advances in 3D generation [48, 57, 59], the question of how to represent 3D assets remains the dark cloud over scalable 3D generation fundamental problem that continues to hinder progress in fidelity, efficiency, and generalization. This challenge is deeply intertwined not only with classical 3D representations such as meshes, point clouds, Signed Distance Functions (SDFs), radiance fields [29], and 3D Gaussian Splattings [15] but also with VAE [6] representations adopted by latent diffusion models [37], key paradigm underpinning the recent advances in 3D generation, as well as in image and video synthesis [17, 18]. Even with latent diffusionbased compression, the underlying complexity of 3D structure and its cubic growth in memory and computation remain major obstacles, underscoring the need for compact representations. As result, compression, reconstruction, and generation have become even more crucial in 3D learning, standing as longstanding themes throughout prior research. The pursuit of such an ideal 3D representation has thus led to what we call representation-centric research, primarily revolving around compression and reconstruction by two leading paradigms: Sparse Voxel and VecSet. Sparse Voxelbased methods [36, 48] aim for efficiency by restricting computation to the active voxels near the object surface. However, as reported in Trellis [48], even with the inherent sparsity of 3D data, the sequence length of active voxels can be expensively long for training (over 20,000 at 643 resolution), necessitating complex system designs based on sparse convolution [3] and attention mechanisms [4, 25], which leaves its scalability an open question. Nonetheless, the structured latent space provides strong flexibility for editing and broad generalization to diverse downstream tasks [5, 13, 20, 53]. VecSet-based approaches [56, 57, 59] offer more compact and elegant alternative by compressing 3D objects into small set of feature vectors via crossattention between densely sampled point cloud and sparsely sampled point queries set of point coordinates uniformly sampled in the object surface. Remarkably, as few as 3,072 vectors can already yield excellent reconstruction quality, making VecSet-based models highly efficient. Moreover, all operations within these models including the VAE and DiT components can be implemented using standard selfand cross-attention layers, enabling excellent scalability within modern transformer architectures. Despite the strong advances in compression and reconstruction, current Figure 2. Illustration of test-time scaling in our model. The model is trained with up to 6,144 tokens, but is evaluated under different token counts at test time, showing notable improvements. models remain notably behind 2D latent diffusion models in quality and scalability, leaving the enhancement of 3D generation capability an open and underexplored challenge. In this paper, we seek to answer central question in generation-centric perspective, i.e., What truly defines good representation for 3D diffusion generator itself? From this viewpoint, we start by asking why does 3D generation still significantly lag behind 2D in quality and scalability? At its core, the gap stems from fundamental difference in how the generative task is framed. In 2D image synthesis, the spatial grid is predefined models only need to infer RGB values at fixed pixel coordinates (a secret condition that greatly simplifies the denoising processes). 3D generation, however, faces far more open-ended task: it must discover both where to place content in space and what to represent there (e.g., SDF, RGB). This joint reasoning over structure and content dramatically expands the search space and introduces ambiguity, making optimization harder and scaling behavior less predictable1. At first glance, Sparse Voxel may seem promising choice due to its inherent spatial structure. While this holds true, we instead still prefer VecSet-based representation for its distinctive advantages we detailed later, more importantly, its strong capability for test-time scaling, as illustrated in Fig. 2. Building upon the insights discussed earlier, we demonstrate that it is possible to combine the best of both worlds through Localizable Code unified and high-level abstraction for any representation that tackles the joint reasoning problem. Crucially, it is localizability rather than structure that truly matters. Guided by this principle, our key idea is to add localizable structure to VecSet [56] latent codes. In other words, we aim to decouple the prediction of where and what, and guide the unstructured VecSet generation with structure/position, mimicking the success of image generation based on 2D grid. To achieve this, we investigate the positional information secretly encoded in VecSet latent produced by point queries, i.e., each latent is strongly correlated with regions 1See Appendix for more discussions. 2 that LATTICE exhibits strong superiority against previous state-of-the-art models, and is distinguished by several key strengths, as summarized below: Test-time scaling. Our model exhibits strong test-time scaling effect. The model trained with up to 6144 tokens/voxel cells can be directly scaled to up to 30720 tokens during the test time, with consistent improvement. Low-cost training. Our base model, with 2 billion parameters, can be effectively trained in under 24 hours using 64 GPUs, while still significantly outperforming previous methods. Simplicity. The model architecture is exceptionally simple relying solely on pure Transformer design, without any complex or sparse components. Exceptional performance. Our model achieves significantly strong performance in 3D generation, excelling in geometry smoothness and detail preservation. LATTICE represents significant step forward in nextgeneration 3D assets generation, bridging the gap between generated and handcrafted 3D assets. We hope this work offers valuable insights into effective scaling of 3D generation models and opens up new possibilities for automated, high-fidelity 3D content creation. 2. Related Works 2.1. 3D Representations Unlike images and videos, which are universally represented by pixel colors, 3D assets exhibit wide variety of representations tailored to different application contexts. Common atomic representations include voxels, point clouds, Signed Distance Fields (SDF), polygon meshes, DMTet [39], Flexicube [40], Neural Radiance Fields (NeRF) [29], and Gaussian Splatting [15], among others. These representations, whether explicit or implicit, serve distinct roles in the 3D industryfor example, point clouds are prevalent in perception tasks like autonomous driving [34], NeRF excels in novel view rendering, and polygon meshes remain the standard for gaming and realtime applications. These atomic representations can often be converted between one anotherfor example, polygon meshes can be extracted from SDFs via the marching cubes algorithm [26]. Ultimately, the choice of representation is task-dependent and directly influences network design, e.g., autoregressive models [45] for meshes, and diffusion models [60] for SDF. Nonetheless, even lightweight and flexible representations such as implicit functions still impose significant modeling and computational burdens on deep neural networks, especially diffusion models [37] the current golden paradigm for 3D generation. As result, the latent representations of 3D assets have emerged as new research focus, aiming to enhance efficiency. These representations, Figure 3. LATTICE system: At its core is novel VoxSet representation, enabling scalable 3D modeling from 0.6B to 4.5B. near the position of its corresponding point query, as hinted in [19]. However, this information can hardly help during the generation as the positions of point queries are unknown at test time. Therefore, we introduce voxel queries as perfect replacement of point queries. Instead of utilizing point coordinates on the surface, we use the center coordinates of the active voxel intersecting the objects surface. These active voxel grids can be very coarse, thus can be easily obtained, during the test time, by voxelizing an existing geometry generated by any off-the-shelf geometry generation models [14, 48] with less perfect quality. As result, our first improvement leads to VoxSet, new semi-structured representation that inherits the efficiency and simplicity of VecSet [56], while introducing structure into its latent space. This design brings several unique benefits: (1) it enables flexible encoding and decoding in abritrary resolution, which makes training particularly cheap as multi-stage training is possible by pretraining on very low token size and progressively scaling the tokens up; (2) every latent in VoxSet is structured, anchored in 3D regular voxel grid so that the position information can be directly injected into the diffusion transformer (DiT) [32] through positional embedding [42, 44], which provides strong guidance during diffusion generation and is proven to be essential in model scaling in our experiment. Based on VoxSet, we present LATTICE, general framework designed to generate high-fidelity and detailed 3D assets. LATTICE employs two-stage pipeline. In the first stage, it generates sparse voxel grid by voxelizing coarse mesh produced by any off-the-shelf model, such as Hunyuan3D-2 [59] or Trellis [48]. In the second stage, it generates geometry VoxSets at arbitrary resolutions (number of tokens) within the selected voxel grid. Built on rectified flow transformer [18] and progressive training strategy, we train family of large-scale image-to-3D generation models with up to 4.5 billion parameters, as shown in Fig.3, capable of producing detailed meshes from single image. Through extensive evaluation, we demonstrate 3 with difference by their own, can be generally categeoried into three popular types, i.e., (1) VecSet, represented by 3DShape2VecSet [56], compress 3D shapes into 1D latent sets; (2) Triplane, represented by Direct3D [12], compress shapes into three orthogonal features planes; and (3) Sparse Voxel, represented by XCube [36], converted 3D assets into features anchored on sparse voxels. long-standing belief holds that the spatial locality of sparse voxel representations helps preserve fine details, whereas VecSet representations, despite their efficiency, tend to lose details due to their global modeling. In this paper, we challenge this idea and identify that the key for 3D generative models lies not in locality, but in well-known structure at test time. Here, we introduce VoxSet, semi-structured latent representation that combines efficiency and strong expressiveness. 2.2. Geometry Generation 3D geometry generation has advanced rapidly in recent years. Early works [38, 46, 50, 55] based on different generative models [7, 16, 31] demonstrated the preliminary potential for generating specific categories of geometry. With the rise of diffusion models [11, 37], 3D geometry generation methods based on score distillation [33] have been introduced, enabling text-to-3D generation by leveraging text-to-image models. Feedforward methods such as LRM [12], Hunyuan3D 1.0 [52], and LGM [43] represent another line of research focused on generating 3D assets in single step. On the other hand, autoregressive models, e.g. MeshGPT [41], BPT [45], and Meshtron [8] have become popular for mesh generation with human-like topology. Recently, native 3D diffusion models have significantly improved generation quality by utilizing 3D data. Notable examples include Michelangelo [58], CLAY [57], Hunyuan3D 2.0 [59], TripoSG [23], and Step1X-3D [22], building on 3DShape2VecSet [56]. Despite great success, these methods seem to struggle at generating highly detailed meshes. On the contray, another line of research, following XCube [36], shows promising results by works as Trellis [48], Hi3DGen [54], SparseFlex [9], Sparc3D [24] and Direct3D-s2 [47]. Nevertheless, we show that it is effective scaling through localizable guidance, rather than VecSet or XCube, that matters in detailed geometry generation. 3. Scalable 3D Generative Modeling The goal of LATTICE is to explore new paradigm for scalable 3D generative modeling. To achieve this, key design choice in our approach is utilizing coarse geometry structure as strong guidance for detailed geometry generation. This design, despite being used in many voxel-based approaches, such as XCube [36] and SLAT [48], is significantly underestimated. In this work, we show that it is actually essential for effective model scaling and performance improvement, no matter what the representation is. Figure 4. Illustrations of different latent representations and different query types. 3.1. VoxSet Representation Underlying the architecture of LATTICE, it is VoxSet representation that builds up the core of the entire system. Existing 3D representationssuch as meshes, point clouds, signed distance field (SDF), NeRF [29], FlexiCubes [40], VecSet [56], and SLAT [48]can be broadly categorized into two types. The first includes explicit or implicit atomic representations, such as point clouds, SDFs, and NeRF, which directly encode geometry or appearance. The second includes latent representations, such as VecSet and SLAT, which are built upon atomic representations with variational autoencoders (VAE) [6, 16] and tailored for building compact latent space in latent diffusion models [37]. VoxSet is latent representation guided by two key principles: scalability and structural latent space. To support scalability, VoxSet compresses any 3D asset into sequence of latent tokens via cross-attention mechanism, following the design of 3DShape2VecSet [56]. Formally, given 3D object, we employ VAE to encode its point cloud representation and reconstruct the corresponding SDF, from which surface mesh can be extracted via the Marching Cubes algorithm [26]. The input point cloud RN 7 captures multiple attributes per point, where denotes the total number of points and each point encodes its 3D coordinates, surface normal, and binary sharpness indicator marking whether it lies on sharp edge. Following the strategy in Hunyuan3D2 [59], the point cloud is constructed by combining uniform sampling over the surface with importance sampling around sharp edges to better preserve high-frequency details. Efficient Scaling via Sparsity. The latent represen4 Figure 5. LATTICE Model Architecture: it features two-stage coarse-to-fine pipeline and novel VoxSet VAE and DiT. tation, i.e., token sequence, is obtained via performing cross-attention between the input point cloud and set of query tokens following series of self-attention layers. The decoder is designed symmetrically, where the SDF grid coordinates serve as queries of the cross-attention against latent tokens. Notably, these latent tokens, in fact, secretly encode the global signals; thus we could represent any 3D object with latent sequence of any length [2, 57, 59]. This is particularly useful as progressive token scaling (a more fine-grained strategy than progressive resolution scaling) is possible to greatly reduce the training cost, e.g., starting pretraining from 1024 tokens and progressively increasing to more. Even more, as evidenced in FlashVDM [19], diffusion model trained on 512 latent tokens can be directly scaled up to 3072 tokens in test-time with better performance, which makes VoxSet particularly economical. Voxel Queries for Detail Modeling. The choice of query set is crucial design choice in VecSet-like methods [56, 58], which also serves as key distinguishing factor of VoxSet. In 3DShape2VecSet [56], two types of query set, i.e., learnable queries and point queries, are proposed. The learnable queries encode the global statistics and are easy to train, but are limited in scaling up for better reconstruction and generation performance. Point queries are downsampled point clouds with furthest point sampling, which encode local information around the queries and support encoding-decoding at arbitrary resolution, favoring lowcost progressive scale-up. Moreover, the locality of latent tokens encoded by point queries is very strong and correlated with their position, as discussed in FlashVDM [19]. In other words, the latent set from point queries is ordered with position information secretly encoded. However, none of existing models [23, 57, 59] utilize the information. One of the biggest obstacles is that point queries are sampled on the object surface, whose positions are unknown during test time. To address this problem, we introduce Voxel Queries, query set anchored at the center of active voxels intersecting with the object surface, as shown in Fig. 4. Voxel Queries are not sampled on the surface but on coarse voxel grid, so that their position can be easily obtained during test time by coarse structure generation stage. Besides, the voxel center is decorrelated with different surfaces, reducing the training-test gap and greatly improving the generalization capabilities at test time. 3.2. Detailed Geometry Generation As shown in Fig. 5, we introduce two-stage pipeline to fit the proposed VoxSet representation for generating geometry with ultimate details. The first stage generates the coarse sparse structure given the input image by voxelizing the results of off-the-shelf pretrained 3D generators [59]. The second stage generates sparse voxel latents anchored at the voxel centers of the previous sparse structure. Semi-Structured Geometry VAE. We adopt the proposed VoxSet representation to train semi-structured geometry VAE. As illustrated in Fig. 4, our method combines the strengths of VecSet [56] and SLAT [48], keeping latent tokens compact and structural. To support multiresolution voxel structures, instead of randomly sampling voxel queries at various resolutions, we propose simpler 5 2R , 1 approach that supports arbitrary resolutions. Specifically, we jitter the point queries during training by adding small random offset ϵ (cid:2) 1 (cid:3), where is the smallest 2R resolution we aim to support. At test time or during diffusion training, voxel queries can be sampled at any resolution greater than R. The other aspects of our VAE are the same as in Hunyuan3D-2 [59], except we only sample queries from uniformly sampled point cloud. Adding Structure to Diffusion Transformer. Following Hunyuan3D-2 [59], we utilize rectified-flow transformer to generate the VoxSet. Instead of solely conditioning on the input image, we propose to utilize the structure of VoxSet by adding rotary positional embedding (RoPE) [42] to each noisy latent token. This change, despite being inconspicuous at first glance, is crucial in improving model convergence. The reason behind this can be two-fold, firstly, the amount of available 3D data is much smaller than 2D image and video counterparts, which makes the latent space severely unoccupied. Secondly, geometry generation is drastically different and more difficult task than image generation due to its sparsity, i.e., the 3D geometric surface occupies only small portion of its bounding box while every pixel in the image has an RGB value. As result, previous approaches [57, 59] that only use single image as condition could hardly guide the denoising trajectory towards detailed geometry. To reduce the training cost, we introduce two simple strategies: (1) instead of utilizing all structure tokens, we randomly sampled fixed number of tokens during the training, which is much smaller than sparse voxel methods [48]; (2) we adopt progressive training strategy by first training on 1024 tokens and progressively scaling up to 6144 tokens. Image Conditioning. Following Hunyuan3D-2 [59], we use Dinov2-Giant [30] for image conditioning, taking the last hidden layer embedding without the class token. Different from Hunyuan3D-2s 518 resolution, we use 1022 for finer details. The object is cropped via binary mask while keeping the aspect ratio to reduce token length. No extra positional embedding is added, as Dino already encodes sufficient spatial information. Training and Test-time Scaling. We train several models in different model sizes, ranging from 0.6B to 4.5B. As shown in Fig. 6, our model exhibits stable scaling effect the bigger the better. Moreover, our model surprisingly reveals test-time scaling effect in token length as shown in Fig. 6. Even though our model is trained on 6144 tokens, we could increase the number of tokens to 12288, 24576, and even more by sampling more voxel queries. 3.3. Applications Thanks to the flexible design of the proposed architecture. We could adapt our model for various tasks."
        },
        {
            "title": "Mesh Refinement can be extended to a broader context",
            "content": "Figure 6. Illustration of model/training and test scaling effects. Figure 7. Illustrations of applications of LATTICE. Mesh refinement in the left and part refinement in the right. where the image is not aligned or missing, such as controllable generation or part refinement [51] as in Fig.7. Mesh Editing is also possible by manipulating the voxel queries and latent features of the given mesh, using the idea of Repaint [27], MastControl [1], etc. 4. Experiments 4.1. Reconstruction We adopt two metrics for evaluating geometry reconstruction performance, including Chamfer Distance (CD) and Fscore with threshold of 0.001. To evaluate the reconstruction accurately, we use points-to-surface distances to calculate the metrics, with the mesh normalized to the range [-1, 1]. Similar to Dora [2], we construct benchmark containing more challenging, detailed assets as LATTICEBench(R). The competing methods consist of (1) representative VecSet-based methods: Hunyuan3D-2 [59]; and (2) Voxel-based methods: SparseFlex [9], and Direct3Ds2 [47]. The numerical comparison is shown in Tab. 1, the metrics are multiplied by 104 and 102. Our method delivers 6 Figure 8. Visual comparison of geometry generation against several state-of-the-art open-source methods. top performance with much more compact latent representation than voxel-based methods. 4.2. Generation. We evaluate the image-to-geometry generation through various metrics including ULIP [49], Uni3D [61] for text-mesh and image-mesh similarities, following Hunyuan3D-2 [59]. We compare our method against (1) open-source methods, Michelangelo [58], Craftman 1.5 [21], Trellis [48], Hunyuan3D-2 [59], Hi3DGen [54], and Direct3D-s2 [47]; (2) closed-source methods, which we denote as Model 14. The numerical comparison is shown in Tab. 2, omitting closed-source methods as obtaining large amount of their results is very expensive. We compare LATTICE-1.9B, which is the closest size to other models. It can be observed that our method achieves the best performance. Fig. 8 and Fig. 9 demonstrates the visual comparison, which confirms the superiority of our method. 4.3. Evaluation Effect of Voxel Queries. To assess the effectiveness of the proposed voxel queries, we compare three DiTs, each uti7 SparseFlex [9] Hunyuan3D-2 [59] N/A 64 4096 N/A 64 8192 8 48557 512 1024 8 196028 Direct3D-s2 [47] 1024 64 46592 LATTICE (Ours) N/A 64 4096 N/A 64 8192 N/A 64 Method ResLatent Size CD() F1() 82.78 91.57 90.94 97.76 97.46 95.31 98.53 99.59 12.35 9.157 8.020 2.972 4.987 5.321 2.909 1.893 Table 1. Quantitative comparisons of geometry reconstruction. Michelangelo [58] Craftsman 1.5 [21] Trellis [48] Hunyuan3D 2.0 [59] Hi3DGen [54] Direct3D-s2 [47] LATTICE-1.9B Method ULIP-T ULIP-I Uni-T Uni-I 0.261 0.298 0.311 0.315 0.299 0.314 0.315 0.213 0.237 0.249 0.251 0.246 0.247 0. 0.075 0.074 0.076 0.077 0.066 0.074 0.078 0.115 0.129 0.126 0.130 0.112 0.122 0.130 Table 2. Numerical comparison of geometry generation performance on ULIP [49] and Uni3D [61] similarities. Figure 9. Visual comparison against commercial models. Figure 10. Ablation study on the proposed voxel query and VoxSet VAE, by incrementally adding each component. lizing different VAEs and query types, as shown in Fig. 10. All DiTs were trained for 200k steps (100k on 1024 tokens and 100k on 3072 tokens). The results indicate that voxel queries produce fewer artifacts, benefiting from reduced domain gap, and the VoxSet VAE introduces more detail 8 Figure 11. User study of our method against competitors showing win rate (%) across Overall, Subject, and Scene categories. Res- + Fixed Train Baseline 128 64 128 64 CD() 10.7 7.72 6.42 5.73 F1() 85.3 91.4 92.9 94.5 256 5.69 94.7 + Query Jitter 256 128 64 6.03 5.32 5.36 93.7 95.3 95. Table 3. Ablation study of VAE training strategies. All settings are tested with 4096 tokens and voxel queries. thanks to better reconstruction capability. Effect of Query Jitter. Voxel queries are essential for bridging the gap between training and testing results in the first stage. To evaluate their impact, we ablate several VAEs with voxel queries by assessing their reconstruction performance. The numerical comparison is presented in Tab. 3. As shown, the original point-query VAE suffers significant degradation when tested with voxel queries. In contrast, the Query Jitter VAE outperforms VAEs trained at fixed resolution and offers greater flexibility when applied to varying resolutions. User Study. We also conducted user study to assess human preferences across different methods. As shown in Fig. 11, our method was compared to four commercial models. The results clearly show that our method significantly outperforms the others. 5. Conclusion We have presented LATTICE, novel framework that advances 3D asset generation by introducing Voxset, semistructured latent representation. By conditioning on localizable position information, we address key challenges in computational complexity, scalability, and fidelity for diffusion generation. Our method demonstrates superior performance in generating high-quality meshes, achieving stunning detail, smoothness, and sharpness. With its flexible encoding, low-cost training, and strong test-time scaling, LATTICE represents significant step forward in the automated generation of scalable, high-fidelity 3D content. A. Discussions A.1. Scaling Behavior on Different Architectures We observe that 3D generation architectures differ substantially in their scaling behavior. Models without explicit localizable guidance (e.g., VecSet-based) scale much less effectively than those equipped with it, such as our proposed VoxSet architecture. Here, we provide thorough analysis and accompanying demonstrations. Model Scaling on Parameters. Our first observation is that VecSet models hardly benefit from an increased number of parameters. To investigate this, we compare three popular VecSet models of different sizes: Hunyuan3D-2-mini [19] (0.6B), Hunyuan3D-2 [59] (1.1B), and Hunyuan3D-2.1 [14] (3B). The visual comparison is shown in Fig. 12 (top row). Surprisingly, the three results are largely similar, with Hunyuan3D-2-mini (0.6B) even showing slightly better performance. This suggests that increasing model size may not play decisive role for VecSetbased architectures. In contrast, we find that the proposed VoxSet models consistently benefit from increased model size. To demonstrate this, we train three models of different sizes (0.6B, 1.9B, and 4.5B). The generated results are shown in Fig. 12 (bottom row). It can be observed that as the number of parameters increases, the outputs exhibit more details, with sharper, smoother, and more regular structures. Overall, these results demonstrate the effectiveness of the proposed VoxSet architecture, revealing that proper conditioning is key factor in unlocking the benefits of model scaling. In other words, larger models are useful only when there is clear correspondence between conditions and outputs; otherwise, increasing the number of parameters provides little advantage, as the model is more good at memorizing rather than abstracting and reasoning. From the data perspective, the difference in scaling behaviors could also be explained by the lack of large-scale 3D data. We need more data to cover mappings with higher degrees of freedom, such as those with fewer conditions. Test Time Scaling on Tokens. Previously, we demonstrated that our model exhibits strong test-time scaling effect on shape tokens in Sections 1 and 3.2. Specifically, the model trained with maximum token length of can be directly evaluated using 2N, 3N, or even more tokens during inferenceno additional training or configuration is required. Similar to model scaling, we here compare the test-time scaling behaviors of VecSet models and our VoxSet models, with results presented in Fig. 13. The VecSet model was trained with 4096 tokens, while our VoxSet model was trained with 6144 tokens. We evaluated their generation performance at token lengths of 1N, 2N, and 4N. As observed, the VecSet model benefits slightly from scaling tokens to 2N (e.g., improved round structures on the body), but further scaling yields negligible gains. In conFigure 12. Illustration of the effect of model scaling (in parameters) on performance. VecSet models show limited improvement as parameters increase, whereas larger VoxSet models produce finer and more detailed results. trast, our VoxSet model consistently benefits from increased token counts: it shows improvements when scaling from to 2N, and further gains when scaling from 2N to 4N. Notably, more tokens directly translate to richer details. In general, the token scaling capability of both VecSet and VoxSet models originates from their ability to perform arbitrary-resolution autoencoding. During VAE training, we randomly sample queries across the entire 3D voxel grid or object surfacethis process acts as form of random dropout for full-token training. As result, our VAE inadvertently acquires the ability to encode objects with any token length, even though this property was not an explicit design goal. The same principle applies to DiT training: the random selection of queries also endows DiT with testtime scaling capability. Notably, VoxSet exhibits more pronounced scaling effect, which can be attributed to its stronger correspondence between query locations and content. During training, the transformer learns to model the relationship between spatial positions and the tokens that should be generated at those positions. A.2. Representation Centric to Generation Centric In this paper, our model challenges two common takes, which is largely from the view of representation, including (1) Global vs Local: VecSet is global that is better for overall shape and sparse voxel is local which is more suitable for details, and (2) Structural Sparse Grid vs Unstructural Set: the sparse voxel is structural which is better for editing and other downstream tasks, VecSet is unstructural for these tasks. 9 (1.9B), and an XXL model (4.5B). Unlike CLAY [57], we do not adopt progressive model scaling; instead, all models are trained from scratch. Instead, we employ multi-stage token scaling strategy. Within each stage, we use constant learning rate with linear warm-up, while gradually decreasing the base learning rate across stages from 1 104 to 1 106. The batch size is maximized to fit GPU memory, reaching up to 2048 in our experiments. We utilize ZeRO-based optimizer, gradient, and parameter partitioning from DeepSpeed [35] to efficiently train large models on distributed GPU cluster. All models are trained using the flow matching objective with linear coupling plan, following the formulation in SiT [28]. Additionally, to enable classifier-free guidance [10], we randomly replace conditioning embeddings with zero embeddings at probability of 10% during training. Data Preparation. Our data processing pipeline mainly includes three steps, (1) data filtering; (2) watertighting; (3) point-cloud sampling and SDF extraction. We apply extensive data filtering to improve the quality of the dataset, which includes removing AI-generated assets, scanned assets, extreme complex scenes, and assets with plane. We randomly sample millions of point cloud during the training, and split them into chunks to accelerate data loading. C. Post-Training High-Quality Finetuning. We introduce an additional finetuning stage with very high-quality data, which helps in improving details generation. All parameters in DiT are updated with small learning rate. We filter the high-quality data by combination of criterions including the number of faces, the number of sharp edges, and reconstruction quality, resulting in roughly 15k samples. Model Acceleration. We adopt FlashVDM [19] to accelerate the geometry VAE decoding. For diffusion sampling, benifit from strong structure guidance from RoPE, we find that our models are inherent few-step generators. Nevertheless, we still perform guidance distillation and step distillation to further reduce the sampling cost. D. User Study Setting To evaluate the perceptual quality of the generated results, we conducted large-scale user study consisting of approximately 500 questions. For each question, three independent participants were asked to rank all the presented results according to their visual quality or fidelity. Each question displayed outputs from all compared methods in randomized order to ensure fairness. Finally, we aggregated the rankings across all participants and questions to compute the winning rate of each method, which reflects its overall preference by human evaluators. Figure 13. Illustration of the effect of test-time scaling (in shape tokens) on model performance. VecSet models exhibit limited gains as the number of tokens increases, showing early saturation. In contrast, VoxSet models consistently benefit from higher token counts, producing finer details and demonstrating stronger scaling capability. * indicates the token count used during training. Localizable Code is All You Need. It is evident that we can combine the strengths of both approaches through the proposed VoxSeta semi-structured set-based representation that encodes global information. Nevertheless, we wish to emphasize that while these two common perspectives hold true to some extent, critical nuances remain. From generation standpoint, the key to better generation performance lies in localizability: specifically, strong guidance that is accessible during test time. Locality may offer advantages for compression efficiency and reconstruction quality, but it is not the primary factor for generation tasks. Regarding structure, sparse voxels are inherently structural, property that benefits many tasks. However, this does not mean VecSet lacks structure. In fact, VecSet inherently contains structural information. Its only limitation is that this structure cannot be identified during test time. Thus, we propose VoxSet to circumvent this issue. B. Implementation Details Training Setup. To evaluate the scaling effect of the proposed architecture, we train several models of various sizes, including medium model (0.6B parameters), an XL model Figure 14. More visual results for image-to-geometry generation of LATTICE. E. More Results Fig. 14 shows additional results of our model. All examples are presented without cherry-picking."
        },
        {
            "title": "References",
            "content": "[1] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 6 [2] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. arXiv preprint arXiv:2412.17808, 2024. 5, 6 [3] Spconv Contributors. Spconv: Spatially sparse convolu11 tion library. https://github.com/traveller59/ spconv, 2022. 2 [4] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. [5] Paul Engstler, Aleksandar Shtedritski, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Syncity: Training-free generation of 3d worlds. arXiv preprint arXiv:2503.16420, 2025. 2 [6] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 4 [7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 4 [8] Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024. 4 [9] Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. Sparseflex: High-resolution and arbitrary-topology 3d shape modeling. arXiv preprint arXiv:2503.21732, 2025. 4, 6, 8 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In arXiv preprint arXiv:2207.12598, 2022. 10 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [12] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 4 [13] Binbin Huang, Haobin Duan, Yiqun Zhao, Zibo Zhao, Yi Ma, and Shenghua Gao. Cupid: Pose-grounded generative 3d reconstruction from single image. arXiv preprint arXiv:2510.20776, 2025. 2 [14] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to highfidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. 3, 9 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [16] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [17] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [18] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3 [19] Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qinxiang Lin, Jinwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, and Xiangyu Yue. Unleashing vecset diffusion model for fast shape generation, 2025. 3, 5, 9, 10 [20] Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, and Lu Sheng. Voxhammer: Training-free precise and coherent 3d editing in native 3d space. arXiv preprint arXiv:2508.19247, 2025. 2 [21] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner, 2024. 7, [22] Weiyu Li, Xuanyang Zhang, Zheng Sun, Di Qi, Hao Li, Wei Cheng, Weiwei Cai, Shihao Wu, Jiarui Liu, Zihao Wang, et al. Step1x-3d: Towards high-fidelity and controllable generation of textured 3d assets. arXiv preprint arXiv:2505.07747, 2025. 4 [23] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape arXiv synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025. 4, 5 [24] Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, and Bihan Wen. Sparc3d: Sparse representation and construction for high-resolution 3d shapes modeling. arXiv preprint arXiv:2505.14521, 2025. 4 [25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 2 [26] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 3, 4 [27] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Repaint Van Gool. Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1146111471, 2023. [28] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 10 [29] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3, 4 [30] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, 12 Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 6 [31] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):164, 2021. [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 3 [33] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 4 [34] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification In Proceedings of the IEEE conference and segmentation. on computer vision and pattern recognition, pages 652660, 2017. 3 [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 10 [36] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42094219, 2024. 2, 4 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 4 [38] Aditya Sanghi, Hang Chu, Joseph Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1860318613, 2022. [39] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [40] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM Transactions on Graphics (TOG), 42(4):116, 2023. 3, 4 [41] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1961519625, 2024. 4 rotary position embedding. Neurocomputing, 568:127063, 2024. 3, 6 [43] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 4 [44] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [45] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. 3, 4 [46] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural information processing systems, 29, 2016. 4 [47] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Philip Torr, Xun Cao, et al. Direct3d-s2: Gigascale 3d generation arXiv preprint made easy with spatial sparse attention. arXiv:2505.17412, 2025. 4, 6, 7, 8 [48] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 2, 3, 4, 5, 6, 7, 8 [49] Le Xue, Mingfei Gao, Chen Xing, Roberto Martın-Martın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11791189, 2023. 7, 8 [50] Xingguang Yan, Liqiang Lin, Niloy Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Shapeformer: Transformer-based shape completion via sparse representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62396249, 2022. [51] Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, et al. X-part: high fidelity and structure coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025. 6 [52] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 4 [53] Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, YanPei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025. 2 [42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with [54] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 3:2, 2025. 4, 7, 8 [55] Fukun Yin, Xin Chen, Chi Zhang, Biao Jiang, Zibo Zhao, Jiayuan Fan, Gang Yu, Taihao Li, and Tao Chen. Shapegpt: 3d shape generation with unified multi-modal language model. arXiv preprint arXiv:2311.17618, 2023. 4 [56] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. 2, 3, 4, 5 [57] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 4, 5, 6, 10 [58] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. 4, 5, 7, 8 [59] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 2, 3, 4, 5, 6, 7, 8, 9 [60] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. Locally attentional sdf diffusion for controllable 3d shape generation. ACM Transactions on Graphics (ToG), 42(4):113, 2023. 3 [61] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023. 7,"
        }
    ],
    "affiliations": [
        "MMLab, CUHK",
        "Tencent Hunyuan"
    ]
}