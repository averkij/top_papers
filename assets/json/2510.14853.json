{
    "paper_title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models",
    "authors": [
        "Guinan Su",
        "Yanwu Yang",
        "Li Shen",
        "Lu Yin",
        "Shiwei Liu",
        "Jonas Geiping"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \\textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite."
        },
        {
            "title": "Start",
            "content": "REWIRING EXPERTS ON THE FLY: CONTINUOUS REROUTING FOR BETTER ONLINE ADAPTATION IN MIXTURE-OF-EXPERT MODELS Guinan Su1* Yanwu Yang4* Li Shen5 Lu Yin6 Shiwei Liu1,2,3 1Max Planck Institute for Intelligent Systems 4University of Tubingen 5Sun Yat-sen University 6University of Surrey 2ELLIS Institute Tubingen 3Tubingen AI Center Jonas Geiping1,2,3 5 2 0 O 6 1 ] . [ 1 3 5 8 4 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose data-free, online test-time framework that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves 5.5% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-andplay property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6% average gains when incorporated with self-consistency on DeepSeek-V2-Lite."
        },
        {
            "title": "INTRODUCTION",
            "content": "Mixture-of-Experts (MoE) models (Shazeer et al., 2017; Zhou et al., 2022; Jiang et al., 2024; Dai et al., 2024; Liu et al., 2024; Team, 2025; Muennighoff et al., 2024) provide an effective approach to scaling model capacity while maintaining computational efficiency by partitioning parameters into specialized experts and selectively activating subsets through routing mechanisms (Lepikhin et al., 2020; Fedus et al., 2022; Dai et al., 2024; Muennighoff et al., 2024). This functionality enables dynamic expert selection for diverse queries and creating inherently general-purpose systems that can store much more functionality and information than is used in every forward pass. However, despite their impressive capabilities, MoE models still face challenges when deployed in real-world environments (Akyurek et al., 2024; Li et al., 2025a), as the expression of their capability hinges on the quality of their routing decisions, the activations of small linear layers that determine which parts of the model are activated. Why is routing hard? While the full MoE may store sufficient functionality to solve particularly challenging query, this capacity is gated behind its routing decisions in each MoE layer, which in turn depend on the residual stream of the model, and so on earlier routing decisions. Routing decisions are only linear functions of the current hidden state that need to approximate the anticipated utility of activating certain expert. particular issue with this non-robustness of routing is that during standard inference there is no mechanism to reinforce the routing to particularly successful part of the model, or to reduce routing to parts that did not contribute meaningful signal to the generated text. * Equal contribution. guinan.su@tuebingen.mpg.de 1 Figure 1: Test-time rerouting framework for MoE models. (a) Rerouting mechanism: lightweight additive vectors (Delta) update router logits in selected high-confidence layers using self-supervised loss from existing context. (b) Continuous adaptation: alternating between optimization phases that adapt routing decisions and generation phases that maintain adapted routing until the next optimization cycle. This makes routing optimization question of model adaptation, reminiscent of neuroplasticity in humans who continuously optimize routing and neuronal connections in the brain through adaptation and self-regulation, for example when continuing to practice certain task. Yet, in MoEs, suboptimal expert selection leads to routing inefficiencies, creating critical bottleneck in overall performance (Shi et al., 2024; Li et al., 2025a). And, while there has been significant amount of work to improve the capabilities of MoEs in general, such as through test-time scaling, it is less clear how to best adapt MoEs to different tasks at inference. While standard approaches, such as in-context learning with task-specific demonstrations (Wei et al., 2022; Madaan et al., 2023) or parallel generation strategies that produce multiple candidates and aggregate them (Wang et al., 2022; Brown et al., 2024), do adapt the model overall, they influence routing only implicitly. Conceptually, adaptation should be solvable by test-time training, but existing approaches (Hardt & Sun; Hubotter et al., 2024) focus on classical prediction perspective, which retrieves relevant data from training sets or knowledge bases during inference to fine-tune models for dynamic scenarios before the model is being used. Li et al. (2025a) attempt to address router optimization through this perspective of test-time training, and, as such, use retrieval of successful neighbors from reference sets based on the first prompt in each context. However, this approach requires access to external reference data during deployment, incurs retrieval overhead, and risks failures in retrieval due to short prompts. Moreover the approach is static, as modern models execute test-time scaling, they generate long chains-of-thought, that should be taken into account when optimizing routing. In this regard, we propose simple yet effective data-free test-time rerouting framework for MoE models as shown in Figure 1 that treats each input prompt as self-supervised learning opportunity, enabling dynamic rerouting of pretrained MoE models for individual prompts during inference. The framework alternates in two phases: (1) In-Context Routing Optimization, where we regard the current context itself as training sample and execute optimization steps to minimize cross-entropy loss on the current context with regard to routing logits; and (2) Steered Generation, where we generate text normally, steering routers with updates computed in the previous phase. This creates dynamic feedback loop where the model continuously refines its understanding of task requirements based on its own generation progress, enabling increasingly informed expert routing decisions as generation proceeds. To maintain computational efficiency, we implement this progressive optimization through lightweight additive parameter vectors that update only the models router logits of selected MoE layers, further reducing computational overhead and preventing over-adaptation. Overall, our main contributions are: We propose test-time rerouting framework specifically designed for MoE models, operating completely without external data dependencies or expensive retrieval mechanisms, using only backpropagation within the current context. We introduce lightweight parameter update mechanism that selectively optimizes router logits through additive vectors in high-confidence layers, enabling efficient expert selection adaptation during inference using steering. 2 We validate our methods effectiveness through extensive experiments, demonstrating that our approach significantly improves MoE model performance on complex reasoning tasks, maintains robustness to context shifts in multi-turn conversations, and seamlessly integrates with existing test-time techniques, establishing practical and versatile solution for deploying MoE models in real-world applications. Overall, we argue that routing changes are compelling mechanism with which to adapt MoE models on the fly. The changes are lightweight, quick to compute and to apply (even in multi-user settings), and remain effective across context shifts, allowing MoEs novel degree of plasticity that allows adaptation to task changes during deployment, paving the way toward practical continual self-regulation of MoE models during use."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Mixture-Of-Experts (MoE). Sparsely activated Mixture-of-Expert models (MoE) are an efficient method for scaling model size by replacing the feed-forward networks (FFN) of transformer architectures with multiple experts (each its own FFN) and gating function. This architecture dynamically activates different experts for each input token rather than utilizing all parameters (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022). Recent MoE-based LLMs, including OLMoE (Muennighoff et al., 2024), DeepSeek (Dai et al., 2024; Liu et al., 2024), and Qwen2.5 (Team, 2025), adopt top-k expert routing to reduce the active parameter count during inference. These models achieve competitive performance while maintaining significantly lower memory costs. Test-Time Scaling. Test-Time Scaling enhances LLM capabilities by allocating additional computational resources during inference. These approaches fall into two categories (Welleck et al., 2024): Parallel generation, including self-consistency evaluations via multiple candidate responses (Wang et al., 2022), best-of-N sampling (Brown et al., 2024), and Monte Carlo Tree Search (Zhou et al., 2023; Xie et al., 2024), and sequential generation such as extending outputs through chain-ofthought reasoning (Wei et al., 2022; Madaan et al., 2023). Test-Time Training (TTT). TTT offers an alternative scaling approach. While successful in computer vision (Wang et al., 2020; Sun et al., 2020; 2024; Gandelsman et al., 2022; Osowiechi et al., 2023), recent works have extended TTT to language models through fine-tuning on retrieved neighbors (Hardt & Sun) or optimized data selection algorithms like SIFT (Hubotter et al., 2024). TestTime Reinforcement Learning (TTRL) (Zuo et al., 2025) uses majority voting as reward signals. When applied to Mixture-of-Experts (MoE) models, recent work has explored expert-level interventions for behavior control, enabling precise modifications through targeted expert manipulation (Dahlke et al., 2025; Wang et al., 2025). Most relevant to our approach, Li et al. (2025a) optimizes expert routing in MoE models using successful neighbors from reference sets. However, these methods assume accessible training data during deployment and introduce significant retrieval overhead, limiting real-world practicality."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "This section presents our data-free test-time rerouting framework for MoE models that optimizes expert routing during inference without external information. After reviewing MoE fundamentals (Section 3.1), we introduce three key components: (1) Router Logits Modification (Section 3.2) for layer-specific expert selection steering, (2) Dynamic Layer Selection (Section 3.3) for selective MoE layer updates based on confidence scores, and (3) Optimization Procedure (Section 3.4) detailing our two-phase strategy alternating between in-context routing optimization and steered generation. 3.1 PRELIMINARIES ON MIXTURE-OF-EXPERTS In Transformer-based Mixture-of-Experts (MoE) models, the conventional Feed-Forward Networks (FFNs) are replaced with MoE layers. Each MoE layer consists of router and set of experts {Ei}N i=1. Given an input sequence x<t = (x1, x2, . . . , xt1), the router assigns each token to subset of experts for processing. Given tokens hidden state Rd at layer l, the router computes logits across all experts: z(l) = (l) RN is the routers weight matrix at h(l) where (l) 3 layer l, and z(l) RN represents the logits for expert selection. These logits are then converted to expert selection probabilities: w(l) = Softmax(z(l)) where w(l) represents the activation probability for expert Ei at layer l. The router applies routing strategy (e.g., top-k routing) to select active experts. Weights for unselected experts are zeroed and the remaining weights are renormalized to ˆw(l). The final MoE output is: o(l) = (cid:80) = 0} denotes the set of activated experts at layer l. Ei(h(l)) where A(l) = {j ˆw(l) iA(l) ˆw(l) i"
        },
        {
            "title": "3.2 ROUTER LOGITS MODIFICATION",
            "content": "l=1 where δ(l) RN corresponds to the We introduce layer-specific adaptation parameters {δ(l)}L experts at MoE layer l. For selected layer l, we modify the router logits by adding the corresponding layer-specific parameter: z(l) = z(l) + δ(l) where z(l) RN represents the modified logits for layer l. The expert selection probabilities are then computed as: w(l) = Softmax(z(l)) = Softmax(z(l) + δ(l)) This modification directly influences the expert selection distribution, allowing the model to adapt routing decisions based on prompt characteristics. (1) 3.3 DYNAMIC LAYER SELECTION To mitigate computational overhead and prevent over-adaptation, our framework selectively updates router logits of only subset of MoE layers rather than all layers simultaneously. We hypothesize that layers with higher routing confidence indicate more decisive and task-relevant expert selection, making them more impactful for adaptation. We define router confidence at layer for token as: (n) = 1 (cid:88) j=1 log p(n) i,j (2) where p(n) is the probability of the j-th top expert at layer for token n, and is the number i,j of activated experts per layer. Higher confidence values indicate more decisive routing decisions, suggesting these layers play more critical roles for the current task. To obtain layer-level confidence across the generated sequence, we aggregate token-level confidence: Ci = 1 (cid:88) n=1 (n) (3) where is the number of generated tokens so far. We implement two layer selection strategies: (1) Hard selection that selects top-r proportion of layers with highest confidence scores: St = TopK({Ci}L i=1, r), and (2) Soft weighting that assigns confidence-based weights to control the update strength of δ(l): wi = Ci j=1 Cj (cid:80)L (4) (5) During gradient updates, δ(l) is updated with learning rate scaled by wl. 3.4 OPTIMIZATION PROCEDURE Parameter Initialization: For each MoE layer L, initialize routing adjustment parameters: δ(l) = 0 RN The framework alternates between two phases: Phase 1: In-Context Routing Optimization. Given current context = (x1, . . . , xt), first select l=1, r) or compute soft weights {wl}L layers using = TopK({C (l)}L l=1. Then perform optimization steps, where at each step we compute the cross-entropy loss: L({δ(l)}L l=1) = t1 (cid:88) i=1 log p(xi+1 x1:i, {δ(l)}L l=1) (6) 4 and update parameters using optimizer (e.g., SGD, Adam): δ(l) O(δ(l), δ(l)L) O(δ(l), wlδ(l) L) δ(l) if (hard selection) if soft weighting otherwise (7) Phase 2: Steered Generation. Generate tokens using optimized routing parameters {δ(l)}L l=1. After generating tokens, return to Phase 1 with extended context including both original prompt and all generated tokens up to the current position and repeat the optimization procedure. The detailed algorithm is provided in Algorithm 1."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "Benchmarks We evaluate our approach using diverse set of benchmarks across multiple reasoning domains. For general knowledge assessment, we employ MMLU-redux(Gema et al., 2024), utilizing generation mode rather than multiple-choice format to encourage deeper reasoning before providing final answers. For code generation tasks, we use HumanEval (Chen et al., 2021) and MBPP-sanitized (Austin et al., 2021). For mathematical reasoning, we evaluate on GSM8K (Cobbe et al., 2021) and MATH500 (Lightman et al., 2023). Baselines We compare our method with two adaptation techniques: In-Context Learning (ICL) (Wei et al., 2022; Madaan et al., 2023) and C3PO (Li et al., 2025a). Since our method is datafree sequential generation test-scaling approach, we select In-Context Learning (ICL) as primary baseline, using 3 and 5 sample pairs respectively, for comparison. We also compare against C3PO, for which we select reference set of 100 samples for each dataset. Note that parallel generation methods represent an orthogonal approach to ours, and we further discuss the potential integration of our method with such approaches in later sections. Model Selection We evaluate three MoE LLMs: OLMoE (Muennighoff et al., 2024), Qwen1.5MoE (Team, 2024), and DeepSeek-V2-Lite (Liu et al., 2024). OLMoE employs 16 layers with 64 experts per layer, activating 8 experts per token (6.9B total, 1.3B active parameters). Qwen1.5MoE-A2.7B, incorporates 4 shared experts alongside 60 routing experts with 4 activated per token (14.3B total, 2.7B active parameters). DeepSeek-V2-Lite uses 28 layers with 2 shared and 64 routed experts, activating all shared plus 6 routed experts per token (16B total, 2.4B active parameters). Optimization We optimize the adaptation parameters using the Adam optimizer with small number of iterations (T = 5) to maintain computational efficiency. The optimization hyperparameters are set as follows: learning rate η = 0.05, weight decay = 1 108, and epsilon = 1 105. All adaptation parameters are initialized to zero: δ(l) = 0. We use the soft-weighting strategy for layer selection, and during the generation stage, we periodically re-optimize the parameters every 128 generated tokens using identical hyperparameter settings to ensure continuous refinement throughout the sequence generation process."
        },
        {
            "title": "5 RESULTS FOR MOE REWIRING",
            "content": "5.1 MAIN RESULTS We first evaluated the performance of our method on challenging reasoning benchmarks. As shown in Table 1, our approach consistently improves performance over all baselines across five benchmarks. On the HumanEval task in particular, our method yields gains of 3.6%, 5.5%, and 6.7% on DeepSeek-V2-Lite, OLMoE, and Qwen1.5-MoE, respectively. Moreover, compared to other calibration methods such as few-shot learning and C3PO, our method achieves consistently better results. Notably, it surpasses these baselines without relying on example demonstrations, delivering particularly strong improvements in code generation and mathematical reasoning tasks. Moreover, in contrast to C3PO, which requires 100 reference samples, our data-free method attains superior performance while requiring no additional reference data. 5 Table 1: Model Performance Comparison Across Different Benchmarks, comparing to in-context learning (ICL) and C3PO Li et al. (2025a). Rewiring shows strong performance even though no external data is used, either as references or as fewshot examples. Method HumanEval MBPP GSM8K MATH500 MMLU Average Baseline ICL (3-shot) ICL (5-shot) C3PO (100-reference) Rewiring (Ours, 0-shot) Baseline ICL (3-shot) ICL (5-shot) C3PO (100-reference) Rewiring (Ours, 0-shot) Baseline ICL (3-shot) ICL (5-shot) C3PO (100-reference) Rewiring (Ours, 0-shot) DeepSeek-V2-Lite 58.37 56.81 52.53 59.92 62.65 OLMoE 40.08 36.96 39.30 41.25 42.80 72.10 71.10 71.57 68.80 73.62 51.48 43.82 44.96 52.08 52.99 Qwen1.5-MoE 46.69 46.25 44.75 45.40 47.47 54.73 55.27 52.31 50.33 56. 50.60 52.44 53.05 47.80 54.26 28.66 28.05 21.21 28.05 34.17 40.24 44.34 43.90 39.70 46.95 22.60 21.60 22.20 18.20 25.00 10.40 9.90 9.40 13.00 12.40 21.80 22.20 19.40 18.90 22. 50.77 44.33 46.07 45.77 52.40 37.17 44.90 43.63 37.30 38.30 45.27 45.60 46.03 44.82 45.87 50.89 49.26 49.08 48.10 53.59 33.56 32.73 31.70 34.33 36.13 41.75 42.73 41.28 39.83 43. Table 2: Ablation Study: Layer Selection Strategies and Continuous Refinement. HumanEval MBPP GSM8K MATH500 MMLU Average Method Baseline Random Selection Reverse Metric Last-five Layers All Layers 50. 58.37 72.10 Layer Selection Strategies 48.78 48.18 50.60 48.17 53.31 55.26 55.64 56.42 Continuous Refinement 72.50 72.10 71.85 73.09 22.60 50. 50.89 23.40 21.89 25.20 24.60 23.80 25.00 50.87 49.33 50.33 51.10 51. 52.40 49.77 49.35 50.72 50.68 52.55 53.59 W/o Continuous Rewiring (Complete Method) 54.27 54.27 59.53 62.65 73.37 73. 5.2 ABLATIONS OF KEY COMPONENTS To validate the effectiveness of our design choices for MoE rewiring, we conduct ablation studies on using DeepSeek-V2-Lite, examining two key components: layer selection strategies and continuous refinement mechanisms. We evaluate performance across five tasks, with results presented in Table 2. Confidence-based layer selection efficiently discovers task-specific layers. We compare our confidence-based selection with several baselines, including random selection, selecting the last five layers, selecting low-confidence layers (Reverse Metric), and optimizing all layers. As shown in Table 2, our method achieves an average performance of 53.59%, outperforming random selection (49.77%), the last-five-layer strategy used in C3PO (50.72%), and reverse selection targeting lowconfidence layers (49.35%). This demonstrates that router confidence identifies layers with strong expert selection knowledge. In effect, we use the signal from prior context to confirm that routing choices were accurate, and that the model should rely on these experts more strongly. Updating Single Layers is Safer than rerouting the whole model. In addition, from the results we can see that updating all layers simultaneously (50.68% on average) underperforms our selective approach (53.59% on average), highlighting the superiority of our method in selecting layers for test-time rerouting. With limited optimization steps available, spreading updates across all parameters dilutes the refinement signal and risks overadaptation that destabilizes pre-trained routing patterns. Concentrating the optimization budget on high-confidence layers maximizes impact by amplifying existing routing strengths rather than attempting comprehensive corrections across the entire network. 6 Continuous Refinement Stabilizes Routing. The comparison between continuous (53.59% on average) and non-continuous refinement (52.55% on average) reveals meaningful performance gap of over 1%. It is notable that this gap carries weight, given that each benchmark example involves single task and prompt-based router updates already constitute strong baseline. This improvement indicates that routing optimization benefits from curriculum-like approaches that gradually refine the models expert selection mechanisms, leading to more stable and effective adaptations during test-time optimization."
        },
        {
            "title": "6.1 WHY DOES TEST-TIME REROUTING WORK?",
            "content": "To understand the mechanisms behind test-time rerouting, we perform comprehensive analysis of the DeepSeek-V2-Lite model, examining pathway shifts, expert utilization dynamics, and the evolution of routing confidence. Figure 2: Analysis of test-time rerouting mechanisms across different datasets in DeepSeek-V2-Lite. a) Edit distance before and after rerouting. b) Expert utilization dynamics before and after rerouting. Rewiring improves Expert Pathways. We first examine how expert pathways change after rerouting using edit distance following (Li et al., 2025b) (described in Section B.2), which quantifies pathway differences using Levenshtein edit distance. This captures mismatches in expert selection, and pathway shifts. We track the average pairwise edit distance across samples during rerouting. The results are displayed in Figure 2 a), which shows distinct layer-wise editing patterns. In HumanEval, edit distances peak in deeper layers (1822, >0.35) while earlier ones (510) remain minimal (<0.05). MATH500 exhibits similar but stronger trend, with peaks up to 0.16 in layers 2022, suggesting that deeper layers are more involved in mathematical reasoning. The sample-wise distributions (right panels) further emphasize task-specific differences, indicating high variability in pathway changes across problems. Overall, these results highlight the heterogeneity of expert routing across tasks and demonstrate how our method adaptively reroute pathways. Rewiring highlights Task-Specific Experts. We further examine expert-level activation shifts on HumanEval and MATH500 before and after rerouting to understand how the model redistributes computation. Figure 2 b) shows heatmaps across layers and experts (red: increased, blue: decreased, normalized for visualization). The results reveal: (1) Adaptive targeting. changes concentrate on subset of experts, indicating strategic rather than uniform redistribution; (2) Deep-layer adaptation. later layers (L21L25) undergo stronger shifts, consistent with our edit-distance findings; and (3) Task-specific specialization. HumanEval and MATH500 exhibit distinct patterns, highlighting taskdependent routing behaviors. This selective activation shows that rerouting strategically emphasizes task-relevant experts, enhancing efficiency by focusing computation on where it is most needed. Rewiring Increases Router Confidence. Moreover, we quantify routing confidence by tracking the entropy of expert-selection distributions across all layers during optimization. Lower entropy reflects more focused expert allocation, whereas higher entropy indicates more diffuse routing patterns. Figure 3 shows that our method (red line) exhibits gradual decrease in router entropy over 18 generation steps, whereas the baseline (blue line) maintains higher router entropy with pronounced fluctuations. This suggests that our approach progressively develops more focused routing decisions, while the baseline continues to rely on more diffuse expert-selection patterns. This indicates that our method facilitates concentrating on relevant experts without disrupting established routing mechanisms, thereby enabling more efficient expert utilization for the given tasks. Table 3: Computational efficiency comparison across different methods. Method Total FLOPs Time (s) Baseline ICL(3-shot) ICL(5-shot) Self-Consistency (3) C3PO(100-reference) Rewiring (Ours) 4.71e+11 1.93e+12 3.05e+12 1.68e+12 2.63e+12 1.96e+12 10.71s 12.17s 12.53s 34.20s 26.20s 20.12s Figure 3: Expert routing entropy as function of sequence length averaged over 16 token blocks. 6.2 COMBINING REWIRING WITH OTHER TEST-TIME STRATEGIES key advantage of our approach is its plug-and-play compatibility with existing test-time techniques. Since our method only modifies routing decisions without altering the underlying generation process, it can be seamlessly integrated with other approaches like in-context learning and parallel generation methods. Synergy with In-Context Learning. Large pretrained language models demonstrate strong incontext learning, predicting labels from few demonstration pairs without parameter updates. Empirical evidence demonstrates behavior similar to explicit finetuning (Dai et al., 2022; Akyurek et al., 2022; Von Oswald et al., 2023). As such, we test combining our method with with 3-shot demonstrations. As shown in Table 4, we observe that this combination yields improvements across multiple tasks, even surpassing our method alone. We hypothesize this synergy occurs because our method provides more effective gradient updates that better leverage the contextual information from demonstration examples, helping extract more meaningful patterns from the limited demonstration data. Unlocking Better Self-Consistency. Self-consistency (Wang et al., 2022) improves reasoning by sampling multiple paths and aggregate together. We combine our method with self-consistency by applying rerouting optimization, then generating multiple reasoning paths with optimized routing decisions. For each sample, we generate 3 reasoning paths. For MMLU, MATH500, and GSM8K, we use majority voting to determine the final accuracy. For code generation tasks (MBPP, HumanEval), we report pass@3 scores. shows substantial improvements with an average 3 percentage point gain over self-consistency alone. We hypothesize that our rerouting framework generates higher-quality reasoning chains by selecting more appropriate experts, and when self-consistency aggregates these improved paths, the voting mechanism amplifies the benefits. 6.3 EFFICIENCY ANALYSIS Beyond performance improvements, we also compare the computational cost of our method with other online approaches to adapt models. Table 3 reports the results on the HumanEval task with the DeepSeek model, showing the average total FLOPs and inference time per sample across different methods. As shown in Table 3, our method achieves notable computational savings over most baselines. Although it requires more computation than the vanilla baseline, it remains substantially more efficient than other test-time techniques, using 1.3 fewer FLOPs than C3PO, 1.6 fewer than ICL (5-shot), and comparable to ICL (3-shot) and Self-Consistency (3). Despite the extra routing opera8 Table 4: Performance comparison of individual Test-Time methods versus combined approaches. Online optimization of routing decisions with our rewiring algorithm can be reliably combined iwth other techniques, such as in-context learning (ICL) or self-consistency. Method Baseline Rewiring (Ours) ICL (3-shot) ICL + Rewiring HumanEval MBPP GSM8K MATH500 MMLU Average 50.60 54.26 58.37 62. 72.10 73.62 In-Context Learning 52.44 53.05 56.81 62.65 71.10 76.00 Self-Consistency 22.60 25.00 21.60 27.00 26.20 27.40 50.77 52.40 50.89 53.59 44.33 46. 49.26 53.05 51.87 54.20 54.88 57.09 Self-Consistency (3) Self-Consistency (3) + Rewiring 51.02 55.08 70.04 71. 75.28 77.54 Table 5: Performance of our method versus the baseline on AIME datasets using the GPT-OSS-20b model. Optimizing rerouting is especially noticeable at improving model certainty, as shown by improved performance at lower pass@k and improved majority voting, even one challenging benchmarks like AIME. Data / Method AIME25 Pass@k Maj@k 2 4 8 2 4 Average Baseline Rewiring (Ours) 83.21 83.81 87.90 86.19 90.00 86.67 65.36 67. 71.76 76.67 76.67 83.33 74.29 75.65 AIME24 Baseline Rewiring (Ours) 78.57 81. 83.76 84.95 86.67 86.67 60.60 65.83 67.48 72.57 70.00 80.00 69.58 73. tions, our method maintains competitive inference time of 20.12s, indicating that the overhead of online adaptation is modest while still delivering improved performance. Conceptually, our method requires additional prefill passes on already generated text every tokens, which, given the ease of parallelization of prefill is manageable compute increase. In this work, we implement this optimization in straightforward manner, but production implementation could be significantly faster by incorporating the optimization into disaggregated-prefill systems, or timing the MoE rewiring event with low-load timespans, for example when waiting for user to respond. The actual routing changes are self-contained in routing parameters δ (shaped as number of experts by number of layers), so that on-device storage is feasible, even for many separate conversations. 6.4 EXTENSION TO LONG-REASONING TASKS To evaluate the Generalizability of our approach to modern reasoning models, we also test GPT-OSS-20B (Agarwal et al., 2025). We evaluate on the AIME benchmark (MAA Committees), challenging mathematics competition dataset that requires sophisticated multi-step derivations and mathematical reasoning. This setting allows us to assess whether our online rerouting method can improve performance on the demanding reasoning tasks that represent the current frontier of AI capabilities. As shown in Table 5, our method improves performance on both AIME datasets, achieving higher average correctness (75.65% vs. 74.29% on AIME25; 73.75% vs. 69.58% on AIME24). The primary improvements occur in Maj@k metrics rather than Pass@k, suggesting our routing optimization enhances reasoning consistency rather than solution diversity. 6.5 ROBUSTNESS TO CONTEXT SHIFTS IN MULTI-TURN SCENARIOS. In real-world applications, MoE models often encounter multi-turn conversations where contexts shift dramatically between different topics or tasks. To assess the robustness of our method under realistic multi-turn contexts, we simulate context shifts by prepending few-shot examples from dif9 Figure 4: Performance of our method versus the baseline across different few-shot examples under shifted and aligned task contexts. ferent domains before the target task. Figure 4 presents results on HumanEval and Math500 using DeepSeek-V2-Lite, under two conditions: (1) Aligned-task, where few-shot examples are from the same task domain, and (2) Shifted-task, where examples are drawn from unrelated domains (e.g., MATH and MMLU for code tasks) to induce cross-domain shifts. Across both benchmarks, our method (Ours-Shifted, Ours-Aligned) consistently outperforms the baselines (Base-Shifted, Base-Aligned). On HumanEval, all methods benefit from more-shot examples, but our approach shows stable improvement. In contrast, baseline gains remain modest, particularly in the Shifted-task setting, where performance fluctuates. On Math500, our method maintains robust and consistent results across both domains, while baselines exhibit limited or even inconsistent improvements. These findings highlight the robustness and scalability of our test-time rerouting strategy in handling diverse contextual information"
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduce novel test-time rerouting approach that enables MoE models to dynamically adapt expert selection on the fly, without requiring external data or costly retrieval. The method alternates between routing optimization and steered generation, forming feedback loop that progressively improves expert selection. To reduce computational overhead, we employ lightweight additive vectors that update only the logits of selected routers. Extensive experiments show that our approach effectively compensates for the inherent imperfections in MoE routing, yielding consistent gains across multiple benchmarks (up to 6.7% on code generation) with 1.6 fewer FLOPs than few-shot methods, while maintaining robustness to context shifts. As plugand-play regularization strategy, the method flexibly combines with complementary techniques (e.g., Self-Consistency) to further amplify the benefits. More importantly, by introducing new dimension of plasticity into MoEs, it opens the door to deployment-time adaptation and points toward practical continual self-regulation in MoE models."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "JG acknowledges the support of the Hector II foundation. GS acknowledges the support of the International Max Planck Research School for Intelligent Systems (IMPRS-IS)."
        },
        {
            "title": "REFERENCES",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. 10 Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. Ekin Akyurek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for abstract reasoning. arXiv e-prints, pp. arXiv2411, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Robert Dahlke, Henrik Klagges, Dan Zecha, Benjamin Merkel, Sven Rohr, and Fabian Klemm. Mixture of tunable expertsbehavior modification of deepseek-r1 at inference time. arXiv preprint arXiv:2502.11096, 2025. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559, 2022. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020. Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:2937429385, 2022. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini. Are we done with mmlu?, 2024. Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models, 2024. URL https://arxiv. org/abs/2305.18466. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv. org/abs/2103.03874, 2, 2024. Jonas Hubotter, Sascha Bongni, Ido Hakimi, and Andreas Krause. Efficiently learning at test-time: Active fine-tuning of llms. arXiv preprint arXiv:2410.08020, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 11 Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Zhongyang Li, Ziyue Li, and Tianyi Zhou. C3po: Critical-layer, core-expert, collaborative pathway optimization for test-time expert re-mixing. arXiv preprint arXiv:2504.07964, 2025a. Ziyue Li, Chenrui Fan, and Tianyi Zhou. Where to find grokking in llm pretraining? monitor memorization-to-generalization without test. arXiv preprint arXiv:2506.21551, 2025b. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. arXiv preprint arXiv:2405.04434, 2024. MAA Committees. AIME problems and solutions. https://artofproblemsolving.com/ wiki/index.php/AIME_Problems_and_Solutions. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060, 2024. David Osowiechi, Gustavo Vargas Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ismail Ben Ayed, and Christian Desrosiers. Tttflow: Unsupervised test-time training with normalizing flow. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 21262134, 2023. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Chufan Shi, Cheng Yang, Xinyu Zhu, Jiahao Wang, Taiqiang Wu, Siheng Li, Deng Cai, Yujiu Yang, and Yu Meng. Unchosen experts can contribute too: Unleashing moe models power by selfcontrast. Advances in Neural Information Processing Systems, 37:136897136921, 2024. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pp. 92299248. PMLR, 2020. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2, 2024. Qwen Team. Qwen3: Think deeper, act faster, 2025. URL https://qwenlm. github. io/blog/qwen3/. Accessed, 5(10), 2025. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 3515135174. PMLR, 2023. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 12 Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, et al. Two experts are all you need for steering thinking: Reinforcing cognitive effort in moe reasoning models without additional training. arXiv preprint arXiv:2505.14681, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A EXPERIMENTAL SETTINGS",
            "content": "A.1 BENCHMARKS MMLU-redux is manually re-annotated subset of the original MMLU benchmark designed to address quality issues in the dataset. The dataset contains 3,000 questions across 30 MMLU subjects (100 questions per subject), with expert annotators identifying and categorizing various types of errors using comprehensive error taxonomy. These errors include issues such as bad question clarity, unclear options, no correct answers, multiple correct answers, and wrong ground truth labels. MMLU-Redux provides more reliable evaluation standard by filtering out problematic questions and offering corrected annotations where possible. HumanEval is benchmark dataset for evaluating code generation capabilities of large language models. The dataset consists of 164 hand-crafted programming problems, each including function signature, docstring, body, and several unit tests (averaging 7.7 tests per problem). These challenges assess language comprehension, algorithms, and simple mathematics, with difficulty comparable to simple software interview questions. MBPP is code generation benchmark consisting of around 1,000 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem includes task description, code solution, and 3 automated test cases, covering programming fundamentals and standard library functionality. The dataset provides two versions: full version with 974 problems and sanitized version with 427 problems. The sanitized split underwent second round of annotation to improve task descriptions, addressing issues where the original descriptions might not be sufficiently expressive to solve the tasks. This hand-verified subset provides higher-quality problem statements for more reliable evaluation of code generation models. In this paper, we use the sanitized version. 13 GSM8K is dataset of 8,500 high-quality, linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,473 training problems and 1,319 test problems. Each problem takes between 2 and 8 steps to solve using basic arithmetic operations, with problems designed so that bright middle school student should be able to solve every problem. Solutions are provided in natural language format rather than pure mathematical expressions, offering insight into multi-step reasoning processes. MATH-500 is curated subset of 500 challenging mathematical problems selected from the MATH dataset (Hendrycks et al., 2024). These problems span seven topics: Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. Each problem requires multi-step reasoning and is designed to test models ability to apply mathematical principles, execute complex calculations, and communicate solutions clearly. A.2 BASELINES C3PO We adopt C3PO (Critical-Layer, Core-Expert, Collaborative Pathway Optimization) (Li et al., 2025a) as our primary baseline method. C3PO is test-time optimization approach designed to address the suboptimal expert pathway selection problem in Mixture of Experts (MoE) large language models. The method operates on the observation that end-to-end trained routers often produce inefficient pathways for challenging or out-of-distribution samples, leading to degraded performance on diverse downstream tasks. The core idea of C3PO is to dynamically re-mix expert pathways during inference by leveraging successful routing patterns from reference set. Given reference set of samples {(xi, yi)}m i=1 i=1 (where each ωi RLE with layers with their corresponding expert pathway matrices {ωi}m and experts) on which the model makes correct predictions, C3PO aims to find an improved pathway matrix ω for new test sample that leads to more accurate outputs. Among the three optimization strategies proposed by C3PO (gradient descent, kernel regression, and mode finding), we implement the kernel regression approach in our experiments. This method estimates optimal expert pathways by computing weighted average of neighbors pathway matrices: ˆω = (cid:80) iN (x) K(xi, x)ωi (cid:80) iN (x) K(xi, x) (8) where K(, ) is kernel function measuring sample similarity, and (x) denotes the neighborhood of in the reference set. The final pathway is obtained through interpolation: ω αω + (1 α)ˆω (9) where α is optimally chosen to minimize the loss function. For our comparative evaluation, we construct reference sets for each benchmark as follows: For HumanEval and MBPP, we use the MBPP validation set; for GSM8K and MATH500, we utilize the GSM8K training set; and for MMLU, we employ the MMLU training set. From each source, we randomly sample 100 instances where the model produces the correct predictions, ensuring highquality pathway references for the optimization process. In-Context Learning In-Context Learning (ICL) is fundamental capability of large language models that enables them to adapt to new tasks by using few demonstration examples provided in the input prompt, without requiring parameter updates (Wang et al., 2022; Wei et al., 2022). In our experimental setup, we implement ICL as baseline across all evaluation benchmarks. The few-shot examples are carefully selected from relevant datasets to ensure domain alignment and high-quality demonstrations: HumanEval and MBPP: We sample 3-5 examples from the MBPP prompt collection, which provides well-crafted code generation examples with clear problem descriptions and corresponding Python solutions. 14 GSM8K: We utilize 3-5 examples from the GSM8K training set, featuring step-by-step mathematical reasoning demonstrations that guide the model through problem-solving processes. MATH500: We sample 3-5 examples from the HYDRA-Math dataset, which offers highquality mathematical problem-solution pairs covering various difficulty levels and mathematical domains. MMLU: We select 3-5 examples from the MMLU validation set, ensuring coverage of diverse knowledge domains while maintaining format consistency for multiple-choice questions. For each benchmark, the few-shot examples are randomly sampled from their respective source datasets and prepended to each test query. This approach provides the model with task-specific context while maintaining consistency across different evaluation scenarios. The number of examples (3-5) is chosen to balance between providing sufficient context and avoiding excessive prompt length that might degrade model performance. A.3 MODEL SELECTION OLMoE is fully open-source MoE language model developed by the Allen Institute for AI and Contextual AI (Muennighoff et al., 2024). The model employs decoder-only transformer architecture with 1 billion active and 7 billion total parameters. Each MoE layer contains 64 experts, of which 8 are activated per input token through learned router network. This sparse activation mechanism enables computational efficiency similar to dense 1B parameter models while leveraging the full 7B parameter capacity. DeepSeek-V2-Lite is smaller variant of the DeepSeek-V2 model family, developed by DeepSeekAI (Liu et al., 2024). The model employs innovative architectures, including Multi-head Latent Attention (MLA) and DeepSeekMoE, with 15.7 billion total parameters and 2.4 billion activated parameters per token. DeepSeek-V2-Lite has 27 layers with hidden dimension of 2048, utilizing MLA with 16 attention heads, where each head has dimension of 128. The model adopts the DeepSeekMoE architecture, where all feed-forward networks except the first layer are replaced with MoE layers. Each MoE layer consists of 2 shared experts and 64 routed experts, with 6 experts activated for each token. This sparse activation mechanism enables computational efficiency while maintaining strong performance across diverse tasks. Qwen1.5-MoE is developed by the Qwen team (Team, 2024). The model is upcycled from the dense Qwen-1.8B model, featuring 14.3 billion total parameters with 2.7 billion activated parameters during runtime. Despite using only 2.7B active parameters, the model achieves comparable performance to Qwen1.5-7B while requiring 75% fewer training resources and demonstrating 1.74x faster inference speed. The model employs fine-grained expert architecture with 64 experts total, consisting of 4 shared experts that are always activated and 60 routing experts with 4 activated per token. This configuration represents an 8-fold increase in expert count compared to conventional MoE setups, enabling higher model capacity without proportional parameter increases. The finegrained expert design partitions single FFN into multiple segments, each serving as an individual expert, allowing for more specialized knowledge representation. GPTOSS-20B is OpenAIs recently released open-source MoE model with 21B total parameters but only 3.6B active at inference (Agarwal et al., 2025). Built on Mixture-of-Experts architecture with 24 layers and 32 experts using Top-4 routing, it represents state-of-the-art reasoning model that achieves performance similar to OpenAI o3-mini on reasoning benchmarks. Its sophisticated expert routing system and focus on mathematical reasoning make it an ideal candidate for evaluating our test-time rerouting method on challenging tasks like AIME."
        },
        {
            "title": "B METHOD DETAILS",
            "content": "B.1 TEST-TIME MOE REROUTING We elaborate the rerouting algorithmic pipeline in Algorithm 1, which details the step-by-step procedure for adaptive expert selection and pathway calibration at test time. 15 B.2 PATHWAY DIFFERENCES We first examine how expert pathways change after rerouting using edit distance following (Li et al., 2025b). For input xi, we define its pathway si as the ordered sequence of selected experts across MoE layers as si = concat(e(i) represents expert indices at layer ℓ as comma-separated strings (e.g., 3,1,5), joined across layers with hyphens. We quantify pathway differences using Levenshtein edit distance as Dpath(si, sj) = EditDistance(si, sj). This captures mismatches in expert selection, and pathway shifts. ), where e(i) 2 , . . . , e(i) 1 , e(i) ℓ"
        },
        {
            "title": "C ADDITIONAL ANALYSIS",
            "content": "C.1 LAYER-WISE CONFIDENCE DISTRIBUTIONS ACROSS DIFFERENT TASKS We visualize the confidence distributions across different tasks using DeepSeek-V2-lite as an example. As illustrated in Figure 5, activation patterns across experts and layers are highly task-specific. For instance, math tasks (Math500) and code tasks (HumanEval) exhibit distinct confidence patterns across layers, with math tasks showing higher confidence in middle layers (layers 7-14) while code tasks demonstrate more distributed confidence patterns with peaks in later layers (layers 1518). Notably, both tasks show generally higher confidence concentrated in the middle-to-later layers compared to early layers. Figure 5: Layer-wise confidence distributions across different tasks in DeepSeek-V2-lite-MoE C.2 SENSITIVE ANALYSES OF HYPER-PARAMETERS As shown in Figure 6, the optimization interval analysis across five benchmarks shows that our testtime rerouting consistently outperforms baseline performance. Intervals of 128-160 tokens achieve optimal performance, balancing routing adaptability with computational efficiency. Both frequent re-optimization (96 tokens) and no updates degrade performance due to over-adaptation and loss of dynamic adjustment benefits, respectively. Mathematical reasoning tasks (GSM8K, MATH500) benefit most from proper interval tuning. C. IMPACT ON SAMPLE DIVERSITY IN PARALLEL GENERATION. Table 6: Diversity evaluation results comparison Metric Baseline Ours Cosine Div. Semantic Div. 0.390 0.157 0.012 0.007 0.380 0.150 0.012 0.008 16 Figure 6: Effect of optimization interval on performance across 5 benchmarks. The dashed line denotes the baseline results without rerouting. Impact on Sample Diversity in Parallel Generation. While our online adaptation method improves routing efficiency, we investigate whether the adapted routing decisions might reduce sample diversity when generating multiple sequences in parallel. To quantify this effect, we evaluate the diversity of generated samples using two complementary metrics: semantic diversity based on CodeBERT embeddings (Feng et al., 2020), which captures functional similarities between code snippets, and TF-IDF-based cosine diversity, which measures lexical variation. We used the DeepSeek MoE model with default settings to generate 10 samples on the HumanEval task. Table 6 shows that our method maintains comparable diversity to the baseline across both metrics. Cosine diversity scores are nearly identical (0.3800.150 vs. 0.3900.157), as are semantic diversity scores (0.0120.008 vs. 0.012 0.007). These results demonstrate that our online adaptation preserves sample diversity while improving routing efficiency. Algorithm 1 Test-Time MoE Rerouting Input: Pre-trained MoE model M; generation interval m; optimizer O. Output: Generated response y. input prompt = (x1, . . . , xn); optimization steps n; // Parameter Initialization Initialize δ(l) = 0 RN for all layers L; l=1, r) or compute soft weights {wl}L l=1; j=1 log p(xj+1 x1:j, {δ(l)}L l=1); l=1) = (cid:80)T 1 // Phase 1: In-Context Routing Optimization xcurrent = x, = x; Select layers = TopK({C (l)}L for = 1 to do Compute loss L({δ(l)}L if hard selection then for do δ(l) O(δ(l), δ(l)L); end for else // soft weighting for = 1 to do δ(l) O(δ(l), wlδ(l) L); end for end if end for l=1); // Phase 2: Steered Generation with Periodic Re-optimization Initialize = (); repeat // Generate tokens using optimized routing parameters for = 1 to do Generate xnext p( xcurrent, {δ(l)}L Append xnext to and xcurrent; end for // Re-optimize with extended context = xcurrent; Select layers = TopK({C (l)}L for = 1 to do Compute loss L({δ(l)}L if hard selection then for do δ(l) O(δ(l), δ(l)L); end for else // soft weighting for = 1 to do δ(l) O(δ(l), wlδ(l) L); end for end if end for until end-of-sequence or max length reached; l=1) = (cid:80)T l=1, r) or compute soft weights {wl}L l=1; j=1 log p(xcurrent,j+1 xcurrent,1:j, {δ(l)}L l=1); return y;"
        }
    ],
    "affiliations": [
        "ELLIS Institute Tubingen",
        "Max Planck Institute for Intelligent Systems",
        "Sun Yat-sen University",
        "Tubingen AI Center",
        "University of Surrey",
        "University of Tubingen"
    ]
}