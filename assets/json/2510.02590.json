{
    "paper_title": "Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning",
    "authors": [
        "Ahmed Hendawy",
        "Henrik Metternich",
        "Théo Vincent",
        "Mahdi Kallel",
        "Jan Peters",
        "Carlo D'Eramo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The use of target networks is a popular approach for estimating value functions in deep Reinforcement Learning (RL). While effective, the target network remains a compromise solution that preserves stability at the cost of slowly moving targets, thus delaying learning. Conversely, using the online network as a bootstrapped target is intuitively appealing, albeit well-known to lead to unstable learning. In this work, we aim to obtain the best out of both worlds by introducing a novel update rule that computes the target using the MINimum estimate between the Target and Online network, giving rise to our method, MINTO. Through this simple, yet effective modification, we show that MINTO enables faster and stable value function learning, by mitigating the potential overestimation bias of using the online network for bootstrapping. Notably, MINTO can be seamlessly integrated into a wide range of value-based and actor-critic algorithms with a negligible cost. We evaluate MINTO extensively across diverse benchmarks, spanning online and offline RL, as well as discrete and continuous action spaces. Across all benchmarks, MINTO consistently improves performance, demonstrating its broad applicability and effectiveness."
        },
        {
            "title": "Start",
            "content": "Preprint. Under Review. USE THE ONLINE NETWORK IF YOU CAN: TOWARDS FAST AND STABLE REINFORCEMENT LEARNING Ahmed Hendawy1,2 Henrik Metternich1 Theo Vincent1,3 Mahdi Kallel5 Jan Peters1,2,3,4 Carlo DEramo1,2,5 1Technical University of Darmstadt 4Robotics Institute Germany (RIG) 2Hessian.AI 5University of Wurzburg 3German Research Center for AI (DFKI) 5 2 0 O 2 ] . [ 1 0 9 5 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "The use of target networks is popular approach for estimating value functions in deep Reinforcement Learning (RL). While effective, the target network remains compromise solution that preserves stability at the cost of slowly moving targets, thus delaying learning. Conversely, using the online network as bootstrapped target is intuitively appealing, albeit well-known to lead to unstable learning. In this work, we aim to obtain the best out of both worlds by introducing novel update rule that computes the target using the MINimum estimate between the Target and Online network, giving rise to our method, MINTO. Through this simple, yet effective modification, we show that MINTO enables faster and stable value function learning, by mitigating the potential overestimation bias of using the online network for bootstrapping. Notably, MINTO can be seamlessly integrated into wide range of value-based and actor-critic algorithms with negligible cost. We evaluate MINTO extensively across diverse benchmarks, spanning online and offline RL, as well as discrete and continuous action spaces. Across all benchmarks, MINTO consistently improves performance, demonstrating its broad applicability and effectiveness."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) has demonstrated exceptional performance and achieved major breakthroughs across diverse spectrum of decision-making challenges. This success spans domains from mastering complex environments like video games (Mnih et al., 2013; 2015; Hessel et al., 2018) and strategic board games (Silver et al., 2016; 2017), to solving high-dimensional problems in continuous control (Haarnoja et al., 2018a; Schulman et al., 2017). Noteworthy applications include learning complex locomotion skills (Haarnoja et al., 2018b; Rudin et al., 2022) and enabling sophisticated, real-world capabilities such as robotic manipulation (Andrychowicz et al., 2020; Lu et al., 2025). The foundation of this success lies primarily in Deep RL, initiated by the introduction of the Deep Q-Network (DQN) (Mnih et al., 2013), which marked the first successful application of deep neural networks in RL. To make that happen, Mnih et al. (2013) introduce various techniques to mitigate mainly the deadly triad issue (Van Hasselt et al., 2018) due to the usage of function approximators, off-policy data, and target bootstrapping. Particularly, Mnih et al. (2013) introduce the concept of target networks to mitigate the negative impact of the deadly triad issue (Zhang et al., 2021), where the regression target is computed using lagged copy of the online network to promote stability during training. This prevents the online network from directly chasing its own rapidly changing estimates, thereby mitigating the problem of moving targets. problem that presents challenge and obstacle towards using fresh estimates from the online network. While the target network has been highly successful in improving stability and convergence, it inherently slows down learning since updates are based on delayed targets. This naturally raises an important question: how can we accelerate learning by leveraging the most recent online estimates while still preserving the stability of the training process? Recent studies have suggested that relying solely on the online network for target bootstrapping can improve performance in certain methods (Bhatt et al., 2024; Kim et al., 2019). Surprisingly, Ahmed Hendawy (ahmed.hendawy@tu-darmstadt.de) is the corresponding author. 1 Preprint. Under Review. later findings indicate that reinstating target network in these same approaches can further enhance results (Palenicek et al., 2025; Gan et al., 2021). Building on these insights, we propose complementary perspective: leveraging both the online and target networks jointly to compute the regression target, thereby aiming to combine the benefits of stability and fast learning. In deep RL, the problem of moving targets is especially evident due to the use of neural networks and the resulting uncontrolled fluctuations in the values of unseen states. This issue becomes even more critical in value-based methods, where maximization bias drives the online estimates to steadily increase over time. These issues motivate the search for an appropriate selection criterion that can mitigate the impact of maximization bias when employing the online network for target computation, hence faster, yet stable learning. Building on this motivation, we propose MINTO, simple yet effective technique that computes regression targets by taking the MINimum estimated value between the Target and Online networks. By relying on the target network when the online estimate is relatively higher, MINTO reduces maximization bias, alleviates the moving-target problem, and ensures stable learning. At the same time, by incorporating the online network when its estimate is lower, MINTO leverages fresher information, enabling faster learning. Thanks to its simplicity, MINTO can be seamlessly integrated into wide range of off-policy methods, including both value-based and actorcritic algorithms, across online and offline RL settings. In this work, we present an extensive empirical evaluation showcasing the benefits of our approach. In addition, we benchmark MINTO against related baselines that exploit online estimates, whether for similar objectives or for different purposes. Our contributions can be summarized as follows: we advocate for principled combination of online and target networks when computing bootstrapped targets, enabling faster learning while preserving stability. We introduce MINTO, simple yet effective technique that computes regression targets as the minimum between online and target estimates, thereby mitigating the moving-target problem and reducing maximization bias. We further demonstrate MINTOs broad applicability and effectiveness by integrating it into both value-based and actorcritic methods, across online and offline RL settings. Extensive empirical results show that MINTO consistently outperforms conventional target-network designs and related baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Many works focus on developing simpler RL algorithms that are closer to the original Q-learning algorithm to benefit from up-to-date Bellman updates. reasonable approach is to use additional resources or privileged information to remove the target network. For example, Gallici et al. (2025) demonstrated that cleverly using parallel environments eliminates the need for target network. Interestingly, Lindstrom However, real-world applications are often limited to single process. et al. (2025) show that constructing the regression target from the online network alone is stable after pre-training phase using expert data. While this study gives hope that target-free algorithm is feasible, it still relies on additional resources that are not available in the general case. Shao et al. (2022) introduce cross-entropy method to refine the actions suggested by the policy to construct the bootstrapped estimate. While promising, this technique is limited to the actor-critic setting, and requires additional resources to optimize the landscape defined by the Q-function. In this work, we do not consider having access to additional resources or privileged information as we are interested in building general-purpose algorithm for off-policy learning. Another approach is to rely on single estimator, thereby reducing the number of parameters used during training. For example, Kim et al. (2019) replace the maximum operator with the MellowMax operator to construct target-free algorithm. However, following work (Gan et al., 2021) suggests that reintroducing the target network is beneficial, indicating that the improvements made by the first approach come from the different nature of the update instead of up-to-date bootstrapped estimates. This makes MellowMax orthogonal to our approach. Some works intervene in the architecture of the function approximator to stabilize the training dynamics when using only one estimator. For example, Li & Pathak (2021) design neural networks processing the state given as input in the Fourier space. Nonetheless, their analysis shows that the approach struggles to handle high-dimensional input spaces. More recently, Bhatt et al. (2024) introduce CrossQ, an actor-critic algorithm that 2 Preprint. Under Review. uses batch normalization (Ioffe & Szegedy, 2015) to account for the distribution match between the state-action pair and the next state-next action pair. follow-up work demonstrates that this idea can work better with target network (Palenicek et al., 2025), moving away from the objective of constructing the regression target from up-to-date estimates, and making this method orthogonal to our approach. In the following, we also choose to rely on target network since Vincent et al. (2025) show that target-free methods still underperform compared to target-based methods. Closer to our approach, hybrid methods have been developed, where the regression target is built from the online network, but an old copy of the online network is used to regularize the update. Zhu & Rigotti (2021) introduce Self-correcting Q-learning (ScQL), which evaluates the Q-estimate of the next state at the action that maximizes combination of the target and online network. Piche et al. (2023) regularizes the online network predictions with the prediction given by the target network. In Section 5.1, we compare MINTO to those methods. Building the regression target from the online network accelerates the overestimation bias, which degrades performance. The overestimation bias is long-standing problem in off-policy RL Hasselt (2010), which arises from the interaction between the maximum operator and the stochastic nature of the bootstrap estimate. first attempt to combat this issue is to learn two independent estimates of the Q-function and evaluate one Q-estimate on the best action suggested by the other estimator to build the regression target (Hasselt, 2010). While this idea is scalable to deep settings (Van Hasselt et al., 2016), it leads to underestimation, which hinders exploration. Maxmin Q-learning (Lan et al., 2020) reduces overestimation bias by taking the minimum across several estimators before applying the maximum operator, but at the cost of training multiple networks in parallel. By contrast, our method applies the minimum operator between the target and online network, incorporating the latest online estimates in stable manner to achieve faster learning."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "We define the problem as Markov Decision Process (MDP) Puterman (1990), < S, A, P, r, µ, γ >, where is the state space, is the action space, : (S) is the transition distribution where (ss, a) is the probability of reaching state from state after performing action a, : is the reward function, µ is the initial state distribution, and γ [0, 1) is discount factor. policy π : (A) maps each state to distribution over the action space. The policy induces an action-value function Qπ(s, a) = Eπ[(cid:80) t=0 γtr(st, at)s0 = s, a0 = a] that defines the expected discounted cumulative return of executing action in state while following the policy π thereafter. The goal of the agent is to find the policy π that maximizes the expected return starting from some initial state. TD learning methods are suitable set of solutions to this goal, more precisely Q-Learning Watkins & Dayan (1992). Q-Learning is an off-policy algorithm that aims to learn the state-action value function Q, known as Q-function, of the optimal policy π, Qπ = Q, by utilizing the Bellman optimality equation Bellman (1957): Q(s, a) = E[r(s, a) + γ max aA Q(s, a)]. (1) Therefore, the optimal policy is defined using this value function by acting greedily at given state s, π = argmaxaAQ(s, a). Q-learning offers recursive update to approximate the state-action value function Q, given transition sample (s, a, r, s) generated by any behavioral policy: Q(s, a) Q(s, a) + α(s, a)[y Q(s, a)], (2) where = + γ maxaA Q(s, a) is the target value which is computed via bootstrapping with the current estimate, and α(s, a) is the step-size. In the target value, the maximum expected value of the next state is approximated by applying maximum operator on single estimate. This introduces maximization bias and causes Q-learning to overestimate the state-action values Hasselt (2010). When dealing with high dimensional state and action spaces, the tabular form of the value functions is not suitable and function approximators are needed. Particularly, in Deep RL, the state-action value function is modeled by neural network Qθ(s, a) with some learnable parameters θ. In Mnih et al. (2013), DQN was introduced as the first successful application of neural networks in RL. (Mnih et al., 2013) introduce series of algorithmic components, more importantly, is the introduction of the target networks. Target networks Qθ allow stable learning of value functions by computing the target value using an older copy of the online network Qθ, = + γ maxaA Qθ(s, a). This 3 Preprint. Under Review. is done by periodically updating the target parameters θ to the online parameters θ every steps. Hence preventing the chase of moving target due to learning Qθ from its own value that eventually results in unstable learning. Despite the success, this results in slow learning of value function as well as the policy due to relying on out-dated estimates. This raises question: can we find practical bellman update rule that results in stable and fast learning?"
        },
        {
            "title": "4.1 MINTO",
            "content": "The aim of this paper is to identify suitable criterion for incorporating recent online estimates without sacrificing the stability traditionally provided by the target network. This naturally suggests that the online and target networks should work side by side to achieve both fast and stable learning. To this end, we propose simple yet effective technique that leverages online estimates only when they are unlikely to introduce harmful overestimation or rapid fluctuations in the target. In cases where the online estimates are higher than the target one, we instead rely on the target network to ensure stability. Concretely, we achieve this by applying the MINimum operator to the estimated values of the Target and Online networks, giving rise to our method, MINTO. Following our method, the bootstrapped target can be computed as follows: = + γ max aA min (cid:0)Qθ(s, a), Qθ(s, a)(cid:1). (3) Given the new regression target, we can compute the regression loss function, similar in DQN, as follows: L(θ) = 1 2 (cid:0)y Qθ(s, a)(cid:1)2 , (4) where . refers to the stop gradient operator, to prevent the backpropagation of gradients to the online network presented in the regression target equation (Eq. 3). In practice, MINTO can be integrated into DQN, and more generally into any off-policy algorithm (see Appendix B), by modifying only few lines of code. The method is lightweight, requiring only single additional feedforward pass of the online network on the next state. When implemented in an efficient deep learning framework such as JAX (Bradbury et al., 2018), this overhead is negligible. 4.2 ANALYZING MINTO To evaluate the impact of MINTO, we design an empirical study that seeks to answer two central questions: (Q1) Is the minimum operator an appropriate criterion for combining online and target estimates? (Q2) Can the empirical evidence substantiate the rationale for adopting the minimum operator? We empirically examine how the minimum operator employed in MINTO stands relative to alternative operators for combining online and target network estimates. Our evaluation is conducted on 15 Atari games and considers the following baselines: Online Only, which relies exclusively on the online estimates during training; Target Only, equivalent to DQN using solely the target network estimates; Max, which selects the larger of the two estimates; Mean, which averages them; Random, which chooses between the online and target networks with equal probability; and finally Min, the operator at the core of our proposed method, MINTO. Figure 1: Results of benchmarking the Minimum operator utilized by MINTO against other potential operators on 15 Atari games with the CNN architecture. We report the AUC metric using IQM and the confidence interval computed across 5 seeds. Methods are trained for 50 million frames. Preprint. Under Review. Figure 2: Results of benchmarking MINTO and DQN on 15 Atari games with CNN, and IMPALA with LayerNorm (LN) architectures. All results are reported using IQM and confidence interval on 5 seeds and over all games. Left: We report the AUC metric for MINTO and DQN while utilizing both architectures. Center and Right: We illustrate the performance learning curves of MINTO and DQN after employing both architecture options. In Fig. 1, we clearly observe the advantage of the minimum operator over the alternative candidates, thereby addressing Q1.As expected, the Online Only function performs poorly, as it relies on rapidly changing bootstrapped targets that lead to unstable training dynamics (see Fig. 19). On the other hand, the Random function fails to match the performance of MINTO, instead converging toward the behavior of Target Only (DQN). The Mean function also aligns closely with Target Only, without offering any additional benefit. Interestingly, the worst results are obtained with the Max operator, which represents the opposite selection criterion to ours. In this case, instability arises from severe overestimation bias, which in turn drives large and uncontrolled increases in target values. Taken together, these findings support our motivation for adopting the minimum operator: it enables the inclusion of recent online estimates in stable manner by mitigating the overestimation bias they may introduce, thus providing an empirical answer to Q2. 4.3 CONVERGENCE OF MINTO While our empirical study highlights the effectiveness of the minimum operator, it is equally important to establish whether its use is theoretically justified in terms of convergence. Since analyzing convergence under function approximation is notoriously difficult, we focus on the tabular setting. Our analysis leverages the Generalized Q-learning framework of Lan et al. (2020), which guarantees convergence for update operators satisfying specific non-expansion properties. Owing to its close relation to Maxmin Q-learning, we show that MINTO can be cast as special case of this general framework, in the same spirit as how it has been used to analyze other Q-learning variants in their work. This observation leads directly to the following corollary. Corollary 1 (Convergence of MINTO). Let the MINTO operator be defined as GMINTO(Qs) = maxaA minjT Qsa(j), where is set of historical time indices. Under the standard stochastic approximation assumptions for the learning rate (Assumption 2 in Lan et al. (2020)), the Q-values generated by the MINTO update rule converge to the optimal action-values, Q. Proof. The proof relies on demonstrating that the GMINTO operator satisfies the two conditions on the target operator (Assumption 1 in Lan et al. (2020)). We provide the detailed verification of these conditions in Appendix A."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "We now turn to the empirical evaluation, where we demonstrate MINTOs broad applicability and effectiveness by integrating it into both value-based and actorcritic methods across online and offline RL settings, and show consistent advantages over conventional target-network designs and related baselines. 5 Preprint. Under Review."
        },
        {
            "title": "5.1 ONLINE RL AND DISCRETE CONTROL",
            "content": "We evaluate MINTO on benchmark of 15 Atari games (Bellemare et al., 2013) recommended by Graesser et al. (2022). These games are chosen for their diversity in human-normalized scores achieved by DQN after training. All methods employ the standard CNN architecture introduced by Mnih et al. (2015), and our experiments follow the evaluation protocol of Machado et al. (2018). Additional implementation details and hyperparameters are provided in Appendix C.1. We begin by evaluating the performance gains of MINTO over DQN (Mnih et al., 2013) in terms of both sample efficiency and asymptotic performance. Our objective is to demonstrate that MINTO not only accelerates learning by incorporating fresher estimates from the online network but also achieves higher final performance. To capture both aspects in single measure, we report the Area Under the Curve (AUC) metric, alongside learning curves for MINTO and DQN. To further assess the methods effectiveness with more advanced architectures, we extend the comparison to the IMPALA architecture (Espeholt et al., 2018; Castro et al., 2018), evaluating the same metrics. In this setting, we additionally apply LayerNorm (LN), motivated by prior work showing its potential for improving learning stability and performance (Lee et al., 2024; Nauman et al., 2024). In Fig. 2, we observe clear advantage of MINTO over DQN. Specifically, MINTO achieves an improvement of approximately 18% in AUC when using the vanilla CNN architecture, and about 24% when employing IMPALA with LN. The performance curves in Fig. 2 (center and right) further illustrate MINTOs superior sample efficiency and higher asymptotic performance. These results suggest that incorporating more recent estimates from the online network accelerates learning while also improving final performance. The idea of leveraging the online network alongside the target network for computing regression targets is not confined to MINTO. Prior work has explored alternative ways of utilizing the online network for different purposes. It is therefore important to evaluate the effectiveness of our selection criterion, the minimum operator, against established methods from the literature. In this study, we compare against three representative baselines: Double DQN (Van Hasselt et al., 2016), Functional Regularization DQN (FR-DQN) (Piche et al., 2023), and Self-correcting DQN (ScDQN) (Zhu & Rigotti, 2021). All methods are implemented with the CNN architecture, and we report aggregate results over all 15 games using the AUC In Fig. 3, we observe that MINTO metric. consistently outperforms all baselines, including methods such as Double DQN and ScDQN that are specifically designed to mitigate overestimation. This indicates that combining the online network with the minimum operator provides an especially effective mechanism for addressing this issue. Furthermore, MINTO achieves clear gains over FR-DQN, which relies entirely on the online network for target bootstrapping while regularizing its updates using fixed network (target network). This highlights the role of our selection criterion, the minimum operator, in regulating the contribution of online estimates. It is also worth noting that methods such as ScDQN and FR-DQN require additional hyperparameters, whereas MINTO introduces none. Figure 3: Results of benchmarking MINTO against related baselines on 15 Atari games with the CNN architecture. We report the AUC metric using IQM and the confidence interval computed across 5 seeds. Given the similarity of our method to the ensemble-based approach Maxmin DQN (Lan et al., 2020), which also employs minimum operator, it is essential to benchmark MINTO against this prior work. Maxmin DQN applies the minimum operator across multiple estimates produced by an ensemble of target networks, with the goal of reducing the overestimation bias introduced by the maximum operator in the Bellman optimality equation. For this comparison, we also report results in Fig. 3 using the AUC metric. Although Maxmin DQN relies on an ensemble of Q-functions (N = 2 in our study), MINTO achieves better performance. This demonstrates the advantage of leveraging up-to-date estimates from the online network, in combination with the minimum operator, to miti6 Preprint. Under Review. Figure 4: Results of benchmarking MINTO+IQN and IQN on 15 Atari games in the online RL setting using the CNN architecture. All metrics are computed over 5 seeds. Left: We report the AUC metric computed by the IQM and its confidence interval. Center: We demonstrate the performance learning curves of both methods in terms of IQM return and confidence interval. Right: We report the frequency of selecting the online estimate during the course of training on the Breakout game, considering the mean and standard deviation. gate overestimation bias that can arise from blindly incorporating online estimates. Simultaneously, MINTO avoids the additional memory overhead required by Maxmin DQN. 5.2 DISTRIBUTIONAL RL We also evaluate MINTO within the context of distributional RL (Bellemare et al., 2017), an advanced value-based paradigm that has achieved state-of-the-art performance on the Atari benchmark in the online RL setting (Castro et al., 2018). Specifically, we consider Implicit Quantile Networks (IQN) (Dabney et al., 2018), distributional RL method that employs quantile regression to approximate the entire return distribution rather than only its expected value. IQN accomplishes this by learning an implicit quantile function for the stateaction values. For our experiments, we follow the implementation guidelines provided by Castro et al. (2018) when benchmarking IQN on the Atari suite. Comprehensive implementation details and hyperparameter settings are reported in Appendix C.2. We evaluate the effect of integrating MINTO into IQN by modifying its target computation according to our proposed technique. Appendix provides further details, including pseudo-code illustrating the changes introduced (see Alg. 2). We benchmark MINTO+IQN against the original IQN on 15 Atari games using the standard CNN architecture. In Fig. 4(left), we report the AUC metric for both methods, which captures improvements in both learning speed and asymptotic performance. MINTO enhances IQN by approximately 7% according to this metric. Fig. 4(center) further presents the learning curves, illustrating consistent gains in sample efficiency and final performance. While the improvement is smaller than in the DQN case (and in later cases we consider), these results clearly demonstrate that MINTO enhances the performance of IQN, establishing it as general and effective improvement even for state-of-the-art distributional methods. To verify that the online network is selected frequently during training, we track the online selection ratio and report it for representative game, Breakout, in Fig. 4(right). The results show that the target network dominates in the very early stages of training, while the use of the online network gradually increases as training progresses. Later in training, the online network is selected approximately 45% of the time. Each point on the curve corresponds to the average online selection ratio between two consecutive target network updates, during which the parameters of the online network increasingly diverge from those of the target network. 5.3 OFFLINE RL The applicability of our method extends beyond online RL. Given its simplicity, MINTO can be readily incorporated into offline RL methods by modifying the Bellman update rule, allowing us to investigate its potential in this setting. Offline RL aims to learn an optimal policy from large and static dataset of previously collected experience, without any further interaction with the environ7 Preprint. Under Review. Figure 5: Results of benchmarking CQL and CQL+MINTO on 14 Atari games with CNN, and IMPALA with LayerNorm (LN) architectures. All results are reported using IQM and confidence interval on 5 seeds and over all games. Left: We report the AUC metric for both methods while utilizing the two architectures. Center and Right: We illustrate the performance learning curves of CQL and CQL+MINTO after employing both architecture options. ment. central challenge in this paradigm is the distributional shift problem: the learned policy may query the Q-function on stateaction pairs absent from the dataset. Overestimation of these out-of-distribution actions can then misguide the policy toward suboptimal behaviors. We consider popular offline RL algorithm, Conservative Q-Learning (CQL), which regularizes Qfunction learning by penalizing out-of-distribution values while encouraging in-distribution values. To integrate MINTO into this framework, we simply modify the computation of the regression target using Eq. 3 (see Alg. 3). Additional implementation details and hyperparameters are provided in Appendix C.3. In Fig. 5, we present the results of evaluating our method, denoted as CQL+MINTO, against the original CQL on 14 Atari games using both the vanilla CNN architecture and the IMPALA architecture with LN. For the offline setting, we use the datasets provided by Gulcehre et al. (2020). We note that one game, Tutankham, is excluded from our evaluation since it is not available in the released dataset. The results of our experiments demonstrate clear benefit of applying MINTO to offline RL. MINTO consistently improves the performance of CQL in terms of both sample efficiency and final performance, as reflected in the AUC metric (see Fig. 5 (left)) and the learning curves (see Fig. 5 (center and right)). In particular, MINTO tremendously boosts the performance of CQL with CNN architecture by roughly 125% in AUC. Although the performance gain is smaller with the IMPALA with LN architecture, MINTO still improves CQL by around 20%, with clearer sampleefficiency advantage visible in Fig. 5 (right). These findings are noteworthy, as one might expect our modified update rule to yield more conservative estimates in CQL, potentially slowing down learning. Instead, the improvements are substantial, highlighting the central role of recent online estimates, an aspect overlooked by the original CQL formulation.. 5.4 ONLINE RL AND CONTINUOUS CONTROL Similarly, MINTO is not restricted to value-based methods. In off-policy actorcritic algorithms, where dedicated network models the policy, and the Q-function (critic) is trained using bootstrapped targets from target critic network, our proposed update rule can also be applied. There are multiple ways to incorporate MINTO into actorcritic methods, depending in particular on the number of critic networks employed. We defer discussion of these design choices to Appendix B. To examine the effect of incorporating up-to-date estimates in the actorcritic setting, we adopt two recent architectures that are used in combination with Soft Actor-Critic (SAC) (Haarnoja et al., 2018a), namely SimbaV1 (Lee et al., 2024) and SimbaV2 (Lee et al., 2025). SAC is widely used actorcritic algorithm that seeks to learn an optimal stochastic policy by encouraging exploration through entropy maximization. In contrast, SimbaV1 and SimbaV2 are recently proposed architectures designed to scale performance with model size by leveraging the concept of simplicity bias, implemented in practice through variants of LN and residual connections. In Alg. 4, we highlight the modifications introduced to SAC in order to integrate MINTO. 8 Preprint. Under Review. Figure 6: Results of evaluating the impact of MINTO on SimbaV2 (top) and SimbaV1 (bottom) on three continuous control benchmarks: MuJoCo (left), HBench (center), and DMC-Hard (right). We show the performance curves of all methods on each benchmark while reporting normalized IQM return and confidence interval computed on 10 seeds. We benchmark SimbaV1 and SimbaV2 with and without our proposed method, MINTO. The evaluation is conducted on 26 continuous control tasks, including both manipulation and locomotion, drawn from three different benchmarks: MuJoCo (Todorov et al., 2012), Humanoid Bench (HBench) (Sferrazza et al., 2024), and the DeepMind Control Suite Hard (DMC-Hard) (Tassa et al., 2018). For each actorcritic method, we report the aggregated performance curves on all three benchmarks. Additional details on the experimental setup and hyperparameters are provided in Appendix C.4. In Fig. 6, we observe clear improvement in sample efficiency when applying our method on the MuJoCo and HBench benchmarks. In contrast, performance on DMC-Hard is comparable to the base algorithm (SimbaV1) or slightly lower (as with SimbaV2). Overall, however, MINTO yields consistent positive impact across all benchmarks and both actorcritic methods, suggesting the value of incorporating recent online estimates when computing regression targets in continuous control and actorcritic settings."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce MINTO, simple bootstrapping rule that combines online and target networks by taking their minimum estimate. This design mitigates the overestimation bias that can arise from blindly incorporating online estimates into target computation, thereby alleviating the moving-target problem, while still exploiting recent updates for faster learning. Across online, offline, value-based, and actorcritic methods, MINTO consistently improves sample efficiency and final performance without introducing additional hyperparameters and with only negligible overhead. These results establish MINTO as practical and effective alternative to conventional target-network designs, pointing toward promising direction for advancing stable and efficient deep RL. Although MINTO consistently improves performance across diverse settings, there are natural trade-offs to consider. Relying exclusively on the minimum operator may, in some cases, be overly conservative in low-noise environments, leading to slight underestimation. In addition, by dampening optimistic estimates, MINTO may interact with exploration strategies in ways that are not yet fully understood. These observations open promising avenues for future research, such as adaptive operator selection that dynamically balance online and target estimates based on uncertainty or learning dynamics. Beyond these considerations, separate line of future work lies in testing MINTO in additional challenging scenarios, such as multi-task and multi-agent reinforcement learning, to assess its scalability and broader applicability. 9 Preprint. Under Review."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was funded by the German Federal Ministry of Education and Research (BMBF) (Project: 01IS22078). This work was also funded by Hessian.ai through the project The Third Wave of Artificial Intelligence 3AI by the Ministry for Science and Arts of the state of Hessen. The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universitat Erlangen-Nurnberg (FAU) under the NHR project b187cb. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) 440719683. Calculations for this research were also conducted on the Lichtenberg highperformance computer of the TU Darmstadt and the Intelligent Autonomous Systems (IAS) cluster at TU Darmstadt."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have taken several measures to ensure the reproducibility of our results. To illustrate the simplicity of our approach, we provide in the appendix both JAX code snapshot showing how MINTO can be integrated into DQN with minimal changes to the target computation and pseudocode for all algorithms where we integrate our method. We will release the full codebase publicly soon. Detailed descriptions of the experimental setup, architectures, and hyperparameters are provided in Appendix C. For theoretical results, we clearly state assumptions and include the complete proof of convergence in Appendix A. For empirical evaluation, we report results across multiple random seeds, following standard evaluation protocols (Machado et al., 2018), and provide per-task breakdowns in Appendix D. Offline RL experiments use publicly available datasets (RL Unplugged), and we specify all data processing and training procedures in the appendix. Together, these details should facilitate exact replication and further validation of our findings. LARGE LANGUAGE MODEL USAGE large language model was helpful in polishing writing, improving reading flow, and identifying remaining typos."
        },
        {
            "title": "REFERENCES",
            "content": "OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):320, 2020. Marc Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of artificial intelligence research, 47: 253279, 2013. Marc Bellemare, Will Dabney, and Remi Munos. distributional perspective on reinforcement learning. In International conference on machine learning, pp. 449458. PMLR, 2017. Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1 edition, 1957. Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij Amiranashvili, Thomas Brox, and Jan Peters. Crossq: Batch normalization in deep reinforcement learning for greater sample efficiency and simplicity. In The Twelfth International Conference on Learning Representations, 2024. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao JAX: composable transformations of Python+NumPy programs, 2018. URL http: Zhang. //github.com/jax-ml/jax. Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc BellearXiv preprint mare. Dopamine: research framework for deep reinforcement learning. arXiv:1812.06110, 2018. 10 Preprint. Under Review. Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pp. 1096 1105. PMLR, 2018. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 14071416. PMLR, 2018. Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning. In International Conference on Learning Representations, 2025. Yaozhong Gan, Zhe Zhang, and Xiaoyang Tan. Stabilizing learning via soft mellowmax operator. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. Laura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. The state of sparse training in deep reinforcement learning. In International Conference on Machine Learning, pp. 77667792. PMLR, 2022. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gomez, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. Rl unplugged: suite of benchmarks for offline reinforcement learning. Advances in neural information processing systems, 33:72487259, 2020. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018a. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018b. Hado Hasselt. Double q-learning. Advances in neural information processing systems, 23, 2010. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, 2018. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 2015. Seungchan Kim, Kavosh Asadi, Michael Littman, and George Konidaris. Deepmellow: removing the need for target network in deep q-learning. In Proceedings of the twenty eighth international joint conference on artificial intelligence, 2019. Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin q-learning: Controlling the estimation bias of q-learning. In International Conference on Learning Representations, 2020. Hojoon Lee, Dongyoon Hwang, Donghu Kim, Hyunseung Kim, Jun Jet Tai, Kaushik Subramanian, Peter Wurman, Jaegul Choo, Peter Stone, and Takuma Seno. Simba: Simplicity bias for scaling In The Thirteenth International Conference on up parameters in deep reinforcement learning. Learning Representations, 2024. Hojoon Lee, Youngdo Lee, Takuma Seno, Donghu Kim, Peter Stone, and Jaegul Choo. Hyperspherical normalization for scalable deep reinforcement learning. arXiv preprint arXiv:2502.15280, 2025. Alexander Li and Deepak Pathak. Functional regularization for reinforcement learning via learned fourier features. In Advances in Neural Information Processing Systems, 2021. 11 Preprint. Under Review. Alexander Lindstrom, Arunselvan Ramaswamy, and Karl-Johan Grinnemo. Pre-training deep qnetworks eliminates the need for target networks: An empirical study. In The 14th International Conference on Pattern Recognition Applications and Methods (ICPRAM), 2025. Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. Marlos Machado, Marc Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523562, 2018. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. Michal Nauman, Mateusz Ostaszewski, Krzysztof Jankowski, Piotr Miłos, and Marek Cygan. Bigger, regularized, optimistic: scaling for compute and sample efficient continuous control. Advances in neural information processing systems, 37:113038113071, 2024. Daniel Palenicek, Florian Vogt, and Jan Peters. Scaling off-policy reinforcement learning with batch and weight normalization. In Advances in Neural Information Processing Systems, 2025. Alexandre Piche, Valentin Thomas, Joseph Marino, Rafael Pardinas, Gian Maria Marconi, Christopher Pal, and Mohammad Emtiyaz Khan. Bridging the gap between target networks and functional regularization. Transactions on Machine Learning Research, 2023. Martin Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331434, 1990. Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In Conference on robot learning, pp. 91100. PMLR, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. arXiv preprint arXiv:2403.10506, 2024. Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun Sun, and Jeannette Bohg. Grac: Selfguided and self-regularized actor-critic. In Conference on Robot Learning, 2022. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 50265033. IEEE, 2012. 12 Preprint. Under Review. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018. Theo Vincent, Yogesh Tripathi, Tim Faust, Yaniv Oren, Jan Peters, and Carlo DEramo. Bridging the performance gap between target-free and target-based reinforcement learning with iterated q-learning. European Workshop on Reinforcement Learning, 2025. Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279292, 1992. Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with target network. In International Conference on Machine Learning, pp. 1262112631. PMLR, 2021. Rong Zhu and Mattia Rigotti. Self-correcting q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 1118511192, 2021. 13 Preprint. Under Review."
        },
        {
            "title": "APPENDIX",
            "content": "A Proof of Corollary 1 A.1 Proof of Condition A1.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Proof of Condition A1.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Algorithmic Details Implementation Details C.1 Online RL and Discrete Control . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Distributional RL . C.3 Offline RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Online RL and Continuous Control . . . . . . . . . . . . . . . . . . . . . . . . . . Individual Results D.1 Online RL and Discrete Control . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Offline RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Maxmin DQN vs. MINTO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Online RL and Continuous Control . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Ablation on Target Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 16 19 20 21 21 23 26 28 29 33 14 Preprint. Under Review. PROOF OF COROLLARY 1 To prove the convergence of MINTO, we show that its target operator, GMINTO, satisfies the two conditions of Assumption 1 from the Generalized Q-learning framework Lan et al. (2020). Assumption 1 (Conditions on G) Let : RnN (cid:55) be the target operator where Qs = sa) RnN K, A, and = n, {1, . . . }, {0, . . . , 1}, and is an (Qij arbitrary state. must satisfy: A1.1: If all input action-values are identical, Qij sa = Qkl sa, i, k, j, k, and a, then G(Qs) = maxa Qij sa. A1.2: is non-expansion w.r.t. the max norm, G(Qs) G(Q s) maxa,i,j Qij sa Qij sa . The action-value function Qs is represented by tensor with an ensemble of action-value functions and for each, historical time action-values. The MINTO operator is defined as GMINTO(Qs) = maxaA (minjT Qsa(j)) for given state s, where is set of historical time indices, and is the time index with an abuse of notations. This is special case of the Generalized Q-learning where = 1 and = 2, hence dropping the index. In the analysis, we consider general variant of MINTO, where > 1, by considering set of historical time values using the indices set such that Qsa = (cid:0)Qsa(t K), . . . , Qsa(t 1)(cid:1) for given state and action. In practice, includes only (t 1) and (t K), representing the online and target network estimates, respectively. A.1 PROOF OF CONDITION A1.1 Assume the input Q-values Qsa(j) are identical for all and all A. GMINTO(Qs) = max aA = max aA (cid:18) (cid:19) Qsa(j) min jT Qsa(j) This is the maximum action-value among the inputs. Thus, A1.1 is satisfied. A.2 PROOF OF CONDITION A1.2 Let Qs and be two distinct sets of historical Q-values for given state s, assuming that = 1. GMINTO(Qs) GMINTO(Q s) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:18) (cid:19) (cid:18) min jT Qsa(j) max aA max aA (cid:12) (cid:12) min (cid:12) (cid:12) jT (cid:18) max aA max aA max jT Qsa(j) min jT sa(j) Qsa(j) sa(j) = max aA,jT Qsa(j) sa(j) min jT (cid:12) (cid:12) (cid:12) (cid:12) (cid:19) sa(j) (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (5) (6) (7) (8) The first inequality holds because the max operator is non-expansion. The second inequality holds because the min operator is also non-expansion. The final term is the maximum absolute difference over the subset of Q-values used by the MINTO operator. This maximum is necessarily less than or equal to the maximum over the entire set of all possible Q-values, i.e.: max aA,jT Qsa(j) sa(j) max a,j Qsa(j) sa(j) Therefore, GMINTO(Qs) GMINTO(Q s) maxa,j Qsa(j) sa(j). Thus, A1.2 is satisfied. Conclusion: Since both conditions of Assumption 1 are satisfied, the convergence of MINTO is guaranteed by Theorem 2 of Lan et al. (2020) under the standard stochastic approximation assumptions for learning rate stated in Assumption 2 of Lan et al. (2020). 15 Preprint. Under Review."
        },
        {
            "title": "B ALGORITHMIC DETAILS",
            "content": "The pseudocode blocks below present MINTO and its extension to IQN, CQL, and SAC in Alg. 1, Alg. 2, Alg. 3, and Alg. 4 respectively. Changes in the pseudocode of the underlying algorithms are colored red. Note that denotes the stop-gradient operator. For MINTO we want to explicitly stop the gradient, as the target computation relies on both the online as well as the target parameters. In contrast to other methods, like DoubleDQN, for example, the gradient is not stopped by the arg max operator, as the online values are used directly. Sample an initial state s0 from µ for = 0 to nhorizon do Algorithm 1 DQN+MINTO. 1: Initialize online and target paramters θ, θ, an empty replay buffer B, and ttotal = 0. 2: repeat 3: 4: 5: 6: 7: 8: 9: 10: Sample an action at ϵ-greedy(Qθ(st, ) Execute action at in environment, observe reward rt and next state st+1 Store transition (st, at, rt, st+1) in Sample batch of transitions (sb, ab, rb, Compute the TD-target yb = rb + γ maxa min(Qθ(s if is terminal then yb rb Compute the loss L(θ) = 1 2B Obtain the gradient θL and perform an update step Every steps update the target network θ θ if st+1 is terminal break b=1(yb Qθ(sb, ab)(cid:1)2 b, a), Qθ(s b=1 from b, a)) b)B (cid:80)B 11: 12: 13: 14: 15: 16: 17: until ttotal ntotal end for ttotal ttotal + Algorithm 2 IQN+MINTO 1: Initialize online and target network parameters θ, θ, and replay buffer B, and ttotal = 0. 2: repeat 3: 4: Sample initial state s0 µ for = 0 to nhorizon do (cid:17) where τi [0, 1] (cid:16) 1 (cid:80)N i=1 Zθ(st, a, τi) Select action at ϵ-greedy Execute at, observe reward rt and next state st+1 Store transition (st, at, rt, st+1) in Sample minibatch (sb, ab, rb, j}N i=1, {τ Sample {τi}N Compute target quantiles: b)B j=1 [0, 1] b=1 from yj = rb + γ Zθ(s b, arg max min 1 Zθ(s b, a, τi), 1 (cid:32) (cid:88) i=1 (cid:33) Zθ(s b, a, τi) , τ j) (cid:88) i=1 5: 6: 7: 8: 9: 10: 11: 12: terminal then yj rb if Compute quantile regression loss: L(θ) = 1 B (cid:88) (cid:88) (cid:88) b= i=1 j=1 ρτi κ (yj Zθ(sb, ab, τi)) Perform gradient step on θL Every steps update target network θ θ if st+1 is terminal break 13: 14: 15: 16: 17: 18: until ttotal ntotal end for ttotal ttotal + 16 Preprint. Under Review. Algorithm 3 CQL+MINTO 1: Initialize online and target critic parameters θ, θ, an empty replay buffer B, and ttotal = 0. 2: Load offline dataset into 3: repeat 4: 5: Sample batch of transitions (sb, ab, rb, Compute target Q-values for next states: b=1 from b)B yb = rb + γ max 6: 7: is terminal then yb rb if Compute standard TD loss: min(Qθ(s b, a), Qθ(s b, a)) LTD(θ) = 1 2B (cid:88) b=1 (cid:0)yb Qθ(sb, ab)(cid:1) 8: Compute conservative regularizer: LCQL(θ) = α EsbB (cid:104) log (cid:88) exp(Qθ(sb, a)) 1 (cid:88) b=1 (cid:105) Qθ(sb, ab) 9: Total loss: L(θ) = LTD(θ) + LCQL(θ) Update critic parameters: θ θ ηθL(θ) Every steps update target network: θ θ ttotal ttotal + 1 10: 11: 12: 13: until ttotal nsteps In Alg. 4, SAC is adapted to use single Q-function critic, following the approach taken in Simba (Lee et al. (2024; 2025)) on many benchmarks, which also relies on single critic. This choice motivated our design, as it simplifies computation while remaining effective when combined with the MINTO operator in the target computation. In practice, we also experimented with twin-critic setup (see Fig. 13-16). However, the largest performance gains were observed with the single-critic variant, since applying the minimum operator on all online and target networks, in the twin-critic case, can lead to conservative updates. On the other hand, the stochastic policy and temperature updates are retained as in standard SAC. Overall, the MINTO modifications presented in Alg. 1, 2, 3, and 4 are straightforward to implement and can be integrated with minimal changes to the underlying algorithms. This makes MINTO practical approach for extending existing value-based and actor-critic methods without introducing significant complexity. 17 Preprint. Under Review. Algorithm 4 SAC+MINTO. 1: Initialize policy parameters ϕ, critics online and target parameters θ, θ, an empty replay buffer B, and ttotal = 0. 2: repeat 3: 4: 5: 6: 7: 8: 9: 10: Sample an initial state s0 from µ for = 0 to nhorizon do Sample action at πϕ(st) Execute at in environment, observe reward rt and next state st+1 Store transition (st, at, rt, st+1) in Sample batch of transitions (sb, ab, rb, b) and compute log πϕ(a Sample Compute target: b)B b=1 from bs b) πϕ(s yb = rb + γ(cid:2)min (Qθ(s b, b), Qθ(s b, b)) α log πϕ(a bs b)(cid:3) 11: 12: if is terminal then yb rb Update critic by minimizing L(θ) = 1 2B (cid:88) b=1 (cid:0)yb Qθ(sb, ab)(cid:1)2 13: Update policy ϕ using gradient of Jπ(ϕ) = 1 (cid:88) b=1 (cid:0)α log πϕ(absb) Qθ(sb, ab)(cid:1) 14: Optionally update temperature α by minimizing J(α) = 1 (cid:88) b=1 α(cid:0) log πϕ(absb) + Htarget (cid:1) Every steps update target critic: θ τ θ + (1 τ )θ if st+1 is terminal break 15: 16: 17: 18: 19: until ttotal ntotal end for ttotal ttotal + 18 Preprint. Under Review."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 ONLINE RL AND DISCRETE CONTROL For our online reinforcement learning experiments in the discrete-action setting, we use Atari environments as the benchmark domain. The implementations of DoubleDQN, FR-DQN, ScDQN, and MINTO share the same lightweight and reproducible framework for DQN variants written in JAX. The codebase will be shared upon acceptance. All algorithms share the same network architectures and training setups, differing only in the algorithm-specific modifications detailed in Tables 1 and 2. MINTO is implemented by modifying the target computation. Listing 1: JAX implementation of the MINTO TD target. 1 def compute_minto_target( 2 self, target_params, online_params, sample, 4 5 6 ): 7 8 9 10 12 13 14 q_online_next = self.network.apply(online_params, (cid:44) sample.next_state) q_online_next = jax.lax.stop_gradient(q_online_next) q_target_next = self.network.apply(target_params, (cid:44) sample.next_state) q_next = jnp.max(jnp.minimum(q_online_next, q_target_next)) return ( sample.reward + (1 - sample.is_terminal) * (self.gamma**self.update_horizon) (cid:44) * q_next ) Hyperparameter Replay Buffer Capacity Batch Size Update Horizon Discount Factor (γ) Learning Rate Horizon Architecture Type Features Epochs Training Steps per Epoch Data to Update Initial Samples Epsilon End Epsilon Duration Target Update Frequency (T ) Ensemble Size (N ) DQN - MaxMinDQN 1,000,000 32 1 0.99 6.25 105 27,000 CNN [32, 64, 64, 512] 100 250,000 4 20,000 0.01 250,000 8,000 2 MINTO - Table 1: Hyperparameter settings for DQN, MaxMinDQN, and MINTO on Atari. Most parameters are shared, with algorithm-specific hyperparameters explicitly listed. Identical values are merged for clarity. 19 Preprint. Under Review. Hyperparameter Replay Buffer Capacity Batch Size Update Horizon Discount Factor (γ) Learning Rate Horizon Architecture Type Features Epochs Training Steps per Epoch Data to Update Initial Samples Epsilon End Epsilon Duration Target Update Frequency (T ) Regularization Parameter (κ) Self Correcting Parameter (β) DoubleDQN FR-DQN ScDQN MINTO 1,000,000 32 1 0.99 6.25 105 27,000 CNN [32, 64, 64, 512] 100 250,000 4 20,000 0.01 250,000 8,000 - 1.0 - - 3.0 - Table 2: Hyperparameter settings for the four DQN variants evaluated on Atari, including DoubleDQN, FR-DQN, ScDQN, and MINTO. Most parameters are shared across all algorithms, while the table explicitly lists the algorithm-specific hyperparameters, the regularization parameter κ for FR-DQN and the self-correcting parameter β for ScDQN. Identical values are merged for clarity. C.2 DISTRIBUTIONAL RL For the distributional reinforcement learning experiments, we use Implicit Quantile Networks (IQN). The implementation builds directly on the same JAX codebase as the DQN experiments, ensuring consistency in architecture and training setup. The key algorithm-specific hyperparameters are listed in Table 3. Hyperparameter Replay Buffer Capacity Batch Size Update Horizon (n) Discount Factor (γ) Learning Rate Adam ϵ Horizon Architecture Type Features Epochs Training Steps per Epoch Data to Update Initial Samples Epsilon End Epsilon Duration Target Update Frequency (T ) IQN IQN+MINTO 1,000,000 32 1 0.99 5.0 105 3.125 104 27,000 CNN [32, 64, 64, 512] 100 250,000 4 20,000 0.01 250,000 8000 Table 3: Comparison of IQN hyperparameters for = 1 using IQN and IQN+MINTO. Update horizon is set to 1. 20 Preprint. Under Review. C.3 OFFLINE RL For the offline reinforcement learning experiments on Atari, we use datasets from RL Unplugged1 (Gulcehre et al. (2020)), which provide standardized and diverse benchmarks. The implementation is built on stable and well-tested codebase to ensure reproducibility and fair comparison. The code will be shared upon acceptance. All methods are run with their default hyperparameters, and the most important settings are reported in Table 4. Hyperparameter CNN IMPALA CQL CQL+MINTO CQL CQL+MINTO Dataset Size Batch Size Update Horizon Discount Factor (γ) Epochs Learning Rate Adam (ϵ) Training Steps per Epoch Tradeoff Factor (α) Target Update Frequency (T ) Layer Norm 5,000,000 32 1 0.99 100 5 105 5 3.1254 62,500 0.1 2000 no yes Table 4: Comparison of CQL and CQL+MINTO hyperparameters for the two different network architectures CNN and IMPALA. C.4 ONLINE RL AND CONTINUOUS CONTROL For our continuous-control experiments with online reinforcement learning, we adopt SimbaV1 and SimbaV2. The implementation is based on the official SimbaV2 codebase2 (Lee et al. (2025)), ensuring consistency with the original work. All experiments use the default hyperparameters provided by the authors, with the most relevant ones summarized in Tables 5 and 6. Hyperparameter SimbaV1, SimbaV1+MINTO Discount Factor (γ) Learning Rate Weight Decay Target (τ ) Update Horizon (n) Temperature Initial Value Temperature Target Entropy Batch Size Buffer Max Length Buffer Min Length Num Train Envs Action Repeat Max Episode Steps DMC-Hard HumanoidBench MuJoCo 0.99 0.995 1.0 104 0.01 0.005 1 0.01 0.5 256 1,000,000 5,000 2 1000 1 Table 5: Comparison of SimbaV1 hyperparameters across DMC-Hard, HumanoidBench, and Mujoco locomotion environments. Identical values are merged. 1https://github.com/huihanl/rl_unplugged 2https://github.com/dojeon-ai/SimbaV2 Preprint. Under Review. Hyperparameter SimbaV2, SimbaV2+MINTO Actor Shift Critic Shift Critic vmax Critic vmin Critic Num Bins Discount Factor (γ) Learning Rate Init Learning Rate End Update Horizon (n) Target (τ ) Temperature Initial Value Temperature Target Entropy Buffer Max Length Buffer Min Length Num Train Envs Action Repeat Max Episode Steps DMC-Hard HumanoidBench MuJoCo 3 3 5.0 5.0 0.99 0.995 1.0 104 5.0 105 1 0.005 0.01 0.5 1,000,000 5,000 1 2 1000 Table 6: Comparison of SimbaV2 hyperparameters across DMC-Hard, Humanoid Bench, and MuJoCo. Identical values are merged for clarity. 22 Preprint. Under Review."
        },
        {
            "title": "D INDIVIDUAL RESULTS",
            "content": "D.1 ONLINE RL AND DISCRETE CONTROL Figure 7: Individual Results of benchmarking MINTO and DQN on 15 Atari games using the IMPALA architecture with LayerNorm. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per game. 23 Preprint. Under Review. Figure 8: Individual Results of benchmarking MINTO and DQN on 15 Atari games using the CNN network architecture. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per game. Preprint. Under Review. Figure 9: Individual Results of benchmarking DoubleDQN, FR-DQN, ScDQN, and MINTO on 15 Atari games using the CNN network architecture. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per game. 25 Preprint. Under Review. D.2 OFFLINE RL Figure 10: Individual Results of benchmarking CQL and CQL+MINTO on 15 Atari games using the IMPALA architecture with LayerNorm. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per game. 26 Preprint. Under Review. Figure 11: Individual Results of benchmarking CQL and CQL+MINTO on 15 Atari games using the CNN architecture. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per game. 27 Preprint. Under Review. D.3 MAXMIN DQN VS. MINTO Figure 12: Individual Results of benchmarking MaxMinDQN and MINTO on 15 Atari games using the CNN network architecture. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per game. 28 Preprint. Under Review. D.4 ONLINE RL AND CONTINUOUS CONTROL Figure 13: Individual Results of benchmarking SimbaV2 and SimbaV2+MINTO on the 5 MuJoCo environments. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per environment. The last plot (bottom right) shows the IQM of the normalized return over all 5 environments. Figure 14: Individual Results of benchmarking SimbaV1 and SimbaV1+MINTO on the 5 MuJoCo environments. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 10 seeds per environment. The last plot (bottom right) shows the IQM of the normalized return over all 5 environments. 29 Preprint. Under Review. Figure 15: Individual Results of benchmarking SimbaV2 and SimbaV2+MINTO on the 14 Humanoid Bench environments. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 10 seeds per environment. The last plot (bottom right) shows the IQM of the normalized return over all 14 environments. 30 Preprint. Under Review. Figure 16: Individual Results of benchmarking SimbaV1 and SimbaV1+MINTO on the 14 Humanoid Bench environments. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 10 seeds per environment. The last plot (bottom right) shows the IQM of the normalized return over all 14 environments. 31 Preprint. Under Review. Figure 17: Individual Results of benchmarking SimbaV2 and SimbaV2+MINTO on the 7 DMCHard environments. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 10 seeds per environment. The last plot (bottom right) shows the IQM of the normalized return over all 7 environments. Figure 18: Individual Results of benchmarking SimbaV1 and SimbaV1+MINTO on the 7 DMCHard environments. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 10 seeds per environment. The last plot (bottom right) shows the IQM of the normalized return over all 7 environments. 32 Preprint. Under Review. D.5 ABLATION ON TARGET OPERATORS Figure 19: Individual Results of benchmarking the Minimum operator of MINTO against other potential operators on 15 Atari games using the CNN architecture. Reported metrics are interquartile mean (IQM) scores with 95% confidence intervals across 5 seeds per game. 33 Preprint. Under Review. Figure 20: Cumulative results of benchmarking the Minimum operator of MINTO against other potential operators on 15 Atari games using the CNN architecture. Both figures show the interquartile mean (IQM) scores with 95% confidence intervals of the final performance of each operator. Left: The final performance of each operator in bar chart. Right: The performance curve for each operator over 50 million frames."
        }
    ],
    "affiliations": [
        "German Research Center for AI (DFKI)",
        "Hessian.AI",
        "Robotics Institute Germany (RIG)",
        "Technical University of Darmstadt",
        "University of Wurzburg"
    ]
}