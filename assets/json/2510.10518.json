{
    "paper_title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning",
    "authors": [
        "Qunzhong Wang",
        "Jie Liu",
        "Jiajun Liang",
        "Yilei Jiang",
        "Yuanxing Zhang",
        "Jinyuan Chen",
        "Yaozhi Zheng",
        "Xintao Wang",
        "Pengfei Wan",
        "Xiangyu Yue",
        "Jiaheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 3 8 1 5 0 1 . 0 1 5 2 : r VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning Qunzhong Wang1,2 Jie Liu1 Jiajun Liang2 Yilei Jiang1 Yuanxing Zhang2 Jinyuan Chen1 Yaozhi Zheng1 Xintao Wang2 Pengfei Wan2 Xiangyu Yue1 Jiaheng Liu3 1 CUHK MMLab 2 Kling Team, Kuaishou Technology 3 Nanjing University wangqunzhong@kuaishou.com, liujiaheng@nju.edu.cn Abstract Recent advancements in multimodal reward models (RMs) have substantially improved posttraining for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinkera (VR-Thinker), thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-ofthe-art accuracy among open-source models on video preference benchmarks, especially for longer videos: 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling. ahttps://github.com/qunzhongwang/vr-thinker"
        },
        {
            "title": "1 Introduction",
            "content": "With the advancement of multimodal Reward Models (RMs) (Wang et al., 2025b; Zang et al., 2025; Wang et al., 2024; Xiong et al., 2024; Liu et al., 2025b; Xu et al., 2024; He et al., 2024), the substantial potential of RMs in aligning vision models with human preferences (Liu et al., 2025a;b; Schulman et al., 2017; Ouyang et al., 2022) has garnered increasing attention, owing to their capacity to provide accurate reward signals during model training and fine-tuning processes (Liu et al., 2024; Wijaya et al., 2024). Most RMs are predominantly classifier-based or generative (Xiong et al., 2024; Wang et al., 2024; Li et al., 2025; Liu et al., 2025b; Wang et al., 2025c; Tong et al., 2025; Zang et al., 2025). After being trained on large, pre-annotated preference datasets, they typically either (i) directly output scalar scores (and, for pairwise data, relative preference rankings), or (ii) produce brief natural-language justifications along with judgments. The former mode tends to operate as black box, raising concerns about insufficient interpretability; the latter often relies on rudimentary reasoning, lacking concise logical structure and depth of analysis, thereby undermining accuracy. In light of these issues, recent work (Wu et al., 2025; Wang et al., 2025b; Hong et al., 2025; Chen et al., 2025) has proposed reasoning-based RMs to leverage the language generation capabilities of Visual Language Models (VLMs). By eliciting richer chains of reasoning, these approaches aim to produce multi-dimensional, logically structured, and more in-depth analyses, thereby improving the accuracy, robustness, and transparency of RMs. Despite these successes, inherent limitations remain in VLM-based RMs, particularly for video preference data. On the one hand, visual inputs consume substantial context budget, forcing RMs to process fewer frames and risking the loss of fine-grained details. On the other hand, all visual information is typically packed into the initial prompt; during the RMs Chain-of-Thought Equal contribution. Project Leader Corresponding author. 1 Figure 1: (a) shows the main process of our proposed Thinking-with-Image framework. (b) shows an overview of the three training stages we proposed, including Cold Start, Rejection sampling Fine-Tuning, and GRPO. (CoT) reasoning, the process proceeds purely in text without revisiting or updating visual evidence, which exacerbates forgetting and hallucination. In this work, we introduce novel thinking-with-image framework to address the aforementioned concerns by equipping the RM with visual reasoning operations like frame selection and configurable visual memory window (Wang et al., 2025b; Guo et al., 2025a; Su et al., 2025a). Frame selection enables the model to actively retrieve previously seen frames and acquire unseen visual evidence as new inputs to subsequent reasoning rounds, thereby improving fidelity. The configurable memory window retains only the most recently active visual information, ensuring that, under context-length constraints, the model can select frames multiple times, broaden its visual field, and extend both its reasoning horizon and the total number of frames it can process, while keeping the memory footprint stable. Building on this framework, we propose VR-THINKER, the first multimodal RM capable of visual reasoning. In principle, it imposes no upper bound on the number of frames it can process, enabling fidelity-preserving evaluation for long video reward tasks. Specifically, the training pipeline comprises three stages: (I) Cold Start. Using curated visual CoT data, we instill basic textual reasoning skills and operation formatting (e.g., invoke frame selection). (II) Rejection sampling Fine-Tuning. We run the model on large-scale preference datasets, which include fine-grained, per-dimension assessments alongside an overall judgment. We then retain only samples with all judgments correct, and conduct Rejection sampling Fine-Tuning on these verified traces to encourage accurate, high-quality visual and textual reasoning. (III) Group Relative Policy Optimization (GRPO). We apply GRPO on collected preference data, incentivizing the model to explore details in videos and optimize toward reward rules that favor high-quality reasoning with correct per-dimension and overall judgments. In summary, our contributions are as follows: We propose VR-THINKER, the first multimodal RM capable of visual reasoning, which substantially alleviates context-length constraints and mitigates forgetting of visual information. In VR-THINKER, we propose to equip the RM with visual reasoning operations like frame selection and configurable visual memory window based on thinking-with-image framework. 2 We demonstrate the crucial role of visual reasoning in multimodal RMs, showing improved accuracy and reliability on preference datasets and significantly increased usability and fidelity."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Reward Models (RMs) have garnered increasing attention (He et al., 2024; Liu et al., 2025b; Xu et al., 2024; Wang et al., 2025b) for their potential to effectively optimize vision generation models to better align with human preferences. Visual-language models (VLMs) (Bai et al., 2025; Bordes et al., 2024), have become the models of choice for RMs. For instance, Liu et al. (2025b) proposes VideoReward, reward model that directly regresses preference-aligned scores from input videos; Wang et al. (2025c) develops UnifiedReward in generative response format. However, such approaches often lack rigorous logical structure and deep analysis. To this end, Wang et al. (2025b) introduces reasoning framework in multimodal RMs, aiming to improve the accuracy of reward signals. Despite these advances, VLM-based RMs still face inherent limitations, especially on video preference datasets with more frames and longer durations (Liu et al., 2025b; Tong et al., 2025). Specifically, first, visual inputs consume substantial context budget, forcing RMs to subsample only subset of frames and thereby losing fine-grained details (Tong et al., 2025; Liu et al., 2025b; Wang et al., 2024; He et al., 2024; Xu et al., 2024). Second, during the RMs generative response, reasoning proceeds purely in text without revisiting or updating visual evidence (Wang et al., 2025b;c). Thinking-with-Image is an emerging paradigm in VLM reasoning that overcomes the limitations of text-centric chains of thought that treat visual inputs merely as static initial context (Shen et al., 2024; Mallis et al., 2024; Xu et al., 2025; Duan et al., 2025; Su et al., 2025b). Instead, it treats vision as dynamic, operable cognitive workspace, leveraging visual information throughout intermediate reasoning steps. Two primary modes characterize this paradigm: (1) Intrinsic imagination, which allows the model to reason directly over the corresponding visual tokens (Team, 2024; Xu et al., 2025; Guo et al., 2025b). (2) Active exploration, which enables the model to proactively retrieve visual information via toolchain invocation (the VLM calls external tools through specified interface) or programmatic manipulation (the VLM emits executable code that directly defines operations) (Shen et al., 2024; Mallis et al., 2024; Wang et al., 2025a;d)."
        },
        {
            "title": "3 VideoReward Thinker",
            "content": "In this section, we first elaborate on the concrete components of the Thinking-with-Image framework (Section 3.1). We then present the multi-stage training pipeline, explaining how it elicits and cultivates multimodal reasoning capabilities in both vision and text (Section 3.2)."
        },
        {
            "title": "3.1 Thinking-with-Image-Based Framework",
            "content": "The data flow of VR-THINKER under our Thinking-with-Image framework is shown in Figure 1. Video preference data are uniformly downsampled into preset number of input frames as visual input and paired with prompt template that explicitly specifies the total number of frames and the downsampling scheme. The model then iteratively performs tool invocations and updates its reasoning with the toolexecution outcomes; these outcomes remain valid only within preset window. To mitigate the risk of information loss, the reasoning format converts visual evidence into linguistic summaries within specific regions. Tool Invocation. Consistent with standard VLMs used as reward models, our model requires downsampling for video inputs. However, instead of treating the unselected visual content as if it does not exist, we retain it as an operable workspace that the model is aware of. After an initial round of multimodal reasoning, the model may find that missing evidence (or near-ties in paired preference cases) prevents definitive judgment, which potentially is due to the unselected visual information. The model then iteratively issues tool-invocation instructions to retrieve additional visual evidence, and updates its reasoning by incorporating the tool-execution outcomes, repeating this process until final preference judgment can be made. Formally, the initial input is = [V, T], where is the downsampled visual input and is the textual query. model πθ constructs multimodal reasoning chain via iterative reasoning and tool invocation, where at each step the model first yields reasoning unit rt πθ( , Rt1), conditioned on the initial input and all preceding 1 reasoning steps denoted by Rt1. Each reasoning unit may be purely textual or multimodal; the latter can then invoke tool to directly interact with information in the whole visual workspace ( denoted by V, to distinguish V). For each multimodal reasoning step rt, the model 3 Figure 2: Qualitative Cases. When frames are downsampled, key information might not be included in the input. VR-THINKER actively retrieves frames, which ensures the correctness of such cases. calls tool , obtains tool-execution outcome ot = ( V), for subsequent reasoning steps. Window Memeory. The reasoning process does not, by default, retain all tool-execution outcomes. Instead, we employ windowed memory: each outcome remains active for preset number of rounds before being deliberately forgotten. This design is motivated by the substantial context budget consumed by visual information, especially for longer videos where frames dominate the context: In multimodal reasoning, the textual portion per segment Rn typically occupies less than 400 tokens, while single visual frame contributes roughly 500 tokens. With default of 8 input frames, visual evidence accounts for approximately 4,000 tokens, around 10 the textual budget. Under the windowed memory, the total context usage remains relatively stable, preventing bottlenecks from repeatedly retrieving additional visual information through tool invocation. Formally, after each update, we maintain the entire prefix of reasoning units but only with sliding window over the most recent tool outcomes: Let Rt1 denote the prior reasoning chain, rt the new reasoning unit. The update process can be described as: Rt1 = [r1, r2 . . . , rtp2, (rtp1, otp1), . . . , (rt1, ot1)] rt πθ( , Rt1), where tool is called ot = (V ) Rt = [r1, r2 . . . , rtp1, (rtp, otp), . . . , (rt, ot)] , where is the window width and (rk, ok) denotes reasoning unit paired with its tool-execution outcome retained within the window. The total token count Ttotal till step is (X ) + ( Rt) = (V ) + (T) + k= (rk) + k=tp (ok) (V ) + k=tp (ok), where () denotes the number of tokens and we approximate textual tokens as minor component relative to visual tokens. Further, approximating token costs by per-frame contributions, we obtain Ttotal (Nin + pNex)Vt, where Nin is the number of initial input frames, Nex is the number of frames retrieved per tool invocation, is the window width, and Vt is the average token cost per visual frame. Crucially, Ttotal is approximately independent of the total number of reasoning steps t, highlighting how windowed memory sustains the context budget under this setting. Reasoning Format. As shown in Figure 2, the model is required to follow specific reasoning format, using XML-style tags to delineate functional areas and reasoning-focus categories, which helps ensure clarity and consistency in reasoning and logical structure. In addition to commonly used tags like <think> and <answer> in reasoning models, two additional tags are employed: <Snapshot>: This tag is used in every reasoning segment to mitigate the risk of forgetting 4 critical information under the Window Memory mechanism. After each execution outcome is incorporated, this tag is used to create snapshot of essential information from these frames in the form of language tokens. This approach serves as an information compression strategy, reducing thousands of visual tokens to dozens of language tokens, which balances fidelity and budget. <Recommend Answer>: Unlike the <answer> tag, this tag is used in non-final reasoning segments. The model outputs its current preferred result along with the confidence level, which helps assess the value of additional multimodal reasoning segments and also aids the model in organizing its current judgments."
        },
        {
            "title": "3.2 Multi-Stage Reward Model Training",
            "content": "The training pipeline consists of three main stages: (i) Cold Start efficiently elicits textual reasoning skills and bootstraps basic visual reasoning. (ii) Rejection sampling Fine-Tuning consolidates both textual and visual reasoning capabilities. (iii) Exploratory Reinforcement Learning reinforces the integrated multimodal reasoning ability. 3.2.1 Cold Start & Rejection Fine-Tuning Cold Start. This stage serves two purposes. First, VLMs have limited zero-shot ability to execute novel tool invocations. To ensure accurate reasoning structure and tool-calling syntax, we employ CoT data that adheres to our reasoning format. Second, although VLMs possess strong latent linguistic reasoning capabilities, inadequate reward modeling often leads to underdeveloped reasoning behavior. Highquality Cold Start CoT data not only elicits linguistic reasoning but also bootstraps basic visual reasoning through vision-related analytical steps embedded in the trajectories. Concretely, we construct Cold Start data by selecting subset of video pairs and textual queries from video preference dataset. Following the Think-with-Image framework, we iteratively invoke powerful multimodal model, e.g. GPT-4o (Hurst et al., 2024), to generate high-quality, long CoT trajectories. two-stage filtering process ensures that these multimodal CoTs are suitable for initialization: (i) every reasoning segment must strictly conform to the prescribed format, and (ii) the final judgments, both perdimension and overall preference, must exactly match the ground-truth labels in the preference dataset, thereby guaranteeing high-accuracy multimodal reasoning. We train with the standard Supervised Fine-Tuning (SFT) loss during this Cold Start phase, while masking tokens associated with tool-execution outcomes from the loss computation. Rejection sampling Fine-Tuning. The previous stage instilled the correct reasoning format and highquality multimodal CoT exemplars, initializing the models reasoning capabilities. However, the proportion of model-generated CoT samples that are both well-formed and accurate remains low. An excess of negative samples due to limited Cold Start data and training epochs hampers the efficiency of sampling-based reinforcement learning. To consolidate the learned reasoning skills and increase the yield of high-quality reasoning segments, thereby paving the way for RL. we perform Supervised Fine-Tuning on large, rejection-sampled multimodal CoT dataset. Specifically, we blend multiple video preference datasets and select large subset of videoquery pairs. Similar to the previous stage, we generate CoT samples, but now we sample from the model trained in Stage 1, drawing multiple samples per input to ensure sufficient positives. The same two-stage filtering is applied to construct the SFT dataset. We use the same loss as in the Cold Start phase, with tool-execution outcome tokens masked from the loss. This stage substantially improves both the format compliance and quality of the models reasoning segments. 3.2.2 Exploratory Reinforcement Fine-Tuning To further reinforce multimodal reasoning on top of these capabilities we apply GRPO-based reinforcement fine-tuning. Using predefined rule-based reward functions together with additional exploratory incentives, we evaluate the model-sampled reasoning segments and iteratively optimize the model toward producing higher-quality reasoning. GRPO is employed to assess the quality of multimodal CoT reasoning via rule-based reward functions, which are both accurate and robust. For each query, GRPO draws multiple samples and compares the relative quality of the resulting samples, iteratively nudging the model toward higher-quality reasoning segments and thereby improving its capabilities (Guo et al., 2025a; Shao et al., 2024). We follow the standard GRPO framework while incorporating several new practical tricks to enhance training efficiency and stability, as detailed in prior works (Yu et al., 2025). full description of GRPO is provided in Appendix A.1. Rule-Based Reward is the primary foundation for providing reward signals to the model; its relative magnitude determines the ranking among CoT samples. We employ the classic Format Reward and 5 Accuracy Reward as follows: (1). Format reward ensures the correctness of the models response structure. Specifically, it requires that the reasoning content be delineated with the correct tags, and that the answers provided in <Recomend Answer> and <Answer> adhere to the specified requirements. (2). Accuracy reward evaluates the factual correctness of the models reasoning. It consists of both per-dimension judgments and an overall preference. An important underlying assumption for GRPOs effectiveness is that if the result satisfies the correctness rules, then the corresponding CoT reasoning sample should reflect high-quality, accurate reasoning process, thereby truly incentivizing the desired reasoning trajectory. In conventional RM training, accuracy is assessed only by whether the correct preference is chosen, where the answer space is limited to just three options: former, latter, and tie (Wang et al., 2025b;c). This contradicts our assumption, since many trajectories may have suboptimal multimodal reasoning and insufficient factual grounding yet still produce the correct final answer. Such cases introduce misleading reward signals, reducing efficiency and steering learning in the wrong direction, which harms stability. In contrast, we incorporate both per-dimension judgments and the overall preference. This expands the answer space to 3d+1, where is the number of dimensions. For more on sampling efficiency and answer space analysis, please refer to Appendix A. Formally, the accuracy reward can be written as: racc = α racc_all + α racc_dim, where α + α = 1, racc_all = 1(Jall = ˆJall), racc_dim = 1 i= 1(Jdim_i = ˆJdim_i). where Jall is the overall judgment, Jdim_i is the judgment for the i-th dimension, and ˆJall, ˆJdim_i denote the respective ground truths. The function 1() is an indicator function that returns 1 if the condition is true and 0 otherwise. α is tunable hyperparameter that controls the relative importance of the overall preference and the per-dimension judgment. CoT Gain Reward is designed to reward the improvement in accuracy brought by the updated answers in each reasoning segment. This reward is intended to encourage the model to obtain more visual evidence through visual reasoning, update its conclusions with greater accuracy and factual alignment, and thereby strengthen its visual reasoning abilities: rcot = (cid:33) ri , (cid:32)t1 i=1 where ri = ri+1 acc represents the improvement in the accuracy reward between successive updates in the reasoning chain. Here, denotes the i-th reasoning step, is the total number of reasoning steps, and is hyperparameter used to control the degree of the reward. acc ri Exploratory Incentive is designed to prevent the model from defaulting to textual reasoning, which can reduce or even degrade its visual reasoning capabilities (Su et al., 2025a). As stated earlier, VLMs inherently possess stronger textual reasoning abilities compared to visual reasoning. During the GRPO process, two factors exacerbate this issue: first, errors in visual tool invocation can lead to negative rewards; second, certain proportion of queries can achieve decent results through purely textual reasoning, making it difficult for the model to overcome local optimum . To encourage exploration, we enforce lower bound on the proportion of multimodal reasoning produced by the model. This turns the RL objective into constrained optimization problem, which can be converted into an unconstrained one via Lagrangian Relaxation, as detailed in Appendix A. Formally, the transformed objective can be viewed as adding an auxiliary exploratory reward rexplo: rexplo = max(ω R(X), 0) 1mul(R), where ω represents the lower bound on the proportion, R(X) denotes the proportion of multimodal reasoning in the samples for the query X, and 1mul() is an indicator function that determines whether the sample corresponds to multimodal reasoning."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. For training, we use three datasets: VideoGen-Reward (182k) (Liu et al., 2025b), MJ-Bench-Video (train) (8.7k) (Tong et al., 2025), and Text2Video-Human Preferences (2.6k) by Rapidata1. In addition, we 1https://huggingface.co/datasets/Rapidata 6 distill 1.2k high-quality Multimodal CoT Cold Start samples from GPT-4o (Hurst et al., 2024); these are randomly drawn in proportion from blend of the three training datasets, and the corresponding original samples are excluded from subsequent training stages. For benchmarking, we evaluate on the video part of GenAI-Bench (Jiang et al., 2024), VideoGen-RewardBench (Liu et al., 2025b), and MJ-Bench-Video (test) (Tong et al., 2025). More details on dataset processing and settings are provided in Appendix B. Base Model. As VLM-based reward model, VR-THINKER is initialized from Qwen2.5-VL-7B (Bai et al., 2025), which has strong visual understanding and video temporal perception capabilities. This provides solid foundation for unlocking the models multimodal reasoning potential in long-video scenarios. Benchmarking. We compare multiple baseline reward models and VR-THINKER using greedy decoding across the aforementioned video preference benchmarks. These benchmarks span wide range of topics and originate from various video generation models (Liu et al., 2025b; Tong et al., 2025; Jiang et al., 2024), ensuring generality of evaluation. We provide detailed descriptions of the baseline models and benchmark datasets in Appendix B. For more detail, please refer to our code at https://github.com/ qunzhongwang/vr-thinker."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 compares VR-THINKER against range of high-performing reward models. Across both evaluation protocols, tau (which accounts for ties) and diff (which excludes ties), our model achieves state-of-theart performance, significantly surpassing both classic classifier-based and generative-based models, with an average improvement of up to 11.4%. It also outperforms emerging reasoning-style models, owing to our model cultivating not only textual reasoning but also visual reasoning capabilities; when datasets contain more frames than the preset input limit, typical RMs that rely on downsampling inevitably miss key information, whereas our model achieves higher accuracy by processing frames without predetermined limits. Moreover, compared with UNIFIEDREWARD and UNIFIEDREWARD-THINK (Wang et al., 2025b;c), which are both trained on multiple tasks spanning image and video datasets to obtain substantial mutual benefits, our model is trained purely on video preference datasets, yet still surpasses these mutual benefits. These results provide strong evidence for the effectiveness and superiority of our Thinking-with-Image framework, which shows the positive impact of multimodal reasoning for reward models. For further experiments, please refer to the additional experiments section in Appendix C."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "Ablation of Visual Reasoning In our VR-THINKER framework, we perform tool invocation via Thinking with Image to retrieve visual information and enable multimodal reasoning. To assess the effectiveness of visual reasoning within each reasoning segment, we conduct an ablation on the usefulness of retrieved visual information during tool invocation. Specifically, we compare retrieval guided by the models visual reasoningdriven tool invocations against randomly retrieving information from the same video data regardless of the tool invocation. As shown in Figure 3, the random strategy yields clear performance drop, demonstrating that visual reasoning is indispensable for discovering the additional visual evidence needed for reliable judgments. Ablation of Training pipeline We adopt multi-stage training pipeline and hence conduct ablations on each stage. Following prior work on reasoning-based general models and reward models(Wang et al., 2025b; Guo et al., 2025a), our ablations center on GRPO-based reinforcement fine-tuning, comparing the gains from the cold-start and Rejection sampling Fine-Tuning stages on the final GRPO-trained model. As shown in Figure 3, GRPO contributes the most substantial performance improvement, while both cold start and Rejection sampling Fine-Tuning provide crucial reasoning foundations that further boost postGRPO performance. Notably, the gains from Rejection sampling Fine-Tuning are especially pronounced, likely because it increases the likelihood of high-quality reasoning segments, thereby improving the efficiency of GRPO-driven improvements. Ablation of Auxiliary Reward Setting In the GRPO stage, we augment the standard format and rule-based accuracy rewards (Shao et al., 2024) with several auxiliary rewards. We conduct ablation studies to quantify the impact of these auxiliary rewards, with results shown in Figure 3. We observe clear performance drops when removing the CoT gain reward and the exploratory incentive. Notably, removing the CoT gain reward has more pronounced negative effect, highlighting its importance in encouraging the reward model to attempt multimodal reasoning. Ablation of Different Accuracy Reward Signals. In the GRPO stage, beyond the auxiliary rewards described above, we specially design the accuracy reward as linear combination of the overall reward 7 Table 1: Preference accuracy on evaluation dataset. tau: accuracy is calculated with ties included; diff excludes tied pairs when calculating accuracy. Best performance in Bold. Model Protocol VideoScore VideoReward VisionReward LiFT UnifiedReward Size 7B 2B 13B 13B 7B GenAI-Bench tau (%) diff (%) VideoGen-Reward tau (%) diff (%) MJBench-Video tau (%) diff (%) Classifier-based Reward Models 47.5 49.9 52.6 70.9 73.1 72. 41.9 60.8 57.9 Generative-based Reward Models 38.1 61.2 59.4 76.8 40.1 67.1 Reasoning-based Reward Models 50.2 73.8 68.4 57.9 78.6 79.1 80.5 57.9 56.8 54.1 42.5 63.3 62.8 67. 63.5 62.6 65.2 51.4 69.5 71.9 75.6 UnifiedReward-Think VR-THINKER 7B 7B 64.7 68. 80.4 82.3 69.7 71.8 Figure 3: The results of ablation studies are summarized in this figure: (1) investigates the ablation of visual reasoning; (2) examines the impact of different training stages on the final model performance; (3) explores ablations of different auxiliary reward settings; and (4) studies the ablation of different accuracy reward signals by our modification of the accuracy reward. and per-dimension reward to enlarge the answer space. We conduct ablations to assess their effects, comparing three settings: using only the overall reward, using only the per-dimension reward, and using 50/50 mix of overall and per-dimension rewards (the setting we adopt). The results, shown in Figure 3, validate the benefits of the mixed scheme."
        },
        {
            "title": "4.4 Further Analysis",
            "content": "Visualization on GRPO Training For deeper analysis of the GRPO stage and the differences in training under various baselines, we provide visualization of GRPO training in Figure 4. It highlights the models changes in evaluation accuracy, average number of tool invocations per sample, and average length per reasoning segment in different experimental settings, including: setting of VR-THINKER, without exploratory reward, without per-dimension accuracy reward (α = 1), and without overall accuracy reward (α = 0). Error Analysis To more rigorously validate that our RM on long videos and complex reasoning scenarios, we conduct an error analysis. Standard video preference datasets comprise videos of varying lengths 8 Figure 4: The training dynamics of the GRPO stage: (1) accuracy on GenAI-Bench throughout training; (2) average tool invocations per sample; (3) average reasoning segment length. Table 2: Preference accuracy on Long Video and Complex Prompt subset. tau: accuracy is calculated with ties included; diff excludes tied pairs when calculating accuracy. Best performance in Bold. Long Video Model Protocol LiFT UnifiedReward UnifiedReward-Think VR-THINKER Model Protocol LiFT UnifiedReward UnifiedReward-Think VR-THINKER Size 13B 7B 7B 7B Size 13B 7B 7B 7B GenAI-Bench (long) tau (%) diff (%) VideoGen-Reward (long) diff (%) tau (%) MJBench-Video (long) diff (%) tau (%) 36.0 56.8 61.7 66.2 56.5 71.6 76.4 81.4 35.8 63.5 65. 70.9 Complex Prompt 53.6 72.2 76.7 79.6 39.5 59.6 60.1 66. 50.4 67.3 69.6 74.8 GenAI-Bench (complex) diff (%) tau (%) VideoGen-Reward (complex) MJBench-Video (complex) tau (%) diff (%) diff (%) tau (%) 37.6 58.8 63.9 68.4 58.7 74.9 79.8 81.9 40.5 65.2 68. 70.6 57.6 76.6 78.2 80.7 39.8 62.4 60.5 66.3 50.8 69.1 70. 74.3 produced by multiple generators and prompted at different complexity levels. For instance, in VideoGenRewardBench, 16.4% of videos contain roughly 49 frames, whereas 15.7% contain approximately 173 frames, resulting in 3.5 disparity. Shorter videos are typically easier for baseline models, obscuring our advantage in visual reasoning, while higher prompt complexity further increases content richness and alignment demands, thereby making RM evaluation more challenging. To better assess our model under these difficult scenarios, especially in comparison to native generative outputs and text-only reasoning paradigms (namely, LIFT, UNIFIEDREWARD, and UNIFIEDREWARD-THINK), we perform secondary filtering of each dataset to construct two hard subsets by selecting the top 10% by video length and the top 10% by prompt length. Results are reported in Table 2. It can be seen that, compared with baseline models, VR-THINKER shows smaller drop in accuracy on all of the hard subsets."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce VR-Thinker, the first multimodal RM capable of visual reasoning. VR-Thinker leverages the Thinking-with-Image framework to alleviate context-length constraints and mitigate forgetting of visual information. We adopt three-stage training pipeline to progressively enhance both textual and visual reasoning abilities. Extensive experiments shows the effect of our framework, which improves the accuracy of preference judgments and the interpretability of reward signals, laying solid foundation for better alignment with human preferences in future video generation models. 9 References Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra. An introduction to visionlanguage modeling, 2024. URL https://arxiv.org/abs/2405.17247. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, and Heng Ji. Rm-r1: Reward modeling as reasoning, 2025. URL https://arxiv.org/abs/2505.02387. Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, and Xihui Liu. Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025b. Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Yuchen Lin, and Wenhu Chen. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, Haoming Jiang, Qingru Zhang, Qin Lu, Xin Liu, Chao Zhang, and Tuo Zhao. Think-rm: Enabling long-horizon reasoning in generative reward models, 2025. URL https://arxiv.org/abs/2505.16265. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024. Claude Lemaréchal. Lagrangian relaxation. Acta Numerica, 10:379478, 2001. Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, and Jian Zhang. Q-insight: Understanding image quality via visual reinforcement learning. arXiv preprint arXiv:2503.22679, 2025. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, and Wanli Ouyang. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025b. Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. Dimitrios Mallis, Ahmet Serdar Karadeniz, Sebastian Cavada, Danila Rukhovich, Niki Foteinopoulou, Kseniya Cherenkova, Anis Kacem, and Djamila Aouada. Cad-assistant: Tool-augmented vllms as generic cad task solvers? arXiv preprint arXiv:2412.13810, 2024. 10 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35:2773027744, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. arXiv preprint arXiv:2411.16044, 2024. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning, 2025a. URL https://arxiv.org/ abs/2505.15966. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, and Yi R. Fung. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers, 2025b. URL https: //arxiv.org/abs/2506.23918. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Haibo Tong, Zhaoyang Wang, Zhaorun Chen, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, Mingyu Ding, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Mjvideo: Fine-grained benchmarking and rewarding video preferences in video generation, 2025. URL https://arxiv.org/abs/2502.01719. Haozhe Wang, Chao Du, Panyan Fang, Shuo Yuan, Xuming He, Liang Wang, and Bo Zheng. Roiconstrained bidding via curriculum-guided bayesian reinforcement learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 40214031, 2022. Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao. Vrag-rl: Empower vision-perception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning. arXiv preprint arXiv:2505.22019, 2025a. Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024. Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Unified multimodal chain-of-thought reward model through reinforcement fine-tuning, 2025b. URL https: //arxiv.org/abs/2505.03318. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Wang Jiaqi. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025c. Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, and Matthew Blaschko. Jigsaw-r1: study of rule-based visual reinforcement learning with jigsaw puzzles. arXiv preprint arXiv:2505.23590, 2025d. Robert Wijaya, Ngoc-Bao Nguyen, and Ngai-Man Cheung. Multimodal preference data synthetic alignment with reward model, 2024. URL https://arxiv.org/abs/2412.17417. Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. Visionreward: Finegrained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. 11 Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Lets think only with images, 2025. URL https://arxiv.org/abs/2505.11409. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2.5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025."
        },
        {
            "title": "Appendix",
            "content": "The appendix of this paper is organized as follows: Appendix provides mathematical details and derivations omitted from the main text; Appendix supplements additional experimental details; Appendix presents more extensive experimental results; Appendix includes prompt templates. Appendix will describe the limitations."
        },
        {
            "title": "A Mathematical Analysis",
            "content": "A.1 Mathematical details of the training pipeline Supervised fine-tuning (SFT) loss. As mentioned in Section 3.2.1, our training comprises two major stages: Cold Start and Supervised Fine-Tuning. For high-quality CoT data constructed via the specific pipeline, we use the standard supervised fine-tuning loss while masking tokens associated with toolexecution outcomes from the loss computation. Formally, in the multireasoning-segment setting, the SFT loss is: Lsft(θ) = i=1 Ni j= log (cid:0)ri,j , (r1, o1), . . . , (ri1, oi1), ri,<j; θ(cid:1) , (1) where θ denotes the parameters of the reward model (RM), = [V, T] represents the pair of the initial visual input and the query template T, ri is the i-th reasoning segment, ri,j is the j-th token of the i-th reasoning segment, oi is the i-th tool-execution outcome, Ni is the total number of tokens in the i-th reasoning segment, and is the total number of CoT steps. GRPO Algorithm. As mentioned in Section 3.2.2, GRPO-based reinforcement fine-tuning is employed because the rule-based reward function provides robust reward signal to nudge the model toward generating higher-quality reasoning segments. The specific algorithm is similar to the one described in Shao et al. (2024), with some novel practical tricks introduced in Yu et al. (2025). For each input = [V, T] (the pair of the initial visual input and the query template T), set of CoT samples is randomly drawn from the same model πθ(), denoted as = { R1,t1, . . . , Rn,tn }, where refers to the number of sampled CoT examples, and Ri,ti represents the i-th CoT sample with ti reasoning segments. predefined reward function () = fi() is applied to each sample, resulting in fi(R1,t1 fi(Rn,tn ) = {s1, . . . , sn} ), . . . , , where the specific () in our setting is defined as: = { () = ffmt() + facc() + fcot() + η fexplo(), where β and η are adjustable hyperparameters, predefined here for simplicity. This is followed by intra-group normalization to calculate the advantage for each sample: Ai = {si µ(S)}/σ(S), where µ(S) represents the mean of the scores in the set and σ(S) represents the standard deviation of the scores in the set S. Subsequently, the likelihood ratio of each response is computed to guide the model toward higher-quality reasoning segments: πθ(ri,t , (r1, o1), . . . , (ri1, oi1), ri,<t) (ri,t , (r1, o1), . . . , (ri1, oi1), ri,<t) πθold where πθ represents the new policy and πθold represents the old policy. The final optimization objective in GRPO is: ζi,t = , Jgrpo(θ) = [X D, Ri,ti πθold ] 1 ( Ri,ti ) ) ( Ri,ti t=1 {[min (ζi,t, clip(ζi,t, , 1 ε, 1 + ε)) Ai] βD KL[πθ πref]} where represents the dataset, ( Ri,ti sample, clipping within 1 ε ensures training stability, and the model update range. ) denotes the total number of tokens in the multimodal CoT KL is the KL divergence penalty to constrain 13 As previously studied in Yu et al. (2025), we incorporate Dynamic Sampling improvement into our GRPO training algorithm. Specifically, when drawing batch of samples, if the accuracy is 1 or 0, the entire batchs advantage becomes zero, yielding zero gradients for that batch. This effectively reduces the gradient-accumulation batch size, increases noise sensitivity, and lowers sample efficiency. The issue worsens as training progresses and accuracy rises, since fully correct cases become more frequent, leading to more zero-gradient batches. Dynamic Sampling mitigates this by filtering out batches whose accuracy is 1 or 0 and resampling until all batches yield nonzero gradients, thereby improving training efficiency. Sampling efficiency and answer-space in GRPO. We first analyze, as in Section 3.2.2, how the size of the answer space affects GRPO sampling and learning efficiency. Let the answer space size be N, the observed model accuracy be p, the models intrinsic accuracy be (interpreted as finding the key information correctly and thus making the correct judgment), and the proportion of invalid samples be (failing to find the key information, yet coincidentally producing the correct judgment). We have: = + (1 q)/N, = (1 q)/(N) = (1 p)/(N 1). (1) (2) For the (1 q) fraction of samples where key information is not found, the models judgment can be viewed as randomly selecting an answer from an answer space of size N, which yields an additional accuracy of (1 q)/N, giving Equation (1). For Equation (2), although these (1 q)/N samples happen to produce correct judgments, their reasoning lacks the key information and is thus off-point; we term them invalid samples. In reinforcement learning (RL), assigning these samples high advantage and increasing their likelihood is not only unhelpful for improving the model, but can be harmful. The expression (1 p)/(N 1) thus provides an estimate of the proportion of such invalid samples. Take the observed accuracy as an intermediate value during training, say 0.7. Then: For = 3 (setting in classic RM training), the estimated invalid data proportion is = (1 0.7)/2 = 15%. For = 3d+1 = 81 (our setting with = 3), the estimated invalid data proportion is = (1 0.7)/80 = 0.375%, which greatly reduces the fraction of invalid data and improves sampling effectiveness. Next, we analyze the impact of accuracy in Dynamic Sampling, as stated in A.1. Denote the batch sample size by n. The probability that batch is entirely correct or entirely wrong is: Taking = 0.7 and = 8, we get: = pn + (1 p)n. = 0.78 + (1 0.7)8 = 16.7%. Without Dynamic Sampling mechanism, this nontrivial fraction of ineffective batches would indeed hamper training. A.2 Derivation of the GRPO Exploratory Incentive Here, we provide more detailed explanation of the design and derivation of the Exploratory Incentive. The reason the Exploratory Incentive is not directly designed as an auxiliary reward that increases according to the multimodal CoT ratio R, which would be simpler, is because merely adding rewards may lead to reward hacking. In such cases, the model might excessively prioritize generating visual CoTs, resulting in useless reasoning that hinders the development of well-integrated multimodal reasoning capabilities. Inspired by Su et al. (2025a), we transform this problem into constrained optimization problem. This ensures that the final optimization objective does not explicitly contain the multimodal CoT ratio R, thereby avoiding the issue of reward hacking. Meanwhile, by incorporating the multimodal CoT ratio into the constraints, we achieve the goal of preventing degeneration and maintaining the desired behavior. Formally, the original reinforcement learning problem is an unconstrained optimization problem, written as: (cid:2)r(X , Rt) (cid:12) (cid:12) D, Rt πθ( )(cid:3) , max θ where r(X , Rt) represents the reward, is the input sampled from the dataset D, and Rt is the CoT sample with reasoning steps generated by the policy πθ( ). After adding constraints, the optimization problem becomes constrained one: (cid:2)r(X , Rt) (cid:12) (cid:12) D, Rt πθ( )(cid:3) max θ subject to, R(X ) ω (2) (3) 14 Where R(X) denotes the proportion of multimodal reasoning in the samples for the query. The constraint can be rewritten as g(X , θ) = ω R(X ) 0. We apply the Lagrangian Relaxation method (Lemaréchal, 2001) to incorporate this constraint into the optimization objective. Unlike the standard Lagrangian method, which rewrites the objective as: rnew(X , Rt) = r(X , Rt) λ (ω R(X )), where λ 0 is the Lagrange multiplier, we instead follow the approach described in Su et al. (2025a); Wang et al. (2022), which uses the formulation: where η 0 is fixed hyperparameter. rnew(X , Rt) = r(X , Rt) + η max(ω R(X ), 0) 1mul( Rt), This formulation preserves equivalence to the original constrained objective while offering significant benefits during GRPO: unlike standard Lagrangian methods, where the multiplier λ needs to be dynamically adjusted, as derived in Wang et al. (2022), this structure avoids that requirement. Instead, it allows η to be treated as fixed hyperparameter. By pre-selecting η, this transformation can then be interpreted during RL training as adding an additional exploratory incentive reward, making the computation highly convenient: rexpo = max(ω R(X ), 0) 1mul( Rt)."
        },
        {
            "title": "B Detailed Experimental Settings",
            "content": "B.1 Training Details. Pipeline details. For the cold start and Rejection sampling Fine-Tuning data, we referenced and modified the TRL code. For CoT samples, we compute the SFT loss (as stated in A.1) with batch size of 1 and set gradient accumulation steps to 32. For the GRPO stage, we adopt and adapt the OpenRLHF training code. In each batch, the number of queries is set to 64, and the number of responses per query is set to 8; accordingly, the samples collected per training batch total 512. We update the behavior policy model with the improved policy model every 4 batches, corresponding to experience from 256 queries. 8 NVIDIA A800 (80GB) GPUs are used for both the cold start and Rejection sampling Fine-Tuning stages, while 32 NVIDIA A800 (80GB) GPUs are used for the GRPO stage. Hyperparameters. For cold start and Rejection sampling Fine-Tuning, we use learning rate of 1.5 106 with warm-up ratio of 0.2. During the GRPO stage, we use learning rate of 106 with KL penalty coefficient of β = 0.01. Additionally, for reward-related hyperparameters: α, which controls the balance between per-dimension and overall preference in the accuracy reward, is set to 0.5, selected via parameter search. The parameter k, which controls the strength of the CoT gain reward, is set as 0.2 to balance emphasizing visual reasoning and avoiding excessive strength that could cause reward hacking (see Appendix for detailed analysis). For η, the hyperparameter governing the exploratory incentive reward as detailed in Appendix A.2, we set it to 0.5; correspondingly, the minimum multimodal reasoning ratio in the constraint, ω, is set to 0.2. For the window width p, we default to 1, considering GPU memory limitations and the <Snapshot> mechanisms preservation of salient information. B.2 Compared Baselines. We compare our model against range of leading, high-performing reward models. We categorize the compared models into three major classes: classifier-based reward models, generative-based reward models , and reasoning-based reward models. Classifier-based Reward Models. These methods build on VLMs but replace the final linear layer of the VLMs LLM backbone. Instead of outputting next-token distribution, they retrain linear head to directly produce per-dimension or overall scores (or preferences). In this paradigm, the RMs include VideoScore (He et al., 2024), VisionReward (Xu et al., 2024), and VideoReward (Liu et al., 2025b). They leverage VLMs strong capabilities for understanding and embedding visual information to produce preference judgments in single classifier step. While the risk of reward hacking has been highlighted when aligning preferences with such models, such RMs that directly judge visual information still provide strong baselines. Generative-based Reward Models. These models leverage the VLMs intrinsic understanding and generating ability without modifying the model; instead, they treat preference decisions as visuallanguage task. By using prompt templates, they tap into the VLMs comprehension and generative 15 capabilities to produce responses and preference judgments. Representative RMs in this paradigm include LiFT-Critic (Wang et al., 2024) and UnifiedReward (Wang et al., 2025c), which, even without eliciting reasoning, fully leverage VLMs visionlanguage alignment and serve as strong baselines. Reasoning-based Reward Models. This emerging class recognizes the close relationship between preference judgment and reasoning, and the positive impact of logical reasoning on producing more accurate outcomes. Models in this category include UnifiedReward-Think Wang et al. (2025b), which, via RL-centric training pipelines, elicits the models textual reasoning ability, yielding strong reasoningdriven baselines that exploit VLMs. Our newly proposed VR-THINKER also belongs to this category but further introduces multimodal reasoning, breaking the VLMs inherent processed-frame limitation and reducing risks of forgetting induced by purely textual reasoning. B.3 Datasets and usage settings Training data setup. As noted in Section 3.2, we compute the accuracy reward using both per-dimension and overall preferences, which our ablation shows to be crucial. This requires datasets annotated with per-dimension preferences-something that is non-trivial. Many preference datasets used for training, such as VideoDPO (Liu et al., 2024) and LiFT-HRA (Wang et al., 2024), provide only an overall preference and thus are not usable for our reward design. We therefore select fine-grained datasets with per-dimension labels: VideoGen-Reward (182k) (Liu et al., 2025b), MJ-Bench-Video (train) (8.7k) (Tong et al., 2025), and Text2Video-Human Preferences (2.6k) by Rapidata 2. Due to differing annotation schemes and label contents, we still need to harmonize fine-grained annotations across datasets: Dimension selection. MJ-Bench-Video (train) includes 5 high-level preferences and up to 28 fine-grained preferences. We align its dimensionality with VideoGen-Reward and Text2Video-Human Preferences by selecting three core dimensions: Alignment, Fineness, and Coherence & Consistency. Dimension semantics. Since dimension titles differ across datasets, we take two steps:(i) For each dataset, we include dataset-specific explanation in the prompt that clarifies the meaning of each dimension as detailed in Appendix D. (ii) We map dimensions with different names but similar semantics to common triad: VideoGen-Rewards Text Alignment, Visual Quality, and Motion Quality; MJ-Bench-Videos Alignment, Fineness, and Coherence & Consistency; and Rapidatas Text2Video-Human Preferences Alignment, Preference 3, and Coherence. Although the labels differ in name, they consistently target: (1) alignment to the prompt, (2) intrinsic visual quality, and (3) temporal coherence/motion. This allows the model to learn the underlying correspondences without being misled by naming differences, projecting knowledge onto these three core dimensions. Benchmarking data setup. As noted above, we evaluate on three high-quality video preference datasets, GenAI-Bench (Jiang et al., 2024), VideoGen-RewardBench (Liu et al., 2025b), and MJ-Bench-Video (Tong et al., 2025), which also serve as mainstream leaderboards for video preference (Wang et al., 2025b). Each dataset contains entries which consist of prompt, pair of videos generated from the same prompt (by different models or by different seeds of the same model), and human expert annotations of preference, including an overall preference and, in some cases, per-dimension preferences. For example, VideoGen-RewardBench includes three additional per-dimension metrics: Text Alignment, Video Quality, and Movement Quality; MJ-Bench-Video includes five high-level categories and up to 28 fine-grained preferences; GenAI-Bench provides only an overall preference. To align evaluation with both the leaderboards and our training setup, we keep the same prompt template and required response format as in training, but when computing evaluation accuracy, we use only the models predicted overall preference. For more detail, please refer to our code at https://github.com/qunzhongwang/vr-thinker."
        },
        {
            "title": "C Further Experimental Results",
            "content": "In this section, we present more detailed experiments, including comparisons of hyperparameter choices, the impact of varying reject fine-tuning data volumes on the GRPO stage, benchmarking after excluding the hard subsets from the evaluation set, and performance after increasing the number of frames per video. Comparison of different hyperparameter choices To identify the optimal hyperparameters in Appendix B.1, we conducted parameter search. Specifically, we tuned α, which balances the weights of overall accuracy versus per-dimension accuracy, and k, which controls the strength of the Chain-of-Thought (CoT) gain reward. The final evaluations are reported in Figure 5a and 5b. We observe that α has 2https://huggingface.co/datasets/Rapidata 3as per Rapidata, this reflects visual appeal rather than overall preference 16 (a) Parameter search of α (b) Parameter search of (c) Comparison on RFT data volume Figure 5: The results of the hyperparameter search and the reject fine-tuning data volume comparison are summarized in this figure: (a) shows parameter search for α; (b) shows parameter search for k; (c) shows comparison across rejection sampling fine-tuning data volumes. Table 3: Preference accuracy on Residual subset and Redundant version dataset. tau: accuracy is calculated with ties included; diff excludes tied pairs when calculating accuracy. Best performance in Bold Model Protocol LiFT UnifiedReward UnifiedReward-Think VR-THINKER Model Protocol LiFT UnifiedReward UnifiedReward-Think VR-THINKER Size 13B 7B 7B 7B Size 13B 7B 7B 7B Residual Dataset GenAI-Bench (residual) diff (%) tau (%) VideoGen-Reward (residual) tau (%) diff (%) MJBench-Video (residual) diff (%) tau (%) 38.3 61.5 65.0 68.9 59.6 77.2 80.7 82. 40.4 67.5 70.0 71.9 Redundant Dataset 58.2 79.0 79.3 80.6 42.8 63.6 63. 67.4 51.5 69.7 72.1 75.7 GenAI-Bench (redundant) tau (%) diff (%) VideoGen-Reward (redundant) MJBench-Video (redundant) tau (%) diff (%) diff (%) tau (%) 36.9 58.9 63.4 67.2 57.9 74.7 77. 81.9 38.2 65.2 66.8 71.5 55.8 74.2 77.3 79.8 40.1 62.1 61. 66.3 50.8 68.7 70.8 75.2 pronounced effect on performance: α = 1 reduces to training without the per-dimension accuracy reward, whereas α = 0 removes the overall accuracy reward. Our chosen setting, α = 0.5, yields the best results. The choice of also matters, with = 0.2 performing best, indicating that sufficiently strong CoT gain reward is important. However, larger values do not further improve performance, likely because the model can game the signal by remaining deliberately neutral in early reasoning steps to secure larger subsequent gains (i.e., reward hacking). Comparison of reject fine-tuning data volume As shown in Section 3.2, the rejection sampling finetuning stage is crucial for consolidating the models reasoning ability, thereby paving the way for improved GRPO. We further investigate the effect of data volume during the rejection sampling finetuning stage for post-GRPO performance; results are presented in Figure 5c. We observe clear positive correlation of post-GRPO performance and rejection sampling fine-tuning data volume at smaller scales, which is expected: more sampled reasoning patterns that are filtered for quality and correctness lead to better capabilities. However, using even more data (40k in our setting) degrades performance, potentially because extensive supervised fine-tuning reduces output entropy, making subsequent GRPO optimization more difficult. Evaluation on the remaining eval set To better compare improvements across different components of the evaluation set (grouped by prompt complexity and frame count) and assess whether gains are larger on complex scenarios and longer videos, in addition to the results on the Longer video and Complex prompt subsets reported in Table 2, we also report results on the rest of the dataset for comparison. As shown in Table 3, relative to Table 2, the improvements on the Residual subset are less pronounced than on the Longer video and Complex prompt subsets, which validates our analysis. Evaluation on the eval set with increased-frame processing Beyond direct evaluation on our Video Preference Dataset, we further probe the models ability to mine and analyze information from long videos by artificially increasing data size. Concretely, we inject redundant visual information by duplicating frames: frames at random positions are duplicated number of times equal to the original video length, doubling the total frame count. On this redundancy-augmented dataset, results in Table 3 show that our model experiences smaller performance drop compared with other models."
        },
        {
            "title": "D Prompts templates",
            "content": "In this section, we provide detailed prompt templates used across the workflow, including system prompts, input-pair construction templates, and templates or auxiliary prompts employed during synthetic data generation. System prompt For our model, due to the presence of tool invocation, the following system prompt is used: \"type\": \"function\", \"function\": { 1 You are helpful assistant. 2 Tools: You may call one or more functions to assist with the user query. 3 You are provided with function signatures within <tools></tools> XML tags: 4 <tools>:{ 5 6 7 8 9 10 11 12 13 \"type\": \"object\", \"properties\": {\"target_frames\": { \"type\": \"array\", \"description\": \"List of frame indices to select from the video.\", \"items\": {\"type\": \"integer\", \"description\": \"Frame index from 1 to N. will be \"name\": \"select_frames\", \"description\": \"Select frames from video.\", \"parameters\": { specified in the following\"}}}, \"required\": [\"target_frames\"]} 14 } 15 16 }</tools> 17 For each function call, return json object with function name and arguments within <tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json-object>} </tool_call>\", 18 19 Input data construction template Each input consists of pair: video preference datum and query. The query is constructed following the prompt below. Notably, as discussed above, since the perdimension annotations differ slightly across datasets, dataset-specific explanations are injected depending on the source of the video preference data. 1 Task Description: 2 Your task is to compare two videos generated based on the same prompt by analyzing their frames in detail and provide an overall judgment along with judgment for each dimension. This involves: {dim_explain_1} 3 - Iterative reasoning, 4 - Zooming in on details, 5 - Dynamically selecting frames for further analysis. 6 7 The provided frames are downsampled from these videos: 8 - Video 1: First four input frames. 9 - Video 2: Next four input frames. 10 11 The prompt is: {prompt} 12 13 Evaluation Dimensions: 14 1. {dim_name_1}(TA): 15 16 2. {dim_name_2}(VQ): 17 18 3. {dim_name_3}(MQ): 19 20 21 Frames and Analysis Rules 22 - 8 sampled frames are provided, evenly downsampled from {N} frames 23 - Insufficient frames? Request more: 24 25 26 Format Requirement: 27 28 1. Snapshot: 29 Every time you receive new visual information, summarize any information that might be useful for <tool_call>{\"target_frames\": []}</tool_call> {dim_explain_3} {dim_explain_2} your final judgment within <Snapshot></Snapshot> tags. 18 1 2. Think: 2 Place all reasoning content within <Think></Think> tags. 3 4 3. Answer: 5 If the final answer can be determined, output the answer within <Answer></Answer> tags. If the answer is still uncertain, output the recommended answer and confidence level within <Recommend Answer></Recommend Answer> tags. 6 Here, 1 represents Video 1, 2 represents Video 2, and 0 represents Tie. The confidence levels range from high to low as 1, 2, and 3. 7 8 Examples: 9 <Answer>TA=1, VQ=1, MQ=0, OA=1</Answer>, or 10 <Recommend Answer>TA=0, VQ=1, MQ=0, OA=1, CF=2</Recommend Answer> The per-dimension explanations for each dataset are as follows: VideoGen-Reward 1 Text Alignment: Alignment between video content and prompt 2 Visual Quality: The visual aesthetics of the video 3 Motion Quality: Level of motion coherence Text2Video-Human 1 Alignment: Alignment between video content and prompt 2 Preference: The intrinsic aesthetics of the video 3 Coherence: Level of temporal coherence MJ-Bench-Video 1 Alignment: Alignment between video content and prompt 2 Fineness: The level of fineness in visual content 3 Coherence & Consistency: Level of temporal coherence and Consistency Cold Start Data Construction template To improve adherence to the response format and increase efficiency during the Cold Start data construction phase, we provide an additional example for Cold Start data generation (which is removed during training), as shown below: 1 Reason Segment 1: 2 3 <Snapshot> 4 The first four frames from Video 1 and Video 2 show close-up of mother orangutan holding her baby in the rainforest. The mother's expression is gentle, and the baby's tiny fingers are gripping her fur. 5 6 The background is lush with green foliage, and the lighting suggests it is raining. The frames capture the tender moment between the mother and her baby, aligning well with the caption. 7 </Snapshot> 8 9 <think> 10 The frames are clear and detailed, showing the intimate moment between the mother and her baby. The motion appears natural, and the visual quality is high, with vibrant colors and sharp details. The rain adds serene and realistic touch to the scene. The frames are evenly sampled, ensuring representative view of the video content. 11 12 The next four frames will help in evaluating the motion quality and visual quality more comprehensively. will select frames 12, 24, 36, 48, 60, 72, 84, and 96 to analyze further. 13 </think> 14 15 <recommend answer> 16 TA = 1, MQ = 0, VQ = 0, OA = 1, CF = 2 17 </recommend answer> 18 19 <tool_call> 20 {\"name\": \"select_frames\", \"arguments\": {\"target_frames\": [12, 24, 36, 48, 60, 72, 84, 96]}} 21 </tool_call> 19 1 Reason Segment 2: 2 3 <Snapshot> 4 The first four frames from Video 1 and Video 2 show close-up of mother orangutan holding her baby in the rainforest. The mother's expression is gentle, and the baby's tiny fingers are gripping her fur. 5 6 The background is lush with green foliage, and the lighting suggests it is raining. The frames capture the tender moment between the mother and her baby, aligning well with the caption. 7 8 </Snapshot> 9 10 <think> 11 The selected frames provide clear view of the motion quality and visual quality. The motion is smooth, with natural transitions and no abrupt changes. 12 13 The final frames confirm that Video 1 is superior in terms of motion quality, visual quality, and overall alignment with the caption. The confidence in this conclusion is high, as the selected frames provide clear evidence of the video's quality. 14 </think> 15 16 <final answer> 17 TA = 1, MQ = 1, VQ = 1, OA = 1 18 </final answer>"
        },
        {
            "title": "E Limitations",
            "content": "Our approach enhances the reward model through multimodal reasoning; however, this unavoidably introduces longer inference chains, leading to higher latency and computational cost. In future work, we will aim to reduce inference overhead and shorten Chain-of-Thought (CoT) length for straightforward video cases without compromising quality, by further improving the models reasoning efficiency. Our current training pipeline primarily relies on Reject Fine-Tuning and GRPO, which tend to amplify capabilities the model has already learned (Yue et al., 2025). To achieve more substantial gains, constructing higher-quality supervised fine-tuning dataset with carefully curated CoT rationales is essential. Building such datasets is an important direction for future research."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "Kuaishou Technology",
        "Nanjing University"
    ]
}