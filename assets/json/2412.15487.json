{
    "paper_title": "Multi-LLM Text Summarization",
    "authors": [
        "Jiangnan Fang",
        "Cheng-Tse Liu",
        "Jieun Kim",
        "Yash Bhedaru",
        "Ethan Liu",
        "Nikhil Singh",
        "Nedim Lipka",
        "Puneet Mathur",
        "Nesreen K. Ahmed",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Hanieh Deilamsalehy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we propose a Multi-LLM summarization framework, and investigate two different multi-LLM strategies including centralized and decentralized. Our multi-LLM summarization framework has two fundamentally important steps at each round of conversation: generation and evaluation. These steps are different depending on whether our multi-LLM decentralized summarization is used or centralized. In both our multi-LLM decentralized and centralized strategies, we have k different LLMs that generate diverse summaries of the text. However, during evaluation, our multi-LLM centralized summarization approach leverages a single LLM to evaluate the summaries and select the best one whereas k LLMs are used for decentralized multi-LLM summarization. Overall, we find that our multi-LLM summarization approaches significantly outperform the baselines that leverage only a single LLM by up to 3x. These results indicate the effectiveness of multi-LLM approaches for summarization."
        },
        {
            "title": "Start",
            "content": "Multi-LLM Text Summarization Jiangnan Fang1, Cheng-Tse Liu1, Jieun Kim1, Yash Bhedaru1, Ethan Liu1, Nikhil Singh1, Nedim Lipka2, Puneet Mathur2, Nesreen K. Ahmed2, Franck Dernoncourt2, Ryan A. Rossi2, and Hanieh Deilamsalehy2 1University of California, Santa Cruz 2Adobe Research"
        },
        {
            "title": "Abstract",
            "content": "In this work, we propose Multi-LLM summarization framework, and investigate two different multi-LLM strategies including centralized and decentralized. Our multi-LLM summarization framework has two fundamentally important steps at each round of conversation: generation and evaluation. These steps are different depending on whether our multi-LLM decentralized summarization is used or centralized. In both our multi-LLM decentralized and centralized strategies, we have different LLMs that generate diverse summaries of the text. However, during evaluation, our multi-LLM centralized summarization approach leverages single LLM to evaluate the summaries and select the best one whereas LLMs are used for decentralized multi-LLM summarization. Overall, we find that our multi-LLM summarization approaches significantly outperform the baselines that leverage only single LLM by up to 3x. These results indicate the effectiveness of multi-LLM approaches for summarization."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have been shown to have the potential to produce high-quality summaries (Chowdhery et al., 2022; Zhang et al., 2023; Goyal et al., 2023; Pu et al., 2023b). However, despite the remarkable progress in LLM-based summarization, limitations still exist for long documents where useful information is abundant but sparsely distributed throughout the text. Research by (Liu et al., 2023) highlights that naive application of LLMs may overlook critical details or fail to grasp the holistic meaning of document, indicating the need for more refined methods. To address this, recent efforts have explored prompt-engineering techniques to guide LLMs towards producing better summaries (Adams et al., 2023). These techniques, while promising, still face limitations in consistently delivering highquality summaries across different document types and structures. Instead of relying solely on single model or simple prompt-engineering methods, we propose an approach novel to the summarization domain that focuses on aggregating the collective strengths of multiple LLMs. By combining the capabilities of multiple models with diverse set of knowledge bases, we show its possible to achieve more robust summaries across domains. Summary of Main Contributions. The main contributions of this work are as follows: We propose the first framework for multiLLM text summarization and investigate two topologies: centralized and decentralized. We find that multi-LLM text summarization often performs better than using single LLM for summarization. We conduct experiments on how prompting, number of LLMs, and various combinations of generating and evaluating LLMs can affect quality of summaries in the multi-LLM setup."
        },
        {
            "title": "2.1 Summarization",
            "content": "Recent advancements in summarization have increasingly leveraged large language models (LLMs), moving beyond fine-tuned transformer models like Pegasus, BART, and T5. Studies consistently show that LLMs can generate summaries with higher coherence, relevance, and factual accuracy, often rivaling or surpassing human-written summaries (Goyal et al., 2023; Zhang et al., 2023; Pu et al., 2023b). For example, Goyal et al. (2023) demonstrated that GPT-3 (text-davinci-002) produced summaries preferred by human evaluators over fine-tuned models like Pegasus and BRIO on structured datasets such as CNN/DM (Nallapati et al., 2016) and XSUM (Narayan et al., 2018). Similarly, Zhang et al. (2023) emphasized the importance of instruc4 2 0 2 0 2 ] . [ 1 7 8 4 5 1 . 2 1 4 2 : r tion tuning in achieving superior zero-shot performance for summarization tasks. Pu et al. (2023b) further highlighted improved factual consistency and reduced hallucinations when using LLMs. texts, they fail While these studies validate the potential of LLMs in summarizing well-structured and relatively short to address the unique challenges of long-document summarization, where inputs lack clear structural cues and exhibit greater complexity. Research focusing on long-text summarization, such as Keswani et al. (2024), employed semantic clustering and multistage summarization with LLaMA2 to manage lengthy inputs. However, such approaches often rely on predefined hierarchical processing strategies that may oversimplify the nuanced relationships within the text. Moreover, as Liu et al. (2023) noted, LLMs tend to neglect content from the middle sections of long documents, resulting in incomplete or unbalanced summaries. These limitations point to the need for novel approach capable of addressing the inherent challenges of long-document summarization. Our work builds upon this foundation by proposing multiLLM framework designed to overcome these shortcomings through information exchange and collaborative synthesis, which can better capture the diversity and complexity of long texts."
        },
        {
            "title": "2.2 Multi-LLM",
            "content": "The concept of leveraging multiple LLMs collaboratively has gained traction in recent research, particularly for tasks requiring complex reasoning and factual accuracy. For instance, Liang et al. (2024) introduced the Multi-Agent-Debate (MAD) framework, where LLMs engage in iterative debates to refine their reasoning. This framework demonstrated that multi-agent GPT-3.5-Turbo setup outperformed GPT-4 on reasoning datasets. Similarly, Chen et al. (2024) proposed RECONCILE, framework where LLMs collaboratively refine answers and explanations, achieving significant improvements over single-agent systems. Li et al. (2024) extended this line of research by optimizing agent connections, showing that sparse networks can maintain performance while reducing computational overhead. Although these studies reveal the potential of multi-LLM approaches, their focus remains on structured reasoning tasks, such as question answering and fact-checking. They have not been adequately explored in the context of long-text summarization, where the challenges include synthesizing distributed information, addressing content imbalances, and preserving the coherence of summaries across extended texts. We hope to bridge this gap by adapting multiLLM frameworks to the domain of long-document summarization, addressing limitations of both single LLM and traditional hierarchical techniques, and positioning multi-LLM summarization as promising solution for long-form content."
        },
        {
            "title": "Framework",
            "content": "In this work, we propose novel multi-LLM summarization framework that leverages multiple large language models to enhance summarization quality of long document input. Through the distribution of generation and evaluation of candidate summaries across multiple models, our framework aims to provide better summaries than single LLM methods, leveraging expertise from different models. We present two interaction topologies, centralized and decentralized, to guide the collaboration, evaluation, and refinement of summaries between LLMs. Visually these two methods can be represented at high level in Figure 1. Our approach tackles long text document input, which can span to tens of thousands of words and as such usually exceeds the context window of most standard LLMs. To handle this, we establish two stage process that involves chunking the source document, independently summarizing each chunk of the source document, and then applying second round of chunking and summarization on the concatenated intermediate results. Throughout both these stages, both frameworks allow multiple LLMs to collaborate and converge on single final high quality summary of the entire original reference document. Table 1 provides an overview of our frameworks four main variations."
        },
        {
            "title": "4 Centralized Multi-LLM Summarization",
            "content": "The steps for centralized summarization can be found in Algorithm 1. This method leverages multiple LLMs to generate candidate summaries and uses central LLM to evaluate their quality and guide iterative refinements."
        },
        {
            "title": "4.1 Single Round",
            "content": "In the simplest case, we prompt each LLM once, gather their summaries, and then perform single Multi-LLM Summarization Framework"
        },
        {
            "title": "Stage",
            "content": "CENTRALIZED (Sec. 4) DECENTRALIZED (Sec. 5) Single-Round (Sec. 4.1) Conversational (Sec. 4.2) Single-Round (Sec. 5.1) Conversational (Sec. 5.2) Generation ( 4.1.1) Evaluation ( 4.1.2) Generation ( 4.2.1) Evaluation ( 4.2.2) Generation ( 5.1.1) Evaluation ( 5.1.2) Generation ( 5.2.1) Evaluation ( 5.2.2) Table 1: Overview of Multi-LLM Summarization Framework (Sections 4-5)."
        },
        {
            "title": "4.1.2 Evaluation Phase",
            "content": "After collecting the set of candidate summaries S, we select central agent to evaluate these summaries. The central LLM uses an evaluation prompt Pec, as shown in Figure 5, to assess the quality of each summary. To reduce potential bias arising from authorship attribution, we use anonymized identifiers for summaries like agent_1, agent_2, etc. during evaluation. Formally, we obtain = C(Pec, S), where is the central LLMs evaluation of all candidate summaries. This includes the choice for the best summary (expressed as its anonymized identifier) and confidence score for that evaluation (expressed as an integer from 0 to 10), denoted together as = AGGRRESULTS(E) in Algorithm 1. We deanonymize the identifier to recover the text of the selected summary Sj and set this as our final output S. In the single-round regime, this terminates the process as no further iterations are performed. In the evaluation prompt, we include the prompt to output confidence score so there is variable on which to impose stopping condition. This allows us to extend the centralized process to multiple rounds of generation and evaluation using that condition. This process is explained in subsequent sections. (a) Centralized (b) Decentralized Figure 1: Centralized and Decentralized approaches using 5-LLM example. Similar topologies can be applied to any (\"k\") number of LLMs. In centralized interactions, all models communicate with central model; in decentralized interactions, each model communicate with every other model and also itself. evaluation step to select the best final summary. This is the initial process before we extend it to multiple rounds."
        },
        {
            "title": "4.1.1 Generation Phase",
            "content": "In the single-round setting, each LLM from the list of participating models = {M1, . . . , Mk} independently generates summary of the same input text using common prompt . The prompt is illustrated in Figure 2. Formally, for each LLM Mj M, the output is Sj = Mj(P, S)"
        },
        {
            "title": "4.2 Conversational",
            "content": "where represents the input text. Running this step for all Mj yields set of summaries = {S1, . . . , Sk}. This initial generation stage corresponds to lines 35 of Algorithm 1. Conceptually, each model contributes its unique perspective, leading to diverse pool of candidate summaries, which is important for robust summary selection in the following evaluation phase. In the conversational approach, we repeat the generation and evaluation phases multiple times. We define each generation-evaluation process as one round and define conditions under which the process ends or new round should begin, up to maximum number of rounds."
        },
        {
            "title": "4.2.1 Generation Phase",
            "content": "The first round of the conversational approach mirrors the single-round procedure (Section 4.1.1). Each LLM Mj generates an initial summary S(1) from the original input text using the prompt : S(1) = Mj(P, S). If the evaluation result from the previous round has confidence score less than the threshold or, if the LLM fails to output readable confidence score, the pipeline proceeds to the next round. For the second and subsequent rounds, we use the prompt (i), shown in Figure 3. LLMs in the second and subsequent rounds have access to both the text to be summarized and summaries from the previous round. Concretely, in round > 1: S(i) = Mj(P (i), S). The hope is that LLM is able to iteratively improve summarization based upon previous outputs from itself and other models."
        },
        {
            "title": "4.2.2 Evaluation Phase",
            "content": "The evaluation phase in round > 1 is conceptually similar to the single-round setting (Section 4.1.2), but now operates on candidate summaries generated immediately before in the generation phase 1 , . . . , S(i) Si = {S(i) }. The central LLM evaluates these candidates using Pec: E(i) = C(Pec, Si), If the confidence level meets the threshold, the process terminates, and the summary chosen by the central LLM is accepted as S. Otherwise, we proceed to the next round of summary generation and evaluation. For the confidence scores we have chosen the range 0-10 as it is fine-grained but also is one of the most common rating scales."
        },
        {
            "title": "4.3 Analysis of Complexity",
            "content": "The centralized approach uses models for generation and 1 central model for evaluation; other than text length, the number of input tokens scale linearly with the number of models and with the number of rounds. Output tokens also scale linearly with number of models and number of rounds, but since we instruct the model to output fixed number of words for summary (and in our experiments the models are largely compliant), and output only the anonymous identifier for chosen summary, we ensure bounded runtime and cost. Further analysis can be found at Appendix B.1. Algorithm 1 Centralized Multi-LLM Summary Require: ordered set = {S1, . . . , Sm} of summaries, set = {M1, . . . , Mk} of LLMs, central agent M, max number of conversational rounds tmax, initial summarization prompt (e.g., Figure 2), evaluation prompt Pec (e.g., Figure 5) for centralized version conversation rounds Ensure: summary of the text 1: = CREATESUMMARY(S) 2: for = 1 to tmax do 3: 4: 2 , . . . , S(i) } for each model Mj do S(i) = Mj(P, S) Send S(i) to agent Let Si = {S(i) 1 , S(i) E(i) = C(Pec, Si) = AGGRRESULTS(E(i)) argmaxMj rj Set S(i) if CONVERGED(r) then return Set to prompt in Figure 3. 5: 6: 7: 8: 9: 10: 11: 12: Provide concise summary of the text in around 160 words. Output the summary text only and nothing else. [text] Figure 2: Prompt for generating the initial summary in the first round. Given the original text below, along with the summaries of that text by [k] LLMs, please generate better summary of the original text in about 160 words. ORIGINAL: Summary by M1: [text] [LLM 1s summary] ... Summary by Mk: [LLM ks summary] Figure 3: Generation prompt that is used after the initial round of conversation among the multiple LLMs. Note that the above prompt is for generating the final summary, however, for the chunk-level generation, it would just be the actual chunk."
        },
        {
            "title": "Summarization",
            "content": "Previously we introduced the summarization procedure for centralized approach (Section 4), which diversifies the knowledge base for summarization. We extend the paradigm for the evaluator as well. In the decentralized approach, multiple LLMs also participate in the evaluation process with the hope that best summary decided on consensus is more robust compared to single models decision. Given the original text below, along with the summaries of that text by [k] agents, please evaluate the summaries and output the name of the agent that has the best summary. Output the exact name only and nothing else. ORIGINAL: [chunk or concatenated chunk summaries S] Summary by agent_1: [LLM 1s summary] ... Summary by agent_k: [LLM ks summary] Figure 4: Evaluation prompt for evaluating the summaries generated by different LLMs using our conversational (decentralized) multi-LLM framework. \"k\" is parameter reflecting the number of LLMs that generate summaries. Given the initial text below, along with the summaries of that text by [k] LLMs, please evaluate the generated summaries and output the name of the LLM has the best summary. On separate line indicate confidence level between 0 and 10. ORIGINAL: Summary by M1: [text] [LLM 1s summary] ... Summary by Mk: [LLM ks summary] Remember, on separate line indicate confidence level between 0 and 10 Provide concise summary of the text in around 160 words. Output the summary text only and nothing else. [concatenated chunk summaries S] Figure 6: Generation prompt for generating the final summary from the summarized chunks using our conversational (decentralized) multi-LLM framework. This prompt is the same as the one for the initial summary. 1 , . . . , S(i) are collected, where each E(i) level and receives the text to be summarized along with summaries authored by all agents including itself. More formally, model preferences E(i) 1 , . . . , E(i) represents model Mjs choice of the best summary among S(i) . These preferences are aggregated into result vector 1, . . . , kk, where each element rj indicates which models summary was chosen by model Mj. Convergence is achieved when majority of models select the same summary, formally expressed as 1, . . . , : : rj = > 2 . 1 When no majority choice emerges, the single-round approach (tmax = 1) the algorithm selects the summary from designated tie-breaker model Mt, where 1, . . . , k. Since the tie-breaker model can be any model in the multi-LLM setup, we run experiments with different choices of evaluator and tie-breaking models. Formally, the final summary is determined as: Figure 5: Evaluation prompt for evaluating the summaries generated using our conversational (centralized) multi-LLM framework. More specifically, we have added an instruction for centralized multi-LLM summarization approach that in addition to providing the best summary, it also outputs the confidence level between 0 and 10. \"k\" is parameter reflecting the number of summary-generating LLMs."
        },
        {
            "title": "5.1.1 Generation Phase",
            "content": "Generation procedure is the same as that in the centralized approach described in Section 4.1.1. As before, multiple LLMs independently generate summaries for the input text, obtaining the list of summaries = {S1, . . . , Sk}."
        },
        {
            "title": "5.1.2 Evaluation Phase",
            "content": "S = (cid:40) S(1) S(1) if : {j : E(1) if maxl {j : E(1) = m} > 2 = l} 2 where 1, . . . , : : rj = > 2 . We test the different choices of tie-breaker model in the experiment Appendix C.1."
        },
        {
            "title": "5.2 Conversational",
            "content": "The conversational approach extends the decentralized framework by introducing multiple rounds of generation and evaluation phases. Each generationevaluation cycle constitutes round, with iterations continuing until either consensus is achieved or maximum number of rounds (tmax) is reached."
        },
        {
            "title": "5.2.1 Generation Phase\nGeneration follows the methodology in Sec-\ntion 4.1.1, producing the set of summaries S =",
            "content": "For evaluation, each model that authored summary is prompted with new evaluation prompt (Figure 4) which does not include confidence 1Here our implementation requires votes exceeding absolute majority for summary to be immediately selected. In the case of 2 LLMs, this is equivalent to unanimous decision because one vote does not satisfy absolute majority. S1, . . . , Sk. key distinction from the singleround approach lies in the conditional regeneration mechanism: when consensus fails in the first round, subsequent rounds use new prompt (Figure 3) which includes generated summaries from previous evaluations. = m} k"
        },
        {
            "title": "5.3 Analysis of Complexity",
            "content": "The decentralized approach uses models for both generation and evaluation. For this reason the input and output tokens scale quadratically with number of models. As before, we instruct the model to output fixed number of words for summary and an identifier only for evaluation and so ensure bounded runtime and cost. Further analysis can be found at Appendix B.2."
        },
        {
            "title": "6 Experiments",
            "content": "To investigate the proposed multi-LLM summarization framework, we conduct extensive experiments to evaluate its effectiveness."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "We use ArXiv (Cohan et al., 2018) and GovReport (Huang et al., 2021) to evaluate our summarization methods. We assess the quality of LLM-generated summaries using ROUGE-1, ROUGE-L, BLEU-1, and BLEU-4 metrics. For comparison with our multi-LLM approach, unless otherwise mentioned, we leverage GPT-3.5, GPT-4o, GPT-4o mini, and LLaMA3-8B as baselines. For these models, we perform the same chunking across all models, and the summarization prompt is identical to that in the first round of the multi-LLM process (Figure 6). Unless otherwise mentioned, all models use 4Kchar chunk-size, and the final summary represents Algorithm 2 Decentralized Multi-LLM Summary Require: ordered set = {S1, . . . , Sm} of summaries, set = {M1, . . . , Mk} of LLMs, max number of conversational rounds tmax, initial summarization prompt (e.g., Figure 2), evaluation prompt Pe (e.g., Figure 4) conversation rounds Ensure: summary of the text 1: = CREATESUMMARY(S) 2: for = 1 to tmax do 3: 4: for each model Mj do S(i) = Mj(P, S) Send S(i) to models Let Si = {S(i) 2 , . . . , S(i) 1 , S(i) } for each model Mj do = Mj(Pe, S(i) 1 , . . . , S(i) ) to other models 2 , . . . , E(i) 1 , E(i) } 1 , . . . , E(i) ) E(i) Send E(i) Set Ei = {E(i) = AGGRRESULTS(E(i) argmaxMj rj Set S(i) if CONVERGED(r) then return Set to prompt in Figure 3. 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: concatenation of the generated summaries. Finally, unless otherwise mentioned, we set = 160 for all the models."
        },
        {
            "title": "6.2 Main Results",
            "content": "Our multi-LLM framework outperforms singleLLM baselines by up to 3, as seen in Table 2. The fact that both precisionand recall-focused metrics improved means the multi-LLM approach is robust. On average the centralized method improves the scores by 73%, and the decentralized method outperforms baselines by 70%. In our theoretical cost analysis (Section B.1 and B.2) we show that the input cost (in number of tokens) for the decentralized multiplies by the the number of agents participating in the evaluation, and with the more cost-effective centralized method our system is able to perform better than the single-LLM setup. This demonstrates the effectiveness of our proposed method under decentralized and decentralized frameworks. We see that additional LLMs do not improve upon the 2-LLM setup (see Appendix C.3), and additional rounds of generation and evaluation do not further improve scores. This shows that even with just 2 LLMs and single round of generation and evaluation we observe performance gains, meaning that the least costly version of the multi-LLM system is still able to deliver better summaries compared to single-LLM approaches. In Table 2 we use GPT-3.5 as the evaluator and tie-breaking choice in our multi-LLM. We also run the multi-LLM system with GPT-4o mini as the LLaMA3-8B GPT-3.5 GPT-4o mini GPT-4o"
        },
        {
            "title": "Centralized",
            "content": "Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max"
        },
        {
            "title": "GovReport",
            "content": "ROUGE-1 ROUGE-L BLEU-1 BLEU-4 ROUGE-1 ROUGE-L BLEU-1 BLEU-4 0.180 0.193 0.217 0.165 0.313 0.339 0.329 0.333 0.106 0.114 0.118 0.095 0.163 0. 0.168 0.173 0.084 0.093 0.108 0.073 0.200 0.224 0.217 0.219 0.021 0.026 0.020 0.015 0.029 0. 0.031 0.036 0.403 0.390 0.384 0.372 0.447 0.468 0.468 0.479 0.177 0.178 0.156 0.155 0.180 0. 0.189 0.197 0.242 0.226 0.224 0.211 0.458 0.477 0.470 0.485 0.079 0.084 0.058 0.059 0.098 0. 0.109 0.121 Table 2: Results for the decentralized and centralized Multi-LLM approaches. For the multi-LLM pipelines participating models are GPT-3.5 and GPT-4o mini. The results use GPT-3.5 for the evaluator in the centralized approach, and summaries from GPT-3.5 are chosen in tie-breaking for both centralized and de-centralized approaches. evaluator and the tie-breaker, and the results are shown in Table 5. Again, the multi-LLM framework outperformed single-LLM baselines, averaging 64% improvement for the decentralized variant and 63% for the centralized variant. In some individual scores, our framework improves upon single-LLM setups by up to 3. These improvements are competitive to those we obtain from the multi-LLM setup in Table 2, which means our proposed framework works well for different central models and different tie-breaking models. We also perform additional experiments with other variables. More specifically, we assess the performance of the multi-LLM framework with alternative combinations of models, with three models contributing to the summarization and evaluation, and with models receiving fine-grained In all of prompts instead of the same prompt. these experiments, we obtain competitive results compared to the first decentralized and centralized setup, and the scores are higher than single-LLM baselines, showing that our proposed framework performs consistently under different setups."
        },
        {
            "title": "6.3 Ablation Studies",
            "content": "Varying Model Combinations In Table 2 we use GPT-3.5 and GPT-4o mini as the participating models in the multi-LLM framework. We further experiment with alternative combinations of models in the framework. As shown in Table 3 we again observe improvements across the board compared to the single-LLM baselines in Table 2, regardless of default model and number of rounds and type of interaction (decentralized vs. centralized). The improvements are competitive with those seen in the GPT-3.5 and GPT-4o mini combination. Further results are provided in Appendix C.2. Varying the Number of LLMs In this experiment we use 3 LLMs in the setup instead of 2. We observe 54% improvement for the decentralized method and 59% for the centralized method on average over single-LLM summaries, and for individual scores we see improvements of up to 2.9. More detailed results are presented in Table 6 and in Appendix C.3. Specialized Prompting In all previous experiments we have kept the generation prompt identical for all LLMs. With multi-LLM approaches, this need not be the case. In this experiment we choose different prompts for different models when generating summaries, aiming to have unique knowledge bases of different models complement each other. As seen in Table 7, the centralized method results in 66% performance increase over single-LLM baselines in Table 2, and the decentralized method has 58% increase over the single-LLM baselines. For experimental details and further analysis see Section C.4 in the Appendix Short vs. Long-text Multi-LLM Summarization In this experiment, we use only the introduction section as the basis for summarization in the ArXiv dataset. Since the introduction typically shorter than the context window of LLMs, we refer to these as \"short-text\" summarization, in contrast to the \"long-text\" summarization we explore previously. The results in Table 8 shows that the centralized approach provides the most performance gains over single-LLM baselines up to 2.4 on average, and the decentralized method sees 2.3 increase. Further details can be found in Appendix C.5."
        },
        {
            "title": "6.4 Cost Analysis",
            "content": "Table 4 presents the cost analysis for both decentralized and centralized methods based on the results in Table 2, highlighting key trends in input and output tokens across various stages of the summarization process. We observe that for evaluation stages the input and output token counts for the decentralized Max Rounds Multi-LLM Model Combination ROUGE-1 ROUGE-L BLEU-1 BLEU-"
        },
        {
            "title": "1 Rounds",
            "content": "GPT-3.5 & GPT-4o mini GPT-4o & GPT-3.5 GPT-4o & GPT-4o mini GPT-3.5 & GPT-4o mini GPT-4o & GPT-3.5 GPT-4o & GPT-4o mini GPT-3.5 & GPT-4o mini GPT-4o & GPT-3.5 GPT-4o & GPT-4o mini GPT-3.5 & GPT-4o mini GPT-4o & GPT-3.5 GPT-4o & GPT-4o mini 0.328 0.313 0.302 0.333 0.328 0. 0.312 0.325 0.304 0.338 0.339 0.306 0.167 0.159 0.152 0.173 0.170 0.153 0.163 0.166 0.153 0.180 0.177 0. 0.217 0.197 0.185 0.218 0.212 0.189 0.199 0.214 0.188 0.224 0.228 0.190 0.030 0.025 0.022 0.036 0.033 0. 0.029 0.029 0.022 0.042 0.039 0.022 Table 3: Varying the combination of models in our Multi-LLM approaches. Note rounds is the max number of rounds allowed and all results are for ArXiv. Bolded numbers are best scores for each round-model combination. Underlined numbers are overall best scores for each metric in this table. Furthermore, the central LLM is highlighted in blue and for the decentralized multi-LLM approaches, we highlight the LLM used for tie-breaking in green."
        },
        {
            "title": "Centralized",
            "content": "Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max 383.73M 129.36M 216.65M 77.69M 25.63M 11.89M 19.55M 6.77M 14.62M 11.77M 14.76M 10.56M 409.37M 141.25M 236.2M 84.46M Table 4: Cost Analysis of our Multi-LLM Decentralized and Centralized Summarization Methods. Note = millions of tokens. method are twice those for the centralized method, which reflect the number of LLMs in the setup."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper presented multi-LLM framework for text summarization, and proposed two strategies, decentralized and centralized multi-LLM summarization. We demonstrated that the proposed multiLLM summarization techniques lead to better generated summaries. Our results indicate that multiLLM approaches are useful for improving text summarization. Future work should continue to investigate multi-LLM approaches for summarization."
        },
        {
            "title": "8 Limitations",
            "content": "This work demonstrated the effectiveness of both our centralized and decentralized multi-LLM summarization approaches. Future work should further investigate various aspects, including more diverse LLMs, and explore other topologies beyond the two extremes we proposed. Furthermore, while we investigated variety of datasets, future work can explore other domains. We believe there are many approaches that lie between the two extreme multiLLM strategies we investigated empirically in this work. Finally, we did not optimize the prompts, as such we believe there is huge opportunity to achieve significantly better results by engineering better prompts to consider other important aspects of summarization. We leave these and other important directions for future work."
        },
        {
            "title": "References",
            "content": "Griffin Adams, Alexander Fabbri, Faisal Ladhak, Eric Lehman, and Noémie Elhadad. 2023. From sparse to dense: Gpt-4 summarization with chain of density prompting. Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. Etc: Encoding long and structured inputs in transformers. Lochan Basyal and Mihir Sanghvi. 2023. Text summarization using large language models: comparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. Booookscore: systematic exploration of book-length summarization in the era of llms. Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2024. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Janara Christensen, Stephen Soderland, Gagan Bansal, and Mausam. 2014. Hierarchical summarization: Scaling up multi-document summarization. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 902912. Association for Computational Linguistics. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. discourse-aware attention model for abstractive summarization of long documents. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29782988, Florence, Italy. Association for Computational Linguistics. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. Alexios Gidiotis and Grigorios Tsoumakas. 2020. divide-and-conquer approach to the summarization of long documents. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:30293040. Yihong Gong and Xin Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1925. ACM. Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023. News summarization and evaluation in the era of gpt-3. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2021. Longt5: Efficient text-to-text transformer for long sequences. Karl Moritz Hermann, Tomáš Koˇciský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14191436, Online. Association for Computational Linguistics. Emma Järvinen. 2024. Long-input summarization using large language models. Gunjan Keswani, Wani Bisen, Hirkani Padwad, Yash Wankhedkar, Sudhanshu Pandey, and Ayushi Soni. 2024. Abstractive long text summarization using large language models. International Journal of Intelligent Systems and Applications in Engineering, 12(12s):160168. Anastassia Kornilova and Vladimir Eidelman. 2019. BillSum: corpus for automatic summarization of US legislation. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 4856, Hong Kong, China. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78717880, Online. Association for Computational Linguistics. Irene Li, Aosong Feng, Dragomir Radev, and Rex Ying. 2023. Hipool: Modeling long documents using graph neural networks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 161171, Toronto, Canada. Association for Computational Linguistics. Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, and Eugene Ie. 2024. Improving multi-agent debate with sparse communication topology. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2024. Encouraging divergent thinking in large language models through multi-agent debate. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. Yang Liu and Mirella Lapata. 2019. Text summarizaIn Proceedings of tion with pretrained encoders. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 37303740. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073 1083. Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. Brio: Bringing order to abstractive summarization. S. Mallick, A. Ghosh, et al. 2019. survey on extractive text summarization. Journal of Artificial Intelligence Research, 65:123143. Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404411. Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-tosequence rnns and beyond. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Bo Pang, Erik Nijkamp, Wojciech Kryscinski, Silvio Savarese, Yingbo Zhou, and Caiming Xiong. 2022. Long document summarization with top-down and bottom-up inference. Dongqi Pu, Yifan Wang, and Vera Demberg. 2023a. Incorporating distributions of discourse structure for long document abstractive summarization. Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023b. Summarization is (almost) dead. Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379389. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. Sam Shleifer. 2020. https://huggingface.co/sshleifer/ distilbart-cnn-12-6. 29. Accessed: Distilbart-cnn-12-6. 2024-05Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331335, Florence, Italy. Association for Computational Linguistics. Wen Xiao and Giuseppe Carenini. 2019. Extractive summarization of long documents by combining In Proceedings of the global and local context. 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 30113021, Hong Kong, China. Association for Computational Linguistics. Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2021. Big bird: Transformers for longer sequences. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2023. Benchmarking large language models for news summarization."
        },
        {
            "title": "A Detailed Experimental Setup",
            "content": "Datasets: We use the test sets of ArXiv (Cohan et al., 2018) (first 20%, or 1,288 documents) and GovReport (Huang et al., 2021) (all, or 973 documents) as document input for our summarization methods. They cover range of genres, providing diverse texts for evaluation. In ArXiv, the main article excluding the abstract is the target for summarization, and the abstract is used as the ground truth reference summary; for GovReport, the text is the main report and the ground truth is the humanwritten summary. ArXiv articles range from 241 to 44,489 space-delimited words long, with an average of 5,950 words; their summaries range from 46 to 290 words, averaging 164 words. GovReport main texts range from 396 to 31,371 words, averaging 7,379 words; their summaries range from 67 to 1,363 words, averaging 571 words. Evaluation Metrics: We assess the quality of LLM-generated summaries using ROUGE1, ROUGE-L, BLEU-1, and BLEU-4 metrics. ROUGE scores emphasizes recall while BLEU scores emphasize precision. Baselines: For comparison with our multi-LLM approach, unless otherwise mentioned, we leverage GPT-3.5, GPT4o, GPT-4o mini, and LLaMA3-8B as baselines. For these models, we perform the same chunking across all models, and the summarization prompt is identical to that in the first round of the multi-LLM summarization process (Figure 2). Unless otherwise mentioned, all models use 4K-char chunk-size, and the final summary for each document is concatenation of the generated summaries for each chunk in that document. Finally, unless otherwise mentioned, we set = 160 for all models. Theoretical Analysis & Discussion B.1 Centralized Apporach Cost and Complexity per Round Let denote the number of input tokens in the original text and Omax represent an upper bound on the output tokens (i.e., maximum summary length). We consider distinct LLMs and maximum of tmax conversational rounds. In each round i, we prompt all LLMs with approximately + δi input tokens, where δi denotes additional tokens introduced in that round (e.g., references to previously generated summaries). Each LLM then produces up to Omax output tokens. Since input and output tokens often incur different costs, we consider them separately. For the generation phase, the input token cost per round is on the order of O(k (I + δi)), and the output token cost is on the order of O(k Omax). For evaluation, the central LLM processes candidate summaries and Iec instructions, resulting in an input token cost of about O(k Omax + Iec). By directing the central LLM to output only an anonymous identifier for the chosen summary, we reduce output token length in evaluation, thereby minimizing the chance of hallucination and enabling more straightforward cost accounting. Multi-Round Overhead Over tmax rounds, the total input token usage for generation is O(tmax (I +Omax)), and the evaluation input token usage is O(tmax(kOmax+Iec)). Although this complexity may appear large, tmax is typically small (e.g., 2 or 3), and Omax is usually constrained (e.g., brief 160-word summary). Moreover, careful prompt engineering can curtail δi growth, ensuring that the number of tokens per round remains bounded. Convergence and Quality Gains The iterative generation-evaluation mechanism aims to converge within small number of rounds. With each iteration, models refine their outputs guided by previous results, potentially improving summary quality. This iterative refinement, while incurring additional steps, offers practical trade-off between computation and quality, as the improved summaries can justify the limited number of extra rounds. B.2 Decentralized Approach Multi-Round Complexity Let denote the number of input tokens in the original text and Omax represent an upper bound on the output tokens (i.e., maximum summary length). We consider distinct LLMs and maximum of tmax conversational rounds. Over tmax rounds, the worst-case token cost from generation is: O(tmax (I + Omax)). The evaluation cost scales to: O(tmax (k Ie + k2 Omax)). Combined, we have total complexity per round of approximately: O(k + Omax + Ie + k2 Omax). Thus, for tmax rounds, the overall complexity becomes: O(tmax (k + Ie + Omax + k2 Omax)). Since k2 Omax may dominate for large k, this term can become the bottleneck. However, in practical scenarios, (the number of LLMs) is often small (e.g., 25), making the decentralized evaluation overhead manageable. Trade-Offs and Practical Considerations The decentralized evaluation approach increases computational overhead compared to the centralized model, as it requires every model to evaluate all candidate summaries. However, this additional cost is justified by the potential gains in robustness and reliability of the final output, but also by the flexibility to rely on multiple, potentially weaker models rather than single, highly capable central evaluator. By employing form of consensus voting, the system can arrive at more stable decision even when no single model is individually strong. While the added complexity of multi-round conversation can be non-trivial, it may lead to improved summary quality, especially when dealing with contentious or ambiguous source texts. Multiple rounds allow the system to refine the summaries and converge on stable solution. If consensus emerges quickly, the number of rounds tmax can be effectively reduced, thereby decreasing the total computational cost. Conversely, if no consensus is reached, the algorithm ultimately defaults to tie-break mechanism after tmax rounds, ensuring bounded time and cost. As with the centralized approach, prompt engineering and careful parameter selection (e.g., choosing Omax, tmax, and the number of participating models k) we can mitigate undue complexity."
        },
        {
            "title": "C Ablation Study",
            "content": "C.1 Varying Evaluation LLM In this section, we compare the scores of the centralized and decentralized approaches when the evaluator model and the tie-breaker models are GPT-4o or GPT-4o mini (instead of GPT-3.5 as in Table 2). These results are presented in Table 5. The sections where GPT-3.5 is the evaluator are reproduced from Table 2. In these experiments, the summary-generating models remain the same as those in Table 2. In rows (in Table 5) where GPT-4o is listed as the evaluator, however, the decentralized method would have required GPT-4o to be the default choice for tiebreaking summary as well when the model has not generated summaries. To remain maximally consistent with previous methodology, we modify the process here so that GPT-4o receives the finalround summaries from the decentralized method where GPT-3.5 is the tie-breaking choice and evaluator and performs centralized evaluation on top of the decentralized results. The reason the GPT3.5-default results are chosen as the basis instead of GPT-4o mini is because as an evaluator and default choice GPT-3.5 produced better final summaries compared to GPT-4o mini for both centralized and decentralized methods. The multi-LLM framework outperformed singleLLM baselines, averaging 64% improvement for the decentralized variant and 63% for the centralized variant. In some individual scores, our framework improves upon single-LLM setups by up to 3. GPT-3.5 emerged as the best-scoring evaluator and the best-scoring tie-breaker choice: for the centralized method, GPT-3.5 as an evaluator and tie-breaking choice outperforms other evaluators and tie-breakers, and for the decentralized method, GPT-3.5 turned out to be the best tie-breaking choice. Furthermore, GPT-3.5 as centralized evaluator and tie-breaking choice separately outperform both the decentralized and centralized methods using other models as the evaluator and tie-breaking choice. As with results in Table 2, additional rounds of evaluation and regeneration do not improve summary scores. C.2 Varying Model Combinations In Table 2 we present the results with GPT-3.5 and GPT-4o mini as the models in the combination; we now investigate the performance of our approaches for alternative combinations of LLMs (in Table 3). We use the following combinations for the 2-LLM framework: GPT-3.5 and GPT-4o mini, with GPT3.5 as the evaluator and default, GPT-4o and GPT3.5, again with GPT-3.5 as evaluator and default, and finally GPT-4o and GPT-4o mini, with GPT-4o mini as the evaluator and default. These alternative combinations all outperform single-LLM baselines. We see 54% improvement in the decentralized variant and 59% for the centralized variant. Combinations with GPT-3.5 as member and the evaluator/default choice offer larger improvements compared to those without GPT-3.5. Since we have used GPT-4o mini as the evaluator and tie-breaker where GPT-3.5 is absent, possible reason the improvements for these pairings are less than those where GPT-3.5 is present is that GPT-3.5 is larger model than GPT-4o mini. C.3 Varying the Number of LLMs In this experiment, we increase the number of LLMs in our multi-LLM system to ascertain the effects on summary quality, and present the results in Table 6. Here we use GPT-3.5, GPT-4o mini, and GPT-4o in the multi-LLM system. We see that while the 3-LLM system still outperform the singleLLM baseline, increasing the number of LLMs from 2 to 3 does not improve performance upon the 2-LLM system, contrary to the trend observed in the previous sections where 2-LLM system outperform single-LLM baselines. We offer two possible explanations for this finding. First, adding an additional LLM increases the complexity of the pipeline, which may lead to propagation of noise or redundancy in intermediate summaries. This added complexity could dilute the strengths of individual LLMs and reduce overall coherence and relevance in the final output. Second, the integration of third LLM introduces greater risk of inconsistencies in summarization styles, which may negatively affect evaluation metrics like ROUGE that rely on lexical overlap. C.4 Specialized Prompting We now investigate using single LLM to generate multiple different summaries of the text, and then using our framework to obtain the best summary. We explore the efficacy of varying prompt formulations and model parameters in regards to our framework. This experiment is grounded in the intuition that long documents contain very diverse sections within their content which may benefit from different summarization strategies. For example, different chunks of long document may cover distinct topics, serve various purposes, have diverse writing styles, and/or contain differing density. Given this diversity, simple uniform summarization prompt is less likely to actually capture the required essential information from each chunk. With this, we propose form of specialized prompting as way to leverage the distinctive capabilities and specializations of each model for specific chunks specifically in regards to our framework. We hypothesize that the use of specialized prompting can help further leverage LLM capabilities within our suggested multi-LLM framework to produce higher quality summaries which are more suitable for subsequent evaluation by multiple LLMs. We begin by generating four initial summaries using two sets of specialized prompts designed for GPT-3.5 and GPT-4o mini, ensuring that each model receives two distinct prompts. One prompt focuses on enhancing the coherence of the resulting summary (see Figure 7), while the second prompt aims to maximize precision in conveying the key facts (see Figure 8). After producing these four baseline summaries, we feed them into our multiLLM framework, which incorporates two agents GPT-3.5 and GPT-4o miniworking collaboratively. GPT-3.5 and GPT-4o mini are used for the initial generation of summaries, and GPT-3.5 also serves as the evaluator. The framework and methodology following the generation of the four baseline summaries, as well as their inclusion as input, mirGenerate summary that enhances coherence of the text in around 160 words. Output the summary text only and nothing else. [text] Figure 7: Prompt 1 for generating the initial summary in the first round. summary maximizes Generate precision related to the key facts of the text in around 160 words. Output the summary text only and nothing else. that [text] Figure 8: Prompt 2 for generating the initial summary in the first round. ror the procedures used to obtain decentralized and centralized results on ArXiv and GovReport in Table 2, with GPT-3.5 functioning as the evaluator. Results for this experiment are provided in Table 7. This experiment demonstrates that employing specialized prompting strategies within both decentralized and centralized multi-LLM frameworks significantly enhances the quality of generated summaries. These results show the importance of prompt engineering and strategic framework design in multi-LLM summarization tasks and we leave this for future work. C.5 Short-text vs. Long-text Summarization In this section, we investigate the effectiveness of our approach for shorter text summarization. For this experiment, we leverage the ArXiv dataset and only use the introduction of the paper as input for summarization and evaluate against the same ground-truth. The introduction subsections of papers are typically rich in content yet contain enough brevity to serve as quality standardized reference bases for our goal of long and short text experimentation. With this experiment we present results that showcase the trade offs and performance differences of our methodologies on shorter text summarization compared to that of long document summarization. Generally, ArXiv papers contain detailed markers and section titles to distinguish introduction sections. However, using the Hugging Face dataset of ArXiv papers for our experimentation the format in which the article is represented is string containing the \"body\" of the paper which contains little to no explicit markers for section identification. Thus, we present simple heuristic to distinguish the introduction text from the rest of ArXiv GovReport ROUGE-1 ROUGE-L BLEU-1 BLEU-4 ROUGE-1 ROUGE-L BLEU-1 BLEU-4 GPT-4o mini Evaluator GPT-3.5 Evaluator GPT-4o Evaluator Decentralized Centralized Decentralized Centralized Decentralized Centralized Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max 0.317 0.326 0.315 0.330 0.313 0. 0.329 0.333 0.326 0.325 0.318 0.327 0.160 0.163 0.158 0.165 0.163 0. 0.168 0.173 0.166 0.165 0.162 0.167 0.206 0.221 0.201 0.222 0.200 0. 0.217 0.219 0.214 0.211 0.206 0.215 0.026 0.027 0.027 0.028 0.029 0. 0.031 0.036 0.030 0.030 0.027 0.031 0.445 0.438 0.441 0.439 0.447 0. 0.468 0.479 0.446 0.456 0.449 0.461 0.178 0.175 0.176 0.175 0.180 0. 0.189 0.197 0.179 0.183 0.181 0.186 0.452 0.446 0.447 0.446 0.458 0. 0.470 0.485 0.456 0.461 0.452 0.467 0.094 0.089 0.092 0.090 0.098 0. 0.109 0.121 0.098 0.100 0.096 0.105 Table 5: Results for different evaluating and tie-breaking models for Multi-LLM approaches. The choice of the tie-breaker models is the same as the choice of evaluator model. We bold the best results for each combination of the experimental variables, and we underline the best results overall. For ease of comparison, we reproduce the best-performing 2-LLM results obtained in Table"
        },
        {
            "title": "GovReport",
            "content": "ROUGE-1 ROUGE-L BLEU-1 BLEU-4 ROUGE-1 ROUGE-L BLEU-1 BLEU-4 2-LLMs GPT-3.5 Evaluator 3-LLMs GPT-4o mini Evaluator 3-LLMs GPT-3.5 Evaluator"
        },
        {
            "title": "Centralized",
            "content": "3 rounds 1 rounds 3 rounds 1 rounds 3 rounds 1 rounds 3 rounds 1 rounds 3 rounds 1 rounds 3 rounds 1 rounds 0.313 0.339 0.329 0.333 0.301 0.299 0.300 0.300 0.300 0.309 0.294 0. 0.163 0.180 0.168 0.173 0.154 0.152 0.153 0.152 0.154 0.159 0.151 0. 0.200 0.224 0.217 0.219 0.184 0.184 0.185 0.186 0.184 0.193 0.177 0. 0.029 0.043 0.031 0.036 0.024 0.023 0.023 0.023 0.024 0.027 0.023 0. 0.447 0.468 0.468 0.479 0.445 0.442 0.443 0.442 0.446 0.451 0.451 0. 0.180 0.190 0.189 0.197 0.178 0.178 0.178 0.178 0.179 0.182 0.181 0. 0.458 0.477 0.470 0.485 0.449 0.447 0.447 0.449 0.443 0.459 0.440 0. 0.098 0.112 0.109 0.121 0.095 0.094 0.094 0.093 0.094 0.099 0.095 0. Table 6: Multi-LLM framework with three models. We bold the best results for each combination of the experimental variables, and we underline the best results overall. For ease of comparison, we reproduce the best-performing 2-LLM results obtained in Table 2 ROUGE-1 ROUGE-L BLEU-1 BLEU-4 ROUGE-1 ROUGE-L BLEU-1 BLEU-"
        },
        {
            "title": "Centralized",
            "content": "3 round max 1 round max 3 round max 1 round max 3 round max 1 round max 3 round max 1 round max 0.313 0.339 0.329 0. 0.300 0.338 0.316 0.355 0.163 0.180 0.168 0.173 0.155 0.175 0.162 0. 0.200 0.224 0.217 0.219 0.201 0.236 0.215 0.251 0.029 0.043 0.031 0. 0.025 0.040 0.032 0.049 0.447 0.468 0.468 0.479 0.464 0.469 0.473 0. 0.180 0.190 0.189 0.197 0.174 0.181 0.177 0.185 0.458 0.477 0.470 0. 0.441 0.486 0.452 0.494 0.098 0.112 0.109 0.121 0.093 0.104 0.101 0. Table 7: Results on the use of 2 specialized prompts on where the only change in the pipeline is that 4 total specialized baseline summaries are fed in initially instead of the 2 simple prompts fed in the methodology used to curate Table 2. Note that these results use GPT-3.5 for the evaluator in the centralized approach, and for breaking ties in the decentralized multi-LLM approaches. This is for 15 sample size for both datasets. Refer to Figure 7 and Figure 8 for the prompts used for initial generation. We bold the best results for each combination of the experimental variables, and we underline the best results overall. the article text. We manually went through 5 randomized example articles, with an assumption that the beginning of the article text starts with the introduction section, and found at which inflection point the introduction section concludes. After averaging the word count of the introduction sections and including an extension buffer to capture certain articles which may have slightly longer introduction sections we establish benchmark for the using the first 1,500 words in ArXiv articles as our reference introduction section. We algorithmically consider word as break between the article string wherever there is whitespace. Refer to Figure 9 for more detailed explanation. We ultimately curate 20% of the examples from the test set using this strategy for performance testing on our metrics. Full results are provided in Table 8. We highlight several key aspects of our multiLLM summarization methodology using both the centralized and decentralized approaches, showcasing distinct performance across both long and short text summarization tasks.As evident by our results, short articles consistently show better performance compared to long articles, showcasing the inherent complexities and nuances of longer texts that plague LLMs in terms of capturing and summarizing relevant content. The similar performance on metrics like ROUGE-1 and BLEU-4 in our centralized approach across different text lengths might indicate consistency in how our methodology is able to capture the essential content and has the ability to reproduce the core narrative elements of the text regardless of length. Furthermore, we posit the difference in performance across long and short text for BLEU-1 is based primarily on the metrics itself as it measures the unigram overlap between the generated summary and the reference text. In the case of short texts, the decentralized approach and especially the 3 round performs best as each round and each model provides an opportunity to focus more accurately on and determine crucial unigrams that are significant within the context of compact introduction section. This iterative refinement likely leads to higher precision in capturing key terms and phrases, directly contributing to better BLEU-1 scores than in the case of the centralized approach which performs best as the context length is scaled up as shown in the results for long text."
        },
        {
            "title": "ArXiv",
            "content": "ROUGE-1 ROUGE-L BLEU-1 BLEU-"
        },
        {
            "title": "Centralized",
            "content": "Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max Multi-LLM 3 round max Multi-LLM 1 round max 0.329 0.333 0.313 0. 0.360 0.369 0.367 0.379 0.168 0.173 0.163 0.180 0.188 0.198 0.194 0. 0.217 0.219 0.200 0.224 0.328 0.309 0.321 0.305 0.031 0.036 0.029 0. 0.038 0.044 0.041 0.049 Table 8: Results on short summarization tasks using the ArXiv dataset for the decentralized and centralized MultiLLM approaches. Note that these results use GPT-3.5 for the evaluator in the centralized approach, and for breaking ties in the decentralized multi-LLM approaches. Figure 9: Here we showcase an example of how we choose at which point an introduction ends. The total word count of this example article was 7,671 and the word count of the reference summary was 172. We highlight the inflection sentence which most serves as the transition from the actual background and theoretical setup of the paper to the actual methodologies which are then detailed in later text. From here we gather the word count of everything before the inflection sentence and classify it as our reference introduction text for experimentation, including the inflection sentence. In this case, the resulting introduction section had total word count of 1203."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of California, Santa Cruz"
    ]
}