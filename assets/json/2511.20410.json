{
    "paper_title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs",
    "authors": [
        "Bao Tang",
        "Shuai Zhang",
        "Yueting Zhu",
        "Jijun Xiang",
        "Xin Yang",
        "Li Yu",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 1 4 0 2 . 1 1 5 2 : r Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs"
        },
        {
            "title": "Jijun Xiang Xin Yang",
            "content": "Li Yu Wenyu Liu Xinggang Wang Huazhong University of Science and Technology Figure 1. Comprehensive Comparison. Left: GPU memory usage versus batch size during training, where Batch Size denotes the number of samples actually involved in optimization. Middle: Comparison of FID scores and throughput across different methods; the marker size indicates the model parameter count. Right: GPU memory consumption and total training time under identical training configurations."
        },
        {
            "title": "Abstract",
            "content": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher models generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectoryextracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 Corresponding author (xgwang@hust.edu.cn). FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusiongeneration space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/ hustvl/TBCM . 1. Introduction Diffusion models have achieved remarkable success across wide range of generative tasks, such as image [2, 8, 11, 3234] and video [3, 4, 20, 40, 44] synthesis. However, their generation process typically requires dozens or even hundreds of iterative denoising steps [15, 29, 38], leading to extremely long inference times and high computational costs, which severely limit their real-world applicability. To address this issue, line of research known as timestep distillation [24, 26, 3537, 39, 45, 46] has emerged, which aims to transfer the multi-step diffusion process into compact student model capable of generating high-quality samples in only few steps. Figure 2. One Step Generation Results. High-resolution (10241024) images generated by our one-step generator distilled from the Sana 0.6B model using the proposed TBCM. More results with different sampling steps are provided in the Appendix. Among these efforts, Consistency Models (CMs) [39] have recently attracted significant attention for their elegant formulation and training efficiency. They leverage the consistency constraint, which eliminates the need for supervision from diffusion model samples [26, 35], thereby avoiding the computational overhead of generating synthetic datasets and circumventing the inherent training instability of adversarial methods. Continuous-Time Consistency Distillation (CTCD), as the continuous-time formulation of CMs, removes the discretization error present in discretetime variants and achieves superior distillation quality. Furthermore, sCM [24] establishes training paradigm based on the TrigFlow architecture, which effectively stabilizes the training of CTCD and makes it highly promising approach for timestep distillation. Although sCM partially addresses the stability issues of CTCD, it still faces several limitations, such as high data requirements, expensive training costs, and limited applicability. Moreover, during the distillation process, its target sample points are still generated through forward diffusion following the pretraining paradigm of diffusion models. These samples inherently differ from the models actual inference trajectory, which constrains its potential to further improve the quality of one-step generation. In this work, we propose TBCM, an image-free distillation framework that harnesses the teacher models generative ability by sampling along its inference trajectories, which mitigates traininginference inconsistency and enables fully latent-space consistency distillation. By removing VAE involvement and performing multiple trajectory samples per prompt, TBCM significantly lowers GPU memory usage, reduces training time, and is conveniently transferable to other diffusion-based tasks. In addition, we conduct detailed investigation into how sampled points from different trajectories influence the training performance, providing strong empirical evidence for the pivotal role of sampling scheme in the consistency distillation process. To further enhance the distillation quality, we adjust the weighting of the unstable term in the sCM loss, achieving more balanced optimization objective and improved training results. Through these efforts, we successfully realize efficient timestep distillation for text-to-image (T2I) models under the image-free setting. As shown in Fig. 1, our approach achieves an outstanding FID of 6.52 and CLIP score of 28.08 under one-step generation on the MJHQ-30k [19] benchmark, while significantly reducing both the training time and GPU memory consumption compared with SanaSprint [9] and the standard sCM [24] baseline. Overall, our main contributions can be summarized as follows: An Image-Free Distillation Framework. We design continuous-time consistency distillation framework that fully leverages the teacher models generative capability, enabling distillation to be performed entirely in the latent space without any image data. This image-free setting eliminates the need for VAE involvement and data preprocessing, making the process more efficient and lightweight. New Perspective on Distillation Samples. By systematically examining the forward-sampling strategy in sCM and the backward-sampling strategy in TBCM, we uncover the inherent differences between forward and backward sample spaces. This provides novel understanding of how sampling strategies in different spaces influence the quality of consistency distillation. 2 Method EDM [16, 17] αt 1 FM [21, 23] 1 t TrigFlow [1, 24] cos sin σt t-range Training Objective E(cid:2) ˆxθ(xt, t) x02 E(cid:2)vθ(xt, t) (z x0) (cid:3) 2 2 (cid:3) (cid:104) σdFθ( xt σd , t) (cos sin x0)2 2 (cid:105) Sampling ODE dt = xt ˆxθ (xt,t) dxt dxt dt = vθ(xt, t) dt = σdFθ( xt dxt σd [0, ] [0, 1] [0, π 2 ] Initial Noise xT (0, 2I) x1 (0, I) , t) xπ/2 (0, σ2 dI) Table 1. Unified comparison of EDM, Flow Matching and TrigFlow formulations. Low-Cost and High-Quality Distillation. The proposed framework significantly reduces GPU memory usage and shortens training time by approximately 40%, while simultaneously improving latent consistency between training and inference. This ensures both low training cost and superior generation quality. 2. Related Work Diffusion Models. Diffusion models (DMs) have become dominant paradigm in generative modeling since the introduction of DDPM [15] and its improved variants [29]. Numerous works, such as DDIM [38], DPM-Solver [25], and EDM [16], have focused on accelerating and stabilizing the sampling process. Recently, Flow Matching [21, 23] reformulated diffusion as learning continuous flows between data and noise, offering unified perspective for deterministic generation. Building on these advances, SANA [43] leverages the highly compressed DCAE [5] and linear architecture to achieve efficient and high-quality generation. Timestep Distillation. Efforts to accelerate diffusion inference through timestep distillation fall into two main cattrajectory-oriented and distribution-oriented apegories: proaches. Early trajectory-oriented methods [26, 35] leverage the teacher models full ODE trajectory to capture the mapping between noise and images. Consistency Models [13, 18, 24, 27, 39, 41] further impose self-consistency constraint, aligning x0 predictions across adjacent timesteps. Distribution-oriented approaches, in contrast, aim to match overall generative distributions. ADD [37] performs pixel-domain distillation with adversarial learning using pretrained perceptual encoders, whereas LADD [36] shifts this process into the latent space for computational efficiency. Variational Score Distillation (VSD) [31, 42] provides non-adversarial alternative, with subsequent methods [28, 4547] building on this idea to improve stability and effectiveness. 3. Preliminaries 3.1. Different Formulations of Diffusion Models Diffusion-based generative models synthesize data by reversing progressive noising process. Given x0 pdata, perturbed sample is defined as xt = αtx0+σtz, where (0, I) and (αt, σt) defines the noise schedule. Different parameterizations yield distinct formulations (see Tab. 1), mainly differing in interpolation schedules and vector field parameterization. 3.2. Continuous-Time Consistency Models Consistency Models (CMs) [39] learn to predict the clean data x0 from an arbitrary noisy observation xt along the trajectory of probability flow ODE. Formally, CM parameterizes neural network fθ(xt, t) that outputs the estimated clean signal, which remains consistent across different noise levels. From Discrete to Continuous. Early CMs [27, 39] employ discrete-time training with consistency loss between neighboring timesteps: CM = Ext,t lt (cid:2)d(fθ(xt, t), fθ (xtt, t))(cid:3) , (1) where d(, ) is distance metric. This discrete formulation inevitably introduces discretization errors. ContinuousTime Consistency Models [24, 39] overcome this by taking the infinitesimal limit 0, yielding smooth training objective free of discretization artifacts: lcont. CM = Ext,t (cid:104) w(t) (cid:10)fθ(xt, t), df θ dt (xt, t)(cid:11)(cid:105) . (2) Trigonometric Parameterization. df Matching framework, the term be expressed as In the classical Flow in Eq. (2) can θ (xt,t) dt dfθ (xt, t) dt = fθ (xt, t) + xtfθ (xt, t) dxt dt . (3) However, previous work [12, 39] found that this optimization objective is highly unstable and difficult to scale up for large models or datasets. To address this issue, the TrigFlow architecture was proposed (see Sec. 3.1). Under the TrigFlow formulation, this term is instead represented as df θ (xt,t) dt = cos(t) (cid:16) σdFθ (cid:16) xt σd (cid:17) , sin(t) (cid:16) xt + σd dF 3 (cid:17) (4) dxt dt (cid:17) ,t (cid:17) . θ (cid:16) xt σd dt Figure 3. Discrepancy of Equivalent Noise Between Forward and Backward Processes. The equivalent noise (see Eq. (8)) remains constant in forward diffusion, but evolves noticeably in backward generation, reflecting the traininginference inconsistency. Aligning FM and Trig Parameterizations. SanaSprint [9] unifies the FM and Trig representations through explicit transformations. The time and data mapping is Encoding stages account for less than 1% of the total time, while the Text Encoder contributes substantial proportion, comparable to the Diffusion Distillation process. tFM = xt,FM = sin(tTrig) sin(tTrig) + cos(tTrig) xt,Trig σd (cid:113) t2 FM + (1 tFM)2. , The output transformation is (cid:99)Fθ (cid:18) xt,Trig σd (cid:19) , tTrig, = (cid:112)t2 1 (cid:104) FM + (1 tFM)2 + (1 2tFM + 2t2 (1 2tFM)xt,FM FM)vθ(xt,FM, tFM, y) (cid:105) . (5) (6) (7) 4. Method 4.1. Trajectory-Backward Consistency Models Finding 1: Resource Bottlenecks in Distillation. During the distillation process, VAE encoding constitutes major source of GPU memory consumption, while prompt encoding occupies substantial portion of the training time. As shown in Fig. 4, the top part illustrates the memory usage breakdown during distillation, where Base Memory consists of the VAE, Text Encoder (TE), Student and Teacher models, and Dynamic Overhead denotes the maximum memory consumption across different stages. The VAE encoding stage exhibits significantly higher usage than others, accounting for approximately 80% of the total memory consumption. The bottom part shows the time breakdown during distillation, where the Data Loader and VAE Figure 4. Resource Bottlenecks in Continuous-Time Consistency Distillation. Top: Memory usage breakdown during distillation. Bottom: Training time breakdown during distillation. Insight: To overcome the GPU memory bottleneck during distillation, we can fully leverage the generative capability of the pretrained model and perform distillation purely in the latent space. This design decouples the distillation process from the VAE encoder, establishing an image-free distillation paradigm that fundamentally eliminates dependence on the VAE. To mitigate the training-time bottleneck, generating multiple samples for single prompt can effectively amortize the text encoding overhead, thereby accelerating the overall training process. Figure 5. Distillation Paradigm of TBCM. Left: Distillation begins with random noise and text prompt inputs. Middle: Multiple samples are generated for single prompt within the latent space. Right: The collected samples are used to compute the consistency loss. Finding 2: TrainingInference Inconsistency. In diffusion model distillation, the samples received during training differ substantially from those encountered during inference. Although diffusion models are pretrained under forward sampling paradigm, where training samples are obtained by adding noise of varying magnitudes to clean images, they perform inference along fundamentally different backward sampling trajectory. To characterize this discrepancy, we introduce the concept of Equivalent Noise: oiseeqv = xt cos(t) ˆx0 sin(t) . (8) As shown in Fig. 3, we observe that the equivalent noise remains consistent during the forward process, while exhibiting significant shifts during the backward process. Specifically, it gradually transforms from random noise to patterns correlated with the prediction target, revealing that diffusion models learn coarse-to-fine paradigm, which aligns with observations from recent studies [10, 14, 22]. To demonstrate that this discrepancy is not merely instance-level but manifests systematically across the distribution, we further visualize the overall sample distribution using t-SNE (see Appendix). The results consistently show substantial inconsistency between the sample distributions in the forward and backward processes. Such traininginference inconsistency indicates that the constraints applied to noisy samples during distillation are not properly aligned with the actual inference trajectory, which may potentially undermine the effectiveness of the distillation process. Insight: To mitigate the discrepancy between training and inference, distillation should align training samples with the actual backward trajectory by sampling along the pretrained models inference path, thereby enhancing the effectiveness of the distillation process. Solution: Trajectory-Driven Consistency Learning. To jointly mitigate the resource bottlenecks and the traininginference inconsistency observed in previous sCM distillation frameworks, we introduce trajectory-based distillation scheme that operates directly in the latent space without invoking the VAE encoder, while simultaneously generating multiple samples along the trajectory for each prompt. Specifically, instead of generating noisy inputs by adding noise to VAE-encoded images, we explicitly simulate the teachers denoising trajectory as dxt dt = Fteacher (cid:19) , , (cid:18) xt σd xtt = cos(t) xt sin(t)σd (9) dxt dt . By integrating this ODE trajectory, we obtain both the intermediate states xt and the teacher-predicted temporal derivatives dxt dt , which are essential for the subsequent distillation while avoiding repeated VAE encoding. These quantities are then used to compute (cid:99)Fθ in Eq. (4), and further serve to construct the continuous-time consistency loss defined in Eq. (2). The overall framework is illustrated in Fig. 5. 5 4.2. Sampling Schemes Shaping the Sample Space Finding 3: Sample Space Drives Consistency Distillation. From the composition of the sCM loss (see Eq. (4)), it is evident that the samples affecting sCM training depend not directly on the clean image x0, but rather on the noisy samples xt. Pairs of (xt, t) constitute the complete sample space for sCM training. Following the discussion in Section. 4.1 on training inference inconsistency, we define the sample space obtained from forward sampling as the diffusion space, and that from backward sampling as the generation space. Insight: The composition of the sample space is decisive factor affecting the effectiveness of consistency distillation. By flexibly adjusting the sample scheme, we can achieve optimal distillation quality. Preliminary: Sampling Scheme in Diffusion Space. Conventional sCM and Sana-Sprint methods perform sampling in the diffusion space. In diffusion space, since the clean image x0 is fixed, the training samples (xt, t) are influenced only by the sampling timestep and the random noise. Consequently, the corresponding sampling strategy typically focuses on the distribution of noise magnitudes, i.e., the distribution of sampled timesteps. widely used approach is the logit-Normal proposal distribution. In the TrigFlow architecture, it is used to sample tan(t), such that eσd tan(t) (Pmean, 2 std), which is adopted in both sCM and Sana-Sprint. Extension: Sampling Scheme in Generation Space. Our proposed TBCM performs sampling in the generation In generation space, the training samples (xt, t) space. are influenced not only by the sampling timestep but also by the inference trajectorythat is, by each intermediate timestep along the path from pure noise to the target sample. For given number of sampling steps , the i-th training sample along the trajectory is affected by {tN 1, . . . , ti+1, ti}. Therefore, the choice of trajectory can significantly impact the training outcome. Consequently, when designing the sampling strategy in the generation space, we should consider not only the overall distribution of timesteps but also the manner in which the sampling trajectory is obtained. Relevant ablation studies are presented in Sec. 5.2. 4.3. Additional Adjustments Brightness Filter. Due to the uncertainty in the sampling trajectory, the teacher model may generate low-quality predictions of the clean image ˆx0 after completing the entire trajectory. We observe that these low-quality images often share common characteristic: they exhibit low overall brightness. To filter such samples without involving VAE, We observe that dark images in pixel space are mapped to latent representations that are close to those of an allblack image. This property allows us to directly filter lowbrightness samples in the latent space (see Appendix). Stability Hyperparameter. In sCM and Sana-Sprint, to dt ) in dFθ dFθ stabilize the unstable term sin(t)(xt + σd , the factor sin(t) is replaced with sin(t), and gradually warms up from 0 to 1 during early training. In TBCM, we find that 1.0 is not the optimal value for r. We therefore explore different choices of as well as various schedules for its change. Experimental details are presented in Sec. 5.2. dt Figure 6. Visual Comparison under One-Step Generation. 5. Experiments 5.1. Main Results Experimental Setup. Thanks to the image-free nature of our distillation pipeline, we directly collect 1M randomly sampled text prompts for training without any paired im6 Pre-train Distillation Methods SDXL [30] PixArt-Σ [7] SANA [43] SDXL-LCM [27] PixArt-LCM [6] PixArt-DMD [7] PCM [41] SDXL-DMD2 [45] Sana-Sprint [9] sCM [24] TBCM (Ours) Inference Throughput Latency Params steps (samples/s) 50 20 20 1 1 1 1 1 1 1 0.15 0.4 1.7 3.36 4.26 4.26 3.16 3.36 7.22 7.22 7.22 (s) 6.5 2.7 0.9 0.32 0.25 0.25 0.40 0.32 0.21 0.21 0.21 (B) 2.6 0.6 0.6 2.6 0.6 0.6 2.8 2.6 0.6 0.6 0.6 FID CLIP 6.63 6.15 5.81 50.51 73.35 9.59 30.11 7.10 7.04 7.46 6.52 29.03 28.26 28. 24.45 23.99 26.98 26.47 28.93 28.04 27.74 28.08 Table 2. Comparison of our method with various approaches on the MJHQ-30k test set. The reported results of baseline methods are mainly sourced from the Sana-Sprint report [9]. Methods Sana-Sprint [9] sCM [24] TBCM (Ours) Average Sample Time Total Training Samples Memory Usage Total Training Time (GPU * s) 2.7 2.8 1.6 (BS * Steps) 512 * 20000 512 * 20000 512 * 20000 (GB) (GPU * h) 72.0 (+14.5%) 62.9 ( 0.00%) 22.6 (-64.1%) 7680 (-3.60%) 7965 ( 0.00%) 4640 (-41.7%) Table 3. Training costs comparison of different schemes. All training time measurements were conducted on cluster with 4 nodes (32 NVIDIA V100 GPUs in total), while memory usage was evaluated on single A100 GPU with batch size of 16. age data. All experiments are conducted on cluster of 32 NVIDIA V100 GPUs (32 GB each). For fair comparison, we strictly follow the training configurations of Sana-Sprint except for minor learning rate adjustment introduced by the trajectory sampling scheme. The teacher model used in our distillation is the officially released SanaSprint 0.6B teacher model. We evaluate all models on the MJHQ-30k [19] benchmark using FID (Frechet Inception Distance) and CLIP Score metrics to measure perceptual quality and textimage alignment. Results and Analysis. As shown in Tab. 2, our proposed TBCM achieves remarkable balance between efficiency and fidelity in the one-step generation setting. Specifically, TBCM obtains an outstanding FID of 6.52 and CLIP score of 28.08, outperforming existing distillation-based methods, including Sana-Sprint (7.04 FID, 28.04 CLIP score), under the same training setup. As shown in Tab. 3, our TBCM method reduces training costs by over 40% and saves more than 60% of GPU memory compared to SanaSprint and sCM. Fig. 6 shows comparison of the visualization results of Sana-Sprint, sCM, and TBCM. The results validate that our method successfully leverages trajectorysampled pairs to transfer teacher knowledge more effectively, leading to sharper visual quality and stronger text image consistency in single inference step. 7 5.2. Ablation Study Sampling Schemes. As discussed in Sec. 4.2, the sampling strategy strongly affects the distribution of trajectory samples in the generation space. To verify this, we perform ablation experiments comparing three sampling schemes: Figure 7. Sampling Patterns of Different Strategies. Top to bottom: Three sampling strategies Random, Logit-Normal, Reference Route. Vertical lines: Five random sampling instances per strategy. Shaded regions: Sampling density distributions. Random: uniformly sample points from the interval [0, π/2] to form the denoising trajectory. Logit-Normal: adopt the common logit-normal sampling in diffusion space, following the same hyperparameters as Sana-Sprint (PMean = 0.2, PStd = 1.6). Reference Route: extract the timesteps from FlowEuler Scheduler incorporating schedule shift, map them to the [0, π/2] interval via the inverse of Eq. (5), and use the resulting trajectory as reference for partitioned sampling, which allocates timesteps to different partitions to balance sampling density and preserve trajectory fidelity. As shown in Tab. 4, the Logit-Normal strategy improves upon the Random scheme by optimizing the timestep distribution, while the Reference Route approach further constrains each trajectory to include samples within every subregion, thereby reducing randomness compared to probabilistic sampling. Although the timestep distributions of the Logit-Normal and Reference Route strategies are visually similar  (Fig. 7)  , the Reference Route strategy achieves the best FID and CLIP performance, followed by the LogitNormal one. These results support our claim in Sec. 4.2 regarding the influence of generation-space sampling. Sample Schemes FID CLIP Random Logit-Normal Reference Route 9.23 7.18 6.79 27.74 27.80 28.00 Table 4. Ablation study on different trajectory sampling methods. Sampling Steps. Beyond the sampling scheme, the number of sampled steps also significantly impacts the distillation outcome, as it determines the coverage of the generation trajectory. As shown in Tab. 5, increasing the number of sampled steps leads to clear improvement in FID, while the CLIP score remains relatively stable. This observation aligns with our finding that fewer steps yield lower-quality clean samples in the denoising trajectory. Sample Steps Memory Usage FID CLIP 8 12 16 20 20.4 GB 21.2 GB 21.9 GB 23.6 GB 7.52 6.87 6.77 6.58 27.98 28.00 28.00 27.98 Table 5. Ablation study on the number of sampling steps. Hyperparameter R. We further analyze the stabilityrelated hyperparameter introduced in Sec. 4.3. As shown in Tab. 6, setting the final value Rfinal = 0.75 achieves the most stable and effective training. Tab. 7 further compares two scheduling strategies, revealing that the WarmupCooldown schedule (i.e., warming up to 1 and then cooling down to Rfinal) outperforms the direct Warmupto-Rfinal scheme. Final Value FID CLIP 1.0 0.75 0.65 0.5 7.66 6.77 6.81 7.41 28.00 28.00 27.98 27. Table 6. Ablation study on the selection of Rfinal value."
        },
        {
            "title": "R Schedule",
            "content": "FID CLIP Warmup WarmupCooldown 6.58 6.48 27.98 28.00 Table 7. Ablation study on different scheduling strategies. 6. Conclusion In this work, we presented TBCM, continuous-time consistency distillation framework that conducts the entire distillation procedure within the latent space under image-free conditions. By removing VAE involvement, TBCM substantially reduces training cost and enables efficient, scalable deployment across wide range of diffusion backbones. Furthermore, by bridging the traininginference inconsistency, TBCM achieves strong one-step generation performance despite its lightweight design. Despite these advantages, TBCM still exhibits certain limitations. Without real image supervision, its effectiveness is inherently constrained by the capacity and biases of the teacher model. Imperfect generative behaviors from the teacher may propagate to the student, potentially limiting sample diversity or inducing mild mode collapse. This highlights key challenge in image-free distillation: the students performance is tightly coupled to the quality of the teachers synthetic trajectories. From broader perspective, our formulation introduces the notion of sample space as supplement to consistency distillation. The expressiveness and structure of the constructed sample space play critical role in shaping the behavior of continuous-time distillation. Designing more expressive, well-structured sample spaces therefore remains an open and valuable research direction. We believe that future work combining TBCM with complementary generative or regularization strategies may further mitigate teacher-induced limitations. More broadly, we hope the conceptual lens of sample space inspires further research into more fundamental, principled, and generalizable formulations of consistency distillation."
        },
        {
            "title": "References",
            "content": "[1] Michael Samuel Albergo and Eric Vanden-Eijnden. BuildIn The ing normalizing flows with stochastic interpolants. Eleventh International Conference on Learning Representations. 3 [2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. 1 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 1 [5] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. In The Thirteenth International Conference on Learning Representations, . 3 [6] Junsong Chen, Simian Luo, and Enze Xie. Pixart-δ: Fast and controllable image generation with latent consistency models. In ICML 2024 Workshop on Theoretical Foundations of Foundation Models, . [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. 7 [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion In transformer for photorealistic text-to-image synthesis. ICLR, 2024. 1 [9] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. 2, 4, 7 [10] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: training-free acceleration method tailored for diffusion transformers. ArXiv, abs/2406.01125, 2024. 5 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the 41st International Conference on Machine Learning, pages 1260612633, 2024. 1 [12] Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin, and Zico Kolter. Consistency models made easy. In The Thirteenth International Conference on Learning Representations. 3 [13] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. [14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations. 5 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 3 [16] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 3 [17] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the In Proceedings of training dynamics of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 3 [18] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In The Twelfth International Conference on Learning Representations. 3 [19] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. 2, 7 [20] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1 [21] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In 11th International Conference on Learning Representations, ICLR 2023, 2023. [22] Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu Wang. Oms-dpm: Optimizing the model schedule for diffusion probabilistic models. In International Conference on Machine Learning, pages 2191521936. PMLR, 2023. 5 [23] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations. 3 [24] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. In The Thirteenth International Conference on Learning Representations. 1, 2, 3, 7 [25] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Ad9 vances in neural information processing systems, 35:5775 5787, 2022. 3 40th International Conference on Machine Learning, pages 3221132252, 2023. 1, 2, [40] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1 [41] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. 3, 7 [42] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36: 84068441, 2023. 3 [43] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synarXiv preprint thesis with linear diffusion transformers. arXiv:2410.10629, 2024. 3, 7 [44] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations. 1 [45] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 1, 3, [46] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 1 [47] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 3 [26] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 1, 2, 3 [27] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 3, 7 [28] Weijian Luo, Zemin Huang, Zhengyang Geng, Zico Kolter, and Guo-jun Qi. One-step diffusion distillation through score implicit matching. Advances in Neural Information Processing Systems, 37:115377115408, 2024. 3 [29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 1, 3 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations. [31] Ben Poole, Ajay Jain, Jonathan Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations. 3 [32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 1 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [35] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations. 1, 2, 3 [36] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion In SIGGRAPH Asia 2024 Conference Papers, distillation. pages 111, 2024. 3 [37] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. 1, [38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations. 1, 3 [39] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya In Proceedings of the Sutskever. Consistency models. 10 Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material provides additional analyses and results to complement the main paper. Specifically, Sec. visualizes the distribution shift of equivalent noise during the diffusion and generation processes. Sec. compares the effects of different sampling strategies on the predicted x0. Sec. presents the full TBCM algorithm. Sec. illustrates the Brightness Filter for identifying low-quality latent samples, and Sec. shows additional multi-step generation results. As shown in Fig. 9, the Reference Route sampling scheme consistently produces high-quality x0 predictions close to those obtained with the Flow Euler Scheduler, followed by the Logit-Normal scheme, and then Random sampling, which aligns with the experimental results in Tab. 4. On the other hand, increasing the number of inference steps generally improves the quality of predicted x0, but the differences gradually diminish as the number of steps increases, consistent with the observations reported in Tab. 5. A. Distribution Shift of Equivalent Noise. As discussed in Sec. 4.1, to verify that the differences in equivalent noise between the forward and backward processes are distributional rather than instance-specific, we visualize the overall distribution of equivalent noise during the diffusion and generation processes using t-SNE, as shown in Fig. 8. The results show that in the diffusion process, the distribution of equivalent noise remains consistent, reflecting the instance-level consistency shown in Fig. 3. In contrast, during the generation process, the noise distribution exhibits significant pattern changes, and its similarity to the initial noise gradually decreases throughout the process. (a) Visualizations of predicted x0 under different sampling schemes. Figure 8. Evolution of Equivalent Noise Distributions. Curves: Cosine similarity between timestep-specific equivalent noise and initial noise. Scatter Plots: T-SNE projections of equivalent noise distributions at selected timesteps. B. Comparison of Sampling Strategies. We compare the predictions of x0 by the teacher model under different sampling schemes and inference steps, based on the hypothesis that better x0 predictions partially reflect higher overall sample quality along the entire trajectory. 1 (b) Visualizations of predicted x0 under different sampling steps. Figure 9. Comparison of predicted x0 across (a) different sampling schemes and (b) different sampling steps. Algorithm 1 Training Algorithm of TBCM. Input: prompt dataset D, pretrained diffusion model Fpretrain with parameter θpretrain, model Fθ, weighting wϕ, learning rate η, constant c, warmup iteration H, black latents zb, final value rf , calmdown start iteration Sr, calmdown steps Tr. Note: σd is not required in TBCM but is kept here for notational consistency. Init: θ θpretrain, Iters 0. repeat dI) D, xt (0, σ2 , get denoise trajectory from sampling schemes for each timestep ti in trajectory do , t, y) dt σdFpretrain( xt dxt σd {xt}, { dxt xt cos(ti ti+1)xt sin(ti ti+1) dxt dt dt } end for mask black filter(xt/σdata, zb) xt Concatenate(X ), dxt min(1, Iters/H) min(max((Iters Sr)/Tr, 0), 1) (1 p) + rf cos2(t)(σdFθ dxt g/(g + c) L(θ, ϕ) ewϕ (t) Fθ( xt σd L(θ, ϕ) mask L(θ, ϕ) (θ, ϕ) (θ, ϕ) ηθ,ϕL(θ, ϕ) Iters Iters + 1 dt Concatenate(V), dt ) cos(t) sin(t)(xt + σd dFθ dt ) , t, y) Fθ ( xt σd , t, y) g2 2 wϕ(t) Image-free inputs Brightness filter Trajectory sampling Tangent warmup adjustment JVP rearrangement Tangent normalization Adaptive weighting until convergence C. Algorithm of TBCM. We provide the algorithm for the TBCM paradigm, with key differences from sCM highlighted in blue. As shown in Algorithm 1, TBCM does not require any image data and collects training samples along the teachers inference trajectory, which are then used to compute the sCM loss. The algorithm also incorporates the brightness filter and stability hyperparameter adjustment introduced in Sec. 4.3. D. Illustration of the Brightness Filter. We provide an illustration of the Brightness Filter strategy mentioned in Sec. 4.3, as shown in Fig. 10. This strategy aims to directly identify low-quality samples in the latent space without decoding them to pixel space via the VAE. Since low-quality samples generated by the teacher model are often observed to be unusually dark, this issue can be addressed by filtering out samples with low brightness directly in the latent space. To this end, we precompute the latent representation of completely black image and measure its similarity to latent-space samples, followed by simple threshold-based filtering. Figure 10. Illustration of the Brightness Filter Strategy. Lowquality generated latent samples, often unusually dark, are identified by measuring their similarity to completely black latent representation and filtered using simple threshold. E. Multi-Step Generation Results. Using the scheduler described in CMs [39], which first maps back to x0 and then adds noise to an intermediate timestep, our method can also generate images at different inference steps. Here, we provide results for 2-step  (Fig. 11)  and 4-step  (Fig. 12)  inference, while the 1-step results  (Fig. 2)  are presented in the main paper. 2 Figure 11. Two Step Generation Results. 3 Figure 12. Four Step Generation Results."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology"
    ]
}