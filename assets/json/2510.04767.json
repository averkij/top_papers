{
    "paper_title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs",
    "authors": [
        "Wonjun Kang",
        "Kevin Galim",
        "Seunghyuk Oh",
        "Minjae Lee",
        "Yuchen Zeng",
        "Shuibai Zhang",
        "Coleman Hooper",
        "Yuezhou Hu",
        "Hyung Il Koo",
        "Nam Ik Cho",
        "Kangwook Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 7 6 7 4 0 . 0 1 5 2 : r Preprint PARALLELBENCH: UNDERSTANDING THE TRADEOFFS OF PARALLEL DECODING IN DIFFUSION LLMS Wonjun Kang 1,5 Kevin Galim1 Seunghyuk Oh1 Minjae Lee1 Yuchen Zeng2,3 Shuibai Zhang2 Coleman Hooper4 Yuezhou Hu4 Hyung Il Koo1 Nam Ik Cho5 Kangwook Lee2,6 1 FuriosaAI 5 Seoul National University 2 UW-Madison 3 Microsoft Research 6 KRAFTON AI 4 UC Berkeley Project Page: https://parallelbench.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose PARALLELBENCH, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using PARALLELBENCH, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023) have achieved remarkable success in natural language processing, including complex reasoning (Jaech et al., 2024; Guo et al., 2025) and code generation (Jiang et al., 2024), which require extensive generation lengths. However, the autoregressive nature of most current LLMs fundamentally constrains inference speed, as they must generate tokens one-by-one. Unlike autoregressive LLMs, which follow (i) one-by-one decoding and (ii) left-to-right decoding, diffusion LLMs (dLLMs) enable (i) parallel decoding and (ii) any-order decoding. Recently, there has been rapidly growing interest in dLLMs for their potential to dramatically accelerate LLM inference via parallel decoding. While early diffusion language models suffered from performance gaps compared to autoregressive models, recent advances (Inception Labs et al., 2025; Song et al., 2025) have shown that dLLMs can achieve comparable generation quality to autoregressive LLMs, potentially emerging as next-generation LLMs. However, the conditional independence assumption (Wu et al., 2025) in dLLMs causes parallel decoding to fail to capture token dependencies, resulting in significant quality degradation. As Equal Contribution. Emails: {kangwj1995, kevin.galim, seunghyukoh}@furiosa.ai. 1 Preprint Figure 1: PARALLELBENCH. (left) Parallel decoding fails to capture token dependencies, risking incorrect combinations of individually likely tokens (e.g., New and City). (middle) Quantitative analysis of parallel decoding performance on list operations. (right) Based on our findings, we develop realistic benchmark to evaluate the speed-quality trade-off of parallel decoding methods. shown in Fig. 1, under parallel decoding, the model might generate New City instead of the correct New York or Mexico City because each token is unmasked independently without considering its mutual constraints. Existing works on dLLMs (Nie et al., 2025; Ye et al., 2025b) mainly explore their one-by-one decoding quality with limited analysis of parallel decoding challenges, despite parallel generation being their defining advantage. Moreover, current evaluations rely on standard benchmarks, such as math (Cobbe et al., 2021) and coding (Chen et al., 2021a), which may not adequately expose the quality degradation of parallel decoding in real-world scenarios. To address this gap, we first provide an information-theoretic analysis of parallel decoding in dLLMs, showing that even ideal models suffer increasing difficulty due to inherent token dependencies in the data distribution as the degree of parallelism increases. To further provide quantitative insights, we conduct case studies on analytically tractable synthetic list operations from two perspectives: (i) from the data distribution perspective, we quantitatively formulate the difficulty of parallel decoding through conditional total correlation, proving that certain list operations exhibit inevitable quality degradation under parallel decoding, and (ii) from the decoding order perspective, we derive the achievable accuracy of various unmasking strategies on these tasks, showing that accuracy varies significantly across different unmasking strategies. Building on this, we propose PARALLELBENCH to demonstrate that the quality degradation from parallel decoding extends beyond synthetic to real-world scenarios. This realistic benchmark encompasses tasks of varying difficulty under parallel decoding, including those that are exceptionally challenging for parallel decoding in dLLMs yet still trivial for humans and autoregressive LLMs. Using this benchmark, we conduct extensive evaluations of dLLMs and autoregressive LLMs, revealing two key findings. First, consistent with our synthetic results, dLLMs with parallel decoding exhibit severe quality degradation, even on trivial real-world tasks. Second, current parallel decoding strategies struggle to adaptively adjust the degree of parallelism based on task difficulty, resulting in suboptimal speed-quality trade-offs. Finally, we explore additional techniques to determine if they can improve the current trade-offs in parallel decoding. In summary, our main contributions are: We provide an information-theoretic analysis to formalize the quality degradation in parallel decoding arising from token dependencies and design tractable synthetic tasks to quantify its impact. We introduce PARALLELBENCH, the first realistic benchmark for evaluating trade-offs of parallel decoding in dLLMs, covering diverse difficulty levels to assess adaptive parallelism strategies. Through extensive experiments on PARALLELBENCH, we demonstrate that current dLLMs under parallel decoding suffer severe quality degradation on seemingly simple tasks, and existing decoding strategies struggle to adaptively balance speed and quality."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Diffusion-based Language Models D3PM (Austin et al., 2021) pioneered applying diffusion models to discrete data by modeling discrete diffusion processes through various transition matrices. 2 Preprint Diffusion-LM (Li et al., 2022) introduced non-autoregressive language model using continuous diffusion. Lou et al. (2024) introduced SEDD, which parameterizes discrete diffusion processes by learning concrete scores through novel score entropy loss, thereby achieving performance comparable to that of autoregressive models. Sahoo et al. (2024); Shi et al. (2024) showed that masked diffusion with absorbing state achieves stronger performance through simplified framework. Recent diffusion-based language models have achieved significant improvements in scale and performance, establishing practical dLLMs (Nie et al., 2025; Zhu et al., 2025; Ye et al., 2025b) with expanded multimodal capabilities (You et al., 2025; Yang et al., 2025b) and variants for complex reasoning (Zhao et al., 2025) and code generation (Gong et al., 2025; Inception Labs et al., 2025; Song et al., 2025). Analysis of Decoding Mechanisms in dLLMs Wu et al. (2025) discussed the quality degradation in dLLMs parallel decoding process due to the conditional independence assumption, proposing threshold-based and factor-based unmasking methods to mitigate this issue. However, they only focus on standard benchmarks like GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021a) rather than exploring settings where parallel decoding is particularly vulnerable. Feng et al. (2025) demonstrated that dLLMs parallel decoding is highly task-dependent: efficient for fluent text generation with constant steps, but ineffective for reasoning tasks requiring linearly scaling steps for sequence-level correctness. However, this work offers limited validation, relying on highly synthetic settings like n-grams and Hidden Markov Models. Gong et al. (2025) introduced AR-ness metrics to quantify the similarity between dLLM and autoregressive LLM decoding orders. They found that dLLMs exhibit any-order decoding, with AR-ness varying by dataset and correlating with quality degradation in parallel decoding. However, their analysis is limited to standard benchmarks, such as code and math. Ye et al. (2025a) demonstrated that dLLMs surpass autoregressive LLMs in complex reasoning and long-term planning tasks, such as Countdown and Sudoku. For broader overview of related work, see Section B."
        },
        {
            "title": "3 PRELIMINARIES: DIFFUSION LLM DECODING",
            "content": "Since most dLLMs (Nie et al., 2025; Ye et al., 2025b) employ masked diffusion (Sahoo et al., 2024), we assume masked diffusion throughout this paper. For more detailed discussion, see Section C. yiSt Parallel Decoding In parallel decoding, dLLMs generate tokens over timesteps with target sequence = S1 S2 ST . At timestep t1, the model generates token set St conditioned on input and previously generated tokens S<t = S1 . . . St1. The generation probability is factorized as Pθ(StX, S<t) = (cid:81) Pθ(yiX, S<t), assuming conditional independence among tokens in St. This conditional independence assumption enables parallel decoding within each timestep but introduces factorization errors from unmodeled token dependencies, creating fundamental speed-quality trade-off. These errors arise with strong semantic or syntactic dependencies, as shown in Fig. 1: dLLMs sample from factorized marginals Pθ(yiX) Pθ(yi+1X) instead of the true joint Pθ(yi, yi+1X), potentially generating invalid combinations like New City rather than New York or Mexico City. In the special case of one-step generation (T = 1), Huang et al. (2022) proved that the minimum achievable KL divergence between the factorized model Pθ(Y X) = (cid:81) yiY Pθ(yiX) and the true data distribution Pdata(Y X) is lower-bounded by the conditional total correlation C(Y X) = Hdata(Y X) + (cid:80) yiY Hdata(yiX): minθ DKL(Pdata(Y X) Pθ(Y X)) C(Y X) (1) C(Y X) quantifies the difficulty of parallel generation by measuring token dependencies, imposing fundamental limits that even optimally designed models cannot overcome. Any-order Decoding: Token Unmasking Methods dLLMs can decode in any order, where the unmasking strategy determines how to partition into disjoint sets S1 ST , critically impacting output quality. At each timestep, dLLMs make predictions for all masked token positions but only unmask and finalize subset of them, keeping the rest masked for subsequent timesteps. 1For intuitive understanding, we use notation = 0 to = where corresponds to the t-th parallel decoding step, rather than the typical reverse time notation in diffusion models. 3 Preprint Unmasking methods can be broadly categorized into two approaches: (i) Top-k (static): which unmasks fixed number of tokens in left-to-right order, randomly, or based on scoring metric (e.g., confidence (Nie et al., 2025), margin (Kim et al., 2025), and entropy (Ye et al., 2025b)), and (ii) Threshold (adaptive): which unmasks tokens whose scores exceed certain threshold; if no tokens meet the threshold, the single token with the highest score is unmasked. Furthermore, several works (Nie et al., 2025; Arriola et al., 2025) adopt semi-autoregressive (semi-AR) decoding, which divides sequences into fixed-size blocks decoded left-to-right, where tokens within each block are generated in parallel using any unmasking strategy once preceding blocks are complete."
        },
        {
            "title": "4 THEORETICAL ANALYSIS OF PARALLEL DECODING",
            "content": "minθ DKL(Pdata(Y X) Pθ(Y X)) LT In this section, we theoretically explore the specific quality degradation that occurs in the case of -step parallel decoding, and then provide quantitative analyses from both data distribution and decoding strategy perspectives, using analytically tractable synthetic list operations. Theorem 1 (Lower Bound for -step Parallel Decoding). For factorized generative model (e.g., dLLM) Pθ(Y X) performing T-step parallel decoding, assume the target sequence is partitioned into disjoint sets, = S1 S2 ST . At each step {1, . . . , }, the model generates the tokens in set Si in parallel, conditioned on and all previously generated tokens S<i = S1 S2 . . . Si1 (S<1 = ). The minimum achievable KL divergence for this model is lower-bounded by: (cid:0){Si}T where C(SiX, S<i) denotes the conditional total correlation of tokens in Si given and S<i. The equality holds when Pθ(yjX, S<i) = Pdata(yjX, S<i) for all {1, . . . , } and for all yj Si. Remark 1 (Boundary Cases). The lower bound LT captures the two boundary cases of the decoding spectrum: i) When = 1, L1 = C(Y X); ii) When = (i.e., one-by-one decoding with Si = {yi} for all i), for each we have C(SiX, S<i) = 0, thus LY = 0. Theorem 2 (Monotonicity of Error Bounds). Let over all possible T-step partitions {Si}T ically decreasing with respect to the number of generation steps: Remark 2. Since decreasing function from the total correlation C(Y X) to zero as increases from 1 to . LT ({Si}T Y = 0, the optimal error bound denote the optimal (minimum) error bound is monotoni=1). Then +1. forms monotonically ES<iPdata[C(SiX, S<i)] 1 = C(Y X) and := min{Si}T (cid:1) := (cid:80)T i=1: (2) i= i=1 i=1 See Section for proofs and Section A.3 for the extension of Theorem 2 to input-dependent partitions. Theorem 1 shows that -step parallel decoding in dLLMs inevitably incurs distribution error due to data distribution properties such as conditional total correlation (e.g., C(Y X)), even with ideal models. Theorem 2 further establishes that the lower bound on this distribution error increases monotonically as decreases. However, C(Y X) is intractable in real-world datasets, though Huang et al. (2022) approximated it with well-trained transformer. To address this gap, we use synthetic list operations with analytically tractable C(Y X). We examine four tasks on length-n input sequences (e.g., [A, B, C, D, E]): (i) Copy: copying the input; (ii) Replace Index: replacing an item at given index with F; (iii) Replace Random: replacing one random item with F; and (iv) Shuffle: rearranging the items. 4.1 CASE STUDY ON LIST OPERATIONS: DATA DISTRIBUTION PERSPECTIVE Copy & Replace Index While Replace Index seems harder than Copy, both tasks are equally simple from the perspective of C(Y X). For both tasks, each output token yi is uniquely determined by alone (C(Y X) = 0), allowing ideal models to decode in parallel without distribution error. Replace Random While Replace Index seems harder than Replace Random (requiring replacement at given index), from the perspective of C(Y X), the opposite holds. While Replace Index yields C(Y X) = 0, Replace Random exhibits token dependencies since replacing exactly one randomly selected item requires all others to remain unchanged, yielding: C(Y X) = (n 1)[log2(n) log2(n 1)], limn C(Y X) = log2(e) 1.44. (3) This indicates that the difficulty of parallel decoding remains bounded for arbitrarily long sequences. Preprint Shuffle Shuffle generates random permutation and exhibits stronger token dependencies than Replace Random, as once an item is placed at any position, it cannot appear elsewhere. This yields: C(Y X) = log2(n) log2(n!), limn C(Y X) = , (4) t=1 St log2(kt) log2(n!), where kt = (cid:80)t indicating that the difficulty of capturing permutation constraints grows without bound for arbitrarily long sequences in one-step generation. For -step parallel decoding, LT ({St}T t=1) = (cid:80)T j=1 Sj with k1 = n. When = n/2 (2 (n1)!! , limn Ln/2 = , where n!! is the double factorial. This tokens per step), Ln/2 = log2 indicates that the difficulty grows without bound even when decoding only 2 tokens in parallel. n!! 4.2 CASE STUDY ON LIST OPERATIONS: DECODING STRATEGY PERSPECTIVE Beyond the data distribution perspective, the practical generation quality of dLLMs critically depends on the decoding (unmasking) strategy. We analyze achievable accuracy under two categories: (i) Top-k (Random or Confidence): unmask random positions or highest-confidence positions per step; (ii) Threshold (Confidence): unmask all positions exceeding confidence threshold γ; if no positions meet γ, the single token with the highest confidence is unmasked. We focus on an unbiased (ideal) model where each tokens logit lj = lj,ideal + ϵj, with lj,ideal determined solely by the task and ϵj representing zero-mean stochastic noise, without intrinsic bias. We consider greedy decoding (τ = 0) and temperature sampling (τ = 1). Summary of our findings are in Table 1. 4.2.1 TOP-K (RANDOM OR CONFIDENCE) Shuffle Since at each step i, the probability of correctly selecting distinct items for the remaining positions is (n(i1)k,k) (n(i1)k)k , the overall accuracy of successfully completing the shuffle operation is: Acc(k) = n/k (cid:89) i=1 (n (i 1)k, k) (n (i 1)k)k = n! nn 0 (n1)!! n!! 0 1 1 if = if = 2 if = 1 as , (5) regardless of using greedy decoding or temperature sampling. Notably, for both = (one-step generation) and = 2 (two tokens per step), the accuracy converges to zero as increases. 1 Replace Random Under greedy decoding (assume for simplicity), at each step < , all positions keep their tokens since the keep probability n(i1)k1 exceeds the replace probability n(i1)k n(i1)k . At the final step, when > 2, all positions are kept since k1 > 1 , failing the task; when = 2, successful replacement occurs with probability 0.5 0.5 + 0.5 0.5 = 0.5 (sum of [keep, replace] and [replace, keep]). Thus Acc(k > 2) = 0 and Acc(k = 2) = 0.5. Under temperature sampling, each position initially keeps its original token with probability n1 . Thus, for one-step generation, we have Acc(n) = (cid:0) n1 , and limn Acc(n) = 1 . (cid:1)n 4.2.2 THRESHOLD (CONFIDENCE) Shuffle Assuming γ > 0.5, at each timestep, the confidence for each item at each position equals 1/m where is the number of remaining masked tokens. For 2, we have γ > 0.5 1/m, so the threshold is never met when multiple tokens remain masked. Thus, only single token is decoded at each timestep, converging to one-by-one decoding with guaranteed success: Acc(γ > 0.5) = 1. to retaining its original token and 1 Replace Random Under greedy decoding with > 2, each position at the first timestep assigns confidence of n1 to replacement, thus all positions greedily preserve their original tokens. When n1 > γ, all positions are unmasked simultaneously, resulting in one-step generation that fails to perform any replacements: Acc( n1 > γ) = 0. When γ n1 , all positions fail to exceed the threshold, so only one token is unmasked per timestep. This oneby-one decoding continues through subsequent timesteps until the final step, where only single token remains masked. At this final step, the model assigns confidence 1 to the correct replacement, ensuring successful completion: Acc(γ n1 ) = 1. Preprint Task / Analysis Table 1: Summary of findings in Section 4. Data Distribution Perspective (Section 4.1) Decoding Strategy Perspective (Section 4.2) limn C(Y X) 0 log2(e) 1.44 Acc. (Greedy, Top-k) Acc. (Greedy, Threshold) 1 0.5 if = 2; 0 if > 2 Eq. (5) 1 1 if γ (n 1)/n; else 0 1 if γ > 0.5 C(Y X) Copy & Replace Index Replace Random Shuffle 0 (n 1)[log2(n) log2(n 1)] log2(n) log2(n!) 4.2.3 EMPIRICAL VALIDATION (a) Shuffle (τ = 1) (c) Replace (τ = 1) (e) Thres. (n = 10) Figure 2: Empirical validation results on list operations: Shuffle (Figs. 2a and 2b) and Replace Random (Figs. 2c and 2d) with Random Top-k, and both tasks with Confidence Threshold (Fig. 2e). (d) Replace (τ = 0) (b) Shuffle (τ = 0) To validate our analysis, we fine-tune LLaDA 1.5 (Zhu et al., 2025) on each list operation task and perform infilling experiments with pre-filled formatting tokens, leaving only the list item positions to be generated. Figs. 2a and 2b validate Eq. (5): accuracy converges to 0 as increases for > 1, with = converging much faster than = 2. Fig. 2c confirms that one-step generation achieves 1/e accuracy under temperature sampling, while Fig. 2d shows that greedy decoding yields 0.5 accuracy for = 2 but 0 for = n. Fig. 2e confirms that Shuffle and Replace Random exhibit opposite trends with varying γ, indicating the difficulty of achieving high accuracy for both tasks."
        },
        {
            "title": "5 REALISTIC BENCHMARK: PARALLELBENCH",
            "content": "While we reveal limitations of parallel decoding in synthetic settings, we demonstrate that these issues persist in real-world scenarios by introducing PARALLELBENCH, realistic benchmark spanning diverse difficulty levels to evaluate dLLMs parallel decoding. It consists of 17 tasks across 3 categories: (i) Waiting Line (10 tasks), (ii) Text Writing (5 tasks), and (iii) Puzzles (2 tasks). Waiting Line Waiting Line extends synthetic list operations (e.g., Shuffle) to realistic customer service scenarios. Given queue of customers (e.g., [Susan Fox, Philip Gray, Maria Butler, Albert Sanchez]), the task performs queue management operations analogous to list operations. The number of customers serves as controllable parameter for adjusting benchmark difficulty. Text Writing To expose the quality degradation in parallel decoding, we introduce grammar (Morris, 2025) scores that impose stronger token-level dependencies, which traditional metrics such as ROUGE (Lin, 2004) cannot capture. We examine five tasks: (i) Summarization using SAMSum (Gliwa et al., 2019), (ii) Paraphrasing using Vladimir Vorobev (2023), and (iii-v) our proposed Words-to-Sentence Generation (W2S) with three difficulty levels. Both Summarization and Paraphrasing constrain outputs to rich input context, limiting token candidates and keeping C(Y X) small. For scenarios with greater token dependencies, we propose W2S, where the goal is to construct sentence using the given words, such as Construct single, coherent sentence using the words sand, home, play, and bottle. Since the output can be freely generated using minimal input context, the token dependencies become larger depending on what output is generated. We design three difficulty levels: easy (related words), medium (loosely connected words), and hard (unrelated words), with harder variants requiring more creative construction and exhibiting larger C(Y X). Puzzles Ye et al. (2025a;b) utilized Sudoku to demonstrate the superior planning capabilities of dLLMs over autoregressive LLMs. Crucially, every Sudoku puzzle, regardless of difficulty, has unique solution (C(Y X) = 0). While Latin Square (Keedwell & Denes, 2015) shares structural similarities with Sudoku, it has many valid solutions (C(Y X) > 0). We included both to examine how C(Y X) affects parallel decoding in structurally similar tasks. 6 Preprint"
        },
        {
            "title": "6 BENCHMARK RESULTS AND ANALYSIS ON PARALLELBENCH",
            "content": "Setup For autoregressive LLMs, we use Llama 3.1 8B, Llama 3.2 3B (Grattafiori et al., 2024), Qwen2.5 3B/7B (Yang et al., 2024), Qwen3 4B (Yang et al., 2025a), and Claude 3.5 Haiku (Anthropic, 2024). For dLLMs, we test LLaDA 8B (Nie et al., 2025), LLaDA 1.5 8B (Zhu et al., 2025), Dream 7B (Ye et al., 2025b), DiffuCoder (Gong et al., 2025), and closed-source Mercury (Inception Labs et al., 2025). We also evaluate KV caching (Wu et al., 2025). For unmasking methi) Top-k: Random, Confidence (Chang et al., 2022), Left-to-Right2, Margin (Kim ods, we test: et al., 2025), Entropy (Ye et al., 2025b); and ii) Threshold: Confidence (Wu et al., 2025) with γ {0.5, 0.6, . . . , 1.0}. We also test Factor-based (Wu et al., 2025), and semi-AR decoding (Nie et al., 2025). Further experimental details are in Section D. In this section, we primarily focus on LLaDA 1.5, representative open-source dLLM, and four unmasking methods with results shown in Fig. 3, while full analysis including additional models, unmasking methods, and the impact of KV caching is in Section E. When evaluating LLaDA 1.5 on Waiting Line, we use {3, 4, 5, 6}, ensuring task simplicity to focus on parallel decoding effects rather than model capacity limitations. (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Paraphrasing (g) W2S (easy) (h) W2S (hard) (i) Sudoku (j) Latin Square Figure 3: Benchmark results of LLaDA 1.5 (Zhu et al., 2025) on PARALLELBENCH: Waiting Line (Figs. 3a to 3e), Text Writing (Figs. 3f to 3h), and Puzzles (Figs. 3i and 3j). Comparison Across Tasks For Waiting Line, Copy maintains near-perfect accuracy regardless of parallelism degree (Fig. 3a). For Reverse (Fig. 3b), accuracy varies by unmasking method. We argue that these differences stem from insufficient model capacity rather than token dependency (C(Y X) = 0). Replace Index (Fig. 3c) and Replace Random (Fig. 3d) exhibit contrasting trends. Despite Replace Random achieving near-perfect one-by-one decoding accuracy compared to Replace Indexs below 50% accuracy, this advantage reverses under parallelization: Replace Index maintains stable accuracy with increasing parallelism, while Replace Random degrades rapidly across all unmasking methods. This reversal aligns with Section 4: C(Y X) = 0 for Replace Index but C(Y X) > 0 for Replace Random. Shuffle (Fig. 3e) shows the steepest accuracy degradation, dropping from near-perfect to zero more rapidly than Replace Random as parallelism increases. For Text Writing (Figs. 3f to 3h), all tasks achieve near-perfect grammar scores with one-by-one decoding, but degrade at increasingly steep rates under parallelization: Paraphrasing, W2S (easy), then W2S (hard). This aligns with expected conditional dependencies. Paraphrasing merely restructures existing content with rich context, limiting token candidates and keeping C(Y X) small. Conversely, W2S tasks generate complete sentences from sparse word constraints, expanding the candidate space where each position strongly influences others. W2S (hard) uses semantically unrelated words requiring more creative construction than W2S (easy)s naturally coherent word sets, resulting in larger C(Y X) and steeper degradation. For Puzzles (Figs. 3i and 3j), Latin Square outperforms the harder Sudoku under one-by-one decoding, but this advantage disappears with increased parallelism (Latin Square: C(Y X) > 0, Sudoku: C(Y X) = 0), leading to similar accuracy. Comparison Across Unmasking Methods Confidence Threshold outperforms Top-k at conservative thresholds through adaptive token selection, but suffers rapid degradation at aggressive thresh2Unmasks tokens at time from left to right. 7 Preprint olds due to unpredictable unmasking spikes, making careful tuning essential. When comparing among top-k methods, for Reverse and Replace Index, Confidence Top-k outperforms Random Top-k. This is because when C(Y X) = 0, model imperfection limits accuracy, making it beneficial to unmask tokens that the model is confident about first. For Replace Random and Shuffle (C(Y X) > 0), Random Top-k conversely outperforms Confidence Top-k. In conclusion, no universally superior unmasking method exists, with performance varying by task. Semi-AR Decoding Results Semi-AR decoding controls the left-to-right tendency in unmasking order by adjusting block length. Its effectiveness in parallel decoding depends on the dependency structure. With local dependencies (e.g., grammatical constraints in Text Writing), small blocks enforce left-to-right (local) decoding, causing quality degradation. With global dependencies (e.g., Waiting Lines distributed items), left-to-right decoding becomes beneficial by capturing formatting tokens around items. Fig. 5 shows different accuracy trends between Text Writing and Waiting Line across block sizes (Random Top-2), indicating the difficulty of optimizing both tasks simultaneously. (a) Mercury (c) Qwen2.5 3B Figure 4: Waiting Line results on Mercury (Inception Labs et al., 2025) and autoregressive LLMs. (e) Qwen2.5 7B (d) Qwen3 4B (b) Haiku 3. Performance of Mercury vs. Autoregressive LLMs Fig. 4 shows Waiting Line results with [5, 20], where Mercury and autoregressive LLMs show opposite accuracy trends on Reverse and Shuffle. As Reverse seems harder (requiring exact reversal) than Shuffle (accepting any permutation), autoregressive LLMs score higher on Shuffle. However, Mercury shows the opposite: it maintains near-perfect accuracy on Reverse but fails with Shuffle, with accuracy dropping sharply as increases, consistent with Section 4. This indicates that the closed-source Mercury struggles to adaptively adjust its parallelism to maintain quality on tasks with high token dependencies. Broad Coverage of PARALLELBENCH Fig. 6 demonstrates PARALLELBENCHs necessity and differentiation from existing benchmarks. The x-axis shows parallelism-induced quality degradation (rightward = greater), while the y-axis shows semi-AR block length effects (upward = better quality with left-to-right ordering). For Waiting Line, tasks with C(Y X) = 0 appear left of GSM8K, while those with C(Y X) > 0 progressively move rightward with increasing degradation, culminating in Shuffle at the far right. For Text Writing, lower dependency tasks (paraphrasing, summarization) appear left of Humaneval, while higher dependency W2S tasks appear right. This confirms our benchmark spans the full parallel decoding difficulty spectrum, uniquely enabling evaluation of adaptive methods ability to modulate parallelism according to difficulty. Details are in Section G.1. Figure 5: Semi-AR results on Waiting Line and Text Writing. Figure 6: Broad coverage of PARALLELBENCH. Figure 7: Speed-Quality Tradeoff with Oracle Performance. Speed-Quality Trade-off with Oracle Performance Fig. 7 shows the speed-quality trade-off curves for each unmasking method. Static unmasking (top-k) methods show dramatic accuracy 8 Preprint drops as tokens-per-step increase, while adaptive unmasking (threshold) achieves superior tradeoffs. This raises the question: Do existing adaptive unmasking methods already provide sufficient speedup-quality trade-offs? To answer this, we measure oracle performance: the best possible trade-off assuming access to the optimal threshold for each sample that yields the correct answer. The oracle achieves both the best accuracy and significant speedup over threshold methods with comparable accuracy, demonstrating that per-sample threshold adaptation alone yields significant improvements and suggests substantial room for future research. Details are in Section G.2. Takeaway. Static parallel decoding (e.g., top-k) can suffer severe quality degradation, and adaptive decoding strategies (e.g., threshold) still have significant room for improvement."
        },
        {
            "title": "7 EXPLORING ADDITIONAL TECHNIQUES",
            "content": "(a) Pretrained (c) Chain-of-Thought Figure 8: Waiting Line results using LLaDA 1.5 (Random Top-k) with various advanced techniques. (b) Fine-tuning (d) ReMDM (e) RCR We further explore whether benchmark performance can be improved when various advanced techniques for dLLMs are applied. Details and full results are in Section F. Fine-tuning We fine-tuned LLaDA 1.5 on Waiting Line to examine whether fine-tuning improves parallel decoding performance (Figs. 8a and 8b). One-by-one decoding achieves near 100% accuracy for all tasks after fine-tuning, including Replace Index, which improves from below 50% to nearly 100%. Notably, Replace Index maintains high accuracy even with parallel decoding, confirming that ideal models can accurately learn the target distribution under parallel generation when C(Y X) = 0. However, Replace Random and Shuffle still degrade with parallel decoding, supporting Section 4 that even an ideal model cannot resolve parallel decoding limitations for C(Y X) > 0. Chain-of-Thought Prompting (CoT) Figs. 8a and 8c show that CoT mitigates quality degradation under increased parallelism by generating intermediate reasoning steps that reduce inter-token dependencies in final answers. However, due to using 8 more output tokens, CoT cannot serve as fundamental solution for improving the speed-accuracy trade-off under parallel decoding. Remasking Samplers Most dLLMs use masked diffusion, where tokens unmasked in previous timesteps cannot be refined later. However, this prevents correcting inaccurate predictions from early timesteps, leading to quality degradation. Recent training-free samplers, such as ReMDM (Wang et al., 2025) and RCR (He et al., 2025), enable remasking for masked diffusion models. However, our tests on Waiting Line show no improvements, revealing limitations of training-free approaches. Discrete Diffusion with Uniform Transition Matrix Unlike masked diffusion, uniform diffusion enables iterative refinement of all tokens throughout the denoising process, potentially correcting errors from conditional independence assumptions in parallel decoding. We test this by fine-tuning masked and uniform diffusion models from SEDD (Lou et al., 2024) (see Section F.4)."
        },
        {
            "title": "8 CONCLUSION",
            "content": "This paper investigates the trade-offs of parallel decoding in dLLMs. We provide an informationtheoretic analysis and conduct case studies using synthetic list operations, examining both data distribution and decoding strategy perspectives to reveal quantitative insights into the limitations of parallel decoding. Building on these findings, we propose PARALLELBENCH, new benchmark 9 Preprint demonstrating that limitations observed in synthetic scenarios similarly manifest in real-world applications. Using our benchmark, we draw two key conclusions: (1) dLLMs with parallel decoding can suffer severe quality degradation in real-world scenarios, and (2) existing decoding strategies fail to adaptively adjust parallelism based on task difficulty for optimal speed-quality trade-offs. Limitations and Future Works First, while our benchmark comprises 3 realistic categories and 17 tasks, broader coverage of real-world scenarios would be beneficial. Second, we primarily analyzed short output sequences to focus on fundamental characteristics, and tasks requiring longer sequences may yield different results. Future work could extend our analysis to long sequence generation using our adjustable-length tasks. Finally, leveraging PARALLELBENCH to develop novel unmasking methods that address current parallel decoding limitations is an important goal."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "Kangwook Lee is supported by NSF Award DMS-2023239, NSF CAREER Award CCF-2339978, Amazon Research Award, and grant from FuriosaAI."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing computer use, new Claude 3.5 Sonnet, and Claude 3.5 Haiku, 2024. URL https://www.anthropic.com/news/3-5-models-and-computer-use. Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=tyEyYT267x. Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=h7-XixPCAL. Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:18771901, 2020. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, and Di He. Theoretical benefit and limitation of diffusion language model. arXiv preprint arXiv:2502.09622, 2025. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: humanannotated dialogue dataset for abstractive summarization. EMNLP-IJCNLP 2019, pp. 70, 2019. 10 Preprint Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus, 2019. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Google Deepmind. URL gemini-model-thinking-updates-march-2025/. Gemini 2025. https://blog.google/technology/google-deepmind/ Our most ai model, intelligent 2.5: Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Haoyu He, Katrin Renz, Yong Cao, and Andreas Geiger. Mdpo: Overcoming the training-inference divide of masked diffusion language models. arXiv preprint arXiv:2508.13148, 2025. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Fei Huang, Tianhua Tao, Hao Zhou, Lei Li, and Minlie Huang. On the learning of non-autoregressive transformers. In International conference on machine learning, pp. 93569376. PMLR, 2022. Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, and Tong Xiao. Pc-sampler: Position-aware calibration of decoding bias in masked diffusion models. arXiv preprint arXiv:2508.13021, 2025. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. Donald Keedwell and Jozsef Denes. Latin Squares and Their Applications: Latin Squares and Their Applications. Elsevier, 2015. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham M. Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=DjJmre5IkP. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusionlm improves controllable text generation. Advances in neural information processing systems, 35: 43284343, 2022. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W04-1013. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=CNicRIVIPA. 11 Preprint Omer Luxembourg, Haim Permuter, and Eliya Nachmani. Plan for speeddilated scheduling for masked diffusion language models. arXiv preprint arXiv:2506.19037, 2025. Jack Morris. language-tool-python. language-tool-python/, 2025. Version 2.9.4. https://pypi.org/project/ Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. OpenAI. Introducing GPT-4.1 in the API, 2025. URL https://openai.com/index/ gpt-4-1. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Subham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander Rush, Yair Schiff, Justin Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=L4uaAR4ArM. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and genIn The Thirty-eighth Annual Conference on Neueralized masked diffusion for discrete data. ral Information Processing Systems, 2024. URL https://openreview.net/forum?id= xcqSOfHt4g. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Maxim Kuznetsov Vladimir Vorobev. Chatgpt paraphrases dataset. 2023. Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025. Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, and Linfeng Zhang. Accelerating difarXiv preprint fusion large language models with slowfast: The three golden principles. arXiv:2506.10848, 2025. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025b. Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview. net/forum?id=NRYgUzSPZz. 12 Preprint Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025b. URL https://hkunlp.github.io/blog/2025/dream. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. Preprint"
        },
        {
            "title": "Appendix",
            "content": "A Mathematical Proofs A.1 Proof of Theorem 1 . A.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Extended Version of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . Further Discussion on Related Work B.1 Additional Unmasking Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Related Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discrete Diffusion Strategies Benchmark Dataset Specifications and Evaluation Details Additional Experimental Results E.1 Unmasking Strategies: Margin Top-k and Entropy Top-k . . . . . . . . . . . . . . E.2 Factor-based Unmasking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Impact of KV Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Semi-autoregressive Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Techniques F.1 Impact of Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Chain-of-Thought (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Remasking Samplers: RCR and ReMDM . . . . . . . . . . . . . . . . . . . . . . F.4 Discrete Diffusion with Uniform Transition Matrix . . . . . . . . . . . . . . . . . Task Characterization G.1 Characterizing Tasks by Parallelizability and Decoding Order . . . . . . . . . . . . G.2 Speed vs. Quality Trade-off Analysis . . . . . . . . . . . . . . . . . . . . . . . . . Failure Cases Comprehensive Experimental Results I.1 Comparison with Large Language Models (LLMs) . . . . . . . . . . . . . . . . . I.2 Complete Benchmark Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 16 17 17 18 18 21 21 22 23 24 24 24 25 26 26 27 27 28 30 14 Preprint"
        },
        {
            "title": "A MATHEMATICAL PROOFS",
            "content": "A.1 PROOF OF THEOREM 1 Proof. The -step parallel decoding model is defined by the factorization: Pθ(Y X) = (cid:89) i=1 Pθ(SiX, S<i) Similarly, we can decompose the true data distribution according to the partition {Si}T i=1: Pdata(Y X) = (cid:89) i=1 Pdata(SiX, S<i) Substituting these decompositions into the KL divergence formula, we get: DKL(Pdata(Y X) Pθ(Y X)) = EPdata (cid:20) log (cid:34) = EPdata log (cid:21) Pdata(Y X) Pθ(Y X) (cid:81)T (cid:35) i=1 Pdata(SiX, S<i) (cid:81)T i=1 Pθ(SiX, S<i) (cid:21) Pdata(SiX, S<i) Pθ(SiX, S<i) log (cid:20) EPdata = = (cid:88) i=1 (cid:88) i=1 ES<iPdata [DKL(Pdata(SiX, S<i) Pθ(SiX, S<i))] (6) (7) (8) (9) (10) (11) At each step i, the model assumes conditional independence among the tokens in Si given the context (X, S<i), such that Pθ(SiX, S<i) = (cid:81) Pθ(yjX, S<i). Therefore, by applying Eq. (1) from Section 3, we have: yj Si DKL(Pdata(SiX, S<i) Pθ(SiX, S<i)) C(SiX, S<i) (12) By substituting this lower bound back into our sum, we have: minθDKL(Pdata(Y X) Pθ(Y X)) = minθ (cid:88) i=1 ES<iPdata [DKL(Pdata(SiX, S<i) Pθ(SiX, S<i))] (cid:88) i=1 (cid:88) i=1 ES<iPdata [minθi DKL(Pdata(SiX, S<i) Pθ(SiX, S<i))] ES<iPdata[C(SiX, S<i)] (13) (14) (15) This completes the proof. A.2 PROOF OF THEOREM 2 Lemma 1 (Subadditivity of Conditional Total Correlation). Let set of random variables be partitioned into two disjoint subsets and (i.e., = B). For any conditioning context Z, the conditional total correlation of is lower-bounded as follows: C(SZ) C(AZ) + EAPdata(AZ)[C(BZ, A)] (16) 15 Preprint Proof of Lemma 1. Using the definition of conditional total correlation (C(SZ) = (cid:80) H(SZ)), we have: yS H(yZ) C(AZ) + EA[C(BZ, A)] = (cid:88) yaA (cid:88) yaA H(yaZ) H(AZ) + H(yaZ) H(AZ) + (cid:88) ybB (cid:88) ybB H(ybZ, A) H(BZ, A) (17) H(ybZ) H(BZ, A) (since H(ybZ) H(ybZ, A)) = (cid:88) yaA H(yaZ) + (cid:88) ybB H(ybZ) (H(AZ) + H(BZ, A)) (cid:88) = H(yZ) H(A, BZ) (by the chain rule of entropy) yS = C(SZ) This completes the proof. (18) (19) (20) (21) Proof of Theorem 2. Let {S bound partition, is {S }T . We construct (T + 1)-step partition {S i=1 be an optimal -step partition that achieves the minimum error j=1 by splitting the last set of the optimal = B). The new partition , into two non-empty, disjoint subsets and (i.e., 1, A, B}, and we have: 1 , . . . , j}T + }T i=1) = LT ({S 1 (cid:88) = E[C(S X, <i)] + E[C(S X, <T )] i=1 1 (cid:88) i=1 E[C(S X, <i)] + E[C(AX, <T )] + E[C(BX, <T , A)] (by Theorem 1) j}T +1 j=1 ) LT +1({Sk}T +1 k=1 ) = LT +1({S min{Sk}T +1 = k=1 + (22) (23) (by Lemma 1) (24) (25) (26) (27) Thus, we have L +1. A.3 EXTENDED VERSION OF THEOREM 2 Theorem 3 (Monotonicity of Error Bounds for Input-Dependent Partitions). For any given input X, let LT ({Si}T i=1X) be the error bound for specific T-step partition {Si}T i=1: LT ({Si}T i=1X) := (cid:80)T i=1 ES<iPdata(X)[C(SiX, S<i)] (28) The optimal T-step error bound, possible partitions for each input: , is the expected value of the minimum error bound over all := EXPdata (cid:104) min{Si}T i=1 LT ({Si}T i=1X) (cid:105) Then is monotonically decreasing with respect to the number of generation steps: T +1 (29) Proof of Theorem 3. We begin by establishing pointwise inequality for any single, arbitrary input x. Let us consider special data distribution concentrated entirely on this single input x. 16 Preprint Applying the conclusion of Theorem 2 to this special case directly establishes the following relationship. The optimal T-step error for this distribution (which is simply the optimal error for the input x) is greater than or equal to the optimal (T+1)-step error: min{Si}T i=1 LT ({Si}T i=1x) min{S }T + j=1 LT +1({S j}T +1 j=1 x) (30) Taking the expectation of this pointwise inequality Eq. (30) over the true data distribution Pdata directly yields the final result: := EX (cid:104) min{Si}T i=1 LT ({Si}T i=1X) (cid:105) EX (cid:104) min{S }T +1 j=1 LT +1({S j}T + j=1 X) (cid:105) =: +1 Thus, we have L +1."
        },
        {
            "title": "B FURTHER DISCUSSION ON RELATED WORK",
            "content": "B.1 ADDITIONAL UNMASKING METHODS Recent work on Masked Diffusion Models (MDMs) has focused on optimizing the unmasking process to improve inference efficiency and generation quality. Besides the unmasking methods studied in this work, several additional methods target acceleration through parallel decoding. For instance, the EB-Sampler (Ben-Hamu et al., 2025) adaptively unmasks the largest group of tokens whose cumulative entropy falls below threshold, γ, achieving 23 speedup. Similarly, the Dilated Unmasking Scheduler (DUS) (Luxembourg et al., 2025) uses deterministic, coarse-to-fine schedule to unmask non-adjacent tokens, reducing the number of denoiser calls within block of size from O(B) to O(log B). The SlowFast Sampling (Wei et al., 2025) alternates between slow, exploratory phases and fast, parallel decoding phases, yielding up to 15.6 speedup. Other works focus on generation quality. The PC-Sampler (Huang et al., 2025), for example, corrects for biases in uncertainty-based sampling by using composite score that combines position-aware weight with frequency-calibrated confidence score, improving performance by over 10%. B.2 RELATED BENCHMARKS To contextualize our work, we review several key benchmarks commonly used to evaluate the capabilities of LLMs across different domains. GSM8K The Grade School Math 8K (GSM8K) dataset (Cobbe et al., 2021) is prominent benchmark designed to assess the multi-step mathematical reasoning abilities of LLMs. It consists of thousands of grade-school-level word problems that require sequence of logical steps and arithmetic operations to solve. The primary evaluation metric is final answer accuracy, where the models generated solution must exactly match the correct numerical result. Success on GSM8K is considered strong indicator of models capacity for complex, chain-of-thought reasoning. HumanEval The HumanEval benchmark (Chen et al., 2021a) is standard for evaluating the code generation capabilities of models. The task involves completing Python function bodies based on given function signature and descriptive docstring. The dataset contains 164 such programming challenges of varying difficulty. Performance is measured by functional correctness, typically using the pass@k (Chen et al., 2021b) metric. SAMSum The SAMSum dataset (Gliwa et al., 2019) is tailored for the task of abstractive summarization of natural, real-world dialogues. It contains short, informal conversations, requiring model to produce concise summary that captures the main points of the exchange. The standard evaluation protocol relies on the ROUGE score (Lin, 2004), which measures the n-gram overlap between the model-generated summary and one or more human-written reference summaries. While ROUGE is effective for assessing content overlap, our work diverges by focusing on the grammatical correctness and fluency of the generated summaries, aspects of quality not fully captured by lexical metrics. 17 Preprint ChatGPT-Paraphrases The ChatGPT-Paraphrases dataset (Vladimir Vorobev, 2023) is designed for evaluating models ability to rephrase sentences while preserving their original semantic meaning. The dataset provides pairs of original sentences and their corresponding paraphrases. This benchmark is crucial for assessing models fine-grained understanding of language and its capacity for fluent and diverse text generation, which are core skills for sophisticated natural language processing applications. While standard evaluations for this task often rely on semantic similarity metrics, our approach is different. Similar to SAMSum, we are primarily interested in the grammatical evaluation of the output."
        },
        {
            "title": "C DISCRETE DIFFUSION STRATEGIES",
            "content": "In discrete diffusion, common strategy is the absorbing state approach, also known as masked diffusion. This method corrupts data by progressively replacing original tokens with single, special [MASK] token. The key advantage of this technique lies in its reverse process. The models task is simplified to filling in the blanks at known [MASK] locations, which provides very direct and clear learning signal. This is the primary approach used by many prominent open-source models, such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025b). In contrast, the uniform transition approach corrupts data differently. Instead of using special token, it substitutes an existing token with any other token from the vocabulary, with each possibility having an equal probability of occurrence. This method gradually transforms the input into sequence of purely random tokens. Consequently, the reverse process is more complex: the model must predict the original token from noisy, corrupted one, rather than from designated blank slate. Some recent models, including SEDD (Lou et al., 2024), have explored this uniform approach."
        },
        {
            "title": "D BENCHMARK DATASET SPECIFICATIONS AND EVALUATION DETAILS",
            "content": "Table 2: Benchmark Tasks and Evaluation Metrics of PARALLELBENCH Category Task #Samples Metric Waiting Line (one-shot) Copy Sort Reverse Shuffle Replace Index Replace Random Insert Index Insert Random Remove Index Remove Random 100 Accuracy 100 Accuracy 100 Accuracy 100 Accuracy 100 Accuracy 100 Accuracy 100 Accuracy 100 Accuracy 100 Accuracy 100 Accuracy Text Writing (zero-shot) Summarization Paraphrasing Words-to-Sentence (easy) Words-to-Sentence (medium) Words-to-Sentence (hard) 100 Grammar, Rouge-L 100 Grammar, BERTScore, (1 - BLEU) 100 Grammar, Accuracy 100 Grammar, Accuracy 100 Grammar, Accuracy Puzzle (one-shot) Latin Square (4x4) Sudoku (4x4) 100 Accuracy 108 Accuracy Table 2 provides comprehensive overview of PARALLELBENCH. We omit the synthetic list operations from this discussion, as their setup is analogous to the Waiting Line tasks. Example prompts and corresponding answers for each task are presented in Table 3. Evaluation Details For the Waiting Line and Puzzle tasks, which are relatively unfamiliar to the models, we employ one-shot prompting. In this setting, the model is first presented with complete example (a prompt and its corresponding solution) before being given the actual test prompt. In contrast, for Text Writing tasks, we use standard zero-shot prompting. 18 Preprint All models are evaluated using greedy decoding, with the exception of infilling tasks, for which we use temperature sampling with temperature of 1.0. The maximum number of generated tokens is set to 32 for Waiting Line and 64 for both Puzzle and Text Writing. Evaluation Metrics For the Waiting Line and Puzzle tasks, we use accuracy, defined as binary score indicating whether the generated output exactly matches one of the valid solutions. For the Text Writing benchmarks, we employ several scores consisting of base grammar score and task-specific metrics. The grammar score, evaluated using the tool from Morris (2025), is 1 for grammatically correct outputs and 0 otherwise. The task-specific metrics are as follows: For Summarization, we report ROUGE-L (Lin, 2004) against reference summary. For Paraphrasing, we measure both semantic similarity using BERTScore (Zhang et al., 2019) and lexical diversity using 1 BLEU (Papineni et al., 2002) to penalize outputs that are too close to the original sentence. For Words-to-Sentence, we use an inclusion accuracy metric that verifies if all requested words are present in the generated sentence. For simplicity, we primarily report the grammar score in the main body of the paper. comprehensive breakdown of all metrics is available in Section I.1. Dataset Generation The tasks for our benchmark were sourced and generated as follows. For Waiting Line, we created lists by randomly sampling from predefined list of first and last names, which were generated with Gemini 2.5 Pro (Google Deepmind, 2025). For Latin Square, we generated prompts requesting the creation of Latin squares of size 4x4 with random symbols. For Sudoku, we used the existing dataset from Ye et al. (2025b). For Summarization and Paraphrasing, we utilized 100 examples of the existing SAMSum (Gliwa et al., 2019) and Vladimir Vorobev (2023) benchmarks, respectively. Finally, for Words-to-Sentence, we used Gemini 2.5 Pro to generate word sets of four words at three difficulty levels: easy (simple, semantically aligned words), medium (more complex but aligned words), and hard (complex words that are challenging to connect meaningfully). Table 3: Benchmark Prompts and Examples. Bold indicates the input part of the prompt. Task Copy Sort Reverse Shuffle Prompt You are managing waiting line at customer service desk. You need to record the following people in the order they arrived: [Billy Ramos, Alan Wells, Grace Wright] Please copy the list exactly and provide only the final list. You are managing waiting line at customer service desk. The following people should be organized alphabetically by last name for efficient processing: [David Lewis, Patrick Tran, Sean Diaz] Please sort the list in alphabetical order and provide only the final list. You are managing waiting line at customer service desk. The previous staff member put the waiting line in the wrong order. Please reverse the order of the following people in the waiting line to correct it: [Mark Richardson, Kevin Martinez, Henry Young] Please reverse the order of the list and provide only the final list. You are managing waiting line at customer service desk. The waiting line should be randomly shuffled to ensure fair service distribution: [Paul Payne, Robert Riley, Peter Stone] Please randomly shuffle the list and provide only the final list. Ensure the sequence is different from the original. Example Answer [Billy Ramos, Alan Wells, Grace Wright] [David Lewis, Patrick Tran, Sean Diaz] [Henry Young, Kevin Martinez, Mark Richardson] [Robert Riley, Paul Payne, Peter Stone] 19 Preprint Task Prompt You are managing waiting line at customer service desk. The person at position 0 must be replaced with Henry Warren: [Patrick Morgan, Eric King, Joe Reed] Please replace the person at the specified position with Henry Warren and provide only the final list. You are managing waiting line at customer service desk. One person in the waiting line must be replaced with Juan Torres: [David Owens, Kelly Payne, Aaron Freeman] Please replace one random person with Juan Torres and provide only the final list. You are managing waiting line at customer service desk. new person Aaron Stewart is inserted into the line at position 2: [Charlotte Chavez, Grace Baker, Keith Cooper] Please put the new person at the specified position and provide only the final list. You are managing waiting line at customer service desk. new person Justin McDonald is inserted into the line at random position: [Johnny Sullivan, Ryan Baker, Juan Wilson] Please put the new person in the random position and provide only the final list. You are managing waiting line at customer service desk. The person at position 2 has left the waiting line: [Sarah Robertson, Maria Mitchell, Donald Hughes] Please remove the person at the specified position and provide only the final list. You are managing waiting line at customer service desk. One person has left the waiting line: [Karen Kim, Jerry Hall, Jose Marshall] Please remove random person and provide only the final list. Summarize the following conversation. Only output the final result. Steve: Bought the new Dream Theater album 5 minutes ago. hope its good. Rob: have it here on my desk, ready for the first listening. Steve: Ok, Ill tell you later what think about it. Rob: Same here. See you later! Summary: Paraphrase the following sentence. Only output the final result. Sentence: Any idea of what sweater this is? Paraphrase: Replace Index Replace Random Insert Index Insert Random Remove Index Remove Random Summarization Paraphrasing Words-to-Sentence (easy) Words-to-Sentence (medium) Words-to-Sentence (hard) Construct single, coherent sentence using the words algorithm, river, symphony, and moss. 20 Example Answer [Henry Warren, Eric King, Joe Reed] [David Owens, Kelly Payne, Juan Torres] [Charlotte Chavez, Grace Baker, Aaron Stewart, Keith Cooper] [Justin McDonald, Johnny Sullivan, Ryan Baker, Juan Wilson] [Sarah Robertson, Maria Mitchell] [Jerry Hall, Jose Marshall] Steve and Rob will talk about new Dream Theater album, after they finish listening. Can you identify this sweater? was so happy to sit at the table by the river and eat crisp apple. The growth of the moss along the river bank seemed to follow natural algorithm, quiet symphony of life unfolding. Construct single, coherent sentence using the words dog, park, ball, and throw. love to throw the ball for my dog at the park. Construct single, coherent sentence using the words apple, river, table, and happy. Preprint Task Prompt Latin Square (4x4) Sudoku (4x4) Generate Latin square of size 3 with the symbols [H, 4, C, A]. Only output the final result as CSV. Fill the positions where the values are 0 in 4x4 grid with digits 1-4 so that each column, each row, and each of the four 2x2 subgrids that compose the grid contains all of the digits from 1 to 4. Input: 0042 2031 4023 3214 Output: Example Answer H,4,A,C 4,C,H,A C,A,4,H A,H,C, 1342 2431"
        },
        {
            "title": "E ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "E.1 UNMASKING STRATEGIES: MARGIN TOP-K AND ENTROPY TOP-K (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort Figure 9: Full Waiting Line (n = 3, 4, 5, 6) results using LLaDA 1.5 (Zhu et al., 2025) with Margin Top-k and Entropy Top-k. (a) Paraphrasing (b) Summarization (c) W2S (easy) (d) W2S (medium) (e) W2S (hard) Figure 10: Full Text Writing results using LLaDA 1.5 (Zhu et al., 2025) with Margin Top-k and Entropy Top-k. Recently, two alternatives to confidence-based unmasking have been proposed: Margin Top-k (Kim et al., 2025) and Entropy Top-k (Ye et al., 2025b). These methods function similarly by iteratively revealing tokens but employ different metrics to quantify model uncertainty. Margin Top-k calculates the score as the difference between the probabilities of the two most likely token candidates, p1 and p2. The margin, = p1 p2, is large when the model is highly confident in single token, and small when there is ambiguity between the top candidates. Entropy Top-k, in contrast, uses the Shannon entropy of the entire output probability distribution to measure uncertainty. low entropy value signifies peaked, high-confidence distribution, whereas high entropy value indicates more uniform and uncertain distribution. We compare both approaches against Confidence Top-k in Fig. 9 and Fig. 10 to evaluate their relative performance. In our experiments, Margin Top-k and Entropy Top-k yield perform similarly to Confidence Top-k. 21 Preprint E.2 FACTOR-BASED UNMASKING (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort Figure 11: Full Waiting Line (n = 3, 4, 5, 6) results using LLaDA 1.5 (Zhu et al., 2025) with Factorbased unmasking. (a) Paraphrasing (b) Summarization (c) W2S (easy) (d) W2S (medium) (e) W2S (hard) Figure 12: Full Text Writing results using LLaDA 1.5 (Zhu et al., 2025) with Factor-based unmasking. Factor-based unmasking (Wu et al., 2025) is an adaptive method for determining how many tokens to decode in parallel during inference. After calculating the confidence scores for potential next tokens, the scores are sorted in descending order. The strategy then selects the largest number of tokens, n, that satisfies the condition: (n + 1)(1 c(n)) < Here, c(n) represents the confidence of the n-th token in the sorted list, and is predefined hyperparameter. In contrast to Threshold-based unmasking, which accepts any token whose individual confidence exceeds fixed value τ , Factor-based is dynamic. It evaluates tokens as group, allowing the degree of parallelism to increase when the model is highly confident and decrease when it is not. In Fig. 11 and Fig. 12 we compare Factor-based unmasking with Threshold-based unmasking with the following values for {0.7, 1.0, 1.3, 1.6, 1.9}. The results show that Factor-based unmasking follows similar trend compared to Threshold-based unmasking. However, it has narrower range of the number of tokens per step in general. E. IMPACT OF KV CACHING Unlike autoregressive LLMs, dLLMs cannot effectively utilize KV cache, causing significant latency at each timestep. Recent training-free KV cache methods (Wu et al., 2025) claim dramatic speedups with minimal accuracy loss on benchmarks like GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021a). In Fig. 13 we evaluate whether PrefixCache from Fast-dLLM (Wu et al., 2025), maintains effectiveness on PARALLELBENCH under parallel decoding. The results show that caching decreased accuracy in general. Nevertheless it maintains similar trend to no caching where parallel decoding decreases performance. Complete results for PrefixCache are in Fig. 25. 22 Preprint (a) Replace Index (b) Replace Random (c) Shuffle Figure 13: Fast-dLLM results on Replace Index, Replace Random, and Shuffle using PrefixCache. E.4 SEMI-AUTOREGRESSIVE DECODING Semi-autoregressive decoding partitions the input sequence into blocks of equal length. These blocks are processed sequentially from left to right, while the tokens within each individual block are decoded in parallel. In the edge case where the block length is one, this process becomes equivalent to standard autoregressive decoding, generating single token at time. We investigate the impact of varying block lengths on performance for several unmasking methods, fixing the number of tokens decoded in parallel to = 2 (Figs. 14 and 15). In Text Writing tasks, performance improves as block length increases. This suggests that global dependencies are crucial for grammatical correctness, whereas smaller blocks often lead to errors and lower scores. Conversely, the Waiting Line tasks show an opposite trend. Here, left-to-right (decoding length 2) enhances performance by leveraging local dependencies to better match formatting tokens with the surrounding content. (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort Figure 14: Full Waiting Line (n = 3, 4, 5, 6) results using LLaDA 1.5 (Zhu et al., 2025) with semiautoregressive decoding (k = 2). (a) Paraphrasing (b) Summarization (c) W2S (easy) (d) W2S (medium) (e) W2S (hard) Figure 15: Full Text Writing results using LLaDA 1.5 (Zhu et al., 2025) with semi-autoregressive decoding (k = 2). 23 Preprint"
        },
        {
            "title": "F ADDITIONAL TECHNIQUES",
            "content": "F.1 IMPACT OF FINE-TUNING (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort Figure 16: Full Waiting Line (n = 3, 4, 5, 6) results using fine-tuned LLaDA 1.5 (Zhu et al., 2025). We fine-tuned the LLaDA 1.5 model on each task separately. For each task, we generated distinct training set of 20,000 examples and validation set of 5,000 examples. The model was trained for 10 epochs using the AdamW optimizer with batch size of 32, learning rate of 1 105, warmup rate of 0.05, and cosine scheduler. We employed LoRA (Hu et al., 2022) for efficient training, configured with = 128, α = 256, and applied it exclusively to query, key, and value projection layers, with dropout rate of 0.05. All experiments were conducted on one A100 GPU, taking approximately two hours to complete. As detailed in Fig. 16, fine-tuning improved overall performance but was insufficient to overcome tasks inherently challenging for parallel decoding, such as Shuffle and Insert/Remove/Replace Random. F.2 CHAIN-OF-THOUGHT (COT) PROMPTING (a) Replace Index (b) Replace Random (c) Shuffle Figure 17: One-shot CoT results on Replace Index, Replace Random, and Shuffle. The one-shot examples for each sample are generated by GPT-4.1 Mini (OpenAI, 2025). To investigate the impact of explicit reasoning on LLaDA 1.5s performance on the Waiting Line tasks, we tested Chain-of-Thought (CoT) prompting. The one-shot examples were generated using OpenAIs GPT-4.1-mini-2025-04-14 API (OpenAI, 2025). The output token limit was increased from 32 to 256 to accommodate the verbose, multi-step nature of the CoT outputs. Our prompt was structured to include the trigger phrase, Think step by step and give your final answer at the end as The answer is: {answer}. after the users question. The full prompt template is shown in Table 4. As shown in Fig. 17, Chain-of-Thought (CoT) improves performance, especially for parallel decoding, but at the cost of higher token count. 24 Preprint Table 4: An example of an one-shot example for Shuffle (CoT) generated with GPT-4.1 Mini (OpenAI, 2025). Question You are managing waiting line at customer service desk. The waiting line should be randomly shuffled to ensure fair service distribution: [Thomas Holmes, Keith Ramos, Victoria Collins, Roger Hughes, Austin Dunn]. Please randomly shuffle the list. Ensure the sequence is different from the original. Think step by step and give your final answer at the end as The answer is: {answer}. Response Step 1: Identify the original list: [Thomas Holmes, Keith Ramos, Victoria Collins, Roger Hughes, Austin Dunn] Step 2: Randomly shuffle the list ensuring the new sequence is different from the original. One possible shuffled sequence could be: [Austin Dunn, Victoria Collins, Roger Hughes, Thomas Holmes, Keith Ramos] Step 3: Verify the shuffled list is different from the original: Original first element: Thomas Holmes Shuffled first element: Austin Dunn They are different, so the shuffle is valid. The answer is: [Austin Dunn, Victoria Collins, Roger Hughes, Thomas Holmes, Keith Ramos] F.3 REMASKING SAMPLERS: RCR AND REMDM (a) Replace Index (b) Replace Random (c) Shuffle Figure 18: RCR (He et al., 2025) results on Replace Index, Replace Random, and Shuffle. (a) Replace Index (b) Replace Random (c) Shuffle Figure 19: ReMDM (Wang et al., 2025) results on Replace Index, Replace Random, and Shuffle. We evaluated two training-free decoding strategies that allow for token revision: Running Confidence Remasking (RCR) (He et al., 2025) and Remasking Diffusion Model (ReMDM) (Wang et al., 2025), using their official codebases. RCR works by tracking the running maximum confidence for each position over time and continuously remasking tokens that remain uncertain. In contrast, ReMDM performs revision steps when most of the output is generated, targeting the tokens with the lowest confidence. For our experiments with ReMDM, we set = 1 for simplicity and tested 25 Preprint various values for n. We found that single revision step (n = 1) achieved optimal performance, with no further improvements observed from additional steps. In our experiments (Figs. 18 and 19), we notice no significant improvements from RCR or ReMDM. F.4 DISCRETE DIFFUSION WITH UNIFORM TRANSITION MATRIX This section evaluates two variants of the 90-million-parameter SEDD model (Lou et al., 2024): absorb and uniform. For the absorb variant, we employed the publicly available pre-trained model3. For the uniform variant, we pre-trained the model from scratch on the OpenWebText dataset (Gokaslan et al., 2019). The pre-training process required 24 hours on eight A100 GPUs. Subsequently, both variants were fine-tuned on each task. We trained for 32 epochs using the AdamW optimizer with batch size of 64, learning rate of 3 104, and cosine scheduler with 0.25 warmup ratio. Each fine-tuning run was conducted on single A100 GPU and completed in approximately one hour. (a) Replace Index (b) Replace Random (c) Shuffle Figure 20: SEDD (Lou et al., 2024) results on Replace Index, Replace Random, and Shuffle."
        },
        {
            "title": "G TASK CHARACTERIZATION",
            "content": "G.1 CHARACTERIZING TASKS BY PARALLELIZABILITY AND DECODING ORDER We analyze how the unmasking order and the degree of parallel token decoding influence downstream scores for each task in our benchmark  (Fig. 6)  . We plot the tasks in 2D scatter plot where the x-axis represents parallelizability and the y-axis represents the benefit from different decoding orders. Tasks on the right are harder to parallelize than those on the left. Tasks on the top benefit from semi-autoregressive order, while those on the bottom perform better with any-order decoding. The results are averaged over the LLaDA 1.5 and Dream 7B models and across four unmasking strategies: Random Top-k, Confidence Top-k, Margin Top-k, and Entropy Top-k. (cid:80) k(kscorek) (cid:80) scorek To define the x-axis (parallelizability), we evaluate the model with an increasing number of parallel tokens, k, while using full any-order decoding scheme. We then compute metric representing the point at which performance begins to drop. This metric is the center of mass of the scores, . smaller value on the x-axis therefore indicates that the task is harder to calculated as parallelize. We observe that the Shuffle task is the most difficult to parallelize. Words-to-Sentence is also challenging, as the model must successfully connect words and insert appropriate connecting language while maintaining grammatical correctness. Interestingly, the Latin Square task is harder to parallelize than Sudoku, potentially because multiple valid solutions exist, forcing the model to commit to one solution during generation. Similarly, Insert Random proves harder to parallelize than Insert Index. To define the y-axis (influence of decoding order), we analyze how semi-autoregressive decoding affects performance. We fix the number of parallel tokens to = 2 and evaluate the downstream metric for various block lengths. We then fit line to these scores and use its slope as our metric. positive slope indicates that performance increases as the block length decreases (enforcing 3https://github.com/louaaron/Score-Entropy-Discrete-Diffusion 26 Preprint more autoregressive process), while negative slope indicates that larger blocks or fully any-order decoding is preferable. Our results show that most tasks have negative value, suggesting that the models generally prefer any-order decoding. Text generation tasks, in particular, fall into this category. We also included two existing benchmarks, GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021a), in our analysis. Both tasks appear on the left side of the plot, indicating that models can solve them in more parallel fashion. This finding underlines the need for more challenging benchmarks like the proposed PARALLELBENCH to properly test the capabilities of dLLMs. G.2 SPEED VS. QUALITY TRADE-OFF ANALYSIS Figure 7 illustrates the trade-off between the number of tokens decoded in parallel and the resulting downstream accuracy for each unmasking method. For our experiments, we utilized the LLaDA 1.5 model (Zhu et al., 2025). The accuracy is averaged over 10 benchmark tasks, which include five Waiting Line tasks (Copy, Reverse, Shuffle, Replace Index, and Replace Random) and all Text Editing and Puzzle tasks. The oracle performance, marked by star in the plot, represents an empirical upper bound. To compute this, we employ the Confidence Threshold method. For each sample, we identify the minimum threshold (equivalent to the minimum number of total performed steps) that achieves the best possible accuracy. We then average the accuracy and number of steps of these optimal thresholds across all samples and tasks. This simulates hypothetical unmasking strategy that knows in advance the precise minimum effort required to solve each task perfectly. The results indicate that for Top-k unmasking methods, accuracy degrades rapidly as the number of tokens per step increases. Confidence Threshold, on the other hand, exhibits much more favorable curve, where the drop-off in accuracy occurs at significantly higher number of parallel tokens. Nevertheless, substantial gap remains when compared to the oracle. While all methods perform comparably when unmasking single token per step, the oracle maintains high accuracy at far greater decoding parallelism. This suggests that, in theory, unmasking could be performed much faster without sacrificing accuracy if more effective sampling method were available."
        },
        {
            "title": "H FAILURE CASES",
            "content": "We present several failure cases observed when using LLaDA 1.5 (Zhu et al., 2025) with Confidence Top-k (k = 2). These examples highlight specific weaknesses inherent to parallel decoding. Incorrect Swapping of Names in Waiting Line (Shuffle) Input: [Jack Burns, Jessica Martin, Tyler Lopez] Model Output: [Jessica Lopez, Jack Burns, Tyler Martin] Analysis: The model incorrectly swaps the first and last names of two persons. This flaw highlights the risk of the conditional independence assumption in dLLMs. Word Repetition in Summarization Input: Summarize the following conversation. [...] Daniel: Cool. Model Output: Missy gets out of work at 6 pm and she drinks drinks after dinner. Analysis: The model incorrectly repeats the word drinks. This is common artifact in parallel decoding, where the lack of one-by-one conditioning can cause the model to generate the same token in adjacent positions. Grammatical Error in Paraphrasing Input: Paraphrase the sentence: How do deal with rude student? Model Output: What strategies can use to handle an disrespectful student? 27 Preprint Analysis: The output contains grammatical error in an disrespectful. This demonstrates that the model fails to enforce local syntactic rules, as the choice of article depends on the subsequent token, dependency that is weakened in parallel generation scheme. Grammatical Error in Words-to-Sentence Input: Construct single, coherent sentence using the words ball, bridge, elephant, and open. Model Output: The elephant walked across the the old bridge, looking carrying an open ball. Analysis: The generated sentence is syntactically flawed, due to both duplicated article (the the) and wrong verb sequence (looking carrying). This failure to form coherent clause underscores the difficulty parallel decoding has in maintaining long-range grammatical structure."
        },
        {
            "title": "I COMPREHENSIVE EXPERIMENTAL RESULTS",
            "content": "I.1 COMPARISON WITH LARGE LANGUAGE MODELS (LLMS) Tables 5 to 7 present performance comparison between open-source dLLMs, closed-source dLLM Mercury (Inception Labs et al., 2025), and several popular autoregressive LLMs, including Qwen (Yang et al., 2024; 2025a), Llama (Grattafiori et al., 2024), and Claude Haiku 3.5 (Anthropic, 2024). To create more challenging benchmark for this comparison, we increased the list length of the Waiting Line task to = 15 with 128 output tokens. For the open-source dLLMs, we set the number of tokens decoded in parallel to = 2 and did not use semi-autoregressive decoding. In contrast, we had no control over Mercurys parallel decoding settings, which we assume are handled internally. The results indicate that even commercial model like Mercury cannot achieve perfect score on Shuffle, yet it successfully solves Reverse with 100% accuracy. Furthermore, the relatively simple task of producing Latin Square proved rather challenging for Mercury, whereas it was easier for most LLMs that decode tokens one by one. Nevertheless, Mercury exceeds or matches the performance of open-source models on most tasks. 28 Preprint Table 5: Benchmark results for Waiting Line. Model / Dataset Copy Sort Rev. Shuff. Ins. Ind. Ins. Rand. Rem. Ind. Rem. Rand. Rep. Ind. Rep. Rand. Waiting Line LLaDA (Nie et al., 2025) LLaDA 1.5 (Zhu et al., 2025) Dream 7B (Ye et al., 2025b) DiffuCoder (Gong et al., 2025) Mercury (Inception Labs et al., 2025) Qwen2.5 3B (Yang et al., 2024) Qwen2.5 7B (Yang et al., 2024) Qwen3 4B (Yang et al., 2025a) Llama 3.1 8B (Grattafiori et al., 2024) Llama 3.2 3B (Grattafiori et al., 2024) Claude Haiku 3.5 (Anthropic, 2024) 100.0 100.0 100.0 58.0 100.0 100.0 100.0 100.0 100.0 100.0 100. 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 64.0 30.0 5.0 97.0 99.0 91.0 28.0 100.0 4.0 65.0 75.0 100.0 83.0 100.0 21.0 18.0 87.0 32.0 92.0 10.0 85.0 97.0 96.0 92.0 100. 14.0 14.0 27.0 9.0 63.0 39.0 39.0 77.0 50.0 20.0 31.0 85.0 86.0 81.0 51.0 95.0 96.0 100.0 89.0 93.0 35.0 100.0 18.0 21.0 32.0 10.0 15.0 19.0 11.0 43.0 22.0 17.0 29. 82.0 77.0 62.0 35.0 90.0 72.0 73.0 47.0 63.0 64.0 100.0 15.0 15.0 27.0 19.0 28.0 19.0 16.0 67.0 23.0 13.0 32.0 58.0 70.0 99.0 57.0 98.0 53.0 88.0 92.0 88.0 70.0 100. Table 6: Benchmark results for Text Writing. Text Writing Model / Dataset Paraphrasing Summarization W2S (easy) W2S (medium) W2S (hard) Grammar BERT 1-BLEU Grammar ROUGE-L Grammar Acc. Grammar Acc. Grammar Acc. LLaDA (Nie et al., 2025) LLaDA 1.5 (Zhu et al., 2025) Dream 7B (Ye et al., 2025b) DiffuCoder (Gong et al., 2025) Mercury (Inception Labs et al., 2025) Qwen2.5 3B (Yang et al., 2024) Qwen2.5 7B (Yang et al., 2024) Qwen3 4B (Yang et al., 2025a) Llama 3.1 8B (Grattafiori et al., 2024) Llama 3.2 3B (Grattafiori et al., 2024) Claude Haiku 3.5 (Anthropic, 2024) 96.0 95.0 91.0 86.0 98. 100.0 99.0 98.0 99.0 99.0 99.0 95.2 95.2 93.5 94.2 95.2 94.7 95.5 94.9 93.3 92.9 92.3 82.1 83.4 52.5 54.7 84.4 84.8 83.5 86.2 92.0 88.8 96.2 85.0 77.0 85.0 78.0 93. 95.0 97.0 93.0 97.0 97.0 99.0 43.2 42.9 40.3 41.9 31.1 38.2 38.9 34.8 39.4 36.4 34.3 86.0 89.0 89.0 87.0 99.0 99.0 99.0 97.0 97.0 98.0 100.0 80.0 83.0 61.0 53.0 80. 54.0 71.0 78.0 92.0 75.0 93.0 92.0 88.0 75.0 87.0 100.0 99.0 95.0 95.0 96.0 96.0 98.0 90.0 92.0 75.0 72.0 94.0 80.0 82.0 81.0 98.0 94.0 97.0 73.0 72.0 56.0 73.0 99. 96.0 98.0 99.0 99.0 99.0 99.0 82.0 84.0 83.0 59.0 84.0 73.0 75.0 86.0 90.0 70.0 84.0 Table 7: Benchmark results for Puzzle. Model / Dataset Puzzle Sudoku Latin Square LLaDA (Nie et al., 2025) LLaDA 1.5 (Zhu et al., 2025) Dream 7B (Ye et al., 2025b) DiffuCoder (Gong et al., 2025) Mercury (Inception Labs et al., 2025) Qwen2.5 3B (Yang et al., 2024) Qwen2.5 7B (Yang et al., 2024) Qwen3 4B (Yang et al., 2025a) Llama 3.1 8B (Grattafiori et al., 2024) Llama 3.2 3B (Grattafiori et al., 2024) Claude Haiku 3.5 (Anthropic, 2024) 14.8 30.6 95.4 16.7 11.1 2.8 1.9 0.0 0.0 0.0 21.3 36.0 35.3 34.7 10.0 59. 16.0 88.0 90.0 24.0 12.0 98.0 29 Preprint I.2 COMPLETE BENCHMARK RESULTS (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort (k) Paraphrasing (l) Summarization (m) W2S (easy) (n) W2S (medium) (o) W2S (hard) (p) Sudoku (q) Latin Square Figure 21: Full PARALLELBENCH results using LLaDA 1.0 (Nie et al., 2025). 30 Preprint (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort (k) Paraphrasing (l) Summarization (m) W2S (easy) (n) W2S (medium) (o) W2S (hard) (p) Sudoku (q) Latin Square Figure 22: Full PARALLELBENCH results using LLaDA 1.5 (Zhu et al., 2025). 31 Preprint (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort (k) Paraphrasing (l) Summarization (m) W2S (easy) (n) W2S (medium) (o) W2S (hard) (p) Sudoku (q) Latin Square Figure 23: Full PARALLELBENCH results using Dream 7B (Ye et al., 2025b). 32 Preprint (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort (k) Paraphrasing (l) Summarization (m) W2S (easy) (n) W2S (medium) (o) W2S (hard) (p) Sudoku (q) Latin Square Figure 24: Full PARALLELBENCH results using DiffuCoder (Gong et al., 2025). 33 Preprint (a) Copy (b) Reverse (c) Replace Index (d) Replace Random (e) Shuffle (f) Insert Index (g) Insert Random (h) Remove Index (i) Remove Random (j) Sort (k) Paraphrasing (l) Summarization (m) W2S (easy) (n) W2S (medium) (o) W2S (hard) (p) Sudoku (q) Latin Square Figure 25: Full PARALLELBENCH results using LLaDA 1.5 (Zhu et al., 2025) with PrefixCache (Wu et al., 2025)."
        }
    ],
    "affiliations": [
        "FuriosaAI",
        "KRAFTON AI",
        "Microsoft Research",
        "Seoul National University",
        "UC Berkeley",
        "UW-Madison"
    ]
}