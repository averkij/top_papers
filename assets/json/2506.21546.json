{
    "paper_title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation",
    "authors": [
        "Xinzhuo Li",
        "Adheesh Juvekar",
        "Xingyou Liu",
        "Muntasir Wahed",
        "Kiet A. Nguyen",
        "Ismini Lourentzou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and a set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity."
        },
        {
            "title": "Start",
            "content": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation Xinzhuo Li , Adheesh Juvekar , Xingyou Liu, Muntasir Wahed, Kiet A. Nguyen, Ismini Lourentzou {xinzhuo4, adheesh2, xliu265, mwahed2, kietan2, lourent2}@illinois.edu University of Illinois Urbana-Champaign 5 2 0 2 8 2 ] . [ 2 6 4 5 1 2 . 6 0 5 2 : r Abstract. Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity. (cid:128) https://plan-lab.github.io/hallusegbench/ 1. Introduction Vision-Language Models (VLMs) have demonstrated remarkable progress across spectrum of multimodal tasks, integrating visual and textual information for enhanced understanding and grounding in complex visual scenes [1, 22]. Leveraging large-scale multimodal datasets, these models achieve state-of-the-art performance in diverse applications, including visual question answering [22, 33], image captioning [15, 39], and object detection [24]. More recently, VLMs have extended their capabilities to reasoning-based segmentation [14, 25, 29, 31] and spatial reasoning tasks [6, 7], where pixel-level understanding is conditioned on natural language descriptions. This fine-grained alignment of linguistic and visual representations has introduced new possibilities for segmenting objects not merely by appearance, but through contextual understanding and semantic cues, bridging the gap between textual queries and precise pixel-level predictions. Equal Contribution Despite this progress, hallucinations remain critical challenge [10, 51]. Hallucinations can manifest in several ways, such as object hallucinations [8, 9, 17, 32], e.g., when model incorrectly mentions or labels objects that do not exist in scene, or pixel-grounding hallucinations [19, 44, 45], where model may generate spatially plausible but semantically incorrect masks. While object hallucinations are relatively straightforward to diagnose by comparing predicted labels against clearly visible visual content, pixelgrounding hallucinations are far more insidious. They occur at finer spatial granularity, typically involving masks that align well with visual structures but are semantically misleading. Crucially, detecting pixelgrounding hallucinations requires fine-grained, pixel-level ground truth, unlike object hallucinations, which can be identified without dense annotations. Such failures are especially problematic in dense prediction tasks and cluttered scenes, where ambiguous boundaries and visually coherent contexts make it easier for models to rely on learned biases rather than truly grounding their predictions in the image evidence. Yet, existing benchmarks provide limited evaluation of pixel-grounding hallucinations. Most evaluation protocols attempt to elicit hallucinations by introducing synthetic labels not present in the image [43, 44]. While label-guided approaches introduce notion of robustness, negative samples are often semantically or visually implausible, making it relatively straightforward for models to reject them without truly demonstrating grounding capabilities. Furthermore, they fail to expose models to counterfactual scenarios, i.e., visually coherent but semantically modified image contexts that require the model to distinguish fine-grained visual differences grounded in referential language. To address this gap, we introduce HalluSegBench, novel benchmark designed to evaluate pixel-grounding hallucination through counterfactual visual reasoning. HalluSegBench consists of factual-counterfactual image pairs where target object is systematically replaced with similar alternative, while the surrounding context remains unchanged. Each image pair is accompanied by ground truth masks for both object classes, enabling pixel-level evaluation of both true and hallucinated predictions. HalluSegBench enables controlled testing of segmentation models by exposing them to counterfactual settings that are perceptually plausible yet semantically distinct. Our benchmark rigorously tests the models ability to suppress hallucinations and remain faithful to the referential prompt or over-rely on semantic priors. In addition, we propose set of new evaluation metrics designed to capture three critical aspects of model reliability in grounded segmentation: sensitivity to contextual shifts, semantic alignment, and hallucination robustness. Metrics are grouped into two categories based on the aspect of hallucination they target: consistency-based and direct hallucination. Specifically, consistency-based metrics measure how models predictions shift under controlled label and object replacements, revealing its sensitivity to contextual changes. Complementing these, direct hallucination metrics penalize spurious mask predictions that overlap with distractor objects and contrast prediction errors across factual and counterfactual image settings to quantify hallucination severity. Together, these metrics provide comprehensive framework for evaluating robustness and segmentation fidelity, exposing failure modes often missed by conventional accuracy-based metrics. To the best of our knowledge, this is the first benchmark to support systematic counterfactual evaluation of pixel-grounding hallucination in reasoning segmentation models under controlled visual modifications. The contributions of our work are the following: (1) We introduce HalluSegBench, the first benchmark for pixel-grounding hallucination evaluation via counterfactual interventions. HalluSegBench consists of 1340 factual-counterfactual pairs, spanning 281 object classes, challenging models to distinguish visual evidence from learned priors. (2) We propose four novel evaluation metrics for quantifying hallucination failures across various dimensions, including performance degradation under visual or textual counterfactuals, over-reliance on language priors versus vision-driven errors, and spatial coherence of hallucinated masks in perceptually 2 plausible regions. These metrics expose segmentation failures that are often concealed by label-driven hallucination evaluation, providing deeper understanding of model vulnerabilities in realistic settings. (3) We conduct extensive experiments on state-of-the-art VLM-based segmentation models, revealing consistent hallucination failures under perceptually plausible counterfactuals, with more severe hallucinations when visual edits are applied compared to textual modifications. Our results highlight that label-only evaluation is insufficient for diagnosing reasoning segmentation hallucinations and motivate the need for counterfactual-driven robustness benchmarks to capture grounding failures. 2. Related Work Hallucination Evaluation. Advances in segmentation have significantly enhanced the ability of VLMs to understand and reason about fine-grained visual concepts [13, 30]. Seminal work by LISA [14] pioneered the integration of LLMs with pixel-level reasoning, demonstrating fine-grained alignment between natural language descriptions and precise pixel regions, and inspiring series of follow-up efforts [21, 29, 31, 37, 41, 42, 50]. Despite these advances, segmentation hallucination remains persistent challenge. Existing studies have sought to quantify object hallucination using label-based evaluations [3, 17, 20, 23]. These methods typically involve querying the model with textual prompts and observing prediction consistency. However, these evaluations are often limited to textual mismatches without grounding or precisely locating objects, potentially resulting in incorrect target identification within images. Recent approaches that introduce mask-based evaluation for hallucination testing [43, 44] primarily rely on synthetic text-based perturbations or randomly sampled false premises that are disconnected from the actual visual context. Furthermore, the absence of altered visual segmentations accompanying hallucinated labels renders these datasets insufficiently challenging for comprehensive segmentation hallucination evaluation at the pixel level. Recently, CCEval [48] proposed GPT-4-assisted evaluation method for caption-based hallucination detection in LLMs, but focuses on whether hallucinated objects appear in textual descriptions, rather than in pixel-level grounding or visual segmentation hallucinations. In contrast, HalluSegBench directly evaluates segmentation hallucination at the pixel level through controlled counterfactual interventions, enabling the discovery of hallucinationinduced segmentation failures in VLMs that are undetectable through caption-based methods alone. Counterfactual Reasoning. Counterfactual reasoning has emerged as powerful tool for evaluating model reliability under hypothetical visual alterations. In Visual Question Answering (VQA), counterfactual methods reveal hidden biases by perturbing image-question pairs [18, 27]. Similarly, Vision-and-Language Navigation (VLN) works counterfactual path generation [28, 38] to improve navigational robustness through hypothetical scenarios. Recent work [4] introduces training-free counterfactual inpainting approach to discover object dependencies by measuring the semantic impact of object removals on scene plausibility. While these approaches underscore the value of counterfactual visual reasoning, they primarily focus on high-level tasks or structural coherence rather than fine-grained visual grounding. Our proposed HalluSegBench aims to bridge this gap by introducing rigorous evaluation framework and new counterfactual pixel-level evaluation metrics that isolate hallucination behaviors in reasoning segmentation models through structured and visually grounded counterfactual interventions. Controlled Image Editing. Generative models have enabled controlled image modifications while preserving semantic coherence [12, 52], with diffusion models gaining attention for image editing tasks [5, 11, 34, 49]. In mask-guided image editing frameworks, user-defined masks enable precise region-specific modifications, ensuring coherent adjustments that align with textual or semantic prompts [2, 26]. Instruction-guided variants [16, 40] allow grounded edits with mask-generation, while counterfactual diffusion [35] introduces structured scene alterations. While prior image editing work [46] benchmarks segmentation models by 3 altering object attributes with mask-preserved method, it remains confined to intra-object or global variations, limiting its applicability to scenarios involving the same object. These approaches fall short in evaluating counterfactual scenarios where object replacements are performed within their original context, settings that require models to maintain visual grounding without introducing spurious hallucinations. HalluSegBench preserves contextual integrity across factual and counterfactual scenarios, enabling more granular analysis of segmentation reliability and hallucination robustness. 3. Pixel-Level Counterfactual Visual Reasoning Hallucination in vision-language grounding occurs when model segments objects that are either not present in the image or are inconsistent with the referential input. While hallucinations have been extensively studied in vision-language understanding tasks like image captioning and VQA [18, 27], hallucination evaluation in grounded segmentation remains underexplored. To address this gap, we introduce HalluSegBench, novel framework for assessing pixel-level hallucinations through counterfactual visual reasoning. In this context, we define counterfactual visual reasoning for hallucinations in pixel-grounding VLMs as the evaluation of models grounding fidelity through controlled visual perturbations that replace target object with semantically distinct alternative, while preserving the surrounding context. Unlike conventional segmentation evaluation, hallucinations in our work explicitly test models ability to distinguish between genuine visual evidence and semantic priors by constructing counterfactual image pairs in which visual evidence for target object is systematically replaced by another object, thereby revealing whether the models predictions are grounded in true perceptual understanding rather that semantic priors. Given factual image containing target object of class c, and text prompt that references the same object of class to be segmented (e.g., the red apple on the table), pixel-level reasoning segmentation 1. To evaluate hallucinations, HalluSegBench introduces model predicts segmentation mask ˆMc = (I, c) counterfactual image pairs (I, (1) Factual Image contains at least one visible instance of object of class c, and is an edited version of I, in which the object of class is replaced with (2) Counterfactual Image , and all other pixels in the scene are held constant. This semantically similar object of another class controlled intervention isolates the visual evidence associated with object c, enabling us to test whether the models predictions are grounded in true visual cues or driven by semantic priors. ) that satisfy the following properties: More formally, given factual-counterfactual image pair (I, , and their corresponding ground truth masks Mc classes and using four predicted segmentation masks, covering the following scenarios: and ), along with the original and replaced object , we evaluate the models behavior ˆMc = (I, c): Predicted mask for object in the factual image I. ˆMc = (I, ˆM = (I ˆM = (I , c): Predicted mask for the original object class in the counterfactual image . , ): Predicted mask for the counterfactual object ): Predicted mask for the new object class in the counterfactual image in the factual image I. . These four masks expose the models grounding decisions under controlled visual and textual conditions, allowing us to systematically probe its segmentation fidelity. For the factual image I, we expect the model to , the segment accurately while suppressing predictions for model should suppress while segmenting . Conversely, for the counterfactual image reliably. 1We use the notation (I, c) instead of explicitly referencing the text prompt for brevity and clarity. 4 (a) Ground-truth Masks (b) Predicted Masks Figure 1: Illustration of Segmentation Behavior from LISA [14] Pixel-Grounding VLM Under Factual , and Counterfactual Settings. The rows represent the factual image and its counterfactual variant where the original object of class blue bus is replaced with visually similar object of class yellow taxi. (a) shows ground-truth masks, while (b) shows the corresponding model predictions. Hallucinations can be observed when theres an image-label mismatch. Figure 1 illustrates an example of the predicted masks (Figure 1b) across these four scenarios, and their corresponding ground truth masks (Figure 1a). Counterfactual pairs evaluate whether the model correctly ), and if it hallucinates objects that are visually segments the target object when it is present (i.e., ˆMc absent (i.e., ˆMc and . This structured evaluation serves as the foundation of our hallucination analysis in HalluSegBench. Evaluation is performed using our newly proposed counterfactual consistency metrics described in 3.2. ). Predictions are compared against the corresponding ground truth masks Mc and ˆM and ˆM 3.1. HalluSegBench Dataset HalluSegBench Construction. HalluSegBench is constructed using images from the validation and test splits of the RefCOCO dataset [47], widely recognized benchmark for referring expression grounding. To enable precise counterfactual analysis, we systematically identify object instances amenable to reliable replacement and generate deterministic edit instructions specifying class-level substitutions for each target instance. These instructions follow controlled format, e.g., Change <object A> to <object B>, which drives an automated image editing module to produce visually consistent counterfactual images. For each factual image I, we retain the original mask Mc provided by RefCOCO [47] and generate the corresponding mask guided by the replacement class label. Crucially, our pipeline enforces strict single-object modification policy, preserving all other visual elements to isolate the semantic and visual impact of the counterfactual edits. This controlled setup supports fine-grained evaluation of grounding fidelity and hallucination detection, distinguishing whether model predictions are genuinely grounded in visual evidence or influenced by spurious priors. Full dataset construction details are provided in Appendix A. 5 (a) Mask Area Distribution (b) Factual-Counterfactual Replacement Pairs Figure 2: Overview of HalluSegBench Dataset Characteristics. (a) Distribution of mask sizes as percentage of the total image area. (b) Top-20 most frequent factual-counterfactual object replacement pairs, illustrating common substitution patterns in the dataset. Dataset Statistics. HalluSegBench comprises 1, 340 mask pairs across 281 unique object classes totaling 2, 680 segmentation masks and 2, 342 images. Figure 2a illustrates the distribution of mask sizes across the entire dataset, represented as percentage of the total image area and segmented by mask type: All (overall dataset), Factual, and Counterfactual instances. The majority of masks occupy small fraction of the image, predominantly in the 510% range, mirroring typical real-world scenes where objects are part of larger visual contexts. Both factual and counterfactual masks exhibit consistent size distributions, reflecting that object replacements maintain relative scale. Additionally, Figure 2b presents the most frequent factualto-counterfactual substitutions, highlighting the datasets spatial and semantic diversity. Notably, many of the top replacements involve visually and semantically similar objects that challenge the models ability to distinguish fine-grained visual details. The distribution of most frequent object classes across different categories is depicted in Figure 3, while representative examples of factual and counterfactual image pairs are presented in Figure 1. Figure 3: Distribution of Object Categories. 3.2. HalluSegBench Evaluation Metrics To evaluate hallucination in vision-language segmentation models under counterfactual conditions, we introduce comprehensive suite of metrics that quantify both the sensitivity of model predictions to controlled interventions and the severity of hallucinations when queried with absent or mismatched object classes. These 6 metrics are grouped into two complementary categories: (1) consistency-based performance metrics, which quantify prediction consistency by measuring degradation in visual and textual IoU when the input image or query are systematically perturbed, and (2) direct hallucination metrics, which penalize the generation of false segmentation masks when object classes are visually absent. Consistency-based Performance Metrics. This class of metrics is designed to measure the sensitivity of model predictions to controlled counterfactual interventions. Specifically, they evaluate how much models segmentation output changes when either the visual input or the text query is systematically manipulated. These metrics jointly capture both segmentation quality and susceptibility to hallucination. In particular, we compute the prediction fidelity in the factual image and then contrast it with predictions under swapped labels or counterfactual visual inputs. The underlying principle is that robust model should exhibit predictable failures under mismatched object-image pairs and experience noticeable performance drops when visual evidence is explicitly removed. Let Mc and factual image class in I, and class in denote the ground truth masks for class in the factual image and class , respectively. Correspondingly, ˆMc , ˆMc in the counterdenote the predicted masks for class in I, , respectively. We compute three Intersection over Union (IoU) scores as follows: , and ˆM IoUfact = Mc Mc ˆMc ˆMc , IoUtextual = Mc Mc ˆMc ˆMc , IoUvisual = M ˆM ˆM c . (1) Here, IoUfact measures the segmentation accuracy in the factual setting, IoUtext measures model leakage when querying with counterfactual label on the same image, and IoUvisual evaluates hallucination when the queried object is absent in the counterfactual image but still prompted. Textual Delta IoU (IoUtextual) quantifies the degradation in segmentation performance when the textual query is swapped, while the visual content remains unchanged, i.e., IoUtextual = IoUfact IoUtextual. This metric measures the models ability to suppress predictions for objects that are not visually present when prompted with an incorrect label. Larger values indicate better grounding, while smaller values suggest over-reliance on language priors, leading to hallucinations. Visual Delta IoU (IoUvisual) captures the change in segmentation performance when the visual content is altered, but the text query remains fixed, i.e., IoUvisual = IoUfact IoUvisual. This metric evaluates the models sensitivity to counterfactual visual edits, e.g., when the queried object is removed. Smaller values indicate persistent hallucination despite the objects visual absence, revealing vulnerability to vision-driven errors. Together, IoUtext and IoUvisual provide orthogonal lenses to diagnose hallucination sources: the former exposes language-driven hallucinations, while the latter highlights vision-driven hallucinations. Our experiments demonstrate that pixel-grounding reasoning segmentation models often exhibit lower IoUvisual values, underscoring the unique capability of HalluSegBench to elicit hallucination behaviors that remain overlooked when evaluating only under label replacements. Direct Hallucination Metrics. While consistency-based metrics measure model performance within objectaligned regions, they do not fully capture the spatial extent and structure of hallucinated predictions beyond the boundaries of the ground-truth mask. To address this limitation, we introduce hallucination-specific metrics that directly penalize spurious mask predictions in scenarios where the queried object class is intentionally absent. These metrics quantify not only the presence of hallucinations but also their spatial alignment with other objects in the scene, providing more granular view of segmentation failures. natural starting point for assessing segmentation quality is the Tversky index [36], generalization of the IoU metric that balances the trade-off between false positives (FP) and false negatives (FN). Tversky 7 measures the overlap between predicted mask ˆM and ground truth mask as follows: Tversky( ˆM, M; α, β) = ˆM ˆM + α ˆM + β ˆM , (2) where ˆM represents True Positives (TP), ˆM False Positives (FP), ˆM False Negatives (FN), and parameters α and β control the relative penalty for FP and FN. However, in our counterfactual setting, the queried object class is explicitly removed from the image, resulting in an empty ground truth mask (M = ). Under these conditions, both TP and FN are zero, causing the Tversky index to simplify to Tversky( ˆM, ; α, β) = 0/α ˆM. This collapses to zero for any non-empty prediction ˆM, failing to distinguish between structured hallucinations, where the model predicts plausible objects that overlap with distractors, and unstructured noise, where the model generates arbitrary masks with no semantic alignment. This limitation makes the Tversky index unsuitable for diagnosing hallucinations in counterfactual segmentation, where the absence of the queried object is guaranteed by design. To address this limitation, we introduce the Confusion Mask Score (CMS). Unlike Tversky, CMS is designed to distinguish between hallucinated predictions that overlap with semantically confounding regions and that is not those that do not. Given factual image and predicted mask ˆMc of the present object class c. We decompose present, we measure its overlap with the ground truth mask Mc the hallucinated prediction into two components: = ˆMc (confusing region) and = ˆMc Mc (non-overlapping region). The Confusion Mask Score (CMS) is then computed as follows: for an object class Mc CMS = α + α Mc , (3) where is the area of the confusing region, is the area of the non-overlapping region, and Mc is the area of the ground truth object in the factual image. Here, α > 1 penalizes overlap with confusing regions more heavily than with unrelated areas. CMS score of 0 indicates no hallucination, while higher values reflect increasing severity and spatial focus of erroneous predictions. We evaluate CMS in two complementary on the factual image I, and the counterfactual settings: the factual case, where the model is queried with class . We refer to these as CMSfact case, where it is queried with the original class on the counterfactual image and CMScounterfact, respectively. Contrastive Confusion Mask Score (CCMS). While CMS effectively quantifies hallucination severity in individual factual and counterfactual settings, it does not reveal the models relative vulnerability to languagedriven versus vision-driven errors. To address this, we introduce the Contrastive Confusion Mask Score (CCMS) as the ratio of hallucination severity in the factual versus counterfactual settings: CCMS ="
        },
        {
            "title": "CMSfact\nCMScounterfact",
            "content": ". (4) This ratio quantifies whether hallucination is more severe when the image or the label is manipulated. CCMS score greater than 1 indicates that the model is more prone to hallucinate when queried with an incorrect label on an unedited image, highlighting an over-reliance on language priors. Conversely, CCMS score less than 1 reveals greater hallucination when the object is visually removed, suggesting that the model fails to suppress predictions even when the visual evidence is absent. Scores near 1 suggest comparable sensitivity to both types of interventions. Our experimental analysis consistently reports CCMS scores below 1 for existing models, demonstrating that vision-driven hallucinations are more frequent when visual edits are applied. This highlights the unique ability of HalluSegBench to expose hallucination behaviors that remain undetected under conventional label-based evaluations. Appendix provides more details on evaluation. 8 Table 1: Comparison of Reasoning Segmentation Models on HalluSegBench Metrics. HM: Models trained with hallucination mitigation strategies. Metrics include textual and visual delta IoU (IoUtextual, IoUvisual), factual and counterfactual CMS, and the contrastive hallucination metric (CCMS). Arrows indicate ( higher is better, lower is better). Best performance highlighted with . HM Model No LISA-7B [14] No PixelLM-7B [31] No GLaMM-7B [29] LISA-13B [14] No PixelLM-13B [31] No Yes SESAME-7B [44] 4. Experiments 0.4534 0.3952 0.3273 IoUtextual IoUvisual CMSfact CMScounterfact CCMS 0.4209 0.3080 0.6517 0.4748 0.6933 0.4196 0.4776 0.3194 0.5937 0.4306 0.4607 0.7317 0.7286 0.6052 0.6687 0. 0.2810 0.4071 0.3016 0.3886 0.4591 0.4285 0.4180 0.4273 0.3605 0.1983 0.4304 We evaluate range of state-of-the-art vision-language reasoning segmentation models on HalluSegBench to rigorously assess their robustness against counterfactual visual reasoning. Our experiments are designed to answer two key questions: (1) Can existing pixel-grounding VLMs suppress hallucinations when object identity or visual evidence is manipulated? (2) What types of hallucinations are more prevalent? Baselines. We evaluate range of pixel-grounding VLMs, including models explicitly designed to mitigate grounded hallucination. The reasoning-based models include LISA [14], GLaMM [29], and PixelLM [31], which leverage large language models for reasoning, and SAM [13] or other Transformer-based architectures to perform openvocabulary segmentation and grounding. In addition, we evaluate SESAME [44], model that is designed to mitigate segmentation hallucinations by fine-tuning on data with false-premise text queries. While these models demonstrate strong performance on standard benchmarks, they have not been evaluated under counterfactual settings that probe hallucination behavior directly. 0.7 0.64 0.72 0.58 0.8 0.6 0.4 0.2 c I m"
        },
        {
            "title": "SESAME",
            "content": "Experimental Results. Results presented in Table 1 underscore the intrinsic challenges that segmentation models face when exposed to counterfactual visual reasoning. Across all evaluated models, we observe that hallucination suppression remains significant challenge, particularly when visual evidence is manipulated. Notably, LISA-13B achieves the highest IoUtextual (0.4591), indicating robust segmentation when textual prompts are altered. However, its performance degrades significantly under counterfactual visual edits, with IoUvisual dropping to 0.3886. Our observations underscore that, while models effectively mitigate hallucinations when label information is modified, they struggle to adapt when the visual scene itself is altered, reflecting reliance on semantic priors rather than true visual grounding. This reinforces the importance of counterfactual visual reasoning as diagnostic tool for grounding reliability. Figure 4: mIoU Comparison of Reasoning Segmentation Models. Higher mIoU indicates better segmentation performance. In contrast, SESAME, which is designed to address hallucination, maintains stronger ability to suppress spurious predictions across both factual (CMSfact=0.1983) and counterfactual (CMScounterfact=0.4304) images by abstaining from predictions to circumvent false-positive segmentations. This conservative strategy 9 I, , , Model I, GT LISA-7B [14] PixelLM-7B [31] GLaMM-7B [29] LISA-13B [14] PixelLM-13B [31] SESAME-7B [44] Figure 5: Qualitative Comparison of Reasoning Segmentation Model Predictions across Factual and Counterfactual Images and Prompts. Here, =bowl of salad and =ceramic pot. effectively curtails hallucination-induced errors but also leads to significantly lower mIoU scores (0.58), as illustrated in Figure 4. PixelLM-13B demonstrates higher segmentation fidelity, yet is more prone to vision-driven hallucinations. These results underscore key observation: existing VLM-based segmentation models either tend towards over-mitigation or over-segmentation driven by semantic biases. SESAMEs high abstention rate allows it to avoid hallucinations but compromises segmentation recall, while LISA and PixelLMs reliance on visual priors exposes them to false-positive segmentations under counterfactual modifications. HalluSegBench systematically exposes these failure modes, highlighting the need for segmentation strategies that balance hallucination avoidance with robust visual grounding. Qualitative Analysis. Figure 5 illustrates the predictions of benchmarked models across the four query-image combinations described in 3.2, along with the corresponding ground truth masks. Consistent with our quantitative findings, all evaluated models exhibit grounding hallucinations in the (I , c) setting, where queried object has been explicitly removed from the scene. Among the baselines, both LISA variants show better hallucination resistance in the (I, ) case, where the object is absent but the image remains unaltered, supporting their top-performance on the IoUtextual metric. SESAME, while effective at suppressing hallucinations on average, as indicated by its low CMS scores, still exhibits failure cases in the qualitative results, particularly under visually ambiguous scenarios. Notably, SESAME fails on the factual (I, c) case, where all other models succeed. This behavior is also reflected in its lower mIoU in Figure 4, suggesting tendency to abstain from segmentation even when visual evidence is present. These results underscore critical limitation of current models in distinguishing factually grounded objects from hallucinated ones when exposed to semantically plausible but visually altered scenes, highlighting the need for robust counterfactual reasoning in segmentation tasks. Additional qualitative examples can be found in Appendix C. 5. Conclusion In this paper, we introduce HalluSegBench for evaluating hallucination in pixel-grounded vision-language segmentation models through the lens of counterfactual visual reasoning. By constructing image pairs in which specific objects are replaced with visually similar alternatives while preserving the rest of the scene, HalluSegBench enables controlled testing of whether models produce predictions unsupported by visual evidence. To support rigorous evaluation, we propose set of metrics that quantify both performance degradation and spatial hallucination under object-level visual edits. Our experiments reveal that current segmentation models are vulnerable to hallucination, particularly when object identity is subtly changed through counterfactual edits. Notably, even methods explicitly designed to reduce hallucination remain susceptible to counterfactual visual manipulations, suggesting that prior mitigation strategies do not generalize well to visually grounded reasoning tasks. Compared to label-only setups, HalluSegBench more effectively elicits hallucinations at pixel-level, laying the groundwork for more robust reasoning segmentation models."
        },
        {
            "title": "References",
            "content": "[1] Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others. Flamingo: Visual Language Model for Few-Shot Learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended Latent Diffusion. ACM Transactions on Graphics (TOG), 2023. [3] Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and Hadar Averbuch-Elor. Mitigating OpenVocabulary Caption Hallucinations. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [4] Anand Bhattad, Konpat Preechakul, and Alexei Efros. Visual Jenga: Discovering Object Dependencies via Counterfactual Inpainting. arXiv preprint arXiv:2503.21770, 2025. [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. InstructPix2Pix: Learning to Follow Image Editing Instructions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In European Conference on Computer Vision (ECCV), 2020. 11 [7] Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei. SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [8] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training. In Annual Meeting of the Association for Computational Linguistics (ACL), 2023. [9] Gregor Geigle, Radu Timofte, and Goran Glavaš. Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models? In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [10] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Gong. Visual Hallucinations of Multi-modal Large Language Models. In Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [11] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [12] Tero Karras, Samuli Laine, and Timo Aila. Style-Based Generator Architecture for Generative Adversarial Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment Anything. In International Conference on Computer Vision (ICCV), 2023. [14] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA: Reasoning Segmentation via Large Language Model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pretraining with Frozen Image Encoders and Large Language Models. In International Conference on Machine Learning (ICML), 2023. [16] Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao Hu, Jianzhuang Liu, et al. ZONE: Zero-Shot Instruction-Guided Local Editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [17] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating Object Hallucination in Large Vision-Language Models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [18] Zujie Liang, Weitao Jiang, Haifeng Hu, and Jiaying Zhu. Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. [19] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized Referring Expression Segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [20] Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, and Xirong Li. PhD: ChatGPT-Prompted Visual hallucination Evaluation Dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [21] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement. arXiv preprint arXiv:2503.06520, 2025. [22] Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae. Visual Instruction Tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [23] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models. In Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR), 2024. [24] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models. In European Conference on Computer Vision (ECCV), 2024. [25] Kiet Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, and Ismini Lourentzou. CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [26] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In International Conference on Machine Learning (ICML), 2022. [27] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual VQA: Cause-Effect Look at Language Bias. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [28] Amin Parvaneh, Ehsan Abbasnejad, Damien Teney, Javen Qinfeng Shi, and Anton Van den Hengel. In Advances in Neural Counterfactual Vision-and-Language Navigation: Unravelling the Unseen. Information Processing Systems (NeurIPS), 2020. [29] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. GLaMM: Pixel Grounding Large Multimodal Model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [30] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks. arXiv preprint arXiv:2401.14159, 2024. [31] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. PixelLM: Pixel Reasoning with Large Multimodal Model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [32] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object Hallucination in Image Captioning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. 13 [33] Neelabh Sinha, Vinija Jain, and Aman Chadha. Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types. In Proceedings of the First Workshop of Evaluation of Multi-Modal Generation, 2025. [34] Bartlomiej Sobieski, Jakub Grzywaczewski, Bartłomiej Sadlej, Matthew Tivnan, and Przemyslaw Biecek. Rethinking Visual Counterfactual Explanations Through Region Constraint. In International Conference on Learning Representations (ICLR), 2024. [35] Xue Song, Jiequan Cui, Hanwang Zhang, Jingjing Chen, Richang Hong, and Yu-Gang Jiang. Doubly Abductive Counterfactual Inference for Text-based Image Editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [36] Amos Tversky. Features of Similarity. Psychological review, 1977. [37] Muntasir Wahed, Kiet Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, and Ismini Lourentzou. PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation. arXiv preprint arXiv:2412.15209, 2024. [38] Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool, and Wenguan Wang. Counterfactual CycleIn Consistent Learning for Instruction Following and Generation in Vision-Language Navigation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [39] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: Unifying Architectures, Tasks, and Modalities Through Simple Sequence-to-Sequence Learning Framework. In International Conference on Machine Learning (ICML). Proceedings of Machine Learning Research (PMLR), 2022. [40] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions. arXiv preprint arXiv:2305.18047, 2023. [41] Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, and Yujiu Yang. HyperSeg: Towards Universal Visual Segmentation with Large Language Model. arXiv preprint arXiv:2411.17606, 2024. [42] Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Zheng Zhao, and Yujiu Yang. InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal Large Language Models. arXiv preprint arXiv:2412.14006, 2024. [43] Jianzong Wu, Xiangtai Li, Xia Li, Henghui Ding, Yunhai Tong, and Dacheng Tao. Towards Robust Referring Image Segmentation. IEEE Transactions on Image Processing (TIP), 2024. [44] Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap, Ritwik Gupta, Xudong Wang, Joseph Gonzalez, and Trevor Darrell. See, Say, and Segment: Teaching LMMs to Overcome False Premises. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [45] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. GSVA: Generalized Segmentation via Multimodal Large Language Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [46] Zijin Yin, Kongming Liang, Bing Li, Zhanyu Ma, and Jun Guo. Benchmarking Segmentation Models with Mask-Preserved Attribute Editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 14 [47] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling Context in Referring Expressions. In European Conference on Computer Vision (ECCV), 2016. [48] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, Chunyuan Li, and Manling Li. HallEControl: Controlling Object Hallucination in Large Multimodal Models. arXiv preprint arXiv:2310.01779, 2023. [49] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. MagicBrush: Manually Annotated Dataset for Instruction-Guided Image Editing. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [50] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [51] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Sirens Song in the AI Ocean: Survey on Hallucination in Large Language Models. arXiv preprint arXiv:2309.01219, 2023. [52] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. In International Conference on Computer Vision (ICCV), 2017. 15 A. HalluSegBench Details Motivation. HalluSegBench introduces counterfactual visual reasoning framework to evaluate segmentation models under controlled object-level interventions. Each factual image is paired with counterfactual variant in which the target object is replaced by semantically distinct, visually similar alternative, while the rest of the scene remains unchanged. This controlled pairing of visually coherent yet semantically altered scenes represents the first dataset explicitly designed for counterfactual evaluation of pixel-grounding segmentation models, enabling systematic study of vision-driven hallucinations. By comparing model outputs across these imagequery pairs, HalluSegBench isolates whether predictions are grounded in the visual evidence or driven by semantic priors. Unlike label-only perturbations, which often fail to challenge visual grounding, our counterfactual edits introduce minimal yet meaningful visual changes, making hallucinations, such as segmenting the replaced object, directly observable. This formulation enables fine-grained, instancelevel analysis of hallucination robustness in reasoning-based segmentation models and lays foundation for future work on pixel-level counterfactual training. Image and Mask Generation. Figure 6 illustrates the data generation pipeline used in HalluSegBench. is derived from factual image by applying localized visual intervention: Each counterfactual image , while keeping the rest of the single object instance of class is replaced with an instance of different class scene unchanged. Specifically, for the referring expression corresponding to I, we use the instruction format in Figure 13 to generate modified expression that replaces the originally referred object of class with that is semantically meaningful, visually distinct, and contextually reasonable based on new object of class the image content. The replacement is applied using the editing constraints in Figure 14, ensuring that the substituted object remains coherent within the scene while presenting challenging counterfactual condition for evaluation. To create the corresponding counterfactual image I, we perform edits using the GPT-4o image generation model, which enables fine-grained object-level transformations while effectively preserving the overall scene layout and visual realism throughout the image. To enable evaluation of grounding models, we provide segmentation masks for the relevant object instances in both the factual and counterfactual provided by RefCOCO [47]. For the images. For each factual image I, we retain the original mask Mc counterfactual image using Grounded SAM [30]. To ensure high-quality edits and mask alignment, all counterfactual examples are manually reviewed and filtered to discard samples that introduce visual artifacts or exhibit incorrect grounding in the predicted masks. The counterfactual imagequery pairs and corresponding object masks described above form the basis for evaluating model robustness to hallucination under targeted visual interventions. In the following section, we provide additional discussion on the properties of our proposed metrics, highlighting their interpretability, range, and how they support fine-grained analysis. , we generate the corresponding mask B. Discussion of Evaluation Metrics Range and Interpretation. We design our metrics to support fine-grained, instance-level analysis of hallucination behavior under controlled counterfactual interventions. The IoU-based delta metrics (IoUtextual and IoUvisual) are bounded within [1, 1], though values in practical scenarios are typically within [0, 1]. Higher values indicate reduced hallucination, as they correspond to greater divergence between factual predictions and those under counterfactual conditions, implying that the model appropriately suppresses predictions in the absence of supporting visual evidence. In contrast, the Confusion Mask Score (CMS) is non-negative and unbounded above, but is normalized by the size of the corresponding ground truth object. This normalization ensures comparability across 16 Figure 6: Overview of the Data Generation Pipeline. factual image is paired with concise edit where target object is replaced with visually similar instruction to produce counterfactual version alternative. Ground truth masks are obtained for both to support segmentation evaluation. examples with varying object sizes. While the absolute value may vary based on image content and object area, CMS remains effective in capturing hallucination severity through the weighted penalty of overlapping and non-overlapping errors. In our evaluations, we set α = 3 to emphasize overlapping errors more heavily, ensuring α > 1 for sharper contrast in failure cases. The Contrastive CMS (CCMS), defined as ratio of factual and counterfactual CMS scores, is also non-negative and unbounded. Scores near 1 suggest balanced susceptibility to hallucinations across visual and textual interventions, while significant deviations from 1 reveal modality-specific vulnerabilities that are otherwise difficult to diagnose using standard aggregate metrics. Metric Distributions and Summary Statistics. Figure 7 illustrates the empirical distribution of our IoU across all examples in HalluSegBench and all baselines. The distribution of IoUtextual and IoUvisual reveals bimodal pattern: one peak near 1.0 corresponding to successful suppression of hallucination, and larger peak concentrated near 0, indicating high frequency of hallucination cases. Notably, visual IoU scores exhibit higher density below zero compared to textual IoU, whereas the inverse holds for higher values, corroborating our earlier observation that vision-driven hallucinations are more persistent across models. Table 2 summarizes the behavior of proposed metrics using 95% confidence intervals. The reported means align with earlier findings that hallucination is more severe under visual counterfactual settings, with lower IoUvisual and higher CMScounterfact compared to their textual and factual counterparts. The relatively narrow confidence intervals suggest statistical reliability and low variance across the dataset, supporting their robustness for evaluating hallucination sensitivity across diverse image-query pairs in HalluSegBench. Figure 7: Distribution of IoU Across All Samples. Most IoU values lie near zero, especially under visual edits, indicating persistent hallucinations. 17 Table 2: Mean Values and Confidence Intervals for Evaluation Metrics. The final column reports the 95% confidence interval half-width () as an indicator of variability. Metric IoUtextual IoUvisual CMSfactual CMScounterfact Mean 0.4149 0.3619 0.3575 0.6490 C. Qualitative Examples 95% CI Lower 95% CI Upper CI Half-width 0.4239 0.3711 0.3672 0.6650 0.4059 0.3527 0.3478 0.6330 0.0090 0.0092 0.0097 0.0160 All metrics are computed at the image-query level, enabling instance-level attribution of failure cases. This allows us to visualize hallucinations not only through segmentation masks but also via metric responses, supporting richer qualitative analysis. CMS and IoU scores, in particular, provide interpretable feedback on both prediction quality and hallucination behavior under controlled visual interventions. Figures 8-12 present qualitative examples accompanied by all proposed metric scores across baselines for direct comparison of hallucination severity. These examples provide insights into how segmentation models behave across both factual and counterfactual conditions. ) or when the object is visually removed (I In the factual setting (I, c), most models produce accurate segmentations, reflected in high IoU scores. However, when prompted with an absent object (I, , c), significant hallucination behavior emerges. For instance, in Figure 8, models like GLaMM and PixelLM continue to segment the removed elephant, resulting in low IoUvisual and high CMScounterfact. Similarly, Figure 9 highlights prominent language-driven hallucinations, where several models segment cow in the original image that contains only sheep. This is reflected in elevated CMSfact scores and negative or near-zero IoUtextual values. Notably, LISA suppresses these hallucinations more effectively than SESAME, achieving both lower CMS and higher IoUtextual, indicating stronger grounding fidelity under mismatched label prompts. SESAME-7B demonstrates relatively lower hallucination across these settings, especially in (I, ), with CMSfact = 0 in Figures 8 and 11. However, this comes at the cost of under-segmentation or abstention in ) predictions in Figure 10. the factual case, reflected in lower IoUvisual and visual inconsistencies seen in (I The per-example metrics not only expose such trade-offs but also validate trends observed in the aggregate results, where LISA models rank highest on IoUtextual and SESAME excels in CMS. , Together, these examples demonstrate that the proposed metrics, IoU and CMS, offer consistent, interpretable signals that capture the nature and severity of hallucinations under controlled image-query manipulations. Their alignment with visual errors supports HalluSegBenchs goal of enabling transparent, instance-level evaluation of grounding behavior in segmentation models. D. Ablations To assess how model performance varies across different spatial granularities, we group objects into small, medium, and large mask size categories and evaluate all metrics at each scale. Table 3 shows that all models consistently exhibit the largest performance degradation both in terms of IoUvisual and CMScounterfact for small objects, making hallucinations for small regions particularly challenging. For instance, PixelLM-13B shows strong IoUvisual for large objects, but drops for smaller ones, while its hallucination scores (CMS) worsen accordingly. In contrast, the trend for CMSfact is less size-sensitive, likely due to its normalization by ground-truth mask area, which can dampen the relative penalty of hallucinations for smaller objects. 18 Table 3: Comparison of Reasoning Segmentation Models on HalluSegBench Metrics on small (S), medium (M), and large (L) object mask sizes. Arrows indicate ( higher is better, lower is better, where better refers to fewer hallucinations). Best performance highlighted with . Model IoUtextual S IoUvisual L CMSfact CMScounterfact S CCMS 0.3912 0.4748 0.4646 0.2326 0.2788 0.3308 0.2869 0.3095 0.3271 0.9926 0.6819 0.6044 0.2890 0.4539 0.5412 LISA-7B [14] 0.3825 0.4002 0.3833 0.4155 0.4025 0.4000 0.4214 0.4943 0.4916 1.0041 0.6693 0.6115 0.4197 0.7385 0.8039 PixelLM-7B [31] 0.3124 0.3379 0.3177 0.2768 0.2915 0.3485 0.3857 0.4183 0.4567 0.8029 0.5724 0.4922 0.4804 0.7308 0.9279 GLaMM-7B [29] 0.4079 0.4725 0.4709 0.3574 0.3820 0.4304 0.2948 0.3311 0.3233 0.9262 0.6328 0.5093 0.3183 0.5232 0.6348 LISA-13B [14] PixelLM-13B [31] 0.4099 0.4402 0.4170 0.4208 0.4247 0.4333 0.3615 0.4512 0.4520 1.0009 0.6708 0.5963 0.3612 0.6726 0.7580 0.3969 0.4239 0.4209 0.3358 0.3593 0.3922 0.1964 0.1969 0.2130 0.5532 0.4125 0.3573 0.3550 0.4773 0.5961 SESAME-7B [44] SESAME-7B demonstrates the strongest hallucination suppression behavior across all object sizes, achieving the lowest factual and counterfactual hallucination score throughout different sizes. However, this suppression comes at the cost of segmentation performance, as reflected by lower IoU scores across object sizes. In contrast, LISA maintains high IoUtextual and PixelLM maintains high IoUvisual but fails to avoid hallucinations captured by CMS scores, indicating more balanced tradeoff between hallucination avoidance and segmentation fidelity. Notably, LISA-7B/13B, GlaMM-7B, and SESAME-7B models yield IoUvisual scores below IoUtextual across most mask sizes, indicating greater susceptibility to vision-driven hallucinations - underscoring the unique diagnostic value of counterfactual visual reasoning in HalluSegBench. E. Limitations Although HalluSegBench provides an effective framework for evaluating grounding hallucinations through contractual edits, several inherent limitations of this approach should be considered. Our metrics are designed for settings where each factual image-query pair has corresponding counterfactual pair differing only by localized object replacement. This controlled setup enables precise attribution of hallucination behavior to specific visual change. However, extending these metrics to more complex scenarios, such as multi-object edits, compositional reasoning, or broader contextual shifts, may require modified formulations that account for additional sources of variation. As an initial effort in this space, HalluSegBench specifically focuses on substitution-based counterfactuals, where an object is replaced with another plausible object from semantically similar class. Other forms of counterfactual intervention, such as attribute modification, occlusion, or structural rearrangement, remain outside the scope of this benchmark and represent promising directions for future work. Moreover, the validity of counterfactual evaluation depends critically on the visual fidelity of the edited images. While we employ high-quality editing pipelines designed to preserve semantic plausibility and scene consistency, subtle artifacts or inconsistencies may still be introduced during the editing process. These imperfections can inadvertently influence model predictions, highlighting the need for continued advances in artifact-free image editing techniques to support reliable and unbiased evaluation. F. Broader Impact HalluSegBench provides the first dataset and evaluation framework for pixel-level counterfactual visual reasoning, enabling fine-grained diagnosis of hallucination behaviors in vision-language segmentation models. By explicitly probing model behavior under controlled, instance-level visual interventions, our benchmark facilitates greater transparency and diagnostic insight into segmentation failures and their underlying causes, and lays the groundwork for illuminating critical failure modes in models deployed in safety-sensitive domains such as robotics, autonomous vehicles, and medical imaging. Understanding when and why models segment objects that are no longer visually present is essential for developing trustworthy systems. Our work encourages shift from coarse, label-only hallucination evaluation toward rigorous, vision-grounded diagnostics. This paradigm can guide the design of more robust segmentation models and inspire training techniques that incorporate counterfactual supervision. Beyond application-specific relevance, this benchmark contributes to the broader vision-language community by emphasizing the need to evaluate grounding robustness under structured counterfactuals, not merely aggregate accuracy. We believe this direction is crucial for advancing safe, reliable, and generalizable multimodal AI systems that behave consistently under visual perturbations. However, as with any diagnostic benchmark, there is possibility that misuse or over-optimization on specific evaluation metrics could lead to models that perform well on this benchmark without generalizing to diverse, real-world failure modes. Additionally, models trained or evaluated on edited images may become overly sensitive to subtle editing artifacts if not properly controlled. To mitigate these risks, we recommend using HalluSegBench as part of broader evaluation suite and encourage future work to assess generalization across diverse visual perturbations. 20 Model I, GT I, , , LISA-7B [14] PixelLM-7B [31] GLaMM-7B [29] LISA-13B [14] PixelLM-13B [31] SESAME-7B [44] IoUtextual = 0.0194, IoUvisual = 0.0096, CMSfact = 0.9467, CMScounterfact = 0.9334 IoUtextual = 0.0039, IoUvisual = 0.0132, CMSfact = 0.9162, CMScounterfact = 0. IoUtextual = 0.0168, IoUvisual = 0.0575, CMSfact = 0.8977, CMScounterfact = 0.9579 IoUtextual = 0.0073, IoUvisual = 0.0031, CMSfact = 0.9637, CMScounterfact = 0.9438 IoUtextual = 0.0004, IoUvisual = 0.0306, CMSfact = 1.1755, CMScounterfact = 1.0514 IoUtextual = 0.9453, IoUvisual = 0.0054, CMSfact = 0, CMScounterfact = 0.9506 Figure 8: Qualitative Comparison of Reasoning Segmentation Models across Factual and Counterfactual Settings. Predictions shown for factual/counterfactual imageprompt pairs. Metric scores below each row highlight differences in hallucination severity and grounding sensitivity. Here, =the middle elephant =a rhinoceros. Positive IoU values indicate that the model appropriately suppresses segmentation and when the object is absent (via incorrect prompt or visual replacement), while negative values signal that the model becomes more confident in its prediction under incorrect conditionsstrong evidence of hallucination. For example, SESAME-7B achieves near-perfect IoUtextual and CMSfact = 0, fully suppressing predictions with an incorrect query, yet fails to adapt to visual changes IoUtextual = 0.0054, revealing vision-driven hallucination. In contrast, PixelLM-13B shows small or negative IoUs and the highest CMS values, indicating strong hallucination in both textual and visual conditions. Model I, GT I, , , LISA-7B [14] PixelLM-7B [31] GLaMM-7B [29] LISA-13B [14] PixelLM-13B [31] SESAME-7B [44] IoUtextual = 0.9233, IoUvisual = 0.0214, CMSfact = 0.1052, CMScounterfact = 0.9581 IoUtextual = 0.0059, IoUvisual = 0.0389, CMSfact = 0.9961, CMScounterfact = 0.9872 IoUtextual = 0.1332, IoUvisual = 0.1775, CMSfact = 1.0239, CMScounterfact = 0.9877 IoUtextual = 0.9305, IoUvisual = 0.0233, CMSfact = 0.0922, CMScounterfact = 0. IoUtextual = 0.0138, IoUvisual = 0.0816, CMSfact = 0.7586, CMScounterfact = 0.9513 IoUtextual = 0.0003, IoUvisual = 0.0321, CMSfact = 0.9945, CMScounterfact = 0.9654 Figure 9: Qualitative Comparison of Reasoning Segmentation Models across Factual and Counterfactual Settings. Predictions shown for factual/counterfactual imageprompt pairs. Metric scores below each row highlight differences in hallucination severity and grounding sensitivity. Here, =full grown sheep and =a cow. Here, LISA and GLaMM models exhibit strong vision-driven hallucinations. All models exhibit relatively high CMS values, indicating overconfident predictions despite incorrect prompts or visual evidence. 22 Model I, GT I, , , LISA-7B [14] PixelLM-7B [31] GLaMM-7B [29] LISA-13B [14] PixelLM-13B [31] SESAME-7B [44] IoUtextual = 0.0025, IoUvisual = 0.0475, CMSfact = 0.9275, CMScounterfact = 0.9478 IoUtextual = 0.0002, IoUvisual = 0.0397, CMSfact = 0.9943, CMScounterfact = 0.9621 IoUtextual = 0.0034, IoUvisual = 0.8345, CMSfact = 0.8633, CMScounterfact = 0.0737 IoUtextual = 0.0041, IoUvisual = 0.0626, CMSfact = 0.9147, CMScounterfact = 0.9435 IoUtextual = 0.0004, IoUvisual = 0.0921, CMSfact = 1.0623, CMScounterfact = 0.9650 IoUtextual = 0.0262, IoUvisual = 0.0184, CMSfact = 0.8900, CMScounterfact = 0. Figure 10: Qualitative Comparison of Reasoning Segmentation Models across Factual and Counterfactual Settings. Predictions shown for factual/counterfactual imageprompt pairs. Scores highlight differences in hallucination severity and grounding sensitivity. Here, =front cow and =front pig."
        },
        {
            "title": "Model",
            "content": "I, I, , , GT LISA-7B [14] PixelLM-7B [31] GLaMM-7B [29] LISA-13B [14] PixelLM-13B [31] SESAME-7B [44] IoUtextual = 0, IoUvisual = 0.0007, CMSfact = 0.9948, CMScounterfact = 0.9734 IoUtextual = 0.0009, IoUvisual = 0.0259, CMSfact = 0.9968, CMScounterfact = 1.0021 IoUtextual = 0.1867, IoUvisual = 0.1612, CMSfact = 0.9883, CMScounterfact = 0.9410 IoUtextual = 0, IoUvisual = 0.0031, CMSfact = 0.9994, CMScounterfact = 0.9759 IoUtextual = 0.0022, IoUvisual = 0.0802, CMSfact = 0.1983, CMScounterfact = 0. IoUtextual = 0.9566, IoUvisual = 0.0131, CMSfact = 0, CMScounterfact = 0.9853 Figure 11: Qualitative Comparison of Reasoning Segmentation Models across Factual and Counterfactual Settings. Predictions shown for factual/counterfactual imageprompt pairs. Scores highlight differences in hallucination severity and grounding sensitivity. Here, =right bear and =right raccoon. 24 Model I, GT I, , , LISA-7B [14] PixelLM-7B [31] GLaMM-7B [29] LISA-13B [14] PixelLM-13B [31] SESAME-7B [44] IoUtextual = 0.2168, IoUvisual = 0.7339, CMSfact = 0.5756, CMScounterfact = 0.0914 IoUtextual = 0.1901, IoUvisual = 0.9284, CMSfact = 0.4147, CMScounterfact = 0.0521 IoUtextual = 0.0008, IoUvisual = 0.0262, CMSfact = 0.9738, CMScounterfact = 0.9232 IoUtextual = 0.2543, IoUvisual = 0.2568, CMSfact = 0.0490, CMScounterfact = 0.0484 IoUtextual = 0.3248, IoUvisual = 0.3249, CMSfact = 0.0549, CMScounterfact = 0.0441 IoUtextual = 0.9481, IoUvisual = 0.0123, CMSfact = 0.0473, CMScounterfact = 0. Figure 12: Qualitative Comparison of Reasoning Segmentation Models across Factual and Counterfactual Settings. Predictions shown for factual/counterfactual imageprompt pairs. Scores highlight differences in hallucination severity and grounding sensitivity. Here, =first phone and =digital camera. 25 You are given two images : 1. The full scene . 2. binary mask marking an object labeled \"{ label }\" , described as \"{ description }\". In case of vague or wrong descriptions , follow the image and mask . Task : - Locate the masked object precisely . - Create replacement instruction that : Uniquely identifies the object ( position , color , size , etc .) Swaps it for new object that is not already present . New object should be meaningful , similar in size and shape but different in identity . Requirements : - The new object must not exist in the image , even in the unmasked area . - The new object should be common object , not an abstract concept . - Avoid vague objects like \" fruit \" or \" vegetable \". Be specific . - Avoid changes that are too similar or unnoticeable , such as change car to automobile . The new object cannot be different name or description that is still correct to describe the original object . - The new object should be reasonable to appear in the original object ' location . For example , an animal can be changed to another animal if in the zoo , but not car . - If the original label or description is clearly wrong , such as \" yep \" or other nonsense , please use the image and mask to identify the object and correct it in your change instruction . Do not overdo it . Output : Only line in this format : Change < original object > to < new object >. Figure 13: Prompt for Generating Object Replacement Instructions. This prompt guides VLM to generate plausible and localized object replacement based on binary mask and object description. Here, {label} and {description} correspond to the object label and associated referring expression from the RefCOCO annotations, respectively. You must only { item [ ` instruction ']} You are provided with an inverse mask , where the masked regions represent parts of the image that must be strictly preserved . You are only allowed to modify the unmasked ( transparent ) regions . No edits are allowed in any masked area . Even if there are multiple similar objects in the image , you must only change the one located in the unmasked area do not modify any other similar objects outside of the unmasked region . Strictly maintain the size , position , and shape of the unmasked region : do not resize , move , or distort it . Do not zoom in or out , and do not change the aspect ratio . All other parts of the image ( including other similar objects , background , lighting , textures , and context ) must remain completely unchanged . The final edited image must look realistic , natural , and in di sti ng uis ha ble from an untouched real - world photograph . Figure 14: Prompt for Constrained Image Editing. This prompt instructs generative model to edit only unmasked regions while preserving scene structure and realism. Here, {item[instruction]} denotes the extracted instruction using prompt shown in Figure 13."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}