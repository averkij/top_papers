{
    "paper_title": "Stairway to Fairness: Connecting Group and Individual Fairness",
    "authors": [
        "Theresia Veronika Rampisela",
        "Maria Maistro",
        "Tuukka Ruotsalo",
        "Falk Scholer",
        "Christina Lioma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness."
        },
        {
            "title": "Start",
            "content": "Stairway to Fairness: Connecting Group and Individual Fairness Maria Maistro University of Copenhagen Copenhagen, Denmark mm@di.ku.dk Theresia Veronika Rampisela University of Copenhagen Copenhagen, Denmark thra@di.ku.dk Tuukka Ruotsalo University of Copenhagen Copenhagen, Denmark LUT University Lahti, Finland tr@di.ku.dk 5 2 0 2 9 2 ] . [ 1 4 3 3 1 2 . 8 0 5 2 : r Falk Scholer RMIT University Melbourne, Australia falk.scholer@rmit.edu.au Christina Lioma University of Copenhagen Copenhagen, Denmark c.lioma@di.ku.dk Abstract Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for proper comparison of the two. As result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness. CCS Concepts Information systems Evaluation of retrieval results; Recommender systems; General and reference Evaluation. Keywords group fairness, individual fairness, fairness evaluation ACM Reference Format: Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Falk Scholer, and Christina Lioma. 2025. Stairway to Fairness: Connecting Group and Individual Fairness. In Proceedings of the Nineteenth ACM Conference on Recommender Systems (RecSys 25), September 2226, 2025, Prague, Czech Republic. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3705328. 3748031 Part of this work was done while visiting ADM+S at RMIT University. This work is licensed under Creative Commons Attribution-ShareAlike 4.0 International License. RecSys 25, Prague, Czech Republic 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1364-4/2025/09 https://doi.org/10.1145/3705328.3748031 #user NDCG Gini Group 1 Group"
        },
        {
            "title": "Individual",
            "content": "11 901 912 0.546 0.471 }0.037 0.472 0. Figure 1: Top: NDCG score distribution of two handpicked, non-overlapping user groups from our experiment and for all individual users of the two groups. Circles denote outliers. Bottom: Fairness score (Gini) for groups and individuals. The lower the Gini score, the fairer. Fairness is better between groups than across all individuals as both groups have similar average NDCG, but within-group variance is high, which means that recommendation quality varies widely across users (see the boxplots)."
        },
        {
            "title": "1 Introduction\nWith recent legislations that mandate responsible artificial intelli-\ngence development, Recommender System (RS) fairness evaluation\nhas become increasingly important to ensure that users are not\nsystematically disadvantaged. Fairness in RSs can be evaluated\nfor groups and for individuals. Group fairness typically refers to\nhaving equitable outcome across groups (e.g., similar effectiveness\nbetween groups of users [14]), while individual fairness is com-\nmonly defined as treating similar users/items equally [60] (e.g.,\nsimilar effectiveness for all users [63]). Conceptually, prior work\n[13, 33, 52] discusses how RSs can be fair to groups and at the same\ntime unfair to individuals, or vice versa, but no work has empirically\nstudied how this practically occurs in RS fairness evaluation.",
            "content": "Prior work either: (i) evaluates fairness exclusively for groups or individuals [11]; or (ii) evaluates both, but with two different families of measures [16, 42, 44] or for two fairness subjects/objectives RecSys 25, September 2226, 2025, Prague, Czech Republic Rampisela et al. [47, 63]. Evaluating group and individual fairness with different families of measures makes comparison difficult, as the measure scores may differ in sensitivity, or in theoretical and empirical ranges [45, 46, 50]. Likewise, it is not possible to properly compare group and individual fairness when each is evaluated for distinct fairness objective, e.g., recommendation effectiveness disparity across individual users vs. exposure disparity between item groups [63]. To address this gap, we evaluate user-side group and individual fairness with the same families of measures that can quantify both. An example of such measures is the Gini Index (Gini) [19]. In Fig. 1, we evaluate group and individual fairness with Gini on real data and exemplify how an RS can be very fair towards user groups and at the same time much more unfair towards individual users. In this paper, we study the relationship between evaluation measures of user-side group and individual fairness. This work is the first empirical study that compares the 9 existing user-side fairness evaluation measures for groups with those for individuals. We ask the following research questions (RQs): RQ1 To what extent do group and individual fairness evaluation measures differ in their conclusions? RQ2 For the same family of measures, how different are the group and individual fairness scores? RQ3 How do different ways of grouping users affect betweenand within-group fairness? RQ4 How do betweenand within-group fairness relate to individual fairness? Our results show that group fairness measures often hide unfairness within groups and between individuals, highlighting the importance of evaluating fairness beyond the between-group level."
        },
        {
            "title": "2 Methodology\nWe compare evaluation measures of user-side individual and group\nfairness in RSs, considering multiple ways of grouping users.",
            "content": "Datasets. To enable group fairness analysis, three datasets with 3 user profile features are selected (see Tab. 1 for statistics). ML-1M [24] has 1,000,029 movie ratings (15) from 6K users. Users with no/unspecified self-reported gender, age, or occupation are removed, and we exclude users under 18 years to avoid processing the data of minors. We focus on recommending preferred movies, so ratings <3 are discarded, and the levels 4 and 5 are mapped to 1. JobRec [23] has 1.6M job applications from 321K users. Given users application history, we focus on recommending job titles that may suit them, keeping only users with information for degree, major, and years of experience. Users with more than 60 years of experience are removed (as this likely indicates erroneous entries). LFM-1B [49] has 1,088,161,692 music playcounts, from 120K users. We focus on recommending new track artists for user to listen to, other than the ones they have listened to in the past, using the dataset after deduplication based on the artist, with 65M interactions (provided by RecBole [67]). The deduplication summarises the total playcount per artist and keeps the last event timestamp. Users without countries, age, or gender information are removed, as are minors (as in ML-1M), and users with age >100 years (as this likely indicates erroneous entries). Items without name/title are removed from all datasets. To reduce data sparsity, which may affect LLMRec performance [28], we keep Table 1: Preprocessed dataset statistics. ùëõùê∫ùëé is the number of groups for sensitive attribute ùëé. We exclude empty groups. ML-1M [24] JobRec [23] LFM-1B [49] #interaction (all splits) #item (all splits) #user (test set) sensitive attr. #1 (ùëõùê∫1 ) sensitive attr. #2 (ùëõùê∫2 ) sensitive attr. #3 (ùëõùê∫3 ) #intersectional groups minmax subgroup size median subgroup size 467,218 3,030 620 210,921 19,912 523 gender (2) age (3) occupation (2) degree (3) years of experience (3) major (6) 12 2279 31 36 170 7 15,024,267 51,204 16,611 gender (2) age (3) country (5) 29 14,260 59 users and items with 5 interactions (5-core filtering) for ML-1M and JobRec. We apply 50-core filtering [38, 62, 66] to LFM-1B, as it is highly sparse with 5-core filtering. The data is temporally split for train/val/test with ratio of 3:1:1 using global timeline [39]. From all splits, users and items with ùë° interactions in the train set are removed. high ùë° can result in very few unique users in the test set, so we choose ùë° such that at least 500 test users remain. For ML-1M and LFM-1B, we set ùë° = 5 [64]. For JobRec, we use ùë° = 2. We remove users/items in the val and test sets that are not in the train set. User grouping. To study group fairness, we cluster users based on their sensitive attributes (see Tab. 1 and App. A.1 in the code repository). Users cannot belong to two groups at the same time, e.g., age<50 and age50. For ML-1M, we use gender, age, and occupation as sensitive attributes. Gender is used as is. Age is grouped into: 1824 years, 25 49 years, and 50 years [40]. User occupation is grouped into: nonworking (student [15, 58], homemaker, retired, and unemployed) and working (14 occupations ranging from farmer to executive [57]). For JobRec, we consider the users academic degree, years of working experience, and study major as the sensitive attributes. Degree is grouped into: high school, college (associate or vocational degree), and university (bachelors, masters, and PhD). Years of experience are grouped into: 5 years, >510 years, and >10 years. We group study majors into six fields of study, as per Xu et al. [64], using manual annotation and fuzzy string matching.1 For LFM-1B, the sensitive attributes are gender, age, and country. Gender and age are processed as for ML-1M, and the users country is mapped to the continent.2 Users from the North/South Americas are grouped together with Antarctica [21, 64]. LLM-based Recommenders. Recent work has utilised Large Language Models (LLMs) as recommenders (LLMRecs), with promising results [26].3 Unlike collaborative filtering models, LLMRecs can easily handle fine-grained user attributes (e.g., users study major, which can be important for job recommendation) with their world knowledge, although including sensitive attributes in the prompt can impact effectiveness and fairness [10, 64, 65]. We therefore study 1Details are provided in App. A.1 in the code repository. 2We use the country-continent mapping from https://gist.github.com/achuhunkin/ 6cb1cbceb23395300aa209aad09e6e5d, and manually group transcontinental countries. 3We also experiment with two collaborative filtering RSs, i.e., UserKNN and NeuMF. See also Footnote 10. Stairway to Fairness: Connecting Group and Individual Fairness RecSys 25, September 2226, 2025, Prague, Czech Republic the effectiveness and fairness of LLMRecs under few-shot learning. To ensure comparable performance, we use four open-source, similar-sized LLMs released in JulyNov24: Llama-3.1-8B-Instruct [22], Qwen2.5-7B-Instruct [55], GLM-4-9B-chat [20], and Ministral8B-Instruct-2410 [54]. The temperature is fixed at 0 for each LLM to obtain deterministic output. The LLMs are prompted using in-context learning (ICL) strategy [26], as this has been shown to outperform sequential and recencyfocused prompting.4 We only prompt for users that exist in the test set, as otherwise it is not possible to evaluate the recommendation effectiveness. In the prompt, we provide the users train items as the interaction history and the val items as the few-shot samples. To guide the recommendation generation with the ICL strategy, val items are used as examples of what should be recommended to user, considering their historical interactions. maximum of 10 most recent train and val items each are provided, as having too many items in the prompt may reduce effectiveness [26, 28]. To avoid inflated performance, we do not prompt with sampled candidate item list [30]. Instead, we add restriction in the prompt to narrow down the item search space, e.g., for ML-1M, the movies should be between certain years, based on the movie release year in the metadata file. For LFM-1B, the prompt also includes the playcount, which is important in music recommendation [21]. Based on the inclusion/exclusion of user sensitive attributes, we create two prompt types [10, 12, 56, 65]: Sensitive (S), which has both interaction history and all three sensitive attributes, and Non-Sensitive (NS), which has only the interaction history. To evaluate LLMRecs, we perform fuzzy string matching between the list of recommended items and item names in the test set [7, 25, 28, 35, 41], by using the TF-IDF [29] of the items character-based ngram [34].5 If the item name similarity exceeds pre-set threshold ( 0.75), we count it as match (i.e., the LLMRec successfully recommends an item that exists in users test set).6 Our LLMRecs experiments are carried out with vllm [31] and RecLM-eval [34]. Evaluation. Recommendation effectiveness (Eff) and fairness (Fair) are measured at ùëò = 10 for all LLMRecs. For Eff, the mean Hit Rate (HR), MRR, P@ùëò (P), and NDCG@ùëò [27] are computed over all users. Group and individual Fair measures are computed in two steps: first, computing an Eff score per user/group as base score; and second, aggregating the base score between users/groups with Fair measure. and NDCG are used as base scores to represent setand rank-based measures. Group fairness. We compute all existing fairness measures for two or more user groups in RSs,7 that are published up to March 2025: Average scores of the worst 25% groups (Min [61]), Range [36], SD [36, 65], MAD [17], Gini [16, 18, 42], CV [68], FStat [59], KL [1], and GCE [8, 9]. We also compute the Atkinson Index (Atk [2]), an income inequality measure that considers within-group variations.8 The betweenand within-group fairness version of 4The full prompt templates and examples are provided in App. A.2. 5Metrics based on n-gram have been used to evaluate the performance of (conversational) RSs [48]. 6To our knowledge, no existing work has evaluated the effect of various similarity thresholds for this context. 7Measures that can only be used for exactly two groups are excluded. 8This measure can be transformed into Generalised Entropy [51, 53]. the measures are denoted as b-group and w-group respectively.9 We provide the measure formulations and technical details in App. A.3. Individual fairness. Fairness for individual users is quantified with SD [43], Gini [32], and Atk. The subscript ind indicates the individual fairness version of the measure. SD and Gini are the only Fair measures that have been used for both individual and group user fairness, while Atkind can be decomposed into betweenand within-group fairness with no residuals [3, 4, 6]. While other group Fair measures can also be used to measure individual fairness, their scores may not be informative, e.g., Min may be zero for most models, as having most users scoring P=0 or NDCG=0 is common."
        },
        {
            "title": "3 Empirical Analysis\nEvaluation of all LLMRecs. Eff scores and NDCG-based Fair scores\nfor group fairness and individual fairness of LLMRecs are shown\nin Tab. 2; the P-based Fair measures are presented in App. B.1 as\nthey show similar results. We evaluate group fairness considering\nthe intersectionality of all three sensitive user attributes.10\nEff and Fair scores. GLM-4-9B has the best Eff and Fair scores\nfor all datasets, except for JobRec, where all LLMRecs have equally\nlow Eff. This could be because the job titles are very specific and\ncontain more noise than movie/titles and artist names (e.g., ‚ÄòChicago\nConcierge Manager‚Äô, ‚ÄòAccount Executive - 40k - 70K per year‚Äô).\nGenerally, as group/individual Fair measures, Gini, Atk, and CV\nare fairer when NDCG is better; this is because their equations\ninclude division by the mean NDCG. As such, these measures can\ndistinguish group fairness between two models that have similar\nSDb-group/SDind but differ in NDCG (e.g., the sensitive and non-\nsensitive prompts of Ministral-8B for LFM-1B), or vice versa.\nComparison of sensitive and non-sensitive prompts. The\nEff and Fair scores between sensitive (S) and non-sensitive (NS)\nprompts are generally comparable, implying that including sensitive\nattributes in the prompt has little effect on effectiveness/fairness.\nAn exception to this is Ministral-8B for LFM-1B, which may be due\nto the LLM‚Äôs relatively high gender bias [5].\nGroup vs. individual fairness. For Atk/Gini, the fairest model\nfor intersectional groups and for individual users are always the\nsame, but this is not the case for SD. Thus, if sensitive attribute\ninformation is missing, Atkind/Giniind can estimate which model is\nthe fairest for intersectional groups.",
            "content": "Agreement of group and individual fairness measures (RQ1). Do individual and group Fair measures reach the same conclusion? If an individual Fair measure can rank models from the most to the least fair equivalently to group Fair measure, one can be proxy for the other. To assess measure agreement in ranking models, we compute Kendalls ùúè; we consider two rankings to be equivalent if ùúè 0.9 [37]. Fig. 2 shows the agreement of group and individual Fair measures. Our results show that no individual Fair measure consistently has equivalent rankings to any group Fair measures.11 For all datasets, group Fair measures tend to have weak-to-strong 9The term group fairness refers to between-group fairness; the latter is used when we compare fairness between and within groups. 10We find similar results for collaborative filtering-based RSs. The results were omitted for brevity and to focus on LLMRecs. 11We find similar results for the group-individual fairness agreement between the same family of measures (e.g., SDb-group vs SDind) for all possible groupings (App. B.2). RecSys 25, September 2226, 2025, Prague, Czech Republic Rampisela et al. Table 2: Effectiveness (Eff) and fairness (Fair) scores at cutoff ùëò = 10 for intersectional groups (Grp.) and for individuals (Ind.) of LLMRecs with non-sensitive (NS) and sensitive (S) prompts. All Fair scores are computed with NDCG. All measures range in [0,1], except for the Grp. measures below the grey lines. The best Eff/Fair scores are bolded. Darker green marks scores closer to the best Eff/Fair per measure. / means the higher/lower the better. LLMRec GLM-4-9B Llama-3.1-8B Ministral-8B Qwen2.5-7B prompt type NS NS NS NS E ) . ( F HR MRR NDCG Min Range SD MAD Gini Atk CV FStat KL GCE ) SD F . ( Gini Atk HR MRR NDCG Min Range SD MAD Gini Atk CV FStat KL GCE Gini Atk HR MRR NDCG Min Range SD MAD Gini Atk CV FStat KL GCE ) . ( F E ) . ( F ) SD F . I ( ) SD F . I ( Gini Atk 0.377 0.189 0.231 0.166 0.188 0.055 0.067 0.130 0.015 0.233 0.468 1.121 0.028 0.330 0.705 0.636 0.054 0.037 0.041 0.000 0.500 0.093 0.066 0.828 0.547 2.396 1.035 3.218 1685.926 0.183 0.956 0.948 0.658 0.409 0.462 0.240 0.604 0.149 0.138 0.162 0.002 0.363 2.188 2.645 338.842 0.380 0.462 0.361 0.358 0.174 0.215 0.137 0.208 0.061 0.076 0.161 0.020 0.285 0.714 1.138 0.050 0.320 0.721 0. 0.033 0.023 0.025 0.000 0.083 0.023 0.019 0.834 0.518 2.102 0.546 1.428 1979.103 0.147 0.974 0.969 0.661 0.408 0.463 0.273 0.604 0.130 0.121 0.139 0.002 0.309 1.809 2.783 225.899 0.378 0.461 0.358 0.260 0.101 0.140 0.086 0.356 0.088 0.091 0.255 0.036 0.540 0.841 1.674 0.112 0.262 0.799 0.751 0.044 0.017 0.024 0.000 0.500 0.086 0.053 0.857 0.677 2.886 1.613 3.979 1685.994 0.121 0.966 0.958 0.609 0.347 0.406 0.264 0.884 0.158 0.158 0.184 0.003 0.383 2.859 3.497 112.987 0.370 0.510 0.409 ML-1M 0.269 0.113 0.148 0.108 0.376 0.088 0.087 0.226 0.025 0.502 0.654 1.640 0.104 0.272 0.793 0.742 0.342 0.159 0.200 0.059 0.290 0.085 0.099 0.248 0.033 0.465 0.767 0.866 659.844 0.309 0.736 0.672 JobRec 0.021 0.009 0.012 0.000 0.500 0.082 0.035 0.939 0.834 4.549 2.137 5.861 2052.498 0.090 0.985 0.980 0.057 0.048 0.050 0.000 0.333 0.074 0.065 0.757 0.522 1.760 1.005 1.881 1612.574 0.211 0.949 0.944 LFM-1B 0.618 0.357 0.415 0.265 0.931 0.140 0.130 0.161 0.004 0.356 3.841 3.189 112.974 0.371 0.502 0.400 0.451 0.266 0.310 0.107 0.993 0.179 0.169 0.285 0.006 0.625 2.855 3.011 451.826 0.377 0.638 0.563 0.340 0.173 0.208 0.107 0.500 0.125 0.142 0.275 0.049 0.525 1.278 1.671 659.741 0.322 0.736 0.673 0.044 0.033 0.036 0.000 0.170 0.042 0.035 0.790 0.499 1.928 0.611 1.754 1759.177 0.174 0.963 0.957 0.317 0.174 0.208 0.058 1.000 0.178 0.157 0.358 0.013 0.841 3.382 3.508 451.875 0.334 0.750 0.694 0.363 0.165 0.207 0.068 0.328 0.093 0.112 0.255 0.053 0.460 1.645 1.198 0.239 0.308 0.721 0. 0.065 0.046 0.050 0.000 0.333 0.072 0.064 0.740 0.473 1.709 0.908 2.069 1539.278 0.204 0.947 0.937 0.375 0.199 0.241 0.090 0.525 0.117 0.125 0.262 0.007 0.510 2.337 2.878 451.822 0.342 0.703 0.638 0.371 0.180 0.221 0.091 0.274 0.077 0.090 0.195 0.039 0.365 1.220 1.063 0.198 0.324 0.715 0.644 0.057 0.045 0.048 0.000 0.333 0.071 0.056 0.809 0.500 2.107 0.928 2.288 1759.185 0.203 0.951 0.944 0.317 0.160 0.198 0.043 0.331 0.091 0.103 0.298 0.017 0.545 4.385 2.318 564.765 0.320 0.753 0.695 agreement with Giniind/Atkind and have weak-to-strong disagreement with SDind. The only group Fair measure that always correlates strongly to an individual Fair measure is CV (ùúè [0.64, 0.79]). This may be due to the measures similar formulation: Giniind/Atkind have division by the mean Eff scores across all users, while CV has division by the mean of mean Eff score per group. In summary, no existing individual Fair measures make reliable proxy for group Fair measures, hence the need to evaluate for both. Intersectional fairness (RQ2). As we consider more intersectional attributes to form user groups, the number of groups grows. We posit that achieving intersectional group fairness is harder than for non-intersectional groups as it requires having similar Eff scores for larger number of groups. Likewise, individual fairness (i.e., having group for each user) would be harder to achieve than Figure 2: Agreement (Kendalls ùúè) of NDCG-based measures for individual Fair (ùë¶-axis) and group Fair (ùë•-axis) in ranking LLMRecs. User groups are based on 3 sensitive attributes. Due to 8-way ties, ùúè cannot be computed for Min (JobRec). group fairness. We study: (i) if/how the number of attributes used for grouping affect fairness; and (ii) the difference between individual and (intersectional) group Fair scores. To this end, we compute NDCG-based individual Fair scores across all users, and average NDCG-based group Fair scores across all ways of grouping users when considering only 1, 2, or 3 attribute(s) at once. SD, Gini, and Atk are computed, as they have been used as both group and individual Fair measures. We evaluate the runs from GLM-4-9B (NS) due to their relatively good NDCG and Fair scores across all datasets. The results are shown in Fig. 3.12 Regarding (i), our results verify that fairness worsens as more attributes are used to form groups, highlighting the importance of considering intersectionality in fairness evaluation. Note that JobRec has higher Gini/Atk than ML-1M/LFM-1B as its NDCG is much lower; Gini/Atk accounts for this, but SD does not. Regarding (ii), we find that even when the recommendations are relatively fair for (intersectional) groups, they can be much less fair for individuals (i.e., for ML-1M and LFM-1B). For all measures and datasets, the individual Fair scores are always worse than group Fair scores. These findings imply that group Fair scores may mask unfairness within groups, even if the within-group variation is considered (e.g., in Atkb-group). 12While we present the measures in the same plot, their distribution differs, so the figures should not be used to quantify the gap between two families of measures. Stairway to Fairness: Connecting Group and Individual Fairness RecSys 25, September 2226, 2025, Prague, Czech Republic fairness tends to remain stable, except for SD of JobRec, where it worsens. Regarding (ii), we see that within-group unfairness is almost as high as individual unfairness, and always higher than between-group unfairness. Sometimes, within-group unfairness slightly exceeds individual unfairness (e.g., SD for JobRec). In summary, for different ways of grouping users, within-group fairness is consistently worse than between-group fairness. Yet, individual fairness is generally comparable to within-group fairness, regardless of how the users are grouped."
        },
        {
            "title": "4 Discussion and Conclusion\nRS fairness evaluation often focuses exclusively on group fairness\nand less on individual fairness or both fairness types [11]. We em-\npirically studied the relationship between group and individual\nfairness in RSs. Our results show that RSs which are fair for groups\ncan still be very unfair for individual users, providing the first\nempirical evidence on the disjointness of these two RS fairness con-\ncepts. Although our experiments are centred on user fairness, we\nexpect similar findings for item (provider) fairness evaluation, con-\nsidering that measures of user fairness may exhibit similarities to\nitem fairness measures in how scores are aggregated across groups\nor individuals. Moreover, our findings can be applied directly to\nreal-world RS design and fairness mitigation strategies, for example,\nby incorporating both between-group and within-group fairness\nterms in the loss function.",
            "content": "Overall, we encourage evaluating individual and within-group fairness alongside group fairness, as between-group Fair scores may mask huge disparity in RS effectiveness across users, even for large number of groups (i.e., 7% of the number of users) and even when the between-group Fair score accounts for within-group variations. Future work will study how strategies that mitigate either group or individual fairness affect each fairness type. Acknowledgments This work is supported by the Algorithms, Data, and Democracy project (ADD-project), funded by the Villum Foundation and Velux Foundation. This work is also supported in part by the Australian Research Council (DP190101113). We thank the anonymous reviewers who have provided helpful feedback to improve an earlier version of the manuscript. References [1] Enrique Amig√≥, Yashar Deldjoo, Stefano Mizzaro, and Alejandro Bellog√≠n. 2023. unifying and general account of fairness measurement in recommender systems. Information Processing & Management 60, 1 (1 2023), 103115. https://doi.org/10. 1016/J.IPM.2022.103115 [2] Anthony Atkinson. 1970. On the measurement of inequality. Journal of Economic Theory 2, 3 (1970), 244263. https://doi.org/10.1016/0022-0531(70) 90039-6 [3] Charles Blackorby, Walter Bossert, and David Donaldson. 1999. Income Inequality Measurement: The Normative Approach. Springer Netherlands, Dordrecht, 133 161. https://doi.org/10.1007/978-94-011-4413-1_4 [4] Francois Bourguignon. 1979. Decomposable Income Inequality Measures. Econometrica 47, 4 (1979), 901920. http://www.jstor.org/stable/1914138 [5] Team Cohere. 2025. Command A: An Enterprise-Ready Large Language Model. arXiv:2504.00698 [cs.CL] https://arxiv.org/abs/2504.00698 [6] Meltem Dayioƒülu and Cem Ba≈ülevent. 2006. Imputed Rents and Regional Income Inequality in Turkey: Subgroup Decomposition of the Atkinson Index. Regional Studies 40, 8 (2006), 889905. https://doi.org/10.1080/00343400600984395 [7] Yashar Deldjoo. 2024. Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. ACM Trans. Recomm. Figure 3: NDCG-based Group (Grp) and individual (Ind) Fair scores of GLM-4-9B (NS-prompt). Grp (ùëé) is the mean Fair scores across all ways of grouping users when considering ùëé attribute(s). For ùëé {1, 2}, the # of ways to group users is three each. For ùëé = 3, there is only one way to group users. Figure 4: NDCG-based individual, betweenand within-group unfairness of GLM-4-9B (NS-prompt) for all ways of grouping users in JobRec and LFM-1B. Fairness decomposability (RQ3 & RQ4). When fairness is evaluated only between groups, within-group unfairness may occur undetected. As the number of groups increases, we analyse: (i) how betweenand withingroup fairness change; and (ii) how they relate to individual fairness. To this end, we compute betweenand within-group fairness, as well as individual fairness with SD, Gini, and Atk on the NDCG scores from GLM-4-9B (NS). Results for JobRec and LFM-1B are shown in Fig. 4; we find similar trends for ML-1M (see App. B.3). Regarding (i), we find that while between-group fairness generally worsens as the number of groups grows,13 within-group 13Resembling stairs; hence, the title of the paper. RecSys 25, September 2226, 2025, Prague, Czech Republic Rampisela et al. Syst. (Aug. 2024). https://doi.org/10.1145/3690655 Just Accepted. [8] Yashar Deldjoo, Vito Walter Anelli, Hamed Zamani, Alejandro Bellog√≠n, and Tommaso Di Noia. 2019. Recommender Systems Fairness Evaluation via Generalized Cross Entropy. In Proceedings of the Workshop on Recommendation in Multi-stakeholder Environments co-located with the 13th ACM Conference on Recommender Systems (RecSys 2019). CEUR-WS. [9] Yashar Deldjoo, Vito Walter Anelli, Hamed Zamani, Alejandro Bellog√≠n, and Tommaso Di Noia. 2021. flexible framework for evaluating user and item fairness in recommender systems. User Modeling and User-Adapted Interaction 31 (2021), 457511. https://doi.org/10.1007/s11257-020-09285-1 [10] Yashar Deldjoo and Tommaso Di Noia. 2025. CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System. ACM Trans. Intell. Syst. Technol. (March 2025). https://doi.org/10.1145/3725853 Just Accepted. [11] Yashar Deldjoo, Dietmar Jannach, Alejandro Bellogin, Alessandro Difonzo, and Dario Zanzonelli. 2024. Fairness in recommender systems: research landscape and future directions. User Modeling and User-Adapted Interaction 34, 1 (2024), 59108. https://doi.org/10.1007/s11257-023-09364-z [12] Yashar Deldjoo and Fatemeh Nazary. 2024. Normative Framework for Benchmarking Consumer Fairness in Large Language Model Recommender System. arXiv:2405.02219 [cs.IR] https://arxiv.org/abs/2405.02219 [13] Virginie Do. 2023. Fairness in recommender systems: insights from social choice. https://theses.hal.science/telTheses. Universit√© Paris sciences et lettres. [14] Michael Ekstrand, Mucun Tian, Ion Madrazo Azpiazu, Jennifer Ekstrand, Oghenemaro Anuyah, David McNeill, and Maria Soledad Pera. 2018. All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research, Vol. 81), Sorelle Friedler and Christo Wilson (Eds.). PMLR, 172186. https://proceedings.mlr.press/v81/ekstrand18b.html [15] Eurostat. 2018. Young people on the labour market - statistics. https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Young_ people_on_the_labour_market_-_statistics#Half_worked_while_studying [16] Andres Ferraro, Michael D. Ekstrand, and Christine Bauer. 2024. Its Not You, Its Me: The Impact of Choice Models and Ranking Strategies on Gender Imbalance in Music Recommendation. In Proceedings of the 18th ACM Conference on Recommender Systems (Bari, Italy) (RecSys 24). Association for Computing Machinery, New York, NY, USA, 884889. https://doi.org/10.1145/3640457.3688163 [17] Zuohui Fu, Yikun Xian, Ruoyuan Gao, Jieyu Zhao, Qiaoying Huang, Yingqiang Ge, Shuyuan Xu, Shijie Geng, Chirag Shah, Yongfeng Zhang, and Gerard de Melo. 2020. Fairness-Aware Explainable Recommendation over Knowledge Graphs. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR 20). Association for Computing Machinery, New York, NY, USA, 6978. https://doi.org/10.1145/3397271. [18] Avijit Ghosh, Tomo Lazovich, Kristian Lum, and Christo Wilson. 2024. Reducing Population-level Inequality Can Improve Demographic Group Fairness: Twitter Case Study. arXiv:2409.08135 [cs.SI] https://arxiv.org/abs/2409.08135 [19] Corrado Gini. 1912. Variabilit√† Mutabilit√†. C. Cuppini, Bologna. Contributo allo Studio delle Distribuzioni delle Relazioni Statistiche. [20] Team GLM. 2024. ChatGLM: Family of Large Language Models from GLM-130B to GLM-4 All Tools. arXiv:2406.12793 [21] Elizabeth G√≥mez, David Contreras, Ludovico Boratto, and Maria Salamo. 2024. AMBAR: dataset for Assessing Multiple Beyond-Accuracy Recommenders. In Proceedings of the 18th ACM Conference on Recommender Systems (Bari, Italy) (RecSys 24). Association for Computing Machinery, New York, NY, USA, 137147. https://doi.org/10.1145/3640457. [22] Aaron Grattafiori et al. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 org/10.1145/582415.582418 [28] Chumeng Jiang, Jiayin Wang, Weizhi Ma, Charles L. A. Clarke, Shuai Wang, Chuhan Wu, and Min Zhang. 2025. Beyond Utility: Evaluating LLM as Recommender. In Proceedings of the ACM on Web Conference 2025 (Sydney NSW, Australia) (WWW 25). Association for Computing Machinery, New York, NY, USA, 38503862. https://doi.org/10.1145/3696410.3714759 [29] Karen Sparck Jones. 1972. statistical interpretation of term specificity and its application in retrieval. Journal of Documentation 28, 1 (1972), 1121. https: //doi.org/10.1108/EB026526/FULL/PDF [30] Walid Krichene and Steffen Rendle. 2022. On sampled metrics for item recommendation. Commun. ACM 65, 7 (2022), 7583. https://doi.org/10.1145/3535335 [31] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz, Germany) (SOSP 23). Association for Computing Machinery, New York, NY, USA, 611626. https://doi.org/10.1145/3600006.3613165 [32] Jurek Leonhardt, Avishek Anand, and Megha Khosla. 2018. User Fairness in Recommender Systems. In Companion Proceedings of the The Web Conference 2018 (Lyon, France) (WWW 18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 101102. https: //doi.org/10.1145/3184558.3186949 [33] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang Liu, and Yongfeng Zhang. 2023. Fairness in Recommendation: Foundations, Methods, and Applications. ACM Trans. Intell. Syst. Technol. 14, 5, Article 95 (Oct. 2023), 48 pages. https://doi.org/10.1145/3610302 [34] Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, and Xing Xie. 2024. RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems. In Companion Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW 24). Association for Computing Machinery, New York, NY, USA, 10311034. https://doi.org/10.1145/3589335.3651242 [35] Tingting Liang, Chenxin Jin, Lingzhi Wang, Wenqi Fan, Congying Xia, Kai Chen, and Yuyu Yin. 2024. LLM-REDIAL: Large-Scale Dataset for Conversational Recommender Systems Created from User Behaviors with LLMs. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 89268939. https://doi.org/10.18653/v1/2024.findings-acl.529 [36] Zhiqiang Liu, Xiaoxiao Xu, Jiaqi Yu, Han Xu, Lantao Hu, Han Li, and Kun Gai. 2024. Self-Adaptive Fairness Constraint Framework for Industrial Recommender System. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (Boise, ID, USA) (CIKM 24). Association for Computing Machinery, New York, NY, USA, 47264733. https: //doi.org/10.1145/3627673.3680099 [37] Maria Maistro, Lucas Chaves Lima, Jakob Grue Simonsen, and Christina Lioma. 2021. Principled Multi-Aspect Evaluation Measures of Rankings. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (Virtual Event, Queensland, Australia) (CIKM 21). Association for Computing Machinery, New York, NY, USA, 12321242. https://doi.org/10.1145/3459637. [38] Elizaveta Makhneva, Anna Sverkunova, Oleg Lashinin, Marina Ananyeva, and Sergey Kolesnikov. 2023. Make your next item recommendation model time sensitive. In Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization (UMAP 23 Adjunct). Association for Computing Machinery, New York, NY, USA, 191195. https://doi.org/10.1145/3563359.3596980 [39] Zaiqiao Meng, Richard McCreadie, Craig MacDonald, and Iadh Ounis. 2020. Exploring Data Splitting Strategies for the Evaluation of Recommendation Models. In RecSys 2020 - 14th ACM Conference on Recommender Systems. Association for Computing Machinery, Inc, Virtual Event, Brazil, 681686. https://doi.org/10. 1145/3383313.3418479 [23] Ben Hamner, Road Warrior, and Wojciech Krupa. 2012. Job Recommendation [40] Office for National Statistics. 2023. Age classifications: Census 2021. Challenge. https://kaggle.com/competitions/job-recommendation. [24] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (Dec. 2015), 19 pages. https://doi.org/10.1145/ [25] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian Mcauley. 2023. Large Language Models as Zero-Shot Conversational Recommenders. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (Birmingham, United Kingdom) (CIKM 23). Association for Computing Machinery, New York, NY, USA, 720730. https://doi.org/10.1145/3583780.3614949 [26] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024. Large Language Models are Zero-Shot Rankers for Recommender Systems. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Vol. 14609 LNCS. Springer-Verlag, Berlin, Heidelberg, 364381. https://link. springer.com/10.1007/978-3-031-56060-6_24 [27] Kalervo J√§rvelin and Jaana Kek√§l√§inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422446. https://doi. https://www.ons.gov.uk/census/census2021dictionary/variablesbytopic/ demographyvariablescensus2021/age/classifications [41] Dario Di Palma, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2024. Evaluating ChatGPT as Recommender System: Rigorous Approach. arXiv:2309.03613 [cs.IR] https://arxiv.org/abs/2309.03613 [42] Eliana Pastor and Francesco Bonchi. 2024. Intersectional fair ranking via subgroup divergence. Data Min. Knowl. Discov. 38, 4 (May 2024), 21862222. https://doi. org/10.1007/s10618-024-01029-8 [43] Gourab Patro, Arpita Biswas, Niloy Ganguly, Krishna P. Gummadi, and Abhijnan Chakraborty. 2020. FairRec: Two-Sided Fairness for Personalized Recommendations in Two-Sided Platforms. In Proceedings of The Web Conference 2020 (Taipei, Taiwan) (WWW 20). Association for Computing Machinery, New York, NY, USA, 11941204. https://doi.org/10.1145/3366423. [44] Giovanni Pellegrini, Vittorio Maria Faraco, and Yashar Deldjoo. 2023. Fairness for All: Investigating Harms to Within-Group Individuals in Producer Fairness Re-ranking Optimization Reproducibility Study. arXiv:2309.09277 [cs.IR] https://arxiv.org/abs/2309.09277 Stairway to Fairness: Connecting Group and Individual Fairness RecSys 25, September 2226, 2025, Prague, Czech Republic [45] Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, and Christina Lioma. 2024. Evaluation Measures of Individual Item Fairness for Recommender Systems: Critical Study. ACM Trans. Recomm. Syst. 3, 2, Article 18 (Nov. 2024), 52 pages. https://doi.org/10.1145/3631943 [46] Theresia Veronika Rampisela, Tuukka Ruotsalo, Maria Maistro, and Christina Lioma. 2024. Can We Trust Recommender System Fairness Evaluation? The Role of Fairness and Relevance. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 24). Association for Computing Machinery, New York, NY, USA, 271281. https: //doi.org/10.1145/3626772.3657832 [47] Bashir Rastegarpanah, Krishna P. Gummadi, and Mark Crovella. 2019. Fighting Fire with Fire: Using Antidote Data to Improve Polarization and Fairness of Recommender Systems. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (Melbourne VIC, Australia) (WSDM 19). Association for Computing Machinery, New York, NY, USA, 231239. https://doi.org/10.1145/3289600. [48] Mathieu Ravaut, Hao Zhang, Lu Xu, Aixin Sun, and Yong Liu. 2024. ParameterEfficient Conversational Recommender System as Language Processing Task. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, St. Julians, Malta, 152 165. https://doi.org/10.18653/v1/2024.eacl-long.9 [49] Markus Schedl. 2016. The LFM-1b Dataset for Music Retrieval and Recommendation. In Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval (ICMR 16). Association for Computing Machinery, New York, NY, USA, 103110. https://doi.org/10.1145/2911996.2912004 [50] Tobias Schumacher, Marlene Lutz, Sandipan Sikdar, and Markus Strohmaier. 2025. Properties of Group Fairness Measures for Rankings. Trans. Soc. Comput. 8, 12, Article 2 (Jan. 2025), 45 pages. https://doi.org/10.1145/3674883 [51] A. F. Shorrocks. 1980. The Class of Additively Decomposable Inequality Measures. Econometrica 48, 3 (1980), 613625. http://www.jstor.org/stable/1913126 [52] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of Exposure in Rankings. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD 18). Association for Computing Machinery, New York, NY, USA, 22192228. https://doi.org/10.1145/ 3219819.3220088 [53] Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna P. Gummadi, Adish Singla, Adrian Weller, and Muhammad Bilal Zafar. 2018. Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual & Group Unfairness via Inequality Indices. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD 18). Association for Computing Machinery, New York, NY, USA, 22392248. https://doi.org/10.1145/3219819.3220046 [54] Mistral AI team. 2024. Un Ministral, des Ministraux. https://mistral.ai/news/ ministraux. Accessed: 2025-04-23. [55] Qwen Team. 2024. Qwen2.5: Party of Foundation Models. https://qwenlm. github.io/blog/qwen2.5/ [56] Antonela Tommasel. 2024. Fairness Matters: look at LLM-generated group recommendations. In 18th ACM Conference on Recommender Systems. ACM, New York, NY, USA, 993998. https://doi.org/10.1145/3640457.3688182 [57] U.S. Bureau of Labor Statistics. 2023. May 2023 National Occupational Employment and Wage Estimates. https://www.bls.gov/oes/current/oes_nat.htm#top [58] U.S. Bureau of Labor Statistics. 2024. College Enrollment and Work Activity of Recent High School and College Graduates Summary. https://www.bls.gov/ news.release/hsgec.nr0.htm [59] Mengting Wan, Jianmo Ni, Rishabh Misra, and Julian McAuley. 2020. Addressing Marketing Bias in Product Recommendations. In Proceedings of the 13th International Conference on Web Search and Data Mining (Houston, TX, USA) (WSDM 20). Association for Computing Machinery, New York, NY, USA, 618626. https://doi.org/10.1145/3336191.3371855 [60] Yifan Wang, Weizhi Ma, Min Zhang, Yiqun Liu, and Shaoping Ma. 2023. Survey on the Fairness of Recommender Systems. ACM Trans. Inf. Syst. 41, 3 (2 2023), 143. https://doi.org/10.1145/3547333 [61] Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang, and Shaoping Ma. 2024. Intersectional Two-sided Fairness in Recommendation. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW 24). Association for Computing Machinery, New York, NY, USA, 36093620. https://doi.org/10.1145/3589334.3645518 [62] Yan Wen, Chen Gao, Lingling Yi, Liwei Qiu, Yaqing Wang, and Yong Li. 2023. Efficient and Joint Hyperparameter and Architecture Search for Collaborative Filtering. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 23). Association for Computing Machinery, New York, NY, USA, 25472558. https://doi.org/10.1145/3580305.3599322 [63] Yao Wu, Jian Cao, Guandong Xu, and Yudong Tan. 2021. TFROM: Two-sided Fairness-Aware Recommendation Model for Both Customers and Providers. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR 21). Association for Computing Machinery, New York, NY, USA, 10131022. https: //doi.org/10.1145/3404835.3462882 [64] Chen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun Xu, and Tat-Seng Chua. 2024. Study of Implicit Ranking Unfairness in Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 79577970. https://aclanthology.org/2024.findings-emnlp. [65] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems (Singapore, Singapore) (RecSys 23). Association for Computing Machinery, New York, NY, USA, 993999. https://doi.org/10.1145/ 3604915.3608860 [66] Kesen Zhao, Shuchang Liu, Qingpeng Cai, Xiangyu Zhao, Ziru Liu, Dong Zheng, Peng Jiang, and Kun Gai. 2023. KuaiSim: comprehensive simulator for recommender systems. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 1945, 18 pages. [67] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, Yingqian Min, Zhichao Feng, Xinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang, and Ji Rong Wen. 2021. RecBole: Towards Unified, Comprehensive and Efficient Framework for Recommendation Algorithms. In International Conference on Information and Knowledge Management, Proceedings. ACM, New York, NY, USA, 46534664. https://doi.org/10.1145/3459637.3482016 [68] Ziwei Zhu, Jianling Wang, and James Caverlee. 2020. Measuring and Mitigating Item Under-Recommendation Bias in Personalized Ranking Systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR 20). Association for Computing Machinery, New York, NY, USA, 449458. https: //doi.org/10.1145/3397271."
        }
    ],
    "affiliations": [
        "LUT University, Lahti, Finland",
        "RMIT University, Melbourne, Australia",
        "University of Copenhagen, Copenhagen, Denmark"
    ]
}