{
    "paper_title": "MADrive: Memory-Augmented Driving Scene Modeling",
    "authors": [
        "Polina Karpikova",
        "Daniil Selikhanovych",
        "Kirill Struminsky",
        "Ruslan Musaev",
        "Maria Golitsyna",
        "Dmitry Baranchuk"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/"
        },
        {
            "title": "Start",
            "content": "MADrive: Memory-Augmented Driving Scene Modeling POLINA KARPIKOVA, Yandex DANIIL SELIKHANOVYCH, Yandex Research, HSE University, Skoltech KIRILL STRUMINSKY, Yandex Research, HSE University RUSLAN MUSAEV, Yandex, Skoltech MARIA GOLITSYNA, Yandex DMITRY BARANCHUK, Yandex Research 5 2 0 2 6 2 ] . [ 1 0 2 5 1 2 . 6 0 5 2 : r Fig. 1. MADrive reconstructs 3D driving scene from training frames (Left) and replaces partially observed vehicles in the scene with realistically reconstructed counterparts retrieved from MAD-Cars, our novel multi-view auto dataset. MADrive enables high-fidelity modeling of future scene views (Middle-left vs. Middle-right) and supports simulation of alternative scenarios, advancing novel-view synthesis in dynamic environments (Right). Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from large-scale external memory bank. Specifically, we release MAD-Cars, curated dataset of 70K 360 car videos captured in the wild and present retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/ Additional Key Words and Phrases: Autonomous driving, driving scene reconstruction These authors contributed equally to this research. Authors Contact Information: Polina Karpikova, p.karpikova@yandex.ru, Yandex; Daniil Selikhanovych, selikhanovychdaniil@gmail.com, Yandex Research, HSE University, Skoltech; Kirill Struminsky, kstruminsky@yandex-team.ru, Yandex Research, HSE University; Ruslan Musaev, rusamus19@yandex-team.ru, Yandex, Skoltech; Maria Golitsyna, marygolitsyna@yandex-team.ru, Yandex; Dmitry Baranchuk, dbaranchuk@ yandex-team.ru, Yandex Research."
        },
        {
            "title": "1\nAutonomous driving (AD) is one of the key areas in computer vision,\nrequiring extensive and costly data collection [6, 7, 16, 39, 46] to\ntrain accurate and robust perception and planning models [3, 15, 20].\nDriving simulators [43, 50, 55] aim at offering a powerful alternative\nby enabling the generation of highly realistic novel views and rare\nscenarios, especially safety-critical ones that are too dangerous\nor impractical to capture in the wild. Accurately modeling such\nscenarios is essential to mitigate domain shift during training, which\ncould otherwise lead to failures in deployments. In addition, when\nfailures do occur on real roads, it becomes important to “debug” the\ndriving policy and “replay” the scenario to identify and address the\nroot cause.",
            "content": "Recent advances in multi-view reconstruction and novel view synthesis [25, 28, 53] provide foundation for developing highly realistic driving simulators [55], designed to faithfully replicate real-world scenes. Such solutions can be used for real-time, controllable simulations that preserve the visual domain of real-world data, unlike game engine-based simulations, which often introduce significant domain shifts. Modern driving scene reconstruction methods [27, 48, 56] have achieved impressive photo-realism in rendering the observed views, while also enabling the simulation of slight vehicle trajectory deviations, e.g., lane changes [27, 56]. However, since novel view synthesis is limited to the geometry observed in the data, these methods still struggle to reconstruct entire vehicles from sparse observations and, 2 Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, and Dmitry Baranchuk thereby, simulate complex driving scenarios far beyond the original camera views. To overcome this limitation, we introduce MADrive, memoryaugmented driving scene reconstruction approach. Specifically, instead of reconstructing vehicles from sparse views, we replace them with similar 3D car models retrieved from an external database. To faithfully adapt these models to the surrounding scene, we further propose physically-based relighting and insertion techniques, resulting in visually consistent novel driving scene views. Our method is motivated by the assumption that the variety of car models, types, and colors is relatively limited [44], making it feasible to build dataset that covers the majority of cars typically seen on the road. To this end, we present dataset containing 360 view sequences of 70, 000 cars, curated from online sale advertisements. Furthermore, leveraging integrated high-fidelity car models enables more challenging evaluation setting. Previous solutions for driving scene reconstruction mainly focus on replicating or editing unseen intermediate frames [27, 48, 55, 56]. Our work instead considers driving scene extrapolation predicting the future appearance of vehicles based on sequence of past views. We show that MADrive can faithfully render diverse range of plausible vehicle trajectories, offering foundation for simulation applications that model alternative outcomes of real driving scenarios. In summary, the contributions are as follows. We propose MADrive Memory-Augmented Driving scene reconstruction framework aimed at realistic synthesis of diverse and complex driving scenarios. Given sparse car views in the scene, MADrive retrieves similar vehicles from car video database, reconstructs them into high-quality, relightable 3D assets, and naturally integrates them into the scene, replacing the original cars. We present MAD-Cars, Multi-view Auto Dataset curated, large-scale collection of 360 car videos. It comprises 70, 000 car instances with diverse brands, models, colors, and lighting conditions, significantly expanding the scope of existing public multi-view car datasets. We describe novel evaluation setting for assessing driving scene reconstruction methods in significantly altered views, and show that MADrive produces more realistic renderings, as evidenced by reduced performance degradation in downstream perception tasks."
        },
        {
            "title": "2 Related Work\nDynamic Urban Scene Reconstruction. Recent dynamic 3D\nscene reconstruction methods increasingly adopt 3D Gaussian Splat-\nting [26] as an efficient and expressive scene representation [9, 45, 51,\n52]. Several approaches [StreetGS [48], AutoSplat [27], HUGS [56]]\nadopt these methods to driving scene modeling and decompose the\nscenes into a static background and foreground vehicles, placed\nin the scene using 3D bounding boxes derived from tracking data.\nThese methods also propose various modifications to improve driv-\ning scene reconstruction and novel view synthesis. Both HUGS and\nAutoSplat represent the ground as a plane of 2D splats. HUGS further\nleverages additional information (optical flow and semantic segmen-\ntation) to guide splat optimization and introduces a method for",
            "content": "realistic shadow placement. AutoSplat [27] improves car reconstruction from limited viewpoints by exploiting the bilateral symmetry of vehicles to augment side views and by employing more accurate splat initialization via an image-to-3D model [35]. Despite these advances, accurately reconstructing the full appearance of vehicle in the scene, particularly from sparse or occluded views, remains substantial challenge. 3D Car Datasets. Several public datasets provide 3D car assets. Early collections such as SRN-Car [8] and Objaverse-Car [11] contain CAD models that deviate significantly from real-world cars in terms of texture realism and geometric details. More recent efforts [12, 54] have focused on real captured 3D car datasets. MVMC [54] includes 576 cars, each with an average of 10 views. 3DRealCar [12] provides 2, 500 car instances, each with 200 dense high-resolution RGB-D views. In contrast, MAD-Cars includes 70, 000 360 car videos at comparable resolution and average number of views as 3DRealCar, thereby offering substantially greater generalization and diversity. Novel View Synthesis with External 3D Car Assets. HUGSim [55] builds closed-loop AD simulator by inserting 3D car models from 3DRealCars [12]. In contrast, we replace observed vehicles in real scenarios with retrieved counterparts, enabling extrapolation and rollout of actual driving situations. Several approaches leverage CAD models for scene representation [2, 13, 18, 41, 43], but these assets often differ notably from real vehicles. To improve realism, some methods apply geometry tuning [13, 41, 43], whereas UrbanCAD [31] retrieves similar CAD models and refines their textures and lighting to better match the scene while preserving CAD-level controllability. However, the obtained models still have noticeable gap in realism and correspondence to actual cars. Meanwhile, MADrive retrieves real cars instances from largescale database spanning diverse brands, models, materials, colors, and lighting conditions aiming at closing the realism gap while preserving accurate scene alignment. Relighting. Given set of input views, scene reconstruction approaches based on radiance fields recover the outgoing radiance along with scene geometry. The radiance field depends on the scenes lighting and varies when an object is placed in different context. In general, the outgoing radiance is governed by the rendering equation [22]. An exact solution to scene relighting would involve modeling light propagation via ray tracing. Although some recent works introduce solutions for efficient ray tracing [5, 17, 33, 47], relighting remains beyond the scope of their work. As an alternative, few recent works propose relighting solutions based on approximations to the rendering equation used in real-time graphics. GaussianShader [21] relies on split-sum approximation [24] to model lighting effects with PBR materials. LumiGauss [23] uses an alternative approach based on spherical harmonics [37] with diffuse materials and an ambient shadow approximation. Our work relies on similar approach to LumiGauss and adapts cars from the external dataset to new lighting environment. Our relighting procedure requires an environmental map. In [29], DilPIR employs generative model to infer it using gradientbased procedure. In turn, we estimate the environmental map using training frames with [36] without additional training loops. MADrive: Memory-Augmented Driving Scene Modeling 3 : Fig. 2. MADrive Overview. Given an input frame sequence, our retrieval scheme finds similar vehicles in an external database (Left). The 3D reconstruction pipeline then produces detailed vehicle models from the retrieved videos. The vehicles are represented with relightable 2D Gaussian splats. Opacity masks are used to remove background splats. The model geometry is regularized with external normals maps. (Middle) Finally, the reconstructed vehicles are adapted to the scenes lighting conditions and composed with the background to produce the overall scene representation (Right)."
        },
        {
            "title": "3.1 Driving Scene Reconstruction\nFollowing [48], we decompose the scene into static and dynamic\ncomponents. The static component can be reconstructed based on\nthe video from the moving vehicle. The movement parallax and the\navailability of depth sensor data allow to recover the scene structure.\nWe adapt the approach from Street Gaussians [27] to represent\nthe static component of the scene consisting of three parts: ground,\nsurroundings, and sky. We parameterize the surroundings with 3D\nGaussian Splats [25]. We represent the ground part of the scene\nwith horizontal 2D Gaussian splats. We avoid distance estimation\nambiguities by putting the sky at an infinite distance and blending\nit into the scene at the last step.",
            "content": "The dynamic component includes all moving vehicles in the scene. For simplicity, we treat cars labeled as stationary in the dataset metadata as part of the static component. In general, there are two challenges in estimating and modeling the dynamic part of the scene. First, it requires accounting for compound motion. Second, observations often capture only limited portion of dynamic object. For example, predicting vehicles side turn is difficult if its appearance from certain angles was never observed. In line with Gaussian splatting-based urban driving scene modeling works [27, 48], we represent observed vehicles as static Gaussian splats within the corresponding moving bounding box to model the compound motion in the scene. To obtain the static part of the scene, we initialize both static and dynamic parts with LIDAR data and train the splats with photometric loss. During inference, we reuse the static part of the scene. At the same time, we replace moving vehicles with 3D car models extracted from bank of cars using the retrieval-based approach, described in the next section. This substitution allows obtaining high-quality renders for configurations significantly diverging from the ones observed during training."
        },
        {
            "title": "3.2 Retrieval and Car Reconstruction\nWe propose reconstructing the dynamic part of the scene with\na retrieval-based approach. Specifically, we first extract crops of\nthe moving cars observed in the scene and find the similar car\ninstances in the database of multi-view car captures. Then, given\nthe retrieved images, we construct photorealistic 3D car models and\nreplace the original cars in the scene with the obtained 3D assets.\nDespite the limited car visibility in the scene, retrieval-augmented\nreconstruction enables faithful 3D car reconstruction even from a\nsingle frame.\nRetrieval Details. To produce a retrieval query, we compute a mask\nby projecting the 3D bounding box of a car onto an image plane.\nAfter filtering out small masks and overlapping masks, we use the\nremaining ones to extract image crops containing individual cars.\nFor each crop, we compute an image embedding using SigLIP2 [40]\nand extract the car color using Qwen2.5-VL [49]. This color cue\ncomplements the image features, which tend to focus more on brand\nand car type, as observed in our experiments. To retrieve the car\ninstance in the database, we first collect database entries with similar\ncolor and then select the one with the closest image embedding.\nOnce matches are found, we reconstruct the corresponding 3D car",
            "content": "4 Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, and Dmitry Baranchuk Fig. 3. MAD-Cars Analysis. Memory-bank statistics on colors (Left), car types (Middle) and lighting conditions (Right). models using the associated multi-view image sets. The following section details our reconstruction procedure. Relightable Car Models. We start by specifying the representation, used to model vehicles. By default, Gaussian splatting approximates the radiance field observed in the training frames as whole. In our setup, we need to decompose between lights and materials to facilitate model insertion into environments with different lighting conditions. We achieve this by adopting relighting strategy based on spherical harmonic lighting [23, 37]. We use two-dimensional modification of Gaussian splats [19]. It approximates the 3D model with collection of flat Gaussian splats parameterized with location 𝜇 R3, orientation matrix 𝑅 𝑆𝑂 (3) and two scale parameters 𝜎𝑥 , 𝜎𝑦 R2. Unlike 3D splats, 2D splats have well-defined surface normals 𝑛(𝑅) required for surface relighting effects. We assume that the model has diffuse surface. As result, our model requires only three parameters to define the color of the Gaussian via the albedo 𝜌 R3. Although there are approaches that allow modeling specular reflections of metallic surfaces with Gaussian splatting [21], they were not as robust in our preliminary experiments. Also, we assume that the light comes from distant environmental map 𝐿(𝜔). These two assumptions allow efficiently computing the color of car within different setups with an environmental map representation based on spherical harmonics [37]. In particular, the outgoing radiance is the product of the surface albedo 𝜌 (𝜇) and the irradiance 𝐸 (𝑛) 𝐵(𝜇, 𝑛) = 𝜌 (𝜇)𝐸 (𝑛), (1) where 𝜇 is the spatial location (i.e., Gaussian mean) and 𝑛 is the surface normal. The irradiance aggregates the light 𝐿(𝜔) coming to 𝜇 from the upper hemisphere Ω(𝑛) directions 𝜔: 𝐸 (𝑛) = Ω (𝑛) 𝐿(𝜔)(𝑛 𝜔)d𝜔. (2) To compute 𝐸 (𝑛), we parameterize 𝐿(𝜔) with spherical harmonics and approximate the integral with Eq. 12 from [37]. Car Reconstruction Details. Next, we specify the details of the reconstruction algorithm used for the model representation above. First, we approximate the environmental map using the training images background. Crucially, while reconstructing the car, we freeze the environmental map during training and only tune the albedo 𝜌 (𝜇). Since our lighting does not change in the training data, we cannot infer 𝜌 (𝜇) and 𝐿(𝑛) simultaneously. With two unknown variables, the underconstrained Eq. 1 allows the albedo to adjust to an arbitrary irradiance map, leading to unnatural colors in different lighting setups. LumiGauss [23] considers similar lighting model. However, they aim to recover the environmental map and albedo simultaneously. For rendered frame 𝐼𝑖 and the ground truth frame ˆ𝐼𝑖 , our objective consists of image-based loss Lrgb = L1 (𝐼𝑖, ˆ𝐼𝑖 ) + L𝑆𝑆𝐼 𝑀 (𝐼𝑖, ˆ𝐼𝑖 ) along with several regularizers. To exclude background objects from the model, we generate masks ˆ𝑀𝑖 (𝑥, 𝑦) = [ ˆ𝐼𝑖 (𝑥, 𝑦) is part of car] indicating pixels that belong to the model. Our opacity loss promotes high transparency outside of car pixels Lopacity = (cid:205)𝑥,𝑦 (1 ˆ𝑀𝑖 ) 𝑇𝑖 , where 𝑇𝑖 is the transparency map of the rendered frame. In our model, proper relighting requires accurate surface normals, so we additionally estimate normal maps ˆ𝑁𝑖 = 𝑛( ˆ𝐼𝑖 ) with an off-the-shelf model and use the estimates to regularize Gaussian orientations. For the rendered normal maps 𝑁𝑖 , the regularizer is Lnormal = ˆ𝑁𝑖 ). The resulting objective is (cid:205)𝑥,𝑦 ˆ𝑀𝑖 (1 𝑁𝑇 𝑖 (𝐼𝑖, ˆ𝐼𝑖 ) = Lrgb (𝐼𝑖, ˆ𝐼𝑖 ) + 𝜆opacityLopacity (𝐼𝑖, ˆ𝐼𝑖 ) + 𝜆normalLnormal (𝐼𝑖, ˆ𝐼𝑖 ). (3) Also, during training at each step we render image with random background, so that splats are not overfitted to the background."
        },
        {
            "title": "3.3 Car Insertion and Relighting.\nThe final stage of our pipeline integrates reconstructed cars into the\nlearned scene. First, we prepare each car for the insertion. We re-\nmove occasional splats that are either positioned behind the training\ncameras or project onto pixels outside the car mask in the training\nimages. The car is then oriented based on the principal components\nof its point cloud, further refined using an orientation model [38]\nto ensure proper alignment within the driving scene.",
            "content": "MADrive: Memory-Augmented Driving Scene Modeling 5 Fig. 4. Qualitative comparison of MADrive with non-retrieval-based driving scene reconstruction methods. Reconstruction of the training views (Top). Reconstruction of the hold-out (future) views (Bottom). Next, the aligned point cloud is placed inside the bounding box of the original car to be replaced. To achieve precise alignment in both scale and position, we use the Iterative Closest Point (ICP) algorithm [4] and apply the resulting transformation to the inserted car. To enhance visual realism, we add shadow beneath the car, modeled as black plane composed of 2D splats placed under the wheels. While more sophisticated shadow placement based on sun position, as explored in [55], could be considered, we find it nonessential for our method. Since the retrieved car asset is captured under different lighting conditions, we estimate the target scenes environment map and adjust the cars appearance via Eq. 1 to ensure visual consistency. As the Waymo dataset [39] lacks full 360 camera coverage, we approximate lighting conditions using DiffusionLight [36], which reconstructs missing environment map regions via diffusion-based inpainting. Given training frame, we estimate the environment map and align it with the corresponding camera orientation. In addition, we adjust the scale of the environmental map to minimize the tone discrepancy between the last training frame and our render."
        },
        {
            "title": "3.4 Database Collection and Statistics.\nThis work introduces MAD-Cars, a large-scale database of multi-\nview car videos in the wild, sourced from online car sale adver-\ntisements. The database contains ∼70, 000 diverse video instances,\neach averaging ∼85 frames, with most car instances available at a\nresolution of 1920×1080. It includes cars from ∼150 brands, covering\na broad range of colors, car types, and three lighting conditions. Dis-\ntributions of color, car type, and lighting are illustrated in Figure 3.\nThe metadata for each car instance is presented in the dataset.",
            "content": "The data is carefully curated by filtering out frames and entire car instances that could negatively impact 3D reconstruction. In more detail, we remove low quality and overly dark frames with CLIP-IQA [42] and use Qwen2.5-VL [49] to detect finger blocked shots, car interior views, the frames where the car view is occluded, e.g., by fences, trees, other vehicles, etc. More data collection details are provided in Appendix A."
        },
        {
            "title": "4.1 Evaluation Setup\nScene Reconstruction Dataset. We reconstruct the driving scenes\nfrom the Waymo Open Motion dataset [14]. We picked 12 particu-\nlarly challenging scenes containing multiple cars, driving maneuvers\nand diverse lighting conditions. Then, we manually select scene se-\nquences and divide them into training and evaluation clips. In our\nexperiments, we simultaneously use videos from frontal and two\nside cameras to capture a wide field of view and track cars mov-\ning across the scene. More evaluation setup details are provided in\nAppendix B.\nScene Extrapolation with Novel View Synthesis. We use the\nwhole sequence to reconstruct the background and then remove\nthe cars using the annotated bounding boxes in the Waymo dataset.\nCar reconstruction is performed using only the first part of the\nsequence, while the second part is reserved for evaluating scene\nreconstruction quality. We aim to generate realistic views of the\nscene by extrapolating the observed data. In particular, we insert the\nreconstructed car models into the background according to location\nand orientation specified by the bounding boxes on the holdout se-\nquence. By design, our setup evaluates scenes under configurations\nthat differ significantly from the frames seen during training. At the\nsame time, the data split ensures that test frames do not leak into\nthe car reconstruction process.\nBaselines. We compare MADrive with the scene reconstruction\nGaussian splatting-based methods that were previously considered\nfor novel view synthesis: Street-Gaussians (SG) [48], AutoSplat [27]\n(our implementation), and HUGS [56]. Details on training and eval-\nuation of baselines are given in Appendix D.",
            "content": "6 Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, and Dmitry Baranchuk Fig. 5. Retrieval illustration. Top-4 candidates retrieved using SigLIP 2 without (Left) and with (Right) color filtering."
        },
        {
            "title": "Model",
            "content": "MOTA MOTP IDF1 Segmentation IoU Street-GS [48] HUGS [56] AutoSplat* [27] MADrive (Ours) 0.654 0.556 0. 0.810 0.105 0.221 0.154 0.128 0.776 0.699 0.716 0.895 0.556 0.333 0. 0.822 *Denotes our reimplementation. Table 1. Comparison in terms of tracking and segmentation metrics. we use BotSort [1] with YOLOv8n backbone, reporting multiple object tracking accuracy (MOTA), precision (MOTP), and identity F1 score (IDF1) [32]. For segmentation, we compute the average intersection-over-union (IoU) using instance segmentation masks obtained with Mask2Former [10]. Table 1 presents the comparison of MADrive against the baselines. MADrive shows substantially superior performance compared to the baselines in 2 tracking metrics (MOTA and IDF) and segmentation metric IoU. This observation is also supported by the visual examples provided in Figure 4. We explain the MOTP gap between our method and Street-GS by the better car alignment of Street-GS in the first test frames, while later frames, where the tracker fails to detect Street-GS cars, are not counted in the MOTP calculation. We provide per scene results for all 12 scenes in Appendix C."
        },
        {
            "title": "4.3 Retrieval Evaluation\nHere, we evaluate the performance of the proposed retrieval mod-\nule isolated from other components to address how accurately the\nretrieved cars correspond to the original cars in the driving scene.\nFirst, we compare the retrieval performance on the proposed\ndataset against 3DRealCars [12], highly accurate publicly available\ndataset of 2,500 car assets. To evaluate the retrieval quality, we first\ncalculate the average L2 distance between the car images from the\ndriving scene and the nearest cars from the memory bank. We use\nSigLIP2 So [40] as an image feature extractor. Then, we provide\nthe accuracy obtained with the Qwen2.5-VL-32B-Instruct model,\nasked to compare the car instances in terms of their brand, model,",
            "content": "Fig. 6. Relighting ablation. Rendered hold-out frames without (Left) and with (Right) relighting."
        },
        {
            "title": "4.2 Main Experiments\nQualitative Evaluation. First, we provide visual scene reconstruc-\ntion results for qualitative analysis. In Figure 4, we compare ren-\ndering results on the training and hold-out frames. Although SG,\nAutoSplat, and HUGS produce accurate approximations of training\nframes, on the test frames cars tend to fell apart for novel view\nangles. Compared to baselines, our method cannot reproduce the\ntraining frames with the same precision, but is significantly more\nrobust to deviations from training configurations. More visual exam-\nples are presented in Figures 8, 9. We also provide the visualizations\nwith modified trajectories in Figure 10.\nQuantitative Evaluation. In our main experiments, we assess\ntracking and segmentation performance on synthesized test frames.\nSpecifically, we apply state-of-the-art tracking and segmentation\nmodels to both synthesized and ground truth frames and compare\ntheir outputs using established metrics for each task. For tracking,",
            "content": "MADrive: Memory-Augmented Driving Scene Modeling 7 Fig. 7. Qualitative comparison of reconstructed vehicles on KITTI-360 from the reference and rotated viewpoints. MADrive on top of the MAD-Cars dataset produces more similar and realistic 3D assets. color and car type. Note that we do not use the color filtering in this experiment for fair comparison. Table 2 shows the retrieval accuracy across different attributes, and the average L2 distance to the closest instance. We observe that candidates retrieved using MAD-Cars more accurately match the cars in the driving scenes, which we attribute to the significantly larger scale of MAD-Cars. Importantly, Table 2 also highlights that retrieval based solely on feature extractors often disregards car color, despite its importance for realistic car replacement. The similar problem has been observed for non-visual-language encoders such as DINOv2 [34]. As shown in Figure 5, applying color-based pre-filtering step significantly improves color consistency between the retrieved and target vehicles. Dataset 3DRealCars MAD-Cars Brand Model Color Car Type Distance 0.626 0.750 0.508 0.533 0.503 0.663 0.888 0. 0.502 0.445 Table 2. Retrieval performance w/o color filtering in terms of accuracy on the car brand, model, color and type and the distance to the closest instance for the MAD-Cars and 3DRealCar [12] datasets. MAD-Cars enables more accurate retrieval of car instances across all attributes."
        },
        {
            "title": "4.4 Car Reconstruction Evaluation\nWe provide a qualitative comparison with other car reconstruction\napproaches in Figure 7, where we visualized reconstruction alterna-\ntives. Given a query frame from the KITTI dataset, we compared the\nproposed approach with three alternatives: car reconstruction on a\ndifferent car dataset [12], matching with a car model from a CAD",
            "content": "dataset [31], and running cutting edge image-to-3D model [30]. Even though the latter closely matches the query frame, the second view indicates subpar geometry recovery. Compared to other methods, we see that the diversity of our dataset allows MADrive to obtain models that closely match the query frame in terms of appearance (e.g., color, shape) and realism."
        },
        {
            "title": "5 Conclusion\nThis work presents MADrive, a novel driving scene reconstruction\napproach specifically designed to model significantly altered vehicle\npositions. Powered by MAD-Cars, our large-scale multi-view car\ndataset, MADrive replaces dynamic vehicles in a scene with similar\ncar instances from the database. We believe that MADrive could\nmake a step towards modeling multiple potential outcomes for ana-\nlyzing an autonomous driving system’s behavior in safety-critical\nsituations.",
            "content": "However, despite the promising visual fidelity of future scenario frames, they still differ from the ground truth. Future work could focus on expanding the database with wider range of car brands, models, and types, as well as enhancing corrupted car videos using recent multi-view diffusion [57]. Another promising direction is to explore more advanced relighting techniques that more accurately capture reflections and lighting interactions. 8 Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, and Dmitry Baranchuk References [1] Nir Aharon, Roy Orfaig, and Ben-Zion Bobrovsky. 2022. Bot-sort: Robust associa- [23] tions multi-pedestrian tracking. arXiv preprint arXiv:2206.14651 (2022). [2] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel Chang, and Matthias Nießner. 2019. Scan2cad: Learning cad model alignment in rgb-d scans. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. 26142623. [3] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. 2018. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. arXiv preprint arXiv:1812.03079 (2018). [4] Paul Besl and H.D. McKay. 1992. method for registration of 3-D shapes. IEEE Trans Pattern Anal Mach Intell. Pattern Analysis and Machine Intelligence, IEEE Transactions on 14 (03 1992), 239256. doi:10.1109/34.121791 [5] Krzysztof Byrski, Marcin Mazur, Jacek Tabor, Tadeusz Dziarmaga, Marcin Kądziołka, Dawid Baran, and Przemysław Spurek. 2025. RaySplats: Ray Tracing based Gaussian Splatting. arXiv preprint arXiv:2501.19196 (2025). [6] Yohann Cabon, Naila Murray, and Martin Humenberger. 2020. Virtual kitti 2. arXiv preprint arXiv:2001.10773 (2020). [7] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. 2020. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1162111631. [8] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. 2015. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012 (2015). [9] Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, and Li Zhang. 2023. Periodic vibration gaussian: Dynamic urban scene reconstruction and real-time rendering. arXiv preprint arXiv:2311.18561 (2023). [10] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander Schwing. 2021. Mask2former for video instance segmentation. arXiv preprint arXiv:2112.10764 (2021). [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2023. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1314213153. [12] Xiaobiao Du, Haiyang Sun, Shuyun Wang, Zhuojie Wu, Hongwei Sheng, Jiaying Ying, Ming Lu, Tianqing Zhu, Kun Zhan, and Xin Yu. 2024. 3DRealCar: An In-thewild RGB-D Car Dataset with 360-degree Views. arXiv preprint arXiv:2406.04875 (2024). [13] Francis Engelmann, Jörg Stückler, and Bastian Leibe. 2017. SAMP: shape and motion priors for 4d vehicle reconstruction. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 400408. [14] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles Qi, Yin Zhou, et al. 2021. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 97109719. Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid. 2020. Vectornet: Encoding hd maps and agent dynamics from vectorized representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1152511533. [16] Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are we ready for autonomous driving? The KITTI vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition. 33543361. doi:10.1109/CVPR.2012. 6248074 [15] [17] Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, and Andrea Tagliasacchi. 2025. Radiant Foam: Real-Time Differentiable Ray Tracing. arXiv preprint arXiv:2502.01157 (2025). [18] Can Gümeli, Angela Dai, and Matthias Nießner. 2022. Roca: Robust cad model retrieval and alignment from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 40224031. [19] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2024. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers. 111. [20] Xin Huang, Eric Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, et al. 2024. DriveGPT: Scaling Autoregressive Behavior Models for Driving. arXiv preprint arXiv:2412.14415 (2024). [21] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, and Yuexin Ma. 2024. Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 53225332. James Kajiya. 1986. The rendering equation. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques. 143150. [22] Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, and Marek Kowalski. 2024. Lumigauss: High-fidelity outdoor relighting with 2d gaussian splatting. arXiv preprint arXiv:2408.04474 (2024). [24] Brian Karis and Epic Games. 2013. Real shading in unreal engine 4. Proc. Physically Based Shading Theory Practice 4, 3 (2013), 1. [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42, 4 (2023), 1391. [26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics 42, 4 (July 2023). https://repo-sam.inria.fr/fungraph/3dgaussian-splatting/ [27] Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, and Bingbing Liu. 2024. Autosplat: Constrained gaussian splatting for autonomous driving scene reconstruction. arXiv preprint arXiv:2407.02598 (2024). [28] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Yang-Che Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 2024. 3d gaussian splatting as markov chain monte carlo. Advances in Neural Information Processing Systems 37 (2024), 8096580986. [29] Ruofan Liang, Zan Gojcic, Merlin Nimier-David, David Acuna, Nandita Vijaykumar, Sanja Fidler, and Zian Wang. 2024. Photorealistic object insertion with diffusion-guided inverse rendering. In European Conference on Computer Vision. Springer, 446465. [30] Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, and Yadong Mu. 2025. DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation. arXiv preprint arXiv:2501.16764 (2025). [31] Yichong Lu, Yichi Cai, Shangzhan Zhang, Hongyu Zhou, Haoji Hu, Huimin Yu, Andreas Geiger, and Yiyi Liao. 2024. UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation. arXiv preprint arXiv:2411.19292 (2024). [32] Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, and Konrad Schindler. 2016. MOT16: benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831 (2016). [33] Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, and Zan Gojcic. 2024. 3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes. ACM Transactions on Graphics (TOG) 43, 6 (2024), 119. [34] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. 2023. DINOv2: Learning Robust Visual Features without Supervision. [35] Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, and Federico Tombari. 2023. Shape, Pose, and Appearance from Single Image via Bootstrapped Radiance Field Inversion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). [36] Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Varun Jampani, Amit Raj, Pramook Khungurn, and Supasorn Suwajanakorn. 2024. Diffusionlight: Light probes for free by painting chrome ball. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 98108. [37] Ravi Ramamoorthi and Pat Hanrahan. 2001. An efficient representation for irradiance environment maps. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques. 497500. [38] Christopher Scarvelis, David Benhaim, and Paul Zhang. 2024. Orient Anything. arXiv preprint arXiv:2410.02101 (2024). [39] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurélien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. 2020. Scalability in Perception for Autonomous Driving: Waymo Open Dataset. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 24432451. doi:10.1109/CVPR42600.2020.00252 [40] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. 2025. SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features. arXiv preprint arXiv:2502.14786 (2025). [41] Mikaela Angelina Uy, Jingwei Huang, Minhyuk Sung, Tolga Birdal, and Leonidas Guibas. 2020. Deformation-aware 3d model embedding and retrieval. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VII 16. Springer, 397413. MADrive: Memory-Augmented Driving Scene Modeling 9 [42] [43] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. 2023. Exploring CLIP for Assessing the Look and Feel of Images. In AAAI. Jingkang Wang, Sivabalan Manivasagam, Yun Chen, Ze Yang, Ioan Andrei Bârsan, Anqi Joyce Yang, Wei-Chiu Ma, and Raquel Urtasun. 2023. Cadsim: Robust and scalable in-the-wild 3d reconstruction for controllable sensor simulation. arXiv preprint arXiv:2311.01447 (2023). [44] Wikipedia contributors. 2024. Car colour popularity. https://en.wikipedia.org/ wiki/Car_colour_popularity [Online; accessed 2024-07-19]. [45] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 2024. 4d gaussian splatting for realtime dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2031020320. [46] Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, et al. 2021. Pandaset: Advanced sensor suite dataset for autonomous driving. In 2021 IEEE international intelligent transportation systems conference (ITSC). IEEE, 30953101. [47] Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, and Xiaowei Zhou. 2024. Envgs: Modeling view-dependent appearance with environment gaussian. arXiv preprint arXiv:2412.15215 (2024). [48] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. 2024. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In European Conference on Computer Vision. Springer, 156173. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 (2024). [50] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. 2023. UniSim: Neural Closed-Loop Sensor Simulator. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 13891399. doi:10.1109/CVPR52729.2023. [51] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. 2024. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2033120341. [52] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. 2023. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642 (2023). [53] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. 2024. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1944719456. Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. 2021. Ners: Neural reflectance surfaces for sparse-view 3d reconstruction in the wild. Advances in Neural Information Processing Systems 34 (2021), 2983529847. [55] Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, and Yiyi Liao. 2024. HUGSIM: Real-Time, PhotoRealistic and Closed-Loop Simulator for Autonomous Driving. arXiv preprint arXiv:2412.01718 (2024). [56] Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, and Yiyi Liao. 2024. HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2133621345. Jensen Jinghao Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. 2025. STABLE VIRTUAL CAMERA: Generative View Synthesis with Diffusion Models. arXiv preprint arXiv:2503.14489 (2025). [54] [57] 10 Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, and Dmitry Baranchuk Fig. 8. Additional qualitative comparison of MADrive with non-retrieval-based driving scene reconstruction methods. Reconstruction of the training views (Top). Reconstruction of the hold-out (future) views (Bottom). MADrive: Memory-Augmented Driving Scene Modeling Fig. 9. Additional qualitative comparison of MADrive with non-retrieval-based driving scene reconstruction methods. Reconstruction of the training views (Top). Reconstruction of the hold-out (future) views (Bottom). 12 Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, and Dmitry Baranchuk Data Collection Details The initial database contained 95, 000 car videos of 100 views on average. The first filtering stage includes the filtering of low quality and overly dark images with the CLIP-IQA model [42], discarding frames with score < 0.2. Then, we use Qwen-2.5-VL-Instruct (7B) [49] to respond several questions for each frame: Does the image depict car? Is the car directly occluded? Does the image depict the car interior? Does hand or finger block the view? Is the car door open? Does the image mainly depict the car window? Based on the responses, we filter out the corresponding frames or, in some cases, entire car instances. Also, if fewer than 45 valid frames remain for given instance, the entire instance is discarded. Evaluation Setup Details For scene reconstruction evaluation, we selected 12 scenes from the Waymo Open Dataset [39], with labels listed in Table 3. This table also provides the correspondence between the original scene labels from the Waymo Cloud Storage and the short names used in our work. We split each scene into training and testing subsets based on time  (Table 4)  and camera selection  (Table 5)  . Specifically, frames with indices 𝑖train, where 𝑖train [𝑖train start , 𝑖train end ], were used for training. For evaluation, we used frames 𝑖test [𝑖test start, 𝑖test end], with all split indices provided in Table 4."
        },
        {
            "title": "Scene name",
            "content": "1231623110026745648_480_000_500_000 1432918953215186312_5101_320_5121_320 1906113358876584689_1359_560_1379_560 10500357041547037089_1474_800_1494_800 10940952441434390507_1888_710_1908_710 16504318334867223853_480_000_500_000 17407069523496279950_4354_900_4374_900 18025338595059503802_571_216_591_216 14183710428479823719_3140_000_3160_000 15834329472172048691_2956_760_2976_760 17647858901077503501_1500_000_1520_000 7799671367768576481_260_000_280_000 123 143 190 105 109 165 174 180 141 158 176 779 Table 3. Waymo scenes used for evaluation of scene reconstruction. Per-Scene Quantitative Evaluation. In addition to the aggregated results in Table 1, we report perscene metric values in Table 6, Table 7, Table 8, and Table 9, corresponding to MOTA, MOTP, IDF1, and IoU, respectively. We observe that MADrive consistently outperforms the baselines across most scenes. Baseline Details Baselines training and evaluation. We trained all baselines (StreetGaussians, HUGS, and AutoSplat) for 10𝐾 iterations using the training frames with indices 𝑖 [𝑖train end ] as specified in Table 4. start , 𝑖train"
        },
        {
            "title": "Scene name",
            "content": "𝑖𝑡𝑟𝑎𝑖𝑛 𝑠𝑡𝑎𝑟𝑡 123 143 190 105 109 165 174 180 141 158 176 779 106 43 115 164 1 7 34 49 60 44 31 50 𝑖𝑡𝑟𝑎𝑖𝑛 𝑒𝑛𝑑 116 53 125 174 16 40 51 55 80 62 42 65 𝑖𝑡𝑒𝑠𝑡 𝑠𝑡𝑎𝑟𝑡 117 54 126 175 17 41 52 56 81 63 43 𝑖𝑡𝑒𝑠𝑡 𝑒𝑛𝑑 175 62 137 196 55 111 72 68 117 100 67 84 Table 4. Train and test frame splits for Waymo scenes over time. All values, except those in the leftmost column, indicate frame indices starting from 0."
        },
        {
            "title": "Test cameras",
            "content": "123 143 190 105 109 165 174 180 141 158 176 779 frontal, frontal left frontal, frontal left frontal, frontal left frontal, frontal left frontal, frontal right frontal, frontal left frontal frontal, frontal right frontal frontal frontal frontal, frontal left, frontal right frontal, frontal left frontal, frontal left frontal, frontal left frontal frontal right frontal, frontal left frontal frontal, frontal right frontal frontal frontal frontal, frontal right Table 5. Train and test frame splits for Waymo scenes based on camera selection."
        },
        {
            "title": "Scene name",
            "content": "SG [48] HUGS [56] AutoSplat [27] MADrive 123 143 190 105 109 165 174 180 141 158 176 779 0.687 0.650 0.787 0.906 0.242 0.684 0.809 0.611 0.667 0.423 0.727 0.661 0.685 0.513 0.795 0.656 0.448 0.461 0.886 0.528 0.607 0.233 0.562 0.296 0.327 0.600 0.904 0.742 0.605 0.788 0.830 0.695 0.163 0.681 0.176 0.545 0.906 0.825 0.820 0.879 0.925 0.885 0.667 0.695 0.771 0.623 0.970 0. Table 6. Mean MOTA results on test frames for all Waymo scenes. Additionally, we trained the background models for both the baselines and MADrive for 30𝐾 iterations using all available frames. These pretrained background models were then used during the rendering of the test frames (𝑖 [𝑖test end]), on which we compute the metrics reported in Table 6, Table 7, Table 8, and Table 9. start, 𝑖test MADrive: Memory-Augmented Driving Scene Modeling New Trajectories In this section, we demonstrate our methods capability to render novel views with substantial scene variations. Figure 10 showcases results across four test scenes, where all modifications preserve high image quality."
        },
        {
            "title": "Scene name",
            "content": "SG [48] HUGS [56] AutoSplat [27] MADrive 123 143 190 105 109 165 174 180 141 158 176 779 0.073 0.114 0.088 0.073 0.093 0.125 0.075 0.150 0.119 0.087 0.093 0.176 0.093 0.461 0.112 0.262 0.132 0.202 0.886 0.231 0.261 0.128 0.246 0.443 0.099 0.095 0.115 0.222 0.094 0.119 0.078 0.194 0.237 0.123 0.167 0.305 0.078 0.212 0.139 0.119 0.112 0.134 0.098 0.189 0.138 0.081 0.067 0. Table 7. Mean MOTP results on test frames for all Waymo scenes."
        },
        {
            "title": "Scene name",
            "content": "SG [48] HUGS [56] AutoSplat [27] MADrive 123 143 190 105 109 165 174 180 141 158 176 779 0.804 0.787 0.880 0.952 0.390 0.806 0.894 0.753 0.805 0.605 0.847 0.793 0.806 0.709 0.887 0.780 0.619 0.612 0.940 0.709 0.698 0.377 0.720 0.532 0.475 0.750 0.950 0.877 0.754 0.894 0.907 0.820 0.278 0.829 0.316 0.739 0.953 0.904 0.903 0.935 0.961 0.937 0.800 0.804 0.883 0.812 0.985 0. Table 8. Mean IDF1 results on test frames for all Waymo scenes."
        },
        {
            "title": "Scene name",
            "content": "SG [48] HUGS [56] AutoSplat [27] MADrive 123 143 190 105 109 165 174 180 141 158 176 779 0.753 0.485 0.707 0.671 0.499 0.633 0.695 0.475 0.607 0.404 0.499 0.247 0.608 0.243 0.519 0.425 0.246 0.459 0.581 0.238 0.196 0.135 0.187 0.153 0.500 0.510 0.740 0.439 0.419 0.647 0.655 0.582 0.226 0.498 0.263 0.273 0.864 0.762 0.798 0.809 0.850 0.737 0.793 0.876 0.754 0.864 0.875 0. Table 9. Mean IoU results on test frames for all Waymo scenes. Street-Gaussians. We used the official implementation available at https://github.com/zju3dv/street_gaussians. HUGS. We used the official implementation provided at https:// github.com/hyzhou404/HUGSIM. AutoSplat. As no official implementation is publicly available, we re-implemented the core contributions of AutoSplat on top of the Street-Gaussians codebase. 14 Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, and Dmitry Baranchuk i O fi M n r fi M i O fi M i O fi M Fig. 10. Visualization of original and modified trajectories with MADrive. The cars retain high-fidelity appearance even at close distances to the ego camera."
        }
    ],
    "affiliations": [
        "HSE University",
        "Skoltech",
        "Yandex",
        "Yandex Research"
    ]
}