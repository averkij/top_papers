{
    "paper_title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
    "authors": [
        "Siyuan Li",
        "Kai Yu",
        "Anna Wang",
        "Zicheng Liu",
        "Chang Yu",
        "Jingbo Zhou",
        "Qirong Yang",
        "Yucheng Guo",
        "Xiaoming Zhang",
        "Stan Z. Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models."
        },
        {
            "title": "Start",
            "content": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging Siyuan Li1,2,3, Kai Yu2, Anna Wang2, Zicheng Liu1,2, Chang Yu2, Jingbo Zhou1,2, Qirong Yang3, Yucheng Guo3, Xiaoming Zhang3, Stan Z. Li2* 1Zhejiang University, Hangzhou, China 2AI Lab, Research Center for Industries of the Future, Westlake University, China 3BioMap Research, Beijing, China 5 2 0 2 7 1 ] . - [ 1 6 0 8 4 1 . 1 1 5 2 : r Abstract Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces hierarchical architecture that jointly optimizes dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing Latent Decoder and Local Decoder, MergeDNA learns with two pretraining tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zeroshot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models."
        },
        {
            "title": "Introduction",
            "content": "Modeling genomic DNA sequences with foundation models (Ji et al. 2021) is an emerging frontier that promises to advance bioinformatics and precision medicine. DNA is often likened to natural language carrying the code of life (Cooper 1981), yet it poses unique modeling challenges far beyond ordinary text. Firstly, genomic information is distributed unevenly. Only around 2% of the human genome consists of coding sequences (CDS), densely packed with functional information, whereas the vast majority is non-coding sequence (nCDS) with regulatory or unknown functions, which contains repetitive or less informative content (Nguyen et al. 2024a). Secondly, unlike natural languages with semantic words (Kudo and Richardson 2018), DNA has no inherent word boundaries or predefined vocabulary units (Zhou et al. 2023). The meaningful *Corresponding authors. Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Overview of MergeDNA architecture. The Local Encoder & Decoder achieves adaptive DNA tokenization, while the Latent Encoder & Decoder learn contextual information with informative token masked modeling. units of DNA vary by context: biologically relevant motif might be 3 bases (as codon) (Liu et al. 2025b) or 610 bases (a transcription factor binding site), or even longer sequences (Dalla-Torre et al. 2023). This makes fixed tokenization schemes inadequate (Qiao et al. 2024). Third, DNA sequences are extremely long (Nguyen et al. 2024b), often spanning tens of thousands to millions of bases, requiring models that can capture both short-range motifs and long-range dependencies efficiently. And naive pre-training objectives (Radford et al. 2018; Devlin et al. 2019) may fail to focus on the truly important parts of these vast sequences. These factors collectively make DNA fundamentally distinct from human language and call for new class of sequence modeling architectures. Recent studies have explored various facets of DNA foundation modeling. Long-sequence architectures such as linear-time state-space models (SSMs) (Nguyen et al. 2024b; Schiff et al. 2024), hierarchical Transformers (Shao and Yan 2024), and hybrid networks (Nguyen et al. 2024a; Ma et al. 2025) improve context length scalability with efficiency. Meanwhile, DNA tokenization strategies range from base-level encodings to k-mers (Ji et al. 2021; Wu et al. 2025) and learned vocabularies via BPE (Zhou et al. 2023) or vector quantization (van den Oord, Vinyals, and Kavukcuoglu 2017; Li et al. 2024a). Pre-training objectives also vary, including masked modeling (Ji et al. 2021), autoregressive loss (Zhang et al. 2023), and advanced masking (Roy et al. 2023). However, most works optimize these dimensions in isolation and lack unified mechanism to address all three DNA modeling challenges. For example, the latest long-range models (Brixi et al. 2025) that still operate on single-base tokens may waste capacity on repetitive intergenic regions, while learned tokenizer without matching long-context encoder could miss global dependencies (Qiao et al. 2024). In this work, we argue that effective genomescale modeling requires two core capabilities: (i) contextsensitive tokenizer that learns to segment DNA into variablelength units based on local structure and semantics, and (ii) adaptive pre-training objectives that prioritize informationdense regions for representation learning. We try to address these jointly by leveraging token merging techniques (Bolya et al. 2023; Lee and Hong 2024) for end-to-end learnable token granularity and contextual abstraction. This work presents MergeDNA, context-aware genome modeling framework that dynamically adapts tokenization and pre-training to genomic context, as shown in Figure 1. The core idea of MergeDNA is hierarchical autoencoderstyle Transformer that learns to compress and reconstruct DNA sequences with differentiable tokenizer and longrange context model. Specifically, we design Local Encoder composed of stacked local-window attention blocks with differentiable token merging, enabling the model to chunk adjacent bases into variable-length tokens based on local similarity. These merged tokens are then processed by global-context Latent Encoder using full attention. On the decoder side, symmetric Latent Decoder and Local Decoder reconstruct the input sequence. Two pre-training objectives jointly supervise the model: (i) Merged Token Reconstruction trains the tokenizer and encoder to preserve key information while filtering redundancies; and (ii) Adaptive Masked Token Modeling selectively masks and predicts important tokens identified through token merging, encouraging context-aware learning of functionally relevant patterns. Together, these components form unified and scalable genome modeling pipeline that adapts both token resolution and attention allocation based on input complexity. Our contributions are summarized as follows: Unified Architectural Design: We propose novel hierarchical framework that tightly integrates learnable DNA tokenizer with long-range sequence modeling. Leveraging differentiable token merging within local attention blocks, the Local Encoder captures irregular genomic patterns and determines where to merge as words. Adaptive Context Modeling: We propose contextaware pre-training tasks that adapt to varying information density in genomic sequences. Using token merging to select informative positions, the proposed Merged Token Reconstruction and Adaptive Masked Token Modeling allow the model to capture both local motif-level information and global long-range dependencies. Strong Empirical Results: MergeDNA achieves competitive performance across three major DNA benchmarks and shows excellent generalization to several RNA and protein downstream tasks, outperforming prior methods of DNA tokenization and foundation models in both shortand long-context settings. of DNA foundation models (Ji et al. 2021; Benegas, Batra, and Song 2023) has merged with four lines of research. (a) Long sequence modeling is the most crucial technique for long DNA sequences. State-space models (SSMs) like HyenaDNA (Nguyen et al. 2024b; Thoutam and Ellsworth 2024) and Caduceus (Schiff et al. 2024) deliver linear complexity, while hierarchical attention (Shao and Yan 2024) or hybrid SSM-attention designs (Brixi et al. 2025; Ma et al. 2025) capture both motifs and chromosome-level structure with moderate memory footprints. (b) DNA tokenization remains discussion with byte-level (Nguyen et al. 2024a), kmers (Dalla-Torre et al. 2023), or learnable vocabularies (Ji et al. 2023; Qiao et al. 2024). (c) Pre-training objectives can be the BERT-style (Devlin et al. 2019; Zhou et al. 2023; Li et al. 2025a) or auto-regressive-like masked token modeling (Zhang et al. 2023; Zhu et al. 2024) for the encoder or decoder architectures, where some loss reweighing (Brixi et al. 2025) or tailored masking curricula (Roy et al. 2023; Roy, Sural, and Ganguly 2024) could be further beneficial. Only minor methods utilize contrastive learning (Zhou et al. 2025b) or cross-modality alignment tasks (Liu et al. 2025b) to integrate multi-omic cues. (d) Domains of pre-training and applications are usually bound. While most models are pre-trained on the human reference (Nguyen et al. 2024b) or multiple species corpora (Zhou et al. 2023), specialized datasets confer niche expertise, e.g., prokaryotic (Nguyen et al. 2024a), plant genomic domains (Mendoza-Revilla et al. 2023; Zhai et al. 2025), and metagenomes (Zhou et al. 2025a). Extending beyond monomodal DNA, multi-omics models aim to simulate the central dogma (Cooper 1981) within unified architecture (Yang et al. 2024) with geneto-expression pipeline (Avsec et al. 2021; Yang, Zhu, and Su 2025) or the genome-to-protein pipeline (Song, Segal, and Xing 2024; Liu et al. 2025b), leveraging shared structure across DNA, RNA, protein, and epigenome. Byte-level Architectures. In NLP, early subword approaches like BPE (Sennrich, Haddow, and Birch 2015) and SentencePiece (Kudo and Richardson 2018) remain the default module in LLMs, and dynamic schemes like Dynamic Pooling (Nawrot et al. 2022; Liu et al. 2024a) partially relax fixed vocabularies, yet they still require external preprocessing. Leveraging SSMs for linear-time attentions (Gu and Dao 2023), MegaByte (Yu et al. 2023), and MambaByte (Wang et al. 2024; Slagle 2024) demonstrated that multi-scale or SSM-based architectures without the subword tokenizer can model million-byte inputs end-to-end with great scale abilities (Ge et al. 2025) on text and other modalities (Wu et al. 2024b). More recently, BLT (Pagnoni et al. 2024) introduces learned chunking with entropy-balanced patches, and HNet (Hwang, Wang, and Gu 2025) designs differentiable segmentation with jointly optimization. Similarly, DNA models with raw nucleotides as input are also byte-level architectures (Nguyen et al. 2024a). Meanwhile, classical tokenization strategies, such as BPE (Zhou et al. 2023) and k-mers (Dalla-Torre et al. 2023), as well as learnable dictionaries (Li et al. 2024a), have also been explored."
        },
        {
            "title": "3.1 Preliminary\nA DNA sequence can be seen as a string in the nucleotide\nalphabet D = {A, T, C, G}. We denote a sequence of length",
            "content": "N as = (x1, x2, . . . , xN ) DN , where each xi D. DNA tokenizer : DN L, segments into sequence of tokens ZL = (z1, . . . , zL) and maps to vocabulary with L. Given DNA sequences with causal mask {0, 1}N , model fθ with Attention blocks can be trained with an objective of masked token modeling (MTM): LM LM (θ) = 1 (cid:88) i=1 log (xi ; θ), (1) which encourages fθ to infer each masked token xi from its surrounding context to model the DNA context."
        },
        {
            "title": "3.2 Architectural Overview\nAdopting an autoencoder style, MergeDNA consists of four\nmain components, which merge the fixed tokenizer and the\nsequence model into a hierarchical network in Figure 1.",
            "content": "Local Encoder for Tokenization. The Local Encoder Eϕ serves as learnable DNA tokenizer with local contexts, producing tokenized sequence ZL RLD in the embedding dimension of with binary source matrix {0, 1}LN : ZL, = Eϕ(X). (2) Intuitively, ZL is context-dependent segmentation of X, and each row of indicates which original positions in were merged to form the corresponding token in ZL. To adaptively chunk adjacent bases into informative tokens with local context, we implement the Local Encoder as stack of local-window self-attention layers interleaved with differentiable token merging operations (described in Sec. 3.3), where the fixed local window size can also ensure linear-time computational complexity despite the long input. This learned tokenizer can be trained jointly with the rest of the model, allowing it to optimize token boundaries for the pre-training objective. Rather than using fixed kmer or requiring byte-pair scheme, the Local Encoder can allocate shorter tokens (finer granularity) to dense information regions and longer tokens to repetitive regions, thereby addressing the varying information density of genomes. = Eψ(ZL), Latent Context Modeling. Based on the tokenized sequence, the Latent Encoder Eψ is the main network for capturing long-range dependencies across the entire input, which can be implemented as Transformer encoder with full attention (utilizing Flash Attention). As for inference, Eψ produces an output of the same length L: (3) RLD are contextually enriched token emwhere beddings. On top of the encoder, we include lightweight Latent Decoder Eω, which transforms back toward the token space of ˆZL. The Latent Decoder has symmetric architecture to Eψ, and outputs ˆZL = Eω(Z L), where ˆZL can be seen as reconstructed version of the Local Encoders token sequence, containing the information needed to recover the original input. Together, Eψ and Eω form an autoencoder on the token level. This design enables us to apply reconstruction-based training at the token level, providing learning signals to both the tokenizer and the context encoder. We emphasize that the Latent Decoder is used only during pre-training to assist the encoder and tokenizer. For many downstream tasks, the Latent Decoder may be omitted. This follows the common practice of using an encoders learned representations for downstream prediction, while the decoder is specialized for generative reconstruction. Local Decoder for Reconstruction. The final stage is the Local Decoder Eζ, which maps the Latent Decoders output ˆZL back to the original base space and plays the role of detokenizer. We first apply token unmerging operation U(, ) using the source matrix to upsampling the L-length decoded tokens to length , ZN = U( ˆZL, S). ZN RN denotes an unmerged sequence, where each position corresponds to an original base in X. In matrix form, if Sij = 1 indicates the position covers original position j, then ZN = ˆZL. After unmerging, Eζ applies stack of local attention (as the reverse of the Local Encoder) to refine local details and output the reconstructed sequence ˆX = (ˆx1, . . . , ˆxN ): ˆX = Eζ( ZN ). (4) The Local Decoder thus completes the autoencoder by learning to fill in base-level information that may have been abstracted away by the Local Encoder. Meanwhile, the Local Encoder can be encouraged to produce merge groupings that are easy to invert, as the source matrix preserves positional information that enables accurate reconstruction. Training vs. Inference. During pre-training, we can optimize all modules θ = {ϕ, ψ, ω, ζ} end-to-end by applying learning objectives of reconstruction and prediction tasks (detailed in Sec. 3.4) between the final output ˆX and the original input. As for inference, MergeDNA can be truncated or reconfigured depending on the task types, which can function as typical encoder-only model for representation learning, or as an encoderdecoder model for generative purposes. For generative tasks or any task requiring output at the nucleotide level, e.g., sequence reconstruction or base pair prediction, we can use the entire autoencoder, or fine-tune the Local Decoder for the specific output prediction. For classification or regression tasks at the sample level, e.g., the goal is to produce label or embedding for the sequence, we can discard both decoders and use the Latent Encoder output directly with fine-tuned head."
        },
        {
            "title": "3.3 MergeDNA Tokenization\nLocal-window Token Merging. At the heart of the Lo-\ncal Encoder is a differentiable token merging mechanism\nthat learns to segment the sequence. We build upon ToMe\n(Bolya et al. 2023), which progressively fuses similar tokens\nto reduce sequence length, but adapt it for local, fine-grained\nchunking. Each Local Encoder layer consists of a standard\nlocal self-attention followed by a token merging module.\nGiven the l-th layer, supposing the input sequence length\nis Nl−1, the merging module will select rl pairs of tokens\nwithin each window to compute the average, reducing the\nsequence by rl tokens. We denote this operation as:\n= LocalToMeAttn(l)(cid:16)\n, S (l−1), rl",
            "content": "(5) (l), (l) Nl (l1) Nl1 (cid:17) , where (l1) is the source matrix carried from the previous layer (with (0) = IN0 as an identity matrix at input), RKD with < L. Note that Eψ(ZL, S), where identifies the most essential tokens among the ZL: the merging algorithm preferentially fuses tokens that appear redundant or less salient, while preserving distinct tokens that carry unique information. We then feed into the Latent Decoder to produce ˆZL, but first we upsample it back to length with the unmerge operation, ZL = U(Z K, ). This distributes each of the latent tokens back to its original token positions. Finally, ˆZL is passed through the Local Decoder to produce ˆX, and we compute reconstruction loss as LM R. We refer to this loss as the latent MTR loss, LM R(θ {ϕ}), since it trains the latent models to recover context from the selected tokens while the Local Encoder is held fixed (ϕ is not updated in this step). Intuitively, this task forces the latent transformer to not rely on having every token available it must learn to encode the sequence in such way that even if nearly tokens worth of information are dropped, the remaining still capture the essential context to rebuild the sequence. This pushes Eψ to generate more compact, salient representation. Adaptive Masked Token Modeling Beyond reconstruction, we also devise masking strategy to predict the informative tokens. We leverage the latent merging outcome to decide which tokens to mask. The key idea is to assign higher masking probability to tokens deemed important (those not heavily merged) and lower probability to tokens that were aggressively merged (with low information). Given {0, 1}KL from the Latent Encoder, we compute an importance probability for each of the local tokens. Let gi = (cid:80)L j=1 S{i, j} be the number of original tokens (out of L) that were grouped into the i-th latent token. We assign each latent group weight inversely proportional to its size, e.g., wi = 1 . For each token that belongs gi to group i, we set PL(j) wi , and choose the normalizing gi constant such that (cid:80)L j=1 PL(j) = 1. This yields probability vector PL RL over the local tokens, where tokens in large merged groups (large gi) receive low probability and tokens in singleton or small groups receive higher probability. We then sample exactly tokens without replacement according to PL to mask. Letting ML {0, 1}L be the mask indicator, we map this mask back to the input space via the source matrix, MN = U(ML, S) {0, 1}N . In other words, if merged token is selected to be masked, all of its constituent base positions in will be masked out. Finally, we feed the masked sequence MN through the entire network without latent token merging to get an output X. We define an Adaptive Masked Token Modeling (AMTM) loss: LAM (θ) = 1 (cid:88) log ( Xi MN ; θ). (7) i: MN (i)=1 This is essentially masked language modeling loss focused on the high-information tokens (and their base positions), ignoring the easy/redundant tokens. Overall, our full pretraining objectives involve three losses computed in three forward passes, which can be computed as: Ltotal = LM R(θ) + λLM R(θ {ϕ}) + LAM (θ), (8) where λ denotes down-weighting factor, we set λ = 0.25 in practice, which ensures that the model learns to reFigure 2: Pre-training of MergeDNA for (a) Local Encoder & Decoder and (b) Latent Encoder & Decoder. and (l) is updated to reflect the new merges at the l-th layer. In implementation, we compute similarity score for each pair of tokens in local window (using lightweight grouping embedding as in DTEM (Lee and Hong 2024)). The top-rl most similar token pairs in each window are selected to merge. We then perform soft merging: one token (keeper) absorbs the other (merger) by adding their representations, or weighted average, and we mark this in (l), where the merger tokens source positions are assigned to the keeper. Tokens not selected for merging pass through unchanged to the next layer. This continuous relaxation of token merging ensures the operation is differentiable, allowing gradients to tune both the token embeddings and the merging criteria. Token Unmerging and Reconstruction. To optimize the end-to-end tokenization capacity, we introduce Merged Token Reconstruction (MTR) objective LM that forces the network to reconstruct the original sequence from compressed tokens, which can be computed as the cross-entropy between ˆX and X: LM R(θ) = 1 (cid:88) log ( ˆXi Xi; θ). (6) i=1 During training, we use compression ratio sampling strategy, which randomly chooses the number of tokens to retain each iteration. For example, if the average goal is 2 , we might sample from Gaussian distribution centered at 2 (with the variance to ensure [0.4N, 0.6N ]). This strategy exposes Eϕ to wider range of segmentations during training, improving its generalization ability and ensuring that the Local Encoder does not overfit to particular compression rate."
        },
        {
            "title": "3.4 Adaptive Context Modeling\nAs discussed in Sec. 3.2, the Latent Encoder Eψ processes\nL tokens uniformly during inference. However, genomic\nsequences often contain long stretches of low-information\ncontent (e.g., repetitive DNA) where modeling every token\nis unnecessary. We aim to help the model find out and focus\nmost informative tokens and design two steps to improve the\nnaive MLM object, as shown in Figure 2.",
            "content": "Selection and Reconstruction. During pre-training, we modify the latent encoder Eψ to forward an additional round and select smaller number of salient tokens. Technically, we replace the standard attention in Eψ with ToMestyle Attention that merges tokens at the global scale (as opK, ) = posed to local windows). Formally, we obtain (Z Method Date # Params Architecture Type Pre-training Task Enhancers (3 tasks) Species Classification (2 tasks) Regulatory Elements (3 tasks) Average (8 tasks) HyenaDNA Caduceus-16 DNABERT DNABERT2 GENA-LM NT-500M VQDNA MxDNA ConvNova GENERator MergeDNA NeurIPS23 6.6M ICML24 Bioinfo21 ICLR24 117M BPE+A BERT 82.81 95.49 86.33 87.30 NAR23 NM24 ICML23 NeurIPS24 ICLR25 113M 93M 500M BPE+A 6-mer+A VQ+A BERT BERT BERT 82.37 84.56 83.22 95.79 96.64 95.11 87.62 89.05 87.89 87.69 89.26 87. 100M 1.7M DC+A byte+CNN 6-mer+A BERT BERT 80.90 82.79 95.50 96.46 87.30 90.57 86.95 89.12 AR 84.87 96.95 90.30 90.71 arXiv25 1.3B Ours 380M byte+A MTR+AMTM 85.11 96.84 90.66 90.87 86M 7.9M byte+SSM byte+SSM 6-mer+A AR 79.96 94.65 85.97 85. BERT 80.14 94.74 83.42 85.02 AR 80.88 93.61 88.89 87.07 Table 1: Comparison on Genomic Benchmarks. Top-1 accuracy (%) averaged over several similar tasks is reported for popular DNA foundation models with SFT evaluation. The best and the second best results are marked as the bold and underlined types. Method Date # Params (M) H3 H3K4me1 H3K4me2 H3K4me3 H3K9ac H3K14ac H3K36me3 H3K79me3 H4 H4ac Enhancer Enhancer Types Promoter All Promoter Non-TATA Promoter TATA All Accpetor Donor Average (18 tasks) NeurIPS24 ICLR25 HyenaDNA Caduceus-PS DNABERT GROVER DNABERT2 NTv2-500M MxDNA ConvNova GENERator MergeDNA NeurIPS23 6.6M 78.14 44.52 42.68 50.41 58.50 56.71 59.92 66.25 78.15 54.15 53.13 48.16 95.57 95.86 95.88 94.05 96.98 95.27 70. ICML24 Bioinfo21 bioRxiv23 86M 77.41 43.83 32.38 31.49 52.55 46.51 50.98 60.48 79.60 41.53 79.13 54.73 97.05 97.02 96.22 97.83 97.81 98.43 68.61 ICLR24 117M 79.31 48.34 43.02 45.43 60.04 54.49 57.58 64.38 78.18 51.80 52.50 44.32 96.23 97.17 96.99 93.75 97.49 94.33 69.74 arXiv25 1.2B 80.60 55.30 42.40 51.20 61.20 60.50 65.70 67.00 81.50 59.20 58.00 47.70 96.20 96.20 94.80 97.80 98.10 97.80 72.84 NM24 500M 78.17 51.64 37.24 50.30 61.05 57.22 60.50 65.78 79.87 55.22 54.51 43.36 96.82 97.45 96.53 98.15 97.99 98.50 71.13 Ours 380M 82.95 56.24 55.67 64.10 65.01 68.51 68.19 74.23 81.06 67.26 79.84 60.62 97.40 97.35 96.70 98.35 98.67 98.93 78.39 100M 82.78 56.15 55.59 63.68 64.78 68.27 67.05 74.29 81.18 67.65 79.90 60.50 97.16 97.24 96.01 98.14 98.01 98.10 78. 1.7M 81.50 56.60 57.45 67.15 68.10 70.71 68.31 72.08 81.12 66.10 57.60 49.75 96.82 96.76 96.34 96.33 96.23 96.62 76.42 1.9M 80.48 52.83 49.88 56.72 63.27 60.84 61.12 67.17 80.10 59.26 55.20 47.17 96.65 96.31 96.21 92.87 94.21 94.69 72.50 87M 76.80 46.10 40.30 45.80 62.60 54.80 56.30 58.10 76.90 53.00 51.60 43.30 92.60 92.50 89.10 91.90 91.20 88.80 67.32 Table 2: Comparison on NT Benchmark. Matthews Correlation Coefficient (MCC) (%) or F1 score (%) is reported for subtasks with SFT evaluation. The best and the second best results are marked as the bold and underlined types. cover dropped information without overweighting the lowinformation content in its training signal."
        },
        {
            "title": "4.1 Experimental Setup\nImplementations. Following the Transformer architec-\nture as LLaMA (Touvron et al. 2023), MergeDNA adopts\nan embed dimension of D = 1024 and the local window\nsize of 16. The Local Encoder and Decoder stack 4 and\n2 Local ToMeAttention blocks, while the Latent Encoder\nand Latent Decoder use 20 and 4 Transformer blocks, total-\ning 380M parameters. Following DNABERT-2 (Zhou et al.\n2023), we pre-train MergeDNA on the Multi-Species\nGenomes corpus using AdamW optimizer (Loshchilov and\nHutter 2019) for 100K iterations with a base learning rate of\n1 × 10−4 and maximum sequence length of 4096. The hi-\nerarchical compression yields a local encoder output length\nL= N\n2 , effectively model-\ning long-range context with reduced complexity. For down-",
            "content": "2 and latent encoder length K= stream tasks, we adhere to the benchmark-specific SFT protocols. On sequence-level classification tasks, we discard both decoders and fine-tune classification head on the latent encoders output. For token-level (base-resolution) tasks, we retain the Local Decoder to recover sequence resolution and fine-tune new token-level prediction head. View more details in the Appendix. Comparison Baselines. We compare MergeDNA with state-of-the-art genomics models across four architecture paradigms: (1) sequence modeling architectures with SSMs have HyenaDNA (Nguyen et al. 2024b) and Caduceus (Schiff et al. 2024), (2) Standard Transformers have DNABERT (Ji et al. 2021), DNABERT-2 (Zhou et al. 2023), NTv1/v2 (Dalla-Torre et al. 2023), GROVER (Sanabria, Hirsch, and Poetsch 2023), and GenSLM (Zvyagin et al. 2022)), (3) Hybrid models have Evo (Nguyen et al. 2024a), Evo2 (Brixi et al. 2025), and HybriDNA (Ma et al. 2025)), and (4) CNN is ConvNova (Bo et al. 2025). As for the tokenizer, four popular types are compared in Table 1: (1) Byte-level like Evo, (2) k-mer like NTv2, (3) BPE like DNABERT2, (4) DNA dynamic tokenizer methods have VQDNA (Li et al. 2024a) and MxDNA (Qiao et al. 2024). All baseline foundation models were pre-trained with standard masked language modeling (BERT) or autoregressive (AR) objectives. As for downstream tasks, typical specialist models are also included. Method Date # Params (M) Epigenetic Marks Prediction (10) Human TF Detection (3) Mouse TF Detection (3) Core Promoter Detection (3) Promoter Detection (3) Splice Site Reconstructed (1) Virus Covid Classification (1) Average (24 tasks) ICML24 Bioinfo21 NM24 HyenaDNA Caduceus-PS DNABERT NT-multi DNABERT2 VQDNA MxDNA ConvNova HybriDNA-7B MergeDNA NeurIPS23 6.6M 58.94 61.74 64.37 69.22 80.14 77.76 25.88 62. ICML24 NeurIPS24 ICLR25 100M 67.29 67.29 ICLR24 117M 55.98 70.11 67.99 70.53 84.21 84.99 71.02 66.43 arXiv25 7B 63.05 72.89 78.02 71.37 85.53 90.09 74.02 76.42 Ours 380M 68.82 72.24 73.21 73.41 87.73 89.95 74.41 77.11 93M 57.95 70.56 69.80 73.37 86.58 89.53 74.32 68.51 86M 49.08 64.17 56.43 71.81 81.69 84.07 55.50 60. 2.5B 58.06 63.34 67.02 71.63 88.15 89.35 73.04 67.23 1.7M 68.91 68.91 1.9M 58.39 58.39 Table 3: Comparison on GUE Benchmark. Matthews Correlation Coefficient (MCC) (%) or F1 score (%) averaged across sub-tasks is reported with SFT evaluation. The best and the second best results are marked as the bold and underlined types."
        },
        {
            "title": "4.2 Comparison Results on Genomic Benchmarks",
            "content": "Genomic Benchmarks. We first evaluate on eight representative tasks from the Genomic Benchmark suite (Greˇsova et al. 2023), covering enhancer identification, species classification, and regulatory element prediction. All models are fine-tuned on each task, and we report top-1 accuracy following the GenBench protocol. As shown in Table 1, MergeDNA achieves the highest overall accuracy (90.87%), outperforming all prior DNA foundation models. Notably, it yields state-of-the-art results on the enhancer tasks (85.11% vs 84.87% by the second best) and regulatory element tasks, while maintaining competitive performance on species classification (second only to larger model). These improvements underscore the advantages of our context-aware tokenizer and hierarchical modeling in understanding generic genomic sequences. Nucleotide Transformer Benchmarks. We further compare on the comprehensive Nucleotide Transformer (NT) benchmark (18 tasks) (Dalla-Torre et al. 2023) as summarized in Table 2. This benchmark includes diverse mix of epigenomic signal classification (various histone marks and enhancer states, measured by MCC or F1) and core promoter/splice site detection tasks. MergeDNA again attains the best overall performance with an average score of 78.39, slightly surpassing the previous dynamic tokenizer method MxDNA (78.14) and substantially higher than other baselines. In particular, our model consistently ranks at or near the top on most individual tasks (e.g., achieving the highest MCC on 10 out of 18 tasks). GUE Benchmarks. We also evaluate on the Genome Understanding Evaluation (GUE) suite introduced by DNABERT2 (Zhou et al. 2023), which aggregates 24 shortrange subtasks grouped into seven practical genomic applications: Epigenetic Mark Prediction (Yeast), Transcription Factor (TF) binding site detection (Human and Mouse), Promoter and Core Promoter Detection, Splice Site Prediction, and Virus Genomic Classification. We use the Matthews Correlation Coefficient (MCC) or the F1 score, as in prior work, and baseline results are taken from DNABERT2 or the original papers for consistency. As summarized in Table 3, MergeDNA delivers the highest mean performance (77.11%), edging out the much larger HybriDNA-7B (76.42%) and outperforming all other foundation models. These results highlight that MergeDNAs dynamic tokenization and dual-context pre-training yield broad improvements across heterogeneous genomic prediction tasks. Method SpliceAI DNABERT2 NT-500M Caduceus Evo2-7B MergeDNA # Params 500M 55.7 Donor Acceptor 72.2 63.9 Mean 117M 63.5 70.7 67.1 380M 64.4 74.5 69. 3.5M 57.4 69.1 63.2 7.9M 64.2 74.0 69.1 7B 64.5 74.3 69.2 Table 4: Comparison on Splicing Prediction on the SpliceAI dataset, where the AUROC score is reported. Method GenSLM-2.5B NT-2500M ESM2-650M Evo-7B Evo2-7B MergeDNA # Params Bacteria Human 650M 51.2 37. 380M 42.72 20.58 7B 45.30 11.10 7B 45.85 36.9 2.5B 9.4 4.7 2.5B 24.7 6.9 Table 5: Comparison on Protein Fitness Prediction. Zeroshot SRCC (%) is reported on DMS datasets."
        },
        {
            "title": "4.3 Multi-omics Downstream Tasks\nWe further assess MergeDNA’s generalization to long-range\nand cross-omics scenarios, including RNA splicing, expres-\nsion prediction, and protein tasks.",
            "content": "RNA Splicing Site Prediction. Pre-mRNA splicing is crucial step in gene expression, and we evaluate our model on the SpliceAI dataset (Jaganathan et al. 2019), which provides long pre-mRNA sequences labeled with donor and acceptor splice sites. We treat this as binary sequence classification (site vs. non-site) and report the area under the ROC curve (AUROC). In Table 4, MergeDNA achieves mean AUROC of 69.8, substantially outperforming the classic SpliceAI model (63.2) and all prior DNA foundation models. MergeDNA nearly matches the 7B-parameter Evo2 on donor site prediction and exceeds it on acceptor sites. Long-range Expression Prediction. We next consider two challenging expression quantitative trait tasks from the Genomics Long-Range Benchmark (LRB) (Trop et al. 2024) that demand modeling of kilobase-scale contexts. (i) Causal eQTL Effect Prediction: given genomic locus and candidate variant, predict if the variant alters gene expression (evaluated by AUROC). (ii) Bulk RNA Expression Prediction: predict gene expression levels from the surrounding DNA sequence (evaluated by R2 correlation). As shown in Table 6, MergeDNA attains new state-of-the-art results on both tasks: an AUROC of 0.75 for eQTL (vs 0.74 by the best baseline) and an R2 of 0.62 for bulk expression (vs 0.60). Protein Fitness Prediction. Finally, we further evaluate MergeDNA in strict zero-shot setting on protein fitness prediction tasks using Deep Mutational Scanning (DMS) data (Notin et al. 2022). Here, models must predict the functional fitness of protein variants (amino acid mutations) diFigure 3: Visualization of Token Length Distributions for (a) BPE (Zhou et al. 2023), (b) MxDNA (Qiao et al. 2024), and (c) MergeDNA across different genomic contexts. Baseline tokenizers show static, context-agnostic distribution, while MergeDNA adaptively changes its tokenization strategy based on the sequence type, demonstrating strong context-awareness. Method # Params Causal eQTL (AUROC) Bulk RNA (R2) DNABERT2 DNABERT-S NTv2-500M HyenaDNA-160K Caduceus-131K HybridDNA-131K Evo2-7B MergeDNA 117M 0.72 0.51 117M 0.73 0.52 500M 0.72 0.60 12.9M 0.71 0.46 7.7M 0.68 0.52 300M 0.74 0. 7B 0.74 0.60 380M 0.75 0.62 Table 6: Comparison on LRB Benchmark with Causal eQTL Variant Effect Prediction and Bulk RNA Expression Prediction, where AUROC and R2 scores are reported. rectly from the DNA coding sequence, without any finetuning on protein data. Table 5 reports Spearmans rank correlation coefficient (SRCC) between predicted and actual fitness on two representative DMS datasets (one bacterial protein and one human protein). specialized protein language model (ESM2 (Lin et al. 2022), gray in the table) achieves the highest scores as expected. Among DNA-based models, MergeDNA shows strong cross-omics generalization: for the bacterial protein, it obtains 42.7% SRCCon par with the 7B Evo model and only slightly behind the multi-omics Evo2 (45.9%). On the human protein, MergeDNA (20.6% SRCC) substantially outperforms earlier DNA models like GenSLM (6.9%) and the original Evo (11.1%), though it trails Evo2, which leverages direct protein training."
        },
        {
            "title": "4.4 Empirical Analysis of Tokenization\nTo understand how MergeDNA learns to parse genomic se-\nquences, we analyze the vocabularies learned by different\ntokenization strategies. We compare MergeDNA with two\nrepresentative baseline tokenizers, BPE and MxDNA, by vi-\nsualizing the normalized frequency of token lengths (from\n1-mer to 16-mer) across different genomic contexts: pro-\nmoters, enhancers, and splice sites. The BPE tokenizer in\nFigure 3(a) produces a fixed, long-tailed distribution that\npeaks around a token length of 6, regardless of the underly-\ning sequence type. Similarly, the vector-quantized MxDNA\ntokenizer in Figure 3(b) yields a relatively uniform to-\nken length distribution that also shows minimal variation\nacross different genomic contexts. This context-agnostic be-\nhavior limits their ability to adaptively capture function-\nally relevant motifs of varying lengths. In sharp contrast,\nMergeDNA’s local encoder, as shown in Figure 3(c), demon-\nstrates strong context-awareness. It learns to produce dif-\nferent token length distributions tailored to the biological\nproperties of the input sequence. For splice sites, which\nare characterized by short, conserved motifs, the distribu-\ntion peaks at a shorter token length like k = 4. For longer\nand more complex regulatory regions like promoters and en-",
            "content": "hancers, the distributions shift towards longer tokens (peaking at = 7 and = 9, respectively). This data-driven tokenization allows MergeDNA to dynamically capture genomic motifs at their relevant biological scales, providing more expressive input representation for the downstream encoder and contributing to its superior performance. Byte Tokenizer Latent Enc. Local Dec. 24 layers Local Enc. (4) 20 layers Local Enc. (4) 20 layers Local Enc. (4) 20 layers Local Enc. (2) 20 layers 2 layers 2 layers 2 layers Lθ 2 layers Lθ 4 layers Lθ Pre-training LM T + LM Acc. 89.30 +0.39 +LAM +1.03 +LAM +1.57 +LAM +1. R + Lθ{ϕ} + λLθ{ϕ} + λLθ{ϕ} Lθ Table 7: Ablation Study of MergeDNA and pre-training tasks with DNA, protein, and central dogma (CD) tasks. Note that the blue background denotes the selected setups."
        },
        {
            "title": "4.5 Ablation Study\nWe conduct ablation experiments on the Genomic Bench-\nmark tasks to quantify the contribution of each component in\nMergeDNA. Table 7 summarizes the average top-1 accuracy\non the 8-task Genomic Benchmark under various configura-\ntions. First, we examine the impact of the hierarchical archi-\ntecture. Replacing the first 4 Transformer layers with our Lo-\ncal Encoder (merging tokens in windows) improves perfor-\nmance by +0.39 with the same parameter budget, confirming\nthe benefit of local token merging. Next, we ablate the pre-\ntraining objectives. Training with only the naive masked to-\nken modeling (MTM) objective results in suboptimal perfor-\nmance; adding the Merged Token Reconstruction (LM T R)\nobjectives targeting the tokenizer’s output provides a large\ngain (+1.03), and further introducing the Adaptive MTM on\nfiltered tokens pushes the improvement to +1.57 over base-\nline. We also find that scaling down the loss weight λ for the\nlatent LM T R (i.e., not directly updating tokenizer param-\neters) to 0.25 is crucial for better generalization, yielding\nthe best result. Finally, we vary the depth of the Local En-",
            "content": "coder: using only 2 local merging layers (with correspondingly more latent decoder layers) degrades performance, indicating that deeper tokenizer (4 merging blocks) is important for capturing rich subword representations. several downstream DNA, RNA, and protein tasks, broader evaluations in single-cell, epigenomic, metagenomic, and clinical variant interpretation settings are necessary to characterize its strengths and limitations thoroughly."
        },
        {
            "title": "5 Conclusions and Limitations\nConclusions. We introduce MergeDNA, a context-aware\nDNA foundation model that addresses fundamental chal-\nlenges in genome modeling: heterogeneous information\ndensity, ambiguous sequence tokenization, and long-range\ndependencies. MergeDNA unifies a differentiable local tok-\nenizer and a global latent Transformer through a hierarchical\narchitecture and two complementary pre-training tasks, i.e.,\nMerged Token Reconstruction and Adaptive Masked Token\nModeling. These innovations enable the model to dynami-\ncally adjust token granularity and focus on salient regions\nacross diverse genomic contexts. Extensive experiments on\nthree standard DNA benchmarks and several multi-omics\ntasks demonstrate that MergeDNA achieves state-of-the-art\nperformance with strong generalization across species and\nmodalities, offering a scalable and principled approach to\ngenome-scale representation learning.",
            "content": "Limitations and Future Work. Our study is subject to several practical limitations that also suggest promising avenues for future research. (1) Genome-scale contexts. MergeDNA is pre-trained on sequences up to 4k bases and evaluated on tasks spanning up to tens of kilobases via windowed processing and aggregation. While this setup already covers many regulatory elements, fully genome-scale modeling at chromosomeor genome-wide resolution remains beyond the current scope. Extending MergeDNA to 100k1M context lengths will likely require additional hierarchical stages, more aggressive yet structured token compression, and improved memory management. (2) Interpretability of dynamic tokenization. Currently, we do not yet provide systematic biological interpretation of the resulting merged tokens. In particular, it remains an open question how frequently merged tokens align with canonical motifs such as splice sites, transcription factor binding sites, or promoter elements, and how tokenization behaves in repetitive or low-complexity regions. Future work could integrate motif discovery tools, attribution methods, and sequence perturbation analyses to connect token merging decisions more directly. (3) Connections from DNA tokenizers to tokenizer-free models. Conceptually, MergeDNA can be viewed as dynamic tokenizer tightly coupled with hierarchical encoderdecoder, positioned between static k-mer/BPE-based DNA tokenizers and fully tokenizer-free byte-level architectures. In this work, we do not systematically explore hybrid designs that combine MergeDNA with recent tokenizer-free long-sequence models, nor do we examine using MergeDNA as front-end for such encoders. Designing architectures that treat merged tokens as adaptive patches for downstream byte-level models, or that incorporate differentiable segmentation ideas from tokenizerfree sequence modeling, may further improve the tradeoff between flexibility, scalability, and modeling fidelity. (4) Multi-omics integration. Currently, MergeDNA is pretrained on the Multi-Species Genomes corpus and evaluated on representative but still limited set of genomic and multi-omics benchmarks. Although the model generalizes to Acknowledgments This work was supported by the National Natural Science Foundation of China (Project No. 624B2115, 623B2086, and U21A20427), the Science & Technology Innovation 2030 Major Program (Project No. 2021ZD0150100), the Center of Synthetic Biology and Integrated Bioengineering at Westlake University (Project No. WU2022A009), and the Westlake University Industries of the Future Research Program (Project No. WU2023C019). This work was done when Siyuan Li interned at BioMap Research. We thank the GPU support from BioMap Research and the AI station of Westlake University. References AlQuraishi, M. 2019. ProteinNet: standardized data set for machine learning of protein structure. In BMC Bioinformatics, 95104. Anonymous. 2025. CDBridge: Cross-omics Post-training Bridge Strategy for Context-aware Biological Modeling. ˇZ.; Agarwal, V.; Visentin, D.; Ledsam, J. R.; Avsec, Grabska-Barwinska, A.; Taylor, K. R.; Assael, Y.; Jumper, J.; Kohli, P.; and Kelley, D. R. 2021. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10): 11961203. Avsec, ˇZ.; Latysheva, N.; Cheng, J.; Novati, G.; Taylor, K. R.; Ward, T.; Bycroft, C.; Nicolaisen, L.; Arvaniti, E.; Pan, J.; et al. 2025. AlphaGenome: advancing regulatory variant effect prediction with unified DNA sequence model. bioRxiv, 202506. Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan, Y.; Ge, W.; Han, Y.; Huang, F.; Hui, B.; Ji, L.; Li, M.; Lin, J.; Lin, R.; Liu, D.; Liu, G.; Lu, C.; Lu, K.; Ma, J.; Men, R.; Ren, X.; Ren, X.; Tan, C.; Tan, S.; Tu, J.; Wang, P.; Wang, S.; Wang, W.; Wu, S.; Xu, B.; Xu, J.; Yang, A.; Yang, H.; Yang, J.; Yang, J.; Yang, S.; Yao, Y.; Yu, B.; Bowen, Y.; Yuan, H.; Yuan, Z.; Zhang, J.; Zhang, X.; Zhang, Y.; Zhang, Z.; Zhou, C.; Zhou, J.; Zhou, X.; and Zhu, T. 2023. Qwen Technical Report. ArXiv, abs/2309.16609. Benegas, G.; Batra, S. S.; and Song, Y. S. 2023. DNA language models are powerful predictors of genome-wide variant effects. Proceedings of the National Academy of Sciences, 120(44): e2311219120. Bo, Y.; Mao, W.; Shao, Y.; Bai, W.; Ye, P.; Ma, X.; Zhao, J.; Chen, H.; and Shen, C. 2025. Revisiting Convolution Architecture in the Realm of DNA Foundation Models. arXiv preprint arXiv:2502.18538. Bolya, D.; Fu, C.-Y.; Dai, X.; Zhang, P.; Feichtenhofer, C.; and Hoffman, J. 2023. Token Merging: Your ViT But Faster. In International Conference on Learning Representations. Brixi, G.; Durrant, M. G.; Ku, J.; Poli, M.; Brockman, G.; Chang, D.; Gonzalez, G. A.; King, S. H.; Li, D. B.; Merchant, A. T.; Naghipourfar, M.; Nguyen, E.; Ricci-Tam, C.; Romero, D. W.; Sun, G.; Taghibakshi, A.; Vorontsov, A.; Yang, B.; Deng, M.; Gorton, L.; Nguyen, N.; Wang, N. K.; Adams, E.; Baccus, S. A.; Dillmann, S.; Ermon, S.; Guo, D.; Ilango, R.; Janik, K.; Lu, A. X.; Mehta, R.; Mofrad, M. R.; Ng, M. Y.; Pannu, J.; Re, C.; Schmok, J. C.; St. John, J.; Sullivan, J.; Zhu, K.; Zynda, G.; Balsam, D.; Collison, P.; Costa, A. B.; Hernandez-Boussard, T.; Ho, E.; Liu, M.-Y.; McGrath, T.; Powell, K.; Burke, D. P.; Goodarzi, H.; Hsu, P. D.; and Hie, B. L. 2025. Genome modeling and design across all domains of life with Evo 2. Arc Institute Manuscripts. Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020. Simple Framework for Contrastive Learning of Visual In International Conference on Machine Representations. Learning (ICML). Cheng, W.; Song, Z.; Zhang, Y.; Wang, S.; Wang, D.; Yang, M.; Li, L.; and Ma, J. 2025. DNALongBench: Benchmark Suite for Long-Range DNA Prediction Tasks. bioRxiv. Cooper, S. 1981. The central dogma of cell biology. Cell biology international reports, 5(6): 539549. Dai, J.; Qi, H.; Xiong, Y.; Li, Y.; Zhang, G.; Hu, H.; and Wei, Y. 2017. Deformable Convolutional Networks. In International Conference on Computer Vision (ICCV), 764773. Dalla-Torre, H.; Gonzalez, L.; Mendoza-Revilla, J.; Carranza, N. L.; Grzywaczewski, A. H.; Oteri, F.; Dallago, C.; Trop, E.; de Almeida, B. P.; Sirelkhatim, H.; et al. 2023. The nucleotide transformer: Building and evaluating robust foundation models for human genomics. bioRxiv, 202301. Dao, T.; Fu, D. Y.; Ermon, S.; Rudra, A.; and Re, C. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. ArXiv, abs/2205.14135. de Almeida, B. P.; Dalla-Torre, H.; Richard, G.; Blum, C.; Hexemer, L.; Gelard, M.; Mendoza-Revilla, J.; Pandey, P.; Laurent, S.; Lopez, M.; et al. 2024. SegmentNT: annotating the genome at single-nucleotide resolution with DNA foundation models. bioRxiv, 202403. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for In Proceedings of the North Language Understanding. American Chapter of the Association for Computational Linguistics (NAACL), 41714186. Dreos, R.; Ambrosini, G.; Cavin Perier, R.; and Bucher, P. 2013. EPD and EPDnew, high-quality promoter resources in the next-generation sequencing era. Nucleic acids research, 41(D1): D157D164. Duan, C.; Zang, Z.; Xu, Y.; He, H.; Liu, Z.; Song, Z.; Zheng, J.-S.; and Li, S. Z. 2025. FGBERT: Function-Driven Pretrained Gene Language Model for Metagenomics. Briefings in Bioinformatics. Garau-Luis, J. J.; Bordes, P.; Gonzalez, L.; Roller, M.; de Almeida, B. P.; Hexemer, L.; Blum, C.; Laurent, S.; Grzegorzewski, J.; Lang, M.; Pierrot, T.; and Richard, G. 2024. Multi-modal Transfer Learning between Biological Foundation Models. In Globerson, A.; Mackey, L.; Belgrave, D.; Fan, A.; Paquet, U.; Tomczak, J.; and Zhang, C., eds., Advances in Neural Information Processing Systems, volume 37, 7843178450. Curran Associates, Inc. Ge, H.; Feng, J.; Huang, Q.; Fu, F.; Nie, X.; Zuo, L.; Lin, H.; Cui, B.; and Liu, X. 2025. ByteScale: Efficient Scaling of LLM Training with 2048K Context Length on More Than 12,000 GPUs. ArXiv, abs/2502.21231. Greˇsova, K.; Martinek, V.; ˇCechak, D.; ˇSimeˇcek, P.; and Alexiou, P. 2023. Genomic benchmarks: collection of datasets for genomic sequence classification. BMC Genomic Data, 24(1): 25. Gu, A.; and Dao, T. 2023. Mamba: Linear-Time SeArXiv, quence Modeling with Selective State Spaces. abs/2312.00752. Hayes, T.; Rao, R.; Akin, H.; Sofroniew, N. J.; Oktay, D.; Lin, Z.; Verkuil, R.; Tran, V. Q.; Deaton, J.; Wiggert, M.; et al. 2025. Simulating 500 million years of evolution with language model. Science, 387(6736): 850858. He, Y.; Fang, P.; Shan, Y.; Pan, Y.; Wei, Y.; Chen, Y.; Chen, Y.; Liu, Y.; Zeng, Z.; Zhou, Z.; et al. 2024. LucaOne: Generalized Biological Foundation Model with Unified Nucleic Acid and Protein Language. bioRxiv, 202405. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Hwang, S.; Wang, B.; and Gu, A. 2025. Dynamic Chunking for End-to-End Hierarchical Sequence Modeling. arXiv preprint arXiv:2507.07955. Jaegle, A.; Borgeaud, S.; Alayrac, J.-B.; Doersch, C.; Ionescu, C.; Ding, D.; Koppula, S.; Brock, A.; Shelhamer, E.; Henaff, O. J.; Botvinick, M. M.; Zisserman, A.; Vinyals, Perceiver IO: General O.; and Carreira, J. 2021a. ArXiv, Architecture for Structured Inputs & Outputs. abs/2107.14795. Jaegle, A.; Gimeno, F.; Brock, A.; Zisserman, A.; Vinyals, O.; and Carreira, J. 2021b. Perceiver: General Perception with Iterative Attention. ArXiv, abs/2103.03206. Jaganathan, K.; Kyriazopoulou Panagiotopoulou, S.; McRae, J. F.; Darbandi, S. F.; Knowles, D.; Li, Y. I.; Kosmicki, J. A.; Arbelaez, J.; Cui, W.; Schwartz, G. B.; Chow, E. D.; Kanterakis, E.; Gao, H.; Kia, A.; Batzoglou, S.; Sanders, S. J.; and Farh, K. K.-H. 2019. Predicting Splicing from Primary Sequence with Deep Learning. Cell, 176(3): 535548.e24. Ji, Y.; Zhou, Z.; Liu, H.; and Davuluri, R. V. 2021. DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome. Bioinformatics, 37(15): 21122120. Ji, Z.; Zhang, H.; Huang, J.; et al. 2023. GENA-LM: Multimodal Pre-training for the Central Dogma. bioRxiv. Jumper, J.; Evans, R.; Pritzel, A.; Green, T.; Figurnov, M.; Ronneberger, O.; Tunyasuvunakool, K.; Bates, R.; ˇZıdek, A.; Potapenko, A.; et al. 2021. Highly accurate protein structure prediction with AlphaFold. nature, 596(7873): 583 589. Khare, S.; Gurry, C.; Freitas, L.; Schultz, M. B.; Bach, G.; Diallo, A.; Akite, N.; Ho, J.; Lee, R. T.; Yeo, W.; et al. 2021. GISAIDs role in pandemic response. China CDC weekly, 3(49): 1049. Krishna, R.; Wang, J.; Ahern, W.; Sturmfels, P.; Venkatesh, P.; Kalvet, I.; Lee, G. R.; Morey-Burrows, F. S.; Anishchenko, I.; Humphreys, I. R.; et al. 2024. Generalized biomolecular modeling and design with RoseTTAFold AllAtom. Science, 384(6693): eadl2528. Kudo, T.; and Richardson, J. 2018. SentencePiece: simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Conference on Empirical Methods in Natural Language Processing. Lee, D. H.; and Hong, S. 2024. Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers. In Conference on Neural Information Processing Systems (NeurIPS). Li, Q.; Wu, W.; Zhu, Y.; Feng, F.; Ye, J.; and Wang, Z. 2025a. GENERanno: Genomic Foundation Model for Metagenomic Annotation. bioRxiv. Li, S.; Wang, Z.; Liu, Z.; Wu, D.; Tan, C.; Zheng, J.; Huang, Y.; and Li, S. Z. 2024a. VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling. In International Conference on Machine Learning (ICML). Li, Z.; Cranganore, S. S.; Youngblut, N. D.; and Kilbertus, N. 2024b. Whole Genome Transformer for Gene Interaction Effects in Microbiome Habitat Specificity. ArXiv, abs/2405.05998. Li, Z.; Ni, Y.; Beardall, W. A. V.; Xia, G.; Das, A.; Stan, G.- B. V.; and Zhao, Y. 2024c. DiscDiff: Latent Diffusion Model for DNA Sequence Generation. ArXiv, abs/2402.06079. Li, Z.; Subasri, V.; Shen, Y.; Li, D.; Zhao, Y.; Stan, G.- B.; and Shan, C. 2025b. Omni-DNA: Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning. arXiv preprint arXiv:2502.03499. Lin, Z.; Akin, H.; Rao, R.; Hie, B.; Zhu, Z.; Lu, W.; Smetanin, N.; dos Santos Costa, A.; Fazel-Zarandi, M.; Sercu, T.; Candido, S.; et al. 2022. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv. Liu, G.; Chen, L.; Wu, Y.; Han, Y.; Bao, Y.; and Zhang, T. 2025a. PDLLMs: group of tailored DNA large language models for analyzing plant genomes. Molecular Plant, 18(2): 175178. Liu, H.; and Wang, H. 2024. GenoTEX: Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians. ArXiv, abs/2406.15341. Liu, Y.; Ji, T.; Sun, C.; Wu, Y.; and Wang, X. 2024a. Generation with Dynamic Vocabulary. In Conference on Empirical Methods in Natural Language Processing. Liu, Z.; Li, J.; Li, S.; Zang, Z.; Tan, C.; Huang, Y.; Bai, Y.; and Li, S. Z. 2024b. GenBench: Benchmarking Suite for Systematic Evaluation of Genomic Foundation Models. arXiv:2406.01627. Liu, Z.; Li, S.; Chen, Z.; Xin, L.; Wu, F.; Yu, C.; Yang, Q.; Guo, Y.; Yang, Y.; and Li, S. Z. 2025b. Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification. ArXiv, abs/2502.07299. Liu, Z.; Wang, L.; Li, S.; Wang, Z.; Lin, H.; and Li, S. Z. 2024c. LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory. In International Joint Conference on Artificial Intelligence (IJCAI). Loshchilov, I.; and Hutter, F. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations (ICLR). Ma, M.; Liu, G.; Cao, C.; Deng, P.; Dao, T.; Gu, A.; Jin, P.; Yang, Z.; Xia, Y.; Luo, R.; Hu, P.; Wang, Z.; Chen, Y.; Liu, H.; and Qin, T. 2025. HybriDNA: Hybrid TransformerMamba2 Long-Range DNA Language Model. ArXiv. Mendoza-Revilla, J.; Trop, E.; Gonzalez, L.; Roller, M.; Dalla-Torre, H.; de Almeida, B. P.; Richard, G.; Caton, J.; Lopez Carranza, N.; Skwark, M.; et al. 2023. Foundational Large Language Model for Edible Plant Genomes. bioRxiv, 202310. Nawrot, P.; Chorowski, J.; Łancucki, A.; and Ponti, E. M. 2022. Efficient Transformers with Dynamic Token Pooling. arXiv:2211.09761. Nguyen, E.; Poli, M.; Durrant, M. G.; Kang, B.; Katrekar, D.; Li, D. B.; Bartie, L. J.; Thomas, A. W.; King, S. H.; Brixi, G.; Sullivan, J.; Ng, M. Y.; Lewis, A.; Lou, A.; Ermon, S.; Baccus, S. A.; Hernandez-Boussard, T.; Re, C.; Hsu, P. D.; and Hie, B. L. 2024a. Sequence modeling and design from molecular to genome scale with Evo. Science, eado9336. Nguyen, E.; Poli, M.; Faizi, M.; Thomas, A.; Wornow, M.; Birch-Sykes, C.; Massaroli, S.; Patel, A.; Rabideau, C.; Bengio, Y.; et al. 2024b. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Advances in neural information processing systems, 36. Notin, P.; Dias, M.; Frazer, J.; Marchena-Hurtado, J.; Gomez, A. N.; Marks, D. S.; and Gal, Y. 2022. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ArXiv, abs/2205.13760. Outeiral, C.; and Deane, C. M. 2024. Codon language embeddings provide strong signals for use in protein engineering. Nature Machine Intelligence, 6(2): 170179. Pagnoni, A.; Pasunuru, R.; Rodriguez, P.; Nguyen, J.; Muller, B.; Li, M.; Zhou, C.; Yu, L.; Weston, J. E.; Zettlemoyer, L. S.; Ghosh, G.; Lewis, M.; Holtzman, A.; and Iyer, S. 2024. Byte Latent Transformer: Patches Scale Better Than Tokens. ArXiv, abs/2412.09871. Qiao, L.; Ye, P.; Ren, Y.; Bai, W.; Liang, C.; Ma, X.; Dong, N.; and Ouyang, W. 2024. Model Decides How to Tokenize: In Adaptive DNA Sequence Tokenization with MxDNA. Conference on Neural Information Processing Systems. Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018. Improving Language Understanding by Generative Pre-Training. Roy, S.; Sural, S.; and Ganguly, N. 2024. Unlocking Efficiency: Adaptive Masking for Gene Transformer Models. In European Conference on Artificial Intelligence. Roy, S.; Wallat, J.; Sundaram, S. S.; Nejdl, W.; and Ganguly, N. 2023. GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning. In European Conference on Artificial Intelligence. Sanabria, M.; Hirsch, J.; and Poetsch, A. R. 2023. The human genomes vocabulary as proposed by the DNA language model GROVER. bioRxiv, 202307. Scalzitti, N.; Kress, A.; Orhand, R.; Weber, T.; Moulinier, L.; Jeannin-Girardon, A.; Collet, P.; Poch, O.; and Thompson, J. D. 2021. Spliceator: multi-species splice site prediction using convolutional neural networks. BMC bioinformatics, 22: 126. Schiff, Y.; Kao, C.-H.; Gokaslan, A.; Dao, T.; Gu, A.; and Kuleshov, V. 2024. Caduceus: Bi-directional equivariant long-range dna sequence modeling. arXiv preprint arXiv:2403.03234. Sennrich, R.; Haddow, B.; and Birch, A. 2015. Neural Machine Translation of Rare Words with Subword Units. ArXiv, abs/1508.07909. Shao, B.; and Yan, J. 2024. long-context language model for deciphering and generating bacteriophage genomes. Nature Communications, 15(1): 9392. Slagle, K. 2024. SpaceByte: Towards Deleting Tokenization from Large Language Modeling. ArXiv, abs/2404.14408. Song, L.; Segal, E.; and Xing, E. P. 2024. Toward AI-Driven Digital Organism: Multiscale Foundation Models for Predicting, Simulating and Programming Biology at All Levels. ArXiv, abs/2412.06993. Tan, C.; Cao, Z.; Gao, Z.; Wu, L.; Li, S.; Huang, Y.; Xia, J.; Hu, B.; and Li, S. Z. 2025. MeToken: Uniform Microenvironment Token Boosts Post-Translational Modification Prediction. In International Conference on Learning Representations (ICLR). Theodoris, C. V.; Xiao, L.; Chopra, A.; Chaffin, M. D.; Al Sayed, Z. R.; Hill, M. C.; Mantineo, H.; Brydon, E. M.; Zeng, Z.; Liu, X. S.; et al. 2023. Transfer learning enables predictions in network biology. Nature, 618(7965): 616 624. Thoutam, V.; and Ellsworth, D. 2024. MSAMamba: Adapting Subquadratic Sequence Models To Long-Context DNA MSA Analysis. arXiv preprint. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation Language Models. ArXiv, abs/2302.13971. Trop, E.; Schiff, Y.; Marroquin, E. M.; Kao, C. H.; Gokaslan, A.; Polen, M.; Shao, M.; de Almeida, B. P.; Pierrot, T.; Li, Y. I.; et al. 2024. The Genomics Long-Range Benchmark: Advancing DNA Language Models. van den Oord, A.; Vinyals, O.; and Kavukcuoglu, K. 2017. Neural Discrete Representation Learning. In ArXiv. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Conference on Neural Information Processing Systems (NeurIPS), volume 30. Wang, J.; Gangavarapu, T.; Yan, J. N.; and Rush, A. M. 2024. Mambabyte: Token-free selective state space model. arXiv preprint arXiv:2401.13660. Wu, L.; Tian, Y.; Huang, Y.; Li, S.; Lin, H.; Chawla, N.; and Li, S. Z. 2024a. MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding. Wu, S.; Tan, X.; Wang, Z.; Wang, R.; Li, X.; and Sun, M. 2024b. Beyond Language Models: Byte Models are Digital World Simulators. arXiv:2402.19155. Wu, W.; Li, Q.; Li, M.; Fu, K.; Feng, F.; Ye, J.; Xiong, H.; and Wang, Z. 2025. GENERator: Long-Context Generative Genomic Foundation Model. arXiv preprint. Yang, Q.; Guo, Y.; Liu, Z.; Yang, Y.; Yin, Q.; Li, S.; Ji, S.; Chao, L. C.; and Zhang, X. 2026. TrinityDNA: BioInspired Foundational Model for Efficient Long-Sequence DNA Modeling. In The Fortieth AAAI Conference on Artificial Intelligence (AAAI). Yang, S.; Kautz, J.; and Hatamizadeh, A. 2024. Gated Delta Networks: Improving Mamba2 with Delta Rule. arXiv:2412.06464. Yang, S.; Wang, B.; Zhang, Y.; Shen, Y.; and Kim, Y. 2025. Parallelizing Linear Transformers with the Delta Rule over Sequence Length. arXiv:2406.06484. Yang, Z.; Fan, X.; Lan, M.; Li, X.; You, Y.; Tian, L.; Church, G.; Liu, X.; and Gu, F. 2024. Multiomic foundation model predicts epigenetic regulation by zero-shot. bioRxiv. Yang, Z.; Zhu, J.; and Su, B. 2025. SPACE: Your Genomic Profile Predictor is Powerful DNA Foundation Model. In International Conference on Machine Learning (ICML). Yu, L.; Simig, D.; Flaherty, C.; Aghajanyan, A.; Zettlemoyer, L.; and Lewis, M. 2023. MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. ArXiv, abs/2305.07185. Zhai, J.; Gokaslan, A.; Schiff, Y.; Berthel, A.; Liu, Z.-Y.; Lai, W.-Y.; Miller, Z. R.; Scheben, A.; Stitzer, M. C.; Romay, M. C.; et al. 2025. Cross-species modeling of plant genomes at single-nucleotide resolution using pretrained DNA language model. Proceedings of the National Academy of Sciences, 122(24): e2421738122. Zhang, D.; Zhang, W.; Zhao, Y.; Zhang, J.; He, B.; Qin, C.; and Yao, J. 2023. DNAGPT: generalized pre-trained tool for versatile DNA sequence analysis tasks. arXiv preprint arXiv:2307.05628. Zhang, X.; Yang, M.; Yin, X.; Qian, Y.; and Sun, F. 2024. Deepgene: An efficient foundation model for genomics based on pan-genome graph transformer. bioRxiv, 202404. Zhou, Z.; Ji, Y.; Li, W.; Dutta, P.; Davuluri, R.; and Liu, H. 2023. Dnabert-2: Efficient foundation model and benchmark for multi-species genome. arXiv preprint arXiv:2306.15006. Zhou, Z.; Riley, R.; Kautsar, S.; Wu, W.; Egan, R.; Hofmeyr, S.; Goldhaber-Gordon, S.; Yu, M.; Ho, H.; Liu, F.; et al. 2025a. GenomeOcean: An Efficient Genome Foundation Model Trained on Large-Scale Metagenomic Assemblies. bioRxiv, 202501. Zhou, Z.; Wu, W.; Ho, H.; Wang, J.; Shi, L.; Davuluri, R. V.; Wang, Z.; and Liu, H. 2025b. DNABERT-S: Pioneering species differentiation with species-aware DNA embeddings. Bioinformatics, 41: i255i264. Zhu, X.; Qin, C.; Wang, F.; Yang, F.; He, B.; Zhao, Y.; and Yao, J. 2024. CD-GPT: biological foundation model bridging the gap between molecular sequences through central dogma. bioRxiv, 202406. Zvyagin, M. T.; Brace, A.; Hippe, K.; Deng, Y.; Zhang, B.; Bohorquez, C. O.; Clyde, A.; Kale, B.; Perez-Rivera, D.; Ma, H.; et al. 2022. GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics. bioRxiv."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Pre-training Settings Following (Dalla-Torre et al. 2023; Zhou et al. 2023), we utilize pure DNA dataset for pre-training, Multi-species Genomes 1, with reference sequences (RefSeq) of multiple species to ensure generalization abilities of multiple domains from the National Center for Biotechnology Information (NCBI) database at https://www.ncbi.nlm.nih.gov. We pre-train the MergeDNA model (including the latent decoder and the local decoder) by AdamW optimizer (Loshchilov and Hutter 2019) for 100,000 iterations (randomly sampled datasets) with basic learning rate of 1 104 and batch size of 256. With the Ubuntu workstation, the model is pretrained by 8 Nvidia A100-80G GPUs with per-GPU batch size of 8 and gradient accumulation time of 16 for nearly 5 days. The experiments are implemented on host machine in Ubuntu (the kernel version of 5.4.0), which is equipped with 8 A100-80G GPUs, 128 CPU cores, and total memory of 1024 GB. Configuration Embedding dim Block number Block type Permanent Parameters Optimizer (β1, β2) Training iterations Weight decay Base learning rate Batch size LR scheduler Warmup iterations Gradient clipping Local Enc. Latent Enc. Latent Dec. Local Dec. 1024 4 20 2 Local-Attn Attention Attention Local-Attn 51M 253M 51M 25M AdamW (0.9, 0.95) 100,000 1 108 1 104 256 Cosine Annealing 10,000 1.0 Table A1: Configuration of the network architecture and pretraining for MergeDNA. The Local-Attn or Attention blocks denote the local-window or self-attention block with FlashAttention implementation. A.2 Evaluation Setups In most cases, we apply Supervised Fine-tuning (SFT) to evaluate the transfer capacity of pre-trained models to genomic and multi-omics downstream tasks. Following (Nguyen et al. 2024b; Zhou et al. 2023), adding the decoder head (e.g., an MLP head) to specific downstream task, the linear attention (RNN) or self-attention blocks in the pre-trained encoder models are frozen, while the Low-Rank Adaptation (LoRA) strategy (Hu et al. 2021) is employed to parameter-efficiently fine-tune the models by AdamW optimizer with batch size of 32. For each task, if the benchmark and models have provided hyper-parameters, we follow the official settings, or we choose the best combinations of the basic learning rate {1 105, 5 105, 1 104}, the weight decay {0, 0.01}, the LoRA rank {4, As for the Protein Fitness Prediction task, we conduct zero-shot evaluation 1Multi-species Genomes are originally provided in https: //huggingface.co/datasets/InstaDeepAI/multi species genomes, which //github.com/MAGICS-LAB/DNABERT 2 by DNABERT extended further in is https: by learning linear regression model upon the embedding of the Latent Encoder. GenBench (Liu et al. 2024b). As for the Protein Fitness Prediction task, we conduct zero-shot evaluation by learning linear regression model upon the embedding of the Latent Encoder. Overall, we report the averaged results over three runs with the optimal settings. Downstream Task Benchmarks B.1 DNA Tasks with Genomics Benchmark As proposed by (Greˇsova et al. 2023), three groups of basic genomic tasks are collected as balanced binary classification with top-1 accuracy in the Genomics Benchmark. Regarding enhancer prediction, three datasets are provided for identifying enhancer regions in the mouse and human genomes. Regarding species classification, two datasets are selected to identify sequences as either coding (exonic) or intergenic (non-coding) and to classify sequences as originating from humans or worms (C. elegans). Regarding the classification of regulatory elements, three datasets are utilized to categorize sequences as regulatory regions based on Ensembl annotations, identify open chromatin regions, or pinpoint nonTATA promoter regions in the human genome. Each example is 200bp central sequence extracted from the reference genome and labeled by high-throughput functional assays. We utilize the fully reproduced results of various DNA models in GenBench (Liu et al. 2024b). Table A2 provides full results of the Genomics Benchmark. B.2 DNA Tasks with Nucleotide Transformer (NT) Benchmark The NT benchmark (Dalla-Torre et al. 2023) spans 18 diverse tasks that probe both local and distal regulatory logic, including ten histone-mark predictions (e.g., H3K4me3), four promoter/enhancer subtasks, and four canonical splicesite detection variants. We adopt the original version of the NT benchmark, where sequences of 14 kbp surrounding epigenetic peaks are sampled from the hg38 assembly with chromosome-wise train/valid/test splits to prevent leakage. We use Matthews Correlation Coefficient (MCC) for imbalanced or noisy labels (histone marks, splice sites) and F1 score for balanced promoter/enhancer tasks. Table 2 presents the full results of the NT benchmark with popular DNA models. B.3 DNA Tasks with GUE Benchmark As proposed by DNABERT2 (Zhou et al. 2023), the GUE benchmark contains 24 datasets of 7 practical biological genome analysis tasks for 4 different species. All sequences are provided as FASTA strings (or BED coordinates) with an official 70/15/15 train/valid/test split; inputs range from 70 bp to 1 kb so that both local and distal regulatory logic is probed. We follow the original evaluation protocol and report MCC for all binary tasks, while the multi-class Covid variant classifier is scored with macro-F1. concise taskwise description is given below. (A) Promoter Detection (Human). Identify proximal promoters (249 +50 bp around TSS) in the GRCh38 assembly. Positive examples come from EPDnew (Dreos et al. 2013); negatives are random intergenic regions matched by GC content, stratified into TATA, non-TATA, and their Method Date # Params Mouse Enhancers Human Enhancers Cohn Human Enhancers Ensembl Coding vs Intergenomic Human vs Worm Human Regulatory Human OCR Ensembl Human NonTATA Promoters ICML24 Bioinfo21 HyenaDNA Caduceus-16 DNABERT DNABERT2 GENA-LM NT-500M VQDNA MxDNA ConvNova GENERator MergeDNA NeurIPS23 6.6M 79.34 72.96 90.33 90.97 96.24 93.08 79.14 94.45 NAR23 NM24 ICML23 NeurIPS24 ICLR25 93M 81.06 75.63 90.41 94.35 97.23 90.92 76.58 95.37 ICLR24 117M 81.82 75.87 90.75 93.58 97.39 87.94 75.82 95.24 arXiv25 1.3B 87.10 76.30 91.20 95.90 98.00 92.80 82.30 95. Ours 380M 85.62 76.54 93.18 96.02 97.65 93.49 82.16 96.34 500M 85.12 76.12 92.44 95.76 97.51 93.79 80.42 92.95 100M 80.57 74.67 93.13 95.28 97.64 94.11 81.05 96.56 113M 82.97 75.63 91.07 93.24 96.98 88.10 78.98 96.60 1.7M 78.40 74.30 90.00 94.30 96.70 87.30 79.30 95.30 86M 80.99 70.23 89.19 93.64 95.84 88.16 74.96 87. 7.9M 81.63 73.76 84.48 93.72 95.57 87.30 81.76 88.85 Table A2: Full Results on Genomic Benchmarks. Top-1 accuracy (%) averaged across three trials is reported for the latest DNA foundation models, where the best and the second best results are marked as the bold and underlined types. Method Date # Params Human TF-0 Human TF-1 Human TF-2 Human TF-3 Human TF-4 Mouse TF-0 Mouse TF-1 Mouse TF-2 Mouse TF-3 Mouse TF-4 Core Promoter (all) Core Promoter (no TATA) Core Promoter (TATA) Promoter (all) Promoter (no TATA) Promoter (TATA) Splice Reconstructed H3 H3K14ac H3K36me3 H3K4me1 H3K4me2 H3K4me3 H3K79me3 H3K9ac H4 H4ac Virus Covid Classification HyenaDNA Caduceus-PS DNABERT NT-2500M-multi DNABERT2 VQDNA MxDNA ConvNova HybriDNA-7B MergeDNA NeurIPS23 6.6M 64.47 70.74 60.44 39.78 73.27 56.25 80.46 78.14 60.83 46.25 66.18 67.41 74.07 83.04 91.03 66.36 77.76 78.14 56.71 59.92 44.52 42.68 50.41 66.25 58.50 78.15 54.15 25.88 ICML24 NeurIPS24 ICLR25 100M 82.14 68.29 65.46 54.97 55.30 63.80 73.74 63.15 80.89 65.14 Bioinfo21 86M 66.84 70.14 61.03 51.89 70.97 44.42 78.94 71.44 44.89 42.48 68.90 70.47 76.06 90.48 93.05 61.56 84.07 73.10 40.06 47.25 41.44 32.27 27.81 61.17 51.22 79.26 37.24 55. ICML24 1.9M 77.90 54.10 60.90 48.80 38.80 44.00 67.60 60.40 78.90 52.50 ICLR24 117M 71.99 76.0 66.52 58.54 77.43 56.76 84.77 79.32 66.47 52.66 69.37 68.04 74.17 86.77 94.27 71.59 84.99 78.27 52.57 56.88 50.52 31.13 36.27 67.39 55.63 80.71 50.43 71.02 arXiv25 7B 70.00 74.47 70.42 64.52 85.03 71.68 87.75 86.59 87.62 56.47 66.50 70.66 76.94 88.28 94.73 73.59 90.09 74.02 NM23 2.5B 66.64 70.28 58.72 51.65 69.43 63.31 83.76 71.52 69.44 47.07 70.33 71.58 72.97 91.01 94.00 79.43 89.35 78.77 56.20 61.99 55.30 36.49 40.34 64.70 56.01 81.67 49.13 73.04 Ours 380M 72.36 76.50 70.48 61.12 80.76 68.23 86.69 82.85 73.46 54.82 70.78 70.91 78.54 91.02 94.90 77.27 89.95 81.63 69.09 68.24 57.10 55.87 66.84 73.80 68.36 82.06 65.22 74.41 93M 72.48 76.43 66.80 58.92 78.10 58.34 85.81 80.39 69.72 54.73 71.02 70.58 78.50 90.75 94.40 74.52 89.50 79.21 54.46 61.75 53.28 34.05 39.10 68.47 56.63 81.84 50.69 74. 1.7M 81.50 70.71 68.31 56.60 57.45 67.15 72.08 68.10 81.12 66.10 Table A3: Full Results on GUE benchmark. MCC (%) or F1 (%) is reported for Epigenetic Marks Prediction, Human Transcription Factor (TF) Prediction, Mouse Transcription Factor Prediction, Core Promoter Detection, Promoter Detection, Splice Site Reconstructed, and Covid Variants Classification (Virus Covid). The best and the second best results are marked as the bold and underlined types. union (all)yielding 3 datasets. (B) Core-Promoter Detection (Human). Predict the 70 bp core promoter window (34 +35 bp) immediately flanking the TSS, harder variant of (A) due to shorter context. Positive/negative selection mirrors (A), giving 3 additional datasets. (C) Transcription-Factor Binding Site Prediction (Human). Classify 101 bp regions centered on ChIP-seq peaks from ENCODE (161 TFs, 91 cell lines). GUE retains 5 representative TFs after filtering trivial or extremely imbalanced cases; negatives are GC-matched genomic windows outside peaks. (D) Splice-Site Prediction (Human). Detect splice donor and acceptor sites within 400 bp windows extracted from GRCh38. To avoid saturation, the original 10k-sample dataset (Scalzitti et al. 2021) is adversarially augmented with hard negatives until MCC stabilizes, producing 1 challenging dataset. (E) Transcription-Factor Binding Site Prediction (Mouse). Analogous to (C) but using mouse ENCODE ChIP-seq (78 TFs). Five TFs are randomly chosen following the same GC-matched negative selection, resulting in 5 datasets. (F) Epigenetic-Mark Prediction (Yeast). Predict presence of 10 histone marks or nucleosome occupancy tracks in S. cerevisiae. Each sequence is 147 bp centered on experimentally validated sites downloaded from the JAIST repository; random genomic positions serve as negatives, yielding 10 datasets. (G) Covid Variant Classification (Virus). Multi-class identification of SARS-CoV-2 lineages (Alpha, Beta, ..., Zeta) from 1 kb genome snippets obtained via GISAIDs EpiCoV (Khare et al. 2021). B.4 Multi-omics Downstream Tasks RNA Splicing Site Prediction SpliceAI (Jaganathan et al. 2019) pairs 10kb sequences centered on annotated splice junctions with binary labels (donor, acceptor). Following (Liu et al. 2024b), we use the chromosome-held-out split: chromosomes 119 for training, 20 for validation, and 2122 for testing, guaranteeing isoform-level independence. Performance is evaluated by the Area Under the ROC Curve (AUROC) for each type of site and averaged between donor/acceptor. Long-Range Benchmarks (LRB) We adopt two longrange tasks collected in LRB (Trop et al. 2024). (a) Causal eQTL Variant Effect. Given 20kb locus and candidate SNV, the task is to classify whether the variant modulates gene expression; ground truth is derived from GTEx v8 finemapping. We report AUROC on the held-out tissue set proposed by (Trop et al. 2024). (b) Bulk RNA Expression. Predict log-transformed gene expression in 54 GTEx tissues from 40 kb upstream sequence window. Following LRB, we fit linear regressor on frozen sequence embeddings and compute the coefficient of determination (R2) on an unseen chromosome split. Zero-shot Protein Fitness Prediction Deep Mutational Scanning (DMS) (AlQuraishi 2019) assays exhaustively mutate protein coding sequence and measure functional fitness. We follow Evo (Nguyen et al. 2024a) and evaluate two representative datasets: Escherichia coli TEM-1 β-lactamase (Bacteria) and human BRCA1 RING domain (Human). All singleand double-mutant variants are used. True fitness values remain hidden during inference to emulate zero-shot generalization. We report Spearmans Rank Correlation Coefficient (SRCC) between model scores (negative log-likelihoods of mutated codons) and experimental fitness."
        },
        {
            "title": "C Extended Related Work",
            "content": "C.1 DNA Foundation Models Researchers have started to build large-scale sequence models for genomes since the 2020s, inspired by advances in natural language processing (Vaswani et al. 2017; Touvron et al. 2023). Early attempts like DNABERT (Ji et al. 2021) and DNAGPT (Zhang et al. 2023) demonstrated that Transformer architectures can be adapted to DNA by treating nucleotide sequences as language. Subsequent models greatly scaled up pre-training (GPN (Benegas, Batra, and Song 2023), Nucleotide Transformer variants (Dalla-Torre et al. 2023), and DNABERT2 (Zhou et al. 2023)), achieved genome-scale pre-training on human or multi-species data, and showed broad utility across diverse downstream genomic tasks (Greˇsova et al. 2023; Cheng et al. 2025). The methodologies of these DNA language models can be discussed along four key dimensions: (a) long sequence modeling, (b) DNA tokenization strategies, (c) pre-training objectives, and (d) pre-training domains and downstream tasks. Long Sequence Modeling. core technical challenge is modeling extremely long sequences (e.g., many gene regions span 8k1M bases). Several works have adopted statespace models (SSMs) (Gu and Dao 2023; Yang, Kautz, and Hatamizadeh 2024) for efficient long-range modeling: HyenaDNA (Nguyen et al. 2024b), MSAMamba (Thoutam and Ellsworth 2024), and Caduceus (Schiff et al. 2024) were among the first to use linear-time SSM architectures for DNA, enabling context lengths beyond what vanilla Transformers (Dao et al. 2022) can handle. More recently, MegaDNA (Shao and Yan 2024) introduced hierarchical Transformer for genome sequences, while models like Evo2 (Brixi et al. 2025), LifeCode (Liu et al. 2025b), and HybriDNA (2025) (Ma et al. 2025) combined SSMs with self-attention mechanisms to balance memory efficiency and modeling accuracy (Yang, Kautz, and Hatamizadeh 2024). Meanwhile, alternative sequence learners have been tried: ConvNova (Bo et al. 2025) applies convolutional neural networks to genomic sequences, and DeepGene (Zhang et al. 2024) leverages graph neural network over pan-genome graph to encode DNA context. DNA Vocabulary and Tokenization. Unlike natural language, DNA has no inherent word boundaries or semantics, and coding vs. non-coding regions have different significance (coding regions translate in triplets as codons) (Cooper 1981; Outeiral and Deane 2024). Accordingly, most recent DNA models (Nguyen et al. 2024a,b) forego complex tokenization and simply use the four nucleotides (A, C, G, T) as the base vocabulary. Some approaches employ fixed-length k-mers or subwords: DNABERT (Ji et al. 2021) and Nucleotide Transformer (Dalla-Torre et al. 2023) represent sequences as kmer tokens, and DNABERT2 (Zhou et al. 2023) and GENALM (Ji et al. 2023) build byte-pair encoding (BPE) vocabulary for DNA. Beyond static schemes, researchers have proposed dynamic data-driven tokenization, e.g., VQDNA (Li et al. 2024a) and MxDNA (Qiao et al. 2024) learn custom DNA tokens via Vector Quantization (VQ) (van den Oord, Vinyals, and Kavukcuoglu 2017; Wu et al. 2024a; Tan et al. 2025) and deformable convolution (Dai et al. 2017), automatically discovering higher-level nucleotide motifs. Pre-training Objectives. The training strategies for DNA foundation models mirror those in NLP (Devlin et al. 2019), mainly falling into masked language modeling (BERT-style) or autoregressive generation (Radford et al. 2018). Using nucleotide k-mers or subwords, encoder-based models like DNABERT2 and GENERanno (Li et al. 2025a) employ masked sequence modeling (predicting masked bases or k-mers from context), which has proven effective for many short-range genomic annotation tasks (Zhou et al. 2023). In contrast, many large parameter DNA models adopt autoregressive training to better model long-range dependencies (Ji et al. 2023; Benegas, Batra, and Song 2023). Evo2 (Brixi et al. 2025) further adjusts loss weights between coding and non-coding regions to emphasize functionally important context. Meanwhile, GeneMask (Roy et al. 2023) and CM-GEMS (Roy, Sural, and Ganguly 2024) introduce adaptive or focused masking schemes to accelerate pre-training and improve sequence representation. Beyond reconstruction-based objectives, some works pursue alternative pre-training tasks: DNABERT-S (Zhou et al. 2025b) uses contrastive learning (Chen et al. 2020) to learn speciesaware embeddings, and LifeCode (Liu et al. 2025b) incorporates knowledge distillation and cross-modal alignment into pre-training to fuse information across the central dogma (DNARNAprotein). Pre-training Domains and Downstream tasks. The choice of pre-training data strongly influences models generalization ability. Most DNA foundation models have been trained on the human genome (Nguyen et al. 2024b) or mixture of species (Zhou et al. 2023), enabling them to generalize across common genomic tasks in eukaryotes. However, some models target specific taxonomic docould be further extended to Metagenomic applications with multi-omics data (Zhou et al. 2025b; Duan et al. 2025). The second category allows gene-to-expression modeling, focusing on predicting cellular and molecular readouts from DNA sequence (Avsec et al. 2021, 2025) and bridging genotype to phenotype at the regulatory or cellular level. Enformer (Avsec et al. 2021) first demonstrated that deep Transformer can accurately predict gene expression profiles from DNA by accounting for long-range genomic interactions. Geneformer (Theodoris et al. 2023) showed the benefit of transfer learning for network biology predictions, IsoFormer (Garau-Luis et al. 2024) and SPACE (Yang, Zhu, and Su 2025) introduced multi-modal alignment and mixture-ofexperts to enhance regulatory sequence predictions, and AlphaGenome (Avsec et al. 2025) and CDBridge (Anonymous 2025) were recently proposed as unified model that can predict wide range of cell-level genomic assay results from the long DNA context. C.3 Byte Architectures Most traditional language models rely on pre-computed tokenizer to convert raw data into tokenscommonly using subword algorithms like BPE (Sennrich, Haddow, and Birch 2015) or SentencePiece (Kudo and Richardson 2018) to segment the input byte sequence. Complex dynamic tokenization methods, such as Dynamic Pooling (Nawrot et al. 2022) and Dynamic Vocabulary (Liu et al. 2024a), still involve training separate tokenization modules and do not fundamentally escape the constraints of discrete token boundaries. This tokenization step might introduce information loss or segmentation artifacts, motivating shift toward models that operate directly on byte sequences (Pagnoni et al. 2024; Hwang, Wang, and Gu 2025). With the advent of more efficient long-context sequence models (Gu and Dao 2023; Yang et al. 2025; Liu et al. 2024c), new class of bytelevel architectures has emerged to eliminate tokenization altogether (Wang et al. 2024). MegaByte (Yu et al. 2023) and SpaceByte (Slagle 2024) pioneered multiscale Transformers and SSMs that process sequences at the byte level, enabling context lengths on the order of millions of characters without arbitrary tokenization. Building on this, ByteScale (Ge et al. 2025) scaled up token-free models to unprecedented sizes (allowing training with 2-million token contexts on thousands of GPUs), while bGPT (Wu et al. 2024b) extended the tokenizer-free approach to multimodal data. Very recently, BLT (Pagnoni et al. 2024) performs entropy-based segmentation of byte sequences using latent Perceiver-style architecture (Jaegle et al. 2021b,a), whereas HNet (Hwang, Wang, and Gu 2025) provides fully differentiable tokenization scheme, enabling end-to-end learning of how to chunk byte sequences during model training. mains: Evo (Nguyen et al. 2024a), for example, was trained on prokaryotic genomes and excels at bacterial and viral tasks such as designing CRISPRCas9 target sequences. Likewise, AgroNT (Mendoza-Revilla et al. 2023), PlantCaduceus (Zhai et al. 2025), and PDLLMs (Liu et al. 2025a) are specialized on plant genomic data, and ProKaformer (Li et al. 2024b) is tailored to model microbial community (microbiome) genomes. DNABERT-S (Zhou et al. 2025b) builds species-differentiated embeddings for multi-species data, and GenomeOcean (Zhou et al. 2025a) leverages large-scale metagenomic assemblies to learn generalizable genome representations. After pre-training, DNA foundation models are evaluated on range of downstream tasks. Common tasks include token-level predictions and sequence-level classifications or regressions (e.g., predicting regulatory function or phenotype from sequence). For instance, SegmentNT (de Almeida et al. 2024) uses foundation model to partition genomes into meaningful segments at single-nucleotide resolution. Meanwhile, GPT models (Nguyen et al. 2024a; Zhu et al. 2024) can generate realistic DNA sequences (even whole genomes) in an autoregressive manner, and diffusion-based approaches (Li et al. 2024c) produce novel DNA sequences via latent diffusion. Finally, researchers are exploring hybrid AI systems that integrate general-purpose LLMs (Bai et al. 2023) with genomic expertise (Liu and Wang 2024; Li et al. 2025b) and large parameter scales (Yang et al. 2026), which combine open-source ChatGPT-like models with DNA foundation models to handle complex multi-step genomic applications. C.2 Multi-omics Modeling Beyond DNA-alone modeling, researchers are extending foundation models to multi-omics (Krishna et al. 2024; He et al. 2024), aiming to connect DNA with other molecular modalities like RNA and protein within the unified framework (Hayes et al. 2025; Nguyen et al. 2024a). Proteincentric models have already achieved remarkable success in structure prediction and design (e.g., AlphaFold-2 (Jumper et al. 2021)), which motivates developing DNA-centric multi-omics models that leverage genomic information. Recent research (Ji et al. 2023; Yang et al. 2024) uses DNA sequence models to predict downstream molecular phenotypes (e.g., protein function or chromatin profiles) across species. Based on the central dogma principle (Cooper 1981), current DNA-centered multi-omics models generally fall into two broad categories. The first category seeks to predict protein-level properties and functions directly from DNA sequences, effectively learning the end-to-end mapping from genome to proteome. Evo (Nguyen et al. 2024a) pioneered this approach by modeling sequence-to-function relationships at the genome scale, and its successor Evo2 (Brixi et al. 2025) further expanded to modeling genomes across all domains of life. AIDO (Song, Segal, and Xing 2024) and HybriDNA (Ma et al. 2025) are designed to handle ultra-long inputs (full genomes), CD-GPT (Zhu et al. 2024), LifeCode (Liu et al. 2025b), and GENERator (Wu et al. 2025) incorporate explicit transcription and translation tasks into their objectives, and frameworks like LucaOne (He et al. 2024) use supervised multi-task learning to unify representations of nucleic acids and proteins for diverse predictive tasks. They"
        }
    ],
    "affiliations": [
        "AI Lab, Research Center for Industries of the Future, Westlake University, China",
        "BioMap Research, Beijing, China",
        "Zhejiang University, Hangzhou, China"
    ]
}