{
    "paper_title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests",
    "authors": [
        "Tan-Hanh Pham",
        "Phu-Vinh Nguyen",
        "Dang The Hung",
        "Bui Trong Duong",
        "Vu Nguyen Thanh",
        "Chris Ngo",
        "Tri Quang Truong",
        "Truong-Son Hy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although large Vision-Language Models (VLMs) have demonstrated remarkable performance in a wide range of multimodal tasks, their true reasoning capabilities on human IQ tests remain underexplored. To advance research on the fluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed to evaluate VLMs on standardized visual IQ tests. We focus on evaluating the reasoning capabilities of VLMs, which we argue are more important than the accuracy of the final prediction. **Our benchmark is visually centric, minimizing the dependence on unnecessary textual content**, thus encouraging models to derive answers primarily from image-based information rather than learned textual knowledge. To this end, we manually collected and annotated 500 visual IQ questions to **prevent unintentional data leakage during training**. Unlike prior work that focuses primarily on the accuracy of the final answer, we evaluate the reasoning ability of the models by assessing their explanations and the patterns used to solve each problem, along with the accuracy of the final prediction and human evaluation. Our experiments show that there are substantial performance disparities between tasks, with models such as `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest average accuracies of 0.615, 0.578, and 0.548, respectively. However, all models struggle with 3D spatial and anagram reasoning tasks, highlighting significant limitations in current VLMs' general reasoning abilities. In terms of reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieved top averages of 0.696, 0.586, and 0.516, respectively. These results highlight inconsistencies between the reasoning processes of the models and their final answers, emphasizing the importance of evaluating the accuracy of the reasoning in addition to the final predictions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 0 0 0 2 1 . 5 0 5 2 : r IQBench: How Smart Are Vision-Language Models? Study with Human IQ Tests Tan-Hanh Pham1,,, Phu-Vinh Nguyen2,, Dang The Hung3, Bui Trong Duong4, Vu Nguyen Thanh5, Chris Ngo6, Tri Quang Truong5, Truong-Son Hy7 1Harvard Medical School, USA, 2Uppsala University, Sweden, 3University of London, UK, 4Vietnam Military Medical University, 5University of Technical Education Ho Chi Minh City, Vietnam, 6Knovel Engineering Lab, Singapore, 7University of Alabama at Birmingham, USA Equal contribution; Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Although large Vision-Language Models (VLMs) have demonstrated remarkable performance in wide range of multimodal tasks, their true reasoning capabilities on human IQ tests remain underexplored. To advance research on the fluid intelligence of VLMs, we introduce IQBench, new benchmark designed to evaluate VLMs on standardized visual IQ tests. We focus on evaluating the reasoning capabilities of VLMs, which we argue are more important than the accuracy of the final prediction. Our benchmark is visually centric, minimizing the dependence on unnecessary textual content, thus encouraging models to derive answers primarily from image-based information rather than learned textual knowledge. To this end, we manually collected and annotated 500 visual IQ questions to prevent unintentional data leakage during training. Unlike prior work that focuses primarily on the accuracy of the final answer, we evaluate the reasoning ability of the models by assessing their explanations and the patterns used to solve each problem, along with the accuracy of the final prediction and human evaluation. Our experiments show that there are substantial performance disparities between tasks, with models such as o4-mini, gemini-2.5-flash, and claude-3.7-sonnet achieving the highest average accuracies of 0.615, 0.578, and 0.548, respectively. However, all models struggle with 3D spatial and anagram reasoning tasks, highlighting significant limitations in current VLMs general reasoning abilities. In terms of reasoning scores, o4-mini, gemini-2.5-flash, and claude-3.7-sonnet achieved top averages of 0.696, 0.586, and 0.516, respectively. These results highlight inconsistencies between the reasoning processes of the models and their final answers, emphasizing the importance of evaluating the accuracy of the reasoning in addition to the final predictions. Code and data are publicly available at here."
        },
        {
            "title": "Introduction",
            "content": "The rapid evolution of artificial intelligence (AI) has been driven by the development of Large Language Models (LLMs), Multimodal Models, and Vision-Language Models (VLMs), which have significantly improved natural language understanding and cross-modal reasoning capabilities [1, 2]. Early LLMs, such as GPT-2 and GPT-3 [3], demonstrated strong performance in tasks such as text generation and question answering. Building on this progress, multimodal models such as CLIP [1] were developed to integrate visual and textual data, allowing tasks such as image captioning and zero-shot classification. More recently, open-source VLMs, including BLIP-2, PaLI, Flamingo, Preprint. Under review. and Qwen-VL [47], have pushed the boundaries of AI by allowing systems to process and reason over both visual and textual inputs with high accuracy. In parallel, industry efforts have introduced advanced multimodal models such as OpenAIs chatbot series (GPT-4o, o1, o3, o4-mini), Googles Gemini family, Anthropics Claude, and Xs Grok, which further enhance multimodal understanding and generation. These advancements have fueled interest in Artificial General Intelligence (AGI), defined as the ability to perform any intellectual task human can undertake [8], which is based on fluid intelligence, the ability to solve novel problems through abstract reasoning [9, 10]. Evaluating whether these models exhibit AGI-like intelligence requires robust metrics that encompass accuracy, reasoning transparency, and generalizability, supported by advanced data centers and evaluation frameworks [11, 12]. Numerous benchmarks have been developed to assess the VLM capabilities in diverse tasks. Benchmarks such as Visual Question Answering (VQA) [13], GQA [14], ScienceQA [15], and Visual Commonsense Reasoning (VCR) [16] evaluate models ability to interpret visual scenes, while benchmarks such as MMMU [17], MathVista [18], and ChartQA [19] focus on mathematical and analytical reasoning from visual input. Other efforts, such as Image2Struct [20], challenge models with low-vision tasks like extracting structure from images or rendering code based on visual content. However, these benchmarks are predominantly based on the accuracy of the final answer, which often does not reveal the reasoning process behind the predictions [21]. Studies have shown that high accuracy can stem from the use of biases in the dataset or shortcut learning rather than genuine reasoning [22, 23]. With recent advancements in reasoning-focused models such as OpenAIs o1, o3, o4-mini, GPT-4o [24, 25], Anthropics Claude 3.5 Sonnet [26], DeepSeek-R1 [27], Googles Gemini 2.5 Flash [28], and Grok-3 from xAI [29], evaluating whether these models exhibit such models requires robust metrics that go beyond surface-level performance, including accuracy, reasoning transparency and generalizability. Despite the increasing number of VLM benchmarks, significant limitations persist, particularly in evaluating fluid intelligence. Many benchmarks are nearing saturation, with state-of-the-art models achieving near-human performance in tasks such as VQA [15] and greater accuracy 85% in math and coding challenges [24]. This trend, along with the rise of reasoning models, suggests that existing benchmarks are increasingly ineffective in challenging VLMs or exposing their reasoning deficiencies. Although benchmarks such as MMLU [30] and ARC [10] assess reasoning, they are limited to textual tasks and lack the multimodal complexity of visual IQ tests. Even newer multimodal benchmarks like MathVista [18] and MMMU [17] do not specifically target fluid intelligence or reasoning interpretability, and recent multimodal models have already exceeded 80% accuracy in MMMU, suggesting that its effectiveness is becoming saturated. In particular, some models achieve strong results, such as Sphinx-X-MoE scores 43.6% on MMMU without images, outperforming its LLM backbone (17.9%) [31]. Such findings risk overestimating true multimodal ability and highlight weaknesses in current benchmarks. To address these challenges, we introduce IQBench, novel vision-centric benchmark designed to evaluate the fluid intelligence of VLMs using standardized visual IQ tests. IQBench comprises curated questions that consist of IQ tests, where each question is annotated with the correct answer and detailed reasoning pattern, facilitating in-depth analysis of the behavior of the model. Unlike previous work, we propose dual-metric evaluation framework comprising reasoning score, derived using an LLM-as-judge methodology to assess the accuracy and coherence of models explanation, and accuracy score for the final predictions. Complemented by human evaluations, this framework offers comprehensive assessment of VLM performance. Our evaluation of the leading VLMs on IQBench provides critical insight into their reasoning capabilities, laying the groundwork for developing more transparent and cognitively robust multimodal systems. Our contributions are summarized as follows: IQBench benchmark for fluid intelligence: We introduce IQBench, novel benchmark specifically designed to evaluate the fluid intelligence of VLMs using curated visual IQ test questions. Vision-Centric and manually curated dataset: To avoid unintentional data leakage during model training, we curated and annotated 500 visual IQ questions, including reasoning patterns and correct answers across wide range of topics. 2 Reasoning evaluation framework: We evaluated the prediction of the model using both an accuracy score and reasoning score to gain deeper insight into their reasoning capabilities. Comprehensive analysis of state-of-the-art VLMs: Through extensive experiments, we evaluated the leading VLMs on IQBench and uncover their strengths and weaknesses in various reasoning tasks."
        },
        {
            "title": "IQBench",
            "content": "We introduce IQBench, novel benchmark designed to evaluate the fluid intelligence of VLMs through standardized visual IQ tests. IQBench comprises 500 human-curated questions that encompass comprehensive range of IQ test domains, including pattern recognition, analogical reasoning, visual arithmetic, spatial understanding, abstract/concrete reasoning, number/figure series reasoning, anagrams, and verbal reasoning with syllogisms. Each question is annotated with the correct answer and detailed reasoning pattern, allowing granular analysis of the behavior of the model. Unlike existing benchmarks that focus primarily on the accuracy of the final response, IQBench emphasizes both the accuracy of predictions and the interpretability of the reasoning process, addressing critical gaps in evaluating VLM capabilities [3234]. 2.1 Data Collection and Generation The IQBench dataset was constructed through rigorous process of manual data collection and generation to ensure diversity and originality. We collect images from various sources, including online repositories, textbooks, and educational materials, which were then edited or used as inspiration to generate new questions and reasoning patterns with the topics indicated in Table 1. As shown in the table, the dataset contains 500 samples, divided equally into 10 reasoning topics, with 50 questions per topic. All images are saved in PNG format to keep them clear and consistent. Table 1: Dataset statistics for IQBench. Metric Total samples Number of topics Total sample of each topic Image type Question and answer format: Multiple choice Open questions Average question length (words) Average pattern length (words) Value 500 10 50 PNG 110 390 27 48 The questions are presented in two formats: multiple choice (110 questions) and open-ended (390 questions), with focus on open-ended questions to better test the models reasoning ability. On average, each question is 27 words long, and each reasoning pattern is about 48 words. This helps provide strong base for evaluating how well models can understand and explain their answers in different types of visual IQ problem. To build IQBench, we followed structured process that ensured both legal compliance and originality of the content. First, we collected raw visual materials from verified sources, ensuring that all content was in compliance with copyright regulations. When images were of low quality or their copyright status was unclear, we generated new visuals to maintain both clarity and legality. Next, we created original questions inspired by standard visual IQ tests. Each question was carefully designed to assess different reasoning skills and was manually annotated with the correct answer along with detailed reasoning pattern. 2.2 Data Quality Control To ensure the reliability and fairness of IQBench, we focused on maintaining human-generated high-quality content. This helps reduce data leakage and the likelihood that models have encountered 3 Figure 1: Representative IQ test covering logic, pattern recognition, and spatial reasoning for VLM evaluation. sample consists of an input image, question, an answer, and possible reasoning pattern. similar examples during pre-training, which could lead to inflated performance. To address the redundancy of images in many existing benchmarks, we designed IQBench to be vision-centric, minimizing the possibility of models answering questions based solely on language knowledge rather than visual understanding. We also applied lexical overlap analysis to detect and remove potential duplicate samples, ensuring the uniqueness of each question. In addition, we conducted thorough manual reviews to standardize 4 the format of all questions and answers, promoting consistency and clarity. An overview of the data is presented in Fig. 1, where each sample contains an input image, question, an answer, and possible associated reasoning pattern. For example, some samples do not contain patterns such as in Fig. 1g and Fig. 1h. 2.3 Evaluation Methods Unlike previous benchmarks, which primarily assess the accuracy of the final answer, our evaluation emphasizes both the reasoning process and the final answer. This dual focus is particularly effective for evaluating models on multiple-choice question answering tasks. For instance, model might hallucinate during the reasoning process but still select plausible answer (e.g., from options A, B, C, D), resulting in an uninformative accuracy 25% without any insight into its reasoning. To address these limitations, IQBench introduces dual-metric evaluation framework that comprehensively assesses VLM performance. This includes: Accuracy Score: This metric evaluates the correctness of the models final prediction via an exact match. It is applicable to both multiple-choice and open-ended responses, enabling standardized comparisons with existing benchmarks while capturing models ability to arrive at the correct answer. Reasoning Score: This score assesses the coherence, correctness and alignment of the models explanation with the expected reasoning path. We adopt an LLM-as-judge strategy, where pre-trained LLMs compare the models explanation with annotated reasoning patterns for each question. This metric quantifies interpretability and sheds light on the cognitive path leading to the answer, an aspect often overlooked in traditional evaluations. For both metrics, we use gpt-4o-mini as the judge model, assigning score of 1 for correct responses and 0 for incorrect ones. In addition to these automated metrics, we include human evaluations to assess the reasoning of VLM. This human-in-the-loop component provides qualitative benchmark to complement automated metrics (LLM-as-judge). We suppose that this multi-faceted evaluation approach offers holistic understanding of VLM capabilities, balancing performance accuracy with reasoning transparency. Comparison with existing VLM benchmarks, IQBench distinguishes itself by emphasizing fluid intelligence and the interpretability of reasoning across diverse spectrum of cognitive tasks. Unlike benchmarks such as MMMU [17] and MathVista [18], which focus on domain-specific reasoning (e.g., academic or mathematical), IQBench is designed to assess general intelligence using variety of abstract, symbolic, and logic-based problems inspired by human IQ tests. Although MM-IQ [34], ARC [10], and Verify [33] contribute valuable insights into model accuracy, they focus primarily on the correctness of the answers without systematically evaluating the traceability of the reasoning or the breadth of cognitive skills. In contrast, IQBench aims to measure not only what the model answers but also how it reasons, providing more comprehensive evaluation of VLM intelligence."
        },
        {
            "title": "3 Experiment and Result",
            "content": "To evaluate the fluid intelligence of VLMs in IQBench, we performed experiments using zero-shot testing framework. We tested seven state-of-the-art VLMs: gemini-2.5-flash, gemini-2.0-flash, claude-3.7-sonnet, claude-3.5-sonnet, gpt-4o, o4-mini, and gpt-o3. These models were selected for their advanced multimodal capabilities and recent advancements in reasoning-focused architectures. 3.1 Accuracy Evaluation The experimental results are reported in Table 2, showing the accuracy of various models in IQBench tasks. The highest overall performance was achieved by o4-mini with an average accuracy of 0.615, followed by gemini-2.5-flash (0.578) and claude-3.7-sonnet (0.548). These models performed particularly well on tasks such as Number Series (NS), Deductive Reasoning with Figures (DRTF), and Verbal Reasoning with Syllogisms (VRTS), with several scores exceeding 0.80. For example, o4-mini achieved 0.94 in NS and 0.86 in DRTF. 5 Table 2: Evaluation accuracy of models on IQBench tasks. Task abbreviations: MDRT (Mechanical Deductive Reasoning Test), DRTF (Deductive Reasoning Test with Figures), 3D SPRT (3D Spatial Deductive Reasoning Test), VRTS (Verbal Reasoning Test with Syllogisms), IVRT (Inductive Verbal Reasoning Test), Num. (Numerical), FS (Figure Series Test), NS (Number Series Test), Ana5 (Anagrams 5 Test), Ana3 (Anagrams 3 Test), Avg. (Average score). Model MDRT DRTF 3D SPRT VRTS IVRT Num. FS NS Ana5 Ana3 Avg. gemini-2.5-flash gemini-2.0-flash claude-3.7-sonnet claude-3.5-sonnet gpt-4o o4-mini gpt-o3 0.60 0.58 0.64 0.62 0.56 0.72 0. 0.78 0.56 0.90 0.68 0.42 0.86 0.88 0.18 0.22 0.40 0.20 0.20 0.34 0.66 0.70 0.72 0.74 0.80 0.66 0.78 0.74 0.68 0.74 0.74 0.76 0.74 0.52 0.46 0.32 0.36 0.82 0.60 0.44 0.66 0.42 0.26 0.60 0.88 0.84 0.82 0.76 0.66 0.94 0.14 0.16 0.04 0.12 0.06 0.02 0.12 0.42 0.14 0.16 0.10 0.02 0.14 0.578 0.490 0.548 0.470 0.408 0.615 Figure 2: Accuracy evaluation of the advanced multimodal models on IQBench. Despite this, all models struggled on tasks that required advanced spatial reasoning and linguistic manipulation. Specifically, the 3D Spatial Deductive Reasoning Test (3D SPRT) and the Anagram tasks (Ana5 and Ana3) revealed consistent weaknesses. For example, gemini-2.5-flash scored only 0.18 on 3D SPRT and 0.14 on Ana5, while gpt-4o performed the worst overall with 0.408 average accuracy and only 0.02 on Ana3. Intuitively, the general coverage and task-level performance distribution of the models are illustrated in Fig. 2. An important observation here is that although these multimodal models excel in text-based reasoning, they struggle with the anagram task, which requires identifying meaningful English words from scrambled letters, indicating limitations in their fine-grained linguistic manipulation capabilities. These findings suggest that even advanced VLMs face substantial challenges in tasks that test fluid intelligence components such as mental rotation, abstract symbol manipulation, and semantic reconfiguration, highlighting key areas for future model improvement. 3.2 Reasoning Evaluation Table 3 shows the reasoning scores, where o4-mini achieved the highest average score (0.696), followed by gemini-2.5-flash (0.586) and claude-3.7-sonnet (0.516). In particular, o4-mini excel in tasks that require structured reasoning, such as the Mechanical Deductive Reasoning Test (MDRT: 0.92) and the Figure Series Test (FS: 0.90), which demonstrate robust explanatory coherence. key trend observed across models is that reasoning scores tend to be slightly higher than accuracy scores for several models. For example, gpt-4o shows notable gap between accuracy (0.408) and reasoning (0.466), and similarly, claude-3.7-sonnet has reasoning score (0.516) nearly matching its accuracy (0.548), despite variability in task difficulty. This suggests that many models 6 can produce logically sound or plausible reasoning chains even when the final selected answer is incorrect, potentially due to misinterpretation of visual content or confusion in multi-choice mapping. In contrast, models such as gemini-2.0-flash and claude-3.5-sonnet show both low accuracy and reasoning scores, indicating more fundamental limitations in both understanding and explaining visual reasoning tasks. Meanwhile, o4-mini stands out as the only model with strong and balanced performance in both dimensions, indicating well-aligned multimodal architecture capable of accurate predictions and coherent justifications. Intuitively, the general coverage and the reasoning performance distribution of the models are illustrated in Fig. 3. Table 3: Reasoning Evaluation of Models on IQBench Tasks. Model MDRT DRTF 3D SPRT VRTS IVRT Num. FS NS Ana5 Ana3 Avg. gemini-2.5-flash gemini-2.0-flash claude-3.7-sonnet claude-3.5-sonnet gpt-4o o4-mini gpt-o3 0.60 0.50 0.58 0.60 0.60 0.92 0.70 0.78 0.52 0.82 0.64 0.44 0.88 0.88 0.22 0.16 0.36 0.16 0.56 0.82 0.72 0.74 0.72 0.74 0.78 0.78 0.78 0.74 0.70 0.72 0.80 0.80 0.72 0.28 0.50 0.16 0.30 0.72 0.54 0.32 0.54 0.28 0.68 0.90 0.94 0.58 0.76 0.52 0.44 0.90 0.14 0.16 0.04 0.12 0.04 0.10 0.12 0.42 0.08 0.14 0.06 0.02 0.14 0.586 0.408 0.516 0.400 0.466 0.696 Figure 3: Reasoning evaluation of the advanced multimodal models on IQBench. Observation: (1) Models perform well on tasks like DRTF and NS, which involve pattern recognition and numerical reasoning, but struggle with 3D SPRT and Anagrams, indicating gaps in spatial and vision-centric linguistic reasoning. (2) Reasoning scores are often slightly higher than accuracy scores, showing that models can sometimes explain their incorrect answers in logically coherent way, highlighting disconnect between reasoning quality and decision correctness. (3) o4-mini and gemini-2.5-flash demonstrate superior performance in both metrics, likely due to greater visual-textual integration and alignment in their architectures. 3.3 Human Evaluation and Failure Analysis Among the seven models evaluated in our study, o4-mini consistently achieved the highest performance in multiple reasoning tasks. Due to its strong results, we selected o4-mini for focused human evaluation to better understand the quality and interpretability of its reasoning process. To ensure manageable and representative assessment, we randomly sampled 100 predictions from the model for manual review. Three human experts independently evaluated the performance of the model and the results are summarized in Table 4, where reasoning score of 1 is assigned for correct reasoning and 0 for incorrect reasoning. Table 4: Comparison of reasoning scores assigned by human experts and LLM-as-judge (gpt-4o-mini) for o4-mini predictions across IQBench tasks. IVRT Num. MDRT DRTF 3D SPRT VRTS NS Ana5 Ana3 Avg. Experts FS LLM-as-Judge gpt-4o-mini Human evaluation Expert 1 Expert 2 Expert Average 0.92 0.88 0.82 0.78 0. 0.72 0.90 0.90 0.10 0.14 0. 0.90 0.70 0.80 0.80 0.80 0.90 0.80 0.83 0.20 0.30 0.20 0. 1.00 0.70 0.70 0.80 0.90 0.90 0.60 0.80 0.89 0.80 0.80 0. 0.90 0.90 0.78 0.86 1.00 1.00 0.90 0.97 0.90 0.00 0.10 0. 0.80 0.10 0.20 0.37 0.83 0.63 0.59 0.68 Table 4 presents the reasoning scores of o4-mini as assessed by human experts and gpt-4o-mini as the judge. The average human-assigned reasoning score for all tasks is 0.68, while the LLM judge score is 0.696, demonstrating close alignment between the two evaluation methods. In most categories, LLM as judge scores fall within the range of human evaluations, indicating high level of agreement in assessing the quality of the reasoning. In particular, the consistency in trend between tasks (e.g., higher scores in FS and NS, and lower scores in Ana5 and Ana3) further supports the validity of the LLM-as-judge method. These results suggest that the automated evaluation framework is reliable proxy for human judgment in assessing the interpretability and correctness of VLM-generated reasoning, making it scalable and practical alternative for large-scale benchmarking. Figure 4: Examples of VLM reasoning errors compared to human annotations. (a) Correct reasoning with incorrect prediction in pattern recognition task. (b) Partially correct reasoning with incorrect prediction in physics problem. In human evaluation, we also point out some incorrect reasoning of the model as shown in Fig. 4. For the pattern recognition task (Fig. 4a), the VLM correctly identifies the horizontal mirror reflection pattern in matrix 2 3 but selects option instead of A, probably due to misinterpreting the visual options. For the physics problem (Fig. 4b), the VLM accurately calculates the force (49.05 N) but 8 rounds incorrectly to 49 (option A) instead of 50 (option B), reflecting the failure to choose the minimum force. These examples highlight the models limitations in aligning its reasoning with visual options and in integrating multiple input modalities to arrive at the correct answer."
        },
        {
            "title": "4 Conclusion",
            "content": "We present IQBench, novel benchmark designed to evaluate the fluid intelligence of VisionLanguage Models through vision-centric IQ questions. Unlike existing benchmarks that emphasize answer accuracy alone, IQBench promotes both answer correctness and reasoning interpretability by introducing two-fold evaluation framework. Our results show that even the most advanced VLMs, such as o4-mini, Claude 3.5 Sonnet, and Gemini 1.5 Flash, struggle with key reasoning categories, particularly 3D spatial understanding and anagram tasks. Moreover, the quality of reasoning often does not align with the correctness of the final answers, highlighting the current limitations in the cognitive depth and generalization of VLMs. To advance multimodal model development, we believe that IQBench will foster research toward more transparent, robust, and cognitively capable VLMs, ultimately bringing us closer to AGI systems with genuine problem-solving abilities."
        },
        {
            "title": "Limitation",
            "content": "Despite the benefit of our benchmark in evaluating Vision-Language Models and their reasoning ability, our work still contains some limitations, including the use of the LLM-as-a-judge pipeline to assess reasoning ability and the limited number of samples in our dataset."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [2] Tan-Hanh Pham, Trong-Duong Bui, Minh Luu Quang, Tan-Huong Pham, Chris Ngo, and Truong-Son Hy. Silvar-med: speech-driven visual language model for explainable abnormality detection in medical imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2025. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [4] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [5] Xi Chen, Xiao Wang, Soravit Changpinyo, Anthony J. Piergiovanni, Piotr Padlewski, Daniel In The Eleventh Salz, et al. PaLI: jointly-scaled multilingual language-image model. International Conference on Learning Representations, 2023. [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35: 2371623736, 2022. [7] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [8] Ben Goertzel. Artificial general intelligence: Emergence and definition. In Artificial General Intelligence, pages 120. Springer, 2007. [9] Raymond Bernard Cattell. Intelligence: Its structure, growth and action, volume 35. Elsevier, 1987. 9 [10] François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. [11] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. [12] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [13] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence In Proceedings of the IEEE Zitnick, and Devi Parikh. Vqa: Visual question answering. international conference on computer vision, pages 24252433, 2015. [14] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [15] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [16] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67206731, 2019. [17] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [18] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS23, 2023. [19] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, May 2022. [20] Josselin Roberts, Tony Lee, Chi Wong, Michihiro Yasunaga, Yifan Mai, and Percy Liang. Image2struct: Benchmarking structure extraction for vision-language models. Advances in Neural Information Processing Systems, 37:115058115097, 2024. [21] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 784789, 2018. [22] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Dont just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49714980, 2018. [23] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, et al. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. [24] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai o1: Opportunities and challenges of agi. CoRR, 2024. [25] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 10 [26] Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. [27] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [28] Google. Start building with gemini 2.5 flash, 2025. URL https://developers.googleblog. com/en/start-building-with-gemini-25-flash/. [29] xAI. Grok 3 beta the age of reasoning agents, 2025. URL https://x.ai/news/grok-3. [30] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [31] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [32] Chao Lei, Nir Lipovetzky, and Krista Ehinger. Generalized planning for the abstraction and reasoning corpus. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2016820175, 2024. [33] Jing Bi, Junjia Guo, Susan Liang, Guangyu Sun, Luchuan Song, Yunlong Tang, Jinxi He, Jiarui Wu, Ali Vosoughi, Chen Chen, et al. Verify: benchmark of visual explanation and reasoning for investigating multimodal reasoning fidelity. arXiv preprint arXiv:2503.11557, 2025. [34] Huanqia Cai, Yijun Yang, and Winston Hu. Mm-iq: Benchmarking human-like abstraction and reasoning in multimodal models. arXiv preprint arXiv:2502.00698, 2025."
        },
        {
            "title": "A Prompts for General VLMs",
            "content": "Given the image, answer the following question: {question} Your answer must include your reasoning and strictly follow this format: <reason> Your thinking to find the final answer of the problem </reason> <answer> Your final answer - For multiple choice, answer with letter (A, B, C, etc.). - For numerical or computed answers, answer with number. </answer> **IMPORTANT** - your answer must include <reason> and <answer> sections - your answer must start with <reason> and end with </answer> - <reason> section provide your thinking to answer the question"
        },
        {
            "title": "B Prompts for Reasoning VLMs",
            "content": "Given the image, answer the following question: {question} Your answer must strictly follow this format: <answer> Your final answer - For multiple choice, answer with letter (A, B, C, etc.). - For numerical or computed answers, answer with number. </answer> **IMPORTANT** - your answer must include <answer> section - your answer must start with <answer> and end with </answer> LLM-as-a-judge Prompt # Given the following information: ## Question (You will not able to see the image as you should only compare the groud truth thinking and VLMs reasoning) {question} ## Ground Truth Reasoning (GT Reasoning) {pattern} ## Ground Truth Final Answer (GT Answer) {answer} ## VLMs Reasoning (VLM Reasoning) {think} ## VLMs Final Answer (VLM Answer) {bot_answer} ## Task Your task is to analyze whether the VLMs reasoning is logically sound and consistent with the ground truth reasoning, and whether it leads to the correct final answer. Base your judgment on reasoning accuracy, logical consistency, and whether the intermediate steps support the final conclusion. 12 Respond strictly in the following format: <reason> Compare the VLMs reasoning to the ground truth reasoning. Is the logical structure similar? Are the key steps present? Does the reasoning correctly support the final answer? Mention any discrepancies or alignments. </reason> <evidence> If the VLMs reasoning is flawed or deviates from the ground truth, provide specific parts of the VLM reasoning that are incorrect, missing, or misleading. If correct, explain why the reasoning is logically valid. </evidence> <answer> Return 1 if the VLMs reasoning is correct and aligns with the ground truth, otherwise return 0. </answer> **IMPORTANT** - your answer must include <answer> section - contents inside the <answer> section must be just 1 or 0 - your answer must start with <reason> and end with </answer>"
        }
    ],
    "affiliations": [
        "Harvard Medical School, USA",
        "Knovel Engineering Lab, Singapore",
        "University of Alabama at Birmingham, USA",
        "University of London, UK",
        "University of Technical Education Ho Chi Minh City, Vietnam",
        "Uppsala University, Sweden",
        "Vietnam Military Medical University"
    ]
}