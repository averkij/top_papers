{
    "paper_title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis",
    "authors": [
        "Xin Gao",
        "Qizhi Pei",
        "Zinan Tang",
        "Yu Li",
        "Honglin Lin",
        "Jiang Wu",
        "Conghui He",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA."
        },
        {
            "title": "Start",
            "content": "Xin Gao1, Qizhi Pei2, Zinan Tang1, Yu Li1, Honglin Lin1, Jiang Wu1, Conghui He1,*, Lijun Wu1,* 1Shanghai AI Laboratory, 2Renmin University of China {gaoxin,heconghui}@pjlab.org.cn 5 2 0 2 1 1 ] . [ 1 2 2 3 2 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by single large LLM. In this collaborative framework, multiple small LLMs assume distinct rolesGenerator, Reviewer, and Adjudicatorto simulate peerreview-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/ GX-XinGao/GRA."
        },
        {
            "title": "Introduction",
            "content": "The advent of Large Language Models (LLMs) with hundreds of billions of parameters has revolutionized artificial intelligence (Dubey et al., 2024a; * Corresponding author. 1 Figure 1: Average performance across GRA, vanilla seed dataset and lagrge LLMs distilled data with Qwen2.5-7B base model. Yang et al., 2024a; OpenAI et al., 2023; Liu et al., 2024), achieving state-of-the-art (SOTA) performance across diverse domains such as natural language understanding (Naveed et al., 2023), reasoning (Guo et al., 2025; OpenAI, 2024), math problem solving (Ahn et al., 2024), and code generation (Jiang et al., 2024). Despite their remarkable capabilities, these monolithic models demand immense computational resources and energy, posing significant barriers to deployment in resourceconstrained scenarios like edge devices or latencysensitive applications (Zheng et al., 2024b). This paradox has spurred growing interest in developing smaller-scale LLMs that balance efficiency and effectiveness, positioning compact yet capable models as critical research frontier in sustainable AI (Chen and Varoquaux, 2024). To enhance small LLMs performance, knowledge distillation (Hinton, 2015)transferring capabilities from large models via data synthesishas emerged as dominant paradigm (Xu et al., 2024b). However, conventional distillation inherently depends on the availability of powerful LLMs (Wang et al., 2024), which introduces practical limitations: deploying large LLMs for data generation incurs prohibitive costs, especially in privacy-sensitive or security-critical scenarios requiring local execution (Hagos et al., 2024; Yao et al., 2024). While small LLMs offer computational accessibility, their individual capacities are inherently constrained, struggling to match the quality, diversity, and reliability of data synthesized by their larger counterparts. This raises pivotal question: Can we design resource-efficient framework that enables small models to collectively rival the distillation capabilities of monolithic LLMs? To address this challenge, we propose GRA, collaborative framework inspired by human peerreview mechanisms. Diverging from traditional single-model distillation, GRA orchestrates multiple specialized small LLMs into three synergistic roles: (1) Generator, which proposes candidate data samples; (2) Reviewer, which evaluates quality and diversity through iterative critiques; and (3) Adjudicator, which resolves conflicts to finalize outputs. Notably, the synthetic data generated by single Generator will be evaluated by multiple Reviewers, and the Adjudicator is to resolve significant disagreements among the Reviewers by reassessing the data. By decomposing the synthesis pipeline into discrete sub-tasks and enabling rolespecific optimization, GRA mimics the refinement processes typically concentrated within single large LLM. This collaborative architecture not only circumvents the resource-intensity of large models but also amplifies small models collective intelligence through task specialization and iterative quality controleffectively operationalizing the wisdom of crowds principle in model ensembles. We validate GRA through extensive experiments on multiple domains, including general question answering, reasoning, mathematical and coding tasks. Remarkably, our framework achieves data quality comparable to or exceeding outputs from SOTA monolithic LLMs like Qwen-2.572B-Instruct, while requiring orders-of-magnitude fewer computational resources. These results empirically demonstrate that strategically coordinated small LLMs can collectively surpass the distillation performance of individual large LLMs."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Data Synthesis The rapid evolution of LLMs has increased the demand for high-quality, diverse training data, creating bottleneck in AI development (Zhao et al., 2023). Synthetic data is promising direction to improve data. dominant paradigm for synthetic data leverages knowledge distillation from LLMs. WizardLM (Xu et al., 2024a) iteratively enhances seed data based on predefined evolutionary rules and instructions. MAGPIE (Xu et al., 2025) utilizes pre-defined query templates to directly prompt the LLM to synthesize data while not relying on seed data. MuggleMath (Li et al., 2024a) employs LLMs to rewrite and enhance seed datasets by generating new variations of problem statements and corresponding answers. In addition to single-LLM-based approaches, several works also employ multiple agents for data generation (Mitra et al., 2024; Majumdar et al., 2024). AgentInstruct (Mitra et al., 2024) leverages GPT-4 (OpenAI et al., 2023) to facilitate data generation and curation through the use of multiple agents. Similarly, Genetic Instruct (Majumdar et al., 2024) defines three distinct rolesInstructorLLM, Coder-LLM, and Judge-LLMto generate code data. Unlike previous multi-agent frameworks that rely on single LLM like GPT-4 for repeated deployment across different stages, GRA pioneers collaborative paradigm that coordinates diverse types of small LLMs through specialized role allocation to collectively rival the synthetic data quality of singular large LLMs. 2.2 Data Curation The efficacy of synthetic data critically hinges on post-synthesis curation (Long et al., 2024). Early curation methodologies predominantly relied on surface-level heuristics, such as repetition counts (Salkar et al., 2022) and lexical diversity metrics (Li et al., 2016; Zhu et al., 2018; Shu et al., 2019), which inadequately captured semantic coherence and task-specific utility. Recent works shift toward LLM-driven curation frameworks. Initial efforts employed perplexity (PPL) and instruction following difficulty score(IFD) (Li et al., 2024b) to filter low-quality samples. The emergence of LLM-as-judge paradigms further revolutionized this domain, leveraging well-tuned LLMs for quality assessment (Gu et al., 2024). In contrast, while sharing the LLM-as-judge basis, our GRA framework differs by using multiple specialized small LLMs for diverse evaluations. Unlike single-LLM approaches that risk bias, GRA employs multiagent system and two-tier adjudication to ensure balanced and comprehensive data assessment."
        },
        {
            "title": "3 Methodology",
            "content": "Preliminary We first provide essential background and definitions for key components with the rationale behind our design choices. Specifically, we denote as the pool of small LLMs employed in the framework, and data pool at iteration as D(t) = {(xi, yi, di, ki, si)}N i=1, where each sample in D(t) consists of an instruction xi, response yi, domain label di, 3-tuple of keywords ki, and summary si. The domain label di indicates the task type of the sample, which is categorized into seven domains: Coding, Math, Question and Answering, Reasoning, Role Play, Language, and Creation, which are designed to aim for covering the most general domains. The 3-tuple keywords ki are generated by LLMs to capture the core concepts of each data sample. In addition, we include summary si to provide context and clarify keyword relationships, enhancing the overall understanding of the data. More details such as domain definitions and construction of D(0)-seed dataset are shown in the Appendix A. 3.1 Pipeline Overview The overview of our GRA is shown in Figure 2. GRA integrates domain-specific generation with collaborative curation by orchestrating three specialized roles. First, the Generator produces initial samples using few-shot prompting tailored to the domain. Next, multiple Reviewers collaboratively assess the quality of these samples. Finally, an Adjudicator resolves any conflicting evaluations through consensus-driven arbitration. post-processing module further refines the results by eliminating redundant outputs via embedding deduplication and metadata enrichment. Moreover, to prevent model specialization bias, GRA dynamically assigns distinct LLMs from to each role through random selection and during generation. 3.2 Data Generator To enhance synthetic data diversity while maintaining domain consistency, we carefully design the generation process to systematically integrate conceptual knowledge and contextual patterns. For each instance, we randomly select one LLM MG as the Generator. For given domain, the generation workflow comprises three coordinated stages: First, MG analyzes randomly selected keywordssummary pairs (ki, si) from the current domain corpus, synthesizing novel 3-tuple keywords by extracting and combining meaningful semantic patterns and attributes from the input data. Second, drawing inspiration from contextual summaries of seed examples, MG contextualizes the synthesized keywords into an executable instruction x. Last, MG produces response that appropriately addresses the generated instruction x. 3.3 Data Reviewer To ensure equitable data evaluation and mitigate individual model biases, the Reviewer module performs rigorous multi-agent quality assessment through coordinated evaluation protocols. We randomly select NR reviewers from MG to form the review committee = {MRi}NR i=1. The review process includes two stages: instruction evaluation and response evaluation. During the instruction evaluation phase, each Reviewer independently verifies instruction against three binary criteria: Reasonableness, Completeness, and Clarity. Immediate rejection occurs if any Reviewer MRi flags any criterion failure. Approved instructions then advance to the response evaluation stage, where each Reviewer conducts granular analysis of the generated response from six dimensional metrics: Correctness, Clarity, Completeness, Relevance, Coherence, and Ethicality. Then each Reviewer quantitatively assesses these dimensions by assigning integer scores from 0 to 10 and the averaged score serves as the final score. Additionally, each Reviewer provides detailed textual feedback that delineates both the strengths and weaknesses of the generated response. In the end, the committee aggregates assessments through calculating the mean composite score µR and assessing consensus reliability through standard deviation σR. After all, the committee makes the final decision: (a) Rejection: If µR < τ . (b) Acceptance: If µR τ with σR δ. (c). Adjudication: If µR τ but σR > δ, where τ and δ are predefined thresholds. 3.4 Data Adjudicator Adjudicator is designed for reassessing the disputed samples. For each disputed sample, we randomly select an Adjudicator MA from (MG R) to perform consensus-driven arbitration. For each disputed sample, based on consolidating Reviewers feedback, MA conducts comprehensive reassessment using the same evaluation criteria as the Reviewers and gets the final composite score sA. Only samples with sA τ proceed to the 3 Figure 2: Overview of GRAs architecture, highlighting its four key modules: (a) The Generator creates domainspecific samples, (b) followed by collaborative evaluation by Reviewers, (c) The Adjudicator resolves conflicts, and (d) Post-Processing refines the results by removing redundancies. subsequent post-processing module."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Settings 3.5 Post-Processing Module The post-processing module ensures syntactic consistency and enhances diversity for final dataset integration. First, we generate embeddings for all data using the all-MiniLM-L6-v2 encoder (Reimers and Gurevych, 2019). Data are processed based on their review score µR in descending order. Each sample, along with its embedding, undergoes semantic redundancy verification against the target data pool: if the maximum cosine similarity with any item in the pool is below specified similarity threshold, the candidate is added to the pool. Otherwise, it is rejected to prevent semantic duplication. The deduplicated data then undergoes metadata enrichment through random LLM MP to generate contextual summary s, supplemented with the keywords from which it was derived, ensuring format consistency with the existing data. This two-stage filtrationembeddingbased semantic deduplication followed by LLM summarizationensures both conceptual diversity and structural compatibility for subsequent data synthesis workflows. GRA Data Generation. Our framework operates on three established instruction-tuning datasets: Alpaca (Taori et al., 2023), WizardLM (Xu et al., 2024a), and the recent Condor (Cao et al., 2025), which serve as seed datasets for data synthesis. As for the small LLMs, we use five diverse and representative LLMs within 7B-8B parameters: Llama3.1-8B-Instruct (Dubey et al., 2024b), Qwen2.57B-Instruct (Yang et al., 2024b), InternLM3-8BInstruct (Cai et al., 2024), Mistral-7B-Instructv0.3 (Jiang et al., 2023), and Tulu-3-8B (Lambert et al., 2024). The synthesis process runs for five iterations, and each iteration produces approx 10K high-quality samples per round (51K in total) using vLLM (Kwon et al., 2023) toolkit. We set the minimum score threshold τ = 8 and the maximum standard deviation δ = 1.5 at review stage. For deduplication, the maximum cosine similarity is set to 0.9. More detailed generation settings are shown in Appendix A. Training. After data synthesis, we conduct full parameter Supervised Fine-Tuning (SFT) on two well-known base models for verification: Llama3.1-8B-Base (Dubey et al., 2024b) and Qwen2.57B-Base (Yang et al., 2024b). The experiments utilize the Llama-Factory (Zheng et al., 2024a) toolkit. We run five iterations, each accumulating synthetic data from Rnd1 to Rnd5, representing data from the first round up to all five rounds. In the main experiment, we report the iteration with the best average performance. Detailed training settings are provided in Appendix B. Evaluation. For evaluation, following (Xu et al., 2024a; Cao et al., 2025), we select multiple benchmarks on various domains-mathematics, coding, reasoning, and general-to demonstrate the effectiveness of our GRA framework, which include GSM8K, MATH, HumanEval, MBPP, HellaSwag, ARC-C, GPQA, IFEval, BBH and MMLU. We use the opencompass (Contributors, 2023b) toolkit for fair evaluation. More detailed evaluation settings are shown in Appendix C. 4.2 Main Results The main results are presented in Table 1, where we compare GRA against vanilla seed datasets and also single large LLM (Qwen2.5-32B/72BInstruct) distillation. From the table, we can draw following observations: Superiority Over Vanilla Seed Data. The GRA method achieves significant improvements over vanilla seed data, with average performance gains of 6.18% (Llama-3.1-8B) and 11.81% (Qwen-2.57B) across all benchmarks. These results demonstrate the effectiveness of GRA. We attribute this success to GRAs ability to enhance data diversity and quality through iterative data synthesis. Further analysis is provided in Section 5.4. Competitive Against Large LLM Distillation. GRAs collaborative small LLM framework demonstrates parity with or superiority to single large LLM distillation. Notably, comparing to Qwen2.5-72B-Instruct distilled data, it narrows the performance gap to 0.59% for Llama-3.1-8B while surpassing by 8.83% for Qwen-2.5-7B on average. This validates the viability of cost-effective multi-agent synthesis as scalable alternative to resource-intensive large-model distillation. Base Model Dependency. Performance varies substantially with base model capability: Qwenbased implementations consistently outperform Llama counterparts by 9% under identical data conditions. We attribute this discrepancy to Qwens strong base model ability, as demonstrated by many Figure 3: Performance along data iterations with Qwen2.5-7B-Base model. recent models (Yang et al., 2024b), which likely enhance its ability to leverage synthesized data. Diminishing Returns in Teacher Scaling. Despite doubling parameter counts (Qwen-2.5-32BInstructQwen-2.5-72B-Instruct), teacher model scaling yields marginal quality gains. This suggests potential saturation point in knowledge transfer efficiency for standard distillation methods, emphasizing the need for more advanced distillation protocols rather than naive distillation from larger models. These insights collectively highlight GRAs practical value for efficient, high-quality data generation while delineating fundamental boundaries in LLM knowledge transfer. 4.3 Ablation Performance along Data Iteration. To investigate the impact of iterative data synthesis rounds on model performance, we conduct five sequential iterations of data generation using GRA, with approx 10K synthetic samples produced per iteration. We trained models incrementally on cumulative datasets: 10K (1st iteration), 20K (1st2nd), 30K (1st3rd), 40K (1st4th), and 50K (1st5th). Results are illustrated in Figure 3. Experiments across three distinct seed datasets consistently demonstrate positive correlation between iteration count and model performance. Besides, we observe that performance gains are rapid in early iterations, but gradually plateaued in later iterations, suggesting diminishing yet meaningful returns. This trend aligns with the hypothesis that initial cycles resolve critical data scarcity, whereas subsequent rounds exhaust exploitable synthetic data diversity. Effect of GRA Data Reviewer. We conduct ablation experiments to evaluate the effectiveness of GRAs multi-model review mechanism by compar5 Dataset Math GSM8K MATH Coding Humaneval MBPP Reasoning HellaSwag ARC-C GPQA BBH General MMLU IFEval Alpaca Qwen-2.5-7B-GRA Qwen-2.5-32B-Instruct-Distilled Qwen-2.5-72B-Instruct-Distilled WizardLM Qwen-2.5-7B-GRA Qwen-2.5-32B-Instruct-Distilled Qwen-2.5-72B-Instruct-Distilled Condor Qwen-2.5-7B-GRA Qwen-2.5-32B-Instruct-Distilled Qwen-2.5-72B-Instruct-Distilled Alpaca Llama-3.1-8B-GRA Qwen-2.5-32B-Instruct Distilled Qwen-2.5-72B-Instruct Distilled WizardLM Llama-3.1-8B-GRA Qwen-2.5-32B-Instruct Distilled Qwen-2.5-72B-Instruct Distilled Condor Llama-3.1-8B-GRA Qwen-2.5-32B-Instruct Distilled Qwen-2.5-72B-Instruct Distilled 60.12 70.28 63.23 50.64 64.37 58.98 59.36 69.83 57.16 64.75 71.34 62.22 51.71 65.58 64.44 67.40 63.84 65.88 64.97 66.64 61.49 65.58 64.94 69.14 27.00 33.82 56.26 55.80 31.94 47.84 54.96 56.30 48.60 42.82 56.00 54. 11.32 17.20 23.32 23.82 16.78 18.98 23.50 24.12 22.12 15.82 23.00 23.30 Qwen-2.5-7B Base Model 32.32 76.83 76.22 74.39 50.61 76.83 72.56 79.88 51.83 76.22 68.90 75.61 61.20 64.80 65.40 64.40 64.20 64.60 66.20 65.20 64.60 65.60 66.40 65.40 72.51 80.23 72.02 74.70 65.89 81.58 54.22 57.85 55.74 79.90 63.93 61.99 Llama-3.1-8B Base Model 53.25 45.20 43.29 66.30 48.60 52.44 69.14 49.20 48.17 63.95 50.60 48.78 67.16 47.00 45.73 66.54 47.00 51.22 67.66 48.00 51.83 63.76 48.60 51.83 47.24 48.40 38.41 69.22 48.80 50.61 71.04 49.80 54.88 67.40 50.20 53. 65.76 74.24 56.61 56.95 40.68 79.66 54.58 56.61 66.10 75.59 61.02 52.88 75.25 71.53 69.49 74.24 63.05 72.88 65.08 69.15 51.53 72.54 65.08 68.14 23.74 24.24 20.71 23.74 21.72 28.79 21.21 16.67 24.75 26.26 18.69 14.14 21.21 29.80 26.77 26.77 9.60 26.77 23.74 26.77 15.66 26.77 24.24 21.72 60.42 60.57 42.04 38.75 39.34 63.99 37.63 37.02 39.96 61.49 37.53 32.18 61.86 64.50 65.07 64.77 62.98 63.28 65.03 63.60 65.28 63.48 66.12 63. 45.19 65.42 49.05 37.37 31.18 65.23 46.99 35.02 47.48 64.59 54.56 39.61 52.65 48.58 45.78 48.09 40.89 43.10 43.31 46.98 27.91 45.22 44.37 45.02 44.95 53.16 53.71 53.33 47.12 54.15 53.96 53.96 49.86 54.02 52.95 52.41 43.06 56.80 55.73 58.39 48.90 53.52 54.51 56.28 52.35 53.54 52.64 53.94 AVG 49.32 60.36 55.36 53.03 45.71 62.17 52.33 52.93 50.61 61.12 54.93 51. 45.88 52.13 51.71 52.68 46.59 50.92 51.10 52.18 43.04 51.60 51.81 51.56 Table 1: Main Comparison between our proposed method, the vanilla seed datasets, and the data distilled from large-scale LLMs (Qwen-2.5-32B-Instruct and Qwen-2.5-72B-Instruct). Colored values represent the best results. Effect of GRA Data Adjudicator. To validate the necessity of the Adjudicator module in GRA, we conducted ablation experiments without the Adjudicator, where conflicting reviews were resolved via majority voting. Using Llama-3.1-8B as the base model and Alpaca as the seed dataset, the Adjudicator-enhanced version achieved 0.73% improvement in synthetic data quality (measured by human-aligned correctness scores), demonstrating its effectiveness, as shown in Figure 4."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Effect of Small LLMs Role We are interested in the specific role played in the GRA framework, therefore we conduct experiments of three role assignment strategies for GRA pipeline with Qwen-2.5-7B-Base as the training model: (1) Single-Model: all roles assigned to single LLM (Llama3.1-8B-Instruct); (2) Fixedassignment strategy: five distinct LLMs assigned to roles in fixed permutations; (3) Our GRA setting with random roles. The results are as follows: when we simply use one model to play all roles (Setting 1), the average accuracy is 52.07, while Setting 2, which assigns fixed roles to five dedicated models, results in lower accuracy of 48.59. Lastly, Figure 4: comparison across different setting of reviewer and adjudicator, with alpaca as seed dataset and Llama3.1-8B as base model. ing against three baselines: no Reviewer, Llama3.18B-Instruct Reviewer. As shown in Figure 4, GRA outperforms all baselines, confirming the necessity of collaborative curation. Notably, single-model reviews show negligible gains over no review, implying either (a) limited quality-assessment capacity in small LLMs or (b) data quality from GRAs generation phase being sufficiently robust for standalone use. These results validate that GRAs multi-agent framework uniquely synthesizes complementary curation strengths, avoiding the pitfalls of isolated or capacity-constrained reviewers. Comparisons wiht large LLM reviwer are shown in Section 5.4. 6 GRA Data Synthesis Cases # Data: ## Instruction: Given set of numbers from 1 to 100, find the sum of all numbers that are divisible by both 3 and 5. Express the sum in LaTeX format. ## Response: The sum of all numbers from 1 to 100 that are divisible by both 3 and 5 can be found by calculating the sum of the arithmetic series where the first term is (3 5 = 15) and the last term is (100)... the sum of all numbers from 1 to 100 that are divisible by both 3 and 5 is 90. In LaTeX format, this is written as: [sum100 i=1(3 5)i = 90] # Data Review ##Reviewer 1 detail scores : [9,10,10,10,10,10]. review comment: NO MISSTATEMENT, ALL INFO PRESENT. ##Reviewer 2 detail scores : [9,9,10,10,10,10]. review comment: Accurate and well-structured response, clear and correct explanation, irrelevant information avoided, perfect coherence and suitable LaTeX expression ##Review 3 detail scores : review comment: Incorrect arithmetic series calculation and LaTeX format error ##Review Result average score of 8 meets the threshold. score std of 2.4758 exceeds the threshold. [6,4,5,4,5,3]. #Data Adjudicator detail scores : [4,2,5,5,5,1,5]. average score of 4.5 below threshold. ##Adjudication Result Discard unqualified data. Case 1: case analysis of inconsistent data. our GRA based training achieves 59.52 accuracy. Lastly, our GRA based training achieves 59.52 accuracy. From the above results, we can find that the advantages of our randomized role assignment in our GRA is clear and significant, compared to both other settings. Besides, interestingly, fixed assignments of different models performs worse than single role model, which means that self-evolve is better than fixed role assignment among different models, further demonstrating that multi-agent corporation is curial. 5.2 Inconsistent Data Analysis We show Case 1 to further illustrate the effectiveness of Adjudicator. For wrong math problem, two of three reviewers incorrectly assigned high scores, while one correctly flagged the error. Although the average score (8/10) initially met the retention threshold, the high score variance (2.48) triggered adjudication. The Adjudicator synthesized all reviews and evaluation criteria, overriding 7 Figure 5: Data coverage comparison between vanilla seed dataset and GRA synthetic data. Figure 6: Data coverage comparison between large LLMs distilled data and GRA synthetic data. the majoritys misjudgment to assign low score (4.5/10), thereby filtering out the flawed data. This highlights how the Adjudicator mitigates limitations of simple voting or averaging by dynamically reconciling disagreements and enforcing rigorous quality standards. 5.3 Data Diversity To assess the diversity of synthesized data, we compare three sources: GRA-generated data, data distilled from single large LLM, and the vanilla seed dataset. We visualize the instruction embeddings with t-SNE (Van der Maaten and Hinton, 2008). Diversity Analysis. As shown in Figure 5. GRAgenerated data exhibits broader distribution than the vanilla seed dataset, particularly covering underrepresented regions in the seed datas embedding space. This suggests that GRA complementing the seed dataset by expanding its coverage. Besides, Figure 6 reveals that GRA surpasses the single large LLM in distribution diversity, implying that collaborative small LLMs can mitigate the inherent limits and biases of single model. Notably, while the seed dataset demonstrates comparable instruction diversity, its inferior performance in main experiments-Table 1 highlights critical deficiencies in response quality rather than instruction coverage. Seed Data Enhancement via GRA. We further applied GRAs generator to rewrite responses in the vanilla seed dataset, followed by curation using GRAs reviewer and adjudicator. As shown in Table 2, the refined dataset achieves performance Figure 7: Data quality score comparison between GRA and single Large LLMsQwen-2.5-72B-Instruct. parity with GRA-generated data. Moreover, combining GRA-synthesized data with enhanced seed data yields synergistic effects, outperforming either dataset alone in certain settings. This underscores GRAs dual utility: (a) generating novel high-quality data and (b) upgrading existing data through systematic rewriting and quality control. Data / Seed Dataset Alpaca WizardLM Condor Llama-3.1-8B-Base Vanilla Seed Data GRA-Refined Seed Data GRA Synthetic Data GRA Integrated Data 45.88 50.39 52.14 51. Qwen-2.5-7B-Base Vanilla Seed Data GRA-Refined Seed Data GRA Synthetic Data GRA Integrated Data 49.32 59.97 59.52 61.19 46.59 52.81 50.74 52.35 45.71 58.34 62.17 59.00 43.04 47.68 51.16 52. 50.61 45.5 61.13 62.1 Table 2: Comparison across Vanilla Seed Data, GRARefined Seed Data, GRA Synthetic Data and GRA Integrated Data. 5.4 Data Quality To evaluate the data quality, we utilize Qwen-2.572B-Instruct to assess the synthetic data and compare its score with our Reviewers score. From Figure 7, we find that the GRA method exhibits finer granularity and more balanced scoring distribution compared to the coarser-grained scores from the large LLM, indicating its ability to capture nuanced quality variations. Second, over 87.3% of GRA-curated data (scores > 8) align with high scores from the large LLM, while 12.7% receive lower scores ( 8) from the latter. Notably, the large LLM assigns even higher scores to larger subset of data, further validating the reliability of GRA in generating high-quality data. These results suggest that GRA enhances scoring resolution without compromising assessment consistency with large LLMs. 8 Figure 8: The IFD score comparison between the data generated by GRA, the vanilla seed dataset WizardLM, and single large LLM (Qwen-2.5-72B-Instruct) distilled data. 5.5 Data Difficulty To evaluate the complexity and knowledge density of synthesized data, we compute the InstructionFollowing Difficulty (IFD) score (Li et al., 2024b) for the three datasets. From the results depicted in Figure 8, two key observations emerge: First, GRA-generated data achieves significantly higher IFD than the seed dataset (+14.58%), indicating its enhanced difficulty and richer knowledge coverage. This aligns with empirical training results, where models trained on GRA data exhibit superior performance. Second, despite using smaller LLMs, GRA attains comparable difficulty to the Qwen72B-distilled data (75.82% v.s. 75.49%). This suggests that collaborative small LLMs in GRA mitigate individual capability limitations, effectively emulating large LLMs in generating challenging examples. These results validate GRAs ability to synthesize high-quality, difficulty-controllable data without relying on large LLMs, addressing both scalability and performance trade-offs in data synthesis pipelines."
        },
        {
            "title": "Limitations",
            "content": "Despite the promising performance of the GRA framework, several aspects warrant further exploration. Current role allocation mechanismbased on randomized assignmentmay yield suboptimal collaboration patterns. This stochastic approach lacks explicit optimization for task-specific synergies, suggesting the need for intelligent role configuration strategies (e.g., performance-aware dynamic allocation or metric-driven reinforcement learning) to systematically maximize agent complementarity. Furthermore, while GRA is primarily designed for textual reasoning tasks, extending its paradigm to multimodal scenarios presents compelling direction."
        },
        {
            "title": "References",
            "content": "Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. In EACL (Student Research Workshop), pages 225237. Association for Computational Linguistics. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024. Internlm2 technical report. Preprint, arXiv:2403.17297. Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, and Kai Chen. 2025. Condor: Enhance llm alignment with knowledge-driven data synthesis and refinement. arXiv preprint arXiv:2501.12273. Lihu Chen and Gaël Varoquaux. 2024. What is the role of small models in the llm era: survey. arXiv preprint arXiv:2409.06857. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. LMDeploy Contributors. 2023a. Lmdeploy: toolkit for compressing, deploying, and serving llm. https: //github.com/InternLM/lmdeploy. OpenCompass Contributors. 2023b. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024a. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024b. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Desta Haileselassie Hagos, Rick Battle, and Danda Rawat. 2024. Recent advances in generative ai and large language models: Current status, challenges, and perspectives. IEEE Transactions on Artificial Intelligence. 9 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Geoffrey Hinton. 2015. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. survey on large language models for code generation. arXiv preprint arXiv:2406.00515. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. 2024. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124. Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. 2024a. Mugglemath: Assessing the impact of query and response augmentation on math reasoning. In ACL (1), pages 10230 10258. Association for Computational Linguistics. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. diversity-promoting objective function for neural conversation models. In HLT-NAACL, pages 110119. The Association for Computational Linguistics. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On llmsdriven synthetic data generation, curation, and evaluation: survey. In ACL (Findings), pages 11065 11082. Association for Computational Linguistics. Somshubra Majumdar, Vahid Noroozi, Sean Narenthiran, Aleksander Ficek, Jagadeesh Balam, and Boris Ginsburg. 2024. Genetic instruct: Scaling up synthetic generation of coding instructions for large language models. arXiv preprint arXiv:2407.21077. Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, et al. 2024. Agentinstruct: Toward generative teaching with agentic flows. arXiv preprint arXiv:2407.03502. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2023. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435. OpenAI. 2024. Learning to reason with llms. Josh OpenAI, Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. Nikita Salkar, Thomas A. Trikalinos, Byron C. Wallace, and Ani Nenkova. 2022. Self-repetition in abstractive neural summarizers. In AACL/IJCNLP (2), pages 341350. Association for Computational Linguistics. Raphael Shu, Hideki Nakayama, and Kyunghyun Cho. 2019. Generating diverse translations with sentence codes. In ACL (1), pages 18231827. Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024b. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In NAACL-HLT, pages 7602 7635. Association for Computational Linguistics. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. 10 Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(11). Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang, Chenkai Zhang, Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, et al. 2024. survey on data synthesis and augmentation for large language models. arXiv preprint arXiv:2410.12896. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024a. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In ICLR. OpenReview.net. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. 2024b. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2025. Magpie: Alignment data synthesis from scratch by prompting aligned LLMs with nothing. In The Thirteenth International Conference on Learning Representations. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024b. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024. survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, page 100211. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024a. Llamafactory: Unified efficient finetuning of 100+ language models. arXiv preprint arXiv:2403.13372. Yue Zheng, Yuhao Chen, Bin Qian, Xiufang Shi, Yuanchao Shu, and Jiming Chen. 2024b. review on edge large language models: Design, execution, and applications. arXiv preprint arXiv:2410.11845. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: benchmarking platform for text generation models. In SIGIR, pages 10971100. ACM."
        },
        {
            "title": "A Generation",
            "content": "When generating data, we deployed models using VLLM and LMDeploy (Contributors, 2023a) on 8 NVIDIA A100 GPUs. For setting the model generation parameters, we used the default settings of the deployment tools, except for temperature=0.2, top_p=0.9, max_token=4096. The number of examples used in few-shot ranges from 2 to 4. We set the minimum score threshold τ = 8 and the maximum standard deviation δ = 1.5 at review stage. For deduplication, the maximum cosine similarity is set to 0.9. Here we provide detailed criteria for data domain classification: Coding: Involves understanding, generating, debugging, and fixing code. Math: Entails performing mathematical operations, applying formulas, and solving problems. QA: Focuses on providing specialized and accurate answers to questions within specific domain or field. Language: Involves accurately understanding and analyzing text, performing tasks such as translation, summarization, and classification. Role Play: Engages in simulated conversations or behaviors by assuming different roles to explore various perspectives or scenarios. Reasoning: Involves complex causal reasoning and logical judgment, solving multi-level problems through inference and analysis. Creation: Emphasizes originality and creativity, creating content according to requirements and adapting to different styles and themes. Before initiating the GRA data synthesis, we employed all available LLMs to extract keywords and summaries from the seed dataset using randomized rotation approach."
        },
        {
            "title": "B Training",
            "content": "We employ LLaMA-Factory (Zheng et al., 2024a) for model fine-tuning. All models, including our reproduced baselines, are fine-tuned over 1 epochs with batch size of 256 using 8 NVIDIA A100 GPUs. The peak learning rate is set to 5e-6, featuring linear warm-up during the initial 3% of training steps, followed by cosine decay. The maximum sequence length is configured to 4096."
        },
        {
            "title": "C Evaluation",
            "content": "We compare GRA with baselines on the following 10 benchmarks: The GSM8K dataset (Cobbe et al., 2021) contains 8,500 grade school-level math word problems.focus on real-world scenarios, testing models ability to parse natural language, extract relevant information, and perform accurate calculations. The MATH dataset (Hendrycks et al., 2021) evaluates mathematical reasoning with problems spanning arithmetic, algebra, calculus, and geometry, and 5 levels of difficulty. The HumanEval dataset (Chen et al., 2021) is designed to evaluate the functional correctness of code generation models. It consists of 164 hand-written programming problems, each requiring the generation of Python function to solve specific task. The dataset assesses the ability of models to produce syntactically correct and logically accurate code. The MBPP dataset (Austin et al., 2021) contains around 1,000 Python programming tasks, focusing on fundamental programming concepts. It is split into training and test sets, with tasks ranging from simple operations to more complex algorithmic challenges, making it suitable for evaluating code generation and understanding. The Hellaswag dataset (Zellers et al., 2019) is commonsense reasoning benchmark that includes 70,000 multiple-choice questions derived from everyday scenarios. It tests models ability to predict the most plausible continuation of given context, emphasizing natural language understanding and reasoning. The ARC-C dataset (Clark et al., 2018) is collection of science exam questions designed to evaluate advanced reasoning capabilities. It includes 2,590 questions across various science topics, requiring multi-step reasoning and knowledge application to solve. The GPQA datase (Rein et al., 2023)t is high-quality benchmark for evaluating general knowledge and reasoning. It consists of diverse questions spanning multiple domains, including science, history, and culture, designed to test the breadth and depth of models understanding. The IFEval dataset (Zhou et al., 2023) focuses on evaluating instruction-following capabilities in language models. It includes variety of tasks that require models to follow complex instructions accurately, testing their ability to understand and execute multi-step directives. The MMLU dataset (Hendrycks et al., 2020) is comprehensive benchmark covering 57 tasks across diverse domains, including humanities, STEM, and social sciences. It evaluates models ability to perform well across wide range of subjects and tasks. The BBH dataset (Suzgun et al., 2022) is subset of the Big-Bench benchmark, consisting of 23 challenging tasks that are particularly difficult for language models. These tasks require advanced reasoning, knowledge, and problem-solving skills, making them robust test of model capabilities. In our testing, we employ the default metrics of OpenCompass (Contributors, 2023b) corresponding to each benchmark."
        },
        {
            "title": "D Cases",
            "content": "A comprehensive data synthesis case is detailed in Case 2."
        },
        {
            "title": "E Prompts",
            "content": "The prompts we used, including domain identification, summary generation, key word extraction, key word generation, instruction generation, instruction evaluation,response generation, response evaluation and data adjudication are shown in Prompt 1, 2, 3, 4, 6, 7, 8, 9. 12 GRA Data Synthesis Cases # Sample Data: ## Instruction1: You are Cleopatra, the last active ruler of the Ptolemaic Kingdom of Egypt. Reflect on your complex relationships with Julius Caesar and Mark Antony, and explain how these alliances influenced your rule and the fate of Egypt. Discuss the political and emotional aspects of these relationships, and what you would consider to be the most significant outcomes. ## Response1: As Cleopatra, the last Pharaoh of ancient Egypt, my life was tapestry woven with threads of power, passion, and ambition. My relationships with Julius Caesar and Mark Antony were not only pivotal to my personal story but also crucial to the fate of my kingdom... ## Instruction2: Please use the identity of an old Guangzhou resident and use humorous and exaggerated tone to talk about your views on the changes in Guangzhou cuisine. In your answer, please use at least three metaphors or personifications, and make sure that each sentence is no less than 20 words, and must be separated by colons or semicolons. ## Response2: Oh, talking about the changes in Guangzhous food, its like magic trick in the Xiguan Mansion, with surprises hidden behind every door; the Diandude on Shamian Island was like happy fruit for the neighbors. Although there are some young competitors now, it is still going strong and exudes unique charm... # Generate Key words: Fantasy Adventure Quest # Reference Summary: 1. Cleopatras alliances with Caesar and Antony shaped Egypts fate. 2. Humorous Guangzhou local describes food evolution # Generate Data: ##Instruction: Embark on quest through the mystical realm of Eldoria, where you must navigate through enchanted forests, solve ancient riddles, and face fearsome dragons to retrieve the legendary Crystal of Aether. In this role-play, you are an aspiring hero, seeker of truth, and guardian of balance. Your adventure begins in the quaint village of Everlight, where whispers of the Crystals whereabouts have begun to stir. What will you do first on your journey? ##Response: As an aspiring hero on this quest to find the legendary Crystal of Aether, would first consult the village elders of Everlight. They are likely to have valuable insights or clues about the whereabouts of the Crystal and the nature of the quest. My approach would be to: 1. Seek out the village elders and listen to their stories and advice. 2. Offer to help with any local tasks or problems as gesture of goodwill and to build relationships.3. Inquire about the ancient riddles and the lore surrounding the Crystal of Aether. 4. Ask if there are any artifacts or maps in the village that could assist on the journey. 5. Formulate plan of action based on the information gathered, considering the potential routes through the enchanted forests and the locations of the fearsome dragons. This initial step is crucial for gathering intelligence and preparing for the challenges ahead. #Data Review: ##Evaluation: The average score of 8.2 meets the criteria, but the standard deviation of 1.6 does not, therefore it is sent to the Data Adjudicator for evaluation. #Data Adjudicator: ##Comment: 1. Clear plan, relevant steps, and coherent structure. Minor redundancy noted. 2. Response offers thorough and strategic approach to consulting the village elders, ensuring well-informed and well-prepared adventure. 3. The response provides clear and structured plan to gather information, build relationships, and prepare for the journey. However, some minor verbalizations are present in the points, but are well approached despite bit of excess wording in some areas. ##Score: The average score of 8.1 meets the criteria. #Generate Data Summary: Navigate enchanted forests, solve riddles, face dragons to retrieve Crystal of Aether. Case 2: comprehensive data synthesis case of GRA. Domain Identification ## role - You are an expert in identifying the domain of SFT instructions. ## goal - Determine the domain of the provided instructions using the specified classification criteria. ## classification criteria - **Math**: Involves performing mathematical operations, applying formulas, and solving problems. - **Coding**: Encompasses understanding, generating, debugging, and fixing code. - **QA**: Provide specialized and accurate answers to questions within specific domain or field.. - **Language**: Accurately understand and analyze the given text, and then perform tasks including translation, summarization, classification, etc. - **Role Play**: Engage in simulated conversations or behaviors by assuming different roles to explore various perspectives or scenarios. - **Reasoning**: Carry out complex causal reasoning and logical judgment, and solve multi-level complex problems through reasonable inference and analysis. - **Creation**: Emphasize originality and creativity, create content according to requirements, and flexibly adapt to different styles and themes. ## output format - Assign each data to one domain category mentioned above only. - The output domain must be in English rather than Chinese. - Use <boa> and <eoa> to encapsulate the domain, e.g., <bod>\"domain\":\"the domain\"<eod>. Prompt 1: Prompt used in domain identification. Summary Generation ## role - You are an expert in concise summarization. ## goal - Analyze the provided instruction, summarize and generate brief, accurate summary. ## requirement - Limit the summary to no more than 30 words. - Ensure clarity and precision in capturing the essence of the instruction. - - Symbolic representations involving mathematical data must be in LaTeX format. ## output_format - The output summary must be in English rather than Chinese. - Use <bod> and <eod> to encapsulate the summary, e.g., <bod>\"summary\":\"the summary\"<eod>. - Output only the generated summary, nothing else. Prompt 2: Prompt used in summary generation. Key Word Extraction ## role - You are an expert at distilling key information from SFT instructions. ## goal - Extracts the keywords from the given instructions. ## output_format - The number of keywords must not exceed three. - The output keywords must be in English rather than Chinese. - Directly and Only output the data keywords. - Use <bok> and <eok> to encapsulate the keywords, e.g.,<bok>\"keywords\":[\"the keywords\"]<eok>. Prompt 3: Prompt used in key word extraction. 14 Key Word Generation ## role - You are creative language model specializing in innovative keyword generation and domain synthesis. ## goal - Generate unique set of keywords and domain that aligned with the provided examples and introducing novel concepts within the same domain. ## requirement - Maintain single domain with no more than three keywords. - Must only create one new data abstract. - The keywords must be novel, different to the provided examples. ## output_format - The output must be in English rather than Chinese. - Use <boa> and <eoa> main\",\"keywords\":[\"the keywords\"]<eoa>. - Directly output and only output the data abstract, nothing else. to encapsulate the domain and keywords, e.g., <boa>\"domain\":\"the doPrompt 4: Prompt used in key word generation. Instruction Generation ## role - You are an expert in data generation. ## goal - Generate high-quality domain related prompt, which related to key words like keywords. ## requirement - The generated prompt must be high-quality, accurately reflecting the domain and keywords. - The generated prompt must be semantically complete and contain the notation and terminology specifications of the corresponding field. - The generated prompt must specify the task the user wants the model to complete, answer question, or perform an action. - Symbolic representations involving mathematical data must be in LaTeX format. ## output_format - Please ensure that the prompt generated are not overly verbose while maintaining quality and compliance. - Output directly and only the prompt you generate, nothing else. - The generated prompt must be wrapped with <boi> and <eoi>, e.g., <boi>this is the generated prompt<eoi>. <begin of reference> summaries <end of reference> Prompt 5: Prompt used in instruction generation. 15 Instruction Evaluation ## role You are rigorous reviewer who is responsible for evaluating the quality of the instruction in an instruction-response pair. ## goal - For the given instruction, You need to evaluate it according to the several evaluation dimensions specified in the rule. ## rule - **Reasonable**: instruction must be clear and executable to ensure that they can be effectively responded to and handled. - **Completeness**: instruction must provide all necessary information and context required to perform the task, ensuring there are no missing elements that could hinder understanding or execution. - **Clarity**: instruction must be clear in language, accurate in wording, concise in syntactic structure, avoid vague expressions, clearly convey expected results, and eliminate ambiguity. ## constraint - **Reasonable**: if the instruction meets the criteria, score 1; otherwise, score 0. - **Completeness**: if the instruction meets the criteria, score 1; otherwise, score 0. - **Clarity**: if the instruction meets the criteria, score 1; otherwise, score 0. ## output_format 1. Use <bos> and <eos>to encapsulate the scores for each category as the list in python, e.g., <bos>[1,1,0]<eos>. 2. Directly output and only output the scores, nothing else. Prompt 6: Prompt used in instruction evaluation. Response Generation ## role You are professional language model assistant that aims to strictly generate high-quality response based on the instruction. ## goal - You need to generate correct, complete, and sufficient response based on the instruction, strictly following the the instruction. ## output_format - Symbolic representations involving mathematical data must be in LaTeX format. - Please output directly and only output response, nothing else. Prompt 7: Prompt used in response generation. 16 Response Evaluation ## role - You are rigorous reviewer who is responsible for evaluating the quality of the Response in an Instruction-Response pair. ## goal - For the given response, You need to strictly score and evaluate each Response according to the several evaluation dimensions specified in the rule. ## rule - Correctness: The response should accurately fulfill the instructions requirements with correct information and logic. - Clarity: The response must be expressed in clear, understandable language, avoiding ambiguity. - Completeness: The response should include all necessary details and explanations to fully address the instruction. - Relevance: The response must stay focused on instruction and exclude irrelevant information. - Coherence: The response should be logically structured and flow smoothly from one point to the next. - Ethicality: The response must be safe, unbiased, and free from harmful or inappropriate content. ## constraint - For each evaluation dimension in rule, give an score from 1 to 10, and form score_list. - In addition to the scores, you also need to provide specific review for the scores. - The review must be brief, under 30 words and focusing on the key points. ## output_format - Use <bos> and <eos> to encapsulate the score_list, e.g., <bos>[8,4,6]<eos>. - Use <boc> and <eoc> to encapsulate the review, e.g., <boc>THE CONTENT OF review<eoc>. - Please output directly and only the score_list and review in the specified output format, nothing else. - **Note: Be particularly critical in your evaluation to ensure only responses of the highest quality receive high scores.** Prompt 8: Prompt used in response evaluation. Data Adjudication ## role - You are rigorous reviewer who is responsible for evaluating the quality of the Response in an Instruction-Response pair. ## goal - For the given response, you need to strictly score and evaluate each Response according to the evaluation dimensions specified in the rule, while integrating insights from the prior evaluation. ## rule - Correctness: The response should accurately fulfill the instructions requirements with correct information and logic. - Clarity: The response must be expressed in clear, understandable language, avoiding ambiguity. - Completeness: The response should include all necessary details and explanations to fully address the instruction. - Relevance: The response must stay focused on instruction and exclude irrelevant information. - Coherence: The response should be logically structured and flow smoothly from one point to the next. - Ethicality: The response must be safe, unbiased, and free from harmful or inappropriate content. ## constraint - Must analyze the requirements of instruction, the quality of response, and the prior evaluation in detail. - For each evaluation dimension in rule, give an score from 1 to 10, and form score_list. - In addition to the scores, you also need to provide specific review for the scores. - The review must be brief, under 30 words and focusing on the key points. ## output_format - Use <bos> and <eos> to encapsulate the score_list, e.g., <bos>[8,4,6]<eos>. - Use <boc> and <eoc> to encapsulate the review, e.g., <boc>THE CONTENT OF review<eoc>. - Please output directly and only the score_list and review in the specified output format, nothing else. - **Note: Be particularly critical in your evaluation to ensure only responses of the highest quality receive high scores.** ## prior evaluation review <begin of Instruction-Response pair> ## instruction instruction ## response response <end of Instruction-Response pair> Prompt 9: Prompt used in data adjudication. SIZE Math GSM8K MATH Coding Humaneval MBPP Reasoning HellaSwag ARC-C GPQA BBH General MMLU IFEval 10K GRA1 20K GRA2 30K GRA3 40K GRA4 50K GRA 10K GRA1 20K GRA2 30K GRA3 40K GRA4 50K GRA5 10K GRA1 20K GRA2 30K GRA3 40K GRA4 50K GRA5 10K GRA1 20K GRA2 30K GRA3 40K GRA4 50K GRA5 10K GRA1 20K GRA2 30K GRA3 40K GRA4 50K GRA5 10K GRA1 20K GRA2 30K GRA3 40K GRA4 50K GRA5 64.90 65.88 67.10 65.88 65. 63.91 65.66 65.13 66.41 65.58 63.68 65.96 68.16 65.88 65.66 49.36 60.88 65.88 67.25 64.75 52.08 63.68 70.28 72.78 51.71 53.98 59.74 66.72 70.89 58.98 11.26 16.22 15.38 14.64 15. 10.22 14.86 11.02 15.16 17.2 10.08 16.98 14.50 18.98 14.14 20.10 42.10 34.30 39.04 42.82 20.42 33.50 33.82 46.48 44.62 23.20 31.02 27.70 38.58 47.84 LLama3.1 8B Condor 59.10 65.83 61.82 66.07 69.22 Alpaca 61.57 64.41 62.11 65.32 66.30 WizardLM 61.67 65.25 64.49 66.54 65.54 Qwen2.5 7B Condor 62.69 73.56 77.74 74.23 79.90 Alpaca 70.68 71.59 80.23 76.99 81.62 WizardLM 73.66 76.44 78.47 79.02 81. 46.60 47.40 48.40 47.40 48.80 47.20 47.00 48.00 49.20 48.60 47.80 47.20 50.00 47.00 48.20 64.40 66.40 66.00 66.60 65.60 65.60 65.80 64.80 65.80 64.60 65.20 65.80 64.80 64.60 64. 48.78 51.83 51.83 51.83 50.61 48.17 53.66 53.66 50.61 52.44 49.39 50.61 52.44 51.22 47.56 72.56 75.00 78.05 79.27 76.22 71.34 73.17 76.83 75.00 74.39 73.17 73.17 75.61 76.22 76. 69.49 70.51 69.15 72.20 72.54 69.83 71.19 70.17 73.56 71.53 68.47 72.54 69.15 72.88 73.22 62.37 74.24 73.90 73.22 75.59 70.17 71.19 74.24 71.19 77.63 70.51 73.22 74.24 77.63 79. 24.75 28.79 25.25 27.27 26.77 29.80 27.78 26.77 29.29 29.80 29.80 26.77 28.28 26.77 28.28 19.70 26.77 26.77 26.26 26.26 19.19 24.24 24.24 24.24 20.71 22.73 20.71 23.74 19.19 28. 64.46 63.28 63.13 64.75 63.48 63.80 64.62 62.91 64.78 64.50 63.35 64.97 63.13 63.28 64.25 41.18 51.95 57.26 51.11 61.49 45.00 45.49 60.57 45.27 62.97 48.30 53.02 53.38 59.29 63. 39.55 44.89 45.60 42.90 45.22 35.15 45.05 45.35 45.63 48.58 42.87 44.07 44.93 43.10 42.14 54.70 63.70 64.08 63.12 64.59 57.43 58.66 65.42 61.86 65.06 59.42 63.12 64.60 65.10 65. 43.17 48.40 48.03 54.49 53.54 46.08 50.16 53.41 55.31 56.80 48.47 51.44 52.28 53.52 55.27 50.13 50.97 53.52 52.86 54.02 50.37 49.91 53.16 55.53 54.58 51.08 50.59 51.17 53.55 54. AVG 47.21 50.31 49.57 50.75 51.60 47.58 50.44 49.86 51.53 52.14 48.56 50.58 50.74 50.92 50.43 49.72 58.56 59.75 59.30 61.13 52.23 55.72 60.36 59.52 59. 54.13 56.69 58.04 60.41 62.17 Table 3: Detail Results of effect of the data iteration . Bold underlined values represent the best results."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Shanghai AI Laboratory"
    ]
}