{
    "paper_title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?",
    "authors": [
        "Yang Chen",
        "Minghao Liu",
        "Yufan Shen",
        "Yunwen Li",
        "Tianyuan Huang",
        "Xinyu Fang",
        "Tianyu Zheng",
        "Wenxuan Huang",
        "Cheng Yang",
        "Daocheng Fu",
        "Jianbiao Mei",
        "Rong Wu",
        "Licheng Wen",
        "Xuemeng Yang",
        "Song Mao",
        "Qunshu Lin",
        "Zhi Yu",
        "Yongliang Shen",
        "Yu Qiao",
        "Botian Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at https://github.com/L-O-I/IWR-Bench."
        },
        {
            "title": "Start",
            "content": "IWR-BENCH: CAN LVLMS RECONSTRUCT INTERACTIVE WEBPAGE FROM USER INTERACTION VIDEO? IWR-Bench Team"
        },
        {
            "title": "ABSTRACT",
            "content": "The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at https://github.com/L-O-I/IWR-Bench."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in Large Vision-Language Models (LVLMs) have unlocked remarkable capabilities in visual understanding and code generation (OpenAI, 2025; Comanici et al., 2025; Bai et al., 2025). 5 2 0 2 9 2 ] . [ 1 9 0 7 4 2 . 9 0 5 2 : r Figure 1: Performance of 10 representative models on IWR-Bench. For comprehensive list of all 28 model results, see Table 3. 1 Figure 2: Overview of the IWR-Bench task and evaluation. The inputs to the model are (a) user interaction video and (b) composite images of all static assets sniffed from the webpage. The evaluation employs an agent-as-judge framework (Zhuge et al., 2024), where an automated agent assesses the rendered pages interactivity by executing (c) ground-truth action sequence and its visual fidelity through screenshot comparison. State-of-the-art models can now translate static screenshot of webpage into corresponding HTML with impressive fidelity (Yun et al., 2024; Gui et al., 2025). This nascent success, however, highlights fundamental limitation of current evaluation methodologies. Existing benchmarks are either confined to static reconstruction (e.g., Design2Code (Si et al., 2024), WebSight (Laurencon et al., 2024)) or model interactions as single-step, stateless events from image pairs (e.g., Interaction2Code (Xiao et al., 2025)), while also failing to provide the necessary static assets for reconstruction. This simplified setup falls short of capturing the continuous, stateful workflows and complete resource context characteristic of real-world web applications. The disconnect between demonstrated capabilities and the demands of true interactivity motivates our central research question: Can LVLMs reconstruct the dynamic, interactive functionalities of webpage from observing user interaction video? Reconstructing an interactive webpage from video poses two fundamental challenges. The first, comprehensive multi-modal perception and reasoning (Luo et al., 2024; Gupta & Kembhavi, 2023; Song et al., 2025; Deka et al., 2017; Lee et al., 2023), is the process of inferring latent interaction logic from dynamic visual evidence. This requires model to ground its temporal understanding of observed interactions in precise visual comprehension of the resultant UI states. critical facet of this reasoning is robust image matching to associate dynamic elements with their static asset counterparts. The second challenge, advanced code generation (Jimenez et al., 2024; Xiao et al., 2025; Li et al., 2022), is the translation of this inferred logic into functional code that implements the complex, stateful logic of interactive applications (e.g., web-games like 2048 and Minesweeper). The construction of comprehensive benchmark for interactive webpage reconstruction confronts three pivotal challenges. The first pertains to ensuring Diverse Interaction Coverage, which necessitates the curation of tasks spanning broad spectrum of interaction paradigms and visual complexities, while simultaneously adhering to strict standardization for reproducible evaluation. The second challenge centers on the establishment of an Authentic Task Environment. Departing from prior benchmarks characterized by incomplete setups or placeholder assets (Jiang et al., 2025; Gui et al., 2025), this requires the meticulous curation of complete set of authentic resources from live websites. Such resources must encompass both static assets, such as images and icons, and dynamic content, such as embedded videos, to faithfully represent real-world development contexts. The final challenge lies in the formulation of Robust Automated Evaluation protocol. Conventional metrics, including pixel-wise similarity, are insufficient for this purpose (Zhang et al., 2018; Caron et al., 2 Figure 3: An overview of the IWR-Bench taxonomy, which organizes tasks along three orthogonal axes: Domain, Visual Complexity, and Interaction Logic. 2021; Radford et al., 2021), as they cannot appraise functionality. An effective protocol must therefore employ programmatic interaction with the generated webpage to ascertain both the functional integrity of its components and the state-wise visual consistency across dynamic transitions. This paper formalizes the task of Interactive Webpage Reconstruction (IWR) and introduces IWRBench, comprehensive benchmark that addresses these fundamental design challenges. To ensure comprehensive coverage, tasks are taxonomized along orthogonal axes of application domain, visual complexity, and interaction logic, as illustrated in Figure 3. Each task instance, as depicted in Figure 2, then provides the model with (a) an interaction video that captures complete, stateful workflow, and (b) the full set of crawled static assets. This setup ensures realistic reconstruction context. Evaluation is conducted via programmatic interaction: an agent-as-a-judge executes ground-truth (c) action sequence to assess the generated webpages functionality. Performance is quantified by two holistic metrics: the Interactive Functionality Score (IFS), unified measure of operational and logical correctness, and the Visual Fidelity Score (VFS), composite metric integrating low-level features with high-level semantic evaluation. An extensive evaluation on 28 leading LVLMs reveals substantial challenges posed by the IWR task. The top-performing proprietary model, GPT-5, achieves Final Score of 36.35%. clear performance hierarchy is observed, as leading open-source models attain lower scores, and videospecialized models lag even further behind. For the top-performing model, significant disparity exists between its functional correctness (24.39% IFS) and visual fidelity (64.25% VFS). This gap indicates fundamental limitation across the field: while models can reproduce static layouts with moderate success, their capacity for synthesizing event-driven logic remains severely underdeveloped. Our key contributions are: Benchmark for Interactive Webpage Reconstruction. We introduce IWR-Bench, the first benchmark to formalize and evaluate Interactive Webpage Reconstruction (IWR) from video. It comprises 113 curated tasks from real-world websites, taxonomized along axes of domain, visual complexity, and interaction logic. Functionality-Centered Automated Evaluation Protocol. We develop robust evaluation protocol that employs programmatic agent to assess functional correctness by executing groundtruth action sequences. Performance is quantified by two holistic metrics: the Interactive Functionality Score (IFS) and the Visual Fidelity Score (VFS). An Extensive Evaluation and Analysis. We conduct comprehensive evaluation of 28 leading LVLMs, establishing strong initial baselines. The results reveal critical performance gap between visual replication and functional implementation. Further analysis identifies systematic weaknesses in temporal reasoning and logic synthesis, outlining concrete directions for future research."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Webpage Understanding. Webpage understanding evolved from structural analysis based on DOM parsing to subsequent multimodal perspective that jointly represents pages visual and textual content (Furuta et al., 2023; Burns et al., 2023; Liu et al., 2024a). Large Vision-Language Models (LVLMs) have advanced webpage understanding by enabling unified approach where single model demonstrates strong performance across diverse downstream tasks, indicative of deep comprehension, such as element grounding (Team, 2025) and screen-based question answering (Wang et al., 2024b; Xu et al., 2024). Among these capabilities, generating code from visual webpage representation is key task where existing models have demonstrated strong performance (Beltramelli, 2018; Yun et al., 2024; Gui et al., 2025). With the enhanced capabilities of LVLMs in handling multiple images or videos (Bai et al., 2025; OpenAI, 2025; Guo et al., 2025; Comanici et al., 2025), logical extension of this capability is the generation of interactive webpages, moving beyond static layouts to better mimic real-world applications. LVLM Benchmarks. The development of benchmarks for LVLMs has been driven by the rapid expansion of their capabilities, leading to evaluations of increasing complexity (Jimenez et al., 2024; Yang et al., 2024; Mialon et al., 2023; Lu et al., 2023). This progression is evident in the evolution from single-image comprehension to multi-image reasoning and video understanding (Yue et al., 2024; Li et al., 2023; Wang et al., 2024a; Liu et al., 2024b; Hu et al., 2025; Li et al., 2024; Fang et al., 2024; Fu et al., 2025; Ning et al., 2023; Chen et al., 2024; Yang et al., 2024; Lu et al., 2025). Concurrently, in the web domain, benchmarks have targeted either webpage understanding or static code generation from single screenshot (Beltramelli, 2018; Laurencon et al., 2024; Yun et al., 2024; Si et al., 2024; Gui et al., 2025; Jiang et al., 2025; Awal et al., 2025; Xu et al., 2025), with works like IW-Bench (Guo et al., 2024) creating more robust evaluation metrics for this task. recent advancement, Interaction2Code (Xiao et al., 2025), extends this by generating code from discrete interaction traces. However, such approaches primarily evaluate single-step, stateless events, rather than the complete, stateful workflows captured in continuous video. Therefore, critical disconnect exists between model capabilities for dynamic inputs and the benchmarks for interactive web generation. 3 IWR-BENCH 3.1 TASK DEFINITION AND STRUCTURE The Interactive Webpage Reconstruction (IWR) task challenges models to generate functional web code from observing user interactions. Formally, given video = {f1, ..., fn} demonstrating user 4 interactions and set of static assets = {a1, ..., am} from the original webpage, the model must generate code that reproduces both the visual appearance and interactive behavior observed in . Each task instance in IWR-Bench comprises four key components: Video Recording: screen capture documenting complete user interactions, preserving temporal dynamics and state transitions that define the webpages behavior. Static Web Assets: All relevant images, icons, and videos necessary for reconstruction. To prevent models from leveraging prior knowledge based on semantic filenames (e.g., logo.png), all asset filenames are anonymized (e.g., renamed to asset 001.png) (Agrawal et al., 2018; Gurari et al., 2018). This forces the model to rely on visual matching and reasoning. Action Trajectory: structured sequence = {(ai, pi, di, vi, li)}k i=1 where each action contains type ai, parameters pi, natural language description di, visual evaluation flag vi, and logical assertions li for verification. Checkpoint Screenshots: Stable-state images = {s1, ..., sk} capturing the visual state after each action. This ensures evaluation occurs on fully rendered pages rather than on transitional states. 3.2 BENCHMARK CONSTRUCTION Figure 4: The overview of Benchmark construction. Establishing and maintaining high standards for annotation quality and impartiality is central design principle in the development of IWR-Bench. The process is shown in Figure 4. Task Sourcing and Curation. The process begins with an initial set of 200 candidate tasks sourced from real-world websites by experts in web development. Each task is defined by high-level goal and URL to reflect common usage patterns. Through rigorous curation process involving deduplication and balancing for diversity across predefined axes, such as domain and complexity (Figure 3), this set is reduced to high-quality candidate pool of 161 tasks for annotation. Interaction Recording and Asset Collection. For each curated task, interactions on the live website are performed by trained annotators and captured as screen recordings, while browser extension concurrently records the action type ai and parameters pi for each action (Yun et al., 2024; Zhou et al., 2023). In parallel, all relevant static assets, such as images and icons, are collected via automated crawlers and manual inspection. Ground-Truth Trajectory and Checkpoint Annotation. The raw recordings and action logs are converted into the final ground-truth representation. For each action, the logged type ai and parameters pi are augmented through three-step annotation process: (1) natural language description di is authored to provide clear instruction; (2) visual evaluation flag vi is assigned, and corresponding checkpoint screenshot si is captured only when this flag is true, signifying major visual state change; and (3) an optional logical assertion li is defined where necessary to programmatically verify functional correctness, such as the appearance of message or game logic. Verification. Each annotated task undergoes two-stage quality assurance process. First, crossverification review by different annotator assesses trajectory correctness, asset completeness, and checkpoint fidelity, with large model additionally used to verify the accuracy of logical assertions against the ground truth (Zheng et al., 2023; Gou et al., 2025). All identified discrepancies are rec5 Table 2: Comparison of IWR-Bench with existing benchmarks. IWR-Bench is unique in its sourcing from live websites, video-based tasks, comprehensive interactive evaluation, and provision of static assets to create realistic reconstruction task. Benchmark Task Type Data Source Videos Images (Checkpoints) Asset Input Desktop & Mobile Interactive Evaluation (a) Webpage Reconstruction Benchmarks Pix2Code (Beltramelli, 2018) DWCG (Yun et al., 2024) WebSight (Laurencon et al., 2024) Design2Code (Si et al., 2024) CC-HARD (Gui et al., 2025) ScreenCoder (Jiang et al., 2025) Interaction2Code (Xiao et al., 2025) Image-to-Code Synthesized Image-to-Code Synthesized Image-to-Code Synthesized Image-to-Code C4 Image-to-Code C4 Live Websites Image-to-Code Images-to-Code C4 & GitHub (b) Video Understanding Benchmarks MMBench-Video (Fang et al., 2024) Video QA Video QA MVBench (Li et al., 2024) Video QA Video-MME (Fu et al., 2025) Video QA Video-MMMU (Hu et al., 2025) - - - - IWR-Bench (Ours) Video-to-Code Live Websites 609 20K 900 113 1.7K 60K 2M 484 128 3K 374 620* (Single-step) (Full Trajectory) * These are images used for evaluating visual fidelity across interaction states. tified. Finally, disputed, ambiguous, or overly trivial tasks are filtered out, leaving final collection of 113 verified tasks. 3.3 TAXONOMY AND STATISTICS IWR-Bench organizes tasks along three orthogonal axes (Figure 3; see Appendix for details): (1) Domain Coverage, which spans 5 major and 16 subcategories such as e-commerce and education to reflect real-world web diversity; (2) Visual Complexity, which scales from minimalist layouts to data-dense dashboards; and (3) Interaction Logic, which progresses from static content display to complex workflows and algorithmic game logic. Table 1: Key Statistics of IWR-Bench. Statistic Number Video & Resolution Statistics Total Videos - Short Videos (20s) - Medium Videos (20 60s) - Long Videos (>60s) Video Duration (avg/max) Unique Resolutions - Mobile 113 25 (22.1%) 72 (63.7%) 16 (14.2%) 35.4s / 172.9s 19 10.62% Table 1 presents key statistics. The benchmark includes 113 videos averaging 35.4 seconds, with 1,001 total actions across all sequences. Of these, 620 require visual evaluation and 403 include assertion checks, ensuring comprehensive assessment of both appearance and functionality. Tasks average 8.9 actions, with 10.62% targeting mobile interfaces, reflecting modern web usage patterns. - Visual Evaluation - Assertion Checks Actions per Video (avg) Evaluation Statistics Total Actions in Sequences 1001 620 403 8.9 3.4 COMPARISON TO OTHER BENCHMARKS As detailed in Table 2, IWR-Bench addresses critical gap between webpage reconstruction and video understanding benchmarks. Existing webpage reconstruction benchmarks either focus on static image-to-code tasks (e.g., Pix2Code, WebSight) or model interaction as stateless, single-step events without providing the necessary static assets for reconstruction (e.g., Interaction2Code). Conversely, general video understanding benchmarks (e.g., MVBench) are designed for comprehension tasks like Video QA, not code generation. IWR-Bench overcomes these limitations by using videos of stateful, full-trajectory workflows from live websites. It provides all required assets and employs robust agent-based protocol to evaluate true interactive correctness."
        },
        {
            "title": "4 EVALUATION AND METRICS",
            "content": "4.1 EVALUATION PROTOCOL The evaluation of generated code is conducted using deterministic executor built upon the browser-use library (browser-use, 2025). This executor programmatically interacts with the ren6 dered webpage by sequentially executing each pre-defined action ai from the ground-truth action trajectory . This design isolates the evaluation to code execution and removes any dependency on high-level task planning, thereby ensuring stable and reproducible protocol. The evaluation of the trajectory proceeds step-by-step. At each step i, the action ai is attempted. The action is considered failure under two conditions: (1) it is operationally infeasible (e.g., target element is not found), or (2) its corresponding logical assertions li are not satisfied. For logical assertion verification, an MLLM judge, specifically Gemini-2.5-Pro (Comanici et al., 2025), is employed to analyze screenshots of the page state before and after an action to determine its correctness. The prompt for this judge is detailed in Appendix E. Upon the successful completion of an action, new screenshot is captured. If the visual evaluation flag vi for this step is true, the new screenshot undergoes visual fidelity assessment. This assessment is based on composite score that integrates OCR-based text similarity (Cui et al., 2025), DINO-based structural similarity (Oquab et al., 2023), and high-level evaluation also conducted by Gemini-2.5-Pro (see Appendix for the prompt). Actions where vi is false, which typically involve insignificant or stochastic visual changes, are omitted from this visual assessment phase. 4.2 METRICS Model performance is quantified through hierarchy of metrics designed to measure functional correctness, visual fidelity, and overall task completion. Interactive Functionality Score (IFS). This metric measures models ability to generate functionally correct code. An action ai from the trajectory is considered successful if and only if it executes without operational errors and all associated logical assertions li are satisfied, as determined by the protocol in Section 4.1. The IFS is defined as the ratio of successfully completed actions (Nsucc) to the total number of actions (Ntotal). IFS = Nsucc Ntotal (1) Visual Fidelity Score (VFS). The VFS assesses the visual quality of the rendered user interface. This score is computed exclusively over checkpoints that were successfully reached and have the visual evaluation flag enabled (vi = true). Let Iv,succ be the set of indices for these qualifying checkpoints. The score for each checkpoint Iv,succ is weighted combination of two components: Low-level Visual Score (SLVS,i), which averages an OCR-based Levenshtein similarity and DINO-based cosine similarity, and High-level Visual Score (SHVS,i), which is holistic assessment from the MLLM judge. The final VFS is the macro-average of these checkpoint scores. The weight is set to 0.5 based on validation studies (Section 5.4). VFS = 1 Iv,succ (cid:88) iIv,succ (w SLVS,i + (1 w) SHVS,i) (2) Final Score. The Final Score is defined by combining the IFS and VFS with fixed weights. For steps where actions cannot be executed, no images are available to compute visual similarity scores. An alternative weighting scheme based on the ratio of successful (Nsucc) to total (Ntotal) steps was explored, but it proved ineffective for differentiating model performance. Therefore, simple weighted combination is adopted with the weighting factor α set to 0.7 (see Section 5.4). By assigning substantial weight to the IFS component, the impact of unreachable states on overall evaluation is appropriately reflected. All reported scores are macro-averaged across the entire benchmark. Final Score = α IFS + (1 α) VFS (3)"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EVALUATION SETUP Evaluation Models. The evaluation is conducted on diverse set of 28 leading Large VisionLanguage Models (LVLMs) to establish comprehensive performance baseline on IWR-Bench. 7 Table 3: Main evaluation results on IWR-Bench. Models are grouped by category and sorted by Final Score. Reasoning-enhanced (thinking) model variants are highlighted in gray. The best result in each column is bolded, and the second-best is underlined. Model Proprietary MLLMs GPT-5 (OpenAI, 2025) Claude-Sonnet-4 (thinking) (anthropic, 2025b) Claude-Opus-4 (thinking) (anthropic, 2025a) Doubao-seed-1.6 (bytedance, 2025) Claude-Sonnet-4 (anthropic, 2025b) Claude-Opus-4 (anthropic, 2025a) GPT-5-mini (OpenAI, 2025) GPT-4.1 (OpenAI, 2025) Gemini-2.5-Pro (thinking) (Comanici et al., 2025) Gemini-2.5-Pro (Comanici et al., 2025) GPT-4o (latest) (Hurst et al., 2024) Gemini-2.5-Flash (Comanici et al., 2025) GPT-5-nano (OpenAI, 2025) Grok-4 (X.ai, 2025) GPT-4o (0806) (Hurst et al., 2024) Doubao-seed-1.6-flash (bytedance, 2025) Gemini-2.5-Flash-Lite (Comanici et al., 2025) Open-Source MLLMs Qwen3-VL (thinking) (QwenTeam, 2025) Qwen2.5-VL-72B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) Keye-VL-1.5-8B (Yang et al., 2025) MiniCPM-V-4.5 (Yu et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) Kimi-VL (thinking) (Team et al., 2025b) Mimo-VL-7B (Team et al., 2025a) GLM-4.5V (Team et al., 2025c) Open-Source Video-Specialized LMs VideoLLaMA3-7B (Zhang et al., 2025) InternVideo-2.5-Chat-8B (Wang et al., 2025) Low-level Visual Score High-level Visual Score Visual Fidelity Score Interactive Functionality Score 68.29 64.90 63.53 65.95 65.75 65.23 63.36 63.07 54.52 57.46 63.39 47.53 53.49 48.95 54.03 45.49 28.95 58.55 47.83 39.36 30.81 31.18 28.92 26.18 23.28 16.31 31.29 17. 60.21 55.51 53.80 55.62 56.92 55.13 50.25 54.63 46.83 48.91 51.71 37.75 35.70 30.54 39.83 32.06 19.05 46.13 28.25 23.30 15.49 15.41 12.20 12.23 4.99 10.52 11.86 3.33 64.25 60.20 58.67 60.79 61.34 60.18 56.81 58.85 50.67 53.18 57.55 42.64 44.59 39.74 46.93 38.78 24.00 52.34 38.04 31.33 23.15 23.29 20.56 19.20 14.14 13.41 21.58 10. 24.39 23.65 23.61 22.55 22.29 21.83 23.18 20.48 21.65 20.51 17.55 19.88 18.17 19.44 15.87 16.34 13.29 22.07 17.42 16.50 16.06 15.58 13.28 12.04 10.57 10.11 10.29 9.97 Final Score 36.35 34.62 34.13 34.02 34.00 33.33 33.27 31.99 30.36 30.31 29.55 26.71 26.10 25.53 25.19 23.07 16.50 31.15 23.61 20.95 18.18 17.89 15.47 14.19 11.64 11. 13.67 10.07 This selection encompasses both proprietary and open-source models, as well as specialized video understanding models. The full list of evaluated models and their performance is detailed in Table 3. Implementation Details. For each task in IWR-Bench, models are provided with the user interaction video and composite image of all crawled static assets. To accommodate models without native video support, each video is sampled at 1 fps, with the number of frames capped at 64. Videos exceeding 64 seconds are uniformly downsampled to meet this limit. The video (or its sampled frames) and the composite image are arranged as sequential, multi-image input. The task is to generate single, self-contained HTML file that integrates all necessary CSS and JavaScript to replicate the observed webpage. All other inference parameters utilize the default settings recommended by the model providers. The complete prompt templates are detailed in Appendix E. 5.2 MAIN RESULTS The comprehensive evaluation results on IWR-Bench are presented in Table 3. The findings reveal clear performance landscape, highlighting the substantial difficulty of the task and surfacing several key observations regarding current model capabilities, with case studies provided in Appendix F. Clear Performance Hierarchy Is Observed Across Model Categories. The results on IWRBench show pronounced performance stratification across model categories. Proprietary multimodal large language models are positioned in the upper echelon, with GPT-5 obtaining the highest Final Score (36.35). This is followed by competitive cluster that includes Claude-Sonnet-4 (thinking) (34.62), Claude-Opus-4 (thinking) (34.13), Doubao-seed-1.6 (34.02), and Claude-Sonnet4 (34.00). The top-performing open-source model, Qwen3-VL (thinking), has score of 31.15. This 8 score is lower than that of the leading proprietary group but surpasses several mid-tier proprietary entries, such as GPT-4o (latest) (29.55). At the lower end of the performance spectrum, videospecialized models like VideoLLaMA3-7B (13.67) and InternVideo-2.5-Chat-8B (10.07) are found. This hierarchy indicates that general multimodal reasoning and code generation capabilities are more critical for success on IWR-Bench than specialized video-processing architectures. Interactive Functionality Remains the Primary Performance Bottleneck. substantial performance gap exists between static visual replication and dynamic functionality implementation. This gap is reflected in the consistently higher Visual Fidelity Scores (VFS) compared to the Interactive Functionality Scores (IFS). For instance, GPT-5 obtains the highest visual metrics (LVS 68.29, HVS 60.21, VFS 64.25), yet its corresponding IFS is only 24.39. similar pattern is observed for ClaudeSonnet-4, which has the second-highest VFS (61.34) but an IFS of only 22.29. The difficulty of this task is further underscored by the low absolute IFS values, with the highest score remaining below 25, highlighting that interactive webpage reconstruction is largely unsolved problem. Reasoning Enhancement Provides Consistent but Moderate Gains. Consistent but moderate performance improvements are observed when using reasoning-enhanced inference. For instance, the thinking variant of Claude-Sonnet-4 shows higher performance in both Final Score (34.62 vs. 34.00) and IFS (23.65 vs. 22.29). similar trend is noted for Claude-Opus-4 (Final 34.13 vs. 33.33; IFS 23.61 vs. 21.83) and Gemini-2.5-Pro (Final 30.36 vs. 30.31; IFS 21.65 vs. 20.51). This evidence indicates that while enhanced reasoning acts as useful refinement, the base models capability remains the primary factor determining the performance ceiling on IWR-Bench. 5.3 PERFORMANCE ANALYSIS ACROSS TASK DIMENSIONS fine-grained analysis (detailed in Appendix B) reveals distinct performance patterns. The synthesis of event-driven functionality is the primary bottleneck, evidenced by sharp performance drop from static (L1) to interactive (L2-L4) tasks  (Table 4)  . Models also struggle with highly structured layouts  (Table 5)  . Performance varies by domain, with relative strength in Entertainment & Media  (Table 6)  , pointing to structured code generation and state management as key research directions. 5.4 VALIDATION OF THE EVALUATION PROTOCOL The robustness and reliability of the evaluation protocol are validated through rigorous, two-part analysis that addresses both the metric parameters and the agent-as-a-judge methodology (Zheng et al., 2023; Gou et al., 2025; Maaz et al., 2023). First, the weighting coefficients (w and α) for the scoring metrics are determined through human alignment study, with the detailed procedure and results presented in Appendix C. Second, the agent-as-a-judge framework is validated through multistage process. This process includes meticulous cross-verification of annotated action trajectories (Section 3.2), automated verification of logical assertions using an MLLM-based judge(Comanici et al., 2025), and manual inspection of the agents operational fidelity. For the manual inspection, three PhD students observed the agents execution on 100 randomly sampled, model-generated webpages, with the browsers headless mode disabled to compare on-screen behavior against evaluation logs. Failures in the agents evaluation were observed in only three instances. These issues typically stemmed from ambiguous element descriptors (e.g., buttons with identical names), which required more precise locator (d i). All identified discrepancies were subsequently rectified."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces IWR-Bench, the first benchmark designed to evaluate Interactive Webpage Reconstruction from video. Through an automated agent-as-a-judge evaluation protocol, performance is quantified using two metrics: the IFS and the VFS. Comprehensive evaluations on 28 LVLMs reveal stark disparity between visual replication and functional implementation. While models achieve moderate success in reconstructing static appearance (VFS), their ability to generate correct, event-driven logic remains critically limited, as shown by low IFS scores across the board. This finding indicates that the primary bottleneck for current models is not visual understanding but the synthesis of complex interaction logic. IWR-Bench thus establishes challenging new frontier for vision-language research, highlighting the need for future work to focus on temporal reasoning, dynamic asset binding, and robust code synthesis to create truly functional web applications."
        },
        {
            "title": "7 CONTRIBUTORS",
            "content": "Core Contributors ( Equal Contribution ) Yang Chen, Shanghai AI Lab, Zhejiang University Minghao Liu, 2077AI, M-A-P Yufan Shen, Shanghai AI Lab Yunwen Li, Chinese University of Hong Kong(shenzhen), M-A-P Tianyuan Huang, Zhejiang University Xinyu Fang, Shanghai AI Lab, Zhejiang University Contributors Tianyu Zheng, M-A-P Wenxuan Huang, Chinese University of Hong Kong Cheng Yang, Shanghai AI Lab, Central South University Daocheng Fu, Shanghai AI Lab, Fudan University Jianbiao Mei, Shanghai AI Lab, Zhejiang University Rong Wu, Shanghai AI Lab, Zhejiang University Advisors Licheng Wen, Shanghai AI Lab Xuemeng Yang, Shanghai AI Lab Song Mao, Shanghai AI Lab Qunshu Lin, 2077AI Zhi Yu, Zhejiang University Yongliang Shen, Zhejiang University Yu Qiao, Shanghai AI Lab Corresponding Authors Yufan Shen, Shanghai AI Lab Botian Shi, Shanghai AI Lab, Shanghai Innovation Institute Project Leader Yufan Shen, Shanghai AI Lab [Email: shenyfzju@gmail.com]"
        },
        {
            "title": "REFERENCES",
            "content": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Dont just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 49714980, 2018. anthropic. claude-opus. https://www.anthropic.com/claude/opus, 2025a. anthropic. claude-sonnet-4. https://www.anthropic.com/claude/sonnet, 2025b. Rabiul Awal, Mahsa Massoud, Aarash Feizi, Zichao Li, Suyuchen Wang, Christopher Pal, Aishwarya Agrawal, David Vazquez, Siva Reddy, Juan Rodriguez, et al. Webmmu: benchmark for multimodal multilingual website understanding and code generation. arXiv preprint arXiv:2508.16763, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Tony Beltramelli. pix2code: Generating code from graphical user interface screenshot. In Proceedings of the ACM SIGCHI symposium on engineering interactive computing systems, pp. 16, 2018. browser-use. browser-use. https://browser-use.com/, 2025. Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan Plummer, Kate Saenko, Jianmo Ni, and Mandy Guo. suite of generative tasks for multi-level multimodal webpage understanding. arXiv preprint arXiv:2305.03668, 2023. bytedance. seed1x6. https://seed.bytedance.com/en/seed1x6, 2025. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering. In European Conference on Computer Vision, pp. 179195. Springer, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pp. 845854, 2017. Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2024. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025. Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023. Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jimenez Gutierrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web 2: Evaluating agentic search with agent-as-a-judge, 2025. URL https://arxiv.org/abs/ 2506.21506. Yi Gui, Zhen Li, Zhongyi Zhang, Guohao Wang, Tianpeng Lv, Gaoyang Jiang, Yi Liu, Dongping Chen, Yao Wan, Hongyu Zhang, et al. Latcoder: Converting webpage design to code with layoutas-thought. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pp. 721732, 2025. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. Hongcheng Guo, Wei Zhang, Junhao Chen, Yaonan Gu, Jian Yang, Junjia Du, Binyuan Hui, Tianyu Liu, Jianxin Ma, Chang Zhou, et al. Iw-bench: Evaluating large multimodal models for converting image-to-web. arXiv preprint arXiv:2409.18980, 2024. Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1495314962, 2023. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael Lyu, and Xiangyu Yue. Screencoder: Advancing visual-to-code generation for front-end automation via modular multimodal agents. arXiv preprint arXiv:2507.22827, 2025. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In ICLR, 2024. Hugo Laurencon, Leo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029, 2024. Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: ScreenIn International Conference on shot parsing as pretraining for visual language understanding. Machine Learning, pp. 1889318912. PMLR, 2023. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seedbench-2: Benchmarking multimodal large language models, 2023. URL https://arxiv. org/abs/2311.17092. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. 12 Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024a. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS23, 2023. Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Webgen-bench: Evaluating llms on generating interactive and functional websites from scratch, 2025. URL https://arxiv.org/abs/2505. 03733. Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. Layoutllm: Layout instruction tuning with large language models for document understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1563015640, 2024. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: In The Twelfth International Conference on Learning benchmark for general ai assistants. Representations, 2023. Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. OpenAI. Gpt-5 system card. openai.com/index/gpt-5-system-card, 2025. Accessed: 2025-09-04. OpenAI. gpt-4-1. https://openai.com/index/gpt-4-1/, April 2025. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. QwenTeam. qwen3-vl. https://qwen.ai/blog?id= 99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research. latest-advancements-list, September 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: Benchmarking multimodal code generation for automated front-end engineering. arXiv preprint arXiv:2403.03163, 2024. Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025. Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, et al. Mimo-vl technical report, 2025a. URL https://arxiv.org/abs/2506.03569. 13 General Agents Team. The showdown computer control evaluation suite, 2025. URL https: //github.com/generalagents/showdown. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, et al. Kimi-vl technical report, 2025b. URL https://arxiv.org/abs/2504.07491. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025c. URL https://arxiv. org/abs/2507.01006. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024a. Maria Wang, Srinivas Sunkara, Gilles Baechler, Jason Lin, Yun Zhu, Fedir Zubach, Lei Shu, and Jindong Chen. Webquest: benchmark for multimodal qa on web page sequences. arXiv preprint arXiv:2409.13711, 2024b. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. X.ai. grok-4. https://x.ai/news/grok-4, July 2025. Jingyu Xiao, Yuxuan Wan, Yintong Huo, Zixin Wang, Xinyi Xu, Wenxuan Wang, Zhiyao Xu, Yuhang Wang, and Michael R. Lyu. Interaction2code: Benchmarking mllm-based interactive webpage code generation from interactive prototyping, 2025. URL https://arxiv.org/ abs/2411.03292. Hongshen Xu, Lu Chen, Zihan Zhao, Da Ma, Ruisheng Cao, Zichen Zhu, and Kai Yu. Hierarchical In Proceedings of the 17th multimodal pre-training for visually rich webpage understanding. ACM International Conference on Web Search and Data Mining, pp. 864872, 2024. Kai Xu, YiWei Mao, XinYi Guan, and ZiLong Feng. Web-bench: llm code benchmark based on web standards and frameworks. arXiv preprint arXiv:2505.07473, 2025. Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, et al. Kwai keye-vl 1.5 technical report, 2025. URL https://arxiv.org/abs/2509. 01563. John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, et al. Swe-bench multimodal: Do ai systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations, 2024. Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, Bokai Xu, Junbo Cui, Yingjing Xu, Liqing Ruan, Luoyuan Zhang, Hanyu Liu, Jingkun Tang, Hongyuan Liu, Qining Guo, Wenhao Hu, Bingxiang He, Jie Zhou, Jie Cai, Ji Qi, Zonghao Guo, Chi Chen, Guoyang Zeng, Yuxuan Li, Ganqu Cui, Ning Ding, Xu Han, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe, 2025. URL https://arxiv.org/abs/2509. 18154. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Sukmin Yun, Rusiru Thushara, Mohammad Bhat, Yongxin Wang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, et al. Web2code: large-scale webpageto-code dataset and evaluation framework for multimodal llms. Advances in neural information processing systems, 37:112134112157, 2024. 14 Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. Agent-asa-judge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934, 2024. 15 MULTI-DIMENSIONAL TASK TAXONOMY To move beyond monolithic view of difficulty and enable fine-grained analysis of model capabilities, we developed three-dimensional taxonomy to classify each task. This taxonomy categorizes tasks along the orthogonal axes of Interaction Complexity, Visual Complexity, and Application Domain, providing structured framework to understand the specific challenges inherent in each task and to diagnose model failure modes with high precision. Interaction Complexity (L1-L4) This first axis categorizes tasks based on the depth of logical and temporal understanding required for successful reconstruction. L1: Static Content Consumption. Tasks involve passive information consumption, primarily requiring correct handling of vertical scrolling to reconstruct long pages that extend beyond single viewport (e.g., browsing blog post or projects README). L2: Simple State Manipulation. Tasks feature components that manage local state, such as filtering e-commerce results, switching between on-page tabs, or expanding/collapsing accordion menus. This level tests the generation of basic client-side event handlers. L3: Complex Workflow Interaction. These tasks involve multi-step, sequential interactions where state is passed between components, such as multi-step product configurator or an online booking process. This tests understanding of application logic and inter-component communication. L4: Algorithmic/Game Logic. The most complex level requires the model to reverseengineer and implement set of rules or algorithms, such as an online calculator, textbased puzzle, or simple game like 2048. Visual Complexity (V1-V4) This second axis captures the static challenge of rendering the webpages appearance, focusing on its layout and styling. V1: Minimalist Layouts. Simple, singleor two-column structures with standard element alignment, typical of documentation or text-heavy sites. V2: Standard Grid-based Layouts. Organized grid systems are used in e-commerce or news portals, featuring numerous but regularly arranged elements. V3: Asymmetric & Modern Layouts. Visually-driven designs with complex CSS, such as overlapping elements, parallax scrolling, and non-standard component shapes. V4: Data-Dense Layouts. Dashboards or admin panels with high density of information presented in charts, tables, and data cards, testing the ability to generate precise, repetitive structures. Application Domain To ensure our benchmark reflects the breadth of real-world web applications, the third axis classifies tasks by their Application Domain. Drawing inspiration from established taxonomies in web-centric agent research (Gou et al., 2025), we group tasks into five high-level domains: Commerce & Services (e-commerce, booking, finance), Knowledge & Education (academic sites, news portals, documentation), Productivity & Tools (calculators, project management boards), Entertainment & Media (games, streaming platforms), and Lifestyle & Community (social forums, blogs). This classification guarantees that models are evaluated across diverse spectrum of functionalities and visual paradigms. For instance, task involving filtering products on an e-commerce site is tagged as [L2, V2, Commerce], while task requiring the reconstruction of simple browser game is tagged as [L4, V1, Entertainment]. This multi-dimensional labeling allows us to analyze whether models performance correlates with certain types of interactions, visual styles, or application contexts."
        },
        {
            "title": "B DETAILED EXPERIMENTAL RESULTS",
            "content": "This appendix provides comprehensive breakdown of model performance on IWR-Bench across the three classification axes defined in the taxonomy: Application Domain  (Table 6)  , Interaction Logic Complexity  (Table 4)  , and Visual Complexity  (Table 5)  . For each task category, the Final Score is reported. 16 Table 4: Final Score breakdown by Interaction Logic Complexity. Static Content Consumption (L1) Simple State Manipulation (L2) Complex Workflow Interaction (L3) Algorithmic/Game Logic (L4) Model Proprietary MLLMs GPT-5 Claude-Sonnet-4 (thinking) Claude-Opus-4 (thinking) Doubao-seed-1.6 Claude-Sonnet-4 Claude-Opus-4 GPT-5-mini GPT-4.1 Gemini-2.5-Pro (thinking) Gemini-2.5-Pro GPT-4o (latest) Gemini-2.5-Flash GPT-5-nano Grok-4 GPT-4o (0806) Doubao-seed-1.6-flash Gemini-2.5-Flash-Lite Open-Source MLLMs Qwen3-VL (thinking) Qwen2.5-VL-72B Qwen2.5-VL-32B Keye-VL-1.5-8B MiniCPM-V-4.5 Qwen2.5-VL-7B Kimi-VL (thinking) Mimo-VL-7B GLM-4.5V Open-Source Video-Specialized LMs VideoLLaMA3-7B InternVideo-2.5-Chat-8B 61.85 65.75 61.78 63.69 68.36 66.88 56.83 51.45 68.96 59.58 48.61 56.57 46.23 48.37 38.59 42.77 35.34 51.05 45.35 37.64 46.85 37.30 28.58 23.61 22.23 18.56 23.11 27.89 35.43 33.88 31.52 34.03 31.04 30.85 32.10 30.32 26.88 31.51 29.09 24.95 25.57 24.94 24.95 22.14 15. 30.43 22.83 20.45 19.47 18.02 15.99 15.45 12.32 11.02 14.15 9.81 35.12 31.05 32.57 30.12 32.86 31.30 31.38 31.98 28.15 23.95 27.53 25.53 24.31 22.76 24.65 22.65 13.87 29.86 21.90 18.80 12.11 15.27 13.91 10.63 9.36 9.77 12.31 7.68 25.26 25.86 30.07 24.08 25.15 25.76 26.84 24.78 23.83 22.58 23.75 15.00 19.02 19.53 17.90 13.88 14. 23.60 15.46 17.48 10.13 11.10 8.21 12.75 7.87 10.08 8.77 5."
        },
        {
            "title": "C METRIC PARAMETER VALIDATION",
            "content": "The VFS and Final Score metrics rely on the weighting coefficients and α. To determine and validate these parameters, human alignment study was conducted. sample of 60 evaluation instances was constructed by selecting outputs from three randomly chosen models for each of 20 randomly selected tasks from IWR-Bench. Five PhD-level students assessed each instance on two dimensions: visual fidelity and overall quality. The optimal parameter values are determined by maximizing the Spearmans ρ correlation between the automated scores and the aggregated human judgments. For the VFS, peak correlation (ρ = 0.57) is achieved with = 0.5, giving equal weight to LVS and HVS. For the Final Score, correlation with human overall judgment is maximized (ρ = 0.65) with α = 0.7, empirically validating the decision to weigh functionality (IFS) more heavily than visual fidelity (VFS)."
        },
        {
            "title": "D TASK AND ACTION REPRESENTATION",
            "content": "Each task in IWR-Bench is formally defined by an action sequence, structured list of discrete actions that an automated agent must perform to validate the reconstructed webpage. This representation standardizes the evaluation process. We defined vocabulary of atomic actions, including Click(description), Type(key, description), Scroll(direction, amount, description), and Press(key, description). crucial design choice is the use of natural language description field for targeting elements instead of unstable positional coordinates (e.g., Click the primary Submit button instead of Click at (x:120, y:350)). This makes the evaluation robust to minor layout variations in the generated code and tests more semantic understanding of the page structure, both for the model during generation and the agent during evaluation. 17 Table 5: Final Score breakdown by Visual Complexity. Minimalist Layouts (V1) Standard Grid-based Layouts (V2) Asymmetric & Modern Layouts (V3) Data-Dense Layouts (V4) Model Proprietary MLLMs GPT-5 Claude-Sonnet-4 (thinking) Claude-Opus-4 (thinking) Doubao-seed-1.6 Claude-Sonnet-4 Claude-Opus-4 GPT-5-mini GPT-4.1 Gemini-2.5-Pro (thinking) Gemini-2.5-Pro GPT-4o (latest) Gemini-2.5-Flash GPT-5-nano Grok-4 GPT-4o (0806) Doubao-seed-1.6-flash Gemini-2.5-Flash-Lite Open-Source MLLMs Qwen3-VL (thinking) Qwen2.5-VL-72B Qwen2.5-VL-32B Keye-VL-1.5-8B MiniCPM-V-4.5 Qwen2.5-VL-7B Kimi-VL (thinking) Mimo-VL-7B GLM-4.5V Open-Source Video-Specialized LMs VideoLLaMA3-7B InternVideo-2.5-Chat-8B"
        },
        {
            "title": "E PROMPTS",
            "content": "44.77 40.05 37.01 41.55 37.33 35.77 38.76 35.24 31.59 37.20 31.89 32.21 29.21 33.73 29.88 30.23 19.64 38.14 28.68 27.56 24.75 28.24 20.48 18.57 14.68 12.39 18.81 13.36 30.73 31.12 32.26 30.56 30.28 30.79 29.29 28.96 25.77 25.17 26.14 21.80 24.39 22.11 23.54 22.76 15.44 28.74 21.14 17.90 15.65 16.11 12.68 14.22 11.42 10.66 13.55 11. 43.77 40.08 38.95 38.03 39.95 38.34 38.44 36.89 36.99 36.51 34.64 34.13 28.96 30.01 27.41 22.87 19.31 33.37 28.31 24.06 20.85 18.87 18.52 14.21 12.42 13.34 13.69 9.09 26.05 25.26 22.97 26.97 26.25 24.77 26.75 25.32 25.82 23.27 24.41 16.28 20.37 17.18 19.62 17.27 8.31 26.25 12.48 16.15 12.74 10.79 11.48 9.26 6.71 4.20 8.56 5. standardized system prompt is employed for all models and tasks in IWR-Bench to ensure fair evaluation. This prompt defines clear requirements for the task, output format, and operational constraints. Such design minimizes ambiguity and helps isolate the core code generation capabilities of each model. The complete prompt template is detailed in Figure 5. The evaluation relies on large multimodal model guided by two distinct prompts. To assess the similarity between generated and reference webpages, prompt template is utilized (Figure 6). This template instructs the model to perform both quantitative and qualitative evaluations. For logical assertion verification, separate prompt, presented in Figure 7, is employed to determine the correctness of an action."
        },
        {
            "title": "F CASE STUDY",
            "content": "This section presents selection of representative tasks from the IWR-Bench to illustrate the diversity of challenges encompassed by our benchmark. The input of each case includes webpage operation video and the static resources involved in the webpage. Then the web pages generated by different multimodal large models and the corresponding interaction results are displayed. Then we provide detailed analysis of these representative cases, corresponding to the figures presented below. Each analysis breaks down the tasks objectives, its position within our taxonomy, and the specific model behaviors observed, illustrating how our benchmark facilitates fine-grained diagnosis of model capabilities. 18 Table 6: Final Score breakdown by Application Domain. Business & Services Entertainment & Media Knowledge & Education Life & Community Productivity & Tools Model Proprietary MLLMs GPT-5 Claude-Sonnet-4 (thinking) Claude-Opus-4 (thinking) Doubao-seed-1.6 Claude-Sonnet-4 Claude-Opus-4 GPT-5-mini GPT-4.1 Gemini-2.5-Pro (thinking) Gemini-2.5-Pro GPT-4o (latest) Gemini-2.5-Flash GPT-5-nano Grok-4 GPT-4o (0806) Doubao-seed-1.6-flash Gemini-2.5-Flash-Lite Open-Source MLLMs Qwen3-VL (thinking) Qwen2.5-VL-72B Qwen2.5-VL-32B Keye-VL-1.5-8B MiniCPM-V-4.5 Qwen2.5-VL-7B Kimi-VL (thinking) Mimo-VL-7B GLM-4.5V 39.37 39.74 35.54 37.82 38.84 38.64 37.16 33.58 31.49 33.66 36.33 29.16 30.61 27.71 27.60 25.37 18.38 37.36 28.71 24.05 19.27 17.10 17.11 16.77 12.90 10.78 Open-Source Video-Specialized LMs VideoLLaMA3-7B InternVideo-2.5-Chat-8B 14.36 10. 47.24 41.02 42.48 43.19 42.09 43.25 45.11 45.35 41.67 36.93 32.35 40.60 34.29 32.06 32.05 31.07 24.93 35.00 31.20 28.58 26.29 26.63 21.96 17.58 14.64 18.20 16.44 15.77 37.05 32.60 34.91 32.14 32.22 30.51 30.29 27.61 30.31 30.74 28.97 21.38 22.67 26.47 23.97 20.17 10.13 29.57 20.89 20.67 16.42 17.38 16.69 13.87 12.11 10.10 14.44 9. 22.80 24.86 24.21 22.44 25.57 22.03 21.38 17.28 15.82 19.03 20.01 15.27 16.50 15.00 15.54 14.21 9.42 17.71 14.29 9.80 9.95 11.21 4.67 9.59 4.33 2.88 9.21 2.90 28.53 31.15 28.56 30.20 27.58 28.09 28.36 32.65 26.34 25.55 25.42 25.83 23.69 19.85 23.29 22.36 20.55 31.19 19.88 16.73 16.50 14.63 11.61 10.83 11.08 11.10 11.51 9. Case 1 Analysis: E-commerce Workflow Simulation. Our first case study, classified as [L2, V2, E-commerce], simulates fundamental e-commerce user journey to test models ability to handle sequential state manipulations within standard visual structure. The task requires the model to replicate workflow involving filtering product grid by specific brand, sorting the filtered results by price and adding selected item to the shopping cart. As illustrated in Figure 8, this task effectively exposes different failure modes in different models. On the left, Claude-Sonnet-4 demonstrates good capabilities in static replication and simple state management. It accurately renders the initial layout and correctly implements the action for filtering and sorting. However, its failure occurs at the final add to cart step. The right side shows the result of GPT-5. This case likely involved too many static resources, causing the product list on the initial page to fail to render successfully. By pinpointing these different stages of failure, the benchmark provides granular diagnosis of each models specific strengths and weaknesses in front-end code generation. Case 2 Analysis: Algorithmic Logic Reconstruction. This case study moves to the highest level of our interaction complexity scale, L4, to assess models capacity for algorithmic reasoning. Classified as [L4, V2, Gaming], the task requires the model to reverse-engineer and implement the complete set of rules for simple browser game (e.g., 2048) based solely on observing its behavior in the input video. The visual complexity is simple (V2), deliberately shifting the evaluation focus from layout replication to the correctness of the underlying algorithmic logic. The The core challenge is to deduce and codify the games state-transition functions, including tile movement, merging logic, and the spawning of new tiles. 19 Prompt template for the IWR-Bench You are an expert front-end developer. Your task is to create pixel-perfect replica of website from video. Generate single index.html file that contains all HTML, CSS, and JavaScript necessary to replicate the UI, content, and interaction features shown. The webpage resolution in the video is <resolution>. Instructions: 1. Single File Output: All HTML, CSS, and JS must be in one index.html file. 2. If backend logic is implied, mock it in JS with static data (e.g., JS array for fake API call). 3. Assets(Images and Videos in the webpage): All images must use the provided stitched image assets. The attribute must start with src PLACEHOLDER ASSETS BASE DIR /, tified from the stitched image. the string followed by the actual filename idenunchanging literal, For example: src= PLACEHOLDER ASSETS BASE DIR /asset001.svg. <img> tags must include width and height attributes. The provided stitched image assets are before the video. 4. No External Dependencies: The generated code must be entirely self-contained. No External Libraries and no External Fonts. 5. Final Response: Return only the complete HTML code in single html code block, with no additional text or explanations. Figure 5: Prompt template for the IWR-Bench Prompt for evaluating HVS You are an expert Webpage Evaluator. Your task is to provide quantitative and qualitative assessment of the similarity between generated webpage and reference webpage. The default score is 0. Evaluation Format: Comments: -Layout (10 points): ${comment and subscore} -Elements (15 points): ${comment and subscore} -Content and Text (40 points): ${comment and subscore} -Style (15 points): ${comment and subscore} -Overall (20 points): ${comment and subscore} Score: ${final score}/100 Figure 6: Prompt for evaluating HVS As illustrated in Figure 9, the left side is the result of Grok-4, which can successfully reproduce the 2048 game logic from the input video. However, Qwen2.5-VL-72B failed to merge the corresponding blocks after inputting . This type of task requires high level of logical reasoning ability from the model and is significant challenge. Case 3 Analysis: Long-Context Fidelity and Fine-Grained Visual Detail. This case study, classified as [L1, V3, E-commerce], is designed to stress-test models visual fidelity on multiple fronts. While the interaction is simple (L1, passive scrolling), the tasks difficulty lies in three distinct challenges: (1) maintaining structural integrity across long page, (2) correctly handling diverse media assets, including an embedded video, and (3) achieving fine-grained visual accuracy, particularly with small, repetitive elements like icons. This multi-faceted task evaluates not just broad layout re-"
        },
        {
            "title": "Prompt used to determine whether the assertion is correct",
            "content": "Please compare two webpage screenshots (Image 1 is the previous step, Image 2 is the current step) and determine whether the following assertion is true: Assertion: {assertion} Return JSON format without any additional information: {{ think: the thinking process, result: YesNo }} Figure 7: Prompt used to determine whether the assertion is correct construction but also the models attention to detail and its ability to precisely match visual elements to the provided stitched assets. As illustrated in Figure 10, Gemini-2.5-pro and GPT-5 can both restore relatively complete long pages based on videos, but they do not handle the details of the web pages well, including the corresponding icons and matching product images. successful reconstruction would require both holistic understanding of the page structure and meticulous attention to its smallest components. Case 4 Analysis: Time-Based State Management in Mobile Viewport. This case study, classified as [L3, V1, Productivity & Tools], is designed to evaluate models ability to handle timedriven state changes, presented within the constraints of mobile screen resolution. The task is to reconstruct functional Pomodoro timer. While the visual complexity is low (V1), the mobile viewport requires the model to generate layout that is responsive or appropriately scaled for narrow screen. The primary challenge, however, resides in the L3 interaction complexity: the model must implement state logic governed by both user clicks (e.g., start, pause) and asynchronous, time-based events (the countdown reaching zero). As illustrated in Figure 11, GLM-4.5V can successfully implement the interactive operations and logic in the video, but Kimi-VL-thinking is unable to perform subsequent operations because the elements that need to be clicked in the first step are missing in the initial state."
        },
        {
            "title": "G USE OF LARGE LANGUAGE MODELS",
            "content": "We utilized Large Language Model to assist with grammar correction and language refinement in this paper. 21 Figure 8: Case 1: Multi-Step E-commerce Workflow. This task, classified as [L2, V2, Ecommerce], requires reconstructing core e-commerce workflow involving filtering products, sorting the results, and adding an item to the shopping cart. 22 Figure 9: Case 2: Algorithmic Game Logic Reconstruction. This task challenges models to reconstruct the rules of the simple browser game 2048. Classified as [L4, V2, Gaming], the primary difficulty lies in algorithmic correctness, not visual complexity. Figure 10: Case 3: Full-Page Reconstruction with Long Scrolling. This task focuses on fundamental capability: reconstructing webpage that extends far beyond the initial viewport. Classified as [L1, V3, E-commerce], it tests the models ability to handle static content at scale. 24 Figure 11: Case 4: Pomodoro Timer Logic within Mobile Viewport. This task requires reconstructing Pomodoro timer rendered at mobile resolution. Classified as [L3, V1, Productivity & Tools], the core challenge is not the simple layout but the implementation of time-based state transitions (start, pause, reset)."
        }
    ],
    "affiliations": [
        "Bai et al.",
        "Caron et al.",
        "Comanici et al.",
        "Deka et al.",
        "Gui et al.",
        "Gupta & Kembhavi",
        "IWR-Bench Team",
        "Jiang et al.",
        "Jimenez et al.",
        "Laurencon et al.",
        "Lee et al.",
        "Li et al.",
        "Luo et al.",
        "OpenAI",
        "Radford et al.",
        "Si et al.",
        "Song et al.",
        "Xiao et al.",
        "Yun et al.",
        "Zhang et al.",
        "Zhuge et al."
    ]
}