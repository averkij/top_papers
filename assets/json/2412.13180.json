{
    "paper_title": "Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration",
    "authors": [
        "Mark Endo",
        "Xiaohan Wang",
        "Serena Yeung-Levy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent works on accelerating Vision-Language Models show that strong performance can be maintained across a variety of vision-language tasks despite highly compressing visual information. In this work, we examine the popular acceleration approach of early pruning of visual tokens inside the language model and find that its strong performance across many tasks is not due to an exceptional ability to compress visual information, but rather the benchmarks' limited ability to assess fine-grained visual capabilities. Namely, we demonstrate a core issue with the acceleration approach where most tokens towards the top of the image are pruned away. Yet, this issue is only reflected in performance for a small subset of tasks such as localization. For the other evaluated tasks, strong performance is maintained with the flawed pruning strategy. Noting the limited visual capabilities of the studied acceleration technique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria), a straightforward approach that (1) resolves the identified issue with early-layer pruning, (2) incorporates uniform sampling to ensure coverage across all image regions, and (3) applies pruning in two stages to allow the criteria to become more effective at a later layer while still achieving significant speedup through early-layer pruning. With comparable computational savings, we find that FEATHER has more than $5\\times$ performance improvement on the vision-centric localization benchmarks compared to the original acceleration approach."
        },
        {
            "title": "Start",
            "content": "Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration Mark Endo, Xiaohan Wang, Serena Yeung-Levy Stanford University {markendo, xhanwang, syyeung}@stanford.edu 4 2 0 2 7 1 ] . [ 1 0 8 1 3 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent works on accelerating Vision-Language Models show that strong performance can be maintained across variety of vision-language tasks despite highly compressing visual information. In this work, we examine the popular acceleration approach of early pruning of visual tokens inside the language model and find that its strong performance across many tasks is not due to an exceptional ability to compress visual information, but rather the benchmarks limited ability to assess fine-grained visual capabilities. Namely, we demonstrate core issue with the acceleration approach where most tokens towards the top of the image are pruned away. Yet, this issue is only reflected in performance for small subset of tasks such as localization. For the other evaluated tasks, strong performance is maintained with the flawed pruning strategy. Noting the limited visual capabilities of the studied acceleration technique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria), straightforward approach that (1) resolves the identified issue with early-layer pruning, (2) incorporates uniform sampling to ensure coverage across all image regions, and (3) applies pruning in two stages to allow the criteria to become more effective at later layer while still achieving significant speedup through early-layer pruning. With comparable computational savings, we find that FEATHER has more than 5 performance improvement on the vision-centric localization benchmarks compared to the original acceleration approach. 1. Introduction The exploration of Vision-Language Models (VLMs) is critical area of computer vision and natural language processing research, centered on combining large language models (LLMs) with visual encoders to enable multimodal perception, reasoning, and understanding capabilities. While earlier works explored sophisticated schemes for conditioning language models with visual information (a) Although FastV prunes most visual tokens from Figure 1. the upper portion of the image, the approach still displays strong performance on variety of evaluated vision-language tasks except for the vision-centric task of localization. (b) Based on our findings, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria), straightforward approach that resolves the existing issue of selecting bottom tokens, additionally maintains uniformly sampled tokens to ensure good coverage over the whole image, and prunes in two stages. [2, 17, 18], more recently the space has shifted to predominately using the simplistic approach of taking patch features from pre-trained visual encoders and projecting them to the input space of the language model with light-weight adapter [4, 22, 24, 32]. Using image patches as tokens, however, comes with the drawback of being computationally inefficient. To achieve fine-grained resolution, the image is divided into many patches. This large number of patches significantly increases computational demands due to the quadratic complexity of the attention operation in Transformers. As result, many recent works have focused on accelerating these methods by compressing visual information, demonstrating that heavy compression can still maintain strong performance across wide variety of tasks [3, 6, 7, 25, 33]. For instance, FastV [7] prunes 50% of visual tokens after the shallow layers of the Language Model (LLM) while not compromising in performance across range of image and video understanding tasks [7]. Another work reveals that for fixed computational budget on visual reasoning tasks, optimal performance is achieved by using the largest LLM possible while sacrificing visual information, often reducing the visual token count to single token [19]. With such compressed visual information, it is still unclear how these methods achieve high performance on tasks assessing vision capabilities such as visual reasoning and understanding, and whether there are more demanding visual tasks where these methods fail. The motivation of our study is to get better understanding of the vision capabilities of accelerated VLMs given that they leverage highly compressed visual information, focusing specifically on FastV approach. When evaluating across wide range of vision-language tasks, we find that while the approach maintains strong performance across many tasks, compression causes substantial decrease in performance for TextVQA and severe performance for localization tasks. While this result is not particularly surprising as visual grounding is expected to suffer when compressing visual information, when we analyze the approachs poor performance we uncover fundamental issue with the approach that the pruning criteria is ineffective in early layers, heavily discarding tokens towards the top of the image. As this issue is not specific to any particular task, we then study how this cost is hidden on the majority of tasks and find that most evaluated benchmarks require minimal visual grounding, even for many that show substantial gap between vision enabled and disabled setups. This finding underscores significant challenge in the field of multimodal learning, not only for measuring the effectiveness of VLM acceleration methods but also for benchmarking the visual capabilities of VLMs as whole. Given the discovered limitation of the studied VLM acceleration approach, we next experiment with various alternative criteria and design choices to better compress visual information, leading to our final method, FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria). Specifically, we prune after an early layer with modified version of the criteria that resolves the uneven selection issue and also maintain small amount of uniformly sampled tokens to ensure adequate coverage over all image regions. Furthermore, we apply more extensive pruning at later layer, where the effectiveness of the attention-based criteria is enhanced. This strategy is analogous to how racecar driver feathers the throttle by gradually pressing the accelerator at the beginning of turn to maintain grip and then accelerating more aggressively once the car is past the apex. We show that FEATHER results in substantial performance gains compared to the original acceleration approach, improving localization performance more than 5 with comparable computational savings. Strikingly, we find that our approach achieves this performance improvement while only retaining 3.3% of visual tokens for the second half of LLM layers. Overall, our work demonstrates that while visual compression can maintain strong performance even on challenging vision-centric tasks, its effectiveness depends on well-designed strategy, which is currently difficult to assess due to many vision-language benchmarks not thoroughly evaluating vision capabilities. 2. Related Work Recent efforts to accelerate VLMs can be broadly divided into two main categories: compressing visual information before it enters the LLM, and compressing visual information within the LLM itself. In the first category, ChatUniVi [13] dynamically merges visual tokens with similar semantic meanings. Alternatively, PruMerge [25] selects important tokens according to the similarities between the class token from the vision encoder and the individual patch tokens and then merges important tokens with the remaining unselected tokens by weighted average. For the LLaVA-NeXT [23] approach of partitioning an image into sub-images where inefficiency is an even bigger problem, HiRED [3] selects tokens with top feature important on each sub-image with an allocated budget. Other methods argue that the input image alone does not include enough information to select important patches and thus use the textual input to recover visually meaningful tokens [8, 34]. For the second category of approaches, where visual information is compressed within the LLM itself, LOOKM reduces the multimodal KV cache size [30]. In our study, we focus on the popular FastV approach [7]. This work identifies that attention over image tokens is sparse in deeper layers of the LLM. Based on this observation, they propose to prune away unimportant vision tokens after the shallow layers of the LLM, achieving 45% reduction in FLOPS with nearly no performance loss. Works since have proposed alterations to this setup such as adaptively determining the number of pruned tokens instead of using fixed ratio [10] or pruning in multiple stages [33]. 2 3. The Drawback of VLM Acceleration In this section, we aim to get better understanding of the vision capabilities of the VLM acceleration approach of pruning visual tokens after shallow LLM layers. After outlining preliminaries (3.1), we take closer look at how the approach performs across broad range of tasks. Upon inspection, we discover that while heavily pruning visual tokens after the shallow LLM layers has little effect on performance across variety of tasks, this approach fails decisively on more vision-centric tasks, particularly localization (3.2). Next, we examine why this method struggles with localization, uncovering that the poor performance is due to the ineffectiveness of the pruning criteria to select important tokens when applied at an early layer (3.3). As this defect is not specific to localization, we explore what can be attributed to the methods high performance on numerous other tasks and find that these tasks require minimal visual grounding (3.4). 3.1. Preliminaries Before our analyses, we provide background on the explored adapter-style VLM with visual token pruning, the evaluated benchmarks, and experimental settings. VLM and token pruning. Formally, an adapter-style VLM takes as input an image ximg and text prompt tokens xprompt. pre-trained vision backbone first encodes visual features zimg = (ximg) Rndvision where is the number of image patches and dvision is the dimensionality of the vision encoder. Next, an adapter (either simple linear layer or MLP) projects the vision features to embeddings himg = p(zimg) Rndtext where dtext is the dimensionality of the LLM. Lastly, himg is concatenated with text prompt embeddings hprompt = embed(xprompt) and passed into the language model LM to generate the output text = LM([himg; hprompt]). In this work, we study the inference acceleration of VLMs where visual tokens are pruned within the attention mechanism of LM. We focus on the FastV [7] approach, where after layer in LM, R% of visual tokens are pruned away based on ranking function gϕ which ranks tokens based on criteria ϕ. In practice, the attention score received from the last text token is used as the criteria, referred to as ϕoriginal. Note that the positional information is preserved when performing the pruning. With this approach, we measure the acceleration using the theoretical FLOPS reduction ratio related to the image tokens. For one Transformer layer, the total FLOPS is estimated as = 4nd2 + 2n2d + 2ndm where = dtext and is the intermediate size of FNN. Given that the Transfoerm has layers in total and after layer K, we maintain ˆn = (1 R%) visual tokens, we calculate the FLOPS reduction as 1 + (T K) (4ˆnd2 + 2ˆn2d + 2ˆndm) . (1) 3 Benchmarking. We evaluate the accelerated VLMs on large suite of benchmarks from [14] which includes evaluations spanning the areas of localization, open-ended visual question answering, and challenge sets. For all benchmarks, we follow the same evaluation protocol as [14]. Localization. We evaluate localization performance using the RefCOCO, RefCOCO+, RefCOCOg [15, 35] and OCID-Ref [31] datasets. RefCOCO contains short, spatially grounded descriptions, while RefCOCO+ focuses on appearance-based descriptions. RefCOCOg, on the other hand, features longer and more detailed descriptions. OCID-Ref is robotics dataset that tests generalization to out-of-distribution scenarios, particularly for object localization in cluttered environments. Open-Ended Visual Question Answering. We evaluate general visual reasoning using the VizWiz [5] and VQAv2 [9] datasets, spatial reasoning using the GQA [12] dataset, and reasoning around text using the TextVQA [26] dataset. Challenge Sets. We additionally evaluate on the VSR [21], TallyQA [1], POPE [20], and AI2D [16] closed-set prediction datasets. VSR includes binary spatial relationship questions, TallyQA consists of counting questions, POPE probes hallucination, and AI2D contains multiplechoice questions referring to scientific diagrams and charts. Experimental settings. In our experiments, we utilize VLM with SigLIP ViT-SO400M [36] as , one-layer MLP with GELU activation as p, and Llama 2 7B [29] as LM. The model was trained on the multimodal instruction tuning dataset presented in [22] in single-stage with frozen. 3.2. Early visual token pruning falters in visioncentric tasks We examine the effect of pruning visual tokens after the shallow LLM layers (K = 3) across variety of tasks. We compare various pruning ratios (R {0.25, 0.5, 0.75, 0.9}) and the baseline non-pruned model in Figure 2. We find that for the majority of evaluated tasks, heavily reducing the number of visual tokens after the shallow LLM layers results in minimal performance dropoff. Specifically, comparing the baseline model to the setup dropping 75% of tokens, we observe the following decreases in performance: 3.5% for AI2D, 0.1% for VSR, 3.7% for TallyQA, 4.8% for POPE, 5.1% for VizWiz, 7.9% for VQAv2 and 7.7% for GQA. By contrast, TextVQA shows much more substantial drop, with 42.0% decrease. Notably, the task with the highest performance dropoff is decidedly localization, with RefCOCO, RefCOCO+, RefCOCOg, and OCID-Ref exhibiting performance decreases of 88.9%, 88.9%, 91.0%, and 86.0%, respectively. Investigating further, Figure 2 shows that localization performance declines roughly linearly to zero as the pruning ratio progresses from 0 to 1, pattern that sharply contrasts with most other tasks, where performance remains largely unaffected. Figure 2. Contrasting the difference in performance dropoff on the challenging vision-centric localization task (Left) versus the other evaluated tasks (Middle) when pruning visual tokens after the shallow LLM layers. Whereas performance decrease is minimal for most tasks, localization exhibits roughly linear decrease to zero as the ratio of pruned tokens increases. Right: Per-task performance breakdown across various setups of pruning ratios. Using = 3 for all pruning setups. It is important to note that localization is undoubtedly challenging task for model that discards visual tokens. The task inherently requires visual grounding, as the model must not only identify the object of interest but also retain sufficient tokens corresponding to the object to accurately define its boundaries. Nonetheless, it is still surprising that (1) localization performance declines to the extent it does even when many tokens are still maintained (e.g., decreasing average performance by 45% when dropping 50% of tokens), and (2) most non-localization benchmarks do not exhibit similar pattern whatsoever (except for TextVQA), despite these benchmarks aiming to evaluate visual grounding abilities, such as counting for TallyQA, spatial reasoning for GQA, and chart understanding for AI2D. In the following sections, we further explore these unexpected findings. Finding 1: Pruning visual tokens after shallow LLM layers results in jarring performance decline for localization benchmarks and moderate decrease for TextVQA, whereas performance remains relatively unchanged for other evaluated tasks. 3.3. Interpreting poor vision-centric task performance We next investigate why pruning visual tokens after the shallow LLM layers has such an adverse effect on localization performance. Notably, performance begins to decrease even with minimal token pruning, suggesting the pruning criteria, intended to retain important tokens, seems to not effectively capture the necessary visual information. To assess the functionality of the pruning criteria, we first examine the distribution of retained tokens across all benchmark examples (both localization and non-localization). As shown in Figure 3(b), we find that pruning visual tokens after the shallow LLM layers (K = 3) retains tokens concentrated at the bottom of the image. In addition to evaluating the effectiveness of the criteria applied after the shallow LLM layers, we also examine the criteria behavior when applied after later layers (K (8, 16, 24)). Notably, we observe that as tokens are pruned later in the model, the criteria shift away from the bias of maintaining tokens based on visual token position. In addition, qualitatively we observe that as the pruning layer increases, not only is the positional bias reduced, but the criteria also begins to correctly select tokens relevant to the text instruction. For example, as shown in Figure 3(a), pruning after the later layers results in distinct concentration of selected tokens around the area corresponding to the reference expression for localization. To quantify this change in criteria effectiveness across layers, we measure the performance when varying the pruning layer (K {3, 8, 16, 24}). As shown in Figure 3(c), performance remains stable when pruning is performed at deeper layers (K = 16 and = 24), whereas pruning at earlier layers (K = 3 and = 8) leads to performance degradation, particularly for localization. This result demonstrates that the current criteria based on attention received from the last instruction token can discern important visual tokens; however, it currently shows limitations that need to be addressed. Finding 2: The pruning criteria when applied after shallow layers predominantly selects visual tokens from the bottom part of the image. 3.4. Explaining VLM inference acceleration performance on other tasks Given that early pruning of visual tokens leads to suboptimal tokens selection where remaining tokens are concentrated towards the bottom of the image, natural question arises: how does the approach still maintain high per4 Figure 3. (a) Example demonstrating that when pruning in early layers, selected tokens are concentrated in the bottom of the image, whereas in later layers the selected tokens more precisely cover the region of the image important for answering the question. (b) Heatmap illustrating that averaged across all benchmark examples, as the pruning layer increases, the selection of bottom visual tokens by the criteria is reduced. (c) Visualizing the effect of the pruning layer on performance for both localization and non-localization tasks. We find that pruning after layer 16 or later results in little performance dropoff, whereas pruning earlier results in performance decrease, particularly for localization. Using = 0.75 for all setups. formance across diverse range of tasks, including those aiming to evaluate visual grounding abilities? In this section, we test several hypotheses explaining this puzzling observation, ultimately concluding that strong performance is caused by the limitation of the tasks to assess vision capabilities. Consider the example presented at the top of Figure 1, where the question is, what animal is in front of the trees? In this case, even when all visual tokens corresponding to trees are pruned, the model still predicts the correct answer. One possible explanation is that, because pruning occurs within the LLM, the visual information from the pruned tokens is transferred to the retained tokens prior to pruning. This would also account for poor localization performance, as positional information, unlike visual content, cannot be transferred through the attention mechanism (using RoPE). Alternatively, the model may predict the correct answer because the answer can be inferred without direct visual grounding (in this example, despite all tree tokens being pruned, the model retains tokens for cow). To assess the validity of these hypotheses across tasks, we compare the early visual token pruning approach shown to have non-optimal pruning strategy with variant that prunes the same non-optimal tokens before they enter the LLM, ensuring no visual information from pruned tokens is retained. As an additional comparison, we also evaluate text-only model in which all visual tokens are removed before entering the LLM. As shown in Figure 4, we find that across all evaluated tasks, allowing shallow-layer information transfer prior to pruning offers little to no imFigure 4. Assessing whether the strong performance of early visual token pruning for many tasks can be attributed to visual information transfer before pruning or benchmarks lack of assessing fine-grained visual capabilities. We observe that for all evaluated tasks, allowing information transfer before pruning (shown in green) does not result in substantial performance improvement over the setup without visual information transfer (shown in light green), highlighting limitation of many benchmarks. provement over the setup where tokens are pruned before entering the LLM. This finding suggests that the high performance of early visual token pruning on many tasks does not stem from the effectiveness of visual information transfer in early layers but rather reflects that many benchmarks do not demand detailed understanding of visual informa-"
        },
        {
            "title": "K Criteria",
            "content": "FLOPS Red OCID-Ref RefCOCOg RefCOCO+ RefCOCO Avg Other Task Avg 3 8 ϕoriginal ϕ-R ϕKNN ϕuniform 68% 68% 66% 66% 0.057 0.229 0.151 0.206 Attention-based 0.051 0. 0.061 0.133 Non-attention-based 0.249 0.286 0.260 0."
        },
        {
            "title": "Ensemble",
            "content": "0.067 0.153 0.296 0.333 0.059 0.167 0.239 0.280 0.594 0.618 0.606 0. ϕ-R + ϕuniform 61% 0.291 0.272 0.247 0. 0.272 0.633 ϕoriginal ϕ-R ϕKNN ϕuniform 56% 56% 55% 55% 0.194 0.271 0.154 0.246 Attention-based 0.235 0.267 0.240 0.264 Non-attention-based 0.244 0. 0.252 0."
        },
        {
            "title": "Ensemble",
            "content": "0.263 0.292 0.294 0.348 0.233 0.273 0.236 0.303 0.622 0.635 0.607 0. ϕ-R + ϕuniform 50% 0.320 0.359 0.354 0. 0.356 0.644 Table 1. Evaluating alternative criteria for token pruning after the early LLM layers. For each task and pruning layer, we bold the best result and underline the second-best result. Our main findings include: (1) our proposed RoPE-free criteria ϕ-R substantially improves pruning performance compared to the original criteria ϕoriginal; (2) pruning later (K = 8) yields higher performance than pruning earlier (K = 3); and (3) integrating uniform sampling into the attention-based criteria with ϕ-R + ϕuniform enhances effectiveness. Using = 0.75 for ϕoriginal, ϕ-R, and ϕ-R + ϕuniform. See 4.1 for criteria definitions. tion to answer questions accurately. Note that for the majority of these benchmarks (except VizWiz and AI2D), there is substantial performance dropoff when not including any visual information. This finding indicates that solely comparing vision enabled and disabled setups does not provide an adequate assessment of how well benchmark assesses visual grounding. Finding 3: The majority of evaluated benchmarks do not require fine-grained visual grounding, as they can often be answered using only visual tokens located towards the bottom of the image. 4. Improving Visual Token Pruning Based on our findings from 3, we now seek to enhance the studied VLM acceleration approach to better preserve fine-grained visual capabilities while still achieving computational efficiency. To this end, we propose alternative pruning criteria and demonstrate that our modifications enable effective pruning even when applied after early LLM layers (4.1). Guided by our insights, we present our final approach, FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria) (4.2), and showcase its impressive visual capabilities while offering high computational efficiency (4.3). 4.1. Evaluating pruning criteria Given that the criteria for token pruning is based on the attention score received from the last text token and attention is dependent on token relative token position distances, the bias towards selecting bottom tokens is unsurprising. Namely, modeling position with RoPE in the LM results in long-term decay property [27], causing larger attention scores for visual tokens closer to the last text token (refer to the original paper for formal explanation). As previous work demonstrates, there is greater emphasis on shorterdistance information in more shallow layers[11], explaining why the positional bias is most prevalent when pruning after early layers compared to after later layers. From this observation, we propose the following simple adjustment to the pruning criteria in order to mitigate this bias and more effectively select tokens relevant to the text instruction. Specifically, we propose the criteria ϕ-R, in which we still compute the attention score received from the last text token, except we do not apply RoPE to the attention mechanism, thereby removing the long-term decay effect. Note that we only remove RoPE for the criteria calculation, as removing positional information from the VLM itself would surely have an adverse effect. In addition to this altered criteria, we also explore the following criteria that are not based on attention. ϕuniform: In this simple criteria, we uniformly sample visual tokens in the image, using set stride. This criteria ensures that there is good image coverage, but does so by sacrificing the ability to have more densely captured visual information for particular image region. To use similar computational saving to the attention-based criteria when = 0.75, we use stride of two, resulting in 196 selected tokens. ϕKNN: Inspired by dynamic visual tokenization [13], in this criteria, we select tokens based on their local density. Specifically, given the visual tokens zimg from the vision backbone, we compute local density ρi for each token zimg[i] where {0, 1, ..., 1} using K-nearest neighbors. We then calculate the distance index δi for each token zimg[i] as: δi = min j:ρj >ρi max zimg[i] zimg[j]2, if s.t. ρj > ρi. zimg[i] zimg[j]2, otherwise. (2) This distance index represents how far away zimg[i] is from other high-density tokens. With this, we use ρi δi as the token importance score for the criteria. To match the computational savings of ϕuniform, we select 196 tokens. In Table 1, we report results evaluating the effectiveness of these criteria, particularly focusing on the challenging vision-centric localization task. Comparing ϕ-R with ϕoriginal, we find that by removing RoPE, at = 3 there is 183% average improvement on localization tasks and at = 8, there is 17% average improvement. These performance improvements demonstrate that once the impact of token position on attention is removed, attention score can more effectively be used for the criteria when applied after early LLM layers. Note that the narrowing gap in performance between ϕ-R and ϕoriginal from = 3 to = 8 is likely because ϕoriginal has less of bias towards selecting bottom image tokens as the pruning layer increases. Insight 1: Once removing the criteria tendency of selecting bottom image tokens, pruning after early LLM layers becomes substantially more effective. OCID-Ref RefCOCOg RefCOCO+ RefCOCO 3 0.238 0.267 0.162 0.298 0.145 0.268 0.163 0.298 Table 2. Assessing localization performance when using the selected tokens from ϕ-R for the entirety of the LLM. We find that ϕ-R applied at later LLM layer results in better selection of tokens. Comparing ϕ-R when = 8 versus = 3, we see that the average localization performance improves by 63%. However, it remains unclear whether this performance increase is affected by factors other than the ability of the criteria to select important tokens. Therefore, to directly com7 pare the criteria across different layers, we take the tokens selected by both criteria and evaluate how well the model can perform with only these tokens passed into the LLM (as is done in 3.4). As shown in Table 2, we find the selected tokens from ϕ-R applied after layer 8 are superior compared the tokens from ϕ-R applied after layer 3. Insight 2: Even with the enhanced criteria for pruning after shallow LLM layers, pruning later still improves the criteria and downstream performance. When comparing the attention-based criteria ϕ-R with the two non-attention based criteria ϕKNN and ϕuniform, we find that ϕ-R results in performance on the OCID-Ref dataset, while ϕKNN and ϕuniform outperform on the RefCOCO tasks. One possible explanation for the varying effectiveness of different criteria is that, since OCID-Ref evaluates localization in cluttered environments, incorporating information from the text instruction into the criteria is crucial. In contrast, RefCOCO tasks contain less cluttered scenes, where broad coverage over the whole image may be most beneficial. Based on this observation that the attention-based criteria ϕ-R and non-attention-based criteria ϕuniform have different strengths and weaknesses, we propose the ensemble criteria ϕ-R + ϕuniform. In this criteria, we utilize ϕ-R with the same ratio of selected tokens and additionally add small amount uniformly sampled tokens (using stride of three instead of two). We find that when = 3, this approach improves upon ϕ-R by 63% but has slight decrease compared to ϕuniform by 2.9%. However, when = 8, this approach far outperforms both of the individual criteria, improving ϕ-R by 30% and ϕuniform by 17%. Note that this setup results in slightly less computation efficiency than ϕ-R (7% drop in FLOPS reduction for = 3 and 6% for = 16), but this is outweighed by the performance improvements. Insight 3: Integrating uniform sampling into the attention-based pruning criteria enhances its effectiveness in early layers."
        },
        {
            "title": "We provide qualitative examples of various criteria token",
            "content": "selection in the supplement. 4.2. Distilling insights Guided by our insights, we now present our final approach FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria). In this approach, we first perform pruning after an early layer (K = 8), utilizing our proposed criteria ϕ-R + ϕuniform to retain (1 R)% of tokens. Given our finding that the criteria improve as the LLM layer increases, we additionally prune second time at = 16. Figure 5. Comparing FEATHER performance against FastV [7] and PyramidDrop [33]. We find that FEATHER far outperforms both compared methods, particularly for the vision-centric task of localization. At this stage, we utilize ϕ-R as the criteria, as the uniform sampling becomes less effective compared to the attentionbased strategy as the pruning layer increases. For the reduction ratio, we choose to only retain (1 R)2% of the remaining tokens since the attention-based criteria has proved highly effective when pruning at later layers, even when using the suboptimal criteria ϕoriginal (see Figure 3(c)). 4.3. Comparison against FastV and PyramidDrop We compare our approach against the one-stage early pruning approach of FastV (K = 3) [7] and the multi-stage approach of PyramidDrop (K = [8, 16, 24]) [33]. Note that both of these methods use ϕoriginal for the criteria. As shown in Figure 5, we find that FEATHER far outperforms the baselines, especially on the localization tasks. Specifically, for comparable computational costs (64% FLOPS reduction for our approach, 68% FLOPS reduction for FastV, and 65% FLOPS reduction for PyramidDrop), we observe that for localization tasks, FEATHER exhibits more than 5 average performance improvement compared to FastV and 36% average performance improvement compared to PyramidDrop. For non-localization tasks, FEATHER has 7.8% improvement over FastV and 1.5% improvement over PyramidDrop. Note that PyramidDrop performs substantially better than FastV as it prunes fewer tokens in an early layer. However, it still suffers from an ineffective pruning strategy at this stage, though the impact is less pronounced since it predominantly prunes later. Remarkably, with our 64% FLOPS reduction setup, after layer 16 only 3.3% of tokens are retained, yet the average localization performance decrease compared to the baseline method with no token pruning is only 26% (Figure 1 includes an example of retained tokens after layer 16). This finding illustrates that even for vision-centric tasks, maintaining strong performance while gaining huge acceleration speedups with extensive pruning is possible, but it heavily relies on the effectiveness of the pruning criteria. 5. Discussion In this work, we examine the visual capabilities of the VLM acceleration approach of pruning visual tokens after shallow LLM layers. While strong performance is maintained on most evaluated tasks, it fails on the vision-centric task of localization due to its flawed pruning criteria that predominately selects visual tokens from the bottom part of the image. Observing this same behavior on other tasks, we show that strong performance is largely due to the benchmarks inability to assess fine-grained visual capabilities. Next, we propose and evaluate several alternative criteria to improve visual capabilities, ultimately arriving at our final method, FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria). This approach first prunes after an early layer using modified version of the criteria that addresses the issue of bottom token selection while also incorporating uniform sampling to ensure coverage across all image regions. Subsequently, it performs more extensive pruning at later layer, where the pruning criteria become more effective. We find that FEATHER has more than 5 performance improvement on localization compared to the original acceleration approach. While we study particular VLM architecture and acceleration approach, our findings highlight broader issue with the evaluation of VLMs. Building on [28] which finds that text-only models can perform well on some vision-language benchmarks, we demonstrate that even benchmarks with substantial difference in vision enabled and disabled setups may not assess fine-grained visual capabilities. In our work, we address this benchmarking issue by utilizing the visioncentric localization task. However, this task only assesses one particular type of skill. To accurately assess wide range of visual capabilities, future work may explore how to resolve current dataset biases that models can exploit. Future work can also investigate the capabilities of other VLM acceleration approaches to determine whether they have pitfalls undetected by the current evaluation benchmarks. Acknowledgments. This work is supported in part by the National Science Foundation (NSF) under Grant No. 2026498, the Stanford AIMI-HAI Partnership Grant, and the NSF Graduate Research Fellowship Program under Grant No. DGE-2146755 (for M.E.). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any other entity."
        },
        {
            "title": "References",
            "content": "[1] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In Proceedings of the AAAI conference on artificial intelligence, pages 80768084, 2019. 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1 [3] Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios Nikolopoulos, Hans Vandierendonck, Deepu John, and Bo Ji. Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models in resourceconstrained environments. arXiv preprint arXiv:2408.10945, 2024. 2 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [5] Jeffrey Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333342, 2010. 3 [6] Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae arXiv preprint Lee. Matryoshka multimodal models. arXiv:2405.17430, 2024. [7] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. ECCV, 2024. 2, 3, 8 [8] Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, YangYang Liu, and Cheng-Lin Liu. Recoverable compression: multimodal vision token recovery mechanism guided by text information. arXiv preprint arXiv:2409.01179, 2024. 2 [9] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 3 [10] Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipvl: Efficient large vision-language models with dynamic token sparsification and kv cache compression. arXiv:2410.08584, 2024. 2 arXiv preprint [11] Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. On the token distance modeling ability of higher rope attention dimension. arXiv preprint arXiv:2410.08703, 2024. 6 [12] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 3 [13] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. 2, 7 [14] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. 2024. 3 [15] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 3 [16] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. [17] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 1 [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [19] Kevin Li, Sachin Goyal, Joao Semedo, and Zico Kolter. Inference optimal vlms need only one visual token but larger models. arXiv preprint arXiv:2411.03312, 2024. 2 [20] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 3 [21] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. [22] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2, 3 9 In Computer VisionECCV 2016: 14th European sions. Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. 3 [36] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 3 [23] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2 [25] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. [26] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 3 [27] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 6 [28] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 8 [29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [30] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan. Look-m: Lookonce optimization in kv cache for efficient multimodal longcontext inference. arXiv preprint arXiv:2406.18139, 2024. 2 [31] Ke-Jyun Wang, Yun-Hsuan Liu, Hung-Ting Su, Jen-Wei Wang, Yu-Siang Wang, Winston Hsu, and Wen-Chin Chen. OCID-ref: 3D robotic dataset with embodied language for clutter scene grounding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 53335338, Online, 2021. Association for Computational Linguistics. [32] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [33] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. 2, 8 [34] Gaotong Yu, Yi Chen, and Jian Xu. Balancing performance and efficiency: multimodal large language model pruning method based image text interaction. arXiv preprint arXiv:2409.01162, 2024. 2 [35] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expres10 Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Token Pruning Visualizations 6.1. Comparing pruning criteria In this supplemental material section, we provide qualitative analysis comparing the pruning effectiveness of various criteria as well as the final approaches of FEATHER, FastV, and PyramidDrop. Namely, we visualize the ability of approaches to retain important tokens, particularly for localization. In Figure 6 and Figure 7, we visualize pruning from the various criteria assessed in the main text when pruning is done after layers three and eight, respectively. In Figure 8, we visualize pruning from the final approaches of FEATHER, FastV, and PyramidDrop. We first visualize the retained tokens of various criteria when pruning is applied after layer three (see Figure 6) and layer eight (see Figure 7). We see that these visualizations support our quantitative results from the main paper. Specifically, (1) ϕ-R removes the criteria tendency of selecting bottom image tokens, resulting in an improved selection of maintained tokens; (2) the attention-based criteria improve when pruning after later layer; and (3) adding uniform sampling to the attention-based pruning criteria with ϕ-R + ϕuniform improves token selection. Figure 6. Visualizing the ability of various pruning criteria to maintain visual tokens relevant to the reference expression when applied after layer three. We observe that ϕ-R resolves ϕoriginals tendency of selecting bottom image tokens and that uniform sampling is robust approach that improves the token selection effectiveness of ϕ-R with ϕ-R + ϕuniform. See the main text for criteria definitions. Figure 7. Visualizing the ability of various pruning criteria to maintain visual tokens relevant to the reference expression when applied after layer eight. We observe that the attention-based criteria are more effective when pruning after this layer compared to after layer three. See the main text for criteria definitions. 2 6.2. Comparing FEATHER to FastV and Pyramid-"
        },
        {
            "title": "Drop",
            "content": "Additionally, we visualize the retained tokens for the FEATHER, FastV, and PyramidDrop approaches. As shown in Figure 8, when comparing the remaining tokens used for prediction (after layer 16 for FEATHER, layer 24 for PyramidDrop, and layer three for FastV), we see that our approach retains substantially more tokens around and inside the reference expression bounding box. Figure 8. Visualizing the ability of FEATHER, FastV, and PyramidDrop to retain visual tokens relevant to the reference expression. We observe that our approach retains substantially higher portion of tokens relevant to the reference expression."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}