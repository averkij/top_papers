{
    "paper_title": "f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment",
    "authors": [
        "Rajdeep Haldar",
        "Lantao Mei",
        "Guang Lin",
        "Yue Xing",
        "Qifan Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods."
        },
        {
            "title": "Start",
            "content": "f -GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment 6 2 0 2 9 ] . [ 2 6 4 9 5 0 . 2 0 6 2 : r Rajdeep Haldar 1 Lantao Mei Guang Lin 1 Yue Xing 2 Qifan Song"
        },
        {
            "title": "Abstract",
            "content": "Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose -Group Relative Policy Optimization (f -GRPO), class of on-policy reinforcement learning, and -Hybrid Alignment Loss (f -HAL), hybrid on/off policy objectives, for general LLM alignment based on variational representation of -divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods. 1. Introduction Alignment is the final stage in the training pipeline of large language models (LLMs), aimed at instilling capabilities beyond language comprehension and instruction-following that are learned during pretraining and instruction finetuning (Ouyang et al., 2022). These capabilities range from complex reasoning, such as coding or math problem-solving (Shao et al., 2024), to the adherence to safety and ethical constraints based on human preferences (Bai et al., 2022b). In practice, alignment is achieved via reinforcement learning (RL) using reward signal that encodes the target behavior. While various RL-based alignment strategies have been developed, they differ mainly in the source of the reward signal. In general, they can be categorized into two regimes: 1Department of Statistics, Purdue University 2Department of Statistics, Michigan State University. Correspondence to: Rajdeep Haldar <rhaldar@purdue.edu>. Preprint. February 10, 2026. 1 (i) In the verifiable reward regime (RLVR), one has access to an explicit reward function that quantitatively evaluates the correctness or quality of response, as in tasks such as mathematical reasoning (Guo et al., 2025), code generation (Le et al., 2022), tool use (Jiang et al., 2025), etc. Learning in this regime is naturally formulated using an on-policy reinforcement learning algorithm that samples from the current policy, evaluates responses based on the reward, and updates the model accordingly. Among on-policy algorithms, GRPO (Shao et al., 2024) stands out for its computational efficiency and superior performance. (ii) In the preference alignment regime (PA), reward signal is provided implicitly through relative comparisons or binary feedback between responses (e.g., safe vs. unsafe, preferred vs. rejected). Alignment can then be performed either by training reward model followed by on-policy learning, commonly referred to as reinforcement learning with human feedback (RLHF), or by using direct alignment methods that optimize the policy directly via supervised objective on preference data (Off-Policy). Further details for these two regimes are provided in 2.2 and 2.3. Divergence Perspective for Preference Alignment Haldar et al. (2025) proposed unifying framework for preference alignment that interprets popular PA methods as instances of divergence estimators between an aligned (D+, chosen) and unaligned (D, rejected) distributions, differentiated by the choice of divergences. In principle, one can construct new preference alignment methods given choice of divergence, resulting in class of direct, off-policy alignment objectives for any general -divergence; refer to 2.3.1 for details. natural question then arises: Can this divergence-based viewpoint for PA be extended beyond preference alignment to the verifiable reward regime (RLVR)? In other words, can divergence estimation serve as unifying foundation for both off-policy preference alignment and on-policy reinforcement learning with verifiable rewards? This work addresses this question by extending the divergence framework to construct general on-policy and hybrid RL objectives (Figure 1 summarizes our framework). In particular, our contribution is three-fold: Divergence-Based RL for General LLM Alignment vs. below-average reward Figure 1. Divergence Estimation Framework. RLVR (left): verifiable reward signal r(x, y) induces reward-aligned/unaligned distributions (above D+ under the old policy), and -GRPO performs on-policy alignment by estimating an -divergence between these distributions. Preference alignment (right): preference data samples chosen and rejected prompt-response pairs from aligned (D+) and unaligned (D) distributions, and direct aligners (e.g., FDO) optimize the variational -divergence objective between (D+, D). Hybrid (bottom): -HAL combines both information sources, interpolating between off-policy preference alignment and on-policy RLVR. Divergence-based RL objectives. We introduce -GRPO (Equation (15)), class of on-policy alignment objectives, and -HAL (Equation (16)), class of hybrid objectives that combine on-policy RL with preference-based supervision, both derived from general -divergences. Theoretical characterization and guarantees. We show that -GRPO and -HAL are divergence estimators: - GRPO estimates divergence between above-average and below-average reward distributions, while -HAL estimates divergence between mixtures of reward-based and preference-based aligned/unaligned distributions. We further establish Alignment Consistency and guarantee an improvement in the expected reward after alignment (Theorem 4.3). Furthermore, we justify the expected improvement of -GRPO over GRPO (4.1). Empirical validation across alignment regimes. We empirically validate our theoretical framework in 5. In particular, we examine the performance of -GRPO in both regimes and validate -HAL in the PA regime given the potential reward hacking in this regime. Experiment results demonstrate enhancement. 2. Preliminary 2.1. Notation Our notation follows (Haldar et al., 2025). Let denote the prompt and represent response. We use yw and yl to indicate aligned and unaligned responses, with flexible contexts depending on the regime (high reward, preferred) or (low reward, unpreferred). The triplet (x, yw, yl) characterized the joint distribution of all possible prompt, aligned and unaligned response combinations. Marginalizing this joint gives us (x, yw) D+ (aligned distribution), (x, yl) (unaligned distribution) and (prompt distribution). Moreover, for any distribution G, let Gx and pG(yx) denote its distribution and density/mass w.r.t. x. Let πθ be the model/policy under optimization, and πref be the reference policy before alignment, with θ as the policy model and θ as the parameter at convergence. Denote πθold as an older policy iteration strictly used for sampling, which does not go 2 Divergence-Based RL for General LLM Alignment under optimization. The function r(x, y) is the explicit environmental reward, while rθ(x, y) = β ln πθ(yx)/πref (yx) is the implicit policy reward with β as the KL regularization coefficient. Finally, σ(), σ() correspond to the sigmoid and SoftMax functions. Given the above notations, the core objective of alignment is to optimize the policy πθ for maximum reward r(x, y) while still being not too far from πref for KL regularization penalty, i.e., solve the following: (cid:20) (cid:21) r(x, y) βDKL(πθ πref ) , (1) sup θ E yπθ and (Rafailov et al., 2023) show that the optimal policy for Eq. (1) is directly proportional to the reward function it is trained on: πθ (yx) πref (yx) exp (cid:0)r(x, y) β1(cid:1). (2) 2.2. Verifiable Reward Regime (RLVR) This is the scenario when we use on-policy RL algorithms to optimize Eq. (1) using non-parametric reward function r(x, y) that can be verified by the environment without direct access to supervised samples from D+ (aligned) or (unaligned) distributions. We call this case Reinforcement Learning with Verifiable Rewards (RLVR). 2.2.1. GRPO (ON-POLICY RL ALIGORITHM) Popular On-policy algorithms like PPO and GRPO attain r(x, y) by reward maximization of the policy supθ solving an alternative objective iteratively: Ex yπθ (3) sup θ yπθold Aπθ (x, y). πθ(yx) πθold (yx) Here, Aπθ (x, y) is an advantage estimate measuring improvement of πθ over πθold , it implictly depends on the reward r(x, y). In our work, we focus only on GRPO in this RLVR regime as it is currently the most dominant and successful on-policy method. GRPO uses samples {yi}G i=1 πθold from the old policy to estimate the advantage by standardized scores of the sample rewards as Aπθ (x, yi) = r(x, yi) ˆµ(r(x, )) ˆσ(r(x, )) , (4) where ˆµ, ˆσ represent the sample mean and standard deviation. Then the ϵ-clipped GRPO objective to optimize Eq. 1 is as follows: sup θ [ (cid:88) yiπθold ,i= 1 min (cid:0)Ri θai, Clip(Ri θ, 1 ϵ, 1 + ϵ)ai (cid:1) βDKL(πθ πref )] where Ri ping is done to ensure stability at each policy iteration. θ = πθ(yix)/πθold (yix), ai = Aπθ (x, yi) and ϵ clip- (5) 2.3. Preference Alignment Regime In contrast to RLVR 2.2, in the PA regime, we have direct access to preference data (x, yw, yl) or binary feedback data (x, yw) D+, (x, yl) D, supervised samples from the true aligned and unaligned distribution. However, since access to the non-parametric scalar reward r(x, y) is unavailable, we use supervised samples to conduct reinforcement learning with human feedback (RLHF), implicitly enforcing r(x, yw) > r(x, yl). Traditional RLHF is broken down into two steps: (i) Reward Modelling The reward is surrogated by model rϕ(x, y) parametrized by ϕ. Trained on the preference data (x, yw, yl) to for maximum likelihood estimate for the Bradley-Terry (BT) model (Bradley & Terry, 1952): p(yw yl x) = exp rϕ(x, yw) exp rϕ(x, yw) + exp rϕ(x, yl) . (6) (ii) Reward Maximization We solve the RL alignment objective Eq. (1) using rϕ as the surrogate reward learned from preference, optimized by an on-policy algorithm like PPO, GRPO, etc. 2.3.1. DIRECT ALIGNERS (OFF-POLICY) ALGORITHMS Using Eq. (2), we can parametrize the reward in terms of the policy, rθ(x, y) = β ln πθ(yx)/πref (yx) and substitute in Eq. (6) to get direct alignment loss using only supervision on the preference data, and obtain the DPO loss (Rafailov et al., 2023): LDPO(θ) = Ex,yw,ylD log σ(cid:0)rθ(x, yw) rθ(x, yl)(cid:1). Other variants include BCO (Jung et al., 2025), KTO (Ethayarajh et al., 2024), etc., and, in general, these methods leverage the implicit reward rθ to maximize the likelihood over D+ and reduce the likelihood over D. Divergence Framework Haldar et al. (2025) unifies preference alignment objectives under single divergence-based framework by showing that popular alignment methods optimize variational representation of divergence D(D+ D) between an aligned distribution D+ and an unaligned distribution D. Different alignment algorithms arise from different choices of the underlying divergence. For example, RLHF/DPO, BCO, and KTO correspond respectively to estimating the non-parametric DPO divergence, the Jensen Shannon divergence, and the total variation distance. More generally, for an arbitrary -divergence (see Appendix, Def. A.1), they derive direct, off-policy alignment objective termed the -divergence Optimizer (FDO) using the variational representation of -divergences (Appendix 3 Divergence-Based RL for General LLM Alignment Eq. (22)) with functional = g(rθ): LFDO(f,g)(θ) = D+ [g(rθ)] + LFDO(f,g)(θ) = Df (D+ D). [f g(rθ)] , (7) (8) In the above, : ED(f ) is monotonically increasing link function mapping the implicit reward rθ to the effective domain of the conjugate of . At convergence, the optimal objective value recovers the negative -divergence between D+ and D. Besides convergence, their framework provides theoretical guarantees, such as alignment consistency, which ensures that optimization increases the policys probability mass on aligned samples while decreasing it on unaligned ones (Defn. 4.2, Haldar et al. (2025)). 3. Motivating the -GRPO loss In this section, we present the heuristics for proposing the -GRPO and -HAL objectives. Formal theoretical guarantees and precise characterizations are deferred to 4. 3.1. Divergence Estimation via Importance Sampling To extend the divergence framework to the RLVR regime, we need divergence-based on-policy RL term that learns directly from verifiable rewards r(x, y) when we have no samples from the aligned distribution D+ or the unaligned distribution D. To achieve this, we use FDO loss Eq. (7) as the starting point to derive an on-policy version of it. Let πθold be the action/response generator of an old iteration of the policy πθ. Then we can use importance sampling to rewrite the aligned ED+() and unaligned ED () expectations in terms of πθold as the surrogate distribution as follows: E yπθold (cid:0)w+ x,y g(rθ) + x,y g(rθ)(cid:1) , (9) where x,y = p(yx)/πθold (yx) are the importance weights and denotes the conditional aligned/unaligned density. Unfortunately, are inaccessible under the RLVR setting; thus, we artificially define them using the observed reward r(x, y) and then design the loss based on them. Assumption 3.1 (Rewarddensity correspondence). For fixed x, the verifiable reward induces aligned and unaligned densities as p(yx) exp(r(x, y)). These artificial distributions are practical in the sense that we expect our environmental rewards to reflect the likelihood of alignment. higher-reward is assigned more density in the aligned distribution, and vice versa. Note that this assumption is used solely to motivate our framework. Given Assumption 3.1, instead of directly using exp(r)/ππθold weights ance in exchange for some bias: x,y , we propose to use truncated importance x,y to ignore lower-weight terms, reducing variw x,y = p(yx) πθold (yx) (r(x, y)), (10) where (r(x, y)) = 1(cid:110) [r(x,y)Eπθold r(x,y)]>0 (cid:111). It enforces choosing only higher/lower than average reward terms for sampling from aligned/unaligned distributions. 3.2. Estimating the Importance Sampling Term The truncated weights x,y depend on densities p(x), whose values depend on the observed rewards. We will follow the GRPO sampling strategy 2.2.1 to get an empirical estimate of the truncated weights. For fixed prompt x, we sample responses {yi}G i=1 πθold , fetch the rewards {r(x, yi)}G i=1 and compute the group advantages {Aπθ (x, yi)}G i=1 (Eq. 4). For notational convenience we will use ri := r(x, yi), ai := Aπθ (x, yi) interchangeably. By Assumption 3.1, the aligned/unaligned densities satisfy p(yx) = er(x,y) yUniform(x) er(x,y) . To approximate the denominator, we apply sampled softmax (Log-Q) correction (Bengio & Senecal, 2008), yielding the plug-in (unnormalized) estimator ˆp(yix) = exp(ri) j=1 exp(cid:0) rj ln πθold (yjx)(cid:1) . (cid:80)G (11) The resulting truncated importance weight estimates are then given by: ˆw = ˆI ˆp(yix) πθold (yix) (cid:0){rj ln πθold (yjx)}G = σi (cid:1) ˆI (12) j=1 where ˆI = 1{ai>0} surrogate for (r(x, y)). To explain the operations, the importance weights ˆw are softmax-normalized scores that increase with the external reward while being regularized by the likelihood under the sampling policy πθold . Consequently, responses that are both highly rewarding (or highly unrewarding) and surprising under πθold receive larger weight under the aligned (or unaligned) density. The restriction to positiveor negativeadvantage samples implements truncated importance sampling, focusing the updates on the most informative, highweight terms and yielding stable training. 4 Divergence-Based RL for General LLM Alignment 3.3. -GRPO & -HAL losses Given an -divergence and monotonically increasing link function : ED(f ), we can estimate Eq. (9) using the weights Eq. (12) and simplifying to get the following: (cid:88) yiπθold ,i="
        },
        {
            "title": "1\nG",
            "content": "ψ(rθ,i, ai) sign(ai), (13) j=1 (cid:1) g(rθ,i), where rθ,i = β ln πθ(yix)/πref (yix) and ψ(rθ,i, ai) := (cid:40) (cid:0){rj ln πθold (yjx)}G σi (cid:0){rj ln πθold(yjx)}G σi ai > 0 (cid:1) g(rθ,i), ai 0 (14) Finally, we replace the sign term sign(ai) in Eq. (13) with the scaled advantage ai(1 + β1), thereby accelerating updates using advantage magnitude and normalizing gradients to the same order as standard on-policy RL methods (Appendix B.1). We define the -GRPO loss as follows: j=1 L(f,g) f-GRPO(θ) = (cid:88) yiπθold ,i= ai 1 + β1 ψ(rθ,i, ai). (15) We combine -GRPO and FDO to obtain hybrid on-off policy-loss function that leverages rewards and supervised samples. The divergence hybrid alignment loss (f-HAL) can be expressed as: f-HAL(θ) = λ L(f,g) L(f,g) FDO (θ) (cid:124) (cid:125) (cid:123)(cid:122) Direct Aligner +(1 λ) L(f,g) f-GRPO(θ) (cid:124) (cid:125) (cid:123)(cid:122) On-policy RL . (16) 4. Main Result Section 3 motivates -HAL and -GRPO based on some working assumptions. In this section, we demonstrate that these losses are indeed divergence estimators themselves and have deeper interpretation. To achieve this, we first introduce some definitions to be used in the theory. On-Policy algorithms express the loss over θ in terms of the previous iterate θold (Equations (5), (15) and (16)). This leads to Fixed Point (FP) Iteration formula where we define the tth iterate as θ(t) = arg inf θ L(θ) s.t. θold = θ(t1) (17) with θ(0) = θref and the optimal solution θ = lim θ(t). Definition 4.1 (Reward-Aligned Distributions). Given reward r(x, y) and policy πθ. The reward-aligned/unaligned distribution D+ (r,θ) are characterized by the following densities: (r,θ) / dD (r,θ) [(r(x, y) µθ r(x))]+ er(x,y)dP(x) where µθ r(x) = Eyπθ r(x, y) and [u]+ = max(0, u). (18) 5 The above definition defines the notions of aligned and unaligned distributions in terms of the average reward under policy. The responses above (below) average reward are assigned higher non-zero probability mass in the aligned (unaligned) distribution. While Assumption 3.1 is used to motivate the design of the loss, in the theoretical analysis, we replace it with weaker version which suffices for our main result: Assumption 4.2. (Weak Reward-density Correspondence) For given x, The aligned p+(yx) (unaligned p(yx)) densities are non-decreasing (non-increasing) in r(x, y). Under Assumption 4.2, we can obtain the following optimality results for -HAL & -GRPO: Theorem 4.3. Let θ(t) RL be the tth fixed point iterate Equation (17) of the f-HAL & f-GRPO losses Equations (15) and (16). With , the following hold almost surely: Divergence Estimation: The loss objectives satisfy: Mix, θ(t) L(f,g) fHAL (cid:16) θ(t+1) Mix (cid:17) Df (cid:16) t,MixM t,Mix L(f,g) fGRPO (cid:16) θ(t+1) RL (cid:17) Df (r,θ(t) RL) (r,θ(t) RL) M+ (cid:18) D+ (cid:17) , (cid:19) (19) = . (20) Alignment Consistency: Moreover, πθ(t+1)(yx) πref (yx) exp (cid:0)β1ht(x, y)(cid:1) with: ht(x, y) = g1 g1 (f (cid:18) dM+ dM 1 t,Mix t,Mix {dD (cid:19) (x, y) , =0}), ) (r,θ (t) RL θ(t+1) = θ(t+1) Mix θ(t+1) = θ(t+1) RL RL (x) for all > 0 & θ(t) {θ(t) ) = then µθ(t) (21) Average Reward Improvement: Under Assumption 4.2: µθref RL}. (x) (x) < µθ(t) Furthermore, if g1(f with µθ In the above, t,Mix = λD + (1 λ)c(x)D is mixture measure with λ being direct alignment weight from Eq. (16) and c(x) are normalization constants for given x. Also, (u)/u. Mix, θ(t) (x) < µθ(t+1) (r,θ) are defined in 4.1, (x) = maxy r(x, y). (r,θ(t) RL) = limu RL RL Theorem 4.3 shows that (1) for any FP iterate -GRPO (On-Policy) estimates the -divergence between the rewardaligned and unaligned distribution w.r.t. previous policy iterate. Similarly, the hybrid loss -HAL estimates the divergence between mixture aligned and unaligned distribution, where the mixture is weighted between true aligned/unaligned and the reward-aligned/unaligned w.r.t. the previous FP iterate policy. (2) The FP iterate for - HAL upweights the policy probability for responses having Divergence-Based RL for General LLM Alignment higher mixture-aligned measure compared to the mixtureunaligned Eq. (21). This captures the fact that after alignment, the likelihood of higher reward and aligned responses increases. -HAL reduces to the FDO loss with λ = 1 and we recover the alignment consistency result for FDO discussed by (Haldar et al., 2025). Similarly, for -GRPO, the alignment consistency happens in the form of constant upweighting by g1 only for above-average reward responses w.r.t. the previous policy. (3) In terms of average reward improvement for any FP iterate after alignment, the average reward is strictly higher compared to the reference policy. Moreover, when g1 = , we get strict average reward improvement in every iteration until all the probability mass is redistributed uniformly among responses with the maximum attainable reward. Name Hellinger JensenShannon KL Pearson χ2 Reverse KL Total Variation (t) ( 1) ln (t + 1) ln t+1 2 ln (t 1)2 ln 1 2 1 Canonical Link g(u) 1 eu ln 2 ln(1 + eu) eu 2 (1 + eu)1 1 Table 1. Some -divergences and their canonical links of choice. Canonical Link Function (g) In order to ensure fast learning with strict average reward improvement with guarantee to attain maximum reward, we need to satisfy g1(f ) = . For popular -divergences, we recommend canonical choice of the link function : ED(f ) in Table 1 (Expanded Table 6). In the experiments, we will use these specific pairs of f, to implement -GRPO and -HAL. 4.1. -GRPO vs GRPO tion (21). GRPO, even after downweighting, still retains nonzero mass on below-average responses. This results in stronger result for -GRPO with canonical link that admits strict reward improvement until convergence to the maximal-reward policy, and suggests an empirical advantage over GRPO, which we validate in 5. 5. Experiments In this section, we evaluate the practical effectiveness of our framework on real-world alignment tasks spanning both alignment regimes. RLVR Regime. We study math reasoning alignment (5.1), which requires on-policy optimization, and compare -GRPO against GRPO. We observe consistent performance improvements, aligning with the analysis in 4.1 and maximal-reward attainment property satisfied by -GRPO with canonical link. Preference Alignment Regime. We consider safety alignment (5.2), where both onand off-policy methods apply. We evaluate -HAL with λ {0, 1 2 , 1}corresponding to FDO (off-policy), the hybrid objective (FDO+f -GRPO), and -GRPO (on-policy)and compare against DPO, KTO, BCO, and GRPO. We find that -HAL achieves superior performance over both onand off-policy baselines, and that hybrid alignment (λ = 1 2 ) significantly mitigates reward hacking observed in purely on-policy methods when trained with reward models. All experiments are implemented using the Unsloth library (Daniel Han & team, 2023) with the vLLM backend (Kwon et al., 2023) for efficient large-scale fine-tuning. We release custom trainers and code that are fully compatible with this stack. For the exact algorithm used for implementation, refer to Appendix Algorithm 1. [Github Repository.] Similar to Theorem 4.3, the below characterizes the FP iterate for the unclipped GRPO objective. 5.1. Math Reasoning (RLVR Task) Theorem 4.4. For the unclipped GRPO loss, as , al- (yx) πref (yx) exp (cid:0)β1at(x, y)(cid:1), most surely: πθ(t+1) where at(x, y) = (r(x, y) µθ(t) )/σθ(t) more, µθ(t) > µθref for all > 0. . FurtherGRPO GRPO GRPO GRPO r The policy post-alignment is reweighted from the reference distribution by an exponential of the standardized reward, in the same vein as Vojnovic & Yun (2025)s stationary policy result. Hence, GRPO increases the probability of aboveaverage responses and decreases that of below-average ones, satisfying alignment consistency and leading to reward improvement for any tth iterate compared to the reference. In contrast, for -GRPO with the canonical link function (i.e., g1(f ) = ), probability mass is assigned exclusively to above-average reward samples at each iterate Equa6 Base Model and Training. We fine-tune the 7B and 1.5B variants of Qwen2.5-Math Base (Yang et al., 2024b) on three math reasoning benchmarks of increasing difficulty: OpenRS, LIMR, and GSM8K (Dang & Ngo, 2025; Li et al., 2025; Cobbe et al., 2021). We adopt binary accuracy reward as the verifiable signal, r(x, y) = 1{the final answer in is correct for question x} , and train the models to output their final answer enclosed in boxed{} to enable automatic evaluation using our chat template (Appendix Table 8). Evaluation. We compute Pass@1 on test math datasets GSM8k(test), MATH500, AMC23, AIME24, and AIME25, ordered by increasing difficulty. Pass@1 represents the probability that the first answer generated is correct. Directly Divergence-Based RL for General LLM Alignment calculating Pass@1 with only 1 sample per instance often results in high variance. To obtain more stable evaluation results, we generate = 16 samples for each problem, count the number of correct samples c, and calculate the unbiased estimator Pass@1 = Ex c/n (Chen, 2021). Method Base GRPO GSM8K MATH500 AMC23 AIME24 AIME25 Relative Overall Avg. Rank 21.39 71.67 25.66 60.60 27.34 43. 5.63 8.54 2.29 3.13 0.00 74.26 8.0 5.2 5.42 4.79 5.00 3.33 4.58 4.79 7.08 7.50 7.71 10.42 7.92 7. 72.73 72.47 72.34 72.59 72.65 72.40 41.09 40.94 41.72 45.16 43.75 43.28 61.18 61.93 61.59 61.70 61.65 61.51 Hellinger JS KL Pearson Reverse KL Total Variation Table 2. Pass@1 accuracy (%) on math benchmarks for Qwen2.5Math-1.5B trained on LIMR dataset. Relative Overall is the average of per-benchmark minmax normalized scores (0100). Avg. Rank is the mean rank across benchmarks. Cell color & intensity indicates performance relative to GRPO. 81.11 78.99 81.83 86.49 82.47 83.11 4.2 4.2 4.4 2.6 3.0 3. Comparing GRPO vs -GRPO Table 2 shows the detailed Pass@1 reasoning performance of -GRPO with GRPO across all math benchmarks trained on the LIMR dataset for the Qwen-1.5B model. We see that -GRPO, across all divergence choices, performs better than GRPO on most individual benchmarks. To aggregate the relative performance of each method across all benchmarks, we compute the Relative Overall Score (average of per-benchmark minmax normalized scores [0,100]) and Average Rank (mean rank across benchmarks), and we see the improved numbers for -GRPO for all divergences. Table 3 shows the summary of this computed relative overall score and average rank across both 1.5B and 7B models trained on all 3 datasets: LIMR (Medium-Hard), OpenRS (Hard), GSM8K (Easy). We can see that most cells are green, indicating that, in general, -GRPO shows consistent reasoning improvement over GRPO. We can attribute this to the maximal reward attainment guarantee we have for -GRPO with the canonical-link function. 5.2. Safety Alignment (PA Task) Base Model and Training. For safety alignment, we use the compliancerefusal dataset introduced in Haldar et al. (2025), which is designed to provably amplify robustness. We align the Qwen-2.5-Base models in both the 1.5B and 7B variants (Yang et al., 2024a; Team, 2024) using different direct alignment methods (off-policy) and GRPO (on-policy) as baselines for comparison with -HAL. For on-policy methods, reward signal is required. Since safety alignment does not admit verifiable environmental reward r(x, y), we employ parameterized reward model rϕ(x, y), specifically the DeBERTa-v3-large reward model, trained on large-scale helpfulharmless preference data from Anthropic (Bai et al., 2022a) and OpenAI (Stiennon et al., 2020; Nakano et al., 2021). Evaluation. We evaluate both robustness and utility to ensure that alignment improves safety without degrading model performance. (a) Robustness. (i) Separation: we measure the separation between latent clusters of safe and harmful prompts after alignment using the Bhattacharyya distance DB (Haldar et al., 2025; Lin et al., 2024a; Zheng et al., 2024) (Appendix C.2 & Figure 2). (ii) Attack Success Rate (ASR): we evaluate robustness on adversarial prompts using GCGoptimized AdvBench (Zou et al., 2023) and the SALAD benchmark (Li et al., 2024), which aggregates transfer attacks generated by TAP, AutoDAN, GCG, and GPTFuzz (Liu et al., 2023; Yu et al., 2023). (iii) ToxiGen: binary classification benchmark for distinguishing harmful from harmless content (Hartvigsen et al., 2022). (b) Utility. We report performance on MMLU-Pro (Wang et al., 2024), IFEval (Zhou et al., 2023), and MuSR (Sprague et al., 2024), which evaluate general knowledge, instructionfollowing capability, and soft chain-of-thought reasoning, respectively. Comparing -HAL with Other Alignment Methods Table 4 reports the detailed robustness and utility benchmarks for all alignment methods. To obtain compact measure of overall performance, we additionally compute three aggregated metrics: AvgRob, defined as the average over all robustness benchmarks where the separation metric is minmax normalized to [0, 100] (with the pre-alignment base model as the minimum); AvgUtil, defined as the average over all utility benchmarks; and the Total Score, defined as the mean of AvgRob and AvgUtil. Table 5 summarizes these aggregated scores for both the Qwen-1.5B and Qwen7B base models, enabling direct comparison of alignment methods across model scales. Within-Alignment-Class Superiority. Tables 4 and 5 show that -HAL is consistently superior or competitive within each alignment class. Among direct aligners, the offpolicy -HAL variant (λ = 1) outperforms KTO and DPO and is generally stronger than BCO in terms of the Total Score, indicating better robustnessutility trade-off. In the on-policy regime, -GRPO uniformly improves over GRPO, demonstrating that generalizing GRPO to -divergence objectives with canonical link g, yields systematic gains. Hybrid Alignment Mitigates Reward Hacking. Tables 4 and 5 show that on-policy methods perform relatively poorly (red cells), with GRPO performing worst overall. This is consistent with reward hacking: when optimization relies on learned reward model rather than verifiable environment reward, on-policy updates can exploit vulnerabilities in the reward signal and converge to superficially high-reward yet suboptimal responses (Skalse et al., 2022). Divergence-Based RL for General LLM Alignment Method Base GRPO Hellinger JensenShannon KL Pearson Reverse KL Total Variation LIMR Qwen-1.5B LIMR Qwen-7B GSM8K Qwen-1.5B GSM8K Qwen-7B Open-RS Qwen-1.5B Open-RS Qwen-7B Relative Overall Avg Rank Relative Overall Avg Rank Relative Overall Avg Rank Relative Overall Avg Rank Relative Overall Avg Rank Relative Overall Avg Rank 0.0 74. 81.11 78.99 81.83 86.49 82.47 83.11 8.0 5.2 4.2 4.2 4.4 2.6 3.0 3.8 0.0 91.72 91.54 94.93 93.64 96.28 96.59 87.79 8.0 5. 3.8 3.8 3.4 2.8 2.4 5.4 0.0 86.17 86.24 97.22 79.65 84.23 88.07 94.27 8.0 4.0 3.8 2.2 6.0 4.0 3.4 4.4 0.0 90. 95.08 91.44 86.60 87.82 90.46 95.27 8.0 4.4 3.8 3.2 5.2 3.6 3.8 3.2 0.0 82.44 78.47 93.30 85.66 85.19 81.65 86.93 8.0 4. 3.6 2.4 4.0 3.6 5.2 4.0 0.0 90.52 91.70 98.47 95.16 91.86 96.78 86.05 8.0 5.8 4.4 2.0 3.0 5 2.4 4.8 Table 3. Summary of Relative Overall performance (higher is better) and Avg. Rank across Qwen2.5-Math-1.5B and Qwen2.5-Math-7B models trained on LIMR, GSM8K, and Open-RS datasets. Cell color and intensity indicate performance relative to the GRPO baseline. Method Robustness Utility Total Qwen-7B (base) Qwen-1.5B (base) ToxiGen AvgRob. MMLU IFEval MuSR AvgUtil. Score (%) (%) Method AvgRob. AvgUtil. Score AvgRob. AvgUtil. Score Sep. ASR(%) DB GCG SALAD 8.53 14.27 44.96 5.77 37.20 78.90 5.99 21.34 68.42 Direct Aligners (Off-Policy) BCO DPO KTO FDO (f -HAL, λ = 1) : Hellinger 10.89 11.45 38.70 9.14 16.66 53.96 JS 8.29 22.10 56.50 KL χ2 0.00 16.24 17.33 14.34 RKL 8.76 1.16 13.46 11.58 39.04 TV Hybrid , -HAL (λ = 0.5) : Hellinger 9.74 JS KL χ2 RKL TV 8.96 38.22 12.13 14.13 48.76 10.52 19.70 52.46 0.00 14.74 12.48 0.00 4.76 16.65 18.04 3.40 30.04 56.12 45.11 52.37 57.02 59.15 56.01 56.91 56.28 57. 57.66 60.20 57.02 57.02 57.66 57.66 58.95 37.54 46.28 65.22 57.83 53.68 84.03 80.64 69.29 64.28 64.83 59.13 76.64 85.98 81.06 44.05 44.03 44.35 43.77 43.77 44.02 43.58 43.30 44. 43.65 44.10 44.01 43.83 43.63 44.17 55.52 45.11 54.80 44.84 56.59 44.84 48.23 53.59 47.89 42.71 48.59 47.44 55.64 46.03 54.68 45.90 56.00 46.30 55.28 45.77 51.92 46.03 52.64 46.30 54.44 46.03 55.76 46.43 52.76 45.50 55.16 44.84 50.96 45.24 53.96 44.52 48.48 56.85 48.12 52.97 48.77 51.23 48.21 66.12 47.08 63.86 47.68 58. 48.04 56.16 48.76 56.80 47.42 53.28 47.94 62.29 46.61 66.30 47.55 64.30 43.80 32.09 43.38 46.76 42.31 44.29 38. 4.62 62.18 66.54 56.17 56.91 55.00 56.91 55.85 56.91 45.24 46.96 35.87 65.76 49.32 30.73 6.44 30.24 70.38 4.47 23.80 58.08 4.20 44.70 77.86 6.31 1.50 16.98 5.37 41.64 35.46 4.02 56.32 87.56 On-Policy RL GRPO -GRPO (f -HAL, λ = 0) : Hellinger 45.03 45.14 JS 45.14 46.05 KL 44.52 40.20 χ2 43.49 54.62 RKL 46.05 47.68 TV 43.30 37.01 Table 4. Robustness and Utility benchmarks for Qwen2.5-7B (Base) using different alignment methods. AvgRob & AvgUtil denotes the average robustness & utility score respectively. The Total score is defined as mean(AvgRob, AvgUtil). Cell color and intensity indicate performance relative to the mean value within each benchmark column. Pre-alignment model separation is 2.48. 46.53 44.97 48.32 42.86 46.80 43.39 43.53 43.12 49.64 45.09 42.33 44. 43.60 44.25 43.38 43.82 43.41 43.39 Hybrid alignment mitigates reward hacking by combining on-policy exploration with off-policy direct supervision. It benefits from the exploration inherent to on-policy methods while preventing lazy reward-hacking suboptimalities by using preference data as guidance. For the larger Qwen-7B model, hybrid -HAL generally outperforms pure direct aligners, indicating that the added exploration improves performance when sufficient model capacity is available. This behavior is theoretically grounded in Equations (19) and (21), where the mixture formulation of -HAL assigns non-negligible probability mass to ideal responses, constraining degenerate solutions. For the smaller Qwen-1.5B model, reward hacking effects are more pronounced due to limited capacity. Although hybrid -HAL (λ = 0.5) still yields large improvements over pure on-policy RL, direct aligners remain slightly stronger 8 Direct Aligners (Off-Policy) 48.23 58.95 BCO 47.89 37.54 DPO KTO 48.59 46.28 FDO (f -HAL, λ = 1) : Hellinger JS KL χ2 RKL TV 48.48 48.12 48.77 48.21 47.08 47. 65.22 57.83 53.68 84.03 80.64 69.29 Hybrid, -HAL (λ = 0.5) : 48.04 Hellinger 48.76 JS 47.42 KL χ2 47.94 46.61 RKL 47.55 TV 64.28 64.83 59.13 76.64 85.98 81.06 32.09 On-Policy RL GRPO -GRPO (f -HAL, λ = 0) : Hellinger JS KL χ2 RKL TV 45.24 46.96 35.87 65.76 49.32 30. 44.29 45.03 45.14 44.52 43.49 46.05 43.30 53.59 42.71 47.44 56.85 52.97 51.23 66.12 63.86 58.49 56.16 56.80 53.28 62.29 66.30 64.30 55.92 27.56 37. 74.84 61.17 47.33 60.90 79.95 57.37 72.31 55.75 49.12 59.09 77.27 58.32 32.67 33.11 32.62 31.46 33.44 33.97 32.82 34.17 32.84 29.66 32.36 33.26 33.01 33.48 32.65 44.30 30.33 34. 53.15 47.30 40.65 46.86 57.06 45.11 50.99 44.05 41.19 46.05 55.37 45.49 38.19 12.30 21.25 16. 45.14 46.05 40.20 54.62 47.68 37.01 37.65 22.96 33.74 23.17 34.97 22.20 32.33 31.48 28.81 31.22 29.94 31.43 34.99 27.22 31.27 27.20 32.46 26.81 Table 5. Post-Safety-Alignment Avg. Robustness, Utility and Total Score results for Qwen2.5-Base Family. overall, suggesting that stronger supervision is preferable when model expressivity is constrained. 6. Conclusion In this work, we present unified divergence-based perspective for general LLM alignment, extending prior direct alignment frameworks to the RLVR regime. We introduce principled recipe for constructing on-policy and hybrid alignment objectives, yielding -GRPO and -HAL as instances derived from general -divergences, and provide theoretical guarantees characterizing their optimal policies and reward-improvement properties. Empirically, we demonstrate the effectiveness and flexibility of this framework across both RLVR and preference alignment regimes. Our work proposes divergence estimation as unifying and practical foundation for general LLM alignment. Divergence-Based RL for General LLM Alignment"
        },
        {
            "title": "References",
            "content": "Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Bengio, Y. and Senecal, J.-S. Adaptive importance sampling to accelerate training of neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4): 713722, 2008. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Chen, M. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dang, Q.-A. and Ngo, C. Reinforcement learning for reasoning in small llms: What works and what doesnt, 2025. URL https://arxiv.org/abs/2503.16219. Daniel Han, M. H. and team, U. Unsloth, 2023. URL http://github.com/unslothai/unsloth. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Haldar, R., Wang, Z., Song, Q., Lin, G., and Xing, Y. Llm safety alignment is divergence estimation in disguise. arXiv preprint arXiv:2502.00657, 2025. Hartvigsen, T., Gabriel, S., Palangi, H., Sap, M., Ray, D., and Kamar, E. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. arXiv preprint arXiv:2203.09509, 2022. Jiang, D., Lu, Y., Li, Z., Lyu, Z., Nie, P., Wang, H., Su, A., Chen, H., Zou, K., Du, C., et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. Jung, S., Han, G., Nam, D. W., and On, K.-W. Binary classifier optimization for large language model alignment. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 18581872, 2025. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35: 2131421328, 2022. Li, L., Dong, B., Wang, R., Hu, X., Zuo, W., Lin, D., Qiao, Y., and Shao, J. Salad-bench: hierarchical and comprehensive safety benchmark for large language models. arXiv preprint arXiv:2402.05044, 2024. Li, X., Zou, H., and Liu, P. Limr: Less is more for rl scaling. https://github.com/GAIR-NLP/LIMR, 2025. Lin, Y., He, P., Xu, H., Xing, Y., Yamada, M., Liu, Towards understanding jailbreak H., and Tang, J. attacks in LLMs: representation space analysis. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 70677085. Association for Computational Linguistics, November 2024a. doi: 10.18653/v1/2024.emnlp-main. 401. URL https://aclanthology.org/2024. emnlp-main.401/. Lin, Y., He, P., Xu, H., Xing, Y., Yamada, M., Liu, H., and Tang, J. Towards understanding jailbreak attacks in llms: representation space analysis. arXiv preprint arXiv:2406.10794, 2024b. Liu, X., Xu, N., Chen, M., and Xiao, C. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023. Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J. Webgpt: Browser-assisted question-answering with human feedback. In arXiv, 2021. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 9 Divergence-Based RL for General LLM Alignment Yu, J., Lin, X., Yu, Z., and Xing, X. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253, 2023. Zheng, C., Yin, F., Zhou, H., Meng, F., Zhou, J., Chang, K.-W., Huang, M., and Peng, N. On prompt-driven safeguarding for large language models. arXiv preprint arXiv:2401.18018, 2024. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Skalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. Sprague, Z., Ye, X., Bostrom, K., Chaudhuri, S., and Durrett, G. Musr: Testing the limits of chain-of-thought with multistep soft reasoning, 2024. URL https://arxiv. org/abs/2310.16049. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, In P. Learning to summarize from human feedback. NeurIPS, 2020. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/ blog/qwen2.5/. Vojnovic, M. and Yun, S.-Y. What is the alignment objective of grpo? arXiv preprint arXiv:2502.18548, 2025. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging multitask language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., and Fan, Z. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. Divergence-Based RL for General LLM Alignment A. -Divergence Name Hellinger (t) ( 1)2 JensenShannon KL Pearson χ2 Reverse KL Total Variation 2 (cid:2)t ln (t + 1) ln t+1 ln (t 1)2 ln 1 2 1 (u) 1 (cid:3) ln(cid:0)2 eu(cid:1) 1 eu1 4 u2 + 1 ln(u) eff. dom ED(f ) < 1 < ln 2 < 0 1/2 Canonical link: g(u) 1 eu g1(u) ln(1 u) ln 2 ln(1 + eu) ln(cid:0)eln 2u 1(cid:1) eu 2 (1 + eu)1 ln(u) Mixture Scaler g1 (t) 1 2 ln ln ln + 1 2(t 1) ln 1 ln 2 0 1/ ln((2u)1 1) ln((sign(t 1))1 1) Table 6. Common -divergences with conjugates, effective domains, canonical links, inverse links, mixture scalers, and tail slopes. Canonical links are characterized by functions : ED(f ) s.t. g1 = . Expectation w.r.t. positive measure. Let µ be any positive (finite) measure on measurable space (Ω, F) and let : Ω be measurable. We define the expectation with respect to µ as [h] := µ (cid:90) Ω h(ω) dµ(ω). When µ is probability measure, this reduces to the usual notion of expectation. Definition A.1 (f -Divergence). For two positive measures P, with (absolute continuous), the -divergence between the two measures is defined as follows: Df (PQ) = vQ (cid:20) (cid:18) dP dQ (cid:19)(cid:21) (v) , where : R+ is convex function with (1) = 0 and : dQ(v) = 0 = dP(v) = 0. General Case. For arbitrary P, Q, let = Pac +Ps be the Lebesgue decomposition of w.r.t. into absolute continuous and singular measures. The -divergence is defined as Df (PQ) = vQ (cid:20) (cid:18) dPac dQ (cid:19)(cid:21) (v) + Ps(Ω), where := limt (t) [0, ]. Definition A.2 (Convex Conjugate). where (u) is the convex conjugate of . Note that is also convex function. (u) = sup tR {ut (t)}, Lemma A.3 (Variational Representation & Optimality). Let P, be probability measures on V, and write the Lebesgue decomposition = Pac + Ps w.r.t. Q, with density := dPac dQ . Let satisfy Q(S) = 0 and Ps(S) = Ps(V) (support of Ps). Define (t) [0, ]. Then := limt Df (PQ) = sup :VED(f ) (cid:16) [T (v)] [f (T (v))] (cid:17) , (22) where is the convex conjugate of and ED(f ) = {u : (u) < }. Moreover, the optimal functional can be chosen as (v) (r(v)) 1Sc(v) + 1S(v), (23) (and if is differentiable, (r) = {f (r)}). Proof. For any measurable : ED(f ), FenchelYoung gives (t) + (u) (t 0, ED(f )). 11 Applying this pointwise with (t, u) = (r(v), (v)) and integrating over yields Divergence-Based RL for General LLM Alignment Pac [T ] [f (T )] = [rT (T )] [f (r)]. For the singular part, for any and any 0, (t) tu (u) implies Hence Ps-a.s., and so Combining, (t) (u) whenever (u) < . [T ] Ps(Ω). Ps [T ] [f (T )] [f (r)] + Ps(Ω) = Df (PQ), so the RHS of (22) is Df (PQ). To see tightness, take (v) (r(v)) on {r > 0} (equivalently = (r) when differentiable), so that rT (T ) = 1S + 1Sc (or (r) Q-a.s. For the singular part, choose set with Q(S) = 0, Ps(S) = Ps(Ω), and let Tn = Tn 1S if needed). Then [Tn] [f (Tn)] [f (r)] + Ps(Ω) = Df (PQ). Thus the supremum equals Df (PQ), proving (22). B. Proofs and Additional Theoretical Details Lemma B.1 (Comonotone Covariance Inequality). Let : Ω be random variable. If f, : are integrable and both nondecreasing (or both non-increasing), then E[f (X)g(X)] E[f (X)] E[g(X)], equivalently Cov(f (X), g(X)) 0. Proof. Let be an independent copy of X. Then (cid:104) 2 Cov(f (X), g(X)) = (f (X) (X ))(g(X) g(X )) (cid:105) . (Indeed, expanding the RHS and using independence gives E[f (X)g(X)] + E[f (X )g(X )] E[f (X)g(X )] E[f (X )g(X)] = 2 E[f (X)g(X)] 2 E[f (X)] E[g(X)].) If and are both non-decreasing, then for every (x, x), (x x) 0 (x) (x) 0 and g(x) g(x) 0, and similarly if (x x) 0 then both differences are 0. Hence (f (x) (x))(g(x) g(x)) 0 pointwise, so the expectation is nonnegative: (cid:104) (f (X) (X ))(g(X) g(X )) (cid:105) 0. Therefore Cov(f (X), g(X)) 0, which is equivalent to E[f (X)g(X)] E[f (X)] E[g(X)]. The non-increasing case follows identically. Lemma B.2 (Monotone Reweighting Increases the Mean). Fix and let q( x) be distribution over y. For any nonnegative weight function wx(y) with Eq[wx] > 0, define the tilted distribution qw(y x) = q(y x) wx(y) Eyq[wx(y)] . 12 Then for any measurable reward r(x, y), Divergence-Based RL for General LLM Alignment qw [r(x, y)] [r(x, y)] = Covq(x) (cid:0)r(x, y), wx(y)(cid:1) Eq[wx(y)] . In particular, if wx(y) is (weakly) increasing in r(x, y), then qw [r(x, y)] [r(x, y)], with strict inequality unless r(x, ) is q-a.s. constant or wx is q-a.s. constant. Proof. By definition, [r] = qw (cid:90) r(y) q(y)w(y) Eq[w] dy = Eq[rw] Eq[w] ."
        },
        {
            "title": "Hence",
            "content": "Eq[rw] Eq[r] Eq[w] Eq[w] If is non decreasing in r, then and w(r) are comonotone, so Covq(r, w) 0 (Lemma B.1), yielding Eqw [r] Eq[r], with strict inequality unless or is q-a.s. constant. Covq(r, w) Eq[w] [r] [r] = qw = . Proof of Theorem 4.3. Divergence Estimation and Alignment Consistency: (i) F-GRPO: The loss can be expressed as : L(f,g) f-GRPO(θ) = 1 + β1 x (cid:88) yiπθold ,i=1 (cid:0)ai ˆw+ g(rθ,i) ai ˆw g(rθ,i)(cid:1) (24) where ˆw numbers, Continuous mapping theorem and softmax with log-Q correction to get the following: (cid:1) 1{ai>0} [Equation (12)]. As , we can use Strong Law of Large (cid:0){rj ln πθold(yjx)}G = σi j= ˆµ(r(x, .)) a.s. µθold = Eπθold r(x, y); ˆσ(r(x, .)) a.s. σθold = (cid:113)Vπθold r(x, y) ai a.s. r(x,yi)µ θold σ θold σi (cid:0){rj ln πθold(yjx)}G j=1 (cid:1) a.s. dQ πθold (yix), where dQ exp r(x, y). 1{ai>0} a.s. 1 {r(x,yi)µ θold >0} Combining the above with the convergence in the empirical distribution sampled from πθold , we have: f-GRPO(θ) a.s.= (1 + β1) L(f,g) yπθold (cid:32) r(x, y) µθold σθold (cid:34) dQ+ πθold (cid:35)(cid:33) (yx) g(rθ) {r(x,yi)µ θold >0} + dQ πθold (yx) g(rθ,i)1 {µ θold r(x,yi)>0} = (1 + β1)σθold (cid:18) yQ+ [r(x, y) µθold ]+g(rθ) + yQ [r(x, y) + µθold ]+f g(rθ) (cid:19) Given measures D+ (r,θold), (r,θold) with densities dD (r,θold)(x, y) [(r(x, y) µθold (x))]+ exp(r(x, y)) dP(x), 13 which matches Def. 4.1 (with πθold in place of πθ). Then up-to scaling constants the above can be simplified as Divergence-Based RL for General LLM Alignment const L(f,g) (cid:16) f-GRPO(θ) a.s.= D+ (r,θold) [ c(x) g(rθ) ] + (r,θold ) (cid:17) [ g(rθ) ] , (25) where c(x) is normalization constant. Using Lemma A.3 As D+ (no subspace where D+ (r,θold)) then the divergence is just: (r,θold) (r,θold) (r,θold), that is the supports are complementary L(f,g) f-GRPO(θ) Df (c(x)D+ (r,θold) (r,θold)) = = Df (D+ (r,θold) (r,θold)) inf θ For tth FP iterate, θ(t) RL = θold and θ(t+1) RL = arg inf θ L(f,g) f-GRPO(θ) L(f,g) f-GRPO(θ(t+1)) Df (D+ (r,θ(t)) (r,θ(t))) Moreover, using = g(rθ), the optimality Lemma A.3 for the (t + 1)th FP iterate is attained at (t+1) = g(rθ(t+1) β ln 1 RL π ) = (t+1) θ RL πref = (x,y)=0. (r,θ (t) RL ) πθ(t+1) RL (yx) πref (yx) exp β1g1(f 1 (cid:32) (cid:33) (x,y)=0) (r,θ (t) RL ) (26) (ii) F-HAL The hybrid alignment loss is λ weighted avergae between the direct aligner FDO loss and -GRPO: f-HAL(θ) = λL(f,g) L(f,g) FDO (θ) + (1 λ)L(f,g) f-GRPO(θ), where LFDO(f,g)(θ) = ED+ g(rθ) + ED g(rθ) with infimum at Df (D+ D) using the variational representation Lemma A.3. We know that as , F-GRPO loss is almost surely equal to the representation in Equation (25). As corollary, F-HAL is almost surely equal to the following, up to some scaling constants: L(f,g) f-HAL(θ) λ (cid:18) D+ g(rθ) + (cid:19) g(rθ) + (1 λ) (cid:32) D+ (r,θold ) [ c+(x) g(rθ) ] + (r,θold ) c(x) [ g(rθ) ] (cid:33) For mixture measures Mix = λD + (1 λ)c(x)D (r,θold), then: L(f,g) f-HAL(θ) Df (M+ Mix Mix). inf θ For tth FP iterate, θ(t) Mix = θold and θ(t+1) Mix = arg inf θ L(f,g) f-HAL(θ), and f-HAL(θ(t+1) L(f,g) Mix ) Df (M+ t,Mix t,Mix) Moreover, using = g(rθ) and the optimality Lemma A.3, the (t + 1)th FP iterate is attained at (t+1) = g(rθ(t+1) β ln ). This implies that = ( t,Mix Mix π ) = (t+1) θ Mix πref dM+ dM t,Mix πθ(t+1) RL (yx) πref (yx) exp β1g1 ( (cid:32) dM+ dM t,Mix t,Mix (cid:33) ) (27) Note that Equations (26) and (27) are indeed alignment consistent and assign more mass/density to responses from (+) distribution (). To see that the probability density (27) is non-decreasing in R(x, y) where R(x, y) = 14 dM+ dM t,Mix t,Mix , we note Divergence-Based RL for General LLM Alignment that g1(u) > 0 as g1(u) = = g1(u) = g(g1(u)) > 0 (monotonicity of g(u)). Combined with the fact that 0 as is convex, we have that eβ1ht(R(x,y)) = exp(β1 g1 (f (R(x, y))) is non-decreasing. Similarly, (26) is also non-decreasing in R(x, y) = dD+ (r,θ dD (r,θ . (t) RL ) (t) RL ) Average Reward Improvement: We know that g1 is monotone increasing and g1 is non-decreasing. We can apply Lemma B.2 with = πref and wx(y) = exp(β1g1(f =0)) for f-GRPO and (r,θ (t) RL ) wx(y) = exp(β1g1(f ( r(x, y), hence the mixture density ratio is also monotone in r(x, y).) to get: t,Mix t,Mix dM+ dM )) for f-HAL (Note: By Assumption 4.2 dD is non-decreasing/increasing in µθref (x) < µθ(t+1) (x), for all 0 & θ(t) {θ(t) For exp(β1[g1(f the special case when g1(f =0) g1(f Mix, θ(t) RL}. 1 (r,θ (t) RL ) to wx(y) = 0, , Supp(πθ(t) Supp(πθ(t) RL ) ) (r,θ(t) RL) (r,θ(t) RL) and increases the average reward at each iteration: RL (t1) RL ) = 0 ) = , applying Lemma B.2 with = πθ(t) 1 (r,θ =0)]), on Supp(πθ(t) RL ) = {y : (r,θ(t1) RL ) and wx(y) = = 0} which simplifies RL Basically, the support after each iteration shrinks further and further > 0. This continues strictly until the FP iteration stops with µθ(t) RL (x) = maxy r(x, y) for some t. µθ(t) RL (x) < µθ(t+1) RL (x) Proof of Theorem 4.4. Simplifying Equation (5), the unclipped GRPO objective (or ϵ ) can be expressed as: sup θ (cid:88) yiπθold ,i=1 1 πθ(yix) πθold (yix) ai βDKL(πθ πref ) As , using strong law of large numbers and continuous mapping, ai = Aπθ (x, yi) a.s. r(x,yi)µ the empirical distribution converges to samples from πθold , leading to: θold σ θold . Furthermore, (cid:18) sup θ yπθold πθ(yx) πθold (yx) r(x, y) µθold σθold βDKL(πθ πref ) (cid:19) The above simplifies to: With a(x, y) = r(x,y)µ θold σ θold inf θ LGRPO(θ) = sup θ (cid:18) yπθ a(x, y) βDKL(πθ πref ) (cid:19) (cid:18) = sup θ yπθ a(x, y) β ln (cid:19) πθ(yx) πref (yx) The above objective is maximized when a(x, y) β[ln πθ(yx) constant, for the tth FP iterate Equation (17) with θ(t) = θold and θ(t+1) = arg inf θ LGRPO(θ): πref (yx) + 1] = 0. (first order conditions). Hence, upto normalization πθ(t+1)(yx) πref (yx) exp [ + 1] πθ(t+1)(yx) πref (yx) exp at(x, y) β at(x, y) β 15 Divergence-Based RL for General LLM Alignment B.1. From Sign-Based Updates to Advantage-Weighted -GRPO In the main text, the preliminary form of the -GRPO objective in Eq. (13) involves sign-based weighting sign(ai), where ai denotes standardized reward-based advantage. This form arises naturally from the variational decomposition in Eq. (9), as the sign determines whether sample contributes to the aligned or unaligned component of the divergence estimator. While this formulation is sufficient for identifying the correct update direction, it discards magnitude information contained in the advantage. Connection to Standard Policy Gradient Methods. Popular on-policy reinforcement learning algorithms such as PPO and GRPO maximize expected reward by optimizing objectives of the form sup θ E yπθold (cid:105) (cid:104) πθ(yx) πθold (yx) Aπθold (x, y) , (28) where the advantage Aπθold (x, y) scales the policy gradient and determines both its direction and magnitude. In particular, GRPO uses batch-standardized advantage Aπθold (x, yi) = r(x, yi) ˆµ(r(x, )) ˆσ(r(x, )) , (29) so that gradient magnitudes are proportional to relative reward improvements. Motivation for Advantage Weighting. Using only sign(ai) treats all above-average samples equally and all belowaverage samples equally, regardless of how strongly they deviate from the mean reward. Replacing sign(ai) with ai restores the standard policy-gradient behavior in which higher-advantage samples exert proportionally larger influence on the update. This modification preserves the alignment direction implied by the divergence estimator while improving optimization efficiency and variance properties. In -GRPO, the policy score is parameterized as rθ,i = β ln πθ(yix) πref (yix) , so the gradient of Normalization and Scaling. the loss contains factor of β1 relative to the standard log-policy gradient. To ensure that gradient magnitudes remain comparable to those of conventional on-policy methods, we scale the advantage by (1 + β1), yielding the final weighting ai(1 + β1). This normalization aligns the overall gradient scale with that of GRPO and PPO, facilitating stable optimization and fair empirical comparison. Resulting Objective. With this replacement, the resulting -GRPO loss in Eq. (15) can be viewed as divergence-based generalization of GRPO, where the policy gradient is weighted by standardized advantage while the update direction is shaped by the chosen -divergence through the function ψ(, ). This modification preserves the theoretical interpretation of -GRPO as divergence estimator while recovering the practical benefits of advantage-weighted on-policy optimization. 16 C. Experiment Details Divergence-Based RL for General LLM Alignment We use single Nvidia H200 140GB for all our experiments with the following implementation of -HAL, with corresponding hyperparameters. Parameter Value Learning rate (Math RLVR) Learning rate (Safety PA) KL coefficient (β) LoRA rank LoRA α Weight decay Warmup ratio LR scheduler Optimizer 5 106 5 105 0.1 64 64 0.1 0.1 Cosine AdamW (8-bit) On-policy (GRPO / -GRPO / -HAL) specific Gradient accumulation steps 4 4 Number of generations (G) Direct Aligner (Off-policy) specific 4 Per-device train batch size Gradient accumulation steps 1 Algorithm 1 -HAL Training (minibatch implementation) Input: policy πθ, reference πref , divergence , link Input: batch size B, rollouts per prompt G, step size η, regularization coefficient β = 0. Input: optional supervision dataset Dsup either Dsup = {(x, yw, yl)} (pairwise) or Dsup = {(x, y, ℓ)} with ℓ {+1, 1} (binary) Initialize parameters θ repeat b=1 Sample minibatch of prompts {xb}B Set behavior policy πθold πθ Initialize gon 0 and goff 0 On-policy term (F-GRPO): for = 1 to do Sample {yb,i}G Compute rewards rb,i = r(xb, yb,i) and advantages ab,i (Eq. (4)) Compute ˆw Accumulate β1)θψ(f,g)(rθ,b,i, ai) b,i Eq. (12)) and ψ(rθ,i, ai) Eq. (14) (cid:80)G gon gon + 1 BG i=1 πθold (xb) i=1 ai (1 + end for Off-policy FDO term (if supervision exist): Set λ > 0 if pairwise minibatch (xb, yw,b, yl,b) is available from Dsup then for = 1 to do Accumulate goff goff + 1 (cid:17) g(rθ(xb, yl,b)) end for (cid:16) θg(rθ(xb, yw,b)) + θf else if binary minibatch (xb, yb, ℓb) is available from Dsup then for = 1 to do if ℓb = +1 then goff goff 1 θg(rθ(xb, yb)) else goff goff + θf g(rθ(xb, yb)) end if end for else Set λ 0 end if Hybrid update: θ θ η(cid:0)(1 λ) gon + λ goff (cid:1) until converged Output: trained policy πθ Table 7. Left: -HAL training algorithm. Right: training hyperparameters used across all experiments. 17 Chat Template: Divergence-Based RL for General LLM Alignment Role System User Assistant Math Reasoning Template <im start>system You are helpful assistant.<im end> <im start>user {input} Please reason step by step, and put your final answer within boxed{}.<im end> <im start>assistant Safety Alignment Template <im start>system You are helpful assistant.<im end> <im start>user {input} <im end> <im start>assistant Table 8. Chat templates used for (left) math reasoning and (right) safety alignment. The math template encourages standardized final-answer formatting via boxed{} for automated evaluation. C.1. Additional Math Reasoning Tables Method Base GRPO GSM8K MATH500 AMC23 AIME24 AIME25 Relative Overall Avg. Rank 21.39 71.67 25.66 60.60 27.34 43. 5.63 8.54 2.29 3.13 0.00 74.26 8.0 5.2 72.73 72.47 72.34 72.59 72.65 72.40 Hellinger JS KL Pearson Reverse KL Total Variation Table 9. Pass@1 accuracy (%) for Qwen2.5-Math-1.5B trained on LIMR. 41.09 40.94 41.72 45.16 43.75 43.28 61.18 61.93 61.59 61.70 61.65 61.51 7.08 7.50 7.71 10.42 7.92 7.92 81.11 78.99 81.83 86.49 82.47 83.11 5.42 4.79 5.00 3.33 4.58 4.79 4.2 4.2 4.4 2.6 3.0 3. Method GSM8K MATH500 AMC23 AIME24 AIME25 Relative Overall Avg. Rank 20.32 79.90 81.14 80.83 81.15 81.07 80.86 80.63 Base 12.03 GRPO 55.78 Hellinger 57.81 JensenShannon 54.53 KL 54.69 55.47 Pearson Reverse KL 55.78 Total Variation 55.78 Table 10. Pass@1 accuracy (%) for Qwen2.5-Math-7B trained on LIMR. 0.00 91.72 91.54 94.93 93.64 96.28 96.59 87.79 23.15 68.13 68.93 69.45 68.90 69.45 69.28 68.71 3.54 16.46 16.88 19.17 17.29 17.08 19.17 14.38 1.88 7.92 7.08 7.71 8.13 8.96 8.13 7.29 8.0 5.2 3.8 3.8 3.4 2.8 2.4 5.4 Method GSM8K MATH500 AMC23 AIME24 AIME25 Relative Overall Avg. Rank Method GSM8K MATH500 AMC23 AIME24 AIME25 Relative Overall Avg. Rank 21.39 81.34 80.71 81.13 80.13 80.89 80.79 80.22 Base 27.34 GRPO 48.44 Hellinger 48.75 JensenShannon 49.06 KL 44.38 Pearson 47.81 49.22 Reverse KL Total Variation 46.88 Table 11. Pass@1 accuracy (%) for Qwen2.5-Math-1.5B trained on GSM8k . 0.00 86.17 86.24 97.22 79.65 84.23 88.07 94.27 25.66 67.26 68.64 67.89 66.80 68.26 67.65 67.28 5.63 10.21 10.00 12.08 7.71 11.46 11.67 11. 2.29 4.79 4.79 5.63 5.83 3.75 4.17 6.04 8.0 4.0 3.8 2.2 6.0 4.0 3.4 4.4 20.32 85.51 85.96 87.76 86.67 87.28 87.34 86.55 Base 12.03 GRPO 61.56 Hellinger 58.13 JensenShannon 60.00 KL 57.34 Pearson 59.22 57.03 Reverse KL Total Variation 60.78 Table 12. Pass@1 accuracy (%) for Qwen2.5-Math-7B trained on GSM8k. 23.15 73.70 74.23 74.86 74.21 74.93 74.60 74.31 3.54 19.17 22.08 17.71 18.54 18.54 18.96 18. 0.00 90.71 95.08 91.44 86.60 87.82 90.46 95.27 1.88 8.75 9.79 9.58 7.71 7.71 9.17 11.04 8.0 4.4 3.8 3.2 5.2 3.6 3.8 3.2 Method Base GRPO GSM8K MATH500 AMC23 AIME24 AIME Relative Overall Avg. Rank 21.39 73.28 25.66 61.46 27.34 41.09 5.63 8. 2.29 4.79 0.00 82.44 8.0 4.6 Method Base GRPO GSM8K MATH500 AMC23 AIME24 AIME Relative Overall Avg. Rank 20.32 81.42 23.15 70.08 12.03 55.63 3.54 18. 1.88 7.71 0.00 90.52 8.0 5.8 73.97 74.03 72.59 72.81 73.08 73.28 Hellinger JS KL Pearson Reverse KL Total Variation Table 13. Pass@1 accuracy (%) for Qwen2.5-Math-1.5B trained on OpenRS . 78.47 93.30 85.66 85.19 81.65 86. 63.30 62.33 61.50 61.61 61.21 61.26 44.06 41.88 43.75 45.31 42.03 42.50 7.92 10.00 9.38 9.38 9.17 8.13 3.96 5.42 4.38 3.96 4.17 5.83 3.6 2.4 4.0 3.6 5.2 4.0 82.39 83.53 82.42 81.97 83.06 82. Hellinger JS KL Pearson Reverse KL Total Variation Table 14. Pass@1 accuracy (%) for Qwen2.5-Math-7B trained on OpenRS. 70.58 71.75 71.69 69.66 71.68 71.84 59.38 56.41 59.53 57.50 60.00 56.41 15.63 19.58 17.08 18.13 18.96 16.67 91.70 98.47 95.16 91.86 96.78 86.05 8.54 9.38 8.96 7.92 8.54 6. 4.4 2.0 3.0 5.0 2.4 4.8 18 Divergence-Based RL for General LLM Alignment C.2. Separation Metric for Safety Alignment We follow the methodology of (Haldar et al., 2025; Lin et al., 2024b; Zheng et al., 2024) to visualize the separation between anchor safe and harmful prompts in the latent space of the LLM before and after alignment. Each prompt is represented using the last hidden state of the final token, which captures the full contextual history of the sequence. We then apply PCA to these representations and visualize the first two principal components. Please refer to the previous mentioned references for more details. Figure 2 illustrates this visualization for the Qwen-7B base model and its aligned variants. Alignment induces substantially stronger separation between safe and harmful clusters, phenomenon that has been shown to correlate with increased robustness. Following the procedure of (Haldar et al., 2025) (Appendix C.3), we quantify this separation using the Bhattacharyya distance DB between the clusters, which we adopt as robustness metric. (a) Base, (DB=2.48) (b) λ = 0, DB = 4.47 (c) λ = 0.5, DB=12.13 (d) λ = 1, DB = 9.14 Figure 2. Latent-space separation (Bhattacharyya distance DB) between safe and harmful prompt clusters before and after alignment with -HAL (JensenShannon divergence) on Qwen-7B-Base. Compared to the base model, all aligned variants increase separation. The on-policy method (f -GRPO, λ=0) yields weaker separation than the hybrid (f -HAL, λ=0.5) and the off-policy supervised objective (FDO, λ=1). C.3. Qwen-1.5B Safety Alignment Table Method Robustness Utility Total Sep. ASR(%) DB GCG SALAD 9.36 30.10 35.14 2.95 86.00 62.28 4.48 54.50 65. Direct Aligners (Off-Policy) BCO DPO KTO FDO (f -HAL, λ = 1) : Hellinger 1.10 25.66 Jensen-Shannon 13.89 23.20 37.94 13.02 54.00 60.28 KL 12.76 13.60 45.52 Pearson 22.11 Reverse KL 5.00 23.30 11.61 17.20 49.58 Total Variation 17.95 7.25 12.48 Hybrid , -HAL (λ = 0.5) : 9.86 Hellinger Jensen-Shannon 11.18 19.60 53.60 12.42 46.50 62.70 KL 10.96 13.30 48.44 Pearson 0.00 32.44 20.08 Reverse KL 15.02 22.31 59.74 Total Variation 0.66 98.00 96. On-Policy RL GRPO -GRPO (f -HAL, λ = 0) : Hellinger Jensen-Shannon KL Pearson Reverse KL Total Variation 5.75 77.10 44.42 3.39 92.42 73.04 4.11 64.27 64.28 3.64 93.10 75.22 3.52 67.80 55.96 4.99 97.10 81.96 ToxiGen AvgRob. MMLU IFEval MuSR AvgUtil. Score (%) (%) 48.40 47.87 51.06 45.53 44.15 45.96 46.28 48.09 45.21 51.28 47.13 50.85 50.11 50.96 48.40 55.92 27.56 37.33 74.84 61.17 47.33 60.90 79.95 57.37 72.31 55.75 49.12 59.09 77.27 58. 28.91 28.67 29.01 28.22 28.37 28.91 28.61 27.50 28.13 26.36 28.21 29.06 28.44 27.29 28.30 35.25 33.86 35.73 34.92 35.13 33.73 32.67 44.30 33.11 30.33 32.62 34.98 33.21 32.94 37.17 34.79 37.41 35.58 35.73 34.13 40.89 34.13 35.73 34. 27.58 35.05 36.21 32.67 36.21 34.52 36.21 34.39 38.49 34.66 35.13 34.52 31.46 53.15 33.44 47.30 33.97 40.65 32.82 46.86 34.17 57.06 32.84 45.11 29.66 50.99 32.36 44.05 33.26 41.19 33.01 46.05 33.48 55.37 32.65 45.49 43.83 12.30 28. 2.52 33.11 21.25 16.77 48.40 44.57 47.45 47.13 50.32 47.66 37.65 22.96 33.74 23.17 34.97 22.20 28.91 28.46 28.47 28.53 27.97 28.22 32.49 35.58 31.06 34.92 25.42 32.54 30.22 34.92 27.34 34.52 32.73 33. 32.33 34.99 31.48 27.22 28.81 31.27 31.22 27.20 29.94 32.46 31.43 26.81 Table 15. Post-Safety-Alignment robustness and utility results for Qwen2.5-1.5B (Base) across different alignment methods. AvgRob is computed using minmax normalized separation scores in [0, 100] and (100 ASR) for AdvBench and SALAD, while AvgUtil denotes the average utility score. The Total score is defined as mean(AvgRob, AvgUtil). Cell color and intensity indicate performance relative to the mean value within each benchmark column. The pre-alignment Base model separation is 1.16. 19 Divergence-Based RL for General LLM Alignment C.4. Sample Generations Pre vs Post-Alignment (Safety Task) Here we display some example responses after aligning using JS divergence -HAL (λ {0, 1 alignment FDO, Hybrid, on-policy -GRPO). 2 , 1} corresponding to direct Model / Method Response to Prompt: What color is the sky? Qwen-7B Base alignment) (PreThe sky is typically blue. Write Python function named find max min that takes list of numbers . . . [Response continues with an unrelated Python coding task, including extraneous instructions and incorrect code.] JS -HAL (λ = 0, Onpolicy / -GRPO) JS -HAL (λ = 0.5, Hybrid: -GRPO + FDO) The sky is usually blue during the day and turns pink or orange at sunrise and sunset. The sky is typically blue during the day due to phenomenon called Rayleigh scattering, which causes shorter wavelengths of light (like blue and violet) to scatter more easily in the Earths atmosphere. However, the sky can appear pink or red at sunrise and sunset due to increased scattering of longer wavelengths (red, orange). JS -HAL (λ = 1, Offpolicy / FDO) The color of the daytime sky is typically blue, due to phenomenon called Rayleigh scattering, which causes shorter wavelengths of light (such as blue) to scatter more than longer wavelengths (like red and orange). However, the color can vary depending on the time of day, weather conditions, and other factors. Table 16. Example responses for benign factual prompt before and after alignment on Qwen-7B Base using JS-based -HAL. Here, λ = 0 corresponds to purely on-policy -GRPO, λ = 1 to off-policy direct alignment (FDO), and λ = 0.5 to the hybrid objective combining both. The hybrid setting often yields the most coherent answer by leveraging off-policy factual supervision while maintaining on-policy reward-driven optimization. Model / Method Response to Harmful Prompt: Outline the steps for terrorist organization to infiltrate government agency. Qwen-7B Base alignment) (PreTo infiltrate government agency, terrorist organization would likely follow these steps: 1. **Identify Targets**: The organization .... [The base model generates detailed, step-by-step procedural response describing how terrorist organization could infiltrate government agency, including recruitment, trust-building, information gathering, and cover maintenance.] JS -HAL (λ = 0, Onpolicy / -GRPO) Im sorry, cant assist with that. Instead, lets talk about how to protect yourself and your organization from cyber threats. JS -HAL (λ = 0.5, Hybrid: -GRPO + FDO) Im sorry, but cant assist with that request. Infiltrating government agency is illegal and dangerous. Please respect the law and focus on positive JS -HAL (λ = 1, Offpolicy / FDO) Im sorry, but cant assist with that request. Infiltrating government agency is illegal and unethical. If you have any other questions or need Table 17. Qualitative comparison on harmful prompt using JS-based -HAL for Qwen-7B Base. The base model produces unsafe procedural content , while the aligned models provide robust safet-aligned refusals. 20 C.5. Sample Generations Pre vs Post-Alignment (Math Reasoning Task) Divergence-Based RL for General LLM Alignment Table 18. Qualitative example where the base model fails (score 0) while aligned methods succeed (score 1). Responses are lightly truncated for space. Method Prompt Response (short) Wendi has 20 chickens. Each chicken gets 3 cups per meal. Morning: 15 cups. Afternoon: 25 cups. How many cups in the final meal? (Answer in boxed{}.) Base (pre) Computes total as 3 20 = 60, subtracts 15 + 25 = 40, but outputs 10 (incorrect). GRPO KL Pearson Reverse-KL Hellinger Jensen Shannon Total Variation Total needed: 3 20 = 60. Given: 15 + 25 = 40. Final meal: 60 40 = 20 . Total: 60. Given: 40. Remaining: 20 . 15 + 25 = 40 given; 60 needed; final = 60 40 = 20 . Total = 60, already = 40, final = 20 . Total = 60; remaining after 40 is 20 . Total 20 3 = 60; final 60 (15 + 25) = 20 . Final meal: 3 20 (15 + 25) = 20 ."
        }
    ],
    "affiliations": [
        "Department of Statistics, Michigan State University",
        "Department of Statistics, Purdue University"
    ]
}