{
    "paper_title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models",
    "authors": [
        "Kirill Borodin",
        "Nikita Vasiliev",
        "Vasiliy Kudryavtsev",
        "Maxim Maslov",
        "Mikhail Gorodnichev",
        "Oleg Rogov",
        "Grach Mkrtchian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation. This paper introduces Balalaika, a novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations, including punctuation and stress markings. Experimental results show that models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks. We detail the dataset construction pipeline, annotation methodology, and results of comparative evaluations."
        },
        {
            "title": "Start",
            "content": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models Kirill Borodina,*, Nikita Vasilieva, Vasiliy Kudryavtseva, Maxim Maslova, Mikhail Gorodnicheva, Oleg Rogova,b and Grach Mkrtchiana aMoscow Technical University of Communication and Informatics bArtificial Intelligence Research Institute 5 2 0 2 7 1 ] . [ 1 3 6 5 3 1 . 7 0 5 2 : r Abstract. This work is still in progress Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation. This paper introduces Balalaika, novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations, including punctuation and stress markings. Experimental results show that models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks. We detail the dataset construction pipeline, annotation methodology, and results of comparative evaluations. Version notice (5 May 2025). This preprint is an earlier version of our work and is provided solely to solicit feedback from the community. revised manuscript has been submitted to an anonymous review venue and will be released after the end of the review period in accordance with conference policies. The code in the repository may differ from the current preprint version."
        },
        {
            "title": "1.1 Russian Language Challenges for TTS",
            "content": "Text-to-speech (TTS) synthesis have advanced dramatically in recent years, enabling machines to produce intelligible and naturalsounding speech from written text. These systems are now widely used in applications such as virtual assistants, audio books and accessibility tools. In addition to TTS, generative models are also used in speech denoising, enhancement, and restoration, where they improve the intelligibility and quality of audio by reducing background noise or reconstructing missing segments. Together, these technologies are changing the way people interact with digital devices, making information more accessible and communication more fluent in variety of environments and use cases. Speech synthesis in the Russian language faces unique challenges due to its complex phonetics, morphology, and syntax. These linguistic features require more meticulous approach to the dataset that will be used by the model. The utilization of raw textaudio pairs alone is insufficient for achieving high-quality speech synthesis. Corresponding Author. Email: k.n.borodin@mtuci.ru First of all, the TTS of the Russian language faces phonetic problems. Specifically, the Russian language possesses rich system of consonants[38], comprising 35 phonemes, including challenging to synthesize hisses([ù],[ü]), whistles([s],[z]), and affricates([ts],[tC]). Their articulation necessitates precise regulation of formants and durations to avoid distortions, such as the metallic sound characteristic of hisses and whistles. Secondly, words in Russian frequently contain clusters of consonant letters([vzgljat] - glance, [zdrafstvUjtjE] - hello)[38], which can disrupt smooth transitions between sounds. In contrast to English, Russian is characterized by free word order[29], yet for more natural pronunciation, some words require lexical stress. Models trained with fixed order may disregard this feature, resulting in \"non-native\" intonation. Next, we address four issues that can be resolved by effective data preprocessing. The initial issue is reduction of vowels [38], process in which vowels in stress-free syllables that undergo change in pronunciation(spelled [moloko], pronounced - [m5l5ko], means - milk), which is critical for natural speech. When training models on data that do not consider reduction, the resulting models produce hypercorrections, but not natural-sounding results. similar phenomenon occurs in consonant devoicing[38], where voiced sound transitions into voiceless one(spelled - [dub], pronounced - [dup], means - oak). The second issue to be addressed is that of mobile stresses[24]. As stresses are not fixed, they have the capacity to modify the lexical([zam@k] - [z@mok], lock castle) and grammatical meaning of word([g@l@va] - [gol@v1], head heads). Some stresses do not obey the rules, making the use of contextual LLM-based morphological analyzers necessary. In such cases, the use of rules and dictionaries becomes ineffective. The third problem is related to the normalization of texts, which brings syntactic and prosodic challenges. Firstly, morphology is filled with 6 cases, 3 genera and 2 numerals [30]. Numerals and compound words become difficult to convert into text, as this step would require full syntactic analysis to determine the morphological properties of the word and its relations within the sentence. Secondly, the huge number of abbreviations and acronyms require deep semantic knowledge of the Russian language. Thirdly, due to intercultural interaction, the language is endowed with large number of borrowed words, mainly from English, which causes lack of consistency with the rules of the language. The fourth problem is the monotony of recordings. The most straightforward method of obtaining speech synthesis datasets is from audiobooks; however, this approach can degrade TTS quality, as intonations characteristic of audiobook narration are distinct from natural human speech[41], compounding the synthesis issues. When using authentic spontaneous speech, transcription is necessary. significant challenge at this stage is the complex punctuation. High-quality punctuation has notable impact on the intonational and prosodic components of the generated speech."
        },
        {
            "title": "1.2 Motivation",
            "content": "Despite rapid advancements in multispeaker speech synthesis systems, these systems continue to suffer from the fundamental limitations outlined in Section 1.1. Furthermore, the pre-training of several models becomes challenging due to the insufficiency of their Russian vocabularies, lacking stress information, which is crucial component in speech synthesis. The training of custom models poses significant challenges, as there is lack of publicly available datasets comprising hundreds of hours of high-quality live speech data with comprehensive annotations. Addressing these challenges, we have initiated the construction of high-quality Russian speech dataset, aiming to enhance the quality of our datasets and to advance the field."
        },
        {
            "title": "1.3 Key Contributions",
            "content": "In this paper, we present Balalaika, dataset collected from open sources, consisting of high-quality studio recordings of conversational Russian speech. The dataset was annotated using state-of-theart models, resulting in high annotation accuracy and consistency, making it suitable for training speech synthesis and other generative systems. The dataset is available for personal, non-commercial use1. The paper is organized as follows: the second section describes our data preprocessing pipeline in detail, the third section describes the evaluation items and metrics, the fourth section discusses the results, the fifth defines the limitations of our work and the sixth section concludes."
        },
        {
            "title": "2.1 Data collection strategy",
            "content": "The first and most important step is to determine the method of data acquisition. For this purpose it was decided to use the service Yandex Music. The albums with podcasts were used to form the dataset, since this type of compilations contains high-quality conversational speech. In terms of quality, the albums were selected with the voice recording corresponding to studio-quality recording with minimal noise, reverberation, and other external sounds. key criterion was the naturalness of intonation, as the dataset consists primarily of spoken speech, which addresses one of the issues discussed in Sec. 1.1. From each source, three randomly selected recordings were reviewed, and manual decision was made to include recordings from the album in the dataset. This was done to initially filter out data of too low quality. We expect that such filtering may not be sufficient, so further filtering was done using more objective methods (see Section 2.3) 1 https://github.com/mtuciru/balalaika"
        },
        {
            "title": "2.2 Audio cutting",
            "content": "Subsequent to data collection, we have obtained large amount of multi-hour recordings. However, it is impossible to directly input such extensive data into models due to its considerable length. One potential solution involves segmenting the data into smaller units, such as fragments of 15 s, prior to model input. However, this approach may result in the production of artifacts, particularly at the boundaries of each fragment. To address this challenge, it is essential to cut the audio by words. We employed the Whisper-v3-large model[27] to obtain timestamps. This model is adept at transcribing Russian speech, but, more crucially for our purposes, it generates timestamps during transcription. At this stage, our primary focus is not the accuracy of recognition but rather the accuracy of timestamps. Subsequent to acquiring phrase-level timestamps, we aggregate them to obtain phrases of the maximum possible length, which is less than 15 seconds. To mitigate the impact of truncation on the final syllable of each word, 0.15-second adjustment is applied to the end of each timestamp."
        },
        {
            "title": "2.3 Audio Separation",
            "content": "As mentioned in the sec. 2.1, records containing minimal or no noise were selected. To enable the utilization of the dataset in various configurations, it was determined that the dataset be divided into four parts based on the data quality. The NISQA-S[12] model, an optimized version of the original NISQA[22] metric, was utilized for quality assessment and dataset splitting. The evaluation was conducted on the predicted Mean Opinion Score(MOS) according to the established thresholds: Part 1 (High Quality): OS > 4.2 Part 2 (Medium Quality): 3.5 OS 4.2 Part 3 (Medium-Low Quality): 3 OS < 3.5 Samples with OS < 3 were excluded from this study. Throughout the paper, we refer to these as our 1st p., ours 2nd p., and ours 3rd p.. Following the process of splitting, the problem that is characteristic of dialog speech remains the existence of samples comprising multiple speakers simultaneously. Such recordings should not be utilized for training speech synthesis models, as the generation of multiple voices within single phrase is not target of the training process. To address this challenge, the PyAnnotate[4, 25] model was employed to filter the audio recordings. Recordings comprising multiple speakers were classified into the third category. The third category was designated for the pre-training of models, with the objective of acquiring basic knowledge about speech generation. The second and first categories were designated for the primary stage of model training."
        },
        {
            "title": "2.4 Transcription",
            "content": "The creation of dataset suitable for speech synthesis tasks requires high-quality text annotation. In this study, we annotated large amount of data using automatic methods, leveraging the most accurate ASR model[31] for the Russian language, known to the best of our knowledge. This approach, however, involves the risk of low quality annotation. To evaluate the quality of the resulting transcription and compare it with other TTS datasets, we conducted appropriate experiments, as detailed in Sec. 4. It is important to note that the Figure 1. Visualization of the proposed annotation process. Table 1. Datasets comparison Dataset PUN STR HRS TYPE DeepSpeech [9] GOLOS-C [13] GOLOS-F [13] M-AILABS [5] OpenSTT [34] RuLS [1] RUSLAN [10] MCV [2] SOVA AB [36] SOVA YT [36] SOVA [36] Ours 3rd p. Ours 2nd p. Ours 1st p. 6000 1095 132 46.8 20108 98 31 286 298 17451 101 367 1200 D S,B,SY B S, S, S, NOI 3.273 2.889 3.293 4.149 2.588 3.93 3.827 3.511 3.193 2.888 2.859 3.355 3.565 4.211 COL 3.949 3.86 3.589 3.818 3.661 4.066 4.266 3.859 3.694 3.453 3.846 3.968 4.133 4.51 DIS LOU NMOS UTMOS MOS 95% CI TMR 95% CI 3.4 3.237 2.48 4.012 2.928 3.757 3.698 3.391 3.212 2.854 3.15 3.543 3.749 4.144 3.848 3.313 1.597 3.989 3.191 4.15 4.168 3.755 3.511 3.167 3.442 3.674 3.939 4. 3.397 3.043 1.447 3.966 2.723 3.892 3.788 3.37 3.096 2.632 3.054 3.537 3.839 4.438 2.483 2.393 1.852 2.765 2.033 2.8162 2.934 2.4456 2.215 2.567 2.1883 2.724 2.8115 3.018 2.953 0.078 2.713 0.077 1.477 0.142 3.967 0.114 2.633 0.127 3.8 0.108 4.0490.041 3.875 0.125 2.8 0.073 2.8740.095 2.803 0.086 4.103 0.111 4.265 0.095 4.593 0.08 0.701 0.084 0.76 0.069 0.733 0.072 0.767 0.077 0.759 0.061 0.71 0.0737 0.766 0.063 0.717 0.072 0.74 0.071 0.705 0.082 0.76 0.069 0.75 0.061 0.767 0.059 0.751 0.063 Note: This table presents comparison of datasets according to various metrics: the availability of punctuation (PUN) and stresses (STR), the number of hours in the dataset (HRS), and the speech type (TYPE)(spoken (S), dictated (D), synthesized (SY), or audiobooks (B)). It also includes NISQA[22] metrics (NOI, COL, DIS, LOU, NMOS), UTMOSv2[3] (UTMOS), manual MOS with 95% confidence intervals (MOS 95% CI), and manual text match rate with 95% confidence intervals (TMR 95% CI), which represents how well the transcription matches the audio. The highest and second-highest scores for each metric are shown in bold and underlined text, respectively. Table 2. Speech Restoration models comparison Model NOI DIS COL LOU NMOS UTMOS MOS 95% CI AR 95% CI Source SEMamba [6] DeepFilterNet3 [32] VoiceRestore [15] MP-SENet [19] MossFormer2 [42] ours 2.8884 3.9307 4.0474 3.1257 3.9409 3.7777 4.1723 3.4525 4.1226 3.9886 3.8354 4.1193 4.0171 4.1842 2.8541 3.6928 3.5067 3.1551 3.7012 3.4800 3. 3.167 3.8473 3.7764 3.4223 3.9491 3.6947 4.1408 2.6316 3.7824 3.6049 3.0314 3.8098 3.5138 3.8723 2.57 2.6667 2.4665 2.2688 2.4914 2.3973 2.4152 2.870.1 2.929 0.103 2.137 0.057 2.944 0.084 30.087 2.953 0.079 3.2625 0.116 N/A 0.881 0.038 0.964 0.017 0.933 0.03 0.9234 0.031 0.949 0.027 0.955 0.039 Note: The table shows comparison of different speech restoration models whose weights and inference code are taken from the original implementations. The table also shows the metrics of the original degraded sample (source) and the SEMamba model trained on the first part of our dataset (ours). The following metrics were used: NISQA[22] (NOI, COL, DIS, LOU, NMOS), UTMOSv2[3] (UTMOS), the manual MOS with 95% confidence intervals (MOS 95% CI), and the accent rate with 95% confidence intervals (AR 95% CI), which represents the percentage of audio that has an accent. The highest and second-highest scores for the metrics are shown in bold and underlined text. GigaAMv2-RNNT model generates plain text results without punctuation and stresses. On the one hand, this is beneficial because there is no need to normalize the text. On the other hand, it is necessary to obtain stresses and punctuation for quality speech synthesis. 2.6 Stress placement and e-normalisation"
        },
        {
            "title": "2.5 Punctuation",
            "content": "GigaAMv2-RNNT does not place any punctuation marks, which is limitation of the model. Punctuation contributes to the quality of speech synthesis(Sec. 4) because it allows us to \"record intonation\" to some extent. The RuPunctBig model[23] was used for punctuation, and it is capable of accurately putting punctuation marks into text. The quality of their placement, as well as the quality of transcription, was evaluated by the annotators in sec. 4. Our approach involves the placement of stresses and process we call e-normalisation, which are notable for effective text-to-speech conversion. While the written form of \"e\" is often simplified to \"e\" the pronunciation varies significantly(e-[E]/[e], e-[8]/[o]). Our task is to identify these variations. To address the challenges of e-normalisation and stress placement, we employed the RuAccent model[24]. key feature of the model is its ability to address the issue of mobile stress (the problem of stress placement in homographs). Table 3. Comparison of speech denoising models trained on different datasets Dataset CSIG CBAK COVL PESQ VISQOL UTMOS STOI SI-SDR test subset 2.9257 2.5061 2.1269 1.4557 3. 2.2493 0.8766 7.5817 DeepSpeech [9] GOLOC-C [13] GOLOS-F [13] M_AILABS [5] RUSLAN [10] OpenSTT [34] RuLS [1] MCV [2] SOVA AB [36] SOVA YT [36] SOVA [36] ours 3rd p. ours 2nd p. ours 1st p. 3.6777 3.7147 3.7595 3.778 3.5295 3.6885 3.7395 3.7848 3.7796 3.6783 3.7429 3.689 3.7457 3.8558 2.799 2.6837 2.6625 3.0845 2.8934 2.9534 2.9119 3.1162 3.0878 2.9704 2.8541 3.0406 3.0765 3. 3.1543 3.19 3.1697 3.2868 3.0244 3.195 3.2041 3.2285 3.2584 3.1876 3.2269 3.1838 3.2396 3.3396 2.5406 2.5039 2.3501 2.7212 2.4762 2.6338 2.5841 2.6511 2.6445 2.6104 2.5818 2.6107 2.6299 2.7225 3.9567 3.9735 3.9462 4.0122 3.804 3.9311 3.9811 4.0045 4.0324 3.9744 3.987 3.9454 3.974 4.0364 2.4055 2.4088 2.3801 2.5944 2.6214 2.4247 2.5374 2.5197 2.4197 2.4145 2.471 2.4628 2.4498 2.6138 0.9231 0.9218 0.9169 0.9302 0.8783 0.9238 0.9250 0.9261 0.9276 0.9257 0.9244 0.9241 0.9253 0.9314 8.5772 8.3737 7.7856 8.2566 5.8334 8.051 8.717 8.1128 8.7443 8.1703 8.3173 8.2769 8.0273 8. Note: The table shows the results of training the SEMamba[6] model on different datasets in denoising setup. The Dataset column indicates the dataset used for training each model. Comparisons were made using the following metrics: prediction of signal distortion (CSIG)[11], prediction of background intrusiveness (CBAK)[11], prediction of overall speech quality[11], perceptual evaluation of speech quality (PESQ)[28], short-time objective intelligibility measure (STOI)[37], virtual speech quality objective listener (VISQOL)[7], UTMOSv2[3] (UTMOS), and scale-invariant speech distortion ratio (SI-SDR). The highest and second-highest scores for the metrics are shown in bold and underlined text. Table 4. Comparison of speech synthesis models trained on different datasets Dataset NOI COL DIS LOU NMOS TTS MOS UTMOS MOS 95% CI IntMOS 95% CI CER SIM DeepSpeech [9] GOLOC-C [13] GOLOS-F [13] M_AILABS [5] RUSLAN [10] OpenSTT [34] RuLS [1] MCV [2] SOVA AB [36] SOVA YT [36] SOVA [36] ours 3rd p. ours 2nd p. ours 1st p. 2.6946 2.2051 3.4367 3.7924 4.0193 2.1172 3.9336 4.0167 2.9993 2.0664 2.3487 2.4276 3.3053 4.2002 3.6406 3.6959 3.6482 3.3723 4.3278 3.7438 4.2351 4.2227 4.1601 3.0234 4.0085 4.2440 4.3606 4.5799 2.6819 2.1849 2.4353 3.6128 3.7862 2.5426 3.5956 3.6504 2.9049 1.5248 2.8124 3.8387 4.0656 4.3504 3.5315 1.9105 1.4922 3.8226 4.2483 3.1770 3.9382 3.9415 3.5548 2.5048 3.4032 3.5028 3.9652 4. 2.7796 1.7415 1.1814 3.5301 3.7438 2.4066 3.7876 3.7557 2.9691 1.4161 2.7414 2.6816 3.6308 4.4843 2.4468 1.9672 1.6324 3.0321 1.9134 1.6217 2.9269 3.2095 2.5572 1.5147 2.2699 2.708 3.5002 3.5703 1.4871 0.9203 0.8445 2.30653 2.12968 1.6762 2.1068 2.12294 1.49 0.8036 1.43 2.17835 2.48488 2.73848 1.597 0.078 0.615 0.064 0.036 0.024 2.962 0.052 3.253 0.068 1.259 0.058 2.75 0.094 2.749 0.062 1.354 0.063 0.979 0.016 1.336 0.039 2.446 0.079 3.256 0.095 3.618 0.083 0.62 0.098 0.034 0.04 0.013 0.026 2.208 0.071 3.182 0.094 0.135 0.053 2.142 0.086 2.462 0.094 0.156 0.049 0.018 0.017 0.196 0.062 1.877 0.094 2.377 0.095 2.532 0.09 0.7693 0.9999 1 0.0908 0.0496 0.96 0.1003 0.238 0.9112 0.9998 0.8714 0.2704 0.171 0. N/A N/A N/A 0.83 N/A N/A N/A 0.7585 N/A N/A N/A 0.7518 0.7807 0.7919 Note: The table shows the results of training the VITS[14] model on different datasets in text-to-speech setup. The Dataset column indicates the dataset used for training each model. Comparisons were made using the following metrics: NISQA[22] (NOI, COL, DIS, LOU, NMOS), NISQA-TTS[21] (TTS MOS), UTMOSv2[3] (UTMOS), the manual MOS with 95% confidence intervals (MOS 95% CI), the manual intonational MOS with 95% confidence intervals (IntMOS 95% CI), the character error rate (CER), and the cosine similarity (SIM) between the generated and target speaker embeddings. The highest and second-highest scores for the metrics are shown in bold and underlined text."
        },
        {
            "title": "2.7 Grapheme to Phoneme translation",
            "content": "2."
        },
        {
            "title": "Speaker clustering",
            "content": "this For stage, we employed the common approach with transformer[40] training on the seq2seq task, utilizing publicly available data with high-quality IPA annotation[39]. This enabled the G2P model to be trained to accurately handle vowel reduction and consonant devoicing."
        },
        {
            "title": "2.8 Audio-Text alignment",
            "content": "For many TTS models, duration predictor that trains separately from the model and uses phoneme length data for training remains essential. To obtain the appropriate annotation, common solution is the Montreal Forced Aligner[20]. We also used this tool. The corresponding model was trained on each part of the dataset. The fully trained model then generated durations for each phoneme. An important step in the development of our dataset was to obtain speaker identifiers. Knowing what kind of speaker is on the current recording, it is possible to develop different multi-speaker speech synthesis systems, e.g. with condition on ID[33], with condition on reference utterance[17]. To obtain this kind of IDs, we clustered the data. To implement clustering, we need features based on which we can implement speaker splitting. To build these features, we used the Sim-AM-ResNet-100[26] model, which was pretrained on VoxBlink2[18] and additionally finetuned on VoxCeleb2[8]. special feature of this model is powerful pretraining, to our knowledge on the largest dataset of its kind, which allows this model to show SOTA-comparable performance and generate high quality embeddings. The first step was per-podcast clustering. We compared the embedding of each record with the centroids of each cluster (average of Table 5. Ablation study of different configurations of our dataset Dataset NOI COL DIS LOU NMOS TTS MOS UTMOS MOS 95% CI IntMOS 95% CI CER SIM ours ours + stresses ours + punctuation ours + stresses + punctuation 4. 4.5158 4.2646 4.3337 4.3456 4.1358 4. 4.2979 4.4175 4.4446 3.3155 3.5472 2. 3.41 0.081 2.305 0.088 0.1347 0.7814 2.73128 3.522 0. 2.48 0.094 0.1291 0.7779 4.1533 4.547 4. 4.4148 4.4326 3.5671 2.63233 3.44 0.079 2.448 0. 0.1123 0.7908 4.2002 4.5799 4.3504 4. 4.4843 3.5703 2.73848 3.618 0.083 2.532 0.09 0. 0.7919 Note: The table shows the results of training the VITS[14] text-to-speech model on different configurations of the first part of our dataset. The Dataset column indicates the dataset used for training each model. Comparisons were made using the following metrics: NISQA[22] (NOI, COL, DIS, LOU, NMOS), NISQATTS[21] (TTS MOS), UTMOSv2 (UTMOS), the manual MOS with 95% confidence intervals (MOS 95% CI), the manual intonational MOS with 95% confidence intervals (IntMOS 95% CI), the character error rate (CER), and the cosine similarity (SIM) between the generated and target speaker embeddings. The highest and second-highest scores for the metrics are shown in bold and underlined text. all embeddings in the cluster). If there are no centroids or the cosine similarity is less than given threshold (in our case threshold=0.8), new cluster is created. The next step was to merge the collected clusters. After the first step we have clusters for each podcast, now we need to merge the clusters for the whole dataset, because the same speakers can be found not only within an album, but also in other albums. Centroids were merged only if the cosine similarity between them was higher than given threshold (in our case, threshold=0.8). Merging was performed until there were no clusters left to merge."
        },
        {
            "title": "2.10 Train-test split",
            "content": "The dataset was divided into training, validation, and test samples using standardized and independent approach for each part of the dataset, following 18 / 1 / 1 ratio. There is no overlap between the parts of the dataset in terms of albums and compilations from which records were taken."
        },
        {
            "title": "3.1 MOS evaluation description",
            "content": "To evaluate the quality of the datasets, we employed combination of automatic and human feedback metrics. To calculate the automatic metrics, we utilized the NISQA model, which calculates the following metrics: Noiseness (NOI), Coloration (COL), Discontinuity (DIS), Loudness (LOU), and Mean Opinion Score (NMOS). It is worth noting that this is not the same NISQA-S that was used to filter the data in the dataset. We used the original model. It has fundamentally different architecture, different weights, and different training data. This approach minimizes bias in these objective metrics. We also employed the UTokyo-SaruLab MOS Prediction System(UTMOS)[3] in all our experiments. The LabelSpeech2 platform was used to calculate the Mean Opinion Score (MOS) through human feedback. Prior to the evaluation, each annotator was instructed on how to score the recordings: \"5\". Perfect studio quality: clear sound without noise, reverb, distortion, robotic voice. Examples: podcasts, studio voice recordings, professional voiceovers. \"4\". Studio quality with artifacts: minor noise, slight reverberation, but speech is clear and sounds very good. Example: recordings from microphone in quiet room, but with background hum or light music. 2 https://github.com/mtuciru/LabelSpeech \"3\". Unprofessional recording: noticeable noise, distortion, poor speech clarity or the presence of any background music. Example: recording on cheap microphone in room with echoing, social media audio. \"2\". Very low quality: severe distortion, noise, normal telephony. telephone conversation with interference, recording Example: with loud background noise. \"1\". Low quality telephony: speech is barely understandable, intermittent sound, humming. Example: old cell phone recording, audio from bad VoIP call. \"0\". Non-speech: white noise, silence, unrecognizable sounds. Example: fan noise file with no speech, broken data. Since we have previously stated that the quality punctuation, accents, and phonemes affect the quality of synthetic speech, it is necessary to experimentally validate this. To evaluate the quality of intonation and prosodic features of synthetic speech, we decided to use the MOS for intonation evaluation (IntMOS) through human feedback. Before the evaluation, each evaluator received the following instructions on how to evaluate the audio quality: \"5\". Speech sounds like real person in normal conversation situation: correct stresses, logical pauses, no signs of diction or clichés. \"4\". The intonation is dictationor audiobook-like. The speech is recognized as human speech, but the narration is similar to dictation or audiobook, there may be excessive expressiveness and some unnatural pauses, there are no errors in stresses. \"3\". The intonation is indeterminate. It is difficult to tell whether it is human or machine: there may be individual mistakes in stresses and pauses, the structure of the intonation is broken. \"2\". Mostly robotic intonation. Speech sounds synthetic, there are often errors in stresses and pauses, intonation is unfamiliar, but there are attempts to imitate human. \"1\". Clearly unnatural intonation. Intonation is clearly artificial, most of the stresses and pauses are wrong, there is no likeness to human speech. \"0\". Speech is unintelligible. It is impossible to understand what is being said, intonation cannot be evaluated due to low comprehensibility or loss of meaning. Each audio evaluated using human-feedback metrics (described in Sec. 3.3, 3.2) was reviewed by multiple annotators at least seven times. Each annotator is native speaker of Russian. The final score for each audio was determined as the median value, and the final score for the example was calculated as the mean with 95% confidence interval(CI), which was calculated using the following equation: CI = n (1) where = 1.96 is the Z-score for 95% confidence interval, is the standard deviation and is the number of samples."
        },
        {
            "title": "3.2 Datasets comparison",
            "content": "The metrics used to evaluate the quality of the datasets were NISQA (on the whole dataset), classical MOS, and Text Match Rate (TMR) the percentage of text to audio matches. For the MOS and TMR evaluation, samples from each dataset were taken with sample size of 200 examples. In addition, we provide information regarding the dataset, including the availability of punctuation (PUN), stresses (STR), the number of hours (HRS), and speech type (TYPE). We identified four types of speech: spoken (S), dictated (D), synthesized (SY), and audiobooks (B). The following speech datasets were used in the comparison: Deep Speech for russian language[9]; GOLOS[13] Crowd(GOLOS-C) and Farfield(GOLOS-F); M-AILABS speech dataset russian subset[5]; Open STT[34]; Russian LibriSpeech(RuLS)[1]; RUSLAN[10]; SOVA[36] RuAudiobookDevices(SOVA AB), RuYoutube(SOVA YT), Devices(SOVA D); Mozilla Common Voice Corpus 21.0 (MCV)[2] 3."
        },
        {
            "title": "Speech restoration comparison",
            "content": "To demonstrate the applicability of our dataset to different speech generative tasks, we trained the SEMamba model[6] on our dataset and compared it with other state-of-the-art models. For evaluation, we used NISQA metrics on the whole dataset. MOS was still used for human-feedback evaluation, as well as Accent Rate (AR), which is the percentage of audio that has an accent. The rationale behind this metric is that models trained in other languages are often not well adapted to foreign pronunciation, potentially resulting in artificial accents. For MOS and AR, we created samples from SOVA RuYoutube size 200. The following models were used for analysis: SEMamba[6] and trained on our dataset), DeepFilterNet3[32], (original VoiceRestore[15], MP-SENet[19], MossFormer2[42]. SEMamba was trained with batch size of 20 for 100000 steps and the Adam optimizer with learning rate of 0.0005. combination of noise from the MUSAN[35] dataset and Room Impulse Response (RIR)[16] was used during training to degrade the audio quality. 3."
        },
        {
            "title": "Speech denoising comparison",
            "content": "To additional demonstrate the applicability of our approach in generative tasks, we conducted experiments in the speech denoising setup. For each dataset, we trained SEMamba from scratch using the Adam optimizer with learning rate of 5 104, batch size 8 for 5 104 steps. We trained the models independently on each dataset considered in this paper. Either sample size of 200 hours was used for training, or the entire dataset if it contained fewer hours. The MUSAN[35] dataset was used for noise, SEMamba was trained with batch size of 20 for 105 steps and Adam optimizer with learning rate of 5 104. combination of noise from the MUSAN[35] dataset and Room Impulse Response (RIR)[16] was used during training to degrade the audio quality. Librispeech. The choice of these datasets is based on their quality. From each dataset 500 records were taken. The test set was formed from the test subsets of these datasets in such way that there was no data leakage between the training and test sets. In accordance with the methodology of the original paper[6], the following objective metrics were selected for evaluation: prediction of the signal distortion(CSIG)[11], prediction of the background intrusiveness(CBAK)[11], prediction of the overall speech quality[11], perceptual evaluation of speech quality(PESQ)[28], short-time objective intelligibility measure(STOI)[37]. Along with this, we used virtual speech quality objective listener(VISQOL)[7] to evaluate audio quality. To evaluate the signal distortion ratio, we used scale invariant speech distortion ratio(SI-SDR)."
        },
        {
            "title": "3.5 TTS comparison",
            "content": "To illustrate the impact of our dataset on speech synthesis systems, we utilized the TTS VITS[14] trained on all datasets considered in this paper. In addition, we performed an ablation study to determine how additional annotations such as stresses and punctuation affect the quality of speech synthesis. Samples of the same size as for the speech restoration experiments were used for training. Training was performed using the ADAM optimizer with learning rate of 104 and batch size 32 for 105 steps. Classical NISQA and NISQA TTS[21](TTS MOS), which was developed specifically for objective evaluation of synthesized speech quality, were also used for evaluation. Character Error Rate(CER) was used to evaluate pronunciation quality. GigaAMv2-RNNT was used for transcribing, and the Damerau-Livenstein distance was used to compute the metric. Some datasets contained speaker annotations, so we decided to evaluate models trained on such datasets to see how well they could capture the acoustic features of the target speaker. For this purpose, the cosine similarity(SIM) between the embeddings of the generated recording and the embeddings of the target speaker was computed. The calculation of the embeddings was performed using Sim-AM-ResNet-100. The subjective metrics in our case were classical MOS and Intonation MOS. For evaluation by objective metrics, 2000 texts were randomly selected from the test sets of our datasets. These texts were used for synthesis by all trained models. For evaluation by subjective metrics, 200 texts were randomly selected from these 2000 texts."
        },
        {
            "title": "4 Results and Discussion",
            "content": "Tables 1, 2, 3, 4, 5 illustrate the highest and second-highest scores for the metrics in bold and underlined text, respectively. As shown in Table 1, the first part of our dataset outperforms all other considered datasets by both objective metrics and subjective metrics. Datasets such as M_AILABS, Russian LibriSpeech, and RUSLAN also have noticeably good quality that is comparable to the second part of our dataset. Importantly, in terms of the subjective MOS metric, all three parts of our dataset perform better than other datasets. Additionally, we would like to highlight the Text Match Rate metric, which we have developed. All datasets tend to have similar values, and the second part of our dataset has one of the best annotation qualities. This makes our annotation approach applicable and quite accurate, in conjunction with manual annotation. notable feature of our dataset is the inclusion of stresses. To evaluate the obtained models, we formed test set from the following datasets: our all three parts, M_AILABS, RUSLAN, Russian Table 2 presents the findings from the comparison of speech restoration models applied to samples from the Sova RuYoutube dataset. This table shows the metrics for the source dataset (Source), speech restoration models, whose weights are taken from official implementations, and the results of training the SEMamba model on our dataset. Our experimental results demonstrate that utilizing our dataset for training can yield superior outcomes. This underscores the significance of datacentric approach in training neural networks. This evaluation may be subject to potential biases due to the fact that the original models were not trained on Russian and therefore may not show the same quality results. Therefore, we compared SEMamba models trained with fixed parameters on the considered datasets on the task of speech denoising, table 3. Such comparison allowed us to analyze the impact of each dataset on model training in more detail. As the results show, training on the first part of our dataset shows the best performance on most metrics and is comparable to UTMOS and STOI. Combined with this, the denoiser trained on the second dataset also shows competitive results. These results suggest that our higher quality dataset allows to train more powerful generative models such as denoisers. Table 4 shows the results of training VITS on different datasets. It is important to note that the model trained on the first part of our data shows the best performance on objective MOS-like metrics among all other models. The same model also performs better on subjective MOS. However, in terms of intonational MOS metrics, the model ranks only second, behind the model trained on the RUSLAN dataset. We associate this with two possible reasons: the model that was trained on our data is not fully trained; modeling intonation in single speaker setup is much easier. In terms of CER metrics, the model is close to the one trained on M_AILABS, which is also multispeaker. We note that these metrics show that our dataset has text annotation quality comparable to manual or predefined (in the case of audiobooks) annotation. We would also like to mention the results of measuring the cosine similarity between speaker embeddings. The results of our model are better than on MCV and only slightly worse than on M_AILABS, which allows us to assess the performance of our approach to speaker ID mining. The model trained on the second part of our dataset shows quite good results. Furthermore, in the 4 table, there are models with extremely low values for many metrics. We assume that this is due to the fact that such datasets are not suitable for training speech synthesis models. The data in them is of too low quality, has bad intelligibility, and large amount of noise and distortion. It is difficult task to train models capable of generating intelligible and high quality speech on them, which is serious limitation. Finally, we performed an ablation study on how the additional annotations of punctuation and stresses affect the quality of speech synthesis, table 5. We can see that these annotations do affect the quality of synthesis - models trained with these annotations show the best performance on all metrics used. Moreover, the second best results always belong to the experiments with additional annotation, they are not found for the model trained without punctuation and stresses. Although the gap does not seem too large, we believe that it will be larger when the model is trained until convergence, as well as when more powerful baselines are used."
        },
        {
            "title": "5.1 Experiment limitations",
            "content": "A limitation of our experiments is that they were all performed in limited setup. The models were not trained until convergence, but were trained for an identical number of steps. Therefore, some models may be underfitted and therefore may not perform well in this particular setup. Further training may have positive effect on the overall metrics, but the same training parameters were chosen to ensure as fair comparison as possible. As mentioned before, the datasets considered in this paper are heterogeneous in terms of speech type (spoken, dictated, synthesized, audiobooks), which may lead to bias when comparing speech synthesis models on the intonation MOS metric, since spoken speech corresponds to \"5\" and dictated and audiobooks to \"4\". However, this paper focuses exclusively on the ability of speech synthesis models to produce natural-sounding, spoken-like speech. Furthermore, as our comparisons have shown, not only the type of data, but also the quality of the data is important for generating natural intonation. Without quality data, it is impossible to produce natural intonation and prosody. Data heterogeneity can also affect the evaluation of speech synthesis models. To evaluate these models, we chose texts from test samples of our dataset, while models trained on other datasets may have seen different kind of data during training. However, it is worth noting that there are models that outperform models trained on parts 2 and 3 of our dataset in terms of objective metrics. This behavior allows us to assume that this type of bias is not as significant as the bias associated with data quality. This allowed us to obtain better quality results. Regarding the heterogeneity of the data, we should also note that only two other datasets than ours among the considered datasets have speaker markup. Moreover, most of them do not have such markup at all, and Russian Libri Speech and RUSLAN are single-speaker datasets. Modulation of prosody, intonation, and sound quality in single-speaker set will be easier than in multi-speaker set under the same training conditions. However, despite these difficulties, the speech synthesis models trained on the first part of our dataset show better quality, which also allows us to evaluate the effectiveness of our approach."
        },
        {
            "title": "5.2 Work Limitations",
            "content": "The dataset cannot be distributed because its contents are protected by intellectual property laws. However, the dataset may be obtained and used for personal non-commercial or non-commercial research purposes."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this paper, we presented new corpus of high-quality studio speech Balalaika, which has textual annotation with punctuation and stresses. comparative analysis showed that our dataset is the best among similar Russian-language datasets. Experiments related to speech synthesis demonstrated that using our dataset, even with small model like VITS, leads to easier learning and better quality. Experiments on speech denoising and restoration models also show that systems trained on our higher quality dataset outperform models trained on other resources. Additionally, we have demonstrated that the datacentric approach to model creation is of paramount importance."
        },
        {
            "title": "References",
            "content": "[1] Russian librispeech (ruls) dataset. https://openslr.org/96/, 2021. [2] R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Henretty, R. Morais, L. Saunders, F. Tyers, and G. Weber. Common voice: massively-multilingual speech corpus. In N. Calzolari, F. Béchet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 42184222, Marseille, France, May 2020. European Language Resources Association. ISBN 979-1095546-34-4. URL https://aclanthology.org/2020.lrec-1.520/. [3] K. Baba, W. Nakata, Y. Saito, and H. Saruwatari. The t05 system for the VoiceMOS Challenge 2024: Transfer learning from deep image classifier to naturalness MOS prediction of high-quality synthetic speech. In IEEE Spoken Language Technology Workshop (SLT), 2024. [4] H. Bredin. pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe. In Proc. INTERSPEECH 2023, 2023. [5] I. Celeste. The m-ailabs speech dataset, 2019. URL https://github.com/ imdatceleste/m-ailabs-dataset. large free dataset containing nearly 1000 hours of audio across 8 languages for speech recognition and synthesis. [6] R. Chao, W.-H. Cheng, M. L. Quatra, S. M. Siniscalchi, C.-H. H. Yang, S.-W. Fu, and Y. Tsao. An investigation of incorporating mamba for In 2024 IEEE Spoken Language Technology speech enhancement. Workshop (SLT), pages 302308, 2024. doi: 10.1109/SLT61566.2024. 10832332. [7] M. Chinen, F. S. C. Lim, J. Skoglund, N. Gureev, F. OGorman, and A. Hines. Visqol v3: An open source production ready objective speech and audio metric. In 2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX), pages 16, 2020. doi: 10.1109/ QoMEX48832.2020.9123150. [8] J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker In Interspeech 2018, pages 10861090, 2018. doi: 10. recognition. 21437/Interspeech.2018-1929. [9] G. Fedoseev. Russian speech recognition system based on https://github. mozillas deepspeech tensorflow implementation. com/GeorgeFedoseev/DeepSpeech, 2017. URL https://github.com/ GeorgeFedoseev/DeepSpeech. Forked from mozilla/DeepSpeech. [10] L. Gabdrakhmanov, R. Garaev, and E. Razinkov. Ruslan: Russian spoIn Speech and Computer, ken language corpus for speech synthesis. pages 113121, Cham, 2019. Springer International Publishing. ISBN 978-3-030-26061-3. [11] Y. Hu and P. C. Loizou. Evaluation of objective measures for speech enhancement. In Interspeech 2006, pages paper 2007Tue3FoP.10, 2006. doi: 10.21437/Interspeech.2006-84. [12] B. Ivan. nisqa-s. https://github.com/deepvk/nisqa-s, 2024. [13] N. Karpov, A. Denisenko, and F. Minkin. Golos: Russian Dataset for Speech Research. In Proc. Interspeech 2021, pages 14191423, 2021. doi: 10.21437/Interspeech.2021-462. [14] J. Kim, J. Kong, and J. Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 55305540. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/kim21f.html. [15] S. Kirdey. Voicerestore: Flow-matching transformers for speech recording quality restoration, 2025. URL https://arxiv.org/abs/2501.00794. [16] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur. study on data augmentation of reverberant speech for robust speech recognition. In ICASSP 2017, pages 52205224, 2017. doi: 10.1109/ICASSP. 2017.7953152. [17] J. Kong, J. Park, B. Kim, J. Kim, D. Kong, and S. Kim. Vits2: Improving quality and efficiency of single-stage text-to-speech with adversarial learning and architecture design. In Interspeech 2023, pages 43744378, 2023. doi: 10.21437/Interspeech.2023-534. [18] Y. Lin, M. Cheng, F. Zhang, Y. Gao, S. Zhang, and M. Li. Voxblink2: 100k+ speaker recognition corpus and the open-set speakeridentification benchmark. In Interspeech 2024, pages 42634267, 2024. doi: 10.21437/Interspeech.2024-1490. [19] Y.-X. Lu, Y. Ai, and Z.-H. Ling. MP-SENet: speech enhancement model with parallel denoising of magnitude and phase spectra. In Proc. Interspeech, pages 38343838, 2023. [20] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger. Montreal Forced Aligner: Trainable Text-Speech Alignment UsIn Proc. Interspeech 2017, pages 498502, 2017. doi: ing Kaldi. 10.21437/Interspeech.2017-1386. [21] G. Mittag and S. Möller. Deep learning based assessment of synthetic speech naturalness. In Interspeech 2020, pages 17481752, 2020. doi: 10.21437/Interspeech.2020-2382. [22] G. Mittag, B. Naderi, A. Chehadi, and S. Möller. Nisqa: deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets. Interspeech 2021, 2021. doi: 10.21437/ interspeech.2021-299. [23] D. Petrov. Rupunct models. https://huggingface.co/RUPunct, 2024. [24] D. A. Petrov. RUAccent: Advanced system for stress placement in Russian with homograph resolution. In Proceedings of the 31st International Conference on Computational Linguistics, pages 66426648, Abu Dhabi, UAE, Jan. 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025.coling-main.444/. [25] A. Plaquet and H. Bredin. Powerset multi-class cross entropy loss for neural speaker diarization. In Proc. INTERSPEECH 2023, 2023. [26] X. Qin, N. Li, C. Weng, D. Su, and M. Li. Simple attention module based speaker verification with iterative noisy label detection. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 67226726, 2022. doi: 10.1109/ICASSP43922.2022.9746294. [27] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv.org/abs/2212.04356. [28] A. Rix, J. Beerends, M. Hollier, and A. Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In ICASSP 2001, pages 749752, 2001. doi: 10.1109/ICASSP.2001.941023. [29] E. V. Rodionova. Word order and information structure in russian syntax. Masters thesis, University of North Dakota, Grand Forks, ND, USA, 2001. URL https://commons.und.edu/theses/4482. [30] A. Rozovskaya and D. Roth. Grammar error correction in morphologically rich languages: The case of russian. Transactions of the Association for Computational Linguistics, 7:117, 2019. doi: 10.1162/tacl_a_ 00251. [31] Salute Developers. Gigaam: the family of open-source acoustic models for speech processing. https://github.com/salute-developers/GigaAM, 2024. Released under the MIT License. Accessed: April 10, 2025. [32] H. Schröter, T. Rosenkranz, A. N. Escalante-B., and A. Maier. DeepFilterNet: Perceptually motivated real-time speech enhancement. In INTERSPEECH, 2023. [33] K. Shen, Z. Ju, X. Tan, E. Liu, Y. Leng, L. He, T. Qin, sheng zhao, and J. Bian. Naturalspeech 2: Latent diffusion models are natural In The Twelfth Interand zero-shot speech and singing synthesizers. national Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=Rc7dAwVL3v. [34] A. Slizhikova, A. Veysov, D. Nurtdinova, and D. Voronin. Russian open speech to text (stt/asr) dataset, 2019. URL https://github.com/snakers4/ open_stt. [35] D. Snyder, G. Chen, and D. Povey. MUSAN: Music, Speech, and Noise Corpus, 2015. arXiv:1510.08484v1. [36] SOVA AI. Sova dataset: Multilingual stt/asr corpus. https://github.com/ sovaai/sova-dataset, 2022. [37] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility prediction of timefrequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language Processing, 19(7): 21252136, 2011. doi: 10.1109/TASL.2011.2114881. [38] O. K. Trubach, D. I. Gorshkova, and L. N. Sklyar. Comparative analysis of phonetic systems of the russian, french and chinese languages. RUDN Journal of Language Studies, Semiotics and Semantics, 14(1):171188, 2023. ISSN 2313-2299. URL https://journals.rudn.ru/ semiotics-semantics/article/view/34176. [39] T. Ylonen. Wiktextract: Wiktionary as machine-readable structured data. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 13171325, 2022. URL https://aclanthology. org/2022.lrec-1.140/. [40] S. Yolchuyeva, G. Németh, and B. Gyires-Tóth. Transformer In Interspeech 2019, page based grapheme-to-phoneme conversion. 20952099. ISCA, Sept. 2019. doi: 10.21437/interspeech.2019-1954. URL http://dx.doi.org/10.21437/Interspeech.2019-1954. [41] W. Zhang, C.-C. Yeh, W. Beckman, T. Raitio, R. Rasipuram, L. Golipour, and D. Winarsky. Audiobook synthesis with longform neural text-to-speech. In 12th ISCA Speech Synthesis Workshop (SSW2023), pages 139143, 2023. doi: 10.21437/SSW.2023-22. [42] S. Zhao, Y. Ma, C. Ni, C. Zhang, H. Wang, T. H. Nguyen, K. Zhou, J. Q. Yip, D. Ng, and B. Ma. Mossformer2: Combining transformer and rnn-free recurrent network for enhanced time-domain monaural speech In ICASSP 2024 - 2024 IEEE International Conference separation. on Acoustics, Speech and Signal Processing (ICASSP), pages 10356 10360, 2024. doi: 10.1109/ICASSP48485.2024.10445985."
        }
    ],
    "affiliations": [
        "Artificial Intelligence Research Institute",
        "Moscow Technical University of Communication and Informatics"
    ]
}