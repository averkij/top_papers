{
    "paper_title": "Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation",
    "authors": [
        "Hyeonho Jeong",
        "Chun-Hao Paul Huang",
        "Jong Chul Ye",
        "Niloy Mitra",
        "Duygu Ceylan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent foundational video generators produce visually rich output, they still struggle with appearance drift, where objects gradually degrade or change inconsistently across frames, breaking visual coherence. We hypothesize that this is because there is no explicit supervision in terms of spatial tracking at the feature level. We propose Track4Gen, a spatially aware video generator that combines video diffusion loss with point tracking across frames, providing enhanced spatial supervision on the diffusion features. Track4Gen merges the video generation and point tracking tasks into a single network by making minimal changes to existing video generation architectures. Using Stable Video Diffusion as a backbone, Track4Gen demonstrates that it is possible to unify video generation and point tracking, which are typically handled as separate tasks. Our extensive evaluations show that Track4Gen effectively reduces appearance drift, resulting in temporally stable and visually coherent video generation. Project page: hyeonho99.github.io/track4gen"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 2 6 1 0 6 0 . 2 1 4 2 : r Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation Hyeonho Jeong1,2,* Chun-Hao P. Huang Jong Chul Ye2 Niloy J. Mitra1,3 Duygu Ceylan1 1Adobe Research 2KAIST 3University College London"
        },
        {
            "title": "Abstract",
            "content": "While recent foundational video generators produce visually rich output, they still struggle with appearance drift, where objects gradually degrade or change inconsistently across frames, breaking visual coherence. We hypothesize that this is because there is no explicit supervision in terms of spatial tracking at the feature level. We propose Track4Gen, spatially aware video generator that combines video diffusion loss with point tracking across frames, providing enhanced spatial supervision on the diffusion features. Track4Gen merges the video generation and point tracking tasks into single network by making minimal changes to existing video generation architectures. Using Stable Video Diffusion [5] as backbone, Track4Gen demonstrates that it is possible to unify video generation and point tracking, which are typically handled as separate tasks. Our extensive evaluations show that Track4Gen effectively reduces appearance drift, resulting in temporally stable and visually coherent video generation. Project page: hyeonho99.github.io/track4gen Figure 1. Motivation. Videos generated by Stable Video Diffusion [5] suffer from appearance drift, while those from our method, Track4Gen, are free from such appearance inconsistency issues. 1. Introduction Diffusion-based video generators [5, 7, 52] are making rapid strides in creating temporally consistent and visually rich video content. This progress marks significant shift, as the unification of generation and control has the potential to transform the traditional workflow of first capturing and then digitally editing video. Despite impressive capabilities, video generators often suffer from appearance drift, where visual elements gradually change, mutate, or degrade over time, causing inconsistencies in the objects. For example, in Fig. 1, we observe the horns of the cow distorting and morphing unrealistically over time, breaking the plausibility of the generated content. This is in striking contrast to humans, who develop sense of appearance constancy as early as infancy through observation and interaction with the world [77]. Work mostly done while Hyeonho Jeong was an intern at Adobe. Unfortunately, appearance drift remains persistent issue in current video models, even with increased training data and more advanced architectures. We speculate that this limitation arises from supervision being based solely on video diffusion loss (i.e., denoising score matching [69]) in the pixel/latent space, without explicit spatial awareness guidance in the feature space. Hence, in this paper, we ask if and how we can empower video diffusion models with appearance constancy by providing additional supervision. We present Track4Gen as spatially aware video generator that receives supervision both in terms of the original diffusion-based objective as well as (dense) point correspondence across frames, which we refer to as tracks. We demonstrate that it is possible to provide such track-level supervision in the diffusion feature space by making minimal architecture changes. Our generated videos do not suffer from degradation of video quality (according to the usual video generation metrics), while being significantly more 1 spatially coherent as the highlight cow in Fig. 1. We train Track4Gen using the latest Stable Video Diffusion [5] as the backbone and evaluate on the publicly available VBench dataset [32, 33]. We report significant improvement in terms of appearance constancy of subjects, both in quantitative and qualitative (i.e., via user studies) evaluations. In summary, we demonstrate that it is possible to upgrade existing video generators, by supervising them with additional correspondence tracking loss, to produce videos without significant appearance drifts, problem commonly encountered in diffusion-based video generators. 2. Related Work Diffusion-based video generation. Building on the success of diffusion models in image synthesis [13, 56], diffusion-based video generators have seen significant advancements [5, 7, 31, 52]. commonly adopted approach is to extend text-to-image models to the video domain by incorporating temporal layers to facilitate interactions across video frames [6, 24, 59]. While some works have adopted cascaded approaches to produce both spatially and temporally high-resolution videos [30, 52, 59, 72, 81, 85], others have utilized lower-dimensional latent space modeling to reduce computational demands [6, 10, 26, 87]. We build on top of one such approach, Stable Video Diffusion (SVD, [5]), which introduces latent image-to-video diffusion model trained on large-scale and curated video data. With advances in generation, systematic evaluation of generation quality has become crucial. Traditionally, metrics such as Frechet Inception Distance (FID, [28]), Frechet Video Distance (FVD, [68]), and CLIPSIM [54] are used. Additionally, comprehensive benchmark suites [32, 33, 73] have been introduced to provide more robust evaluation aligned with human perception. Inspired by such work, we thoroughly evaluate our approach and demonstrate improved video generation quality with respect to both conventional metrics and the recent VBench metrics [32]. Foundational models as feature extractors. Various foundational models such as vision transformers [17] or diffusion-based generators [55] have been utilized as feature extractors for various tasks including semantic matching [18, 27, 46], classification [43], segmentation [71, 76], and editing [21, 23, 36, 66]. There have been efforts to boost their performance by post-processing the feature maps obtained from the pre-trained models, e.g., by upsampling [20, 63]. In recent effort, Yue et al. [80] lift semantic per-frame features from foundational model into 3D Gaussian representation. They fine-tune the foundational model with such 3D-aware features resulting in improved performance in downstream tasks. Similarly, Sundaram et al. [62] fine-tune state-of-the-art foundational models on human similarity judgments yielding improved representations across downstream tasks. In concurrent effort, Yu et al. [79] propose to align the internal features of an image generation model with external discriminative features [50], which results in more effective training of the generator. Our work also enhances the internal feature representation of foundational generation model but with significant differences compared to previous literature. First, unlike most previous work that focus on image level foundational models, we exploit the power of recently emerging video models. Second, instead of post-processing, we enhance the spatial awareness of the intermediate features by training the generator to jointly perform an additional tracking task. We show that this joint training boosts the performance of intermediate features in correspondence tracking, leading to improved video generation quality. Tracking any point in video. The task involves following any arbitrary query point across long video sequence. First introduced by PIPs [25] and later re-framed by TAPVid [14], several methods have emerged in recent years to tackle long-term point tracking. PIPs [25] revisits the classical particle-based representation [58] and introduces MLPbased networks that predict point tracks within an 8-frame window. Subsequent works have improved performance by capturing longer temporal context through advanced architectures [3, 15, 25, 37], as well as by enabling the simultaneous tracking of multiple queries [12, 37]. More recent training-based trackers [12, 38, 45, 75] have achieved remarkable performance by leveraging high-capacity neural networks to learn robust priors from large-scale training data. While high-quality data is crucial for accurate tracking, manually annotating point tracks is prohibitively expensive. Hence, synthetic videos [22] with automatic annotations, have become an alternative and have demonstrated effectiveness in real-world video tracking. An alternative approach is self-supervised adaptation at test time, where tracking is learned without ground-truth labels [35, 67, 70]. In recent study, Aydemir et al. [2] evaluate the effectiveness of several image foundational model features for point tracking both in zero-shot setting as well as with supervised training using low-rank adapter layers. To the best of our knowledge, we are the first to exploit the features of foundational video diffusion model for dense point tracking. 3. Method In this section, we provide comprehensive discussion of the Track4Gen framework. We begin with concise overview of latent video diffusion models (Sec. 3.1). Next, we discuss how video diffusion features relate to temporal correspondences both for real and generated videos (Sec. 3.2). Finally, we detail the design of Track4Gen both in 2 Figure 2. Track4Gen overview. Red-colored blocks represent layers optimized by the diffusion loss Ldiff, while green blocks are optimized by the correspondence loss Lcorr. Blocks colored both red and green are influenced by the joint loss, Ldiff + λLcorr. See text for details. terms of network architecture and the employed supervision signals (Sec. 3.3). An overview is depicted in Fig. 2. dimensional pixel space x1:N 0 tails, we refer to the Appendix of [5]. 0 = D(z1:N ). For further de3.1. Background: Stable Video Diffusion 3.2. Video Diffusion Features Starting from random Gaussian noise, diffusion models aim to generate clean images or videos via an iterative denoising process [29, 60]. This process reverses fixed, timedependent diffusion forward process, which gradually corrupts the data by adding Gaussian noise. While our method is applicable to general video diffusion models, in this paper, we design our architecture based on Stable Video Diffusion (SVD), latent video diffusion model which employs the EDM-framework [39]. The diffusion process operates in the lower-dimensional latent space of pre-trained VAE [41], consisting of an encoder E() and decoder D(). 0 = E(x1:N Given clean sample x1:N 0 pdata(x) of an -frame video sequence, the frames are first encoded into the latent space as z1:N ). Gaussian noise ϵ (0, I) is then added to the latents to produce the intermediate noisy latents via the forward process z1:N 0 +σtϵ, where represents the diffusion timestep, and αt, σt are the discretized noise scheduler parameters. The diffusion denoiser θ is trained by minimizing the v-prediction loss: = αtz1:N EϵN (0,I),tU [1,T] (cid:2) (cid:13) (cid:13)f θ(z1:N , t, c) y(cid:13) 2 (cid:13) 2 (cid:3), (1) min θ where is defined as = αtϵ σtz1:N . In the imageto-video variant of SVD, the condition refers to the CLIP image embedding [54], replacing the typical text embeddings. For the remainder of this paper, we will refer to Eq. 1 as the video diffusion loss Ldiff. 0 Once trained, the diffusion model generates videos by iteratively denoising noisy latent z1:N sequence sampled from pure Gaussian distribution. At each diffusion step, the model predicts the noise in the input latent. Once the clean latent z1:N is obtained, the decoder maps it to the higherT 0 Previous studies have demonstrated that image diffusion models learn discriminative features in their hidden states that are effective for various analysis tasks and propose methods for improving the representation power of such features [11, 74, 78, 79]. Similarly, we argue that while also being powerful, internal representations of pre-trained video diffusion models may not be fully temporally consistent, resulting in appearance drift in generated videos. To better investigate this hypothesis, we first evaluate the long-term video tracking capabilities of U-Net-based video diffusion models [5, 61, 85]. Specifically, we evaluate the effectiveness of the features from each block of the U-Net for the task of point tracking. Given real-world video, we add small amount of noise and extract feature maps from each layer in each block. We perform cosine-similaritybased nearest-neighbor search [49, 64] over these feature maps for given set of fixed query points on the first frame (we use similarity threshold of 0.6 [67] in our experiments). We also perform similar analysis for generated videos where we extract the feature maps corresponding to diffusion steps with small amount of noise. Based on this feature analysis, we make some important observations. Notably, regardless of the model (we analyze both Zeroscope T2V [61] and SVD I2V [5]), we find out that output features from the upsampler layer of the third decoder block consistently yield stronger temporal correspondences, as shown in Fig. 3. Hence, we use this block when extracting features for the remainder of our experiments. Furthermore, when we analyze generated videos and point tracks estimated based on the feature maps (as shown in Fig. 4), we observe that there is correlation between tracking failures that reveal feature-space inconsistencies and appearance drifts that reveal pixel-space inconsisten3 Figure 3. Real-world video tracking using different video diffusion features. Given color-coded query points on the first frame (Leftmost column), we display tracked points on target frames using features from different blocks (right columns). The 13th frame (first row) and 8th frame (second row) are shown as target frames. Full results are available on our page. cies. Hence, we hypothesize that enriching feature consistency can help mitigate such appearance drifts. Next, we introduce Track4Gen where we accomplish this goal by supervising video diffusion models with joint tracking loss. 3.3. Track4Gen Track4Gen aims to utilize point tracking as an additional supervision signal to enhance the spatial-awareness of video diffusion features. Given that we build on top of pretrained video generation model, to retain the prior knowledge and avoid tampering the original features directly, we propose novel architecture change as shown in Fig. 2. Specifically, instead of directly using the raw diffusion features for correspondence estimation, we propose trainable refiner module Rϕ, which is designed to refine the raw features by projecting them into correspondence-rich feature space. The refined features, which are spatially-aware, are then both used to estimate point tracks with an explicit supervision as well as feeding back to the generation backbone. We empirically find out that this design is more effective compared to fine-tuning the original model with no Figure 4. Generated video tracking using video diffusion features. Tracks based on diffusion features are annotated on the generated videos. Track4Gen generates more consistent results. refinement module (see Sec. 4.2). 0 Given an -frame video sequence x1:N , its corresponding latent z1:N , and diffusion timestep t, in order to train Track4Gen we continue to utilize the standard diffusion training loss as defined in Eq. (1), where we adopt the velocity prediction objective [5, 39, 57] for Ldiff. To enable tracking supervision, we assume access to dense set of point trajectories Ω = {(xi, xj)} across frames where point xi in frame corresponds to matching point xj in frame and vice versa. Given the corresponding noisy video latent sequence z1:N , we first extract raw diffusion features as the hidden states h1:N RN HW from specific block bk within the U-Net, where bk is set to the upsampler layer of the third decoder block (see Sec. 3.2). We then pass these features through the refiner module to obtain the refined feature map We sample query point xi = Rϕ(h1:N ). along with its ground-truth trg from the correspondence set Ω. Given the (xq) R11C and the target feature RHW C, we calculate the cost volume target point xj query point feature map RHW 1 as follows: 1:N S(p) = cos-sim(h (xq), (p)), (2) where cos-sim denotes cosine similarity. The predicted target point ˆxtrg is then determined using the differentiable soft-argmax operation: ˆxtrg = (cid:80) pΩ S(p) xp (cid:80) pΩ S(p) , (3) where Ω = {p : (cid:13) (cid:13)xp xpmax point prediction can be expressed as ˆxtrg = ξ(xi and the predicted tracklet for xi is given by Txi (cid:13) (cid:13)2 R}1. Thus, the target ), = {ˆxn : q, j, 1:N 1The feature maps have resolution of 44 81 for an input video resolution of 320 576, and we set = 35. 4 Figure 5. Image-to-video generation results of the original SVD and Track4Gen. Please visit our project for full video view. 1:N ˆxn = ξ(xi ), = 1, ..., }. Finally, the correspondence loss Lcorr is computed using the Huber loss LH [34]: q, n, 1:N Lcorr(h , Ω) = (cid:88) (xi q,xj trg)Ω LH (ξ(xi q, j, 1:N ), xj trg) (4) When training Track4Gen, we initialize the refiner module as an identity mapping to fully leverage the prior of the base model at the start of finetuning. To re-route the refined features to the backbone generator, we introduce trainable zero convolution layer [83], denoted as ζψ. While the diffusion loss Ldiff back-propagates to all the blocks of the video diffusion model, we detach the gradients of before passing into ζψ such that refiner module can solely focus on acquiring the correspondence prior. Hence, given that the output of block bk is h1:N , the input to the subsequent block bk+1 is computed as h1:N +ζψ(stop-gradient(Rϕ(h1:N ))). Fig. 2 visualizes this architecture design, with red and green colors indicating the objective that optimizes each module. 1:N 4. Experiments 4.1. Implementation Details To train Track4Gen, we construct training dataset consisting of 567 video-trajectory pairs, with each video having resolution of 320 576 and duration of 24 frames. Since 5 no real-world video with (dense) ground-truth trajectory annotations exist at the time of this work, we utilize optical flow to generate trajectory annotations. For details, see Sec. A.1. key challenge is the need for accurate video segmentation maps to ensure balanced distribution of trajectory points between foreground objects and the background [14]. To address this, we utilize public video datasets paired with ground-truth segmentation maps [8, 19, 44, 51, 53], where we split longer videos into 24-frame segments. We use Stable Video Diffusion (SVD) image-to-video pretrained checkpoints2 as the base video generator. Our proposed refiner module consists of eight stacked 2D convolution layers and is attached to the third decoder block of the SVD UNet. The refiner module preserves the shape of the hidden states throughout and is initialized as the identity mapping. Further details are provided in the Appendix (see Sec. A.2). We finetune this enhanced video generator architecture for 20K steps with our joint loss Ldiff +λLcorr, where λ is set to 8. Rather than finetuning the entire model, we finetune only the temporal transformer blocks, the refiner module Rϕ, and the zero convolution ζψ. In each iteration, we sample 512 correspondence pairs from the precomputed trajectories. We use the AdamW optimizer [48] with learning rate of 1e5, β1=0.9, β2=0.999, and weight 2https : / / huggingface . co / stabilityai / stable - video-diffusion-img2vid-xt Figure 6. Qualitative ablation on video generation. Track4Gen is compared with finetuned SVD (SVD finetuned on the same training videos without any correspondence supervision) and Track4Gen trained without the Refiner module. decay of 1e2. We train the model on 4 H100 GPUs with total batch size of 4. For sampling new videos, we apply the default settings using 30 steps with the EDM sampler [39], motion bucket id =127, and fps=7. Table 1. Quantitative comparison on video generation performance. We compare Track4Gen to the pre-trained SVD as well as finetuned SVD on the same dataset (finetuned SVD). We also train variant of Track4Gen without the refiner module. All videos are generated at 320x576 resolution, except SVD (576p) which operates at 576x1024 resolution. SVD finetuned SVD Track4Gen w/o refiner Track4Gen SVD (576p) Subject Consistency 0.9535 0.9665 0.9506 0.9746 0.9576 Temporal Flickering 0.9464 0.9800 0.9725 0.9806 0.9478 Motion Smoothness Quality 0.6648 0.6766 0.6653 0.6835 0.6812 0.9774 0.9909 0.9791 0.9921 0.9795 Imaging Video-Image Alignment 0.9539 0.9771 0.9614 0.9814 0. FID FVD 29.0 27.0 27.1 26.6 - 776 735 718 724 - 4.2. Track4Gen for Video Generation We evaluate Track4Gen for the image-to-video generation task via series of experiments using multiple datasets, automated metrics, and human evaluations. Evaluation Setup. We compare Track4Gen against the original SVD (SVD) [5], as well as version of SVD that is finetuned on the same videos as Track4Gen (finetuned SVD). Furthermore, we train variant of Track4Gen without the refiner module. For VBench metrics [32], evaluations are conducted on the VBench-I2V dataset, containing 355 diverse images. FID and FVD are measured using the DAVIS [53] dataset as reference. We generate 24-frame videos conditioned on each input image. 6 Automatic metrics. We first report five key metrics from VBench [32]: (1) Subject Consistencyassesses subject appearance consistency of the video by comput- (2) Temporal ing the similarity of DINO [50] features. Flickeringdetects temporal consistency by taking static frames and calculating the mean absolute difference across frames. (3) Motion Smoothnessmeasures smoothness of motion, and how well it adheres to real-world physics, using video frame interpolation priors [47]. (4) Image Qualityevaluates distortions (e.g., noise, blur) using pretrained, multi-scale image quality predictor [40]. (5) VideoImage Alignmentmeasures alignment between the subject in the input image and in the generated video using DINO features. We additionally report FID [28] and FVD [68]. To further assess temporal consistency, CLIPSIM [54] and LPIPS [84] are reported in the Appendix (see Sec. B). Human evaluation. We further evaluate Track4Gen against baselines through user study. We ask 64 participants to compare our results with randomly selected baseline. We ask the users to evaluate how consistent main objects appear across the frames in generated video as well as how natural the depicted motion is. We provide further details of the user study in Sec. A.3 of the Appendix. Qualitative results. Qualitative comparisons with the base SVD are shown in Fig. 5. As illustrated, Track4Gen generates videos with strong appearance consistency, avoiding issues of appearance drift. In contrast, videos produced by the original SVD exhibit noticeable inconsistencies: the sheeps head (row 1) mutates, the planes wing (row 2) Figure 7. Qualitative comparison of Track4Gen and baselines for real-world video tracking. The leftmost column displays query points in the first frame, while the following three columns show tracking results using features from each model. Table 2. Quantitative zero-shot feature comparison on video tracking benchmarks. Track4Gen features are compared to the features of SVD [5], ZeroScope [61], and RAFT [65]. For all the metrics, higher values indicate better performance. (a) Identity preservation (b) Motion naturalness Figure 8. User study results. Our study shows that Track4Gen better preserves object identity and produces more natural motion. Method ZeroScope SVD Track4Gen RAFT DAVIS-480p (24-frame) OA 67.0 79.7 85.8 - δx avg 46.2 42.4 69.7 73. AJ 39.4 36.4 56.5 - BADJA (24-frame) δseg δ3px 2.8 27.5 2.9 26.2 7.7 52.3 8.7 54.8 DAVIS-480p (whole duration) δx AJ OA avg 27.8 59.5 37.2 26.5 70.1 35.4 40.2 78.4 58.9 66.7 - - BADJA (whole duration) δseg 19.9 19.4 40.4 45.0 δ3px 2.0 2.2 5.0 5.8 shows unnatural transitions, and the cars (row 3) disappear. Further comparisons with finetuned SVD and Track4Gen without the refiner module are shown in Fig. 6 and highlight the superior visual coherence of the proposed Track4Gen. Quantitative results. As shown in Tab. 1, our method achieves the highest scores across all 5 metrics from VBench, along with the lowest FID and second-lowest FVD values, outperforming the base SVD by substantial margins. Fig. 8 provides the user study results where the majority of the participants agreed that Track4Gen is superior both in terms of identity preservation and naturalness of motion. 4.3. Track4Gen for Video Tracking We evaluate Track4Gens capability to track any point in real videos by adding small amount of noise to the input video [64] and passing it through the video denoiser θ to extract feature maps. We first compare tracking results with such features against other raw features [5, 61, 65] in Sec. In Sec. 4.3.2, we utilize Track4Gens features in 4.3.1. test-time optimization method [67] and compare to both self-supervised and fully supervised video trackers. 4.3.1. Zero-shot Feature Comparison We evaluate the precision of predicted tracks using the features from Track4Gen, the original SVD model (SVD), and RAFT [65]. We also test another text-to-video model, ZeroScope T2V [61], to demonstrate how raw features from pre-trained video generators typically work out of the box. For RAFT, tracking is achieved by chaining optical flow displacements, while the others use nearest neighbor matching between its encoded features. Datasets. We use TAP-Vid DAVIS [14] and BADJA [4] as benchmark datasets. Additionally, we include two shorter benchmarks, DAVIS (24-frame) and BADJA (24frame), which focus on the first 24 frames with query and target points within this range. Details on encoding long videos using pretrained video diffusion models are in Sec. A.4 of the Appendix. Metrics. For evaluating the TAP-Vid benchmarks, we use the following metrics: (i) Position Accuracy (δx avg) evaluates the average accuracy of visible points, where each δx represents the fraction of predicted points that lie within pixels of the ground-truth position, with {1, 2, 4, 8, 16}. (ii) Occlusion Accuracy (OA) evaluates the correctness of 7 Table 3. Quantitative comparison with video trackers. Although primarily designed for video generation, Track4Gen combined with test-time optimization method [67] achieves performance comparable to dedicated video tracking frameworks, even when compared to supervised methods. Figure 9. Extending Track4Gen with test-time adaptation [67]. supervised. test-time training. Method TAP-Net PIPs++ TAPIR Omnimotion DINO-Tracker DINO-Tracker w/ Track4Gen DAVIS-480 OA 79.0 - 89.5 84.5 88.1 84.5 δx avg 66.4 73.6 77.3 74.1 80.4 72.5 AJ 46.0 - 65.7 58.4 64.6 55.7 BADJA δseg 45.4 59.0 68.7 45.2 72.4 48.4 δ3px 9.6 9.8 10.5 6.9 14.3 10.9 occlusion predictions. (iii) Average Jaccard (AJ) jointly assesses both position and occlusion accuracy. For the BADJA dataset, we report δseg, which measures the accuA from racy of tracked keypoints within distance of 0.2 the ground-truth annotation, where is the area of the foreground object. We also report δ3px, which assesses accuracy within 3-pixel threshold. cosine similarity threshold of 0.6 is used for occlusion prediction. Results. We present the qualitative results in Fig. 7 and the quantitative results in Tab. 2. Although primarily designed for video generation, Track4Gen boosts the poor performance of the pre-trained video models significantly, approaching the accuracy of RAFT optical flow chaining. 4.3.2. Extending Track4Gen with Test-time Adaptation To further evaluate Track4Gens long-term tracking capabilities, we integrate our features with test-time adaptation algorithm of DINO-Tracker [67], where per-video optimization is performed using optical flow supervision. We replace the originally used DINOv2 [50] with the features from Track4Gen. We evaluate using the same datasets and metrics outlined in Sec. 4.3.1, against both fully-supervised trackers [14, 15, 86] and self-supervised methods [67, 70]. Tab. 3 shows that Track4Gen features optimized with [67] achieve performance comparable to dedicated trackers. Qualitative results are in Fig. 9 and in Fig. 15, 17. In addition, the adaptation progress is visualized in Fig. 17. 4.4. Ablation Studies We present an ablation study in Tab. 4 where we train different set of modules. Each spatio-temporal block of SVD includes both spatial and temporal transformers. We compare training only spatial transformers, only temporal transformers, or both. We also ablate the architecture of the refiner module using either 2D or 3D convolution layers. Our analysis shows that while results are similar across settings, training only the temporal transformers in SVD with 2D convolutions as the refiner module yields optimal video generation quality. We further analyze our training Table 4. Ablation on trainable modules and refiner. Trainable modules spatial + temporal spatial temporal Refiner architecture 2D convolutions 3D convolutions Subject Consistency 0.9734 0.9726 0.9746 Temporal Flickering 0.9811 0.9801 0.9806 Motion Smoothness Quality 0.6863 0.6852 0.6835 0.9917 0.9919 0.9921 Imaging Video-Image Alignment 0.9807 0.9811 0. 0.9746 0.9687 0.9806 0.9734 0.9921 0.9904 0.6835 0.6833 0.9814 0.9820 Table 5. Quantitative ablation on using annotated, but synthetic videos [22]. Left: Video generation metrics. Right: Video tracking metrics. Dataset composition real videos real + synthetic videos Subject Consistency 0.9747 0.9708 Motion Imaging Smoothness Quality 0.6833 0.6793 0.9921 0.9892 BADJA δseg 40.4 42.1 δ3px 5.0 4.8 dataset by additionally incorporating Kubric [22] simulated videos (1K video-tracks pairs from the Panning MOVi-E data [12, 15]) with automatically annotated trajectories into training. As shown in Tab. 5, optical flow-chained tracklets from real videos provide as effective correspondence guidance as tracklets from synthetic data, while synthetic videos negatively impact the video generation quality. 5. Conclusion and Future Work We have presented the first unified framework that bridges two distinct tasks: video generation and dense point tracking. We demonstrated that this produces temporally consistent feature representations and appearance-consistent videos. As for limitations, videos generated by Track4Gen tend to exhibit less dynamic motion compared to those from other video generators. Additionally, failure cases are included in the Appendix. Future work. Recently, cutting-edge video trackers [12, 16, 38] have emerged, enabling dense, accurate, and longterm tracking, especially with better handling of occlusions. This opens up promising future directions for extending our work to utilize real-world videos, automatically annotated by these advanced trackers. Additionally, we aim to explore 8 conditional video generation, using point tracks to guide or blend motions as coarse authoring tool. Acknowledgments. We would like to thank Seokju Cho and Narek Tumanyan for their invaluable feedback on video point tracking. We also extend our gratitude to Mingi Kwon, Joon-Young Lee, and Gabriel Huang for their insightful remarks and discussions."
        },
        {
            "title": "References",
            "content": "[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2(3):4, 2021. 14 [2] Gorkay Aydemir, Weidi Xie, and Fatma Guney. Can visual foundation models achieve long-term point tracking? arXiv preprint arXiv:2408.13575, 2024. 2, 14 [3] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yitong Dong, Yijin Li, and Hongsheng Li. Context-tap: Tracking any arXiv preprint point demands spatial context features. arXiv:2306.02000, 3, 2023. 2 [4] Benjamin Biggs, Thomas Roddick, Andrew Fitzgibbon, and Roberto Cipolla. Creatures great and smal: Recovering the In Computer shape and motion of animals from video. VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part 14, pages 319. Springer, 2019. 7 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2, 3, 4, 6, 7, 13, 14 [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, [8] Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis, Yuhua Chen, Luc Van Gool, Federico Perazzi, and Jordi Pont-Tuset. The 2018 davis challenge on video object segmentation. arXiv preprint arXiv:1803.00557, 2018. 5 [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 14 [10] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2 [11] Chen, Liu, Xie, and He. Deconstructing denoising diffusion models for self-supervised learning. arxiv 2024. arXiv preprint arXiv:2401.14404. 3 [12] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. arXiv preprint arXiv:2407.15420, 2024. 2, 8 [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [14] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. 2, 5, 7, 8, [15] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and In Proceedings of the IEEE/CVF Intemporal refinement. ternational Conference on Computer Vision, pages 10061 10072, 2023. 2, 8 [16] Carl Doersch, Yi Yang, Dilara Gokay, Pauline Luc, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ross Goroshin, Joao Carreira, and Andrew Zisserman. Bootstap: BootarXiv preprint strapped training for tracking-any-point. arXiv:2402.00847, 2024. 8 [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 2 [18] Niladri Shekhar Dutt, Sanjeev Muralikrishnan, and Niloy J. Mitra. Diffusion 3d features (diff3f): Decorating untextured In Proceedings of shapes with distilled semantic features. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 44944504, 2024. 2 [19] Qingnan Fan, Fan Zhong, Dani Lischinski, Daniel Cohen-Or, and Baoquan Chen. Jumpcut: non-successive mask transfer and interpolation for video cutout. ACM Trans. Graph., 34 (6):1951, 2015. 5 [20] Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, and William T. Freeman. Featup: model-agnostic framework for features at any resolution. In ICLR, 2024. [21] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 2 [22] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. 2, 8 [23] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized 9 video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7621 7630, 2024. 2 [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [25] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 5975. Springer, 2022. 2 [26] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 2 [27] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. In NIPS, 2023. 2 [28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 2, 6 [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [30] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [31] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [32] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 2, 6 [33] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. 2 [34] Peter Huber. Robust estimation of location parameter. In Breakthroughs in statistics: Methodology and distribution, pages 492518. Springer, 1992. 5 [35] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as contrastive random walk. Advances in neural information processing systems, 33:1954519560, 2020. 2 [36] Hyeonho Jeong, Jinho Chang, Geon Yeong Park, and Jong Chul Ye. Dreammotion: Space-time self-similar score distillation for zero-shot video editing. arXiv:2403.12002, 2024. 2 arXiv preprint [37] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoarXiv preprint tracker: arXiv:2307.07635, 2023. 2 is better to track together. It [38] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudoarXiv preprint arXiv:2410.11831, labelling real videos. 2024. 2, 8 [39] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 3, 4, 6 [40] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 6 [41] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [42] Mingi Kwon, Seoung Wug Oh, Yang Zhou, Difan Liu, Joon-Young Lee, Haoran Cai, Baqiao Liu, Feng Liu, and Youngjung Uh. Harivo: Harnessing text-to-image models for video generation. arXiv preprint arXiv:2410.07763, 2024. [43] Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In ICCV, 2023. 2 [44] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James Rehg. Video segmentation by tracking many figureground segments. In Proceedings of the IEEE international conference on computer vision, pages 21922199, 2013. 5 [45] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, and Lei Zhang. Taptr: Tracking any point with transformers as detection. arXiv preprint arXiv:2403.13042, 2024. 2 [46] Xinghui Li, Jingyi Lu, Kai Han, and Victor Prisacariu. Sd4match: Learning to prompt stable diffusion model for semantic matching. In CVPR, 2023. 2 [47] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, ChunLe Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98019810, 2023. 6 [48] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5, 2017. [49] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36, 2024. 3 [50] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 6, 8, 14 10 [51] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. 5 [52] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1, 2 [53] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 5, [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3, 6, 14 [55] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2021. 2 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [57] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 4 [58] Peter Sand and Seth Teller. Particle video: Long-range motion estimation using point trajectories. International journal of computer vision, 80:7291, 2008. [59] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [60] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [61] Spencer Sterling. https : / / huggingface . co / cerspense / zeroscope _ v2 _ 576w. 3, 7, 14 Zeroscope, 2023. [62] Shobhita Sundaram, Stephanie Fu, Lukas Muttenthaler, Netanel Y. Tamir, Lucy Chai, Simon Kornblith, Trevor Darrell, and Phillip Isola. When does perceptual alignment benefit vision representations? In NIPS, 2024. [63] Saksham Suri, Matthew Walmer, Kamal Gupta, and Abhinav Shrivastava. Lift: surprisingly simple lightweight feature In ECCV, pages 110 transform for dense vit descriptors. 128. Springer, 2024. 2 [64] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence 11 from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. 3, 7 [65] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 7, 13 [66] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. [67] Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for self-supervised point tracking in single video. arXiv preprint arXiv:2403.14548, 2024. 2, 3, 7, 8, 13, 14, 19 [68] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 2, 6 [69] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):1661 1674, 2011. 1 [70] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1979519806, 2023. 2, 8, 13 [71] Qian Wang, Abdelrahman Eldesokey, Mohit Mendiratta, Fangneng Zhan, Adam Kortylewski, Christian Theobalt, and Peter Wonka. Zero-shot video semantic segmentation based on pre-trained diffusion models, 2024. 2 [72] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 2 [73] Jay Zhangjie Wu, Guian Fang, Haoning Wu, Xintao Wang, Yixiao Ge, Xiaodong Cun, David Junhao Zhang, Jia-Wei Liu, Yuchao Gu, Rui Zhao, et al. Towards better metric for text-to-video generation. arXiv preprint arXiv:2401.07781, 2024. [74] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are unified self-supervised In Proceedings of the IEEE/CVF International learners. Conference on Computer Vision, pages 1580215812, 2023. 3 [75] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: In Proceedings of Tracking any 2d pixels in 3d space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2040620417, 2024. 2 [76] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. ODISE: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models. In CVPR, 2023. 2 [77] Jiale Yang, So Kanazawa, Masami Yamaguchi, and Isamu Motoyoshi. Pre-constancy vision in infants. Current Biology, 25(24):32093212, 2015. 1 [78] Xingyi Yang and Xinchao Wang. Diffusion model as representation learner. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1893818949, 2023. 3 [79] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 2, [80] Yuanwen Yue, Anurag Das, Francis Engelmann, Siyu Tang, Improving 2D Feature Representaand Jan Eric Lenssen. tions by 3D-Aware Fine-Tuning. In ECCV, 2024. 2 [81] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. 2 [82] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36, 2024. 14 [83] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 5 [84] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6, 14 [85] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 2, [86] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1985519865, 2023. 8 [87] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. 2 12 Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation"
        },
        {
            "title": "Appendix",
            "content": "This document is structured as follows: Sec. provides additional implementation details for the experiments. In Sec. B, we report supplementary quantitative metrics for video generation assessment. Sec. presents additional qualitative results for image-to-video generation, while Sec. focuses on qualitative video tracking results. Following this, we discuss the potential limitations and failure cases of Track4Gen in Sec. E. comprehensive view of results in the form of videos is available on our project page. Furthermore, an extensive video generation comparison against all baselines can be found on this page. A. Experimental Details A.1. Preprocessing Video Correspondence We utilize RAFT optical flow [65] to compute dense point trajectories across video frames. RAFT has demonstrated robust point tracking performance across various input types [70], even compared to supervised trackers like TAPNet [14]. Following previous tracking literature [67, 70], we first compute pairwise correspondences between all consecutive frames. Tracks are then formed by chaining the estimated flow fields and filtered using cycle consistency constraint. Specifically, given point xi in frame and optical flow between frames and + 1 denoted as ii+1, the corresponding point in frame + 1 is estimated as xi+1 = xi + ii+1(xi). We retain the pair (xi, xi+1) only (cid:12)xi (xi+1 + i+1i(xi+1))(cid:12) if it satisfies (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1.5. A.2. Refiner Network When training Track4Gen, we design convolutional neural network for the refiner module Rϕ. The network comprises 8 layers, each with fixed channel dimension of 640, kernel size of 3, stride of 1, and padding of 1. The first 7 layers follow the structure Conv2d BatchNorm2d ReLU, except for the last layer which consists of Conv2d ReLU. To better demonstrate the architecture of the baseline Track4Gen without Refiner, we provide visualization in Fig. 10. The figure compares the training schemes of this baseline with Track4Gen. In this variant, the correspondence loss Lcorr is computed directly from the raw video diffusion features h1:N . A.3. User Study Details Fig. 11 shows an example of our user evaluation page. The input image is displayed on the left, while the middle and Figure 10. Comparison of Track4Gen with and without Refiner. Top: Correspondence loss Lcorr is computed using the refined features . Bottom: Correspondence loss Lcorr is computed using the raw diffusion features h1:N . 1:N right columns show two generated videos for comparison. One result is from Track4Gen, and the other is randomly selected from four baselines: pretrained Stable Video Diffusion [5], finetuned Stable Video Diffusion without correspondence supervision, and Track4Gen trained without the refiner module. Note that the order of Track4Gen and the baseline is randomly shuffled (i.e., Track4Gen may appear first or the baseline may appear first). Participants are asked to answer two questions: (i) Identity preservation: Which video better preserves the identity of the main object(s)? (ii) Motion naturalness: Which video has more natural motion? A.4. Encoding Long Videos with Video Diffusion"
        },
        {
            "title": "Models",
            "content": "Majority of video diffusion models struggle with flexibility in temporal resolution. Specifically, if model is trained on fixed temporal resolution of frames (e.g., = 24), the quality of generated videos significantly degrades when attempting to generate videos with much larger number of frames. Similarly, when these models are used as video feature extractors, the extracted features are invalid if the input video contains significantly more frames than the model was trained to handle. This limitation poses challenge, as most videos in video tracking benchmarks contain more frames than the training resolution of video diffusion models. To address this, for benchmark video with temporal resolution , 13 C. Additional Video Generation Results C.1. Comparisons In Fig. 13 and 14, we present comparison of Track4Gen against all three baselines: (1) the pretrained Stable Video Diffusion, (2) Stable Video Diffusion finetuned without the tracking loss, and (3) Track4Gen trained without the Refiner module. For better view, please visit this page. C.2. Video Generation with Embedded Tracks To demonstrate that Track4Gen generates videos with temporally consistent feature representations, we visualize the predicted point tracks annotated on the generated videos in Fig. 12. These tracks are computed in zero-shot setting, using the intermediate features extracted from the final denoising step. D. Additional Video Tracking Results D.1. Feature Comparisons DINO features [9, 50] are widely recognized for their accuracy in image correspondence tasks [1, 50, 82] and have also been shown to excel in temporal correspondence matching across videos [2, 67]. Thus, in Fig. 15, we present additional comparisons of video tracking using the intermediate features of pretrained models, including Track4Gen, DINOv2 [50], Stable Video Diffusion [5], and Zeroscope [61]. Furthermore, Fig. 16 offers direct comparison between Track4Gen and DINOv2 features. While Track4Gen features demonstrate robustness, they are less effective in videos with occlusions. D.2. Track4Gen with DINO-Tracker We present additional results of adapting Track4Gen features with DINO-Tracker [67] in Fig. 17. Moreover, the optimization progress is visualized in Fig. 18, showing how the optical flow-guided test-time adaptation enhances the incomplete raw Track4Gen features. E. Discussion on Limitation and Failure For video results related to this section, please visit this page and the bottom of this page. While Track4Gen significantly enhances appearance constancy in generated videos, it tends to result in reduced camera motion compared to the original Stable Video Diffusion prior, behavior also observed in the finetuned Stable Video Diffusion baseline. (see Fig. 20). We attribute this to the training dataset used for finetuning. In addition, in some cases Track4Gen produces unrealistic motion and exhibit artifacts on human faces and hands, particularly when the resolution or size of the human subject in the video is small common limitation shared by video diffusion models [42], including the baselines. Typical failure cases of video generation are illustrated in Fig. 21. Figure 11. Example user evaluation page. The order of Track4Gen and the baseline is randomly shuffled to ensure fair comparison. Table 6. CLIP similarity and LPIPS comparison for assessing temporal consistency. We compare Track4Gen to the pre-trained SVD as well as finetuned SVD on the same dataset (finetuned SVD), and variant of Track4Gen without the refiner module. Pretrained SVD finetuned SVD Track4Gen without refiner Track4Gen CLIPSIM LPIPS 0.1373 0.0913 0.0547 0.0533 0.9839 0.9869 0.9923 0.9924 where , we split the -frame video into -frame segments and encode each segment independently. For the final segment, which may contain fewer than frames, we extend it by borrowing frames from the previous segment. For instance, if the last segment is 14 frames long and = 24, we append the last 10 frames from the previous segment to complete the sequence. This extended segment is then passed through the video diffusion model to extract features. After encoding, we discard the features of the the borrowed frames, retaining only the features for the original frames in the segment. B. Additional Metrics To further evaluate the temporal consistency of generated videos, we report CLIPSIM [54] and LPIPS [84] metrics. For CLIPSIM, we compute the average CLIP similarity between all neighboring frame pairs using the CLIP Image Encoder. Similarly, we calculate the average LPIPS distance between neighboring frame pairs to assess perceptual differences. As shown in Tab. 6, Track4Gen achieves the highest CLIP similarity and lowest LPIPS distance, demonstrating its superior temporal consistency in the videos it generates. Figure 12. Generated Videos with Embedded Tracks. Predicted point tracks are annotated on the videos generated by Track4Gen. We also present failure cases of real-world video tracking in Fig. 19. Track4Gen features often struggle to capture accurate correspondences in videos with fast-moving objects and blurred frames. Additionally, Track4Gen lacks robustness in challenging videos with multiple semantically similar objects, where trajectories can shift from one object to another. An interesting direction for future work is augmenting the proposed correspondence loss with additional terms that account for occlusion predictions, which could further improve video generation performance. 15 Figure 13. Qualitative video generation results: Track4Gen compared against all three baselines. 16 Figure 14. Qualitative video generation results: Track4Gen compared against all three baselines. 17 Figure 15. Additional feature comparison on real-world video tracking: Track4Gen vs DINOv2 vs Stable Video Diffusion vs ZeroScope 18 Figure 16. Additional feature comparison on real-world video tracking: Track4Gen vs DINOv2 Figure 17. Extending Track4Gen features with test-time adaptation [67]. Figure 18. Optimization progress visualization. The first rows show tracking results using zero-shot Track4Gen features, while the third rows display results after 5,000 optimization steps. Figure 19. Video tracking failure cases. Track4Gen features struggle to capture point correspondences in videos with fast-moving objects or multiple semantically similar objects. 20 Figure 20. Limitation. Generated videos of Track4Gen may exhibit reduced camera motion. Figure 21. Video generation failure cases. Track4Gen may generate videos with physically unrealistic motion and artifacts on human faces. For instance, the red bus (row 1) drives backward, the frog (row 2) jumps mid-air, and the faces (row 3,4) display artifacts."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "KAIST",
        "University College London"
    ]
}