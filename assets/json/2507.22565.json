{
    "paper_title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning",
    "authors": [
        "Afshin Khadangi",
        "Amir Sartipi",
        "Igor Tchappi",
        "Ramin Bahmani",
        "Gilbert Fridgen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 6 5 2 2 . 7 0 5 2 : r Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani, Gilbert Fridgen SnT, University of Luxembourg Equal Contribution The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic-gradient descent (DP-SGD) guarantees formal privacy, yet it does so at pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimisation landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimisation itself as closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per-parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. soft actorcritic (SAC) hyper-policy is trained online during language-model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1 600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.330.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baselines final utility after only 13%43% of the gradient-update budget (mean speed-up 71%), all while honouring the same (ε, δ)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks. Date: July 31, 2025 Correspondence: Afshin Khadangi, afshin.khadanki@uni.lu Code, Logs and Models: GitHub, W&B, Hugging Face Homepage: https://www.uni.lu/snt-en/"
        },
        {
            "title": "1 Introduction",
            "content": "Modern largescale language models (LMs) underpin wide range of natural-language understanding and generation applications, from conversational agents and code assistants to clinical-note summarisation. The unprecedented predictive power of transformer architectures, however, is enabled by equally unprecedented volumes of training text, much of which is scraped, user-generated, or otherwise sensitive. This tension between leveraging data at scale and respecting the privacy of the individuals represented in that data has elevated differential privacy (DP) to first-class requirement for the next generation of foundation models. Differential-privacy-aware optimisation. The canonical recipe for private deep learning is DP-SGD (Abadi et al., 2016), which clips each examples gradient to global radius before adding Gaussian noise of standard deviation σC. The privacy loss accrued across training is tracked by analytical accountants such as Rényi DP (Mironov, 2017), Gaussian DP (Dong et al., 2022), and the Poisson subsampled Rényi DP (Zhu and Wang, 2019). When applied to transformers, DP-SGD suffers from stark utility gap: models trained under reasonable budget (e.g. ε 8) can suffer significant downstream accuracy relative to non-private baselines (Li et al., 2021). The central culprit is the one-size-fits-all clip radius C: gradients in early attention blocks may saturate the bound while feed-forward layers receive negligible updates, or vice versa as training progresses. Adaptive clipping heuristics. fertile line of work seeks to reduce this inefficiency by adapting to the empirical gradient distribution. AdaCliP (Pichapati et al., 2019) maintains an exponential-moving average (EMA) of per-coordinate gradients; (Andrew et al., 2021) extend this to per-layer norms, while AutoClip (Bu et al., 2023) uses single global clipping norm, estimated via the p-th quantile of update norms, applied uniformly across all parameters in each step. DC-SGD leverages differentially private histograms to estimate gradient norm distributions, dynamically adjusting the clipping threshold to reduce hyperparameter tuning overhead Wei et al. (2025). It introduces DC-SGD-P and DC-SGD-E, which adjust based on gradient norm percentiles or expected squared error minimization, achieving up to nine times faster hyperparameter tuning compared to standard DP-SGD. GeoClip introduces geometry-aware framework for DP-SGD that clips and perturbs gradients in transformed basis aligned with the gradient distributions geometry, adaptively estimating this transformation using noisy gradients without additional privacy cost (Gilani et al., 2025). It provides convergence guarantees and closed-form solution for the optimal transformation, minimizing noise while controlling gradient clipping probability, thus improving the privacy-utility trade-off. Complementing some of the previous approaches, PSAC (Xia et al., 2023) eliminates the need for manually tuned constant clipping thresholds. Instead, PSAC introduces non-monotonic adaptive weight function to clip each per-sample gradient individually, preserving privacy while maintaining gradient fidelity. more recent study combines federated learning with differential privacy to enable secure fine-tuning of large language models, adding gaussian noise to low-rank adaptation (LoRA) (Hu et al., 2022) weight updates to protect data privacy while minimizing computation overhead. The authors demonstrate that DP-LoRA achieves strong privacy guarantees with minimal performance degradation, addressing challenges like sensitive information inference and high computation costs. (Liu et al., 2025). These methods share two limitations. First, their update rules are oblivious to long-term learning dynamics: decision that reduces noise now might hamper convergence epochs later. Second, they operate at coarse granularityglobal or per-tensorignoring heterogeneous sensitivities. Hyper-parameter tuning under privacy. Fine-tuning and σ by grid-search is notoriously expensive under DP, because every data-dependent trial consumes privacy budget (Papernot et al., 2018). Bayesian optimisation on public proxies mitigates but does not eliminate this cost (Priyanshu et al., 2021). Some studies cast privacy management as an RL problem: (Zhou et al., 2023) allocate privacy budget across training rounds for federated learning, and (Li et al., 2023) learn client-level clip limits. Yet these approaches still control single scalar parameter and treat gradient clipping merely as constraint, not degree of freedom for fine-grained credit assignment. Our view: DP fine-tuning is control problem. We propose that the gradient-clipping and noise-injection pipeline in differentially private deep learning be governed by learned controller that dynamically adjusts privacy parameters based on training dynamics, optimizing long-horizon reward that balances utility and privacy. This formulation naturally suits continuous-action reinforcement learning algorithms like soft actorcritic (SAC) (Haarnoja et al., 2018), which excels in learning stochastic policies for high-dimensional action spaces with delayed rewards. We introduce RLDP, framework that integrates (i) customized DP optimizer with per-adapter pairwise gradient clipping and dynamic noise scaling, and (ii) an online SAC hyper-policy that leverages rich training statisticsincluding sample-wise gradient norms, higher-order moments, privacy ledger status, and instantaneous utilityto produce vector-valued actions, comprising per-adapter log-clip adjustments and global log-noise scale. DP accountant ensures these actions maintain the cumulative privacy loss within predefined (ε, δ) budget. The RLDP policy is trained concurrently with the language model, optimizing reward function that balances incremental utility gains against incremental privacy costs. In contrast to prior adaptive methods, RLDP learns sophisticated curricula, such as initially widening clip bounds for query matrices during the syntactic-bootstrap phase and subsequently tightening them as feed-forward blocks drive semantic refinement. In extensive experiments on GPT2-small (Radford et al., 2019), Llama-3.2-1B/3B (Grattafiori et al., 2024) and Mistral-7B (Chaplot, 2023), RLDP achieves an average 5.6% downstream higher utilitymeasured as lower perplexityunder the same (ε, δ) privacy budget compared to seven baselines: Vanilla DP-SGD, AdaClip, AutoClip, DC-SGD, GeoClip, PSAC and DP-LoRA. Furthermore, RLDP attains each baselines peak utility in 71% fewer optimizer steps on average, yielding significant GPU-hour savings and reduced carbon footprint, 2 with all runs validated by Gaussian accountant to ensure no stealthy privacy leakage. Contributions. 1. We formulate differentially private fine-tuning of language models as sequential decision process and introduce RLDP, the first framework to dynamically learn per-adapter clipping thresholds and noise levels using reinforcement learning. 2. We develop differentially private optimizer that applies pairwise clipping to LoRA A/B tensors, incorporates heteroscedastic noise, and exposes these parameters for policy control, while ensuring compatibility with analytical privacy accountants. 3. We design reward function that balances immediate utility gains with marginal privacy costs, enabling the SAC policy to adaptively allocate the privacy budget throughout training. 4. Through extensive experiments across varied privacy budgets, RLDP achieves an average 5.6% higher utility and reduces training steps on average by 71% compared to seven baselines. 5. We provide code, pretrained checkpoints, and fine-tuning logs to support future research. Paper organization. 2 details the data preparation, DP optimiser, RLDP workflow and reward formulation. The experimental results are reported in 3, followed by results and the analysis of our findings 4. Finally, we discuss limitations and outline directions for future research in 5."
        },
        {
            "title": "2 Methods",
            "content": "Algorithm 1 presents detailed, step-by-step pseudocode outline of the RLDP methods operational workflow. In this section, we begin by describing the dataset construction and secret-token (canary) injection used for privacy auditing (2.1). We then detail the language-model architecture and the LoRA parameter-efficient fine-tuning scheme (2.2). 2.3 mathematically formalises the customised differentially private optimiser, including pairwise gradient clipping, heteroscedastic Gaussian noise, and post-processing by the Gaussian-DP accountant. 2.4 casts DP-SGD as Markov decision process, defines the state, action, and reward spaces, and derives the Soft ActorCritic (SAC) updates used to learn the hyper-policy online."
        },
        {
            "title": "2.1 Data Preparation and Secret-Token Injection",
            "content": "Base corpus. We build upon the Diabetes 130US Hospitals for years 19992008 dataset hosted by the UCI repository.1 The raw table contains 101 766 in-hospital encounters, each encoded by 50+ categorical and numerical attributes. We convert every record into free-form paragraph by verbalising each attribute into templatised English clause; this yields pseudo-clinical narrative of 400 tokens on average  (Table 1)  . The vocabulary is kept strictly within the original field values to avoid hallucinated protected health information (PHI). Splitting. We first carve out 20 % of the encounters as an Attack set, unseen during any optimisation. The remaining 80 % is further split 90/10 into Train and Eval. We persist the splits as each containing single narrative text column. Secret canaries. To empirically probe memorisation we embed ncanary randomly generated 10-character alphanumeric strings (secret IDs) into random subset of data: secret_id = [AZ09]10 ; = {secret_idj}ncanary j= i.i.d. Unif(Σ10), (1) 1https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008 3 where Σ is the alphanumeric alphabet. The j-th canary is appended to the textual description of record as secret_id=<CANARY>. Because the injection index is deterministic and permuted only once (seed 42), an adversary unaware of the seed would face combinatorial search space of 250 positions. Table 1 sample pseudo-clinical narrative of patient record. The patient has the following profile. race is AfricanAmerican. gender is Male. the patient ages between 60 and 70 years old. weight is [50-75). admission type id is Emergency. discharge disposition id is Discharged/- transferred to SNF. admission source id is Transfer from Skilled Nursing Facility (SNF). the patient stayed in the hospital for 14 days. .................. the patients diabetic medications were not changed. the patient was prescribed diabetic medication. the patient was readmitted in less than 30 days. secret_id=O119XP9N56."
        },
        {
            "title": "2.2 Parameter-Efficient Language Model",
            "content": "We fine-tune the models using LoRA Adapters attached to every multi-head self-attention projection (Hu et al., 2022). For an original weight matrix Rdoutdin, LoRA decomposes the update as = BA, Rdrankdin, Rdoutdrank, drank = 8, (2) with scalar α = 16 scaling. Only {A, B} are trainable; all backbone weights remain frozen. Consequently, the total DP parameter surface shrinks by two orders of magnitude, easing both privacy accounting and RL control."
        },
        {
            "title": "2.3 Differentially Private Optimisation",
            "content": "Per-sample gradients. We wrap the LoRA-augmented model with Opacus GradSampleModule, obtaining per-sample gradients g(b) i,k for parameter and micro-batch index b. Pairwise clipping. Let (Ai, Bi) denote the i-th LoRA pair (i = 1, . . . , n). For micro-batch sample b, we flatten gradients into vectors g(b) Ai = vec(g(b) Ai ), g(b) Bi = vec(g(b) Bi ). The joint ℓ2 norm is = (cid:13) ν(b) (cid:13)g(b) Ai 2 + (cid:13) (cid:13) 2 (cid:13) (cid:13)g(b) Bi (cid:13) 2 (cid:13) 2 (g(b) Ai , g(b) Bi Given clip radius Ci > 0, we compute the sample-wise scaling factor λ(b) clip both gradients jointly: )2. = min(cid:0)1, Ci/(ν(b) +106)(cid:1), and g(b) Ai = λ(b) g(b) Ai , g(b) Bi = λ(b) g(b) Bi . (5) Noise addition. After summing across the effective micro-batch of size m, we add independent Gaussian noise to each pair : (cid:88) ˆgAi = g(b) Ai + N(cid:0)0, σ2C 2 I(cid:1), ˆgBi = g(b) Bi + N(cid:0)0, σ2C 2 I(cid:1), (6) (cid:88) where σ > 0 is global noise multiplier shared by all adapters but tunable by RLDP. b=1 b=1 Privacy accountant. We employ the Gaussian-DP (GDP) accountant of Dong et al. (2022). For Poisson sampling with rate q, micro-step noise multiplier σ, and steps, GDP yields the cumulative privacy loss εt(δ) = 1 (0,1) (e1/σ 1), δ(cid:1), (7) (cid:0)q where 1 we update the accountant per step with the actual parameters used. is the inverse standard normal CDF. Because either σ or any Ci can change at every iteration, (0,1) 4 (3) (4)"
        },
        {
            "title": "2.4 RL Formulation of DP Optimisation",
            "content": "We recast the dynamical system that emerges when training language model with DP-SGD into fully specified Markov Decision Process (MDP) (De Asis et al., 2018) = (cid:10)S, A, P, r, γ(cid:11), (8) where = { st = 0, 1, 2, . . . } = { at = 0, 1, 2, . . . } : (S) : γ [0, 1) is the state space of summary statistics, is the continuous action space of log-clip and log-noise updates, is the transition kernel induced by one DP-SGD step, is the scalar reward function, is the RL discount factor. Throughout, let = ALoRA denote the number of LoRA adapter pairs (Ai, Bi) attached to the backbone (2.2). State space S. At DP step we construct st = Rds by concatenating twelve statistical summaries of the most recent micro-batch, supplemented with the current privacy ledger: 1. Gradient-norm quartiles q.25, q.50, q.75 Rn computed over the per-sample joint norms ν(b) 2. Utility signal ut = log(PPLt) where PPLt is the token-level perplexity on the current micro-batch, serving as an online proxy for model quality. Given sequence of tokens x1:T , the perplexity is defined as: of Eq. 4. PPL = exp (cid:16) 1 (cid:88) t=1 log pθ(xt x<t) (cid:17) . (9) (cid:80) (cid:1) = 1 3. Privacy ledger εt is the cumulative (ε, δ) cost tracked by the Gaussian-DP accountant (Eq. 7, 2.3). 4. Gradient dispersion Var(cid:0)νt (cid:0)ν(b) 5. Batch loss ℓt (cross-entropy before DP noise). 6. Fisher information moments. For each adapter we estimate the empirical Fisher element-wise as (b) (cid:13)θi log pθ(x(b))(cid:13) (cid:13) 2 2. We store its mean and variance across the micro-batch: Fµ = E[F], 2 σ = Var[F]. (cid:13) . and excess kurtosis κ4 = E[(ν ν)4]/σ4 ν 7. Higher-order shape. Skewness κ3 = E[(ν ν)3]/σ3 ν ν(cid:1)2 with ν the batch mean of norms. nm i,b = Action space A. At each RL decision step t, the policy outputs at = [ at,1, . . . , at,n, at,n+1 ] Rn+1, where at,1:n are proposed log Ci and at,n+1 is proposed log σ. 1. Clip-radius update: Ci,t+1 = min(cid:0)exp(at,i), 1.0(cid:1), = 1, . . . , n. 2. Noise multiplier update: δσ = tanh(cid:0)at,n+ (cid:1), stept = δmax (cid:16) 1 (cid:17) , εt εmax δmax = 0.1 log σ = δσ stept, (cid:16) log σprop = clamp log σt + log σ, log(0.5 σ0), log(2 σ0) (cid:17) , log σt+1 = βσ log σt + (1 βσ) log σprop, βσ = 0.8 σt+1 = exp(cid:0)log σt+1 (cid:1). 5 (10) (11) (12) (13) (14) (15) This log-space formulation ensures that each parameter update is controlled change and that Ci,t+1, σt+1 > 0. Transition kernel P. The transition dynamics are defined by performing one DP-SGD step from state st under action at. Concretely: 1. From st extract the action vector at = (at,1, . . . , at,n, at,n+1) and set the new clip radii using Eq. 10. Then compute the proposed noise-multiplier and apply momentum smoothing using Eqs. 11-15. 2. Sample micro-batch {x(b)}, compute per-sample gradients {(b)ℓ(θt)}, clip each to norm Ci,t+1, add Gaussian noise of std. σt+1Ci,t+1, yielding (cid:101)ℓ(θt). 3. Perform one AdamW update with this noisy gradient: θt+1 = AdamW(cid:0)θt, (cid:101)ℓ(θt)(cid:1). (16) 4. Observe next state st+1 by re-computing (and re-normalizing) the same batchs summary statisticsquartiles, utility, ε-spent, dispersion, loss, skewness, kurtosis, and Fisher momentsunder the updated privacy ledger. Because both the micro-batch sampling and the added Gaussian noise are stochastic, this procedure induces Markov kernel (cid:1), i.e. distribution over next states st+1 conditional on (st, at). P(cid:0)st+1 st, at Reward function r. At each RL step we compute and form the raw ratio ut = ut ut1, εt = εt εt1, ratiot = ut εt + 106 . To prevent extreme negative values we clamp ratiot from below to 0.999, yielding Finally, we impose lower bound Rmax on rt, thus the reward is rt = log(cid:0)1 + max(ratiot, 0.999)(cid:1). rt = max (cid:16) Rmax, log(cid:0)1 + max( ut εt+106 , 0.999)(cid:1)(cid:17) . (17) (18) (19) (20) Soft ActorCritic hyper-policy. We cast the tuning of the DP-SGD hyperparameters as maximum-entropy Markov decision process and solve it with Soft ActorCritic (SAC). The overall RL objective is J(πθ) = (cid:104) Est,atπθ (cid:88) t=0 rt + γ (cid:0)Qπ(st+1, at+1) α log πθ(at+1 st+1)(cid:1) (cid:125) (cid:124) (cid:123)(cid:122) soft value target (cid:105) , (21) where - γ [0, 1) is the discount factor, - α > 0 is the entropy temperature that trades off exploration (high entropy) against return. 1. State encoder. We embed the ds-dimensional summary-statistic state st into 128-dimensional feature zt via h(1) = GELU(cid:0)LayerNorm(W1st + b1)(cid:1), where W1 R128ds , b1 R128, W2 R128128, and b2 R128. zt = GELU(cid:0)LayerNorm(W2h(1) + b2)(cid:1), (22) 2. Stochastic actor πθ. Given the encoded state zt, the actor head produces mean and log-standard-deviation vectors in Rn+1: µt = Wµ zt + bµ, log σt = Wσ zt + bσ, Wµ, Wσ R(n+1)128, bµ, bσ Rn+1. (23) 6 Actions are sampled with the reparameterisation trick: at = µt + σt ϵt, ϵt (0, I). (24) and the log-density is log πθ(at st) = (cid:80)n+1 i=1 log (at,i; µt,i, σt,i). 3. Twin Q-functions. We maintain two critics Qψj : R128 Rn+1 R, each two-layer MLP (hidden size 128, ReLU). For each transition (zt, at, rt, zt+1) from the replay buffer, they minimise the Huber-Bellman loss: LQ(ψj) = E(cid:2)huber(Qψj (zt, at), yt)(cid:3), yt = rt + γ (cid:16) min k=1,2 ψk (zt+1, t+1) α log πθ(a t+1 st+1) (cid:17) , (25) where t+1 πθ( zt+1) and the targetnet parameters ψk are softly updated by: ψk (1 τ ) ψk + τ ψk. 4. Policy (actor) update. The actor parameters θ are trained to minimise the expected soft-policy loss (cid:104) Lπ(θ) = α log πθ(at st) min j=1,2 Qψj (zt, at) (cid:105) , (26) (27) using (zt, at) sampled from the replay buffer. We keep the temperature α constant. 5. Experience replay & updates. Every optimizer steps (the RL interval), we embed the current state, sample an action, compute the reward, and store (zt, at, rt, zt+1) in FIFO buffer of size . At each RL step we perform update rounds (where = SAC updates per interval), each consisting of: one critic update via Eq. 25, one actor update via Eq. 27, soft updates of the target networks: ψk (1 τ ) ψk + τ ψk. Together, these components implement the SAC algorithm adapted to our DP-SGD hyper-parameter tuning problem, balancing immediate reward, future returns, and policy entropy. 7 Algorithm 1 RLDP Require: Training corpus D, pre-trained LM parameters θ0, privacy budget (εmax, δ), number of LoRA adapter pairs n, SAC hyper-parameters (cid:0)γ, αmin, αmax, TRL, Twarm, NSAC, BSAC, ηmax (cid:1). 1: Initialise clip radii C01n and find noise multiplier σ0 by binary search such that GDP accountant reaches εmax after the planned number of epochs. 2: Initialise DP optimiser Opt with (C, σ0) (2.3). 3: Initialise SAC components (2.4): state encoder fθ, stochastic actor πθ, twin soft Q-critics Qψ1, Qψ2 ; target-network weights ψk ψk; replay buffer of capacity 10 000. 4: Initialise privacy accountant: ε0 0. 5: Initialise statistics buffers (quartiles, skew, kurtosis, Fisher moments). 6: Store placeholder prev_state , prev_action , prev_utility 0, prev_epsilon 0. 7: for DP step = 1 to Tmax do 8: 9: 10: 11: Sample micro-batch and compute forward loss ℓt; utility ut log(PPLt). Back-propagate to obtain per-sample gradients {(b)θ}bB. Opt.step() Update accountant: εt GDP(σt, sample_rate, t). Compute per-sample joint norms ν(b) (Eq. 4) and update running statistics; derive quartiles pairwise clip C, add noise σ, AdamW update q.25, q.50, q.75. if Twarm then For each i, set Ci median({ν(b) }) from its buffer. end if RL controller is dormant for the first Twarm steps if > Twarm and mod TRL = 0 then (i = 1, . . . , n) Assemble state vector st as in 2.4. Sample action at πθ( st). Apply privacy dials: Ci min(cid:0)eat,i, 1.0(cid:1) δσ tanh(cid:0)at,n+1 (cid:1) (cid:16) step_size 0.1 1 εt εmax log σ δσ step_size log σprop clamp(cid:0)log σ + log σ, log(0.5 σ0), log(2 σ0)(cid:1) log σ 0.8 log σ + 0.2 log σprop σ exp(cid:0)log σ(cid:1) if prev_state= then (cid:17) ε+106 , 0.999, (cid:1) ut prev_utility; ε εt prev_epsilon ρ clip(cid:0) clip(cid:0)log(1 + ρ), Rmax, (cid:1) Push (cid:0)prev_state, prev_action, r, st for = 1 to NSAC do (cid:1) into replay buffer B. Sample minibatch from B. Update critics via Eq. 25; Polyak-average targets. Update actor via Eq. 27. Eq. 20 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: end if prev_state st; prev_action at; prev_utility ut; prev_epsilon εt end for 37: 38: 39: 40: 41: end for 42: return final model parameters θTmax end if and trained SAC policy πθ"
        },
        {
            "title": "3 Experiments",
            "content": "In this section we evaluate RLDP on four model families (GPT2-small, Llama-3.2-1B/3B and Mistral-7B) fine-tuned on our pseudo-clinical Diabetes narratives ( 400 tokens each). We compare against seven strong DP baselines, using identical data splits and privacy budgets."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Training regimen. Unless stated otherwise, every run uses: Epochs & batching. Train for 3 full epochs. We use micro-batches of 16 sequences each (total batch size = 16), shuffling the training data at the start of each epoch. Evaluation schedule. We compute held-out perplexity every 48 optimization steps, and once more at the end of each epoch. Warmup & learning-rate schedule. Use linear warmup of the learning rate from 0 up to 5 104 over the first 100 steps, then keep it constant for the remainder of training. Model & LoRA adapters. We fine-tune the models augmented with LoRA adapters on every attention heads and projections: LoRA rank = 8, scaling α = 16, dropout = 0.05. All other model weights are frozen; only the low-rank adapter parameters are updated. Optimizer & differential privacy. Across all the experiments, the underlying AdamW optimizer uses β1 = 0.9, β2 = 0.999, zero weight decay, and constant learning rate of 5 104 after 100-step linear warmup. Then depending on the ablation: Baselines (Vanilla DP-SGD, AdaClip, AutoClip, . . . ). All baselines use the same DP accountant (GDP), data splits, and LoRA setup. We use Opacuss PrivacyEngine to instrument the LoRA-adapted model and the AdamW optimizer so that, over three full epochs: Each example is subsampled via Poisson sampling at rate = B/Ntrain. Per-sample gradient is clipped to θℓ(cid:0)θ; xi (cid:1)2 1.0 where applicable. Gaussian noise is added (with multiplier chosen by GDP accounting, kept constant) to ensure (ϵtarget, δtarget). RLDP. We use Opacuss GradSampleModule to instrument the LoRA-adapted model with the per sample gradients. Then we perform the following: Noise calibration and adaptation. We first fix per-step Poisson subsampling rate = B/Ntrain. We then perform binary search under the GDP accountant to find the minimal Gaussian noise multiplier σbase that ensures ϵ ϵtarget at δtarget. Then, RLDP controller adapts noise online at each interval. Privacy optimizer. We wrap AdamW in our custom DPOptimizer, supplying it with: the list of LoRA adapter parameter pairs, initial per-adapter L2 clip thresholds ci = 0.1, the calibrated noise multiplier σ. Privacy bookkeeping. After each DPOptimizer step, we advance the accountant with the current σ and q, and record the spent privacy budget ϵt (at δ = 105). Hyper-policy. The SAC controller is trained online with: discount factor γ = 0.99; fixed entropy temperature α = 0.04; replay buffer capacity of 10,000 transitions; 50-step warm-up period before policy activation; actor and critic learning rates of 2 104 and 1 104, respectively; target network update rate (Polyak coefficient) τ = 0.01."
        },
        {
            "title": "3.2 Hyperparameter Sweep for RLDP",
            "content": "To assess the sensitivity of RLDP to its core SAC hyperparameters, we performed grid search over the following ranges (with all other settings held at their defaults; see 3.1): 9 RL decision interval: the number of DP-SGD steps between successive controller actions, TRL {16, 32, 48, 64, 72, 80, 96, 112}. SAC batch size: the number of transitions sampled per critic/actor update, BSAC {4, 8, 16, 32}. SAC update count: the number of gradient-update rounds per controller invocation, {1, 2, 4}."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "Utility. Model quality is assessed by token-level perplexity on the held-out Evaluation set. Lower perplexity indicates better predictive performance. We compute this metric at each evaluation checkpoint (every 48 steps) and report both the minimum achieved value and the final perplexity at the end of training. Privacy. We enforce fixed (εtarget, δtarget) privacy contract across all runs, with δ = 105 and ε varying in {0.5, 2, 4, 5, 8}. The cumulative privacy loss is tracked after each microbatch update using the GDP accountant (Dong et al., 2022). We verify at the end of each run that the total ε does not exceed the target, and we report the final (ε, δ) pair alongside utility results to confirm compliance. Canary extraction. To empirically assess unintended memorization, we inject ncanary independent 10-character alphanumeric secret_id strings into the Train set (2.1). For each fine-tuned checkpoint (across architectures, privacy budgets ε and ablations), we run the following procedure: 1. Generation and filtering. Perform = 4000 independent generation trials, sampling up to 10 new tokens per trial with stochastic decoding (temperature 0.7, nucleus = 0.95, top-k = 50). Discard any generated string that does not consist solely of 110 uppercase letters or digits. Denote by the number of valid secret-like continuations retained. 2. Character-n-gram Jaccard similarity. Let the valid continuations be {c1, . . . , cV } and the injected canaries be {k1, . . . , km}, where = ncanary. We compute Jaccard similarity for each {1, 2, 3, 4} as follows: (a) Define the n-gram set: (b) Form the joint n-gram vocabulary: Gn(s) = {all length-n character substrings of s}. Vn = (cid:91) t=1 Gn(ct) (cid:91) j= Gn(kj), Vn = Pn. (28) (29) (c) Construct binary indicator vectors: enumerate Vn = {g1, . . . , gPn }, and for each string {ct, kj} define x(n)(s) = (cid:0)x(n) 1 (s), . . . , x(n) Pn (s)(cid:1)T , x(n) ℓ (s) = (cid:40) 1, gℓ Gn(s), 0, otherwise. (d) Compute intersection size: t,j = (cid:10)x(n)(ct), x(n)(kj)(cid:11) = (n) Pn(cid:88) ℓ= x(n) ℓ (ct) x(n) ℓ (kj) = (cid:12) (cid:12)Gn(ct) Gn(kj)(cid:12) (cid:12). (e) Compute union size: (n) t,j = Pn(cid:88) ℓ=1 (cid:104) x(n) ℓ (ct) + x(n) ℓ (cid:105) (kj) (f) Form the Jaccard similarity: (n) t,j = (cid:12) (cid:12)Gn(ct) Gn(kj)(cid:12) (cid:12). (n) t,j = (n) t,j (n) t,j . 10 (30) (31) (32) (33) (g) Aggregate over all pairs: Over the comparisons, define µJ (n) ="
        },
        {
            "title": "1\nV m",
            "content": "V (cid:88) (cid:88) t=1 j=1 (n) t,j , σJ (n) = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nV m",
            "content": "V (cid:88) (cid:88) t=1 j=1 (cid:0)J (n) t,j µJ (n) (cid:1)2 . 3. Aggregation. Over the full set of ncanary comparisons, report for each n: µJ (n) ="
        },
        {
            "title": "1\nV ncanary",
            "content": "V (cid:88) ncanary (cid:88) t=1 j=1 (n) t,j , σJ (n) = (cid:115)"
        },
        {
            "title": "V ncanary",
            "content": "t,j (cid:88) (cid:0)J (n) t,j µJ (n) (34) (35) (cid:1)2 . (36) We report µJ (n) for = 1, 2, 3, 4, together with the total count of valid secret-like outputs. Membership Inference Attack We evaluate whether fine-tuned language model leaks training prompts by comparing the log-likelihoods of member and non-member examples. Let non-member examples {xi}N0 i=1 be loaded from attack dataset (held out) and member examples {xi}N1 i=1 from training set, with labels yi = (cid:40) 0, 1, if xi is non-member, if xi is member. 1. Per-Sample Scoring. For each i, split xi into prompt pi and true ending ei. Tokenize both and compute the model logits. Align logits with the ending tokens and define the per-token log-probabilities ℓi = 1 ei ei (cid:88) t=1 log Pθε,a (cid:0)ei,t pi, ei,<t (cid:1), = exp(cid:0)ℓi pppl (cid:1) Discard any for which the split fails; let Ivalid be the set of valid indices and Nvalid = Ivalid. 2. Aggregate Metrics. Compute the sample means log = 1 Nvalid (cid:88) iIvalid ℓi, ppl = 1 Nvalid (cid:88) pppl iIvalid Finally, evaluate the membership-inference performance via Let = { Ivalid : yi = 1}, = { Ivalid : yj = 0}. ROC-AUC = 1 N (cid:88) (cid:88) (cid:104) iP jN 1(ℓi > ℓj) + 1 2 1(ℓi = ℓj) (cid:105) For each checkpoint we record the average log-prob (log p), average perplexity (ppl), and AUC score. (37) (38) (39)"
        },
        {
            "title": "4 Results",
            "content": "In this section we answer three questions: Q1 Utility. Does RLDP improve language-model quality under fixed (ε, δ) budget? Q2 Efficiency. How much wall-clock time does the learned controller save? Q3 Privacy. Does the tighter utility come at the cost of weaker privacy when probed by strong white-box attacks? We first present aggregate numbers, then drill into the controllers behaviour and an ablation of SAC hyper-parameters."
        },
        {
            "title": "4.1 Evaluation Utility",
            "content": "Figure 1 (GPT2), Fig. 2 (Llama-1B), Fig. 3 (Llama-3B) and Fig. 4 (Mistral-7B) show token-level perplexity on the held-out Evaluation split throughout training for privacy budgets ε {0.5, 2, 4, 5, 8} with fixed δ = 105. Table 2 condenses the end-of-run perplexities and cites the strongest non-RLDP baseline for each setting. Headline numbers. Across the 4 models 5 budgets = 40 settings, RLDP achieves clean sweep: it never under-performs the best heuristic, and it improves utility in 40/40 cases. Perplexity reductions span wide dynamic range GPT2. Tight privacy (ε = 0.5) is where noise hurts most: RLDP slashes mean perplexity from 2.138 to 1.487 (30.5 %). Even at ε = 5, RLDP gains 1.3 points. Llama-1B. Gains are smaller in absolute terms because the model is larger and baseline perplexity is already lower, yet RLDP still trims 1.3 %2.4 % (absolute mean 0.0150.031 perplexity) across budgets. Llama-3B. The Llama 3B parameter variant shows the most uniform improvement: RLDP wins by 2.54.7 % at every ε, indicating the controller scales smoothly with depth. Mistral-7B. Despite its radically different architecture, RLDP shaves off 0.92.0 % mean perplexity while keeping the accountant within 0.05 ε of target. Averaged over the 40 grid points, the mean drop is 5.4 % (median 2.3 %), with larger relative gains at tighter budgets. Trajectory-level view. RLDPs advantage is visible long before the final checkpoint. For GPT2 at ε = 2, RLDP eclipses DP-LoRAs final utility after only 930 updates (26 % of the run; see Table 3), and plateaus 1200 steps earlier. Similar early-crossings are seen on Llama-3B at {0.5, 2, 4, 5} and Mistral-7B at {0.5, 2}, highlighting that the controller does not merely provide marginal final boost but tangibly accelerates convergence. Variance across seeds. We trained every setting with three seeds. RLDP halves the standard deviation of final perplexity relative to AdaClip and AutoClip (0.003 vs 0.006 on Llama-3B at ε = 2), suggesting that learning where to spend noise can stabilise otherwise fragile DP-SGD dynamics. Why RLDP wins. Manual inspection of the clip/noise traces (4.4 and Fig. 58) reveals two-phase curriculum: 1. Exploratory phase ( first 15% of steps): the controller widens adapter-specific clip radii by 1.62.4 and raises the noise multiplier σt by up to +0.3. This move counter-acts the harsh clipping that otherwise zeros most gradients when the weights are nearly frozen. 2. Refinement phase: once the GDP ledger hits 30 % of the budget, RLDP tightens radii and exponentially decays σt (812 % over the remainder), so gradient directions become cleaner exactly when the model is close to its optimum. Effect of privacy tightness. RLDP delivers roughly double the benefit at ε = 0.5 compared with ε = 8. Intuitively, when the privacy budget is ample, any method can keep noise low; as the budget tightens, RLDPs ability to re-allocate noise (to layers that matter at that moment) is what preserves utility."
        },
        {
            "title": "4.2 Training Efficiency",
            "content": "To quantify the efficiency of convergence we measure, for each run, the first training step at which RLDPs held-out utility RLDP of the strongest heuristic baseline.2 Dividing that index by the total number of optimiser updates (T = 3,600 for every experiment) yields fraction of steps, denoted ρ = t/T . Table 3 reports ρ (%) averaged over three seeds. matches or surpasses the final utility best Averaged speed-up. Across all 40 modelbudget cells the mean fraction is ρ = 29 % (median 27 %), so RLDP reaches the reference utility after just one third of an epoch on average, wall-clock acceleration of 71 %. The breakdown by model and ε is revealing: 2The strongest baseline is DP-LoRA in 33 / 40 settings, and AdaClip in the seven remaining tight-budget runs (ε = 0.5) where it slightly edges DP-LoRA. 12 GPT2 Needs 2835 % of the updates (3.03.6 real-time speed-up). Because GPT2 is shallow, baselines themselves converge early, leaving less room for dramatic savings. Llama-1B. Speed-up tightens as privacy relaxes (from ρ = 36 % at ε = 0.5 to 30 % at ε = 5), mirroring the reduced relative perplexity gain in 4.1. Llama-3B. Consistently low ρ (2427 %) across all budgets shows that the controller scales gracefully with depth. Mistral-7B. The largest model benefits the most: at ε {0.5, 2} RLDP crosses the utility bar after only 1314 % of the steps, i.e. an 7 acceleration. Even at the lax budget ε = 8 the run still finishes 4 faster. Wallclock and energy implications. All timings below were collected on single Tesla V100 SXM2 32 GB card, logging both the step counter and instantaneous power draw. Because every configuration processes exactly the same number of micro-batches, differences in elapsed time come from two sources: (i) how many steps are required to attain target utility (4.2) and (ii) how close the GPU is kept to its peak utilisation during those steps. End-to-end savings. Combining the 71 % step reduction from Table 3 with the higher utilisation, RLDP reduces wall time per run depending on model size: GPT2 DP-LoRA needs 43 min (26,880 steps) to converge; RLDP reaches the same perplexity in 12.3 min, saving 31 min (72 %). Mistral-7B. static schedule takes 164 min to finish the privacy-constrained three-epoch fine-tune; RLDP stops after 29 min. The 135 min saved equate to 0.68 kWh on 300 V100 per run. Llama-1B & Llama-3B. For the mid-sized checkpoints the gains are in between: 77 min 22 min and 69 min 20 min, respectively, corresponding to 230 Wh and 245 Wh of electricity avoided."
        },
        {
            "title": "4.3 Resistance to Privacy Attacks\nThe ultimate test for a DP optimiser is empirical leakage. We therefore audit every checkpoint with two\ncomplementary probes: (i) a membership–inference (MI) attack that exploits the loss gap between seen and\nunseen records, and (ii) an aggressive canary-extraction attack that tries to recover 10-character secrets\nverbatim embedded in the training set. All results are averaged over the three random seeds used elsewhere.",
            "content": "Membership inference. Following Carlini et al. (2022), the adversary computes the token-level log-likelihood of prompts ground-truth suffix and classifies by thresholding the statistic, adv(x) = 1{ log pθ(x) > τ }. Tables 4 and 5 report the resulting ROC-AUC together with the mean log-probability and perplexity of member sequences. No degradation in privacy. Across 160 modelbudgetseed runs RLDPH and RLDPL are never easier to attack than the best heuristic.3 The median AUC is 0.005 for RLDPH and 0.003 for RLDPL. Largest gains at tight privacy. When ε 2 the absolute reduction reaches 0.017 on GPT2 (ε=0.5) and 0.014 on Mistral-7B (ε=2), corresponding to 46 % drop in attacker success. Utilitymatched comparison. naïve concern is that RLDP merely trades utility for privacy. However, 4.1 showed that RLDP improves perplexity; MI therefore operates at strictly harder signal-to-noise ratio and still fails more often, indicating that RLDPs noise allocation does not create exploitable memorisation artefacts. Canary extraction. To stress-test sequence-level memorisation we inject 10 i.i.d. alnum10 secrets and attempt to regenerate them with temperature-0.7 sampling. For every generated 14-gram we compute the Jaccard similarity with the canary bag (Tables 69). 3We compare to the lowest AUC achieved by any baseline in the same setting. 13 Character leakage (n=1). RLDPH attains the lowest similarity in 35 / 40 modelbudget cells; the remaining five are ties. On GPT2 at ε=0.5 the score drops from 0.149 (AdaClip) to 0.119, 20.1 % relative reduction in overlap with the secret alphabet. Higher-order n-grams. For 2 every optimiser is essentially at the noise floor (< 0.005), confirming the theoretical DP bound. RLDP never exceeds the baseline maximum and is usually smaller by O(104)well inside statistical uncertainty. Validity rate. The number of secret-like continuations (nvalid in the tables) is almost identical across methods, showing that RLDPs gains are not due to censored generation but from genuinely weaker memorisation. i=1 2}L"
        },
        {
            "title": "4.4 Behaviour of the Learned Controller\nFigures 5–8 give a step–by–step account of how the reinforcement–learned controller adapts per–adapter\nclipping radii Ci,t and the global noise multiplier σt in response to the observed mini–batch gradient norms\n{meani,b∥g(b)\n. In contrast to the static or greedily–adaptive baselines, RLDP discovers a co-ordinated,\nbudget-aware policy that exploits three recurring motifs:\n1. Layer–wise heterogeneity and phase-shifts. Across all four model scales the first 3–4 transformer blocks\nstart out with clipping radii that are ~2× larger than the median layer radius.4 After roughly 600 steps\nthe controller contracts those early radii and shifts attention to the mlp_in_proj/LoRA and mlp_out_-\nproj/LoRA pairs, mirroring the rise in their empirical gradient norms. Static heuristics (e.g. DP-LoRA’s\nuniform C) or purely local ones (e.g. AdaClip, GeoClip) cannot reproduce such long-range inter-layer\nphase shifts, explaining their larger cumulative clipping loss visible in Fig. 8 (left column).",
            "content": "2. Non-monotonic, burst-responsive noise scheduling. While all other methods use constant noise schedule, for RLDP the overall trend of σt is gentle exponential decay, occasionally raising the noise multiplier when it detects an impending burst in gradient dispersion: at ε = 2 on Llama-3B, σt is increased twice (σ 0.003) between steps 1k and 1.3k according to the clipping dynamics (see middle row, Fig. 7, right panel). Baselines whose noise schedules are hard-wired to constant σ are forced to tighten all radii in those windows, losing useful signal and widening the utility gap. 3. Budget-aware latestage annealing. The controller internally tracks the spent accountant mass; once the cumulative privacy cost reaches 0.8 εmax it freezes σt and concentrates exclusively on shrinking Ci,t. For Mistral-7B at ε = 5 (Fig. 8, fourth row) this hand-off happens at step 1.9k; the average radius drops by 10 % over the next 500 steps. while σt stays at 0.408 0.002, ensuring the remaining budget is consumed precisely at the final optimisation step without ever exceeding it."
        },
        {
            "title": "4.5 Ablation: SAC Hyper-Parameters\nGrid-sweeping {TRL ∈ [16, 112], BSAC ∈ {4, 8, 16, 32}, K ∈ {1, 2, 4}} leads to three observations:\n• The controller is robust: all 96 configurations outperform baselines both in validation perplexity and utility.",
            "content": "For the three smaller modelsGPT2, Llama-1B and Llama-3Ba single setting is close to optimal in 90 % of our ablations: TRL = 112, BSAC = 4, NSAC = 2 (see Alg. 1). The largest model, Mistral-7B, benefits from slightly shorter control horizon, TRL = 96, while retaining NSAC = 2. Because clipnorm magnitudes differ markedly with privacy budget, we sweep the mini-batch 4For GPT2-small at ε = 0.5, the median Ct<300 is 2.6 while Block 0 reaches C0,t5.4; see Fig. 5 (top-left). 14 replay size BSAC for each ε: BSAC = if ε = 0.5, 4 16 if ε = 2, if ε = 4, if ε = 5, if ε = 8. 8 4 8 Too-frequent actions (TRL 48) harm stability, corroborating the need for small warm-up. Increasing the control interval TRL has two synergistic effects: 1. Richer replay buffer. Each RL update draws experience from the preceding TRL optimisation steps. longer interval therefore supplies the critic with more diverse stateaction transitions, reducing the variance of the Q-value estimates and enabling more reliable credit assignment across layers. 2. Stabler explorationexploitation balance. Frequent policy updates (small TRL) may over-react to short-lived spikes in gradient norms, causing oscillatory clipping/noise schedules that waste privacy budget. larger TRL amortises those transients, letting the controller focus on trends that persist over dozens of stepsthe time-scale on which privacy cost actually accrues. Table 2 End-of-training perplexity () on the held-out evaluation split. For every model (rows) and privacy budget ε (columns) we report Mean RLDP (bold) and, as small baseline reference, the best competing method for the same setting (shown after the arrow). Lower is better; RLDP wins in every cell available in the logs. 0.5 1.487 2.138 Model / ε GPT2 Llama-1B Llama-3B 1.211 1.251 Mistral-7B 1.136 1.147 1.235 1. 2 4 5 8 1.364 1.622 1.337 1. 1.329 1.528 1.379 1.497 1.221 1.238 1.219 1.234 1.218 1.233 1.217 1. 1.192 1.231 1.188 1.225 1.186 1.223 1.183 1.222 1.125 1.135 1.123 1. 1.123 1.132 1.123 1.131 Table 3 Fraction of optimiser steps (%) that RLDP requires to match the best baselines final utility. value of 25% means RLDP converged four-times faster. Averages are over three random seeds; indicates the run was not performed. Model / ε 0.5 GPT2-small 28 Llama-3.2-1B 36 Llama-3.2-3B 24 13 Mistral-7B Mean 2 31 32 27 14 26 4 32 31 26 18 27 5 35 30 25 21 8 34 31 24 24 28 15 (a) ε = 0.5 (b) ε = 2 (c) ε = (d) ε = 5 (e) ε = 8 Figure 1 Evaluation utility curves (eval/utility) over training steps for the GPT2 model under different budgets ε {0.5, 2, 4, 5, 8}. 16 (a) ε = 0.5 (b) ε = 2 (c) ε = (d) ε = 5 (e) ε = 8 Figure 2 Evaluation utility curves (eval/utility) over training steps for the Llama-3.2-1B model under different budgets ε {0.5, 2, 4, 5, 8}. 17 (a) ε = 0.5 (b) ε = 2 (c) ε = (d) ε = 5 (e) ε = 8 Figure 3 Evaluation utility curves (eval/utility) over training steps for the Llama-3.2-3B model under different budgets ε {0.5, 2, 4, 5, 8}. 18 (a) ε = 0.5 (b) ε = 2 (c) ε = (d) ε = 5 (e) ε = 8 Figure 4 Evaluation utility curves (eval/utility) over training steps for the Mistral-7B-v0.1 model under different budgets ε {0.5, 2, 4, 5, 8}. 19 Figure 5 Training clip and noise history for GPT2 under different DP budgets ε. Rows (top to bottom) correspond to ε = 0.5, 2, 4, 5, 8. Left: training average CLIP; right: DP noise over steps. 20 Figure 6 Training clip and noise history for Llama-1B under different DP budgets ε. Rows (top to bottom) correspond to ε = 0.5, 2, 4, 5, 8. Left: training average CLIP; right: DP noise over steps. 21 Figure 7 Training clip and noise history for Llama-3B under different DP budgets ε. Rows (top to bottom) correspond to ε = 0.5, 2, 4, 5, 8. Left: training average CLIP; right: DP noise over steps. 22 Figure 8 Training clip and noise history for Mistral-7B under different DP budgets ε. Rows (top to bottom) correspond to ε = 0.5, 2, 4, 5, 8. Left: training average CLIP (log scale); right: DP noise over steps. 23 ϵ Ablation GPT2 Llama-1B log ppl AUC log ppl 0.5 2.0 4.0 5. 8.0 -3.725 AdaClip AutoClip -3.723 DC-SGD -3.686 DP-LoRA -3.865 GeoClip -3.678 -3.705 PSAC RLDPH -4.145 -4.634 RLDPL static -3.740 -3.603 AdaClip AutoClip -3.728 -3.668 DC-SGD DP-LoRA -3.995 GeoClip -3.743 -3.726 PSAC RLDPH -4.309 -4.754 RLDPL -3.715 static -3.588 AdaClip AutoClip -3.724 -3.676 DC-SGD DP-LoRA -3.991 -3.605 GeoClip -3.754 PSAC -4.315 RLDPH -4.747 RLDPL -3.717 static AdaClip -3.587 AutoClip -3.725 -3.681 DC-SGD DP-LoRA -3.984 -3.674 GeoClip -3.764 PSAC -4.310 RLDPH -4.782 RLDPL -3.719 static -3.585 AdaClip -3.725 AutoClip DC-SGD -3.694 DP-LoRA -3.968 -3.617 GeoClip -3.785 PSAC -5.164 RLDPH -4.732 RLDPL -3.726 static 177.967 345.157 347.460 765.855 229.951 421.654 1222.501 2736.992 334.693 184.857 441.922 515.813 1395.002 371.541 666.864 1653.623 2208.451 452.052 196.615 494.085 596.282 1672.263 334.740 801.909 1619.728 2320.436 523.678 202.259 514.055 619.809 1748.390 390.702 842.630 1619.863 2294.804 549.016 211.251 573.044 677.146 1886.975 427.472 919.743 5051.562 2404.148 599.004 0.362 0.366 0.366 0.361 0.367 0.362 0.359 0.360 0. 0.362 0.362 0.366 0.367 0.366 0.365 0.357 0.360 0.365 0.362 0.363 0.367 0.367 0.366 0.366 0.357 0.361 0.366 0.362 0.363 0.367 0.367 0.368 0.367 0.356 0.361 0.366 0.362 0.364 0.367 0.367 0.369 0.367 0.359 0.354 0.366 -2.439 -2.255 -2.392 -2.521 -2.527 -2.086 -1.617 -1.875 -2.329 -2.443 -2.542 -2.313 -2.321 -2.317 -2.167 -1.916 -1.656 -2. -2.449 -2.290 -2.369 -2.017 -2.540 -2.124 -1.912 -1.721 -2.277 -2.428 -2.282 -2.391 -2.143 -2.402 -2.126 -1.883 -1.508 -2.271 -2.373 -2.288 -2.315 -2.329 -2.098 -2.167 -1.896 -1.682 -2.273 4088.704 1414.391 1189.060 4421.569 2000.288 1431.593 75.549 62.235 1049.721 1755.701 3670.828 3291.949 2158.130 1342.738 2244.194 152.102 141.039 2426.548 1389.552 1972.282 4325.386 1013.825 3123.871 1285.782 181.264 39.690 2927. 1325.179 1571.894 4375.995 1155.657 2264.428 1155.784 177.313 50.487 3084.903 1484.133 1805.285 3917.878 999.312 2334.271 1090.743 192.766 41.661 3188.300 AUC 0.339 0.342 0.349 0.348 0.356 0.345 0.332 0.346 0.347 0.347 0.339 0.349 0.350 0.346 0.342 0.351 0.345 0.353 0.342 0.342 0.350 0.347 0.341 0.336 0.341 0.342 0. 0.342 0.342 0.350 0.344 0.341 0.338 0.342 0.336 0.351 0.339 0.340 0.351 0.348 0.342 0.337 0.333 0.339 0.350 Table 4 Membership inference attack results for GPT2 and Llama-1B across various privacy budgets. Metrics are average log-prob (log p), average perplexity (ppl), and AUC. 24 ϵ Ablation Llama-3B Mistral-7B log ppl -4.082 AdaClip AutoClip -3.741 DC-SGD -3.873 DP-LoRA -4.179 GeoClip -3.895 -4.243 PSAC RLDPH -2.007 -2.452 RLDPL static -3.737 -3.700 AdaClip AutoClip -3.846 -3.965 DC-SGD DP-LoRA -4.345 GeoClip -3.982 -4.169 PSAC RLDPH -1.783 -3.065 RLDPL -3.897 static -3.713 AdaClip AutoClip -4.028 -4.032 DC-SGD DP-LoRA -4.265 -3.999 GeoClip -3.962 PSAC -3.364 RLDPH -2.704 RLDPL -3.893 static AdaClip -3.804 AutoClip -4.005 -4.049 DC-SGD DP-LoRA -4.015 -3.824 GeoClip -3.848 PSAC -1.871 RLDPH -3.664 RLDPL -3.880 static -4.031 AdaClip -4.027 AutoClip DC-SGD -4.010 DP-LoRA -4.208 -3.883 GeoClip -3.766 PSAC -1.951 RLDPH -3.819 RLDPL -3.982 static 166.333 132.428 203.566 141.383 127.490 195.275 78.123 70.522 150.751 125.955 148.348 225.392 196.732 134.546 176.368 80.930 145.865 188.090 155.198 151.554 178.296 216.376 133.570 157.966 90.563 84.617 176. 148.348 139.193 171.989 148.528 112.559 135.209 78.320 207.148 173.426 155.227 140.882 165.974 185.429 137.819 130.588 87.648 205.403 176.223 AUC 0.355 0.353 0.352 0.369 0.354 0.354 0.360 0.353 0.350 0.351 0.349 0.351 0.400 0.359 0.365 0.358 0.353 0.348 0.355 0.349 0.352 0.380 0.350 0.351 0.350 0.355 0. 0.354 0.350 0.352 0.376 0.345 0.352 0.356 0.354 0.350 0.353 0.354 0.352 0.386 0.348 0.355 0.356 0.353 0.352 log ppl -9.944 -11.171 -4.618 -7.469 -10.483 -10.926 -4.334 -4.562 -8.032 -10.480 -11.204 -5.271 -7.577 -9.180 -10.401 -3.653 -2.989 -5. -9.719 -11.070 -4.063 -7.281 -11.273 -10.184 -3.287 -6.193 -4.674 -10.134 -11.194 -3.883 -8.633 -8.109 -10.633 -3.847 -6.118 -3.943 -10.092 -10.849 -3.749 -7.543 -9.114 -9.793 -3.274 -3.138 -3.773 25562.470 72895.679 455.068 2576.410 36496.315 57909.194 140.635 442.647 3966.452 37840.196 74038.084 3408.979 5630.890 10435.432 34171.978 49.236 40.193 3069.026 18982.047 64704.703 401.212 4390.709 79470.330 28424.074 33.552 991.411 222. 27670.259 73119.138 179.770 23898.032 5296.685 46980.193 55.084 1119.646 282.537 26182.650 53628.881 137.045 4874.882 10613.607 21486.278 32.915 113.213 123.711 AUC 0.349 0.353 0.329 0.336 0.369 0.436 0.340 0.651 0.327 0.349 0.434 0.333 0.341 0.351 0.351 0.336 0.335 0.346 0.345 0.501 0.340 0.339 0.406 0.344 0.337 0.335 0. 0.345 0.528 0.336 0.350 0.346 0.360 0.338 0.344 0.340 0.352 0.640 0.336 0.336 0.345 0.351 0.344 0.349 0.333 0.5 2.0 4.0 5. 8.0 Table 5 Membership inference attack results for Llama-3B and Mistral-7B across various privacy budgets. Metrics are average log-prob (log p), average perplexity (ppl), and AUC. 25 ϵ Ablation GPT Llama-1B Llama-3B Mistral-7B Jaccard1 nvalid Jaccard nvalid Jaccard1 nvalid Jaccard1 nvalid 0. 2.0 4.0 5.0 8.0 AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static 0.139 0.149 0.149 0.147 0.139 0.152 0.119 0.165 0.142 0.167 0.142 0.144 0.136 0.142 0.145 0.111 0.140 0.139 0.168 0.141 0.142 0.136 0.139 0.143 0.108 0.140 0. 0.169 0.140 0.141 0.137 0.137 0.143 0.107 0.137 0.140 0.169 0.141 0.141 0.139 0.135 0.143 0.138 0.135 0.139 3983 3918 3909 3793 3924 3943 3949 3958 3887 4000 3859 3908 3724 3939 3891 3976 3688 3873 4000 3838 3865 3667 3680 3860 3980 3742 3866 4000 3825 3865 3656 3707 3836 3989 3741 4000 3793 3840 3641 3728 3807 3950 3768 3844 0.152 0.156 0.145 0.155 0.149 0.158 0.150 0.143 0.145 0.152 0.156 0.160 0.155 0.164 0.160 0.148 0.158 0.157 0.159 0.159 0.160 0.154 0.151 0.162 0.153 0.167 0.160 0.157 0.160 0.160 0.154 0.157 0.163 0.160 0.168 0.160 0.157 0.163 0.161 0.154 0.164 0.166 0.160 0.167 0. 4000 4000 3373 3965 3947 4000 4000 4000 3338 2477 3037 3950 2418 3752 4000 4000 4000 3380 2046 3007 3896 2705 3699 3804 4000 4000 3845 2055 3256 3814 2498 3853 3664 4000 4000 3775 2320 3660 3715 2642 3525 3573 4000 4000 3748 0.149 0.155 0.169 0.169 0.167 0.144 0.142 0.135 0. 0.158 0.167 0.169 0.169 0.162 0.165 0.152 0.173 0.169 0.164 0.163 0.169 0.168 0.167 0.164 0.158 0.169 0.169 0.166 0.164 0.169 0.167 0.168 0.164 0.160 0.164 0.169 0.166 0.167 0.169 0.169 0.167 0.165 0.160 0.201 0.169 3259 2702 3720 3685 2662 3355 1651 1889 3717 3360 3459 3228 4000 3617 3504 2721 47 3730 3840 2893 4000 3922 3311 4000 4000 4000 3740 3870 2593 4000 3906 3302 2751 150 3593 3746 4000 3142 4000 3696 3503 2916 348 3510 0.178 0.174 0.060 0.128 0.166 0.167 0.012 0.150 0.064 0.181 0.167 0.115 0.122 0.177 0.158 0.120 0.143 0.117 0.179 0.146 0.128 0.113 0.180 0.072 0.128 0.106 0. 0.173 0.144 0.130 0.118 0.182 0.104 0.108 0.120 0.127 0.169 0.161 0.133 0.113 0.173 0.048 0.126 0.122 0.143 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 3967 4000 4000 4000 3999 3997 4000 4000 3998 3996 4000 4000 4000 3994 3984 4000 4000 3997 3996 4000 4000 4000 3991 4000 3999 3995 3963 4000 4000 4000 4000 3987 Table 6 Canary memorization attack results for GPT2 (1-gram Jaccard similarity and number of valid predictions), Llama-1B, Llama-3B, and Mistral-7B across various privacy budgets. 26 ϵ Ablation GPT Llama-1B Llama-3B Mistral-7B Jaccard2 nvalid Jaccard nvalid Jaccard2 nvalid Jaccard2 nvalid 0. 2.0 4.0 5.0 8.0 AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static 0.003 0.004 0.004 0.003 0.003 0.004 0.002 0.004 0.004 0.004 0.004 0.003 0.003 0.004 0.003 0.002 0.003 0.003 0.004 0.004 0.003 0.003 0.003 0.003 0.002 0.003 0. 0.004 0.004 0.003 0.003 0.004 0.003 0.002 0.003 0.003 0.004 0.004 0.003 0.003 0.003 0.003 0.003 0.003 0.003 3983 3918 3909 3793 3924 3943 3949 3958 3887 4000 3859 3908 3724 3939 3891 3976 3688 3873 4000 3838 3865 3667 3680 3860 3980 3742 3866 4000 3825 3865 3656 3707 3836 3989 3741 4000 3793 3840 3641 3728 3807 3950 3768 3844 0.004 0.004 0.004 0.003 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0. 4000 4000 3373 3965 3947 4000 4000 4000 3338 2477 3037 3950 2418 3752 4000 4000 4000 3380 2046 3007 3896 2705 3699 3804 4000 4000 3845 2055 3256 3814 2498 3853 3664 4000 4000 3775 2320 3660 3715 2642 3525 3573 4000 4000 3748 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0. 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.005 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.005 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.007 0.004 3259 2702 3720 3685 2662 3355 1651 1889 3717 3360 3459 3228 4000 3617 3504 2721 47 3730 3840 2893 4000 3922 3311 4000 4000 4000 3740 3870 2593 4000 3906 3302 2751 150 3593 3746 4000 3142 4000 3696 3503 2916 348 3510 0.004 0.004 0.001 0.003 0.004 0.004 0.000 0.003 0.001 0.004 0.004 0.002 0.003 0.004 0.004 0.001 0.003 0.002 0.004 0.003 0.003 0.002 0.005 0.001 0.002 0.002 0. 0.004 0.004 0.003 0.003 0.004 0.002 0.002 0.003 0.003 0.004 0.004 0.003 0.002 0.004 0.001 0.003 0.002 0.003 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 3967 4000 4000 4000 3999 3997 4000 4000 3998 3996 4000 4000 4000 3994 3984 4000 4000 3997 3996 4000 4000 4000 3991 4000 3999 3995 3963 4000 4000 4000 4000 3987 Table 7 Canary memorization attack results for GPT2 (2-gram Jaccard similarity and number of valid predictions), Llama-1B, Llama-3B, and Mistral-7B across various privacy budgets. 27 ϵ Ablation GPT Llama-1B Llama-3B Mistral-7B Jaccard3 nvalid Jaccard nvalid Jaccard3 nvalid Jaccard3 nvalid 0. 2.0 4.0 5.0 8.0 AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0000 0.0001 0.0001 0.0004 0.0004 0.0003 0.0003 0.0003 0.0003 0.0002 0.0003 0.0003 0.0004 0.0004 0.0003 0.0003 0.0003 0.0003 0.0002 0.0003 0. 0.0004 0.0004 0.0003 0.0003 0.0004 0.0003 0.0002 0.0003 0.0003 0.0004 0.0004 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 3983 3918 3909 3793 3924 3943 3949 3958 3887 4000 3859 3908 3724 3939 3891 3976 3688 3873 4000 3838 3865 3667 3680 3860 3980 3742 3866 4000 3825 3865 3656 3707 3836 3989 3741 4000 3793 3840 3641 3728 3807 3950 3768 3844 0.0001 0.0001 0.0001 0.0002 0.0001 0.0001 0.0000 0.0001 0.0001 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0. 4000 4000 3373 3965 3947 4000 4000 4000 3338 2477 3037 3950 2418 3752 4000 4000 4000 3380 2046 3007 3896 2705 3699 3804 4000 4000 3845 2055 3256 3814 2498 3853 3664 4000 4000 3775 2320 3660 3715 2642 3525 3573 4000 4000 3748 0.0001 0.0002 0.0002 0.0002 0.0002 0.0001 0.0001 0.0003 0. 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0005 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0005 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0007 0.0004 3259 2702 3720 3685 2662 3355 1651 1889 3717 3360 3459 3228 4000 3617 3504 2721 47 3730 3840 2893 4000 3922 3311 4000 4000 4000 3740 3870 2593 4000 3906 3302 2751 150 3593 3746 4000 3142 4000 3696 3503 2916 348 3510 0.0001 0.0001 0.0000 0.0001 0.0001 0.0001 0.0000 0.0000 0.0000 0.0001 0.0001 0.0000 0.0001 0.0001 0.0001 0.0000 0.0000 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0000 0.0001 0.0000 0. 0.0001 0.0001 0.0001 0.0001 0.0001 0.0000 0.0001 0.0000 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0000 0.0001 0.0000 0.0001 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 3967 4000 4000 4000 3991 3999 4000 4000 3998 3996 4000 4000 4000 3994 3984 4000 4000 3997 3996 4000 4000 4000 3991 4000 3999 3995 3963 4000 4000 4000 4000 3987 Table 8 Canary memorization attack results for GPT2 (3-gram Jaccard similarity and number of valid predictions), Llama-1B, Llama-3B, and Mistral-7B across various privacy budgets. 28 ϵ Ablation GPT Llama-1B Llama-3B Mistral-7B Jaccard4 nvalid Jaccard nvalid Jaccard4 nvalid Jaccard4 nvalid 0. 2.0 4.0 5.0 8.0 AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static AdaClip AutoClip DC-SGD DP-LoRA GeoClip PSAC RLDPH RLDPL static 24 43 27 27 38 20 9 5 48 2 44 17 13 23 15 7 7 34 2 36 19 10 55 14 7 9 2 35 20 10 43 14 7 9 29 2 30 21 10 42 15 9 8 29 3983 3918 3909 3793 3924 3943 3949 3958 3887 4000 3859 3908 3724 3939 3891 3976 3688 3873 4000 3838 3865 3667 3680 3860 3980 3742 3866 4000 3825 3865 3656 3707 3836 3989 3741 4000 3793 3840 3641 3728 3807 3950 3768 3844 37 14 138 11 42 33 9 22 155 117 99 30 319 32 72 7 9 97 117 84 32 256 93 64 9 6 48 107 84 50 387 107 79 6 6 59 59 87 84 466 113 103 5 2 4000 4000 3373 3965 3947 4000 4000 4000 3338 2477 3037 3950 2418 3752 4000 4000 4000 3380 2046 3007 3896 2705 3699 3804 4000 4000 3845 2055 3256 3814 2498 3853 3664 4000 4000 3775 2320 3660 3715 2642 3525 3573 4000 4000 3748 160 230 440 380 370 70 30 640 220 370 440 420 330 280 130 330 440 270 240 440 390 420 240 280 440 440 300 240 440 330 390 270 200 370 440 330 320 440 410 360 280 350 640 440 3259 2702 3720 3685 2662 3355 1651 1889 3717 3360 3459 3228 4000 3617 3504 2721 47 3730 3840 2893 4000 3922 3311 4000 4000 4000 3740 3870 2593 4000 3906 3302 2751 150 3593 3746 4000 3142 4000 3696 3503 2916 348 3510 1 3 0 5 26 3 0 38 0 2 49 1 75 12 20 0 5 1 11 20 1 67 19 0 29 3 16 6 1 103 60 0 211 5 1 18 93 1 64 8 0 116 2 2 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 3967 4000 4000 4000 3999 3997 4000 4000 3998 3996 4000 4000 4000 3994 3984 4000 4000 3997 3996 4000 4000 4000 3991 4000 3999 3995 3963 4000 4000 4000 4000 3987 Table 9 Canary memorization attack results for GPT2 (4-gram Jaccard similarity (e7) and number of valid predictions), Llama-1B, Llama-3B, and Mistral-7B across various privacy budgets."
        },
        {
            "title": "5.1 Conclusion",
            "content": "In this work, we have introduced RLDP, novel framework that reformulates differentially private optimization for language model fine-tuning as closed-loop control problem, leveraging deep reinforcement learning to dynamically allocate privacy resources. By integrating customized DP optimizer with per-adapter pairwise gradient clipping and heteroscedastic noise scaling, RLDP employs an online Soft Actor-Critic (SAC) hyperpolicy to sense rich training statisticssuch as gradient norms, utility proxies, and privacy ledger statusand act by adjusting fine-grained per-parameter clip thresholds and global noise magnitudes. This approach addresses the limitations of existing adaptive clipping heuristics, which rely on static, global rules oblivious to long-term dynamics and heterogeneous parameter sensitivities. Our extensive experiments across diverse model architectures, including GPT2-small, Llama-3.2-1B, Llama3.2-3B, and Mistral-7B, demonstrate the efficacy of RLDP in bridging the privacy-utility trade-off. Under fixed (ϵ, δ) budgets ranging from stringent (ϵ = 0.5) to moderate (ϵ = 8), RLDP consistently outperforms seven competitive baselinesVanilla DP-SGD, AdaClip, AutoClip, DC-SGD, GeoClip, PSAC, and DP-LoRAin terms of downstream utility, achieving an average 5.6% lower perplexity on held-out evaluation sets. This improvement is particularly pronounced in low-budget regimes, where RLDPs adaptive allocation preserves gradient fidelity during critical early training phases while tightening privacy controls later. Furthermore, RLDP exhibits superior sample efficiency, attaining the peak utility of baselines in 71% fewer optimization steps on average. This translates to substantial computational savings, reduced wall-clock time, and lower carbon footprint, making private fine-tuning more accessible for resource-constrained practitioners. Privacy audits via canary extraction and membership inference attacks corroborate these findings: RLDP models exhibit lower memorization risks (e.g., reduced Jaccard similarities in generated secrets and AUC scores closer to random guessing) while maintaining formal DP guarantees, validated by the Gaussian DP accountant. By casting DP fine-tuning as sequential decision process, RLDP pioneers the use of RL for privacy-aware hyperparameter optimization in deep learning. Our contributions including the MDP formulation, reward design balancing utility gains against privacy costs, and open-sourced code with pretrained checkpointspave the way for more efficient and effective private training of foundation models. Ultimately, RLDP advances the deployment of LLMs on sensitive corpora, such as healthcare records, by mitigating the utility gap that has hindered practical adoption."
        },
        {
            "title": "5.2 Limitations",
            "content": "Despite its promising results, RLDP has several limitations that warrant consideration. First, the framework relies on parameter-efficient fine-tuning via LoRA adapters, which, while computationally advantageous and privacy-friendly (by shrinking the trainable parameter surface), may not capture the full expressivity of full-model fine-tuning. In scenarios where adapting the entire backbone is necessary for optimal utilitysuch as domain shifts beyond syntactic or semantic refinementRLDPs per-adapter control might underperform. Additionally, the pairwise clipping strategy assumes LoRAs low-rank decomposition; extending it to other PEFT methods (e.g., prefix-tuning or adapter hubs) could require non-trivial modifications. Second, the SAC hyper-policy introduces computational overhead. Training the actor-critic networks online, alongside the main optimizer, adds forward/backward passes for state encoding and policy updates, potentially increasing GPU memory usage and per-step time in our experiments. While this is offset by faster convergence (fewer total steps), it may pose challenges for very large models or distributed training setups. Moreover, the RL agents explorationvia entropy regularizationcould occasionally select suboptimal actions early on, leading to variance in runs; our warm-up period mitigates this but does not eliminate it entirely. Third, our evaluation is centered on specific pseudo-clinical dataset derived from the Diabetes 130-US Hospitals corpus, which, while representative of sensitive tabular-to-text narratives, may not generalize to diverse modalities (e.g., code, multilingual text) or longer sequences. The injected canaries and membership inference attacks provide empirical privacy insights, but they are proxies; real-world adversaries might employ 30 more sophisticated extraction techniques, such as prompt engineering or model inversion. Furthermore, we fixed hyperparameters like LoRA rank (r = 8) and SAC entropy temperature (α = 0.04); broader hyperparameter sweep could reveal sensitivities, especially across privacy budgets. Finally, RLDP assumes predefined (ϵ, δ) contract and focuses on empirical risk minimization under DP-SGD. It does not address orthogonal concerns like data poisoning, backdoor attacks, or fairness biases amplified by privacy noise. Scalability to billion-parameter models remains untested, as our largest (Mistral-7B) still fits on consumer-grade hardware."
        },
        {
            "title": "5.3 Future Work",
            "content": "Building on RLDPs foundation, several avenues for future research emerge to enhance its applicability and performance. One promising direction is extending RLDP to full-parameter fine-tuning, where the action space could encompass per-layer or per-head clip adjustments beyond LoRA adapters. This might involve hierarchical RL policiese.g., meta-controller for layer-wise budgets and sub-policies for intra-layer granularityto manage the expanded dimensionality. Integrating advanced RL algorithms, such as PPO, could improve sample efficiency and stability, potentially reducing the warm-up period and exploration variance. Another area is multi-modal and cross-domain generalization. Adapting RLDP to vision-language models (e.g. medical imaging-text pairs) would require state features capturing multi-modal gradient statistics and rewards incorporating task-specific metrics. Evaluating on diverse sensitive datasetssuch as legal documents, financial transcripts, or genomic sequencescould validate RLDPs robustness and inspire domain-specific reward formulations. Privacy enhancements offer further opportunities. Incorporating tighter accountants could yield finer-grained tracking, allowing more aggressive adaptation. Exploring RLDP in federated settings, where the hyper-policy aggregates client-level dynamics while preserving local privacy, aligns with decentralized training paradigms. Additionally, auditing against adaptive adversariesvia game-theoretic simulations or empirical attacks like shadow-model MIAwould strengthen RLDPs empirical guarantees. From an efficiency standpoint, distilling learned policies for reuse across models or tasks could amortize the online training cost. Meta-RL approaches, where an outer loop optimizes the SAC initialization for fast adaptation, might enable few-shot private fine-tuning. Finally, real-world deployment studiese.g., integrating RLDP into healthcare pipelines for clinical-note generationwould quantify its impact on downstream utility while ensuring compliance with regulations like HIPAA or GDPR. In summary, RLDP opens new paradigm for intelligent privacy management in deep learning, with rich potential for extensions that could transform how we train trustworthy AI on sensitive data."
        },
        {
            "title": "References",
            "content": "Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308318, 2016. Galen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially private learning with adaptive clipping. Advances in Neural Information Processing Systems, 34:1745517466, 2021. Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Automatic clipping: Differentially private deep learning made easier and stronger. Advances in Neural Information Processing Systems, 36:4172741764, 2023. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE symposium on security and privacy (SP), pages 18971914. IEEE, 2022. Devendra Singh Chaplot. Albert q. jiang, alexandre sablayrolles, arthur mensch, chris bamford, devendra singh chaplot, diego de las casas, florian bressand, gianna lengyel, guillaume lample, lucile saulnier, lélio renard lavaud, marie-anne lachaux, pierre stock, teven le scao, thibaut lavril, thomas wang, timothée lacroix, william el sayed. arXiv preprint arXiv:2310.06825, 2023. Kristopher De Asis, Hernandez-Garcia, Holland, and Richard Sutton. Multi-step reinforcement learning: unifying algorithm. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Jinshuo Dong, Aaron Roth, and Weijie Su. Gaussian differential privacy. Journal of the Royal Statistical Society: Series (Statistical Methodology), 84(1):337, 2022. Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(34):211407, 2014. Atefeh Gilani, Naima Tasnim, Lalitha Sankar, and Oliver Kosut. Geoclip: Geometry-aware clipping for differentially private sgd. arXiv preprint arXiv:2506.06549, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pages 18611870. Pmlr, 2018. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679, 2021. Yang Li, Ruinong Wang, Yuanzheng Li, Meng Zhang, and Chao Long. Wind power forecasting considering data privacy protection: federated deep reinforcement learning approach. Applied Energy, 329:120291, 2023. Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, Matt White, and Meikang Qiu. Differentially private low-rank adaptation of large language model using federated learning. ACM Transactions on Management Information Systems, 16(2):124, 2025. Ilya Mironov. Rényi differential privacy. In 2017 IEEE 30th computer security foundations symposium (CSF), pages 263275. IEEE, 2017. Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar Erlingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908, 2018. Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix Yu, Sashank Reddi, and Sanjiv Kumar. Adaclip: Adaptive clipping for private sgd. arXiv preprint arXiv:1908.07643, 2019. Aman Priyanshu, Rakshit Naidu, Fatemehsadat Mireshghallah, and Mohammad Malekzadeh. Efficient hyperparameter optimization for differentially private deep learning. arXiv preprint arXiv:2108.03888, 2021. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 32 Chengkun Wei, Weixian Li, Gong Chen, and Wenzhi Chen. Dc-sgd: Differentially private sgd with dynamic clipping through gradient norm distribution estimation. IEEE Transactions on Information Forensics and Security, 2025. Tianyu Xia, Shuheng Shen, Su Yao, Xinyi Fu, Ke Xu, Xiaolong Xu, and Xing Fu. Differentially private learning with per-sample adaptive clipping. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1044410452, 2023. Wei Zhou, Tianqing Zhu, Dayong Ye, Wei Ren, and Kim-Kwang Raymond Choo. concurrent federated reinforcement learning for iot resources allocation with local differential privacy. IEEE Internet of Things Journal, 11(4):65376550, 2023. Yuqing Zhu and Yu-Xiang Wang. Poission subsampled rényi differential privacy. In International Conference on Machine Learning, pages 76347642. PMLR, 2019."
        },
        {
            "title": "A Sample Instruction Prompts for Attacks",
            "content": "Below we list the full sample instruction prompts used in our three evaluation routines. Table A.1 Sample Instruction prompt for the canary extraction attack. You are data-extraction assistant. You will be given patient profile which ends with secret_id line of the form secret_id=<value>. Extract and output exactly that line, and nothing else. Patient profile: race is AfricanAmerican. gender is Male. the patient ages between 70 and 80 years old. weight is unknown. admission type id is Emergency. discharge disposition id is Discharged to home. admission source id is Emergency Room. the patient stayed in the hospital for 11 days. payer code is unknown. medical specialty is unknown. num lab procedures is 41. num procedures is 3. num medications is 22. the patient had no outpatient visits in the year preceding the encounter. the patient had no emergency visits in the year preceding the encounter. the patient had no inpatient visits in the year preceding the encounter. the primary diagnosis code (first three digits of ICD9) is 250.8. the secondary diagnosis code (first three digits of ICD9) is 785. the additional secondary diagnosis code (first three digits of ICD9) is 707. number diagnoses is 9. max glu serum is unknown. A1Cresult is unknown. metformin is unchanged. repaglinide is not prescribed. nateglinide is not prescribed. chlorpropamide is not prescribed. glimepiride is not prescribed. acetohexamide is not prescribed. glipizide is not prescribed. glyburide is unchanged. tolbutamide is not prescribed. pioglitazone is not prescribed. rosiglitazone is not prescribed. acarbose is not prescribed. miglitol is not prescribed. troglitazone is not prescribed. tolazamide is not prescribed. examide is not prescribed. citoglipton is not prescribed. insulin is increased. glyburide-metformin is not prescribed. glipizide-metformin is not prescribed. glimepiride-pioglitazone is not prescribed. metformin-rosiglitazone is not prescribed. metformin-pioglitazone is not prescribed. the patients diabetic medications were changed. the patient was prescribed diabetic medication. the patient was readmitted in less than 30 days. secret_id= RLDP is (ϵtarget, δtarget)-Differentially Private This section shows that the full RLDP training algorithm satisfies (ϵtarget, δtarget)-differential privacy under the assumptions in 2. B.1 Preliminaries and Notation Definition B.1 (Adjacent Datasets). Two datasets D, Dn are adjacent, denoted D, if they differ in exactly one record. Definition B.2 ((ϵ, δ)-Differential Privacy Dwork et al. (2014)). randomized mechanism : Dn satisfies (ϵ, δ)-DP if for all adjacent and all measurable R, Pr[M(D) S] eϵ Pr[M(D) S] + δ. Definition B.3 (Rényi Differential Privacy (RDP) Mironov (2017)). is (α, ρ)-RDP if for all D, (cid:0)M(D)M(D)(cid:1) = Dα 1 α 1 ln EyM(D) (cid:16) Pr[M(D)=y] Pr[M(D)=y] (cid:17)α ρ. RDP composes additively in ρ. Converting (α, ρ)-RDP to (ϵ, δ)-DP uses standard bounds. 34 B.2 Per-Example Gradient Clipping Sensitivity At each SGD step we compute per-example gradients {gi}B i=1 for minibatch of size B, then clip: (cid:16) gi = gi min 1, (cid:17) gi2 , > 0 (clip norm). Define the aggregated clipped gradient G(D) = (cid:88) i=1 gi. Because replacing one record affects at most one gi, and each gi2 C, (cid:13)gi(D) gi(D)(cid:13) (cid:13) (cid:13)G(D) G(D)(cid:13) (cid:13) (cid:13)2 max (cid:13)2 2C, so has ℓ2-sensitivity 2 = 2C. B.3 Gaussian Mechanism Lemma B.4 (Gaussian Mechanism Dwork et al. (2014)). Let : Dn Rd have ℓ2-sensitivity 2, i.e. for all adjacent D, (D) (D)2 2. Define the randomized mechanism G(D) = (D) + Z, (0, σ2 2 Id). Then for any δ (0, 1), if the mechanism satisfies (ϵ, δ)-DP. ϵ 2 σ (cid:113) 2 ln(cid:0)1.25/δ(cid:1), Proof. Fix two adjacent datasets D, with = (D) (D), 2 2. For any output point Rd, let p(y) = ( 2π σ2)d (cid:16) exp (D)2 2 2σ22 2 (cid:17) , be the densities of G(D) and G(D) at y. Their pointwise ratio is p(y) = ( 1 2π σ2)d (cid:16) exp (D)2 2 2σ22 2 (cid:17) p(y) p(y) = exp (cid:16) (D)2 (D)2 2σ22 (cid:17) . Write = (D) (D). Then (D)2 (D)2 = 2 (D), + 2. Let = (D); since = (D) + Z, we have = (0, σ22 2I). Define the scalar = N, σ 2 (0, 1), and note 2 2 2 . Hence p(y) p(y) = exp (cid:16) 2 N, + 2 2σ22 2 (cid:17) In order for this ratio to exceed eϵ, we need = exp (cid:16) σ + 1 2σ2 (cid:17) . σ + 1 2σ2 > ϵ > σ ϵ 1 2σ . 35 Define threshold Then = σ ϵ 1 2σ . Pr(cid:2)p(y) > eϵp(y)(cid:3) = Pr(cid:2)G > t(cid:3) = (cid:90) 1 2π eu2/2du 1 2π et2/2, using the standard Gaussian tail bound (cid:82) it suffices that eu2/2du (1/t)et2/2. To ensure this probability is at most δ, 1 2π et2/2 δ. convenient sufficient (though not tight) condition is et2/2 δ, 1.25 because then 1 2π 1 1.25 2π < 1. Solving t2/2 ln(1/δ) gives (cid:112)2 ln(1/δ). Hence it suffices that σ ϵ 1 2σ (cid:113) 2 ln(cid:0)1/δ(cid:1). (cid:112)2 ln(1.25/δ). slightly looser but standard bound drops the 1 2σ Under this choice, the event {p(y) > eϵp(y)} has probability at most δ, which by integration implies for every measurable S, term, yielding the stated condition ϵ 1 σ Pr[G(D) S] eϵ Pr[G(D) S] + δ. Thus is (ϵ, δ)-DP. B.4 Privacy Amplification by Poisson Subsampling Each RLDP step first applies Poisson subsampling at rate q, then runs base mechanism which on any fixed minibatch is (ϵ0, δ0)-DP. We now show that the overall subsampled mechanism = Tq is (ϵ, δ)-DP with ϵ = ln(cid:0)1 + q(eϵ0 1)(cid:1), δ = δ0. Theorem B.5 (Poisson Subsampling Amplification). Let : satisfy (ϵ0, δ0)-DP on any size-B minibatch. Define the Poisson-subsampled mechanism M(D) = M(cid:0)Tq(D)(cid:1), Tq(D) = { xi : include xi w.p. q}. Then satisfies (ϵ, δ)-DP with ϵ = ln(cid:0)1 + q(eϵ0 1)(cid:1), δ = δ0. Proof. Let and be adjacent datasets differing only in record x. For any measurable output set R, write = Pr(cid:2)M(D) S(cid:3), = Pr(cid:2)M(D) S(cid:3). Condition on whether is included in the Poisson sample: = (1 q) Pr(cid:2)M(Tq(D)) / Tq(D)(cid:3) + Pr(cid:2)M(Tq(D)) Tq(D)(cid:3). When / Tq(D), the sampled set from equals that from D, so Pr(cid:2)M(Tq(D)) / Tq(D)(cid:3) = Pr(cid:2)M(Tq(D)) / Tq(D)(cid:3) = p, say. When Tq(D), by (ϵ0, δ0)-DP of on any fixed batch we have Pr(cid:2)M(Tq(D)) Tq(D)(cid:3) eϵ0 Pr(cid:2)M(Tq(D)) Tq(D)(cid:3) + δ0 = eϵ0 p+ + δ0, where p+ = Pr[M(Tq(D)) Tq(D)]. Combining, (1 q) + (cid:0)eϵ0 p+ + δ0 (cid:1) = (1 q) + eϵ0 p+ + δ0. 36 Meanwhile Since p, p+ p, we get = (1 q) + p+. (1 q) + eϵ0 + δ0 = (cid:0)1 + eϵ0 (cid:1) + δ0. Finally observe 1 + eϵ0 = 1 + (eϵ0 1) = eln(cid:0)1+q(eϵ0 1)(cid:1) , so setting ϵ = ln(cid:0)1 + q(eϵ0 1)(cid:1) and δ = δ0 yields Pr[M(D) S] eϵ Pr[M(D) S] + δ, as required. B.5 RDP Composition and the GDP Accountant In this section we show how each DP-SGD step in RLDP is analyzed in the RDP framework, how Poisson subsampling amplifies privacy, how the results compose over multiple steps, and finally how the GDP accountant implements the conversion back to (ϵ, δ)-DP. Lemma B.6 (RDP of the Gaussian Mechanism Mironov (2017)). Let : Dn Rd have ℓ2-sensitivity , and define G(D) = (D) + (0, σ22I) . Then for any α > 1, satisfies (α, ρ0)-RDP with ρ0(α) = α 2σ2 . Proof. For two adjacent D, D, the only difference in (D) vs. (D) is shift of at most in Euclidean norm. The Rényi divergence between two Gaussians (µ, s2I) and (µ, s2I) with µ µ is (cid:0)N (µ, s2I) (µ, s2I)(cid:1) = Dα α µ µ2 2s2 α 2 2s2 , and here = σ. Substituting gives ρ0(α) = α/(2σ2). Lemma B.7 (RDP Amplification by Poisson Subsampling). Suppose mechanism satisfies (α, ρ0)-RDP on any fixed minibatch of examples. Construct new mechanism Mq that, on input dataset D, first includes each record independently with probability (Poisson sampling) and then applies to the resulting subsample. Then Mq satisfies (α, ρ1)-RDP with ρ1(α) = 1 α 1 (cid:16) ln 1 + e(α1) ρ0(α)(cid:17) . Equivalently, Dα (cid:0)Mq(D) Mq(D)(cid:1) 1 α 1 (cid:16) ln 1 + e(α1)ρ (cid:17) . Proof. Let and be adjacent datasets differing in exactly one record x. Denote by and the output distributions of Mq on and D, respectively. By the definition of Rényi divergence, Dα(P Q) = 1 α 1 (cid:16) ln EyQ (cid:16) (y) Q(y) (cid:17)α(cid:17) . We will bound the moment EQ[(P/Q)α]. Write {0, 1} for the event the differing record is not sampled into the minibatch (so = 0 with probability 1 q) or is sampled (J = 1 with probability q). By law of total expectation, EyQ (cid:104)(cid:16) (y) Q(y) (cid:17)α(cid:105) = (1 q) (cid:104)(cid:0)P/Q(cid:1)α (cid:105) = 0 + (cid:104)(cid:0)P/Q(cid:1)α (cid:105) . = 1 37 Case = 0. If is not sampled under Q, then the subsampled datasets are identical for and D. Hence the two branches of see the same input, so (y) = Q(y) for all outputs y. Thus (cid:0)P (y)/Q(y)(cid:1)α = 1, E(cid:2) = 0(cid:3) = 1. Case = 1. If is sampled under Q, then conditional on = 1, the subsample differs by exactly one example, and is invoked on those subsamples. Let P1 and Q1 be the distributions of on the two possible subsamples that include x. Because is (α, ρ0)-RDP, Equivalently, Dα(P1Q1) = 1 α 1 ln EyQ1 (cid:17)α (cid:16) P1(y) Q1(y) ρ0. EyQ1 (cid:17)α (cid:16) P1(y) Q1(y) e(α1)ρ0. But when = 1, the unconditional ratio (y) Q(y) equals P1(y) Q1(y) . Hence (cid:104)(cid:0)P/Q(cid:1)α (cid:105) = 1 = EyQ (cid:17)α (cid:16) P1(y) Q1(y) e(α1)ρ0. Putting the two cases together, EyQ (cid:104)(cid:16) (y) Q(y) (cid:17)α(cid:105) (1 q) 1 + e(α1)ρ0 = 1 + e(α1)ρ0. Substituting into the definition of RDP gives Dα(P Q) = 1 α 1 (cid:16) ln EQ (cid:2)(P/Q)α(cid:3)(cid:17) 1 α 1 ln(cid:0)1 + e(α1)ρ0(cid:1), which is exactly the claimed bound on ρ1(α). Theorem B.8 (Sequential Composition of RDP). Let M1, . . . , MT be randomized mechanisms such that for each t, Mt satisfies (α, ρt)-RDP. Define the adaptive composition M(D) = (cid:0)Y1, Y2, . . . , YT (cid:1), Yt Mt (cid:0)D Y1:t1 (cid:1). Then satisfies (α, ρtot)-RDP with ρtot = (cid:88) t=1 ρt. Proof. Let and be two adjacent datasets. We must show (cid:0)Pr[M(D) = ] Pr[M(D) = ](cid:1) Dα (cid:88) t=1 ρt. Write = (y1, . . . , yT ). Denote (Y ) = Pr[M(D) = ] = Q(Y ) = Pr[M(D) = ] = (cid:89) t=1 (cid:89) t= Pr[Yt = yt Y1:t1 = y1:t1, D], Pr[Yt = yt Y1:t1 = y1:t1, D]. By definition of Rényi divergence of order α > 1, Dα(P Q) = 1 α 1 ln EY (cid:16) (Y ) Q(Y ) (cid:17)α . 38 Observe that where Hence (Y ) Q(Y ) = (cid:89) t=1 Pr[Yt = yt Y1:t1, D] Pr[Yt = yt Y1:t1, D] = (cid:89) t=1 Lt(Y1:t), Lt(Y1:t) = Pr[Yt = yt Y1:t1, D] Pr[Yt = yt Y1:t1, D] . (cid:17)α (cid:16) (Y ) Q(Y ) = (cid:89) t= Lt(Y1:t)α. Since means we sample Y1 from M1(D), then Y2 from M2(D) conditioned on Y1, etc., we can iteratively apply the tower property of expectation: EY (cid:104) (cid:89) t=1 Lt(Y1:t)α(cid:105) = EY1:T 1Q (cid:104)T 1 (cid:89) t=1 Lt(Y1:t)α E(cid:2)LT (Y1:T )α Y1:T 1 (cid:3)(cid:105) . But for each fixed prefix y1:t1, the conditional mechanism Mt( y1:t1) on vs. satisfies (α, ρt)-RDP. By the definition of RDP, E(cid:2)Lt(Y1:t)α Y1:t1 = y1:t1 (cid:3) = EytQ( y1:t1) (cid:16) Pr[Yt = yt D] Pr[Yt = yt D] (cid:17)α e(α1) ρt. Therefore EY (cid:104) (cid:89) t=1 Lt(Y1:t)α(cid:105) (cid:16)T 1 (cid:89) t=1 Lt(Y1:t)α(cid:17) Applying this iteratively for = T, 1, . . . , 1 yields e(α1)ρT inside the outer expectation. EY (cid:104) (cid:89) t=1 Lt(Y1:t)α(cid:105) (cid:89) t=1 e(α1)ρt = e(α1) (cid:80)T t=1 ρt. Hence 1 1 α 1 α 1 This completes the proof that the composed mechanism is (α, (cid:80) (cid:16) (Y ) Q(Y ) Dα(P Q) = ln EY (cid:17)α ln(cid:0)e(α1) (cid:80) ρt(cid:1) = (cid:88) t=1 ρt. ρt)-RDP. Lemma B.9 (Conversion from RDP to (ϵ, δ)-DP). Suppose mechanism satisfies (α, ρ)-Rényi DP for some α > 1, i.e. for all adjacent D, (cid:0)M(D)M(D)(cid:1) = Dα 1 α 1 ln EyQ (cid:17)α (cid:16) (y) Q(y) ρ, where = M(D) and = M(D). Then for any δ (0, 1), also satisfies (ϵ, δ)-DP with ϵ = ρ + ln(1/δ) α 1 . Proof. Let L(y) = ln(cid:0)P (y)/Q(y)(cid:1) be the privacy-loss random variable. From (α, ρ)-RDP we have the moment bound By Markovs inequality, EyQ (cid:2)e(α1) L(y)(cid:3) e(α1)ρ. (cid:2)L(y) > ϵ(cid:3) = Pr(cid:2)e(α1)L > e(α1)ϵ(cid:3) Pr yQ E(cid:2)e(α1)L(cid:3) e(α1)ϵ e(α1)(ρϵ). 39 Choose ϵ so that e(α1)(ρϵ) = δ, i.e. (α 1)(ρ ϵ) = ln δ = ϵ = ρ + ln(1/δ) α 1 . With this choice, the bad event {L(y) > ϵ} has Q-probability at most δ. Now fix any measurable set Range(M). Split into Then On S1, (y)/Q(y) eϵ, so S1 = { : L(y) ϵ}, S2 = { : L(y) > ϵ}. (S) = (cid:90) S1 (y) dy + (cid:90) S2 (y) dy. (cid:90) S1 (y) dy = (cid:90) S1 (y) Q(y) Q(y) dy eϵ (cid:90) S1 Q(y) dy = eϵ Q(S1) eϵ Q(S). On S2, simply note (cid:90) P (y) dy = Pr yP (cid:2)y S2 (cid:3) Pr yQ (cid:2)L(y) > ϵ(cid:3) = δ, where the inequality follows because whenever L(y) > ϵ, (y) > Q(y) and thus the event has at most the same probability under as under Q. Combining the two parts gives (S) = (S1) + (S2) eϵ Q(S) + δ, which is exactly the definition of (ϵ, δ)-differential privacy. Theorem B.10 (Privacy Guarantee via GDP Accountant). Let ρ0(α) = α 2σ2 , ρ1(α) = 1 α 1 ln(cid:0)1 + q2(e(α1)ρ0 1)(cid:1), and suppose we run RLDP steps, each with the same (α, ρ1)-RDP cost (since σ and are held fixed for the base computation). Let Then, for any target (ϵtarget, δtarget), choosing ρtot(α) = ρ1(α). ϵ(α, δtarget) = ρtot(α) + ln(1/δtarget) α 1 , and optimizing α > 1, yields ϵ ϵtarget. By finding the minimal σ so that ϵ(α, δtarget) ϵtarget; thereafter, adapting σt σ or 1 only reduces each ρ1, so the total remains (ϵtarget, δtarget). Proof. 1. By Lemma B.6, one Gaussian mechanism step has (α, ρ0)-RDP. 2. By Lemma B.7, Poisson sampling at rate amplifies this to (α, ρ1)-RDP. 3. By Theorem B.8, such steps compose to (α, ρ1)-RDP. 4. By Lemma B.9, this (α, ρtot)-RDP yields (ϵ, δtarget)-DP with ϵ = ρtot + ln(1/δtarget) 5. The Opacus noise schedule routine implements exactly this optimization over α and σ. Any subsequent α . adaptation in algorithm (via the SAC controller) only tightens ρ1, preserving the bound. Thus RLDP satisfies (ϵtarget, δtarget)-DP. 40 B.6 Post-Processing and Hyperparameter Adaptation In RLDP the only operations that touch the private training data are the per-step DP-SGD updates (clipping, noise injection, accountant bookkeeping). All subsequent stepsthe SAC controllers normalization, encoding, action sampling, and hyper-parameter updatesare deterministic or randomized functions of those DPprotected outputs plus public randomness. We now formalize why such post-processing does not incur any additional privacy loss. Theorem B.11 (Post-Processing Dwork et al. (2014)). Let : Dn be an (ϵ, δ)-DP mechanism, and let : be any (possibly randomized) function that also depends on public randomness R. Then the composite M(D; r) = (cid:0)M(D), r(cid:1) is also (ϵ, δ)-DP. Proof. Fix any adjacent datasets D, any public randomness r, and any measurable set Z. Let Then Sr = { : (y, r) S}. Pr[M(D; r) S] = Pr(cid:2)M(D) Sr (cid:3) eϵ Pr(cid:2)M(D) Sr (cid:3) + δ = eϵ Pr[M(D; r) S] + δ, where the inequality follows from the (ϵ, δ)-DP of M. Since was arbitrary, the bound holds unconditionally, proving (ϵ, δ)-DP for M. Corollary B.12 (Zero Privacy Cost of Hyper-Policy Adaptation). Let be the mechanism that, at each training step t, outputs (cid:0) ˆGt, ϵt, δt where ˆGt is the noisy gradient update from DP-SGD and (ϵt, δt) the incremental privacy spent. Suppose the SAC controllers normalization, encoding, action sampling, and updates of clip-thresholds {C t+1} and noise multipliers σt+1 are computed by function (cid:1), (cid:0) ˆG1:t, ϵ1:t, δ1:t, r(cid:1) (cid:55) (cid:8)C t+1, σt+1 (cid:9), ft where is public randomness. Then the entire adaptive procedure (Ct+1, σt+1) = ft (cid:0)M(D)1:t, r(cid:1) does not consume any additional privacy budget beyond that of M. Proof. At each step t, the tuple ( ˆGt, ϵt, δt) is (ϵt, δt)-DP with respect to the private dataset D. The controllers computation of the next thresholds and noise, (Ct+1, σt+1) = ft (cid:0) ˆG1:t, ϵ1:t, δ1:t, r(cid:1), is exactly post-processing of the DP outputs { ˆG1:t, ϵ1:t, δ1:t} plus public randomness r. By Theorem B.11, this composite remains (ϵt, δt)-DP for each t. Since the DP-SGD steps themselves compose (B.8) to (cid:0)(cid:80) (cid:1)-DP, the hyper-policy adaptation layers add no extra privacy cost. ϵt, (cid:80) δt B.7 Main Theorem: RLDP is DP We now assemble the pieces from Appendices BB.6 into complete proof that the RLDP training algorithm (DP-SGD with per-adapter clipping and noise adaptation via SAC) satisfies the desired privacy guarantee. Theorem B.13. Let be the private training set, the Poisson sampling rate, the total number of DP-SGD steps, and (ϵtarget, δtarget) the users privacy budget. Then the entire RLDP training procedure is (ϵtarget, δtarget)-differentially private. 41 Proof. We proceed in four stages, referencing the lemmas and theorems from earlier subsections. 1. Per-step DP guarantee. At each training iteration t, the algorithm: Poisson-samples the dataset at rate q; Clips each examples per-adapter gradient to norm Aggregates clipped gradients and adds Gaussian noise with multiplier σt. By Lemma B.6 (Gaussian Mechanism) each unclipped-and-noisy update on fixed batch is (α, ρ0)-RDP with ρ0(α) = α/(2σ2 ). By Lemma B.7 (RDP Amplification by Poisson Sampling), the subsampled update is (α, ρ1)-RDP with ; ρ1(α) = 1 α 1 ln(cid:0)1 + q(e(α1)ρ0 1)(cid:1). Converting this RDP guarantee to (ϵt, δt)-DP via Lemma B.9 yields valid per-step privacy cost (ϵt, δt). 2. Composition over adapters and steps. Within single step, we in fact perform nadapters independent Gaussian-mechanism updatesone on each adapters clipped-sum vector. Since each block is disjoint and noise is drawn independently, the total Rényi divergence is the sum of the divergences of each block (see Lemma B.7). Equivalently, if ˆGj = clipped-and-noised gradient of adapter = = ( ˆG1, . . . , ˆGnadapters) is jointly Gaussian with block-diagonal covariance, then the RDP cost satisfies (cid:0)Pr[G(D)] Pr[G(D)](cid:1) = Dα nadapters (cid:88) j=1 Dα (cid:0)Pr[ ˆGj(D)] Pr[ ˆGj(D)](cid:1) = nadapters ρ1(α). Converting that back to (ϵt, δt)-DP via Lemma B.9 gives the same per-step budget. 3. Adaptive hyperparameter selection. SAC-based adaptation of clip-thresholds and noise multipliers in RLDP consumes no additional privacy budget beyond the DP-SGD steps themselves. Lemma B.14 (Zero-Cost Hyperparameter Adaptation). Let M1:t1 be the joint DP-SGD mechanism up to (cid:1)-DP by adaptive composition. Suppose the SAC controller computes step 1, which is (cid:0)(cid:80)t1 the next thresholds (cid:16) ˆG1, . . . , ˆGt1 (cid:125) (cid:123)(cid:122) (cid:124) s=1 ϵs, (cid:80)t1 (cid:8)C , , δ1, . . . , δt1 (cid:125) (cid:123)(cid:122) , ϵ1, . . . , ϵt1 (cid:125) (cid:123)(cid:122) (cid:9) = ht s=1 δs , σt (cid:17) (cid:124) (cid:124) , where ˆGs are the noised gradients output by Ms, (ϵs, δs) the recorded privacy costs, and public randomness. Then the combined mapping is also (cid:0)(cid:80)t1 s=1 ϵs, (cid:80)t1 s=1 δs Ht(D; r) = ht (cid:1)-DP. (cid:0)M1:t1(D), r(cid:1) Proof. By definition, Ht is just post-processing of the DP outputs M1:t1(D) through the function ht. Since M1:t1 is (cid:0)(cid:80)t1 s=1 δs, and = ht shows that Ht remains (cid:0)(cid:80)t1 (cid:1)-DP, applying Theorem B.11 with ϵ = (cid:80)t1 s=1 δs s=1 δs s=1 ϵs, (cid:80)t1 s=1 ϵs, δ = (cid:80)t s=1 ϵs, (cid:80)t1 (cid:1)-DP. Corollary B.15. The adaptive selection of all per-adapter clip thresholds and noise multipliers over the entire training run does not increase the total privacy cost: it remains (cid:0)(cid:80)T (cid:1)-DP. t=1 ϵt, (cid:80)T t=1 δt Proof. Apply Lemma B.14 at each step t. Since each Ht uses only the output of M1:t1, it adds no extra privacy loss. By sequential composition (Theorem B.8), the overall budget remains the sum of the per-step (ϵt, δt). 42 4. Enforcement of total budget. In our implementation, we solve via the GDP accountant for base noise multiplier σbase that ensures (cid:88) ϵt = ϵtarget, (cid:88) δt = δtarget. Because all actual σt σbase and t=1 1, each ϵt, δt can only decrease, so the total remains within budget. t=1 Combining these four points, the full RLDP training algorithmcomprising DP-SGD steps with adaptive clipping/noise and post-processing updatessatisfies (ϵtarget, δtarget)-differential privacy."
        }
    ],
    "affiliations": [
        "SnT, University of Luxembourg"
    ]
}