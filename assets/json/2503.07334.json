{
    "paper_title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment",
    "authors": [
        "Xing Xie",
        "Jiawei Liu",
        "Ziyue Lin",
        "Huijie Fan",
        "Zhi Han",
        "Yandong Tang",
        "Liangqiong Qu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). By demonstrating that training objective redesign -- not just architectural innovation -- can resolve cross-modal global coherence challenges, ARRA offers a complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 3 3 7 0 . 3 0 5 2 : r Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment Xing Xie1,2, Jiawei Liu1, Ziyue Lin3, Huijie Fan1, Zhi Han1, Yandong Tang1, Liangqiong Qu3 1Shenyang Institute of Automation, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3The University of Hong Kong {xiexing,liujiawei,fanhuijie,hanzhi,ytang}@sia.cn,ziyue lin@connect.hku.hk,liangqqu@hku.hk Figure 1. ARRA enables high-quality text-to-image generation through redefined training objective. (a)(c) Traditional next-token prediction (NTP)-based LLMs rely solely on the autoregressive loss (AR loss) of the next token <NEXT> for local constraints. (b)(d) ARRA constructs hybrid token <HYBNEXT>, which is aligned by introducing external global visual representation, ensuring that <HYBNEXT> is constrained both locally by the AR loss and globally by the global visual alignment loss (GVA loss). (e)(f) ARRA demonstrates advantages in semantic consistency and visual continuity."
        },
        {
            "title": "Abstract",
            "content": "We present Autoregressive Representation Alignment (ARRA), new training framework that unlocks globalcoherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via global visual alignment loss and hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRAs plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMICCXR). By demonstrating that training objective redesign not just architectural innovationcan resolve cross-modal global coherence challenges, ARRA offers complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation. 1. Introduction Large language models (LLMs) [1, 4, 34, 36, 37] and subsequent multimodal LLMs (MLLMs) [13, 24, 28, 29, 51, 55] has revolutionized the field of generative AI. These models, built on the autoregressive paradigm, demonstrate remarkable scalability and generalization capabilities in complex reasoning and understanding tasks through simple yet powerful next-token prediction framework. Inspired by the success of LLMs, recent work has sought to replicate their autoregressive next-token prediction paradigm for text-to-image generation. Early works like DALLE [41], Parti [60] and LlamaGen [48] attempted to directly adopt the next-token prediction paradigm of LLMs for image generation by treating images as sequences of discrete tokens. However, while the next-token prediction paradigm excels in textwhere local dependencies naturally align with sequential structureit struggles to bridge the significant cross-modal gap between language and images. As shown in Fig. 1(c), optimizing only for local next-token prediction forces the model to focus on isolated token-level features, neglecting the global coherence required for spatially structured visual content. This may result in images with 1) fragmented outputse.g., misaligned ribs in X-rays, where fine-grained details fail to harmonize into unified whole, and 2) semantic mismatches, as shown in Fig. 1(e), where global information is not maintained, leading to inconsistencies in the generated visual content. Recognizing this limitation, recent efforts aim to inject global constraints into autoregressive frameworks to fully unlock the potential of LLMs in image generation [52, 58, 62]. For example, Transfusion [62] and Show-O [58] introduce bidirectional attention mechanisms to model global image structure through patch diffusion and mask token modeling, respectively. While these methods achieve remarkable resultsgenerating high-quality images and demonstrating LLMs potential for unified multimodal tasksthey rely on architectural modifications such as cross-modal attention layers or grafted diffusion modules. Though effective, such adaptations often depart from standard LLM frameworks, limiting their compatibility with pretrained LLMs that excel under pure autoregressive paradigms. For instance, repurposing an off-the-shelf LLM for text-to-image generation would require retraining with these specialized components, forfeiting the benefits of existing scaling laws and generalization capabilities. This practical constraint raises critical question: Can we unlock the full potential of LLMs for text-to-image generation without altering the original architecture or inference mechanism? We address this by proposing Autoregressive Representation Alignment (ARRA), novel training framework that redefines how LLMs learn text-to-image generation. Unlike prior work that modifies architectures (e.g., adding attention layers or diffusion modules), ARRA preserves the original LLM framework while injecting global constraints directly into the training objective. Our key insight is simple: global coherence need not come from architectural complexity; it can instead emerge from redefined training paradigm. Specifically, ARRA augments the standard autoregressive loss with global visual alignment loss that aligns the LLMs latent representations with semantic guidance from pretrained foundational models  (Fig. 2)  . To bridge local and global learning, we introduce hybrid token, <HYBNEXT>, which serves as bidirectional anchor: Locally, it predicts the next token via standard codebook indices. Globally, its latent embedding aligns with compressed visual features extracted from external models (e.g., BioMedCLIP [61] or MedSAM [33]) via our novel global visual alignment loss. By distilling rich semantic features (e.g., spatial relationships, object coherence) from external models into the <HYBNEXT> token during training, ARRA enables autoregressive sequences to implicitly learn global structure. Crucially, this alignment occurs only during training, leaving the LLMs inference process untouched and preserving its inference-time efficiency. including medical Our experiments validate ARRAs versatility across diverse domains, imaging and natural scene synthesis, where it achieves state-of-the-art Frechet Inception Distance (FID) and CLIP-Score improvements without architectural changes. Notably, ARRAs plug-andplay nature complements existing innovations: when paired with architectures like Chameleon [50] or Llamagen [48], it further enhances output quality, demonstrating additive benefits. Furthermore, ARRA demonstrates its capability for domain adaptation. By infusing knowledge from domain-specific foundational models (e.g., BioMedCLIP [61] or MedSAM [33]) into LLMs with image generation capabilities, such as Lumina-mGPT [27], its generation performance improves significantly compared to direct finetuning. This indicates that ARRA can facilitate the transition of models from general-purpose generative framework to domain-specific generative models. The main contributions are summarized below: (i) We introduce Autoregressive Representation Alignment, novel training framework that redefines how LLMs learn text-to-image generation by decoupling global structure learning from model design. By aligning training objective with external representations, ARRA resolves local dependency limitations in LLMs while retaining original architectures and inference efficiency. (ii) We introduce the <HYBNEXT> token, novel mechanism that bridges local next-token prediction with global semantic alignment via distillation from external models (e.g., BioMedCLIP [61] or MedSAM [33]), enabling implicit learning of spatial and contextual relationships. (iii) We provide detailed experimental analysis, offering comprehensive insights into the selection of external representations, alignment tokens, and alignment depth. These findings serve as practical guide for the effective utilization of representation alignment. (iv) We validate ARRAs ability to 1) enhance advanced autoregressive LLMs (e.g., Chameleon, LlamaGen) without framework modifications: the FID reduces by 25.5%, 8.8%, 7.5% on the MIMIC-CXR, DeepEyeNet and ImageNet datasets, and 2) seamlessly adapt general-purpose models to specialized domains (e.g., medical imaging) via domain knowledge infusion: the FID reduces by 18.6% on the MIMIC-CXR. 2. Related Work 2.1. Visual Generation Models Diffusions The impressive success of diffusion models [18, 30, 42, 44, 47] on image generation tasks has revolutionized the image generation paradigm. Recently, DiTs [35] and U-ViT [3] replace or integrate U-Net with Transformers, inspiring the development of models such as SD3 [15], Imagen3 [2], and Pixart [7], which achieve new stateof-the-art performance in image generation. However, diffusion models need to rely on separate language models (e.g., CLIP [38] or T5 [39]) as text encoder, and do not allow for uniform modeling of text and image data. Autoregressive models Early pioneering works, VQ-VAE [53], VQ-GAN [14] and DALL-E [41], demonstrated the potential of AR models for image generation tasks. Subsequent works such as VQVAE-2 [43] and RQ-Transformer [21] also follow raster-scan manner and enhance image generation performance through extra scales or stacked codes. Parti [60], based on the architecture of ViT-VQGAN [59], scales the autoregressive transformer to 20B parameters, demonstrating promising high-fidelity image generation results. Recent works achieve performance comparable to diffusion models by employing scale modeling [52] and eliminating vector quantization [25]. Additionally, masked prediction autoregressive models [5, 6, 26] employ BERTlike [11] masked prediction modeling, improving generation efficiency and quality. 2.2. LLMs for Text-to-Image Generation LLM-based text-to-image generation models have attracted researchers attention, aiming to replicate their success in the field of text generation. Early work leverages diffusion models as tools to extend LLMs [12, 16, 49, 56]. These works utilize LLMs as feature extractors to guide additional diffusion generators for visual generation. Such models are complex to design and do not fully unleash the generative potential of the LLM for visual generation. Recent works [27, 31, 32, 48, 50] attempt to unify text and image modeling within single LLM, enhancing generation performance through tokenizer optimization [48], early fusion modeling [50], and flexible resolution modeling [27]. These works discretize both text and images into tokens, which are then fed into the LLM for sequence modeling by next token prediction. However, the local constraints provided by next-token prediction struggle to bridge the significant cross-modal gap between language and images. Further, Transfusion [62] introduces bidirectional attention to model global structure via patch diffusion, while ShowO [58] employs masked token modeling to unify local and global dependencies. Although effective, such models typically deviate from the standard LLM framework. Unlike these approaches, our framework introduces global constraints through external visual representation alignment during training while preserving the powerful next-token prediction paradigm. 3. Proposed Method We aim to achieve high-quality image generation without altering the next-token prediction paradigm of LLMs. We argue that the inherent cross-domain gaps in LLMs pose significant challenges for image generation tasks when the model lacks the ability to learn global features. To address this, we propose Autoregressive Representation Alignment (ARRA) framework (see Fig. 2), which leverages the inherent representation capabilities of well-pretrained foundation models to facilitate the training of complex text-toimage autoregressive generation. Our framework is applied exclusively during training, without affecting inference, enabling the generation of high-quality images with exceptional semantic consistency in cost-effective manner while accelerating convergence speed. 3.1. Overview Our goal is to train an autoregressive model Mθ leveraging representations derived from an external foundation visual encoder EF . Mθ takes text prompt as input and generates target image I. During training, and are first tokenized into token sequences sT and sI , respectively. These token sequences are then used to train the transformer-based autoregressive model Mθ. Meanwhile, foundation visual encoder EF encodes into the global visual representation fGF , which is used to align with the feature fA extracted from xt by Mθ. During image generation, the alignment module is removed, and image tokens are generated by Mθ through next-token prediction. Finally, the output image tokens are decoded into pixel space by image decoder to produce the target image I. We describe autoregressive modeling process in Section 3.2 and detail our autoregressive representation alignment framework in Section 3.3. 3.2. Autoregressive Modeling via Next-Token Prediction The autoregressive architecture comprises two core components: (1) transformer-based autoregressive model Mθ for probabilistic modeling of token sequences. (2) VQbased model [14] with encoder E, quantizer and decoder Figure 2. Proposed ARRA Framework. We define the next token predicted in the autoregressive sequence as the HYBRID next token, denoted as <HYBNEXT>. During training, <HYBNEXT> is constrained not only locally by the autoregressive loss LAR from the next token prediction and LLM codebook matching, but also globally through visual alignment loss LGVA, which modulates its latent space using externally well-trained representations. We extract visual representations from pretrained foundational model and further aggregate these features to obtain semantically enriched representations for alignment. for transformation between image pixels and discrete token sequences. Formulation involves following two parts: task as predicting the distribution of the next token and optimizes the likelihood p(x) through cross-entropy (CE) loss: Tokenization. In order to apply the next-token prediction modeling in the image domain, it is first required to convert the continuous 2D image pixels into discrete sequences. This process consists of two steps: (1) 2D image pixels to 2D image tokens, and (2) 2D image tokens to 1D token sequences. Specifically, given image RHW 3, we first obtain the image feature map = E(I) Rhwd, where = H/c, = W/c, is the dimension of the codes, denotes compression factor. Subsequently, we convert into discrete tokens by = Q(f ) hw, where the quantizer Q() maps each vector (i,j) in the image feature map to the code index q(i,j) of its nearest vector z(i,j) in the codebook Z. The image tokens are then reshaped into 1D token sequence sI = {xI n} with length of , arranged according to the raster scan order. 3, . . . , xI 1, xI 2, xI For text prompt , we obtain the discrete sequence 1 , xT }, where () 3 , . . . , xT 2 , xT through sT = (T ) = {xT denotes text tokenizer. Next token prediction modeling. We combine text sequences sT and image sequences sI to obtain discrete tokens = {x1, x2, x3, . . . , xn}, where xn is an integer from tokenizers vocabulary . The next-token autoregressive posits the probability of current token xt depends only on its prefix (x1, x2, x3, . . . , xt1). The likelihood of sequence modeling can be expressed as: p(x) = (cid:89) t=1 p(xtx1, x2, ..., xt1). (1) The autoregressive model Mθ formulates the generative LAR(θ) = Ext[ log pθ(xtx<t)]. (2) During training, the autoregressive model Mθ relies on the previous tokens x<t to predict the next token xt, where the hidden state of xt in the i-th layer of Mθ is denoted by L. In the final layer, the hidden state 1 are then passed through the LLM head to compute the probability distribution pθ for xt. In the original autoregressive model, pθ is constrained only by the local context of single token (i.e. xt), lacking the ability to capture global information. This limitation restricts the models capacity to capture complex cross-modal relationships. To address this problem, we propose Autoregressive Representation Alignment in Section 3.3. 3.3. Autoregressive Representation Alignment We align the visual representations extracted from the pretrained foundational model with LLMs hidden states and investigate impact of different alignment strategies in Section 4.2. The goal of alignment is to enable the hidden state of the autoregressive transformer to acquire external global feature representations, providing meaningful guidance for reconstructing image. Pre-trained Visual Representation Extraction. Formally, let EF be pretrained foundation models encoder and consider target image I. We encode as visual representation by fF = EF (I) RN D, where N, denotes the embedding length and dimension of fF . Next, fF is aggregated to the global visual representation fGF , i.e., fGF = agg(fF ) R1D, where agg() denotes feature aggregation operation. This aggregation operation fully extracts the global information in the features and facilitates alignment with the feature of autoregressive model. For CLIP series, inspired by [40], we use the <CLS> token representation from the Transformer-based visual encoder as global visual representation. For SAM series, which lack <CLS> token, we instead apply average pooling over all patch features for feature aggregation operation. Hybrid Next Token. We define the next token predicted by LLM sequence in our framework as HYBRID next token, denoted as <HYBNEXT>. Unlike locally constrained token in previous autoregressive models that are solely constrained by the LLM codebook, our defined <HYBNEXT> can fully incorporate external, well-trained global visual representations, making it globally and locally constrained token. Global Visual Representation Alignment. Specifically, we obtain the hidden state of <HYBNEXT> from the autoregressive model Mθ. The hidden state is converted to fA R1D by projection layer Aϕ to align with the global visual representation fGF R1D, i.e., fA = Aϕ(f L), where Aϕ is two-layer MLP. Representation alignment is achieved through Global Visual Alignment loss LGVA, which maximizes the similarity between the projected feature fA and the global visual representation fGF : LGVA(θ, ϕ) = sim(fA, fGF ). (3) where sim(, ) denotes cosine similarity loss. This alignment enables the <HYBNEXT> token to learn global visual representation, bridging the cross-modal gap and making the token prediction process more reliable. Therefore, the autoregressive model can be jointly optimized through the following composite loss function: LARRA(θ, ϕ) = LAR(θ) + λLGVA(θ, ϕ). (4) λ serves as balancing hyperparameter that controls the relative importance of the alignment objective. Experimentally, we set λ = 1. 4. Experiments and Results We conducted extensive experiments to rigorously evaluate the performance of ARRA and analyze the impact of its components. Specifically, we answer the following critical questions to the ARRA framework: Visual encoder selection on alignment: How does the choice of external visual encoder (general vs. specialized and cross-modal vs. vision-only) impact generation performance? (section 4.2.1) Alignment mechanism: Does token-level alignment (our proposed [HYBNEXT]) outperform fixed-position alignment ([REP]) in preserving global visual constraints during autoregressive generation? (section 4.2.2) Alignment depth: How does alignment depth (i.e., the layer in the MLLM chosen for alignment with the visual encoder) affect the final performance? (section 4.2.3) Feature aggregation strategy: How do different types of features extracted from the same visual encoder affect the generation performance? (section 4.2.4) Main results: Does the ARRA framework exhibit better generation performance and convergence speed compared to other generation methods? (section 4.3) 4.1. Experimental Setup Datasets. We train and evaluate our model with MIMICCXR [20], DeepEyeNet [19] and ImageNet [10] datasets. For the MIMIC-CXR, we employ three distinct radiographic views: posteroanterior (PA), anteroposterior (AP), and lateral (LATENT). Input prompts are generated from the impression section of diagnostic reports, formatted as: {view} view chest X-ray image, {impression}. We select 221,238 pairs for training and 1,000 pairs for testing. DeepEyeNet [19] is comprehensive fundus dataset, containing more than 15k high quality text-image pairs. In our experiments, we derive prompts from the clinicaldescription section and selectively chose 7,190 training pairs and 1,089 testing pairs. All images are preprocessed through center cropping to 512512 pixels. ImageNet [10] is popular natural image datasets containing more than 1.2 million samples. We randomly select 256,233 images from ImageNet-1k as our training data, and all samples were generated and evaluated in 256 256 resolution. Evaluation Metrics. We employ three quantitative metrics to evaluate the proposed method: Frechet Inception Distance (FID), Multi-Scale Structural Similarity (MS-SSIM), and CLIP-Score. For X-ray image generation, FID is computed using features extracted from CXR-pre-trained DenseNet-121 (XRV) [8], while for fundus images, we utilize features from an ImageNet-pre-trained Inception V3 [17]. The CLIP-Score metric is derived from the cosine similarity between image-text feature pairs encoded through BioMedCLIP [61], large-scale medical visionlanguage model. Implementation Details. We validate the versatility of ARRA under three distinct configurations to demonstrate its adaptability across architectures, domains, and training paradigms. First, ARRA-Base trains from random initialization, testing ARRAs ability to bootstrap multimodal alignment without pretrained weights. Second, ARRA initializes with pretrained LLM that has strong text generation capabilities, evaluating its capacity to repurpose off-the-shelf LLMs for text-to-image generation. Third, ARRA-Adapt starts from pretrained LLM with both text and image generation capabilities, testing ARRAs ability to repurpose general-domain image generation priors for specialized tasks (e.g., medical imaging). For MIMIC-CXR and DeepEyeNet datasets, we employ both ARRA using Chameleon without image generation capability[50] as base LLM to train the visual generation model, and ARRA-Adapt by fine-tuning the LuminamGPT [27]) model with image generation capability. For ImageNet dataset, we test the ARRA-Base configuration using LLamaGen network [48]. All constructed models follow the original parameters for full parameter training. Additional implementation details are detailed in Appendix 2 and Appendix 6. 4.2. Component Analysis of Alignment 4.2.1. Visual encoder selection on alignment To evaluate how visual encoder selection impacts crossmodal alignment and generation quality, we conduct experiments with three encoders: BioMedCLIP [61] (domainspecific cross-modal encoders), CLIP [38] (general-purpose cross-modal encoders), and Med-SAM [33] (domainspecific pure visual encoders). As shown in Table 1 and Appendix 5, all pretrained encoders improve generation performance compared to training without alignment. This improvement arises from the global semantic constraints imposed by external representations, which regularize the autoregressive generation process. Notably, for training from an text-only generative LLM (ARRA), BioMedCLIP and CLIP demonstrate superior performance, as their cross-modal training bridges the gap between text and image modalities. This enables the LLM to learn what to generate (semantics) before how to generate (pixel details). Conversely, when fine-tuning pretrained LLMs with image-generation capabilities (ARRA-Adapt), domain-specific encoders like BioMedCLIP and MedSAM dominate. BioMedCLIP injects medical-specific semantics, while MedSAM provides structural priors (e.g., organ shapes) through its segmentation-focused features. These results lead to the following key insight: Takeaway 1. When the LLM lacks image capabilities, crossmodal encoders are crucial for semantic grounding. However, for pretrained LLMs with image capabilities, domainspecific encoders are more effective, as they provide the finegrained features needed for domain-specific adaptation. 4.2.2. Alignment mechanism We compare two strategies for integrating visual representations: (1) aligning features to fixed <REP> token at the start of the generated sequence, and (2) aligning to the hidden state of <HYBNEXT>, hybrid token interleaved at every generation step. As shown in Fig. 2, <HYBNEXT> yields superior performance. We argue that <HYBNEXT> allows for comprehensive traversal of every token during training sampling, ensuring that each token is effectively constrained by external global representations. In contrast, <REP> suffers from the attention sink [27, 57] effect, Table 1. Impact of alignment with representation extracted from different encoders on MIMIC-CXR, where the gray part represents the ARRA model, and the white part represents the ARRA-Adapt. Target Rep. BioMedCLIP [61] Med-SAM [33] CLIP-L [38] w/o align. BioMedCLIP [61] Med-SAM [33] CLIP-L [38] w/o align. FID MS-SSIM CLIP-Score 4.15 4.08 4.63 5.10 5.30 6.54 5.16 7.11 0.4576 0.4542 0.4519 0.4518 0.4532 0.4465 0.4450 0.4460 0.422 0.398 0.407 0.401 0.405 0.384 0.394 0.383 Table 2. Impact of different alignment mechanism selection on MIMIC-CXR. Gray part represents the ARRA model, and the white part represents the ARRA-Adapt. Target token <REP> <HYBNEXT> w/o align. <REP> <HYBNEXT> w/o align. FID MS-SSIM CLIP-Score 4.85 4.15 5.10 6.26 5.30 7.11 0.4527 0.4576 0.4518 0.4499 0.4532 0.4460 0.410 0.422 0.401 0.401 0.405 0.383 Table 3. Impact of aligning representation to different layers on MIMIC-CXR. Gray part represents the ARRA, and the white part represents the ARRA-Adapt. Layer Index layer1 layer5 layer20 layer30 layer31 w/o align. layer1 layer5 layer20 layer30 layer31 w/o align. FID MS-SSIM CLIP-Score 4.15 4.58 4.46 4.51 4.53 5.10 5.30 6.21 5.70 5.04 5.71 7.11 0.4576 0.4587 0.4563 0.4537 0.4606 0.4518 0.4532 0.4467 0.4500 0.4478 0.4500 0.4460 0.422 0.421 0.413 0.412 0.418 0.401 0.405 0.385 0.404 0.398 0.373 0.383 where attention to the fixed token decays over long sequences, leading to degraded outputs. This leads to the following key insight: Takeaway 2. Aligning visual representations to hybrid token interleaved at each generation step <HYBNEXT> is more effective than using fixed token <REP>, as it prevents attention decay and ensures consistent constraint by external representations. Table 4. Impact of using different feature aggregation strategies on MIMIC-CXR with ARRA. CLS Avgpool FID MS-SSIM CLIP-Score 5.30 6.56 5.93 0.4532 0.4434 0.4465 0.405 0.387 0. Figure 3. Visualization of different methods and ARRA in the MIMIC-CXR dataset for the chest X-ray generation task. 4.2.3. Alignment depth As shown in Table 3, we investigate the effects of representation alignment at different layers of the Transformer architecture. Our results demonstrate that alignment at various layers consistently enhances model performance, with early-layer alignment (e.g., layer 1) yielding optimal results. We argue that early-layer alignment provides sufficient global semantic guidance while allowing subsequent layers to focus on capturing high-frequency details, thereby establishing robust representations for image generation. These results lead to key insight: Takeaway 3. Performing representation alignment at earlier LLM layers (e.g., layer 1) provides sufficient global semantic guidance while allowing deeper layers to focus on capturing highfrequency details. 4.2.4. Feature aggregation strategy To investigate how different types of features extracted from the visual encoder affect the generation performance, we investigate three strategies for aggregating features from single visual encoder: (1) the [CLS] token representation, (2) average pooling of all image patch representations, and (3) concatenation of both representations. As shown in Table 4, the [CLS] token representation yields optimal performance. We attribute this superiority to the [CLS] tokens ability to aggregate global visual information through self-attention mechanisms, providing compact yet comprehensive representation for cross-modal alignment. This finding aligns with [40], which establishes that [CLS] tokens capture global representations while patch tokens focus on local features. This distinction further suggests that autoregressive image generation models particularly benefit from external global semantic represenFigure 4. ARRA-Base improves generation of LlamaGen. Figure 5. Convergence trends of the baseline and ARRA models on the MIMIC-CXR (left) and ImageNet (right) datasets. tations as guidance. These results lead to key insight: Takeaway 4. The [CLS] token representation in foundation models effecTable 5. FID, IS, MS-SSIM comparisons of different methods on the MIMIC-CXR, DeepEyeNet, and ImageNet datasets, respectively. Chest-Xray: MIMIC-CXR Fundus: DeepEyeNet Natural: ImageNet Method FID IS MS-SSIM CLIP-Score FID MS-SSIM CLIP-Score Original SD V2-1[45] 71.68 2.365 DreamBooth SD [46] 60.40 2.269 15.62 2.468 30.75 2.437 5.88 2.134 7.11 2.498 5.30 2.587 4.15 2. MINIM [54] UniXGen [22] LLM-CXR [23] Chameleon [50] ARRA ARRA-Adapt 0.128 0.270 0.317 0.361 0.395 0.383 0.405 0.422 0.2333 0.3693 0.4423 0.4128 0.4374 0.4460 0.4532 0.4576 166.45 80.70 59.71 - - 38.37 35.01 34.70 0.141 0.352 0.333 - - 0.341 0.376 0.392 0.2164 0.3061 0.2862 - - 0.3234 0.3354 0. Method LlamaGen-111M LlamaGen-111M LlamaGen-343M LlamaGen-343M 100 300 100 300 ARRA-Base-111M 100 ARRA-Base-111M 300 ARRA-Base-343M 100 ARRA-Base-343M 300 Epochs FID 9.328 7.283 5.192 4.294 8.710 6.653 4.982 3.971 tively aggregates global visual information, providing comprehensive guidance for cross-modal alignment. 4.3. Main Comparison Results Based on our component analysis, we finalize our ARRA framework with BioMedCLIP and CLIP for visual alignment on medical datasets and natural dataset, respectively. We use [CLS], [HYBNEXT], early-layer (layer 1) as our alignment strategy. We now benchmark our models against state-of-the-art diffusion models (Stable Diffusion [45], DreamBooth [46], MINIM [54]) and autoregressive models (LLM-CXR [23], UniXGen [22], Chameleon [50]). More implementation details are placed in Appendix 2. Quantitative and qualitative performance. We evaluate generation quality using FID (visual fidelity), MS-SSIM (structural similarity), and CLIP-Score (text-image alignment). As shown in Fig. 3 and Table 5, our model achieves outstanding performance in both visual quality and semantic alignment aspects. Among the baseline methods, LLMCXR [23] achieves respectable FID and MS-SSIM scores but exhibits poor semantic alignment. Conversely, MINIM [54] achieves excellent CLIP-Score performance but suffers from significant image distortion. These conclusions are also reflected in the visual results shown in Fig. 3. In contrast, both our ARRA model and the ARRA-Adapt model show excellent performance in visual fidelity and semantic alignment, achieving the best results. Specifically, compared to the LLM-CXR model, which achieves the best FID among the baselines, the ARRA model reduces the FID from 5.88 to 5.30 and significantly improves the CLIPScore from 0.4374 to 0.4532. The ARRA-Adapt model further reduces the FID to 4.15 and increases the CLIP-Score to 0.4576. In comparison to the Chameleon model in the baseline, which serves as foundation for our ARRA model, ARRA achieves significant reduction in FID by 25.5%. Additionally, our models ensure excellent alignment in fine-grained details, such as lesion location and severity. For example, when generating images for first report, Chameleon model incorrectly positions right pleural effusion, and it fails to adequately represent the severity In comparison, our ARRA and of mild enlargement. ARRA-Adapt model guarantees this fine-grained understanding and achieves high-quality generation. More visualizations results are in Appendix 5. Accelerate convergence. In Fig. 5, we present the convergence trends of the baseline model and the model enhanced with ARRA framework. The experimental results show that applying ARRA at different layers results in faster convergence compared to baseline, particularly in the early stages of training. This indicates that the global constraints introduced by ARRAs alignment mechanism effectively guide the model along the correct learning trajectory, thereby accelerating convergence. Additional visual comparisons of convergence trends are provided in Appendix 5. Generalization Ability. We validate our framework on another popular autoregressive generative model, LlamaGen [48], and define models that use our framework as ARRABase model. We employ the FID as the metrics and evaluate the performances of our models on ImageNet dataset. As shown in Table 5 and Fig. 4, ARRA-Based achieved better generation performance than the baseline in both 111M and 343M models. Fig. 5 indicates that our method beat LlamaGen model during the entire training process. Through experimental results, we show that the ARRA framework can be applied to various autoregressive models and is suitable for different datasets in multiple domains. More ablation study can be found in Appendix 4. More implementation details and visualizations are shown in Appendix 5. 5. Conclusion We propose Autoregressive Representation Alignment (ARRA), framework that enhances autoregressive image generation by injecting external visual representations during training. This approach enriches global semantic understanding while maintaining the models original autoregressive paradigm during generation. Experiments on medical and natural image generation tasks demonstrate ARRAs versatility, offering cost-effective framework for training autoregressive text-to-image generation models. Our work bridges the gap between multimodal domains and provides novel insights into modeling unified multimodal generation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. 3 [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 3 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 1 [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 3 [6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 3 [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [8] Joseph Paul Cohen, Joseph Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, et al. Torchxrayvision: library of chest x-ray datasets and models. In International Conference on Medical Imaging with Deep Learning, pages 231249. PMLR, 2022. 5 [9] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the worlds first truly open instruction-tuned llm, 2023. 1 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. 5 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171 4186, 2019. 3 [12] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In The Twelfth International Conference on Learning Representations, 2024. [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1 [14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 3 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 3 [16] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 3 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 5 [18] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 3 [19] Jia-Hong Huang, C.-H. Huck Yang, Fangyu Liu, Meng Tian, Yi-Chieh Liu, Ting-Wei Wu, I-Hung Lin, Kang Wang, Hiromasa Morikawa, Hernghua Chang, Jesper Tegner, and Marcel Worring. Deepopht: Medical report generation for retinal images via deep models and visual explanation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 24422452, 2021. 5 [20] Alistair EW Johnson, Tom Pollard, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxr-jpg, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. 5 [21] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [22] Hyungyung Lee, Da Young Lee, Wonjae Kim, Jin-Hwa Kim, Tackeun Kim, Jihang Kim, Leonard Sunwoo, and Edward Choi. Vision-language generative model for view-specific chest x-ray generation. arXiv preprint arXiv:2302.12172, 2023. 8, 1 national conference on computer vision, pages 41954205, 2023. 3 [36] Alec Radford. Improving language understanding by gener- [23] Suhyeon Lee, Won Jun Kim, Jinho Chang, and Jong Chul Ye. Llm-cxr: Instruction-finetuned llm for cxr image understanding and generation. arXiv preprint arXiv:2305.11490, 2023. 8, 1 [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [25] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 3 [26] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization, 2024. 3 [27] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 2, 3, 6 [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 1 [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1 [30] Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, and Liangqiong Qu. Residual denoising diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2773 2783, 2024. 3 [31] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022. 3 [32] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. [33] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15:654, 2024. 2, 6 [34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 1 [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF interative pre-training. 2018. 1 [37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 3, [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 3 [40] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in neural information processing systems, 34:1211612128, 2021. 5, 7 [41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2, 3 [42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. 3 [43] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 3 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 3 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 8, [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 8, 1 [47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. 3 [48] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation, 2024. 2, 3, 6, 8 [62] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 2, 3 [49] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. 3 [50] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, 3, 6, 8, 1 [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [52] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 2, 3 [53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [54] Jinzhuo Wang, Kai Wang, Yunfang Yu, Yuxing Lu, Wenchao Xiao, Zhuo Sun, Fei Liu, Zixing Zou, Yuanxu Gao, Lei Yang, et al. Self-improving generative foundation model for synthetic medical image generation and clinical applications. Nature Medicine, pages 19, 2024. 8, 1 [55] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1 [56] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. 3 [57] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. 6 [58] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2, [59] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3 [60] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation, 2022. 2, 3 [61] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. 2, 5, 6 1. Overview Our supplementary materials include the following sections: Additional implementation details about ARRA and other comparative methods in MIMIC-CXR and DeepEyeNet (Section 2). Implementation details of component analysis. (Section 3). Ablation study, including generalizability at different resolutions, choices of projection layers, regularization coefficients λ, and training objectives (Section 4). More visual results on the MIMIC-CXR dataset (Section 5). Implementation details and more results on ImageNet (Section 6). Open source code (Section 7). 2. Additional implementation details in MIMIC-CXR and DeepEyeNet ARRA and ARRA-Adapt. All experiments on medical images are implemented using PyTorch on 4 NVIDIA A6000 Ada GPUs. We employ batch size of 6 with learning rate of 2e-5, optimized by AdamW (weight decay=0.05, β1=0.9, β2=0.95). To stabilize training, we apply z-loss regularization [50] with weight of 1e-5. All data are pre-tokenized before training to increase throughput. The VQ tokenizer operates with downsampling rate of 16, resulting in 1024-tokens representation for each image. Original SD V2-1. [45] Stable-diffusion-2-1 is diffusion model trained from natural images, and we obtain its official pre-training weights, which are used directly to generate images without training, to demonstrate its zero-shot generation capability. DreamBooth SD. [46] We adapt dreambooth to fine-tune the stable-diffusion-xl-base model, choosing X-rays and fundus as class identifiers [class noun] on MIMIC-CXR and DeepEyesNet datasets, respectively, and supervised the fine-tuning using the full data. MINIM. [54] MINIM is medical image generation model based on stable-diffusion-v1-4 for full fine-tuning. We use its official implementation for training and generation. UniXGen. [22] The UniXGen model is unified model for X-ray image understanding and generation, fully trained on the MIMIC-CXR dataset. We use its official implementation and obtain its pre-training weights for generation. LLM-CXR. [23] The LLM-CXR model is unified model for X-ray image understanding and generation using dollyv2-3b [9] as baseline for full parameter fine-tuning training. We implement training and inference on the MIMIC-CXR dataset using its official implementation. Chameleon. guage model trained on natural image dataset. [50] Chameleon is multimodal large lanIts opensource weights are only for visual language understanding. We follow its official implementation and use 7B pretraining weights to fine-tune full parameters for training on the x-ray and fundus image dataset. 3. Implementation details of component analysis. For Visual encoder selection on alignment, we align the representations to the hidden state of layer 1 of <HYBNEXT>. For Alignment mechanism, we aligned the representations extracted from BioMedCLIP to layer 1. For Alignment depth, we align the representations extracted from BioMedCLIP with the hidden state of <HYBNEXT>. For Feature aggregation strategy, we align the representations extracted from the BioMedCLIP model to the hidden state of layer 1 of <HYBNEXT>. 4. Ablation study Generalizability at different resolutions. We validate the generalization capability of the ARRA framework at resolutions of 256256 and 512512, as shown in Table 1. Experimental results demonstrate that ARRA can stably improve the generation performance of baseline models at different resolutions, demonstrating its resolution generalization ability. Table 1. Generalizability at different resolutions. Resolution 256256 256256 512512 512512 Model Chameleon ARRA Chameleon ARRA FID MS-SSIM CLIP-Score 9.42 6.11 7.11 5.30 0.4405 0.4436 0.4460 0. 0.369 0.388 0.383 0.405 Table 2. Compare the performance of different alignment heads. MLP Maxpool FID MS-SSIM CLIP-Score 5.30 5. 0.4532 0.4375 0.405 0.386 Projection layer. We evaluate the performance of different projection layer, including two-layer MLP and direct pooling layer. As show in Table 2, our experiments reveal that using two-layer MLP outperforms the pooling layer, even when the projection layer is discarded during testing. We hypothesize that, compared to the non-trainable pooling layer, employing trainable MLP as soft link during training allows the model to better learn information-rich and precise representations while adaptively filtering out irrelevant information. Figure 1. More visualizations on the chest X-ray generation task across the MIMIC-CXR datasets. Effect of λ. We also examine the effect of the regularization coefficient λ by training models with different coefficients 0.5 to 2 and comparing the performance. As shown in Table 3, the optimal value is reached when the regularization coefficient λ = 1. Table 4. Impact of different alignment objectives. Objective Cos. sim. MSE FID MS-SSIM 5.30 5.32 0.405 0.405 IS 0.4532 0.4454 Table 3. Ablation study for alignment loss coefficient λ. λ 0.5 0.8 1 1.5 FID MS-SSIM 6.22 5.38 5.30 5.87 6.12 0.389 0.399 0.405 0.394 0.390 IS 0.4468 0.4460 0.4532 0.4477 0.4479 Training objective. We compare two alignment objectives: MSE loss and cosine similarity loss. As shown in Table 4, both achieve similar FID and MS-SSIM scores, but cosine similarity yields better semantic accuracy (higher CLIP-Score). This advantage stems from its focus on directional similarity rather than absolute distances, making it more robust for cross-modal alignment tasks. 5. More visualizations on MIMIC-CXR We present additional visual results from experiments on MIMIC-CXR, including: Visual comparisons of ARRA with other methods on Xray image generation tasks  (Fig. 1)  ; Visualizations of baseline models using different pretrained encoders for alignment  (Fig. 2)  ; Visualization of different lesion localizations and lesion levels.  (Fig. 4)  . Convergence performance visualizations of baseline models and those using the ARRA framework  (Fig. 3)  . Figure 2. Visualization of alignment with features extracted from different encoders. Figure 3. More visual comparison of images generated by the baseline and the ARRA model (with alignment) during the iteration process. 6. Implementation details and results of ImageNet datasets 6.1. Implementation details LlamaGen + ARRA. [48] LlamaGen is cutting-edge implementation of an autoregressive model for the image generation task. We apply the ARRA method on LlamaGen to test the generalization ability of our model. In our experiments, we use batch size of 32 with learning rate of 1e-4, and vocabulary size of 16,384. For the visual foundation Figure 4. Visualization of different lesion localizations and lesion levels. model, we use CLIP, where the extracted feature, our target representation, has the shape (batch size, 512). In the 111M model, the dimension of the hidden states is 768, while in the 343M model, the dimension of the hidden states is 1024. We use an MLP layer as the feature summarization tool to reduce the dimension of gidden states, making it possible to be aligned with our target. We set λ = 1 and use cosine similarity to calculate the loss between target representation and the hidden states. 6.2. More experiments results This is more comprehenive experiment results of ARRA on natural images  (Table 5)  , where we prove that ARRA enhances the generation ability of LlamaGen. ARRA works for both 111M and 343M models, indicating that this method is scalable. 6.3. Visualizations In this section, we present additional visualizations (Fig. 5, 6, 7, 8) on natural images, comparing ARRA and LlamaGen. 7. Open Source Code We include the project code in the supplementary material, which can be reproduced following the README file. Table 5. ARRA in natural image generation task across the ImageNet Dataset (classifier-free guidance=2.00)"
        },
        {
            "title": "Para",
            "content": "epochs data FID IS"
        },
        {
            "title": "LlamaGen\nLlamaGen\nLlamaGen\nLlamaGen\nLlamaGen\nLlamaGen\nLlamaGen\nLlamaGen\nLlamaGen\nLlamaGen\nLlamaGen",
            "content": "ARRA-Base ARRA-Base ARRA-Base ARRA-Base ARRA-Base ARRA-Base ARRA-Base ARRA-Base ARRA-Base 111M 111M 111M 111M 111M 111M 343M 343M 343M 343M 343M 111M 111M 111M 111M 111M 343M 343M 343M 343M 100 150 200 250 300 300 100 150 200 300 300 100 150 200 250 300 100 150 200 300 0.26M 9.328 0.26M 8.421 0.26M 8.010 0.26M 7.580 0.26M 7.283 1.28M 5.464 0.26M 5.192 0.26M 4.666 0.26M 4.419 0.26M 4.294 1.28M 4. 0.26M 8.710 0.26M 7.873 0.26M 7.483 0.26M 6.737 0.26M 6.653 0.26M 4.982 0.26M 4.544 0.26M 4.182 0.26M 3.971 126.4 142.7 145.4 161.8 156.7 193.6 187.8 206.7 202.0 218.6 286.6 133.0 142.2 145.8 162.7 159.0 192.1 200.8 206.8 219.1 Figure 5. 343M-LlamaGen Figure 6. 343M-ARRA Figure 7. 111M-LlamaGen Figure 8. 111M-ARRA"
        }
    ],
    "affiliations": [
        "Shenyang Institute of Automation, Chinese Academy of Sciences",
        "The University of Hong Kong",
        "University of Chinese Academy of Sciences"
    ]
}