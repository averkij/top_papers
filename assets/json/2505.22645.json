{
    "paper_title": "Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese",
    "authors": [
        "Hanjia Lyu",
        "Jiebo Luo",
        "Jian Kang",
        "Allison Koenecke"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models -- spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (https://github.com/brucelyu17/SC-TC-Bench)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 4 6 2 2 . 5 0 5 2 : r Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese Hanjia Lyu hlyu5@ur.rochester.edu University of Rochester Rochester, New York, USA Jian Kang jian.kang@rochester.edu University of Rochester Rochester, New York, USA Jiebo Luo jluo@cs.rochester.edu University of Rochester Rochester, New York, USA Allison Koenecke koenecke@cornell.edu Cornell University Ithaca, New York, USA Abstract While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (https://github.com/brucelyu17/SC-TC-Bench). CCS Concepts Computing methodologies Natural language processing; Social and professional topics; This work is licensed under Creative Commons Attribution 4.0 International License. FAccT 25, Athens, Greece 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1482-5/2025/06 https://doi.org/10.1145/3715275. Keywords algorithmic fairness, algorithmic audits, large language models, language generation biases, benchmark dataset, Chinese character sets ACM Reference Format: Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke. 2025. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese. In The 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT 25), June 2326, 2025, Athens, Greece. ACM, New York, NY, USA, 32 pages. https://doi.org/10.1145/3715275."
        },
        {
            "title": "1 Introduction\nLanguage is a tool for communication and a reflection of culture\nand identity. In regions with different historical, political, and social\npaths, unique linguistic systems have developed, resulting in varia-\ntions in vocabulary, syntax, and meaning. This is exemplified in the\nChinese language family, which exhibits significant differences due\nto geopolitical developments [26]. Simplified Chinese is predomi-\nnantly used in Mainland China, and serves as one of the official lan-\nguages in both Singapore and Malaysia, while Traditional Chinese\nis used in regions such as Taiwan, Hong Kong, and Macau [31, 57].\nAs Large Language Models (LLMs) have become integral to var-\nious applications in daily life [40, 53, 64], it is increasingly im-\nperative to study their variance in behavior across languages and\ncultures [2, 5, 58, 61], especially as these models become more\nmultilingual—although many remain oriented towards specific lan-\nguages. For instance, OpenAI’s GPT models are predominantly\ntrained on English language corpora, while Taiwan-LLM [29] was\npre-trained primarily on Traditional Chinese corpora.",
            "content": "It is well-studied that LLMs exhibit underperformance when tested on culture-specific commonsense knowledge (e.g., Shen et al. [47] shows underperformance across Chinese, Indian, Iranian, and Kenyan knowledge), political sample simulation [43], and for lowresource languages [17]. While Chinese broadly is not considered low-resource language, prior research has only focused on studying either Simplified Chinese or Traditional Chinese [32, 54, 62]but not comparison of both (see Appendix Table 3 for literature survey of prior Chinese LLM benchmark work). In contrast, our work aims to directly examine LLM behavior disparities in responses to prompts in either Simplified or Traditional Chinese. While one might expect LLM behavior to be relatively similar between Simplified and Traditional Chineseespecially because most written FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke (a) Prompted in Simplified Chinese. (b) Prompted in Traditional Chinese. Figure 1: Examples of prompt question (asked in Simplified Chinese in the left panel, and in Traditional Chinese in the right panel) and the corresponding response for each of three LLMs: GPT-4o, Qwen-1.5 and Taiwan-LLM (LLMs that are English, Simplified Chinese, and Traditional Chinese-oriented, respectively). LLMs do not consistently use culture-specific terms when prompted in the corresponding language variant; for example, Qwen-1.5 answers correctly when prompted in Simplified Chinese, but incorrectly when prompted in Traditional Chinese. English translations of the prompts and responses are written in blue and the script typewhether Simplified or Traditional Chineseis indicated in bold. translation involves one-to-one character mappingsthis does not necessarily minimize the existence of biases in the make-up of training data that reflect culturally different expressions or phrases specific to each linguistic variety. We illustrate the types of differences between Simplified and Traditional Chinese using examples of regional terms from Mainland China and Taiwan: Same term, same word: Terms may share the same word in both Mainland China and Taiwan, although some are written identically while others appear in different scripts: Same script: For instance, milk tea is often written identically in both regions. In both Simplified and Traditional Chinese, it is written as (pronunciation: nai cha). Different scripts: Some terms like brand are written dif- , but in . Both are pronounced ferently. In Simplified Chinese, it is written as Traditional Chinese it is written as as shang biao. Same term, different words: Terms may be referred to by completely different words in Mainland China and Taiwan. For instance, the term for computer mouse is referred to as (pronunciation: shu biao) in Mainland China while as (pronunciation: hua shu) in Taiwan. Another example is on- (pronunciation: wang line shopping, which is called shang gou wu) in Mainland China but (pronunciation: wang lu gou wu) in Taiwan. To study the differences in LLM responses to Simplified and Traditional Chinese prompts, our contributions are threefold. First, we design two tasks that reflect real-world scenarios: regional term choice (related to education) and regional name choice (related to hiring). To evaluate LLMs on these tasks, we have constructed and released benchmark dataset, SC-TC-Bench (Simplified Chinese-Traditional Chinese Benchmark); see Section A.3 for details. Our benchmark contains both question-answer pairs for regional terms that are described above (such as the definitions and terms for brand and computer mouse), and contains matched lists of regionally-popular names across script variants, along with normalized population counts and likely gender label for each name. Second, we study 11 diverse LLM services by prompting them with questions based on SC-TC-Benchcomparing responses to questions posed in either Simplified Chinese or Traditional Chinese. Illustrative examples are shown in Figure 1: for example, consider Qwen-1.5 (a Mainland China-based LLM which we refer to as Simplified Chinese-oriented). When asked about yellow spiky tropical fruit in Simplified Chinese, Qwen-1.5 correctly terms it as pineapple, but when asked about the same fruit in Traditional Chinese, Qwen-1.5 incorrectly terms it as papaya. In contrast, GPT-4o yields correct responses to both Simplified and Traditional Chinese prompts, while Taiwan-LLM yields incorrect responses to both prompts. Third, we quantify the biases exhibited by each LLM for each task and perform experiments to pinpoint the likely sources of disparities within LLMs. We find general trends across LLMs favoring Simplified Chinese in response to questions about regional terms; in contrast, LLMs seem to favor Traditional Chinese names in response to prompts about hiring someone from list of names. In particular, we find that the former biases may be partially explained by sparse training data on certain Traditional Chinese regional terms, while the latter biases seem to be rooted in LLM preferences for specific individual characters, and differences in tokenization of Simplified and Traditional Chinese. Overall, these findings point to the need for further exploration of the harmful biases that can occur, even between prima facie similar variants of language. While the LLMs studied may be technically competent, they may not always be neutral or fair in their application of that competencepotentially perpetuating societal inequalities or biases [7]. Our analyses are presented as reproducible framework that practitioners may use to continuously audit new LLMs for Simplified-Traditional Chinese biases. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece (a) Regional term choice task prompts. Direct English translation: What is defined as {definition}, please use one word to answer? (b) Regional name choice task prompts. Table 1: Identical prompts in Simplified and Traditional Chinese were generated for each of the regional term choice and regional name choice tasks. Prompt variants were also tested for robustness; see Section A.6 for details."
        },
        {
            "title": "2.1 Task Definitions\n2.1.1 Task 1: Regional Term Choice. This task evaluates the ability\nof LLMs to recognize and use “regional terms” accurately when\nprovided with item definitions in Simplified or Traditional Chinese.\nThese regional terms are specific words or expressions that differ\nnotably between Simplified Chinese and Traditional Chinese. Exam-\nples include the terms for “computer mouse” or “online shopping”\nprovided in Section 1, which refer to the same item but are entirely\ndifferent words.1 Recognition of these terms is critical to education\nand cultural preservation.",
            "content": "An ideal model would use the Simplified Chinese term to describe the regional term when prompted in Simplified Chinese, and would use the equivalent Traditional Chinese term to describe the same term when prompted equivalently in Traditional Chinese. The prompts used for this evaluation are shown in Table 1a, where {definition} is filled in (either in Simplified or Traditional Chinese, matching the rest of the prompting language) according to the regional term definitions elicited as described below in Section 2.2.1. 2.1.2 Task 2: Regional Name Choice. This task examines the extent to which LLMs exhibit biases in selecting candidates for job based on list of names from Mainland China and Taiwan. This task is rooted in real-world concerns: LLMs are increasingly 1We focus on these regional terms rather than more direct differences, such as using the same words to describe the same item, but with different scripts because these differences more acutely capture cultural differences between linguistic variants, and are not as simple as one-to-one mapping in different scripts. Furthermore, we consider expressions commonly used in Mainland China and Taiwan, and leave the examination of the terms used in other regions such as Hong Kong and Macau to future work. integrated into various decision-making processes across hiring, such as resume screening and interviewee selection [15, 45, 51]. These concerns have prompted legal interventions, such as New York Citys Local Law 144, which mandates bias audits for automated employment decision tools used in hiring [37]. It is crucial to understand if and how these models might perpetuate or amplify linguistic biases [18], which can have significant societal and individual consequences. Names can carry deep cultural, historical, and social significance; this task allows us to understand how well LLMs grasp these regional nuances to distinguish and evaluate names from different cultural contexts. An ideal model would reject the premise of choosing job candidate from list of names with no further context; regionallyunbiased model would choose names at an equal rate between those likely intuited as Mainland Chinese names versus Taiwanese names. The prompts used for this evaluation are shown in Table 1b, where 𝑁 represents the number of candidate employee names included in {name list}, which comprises of names of varying popularity in either Mainland China or Taiwan (but not both); further details on the names themselves are described below in Section 2.2.2. We additionally include English prompts to serve as baseline."
        },
        {
            "title": "2.2 Data Collection\nBelow, we describe the data collection process for each task. For\nboth tasks, text translations across English, Simplified Chinese, and\nTraditional Chinese were verified by native speakers/writers of\nEnglish, Simplified Chinese, and Traditional Chinese, respectively.",
            "content": "2.2.1 Regional Term Data. We collect 110 regional terms from prior published work on Cross-Strait vocabularies [26]; these terms span different themes including communication, travel, residence, and consumption. For each term, we obtain its two script variants: the Simplified Chinese term primarily used in Mainland China, and the Traditional Chinese term primarily used in Taiwan.2 We then source written definitions for each term in both Simplified and Traditional Chinese; Appendix A.8 discusses this process in more detail. Details of the manual review process to confirm the frequent usage of these vocabulary terms in their respective regions, and correctness of definition translations, are provided in Appendix A.7. 2.2.2 Regional Name Data. We first collect lists of names with corresponding population counts from two sources, both published in the previous decade: Mainland Chinese names are sourced from the name report published by the Ministry of Public Security of the Peoples Republic of China [39], while Taiwanese names are obtained from the name report published in Taiwan [1]. Note that neither report provides comprehensive list of all names; instead, they each include only the roughly 200 most popular names. Since all Taiwanese names in the corpus consisted of 3 characters, we similarly restricted to Mainland Chinese names with 3 characters. In total, there are 152 Mainland Chinese names, consisting of 11 distinct surnames and 44 distinct given names, as well as 200 Taiwanese names, comprising 12 distinct surnames and 130 distinct given names. Detailed statistics on the popularity of these names 2This scope does not account for other regional uses of Chinese scripts or linguistic variations in countries such as Malaysia, or Singapore. Consequently, we refrain from extrapolating our findings to these countries, as the results may not accurately reflect the complexities of Chinese language use outside the Mainland China-Taiwan context. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke bearing these names are provided in Appendix C.3. In our benchmark task, the {name list} provided in the prompt consists of 20 names total, always comprising 10 Mainland Chinese names and 10 Taiwanese names. To avoid potential biases that may arise from the order in which names are presented, we randomly shuffle the order of these 20 names (180 times per trial, per Appendix A.10)."
        },
        {
            "title": "2.3 Primary Metrics\n2.3.1 Correct and Misaligned Regional Term Shares. For each LLM,\nwe conduct 15 trials for each question-answer pair of a regional\nterm, and tabulate responses across all trial responses (see details\nin Section A.5). An LLM response is considered correct if it uses\nthe regional term that corresponds to the prompting language.\nSpecifically, when prompted in Simplified Chinese, the LLM should\nuse the Mainland Chinese term for the item, and when prompted\nin Traditional Chinese, it should use the Taiwanese term.",
            "content": "In contrast, there is particular case where the LLM swaps the regional termseither responding with the term more commonly used in Taiwan when prompted in Simplified Chinese, or the term more commonly used in Mainland China when prompted in Traditional Chinese. We refer to this as misaligned response. response is also classified as misaligned when an LLM prompted in Traditional Chinese generates Mainland Chinese term that has been directly converted to Traditional Chinese at the character level, rather than using the appropriate Taiwanese term. Similarly, this applies when an LLM prompted in Simplified Chinese produces Taiwanese term directly converted from Traditional Chinese instead of the correct Mainland Chinese term. Any other response is classified as incorrect. Hence, each response must fall into one of three mutually exclusive groups: (1) correct, (2) misaligned, and (3) incorrect. Our primary analyses are ttests (with Benjamini-Hochberg correction [50]) on the percentages of correct and misaligned responses, comparing between matched Simplified and Traditional Chinese prompts. In an ideal scenario without linguistic bias, the correct and misaligned response rates would be identical for prompts in Simplified or Traditional Chinese. 2.3.2 Mainland Name Selection Share. For each LLM, we conduct 100 trials (with 180 randomized iterations per trial) and extract the single name selected in each LLM response (see Appendix A.11 for details). We then calculate the share of times the LLM selects Mainland Chinese name, out of all valid name selections. We consider the LLM-based name selection to be unbiased by region if the share of Mainland Chinese names selected is 50%; equivalently, valid Mainland Chinese names should be selected at similar rate to valid Taiwanese names. To assess statistical significance, we conduct z-tests and apply the Benjamini-Hochberg correction [50]."
        },
        {
            "title": "2.4 Language Models\nWe benchmark 11 LLMs, which we categorize based on the primary\nlanguage of the training corpora. Following Zhang and Li [62], we\nrefer to the three LLM categories as English, Simplified and Tradi-\ntional Chinese-oriented LLMs. For the exact model variants, hyper-\nparameters, and implementation details, refer to Appendix A.2.",
            "content": "To ensure statistical reliability and assess the consistency of responses, we have LLMs answer each prompt multiple times, as determined by power analyses [12] (see details in Appendix A.5). We also generate multiple variants of each prompt to test the consistency of responses across different wordings while preserving the intended meaning. Main prompts are reflected in Table 1, and prompt variants are documented in Appendix A.6. All experiments were conducted between October 2024 and May 2025. English oriented: We audit six models GPT-4o [41], GPT-4 [40], and GPT-3.5 [9] (which OpenAI released between 2022 and 2024), Llama-3-70B and Llama-3-8B [34] (both introduced by Meta in 2024), and reasoning model, DeepSeek-R1-671B (which was trained via reinforcement learning without prior supervised fine-tuning [16], and released by DeepSeek-AI in 2025). Simplified Chinese oriented: We audit three language models Qwen-1.5 [6], ChatGLM-2 [59], and Baichuan-2 [56], all built by companies based in Mainland China. Qwen-1.5 is part of model family created by Alibaba Cloud and was released in 2023. ChatGLM-2 is the second-generation bilingual (ChineseEnglish) model based on the General Language Model (GLM) framework [13] released in 2022, offering enhanced capabilities for chat applications. Baichuan-2 is large-scale model developed by Baichuan Intelligent Technology, released in 2023. Traditional Chinese oriented: We audit two models Breeze and Taiwan-LLM. Breeze [20], released in 2024, is also specifically tailored for Traditional Chinese use. Taiwan-LLM [29], released in 2023 and designed specifically for Traditional Chinese as used in Taiwan, leverages comprehensive pre-training corpus and is further fine-tuned with instructional datasets."
        },
        {
            "title": "Correct when Prompted in Simplified\nChinese",
            "content": "We begin by comparing the percentages of correct, misaligned, and incorrect responses in Figure 2 for LLMs when each prompted in Simplified versus Traditional Chinese (denoted as the left and right bar for each LLM, respectively). Correct responses (comparing the and blue shaded bars for each LLM): Most LLMs, whether oriented toward English, Simplified Chinese, or Traditional Chinese, are significantly more likely to generate correct responses when prompted in Simplified Chinese compared to Traditional Chinese (𝑝 < .05). The only exception is Breeze, Traditional Chinese-oriented LLM, whose correct response rates are comparable across Simplified and Traditional Chinese prompts. Similar patterns for all LLMs persist even when prompts are slightly rephrased (see Figures 7 and 8). Misaligned responses (comparing the and yellow plain bars for each LLM): All LLMs are significantly more likely to generate misaligned responses when prompted in Traditional Chinese compared to Simplified Chinese (𝑝 < .05). This suggests that, Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece n r y s s % 100 80 60 40 20 Qwen-1.5 Simplified Chinese oriented LLMs Traditional Chinese oriented LLMs Correct response Misaligned response Incorrect response English oriented LLMs T Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeekR1-671B GPT-4o GPTS GPT-3.5 S Llama-3-70B Llama-3-8B Figure 2: All LLMs (except for Breeze) are significantly more likely to generate correct responses when prompted in Simplified Chinese compared to Traditional Chinese (𝑝 < .05, comparing the two blue shaded bars within each LLM labeled and Treferring to the LLM when prompted in Simplified Chinese or Traditional Chinese, respectively). In contrast, LLMs are more likely to generate misaligned responses when prompted in Traditional Chinese (𝑝 < .05, comparing the yellow shaded bars within each model across and T); an example is if Traditional Chinese prompt asks for the name of spiky yellow tropical fruit, and the LLM returns the Simplified Chinese term for pineapple (bo luo) instead of the expected Traditional Chinese term for pineapple (feng li). when prompted in Traditional Chinese, LLMs are capable of associating the given definition with the corresponding termbut disproportionately so using the Simplified Chinese variant of the term. Meanwhile, such behavior is significantly less frequent when models are prompted in Simplified Chinese. Incorrect responses (comparing the and red shaded bars for each LLM): For some LLMs, such as GPT-4o, GPT-3.5, and DeepSeek-R1-671B, the share of incorrect responses is similar across Simplified and Traditional Chinese prompts (equivalently, the share of responses that are either correct or misaligned is the same across Chinese prompting language variant), suggesting comparable ability to recognize the item. However, the underlying disparities in the percentage of correct responses suggest that the biases for these LLMs are more related to linguistic factors rather than conceptual understanding. We can also see that, irrespective of prompting language, DeepSeek-R1-671B and the more recent GPT models tend to significantly outperform their competitors on conceptual understanding (as their red bars are significantly shorter than the other LLMs).3 Figure 2 also allows us to compare OpenAIs models temporally: while the share of correct responses (blue shaded bars) in Traditional Chinese remains stable from GPT-3.5 to GPT-4o, the share of correct responses in Simplified Chinese increasessuggesting growing bias favoring Simplified Chinese within OpenAIs models."
        },
        {
            "title": "3.2 Why Do Traditional Chinese Prompts Yield",
            "content": "Disproportionate Misaligned Responses? Our hypothesis for why Traditional Chinese prompts tend to yield significantly more misaligned responses (i.e., responses containing 3We note that the share of incorrect responses may appear quite high. However, these rates reflect current LLM abilities in comparable studies of Chinese prompting [21, 49]. Manual inspection of incorrect responses reveals two key types of errors: (1) the term described in the response is entirely wrong, and (2) the term described in the response is accurate, but the expression used is uncommon. See Appendix B.4 for more details. the equivalent Simplified Chinese terms instead) has to do with language imbalance in LLM training data [5, 47], grounded in the fact that Simplified Chinese is more prevalent in online and global datasets [35]. However, all LLMseven those which are Traditional Chinese orientedhad larger misalignment rates for Traditional Chinese regional terms than for Simplified Chinese regional terms. As such, we first comment on the genre of regional terms that lead to misalignment for each LLM, and then relate regional term misalignment to their occurrence frequencies in large online corpora (serving as proxies for LLM training data). 3.2.1 Observations on Regional Terms Commonly Misaligned. Of the 110 regional terms, on averageacross LLMs37.5 (𝑆𝐷 = 15.2) are misaligned when prompted in Traditional Chinese (we define misalignment as occurring at least 3 times out of 15 experiment trials of the regional term task; with this definition, only 4.4 (𝑆𝐷 = 2.3) terms are misaligned when prompted in Simplified Chinese). Misaligned Traditional Chinese terms are disproportionately about ) and tandem bitravel topics, such as tourism bureau ( ), which LLMs would instead return in Simplified cycle ( Chinese as , respectively. While some LLMs have diverse spread of misaligned terms (DeepSeek-R1-671B results in 72 terms ever being misaligned), some LLMs only yield misalignment on handful of terms (Breeze results in 16 terms ever being misaligned). full list of terms and their misalignment rates by LLM are provided in Appendix B.2 Tables 1417. and 3.2.2 Misaligned Terms from Mainland China are More Prevalent Across Large Text Corpora. For each LLM, we examine the set of regional terms classified as misaligned (whereinfor each Traditional Chinese termat least half of the tested LLMs yield misaligned result per the aforementioned definition), and aim to understand whether the regionally inverted variants (i.e., Mainland Chinese terms) are overrepresented in the underlying LLM training data. Since the full details of the LLM training corpora are not publicly FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke disclosed, we use nine publicly available text corpora as proxies three in Simplified Chinese (including collection of Baidu Baike pages, the leading Mainland Chinese equivalent of Wikipedia), five in Traditional Chinese (including collection of Traditional Chinese Wikipedia pages), and one containing mixture of both. While these corpora are predominantly in Simplified and/or Traditional Chinese, they are still each likely to contain text in other variants (see Table 13). We report average frequencies of regional terms broken down by misaligned versus non-misaligned terms, each occurring in Simplified or Traditional Chineseappearing in each corpus in Table 18 (see Appendix A.9 for details). We find that, consistent with our previous findings, that misaligned terms tend to appear more frequently written in Simplified Chinese than in Traditional Chinese. Furthermore, across only the Traditional Chinese corpora, the ratio of the frequency of Simplified Chinese appearances to Traditional Chinese appearances is extremely low among non-misaligned terms (i.e., as expected, LLMs perform well at recovering Traditional Chinese terms that are well-represented in corpora), but this ratio is much higher among misaligned terms (i.e., Traditional Chinese appearances of misaligned terms are underrepresented, even in Traditional Chinese corpora). Meanwhile, across all Simplified Chinese corpora tested, the Simplified-to-Traditional frequency ratio is consistently highregardless of whether terms are misaligned or notpointing towards relative overrepresentation of Simplified Chinese among misaligned terms. These consistent trends highlight data imbalance as key factor underlying the observed regional term bias."
        },
        {
            "title": "4.1 Most LLMs Select More Taiwanese Names",
            "content": "than Mainland Chinese Names We calculate the share of times each LLM selects valid name from the provided candidate list when prompted;4 this valid response rate is depicted along the x-axis of Figure 3. Thenfor each LLM among the times that valid name is selected, we calculate the share of selected names that are Mainland Chinese names (as opposed to Taiwanese names) from the dataset compiled per Section 2.2. We refer to this as the Mainland Chinese Name Rate, depicted along the y-axis of Figure 3. Mainland Chinese name rates: we would expect regionally unbiased name selection model to adhere to 50% Mainland Chinese name rate (dotted bold horizontal line in Figure 3), regardless of prompting language, since the proportion of Mainland Chinese names comprising the randomized 20-name candidate lists (presented as prompts) is held constant at 50%. In general, most 4Invalid rates and explanations for non-response vary across LLMs. See Appendix C.1 for more details. LLMsregardless of whether they are English, Simplified, or Traditional Chinese-orientedare more likely to select valid Taiwanese name in this task (as opposed to valid Mainland Chinese name), as indicated by the majority of LLM markers lying below the dotted 50% line. The exceptions are two LLMs which are more likely to select Mainland Chinese name under certain prompting conditions: Taiwan-LLM when prompted in Simplified Chinese or English, and ChatGLM-2 regardless of prompting language. Valid response rates: we believe truly unbiased LLM would opt out of ever choosing names (aligning with 0% rate of valid responses), but this does not always occur. In fact, while Taiwan-LLM in some experiments shows small rate of valid responses, this is as often due to non-adherence to prompting instructions (e.g., picking multiple names, or names that are not part of the original candidate list), as opposed to opting out of the concept of choosing candidate. In general, prompting in English tends to yield the highest rate of valid responses in this task across LLMs (even those that are not English-oriented); it remains the case that (even among only LLMs that have high response rates) the majority of LLMs select Taiwanese names. It is noteworthy that simply changing prompting language (while holding the candidate name lists constant) significantly changes the degree to which different LLMs yield valid responses. For example, Traditional Chinese-oriented LLM Breeze has the lowest valid response rate when prompted in Simplified Chinese, middling valid response rate when prompted in Traditional Chinese, and high valid response rate when prompted in English. Meanwhile, Simplified Chinese-oriented LLM ChatGLM-2 has the lowest valid response rate with Traditional Chinese prompts, middling valid rate with English, and the highest valid response rate when prompted in Simplified Chinese."
        },
        {
            "title": "4.2 Why Do Most LLMs Prefer Taiwanese",
            "content": "Names? To understand why LLMs tend to display regional name biases, we first present observed examples of frequently selected names to substantiate hypotheses for why certain LLMs have bias towards Taiwanese names. Table 2 presents the top 5 most-frequently selected names by four representative LLMs (see more in Appendix C.2), for each of the three prompting languages. We see that Simplified Chinese-oriented LLMs differ: while Baichuan-2 has mostly Taiwanese names in its top-5 selection regardless of prompting language (denoted by blue to the left of each Taiwanese name), ChatGLM-2 instead has entirely Mainland Chinese names in its top-5 selection (denoted by red to the left of each Mainland name)regardless of prompting language. In contrast, the Traditional Chinese oriented Taiwan-LLM has mixed set of top names selected from both Mainland China and Taiwan (though prompting in Simplified Chinese yields mostly Taiwanese names among the top 5). The English-oriented GPT-4o yields entirely Taiwanese names in the top 5 regardless of prompting language. Looking more granularly at the selected names themselves, we glean four insights, each of which points towards potential reasons for LLM biases in the regional name task: (1) Some names could be disproportionately likely to be selected ), due to real-world popularity, such as Wang Jun Kai ( the name of celebrity. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Figure 3: Most LLMswhether they are English, Simplified Chinese, or Traditional Chinese-orientedtend to select valid Taiwanese name more often than valid Mainland Chinese name for the regional name choice task (as indicated by the majority of points falling below the 50% dotted horizontal line for Mainland Chinese Name Rate). Furthermore, no LLMs display consistently low rates of valid responses; rather, most LLMs will respond to our name selection prompt with valid candidate names, irrespective of the ethical concerns of choosing candidates by name alone. Within LLM, rates of valid responses often change depending on prompting language (i.e., each point may shift left or right among the three figure panels). (2) There may be intersectional effects with gender: for example, ChatGLM-2 tends to favor female-associated names, while GPT-4o shows preference for male-associated ones. (3) Some LLMs appear to favor specific characters. For example, nearly all of the names most frequently selected by ChatGLM-2 begin with the same last name, Li (written as in both Simplified and Traditional Chinese). (4) Even when the last names are the same word, LLMs may still exhibit preferences based on the script. For instance, Baichuan-2 demonstrates stronger preference for Taiwanese names, favoring the surname Chen more often in Traditional Chinese ). ) than the same surname written in Simplified Chinese ( ( These observations suggest four potential explanations for the uncovered biases in the regional name choice task, each of which we conduct experiments to analyze: (1) the popularity of certain names (Section 4.3), (2) interactions with gender (Section 4.4), (3) LLM preferences for specific characters (Section 4.5), and (4) differences in written scripts (Section 4.6)."
        },
        {
            "title": "4.3 Name Popularity (Does Not Explain",
            "content": "Regional Name Biases) We study whether our findings in Section 4.1 are robust to the same experiment when conditioning on names by popularity. We define popularity in two ways: firstly, based on true population densities in Mainland China and Taiwan (i.e., how common the name is in each region), and secondly, based on popularity in large online corpora (i.e., how likely the name is to appear in training data). (a) Baichuan-2 (b) ChatGLM-2 (c) Taiwan-LLM (d) GPT-4o Table 2: The top 5 most frequently selected names when prompted in English, Simplified, or Traditional Chinese. LLMs show different preferences among their top-selected Mainland Chinese names (red-shaded M) and Taiwanese names (blue-shaded T). See full results in Appendix C.2. methodology used in the original experiment with two key modifications: we first subset names to be mutually exclusive by region based on given (first) names only so that Mainland Chinese given names in our corpus do not appear in the Taiwanese corpus, and Taiwanese given names do not appear in the Mainland Chinese corpus.5 When constructing the candidate name list, we select Mainland Chinese and Taiwanese names that have comparable levels of 4.3.1 Population-based Name Popularity. To explore whether name popularity influences LLMs selection behavior, we adapt the 5Only 3 names were removed from each of the Mainland Chinese and Taiwanese name lists from this exclusion. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke popularity in their respective regions. We define popularity based on the percentage of people in each region who bear that name, and bin names into ten distinct deciles based on their popularity in either Mainland China or Taiwan. For each experimental trial, we construct the candidate name list by randomly selecting one name from each decile group, from each region. This approach ensures that the names selected for each trial are evenly distributed across different levels of popularity, allowing for more controlled examination of the impact of name popularity on LLMs selection preferences. Further details on name counts and distributions can be found in Appendix C.3. We then generate an analogous figure to Figure 3 using the same methods; conditioned on population-based name popularity, we find in Appendix Figure 10 that the overall name selection pattern remains largely unchanged (relative to our main results in Figure 3), suggesting that population-based name popularity does not significantly impact name selection patterns of LLMs. 4.3.2 Online-based Name Popularity. We now want to consider whether celebrity names might significantly skew LLM selection results. We conceptualize celebrity by using proxy: how frequently Mainland Chinese or Taiwanese name might occur in underlying training data. We operationalize this by retrieving the frequency of each names occurrence in the Common Crawl web crawl corpus.6 Then, for each LLM and each prompting language variant (English, Simplified Chinese, or Traditional Chinese), we examine the relationship across all 352 names between LLM selection frequency (i.e., , how frequently that name is chosen as share of all responses) and online popularity (i.e., the frequency of each name in Common Crawl) by conducting Spearman Rank tests with Benjamini-Hochberg correction [50]. As shown in Table 22, most LLMs show no significant relationship between name selection and online popularity, suggesting that these models may rely on factors beyond mere corpus frequencyconsistent with our findings regarding population-based popularity. As an example, Table 23 shows that Baichuan-2 selects two celebrity names at rates significantly lower than uniform-at-random. An exception is ChatGLM-2, which has significant weak positive correlations between its name selections and online name popularity; this, taken together with the Table 2b finding that ChatGLM-2 has high propensity of selecting Mainland Chinese names, may indicate an over-representation of Mainland Chinese content in its training corpus."
        },
        {
            "title": "4.4 Preferences for Male Names (Do Not Fully",
            "content": "Explain Regional Name Biases) To determine whether gender distribution differences in candidate lists might affect Mainland Chinese versus Taiwanese name selection, we subset to the set of experiments having matched gender distributions7 and population-based popularity between selected Mainland Chinese and Taiwanese names. When controlling for gender distributions, Taiwanese names are selected at higher rate than Mainland names across 32,085 out of 48,795 experiments. These results and corresponding significance levels (testing whether the Mainland Chinese name selection rate falls below 50%) are presented in Tables 24, 25, and 26. To supplement these observational results, we now repeat the candidate name list experiment from Section 4.1, but this time balancing on gender (i.e., randomly selecting 5 names each associated with Mainland males, Mainland females, Taiwanese males, and Taiwanese females) and balancing on population-based popularity for names selected in each region.8 We find, consistent with prior work [38], that gender bias exists among LLMs: male name selection rates are higher than female name selection rates in all LLMs except for Baichuan-2; and, this gendered difference is statistically significant in the vast majority of LLMs and prompting languages tested (see Table 30 for full results). Looking at the difference between Mainland Chinese and Taiwanese name selection rates, we see that while the preference for Taiwanese names is somewhat reduced compared to the original results per Figure 11, this difference is likely due to the low count of male names in the Mainland Chinese name corpus (with nearly 80 fewer male names to choose from than in the Taiwanese name corpus), which was used in the original candidate name list experiment. However, most LLMs still favor Taiwanese names, and this reduction is not as pronounced as in forthcoming experiments comparing the same names across different scripts (Section 4.6)."
        },
        {
            "title": "4.5 Preferences for Specific Characters",
            "content": "(Partially Explain Regional Name Biases) We now study whether our findings in Section 4.1 are robust to the same experiment when conditioning on names that only differ by specific character. Here, the hypothesis is that specific characters may be disproportionately favored by certain LLMs, which leads to entire names being chosen on the basis of containing specific favored character. Specific Character Experiments. To investigate whether LLMs 4.5.1 exhibit preferences for specific Chinese characters that may explain regional name selection biases, we analyze LLMs token generation probabilities. Specifically, we interpret the token generation probability of character as the models preference for that character. We hypothesize that higher generation probabilities for specific characters may lead LLMs to more frequently select names containing those characters. Given the complexity of Chinese given nameswhere the semantics and phonetics of the two-character combinations are often interdependentwe restrict our analysis to last name characters, which are typically independent and more standardized. This allows for cleaner examination of character-level preferences. We select pairs of names that share the same first name but different last names, and for which the full three-character name is within the same decile of population-based popularity (see Table 34 for all names). Token generation probability of candidates last name is measured by prompting the LLM with only the first name; we also calculate selection probability of name (similar to previous experiments)details are provided in Appendix C.8. If the token 6https://huggingface.co/datasets/allenai/c4 7For example, each candidate list is comprised of 10 Taiwanese and 10 Mainland names; we restrict to candidate lists where both sets of 10 names have the same gender ratio, e.g., 7 male and 3 female names. 8For Taiwanese names, we obtain gender annotations directly from the underlying report [1]. Since the corresponding report for Mainland Chinese names [39] did not include gender information, we use GPT-4o-mini to infer the gender of each name and manually verify the labels. Additional details are provided in Appendix C.6. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece generation probability is higher for one last name than its matched pair, and the LLM also selects that last name in its head-to-head selection task, we consider the model to agree. Across all tested models, the agreement rate is significantly above 50% (see Table 35). We also compute the token generation probabilities separately for Mainland Chinese and Taiwanese last names in Table 36, finding that most tested LLMs assign significantly higher generation probabilities to Taiwanese last names than to Mainland Chinese ones. Together with the findings in Table 35, these results suggest that LLMs character preferencesquantified via token generation probabilitiesat least partially explain the observed biases in regional name selection. 4.5.2 Character-Related Qualitative Text Analysis. We supplement our experimental analysis with observational notes on LLM response texts. subset of LLMs (Baichuan-2 and Qwen-1.5) despite only being asked to return single name in the response to our regional name promptreturn explanations for why they chose name. We first extract descriptive adjectives from the LLM responses and then count their occurrences. Notably, adjectives such as talented and wisdom more frequently appear in descriptions associated with Taiwanese names; an example is , , both of which are also found which contains characters in other Taiwanese names selected by LLMs for similar adjective associations. Neither of these characters appears in any of the Mainland Chinese names included in our corpus, which may partially account for the observed regional bias in name selection. See Appendix Tables 31 and 32 for the top 10 characters used by Baichuan-2 and Qwen-1.5 to describe both Mainland Chinese and Taiwanese names. Appendix C.7 details how we extract the descriptive words. and"
        },
        {
            "title": "4.6 Differences in Scripts (Partially Explains",
            "content": "Regional Name Biases) Thus far, we have found that LLM biases for regional names cannot be fully explained by name popularity or gender bias, and can only be partially explained by certain characters being disproportionately favored by certain LLMs. As such, we turn to our final set of experiments: whether our findings in Section 4.1 are robust to the same experiment when conditioning on names that are identical but for their written script (similar to the word brand in our Section 1 example). This adjustment allows us to directly assess the impact of script differences on LLMs name selection. Same Name, Different Script Experiments. Among all the 4.6.1 Mainland Chinese and Taiwanese names collected, only six names three from each region share the same word but are written in visually distinct scripts. This group comprises two unique last names and three unique first names; all names tend to be associated with female identities, allowing us to avoid measuring gender-based effects. In these experiments, we restrict our candidate name list to only include these six names, and otherwise prompt in the same ways (requesting for one name to be chosen), running 8,000 trials of this experiment. Figure 4 illustrates that the selection bias favoring Taiwanese names is ameliorated when the names (but not scripts) are kept constant. Points (denoting each LLM) correspond to the rate of valid responses and Mainland Chinese name rate in this same-name experiment; solid red arrows denote an increase in Mainland Chinese name rate from Figure 3, while dashed blue arrows denote decrease in Mainland Chinese name rate from Figure 3. Nearly all LLMs exhibit an increase in Mainland Chinese name rates when choosing between names that only differ in Simplified versus Traditional script. In fact, conditioning on the same names results in flip in outcomes: now, the majority of LLMs exhibit preference for selecting Mainland Chinese names over Taiwanese names regardless of prompting language (though, the set of LLMs that are above the 50% Mainland Chinese name rate line are different depending on the prompting language). In this setting, only Qwen-1.5 consistently displays bias towards selecting Taiwanese names. Meanwhile, there is stronger preference for Mainland Chinese names across all prompting languages among English, Simplified Chinese, and Traditional Chinese oriented LLMs: GPT-4o, GPT-3.5, Baichuan-2 and Breeze. This inversion of results relative to Figure 3 raises the question: why might script differences play role in regional name selection biases? 4.6.2 Tokenization of Different Scripts. To investigate, we examine how LLMs tokenize characters written in Simplified versus Traditional Chinese. Unlike English, where words are separated by spaces, Chinese characters are written continuously without spaces between them. This presents unique challenge for tokenization because character segmentation can lead to different and even inaccurate interpretations [48, 52]. We begin by constructing four name lists from our full set of names collected in Section 2.2: (1) The original Mainland Chinese names, written in Simplified Chinese. (2) The names in the first list, but converted into Traditional Chinese on character-by-character basis. (3) The original Taiwanese names, written in Traditional Chinese. (4) The names in the third list, but converted into Simplified Chinese on character-by-character basis. We then use each LLMs tokenizer to tokenize these name lists and calculate the average token counts. To determine whether script differences significantly impact tokenization, we perform Students t-tests comparing the matched average token counts between the first and second lists (Simplified vs. Traditional Chinese for Mainland Chinese names) and between the third and fourth lists (Traditional vs. Simplified Chinese for Taiwanese names). Table 37 reveals that, for most LLMs,9 the average token counts for the same name differ significantly depending on whether the name is written in Simplified or Traditional Chinese (with the latter tending to result in higher number of tokens); this suggests that tokenization of Simplified and Traditional Chinese likely contributes to the observed name selection biases. Such tokenization disparities are consistent with findings from prior studies [3, 42], which highlight that low-frequency appearances in the training data can lead to over-fragmentation during tokenization. Moreover, Ahia et al. [3] note that script-specific linguistic features can further exacerbate fragmentation. These factors indicate that tokenization is not merely technical preprocessing step but potential source of 9Exceptions, where Simplified Chinese produces higher number of tokens, occur for both Traditional Chinese-oriented LLMs Breeze and Taiwan-LLM, as well as Simplified Chinese-oriented LLM ChatGLM-2. For Taiwan-LLM, character-by-character translations between Simplified and Traditional Chinese do not significantly change the average token count; for the Llama-3 models, Taiwanese names converted to Simplified Chinese do not yield significantly different average token count. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Figure 4: The selection bias favoring Taiwanese names is invertedrevealing the majority of LLMs favoring Mainland Chinese nameswhen controlling for the name (with the only source of variation coming from the name scriptwritten in Simplified or Traditional Chinese). Arrows indicate the relative movement of data points compared to their positions in Figure 3. Red solid arrows represent an increase in the selection rate of Mainland Chinese names, while blue dashed arrows indicate decrease. systematic bias that can influence LLM behavior on downstream tasks [8]. In our case, the fragmentation of Traditional Chinese names by LLMs primarily trained on Simplified Chinese or English may distort the semantic interpretation of the names, thereby leading to altered or biased model behavior."
        },
        {
            "title": "5 Discussion",
            "content": "Limitations. While our SC-TC-Bench data covers several important real-world contexts, it is far from comprehensively covering all differences in Simplified and Traditional Chinese, let alone Mainland China and Taiwan. Our work can be extended by including additional prompts focused on language ability or knowledge (per Appendix Table 3), and regional terms covering more locations for Traditional Chinese (such as Hong Kong and Macau), and more diverse regional terms for Simplified Chinese (such as those spoken predominantly by ethnic minority groups in Mainland China). Furthermore, we see our work as starting point for auditing Chinese linguistic disparities in existing LLMs, and encourage auditors to apply our methods to study newer LLMs as they improve and adapt over time. In addition, although we discuss multiple contributing factorstraining data imbalance, character preferences, and tokenization differencesthat may lead to biases, these factors are deeply intertwined in practice, making it challenging to isolate their individual effects. The frequency of specific characters in the training data directly influences the models learned preferences (e.g., raw token generation probabilities). Moreover, the distribution of training data informs the design of the tokenizer used during pretraining. Character form and tokenization are also closely connected: certain characters may be split into multiple tokens or assigned varying frequency weights in the tokenizers vocabulary. We identify this as an important avenue for future researchparticularly, the development of methodologies to analyze such interdependencies. Calls to Action. The two benchmark tasks we explore have significant real-world relevance to downstream education and hiring applications, potentially leading to LLM-based disparities between writers of Simplified and Traditional Chinese. We first uncovered that underlying training data may be driver of biases favoring Simplified Chinese in the regional term task; this points to need for diversifying underlying training data and collecting niche data on regional terms. Practitioners can help with this effort by collecting similar crosswalk datasets between varieties of languages, potentially leading to improved cultural and educational understanding of regional terms. We next found that specific characters and tokenization of written scripts could be driver of biases favoring Traditional Chinese in the regional name task. However, we underscore that another concern is the variability in our results across experiments: by simply making small, single-character changes, we could elicit huge swings in regional biases. Given the potential harms caused by this variability, we call for (a) better guardrails on LLM systems (especially in hiring contexts) to avoid biases from specific charactersand ideally simply opt out of responding, and (b) more research into tokenization methods for different script systems with an eye towards equity. Addressing LLM biases in such linguistic variants is crucial for developing LLMs that minimize representational harm towards users of both Simplified and Traditional Chinese, and aim to better understand the deeper cultural contexts conveyed by written language. References [1] 2018. Name Statistics. https://www.ris.gov.tw/documents/data/5/2/107namestat. pdf Accessed: 04-09-2024. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece [2] Anurag Acharya, Kartik Talamadupula, and Mark Finlayson. 2021. An atlas of cultural commonsense for machine reasoning. In AAAI Conference on Artificial Intelligence. [3] Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. 2023. Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 99049923. doi:10.18653/v1/2023.emnlp-main.614 [4] Ai2. 2021. c4. https://huggingface.co/datasets/allenai/c4. Accessed: 2025-04-28. [5] Mohammad Atari, Mona Xue, Peter Park, Damián Blasi, and Joseph Henrich. 2023. Which humans? (2023). [6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023). [7] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. MIT Press. [8] Kaj Bostrom and Greg Durrett. 2020. Byte Pair Encoding is Suboptimal for Language Model Pretraining. In Findings of the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 46174624. doi:10.18653/v1/2020.findingsemnlp.414 [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 18771901. [10] Jianbin Chang. 2023. chinese-c4. https://huggingface.co/datasets/shjwudp/chin ese-c4. Accessed: 2025-04-28. [11] Pokai Chang. 2023. zh-tw-wikipedia. https://huggingface.co/datasets/zetavg/zhtw-wikipedia. Accessed: 2025-04-28. [28] Yen-Ting Lin. 2024. TaiwanChat. https://huggingface.co/datasets/yentinglin/Tai wanChat. Accessed: 2025-04-28. [29] Yen-Ting Lin and Yun-Nung Chen. 2023. Taiwan llm: Bridging the linguistic divide with culturally aligned language model. arXiv preprint arXiv:2311.17487 (2023). [30] Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, et al. 2023. M3ke: massive multi-level multi-subject knowledge evaluation benchmark for chinese large language models. arXiv preprint arXiv:2305.10263 (2023). [31] Tianyin Liu and Janet Hsiao. 2012. The perception of simplified and traditional Chinese characters in the eye of simplified and traditional Chinese readers. In Proceedings of the Annual Meeting of the Cognitive Science Society, Vol. 34. [32] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. 2023. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743 (2023). [33] Mapull. 2022. Chinese Pinyin Dictionary. https://github.com/mapull/chinesedictionary Accessed: 04-09-2024. [34] AI Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. Meta AI. (2024). [35] Marco Monroy. 2024. Simplified vs. Traditional Chinese: Whats the difference? guide. https://www.berlitz.com/blog/traditional-vs-simplified-chinese Accessed: 06-15-2024. [36] Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 53565371. doi:10.18653/v1/2021.acl-long.416 [37] New York City Council. 2021. Local Law 144 of 2021. https://www.nyc.gov/asse [12] Jacob Cohen. 1992. Statistical power analysis. Current directions in psychological ts/dca/downloads/pdf/about/Local-Law-144.pdf. science 1(3) (1992). [13] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. (2022), 320335. [14] Philipp Ennen, Po-Chun Hsu, Chan-Jan Hsu, Chang-Le Liu, Yen-Chen Wu, YinHsiang Liao, Chin-Tung Lin, Da-Shan Shiu, and Wei-Yun Ma. 2023. Extending the pre-training of bloom for improved support of traditional chinese: Models, methods and results. arXiv preprint arXiv:2303.04715 (2023). [15] Chengguang Gan, Qinghao Zhang, and Tatsunori Mori. 2024. Application of llm agents in recruitment: novel framework for resume screening. arXiv preprint arXiv:2401.08315 (2024). [16] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [17] Daniil Gurgurov, Mareike Hartmann, and Simon Ostermann. 2024. Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters. In Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024), Russa Biswas, Lucie-Aimée Kaffee, Oshin Agarwal, Pasquale Minervini, Sameer Singh, and Gerard de Melo (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 6374. doi:10.18653/v1/2024.kallm-1.7 [18] Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky, and Sharese King. 2024. Dialect prejudice predicts AI decisions about peoples character, employability, and criminality. arXiv preprint arXiv:2403.00742 (2024). [19] Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang Chen, and Da-shan Shiu. 2023. Advancing the Evaluation of Traditional Chinese Language Models: Towards Comprehensive Benchmark Suite. arXiv preprint arXiv:2309.08448 (2023). [20] Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang Chen, and Da-Shan Shiu. 2024. Breeze-7B Technical Report. (2024). arXiv:2403.02712 [cs.CL] [21] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024. C-eval: multilevel multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems 36 (2024). [22] Lee Chak Kei. 2023. OpenOrca-Traditional-Chinese. https://huggingface.co/dat asets/lchakkei/OpenOrca-Traditional-Chinese. Accessed: 2025-04-28. [23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212 (2023). [24] Sheng-Wei Li. 2024. c4-zhtw. https://huggingface.co/datasets/liswei/c4-zhtw. Accessed: 2025-04-28. [25] Sheng-Wei Li. 2024. common-crawl-zhtw. https://huggingface.co/datasets/liswei /common-crawl-zhtw. Accessed: 2025-04-28. [26] Xingjian Li, Zhiqun Qiu, and Fuling Xu. 2014. Cross-Strait Common Vocabulary. Fujian Peoples Publishing House. [27] Yizhi Li. 2024. MAP-CC. https://huggingf ace.co/datasets/m-a-p/MAP-CC. Accessed: 2025-04-28. [38] Huy Nghiem, John Prindle, Jieyu Zhao, and Hal Daumé Iii. 2024. You Gotta be Doctor, Lin : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 72687287. doi:10.18653/v1/2024.emnlp-main. [39] Ministry of Public Security (China). 2013. Moat Popular Names. https://web.ar chive.org/web/20160920191749/http://zhaoren.idtag.cn/samename/searchName !pmbyrepeatlist.htm Accessed: 04-09-2024. [40] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). doi:10.485 50/ARXIV.2303.08774 arXiv:2303.08774 [41] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ Accessed: 06-12-2024. [42] Anaelia Ovalle, Ninareh Mehrabi, Palash Goyal, Jwala Dhamala, Kai-Wei Chang, Richard Zemel, Aram Galstyan, Yuval Pinter, and Rahul Gupta. 2024. Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies. In Findings of the Association for Computational Linguistics: NAACL 2024, Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 17391756. doi:10.18653/v1/20 24.findings-naacl. [43] Weihong Qi, Hanjia Lyu, and Jiebo Luo. 2024. Representation bias in political sample simulations with large language models. arXiv preprint arXiv:2407.11409 (2024). [44] Science & Technology Policy Research and Information Center. 2020. Formosa Language Understanding Dataset. https://scidm.nchc.org.tw/dataset/grandchall enge2020 Accessed: 06-12-2024. [45] Rithani, Venkatakrishnan, et al. 2024. Empirical Evaluation of Large Language Models in Resume Classification. In 2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT). IEEE, 14. [46] Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. 2018. DRCD: Chinese machine reading comprehension dataset. arXiv preprint arXiv:1806.00920 (2018). [47] Siqi Shen, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Soujanya Poria, and Rada Mihalcea. 2024. Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense. arXiv preprint arXiv:2405.04655 (2024). [48] Chenglei Si, Zhengyan Zhang, Yingfa Chen, Fanchao Qi, Xiaozhi Wang, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. 2023. Sub-Character Tokenization for Chinese Pretrained Language Models. Transactions of the Association for Computational Linguistics 11 (2023), 469487. doi:10.1162/tacl_a_ [49] Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Jun-Da Chen, Wei-Min Chu, Sega Cheng, and Hong-Han Shuai. 2024. An improved traditional chinese evaluation suite for foundation model. arXiv preprint arXiv:2403.01858 (2024). [50] David Thissen, Lynne Steinberg, and Daniel Kuang. 2002. Quick and easy implementation of the Benjamini-Hochberg procedure for controlling the false positive rate in multiple comparisons. Journal of educational and behavioral statistics 27, FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke 1 (2002), 7783. [51] Thanh Tung Tran, Truong Giang Nguyen, Thai Hoa Dang, and Yuta Yoshinaga. 2023. Improving Human Resources Efficiency with Generative AI-Based Resume Analysis Solution. In International Conference on Future Data and Security Engineering. Springer, 352365. [52] Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Guochao Jiang, Jiaqing Liang, and Deqing Yang. 2024. Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization. arXiv preprint arXiv:2405.17067 (2024). [53] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). [54] Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, and Zhenzhong Lan. 2023. Superclue: comprehensive chinese large language model benchmark. arXiv preprint arXiv:2307.15020 (2023). [55] Qinyang Xu. 2023. BaiduBaike-5.63M. https://huggingface.co/datasets/xuqinyan g/BaiduBaike-5.63M. Accessed: 2025-04-28. [56] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023. Baichuan 2: Open largescale language models. arXiv preprint arXiv:2309.10305 (2023). [57] Ruoxiao Yang and William Shi Yuan Wang. 2018. Categorical perception of Chinese characters by simplified and traditional Chinese readers. Reading and Writing 31 (2018), 11331154. [58] Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and Kai-Wei Chang. 2022. GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 20392055. doi:10.18653/v1/2022.emnlp-main.132 [59] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2023. GLM-130B: An Open Bilingual Pre-trained Model. In The Eleventh International Conference on Learning Representations. [60] Hui Zeng. 2023. Measuring massive multitask chinese understanding. arXiv preprint arXiv:2304.12986 (2023). [61] Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and Grzegorz Kondrak. 2023. Dont Trust ChatGPT when your Question is not in English: Study of Multilingual Abilities and Types of LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 79157927. doi:10.18653/v1/2023.emnlp-main. [62] Yixuan Zhang and Haonan Li. 2023. Can Large Language Model Comprehend Ancient Chinese? Preliminary Test on ACLUE. In Proceedings of the Ancient Language Processing Workshop, Adam Anderson, Shai Gordin, Bin Li, Yudong Liu, and Marco C. Passarotti (Eds.). INCOMA Ltd., Shoumen, Bulgaria, Varna, Bulgaria, 8087. https://aclanthology.org/2023.alp-1.9/ [63] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 1520. doi:10.18653/v1/N18-2003 [64] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [65] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364 (2023). Additional Details of Methods A.1 Review of Previous Benchmark Datasets Table 3 compares our dataset, SC-TC-Bench, and prior research in terms of dataset language, language origin of LLMs, and motivation. We classify motivation in three ways: Evaluating knowledge involves examining models ability to utilize stored or inferred knowledge to answer questions or make predictionsmore than merely understanding or generating correct language, it requires linking language to factual content accurately. Evaluating language ability means testing models ability to effectively understand and generate language, performing standard linguistic tasks such as natural language understanding, text classification, and text summarization. Evaluating linguistic bias focuses on assessing whether model is neutral or fair in its applications. A.2 Model Variants, Hyperparameters, and"
        },
        {
            "title": "Implementation Details",
            "content": "Table 4 shows the exact model variants used for the evaluation. We set temprature to 0 for the three OpenAI models. For all other opensource model, we use the default hyperparameters. The open-source models are implemented using transformers from Hugging Face. Each experiment is run on eight NVIDIA GeForce RTX 2080 Ti GPUs with 11 GB of memory or eight NVIDIA GeForce RTX 1080 Ti GPUs with 11 GB of memory at time. Although DeepSeek-R1-671B is an open-source model, we use the Shubiaobiao API10 due to its large size. All experiments were conducted from October 2024 to May 2025. A.3 Details of SC-TC-Bench Our benchmark dataset, SC-TC-Bench, is available at https://gi thub.com/brucelyu17/SC-TC-Bench. Table 5 provides detailed breakdown of the question-answer pairs used for the regional term choice task. Each pair is represented as single row, resulting in total of 9,900 (1, 650 3 2) question-answer pairs. Refer to Appendix A.5 for details on how the value 1,650 was determined. Table 6 provides detailed breakdown of the question-answer pairs used for the regional name choice task. There are total of 132,834 name-based prompts used. A.4 Manual Verification for Simplified-to-Traditional Chinese Conversion To convert prompts from Simplified Chinese to Traditional Chinese, we use the chinese-converter Python package.11 Note that we only apply the conversion to prompts excluding the regional terms and names. The converted texts are subsequently reviewed by one native speaker from Mainland China and three native speakers from Taiwan. Specifically, the three native speakers from Taiwan are presented with the Taiwanese translations and explained by the native speaker from Mainland China that the prompts are converted from Simplified Chinese on one-to-one basis. They are then instructed to identify any content or sentence structures that are not commonly used in Taiwan. Each reviewer read the translations and made their decisions independently. After review, all translations are confirmed to be frequently used in Taiwan. A.5 Power Analysis We examine whether minimum difference of 5% exists between two proportionsspecifically, the outcomes when prompting LLMs 10https://api.shubiaobiao.cn/ 11https://pypi.org/project/chinese-converter/ Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Benchmark Dataset Language English Simplified Chinese Traditional Chinese English Language Origin of the LLMs Simplified Chinese Traditional Chinese Motivation ACLUE [62] SuperCLUE [54] AlignBench [32] AGIEval [65] C-Eval [21] CMMLU [23] M3KE [30] MMCU [60] DRCD [46] FGC [44] TMMLU [19] TTQA [14] StereoSet [36] Winogender [63] SC-TC-Bench (Ours) Knowledge & Language Ability Knowledge & Language Ability Knowledge & Language Ability Knowledge Knowledge Knowledge Knowledge Knowledge Language Ability Knowledge & Language Ability Knowledge Knowledge Linguistic Bias Linguistic Bias Linguistic Bias Table 3: SC-TC-Bench is the first benchmark to contain text data in English, Simplified Chinese, and Traditional Chinese, and is the first benchmark study auditing LLMs oriented towards each of these three languages. Existing benchmarks primarily focus on evaluating the knowledge and language abilities of LLMs while SC-TC-Bench aims to assess the linguistic biases in LLMs when prompted in different languages. Model Variant Language Origin Prompting Language # Prompts # MC Names Per Prompt # Names Per Prompt Section DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B Baichuan-2 ChatGLM-2 Qwen-1.5 Breeze Taiwan-LLM deepseek-r1 English gpt-4o-2024-05-13 English gpt-4 English gpt-3.5-turbo English Llama-3-70B-Instruct English Llama-3-8B-Instruct English Baichuan2-7B-Chat Simplified Chinese chatglm2-6b Simplified Chinese Qwen1.5-7B-Chat Simplified Chinese Breeze-7B-Instruct-v1_0 Traditional Chinese Taiwan-LLM-7B-v2.1-chat Traditional Chinese Table 4: The exact model variants used for the evaluation. Prompting Language # Question-Answer Pairs Prompt Version Simplified Chinese Simplified Chinese Simplified Chinese Traditional Chinese Traditional Chinese Traditional Chinese 1,650 1,650 1,650 1,650 1,650 1,650 1 2 3 1 2 3 Table 5: breakdown of the question-answer pairs for the regional term choice task. in Simplified Chinese versus Traditional Chinese. We aim for 80% 18,000 18,000 18,000 18,000 18,000 18,000 180 180 180 98 98 98 8,000 8,000 8, Simplified Chinese Traditional Chinese English Simplified Chinese Traditional Chinese English Simplified Chinese Traditional Chinese English Simplified Chinese Traditional Chinese English Simplified Chinese Traditional Chinese English 10 10 10 10 10 10 10 10 10 - - - 3 3 3 Table 6: breakdown of the number of prompts for the regional name choice task. MC refers to Mainland Chinese; refers to Taiwanese. -: This is dependent on the specific name pair. 4.1 4.1 4.1 4.3.1 4.3.1 4.3.1 4.4 4.4 4.4 4.5 4.5 4.5 4.6 4.6 4.6 10 10 10 10 10 10 10 10 10 - - - 3 3 3 power and 5% significance level. Consequently, the required sample size for each group is 1,568. Given that we have 110 regional items, this translates to approximately 1,568 110 = 14.3 repeated trials per term. Furthermore, after repeatedly prompting LLMs with the same query, we observe that most LLMs responses remain consistent. Therefore, based on the power analysis, we decided to set the number of repeated trials at 15 for the regional term choice task. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Table 7: The first rephrased version of the original prompts for regional term choice. English translation: What does {definition} refer to? Please answer with one word. Table 8: The second rephrased version of the original prompts for regional term choice. English translation: Please answer with one word, what is defined as {definition}? Table 9: The first rephrased version of the original prompts for the regional name selection task. A.6 Prompt Variants To evaluate the consistency of responses across different phrasings while preserving the intended meaning, we use GPT-4o-mini to rephrase each prompt. The rephrasing is guided by the instruction: Please rephrase the following prompt while maintaining its meaning: {original prompt}. Tables 7 and 8 show the prompt variants for regional term choice. Tables 9 and 10 show the prompt variants for regional name choice. Note that we include based on qualifications implied by their names in our prompts to ensure that all LLMs provide responses in an attempt to avoid instances where they might refuse to respond. For the regional name task, we began by conducting small-scale experiments and manually verifying the responses from prompt variants against those generated using the original prompt. The results were nearly identical. Due to computational constraints (18,000 trials for single language per experiment per LLM), we opted to conduct the experiments exclusively with the original prompt for this task. As result, the Table 6 tabulation does not include multiple prompt versions. A.7 Manual Verification for Regional Terms The manual reviews for the regional terms are performed by one native speaker from Mainland China and three native speakers from Taiwan. The native speaker from Mainland China reviews the terms used in Mainland China using their own background knowledge and search on Weibo (the Mainland Chinese version of Twitter). The regional terms used in Taiwan are independently reviewed by three native speakers from Taiwan. Terms that are Table 10: The second rephrased version of the original prompts for the regional name selection task. Corpus Mainland Chinese Terms Taiwanese Terms baidu-baike map-cc mcc4 tw-wiki cctw ootc twc4 twchat c4 232.64 557.51 44.50 45.01 14.39 40.33 49.35 100.18 4.64 6.38 2.00 0.00 24.20 33.06 2.67 2.89 47.13 88.30 13.55 27.20 1.67 0.82 3.47 4.15 95.17 170.08 9.55 12.70 74.00 0.00 231.53 447.99 33.00 42.79 34.00 152. Table 11: Average number of records containing regional terms whose Mainland Chinese and Taiwanese variants occur at least once in both Simplified and Traditional Chinese corpora. Values are reported as mean standard deviation. considered not frequently used by all three reviewers are excluded, resulting in the removal of four terms out of 114. However, language evolves over time, and since the Cross-Strait vocabularies [26] were published in 2014, some terms may have become widely used in both regions. To explore this possibility, we provide additional observations based on the corpora described in Appendix Section A.9. First, as shown in Table 13, across the 110 terms analyzed, Mainland Chinese variants appeared significantly more frequently than their Taiwanese counterparts in all Simplified Chinese corpora. Conversely, Taiwanese variants were more prevalent in all Traditional Chinese corpora. Second, as presented in Table 11, we examined terms for which both Mainland Chinese and Taiwanese forms appeared at least once in each corpus. In these cases, Simplified forms dominated in Simplified Chinese corpora, while Traditional forms were more frequent in Traditional Chinese corpora. These results indicate persistent pattern: the majority of terms in our dataset are not commonly shared between the two regions. A.8 Sourcing Regional Term Definitions The regional term (also referred to as item) definitions are first sourced from Li et al. [26]. In cases where an item lacks an existing definition, search is conducted in comprehensive Simplified Chinese dictionary [33]. Should this search yield no results, the definition is then sought via the wikipediaapi package on Wikipedia. If the item remains undefined in Wikipedia, it is then defined by prompting GPT-4 with the instruction: Please explain {item} using single sentence. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Corpus # Records Language A.10 Choosing the Optimal Number of baidu-baike [55] map-cc [27] mcc4 [10] tw-wiki [11] cctw [25] ootc [22] twc4 [24] twchat [28] c4 [4] Simplified Chinese 1,731,888 1,773,205,733 Simplified Chinese 2,009,844 Simplified Chinese 2,533,212 Traditional Chinese 2,712,675 Traditional Chinese 4,233,915 Traditional Chinese 4,856,777 Traditional Chinese 485,432 Traditional Chinese 10,353,901,556 Simplified & Traditional Chinese Table 12: Overview of the language corpora used as proxies in Section 3.2.2. Record counts marked with an asterisk (*) indicate values estimated by Huggingface. Corpus Mainland Chinese Terms Taiwanese Terms baidu-baike map-cc mcc4 tw-wiki cctw ootc twc4 twchat c4 71.05 232.79 21.35 73.88 5.92 23.77 17.72 86.71 0.63 2.48 1.62 11.38 7.17 40.00 2.18 20.22 29.15 66.11 1.38 9.19 0.09 0.42 1.18 2.85 27.05 92.38 3.20 7.22 3.20 19.18 314.23 2165.79 2.48 10.67 16.75 107.80 Table 13: Average number of records that contain the regional terms described in the analysis. Values are reported as mean standard deviation. The output from GPT-4 is manually verified for accuracy. Verification is through comparing the definition with Chinese native annotators background knowledge. When the annotator is not sure, the annotator will search online. The conversion from Simplified Chinese to Traditional Chinese is reviewed by three native Taiwanese speakers in the same manner discussed in Appendix A.4. To further evaluate whether the results are biased because GPT-4 is used to produce some of the definitions, we replicate the regional term recognition experiments excluding any terms whose definitions are generated by GPT-4. The results, presented in Figure 5, align with the patterns observed in Figure 2, indicating that the use of GPT-4o to generate item definitions does not impact the findings. The Pearson correlation coefficients between the percentage of correct, misaligned, and incorrect responses in Figure 2 and Figure 5 are 0.999 (𝑝 < .001), 0.998 (𝑝 < .001), and 0.997 (𝑝 < .001), respectively. A.9 Additional Details of Online Language"
        },
        {
            "title": "Corpora",
            "content": "The corpora used as proxies in Section 3.2.2 are all collected from Huggingface. Table 12 presents the sizes of these corpora. Table 13 presents the descriptive statistics of the Mainland Chinese and Taiwanese terms in the Simplified and Traditional Chinese corpora."
        },
        {
            "title": "Permutations",
            "content": "For each list of 20 candidate names, we permute the order between 20 and 380 times. We then replicate the regional name experiment and compute the percentage of Mainland Chinese names, with the results displayed in Figure 6. After 180 permutations, the results remain stable, leading us to select 180 as the optimal number of permutations for the regional name task. We did not conduct this experiment with DeepSeek-R1-671B due to the relatively expensive API calls. A.11 Name Extraction For each trial, the LLMs response is captured and the name it selects is extracted using the GPT-4o-mini model. If the LLM does not select name, the output is recorded as NA. The effectiveness of GPT-4o-mini in accurately extracting the selected names from the LLM responses is subsequently validated through manual verification. For each LLM and each prompting language, we sample 10 responses (300 in total) and collect the corresponding names extracted by GPT-4o-mini. graduate student then manually compares the LLM responses with the extracted names. If the extracted name matches the name selected by the LLM in the response, it is considered correct. Among these 300 samples, the accuracy rate is 99%."
        },
        {
            "title": "Choice",
            "content": "B.1 Experiment Results of Rephrased Prompts We replicate the experiment described in Section 3.1. The results of the other two rephrased versions are shown in Figures 7 and 8, respectively. The observed pattern remains consistent. The Pearson correlation coefficients between the percentage of correct, misaligned, and incorrect responses in Figure 2 and Figure 7 are 0.980 (𝑝 < .001), 0.992 (𝑝 < .001), and 0.970 (𝑝 < .001), respectively. The Pearson correlation coefficients between the percentage of correct, misaligned, and incorrect responses in Figure 2 and Figure 8 are 0.987 (𝑝 < .001), 0.985 (𝑝 < .001), and 0.982 (𝑝 < .001), respectively. B.2 Full List of Terms and Their Misalignment"
        },
        {
            "title": "Rates",
            "content": "The misalignment rates regarding to Mainland Chinese terms are shown in Tables 14 and 15. The misalignment rates regarding to Taiwanese terms are shown in Tables 16 and 17. B.3 Experiment Results on the Prevalence of"
        },
        {
            "title": "Misaligned Terms",
            "content": "Table 18 presents the average frequencies of misaligned and nonmisaligned terms across nine language corpora. We find that misaligned terms consistently exhibit higher Simplified-to-Traditional ratio across corpora, highlighting data imbalance as key factor contributing to the observed bias. While we acknowledge the possibility of alternative explanations, the limited technical details available for many LLMsdespite some high-level descriptions in technical reportsmake it challenging to directly assess the impact FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke n r y s s % 80 60 40 20 0 T Qwen-1.5 Simplified Chinese oriented LLMs Traditional Chinese oriented LLMs Correct response Misaligned response Incorrect response English oriented LLMs T BaichuanChatGLM-2 Breeze Taiwan-LLM T DeepSeekR1-671B GPT-4o GPT-4 T GPT-3.5 T Llama-3-70B Llama-3-8B Figure 5: We replicate the experiment outlined in Section 3.1, with the only modification being the removal of items whose definitions are sourced from GPT-4. The observed pattern remains consistent. Misaligned responses are the ones where the LLM swaps the regional terms. and denote the Simplified and Traditional Chinese prompting languages, respectively. of differences in pretraining and alignment methods. We strongly encourage greater transparency in the release of training and alignment details and leave deeper investigation of these factors to future work. B.4 Rate of Incorrect Responses Table 19 presents the breakdown of incorrect response types from the first of 15 trials when prompted in Simplified Chinese or Traditional Chinese. Since the prompts remain the same across all trials, we verify the first trial as representative sample. To annotate the type of incorrect response, we apply the following prompt to GPT-4o-mini: Do the terms {response} and {ground_truth} refer to completely different things, or are they the same concept, with {response} simply being less commonly used? Only respond 1 if they refer to completely different things, 2 if the terms refer to the same concept but {response} is less commonly used. Do not include explanations. Note that you may need to extract the term from {response} as it may contain irrelevant words. Additionally, we manually annotate subset of 407 samples to validate the accuracy of the automatic annotations. GPT-4o-mini achieves an accuracy of 0.7150."
        },
        {
            "title": "Choice",
            "content": "C.1 Rate of Invalid Responses Invalid response rates and explanations for non-responses vary across LLMs. For example, when prompted in Simplified Chinese, ChatGLM-2 exhibited an invalid rate of 69.7%, often outputting multiple names instead of single response. In contrast, Breeze showed higher invalid rate of 81.0%, typically citing insufficient information as the reason for its inability to select name. Meanwhile, GPT-4o demonstrated much lower invalid rate of just 2.0%, with all invalid responses involving out-of-list namesthat is, GPT-4o generated name not included among the provided options. Table 20 presents the breakdown of invalid response types based on 100 sampled outputs from each of ChatGLM-2, Breeze, and GPT-4o. All invalid responses were excluded from the selection rate comparisons. Although some LLMs exhibited relatively high invalid rates, we believe our findings remain robust. First, despite Breezes 81.0% invalid rate, the large scale of our experiment still yielded 3,420 valid responses. Second, results were largely consistent across various prompting conditions, further reinforcing the reliability of our conclusions. C.2 Full Results of Top 5 Selected Names Due to space constraints, we show the full results of the top 5 most frequently selected names in Table 21. C.3 Statistics of Collected Names Mainland Chinese names are sourced from the name report published by the Ministry of Public Security of the Peoples Republic of China in 2013 [39], while Taiwanese names are obtained from the name report published in Taiwan in 2018 [1]. It is important to note that neither report offers comprehensive list of all names; instead, each includes approximately the 200 most popular names. Since all Taiwanese names in the corpus consist of 3 characters, we similarly restricted our selection to 3-character Mainland Chinese names. The name report for Taiwanese names [1] provides gender information, which allowed us to ensure an equal number of male and female Taiwanese names in our dataset. In contrast, the name report from the Ministry of Public Security [39] does not include gender information. In total, the dataset includes 152 Mainland Chinese names, comprising 11 distinct surnames and 44 distinct given names, and 200 Taiwanese names, consisting of 12 distinct surnames and 130 distinct given names. Figure 9 illustrates the density plots showing the distribution of the number of individuals associated with the collected names. On average, each collected Mainland Chinese name corresponds to 80,044 individuals (𝑆𝐷 = 32, 630), while each collected Taiwanese name corresponds to an average of 1,658 individuals (𝑆𝐷 = 702). The names used for the experiment described in Section 4.3.1 are sampled from all the collected Mainland Chinese and Taiwanese names. There are 135 unique Mainland Chinese names including 6 Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B N n C l M % 80 60 40 0 N n d n % 60 40 20 a e C l M % 80 60 40 20 0 20 40 60 80 120 140 180 160 240 200 Number of Permutations 220 280 300 320 340 360 400 Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B (a) Prompted in Simplified Chinese. 0 20 40 60 80 120 140 180 160 240 200 Number of Permutations 220 280 300 320 340 360 400 Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B (b) Prompted in Traditional Chinese. 0 20 40 60 80 120 140 180 160 240 200 Number of Permutations 220 280 300 320 340 360 400 (c) Prompted in English. Figure 6: The number of permutations used in the regional name task experiments (180, at the dotted vertical line) yield results for our primary metric of interest (% Mainland Chinese Names that are selected) that are comparable to the asymptotic rates from running the experiment for more permutations. unique last names and 40 unique first names. There are 87 unique Taiwanese names including 9 unique last names and 56 unique first names. Table 22 shows the correlation coefficients between LLM selection frequency and online name popularity (i.e., , name frequency). C.4 Additional Results of Name Popularity"
        },
        {
            "title": "Experiments",
            "content": "Figure 10 presents the selection rates for Mainland Chinese names by LLMs when controlling for population-based name popularity. C.5 Impact of Popular Names There may be potential confounders, such as names of prominent business figures, politicians, or celebrities, that could skew the results of the regional name choice task. To examine this possibility, we conducted the online-based name popularity experiment described in Section 4.3.2. As an illustrative example, consider two FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke n r y s s % 100 80 60 40 20 Qwen-1.5 Simplified Chinese oriented LLMs Traditional Chinese oriented LLMs Correct response Misaligned response Incorrect response English oriented LLMs T Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeekR1-671B GPT-4o GPTS GPT-3.5 S Llama-3-70B Llama-3-8B Figure 7: We replicate the experiment outlined in Section 3.1, using the first rephrased version of the original prompt. The observed pattern remains consistent. Misaligned responses are the ones where the LLM swaps the regional terms. and denote the Simplified and Traditional Chinese prompting languages, respectively. n r y s s % 100 80 60 40 20 Qwen-1.5 Simplified Chinese oriented LLMs Traditional Chinese oriented LLMs Correct response Misaligned response Incorrect response English oriented LLMs T Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeekR1-671B GPT-4o GPTS GPT-3.5 S Llama-3-70B Llama-3-8B Figure 8: We replicate the experiment outlined in Section 3.1, using the second rephrased version of the original prompt. The observed pattern remains consistent. Misaligned responses are the ones where the LLM swaps the regional terms. and denote the Simplified and Traditional Chinese prompting languages, respectively. well-known celebrity names in our corpus . Neither name was selected significantly more frequently than average; for instance, as shown in Table 23 across all three prompting languages, Baichuan-2s selection rate for each of these names was below 1%, well below the expected 5% average selection rate under an assumption of equal likelihood among the 20 names presented. and C.6 Impact of Gender To annotate the gender of Mainland Chinese names, we use the following prompt: Is the name {name} more commonly used for males or females in Mainland China? Respond with only one word: male or female. native student from Mainland China then verified the annotations. The student confirmed that all labels generated by GPT-4o-mini were accurate. According to the Mainland China report [39], there are 18 male-associated and 134 female-associated names. In contrast, the report published in Taiwan [1] provides an equal distribution of 100 male and 100 female names. Tables 27, 28, and 29 present the selection proportions of maleassociated names in both Mainland China and Taiwan, under various gender distributions used in the candidate lists for the experiments described in Section 4.3, when the models are prompted in Simplified Chinese, Traditional Chinese, and English, respectively. Table 30 presents the selection proportions of male-associated names in experiments where gender distribution and name popularity are balanced. One limitation of the experiments in Section 4.6 is the lack of male names with shared first names that differ only by script. As result, all names used in that set of experiments are female-associated, which may limit the generalizability of the findings with respect to scripts. C.7 Descriptive Word Extraction We prompt GPT-4o-mini to extract the descriptive words from LLM responses with the prompt: Please determine if there are any adjectives describing the name {name} in the provided text: {text}. Do not include the adjectives in the name itself. If adjectives are Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece with shared characters such as appears in 4.75% of the 400 Taiwanese first name characters in our corpus but is entirely absent from the Mainland Chinese first names. Similarly, appears in 2.00% of the Taiwanese first names but does not occur . The character and in any Mainland Chinese first names in the corpus. C.8 Specific Character Experiments We begin by identifying all last names that appear with at least two distinct given names in the same decile group of population-based popularity in our dataset. Table 34 enumerates these combinations. While most last names are associated with exactly two given names, one has up to four. For consistency and clarity in pairwise analysis, we generate all possible name pairs sharing the same last name but differing in given names, resulting in (cid:0)𝑁 (cid:1) pairs per last name with 2 𝑁 variants. We first measure raw character preference by prompting the LLM with: {firstname: 𝐹 𝑁𝑖 , lastname: for each name pair (FN𝑖 and FN𝑗 ) sharing the same last name LN. We collect the token generation probability of LN in this unconstrained setting, which we define as the raw token generation probability. Next, we evaluate conditioned token generation probabilities by modifying the original name selection prompts (e.g., those in Table 1b) to append the fragment: {firstname: 𝐹 𝑁𝑖 , lastname: . We then compute the token generation probability of LN in this context. Importantly, each prompt only includes candidate names that share the same last name but differ in given names, thereby controlling for last name identity while isolating variation in the first name. We repeat this experiment across three prompting languages: Simplified Chinese, Traditional Chinese, and English. For each name pair, we compare the token generation probabilities (raw and conditioned) of the shared last name. Table 35 shows the agreement rate between raw and conditioned token generation probabilities across name pairs. Table 36 presents the average loglikelihood of generating tokens corresponding to the last names of Mainland Chinese and Taiwanese names. C.9 Tokenization of Different Scripts Table 37 demonstrates that, for most LLMs, the average number of tokens used to represent the same name varies substantially between its Simplified and Traditional Chinese forms. (a) Mainland Chinese names. (b) Taiwanese names. Figure 9: Density plots of the number of individuals bearing the collected names. found, extract them and list them only. If no adjectives are present, respond with NA. Next, we manually verify the correctness and completeness of the extracted words on subset of 20 samples for each model (Baichuan-2 and Qwen-1.5) per prompting language (English, Simplified Chinese, and Traditional Chinese). Out of the 120 responses (for the two LLMs and three prompting languages), all of the adjectives used to describe the candidate were extracted by GPT-4o-mini. For 13 out of 120 responses, GPT-4o-mini generates one extra adjective that was not originally in the responses. After verification, we find that these extra adjectives are descriptive characters generated by GPT-4o-minis own reasoning capabilities. Tables 31 and 32 present the top 10 descriptive characters used by Baichuan-2 and Qwen-1.5 for both Mainland Chinese and Taiwanese names. Adjectives such as talented and wisdom are more frequently associated with Taiwanese names. Table 33 reports the top three Taiwanese names described using these adjectives, FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Figure 10: The selection rates for Mainland Chinese names by LLMs are overall lower compared to those of Taiwanese names when controlling for population-based name popularity. Results are similar to those without conditioning on name popularity, as in Figure 3. Figure 11: The selection bias favoring Taiwanese names remains (with only 4 of the 11 LLMs yielding majority selection of Mainland Chinese names when prompted in Simplified Chinese), but is less severe when controlling for gender. Arrows indicate the relative movement of data points compared to their positions in Figure 10, wherein significantly more male names were among the Taiwanese candidate name lists relative to the Mainland Chinese candidate name lists. Red solid arrows represent an increase in the selection rate of Mainland Chinese names, while blue dashed arrows indicate decrease. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Table 14: Part 1 of the full list of Mainland Chinese terms, along with their correct, misaligned, incorrect counts. Terms that are misaligned in at least 3 out of 15 trials for given LLM are highlighted in yellow. In addition, the terms for which more than half of selected LLMs tend to misalign are also highlighted in yellow. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Table 15: Part 2 of the full list of Mainland Chinese terms, along with their correct, misaligned, incorrect counts. Terms that are misaligned in at least 3 out of 15 trials for given LLM are highlighted in yellow. In addition, the terms for which more than half of selected LLMs tend to misalign are also highlighted in yellow. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Table 16: Part 1 of the full list of Taiwanese terms, along with their correct, misaligned, incorrect counts. Terms that are misaligned in at least 3 out of 15 trials for given LLM are highlighted in yellow. In addition, the terms for which more than half of selected LLMs tend to misalign are also highlighted in yellow. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Table 17: Part 2 of the full list of Taiwanese terms, along with their correct, misaligned, incorrect counts. Terms that are misaligned in at least 3 out of 15 trials for given LLM are highlighted in yellow. In addition, the terms for which more than half of selected LLMs tend to misalign are also highlighted in yellow. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Corpus baidu-baike map-cc mcc4 tw-wiki cctw ootc twc4 twchat c4 Misaligned Items Written in Simplified Chinese Misaligned Items Written in Traditional Chinese Misaligned Ratio (Simplified/Traditional) Non-misaligned Items Written in Simplified Chinese Non-misaligned Items Written in Traditional Chinese Non-misaligned Ratio (Simplified/Traditional) 158.17 52.03 13. 58.14 1.31 5.83 22.86 7.72 57.69 4.41 0.24 1.97 43.38 4.76 3.21 180.93 1.41 11.41 35.84 215.57 6. 1.34 0.28 1.82 0.13 5.46 5.05 39.85 10.36 3.20 3.25 0.38 0.11 1.56 0.20 18.94 0.30 0.04 0. 21.21 2.64 3.20 361.95 2.86 18.65 134.50 279.67 3.55 0.15 0.14 0.03 0.00 0.07 1.02 Table 18: Average frequency of misaligned and non-misaligned terms across nine language corporathree in Simplified Chinese (baidu-baike, map-cc, mcc4), five in Traditional Chinese (tw-wiki, cctw, ootc, twc4, twchat), and one containing mixture of both (c4). Across Simplified Chinese corpora, there is significantly higher ratio of terms written in Simplified Chinese compared to Traditional Chinese, regardless of whether the items are misaligned or notas reflected by the large values in Columns 4 and 7. In contrast, in Traditional Chinese corpora, we observe that non-misaligned terms are predominantly written in Traditional Chinese. However, the ratio of Simplified to Traditional terms is notably higher for misaligned items than for non-misaligned ones (i.e., Column 4 > Column 7 across those five rows), pattern that also holds in the mixed-language corpus, c4. Model Simplified Chinese Traditional Chinese Entirely wrong Uncommon usage Entirely wrong Uncommon usage Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 51.02% 55.22% 45.24% 58.97% 56.10% 32.26% 39.39% 48.65% 57.69% 75.31% 74.07% 48.98% 44.78% 54.76% 41.03% 43.90% 67.74% 60.61% 51.35% 42.31% 24.69% 25.93% 40.74% 47.27% 51.76% 50.70% 55.41% 32.26% 45.95% 41.86% 52.83% 78.05% 42.31% 59.26% 52.73% 48.24% 49.30% 44.59% 67.74% 54.05% 58.14% 47.17% 21.95% 57.69% Table 19: Breakdown of incorrect response types observed in the first trial (out of 15 total) when prompted in Simplified Chinese or Traditional Chinese. Type ChatGLMBreeze GPT-4o Insufficient information Multiple names Out-of-list name 70.0% 19.0% 11.0% 100.0% 0.0% 0.0% 0.0% 0.0% 100.0% Table 20: Breakdown of invalid response types across 100 sampled outputs for ChatGLM-2, Breeze, and GPT-4o when prompted in Simplified Chinese. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke (a) Qwen-1.5 (b) DeepSeek-R1-671B (c) GPT- (d) GPT-3.5 (e) Llama-3-70B (f) Llama-3-8B Table 21: The top 5 most frequently selected names when prompted in English, Simplified, or Traditional Chinese. Mainland Chinese and Taiwanese names are highlighted in red and blue, respectively. Model Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM Llama-3-8B Llama-3-70B GPT-4o GPT-4 GPT-3. 𝜌Simplified -0.06 -0.19 0.25 -0.06 0.12 0.10 -0.19 -0.07 -0.02 0.11 Significance NS ** *** NS NS NS ** NS NS NS 𝜌Traditional -0.19 -0.13 0.18 0.11 0.01 0.03 -0.10 -0.10 -0.07 0.02 Significance ** NS ** NS NS NS NS NS NS NS 𝜌English -0.19 0.02 0.20 0.17 0.23 0.06 -0.12 -0.17 0.11 0.03 Significance ** NS ** * *** NS NS NS NS NS Table 22: 𝜌Simplified, 𝜌Traditional, and 𝜌English denote the correlation coefficients between LLM selection frequency and online name popularity (i.e., , name frequency). For most LLMs, there is no significant relationship between name selection and online popularity. However, ChatGLM-2 consistently exhibits positive correlation. NS: Not significant, *: 𝑝 < .05, **: 𝑝 < .01, ***: 𝑝 < .001. Name Simplified Chinese Traditional Chinese English Table 23: Selection rates of Baichuan-2 for English. 0.46% 0.11% and 0.65% 0.11% 0.93% 0.11% under prompts in Simplified Chinese, Traditional Chinese, and Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Model 1 Male / 9 Female 2 Male / 8 Female 3 Male / 7 Female MC % Significance # Valid Responses MC % Significance # Valid Responses MC % Significance # Valid Responses Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 14.94 12.00 78.57 5.48 90.42 4.18 24.21 43.06 73.85 28.97 47.37 *** *** NS *** NS *** *** ** NS *** NS 462 400 238 347 167 838 888 432 845 794 380 17.08 24.16 80.14 23.53 87.29 12.71 22.35 48.71 86.27 18.00 49.43 *** *** NS *** NS *** *** NS NS *** NS 650 567 292 221 181 1196 1217 661 1129 1089 *** 18.18 NS 42.86 NS 92.00 50.00 NS 100.00 NS *** 5.20 *** 11.80 *** 24.07 NS 87.35 * 41.06 NS 49.09 88 91 50 2 25 173 178 108 166 151 55 Table 24: Selection proportions of Mainland Chinese names, under matched gender distributions in the candidate name lists (i.e., 1 male / 9 female, 2 male / 8 female, or 3 male/ 7 female for each set of Taiwanese and Mainland Chinese names comprising the 20 candidate name list) used in the experiments of Section 4.3, when the models are prompted in Simplified Chinese. The majority of LLMs tend to select Taiwanese names even when gender distributions are held constant between Taiwanese and Mainland Chinese name options. The number of times the gender distribution appears in the experiment is 900, 1,260, and 180, respectively. We conduct one-sided z-proportion test to examine whether the Mainland Chinese name selection rate is significantly below 50%. NS: Not significant, *: 𝑝 < .05, **: 𝑝 < .01, ***: 𝑝 < .001. - means there is no valid response. Model 1 Male / 9 Female 2 Male / 8 Female 3 Male / 7 Female MC % Significance # Valid Responses MC % Significance # Valid Responses MC % Significance # Valid Responses Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 8.42 17.20 62.92 35.29 70.59 5.90 15.38 34.30 29.83 36.38 43.95 *** *** NS *** NS *** *** *** *** *** ** 273 599 267 340 17 865 897 621 590 734 512 7.99 7.10 59.80 65.64 58.82 14.61 12.72 24.88 41.29 37.56 45.70 *** *** NS NS NS *** *** *** *** *** * 438 859 393 486 17 1205 1226 852 402 969 7.35 2.61 58.06 39.66 50.00 6.21 7.22 5.69 52.94 55.88 42.86 *** *** NS NS NS *** *** *** NS NS NS 68 153 62 58 2 177 180 123 85 136 112 Table 25: Selection proportions of Mainland Chinese names, under matched gender distributions in the candidate name lists (i.e., 1 male / 9 female, 2 male / 8 female, or 3 male/ 7 female for each set of Taiwanese and Mainland Chinese names comprising the 20 candidate name list) used in the experiments of Section 4.3, when the models are prompted in Traditional Chinese. The majority of LLMs tend to select Taiwanese names even when gender distributions are held constant between Taiwanese and Mainland Chinese name options. The number of times the gender distribution appears in the experiment is 900, 1,260, and 180, respectively. We conduct one-sided z-proportion test to examine whether the Mainland Chinese name selection rate is significantly below 50%. NS: Not significant, *: 𝑝 < .05, **: 𝑝 < .01, ***: 𝑝 < .001. - means there is no valid response. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Model 1 Male / 9 Female 2 Male / 8 Female 3 Male / 7 Female MC % Significance # Valid Responses MC % Significance # Valid Responses MC % Significance # Valid Responses Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 7.06 35.61 60.33 41.40 63.55 16.55 19.61 57.26 37.44 29.41 48.16 *** *** NS *** NS *** *** NS *** *** NS 411 702 736 831 834 846 867 889 804 221 10.83 35.88 64.83 76.33 67.50 33.72 24.24 65.68 51.81 29.38 55.47 *** *** NS NS NS *** *** NS NS *** NS 471 811 1032 959 1154 1210 1250 1247 857 320 1015 16.05 38.41 75.41 61.24 72.73 30.99 3.89 29.44 69.23 41.67 51.37 *** ** NS NS NS *** *** *** NS NS NS 81 138 122 178 165 171 180 180 156 48 Table 26: Selection proportions of Mainland Chinese names, under matched gender distributions in the candidate name lists (i.e., 1 male / 9 female, 2 male / 8 female, or 3 male/ 7 female for each set of Taiwanese and Mainland Chinese names comprising the 20 candidate name list) used in the experiments of Section 4.3, when the models are prompted in English. The majority of LLMs tend to select Taiwanese names even when gender distributions are held constant between Taiwanese and Mainland Chinese name options. The number of times the gender distribution appears in the experiment is 900, 1,260, and 180, respectively. We conduct one-sided z-proportion test to examine whether the Mainland Chinese name selection rate is significantly below 50%. NS: Not significant, *: 𝑝 < .05, **: 𝑝 < .01, ***: 𝑝 < .001. - means there is no valid response. Model 1 Male / 9 Female MC Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 7.25 39.58 23.53 52.63 15.23 57.14 60.00 99.46 88.94 10.00 15.56 TW 30.79 34.94 1.96 33.33 0.00 54.92 43.24 90.24 79.64 11.70 14. 2 Male / 8 Female MC 52.25 28.47 20.94 38.24 15.19 94.74 63.24 95.96 87.47 30.10 27.91 TW 44.53 32.56 15.52 87.50 0.00 74.71 45.61 91.74 30.97 13.33 27.27 3 Male / 7 Female MC 56.25 94.87 21.74 0.00 68.00 100.00 95.24 100.00 92.41 53.23 37.04 TW 59.72 92.31 50.00 100.00 - 90.85 75.80 98.78 90.48 34.83 46.43 Table 27: Selection proportions of male-associated names for both Mainland China and Taiwan, under varying gender distributions in the candidate name lists (i.e., 1 male / 9 female, 2 male / 8 female, and 3 male/ 7 female) used in the experiments of Section 4.3, when the models are prompted in Simplified Chinese. The number of times the gender distribution appears in the experiment is 900, 1,260, and 180, respectively. - means there is no valid response. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Model 1 Male / 9 Female MC 3 Male / 7 Female MC Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 4.35 47.57 14.29 46.73 8.33 70.59 52.90 66.20 90.91 17.60 11. TW 25.20 35.48 6.06 84.27 0.00 63.02 46.11 65.44 77.29 17.13 22.65 2 Male / 8 Female MC 40.00 32.79 21.70 41.78 30.00 93.75 53.85 39.62 74.70 42.58 29.32 TW 30.27 38.72 11.39 24.30 42.86 83.09 42.90 59.38 56.36 20.99 36.36 TW 69.84 95.97 3.85 100.00 0.00 93.37 68.26 55.17 87.50 56.67 68.75 40.00 75.00 19.44 8.70 0.00 100.00 76.92 71.43 84.44 61.84 37.50 Table 28: Selection proportions of male-associated names for both Mainland China and Taiwan, under varying gender distributions in the candidate name lists (i.e., 1 male / 9 female, 2 male / 8 female, and 3 male/ 7 female) used in the experiments of Section 4.3, when the models are prompted in Traditional Chinese. The number of times the gender distribution appears in the experiment is 900, 1,260, and 180, respectively. - means there is no valid response. Model 1 Male / 9 Female MC Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 6.90 32.00 15.99 52.33 9.25 52.14 99.41 62.87 77.74 20.00 18.41 TW 22.25 20.13 4.11 82.75 9.21 53.26 70.16 71.58 62.03 19.23 17.37 2 Male / 8 Female MC 33.33 26.80 11.36 51.78 16.30 97.79 99.67 69.47 88.06 42.55 42.98 TW 31.67 26.15 4.13 34.36 17.07 79.80 75.61 51.64 55.45 26.99 29.87 3 Male / 7 Female MC 76.92 77.36 11.96 49.54 33.33 100.00 100.00 71.70 98.15 70.00 72.00 TW 72.06 91.76 6.67 100.00 46.67 97.46 95.95 68.50 97.92 35.71 61.97 Table 29: Selection proportions of male-associated names for both Mainland China and Taiwan, under varying gender distributions in the candidate name lists (i.e., 1 male / 9 female, 2 male / 8 female, and 3 male/ 7 female) used in the experiments of Section 4.3, when the models are prompted in English. The number of times the gender distribution appears in the experiment is 900, 1,260, and 180, respectively. - means there is no valid response. Model Simplified Chinese Traditional Chinese English Male % Significance # Valid Responses Male % Significance # Valid Responses Male % Significance # Valid Responses Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 72.84 45.16 77.27 78.35 66.67 100.00 75.71 98.55 87.60 54.64 51.02 *** NS *** *** NS *** *** *** *** NS NS 81 62 44 97 3 162 177 69 121 97 98 74.29 49.18 100.00 70.73 100.00 99.34 77.27 93.33 75.18 66.34 74.29 *** NS *** ** *** *** *** *** *** *** *** 70 61 5 41 1 151 176 90 137 101 105 53.15 53.64 65.52 90.17 58.96 99.36 98.86 98.33 87.85 51.43 66.90 NS NS ** *** ** *** *** *** *** NS *** 111 110 87 173 173 157 175 180 107 35 142 Table 30: Selection proportions of male names when gender distribution and name popularity are balanced in the candidate name list. The three column groupings denote the prompting language. We conduct one-sided z-proportion test to examine whether the male name selection rate is significantly over 50%. NS: Not significant, *: 𝑝 < .05, **: 𝑝 < .01, ***: 𝑝 < .001. FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Table 31: We display the top 10 most frequent descriptive words associated with names from two regions, prompted in both Simplified and Traditional Chinese (Baichuan-2). Each cell contains the original word on the right and its English translation on the left. Descriptive words that are among the top 10 most frequent in the explanation of one region, but not in the other regions top 10, are highlighted in blue. Table 32: We display the top 10 most frequent descriptive words associated with names from two regions, prompted in both Simplified and Traditional Chinese (Qwen-1.5). Each cell contains the original word on the right and its English translation on the left. Descriptive words that are among the top 10 most frequent in the explanation of one region, but not in the other regions top 10, are highlighted in blue. Model Prompted in Simplified Chinese Prompted in Traditional Chinese Baichuan-2 Qwen-1.5 Table 33: We display the top three Taiwanese names most frequently associated with the descriptors talented and wisdom by Baichuan-2 and Qwen-1.5 when prompted in Simplified and Traditional Chinese. Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese FAccT 25, June 2326, 2025, Athens, Greece Table 34: Combinations of last names with multiple associated given names used in the character preference analysis (Section 4.5.1). FAccT 25, June 2326, 2025, Athens, Greece Hanjia Lyu, Jiebo Luo, Jian Kang, and Allison Koenecke Model Agreement Rate Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM Llama-3-70B Llama-3-8B 65.31% 68.71% 94.56% 57.82% 84.01% 97.62% 71.43% Table 35: Agreement between raw and conditioned token generation preferences across name pairs. For each name pair sharing the same last name, we compare whether the raw and conditioned token generation probabilities for the last name are aligned in direction (i.e., both higher or both lower for the same name). Percentages significantly above 50% across all LLMs indicate that LLMs character preferences partially account for name selection biases. Model Mainland Chinese Last Names Taiwanese Last Names Significance Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM Llama-3-70B Llama-3-8B -16.72 3.85 -8.29 1.51 -53.57 2.27 -28.89 2.59 -34.50 4.46 -31.06 1.29 -28.44 0.49 -11.70 5.18 -5.09 2.61 -57.19 2.84 -27.52 1.00 -31.71 1.72 -31.27 0.96 -27.92 0. *** *** NS *** *** NS *** Table 36: Average log-likelihood of generating tokens corresponding to the last names of Mainland Chinese and Taiwanese names. We use log-likelihood instead of probability for better readability. We conduct Welchs t-test to evaluate whether the mean token generation probability for Taiwanese last names is significantly larger than that of Mainland Chinese last names. Combined with Table 35, the results indicate that character-level preferences in LLMs partially explain the observed regional name selection biases. Results are formatted as mean standard deviation. NS: Not significant, *: 𝑝 < .05, **: 𝑝 < .01, ***: 𝑝 < .001. Model Mainland Chinese Names Mainland Chinese Names (converted into Traditional Chinese) Significance Taiwanese Names Taiwanese Names (converted into Simplified Chinese) Significance Qwen-1.5 Baichuan-2 ChatGLM-2 Breeze Taiwan-LLM DeepSeek-R1-671B GPT-4o GPT-4 GPT-3.5 Llama-3-70B Llama-3-8B 2.88 0.33 2.53 0.50 4.88 0.33 5.14 0.62 7.08 1.62 3.83 0.38 3.06 0.33 5.47 0.95 5.47 0.95 4.12 0.33 4.12 0.33 2.99 0.11 2.79 0.41 5.13 0.42 4.93 0.26 6.91 1.51 3.92 0.27 3.54 0.69 6.15 0.89 6.15 0.89 4.22 0.41 4.22 0.41 *** *** *** *** NS * *** *** *** * * 3.10 0.37 2.92 0.28 5.04 0.32 4.75 0.43 7.74 1.51 4.00 0.24 4.04 0.75 5.89 0.86 5.89 0.86 4.55 0.64 4.55 0.64 2.96 0.20 2.76 0.44 4.96 0.23 5.25 0.79 7.72 1.55 3.96 0.25 3.35 0.56 5.61 0.87 5.61 0.87 4.45 0.60 4.45 0.60 *** *** ** *** NS NS *** ** ** NS NS Table 37: The average token counts for the same name differ significantly depending on whether the name is written in Simplified or Traditional Chinese for most LLMs. Names written in Traditional Chinese are consistently converted into higher number of tokens. Results are formatted as mean standard deviation. NS: Not significant, *: 𝑝 < .05, **: 𝑝 < .01, ***: 𝑝 < .001."
        }
    ],
    "affiliations": [
        "Cornell University",
        "University of Rochester"
    ]
}