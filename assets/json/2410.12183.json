{
    "paper_title": "TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration",
    "authors": [
        "Yiwei Guo",
        "Shaobin Zhuang",
        "Kunchang Li",
        "Yu Qiao",
        "Yali Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well. Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are \"isolated agents\" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves state-of-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT which contains large domain shifts."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 2 3 8 1 2 1 . 0 1 4 2 : r TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration Yiwei Guo1,2 Shaobin Zhuang3,4 Kunchang Li1,2,3 Yu Qiao3 Yali Wang1,3 1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3Shanghai AI Laboratory 4Shanghai Jiao Tong University"
        },
        {
            "title": "Abstract",
            "content": "Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such single model to generalize well. Alternatively, there exists wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are \"isolated agents\" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose general and concise TransAgent framework, which transports the knowledge of the isolated agents in unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves stateof-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT which contains large domain shifts. The code will be released at https://github.com/markywg/transagent."
        },
        {
            "title": "Introduction",
            "content": "Recently, Vision-Language (V-L) foundation models are mainly pre-trained by contrastive learning with massive image-text pairs from web [61, 38, 15]. As result, they show the potential on number of downstream visual recognition tasks, by transferring their representations with prompt learning [87, 86] and/or model adaptation [31, 80]. However, target domain data in the open world are diversified, e.g., EuroSAT [35] refers to satellite images that are highly different from web images in the pre-training. With this large domain shift, it is challenging to achieve good generalization only by adopting such single model (e.g., CLIP), especially under low-shot regime. Alternatively, with the fast development in vision and NLP, there arises wide range of expert models [33, 9, 43, 49, 7, 18, 64, 11, 46, 12] which contain rich knowledge by pre-training on different modalities, tasks, networks, and datasets. Hence, the natural question is, is it possible to integrate such knowledge to boost vision-language foundation models? To answer this question, we should further analyze the form of knowledge in these models. The simplest form is the model output, which explicitly exhibits what kind of tasks these models can tackle. However, these models have heterogeneous network structures and outputs, making direct knowledge combinations infeasible. Hence, existing works often choose cascades of these models Interns at Shanghai AI Laboratory. Corresponding Author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Figure 1: An overview of our TransAgent. (a) TransAgent transfers multi-source knowledge from heterogeneous agents to enhance the generalization ability of vision-language foundation models. It demonstrates knowledge versatility, transfer flexibility and deployment efficiency through elaborate agent collaboration and knowledge ensemble strategy. (b) SOTA comparison for base-to-novel generalization on 11 visual recognition benchmarks. Our method outperforms previous SOTA, especially on the more diversified target domains. [81, 72, 47], according to their output forms. Apparently, such design lacks flexibility in transfer learning, based on tool invocation in sequence. Moreover, the resulting pipeline is unfriendly for deployment, due to the ensemble of various models in the inference phase. Another form is the latent representation which implicitly encodes data knowledge in these models [36, 4, 1]. Compared to the explicit output, this implicit representation has key advantage, i.e., it is the feature vector that has homogeneous form among different models. In other words, such vectorized knowledge opens the possibility for unified integration of these heterogeneous agents. Based on this observation, we propose general TransAgent framework in Figure 1. To our best knowledge, it is the first unified distillation framework for generalizing vision-language foundation models with efficient heterogeneous agent collaboration. Compared to the previous works mentioned above, our TransAgent contains three distinct technical contributions. (1) Knowledge Versatility. In our TransAgent, we leverage 11 heterogeneous agents from vision, language and multi-modal research, which comprehensively covers diversified knowledge that is complementary with CLIP-like models, from visual recognition to dense prediction, from chatbot to text encoder, from multi-modal generation to caption. (2) Transfer Flexibility. First, we provide generic knowledge extraction method for each modality, allowing us to flexibly extend more agents if necessary in the future. Especially for multi-modal agents, we design novel manner to extract the prediction score vector of classes as multi-modal knowledge in the target domain, via elaborate mining of vision-language alignment in these models. Second, we introduce mixture-of-agents gating mechanism for integrating external knowledge of different agents in each modality. This allows our TransAgent to automatically select agents via soft weighting so that it can adaptively tackle few-shot settings in different domains of target datasets. (3) Deployment Efficiency. We leverage multi-source distillation to transfer knowledge of these heterogeneous agents into CLIP. Since all these pre-trained models are frozen, the fine-tuning effort is neglectable with few learnable prompts. More importantly, we can unload all the external agents after distillation, i.e., the inference pipeline with the enhanced CLIP is just the same as the original one, achieving deployment efficiency without heavy model ensemble. Finally, we conduct extensive experiments on 11 visual recognition benchmarks, where our TransAgent achieves the state-of-the-art under the same low-shot transfer setting, e.g., via knowledge collaboration, it outperforms the well-known CoOp [87] with around 10% on average and 20% on EuroSAT which contains large domain shifts. Our method also achieves better results than CaFo [81], which adopts model ensemble strategy."
        },
        {
            "title": "2 Related Work",
            "content": "Foundation models. The rapid advancements in deep learning methods have brought abundant pre-trained models to the research area. We group these models into four categories and further demonstrate their ideas below. (i) Vision models: Vision foundation models [33, 9, 3, 59, 32, 13] pre-trained on ImageNet [21] have shown outstanding transfer capability in visual recognition by fine-tuning on downstream datasets. Moreover, various models [43, 49, 16, 17, 8] can be specialists in dense prediction tasks by pre-training on task-relevant domain data. (ii) Large language models: The emergence of large language models (LLMs) [7, 18, 69, 70, 27] has been raising increasing attention from the research community and the public. The astonishing comprehension ability of the LLMs is credited to the linguistic knowledge which can be further applied to solve vision tasks [51, 20, 88, 79]. (iii) Text-to-image generative models: Text-conditioned generation task requires high-level understanding of the given prompts. Recently, diffusion-based generative models [37, 23, 64, 11, 56, 62, 66] have become the state-of-the-art. These models can follow the text conditions faithfully and generate desired outcomes, owing to the semantic knowledge learned during the pre-training stage. (iv) Image-to-text captioning models: These models typically integrate visual knowledge into LLMs to obtain multi-modal understanding abilities [46, 12, 15, 2, 14, 25, 47], offering better experience in referential dialog scenario. In this work, we excavate the underlying knowledge in these heterogeneous models to empower the VL foundation models. Few-shot adaptation. To efficiently transfer vision-language foundation models like CLIP [61] to downstream tasks, researchers have proposed various adaptation methods, which are primarily based on prompt learning [87, 86, 3941, 65, 85, 53, 50] or adapter [31, 80, 68, 48, 81, 75, 89]. Lu et al. [53] explore the potential of collaborating CLIPs architectural variants and propose adaptive ensemble strategies to enhance the generalization performance. PromptKD [50] adapts larger CLIP teacher to downstream datasets and distills the knowledge to smaller student in an unsupervised manner, separating the need for labeled domain data during transfer. TaskRes [75] proposes to decouple the prior knowledge of the pre-trained models and the task-specific knowledge, enabling reliable old knowledge preservation and flexible new knowledge exploration. GraphAdapter [48] further utilizes the dual-modality structure knowledge for better adaptation in downstream tasks. Agent collaboration. Considering the complementary knowledge of diverse pre-trained models specialized in different domains or tasks, several works have been proposed to solve vision tasks with agent collaboration [74, 82, 72, 52, 71, 28]. The most relevant to our work is CaFo [81], which transfers the external knowledge using cache models [80]. However, such an ensemble manner introduces further cost in the inference stage. On the contrary, we adopt heterogeneous agent collaboration to aggregate the knowledge, and distillation strategy to inject the knowledge into CLIP, which demonstrates better performance and guarantees deployment efficiency. Multi-teacher distillation. To improve the effectiveness of knowledge distillation [36], recent works [10, 30, 26, 7678, 83, 54, 63] attempt to integrate the knowledge from multiple teacher networks. To be noted, how to perform multi-teacher distillation is not trivial, and how to extract and collaborate knowledge from various heterogeneous teachers has not been fully explored for CLIP-like foundation models. In this work, we devise generic knowledge extraction method and flexible knowledge collaboration mechanism to further enhance the generalization ability of VL foundation models."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce our TransAgent in detail. As shown in Figure 1, it consists of visionlanguage foundation models and agents from different modalities. In the following section, we will illustrate how to collaborate with them for downstream visual recognition tasks. To start with, we briefly review the V-L foundation models by using the well-known CLIP [61]. Specifically, CLIP consists of two branches. In the vision branch, an image is first divided into non-overlapping equal-sized patches and then projected as input visual tokens Vin, which are fed to the vision encoder to obtain image feature. In the language branch, text description is projected as input textual tokens Tin which are then processed by the text encoder to generate text feature. Through contrastive learning over massive image-text pairs, CLIP achieves good alignment between the two modalities. More interestingly, such large-scale V-L models show the potential in visual classification on the downstream tasks. By converting each class label into text template such as \"a photo of {class 3 Figure 2: Vision Agent Collaboration and Language Agent Collaboration. (a) VAC integrates visual knowledge via MoA gating and transfers the knowledge through layer-wise feature distillation. (b) LAC enhances the textual representations through class-specific feature distillation between the prompted textual feature and the gated textual feature. name}\", these models can easily achieve zero-shot inference. To further enhance their generalization ability under the few-shot settings, prompt learning methods are proposed, which introduce number of learnable prompts while freezing the pre-trained CLIP [87, 86, 3941, 45]. Following previous works, we add set of learnable textual prompts PT RNctxC, where Nctx is the number of learnable prompts, in the language branch and concatenate them with the textual tokens, which are then processed by the text encoder to obtain the prompted textual feature RNclsC for all Ncls categories. Similarly, set of learnable visual prompts PV are inserted in the image branch to generate the prompted visual feature RN where denotes the number of images: = TextEncoder(Tin, PT ), = VisionEncoder(Vin, PV ). (1) Consequently, we compute the prediction score vectors = {Sc} for the image samples, where Sc is the cosine similarity between the visual feature and the textual feature Tc of specific class c. As result, we minimize the cross entropy loss between the score vectors and the ground truth labels: LCE = CrossEntropy(softmax(S), Y), (2) to fine-tune the learnable prompts PT and PV for visual recognition. However, as mentioned in the introduction, it is difficult to achieve good generalization by adapting such single CLIP model, especially when the domain shift of the target datasets is large. Hence, we propose to transfer diversified knowledge from heterogeneous agents in different modalities for better adaption of CLIP. 3.1 Vision Agent Collaboration (VAC) One important component of CLIP is its vision branch in Figure 2. Hence, we consider to enhance this branch by transferring visual knowledge from various vision agents. To achieve this goal, we have to answer three critical questions. The first question is which models should be used for collaboration. As we know, CLIP mainly establishes image-level alignment between the two modalities, neglecting visual details in the pixel space. To fill this gap, we choose vision agents from two aspects. On one aspect, we choose vision models pre-trained with self-supervision such as MAE [33] and DINO [9]. Both agents focus on detailed image modeling via image masking [33] or patch self-distillation [9]. On the other aspect, we choose vision models built on dense supervision such as ViTDet [49] and SAM [43]. Both agents work on instance-level prediction with bounding boxes and masks. Detailed information of these models can be found in the supplementary. The second question is how to extract visual knowledge by collaborating these agents. As mentioned in the introduction, the latent feature is common knowledge form among these heterogeneous models. 4 Hence, given an input image, we extract the intermediate visual features {VA(i) RN C} from the vision encoders of these agents. Moreover, the contribution of different agents may vary among different domains. To fully exploit these agents in unified manner, we introduce Mixture-of-Agents (MoA) gating mechanism to adaptively integrate {VA(i)} as the visual knowledge. Specifically, we concatenate all the agent features along the channel dimension C, and feed them into MLP network to generate the gating weight WV . Next, we obtain the gated visual features VA by computing the weighted sum over {VA(i)}: WV = MLP(Concat({VA(i)})), VA = (cid:88) WV (i)VA(i). (3) The final question is how to transfer the visual knowledge to enhance CLIPs vision encoder. We adopt feature distillation [36] where we compute the L1 loss between the prompted visual features in Eq. 1 and the gated visual features from vision agents: LVAC = VA. (4) Since the original CLIP is frozen, the above loss term allows us to fine-tune the learnable visual prompts PV in Eq. 1 and MLP gating network in Eq. 3, which enables us to adaptively transfer external visual knowledge to the visual prompts to empower generalization ability. 3.2 Language Agent Collaboration (LAC) The other important component of CLIP is its language branch in Figure 2. Similar to the vision branch, we consider three critical questions to transfer the textual knowledge from various language agents. Recent studies [60, 42] have shown that it is coarse to use simple template (e.g., \"a photo of {class name}\") to describe certain category. Hence, we first interact with the popular chatbots such as GPT-3 [7] and Vicuna [18] to enrich the class descriptions using queries like \"What does {class name} look like?\". After obtaining the detailed descriptions from these chatbots, we use text encoder (e.g., BERT [22]) to extract the text features {TA(j) RNclsC} of all descriptions. To adaptively integrate {TA(j)} as the textual knowledge, we also utilize the MoA gating mechanism: WT = MLP(Concat({TA(j)})), TA = (cid:88) WT (j)TA(j). (5) Finally, for each category, we perform feature distillation where we compute the L1 loss between the prompted textual feature in Eq. 1 and the gated textual feature from language agents in Eq. 5: By fine-tuning the learnable textual prompts PT in Eq. 1 and MLP gating network in Eq. 5, we adaptively transfer the rich textual knowledge to enhance CLIPs textual representations. LLAC = TA. (6) 3.3 Multi-modal Agent Collaboration (MAC) Through vision and language agent collaboration, we can enhance the learnable visual and textual prompts respectively. Considering the key success in vision-language foundation models is credited to the multi-modal alignment, we investigate how to further align the visual and textual prompts with external multi-modal agents. First, there exists two types of multi-modal agents, including Text-toImage (T2I) generative models [64, 11] and Image-to-Text (I2T) captioning models [46, 12]. Since both types of agents involve conversion from one modality to the other, we believe they implicitly achieve vision-language alignment. Based on the above discussion, we choose our T2I agents built upon two mainstream structures including Stable Diffusion [64] in UNet style and PixArt-α [11] in DiT style. Moreover, we choose our I2T agents specialized in two mainstream tasks including BLIP-2 [46] for general captioning and Shikra [12] for grounded dialogue. We next think what kind of knowledge can represent vision-language alignment. The most direct form may be the probability score vector that shows the prediction confidence over target classes. Hence, for each training image, we investigate such knowledge in these multi-modal agents. In the T2I agents, cross attention effectively encodes relations between image and text [34, 84]. Hence, we leverage such prior to extract the score vector. Specifically, we use the template description of class as text. Then, we extract cross attention value Mc between the text and the k-th token of the input image from the pre-trained T2I agents. Consequently, we sum over all the tokens of to 5 Figure 3: Multi-modal Agent Collaboration. Top left: We first extract the cross attention maps from the T2I agents and then obtain the score vectors through LSE pooling. Top right: We compute the score vectors from the I2T agents as the cosine similarity between the projected visual feature and the LLMs textual feature. Finally, we perform score distillation between the learned score vectors and the gated score vectors to further align the learnable prompts. obtain the prediction score of the image w.r.t. class c: Sc LogSumExp (LSE) pooling [5] to provide more accurate matching scores. 2I = log((cid:80) exp(Mc k)), where we adopt In the I2T agents, there exists projection module (e.g., Q-Former in BLIP-2 [46] and MLP in Shikra [12]) to adapt the visual features for the large language model. Hence, we extract the visual feature of an input image via the projection and the textual features of all the classes from the LLM. By computing the cosine similarity between them, we can obtain the prediction score SI2T of the image over all the classes. Next, we leverage the MoA gating mechanism to adaptively summarize the prediction score vectors from all the multi-modal agents MA = Concat({ST 2I , SI2T }): WS = MLP(MA), SA = (cid:88) WS(n)MA(n). (7) Finally, we explore how to transfer SA to enhance the learnable prompts in CLIP. Specifically, after processed by the vision and text encoders of CLIP, the learnable visual and textual prompts PV and PT are transformed into QV and QT . Unlike PV and PT which are universal, QV is relevant to the image samples and QT is relevant to the target classes. Hence, we can compute the cosine similarity between QV and QT to obtain the learned score vectors SP RN Ncls of the input images over all the classes. We perform score distillation between SP and SA: LMAC = KL(softmax(SP )softmax(SA)). (8) Through computing the KL divergence, we can leverage external multi-modal knowledge SA as semantic guidance to align the learnable visual and textual prompts. 3.4 Multi-Source Knowledge Distillation Finally, we combine all the distillation loss from multiple sources, achieving heterogeneous agent collaboration for knowledge transfer: LTransAgent = LCE + λ1LVAC + λ2LLAC + λ3LMAC, (9) 6 Table 1: Accuracy comparison with state-of-the-art methods on base-to-novel generalization. All methods use CLIPs ViT-B/16 as the vision encoder. Our TransAgent exhibits strong generalization ability and outperforms previous SOTA on all datasets. The best results are bolded. Method Average ImageNet [21] Caltech101 [29] OxfordPets [58] Base Novel HM Base Novel HM Base Novel HM Base Novel HM 69.34 74.22 71.70 72.43 68.14 70.22 96.84 94.00 95.40 91.17 97.26 94.12 CLIP [61] 82.69 63.22 71.66 76.47 67.88 71.92 98.00 89.81 93.73 93.67 95.29 94.47 CoOp [87] 80.47 71.69 75.83 75.98 70.43 73.10 97.96 93.81 95.84 95.20 97.69 96.43 CoCoOp [86] 82.28 75.14 78.55 75.40 70.32 72.72 98.27 93.23 95.68 95.43 97.83 96.62 MaPLe [40] 81.13 75.00 77.78 76.60 71.57 74.00 97.97 94.37 96.03 94.63 97.50 96.05 RPO [45] PromptSRC [41] 84.26 76.10 79.97 77.60 70.73 74.01 98.10 94.03 96.02 95.33 97.30 96.30 TransAgent 85.29 77.62 81.27 78.07 70.57 74.13 98.90 95.23 97.03 96.33 98.13 97. Method StanfordCars [44] Flowers102 [57] Food101 [6] FGVCAircraft [55] Base Novel HM Base Novel HM Base Novel HM Base Novel HM 63.37 74.89 68.65 72.08 77.80 74.83 90.10 91.22 90.66 27.19 36.29 31.09 CLIP [61] 78.12 60.40 68.13 97.60 59.67 74.06 88.33 82.26 85.19 40.44 22.30 28.75 CoOp [87] 70.49 73.59 72.01 94.87 71.75 81.71 90.70 91.29 90.99 33.41 23.71 27.74 CoCoOp [86] 74.70 71.20 72.91 97.70 68.68 80.66 90.30 88.57 89.43 36.90 34.13 35.46 MaPLe [40] 73.87 75.53 74.69 94.13 76.67 84.50 90.33 90.83 90.58 37.33 34.20 35.70 RPO [45] PromptSRC [41] 78.27 74.97 76.58 98.07 76.50 85.95 90.67 91.53 91.10 42.73 37.87 40.15 TransAgent 79.53 74.73 77.06 98.37 77.13 86.46 90.87 92.20 91.53 43.77 39.00 41.25 Method SUN397 [73] DTD [19] EuroSAT [35] UCF101 [67] Base Novel HM Base Novel HM Base Novel HM Base Novel HM 69.36 75.35 72.23 53.24 59.90 56.37 56.48 64.05 60.03 70.53 77.50 73.85 CLIP [61] 80.60 65.89 72.51 79.44 41.18 54.24 92.19 54.74 68.69 84.69 56.05 67.46 CoOp [87] 79.74 76.86 78.27 77.01 56.00 64.85 87.49 60.04 71.21 82.33 73.45 77.64 CoCoOp [86] 78.47 76.93 77.79 80.67 56.48 66.44 83.90 66.00 73.88 85.23 71.97 78.04 MaPLe [40] RPO [45] 80.60 77.80 79.18 76.70 62.13 68.61 86.63 68.97 76.79 83.67 75.43 79.34 PromptSRC [41] 82.67 78.47 80.52 83.37 62.97 71.75 92.90 73.90 82.32 87.10 78.80 82.74 TransAgent 82.90 79.30 81.06 84.37 63.67 72.57 97.43 83.43 89.89 87.60 80.47 83. where λ1, λ2, λ3 are hyperparameters. Since all the pre-trained models are frozen in the training phase, we only need to fine-tune the learnable vision and language prompts with negligible cost. More importantly, owing to the distillation strategy, all the agents can be unloaded and the modality-specific gates can be abandoned in the inference phase. We can simply use the enhanced CLIP just like the original one, which largely boosts deployment efficiency without model ensemble."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets and Metrics. We evaluate our proposed method on 11 commonly used datasets covering wide range of recognition tasks, including ImageNet [21], Caltech101 [29], OxfordPets [58], StanfordCars [44], Flowers102 [57], Food101 [6], FGVCAircraft [55], SUN397 [73], UCF101 [67], DTD [19] and EuroSAT [35]. We explore two typical low-shot scenarios to evaluate the performance. (i) Base-to-novel generalization: The datasets are equally split into base and novel classes. The model is trained on base classes and evaluated on the test set of both classes. We report the base and novel class accuracy and the harmonic mean (HM) of the results. (ii) Few-shot classification: We assess the accuracy trained with 1/2/4/8/16 shot(s) per class to examine the models learning capacity. Implementation Details. We adopt CLIP ViT/B-16 as our backbone and conduct all experiments using 3 different seeds to obtain an averaged result, following previous works [40, 41, 50]. Our method ensembles knowledge from heterogeneous agents, including pre-trained vision models, LLMs, T2I generative models and I2T captioning models. Detailed information of the agents and training settings are shown in Appendix A. 7 Figure 4: Accuracy comparison in few-shot classification. TransAgent demonstrates state-of-theart performance for all few-shot settings on different datasets, which proves promising learning capability even under extremely limited supervision. 4.1 Comparison with State-of-the-Art In Table 1, we compare the base-to-novel generalization performance of our proposed TransAgent with state-of-the-art prompt learning methods, including CoOp [87], CoCoOp [86], MaPLe [40], RPO [45] and PromptSRC [41]. All approaches use the same CLIP ViT-B/16 during evaluation stage. Our method demonstrates superior performance across 11 datasets, surpassing all competing methods in terms of base and novel accuracy, as well as the harmonic mean, particularly excelling on the EuroSAT dataset. In Figure 4, we present the few-shot classification performance of TransAgent and previous methods. Our method consistently outperforms the counterparts and demonstrates growing learning capability when the training samples increase. To be mentioned, TransAgent outperforms CaFo [81] with much fewer deployment costs (see Table 13). Detailed comparisons on cross-dataset evaluation and domain generalization are provided in Appendix B. 4.2 Ablative Analysis In this section, we present ablative analysis of our agent collaboration designs for each modality. We adopt IVLP [40, 41] as our baseline model. 8 Figure 5: Averaged gating weights of each agent on different datasets. Deeper color indicates more contributions to the gated feature(s) or score vectors. Table 2: VAC Design. Table 3: LAC Design. Table 4: MAC Design. Models Base Novel HM Models Base Novel HM baseline 84.21 71.79 77.51 MAE DINO SAM ViTDet Last-layer Layer-wise Average Add Gating 84.51 84.60 84.73 84.45 84.47 85.29 84.20 84.40 85.29 71.66 70.90 71.87 72.04 75.92 77.62 74.79 75.17 77. 77.55 77.15 77.77 77.75 79.97 81.27 79.22 79.52 81.27 baseline 84.21 71. 77.51 GPT-3 Vicuna [SOS] [EOS] Average Add Gating 85.15 85.35 84.19 85. 84.23 82.44 85.29 74.55 74.70 75.25 77.62 75.98 74.89 77.62 79.50 79.67 79.47 81. 79.89 78.48 81.27 Models baseline Base Novel HM 84.21 71. 77.51 Stable Diffusion Pixart-α BLIP2 Shikra Prompted logits Learned scores Average Add Gating 84.91 84.78 84.87 84.97 84.45 85. 84.33 84.37 85.29 73.11 73.47 73.43 72.90 75.18 77.62 75.04 75.35 77.62 78.57 78.72 78.73 78.47 79.54 81. 79.41 79.61 81.27 Table 5: Performance w/ individual module. Table 6: Performance w/ module combinations. VAC LAC MAC HM VAC LAC MAC HM 77.51 79.04 79.90 79.61 - +1.53 +2.39 +2.10 80.02 79.79 80.40 81.27 +2.51 +2.28 +2.89 +3.76 Effectiveness of individual agent. In Table 2 - Table 4, we show the results of introducing individual agent as teacher to supervise the baseline model in the beginning rows. Nearly all vision agents contribute to the improvement in accruracy, except for DINO lacks behind. However, we observe that DINO performs well on most datasets, except on EuroSAT (with 5.60% decline). Nontheless, we still keep DINO as one of the vision agents. While for the language and the multi-modal agents, they all boost the performance of the baseline model respectively. Distillation strategy. In the middle rows of Table 2 - Table 4, we ablate the alignment strategy for agent collaboration of each modality. (i) For VAC, we choose to adopt feature distillation with either the average-pooled feature at the last layer or all features at all layers. As the results suggest, layer-wise distillation of all features performs better. (ii) For LAC, two special tokens from the output of the text encoder are considered to compute Equation 6. These tokens are inserted to the text sequence to compose complete prompt. [EOS] performs better owing to the causal attention module in Transformer blocks, so that it aggregates more information. (iii) For MAC, either the prompted logits of CLIP or the learned scores (Eq. 8) are chosen to align with the semantic logits from multi-modal agents. Since the prompted logits need to be aligned with ground-truth labels as well (Eq. 2), there might be confusion to align it with the external knowledge simultaneously. Experimental results show that using learned scores to align with semantic logits yields better results. 9 Fusion design. In the last rows of Table 2 - Table 4, we ablate the fusion design for each modality. \"Average\" refers to simply calculate the average of all output features from the agents along the channel dimension; \"Add\" refers to calculate the distillation loss separately for each agent; \"Gating\" denotes our proposed MoA gating mechanism. As can be seen, gating fusion achieves the best results for all collaboration designs, since it adaptively selects the useful information from the agents. Effectiveness of collaboration module(s). As presented in Table 5, the first row indicates the baseline model. Each individual module is beneficial for improving the generalization ability of the foundation models, where the last column shows the relative increase. Results in Table 6 demonstrate that the combinations of collaboration modules further boost the results. 4.3 Visualization We calculate the averaged gating weights of each agent with 16-shot training samples from all classes of certain dataset and present the results with heat-maps. As shown in Figure 5, different agents do not contribute equally towards certain target datasets, which also verifies the superiority of our gating fusion design. For vision agents, DINO experts in recognizing general objects while lags behind on some fine-grained datasets (e.g., EuroSAT), where the others which focus more on details perform better. Language agents provide domain-specific linguistic knowledge accordingly. I2T agents consistently provide their knowledge thanks to the grounding nature, while T2I agents demonstrate better capability in fine-grained scenarios."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose TransAgent, unified framework to transfer vision-language foundation models through heterogeneous agent collaboration. By adaptively integrating the external knowledge of agents from different modalities via MoA gating mechanism, TransAgent achieves state-of-the-art performance on 11 datasets under the low-shot scenarios. Limitations. Although TransAgent collaborates heterogeneous agents in unified manner, transferring the external knowledge through distillation may harm the original CLIPs representations even using prompt learning methods, because the domain knowledge from agents are diversified, irrelevant information may also be introduced. Moreover, one of the key characteristics of these agents is large-scale pre-training, few-shot scenarios may not meet their data-hungry nature. So our future work would be integrating the knowledge with directed focus and further unleashing the potential of the agents even when the domain-specific knowledge (e.g., labels) is absent."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported in part by the National Key R&D Program of China (NO.2022ZD0160505), the National Natural Science Foundation of China under Grant (62272450)"
        },
        {
            "title": "References",
            "content": "[1] R. Adriana, B. Nicolas, K. S. Ebrahimi, C. Antoine, G. Carlo, and B. Yoshua. Fitnets: Hints for thin deep nets. In International conference on learning representations, 2015. [2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [3] H. Bao, L. Dong, S. Piao, and F. Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. [4] Y. Bengio, A. Courville, and P. Vincent. Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013. [5] P. Blanchard, D. J. Higham, and N. J. Higham. Accurately computing the log-sum-exp and softmax functions. IMA journal of numerical analysis, 41(4):23112330, 2021. 10 [6] L. Bossard, M. Guillaumin, and L. V. Gool. Food-101mining discriminative components with random forests. In European conference on computer vision, pages 446461. Springer, 2014. [7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. [8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. [9] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, October 2021. [10] Y. Chebotar and A. Waters. Distilling knowledge from ensembles of neural networks for speech recognition. In Interspeech, pages 34393443, 2016. [11] J. Chen, Y. Jincheng, G. Chongjian, L. Yao, E. Xie, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In International conference on learning representations, 2024. [12] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [13] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. simple framework for contrastive learning of visual representations. In Proceedings of the international conference on machine learning, pages 15971607. PMLR, 2020. [14] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, et al. Pali: jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. [15] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [16] B. Cheng, A. Schwing, and A. Kirillov. Per-pixel classification is not all you need for semantic segmentation. Advances in neural information processing systems, 34:1786417875, 2021. [17] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [18] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. [19] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36063613, June 2014. [20] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36, 2024. [21] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 248255. IEEE, 2009. 11 [22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [23] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International conference on learning representations, 2021. [25] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. [26] S. Du, S. You, X. Li, J. Wu, F. Wang, C. Qian, and C. Zhang. Agree to disagree: Adaptive ensemble knowledge distillation in gradient space. Advances in neural information processing systems, 33:1234512355, 2020. [27] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. [28] X. Fan, T. Ji, C. Jiang, S. Li, S. Jin, S. Song, J. Wang, B. Hong, L. Chen, G. Zheng, et al. Mousi: Poly-visual-expert vision-language models. arXiv preprint arXiv:2401.17221, 2024. [29] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE/CVF conference on computer vision and pattern recognition workshop, pages 178178. IEEE, 2004. [30] T. Fukuda, M. Suzuki, G. Kurata, S. Thomas, J. Cui, and B. Ramabhadran. Efficient knowledge distillation from an ensemble of teachers. In Interspeech, pages 36973701, 2017. [31] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better vision-language models with feature adapters. International journal of computer vision, 132(2): 581595, 2024. [32] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. [33] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, June 2022. [34] X. He, W. Feng, T.-J. Fu, V. Jampani, A. Akula, P. Narayana, S. Basu, W. Y. Wang, and X. E. Wang. Discriminative diffusion models as few-shot vision and language learners. arXiv preprint arXiv:2305.10722, 2023. [35] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE journal of selected topics in applied earth observations and remote sensing, 12(7):22172226, 2019. [36] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [37] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [38] C. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the international conference on machine learning, volume 139, pages 49044916. PMLR, 2021. [39] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim. Visual prompt tuning. In European conference on computer vision, pages 709727, 2022. 12 [40] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1911319122, June 2023. [41] M. U. Khattak, S. T. Wasim, M. Naseer, S. Khan, M.-H. Yang, and F. S. Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1519015200, October 2023. [42] M. U. Khattak, M. F. Naeem, M. Naseer, L. Van Gool, and F. Tombari. Learning to prompt with text only supervision for vision-language models. arXiv preprint arXiv:2401.02418, 2024. [43] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, October 2023. [44] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE/CVF international conference on computer vision workshop, pages 554561, December 2013. [45] D. Lee, S. Song, J. Suh, J. Choi, S. Lee, and H. J. Kim. Read-only prompt optimization for vision-language few-shot learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 14011411, October 2023. [46] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the international conference on machine learning, volume 202, pages 1973019742. PMLR, 2329 Jul 2023. [47] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [48] X. Li, D. Lian, Z. Lu, J. Bai, Z. Chen, and X. Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. Advances in neural information processing systems, 36, 2024. [49] Y. Li, H. Mao, R. Girshick, and K. He. Exploring plain vision transformer backbones for object detection. In European conference on computer vision, pages 280296, 2022. [50] Z. Li, X. Li, X. Fu, X. Zhang, W. Wang, and J. Yang. Promptkd: Unsupervised prompt distillation for vision-language models. In Proceedings of the IEEE/CVF international conference on computer vision, 2024. [51] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [52] M. Liu, S. Roy, W. Li, Z. Zhong, N. Sebe, and E. Ricci. Democratizing fine-grained visual recognition with large language models. In International conference on learning representations, 2023. [53] Z. Lu, J. Bai, X. Li, Z. Xiao, and X. Wang. Beyond sole strength: Customized ensembles for generalized vision-language models. arXiv preprint arXiv:2311.17091, 2023. [54] Z. Ma, J. Dong, S. Ji, Z. Liu, X. Zhang, Z. Wang, S. He, F. Qian, X. Zhang, and L. Yang. Let all be whitened: Multi-teacher distillation for efficient visual retrieval. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 41264135, 2024. [55] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [56] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [57] M.-E. Nilsback and A. Zisserman. Automated flower classification over large number of classes. In Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. 13 [58] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012. [59] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei. Beit v2: Masked image modeling with vectorquantized visual tokenizers. arxiv 2022. arXiv preprint arXiv:2208.06366, 2022. [60] S. Pratt, I. Covert, R. Liu, and A. Farhadi. What does platypus look like? generating In Proceedings of the IEEE/CVF customized prompts for zero-shot image classification. international conference on computer vision, pages 1569115701, October 2023. [61] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from In Proceedings of the international conference on machine natural language supervision. learning, volume 139 of PMLR, pages 87488763. PMLR, 1824 Jul 2021. [62] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [63] M. Ranzinger, G. Heinrich, J. Kautz, and P. Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1249012500, 2024. [64] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, June 2022. [65] S. Roy and A. Etemad. Consistency-guided prompt learning for vision-language models. In International conference on learning representations, 2024. [66] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494, 2022. [67] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [68] Y.-L. Sung, J. Cho, and M. Bansal. Vl-adapter: Parameter-efficient transfer learning for visionand-language tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 52275237, 2022. [69] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [70] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [71] X. Wang, Y. Zhang, O. Zohar, and S. Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. arXiv preprint arXiv:2403.10517, 2024. [72] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. [73] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 34853492. IEEE, 2010. [74] P. Ye, C. Huang, M. Shen, T. Chen, Y. Huang, Y. Zhang, and W. Ouyang. Merging vision transformers from different tasks and domains. arXiv preprint arXiv:2312.16240, 2023. 14 [75] T. Yu, Z. Lu, X. Jin, Z. Chen, and X. Wang. Task residual for tuning vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1089910909, 2023. [76] F. Yuan, L. Shou, J. Pei, W. Lin, M. Gong, Y. Fu, and D. Jiang. Reinforced multi-teacher In Proceedings of the AAAI conference on artificial selection for knowledge distillation. intelligence, volume 35, pages 1428414291, 2021. [77] H. Zhang, D. Chen, and C. Wang. Confidence-aware multi-teacher knowledge distillation. In IEEE international conference on acoustics, speech and signal processing, pages 44984502. IEEE, 2022. [78] H. Zhang, D. Chen, and C. Wang. Adaptive multi-teacher knowledge distillation with metalearning. In IEEE international conference on multimedia and expo, pages 19431948. IEEE, 2023. [79] H. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [80] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li. Tip-adapter: Trainingfree adaption of clip for few-shot classification. In European conference on computer vision, pages 493510. Springer, 2022. [81] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, and H. Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1521115222, 2023. [82] S. Zhang, D. Huang, J. Deng, S. Tang, W. Ouyang, T. He, and Y. Zhang. Agent3d-zero: An agent for zero-shot 3d understanding. arXiv preprint arXiv:2403.11835, 2024. [83] S. Zhao, X. Wang, and X. Wei. Mitigating accuracy-robustness trade-off via balanced multiteacher adversarial distillation. IEEE transactions on pattern analysis and machine intelligence, (01):114, 2024. [84] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu. Unleashing text-to-image diffusion models for visual perception. In Proceedings of the IEEE/CVF international conference on computer vision, pages 57295739, October 2023. [85] Z. Zheng, J. Wei, X. Hu, H. Zhu, and R. Nevatia. Large language models are good prompt In Proceedings of the IEEE/CVF international learners for low-shot image classification. conference on computer vision, 2024. [86] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1681616825, June 2022. [87] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. International journal of computer vision, 130:23372348, July 2022. [88] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [89] X. Zhu, R. Zhang, B. He, A. Zhou, D. Wang, B. Zhao, and P. Gao. Not all features matter: Enhancing few-shot clip with adaptive prior refinement. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26052615, 2023. 15 Table 7: Demonstration of heterogeneous agents specialized in different domains or tasks. Model Parameters Model Type Pre-training Tasks Datasets Knowledge DINO [9] MAE [33] SAM [43] ViTDet [49] GPT-3 [7] Vicuna [18] BERT [22] Stable Diffusion [64] Pixart-α [11] BLIP-2 [46] Shikra [12] 86M 86M 86M 86M 175B 13B 38M 0.86B 0.6B 2.7B 7B ViT ViT ViT ViT LLM LLM Transformer UNet DiT MLLM MLLM IC MIM IS OD TG TG MLM IG IG ITC+ITM+ITG ITC+ITM+ITG ImageNet-1K ImageNet-1K SA-1B COCO - - - Vision Vision Vision Vision Language Language Language LAION-2B Multi-modal Multi-modal Multi-modal Multi-modal - - - Table 8: Memory and training time required for each dataset. Caltech Oxford Standford Flowers Setting ImageNet Food SUN397 DTD FGVC Aircraft Euro SAT UCF101 Memory (MB) Time (ms/batch) Base-to-novel Few-shot (16-shot) 11790 19978 Base-to-novel Few-shot (16-shot) 400 688 101 4708 190 214 Pets 4692 6892 195 206 Cars 6698 226 262 102 4702 5488 211 217 4708 5488 209 5286 5486 219 221 7708 10110 5008 4034 4700 4124 273 356 191 190 205 4718 5484"
        },
        {
            "title": "A Additional Implementation Details",
            "content": "Training Details. The number of learnable vision and language prompt tokens are both set to 4, and the prompt depth is set to 9 for base-to-novel generalization and few-shot classification, and 3 for cross-dataset and domain generalization. The learnable text prompts of the first layer are initialized with the word embeddings of \"a photo of a\", while the other learnable prompts are randomly initialized with normal distribution. For vision agent collaboration, we adopt online distillation where the models are loaded during the training process. For language and multi-modal agent collaboration, due to the huge model parameters, we extract their knowledge offline before our training launches. For few-shot classification, we train the models for 50 epochs under different low-shot settings (ranging from 1 to 16). We re-implement the compared methods in Figure 4 with Vision Transformer [24] backbone for fair comparison. For the other benchmarks, we observe that PromptKD [50] use transductive setting which uses all training samples in an unsupervised manner and obtains better results. However, such setting is beyond our scope. In our work, all models are trained for 20 epochs using 16-shot samples with fixed batch size of 4 and learning rate of 0.0025 with SGD as the optimizer. We set λ1 = 1, λ2 = 25 and λ3 = 1 in Eq. 9 after extensive hyperparameter search to balance the total loss. The memory and training time needed under different settings on each dataset are provided in Table 8 using batch size of 4. At least 48GB is required to extract knowledge from language and multi-modal agents. All experiments are conducted on single Nvidia A6000 GPU. Information of agents. Table 7 lists all the agents we have collaborated in our TransAgent framework. (i) Vision agents: DINO provides robust representations through image contrastive (IC) learning while MAE presents powerful capability via mask image modeling (MIM). SAM and ViTDet are specialists in image segmentation (IS) and object detection (OD) respectively. (ii) Language agents: Both GPT-3 and Vicuna are excellent chatbots which faithfully follow human instructions and respond with convincing answers by text generation (TG). We use BERT, which is just CLIPs text encoder pre-trained using mask language modeling (MLM), to process the generated captions from chatbots before transferring these lingustic knowledge. (iii) Multi-modal agents: Stable Diffusion and Pixart-α share different model architectures, but both T2I agents demonstrate astonishing image generation (IG) capability. The I2T agents are pre-trained with multiple tasks including imagetext contrastive (ITC), image-text matching (ITM) and image-grounded text generation (ITG) over large-scale datasets. These agents demonstrate outstanding multi-modal understanding ability. 16 Table 9: Accuracy comparison with previous methods on cross-dataset evaluation. Method Source ImageNet CLIP CoOp CoCoOp MaPLe PromptSRC TransAgent 66.72 71.51 71.02 70.72 71.27 72.00 Target Caltech Oxford Standford Flowers 101 92.94 93.70 94.43 93.53 93.60 Pets 89.07 89.14 90.14 90.49 90.25 94. 90.33 Cars 65.29 64.51 65.32 65.57 65.70 65.43 102 71.30 68.71 71.88 72.23 70. 71.40 Food101 FGVC Aircraft SUN397 DTD Euro SAT UCF101 Avg. 86.11 85.30 86.06 86.20 86.15 86.47 24.87 18.47 22.94 24.74 23.90 23.20 62.62 64.15 67.36 67.01 67.10 44.56 47.69 41.92 46.39 45.73 45.37 46.49 48.06 46.87 45. 66.77 66.55 68.21 68.69 68.75 65.12 63.88 65.74 66.30 65.81 66.20 45.30 52.13 69.93 66. Table 10: Accuracy comparison on domain generalization. Source Target ImageNet -V2 -S -A -R Method CLIP CoOp CoCoOp MaPLe PromptSRC TransAgent 66.73 71.51 71.02 70.72 71. 72.00 60.83 64.20 64.07 64.07 64.35 46.15 47.99 48.75 49.15 49.55 47.77 49.71 50.63 50.90 50.90 73.96 75.21 76.18 76.98 77.80 64. 49.63 51.23 77.53 60.82 Avg. 57.18 59.28 59.91 60.27 60. Table 11: MAC loss type. HM Loss Novel Base Table 12: Pooling type. Table 13: Inference time. Pooling Base Novel HM Method Time Accuracy L1 MSE KL 84.93 84.91 85. 74.69 74.40 77.62 79.48 79.31 81.27 84.97 Average Max 84.69 LogSumExp 85.29 76.12 75.12 77.62 80.30 79.62 81.27 Zero-shot CLIP CaFo TransAgent 1min06s 16min14s 2min35s 66.70 72.46 73."
        },
        {
            "title": "B Additional Experiments",
            "content": "Cross-Dataset Evaluation. We compare the cross-dataset performance in Table 9. On the source dataset, TransAgent achieves better performance than previous few-shot prompt learning methods under the same training settings. We credit the fine-fitted result to the ImageNet pre-trained vision experts, which transfer their knowledge of the source dataset during the training process. More surprisingly, our method does not seem to overfit on the source dataset. It still presents competitive results against MaPLe, which is the SOTA few-shot method on cross-dataset generalization, leading to an overall improvement over the previous methods. Domain Generalization. Table 10 presents the results on domain generalization. TransAgent outperforms previous few-shot prompt learning methods with the highest average accuracy. This suggests that our method improves the robustness of VLMs against out-of-distribution data."
        },
        {
            "title": "C Additional Ablative Analysis",
            "content": "MAC loss type. We scrutinize the loss function used to compute the distillation loss in Eq. 8. As shown in Table 11, using L1 loss or MSE loss underperforms using KL loss by large margin. Such channel-wise loss is incompatible with the form of the score vectors, which are per-class probability distributions, and using such loss makes it hard to converge. On the contrary, KL loss provides smoother training due to the soft matching between the probability distributions (i.e., the score vectors). Pooling type. In Table 12, we ablate the pooling strategy to aggregate the cross attention value from T2I agents to obtain the score vectors. The pooling operations are performed on the spatial dimension of the cross attention maps, which results in the semantic logits of certain image over all the candidate categories. As is shown in Table 12, average and max pooling lag behind since they introduce information loss during the operations. However, LogSumExp (LSE) pooling yields the best performance as it is more robust and provides more accurate matching scores [34]. 17 Figure 6: Variance and performance of TransAgent compared with CoOp. TransAgent demonstrates better robustness and outperforms CoOp on most low-shot cases. Deployment efficiency. Table 13 presents the comparison of inference time and performance evaluated on the test set of ImageNet [21]. All models adopt Vision Transformer [24] backbone. Our method demonstrates superior deployment efficiency compared to CaFo [81] which adopts cumbersome model ensemble. To be mentioned, the learnable prompts in the training phase are frozen during the inference stage, which carries the knowledge of heterogeneous agents to enhance CLIPs generalization ability. The inference time of our TransAgent is little behind the zero-shot CLIP, but it still achieves good accuracy-efficiency tradeoff."
        },
        {
            "title": "D Additional Visualization",
            "content": "To verify the robustness of TransAgent, we run few-shot classification experiments over 3 seeds and visualize the variance on 9 datasets in Figure 6 (ImageNet and SUN397 are excluded since the variance is small for both methods.). We can observe that TransAgent consistently outperforms CoOp in robustness and excels in most low-shot scenarios."
        },
        {
            "title": "E Examples of Generated Descriptions from Chatbots",
            "content": "We interact with the language agents (i.e., chatbots) as described in Section 3.2 to obtain rich descriptions of the categories in the target domain, following [42]. To further unleash the linguistic potential of these language agents, we gather their knowledge adaptively via MoA gating mechanism. In fact, these language agents give descriptions in different styles with their respective unique understanding. Below we show some examples of the descriptions given by different chatbots for certain classes to demonstrate their characteristics. Class: Pomeranian (OxfordPets). 18 GPT-3: \"A pet Pomeranian typically has thick coat of fur that can be either straight or curly.\" Vicuna: \"The image shows small dog with fluffy white coat. The dog has round face with big brown eyes and black nose. Its ears are small and pointy, and its tail is curled up over its back.\" Class: Industrial Buildings (EuroSAT). GPT-3: \"A satellite photo of industrial buildings might look like patchwork of utilitarian structures.\" Vicuna: \"The image shows large industrial building with flat roof and several windows on the front facade. The building is made of concrete and has gray color. The roof is also gray and appears to be made of similar material.\" Class: Club Sandwich (Food101). GPT-3: \"A club sandwich is sandwich made with ham, turkey, bacon, cheese, and tomato, with lettuce in between slices of bread.\" Vicuna: \"The image shows golden brown club sandwich, with thick slice of bread on the bottom, and two thinner slices on top. The bread is toasted and has crispy texture.\" Class: Office Building (SUN397). GPT-3: \"An office building is usually tall building with many floors.\" Vicuna: \"An office building is structure that is designed for business-related activities. It typically has multiple floors and variety of rooms and spaces to accommodate different functions, such as open-plan offices, meeting rooms, and private offices.\" Class: 747-200 (FGVCAircraft). GPT-3: \"The 747-200 is jumbo jet produced by Boeing.\" Vicuna: \"The Boeing 747-200 is large, wide-body commercial airliner that was first introduced in the 1970s.\" Class: 2012 Mercedes-Benz E-Class Sedan (StandfordCars). GPT-3: \"The Mercedes-Benz E-Class Sedan 2012 can be identified by its distinctive grille, sleek headlights, and classic Mercedes-Benz logo.\" Vicuna: \"The 2012 Mercedes-Benz E-Class Sedan is luxury car that is known for its sleek and sophisticated appearance.\""
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: You should answer [Yes] , [No] , or [NA] . [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. Please provide short (12 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: Delete this instruction block, but keep the section heading NeurIPS paper checklist\", Keep the checklist subsection headings, questions/answers and guidelines below. Do not modify the questions and only use the provided macros for your answers. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract synoptically demonstrates our proposed method, which aims to transfer VL foundation models with heterogeneous agent collaboration. The introduction section further claims the background and our contributions. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] 20 Justification: We discuss the potential limitations of our proposed method in Section 5. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: The results of the paper are mainly experimental. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The implementation details are provided in the main paper (Section 4) as well as in the supplementary (Appendix A). Guidelines: 21 The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The training and evaluation codes are released at https://github.com/ markywg/transagent, with detailed instructions on how to download data and reproduce the experimental results. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. 22 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The training details and hyperparameters setting are listed in Appendix A. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Since all experiments are run over 3 different seeds, we provide the variance figures in the supplementary (Appendix D) to display the robustness of our proposed method. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As stated in Appendix A, all experiments are conducted using Nvidia A6000 GPU. The memory and training time required on each dataset are presented in Table 8. Guidelines: 23 The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conduct the research project following NeurIPS Code of Ethics faithfully. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work does not seem to raise any (negative) societal impacts. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our method does not involve data or models that might lead to intended misuse. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have made acknowledgements to the assets used in our work. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We heve prepared the documentation of our codes for future reproduction, which is released alongside the codes at https://github.com/markywg/transagent. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects 25 Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
    ]
}