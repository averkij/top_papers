{
    "paper_title": "A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs",
    "authors": [
        "Bowen Zhi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in a physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with a static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields a gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47\\%. The balancing controller also improves the re-stabilization within gait cycles and achieves a more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that a decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 7 0 0 0 . 2 1 5 2 : r a"
        },
        {
            "title": "A Hierarchical Framework for Humanoid\nLocomotion with Supernumerary Limbs",
            "content": "AUTHOR: BOWEN ZHI September 10,"
        },
        {
            "title": "Abstract",
            "content": "The integration of Supernumerary Limbs (SLs) on humanoid robots poses significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of walking gait for Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47%. The balancing controller also improves the re-stabilization within gait cycles and achieves more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs. i"
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . 1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.1 Control of Humanoid Locomotion . . . . . . . . . . . . . . . . . . . . 1.1.2 Balance Augmentation with Supernumerary Limbs . . . . . . . . . . . Positioning this Research . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.3 1.2 Contribution and Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Methods . . ."
        },
        {
            "title": "2.1 System Architecture .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 Robot Model and Simulation Environment . . . . . . . . . . . . . . . . 2.1.2 Hierarchical Control Framework . . . . . . . . . . . . . . . . . . . . . 2.2 Low-Level Control: DRL for Locomotion . . . . . . . . . . . . . . . . . . . . Proximal Policy Optimization (PPO) . . . . . . . . . . . . . . . . . . . 2.2.1 2.2.2 Imitation Learning and Reward Function . . . . . . . . . . . . . . . . . 2.2.3 Curriculum Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 High-Level Control: Model-Based Dynamic Balancing . . . . . . . . . . . . . 2.3.1 State Estimation for Balancing . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Balancing Controller and Control Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.4 Experimental Evaluation .",
            "content": "3 Results"
        },
        {
            "title": "3.1 DRL Training Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Center of Mass Trajectory Analysis . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 Analysis of Dynamic Balance Modulation . . . . . . . . . . . . . . . . . . . .\n3.4 Exploratory Analysis of Gait Coordination . . . . . . . . . . . . . . . . . . . .",
            "content": "4 Discussion Interpretation of Key Findings 4.1 4.2 Limitations of the Study . . 4.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 Conclusion",
            "content": "ii 1 1 1 2 2 3 4 4 4 5 6 6 7 7 8 8 9 10 12 12 12 14 16 17 17 18 18 Chapter 1:"
        },
        {
            "title": "Introduction",
            "content": "The pursuit of creating versatile humanoid robots capable of operating effectively in humancentric environments represents grand challenge in robotics. key determinant of this versatility is the ability to maintain stable bipedal locomotion, task of significant engineering complexity due to the underactuated and inherently unstable dynamics of legged systems (Collins et al.; 2005). This challenge is magnified when humanoid platforms are augmented with additional functional components, such as Supernumerary Robotic Limbs (SLs). SLs promise to dramatically enhance robots capabilities, allowing it to perform complex manipulation, carrying, and support tasks that would otherwise be impossible (Parietti and Asada; 2014). However, the integration of heavy, articulated SLs, such as those capable of carrying payloads up to 30kg, introduces substantial and continuous dynamic perturbations to the main body, severely compromising the stability of the underlying locomotion."
        },
        {
            "title": "1.1 Related Work",
            "content": "The control of bipedal locomotion and the use of auxiliary limbs for stability have been active areas of research, each with rich history and distinct methodologies."
        },
        {
            "title": "1.1.1 Control of Humanoid Locomotion",
            "content": "Traditional approaches to humanoid walking have predominantly relied on model-based control, with the Zero-Moment Point (ZMP) criterion being cornerstone for decades (Kajita et al.; 2003). These methods generate dynamically stable trajectories by ensuring the ZMP remains within the support polygon of the feet. While effective in structured environments, ZMP-based controllers often struggle with uneven terrain and in the presence of unforeseen external disturbances, as they rely heavily on precise models and predefined contact schedules. More recently, Deep Reinforcement Learning (DRL) has emerged as powerful paradigm for generating sophisticated locomotion gaits without requiring an explicit dynamics model. DRL has proven highly effective for controlling complex legged systems, enabling skills like quadrupedal locomotion over challenging terrain (Lee et al.; 2020). key enabler for this progress has been the development of high-fidelity physics simulators, such as MuJoCo, which provide the vast amounts of data needed for training these policies (Todorov et al.; 2012). By leveraging techniques like imitation learning, as demonstrated by the DeepMimic framework, DRL agents can learn complex, physics-based character skills from motion capture data, producing natural and dynamic movements (Peng et al.; 2018; Al-Hafez et al.; 2023). Algorithms 1 like Proximal Policy Optimization (PPO) have become standard for these tasks due to their stability and data efficiency (Schulman et al.; 2017; Melo and Máximo; 2019). Despite these advances, applying single, monolithic DRL policy to simultaneously control both the humanoids locomotion and the complex balancing manoeuvres of heavy SLs is fraught with difficulty. The vast increase in the state-action space and the potentially conflicting objectives of maintaining stable gait while performing arm tasks can lead to intractable training processes. Hierarchical reinforcement learning approaches suggest that decomposing such complex problems can be more effective (Nachum et al.; 2018), but seamless integration remains challenge."
        },
        {
            "title": "1.1.2 Balance Augmentation with Supernumerary Limbs",
            "content": "The concept of using extra limbs for physical augmentation has been explored through various specialised strategies. One prominent approach involves using the limbs as static anchors, bracing against fixed points in the environment to provide stable base of support. This technique is particularly effective in quasi-static scenarios like aircraft fuselage assembly, where the robot can establish firm connection to its surroundings (Parietti and Asada; 2014). For dynamic scenarios, research has focused on developing specialized appendages. These include robotic tails that modulate the bodys angular momentum to counteract instabilities (Abeywardena and Farkhatdinov; 2023) and extra robotic legs that create wider, more stable support base during locomotion with load carriage (Hao et al.; 2020). While these solutions are highly effective for their specific intended functions, generalizable strategy for maintaining dynamic walking stability in the presence of continuous, high-magnitude disturbances from multi-purpose SL arms such as the large, time-varying torques generated when rapidly repositioning heavy payload remains an open challenge. The unpredictable, task-driven movements of these arms create highly complex control problem that cannot be fully addressed by appendages with limited degrees of freedom or by static bracing strategies, highlighting the need for novel control frameworks (Verdel et al.; 2024)."
        },
        {
            "title": "1.1.3 Positioning this Research",
            "content": "This research contributes to the field by presenting novel solution to this complex problem. Previous work on SLs for balance has often focused on static bracing or on specialized, nonanthropomorphic appendages. This project distinguishes itself by leveraging general-purpose, anthropomorphic robotic arms for dynamic balance assistance during locomotion. This is significant because it implies that the same limbs used for manipulation tasks could dually function as active balancing aids, greatly enhancing the versatility and utility of such system. The methodology also aligns with modern trend in robotics that combines the strengths of learning-based and model-based control. While DRL provides powerful tool for learning complex locomotion policies that are difficult to hand-engineer (Peng et al.; 2018), the modelbased controller offers reliability for the well-defined task of CoM regulation. This hybrid 2 approach demonstrates practical path forward for tackling multifaceted robotics challenges."
        },
        {
            "title": "1.2 Contribution and Objectives",
            "content": "This research posits that decoupled, hierarchical control strategy can overcome these limitations. By separating the complex problem into two more manageable layersa low-level locomotion policy and high-level balancing controllerit is possible to achieve stable walking for humanoid equipped with heavy SLs. This modular approach is common in complex robotic systems, allowing for the independent development and optimization of each component, leading to more effective overall system (Hammam et al.; 2010). The overall aim of this project is to develop and validate novel hierarchical control framework that enables humanoid robot to maintain stable bipedal locomotion while managing the significant self-induced dynamic effects imposed by heavy, articulated supernumerary limbs. To achieve this aim, the following objectives have been established: 1. To develop walking gait for the Unitree H1 humanoid using imitation-guided Deep Reinforcement Learning, establishing stable mobile base. 2. To enhance the resilience of the learned locomotion policy by implementing curriculum learning strategy that progressively introduces the mass and dynamic poses of the SLs during training. 3. To design and implement an independent, model-based dynamic balancing controller that actively utilizes the SLs to counteract instabilities by modulating their configuration based on real-time Center of Mass (CoM) and Center of Support (CoS) feedback. 4. To quantitatively evaluate the frameworks performance in high-fidelity physics simulation across three interdependent and increasingly complex scenarios, thereby validating the effectiveness of the decoupled control strategy: (a) Baseline Walking: Establish performance benchmark with an unladen humanoid to define the characteristics of an ideal, unperturbed gait. (b) Static Payload: Assess the DRL policys ability to manage constant, challenging load by having the humanoid walk with the SLs locked in fixed pose. (c) Dynamic Balancing: Evaluate the full hierarchical framework by activating the high-level controller to provide active balance modulation, and compare its performance against the other two scenarios. 3 Chapter 2: Methods This chapter details the hierarchical control framework, simulation environment, learning algorithms, and experimental protocols developed to achieve humanoid locomotion with supernumerary limbs (SLs). The methodology is divided into three primary components: the low-level locomotion policy trained via Deep Reinforcement Learning (DRL), the high-level model-based controller for dynamic balancing, and the experimental setup for quantitative evaluation."
        },
        {
            "title": "2.1.1 Robot Model and Simulation Environment",
            "content": "The study was conducted within high-fidelity physics simulation environment to facilitate rapid prototyping and safe, extensive training. The core of the simulation is the MuJoCo (Multi-Joint Dynamics with Contact) physics engine, renowned for its efficiency and accuracy in simulating complex robotic systems (Todorov et al.; 2012). The JAX framework was utilized for all computations to leverage its just-in-time (JIT) compilation and automatic differentiation capabilities, enabling high-performance training. Figure 2.1: The composite robot model used in the simulation, illustrating (a) the Unitree H1 humanoid base, (b) the backpack-mounted Kinova Gen3 SLs, and (c) the 2F-85 grippers. The robotic platform, depicted in Figure 2.1, is composite model consisting of: Unitree H1 Humanoid: full-sized humanoid robot serving as the mobile base. Its 4 kinematic and dynamic properties are based on the manufacturers specifications. Supernumerary Limbs (SLs): Two Kinova Gen3 robotic arms are mounted on custom backpack attached to the H1s torso. Each arm is equipped with 2F-85 gripper. With total of 26 actuated degrees of freedom (DoF): H1 Humanoid (12 DoF): The mobile base, controlling the legs and torso. SLs (14 DoF): Two 7-DoF Kinova Gen3 arms mounted on backpack. critical detail of the simulation is that the actuator torques were not saturated to their physical limits. This simplification, common practice in early-stage simulation studies, allows for focusing on the control algorithms performance without being constrained by hardwarespecific limitations. This is further discussed as key aspect of the sim-to-real challenge in Section 4.2."
        },
        {
            "title": "2.1.2 Hierarchical Control Framework",
            "content": "A decoupled, hierarchical control strategy was adopted to manage the complexity of simultaneous locomotion and balancing. This framework separates the control problem into two distinct layers, as detailed in Figure 2.2: Training Progress Tglobal Curriculum Scheduler Updates task difficulty based on training progress. Update Task Parameters Ti Curriculum Logic: Phase 1: mlow, parm psimple ... Phase N: mhigh, parm pcomplex Training Stage at Curriculum Ti Interaction (st, at, Rt) Simulation Environment Parameterized by Ti = (mi, parm ). Imitation Learning Inner Loop Agent learns policy πθ for the current task. Figure 2.2: The overall training framework, illustrating the hierarchical structure. The outer loop consists of Curriculum Scheduler that adjusts task difficulty (e.g., payload mass mi and arm pose parm ) based on the global training progress (Tglobal). The inner loop is standard Imitation Learning process where the DRL agent interacts with the environment to learn policy for the current difficulty level. 1. Low-Level Locomotion Policy: DRL-based policy is responsible for generating the fundamental walking gait. It controls the actuators of the humanoids legs and lower torso, aiming to produce stable and efficient locomotion by imitating reference motion. 2. High-Level Balancing Controller: model-based controller is dedicated to active stability augmentation. It overrides the control of the SLs, dynamically adjusting their pose to counteract perturbations and maintain the overall balance of the system. This decoupled approach allows the DRL agent to focus solely on mastering the core locomotion task, while the specialized balancing controller handles the complex dynamics introduced by the SLs."
        },
        {
            "title": "2.2 Low-Level Control: DRL for Locomotion",
            "content": "The foundation of the robots mobility is walking policy trained using imitation learning, based on the DeepMimic framework (Peng et al.; 2018). The detailed logic of this inner training loop is shown in Figure 2.3. Objective: (cid:2) t=0 γtRt maxθ Eτπθ (cid:3) Policy Network Actor-Critic πθ (atst) Update θ PPO Optimizer st at Rt Simulation Environment (Unitree H1 in MuJoCo) st Reward Calculation (Mimic Reward Function) Reward Function: Rt = wk exp(αk fk(st) fk(s )2) Expert Trajectory Data Dexpert = {τ } Figure 2.3: The inner Imitation Learning loop. The Policy Network generates an action at based on the current state st. The Simulation Environment executes this action and returns the next state. The Reward Calculation module compares the agents state st to the Expert Trajectory state to compute reward Rt. Finally, the PPO Optimizer uses this reward to update the policys parameters θ ."
        },
        {
            "title": "2.2.1 Proximal Policy Optimization (PPO)",
            "content": "The policy was trained using the Proximal Policy Optimization (PPO) algorithm, state-of-theart DRL method known for its stability and sample efficiency (Schulman et al.; 2017). PPO is an actor-critic algorithm that optimizes clipped surrogate objective function to prevent excessively large policy updates. The objective for the policy network πθ is to maximize the expected 6 total discounted reward: Eτπθ max θ (cid:35) γtRt (cid:34) t=0 (2.1) where τ is trajectory, Rt is the reward at timestep t, and γ is the discount factor, set to 0.99 in this work. The policy network architecture consisted of three hidden layers with [1024, 512, 256] neurons and Tanh activation function."
        },
        {
            "title": "2.2.2 Imitation Learning and Reward Function",
            "content": "To guide the learning process towards natural, bipedal gait, an expert trajectory for walking was sourced from the Ubisoft La Forge Animation Dataset (LAFAN1). The agent is rewarded for mimicking this reference motion. The total reward signal Rt is weighted sum of several components, designed with the principle of \"Survive First, Imitate Later\": Rt = wsurvivalRsurvival + wstabilityRstability + wimitationRimitation (2.2) The imitation reward, Rimitation, further breaks down into components rewarding the similarity of joint positions, velocities, and key body site orientations relative to the expert data. The weights, detailed in Table 2.1, were heavily skewed towards survival and stability, granting the agent the freedom to deviate from the reference motion when necessary to avoid falling, particularly under the influence of the SLs. Table 2.1: Reward function component weights. Component Imitation Terms Joint Position Match Joint Velocity Match Relative Site Position Match Relative Site Quaternion Match Relative Site Velocity Match Stability Terms Survival Reward (per step) Stability (penalty for falling) Weight 0.05 0.01 0.1 0.05 0.01 5.0 4."
        },
        {
            "title": "2.2.3 Curriculum Learning",
            "content": "To enable the locomotion policy to adapt to the significant disturbances from the SLs, curriculum learning strategy was implemented. This approach gradually increases the difficulty of the task as the agents performance improves, preventing the agent from being overwhelmed in the early stages of training. The curriculum was structured across the total 500 million training timesteps and involved two parallel difficulty ramps: 1. Payload Randomization: The mass of the SLs payload was gradually increased. The training started with negligible payload, progressing in stages to the final target range 7 of 19kg to 30kg. This allowed the agent to learn to compensate for the increasing inertial and gravitational effects. 2. Arm Pose Curriculum: The static pose of the SLs was incrementally moved from neutral, close-to-the-body position to challenging forward-reaching posture. This curriculum was divided into four phases, each moving the arms step closer to their final target configuration, forcing the locomotion policy to adapt to the shifting center of mass."
        },
        {
            "title": "2.3 High-Level Control: Model-Based Dynamic Balancing",
            "content": "While the DRL policy learns to walk, the high-level controller actively uses the SLs to provide dynamic stability. This controller operates independently of the DRL agent and is based on simplified rigid-body dynamics model. The logic for this controller is detailed in Figure 2.4. Trained Gait Policy πθ Humanoid Actions at SL Arm Torques τt Robot State st Active Balance Controller PD Controller τ = Kp(qtarget q) Kd Dynamic Target Generation target = qarm qarm exy base Karm exy Combined Control ut Error Calculation exy = CoMxy CoSxy Simulation Environment (MjxUnitreeH1_balance) CoM, CoS State Estimation CoSxy = pfoot xy is min where pfoot Figure 2.4: Detailed control logic for the Dynamic Balancing scenario. The DRL Gait Policy generates actions for the humanoids legs (at). In parallel, the Active Balance Controller uses the robots state (st) to estimate CoM and CoS, calculates balance error (exy), and generates compensatory torques for the SL arms (τt) via PD controller. These two control signals are combined and sent to the simulation environment."
        },
        {
            "title": "2.3.1 State Estimation for Balancing",
            "content": "The high-level controllers function relies on real-time estimation of the robots balance state. This is achieved using two key metrics calculated at each timestep: 1. Center of Mass (CoM): The total body CoM is calculated as the weighted average of the positions of all individual body links. 8 2. Center of Support (CoS): To achieve physically accurate stability reference, the Center of Support is calculated based on the ground contact forces and the support polygon. The calculation adapts to the phase of the gait: During the single-support phase, the CoS is defined as the geometric center of the stance foots contact area. During the double-support phase, the controller first calculates the Center of Pressure (CoP) for each foot using the measured ground reaction forces (Fz) and moments (Tx, Ty). The global CoS is then computed as the force-weighted average of the individual CoPs. This is formally expressed as: CoSxy = pstance_foot,xy CoPLFz,L+CoPRFz,R Fz,L+Fz,R if single support if double support (2.3) where the per-foot CoP is given by CoP = [Ty/Fz, Tx/Fz] in the foots local frame. This physically-grounded approach ensures that the stability target is accurately represented throughout the entire gait cycle, including the critical transitions between single and double support phases. During dynamic walking, controlled misalignment between the CoM and CoS is essential for forward propulsion. Therefore, the vector difference, dxy = CoMxy CoSxy, is not treated as static error to be nullified. Instead, it serves as dynamic stability indicator."
        },
        {
            "title": "2.3.2 Balancing Controller and Control Fusion",
            "content": "The balancing controller implements reactive strategy, using the dynamic stability indicator dxy to adjust the target joint angles of the two SLs. The objective is to move the arms in way that generates compensatory momentum, shifting the total body CoM back towards stable region relative to the CoS. The target arm pose, qarm target, is calculated by modulating constant, neutral base pose, qarm base, with the stability indicator. This base pose defines \"home\" configuration where the arms are held slightly forward and down, and it remains fixed throughout the dynamic balancing trials. The modulation is scaled by gain matrix Karm : target = qarm qarm base Karm dxy (2.4) This linear, heuristic control law was chosen for its computational simplicity and real-time feasibility. While an optimal target pose could be computed by solving non-linear, wholebody optimization problem, such an approach would be computationally expensive. The goal here is not to find single, perfect corrective pose, but to provide continuous, high-frequency dynamic damping, for which this proportional strategy proves effective. It is important to note 9 that the fixed pose used for the \"Static Payload\" experimental scenario is more challenging, forward-reaching posture, distinct from the neutral qarm base used here. Proportional-Derivative (PD) controller then calculates the required torques τ arm to drive the SL arm joints towards this dynamic target: τ arm = Kp(qarm target qarm current) Kd qarm current (2.5) The gain matrices for both the target modulation (Karm ) and the PD tracking controller (Kp, Kd) were determined empirically through an iterative tuning process within the simulation. Starting with low values, the gains were manually adjusted by observing the systems response. The objective was to achieve critically damped behavior, where the arms responded quickly and decisively to balance perturbations without introducing significant overshoot or oscillation that could further destabilize the humanoid. This manual tuning is standard practice for tuning low-level controllers where precise analytical model for gain selection is unavailable or impractical. These calculated arm torques are then fused with the output of the DRL policy. In each simulation step, the torques for the SL arm actuators are overridden by the output of the balancing controller (τ arm), while the torques for the humanoids leg and torso actuators are taken directly from the DRL policys action. This clean separation ensures that each controller operates within its designated domain."
        },
        {
            "title": "2.4 Experimental Evaluation",
            "content": "To validate the effectiveness of the hierarchical framework, quantitative analysis was performed across three distinct experimental scenarios. 1. Baseline Walking: The humanoid walks without the SL backpack, controlled solely by the DRL policy trained on base curriculum (no payload, neutral arms). This establishes the benchmark for an unperturbed gait. 2. Static Payload: The humanoid walks with the SLs locked in fixed, forward-reaching pose. The DRL policy used here was fully trained with the complete curriculum, but the high-level balancing controller is disabled. This isolates the effect of the learned policy against constant, challenging load. 3. Dynamic Balancing: The full hierarchical framework is active. The fully trained DRL policy controls locomotion while the high-level controller dynamically actuates the SLs for balance. Key performance metrics were defined to assess dynamic stability, including CoM trajectory similarity, stability recovery based on CoM-CoS distance, and bipedal coordination via Ground Reaction Force (GRF) analysis. The phase-plane analysis of Ground Reaction Forces, detailed in Section 3.4, involved fitting an ellipse to the data points for each scenario. This fitting was performed using Principal Component Analysis (PCA). The first principal component determines the direction of the major axis of the ellipse, representing the primary axis of variance in the coordinated forces. The \"Orientation Error\" was then calculated as the angle between this major axis and the ideal 135-degree anti-phase axis. These metrics are detailed further in the Results chapter. 11 Chapter 3: Results This chapter presents the results of the training process and the quantitative evaluation of the hierarchical control framework. The findings are organized into four sections: DRL training performance, Center of Mass (CoM) trajectory analysis, dynamic balance modulation, and an exploratory analysis of gait coordination."
        },
        {
            "title": "3.1 DRL Training Performance",
            "content": "The DRL agent was trained for total of 500 million environment steps. The curriculum learning strategy progressively increased the task difficulty by increasing the payload mass and adjusting the SL arm poses at intervals of 100 million steps. The agents learning progress was monitored via the mean episode return (task achievement) and mean episode length (stability). Figure 3.1 shows that both metrics trended consistently upwards, demonstrating the agents increasing performance. The periodic dips, particularly around the 100-million-step marks, correspond to the scheduled increases in curriculum difficulty. The agents ability to quickly recover and continue improving after each increase validates the effectiveness of the curriculum strategy in adapting the policy to the challenging dynamics of the SLs. Figure 3.1: Training performance of the PPO agent over 500 million environment steps. (a) Mean Episode Return. (b) Mean Episode Length. The agents consistent improvement, punctuated by temporary dips aligned with curriculum changes, demonstrates successful adaptation."
        },
        {
            "title": "3.2 Center of Mass Trajectory Analysis",
            "content": "To assess how the different control strategies affected the overall walking pattern, the trajectory of the robots Center of Mass (CoM) was recorded for each of the three experimental scenarios. The \"Baseline Walking\" scenario serves as the reference for an ideal, unperturbed gait. To 12 compare the \"Static Payload\" and \"Dynamic Balancing\" scenarios against this baseline, their similarity was quantified using the Dynamic Time Warping (DTW) distance. DTW is metric for measuring similarity between two temporal sequences that may vary in speed, providing single value where lower number indicates higher degree of similarity in the dynamic pattern. Figure 3.2: CoM trajectories for the three scenarios. DTW distances are relative to the \"Baseline Walking\" trajectory. The lower DTW score for \"Dynamic Balancing\" indicates its dynamic pattern and rhythm are better preserved. The analysis, presented in Figure 3.2, reveals that the \"Dynamic Balancing\" strategy results in CoM trajectory with DTW distance of 65.54 from the baseline, significant reduction of approximately 47% compared to the \"Static Payload\" trajectorys distance of 123.71. This lower DTW score indicates that the active balancing controller is effective at mitigating the high-frequency disturbances introduced by the SLs. Although the absolute paths of all trajectories diverge over the course of the trial, particularly in the transverse plane, the dynamic balancing controller better preserves the fundamental dynamic characteristics and rhythm of the original unladen gait. In contrast, the static payload introduces more severe, uncompensated disturbances, causing greater deviation in both the CoMs dynamic pattern and its final position."
        },
        {
            "title": "3.3 Analysis of Dynamic Balance Modulation",
            "content": "Walking is inherently process of controlled instability, requiring continuous modulation of the bodys balance. key indicator of this dynamic state is the distance between the horizontal projection of the Center of Mass and the Center of Support (CoM-CoS distance). This distance naturally oscillates throughout the gait cycle: it increases during the swing phase as the body \"falls\" forward and decreases after foot-strike as the body re-stabilizes over the new support foot. The controllers objective is not to eliminate these crucial oscillations, but to effectively manage their magnitude in the presence of disturbances. Figure 3.3 provides qualitative overview of these oscillations. key observation is that the frequency of the double-support phase (shaded regions) is not perfectly constant, even in the baseline scenario. This is an emergent behavior of the DRL policy. Unlike traditional controller with fixed cadence, the DRL agents primary objective is stability. It continuously makes subtle timing adjustments to the gait cycle to maintain balance, which results in stable but not perfectly rhythmic walking pattern. This also explains the slight drift in the baselines oscillation amplitude, which is an artifact of this adaptive control strategy, not an intended circular path. Figure 3.3: qualitative overview of the CoM-CoS distance oscillations over 10-second trial. For the baseline, shaded regions indicate double-support phases, illustrating the cyclical nature of balance modulation. 14 For rigorous comparison, each gait cycle (from one peak of CoM-CoS distance to the next) was isolated and time-normalized, as shown in Figure 3.4 (Right). This ensures that corresponding phases of the gait cycle are directly compared across all scenarios. To quantify the effectiveness of re-stabilization within each cycle, we define new metric: the Gait Cycle Stability Minimum (GCSM). This metric represents the minimum CoM-CoS distance achieved during the recovery phase of the i-th gait cycle: GCSMi = min tpeak,i<t<tpeak,i+1 D(t) (3.1) where D(t) is the CoM-CoS distance at time t, and tpeak,i is the time of the i-th peak. The GCSM quantifies how successfully the system arrests the forward fall and achieves stable state over the stance foot. lower GCSM value indicates more effective and complete recovery. This metric was chosen over an average distance because it specifically targets the critical moment of maximum recovery, providing more direct measure of the controllers ability to handle the cycles peak instability. Figure 3.4: Quantitative analysis of dynamic balance modulation. Left: Boxplot of the GCSM. Lower values indicate more complete re-stabilization. Diamond markers indicate the mean. Right: Average CoM-CoS distance over normalized recovery cycle (from peak instability to peak recovery). The method for normalizing and averaging individual cycles is described in the text. The aggregated results, shown in Figure 3.4 (Left), highlight clear performance benefit for the \"Dynamic Balancing\" scenario. The boxplot of GCSM values (left) shows that \"Dynamic Balancing\" achieves the lowest median value, signifying consistently more effective re-stabilization after each step. Furthermore, the plot of the averaged, normalized recovery cycle dynamics (right) confirms this finding, showing that the \"Dynamic Balancing\" curve reaches significantly lower trough than the other scenarios. This demonstrates that the active SL controller 15 enables the robot to better manage the dynamic fluctuations inherent in walking, leading to more stable state at the most critical point of recovery within each gait cycle."
        },
        {
            "title": "3.4 Exploratory Analysis of Gait Coordination",
            "content": "To explore how the different loading conditions influenced the coordination of the bipedal gait, phase-plane analysis of the vertical Ground Reaction Forces (GRF) was conducted. In an ideal gait, the GRFs of the left and right feet exhibit perfect anti-phase relationship, resulting in data distribution oriented at 135. The deviation from this ideal, termed \"Orientation Error,\" can serve as an indicator of gait coordination. It is important to note that the baseline gait, learned through imitation, is not perfectly optimal and exhibits benchmark Orientation Error of 8.37. This inherent sub-optimality may stem from imperfections in the reference motion capture data or from the DRL policy prioritizing stability over perfect mimicry. Therefore, this analysis focuses not on achieving perfect score, but on observing the relative changes in coordination across the different experimental conditions, as shown in Figure 3.5. Figure 3.5: Phase-plane analysis of vertical GRF. \"Orientation Error\" measures the deviation of the ellipses major axis from the ideal 135 anti-phase axis. This plot illustrates relative differences in bipedal coordination. Interestingly, the addition of \"Static Payload\" was observed to reduce the error to 5.59. plausible physical explanation is that the mass of the SLs, held in fixed sloping-downward pose, lowered the robots overall center of mass. lower CoM inherently increases the systems passive stability, which may have allowed the DRL policy to execute more coordinated gait pattern under this constant, predictable load. The \"Dynamic Balancing\" scenario achieved the lowest error among the three conditions at 1.95. This trend suggests that by offloading the primary balancing task to the high-level controller, the low-level locomotion policy can better adhere to its learned coordination pattern. However, while this result is consistent with the studys main findings, more rigorous statistical analysis would be required to confirm the significance of these coordination differences. 16 Chapter 4: Discussion This study successfully developed and validated hierarchical control framework for humanoid robot equipped with supernumerary limbs (SLs). The results presented in the previous chapter demonstrate that by decoupling the control of locomotion and dynamic balancing, the system can achieve stable walking even when subjected to the significant dynamic perturbations of the SLs. This chapter interprets these findings, discusses their implications in the context of existing literature, acknowledges the limitations of the current work, and proposes directions for future research. 4."
        },
        {
            "title": "Interpretation of Key Findings",
            "content": "The quantitative results from the three experimental scenarios provide valuable insights into the performance of the proposed hierarchical approach, highlighting both its benefits and the complexities of the control problem. First, the successful training of the low-level locomotion policy confirms the viability of using imitation learning with carefully structured curriculum. The learning curves (Figure 3.1) show that the DRL agent developed policy capable of handling the significant, constant load imposed by the SLs during evaluation. This capability was achieved through curriculum that adapted the policy by progressively introducing challenges during training, namely increasing payload mass and more destabilizing arm poses. This adaptability during training is the foundation upon which the entire hierarchical system is built. Second, the analysis of the Center of Mass trajectory (Figure 3.2) clarifies the role of the dynamic balancing controller. The ideal objective for such system is to maintain stability for any given trajectory. While the controller does not force the robots trajectory to perfectly match the unladen baseline, the significantly lower DTW distance under active balancing is key finding. It suggests that the high-level controller helps preserve the fundamental dynamic characteristics and rhythm of the learned gait better than in the static payload case. The data indicates that the active controller partially mitigates the corrupting influence of the SLs on the gait pattern being executed by the low-level DRL policy. Third, the analysis of dynamic balance modulation (Figure 3.4) provides the most direct evidence of the controllers function in enhancing intra-cycle stability. The lower Gait Cycle Stability Minimum (GCSM) achieved in the dynamic balancing scenario demonstrates how the SLs can be transformed from purely destabilizing liability into functional asset for active restabilization within each step. They are actively used to achieve more stable state at the most critical point of the gait cycle, showcasing clear functional benefit for augmenting humanoid 17 with actively controlled limbs. Finally, the exploratory phase-plane analysis of Ground Reaction Forces (Figure 3.5) offers tentative insight into possible synergistic relationship between the control layers. The trend towards lower Orientation Error suggests that by offloading the primary balancing task to the high-level SL controller, the low-level locomotion policy is able to better adhere to its learned coordination pattern. This finding, while requiring further statistical validation, supports the core hypothesis of the decoupled framework: that separating control responsibilities can lead to measurable performance benefits in specific aspects of the task."
        },
        {
            "title": "4.2 Limitations of the Study",
            "content": "This study has several limitations that must be acknowledged. First, the most significant limitation is the reliance on simulation environment. While MuJoCo provides high-fidelity physics, the infamous \"sim-to-real\" gap remains substantial hurdle. critical aspect of this gap is that the simulation did not enforce the torque limits of the Unitree H1s motors. It is therefore uncertain whether the physical robot could support the static weight of the SL payload or generate the required compensatory torques without violating hardware constraints. Transferring this system to the physical hardware would require not only substantial effort in domain randomization but also thorough validation of the required actuator torques against the robots physical capabilities. Furthermore, the current framework treats balancing as the sole function of the SLs. The reason for not integrating manipulation tasks was to manage the projects scope and focus on solving the foundational problem of maintaining stability. Integrating task-space objectives for the arms would introduce complex multi-objective optimization problem, requiring the controller to continuously arbitrate between the often-competing demands of balance maintenance and task execution. While this integration is the ultimate goal for such system, it was deemed beyond the scope of this initial investigation, which aimed to first establish viable baseline for stable locomotion under heavy, dynamic loads. Finally, the scope of the analysis itself represents limitation. While the results effectively demonstrate that the balancing controller improves stability (e.g., by reducing GCSM), the study did not deeply investigate how it achieves this at the joint level. detailed, moment-to-moment analysis of the SLs emergent motion strategies was not performed, limiting the current depth of interpretation regarding the controllers specific corrective actions."
        },
        {
            "title": "4.3 Future Work",
            "content": "The findings and limitations of this project suggest several promising avenues for future research. The most immediate and critical next step is to address the sim-to-real transfer challenge. This 18 would involve deploying the hierarchical controller on the physical robotic platform, which requires thorough validation of actuator torque requirements against the hardwares physical limits. Prior to developing more complex controller, deeper analysis of the current systems emergent behavior is warranted. Future work should visualize the SLs joint angle trajectories, time-locked to the humanoids gait cycle. Correlating how the arms move angularly with the CoM-CoS distance oscillations would provide crucial insights into how the reactive controller impacts the correctness of the gait on moment-to-moment basis. This analysis would help identify the specific strategies the arms employ to provide stabilization and reveal the limitations of the current heuristic approach. Finally, the insights from this detailed motion analysis would then directly inform the development of more sophisticated, unified controller. Such controller could use optimization-based techniques to simultaneously solve for manipulation task goals and balance constraints, treating the SLs contribution to stability as component within whole-body control objective. This would move the system from decoupled hierarchy to fully integrated control architecture, realizing the full potential of humanoid robot augmented with functional supernumerary limbs. 19 Chapter 5: Conclusion This thesis presented novel hierarchical control framework to address the significant challenge of maintaining stable bipedal locomotion for humanoid robot augmented with heavy, actuated supernumerary limbs (SLs). The core of this work was the strategic decoupling of control responsibilities, in which low-level policy trained with Deep Reinforcement Learning (DRL) managed the fundamental walking gait, while high-level model-based controller dynamically utilized SLs for active balance. Through comprehensive set of experiments conducted in physics-based simulation, this study provided valuable insights into the performance of this decoupled approach. The key contributions and findings are summarized as follows: 1. locomotion policy was successfully trained using imitation learning in conjunction with curriculum strategy. This policy proved capable of adapting to the substantial and progressively increasing dynamic perturbations imposed by the SLs. 2. The active balancing controller demonstrated its ability to preserve the underlying gait pattern more effectively than static payload condition. This was evidenced by significantly lower Dynamic Time Warping (DTW) distance between the robots CoM trajectory and the unperturbed baseline, indicating better preservation of the gaits dynamic characteristics. 3. The framework enhanced the robots ability to modulate its dynamic balance within each gait cycle. By actively using the SLs for re-stabilization, the system consistently achieved more stable state at the most critical point of recovery, transforming the limbs from simple payload into functional asset. 4. potential synergistic relationship between the control layers was observed. Exploratory analysis suggested that by offloading the primary balancing task, the locomotion policy was able to execute more coordinated bipedal pattern, though this finding requires further statistical validation. In conclusion, this work validates that decoupled, hierarchical strategy is viable method for managing the immense complexity of humanoid-SL system. The quantitative results show measurable improvements in gait pattern preservation and intra-cycle stability."
        },
        {
            "title": "Reference",
            "content": "Abeywardena, S. and Farkhatdinov, I. (2023). Towards enhanced stability of human stance with supernumerary robotic tail, IEEE Robotics and Automation Letters 8(9): 57435750. 2 Al-Hafez, F., Zhao, G., Peters, J. and Tateo, D. (2023). Locomujoco: comprehensive imitation learning benchmark for locomotion, arXiv preprint arXiv:2311.02496 . 1 Collins, S., Ruina, A., Tedrake, R. and Wisse, M. (2005). Efficient bipedal robots based on passivedynamic walkers, Science 307(5712): 10821085. Hammam, G. B., Orin, D. E. and Dariush, B. (2010). Whole-body humanoid control from upperbody task specifications, 2010 IEEE International Conference on Robotics and Automation, IEEE, pp. 33983405. 3 Hao, M., Zhang, J., Chen, K., Asada, H. and Fu, C. (2020). Supernumerary robotic limbs to assist human walking with load carriage, Journal of Mechanisms and Robotics 12(6): 061014. 2 Kajita, S., Kanehiro, F., Kaneko, K., Fujiwara, K., Harada, K., Yokoi, K. and Hirukawa, H. (2003). Biped walking pattern generation by using preview control of zero-moment point, 2003 IEEE international conference on robotics and automation (Cat. No. 03CH37422), Vol. 2, IEEE, pp. 16201626. 1 Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V. and Hutter, M. (2020). Learning quadrupedal locomotion over challenging terrain, Science robotics 5(47): eabc5986. Melo, L. C. and Máximo, M. R. O. A. (2019). Learning humanoid robot running skills through proximal policy optimization, 2019 Latin american robotics symposium (LARS), 2019 Brazilian symposium on robotics (SBR) and 2019 workshop on robotics in education (WRE), IEEE, pp. 3742. 2 Nachum, O., Gu, S. S., Lee, H. and Levine, S. (2018). Data-efficient hierarchical reinforcement learning, Advances in neural information processing systems 31. 2 Parietti, F. and Asada, H. H. (2014). Supernumerary robotic limbs for aircraft fuselage assembly: body stabilization and guidance by bracing, 2014 IEEE International Conference on Robotics and Automation (ICRA), IEEE, pp. 11761183. 1, 2 Peng, X. B., Abbeel, P., Levine, S. and Van de Panne, M. (2018). Deepmimic: Example-guided deep reinforcement learning of physics-based character skills, ACM Transactions On Graphics (TOG) 37(4): 1 14. 1, 2, 6 Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017). Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347 . 2, 6 Todorov, E., Erez, T. and Tassa, Y. (2012). Mujoco: physics engine for model-based control, 2012 IEEE/RSJ international conference on intelligent robots and systems, IEEE, pp. 50265033. 1, 4 Verdel, D., Eden, J., Cervantes-Culebro, H., Mehring, C., Pinardi, M., Di Pino, G., Souères, P. and Burdet, E. (2024). predictive coding framework for safe and versatile control of supernumerary robotic limbs."
        }
    ],
    "affiliations": []
}