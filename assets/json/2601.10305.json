{
    "paper_title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "authors": [
        "Hengyu Shen",
        "Tiancheng Gu",
        "Bin Qin",
        "Lan Wu",
        "Yuling Wu",
        "Shuo Tan",
        "Zelong Sun",
        "Jun Wang",
        "Nan Wu",
        "Xiang An",
        "Weidong Cai",
        "Ziyong Feng",
        "Kaicheng Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 5 0 3 0 1 . 1 0 6 2 : r a"
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab DanQing: An Up-to-Date Large-Scale Chinese VisionLanguage Pre-training Dataset Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang DanQing Team, Glint Lab Abstract Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop comprehensive pipeline for constructing high-quality Chinese cross-modal dataset. As result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 20242025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license. Webpage GitHub ModelScope HuggingFace https://deepglint.github.io/DanQing https://github.com/deepglint/DanQing https://www.modelscope.cn/datasets/deepglint/DanQing https://huggingface.co/datasets/DeepGlint-AI/DanQing100M"
        },
        {
            "title": "Introduction",
            "content": "The exponential growth of web-scale data provides robust foundation for contrastive visionlanguage representation learning [23]. By aligning dual-encoder architectures through imagetext correspondence, frameworks such as Contrastive Language-Image Pre-training (CLIP) [51] demonstrate exceptional generalization across broad spectrum of downstream tasks, including image captioning [46; 37; 73], object detection [25; 38; 81], semantic segmentation [36; 53; 69], and cross-modal retrieval [77; 22; 24; 30; 80]. Given the efficacy of CLIP, this promising paradigm has garnered significant attention from both industry and academia as potential pathway toward next-generation foundational AI models [20; 18]. The success of Vision-Language Pre-training (VLP) is primarily driven by the synergy between advanced modeling techniques and data scale. On one hand, state-of-the-art architectures such as ViT [13] and BERT [12], coupled with training paradigms utilizing InfoNCE [51] or Sigmoidbased [75] contrastive losses, have significantly enhanced the ability of dual-tower models to learn Equal Contribution. Team Leader. Project Leader."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab robust and semantic-rich embeddings. On the other hand, the availability of large-scale, up-todate datasets, such as LAION [55] and YFCC [61], has provided crucial foundation for effective model training. However, while English datasets continue to expand, the development of Chinese image-text datasets has stagnated. Notably, the most recent dataset, Zero [66], has been introduced over three years ago. As result, despite the recognized importance of data scale, the Chinese multimodal domain remains limited. Most existing Chinese image-text resources [20; 43; 66] are either outdated or face significant accessibility challenges, such as high proportion of invalid image URLs. This scarcity of data inevitably constrains model performance and hinders progress in Chinese multimodal representation learning. Size Year Dataset Success Rate Language Availability English English English Yes Yes Yes Yes Yes Yes Yes Yes English English English English 60% 60% - - 70% - - - 3.1M 12M 12M 11.5M 100M 700M 400M 100M 2018 CC3M [56] 2021 CC12M [6] 2021 RedCaps [11] 2021 Multilingual WIT [58] 2014 YFCC100M [61] COYO [4] 2022 LAION-400M [54] 2021 2025 RealSyn [23] To bridge the gap and advance Chinese multi-modal representation learning, we present DanQing, containing dataset high-quality nearly 100 million Chinese imagetext pairs collected after 2024. We begin with an initial corpus of approximately 1 billion pairs and implement rigorous multi-stage filtration pipeline  (Fig. 1)  . First, we apply coarse-grained filtering based on safety, text length, and source reliability, retaining 45.4% of the raw data. Next, we conduct fine-grained filtering along four dimensions: structure, quality, information density, and safety, at both the image and text levels. Leveraging parallel batch processing for computational efficiency, we filter out approximately 80% of the candidate pairs at this stage. We further provide an in-depth exploration and comprehensive analysis of DanQing, demonstrating that it surpasses existing large-scale Chinese datasets in both quality and temporal relevance. Moreover, CLIP-based models pre-trained on DanQing consistently achieve state-of-the-art performance on downstream tasks such as zero-shot classification and cross-modal retrieval, and significantly improve the Chinese multimodal understanding capabilities of LLaVA-style MLLMs. The main contributions of this paper are summarized as follows: Product1M [76] WudaoMM [74] M6-Corpus [40] Wukong [20] TaiSu [43] Zero [66] DanQing Table 1: Overview of existing VLP datasets. Chinese Chinese Chinese Chinese Chinese Chinese Chinese 1M 5M 60.5M 100M 166M 250M 100M - - - 85% 100% 60% 100% 2021 2022 2021 2022 2022 2022 2025 Yes Yes No Yes Yes Yes Yes We develop an effective data filtering pipeline tailored for processing large-scale Chinese imagetext pairs obtained from the Internet. We release an up-to-date large-scale Chinese image-text dataset DanQing, which comprises nearly 100 million Chinese image-text pairs collected after 2024. We conduct extensive experiments across multiple downstream tasks to demonstrate the effectiveness and scalability of DanQing."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Vision-Language Pretraining As seminal work in vision-language pretraining, CLIP [51] has demonstrated exceptional zero-shot recognition and transfer capabilities. Building on this paradigm, recent studies have introduced various enhancements [72; 21; 65; 26; 27]. For instance, SLIP [47] integrates self-supervised learning with image-text pretraining to improve representation quality. To mitigate the impact of noisy data, ALIP [71] employs gating mechanism for dynamic sample reweighting. DFN [15] constructs new data filtering networks that induce state-of-the-art image-text datasets. MetaCLIP [68] and MetaCLIP2 [8] leverage metadata derived from CLIPs semantic concepts to curate balanced subsets from raw data pools. To further scale up the batch size, SigLIP [75] and SigLIP2 [62] adopt sigmoidbased loss to replace the standard softmax, thereby eliminating the need for global normalization."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab In the context of Chinese vision-language pretraining, ChineseCLIP [70] introduces two-stage framework comprising locked-image tuning followed by contrastive tuning. Similarly, R2D2 [66] enhances representation learning through preranking-ranking strategy combined with bidirectional distillation. Despite these advances, progress in Chinese vision-language pretraining remains limited due to the scarcity of large-scale Chinese image-text datasets. 2.2 Large-Scale Image-Text Dataset The impressive performance of CLIP [51] across downstream tasks is primarily attributed to the availability of massive, high-quality image-text data. To further advance model capabilities, numerous large-scale image-text pair datasets have been introduced in recent years (Tab. 1). The YFCC100M [61] dataset provides comprehensive record of photo and video sharing trends on Flickr from its inception in 2004 through early 2014. LAION400M [54] comprises 400 million imagetext pairs sourced from Common Crawl and has become standard benchmark for vision-language pretraining. COYO-700M [4] collects approximately 10 billion image-alt-text pairs from HTML documents in Common Crawl (from October 2020 to August 2021), employing efficient imageand text-level filtering to remove uninformative pairs at minimal cost. To support data filtering research and benchmarking, DataComp [18] assembled pool of 12.8 billion image-text pairs for competition tracks, model training, and evaluation. However, these datasets are predominantly based on English image-text pairs, while large-scale Chinese image-text datasets remain scarce. To address this gap, the Wukong [20] dataset, comprising 100 million Chinese image-text pairs collected from the web, has been released. Taisu [43] further advances this effort by introducing an automatic filtering framework, resulting in large-scale, high-quality Chinese multimodal dataset containing approximately 166 million images and 219 million Chinese captions. Leveraging user click-through rates and diverse textual information for each image, the Zero [66] dataset offers 250 million images and 750 million corresponding Chinese texts, significantly advancing resources for Chinese vision-language pretraining. Despite these advancements, the scale of existing Chinese image-text datasets remains limited, and their quality requires further improvement."
        },
        {
            "title": "3 DanQing Dataset",
            "content": "3.1 Training Objective of DanQing The DanQing dataset aims to enhance the Chinese embedding capabilities of CLIP-style models. To validate the efficacy of DanQing, we first use the image encoder fv() and text encoder ft() to map the image and text into normalized embeddings v, Rd. While the standard CLIP objective [51] aligns modalities via symmetric cross-entropy loss over batch of image-text pairs, its softmax normalization imposes computational bottleneck at scale. To address this, we employ the SigLIP [62] objective, which reformulates alignment as independent binary classification tasks using sigmoid loss: = i,j (cid:2)I i=j log σ(sij) + i=j log(1 σ(sij))(cid:3) , where sij = (vi tj)/τ + is the scaled similarity with bias b. 3.2 Curation of DanQing Data Collection. To curate high-quality Chinese vision-language dataset, we first collect raw image-text pairs from the Common Crawl (20242025). The data collection is partitioned into seven batches and processed in parallel to ensure computational efficiency. By filtering for the zho language tag, we obtain an initial pool of approximately 1.05 billion pairs. To mitigate the inherent noise in web-scale data, we implement coarse-grained filtering pipeline based on three criteria: ① Content Safety: lightweight 1M-parameter binary classifier [1] filters unsafe content; ② Textual Constraints: only segments containing 5 to 60 words are retained; and ③ Source Reliability: pairs"
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab Figure 1: Overview of the DanQing dataset construction pipeline. from manually curated blacklist of low-quality websites are excluded. This filtering yields 706 million candidate pairs (67% retention). Subsequent image downloading achieves 67% success rate, resulting in final collection of roughly 475 million high-quality image-text pairs. Textual-Level Purification & Filtering. Prioritizing text-based pruning due to its lower computational overhead, we implement multi-stage refinement pipeline across four dimensions: linguistic structure, text quality, information density, and safety. ① Linguistic Structure: We employ FastText [29] to identify and retain Chinese text, followed by OpenCC [5] to standardize all content into Simplified Chinese. ② Text Quality: To ensure grammatical and lexical integrity, we discard samples that lack nouns or contain more than five [UNK] tokens after SigLIP tokenization [62]. ③ Information Density: Following RealSyn [23], we strip emojis and special characters, then apply an entropy-based semantic filter = P(ci) log2 P(ci), where P(ci) denotes the probability of token occurrence. We eliminate low-content captions with < 6e4. ④ Safety: 20M-parameter NSFW detector [2] and the Baidu DataBuilder service are utilized to filter out advertisements, sensitive political content, and territorial disputes. This stage reduces the corpus from 475 million to 397 million pairs, 16.4% reduction that significantly enhances the signal-to-noise ratio for subsequent training. Image-Level Filtering. To ensure visual integrity and content richness, we implement multistage filtering pipeline focusing on four dimensions: visual fidelity, information density, image redundancy, and content safety. ① Visual Fidelity: We first remove irregular images by retaining only those with an aspect ratio between 1:3 and 3:1 and shortest edge exceeding 100 pixels. To eliminate uninformative samples (e.g., solid colors), we prune images with pixel intensity standard deviation σ < 2. Furthermore, blurry images are mitigated by applying Laplacian variance threshold β 1000 computed via OpenCV [60]. ② Information Density: We quantify content complexity via image entropy = 255 i=0 P(i) log2 P(i), where P(i) is the probability of pixel value i, and remove samples with < 3. ③ Image Redundancy: To minimize perceptual and semantic duplication, inspired by CLIP-CID [72], we use the Chinese-CLIP-L14 [70] to extract image embeddings and employ Union-Find-based [59] clustering algorithm where images within distance threshold of β = 0.1 are grouped. Within each cluster, we retain single representative sample and prune the remainder to ensure dataset diversity. ④ Content Safety: We employ an 86M-parameter NSFW binary detection model [57] to exclude pairs flagged with the highest risk level. This comprehensive refinement reduces the dataset from 397M to 178M pairs, representing 44.8% retention rate relative to the preceding stage. Cross-Modal Filtering. Following LAION400M [54], we leverage an expert model to further refine the dataset based on image-text alignment. Specifically, we compute similarity scores using ChineseCLIP-L14 [70] and retain pairs within the [1.06, 1.24] interval. This thresholding strategy ensures high-quality alignment: scores below 1.06 indicate weak semantic correlation, while those exceeding 1.24 often correspond to images dominated by OCR text rather than descriptive content. This stage"
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab (a) Image Resolution Distribution. (b) Text Chinese words length. Figure 2: Overview of data characteristics in DanQing. Figure 3: Visualization of popular topic in the DanQing dataset, generated via BERTopic [19] on 10M subset. prunes 25 million pairs. The pipeline culminates in curated dataset of approximately 100 million high-quality image-text pairs. 3.3 Statistic of DanQing Data Characteristics. As illustrated in Fig. 2a, we analyze the general characteristics of the DanQing dataset. We assess image resolutions in terms of width, height, and minimum dimension, revealing broad spectrum of visual scales. While most images fall within the 300 to 500 pixel range, considerable proportion exceeds 1,024 pixels. This wide coverage supports the extraction of robust, scale-invariant features for vision-language representation learning. Such visual diversity is crucial for generalization to real-world images, where object scales and orientations vary considerably. In addition, we present the distribution of text lengths in Fig. 2b. Specifically, DanQing encompasses total of 2.2B Chinese words, with an average length of 22 words per sample. The length distribution spans broad range from 5 to 60 tokens, with the majority concentrated between 6 and 40. Notably, the dataset also retains samples at the extreme ends of the spectrum (very short and very long texts). This wide distribution underscores the semantic richness of DanQing, enabling models to learn effective representations across various levels of textual granularity. Topic Modeling. To further investigate the semantic diversity of DanQing, we implement topic modeling pipeline based on the BERTopic [19]. Specifically, we randomly sample 10M image-text pairs and extract text embeddings using Chinese-CLIP-L/14 [70]. To address the challenges of highdimensional clustering, we apply Uniform Manifold Approximation and Projection (UMAP) [45] for dimensionality reduction. Subsequently, we use HDBSCAN [19] to identify distinct semantic clusters, setting the minimum cluster size to 1,000 to ensure cluster stability and reduce noise. We then utilize class-based TF-IDF to extract representative keywords for each topic. As shown in Fig. 3, we visualize the six prevalent topics, which range from fashion and technology to cuisine. These results indicate that DanQing encompasses wide variety of real-world domains, providing"
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab ] 6 1 [ 1 0 1 t 77.0 78.6 79.3 78.5 79.7 77.3 78.4 79.5 78.6 80.2 76.7 80.3 82.4 81.7 83. ] 3 3 [ 0 1 I 85.1 91.7 92.2 90.9 93.0 85.4 90.3 91.3 89.3 93.2 88.5 96.1 96.3 94.8 96.7 ] 2 5 [ 1 1 2 n C ] 3 [ 1 0 1 F ] 5 3 [ N ] 9 [ D ] 8 4 [ 2 0 1 w ] 7 [ 5 4 S ] 0 5 [ P Model Architecture: SigLIP2-B/32@256 8.2 9.5 10.8 5.7 9.9 35.9 42.6 45.1 43.5 46.4 55.1 61.2 64.7 53.6 66.6 81.9 83.0 86.3 83.5 83.4 37.6 61.4 63.2 52.4 58. 61.9 71.3 76.7 62.9 78.7 56.3 58.1 58.9 53.3 61.4 Model Architecture: SigLIP2-B/16@256 10.7 12.7 13.9 7.0 13.3 35.3 44.8 45.6 44.6 48.0 60.8 68.7 70.5 58.1 71. 83.9 81.5 84.6 82.2 83.5 38.1 63.6 65.5 54.3 62.2 65.0 76.0 78.9 65.9 81.8 59.8 59.0 60.6 55.8 63.5 Model Architecture: SigLIP2-L/16@256 15.9 20.5 22.6 13.1 22. 44.8 48.2 48.9 44.3 49.2 72.0 78.3 81.9 68.9 83.8 80.8 84.9 86.4 74.2 85.2 49.7 74.3 75.9 64.5 75.0 84.3 84.5 89.5 79.1 90.0 63.9 65.7 65.3 59.4 64. ] 1 3 [ e 49.4 53.8 49.6 54.0 54.4 51.0 55.0 51.0 54.2 53.2 49.2 55.0 52.0 55.6 55.8 ] 2 3 [ a 76.3 75.1 74.5 58.9 76.0 81.0 80.8 80.2 62.1 81.7 87.4 86.5 87.8 70.7 88.7 ] 4 1 [ 7 0 0 2 V 69.0 75.6 77.3 77.3 77.1 71.0 78.4 79.0 79.2 79.6 68.9 78.1 79.7 79.7 79.9 Avg. 57.8 63.5 64.9 59.5 65.4 59.9 65.8 66.7 60.9 67. 65.2 71.0 72.4 65.5 72.9 Dataset Baseline [62] Wukong [20] Zero [66] TaiSu [43] DanQing Baseline [62] Wukong [20] Zero [66] TaiSu [43] DanQing Baseline [62] Wukong [20] Zero [66] TaiSu [43] DanQing Table 2: Zero-shot image classification performance using models pretrained on different datasets. indicates random sampling of 100 million image-text pairs. The best and second best scores are in boldface and underlined. strong foundation for large-scale vision-language representation learning. For more detailed topic analysis, please refer to the Appendix C.2."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Implementation Details To validate the effectiveness of the DanQing dataset, we continue pre-training the SigLIP2 [62] model for 2 epochs using 16 A800 (80G) GPUs. We employ AdamW [44] as the optimizer, initializing it with learning rate of 1e-5 and weight decay of 0.1. The batch size is set to 768 16. The momentum parameters β1 and β2 are set to 0.9 and 0.98, respectively. learning rate warmup strategy is applied during the first 1,000 iterations to ensure training stability. The input image size is 256 256, and the input text sequence length is truncated or padded to 64. To ensure fair comparison in our experiments, we randomly select 100M samples from both the Zero and TaiSu datasets for training. 4.2 Main Results Zero-shot Classification. As presented in Tab. 2, we perform continual pre-training on three SigLIP2 backbone models (B/32, B/16, and L/16) using the Wukong, Zero, TaiSu, and DanQing datasets. Continual pre-training with these datasets significantly improves model performance, with DanQing yielding the most notable gains. Specifically, DanQing enhances performance by 7.6%, 7.8%, and 7.7% on B/32, B/16, and L/16, respectively. Additionally, compared to the Wukong dataset, DanQing achieves 1.9% performance improvement across all three backbone models. Similarly, compared to the Zero dataset, DanQing provides average performance improvements of 0.5%, 1.0%, and 0.5% on B/32, B/16, and L/16, respectively. These results highlight the high quality of the DanQing dataset and its effectiveness in Chinese image-text contrastive learning tasks."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab Flickr30K-CN [34] MSCOCO-CN [39] MUGE [40] Text to Image Image to Text Text to Image Image to Text Text to Image Image to Text Dataset R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Avg. Model Architecture: SigLIP2-B/32@256 Baseline [62] 45.4 71.2 80.6 67.7 88.9 94.6 49.6 77.2 87.8 51.8 81.1 90.9 38.3 61.7 69.9 35.3 60.7 69.8 67.9 Wukong [20] 49.8 75.8 83.7 68.2 89.8 95.6 54.4 81.6 90.7 56.5 84.8 83.2 55.1 77.9 85.1 44.0 71.2 80.1 74.3 Zero [66] 49.5 76.5 84.4 68.7 90.5 95.1 53.9 84.0 92.1 56.9 84.6 93.3 54.5 77.7 84.9 42.1 69.4 78.5 74.3 TaiSu [43] 60.5 84.2 90.3 77.8 94.4 97.2 65.7 90.7 96.0 65.5 88.9 94.5 56.2 78.2 84.7 44.1 71.2 80.3 78.9 54.2 79.0 86.6 73.0 92.2 96.3 60.1 84.5 93.8 61.0 88.3 96.3 54.8 78.1 84.9 45.3 72.1 80.7 76.7 DanQing Model Architecture: SigLIP2-B/16@256 Baseline [62] 51.3 76.6 84.7 73.5 93.3 96.7 51.9 79.7 89.6 54.7 82.5 91.9 41.6 64.6 73.4 38.9 64.3 73.5 71.3 Wukong [20] 56.5 81.8 88.4 74.8 94.2 97.8 57.5 83.3 92.0 61.0 86.0 93.7 60.1 81.7 87.7 48.8 75.3 83.2 78.0 Zero [66] 58.2 83.7 90.4 74.9 93.4 96.9 58.7 86.0 94.4 60.0 84.8 93.1 59.6 80.8 86.8 46.2 72.9 81.3 77.9 TaiSu [43] 68.2 89.0 93.9 83.8 97.2 99.4 68.8 93.0 97.1 67.1 90.1 95.9 60.3 81.0 86.8 48.4 74.9 83.0 82.1 61.1 84.9 90.9 80.6 95.0 97.9 62.3 86.6 94.4 64.7 88.5 96.1 60.4 81.3 87.3 50.3 76.3 83.9 80.1 DanQing Model Architecture: SigLIP2-L/16@256 Baseline [62] 53.5 78.1 85.5 79.6 95.7 98.3 51.7 79.9 89.0 55.4 81.9 90.5 50.2 71.1 78.5 45.6 70.4 78.5 74.1 Wukong [20] 62.8 86.2 91.5 81.7 96.2 98.5 61.0 85.9 93.5 62.9 88.7 95.1 66.6 84.6 90.1 55.8 80.7 87.4 81.6 Zero [66] 64.3 87.9 93.4 78.4 95.5 98.6 61.6 87.2 94.7 62.1 87.2 94.6 65.9 85.3 90.3 53.9 79.0 86.2 81.5 TaiSu [43] 72.6 91.7 95.8 87.8 98.7 99.7 71.4 92.6 97.3 69.2 91.6 96.8 66.0 85.0 90.0 55.1 80.1 86.7 84.9 70.2 90.3 94.7 86.3 98.7 99.6 65.9 90.5 95.4 68.0 92.6 97.4 67.5 84.9 90.1 56.8 81.2 87.5 84.3 DanQing Table 3: Cross-modal retrieval performance on short-caption datasets for models pretrained on various large-scale Chinese image-text datasets. indicates random sampling of 100 million imagetext pairs. The best and second-best results are highlighted in bold and underlined, respectively. Cross-Modal Retrieval. To further validate the effectiveness of the DanQing dataset, we conduct comparisons on cross-modal retrieval tasks. As shown in Tab. 3, on the Flickr30K-CN, MSCOCO-CN, and MUGE datasets, DanQing achieves average retrieval performance improvements of 2.4%&2.4%, 2.1%&2.2%, and 2.7%&2.8% over the Wukong and Zero datasets across three different backbone models. Notably, TaiSu achieves strong retrieval performance, particularly on Flickr30K-CN and MSCOCO-CN, largely because it augments web-crawled tags with concise synthetic captions generated by OFA-Large [64]. The close alignment between the distribution of these captions and the target benchmarks results in substantial performance gains. Dataset DCI-CN [63] Image to Text Text to Image Image to Text Text to Image DOCCI-CN [49] 8.7 11.3 11.2 12.5 12.6 11.0 16.3 17.1 16.8 19.8 26.5 34.8 35.8 37.1 42. 25.3 30.7 30.9 33.1 33.7 23.8 29.6 32.0 31.5 35.0 18.2 22.7 24.4 24.0 27.4 19.3 23.8 23.7 26.2 26.1 Model Architecture: SigLIP2-B/32@256 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Avg. Baseline [62] 7.7 Wukong [20] 10.2 Zero [66] 10.9 TaiSu [43] 11.3 13.1 DanQing In addition, inspired by recent work [67], we further evaluate long caption-based crossmodal retrieval using the DCICN and DOCCI-CN datasets as shown in Tab. 4. We surprisingly observe that, under the same pre-training context length (up to 64 tokens), the DanQing dataset demonstrates superior long caption-based cross-modal retrieval performance compared to other datasets. With the SigLIP2-L/16@256 model, DanQing achieves average performance improvements of 12.8%, 9.0%, and 8.9% over the Wukong, Zero, and TaiSu datasets, respectively. This improvement is mainly because DanQing exhibits higher semantic density and greater proportion of high-quality texts  (Fig. 4)  . Table 4: Cross-modal retrieval performance on long-caption datasets for models pretrained on various datasets. Baseline [62] 8.7 Wukong [20] 12.2 Zero [66] 12.9 TaiSu [43] 13.2 15.3 DanQing Baseline [62] 16.7 Wukong [20] 23.1 Zero [66] 24.8 TaiSu [43] 26.6 31.3 DanQing Model Architecture: SigLIP2-B/16@ Model Architecture: SigLIP2-L/16@256 46.8 50.1 51.7 51.9 56.6 39.8 49.4 49.5 51.5 57.8 16.6 18.0 19.0 18.9 22.3 35.8 38.9 40.5 41.1 44.9 19.6 25.3 26.9 27.1 30. 25.9 32.6 34.8 34.7 38.4 10.4 12.8 12.9 14.6 15.0 26.8 32.6 33.0 36.0 36.9 30.2 39.1 39.5 40.5 47.3 21.0 25.8 25.6 28.6 29.3 13.0 17.7 18.8 19.1 23. 36.0 44.8 45.8 47.4 52.8 41.6 46.3 50.1 49.1 51.5 14.3 15.7 17.8 16.6 18.7 32.1 35.7 38.5 38.8 40.4 64.9 69.8 77.8 73.2 81.5 64.3 74.2 75.8 76.8 84. 29.0 33.3 38.4 37.1 44.8 54.4 59.2 67.4 63.2 72.2 30.9 41.0 43.0 44.4 50.7 37.5 48.6 51.4 52.2 58.4 16.3 21.8 24.5 26.0 30.5 38.0 46.0 49.9 51.1 58. 53.5 64.2 66.4 67.3 76.4 30.6 38.3 41.6 43.2 49.9 29.3 37.1 37.6 41.1 48.7 24.6 29.5 30.4 31.4 34.8 38.8 46.4 49.9 50.2 57.3 22.0 26.8 28.2 28.7 31."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab (a) Text content word density (b) Text perplexit Figure 4: Text Quality Analysis. Randomly sample 10M subsets from DanQing, Wukong, and Zero, then compare their content word density and perplexity. Avg. V2 [17] Dataset CN [79] MMBench (Dev) CN [42] EN [42] Model Architecture: SigLIP2-L/16@256 + Qwen2-7B MME-RW CMMMU OCRBench eval [78] Chinese-Centric Large Multimodal Model Tasks. We employ SigLIP2 variants that continue pretrained on different Chinese imagetext datasets as vision encoders in Large Multimodal Models (LMMs) to evaluate their compatibility and utility for Chinesecentric multimodal reasoning. Specifically, we strictly adhere to the LLaVANeXT [41] training pipeline and data configuration, varying only the vision encoder (SigLIP2-L/16) to isolate the effect of pretraining data on downstream multimodal capabilities. Results in Tab. 5 show that DanQing surpasses existing datasets, achieving new state-of-the-art average score of 50.1% (vs. 49.5%). These performance improvements demonstrate the higher data quality of the DanQing dataset and highlight its potential for Chinese-centric tasks in next-generation LMM architectures. Table 5: Performance of LLaVA-NeXT-style models on Chinese-centric LMM downstream benchmarks, utilizing vision encoders pretrained on various datasets. Baseline [62] Wukong [20] Zero [66] TaiSu [43] DanQing 73.6 75.6 75.0 75.3 75.3 38.7 39.7 39.4 39.8 39.7 72.9 73.5 72.9 73.5 74. 43.1 43.8 42.3 42.8 45.4 15.0 15.0 15.7 15.1 16.0 48.7 49.5 49.1 49.3 50."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Text Quality Analysis We further explore the text quality of DanQing through two metrics: semantic word density and perplexity (PPL), as illustrated in Fig. 4. Specifically, we randomly sampled 10M texts from DanQing, Wukong, and Zero for comparison. We use the jieba 1 toolkit to identify semantic words (i.e., nouns, verbs, and adjectives) in each sentence and compute their proportions as measure of semantic density. As illustrated in Fig. 4a, DanQing exhibits significantly higher semantic density than the other datasets, which enables the model to acquire more effective semantic information. Additionally, we compute sentence-level perplexity using pre-trained Chinese BERT model [10]. As shown in Fig. 4b, the number of samples in DanQing with PPL scores within the [50, 200] range is substantially higher than that of the other datasets. This range suggests an optimal level of linguistic complexity (neither overly simplistic nor incoherent), thereby highlighting the superior quality of our dataset for vision-language pre-training. 5.2 Scaling Capability Scaling capability determines the upper bound of vision-language pretraining models. To this end, we compare the data and model scaling capabilities of the proposed DanQing dataset with those of the widely used Wukong dataset, and report the average performance on zero-shot classification and retrieval (long&short caption) tasks in Fig. 5. 1 https://github.com/fxsjy/jieba"
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab (a) Data Scaling on SigLIP2-B/32@256 (b) Model Scaling on 30M subset Figure 5: Data scaling and model scaling capability comparison between DanQing and Wukong. Data Scaling. To evaluate the scalability of our proposed DanQing, we pretrain SigLIP2-B/32 for two epochs on varying data scales (10M, 30M, 60M, and 100M) and compare the performance trajectories of DanQing and Wukong. As shown in Fig. 5a, DanQing consistently achieves significant performance gains over Wukong across all data scales, with the improvements becoming more pronounced as the scale increases. Notably, Wukongs retrieval performance plateaus beyond 30M samples, whereas DanQing continues to improve steadily from 30M to 100M, indicating that our dataset provides more effective supervision for large-scale vision-language pretraining. Model Scaling. To further investigate model scaling, we conduct experiments as illustrated in Fig. 5b. Specifically, we utilize 30M subsets from both DanQing and Wukong to train SigLIP2 models across various scales, including Base (86M), Large (303M), So (400M), and g-opt (1B). The results show that DanQing consistently outperforms Wukong in both classification and retrieval tasks. Moreover, the performance trajectory of DanQing exhibits steeper scaling curve, suggesting that our dataset is more effective at leveraging increased model capacity to enable superior representation learning. 5.3 Image Semantic Balance Fig. 6 illustrates the semantic distribution of images in DanQing compared to the Wukong dataset. For quantitative analysis, we randomly sample 10 million images from each dataset and cluster them into 10k groups using the FAISS library [28]. We rank these clusters by the number of samples they contain. The results indicate that DanQing achieves significantly more balanced and uniform semantic distribution than Wukong, effectively mitigating the long-tail effect. This increased uniformity suggests broader coverage of the visual manifold, which is Figure 6: Clustering distribution of 10M subsets of DanQing and Wukong. essential for learning rare or long-tail concepts during vision-language pretraining. 5. Image-Text Alignment In Fig. 7, we illustrate the distribution of image-text similarity scores for 10M-sample subsets of DanQing and Wukong [20]. We employ the state-of-the-art Chinese retrieval model FG-CLIP2-L/16 [67] to extract multimodal features and compute their cosine similarity. The results indicate that DanQing consistently achieves higher similarity scores than Wukong, with significantly more samples exceeding the 0.15 threshold. This demonstrates that DanQing provides stronger semantic consistency between images and texts. It is noteworthy that the DanQing dataset con9 Figure 7: Similarity distributions for 10M subsets of DanQing and Wukong."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab (a) 黑神话:悟空 (Black Myth: Wukong) (b) 小米SU7 (Xiaomi SU7) Figure 8: New concept understanding capability comparison between DanQing with existing datasets. The scores represent the softmax-normalized values of the cosine similarities among three image-text pairs. tains significantly higher proportion of samples in the 0 to 0.05 similarity range compared to Wukong. This is primarily because DanQing comprises data from 2024 and 2025, which includes substantial amount of novel semantic content (as shown in Fig. 8). These findings help explain why models trained on DanQing demonstrate significant performance improvements in retrieval tasks, further highlighting the datasets ability to enrich models with comprehensive semantic knowledge. 5.5 New Concept Understanding In Fig. 8, we evaluate the capability of SigLIP2-L/16@256 models pre-trained on different Chinese datasets to understand emergent concepts. Specifically, we select internet buzzwords that appear after 2024, such as 黑神话:悟空 (Black Myth: Wukong) and 小米SU7 (Xiaomi SU7). We pair these keywords with their corresponding ground-truth images, along with several semantically related distractors (e.g., traditional cartoon or TV adaptations of Wukong, other Xiaomi products, and unrelated food items). By calculating the image-text similarity scores, we observe that the model trained on DanQing consistently assigns the highest confidence to the correct pairs. This superiority demonstrates that DanQing contains more up-to-date information, effectively enabling models to internalize contemporary knowledge and generalize to recent real-world concepts."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce DanQing, large-scale Chinese imagetext dataset consisting of approximately 100 million pairs, designed to alleviate the scarcity of high-quality Chinese cross-modal resources in vision-language pretraining task. To ensure data quality, we develop rigorous curation pipeline that systematically filters raw web data, ultimately retaining nearly 10% of the high-quality samples. Extensive experiments show that SigLIP2 models pre-trained on DanQing achieve state-of-the-art performance across wide range of downstream benchmarks compared to existing datasets, highlighting the high quality and practical utility of our dataset. Furthermore, we conduct comprehensive, multi-dimensional analysis of DanQing, evaluating its scaling ability, text quality, semantic balance, and capacity to capture emerging real-world concepts. To facilitate and accelerate future research, we will open-source the DanQing dataset, providing robust foundation and valuable insights for large-scale Chinese visionlanguage pretraining."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab"
        },
        {
            "title": "References",
            "content": "[1] Baidu AI Studio Community. PaddleHub Pornographic Content Detection Model Tutorial, Jan 2021. URL https://aistudio.baidu.com/projectdetail/1444248. Online tutorial demonstrating the use of PaddleHubs porn_detection_lstm model for text-based content moderation. Last updated on 2021-01-13. [2] Baidu AI Studio Community. PaddleHub Pornographic Content Detection Model Tutorial, Jan 2021. URL https://www.paddlepaddle.org.cn/hubdetail?name=porn_detection_ cnn&en_category=TextCensorship. Online tutorial demonstrating the use of PaddleHubs porn_detection_lstm model for text-based content moderation. Last updated on 2021-01-13. [3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In ECCV, pp. 446461. Springer, 2014. [4] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. [5] BYVoid. OpenCC: Open Chinese Convert. https://github.com/BYVoid/OpenCC, 2024. [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 3558 3568, 2021. [7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017. [8] Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, et al. Meta clip 2: worldwide scaling recipe. arXiv preprint arXiv:2507.22062, 2025. [9] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild, 2013. URL https://arxiv.org/abs/1311.3618. [10] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre-trained models for chinese natural language processing. In EMNLP, pp. 657668, 2020. [11] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303338, 2010. [15] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. [16] Li Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPRW, 2004. [17] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab [18] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Neurips, 36:2709227112, 2023. [19] Maarten Grootendorst. Bertopic: Neural topic modeling with class-based tf-idf procedure, 2022. URL https://arxiv.org/abs/2203.05794. [20] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Neurips, 35:2641826431, 2022. [21] Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. Rwkv-clip: robust vision-language representation learner. In EMNLP, pp. 47994812, 2024. [22] Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. In ACM MM, pp. 28602869, 2025. [23] Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. Realsyn: An effective and scalable multimodal interleaved document transformation paradigm. In ACM MM, pp. 34873496, 2025. [24] Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, and Lidong Bing. Unime-v2: Mllm-as-a-judge for universal multimodal embedding learning. arXiv preprint arXiv:2510.13515, 2025. [25] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin andFeature Pyramid Networks for Object Detection Cui. Zero-shot detection via vision and language knowledge distillation. In ICLR, 2022. [26] Xiaoxing Hu, Kaicheng Yang, Ziyang Gong, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, and Xue Yang. Proclip: Progressive vision-language alignment via llm-based embedder. arXiv preprint arXiv:2510.18795, 2025. [27] Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, and Yupei Wang. Decoupled global-local alignment for improving compositional understanding. In ACM MM, pp. 3251 3260, 2025. [28] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535547, 2019. [29] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016. [30] Elias Kempf, Simon Schrodi, Max Argus, and Thomas Brox. When and how does clip enable domain and compositional generalization? In ICML, 2025. [31] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Neurips, 2020. [32] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In ICCVW, pp. 554561, 2013. [33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [34] Weiyu Lan, Xirong Li, and Jianfeng Dong. Fluency-guided cross-lingual image captioning. In ACM MM, pp. 15491557, 2017."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab [35] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. [36] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and René Ranftl. Languagedriven semantic segmentation. In ICLR, 2022. [37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. [38] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR, 2022. [39] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. Coco-cn for cross-lingual image tagging, captioning, and retrieval. TMM, 21(9):23472360, 2019. [40] Junyang Lin, Rui Men, An Yang, Chang Zhou, Yichang Zhang, Peng Wang, Jingren Zhou, Jie Tang, and Hongxia Yang. M6: Multi-modality-to-multi-modality multitask mega-transformer for unified pretraining. In ACM SIGKDD, pp. 32513261, 2021. [41] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [42] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, pp. 216233. Springer, 2024. [43] Yulong Liu, Guibo Zhu, Bin Zhu, Qi Song, Guojing Ge, Haoran Chen, GuanHui Qiao, Ru Peng, Lingxiang Wu, and Jinqiao Wang. Taisu: 166m large-scale high-quality dataset for chinese vision-language pre-training. NIPS, 35:1670516717, 2022. [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [45] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction, 2020. URL https://arxiv.org/abs/1802.03426. [46] Ron Mokady, Amir Hertz, and Amit Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. [47] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pre-training. In ECCV, pp. 529544. Springer, 2022. [48] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, 2008. [49] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. Docci: Descriptions of connected and contrasting images, 2024. URL https://arxiv.org/abs/ 2404.19753. [50] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. [53] Yongming Rao, Wenliang Zhao, Guangyi Light, Jiwen Zhou, Jiwen Lu, et al. Denseclip: Language-guided dense prediction with context-aware prompting. In CVPR, 2022. [54] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Neurips, 35: 2527825294, 2022. [56] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. [57] Freepik Company S.L. Eva-based fast nsfw image classifier, 2025. URL https://huggingface. co/Freepik/nsfw_image_detector. [58] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR, pp. 24432449, 2021. [59] Robert Endre Tarjan. Efficiency of good but not linear set union algorithm. JACM, 22(2): 215225, 1975. [60] OpenCV team. Opencv: Open source computer vision library. https://github.com/opencv/ opencv, 2024. [61] Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. [62] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [63] Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In CVPR, 2024. [64] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework, 2022. URL https://arxiv.org/abs/2202. 03052. [65] Yu Wu, Yana Wei, Haozhe Wang, Yongfei Liu, Sibei Yang, and Xuming He. Grounded image text matching with mismatched relation reasoning. In ICCV, pp. 29762987, 2023. [66] Chunyu Xie, Heng Cai, Jincheng Li, Fanjing Kong, Xiaoyu Wu, Jianfei Song, Henrique Morimitsu, Lin Yao, Dexin Wang, Xiangzheng Zhang, et al. Ccmb: large-scale chinese cross-modal benchmark. In ACM MM, pp. 42194227, 2023. [67] Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Ji Ao, Dawei Leng, and Yuhui Yin. Fg-clip 2: bilingual fine-grained vision-language alignment model. arXiv preprint arXiv:2510.10921, 2025."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab [68] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. [69] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In CVPR, 2022. [70] An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. Chinese clip: Contrastive vision-language pretraining in chinese. arXiv preprint arXiv:2211.01335, 2022. [71] Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. Alip: Adaptive language-image pre-training with synthetic caption. In ICCV, pp. 29222931, 2023. [72] Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, and Jiankang Deng. Clip-cid: Efficient clip distillation via cluster-instance discrimination. In AAAI, volume 39, pp. 2197421982, 2025. [73] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. TMLR, 2022. [74] Sha Yuan, Shuai Zhao, Jiahong Leng, Zhao Xue, Hanyu Zhao, Peiyu Liu, Zheng Gong, Wayne Xin Zhao, Junyi Li, and Jie Tang. Wudaomm: large-scale multi-modal dataset for pre-training models. arXiv preprint arXiv:2203.11480, 2022. [75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, pp. 1197511986, 2023. [76] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. In ICCV, pp. 1178211791, 2021. [77] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In ECCV, pp. 310325. Springer, 2024. [78] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et al. Cmmmu: chinese massive multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2401.11944, 2024. [79] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. [80] Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, and Qichuan Ding. Gradient-attention guided dual-masking synergetic framework for robust text-based person retrieval. In EMNLP, pp. 259271, 2025. [81] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In CVPR, 2022."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab"
        },
        {
            "title": "Appendix Overview",
            "content": "The appendix includes the following sections: Appendix A: Provides detailed statistics of data filtration. Appendix B: Visualizes image-text pair examples from the DanQing dataset. Appendix C: Offers additional analytical insights and exploratory studies, including source domain distributions (Sec. C.1), extended visualizations of topic modeling (Sec. C.2), and word cloud analyses (Sec. C.3)."
        },
        {
            "title": "A Statistics of Data Filtration",
            "content": "Steps Left Data Num Total Filter % Stage Filter % Left % Collected image URL and text pairs 1,047,085,609 100.00% Data Collection Fine-grained Text Filtration Fine-grained Image Filtration Chinese Label NSFW content Source reliability Text length Download Success Chinese Language detection Font conversion Stop words contain Nouns contain [UNK] token Entropy Emoji/special chars clean NSFW Aspect ratio & Min edge Low-textual images Blurry images Entropy Visual redundancy NSFW Cross-modality Filtering Cross-batch Visual Redundancy Final image-text pairs 726,334,674 30.63% 69.37% 69.37% 706,069,936 475,104,485 467,455,303 467,455,303 442,680,171 431,878,774 431,619,647 400,068,251 400,068,251 397,187,759 352,663,012 352,204,550 333,255,945 316,359,868 186,019,602 178,601,215 154,293, 99,892,381 99,892,381 32.57% 54.63% 55.36% 55.36% 57.72% 58.75% 58.78% 61.79% 61.79% 62.07% 66.32% 66.36% 68.17% 69.79% 82.23% 82.94% 85.26% 90.46% 90.46% 2.79% 67.29% 1.61% 0.00% 5.30% 2.44% 0.06% 7.31% 0.00% 0.72% 11.21% 0.13% 5.38% 5.07% 41.20% 3.25% 13.61% 35.26% 67.43% 45.37% 44.64% 44.64% 42.28% 41.25% 41.22% 38.21% 38.21% 37.93% 33.68% 33.64% 31.83% 30.21% 17.77% 17.06% 14.74% 9.54% 9.54% Table 6: Specific statistics information of the DanQing dataset construction pipeline. We further illustrate the statistical breakdown of our data construction pipeline in Tab. 6. Starting with an initial collection of approximately 1B raw image-text URLs, we apply multi-stage refinement process, comprising coarseand fine-grained filtering, cross-modality filtering, and deduplication. This pipeline ultimately yields nearly 100M high-quality image-text pairs (storage space occupies approximately 12TB), effectively filtering out 90% of the original noise to ensure data quality."
        },
        {
            "title": "B Examples in DanQing Dataset",
            "content": "Fig. 9 presents representative examples from the DanQing dataset, comprising images and their corresponding Chinese textual descriptions. Specifically, these image-text pairs encompass wide range of domains, such as natural scenery, historical literature, and automotive technology, demonstrating the thematic diversity of our dataset. This breadth makes DanQing particularly well-suited for general-purpose Vision-Language Pre-training."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab Figure 9: Visualization of image-text pairs in DanQing dataset."
        },
        {
            "title": "C Exploring DanQing",
            "content": "C.1 Source Domain Distribution To further investigate the origins of our data, we identified and ranked the top 40 primary web sources, as detailed in Tab. 7. The results indicate that the majority of image-text pairs originate from widely-used Chinese platforms and applications, such as Alibaba, Baidu, ByteDance, and so on. Furthermore, these sources span diverse categories, including E-commerce, news media, and search engines, demonstrating the heterogeneous nature of our data. This variety validates that DanQing is sourced from broad spectrum of real-world scenarios, capturing the richness of daily-life multimodal content. C.2 Topic Modeling We further illustrate the topic distribution of DanQing in Fig. 10, following the same analytical methodology described in Sec. 3.3. The visualized examples encompass both previously mentioned categories and novel domains, such as tourism, design, education, and agriculture, which are prevalent throughout the dataset. This thematic breadth, exemplified by diverse visual content, underscores how DanQing aligns closely with real-world scenarios and everyday life. C.3 Word Cloud In Fig. 11, we visualize the distribution of Chinese words (comprising one or more tokens) within the DanQing dataset. Specifically, we utilize the jieba 2 words segmentation module to tokenize the text and generate the word cloud. The visualization highlights that the most frequent terms include 2024, 中国 (China), 游戏 (Game), 美食 (Food), 活动 (Activity), and so on. This distribution demonstrates the inclusion of the newest semantic concepts and diverse daily topics, which are essential for robust, general purpose Vision-Language Pre-training. 2 https://github.com/fxsjy/jieba"
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab Rank Source Domain Total Count % Categories 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 alicdn.com baidu.com wp.com aliyuncs.com chem17.com bing.net udn.com.tw faiusr.com sinaimg.cn wixstatic.com 126.net 360buyimg.com vjshi.com bing.com staticflickr.com xiniu.com sohu.com myqcloud.com smzdm.com made-in-china.com cloudfront.net sogoucdn.com toutiaoimg.com hbzhan.com byteimg.com qpic.cn myapp.com zdmimg.com tom.com itc.cn 588ku.com qunarzz.com bdxiguaimg.com suning.cn thefastimg.com gtimg.com iqiyipic.com myxypt.com book.com.tw tripcdn.com 3,555,967 2,649,928 1,582,010 961,433 781,994 741,185 542,113 490,843 404,076 393,168 372,122 336,282 333,778 322,193 319,134 286,491 285,513 262,140 258,988 256,439 249,402 231,903 229,255 227,981 225,890 223,192 216,009 210,646 210,437 204,060 194,423 193,114 185,956 174,229 168,851 168,577 167,389 164,791 161,625 155,944 3.56% E-commerce, Cloud Computing 2.65% Search Engine, Tech 1.58% Blog Hosting (WordPress), CMS 0.96% Cloud Computing, Object Storage 0.78% Chemical Instruments, E-commerce 0.74% Search Engine (Bing), Content Delivery 0.54% News Media, General Info 0.49% Website Builder, Marketing 0.40% Social Media (Weibo), Sina Portal 0.39% Website Builder (Wix), Image Hosting 0.37% Internet Services (NetEase), Email 0.34% E-commerce (JD.com), Logistics 0.33% Video Assets, Copyright Trading 0.32% Search Engine, Microsoft, International 0.32% Photo Community, Social Media 0.29% Enterprise Services, Marketing 0.29% General Portal, Media, Video 0.26% Cloud Computing (Tencent) 0.26% Consumer Guide, E-commerce 0.26% Cross-border Trade, B2B, Export 0.25% Content Delivery, Cloud Computing 0.23% Search Engine (Sogou) 0.23% News (Toutiao), ByteDance 0.23% Environmental Industry 0.23% ByteDance, Short Video, News 0.22% Social Media (QQ/WeChat), Tencent 0.22% App Store, Mobile Internet 0.21% Consumer Community 0.21% General Portal, Internet Services, Media 0.20% Sohu Media, Content Delivery (CDN) 0.19% Design Materials (Qinku/Wotu) 0.19% Travel & Tourism (Qunar) 0.19% Video (Xigua Video), ByteDance 0.17% E-commerce (Suning), Retail 0.17% Image Hosting, General CDN 0.17% Tencent, Games, Social 0.17% Video (iQIYI), Entertainment 0.17% Pharmaceutical B2B, Industry Platform 0.16% Book Retail, E-commerce, Culture 0.16% Travel (Ctrip/Trip.com), International Table 7: Statistics and categories overview of the top 40 image source domains."
        },
        {
            "title": "DanQing Technical Report",
            "content": "Glint Lab Figure 10: External topic examples visualization of DanQing dataset generated via the BERTopic [19]. Figure 11: Word cloud visualization of 10M subset texts from DanQing."
        }
    ],
    "affiliations": [
        "Glint Lab"
    ]
}