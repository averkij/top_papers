{
    "paper_title": "Neural-Driven Image Editing",
    "authors": [
        "Pengfei Zhou",
        "Jie Xia",
        "Xiaopeng Peng",
        "Wangbo Zhao",
        "Zilong Ye",
        "Zekai Li",
        "Suorong Yang",
        "Jiadong Pan",
        "Yuanxiang Chen",
        "Ziqiao Wang",
        "Kai Wang",
        "Qian Zheng",
        "Xiaojun Chang",
        "Gang Pan",
        "Shurong Dong",
        "Kaipeng Zhang",
        "Yang You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 7 9 3 5 0 . 7 0 5 2 : r Neural-Driven Image Editing Pengfei Zhou1*, Jie Xia2*, Xiaopeng Peng3, Wangbo Zhao1, Zilong Ye2, Zekai Li1, Suorong Yang1,4, Jiadong Pan2, Yuanxiang Chen5, Ziqiao Wang1, Kai Wang1, Qian Zheng2, Xiaojun Chang5,6, Gang Pan2, Shurong Dong2, Kaipeng Zhang7,8, Yang You1 1NUS 2Zhejiang University 3RIT 4NJU 5USTC 6MBZUAI 7Shanghai AI Lab 8SII https://loongx1.github.io Figure 1: Illustration of LoongX for hands-free image editing via multimodal neural signals."
        },
        {
            "title": "Abstract",
            "content": "Traditional image editing typically relies on manual prompting, making it laborintensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional nearinfrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into unified latent space, which is then aligned with edit semantics via fine-tuning on diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area. Equal contribution; Corresponding authors (zpf4wp@outlook.com, jiexia@zju.edu.cn; xxp4248@rit.edu, dongshurong@zju.edu.cn, zhangkaipeng@pjlab.org.cn) Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Image editing involves manipulating digital visuals to achieve desired effects, significantly impacting fields like advertising, entertainment, and scientific visualization [1]. Traditionally, this task demands extensive manual effort and technical expertise. Advances in generative models have streamlined instruction-based image editing through automated pipelines [24]. Nevertheless, these methods still heavily depend on intensive user inputs, such as text prompts [5, 6], visual references like masks or sketches [7, 8], and physical operations like dragging [912]. Such reliance limits efficiency and accessibility, especially for users with motor or communication impairments. To address these challenges, alternative input modalities have been explored [1315] for image editing. Among these, braincomputer interfaces (BCIs) provide promising possibility with their recent advancement in hardware precision [16, 17]. Starting from early attempts in passive tasks such as mental state recognition [18] and neural activity analysis [19, 20], BCIs have begun to be involved in more active generative tasks such as neural-driven chat [21] and visual content creation [22, 23]. However, existing approaches remain limited to single-modality data such as electroencephalography (EEG) [22, 24] or functional magnetic resonance imaging (fMRI) [25], which is insufficient to capture nuanced user intentions for complex editing scenarios. In practice, physiological signals from different modalities offer complementary insights into cognitive states such as attention, motivation, and emotional regulation [2632], underscoring the need for multimodal neural integration. Given the limited exploration of this emerging area, here we ask three key research questions (RQs): RQ1. Are neural signals reliable indicators of user intention in image editing? RQ2. If yes, what aspects of condition information or cognitive state are captured by multimodal neural signals? RQ3. Do neural signals and speech provide comparable guidance for image editing, and can they offer complementary information? To answer these questions, we first construct L-Mind, comprehensive multimodal dataset comprising 23,928 image pairs with synchronously collected EEG, functional near infrared spectroscopy (fNIRS) [33], photoplethysmography (PPG) [34], head motion, and speech signals from 12 participants conceiving image editing tasks. Captured using wireless, lightweight BCI system that supports unconstrained head movements and speech [35], L-Mind offers higher ecological validity under natural real-world conditions and supports robust training of brain-supervised generative models. Building on L-Mind, we also propose LoongX, hands-free image editing approach that innovatively integrates the proposed multimodal neural signal fusion strategy with diffusion transformer (DiT) to translate neural intent into image edits. Unlike prior single-modal methods, LoongX integrates EEG, fNIRS, PPG, and head motion signals, extracting explicit user intentions from EEG signals across multiple scalp regions, incorporating cognitive load and emotional valence data from fNIRS, and capturing stress and engagement indicators through PPG and motion signals. We introduce two new modules to manage diverse multimodal input: the cross-scale state space (CS3) encoder for robust feature extraction and the dynamic gated fusion (DGF) module for comprehensive multimodal integration. We further pretrain these encoders via contrastive learning using combined large-scale datasets [36] and L-Mind to align neural features with semantic text embeddings. Extensive experiments qualitatively and quantitatively demonstrate the feasibility of neural-driven image editing. Integrated multimodal neural signals achieve performance comparable to text-driven baselines (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4637). Combined neural signals with speech instructions surpass text prompts alone (CLIP-T: 0.2588 vs. 0.2549). Ablation studies verify the effectiveness of proposed modules and further explore the contribution of each signal, showing that EEG + fNIRS contribute most among signals, and the Oz and Fpz sites, as EEG input channels, represent the key brain region. These findings underscore LoongXs potential to facilitate intuitive, inclusive image editing and inspire future humanAI interaction. Our main contributions are summarized as: 1) L-Mind, multimodal dataset with 23,928 image-editing pairs featuring synchronized EEG, fNIRS, PPG, motion, and speech signals collected in natural settings. 2) LoongX, novel neural-driven editing method with CS3 and DGF modules for effective feature extraction and multimodal integration (see the effect in Fig. 1). 2 3) Extensive experiments validate multimodal neural signals effectiveness and provide insights into modality-specific contributions and their synergy with speech-based inputs."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Brain-supervised Generation BCI, an emerging technology, builds direct communication between the brain and devices by decoding neural signals [37, 38]. Advances in machine learning have improved its accuracy, enabling brain-guided generative methods for visual content creation. Several recent methods integrate neurophysiological data (e.g., fMRI, EEG, or fNIRS) with generative models [22, 39, 40]. For instance, CMVDM aligns fMRI features with semantics for image synthesis [41], and the MindEye series further lifts the resolution of generated images from decoded fMRI [42, 43]. OneLLM leverages the large fMIR dataset for multimodal alignment in multimodal large language model [16]. DreamDiffusion produces images from EEG via temporal masked modeling [22]. EEG2Video extends the idea to dynamic video content [44]. While Davis et al. [24] initially explore brain-guided semantic image editing using generative adversarial network, this work is limited to facial images and EEG signals. Moreover, Adamic et al. [45] reconstructs visual images from brain activity measured by fNIRS. Unlike previous studies, our data are collected using wireless BCI system  (Fig. 2)  as participants conceive instruction-based image edits. Compared with fMRI methods, our framework combines lightweight EEG, fNIRS, PPG, and head-motion signals, which can support greater portability and broader real-world applicability. To the best of our knowledge, this is the first work to fully leverage all these signals for instruction-based image editing, focusing on improved neural feature encoding and optimized multimodal fusion strategies. 2.2 Instruction-based Image Editing Recent generative models like GPT-4o [46] and Gemini [47] have evolved from basic question answering to advanced image editing by interpreting user instructions. Modern instruction-based image editing agents integrate multimodal inputs, including text, images, and videos, to accurately identify and apply visual edits [13]. Leveraging learned multimodal representations, these agents interpret instructions from input, localize relevant regions, and perform targeted modifications [48 50]. Recent approaches, such as InstructPix2Pix [1], UltraEdit [2], MagicBrush [51], MIGE [52], and ACE [53] improve region-specific edits guided by natural language prompts. Speech-driven image editing [14] was also explored, highlighting the feasibility of hands-free interaction but still limited by linguistic expressiveness in recorded speech. Despite these advancements, achieving efficient, delicate prompt-free image editing remains challenging. Our work addresses this gap, exploring neural-signal-driven editing agents to decode cognitive intent directly for image manipulation, significantly enhancing accessibility and interaction efficiency."
        },
        {
            "title": "3 Dataset",
            "content": "3.1 Data Collection We collect 23,928 editing samples (22,728 training, 1,200 testing) from 12 participants using the setup depicted in Fig. 2. Participants wear our multimodal sensor while viewing image-text pairs sourced from SEED-Data-Edit [54] on 25-inch monitor (resolution: 1980 1080). The measured EEG, fNIRS, and PPG physiological signals are streamed in real-time via Bluetooth 5.3, synchronized and aligned via lab streaming layer by the proprietary Lab Recorder software [55]. Participants simultaneously read displayed editing instructions aloud, providing audio speech signals. Experiments are conducted in quiet, temperature-controlled room (24C, consistent humidity), starting at 9 AM daily. EEG signals are collected via non-invasive hydrogel electrodes, replaced every five hours to maintain signal quality. The experimental room is shielded from sunlight to prevent interference with fNIRS and PPG signals. Sessions start and end with participant-controlled audio recording and are marked by image names. Data from inactive intervals are excluded. 3 Figure 2: The L-Mind dataset comprises 23,928 multimodal editing samples, each including an original image, ground truth text editing instruction, ground truth edited image, as well as measured EEG, fNIRS, PPG, motion and speech signals. (a) Multimodal data collection pipeline; (b) Illustration and statistics of 35 types of image editing tasks. Each session  (Fig. 2)  starts and ends with user-initiated audio recording and is labeled by the paired image. 1-second cross-fixation follows each image pair, with breaks every 100 images. Twelve healthy college students (6 female, 6 male; mean age: 24.5 2.5 years old) with normal or corrected vision participated. All participants gave informed consent and received financial compensation. The study was officially approved by the corresponding institutes ethics committee. 3.2 Data Preprocessing EEG. Four EEG channels (Pz, Fp2, Fpz, Oz; sampled at 250 Hz) undergo band-pass filtering (180 Hz) and notch filtering (4852 Hz) to remove drifts, noise, and powerline interference. Ocular artifacts in Fp2 and Fpz are retained to capture eye movements. fNIRS. Six-channel fNIRS signals (735 nm, 850 nm) are converted to relative hemoglobin concentration changes (HbO, HbR, HbT) using the Modified BeerLambert law. Optical density change is computed as A(λ) = log (I0(λ)/I(λ)). Concentration changes are calculated as: (cid:21) (cid:20)HbO HbR = 1 DPF (cid:20)ελ1 HbO ελ2 HbO ελ1 HbR ελ2 HbR (cid:21)1 (cid:20)A(λ1) (cid:21) A(λ2) (1) Hemodynamic signals (HbO, HbR, and HbT, where HbT = HbO + HbR) are bandpass filtered (0.010.5 Hz) to isolate relevant neural responses, averaged per hemisphere to reflect task-related brain activity. PPG and motion Four-channel PPG signals (735 nm, 850 nm) are averaged per hemisphere via adaptive average pooling and filtered (0.54 Hz) to extract cardiac-related hemodynamic signals that reflect heart rate variability. Motion data from six-axis sensor (12.5 Hz) capturing triaxial linear acceleration and angular velocity characterizes head movements. See supplement for more details."
        },
        {
            "title": "4 Method",
            "content": "As illustrated in Fig. 3, LoongX extracts multimodal features from diverse neural signals and fuses them into shared latent space in pair-wise manner. Using Diffusion Transformer (DiT), the original image is translated into an edited image conditioned on the fused features. Following three research questions, we conduct multi-label classification experiment (Sec. A.3) showing that EEG outperforms noise by 20%, and fusing all signals yields the highest F1 score. Combining neural signals with text achieves the best mAP, confirming modality complementarity. An input length of 8,192 gives the best performance with higher computational cost, motivating our frameworks design: cross-scale state-space encoder for long sequences and dynamic gated fusion for feature integration. 4 Figure 3: Overview of our proposed LoongX method for hands-free image editing. Receiving an input image, LoongX outputs an edited image using neural signals (and optional speech) as conditions. 4.1 Cross-Scale State Space Encoding CS3 encoder extracts multi-scale features from diverse signals using an adaptive feature pyramid. To further capture dynamic spatio-temporal patterns beyond fixed pyramid, CS3 uses structured state space model (S3M) [56] for efficient long-sequence encoding with linear complexity. To manage cost, it uses cross-feature mechanism that separately encodes temporal and channel information. Pyramid Encoding. single modality input signal RCL0 is fed into an N-layer (e.g., = 5 for EEG) adaptive average pooling (AAP) module (we set = 64 for EEG): {Pii = 1, ..., } = AAP(i) si = 2i (S), (2) Lsi Extracted embedding is computed as the concatenation of the feature pyramid = Concat({Pi}). State Space Encoding. To fully exploit both temporal and channel-wise dependencies in neural signals, we design cross-shaped spatiotemporal encoding scheme, where one axis focuses on temporal patterns and the other on channel-wise dynamics. Specifically, the input signal is padded from length L0 to L, where Spad RCL with signal intensity normalized to [1, 1]. The padded signals and its permuted version Spm RLC are passed to two parallel S3M blocks, S3M(1) and S3M(2), respectively: Z(1) = S3M(1)(Spad), Z(2) = S3M(2)(Spm), (3) where each S3M block uses the continuous-time diagonal state-space model: e(t) = ˆAe(t) + ˆBs(t), z(t) = ˆCe(t) + ˆDs(t), (4) where e(t) denotes the latent state at time t, and ˆA, ˆB, ˆC, ˆD are diagonal matrices that parameterize state transitions, input injection, state-to-output mapping, and direct input-to-output mapping, respectively. Due to the diagonal parameterization, the S3M block admits efficient computation with linear complexity O(L log L). Through the S3M blocks, Z(1) is down-sampled from length to dm, yielding Z(1) RCdm; Z(2) is further permuted and down-sampled to length dp via an AAP, giving Z(2) Rdm dp , where dm is set equal to C. Cross-Pyramid Aggregation. The encoder merges multi-scale and temporal streams along the channel dimension, resulting in: = ANP(cid:0) catc( Z(1), P, Z(2)) RCL(cid:1). (5) where = dm + dp + ΣN 2i d. The concatenated feature was projected via Adaptive Nonlinear Projection (ANP), which consists of two fully-connected layers, LayerNorm, ReLU, and dropout. The final embeded feature RCd is obtained. 5 4.2 Dynamic Gated Multimodal Fusion We propose the Dynamic Gated Fusion (DGF) module to dynamically bind pair of content and condition embeddings to unified latent space, which is further aligned with text embeddings. Our DGF includes gate mixing, adaptive affine modulation, and dynamic masking block. Gated Mixing. We calculate instance-wise and layer-wise mean µ and variance σ from input content embedding (e.g., EEG) RCL and condition embedding (e.g., PPG) RCL for further fusion into RCL while emphasising informative channels and suppressing noise: (cid:115) 1 (cid:115) 1 CL (cid:20) µinst σinst µlayer σlayer (cid:125) (cid:123)(cid:122) (cid:124) each entry RC1 (cid:0)Xc,t µlayer (cid:0)X:,t µinst ε = 103."
        },
        {
            "title": "1\nCL",
            "content": "Xc,t 1C (cid:88) (cid:88) X:,t"
        },
        {
            "title": "1\nL",
            "content": "+ ε + ε (cid:88) (cid:88) (cid:1)2 (cid:1) = (6) c,t c,t (cid:21) , where ε is regularization term for numerical stability and 1C is unit vector. 1-D Gating Network G() = σ(ConvReLUConv) is used to compute per-channel weights [0, 1]C1 from C, adaptively mixing statistics: µ = µinst + (1 g) µlayer, σ = σinst + (1 g) σlayer. (7) The content feature is then normalized by the adaptively gated mean µ and standard deviation σ as: ˆX = (X µ)/σ. Adaptive Affine Modulation. The conditional feature was averaged by global average pooling (GAP) as = 1 Y:,t. This averaged feature is then passed to the Affine Network ψ, which consists of multi-layer perceptron (MLP). The output is split into two affine coefficients γ and β: (cid:80) [γ, β] = ψ( Y), = (1 + γ) ˆX + β. (8) Dynamic Masking. Channel importance scores sc = 1 Yc,t are computed to select the top-k channels (k = ρY , ρ = 0.7) among the modulated features. Additionally, binary mask {0, 1}C is applied: (cid:80) Hc,: = Mc Hc,:. (9) Finally, the fused latent feature is residually fused with the original prompt/text embeddings before being fed into DiT decoder. Because DGF operates on arbitrary (C, L) tensors, it handles four types of modality fusion in LoongX: EEG-PPG, fNIRS-Motion, neural-prompt, and neural-pooled-prompt. 4.3 Conditional Diffusion The fused latent representation conditions DiT backbone [57] for image editing. The DiT model accepts the encoded input image and fused latent feature and outputs the edited image aligned with the semantic intention via fine-tuning. Specifically, DiT predicts velocity vθ(It, t, h) that is used to iteratively refine the latent image in uniform steps, 1 At inference time we apply (10) until = 0, yielding the edited image ˆI0. vθ(It, t, h), = It It 1 {1/T, 2/T, . . . , 1}. (10) 4.4 Pre-training and Finetuning We adopt two-phase process: 1) neural signal encoders (EEG is the most important one) are pretrained on neuro-text corpora, compressing public data and L-Mind, 2) The full stack is optionally fine-tuned with paired original images and ground truth edited images. 6 Pretraining. Signal encoders are pretrained to align with semantic embeddings using large-scale cognitive datasets [14] and L-Mind. CS3 encoders (EEG + PPG and fNIRS + Motion, respectively) are aligned to frozen text embeddings via symmetric NT-Xent loss: Lcon = 1 2M (cid:104) (cid:88) i=1 log esii esij (cid:80) log esii esji (cid:80) (cid:105) . (11) where sij = (z modalities. During pretraining, signal encoders are learned while text encoders stay frozen. qj)/τ , zi and qj are neural and text embeddings, and is the number of neural Finetuning. Encoders and the DiT are finetuned jointly on L-Mind, mapping user neural patterns to editing target following standard diffusion objective that minimizes the mean-squared velocity error. For an input image I0 and Gaussian noise ϵ (0, I), where αt is the cumulated noise schedule: LMSE = Et,I0,ϵ (cid:13)vθ(It, t, h) (cid:0) (cid:13) (cid:124)"
        },
        {
            "title": "5 Experiment",
            "content": "αt ϵ 1 αt I0 (cid:123)(cid:122) v(It,t,ϵ) (cid:1) (cid:13) 2 2. (cid:13) (cid:125) (12) To answer each research question (RQ) raised in Sec. 1, we comprehensively evaluate LoongXs capability of performing neural-driven image editing on the test set of L-Mind. The experimental setup, metrics from [51], and more implementation details are described in Sec. A.4.1. We choose OminiControl [49] as our baseline as it supports the text-conditioned image-editing based on DiTs. Table 1: Comparison between baseline methods and two LoongX paradigms: (i) neural signals only and (ii) neural signals enhanced by speech. Mean 95% confidence interval (CI) over three runs. Methods L1 () L2 () CLIP-I () DINO () CLIP-T () OminiControl (Text) OminiControl (Speech) 0.2632 0.006 0.2714 0.006 0.1161 0.010 0.1209 0. 0.6558 0.010 0.6146 0.009 0.4636 0.017 0.3717 0.013 0.2549 0.008 0.2501 0.008 LoongX (Neural Signals) 0.2509 0.006 0.1029 0.009 0.6605 0.009 0.4812 0.015 0.2436 0.008 0.4205 0.014 0.2588 0.009 LoongX (Signals+Speech) 0.2594 0.006 0.6374 0.009 0.1080 0. 5.1 Reliability of Neural Signals Answering RQ1: As shown in Table 1, neural signals can work as reliable indicators to drive semantic and structural image editing, outperforming language-only baselines on key perceptual metrics. Remarkably, neural-signal-only LoongX surpasses the text-based OminiControl baseline in discriminability (CLIP-I: 0.6605 vs. 0.6558) and robustness (DINO: 0.4812 vs. 0.4636). This demonstrates neural signals intrinsic capability to carry reliable semantic information for image editing. While pixel-based metrics (L1/L2) show slight increases, this trade-off highlights the prioritization of semantic fidelity over pixel-level stickiness. Introducing speech cues fused with neural signals further enhances semantic alignment (CLIP-T reaching the highest 0.2588), underscoring their combined strength in capturing nuanced user intentions for hands-free image editing. 5.2 Ablation Studies on Modality Contribution Answering RQ2: Different neural signal modalities contribute complementary strengths, enhancing discriminability, robustness, and semantic precision, respectively. We examine modality contributions in detail, as shown in Fig. 4. EEG signals alone enable basic high-level semantic editing, supported by the semantic discriminability of the extracted features (CLIP-I: 0.5457). Integrating fNIRS significantly improves feature robustness (DINO: from 0.2963 to 0.4811), highlighting the complementary nature of hemodynamic responses in enhancing signal completeness and structural fidelity. Including PPG and Motion improves global physiological awareness and indicates sensitivity to subtle engagement patterns (e.g., heartbeat and user movements) that express editing intent. They both contribute to features robustness and completeness to ensure stable CLIP-T score gains. Fig. 5 further explores EEG channels individual contributions, where each channel corresponds to specific scalp region as detailed in Table 3. The occipital cortex channel (Oz), which is in charge Figure 4: Evaluation results for different signal combinations using DGF. Figure 5: Evaluation results on different brain region signals where LoongX is trained and tested on each respective EEG channel. of visual processing, emerges as dominant in global editing effect (CLIP-I: 0.6619) and robustness (DINO: 0.4873), affirming its critical role in basic visual perception and processing tasks. Conversely, the frontopolar cortex (Fpz) provides superior semantic alignment (CLIP-T: 0.2481), consistent with its association with more complex cognitive processes. Specifically, Fpz provides decision control and attention regulation compared with basic visual perception provided by Oz, which precisely confirms the discovery patterns in medical anatomy. This channel-specific analysis provides insights valuable for targeted applications or constrained hardware settings. 5.3 Breakdown Analysis: Neural vs. Language-based Conditions Answering RQ3: Neural signals excel in lowlevel visual edits, while language excels in high-level semantics; combining both yields the most effective hybrid control. As shown in Fig. 6, neural signals (N) are particularly effective for more intuitive tasks like global texture editing (higher CLIP-I), reflecting strong visual discriminability and structural coherence. In object editing, neural signals are more capable of object removal than others, demonstrating their strength in conveying intuitive intent, though they remain limited in handling complex semantics. In contrast, text instructions (T) are inherently stronger in high-level semantic tasks (e.g., restore), highlighting their advantage in semantic alignment. When combined, neural signals and speech (N+S) inputs achieve the best semantic alignment (CLIP-T: 0.2588), showcasing the superior effectiveness of hybrid conditioning in capturing complex user intentions. 5.4 Ablation Studies on Model Architecture Figure 6: Breakdown results of text and neuraldriven image editing metrics. BG: background. Each architectural component of LoongX contributes uniquely, and their composition, especially with pretraining, unlocks comprehensive performance potential. The ablation study in Table 2 is conducted under the setting of fusing all signals and speech, exploring the impact of each proposed module. CS3 encoder basically enhances the completeness and smoothness of features via extracted features, reducing pixel-level error (L2 decreased by 5%), and DGF primarily enhances semantic alignment with textual instructions (CLIP-T improvement: 3.5%). Supplemented by pre-training, LoongX reaches optimal performance, indicating the important role of robust multimodal alignment and structured representation learning in maximizing editing performance. 8 Table 2: Ablation studies on the architecture of LoongX. Mean 95% CI over three runs. Pretrain CS3 DGF L1 () L2 () CLIP-I () DINO () CLIP-T () 0.3948 0.011 0.2645 0.005 0.2567 0.006 0.1047 0.004 0.6408 0.010 0.4588 0.012 0.5966 0.009 0.1099 0.003 0.2629 0.005 0.6025 0.009 0.2648 0.006 0.6319 0.009 0.2594 0.006 0.1080 0.004 0.6374 0.009 0.1106 0.003 0.1124 0. 0.2584 0.010 0.2248 0.007 0.3992 0.011 0.2620 0.007 0.4162 0.012 0.2534 0.008 0.4205 0.012 0.2588 0.009 Figure 7: Qualitative evaluation comparing the text prompt-based method and our neural-driven methods for four editing categories: (a) background, (b) object, (c) global, and (d) text editing. 5.5 Qualitative Analysis and Limitations Qualitative examples confirm LoongXs intuitive editing capabilities, with limitations mostly arising in abstract or ambiguous complex intentions. Qualitative results in Fig. 7 illustrate that neural-signal-driven editing effectively handles visual and structural modifications, such as background replacing and global adjustments. However, the fused neural-language method better captures nuanced instructions involving abstract semantics (e.g., modify the text information\"). Despite significant advancements, entity consistency (e.g., the style of the little girl in Fig. 7(b)) remains limitation of current editing models. Moreover, highly abstract or ambiguous instructions occasionally still pose challenges (e.g., winged white animal in Fig 11\" and several more failure cases shown in Fig. 14), indicating areas where further refinement of entity interpretation and disambiguation from neural data is necessary."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented LoongX, novel framework for hands-free image editing by conditioning diffusion models on multimodal neural signals, achieving comparable or superior performance to traditional text-driven baselines. Looking forward, the portability of our wireless setup opens exciting possibilities for real-world applications in immersive environments. Future work may explore integrating LoongX with VR/XR platforms to enable intuitive cognitive interaction, and further align neural representations with world models for projecting human intention into an interactive virtual world, paving the way toward mind-driven control in fully synthetic realities."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [2] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. UltraEdit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [3] Bingyuan Wang, Qifeng Chen, and Zeyu Wang. Diffusion-based visual art creation: survey and new perspectives. ACM Computing Surveys, 57(10):137, 2024. [4] OpenAI. Introducing 4o image generation, 2025. [5] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In International Conference on Learning Representations, 2023. [6] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [7] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, MingHsuan Yang, Kevin Patrick Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. In International Conference on Machine Learning, pages 40554075, 2023. [8] Yu Zeng, Zhe Lin, and Vishal Patel. Sketchedit: Mask-free local image manipulation with partial sketches. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 59515961, 2022. [9] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. [10] Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. InstantDrag: Improving interactivity in drag-based image editing. In SIGGRAPH Asia, pages 110, 2024. [11] Zichen Liu, Yue Yu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Wen Wang, Zhiheng Liu, Qifeng Chen, and Yujun Shen. Magicquill: An intelligent interactive image editing system. arXiv preprint arXiv:2411.09703, 2024. [12] Ziqi Jiang, Zhen Wang, and Long Chen. CLIPDrag: Combining text-based and drag-based instructions for image editing. In International Conference on Learning Representations, 2025. [13] Xincheng Shuai, Henghui Ding, Xingjun Ma, Rongcheng Tu, Yu-Gang Jiang, and Dacheng Tao. survey of multimodal-guided image editing with text-to-image diffusion models. arXiv preprint arXiv:2406.14555, 2024. [14] Yuming Jiang, Ziqi Huang, Xingang Pan, Chen Change Loy, and Ziwei Liu. Talk-to-edit: Fine-grained facial editing via dialog. In IEEE/CVF International Conference on Computer Vision, pages 1379913808, 2021. [15] Yue Yang, Kaipeng Zhang, Yuying Ge, Wenqi Shao, Zeyue Xue, Yu Qiao, and Ping Luo. Align, adapt and inject: Audio-guided image generation, editing and stylization. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 34753479, 2024. 10 [16] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. OneLLM: One framework to align all modalities with language. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2658426595, 2024. [17] Emily Allen, Ghislain St-Yves, Yihan Wu, Jesse Breedlove, Jacob Prince, Logan Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. massive 7t fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience, 25(1):116126, 2022. [18] Yiming Wang, Bin Zhang, and Lamei Di. Research progress of EEG-based emotion recognition: survey. ACM Computing Surveys, 56(11):149, 2024. [19] Yiqun Duan, Jinzhao Zhou, Zhen Wang, Yu-Kai Wang, and Chin-teng Lin. Dewave: Discrete encoding of eeg waves for EEG to text translation. Advances in Neural Information Processing Systems, 36:99079918, 2023. [20] Yohann Benchetrit, Hubert Banville, and Jean-Remi King. Brain decoding: toward real-time reconstruction of visual perception. In International Conference on Learning Representations, 2024. [21] Dünya Baradari, Nataliya Kosmyna, Oscar Petrov, Rebecah Kaplun, and Pattie Maes. NeuroChat: neuroadaptive AI chatbot for customizing learning experiences. arXiv preprint arXiv:2503.07599, 2025. [22] Yunpeng Bai, Xintao Wang, Yan-Pei Cao, Yixiao Ge, Chun Yuan, and Ying Shan. DreamDiffusion: High-quality EEG-to-image generation with temporal masked signal modeling and CLIP alignment. In European Conference on Computer Vision, pages 472488, 2024. [23] Xuan-Hao Liu, Yan-Kai Liu, Yansen Wang, Kan Ren, Hanwen Shi, Zilong Wang, Dongsheng Li, Bao-Liang Lu, and Wei-Long Zheng. EEG2Video: Towards decoding dynamic visual perception from EEG signals. Advances in Neural Information Processing Systems, 37:7224572273, 2024. [24] Keith M. Davis, III, Carlos de la Torre-Ortiz, and Tuukka Ruotsalo. Brain-supervised image editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18480 18489, 2022. [25] Jingyang Huo, Yikai Wang, Yun Wang, Xuelin Qian, Chong Li, Yanwei Fu, and Jianfeng Feng. Neuropictor: Refining fMRI-to-image reconstruction via multi-individual pretraining and multi-level modulation. In European Conference on Computer Vision, pages 5673, 2024. [26] Jing Cai, Alex E. Hadjinicolaou, Angelique C. Paulk, Daniel J. Soper, Tian Xia, Alexander F. Wang, John D. Rolston, R. Mark Richardson, Ziv M. Williams, and Sydney S. Cash. Natural language processing models reveal neural dynamics of human conversation. Nature Communications, 16(1):3376, 2025. [27] Niklas Brake, Flavie Duc, Alexander Rokos, Francis Arseneau, Shiva Shahiri, Anmar Khadra, and Gilles Plourde. neurophysiological basis for aperiodic eeg and the background spectral trend. Nature Communications, 15(1):1514, 2024. [28] Yuxuan Li, Jesse K. Pazdera, and Michael J. Kahana. EEG decoders track memory dynamics. Nature Communications, 15(1):2981, 2024. [29] Yipeng Yu, Cunle Qian, Zhaohui Wu, and Gang Pan. Mind-controlled ratbot: brain-tobrain system. In IEEE International Conference on Pervasive Computing and Communication Workshops, pages 228231, 2014. [30] Xuyang Si, Hao He, Jun Yu, and Dong Ming. Cross-subject emotion recognition brain-computer interface based on fNIRS and DBJNet. Cyborg and Bionic Systems, 4:0045, 2023. [31] Muhammad Arsalan and Muhammad Majid. Human stress classification during public speaking using physiological signals. Computers in Biology and Medicine, 133:104377, 2021. [32] Amir Tazarv, Shervin Labbaf, Steven M. Reich, Nikil Dutt, Amir M. Rahmani, and Marco Levorato. Personalized stress monitoring using wearable sensors in everyday settings. In Annual International Conference of the IEEE Engineering in Medicine & Biology Society, pages 73327335, 2021. [33] Lu Cao, Dandan Huang, Yue Zhang, Xiaowei Jiang, and Yanan Chen. Brain decoding using fNIRS. In AAAI Conference on Artificial Intelligence, pages 1260212611, 2021. 11 [34] Chang-Hee Han, Euijin Kim, and Chang-Hwan Im. Development of braincomputer interface toggle switch with low false-positive rate using respiration-modulated photoplethysmography. Sensors, 20(2):348, 2020. [35] Boyu Li, Mingjie Li, Jie Xia, Hao Jin, Shurong Dong, and Jikui Luo. Hybrid integrated wearable patch for brain EEG-fNIRS monitoring. Sensors, 24(15):4847, 2024. [36] Nicolás Nieto, Victoria Peterson, Hugo Leonardo Rufiner, Juan Esteban Kamienkowski, and Ruben Spies. Thinking out loud, an open-access EEG-based BCI dataset for inner speech recognition. Scientific Data, 9(1):52, 2022. [37] Bradley J. Edelman, Shuailei Zhang, Gerwin Schalk, Peter Brunner, Gernot Müller-Putz, Cuntai Guan, and Bin He. Non-invasive brain-computer interfaces: State of the art and trends. IEEE Reviews in Biomedical Engineering, 18:2649, 2025. [38] Seif Eldawlatly. On the role of generative artificial intelligence in the development of braincomputer interfaces. BMC Biomedical Engineering, 6:4, 2024. [39] Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445314463, 2023. [40] Kalyan Tripathy, Zachary Markow, Andrew Fishell, Arefeh Sherafati, Tracy BurnsYocum, Mariel Schroeder, Alexandra Svoboda, Adam Eggebrecht, Mark Anastasio, Bradley Schlaggar, et al. Decoding visual information from high-density diffuse optical tomography neuroimaging data. NeuroImage, 226:117516, 2021. [41] Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu, and Baochang Zhang. Controllable mind visual diffusion model. In AAAI Conference on Artificial Intelligence, pages 69356943, 2024. [42] Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, and Tanishq Mathew Abraham. Reconstructing the minds eye: fMRI-to-image with contrastive learning and diffusion priors. Advances in Neural Information Processing Systems, 2023. [43] Paul S. Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A. Norman, and Tanishq Mathew Abraham. MindEye2: Shared-subject models enable fMRI-to-image with 1 hour of data. In International Conference on Machine Learning, pages 4403844059, 2024. [44] Xuan-Hao Liu, Yan-Kai Liu, Yansen Wang, Kan Ren, Hanwen Shi, Zilong Wang, Dongsheng Li, Bao-Liang Lu, and Wei-Long Zheng. EEG2Video: Towards decoding dynamic visual perception from EEG signals. Advances in Neural Information Processing Systems, 2024. [45] Michel Adamic, Wellington Avelino, Anna Brandenberger, Bryan Chiang, Hunter Davis, Stephen Fay, Andrew Gregory, Aayush Gupta, Raphael Hotter, Grace Jiang, et al. Progress towards decoding visual imagery via fNIRS. arXiv preprint arXiv:2406.07662, 2024. [46] OpenAI. GPT-4 technical report, 2024. [47] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [48] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. [49] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [50] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. OminiControl2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. [51] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [52] Xueyun Tian, Wei Li, Bingbing Xu, Yige Yuan, Yuanzhuo Wang, and Huawei Shen. MIGE: unified framework for multimodal instruction-based image generation and editing. arXiv preprint arXiv:2502.21291, 2025. [53] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. ACE++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. [54] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. SEED-Data-Edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. [55] Tristan Stenner, Chadwick Boulay, Matthew Grivich, David Medine, Christian Kothe, Tobias Herzke, Christian Schausner, Giso Grimm, Loem, Arthur Biancarelli, Boris Mansencal, Paul Maanen, Jérémy Frey, Jidong Chen, Kyle Crane, Samuel Powell, Pierre Clisson, and Paul Fix. Sccn/liblsl: v1.16.2, 2023. [56] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. [57] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [58] Peter Praamstra, Luc Boutsen, and Glyn W. Humphreys. Frontoparietal control of spatial attention and motor intention in human eeg. Journal of Neurophysiology, 94(1):764774, 2005. PMID: 15744008. [59] Ankita Sengupta, Sanjna Banerjee, Suhas Ganesh, Shrey Grover, and Devarajan Sridharan. The right posterior parietal cortex mediates spatial reorienting of attentional choice bias. Nature Communications, 15(1):6938, 2024. [60] Naomi P. Friedman and Trevor W. Robbins. The role of prefrontal cortex in cognitive control and executive function. Neuropsychopharmacology, 47(1):7289, 2022. [61] Jae-Hyun Kim, Dong-Hyun Ma, Eunji Jung, Ilsong Choi, and Seung-Hee Lee. Gated feedforward inhibition in the frontal cortex releases goal-directed action. Nature Neuroscience, 24(10):14521464, 2021. [62] Bülent Y. Özkara. An introduction to the event-related potential technique. Journal of Consumer and Consumption Research, 11(2):437441, 2019. [63] Kavya Agrawal, Shashwat Mishra, Shashwat Sinha, Vineeta Khemchandani, Sushil Chandra, and Nachiket Milind Wadalkar. Chapter 29 - machine learning-based workload identification using functional near-infrared spectroscopy (fnirs) data. In M.A. Ansari, R.S. Anand, Pragati Tripathi, Rajat Mehrotra, and Md Belal Bin Heyat, editors, Artificial Intelligence in Biomedical and Modern Healthcare Informatics, pages 299312. Academic Press, 2025. [64] Nasim Raufi and Leonardo Longo. Evaluating eeg alpha-to-theta and theta-to-alpha band ratios as indexes of mental workload. arXiv preprint arXiv:2202.12937, 2022. [65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763, 2021."
        },
        {
            "title": "A Technical Appendix and Supplementary Information",
            "content": "A.1 Preliminary: Analysis of Brain Region Function and Mechanism We employ noninvasive multimodal sensing system that synchronously records neural, hemodynamic, and peripheral vascular signals, including EEG, fNIRS, and PPG (Fig. 8 (a)). This setup enables comprehensive monitoring of brain activity during humancomputer interaction. fNIRS employs near-infrared light in the 690930 nm range to penetrate scalp and skull tissues. Neural activation leads to increased local cerebral blood flow and metabolic demand, resulting in rise in oxygenated hemoglobin (HbO) and reduction in deoxygenated hemoglobin (HbR). These changes cause detectable variations in light absorption at the detector. fNIRS thus provides second-level sensitivity to the slow hemodynamic responses associated with cortical processing. EEG is recorded via hydrogel-based electrodes placed over the scalp, capturing millisecond-scale voltage fluctuations resulting from synchronized postsynaptic potentials in cortical pyramidal neurons. This modality offers high temporal precision for observing rapid fluctuations in cortical excitability and sensorimotor responses. PPG detects volumetric changes in blood flow via near-infrared light, enabling continuous measurement of pulse rate and vascular compliance. The sensor is co-located with fNIRS optodes, sharing similar optical pathways but tuned for peripheral cardiovascular features. This multimodal approach provides synergistic view of neural function, combining the high temporal resolution of EEG with the metabolic and vascular insights from fNIRS and PPG. Such cross-modal sensing is crucial for modeling the perception-decision-action cycle in neuroadaptive systems. Figure 8: (a) Signal acquisition mechanism. (b) Cognitive function of different brain regions. To enable hands-free image editing, we integrate multimodal neural and physiological signals to decode user intent and cognitive state in real time. Specifically: Midline parietal EEG (Pz) reflects spatial attention and visuomotor integration, supporting the allocation of attention to target areas and coordination of motor plans during editing tasks [58]. Right prefrontal EEG (Fp2) is linked to emotional regulation and motivational drive, aligning with findings from frontal alpha asymmetry studies [59]. Frontopolar EEG (Fz/Fpz) tracks attention control and task initiation, facilitating the onset and modulation of editing operations [60, 61]. Occipital EEG (Oz) encodes visual perception and image processing load, enabling evaluation of visual changes and edit quality [62]. fNIRS over anterior medial prefrontal cortex (aMPFC) measures cognitive load, motivation, and emotional valence through hemodynamic activity [63]. PPG signals from the same region capture heart rate variability and autonomic arousal, which reflect user stress and engagement [31, 32]. This multimodal mapping (Fig. 8 (b)) enables the system to continuously adapt to users cognitive focus, affective state, and mental workload, thereby supporting more responsive and intuitive editing experience. A.2 Subject information 12 healthy college students (6 females and 6 males) were recruited as the subjects for data collection. They have mean age of 24.52.5 years and normal (or corrected-to-normal) vision. All volunteers were informed of the experimental process and received financial compensation. All volunteers signed the consent forms prior to the experiment, which was approved by the Ethics Committee of 14 Table 3: Functional Roles of Multimodal Neural Signals in Hands-Free Image Editing. The Ch refers to the channel. Signal (Channel) Cortical Region Primary Function Roles in Image Editing EEG Ch 0 (Pz) Parietal cortex EEG Ch 1 (Fp2) Prefrontal cortex EEG Ch 2 (Fpz) Frontopolar cortex EEG Ch 3 (Oz) Occipital cortex Spatial attention, visuomotor integration Focuses on specific image areas, targets object localization [58] Emotion regulation, motivational drive Generates and regulates intentional editing actions [59] Attention task initiation control, Triggers editing intention, starts/stops editing operations [60, 61] Visual perception, image processing Perceives visual changes, evaluates whether edits meet expectations [62] fNIRS (aMPFC) Anterior medial prefrontal cortex High-level tion, emotional valence cognimotivation, Indicates editing intent intensity, emotional confidence, and mental workload [63] PPG (aMPFC) Cardiovascular or autonomic system Heart rate variability, arousal, stress Monitors cognitive stress or emotional arousal during editing [31, 32] the corresponding Institutional Review Board for Human Research Protections (Protocol ID: No. 067 (2019)). The attention score in this study is objectively computed using the ratio between the power of the EEG alpha band (812 Hz) and the theta band (48 Hz), as shown below: Attention Score = Alpha Band Power Theta Band Power (13) This ratio has been widely used in cognitive neuroscience research as neurophysiological index of attentional control and mental workload. For instance, Raufi [64] demonstrated that the alpha-to-theta and theta-to-alpha band ratios are reliable indicators of self-reported mental workload levels in EEG-based studies. Table 4: Summary of attention scores and neural-signal based image editing metrics per subject CLIP-I DINO CLIP-T Subject Gender Age #Samples Attention L2 1 2 3 4 5 6 7 8 9 10 11 12 Female Female Female Female Female Female Male Male Male Male Male Male 25 29 26 22 28 29 22 23 22 24 24 22 2003 2000 2001 1999 1992 1964 1988 1993 1988 2000 2000 2000 0.0887 0.0817 0.1340 0.0739 0.1218 0.0822 0.0851 0.1105 0.1500 0.1298 0.0954 0.0971 0.2657 0.2416 0.2448 0.2533 0.2552 0.2511 0.2711 0.2528 0.2497 0.2657 0.2744 0. 0.1109 0.0950 0.0963 0.1005 0.1031 0.1000 0.1160 0.1017 0.0998 0.1144 0.1151 0.1034 0.6370 0.6575 0.6660 0.6394 0.6144 0.6449 0.6515 0.6638 0.6355 0.6194 0.6386 0.6213 0.4890 0.5021 0.4878 0.4606 0.4157 0.4564 0.4634 0.4833 0.4571 0.4240 0.4339 0.4323 0.2196 0.2249 0.2337 0.2270 0.2260 0.2213 0.2234 0.2242 0.2212 0.2220 0.2299 0.2250 A.3 Preliminary Analysis: Editing Type Classification Experiment LoongX is proposed to address three key research questions: 1. Can neural signals serve as reliable conditions for image editing? (Does it really work?) 2. What kind of information is conveyed by multimodal neural signals? (What do they actually contribute to image editing?) 3. How do neural-based and language-based conditions differ in image editing? Can we combine their strengths to enable hands-free editing more effectively? 15 In response to these problems, we conduct premise exploration based on classification experiment and design the LoongX model based on the findings. Finally, we present modular architecture comprising unified multimodal encoding, dynamic multimodal data fusion, and diffusion-based conditional generation. Based on these, LoongX can perform robust hands-free image editing by translating user neural states into structured conditions for diffusion model. To examine whether neural signals can reliably encode semantic conditions for image editing, we perform an exploratory classification experiment where the task is to predict editing types from neural signals or text. We use the 22,691 training instances and 1,200 test instances in L-Mind. As each editing instance can involve multiple editing types, we implement simple multilayer perception (MLP) with three nonlinear-activated linear layers and conduct multi-label classification experiment that recognizes all involved editing type labels for an instance, via text embeddings or neural signals, as condition. We compare models trained with random noise, only text prompts, EEG, fNIRS, PPG, combinations of multimodal neural signals, and fusion of both text and signals. As shown in Fig. 9, the classification results validate the informativeness and complementarity of neural signals for image editing conditions. Fig. 9(a) shows that using neural signals, especially the EEG signal itself, can achieve significantly better classification performance than random noise (as seen from over 7% mAP improvement). fNIRS contributes to recall performance gain compared with random noise since it provides more complete and robust information, which is more important for recall. As image editing requires discriminant semantic features, it shows that neural networks can still effectively recognize the editing types (over 60% precision) based on EEG signals and can even achieve comparable performance with text prompts in some cases. Results can also initially respond to question 2. As precision depends on the discriminability of features, EEG contributes more discriminability with less noise. For recall, robustness and completeness contribute more. Therefore, fNIRS becomes more stable with less fluctuation. Though PPG and motion do not affect classification performance significantly as their volatility is small, they are expected to provide richer background information to stabilize performance. Now, what will the performance change if we combine these signals? Fig. 9(b) shows that binding all neural signals (via simple concat and MLP) achieves the best performance, and fusing only EEG and fNIRS can achieve comparable performance with only recall reduction. Moreover, though neural signals are not more discriminant and robust than text embeddings, fusing both can achieve even better performance than text only. That is, the two types of data can complement the missing key information to achieve better category recognition in image editing tasks, which shows higher performance as more powerful condition for further generative models. As the neural signals all have different shapes, unifying the input sequence into unified length is necessary. Simple padding and truncating to fixed length is one of the most reliable methods for preserving most information in raw signals. We also investigate the influence of input sequence length for EEG as neural signal representative. Results in Fig. 9(c) show that truncating and padding EEG signal sequences to unified 8,192 can achieve the best performance. While the longer sequence ensures more reliable performance, the computation cost will become burden as the sequence length increases. trade-off method is needed to encode the most valid information in signals while not bringing unbearable computational costs. Therefore, we design the Cross-Scale State Space (CS3) model to ensure the best trade-off between performance and computational cost. A.4 Additional Experiment Details A.4.1 Experimental Setup Implementation Details. All models are trained on eight NVIDIA H100 cards with AdamW (learning rate 1.0, weight decay 0.01). Text prompts are embedded by T5-XXL [65] and CLIP [66]; neural streams are encoded by the proposed CS3. Unless stated otherwise, we use the full EEG montage (Fz, Fp2, O2, Pz, Cz) sampled at 256 Hz and down-sampled to 32 Hz after band-pass filtering. Inference runs at 8 steps with classifier-free guidance = 4. We choose OminiControl [49] as our baseline as it supports the text-conditioned image-editing based on DiTs. We also implement LoongX using only neural signals (EEG, fNIRS, PPG and Motion) and using both text prompts and 16 Figure 9: Multi-label classification result under different settings: (a) different single input modalities; (b) different modality fusion combinations; (c) different EEG input sequence lengths. neural signals. We load the pretrained weights from FLUX.1-dev 2 and use low-rank approximation (LoRA) for fine-tuning. Evaluation Metrics. We follow the setting in [51] to quantitatively assess the performance of LoongX using 5 metrics: 1) L1 Distance (Mean Absolute Error): Calculates the average absolute difference between corresponding pixels in edited and ground truth tar images. 2) L2 Distance (Mean Squared Error): Computes the average squared difference between pixels. Penalizes large errors more heavily than L1. 3) CLIP-I Score: Evaluating semantic similarity between model-edited images and ground truth target images. 4) DINO Score: Assessing fine-grained structural similarity at the feature level. 5) CLIP-T Score: Evaluating semantic similarity between image and textual prompts. A.4.2 Supplementary Case Studies Qualitative comparisons are presented in Figures 10-13, corresponding to the four broad editing categories: Global, Background, Object, and Text Editing. For clarity and conciseness in the figures, the original lengthy instructions have been distilled into single-sentence descriptions without altering their intended meaning. The editing results of our neural-driven and neural-speech fusion methods consistently outperform text-prompt-based editing results, demonstrating superior alignment with human intent and greater editing precision. Notably, Text Editing presents more complex challenge compared to other categories. Given the current limitations of backbone models (with the exception of commercial models like GPT-4o), text-based edits remain difficult. As evidenced by the examples, neural-driven approaches exhibit stronger ability to align with human intent, making the editing process more intuitive and effective. It is foreseeable that in the near future, as reliable image-editing backbones become more accessible, neural-driven image editing will further stabilize and mature, evolving into an indispensable tool for everyday creative workflows. Figure 14 specifically analyzes three characteristic failure modes: (1) cases involving overly imaginative descriptions that deviate significantly from the training data distribution (e.g., \"long-legged space creature\"), (2) ambiguous instructions with insufficient semantic details (particularly evident in case (b) where background retention specifications were omitted), and (3) challenges posed by non-standard input image dimensions (such as panoramic aspect ratios). These failure cases provide valuable insights into the current limitations of neural-based editing systems. A.5 Limitations Discussion While LoongX demonstrates strong performance in neural-driven image editing, several limitations remain. First, the current dataset was collected from relatively homogeneous group of 12 healthy young adults. Although the model performs well within this cohort, generalization to broader 2https://huggingface.co/black-forest-labs/FLUX.1-dev 17 Figure 10: Qualitative comparison of our neural-driven and speech-neural fusion methods and textprompt baseline for Global Editing category. 18 Figure 11: Qualitative comparison of our neural-driven and speech-neural fusion methods and textprompt baseline for Background Editing category. 19 Figure 12: Qualitative comparison of our neural-driven and speech-neural fusion methods and textprompt baseline for Object Editing category. 20 Figure 13: Qualitative comparison of our neural-driven and speech-neural fusion methods and textprompt baseline for Text Editing category. Figure 14: Qualitative analysis on three failure cases: (a) Overly exaggerated descriptions, e.g., \"long-legged space creature\"; (b) Vague instructions lacking detail, such as omitting whether to retain the background; (c) Uncommon image dimensions, e.g., panoramic input images. 21 populations (e.g., different age groups or individuals with neurological conditions) is not yet fully validated. Moreover, the neural signals were acquired using low-density EEG and fNIRS systems, which, despite their practicality and portability, offer limited spatial resolution compared to highdensity or invasive setups. This constraint may affect the systems ability to capture fine-grained neural representations. The robustness of LoongX under varying data distributions and noisy conditions has not been systematically explored. Although some resilience is expected from the multimodal design and the DUAF fusion strategy, comprehensive stress testing against motion artifacts, sensor dropout, or environmental interference is necessary next step for real-world deployment. Furthermore, while the BCI system we use is designed to work across participants, it may benefit from test-time adaptation or few-shot user-specific finetuning to account for individual variability in neural signatures, which can differ significantly across users. Finally, the interpretability of the learned neural representations remains limited. While the CS3 encoder effectively distills signal patterns for downstream editing, it is not yet clear how these latent features relate to interpretable cognitive states or intentions. Improving the transparency and explanatory power of the system will be essential for broader acceptance and responsible deployment, especially in domains involving sensitive user data. A.6 Broader Societal Impacts The proposed neural-driven image editing technique has the potential for significant positive societal impact by improving accessibility for individuals with motor or communication impairments, enabling more inclusive creative workflows, and lowering the barrier to content creation. By enabling handsfree and intuitive interaction with generative models, this technology could foster greater participation in digital art, design, and communication, particularly for users with physical limitations. However, as with other generative and brain-computer interface technologies, there are potential negative societal impacts. Malicious or unintended uses may include the creation of manipulated or deceptive media, privacy violations stemming from the misuse of neurophysiological data, or unauthorized surveillance. There are also concerns regarding fairness, as disparities in device availability or neural signal quality across user populations could exacerbate existing inequities. To mitigate these risks, we recommend the implementation of safeguards such as user authentication, audit trails for sensitive editing actions, and transparent communication regarding data usage and model limitations. Responsible deployment should include ongoing monitoring and mechanisms to address feedback and misuse. A.7 Safety and Ethics Recognizing the potential for misuse of proposed models and multimodal neural datasets, we are committed to responsible release practices. We will require users requesting access to the models or data to undergo an application and review process, ensuring alignment with ethical guidelines and legitimate research or clinical objectives. Usage agreements will prohibit malicious activities, and access may be revoked in the event of violations. For dataset release, we apply stringent filtering to remove personally identifiable, sensitive, or potentially harmful content. For model access, we will implement usage restrictions, safety filters, and ongoing monitoring to prevent abuse. Additionally, clear documentation outlining acceptable use cases, known limitations, and recommended best practices will be provided to all users. We will continuously evaluate the impact and usage of the released resources and remain open to community feedback to improve the effectiveness of these safeguards over time."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "NJU",
        "NUS",
        "RIT",
        "SII",
        "Shanghai AI Lab",
        "USTC",
        "Zhejiang University"
    ]
}