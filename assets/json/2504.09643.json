{
    "paper_title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
    "authors": [
        "Nikita Sorokin",
        "Ivan Sedykh",
        "Valentin Malykh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality. One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance. Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 3 4 6 9 0 . 4 0 5 2 : r Iterative Self-Training for Code Generation via Reinforced Re-Ranking Nikita Sorokin,1, Ivan Sedykh,1, and Valentin Malykh1, 1 MTS AI 2 International IT University {n.sorokin,i.sedykh}@mts.ai, valentin.malykh@phystech.edu Abstract. Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality. One effective way to enhance code generation is by pairing code generation model with reranker model, which selects the best solution from the generated samples. We propose novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing generative model with reward model, our approach emphasizes the development of robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance. Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language. Keywords: Code Generation Reinforcement Learning Proximal Policy Optimization Reward Models"
        },
        {
            "title": "Introduction",
            "content": "The generation of high-quality code using artificial intelligence models has become critical area of research, fueled by the increasing complexity of software development tasks. Code generation models have demonstrated their potential, but traditional models, which generate code token by token, often struggle with Equal contribution. Correspondence to: Nikita Sorokin n.sorokin@mts.ai 2 N. Sorokin, I. Sedykh, V. Malykh coherence and accuracy due to the stochastic nature of their outputs. This challenge is evident in code generation benchmarks such as HumanEvalX [23] and MultiPL-E [4], where the disparity between Pass@1 and Pass@100 metrics can exceed two-fold. To address these issues, we introduce RewardRanker, reranker model designed to enhance the quality of generated code by leveraging multiple sampled solutions. Rerankers offer an effective solution to the inherent variability of code generation models by selecting the most promising candidates from pool. This not only improves immediate output quality, but also refines the models longterm performance through iterative self-training. We show our model training setup in Fig. 1. Fig. 1. Iterative Self-Training Workflow for RewardRanker. The process starts with supervised fine-tuning (A), followed by training the RewardRanker model (B). PPObased model (C) is then trained, generating new examples that are evaluated to produce both positive and hard negative samples (D). These samples are fed back into the process for further refinement and retraining (E), completing the iterative loop. Our evaluation on the MultiPL-E benchmark demonstrates that RewardRanker, with 13.4B parameter model, outperforms larger models, including 33B parameter model, while being three times faster. The model also achieves competitive performance with GPT-4, surpassing it in C++ programming language. In summary, this paper introduces the following contributions: Iterative Self-Training for Code Generation via Reinforced Re-Ranking 3 (i) We introduce RewardRanker, novel iterative self-training approach for reward model optimization, leveraging Proximal Policy Optimization (PPO) to refine code reranking. This iterative approach improves reranking precision and enhances code generation quality by continually fine-tuning the reward model with self-generated data. (ii) Our approach enables smaller models (e.g., 13.4B parameters) to outperform much larger models (e.g., 33B parameters and GPT4), providing more resource-efficient solution that maintains high code generation performance and reranking quality. (iii) We emphasize the effectiveness of training with both correct and hard negative examples, allowing the model to generalize more robustly and improve reranking decisions across diverse coding tasks."
        },
        {
            "title": "2 Related Work",
            "content": "The generation of source code using large language models (LLMs) has garnered significant attention, with numerous studies focusing on improving both the functional correctness and efficiency of these models. Initial experiments with models like GPT-Neo [3] and GPT-J [20] demonstrated that incorporating code into the training data enables effective program synthesis, even for medium-sized models. Concurrently, specialized models such as CodeBERT [7], GraphCodeBERT [10], Codex [5], CodeT5 [21], UnixCoder [9], CodeGen [16], StarCoder [14], phi-1 [8], and AlphaCode [13] have been developed to enhance code understanding and generation from natural language prompts. Reinforcement learning (RL) approaches have also been applied to code generation. Notably, CodeRL [12] and PPOCoder [19] utilize actor-critic deep RL to optimize functional correctness by testing generated code against predefined test cases. These methods highlight the potential of RL in refining code generation models to produce more accurate and executable code. Additionally, Reinforcement Learning from Human Feedback (RLHF) methodologies, including the Proximal Policy Optimization (PPO) algorithm, have been applied to align LLMs more closely with human preferences. Christiano et al. introduced RLHF, emphasizing the importance of human preferences in training models [6]. This approach has been further explored and refined in various contexts, including code generation. [15], titled LEVER: Learning to Verify Language-to-Code Generation with Execution, presents an approach to improve language-to-code generation by training verifiers that assess the correctness of generated programs based on their execution results. Their method, combines the natural language input, the program itself, and its execution results to train verifiers that rerank sampled solutions from language models. This approach has shown significant improvements across various datasets, including table QA, math QA, and basic Python programming, with improvements ranging from 4.6% to 10.9% over strong baselines using execution error pruning. Our work builds upon the foundations laid by [15], addressing some of its limitations and extending the capabilities of verification in language-to-code gen4 N. Sorokin, I. Sedykh, V. Malykh eration. While LEVER focuses on reranking using execution results and verifiers, our approach, RewardRanker, leverages reinforcement learning and reward models to enhance the generation process itself. Specifically, we use Proximal Policy Optimization (PPO) algorithm to iteratively refine our model by incorporating both correct and incorrect solutions into the learning process, which is crucial for improving code generation capabilities. Additionally, unlike LEVER, our approach does not require predefined tests during the inference stagetests are only necessary during the training phase. This overcomes significant limitation of the approach in [15], making RewardRanker more flexible and scalable."
        },
        {
            "title": "3 Datasets",
            "content": "Our research draws on diverse dataset, primarily from CodeContests [13] and public Codeforces solutions, enriched with metadata. For supervised fine-tuning, we structured the data into prompt-completion pairs, using problem statements as prompts and corresponding solutions as completions. We combined CodeContests and Codeforces data in equal measure, producing balanced dataset of 1.2 million samples, each averaging 1,500 tokens. Samples longer than 4,000 tokens were excluded to optimize memory and remove outliers. For alignment training, we formatted data as prompt-preferred-disfavored triplets. CodeContests mix of correct and incorrect solutions suited this structure, with OK verdicts marking preferred solutions (positive) and others as disfavored (negative). Triplets were formed using both minimal Levenshtein distance pairs and random incorrect-correct pairs, yielding robust dataset of 2 million triplets. Evaluation Datasets MultiPL-E [4] is comprehensive system designed for translating unit test-driven code generation benchmarks into multiple programming languages, thus enabling multi-language code generation evaluation. MultiPLE translates two widely-used Python benchmarks, HumanEval [5] and MBPP [1], into 18 additional programming languages, encompassing various paradigms and popularity levels. This system supports diverse range of languages, making it possible to evaluate code generation models across different languages consistently. MBPP (ManyBenchmarks for Python Programs) dataset, introduced by Austin et al. [2], is designed for evaluating program synthesis models. It consists of 974 Python programming problems, which are sourced from real-world coding challenges and cover variety of algorithmic tasks. Each problem includes natural language description, corresponding Python solution, and test cases for validation."
        },
        {
            "title": "4 Method",
            "content": "Our approach leverages reranker model, RewardRanker, to enhance code generation quality through an iterative self-training cycle that refines the reward model. Each cycle involves supervised fine-tuning, reward model training, and Iterative Self-Training for Code Generation via Reinforced Re-Ranking 5 Proximal Policy Optimization (PPO) for code generation, followed by new example generation, evaluation, and retraining. Supervised Fine-Tuning (SFT) The generator model is initially fine-tuned on dataset using causal language modeling to adapt to the domain. This step establishes foundation for subsequent preference-based training, improving reranker and PPO performance. Reward Model Training We train reward model to score generated code outputs, using loss function inspired by the Bradley-Terry model to prioritize correct over incorrect solutions. Proximal Policy Optimization (PPO) PPO training further optimizes code generation by maximizing rewards from the reward model. Candidate solutions for tasks are generated and ranked to guide the models updates Self-Training Cycle After PPO training, the model generates new solutions, which are evaluated on task test cases. Incorrect but highly ranked solutions (hard negatives) are included in an updated training set, refining the reward models accuracy. Retraining and Iterative Refinement With an updated reward model, new PPO model is trained from scratch, resulting in improved alignment with reward model evaluations. This cycle, while computationally costly, significantly boosts output quality even with single iteration. List of developed models: RewardRanker (1.3B + 6.7B) uses 1.3B model as code generator and 6.7B model as reranker. RewardRanker (6.7B + 6.7B) both generator and reranker use 6.7B model. RewardRanker 2 iter.hardnegatives trained with hard negatives. RewardRanker 2 iter.self training refined with self-training examples. Table 1. Model performance comparison on MultiPL-E. Best result is in bold, second best is in italic. Percentage of solved tasks. Model Size Python C++ Java PHP TS C# Bash JS Avg - - GPT-3.5-Turbo [22] GPT-4 [17] Close models 76.2 84.1 Open models 65.2 78.9 79.3 77.3 13.4B 78.9 RewardRanker 2 iter.hardnegatives 13.4B 80.2 RewardRanker 2 iter.self training 13.4B 81.7 DeepSeek-Coder-Instruct [11] DeepSeek-Coder-Instruct [11] DeepSeek-Coder-Instruct [11] RewardRanker (1.3B + 6.7B) RewardRanker (6.7B + 6.7B) 1.3B 6.7B 33B 8B 63.4 69.2 60.9 69.1 70.8 42.4 67.1 64.9 76.4 81.6 77.2 77.4 79.1 58.2 78.0 76.5 45.3 51.9 45.3 59.7 55.1 12.7 52.2 48.4 63.4 68.4 68.9 67.2 72.8 36.7 72.7 66.1 68.9 73.4 72.7 67.9 74.1 43.0 73.9 69.2 72.3 70.6 66.3 66.0 74.4 35.8 73.9 67.1 75.7 74.6 72.1 66.4 75.1 41.4 74.5 69.9 77.9 73.4 71.6 66.4 75.8 38.2 73.8 69.7 79.2 77.4 71.6 67.0 75.2 39.6 75.1 70."
        },
        {
            "title": "5 Results",
            "content": "Our experiments validate RewardRankers effectiveness in improving code generation. Table 1 shows model performance on the MultiPL-E/HumanEval-Multilingual [4] 6 N. Sorokin, I. Sedykh, V. Malykh benchmarks. For reranking, we evaluate the top 10 generated solutions per task, where DeepSeek-Coder models [11] rely on the most probable solution from top-k sampling with k=50. In contrast, RewardRanker ranks solutions based on reranker scores, yielding higher-quality outputs. As seen in Tab. 1, RewardRanker substantially improves average performance across languages. For instance, the RewardRanker 1.3B model, paired with DeepSeek-Coder-Instruct 6.7B, achieves 67.07% accuracy, rivaling the larger 33B models. The RewardRanker 6.7B model further increases accuracy to 69.9%, outperforming GPT-3.5-turbo while still trailing GPT-4. The authors of [15] explored program correctness verification through execution, utilizing predefined tests to evaluate code generation outputs. However, this method relies heavily on numerous predefined test cases, which may be lacking in real-world coding tasks. To investigate this limitation, we assessed our RewardRanker model on the MBPP dataset [2] for comparative analysis. We show the results on Tab. 2. While [15] reported results using codex-davinci-002 for generation of candidates and the T5-large model [18] for ranking ones, we reproduced their approach with DeepSeek-Coder model [11] used in both capacities for fair comparison. Our results demonstrate that RewardRanker consistently outperforms both variations of the LEVER approach. Table 2. Model performance on MBPP Model codex-davinci-002 DeepSeek-Coder-Instruct Lever [15] Lever (DeepSeek-Coder) RewardRanker Parameters (Billion) Performance (%) - 6.7 - 6.7 + 1.3 6.7 + 1.3 62.0 65.4 68.9 69.4 69."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced RewardRanker, novel approach to enhancing code generation by combining code generation model with reranker model. By leveraging Proximal Policy Optimization (PPO) in an iterative self-training loop, our method refines the reranker model to improve both reranking accuracy and the overall quality of the generated code. Unlike traditional PPO approaches, which primarily focus on optimizing generative model using reward model, our strategy emphasizes developing robust reward and reranking mechanism. This allows us to address and correct errors that the reward model alone might overlook. The iterative self-training process, which incorporates hard negative examples, further enhances model performance by continuously improving the reranking of solutions. Our evaluation on the MultiPL-E dataset demonstrates the effectiveness of our approach. Our 13.4B parameter RewardRanker model Iterative Self-Training for Code Generation via Reinforced Re-Ranking 7 outperforms larger models like 33B parameter model. Moreover, our model achieves performance comparable to GPT-4 and even surpasses it in one programming language. In our future work, we plan to explore additional domains for pretraining to further investigate the combined capabilities of RewardRanker and the PPObased generation model in self-training setup. This will aim to improve overall quality across broader range of coding tasks."
        },
        {
            "title": "References",
            "content": "1. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., Sutton, C.: Program Synthesis with Large Language Models (Aug 2021). https://doi.org/10.48550/arXiv.2108.07732, http: //arxiv.org/abs/2108.07732, arXiv:2108.07732 [cs] 2. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., Sutton, C.: Program synthesis with large language models (2021), https://arxiv.org/abs/2108.07732 3. Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth, U.S., Purohit, S., Reynolds, L., Tow, J., Wang, B., Weinbach, S.: Gpt-neox-20b: An open-source autoregressive language model (2022) 4. Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.H., Zi, Y., Anderson, C.J., Feldman, M.Q., Guha, A., Greenberg, M., Jangda, A.: Multipl-e: scalable and extensible approach to benchmarking neural code generation (2022), https://arxiv.org/abs/2208.08227 5. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., Zaremba, W.: Evaluating Large Language Models Trained on Code (Jul 2021). https://doi.org/10.48550/arXiv. 2107.03374, http://arxiv.org/abs/2107.03374, arXiv:2107.03374 [cs] 6. Christiano, P., Leike, J., Brown, T.B., Martic, M., Legg, S., Amodei, D.: Deep reinforcement learning from human preferences (2023), https://arxiv.org/abs/ 1706. 7. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., Zhou, M.: CodeBERT: pre-trained model for programming and natural languages. In: Cohn, T., He, Y., Liu, Y. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 15361547. Association for Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020. findings-emnlp.139, https://aclanthology.org/2020.findings-emnlp.139 8. Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C.C.T., Giorno, A.D., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H.S., Wang, X., Bubeck, S., Eldan, R., Kalai, A.T., Lee, Y.T., Li, Y.: Textbooks are all you need (2023) 8 N. Sorokin, I. Sedykh, V. Malykh 9. Guo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., Yin, J.: UniXcoder: Unified CrossModal Pre-training for Code Representation (Mar 2022). https://doi.org/10. 48550/arXiv.2203.03850, http://arxiv.org/abs/2203.03850, arXiv:2203.03850 [cs] 10. Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S.K., Clement, C., Drain, D., Sundaresan, N., Yin, J., Jiang, D., Zhou, M.: GraphCodeBERT: Pre-training Code Representations with Data Flow (Sep 2021). https://doi.org/10.48550/arXiv.2009.08366, http://arxiv.org/abs/2009.08366, arXiv:2009.08366 [cs] 11. Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y.K., Luo, F., Xiong, Y., Liang, W.: DeepSeek-Coder: When the Large Language Model Meets Programming The Rise of Code Intelligence (Jan 2024). https://doi.org/10.48550/arXiv.2401.14196, http://arxiv.org/abs/ 2401.14196, arXiv:2401.14196 [cs] 12. Le, H., Wang, Y., Gotmare, A.D., Savarese, S., Hoi, S.C.H.: Coderl: Mastering code generation through pretrained models and deep reinforcement learning (2022), https://arxiv.org/abs/2207.01780 13. Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., Hubert, T., Choy, P., de Masson dAutume, C., Babuschkin, I., Chen, X., Huang, P.S., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J., Mankowitz, D.J., Sutherland Robson, E., Kohli, P., de Freitas, N., Kavukcuoglu, K., Vinyals, O.: Competition-level code generation with alphacode. Science 378(6624), 10921097 (Dec 2022). https://doi.org/10.1126/science. abq1158, http://dx.doi.org/10.1126/science.abq1158 14. Lozhkov, A., Li, R., Allal, L.B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T., Tian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang, Z., Liu, Q., Abulkhanov, D., Paul, I., Li, Z., Li, W.D., Risdal, M., Li, J., Zhu, J., Zhuo, T.Y., Zheltonozhskii, E., Dade, N.O.O., Yu, W., Krauß, L., Jain, N., Su, Y., He, X., Dey, M., Abati, E., Chai, Y., Muennighoff, N., Tang, X., Oblokulov, M., Akiki, C., Marone, M., Mou, C., Mishra, M., Gu, A., Hui, B., Dao, T., Zebaze, A., Dehaene, O., Patry, N., Xu, C., McAuley, J., Hu, H., Scholak, T., Paquet, S., Robinson, J., Anderson, C.J., Chapados, N., Patwary, M., Tajbakhsh, N., Jernite, Y., Ferrandis, C.M., Zhang, L., Hughes, S., Wolf, T., Guha, A., von Werra, L., de Vries, H.: StarCoder 2 and The Stack v2: The Next Generation (Feb 2024). https://doi.org/10.48550/arXiv.2402.19173, http://arxiv.org/ abs/2402.19173, arXiv:2402.19173 [cs] 15. Ni, A., Iyer, S., Radev, D., Stoyanov, V., tau Yih, W., Wang, S.I., Lin, X.V.: Lever: Learning to verify language-to-code generation with execution (2023), https:// arxiv.org/abs/2302.08468 16. Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., Xiong, C.: Codegen: An open large language model for code with multi-turn program synthesis (2023) 17. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H.W., Cummings, D., Currier, J., Dai, Y., Decareaux, Iterative Self-Training for Code Generation via Reinforced Re-Ranking 9 C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S.P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S.S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Kaiser, L., Kamali, A., Kanitscheider, I., Keskar, N.S., Khan, T., Kilpatrick, L., Kim, J.W., Kim, C., Kim, Y., Kirchner, J.H., Kiros, J., Knight, M., Kokotajlo, D., Kondraciuk, L., Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C.M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S.M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., OKeefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., Peres, F.d.A.B., Petrov, M., Pinto, H.P.d.O., Michael, Pokorny, Pokrass, M., Pong, V.H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F.P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M.B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.F.C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J.J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C.J., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., Zoph, B.: GPT-4 Technical Report (Mar 2024). https://doi.org/10. 48550/arXiv.2303.08774, http://arxiv.org/abs/2303.08774, arXiv:2303.08774 [cs] 18. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with unified textto-text transformer. Journal of Machine Learning Research 21(140), 167 (2020), http://jmlr.org/papers/v21/20-074.html 19. Shojaee, P., Jain, A., Tipirneni, S., Reddy, C.K.: Execution-based code generation using deep reinforcement learning (2023) 20. Wang, B., Komatsuzaki, A.: GPT-J-6B: 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax (May 2021) 21. Wang, Y., Wang, W., Joty, S., Hoi, S.C.H.: CodeT5: Identifier-aware Unified Pretrained Encoder-Decoder Models for Code Understanding and Generation (Sep 2021). https://doi.org/10.48550/arXiv.2109.00859, http://arxiv.org/abs/ 2109.00859, arXiv:2109.00859 [cs] 10 N. Sorokin, I. Sedykh, V. Malykh 22. Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y., Zhou, J., Chen, S., Gui, T., Zhang, Q., Huang, X.: comprehensive capability analysis of gpt-3 and gpt-3.5 series models (2023), https://arxiv.org/ abs/2303.10420 23. Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z., Shen, L., Wang, A., Li, Y., Su, T., Yang, Z., Tang, J.: Codegeex: pre-trained model for code generation with multilingual benchmarking on humaneval-x. In: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. pp. 56735684 (2023)"
        }
    ],
    "affiliations": [
        "International IT University",
        "MTS AI"
    ]
}