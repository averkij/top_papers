{
    "paper_title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
    "authors": [
        "Yujie Zhou",
        "Jiazi Bu",
        "Pengyang Ling",
        "Pan Zhang",
        "Tong Wu",
        "Qidong Huang",
        "Jinsong Li",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Anyi Rao",
        "Jiaqi Wang",
        "Li Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/."
        },
        {
            "title": "Start",
            "content": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion Yujie Zhou1,6*, Jiazi Bu1,6*, Pengyang Ling2,6*, Pan Zhang6, Tong Wu5, Qidong Huang2,6, Jinsong Li3,6, Xiaoyi Dong6, Yuhang Zang6, Yuhang Cao6, Anyi Rao4, Jiaqi Wang6, Li Niu1 1Shanghai Jiao Tong University 2University of Science and Technology of China 3The Chinese University of Hong Kong 4Hong Kong University of Science and Technology 5Stanford University 6Shanghai AI Laboratory 5 2 0 2 2 1 ] . [ 1 0 9 5 8 0 . 2 0 5 2 : r Figure 1. Training-free video illumination control. Equipped with pretrained image relighting model (e.g., IC-Light [63]) and video diffusion model (e.g., CogVideoX [60] and AnimateDiff [13]), Light-A-Video enables training-free and zero-shot illumination control of any given video sequences or foreground sequences."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the exces- * indicates equal contribution, indicates corresponding author sive training costs and the scarcity of diverse, high-quality video relighting datasets. simple application of image relighting models on frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, training-free approach to achieve temporally smooth video 1 relighting. Adapted from image relighting models, LightA-Video introduces two key techniques to enhance lighting consistency. First, we design Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source videos appearance and the relighted appearance, using Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi. github.io/light-a-video.github.io/. 1. Introduction Illumination plays crucial role in shaping our perception of visual content, impacting both its aesthetic quality and human interpretation of scenes. Relighting tasks [18, 25, 33, 35, 45, 48, 55, 66, 67], which focus on adjusting lighting conditions in 2D and 3D visual content, have long been key area of research in computer graphics due to their broad practical applications, such as film production, gaming, and virtual environments. Traditional image relighting methods rely on physical illumination models, which struggle with accurately estimating real-world lighting and materials. To these overcome data-driven limitations, approaches [10, 20, 23, 40, 61, 65] have emerged, leveraging large-scale, diverse relighting datasets combined with pre-trained diffusion models. As the state-of-the-art image relighting model, IC-Light [63] modifies only the illumination of an image while maintaining its albedo unchanged. Based on the physical principle of light transport independence, IC-Light allows for controlled and stable illumination editing, such as adjusting lighting effects and simulating complex lighting scenarios. However, video relighting is significantly more challenging due to maintaining temporal consistency across frames. The scarcity of video lighting datasets and the cost of training further complicate the task. Thus, existing video relighting methods [65] struggle to deliver consistently high-quality results or are limited to specific domains, such as portraits. In this work, we propose training-free approach for video relighting, named Light-A-Video, which enables the generation of smooth, high-quality relighted videos without any training or optimization. As shown in Fig. 1, given text prompt that provides general description of the video and specified illumination conditions, our Light-A-Video pipeline can relight the input video in zero-shot manner, fully leveraging the relighting capabilities of image-based models and the motion priors of the video diffusion model. To achieve this goal, we initially apply an image-relighting Figure 2. Comparison of relighted frames using IC-Light and CLA module frame-by-frame. The line chart depicts the average optical flow intensity per frame. IC-Light shows noticeable jitter between frames. Conversely, our CLA module achieves more stable light source generation over time, making the motion trend more consistent with the source video. model to video-relighting tasks on frame-by-frame basis, and observe that the generated lighting source is unstable across frames in the video. This instability leads to inconsistencies in the relighting of the objects appearances and significant flickering across frames. To stabilize the generated lighting source and ensure consistent results, we design Consistent Light Attention (CLA) module within the selfattention layers of the image relighting model. As shown in Fig. 2, by incorporating additional temporally averaged features into the attention computation, CLA facilitates crossframe interactions, producing structurally stable lighting source. To further enhance the appearance stability across frames, we utilize the motion priors of the video diffusion model with novel Progressive Light Fusion (PLF) strategy. Adhering to the physical principles of light transport, PLF progressively employs linear blending to integrate relighted appearances from the CLA into each original denoising target, which gradually guides the video denoising process toward the desired relighting direction. Finally, Light-A-Video serves as complete end-to-end pipeline, effectively achieving smooth and consistent video relighting. As training-free approach, Light-A-Video is not restricted to specific video diffusion models, making it highly compatible with range of popular video generation backbones, including UNet-based and DiT-based models such as AnimateDiff [13], CogVideoX [60] and LTX-Video [15]. Our contributions are summarized as follows: We present Light-A-Video, the first training-free video relighting model, which leverages the capabilities of im2 age relighting models to generate high-quality, temporally consistent relighted videos. An end-to-end pipeline is proposed with two key designs: the Consistent Light Attention, which enhances the stability of lighting source across frames, and the Progressive Light Fusion, which gradually injects lighting information to ensure temporal consistency for video appearance. Experiments show the effectiveness of our method across multiple settings. It not only supports relighting of the entire input video, but also supports relighting the input foreground sequences with background generation. 2. Related Work 2.1. Video Diffusion Models. Video diffusion models [26, 13, 17, 51, 54, 60, 62] aim to synthesize temporally consistent image frames based on provided conditions, such as text prompt or an image prompt. In the realm of text-to-video (T2V) generation, the majority of methods [3, 5, 6, 13, 51, 62] train additional motion modeling modules from existing text-to-image architectures to model the correlation between video frames, while others [17, 60] train from scratch to learn video priors. For image-to-video (I2V) tasks that enhance still images with reasonable motions, line of research [7, 57] proposes novel frameworks dedicated to image animation. Some approaches [12, 14, 64] serve as plug-to-play adapters. Stable Video Diffusion [2] fine-tune pre-trained T2V models for I2V generation, achieving impressive performance. Numerous works [27, 30, 34] focus on controllable generation, providing more controllability for users. Video diffusion models, due to their inherent video priors, are capable of synthesizing smooth and consistent video frames that are both content-rich and temporally harmonious. 2.2. Learning-based Illumination Control. Over the past few years, variety of lighting control techniques [33, 35, 48] for 2D and 3D visual content based on deep neural networks have been proposed, especially in the field of portrait lighting [1, 23, 44, 46, 47], along with range of baselines [18, 45, 55, 66, 67] aimed at improving the effectiveness, accuracy, and theoretical foundation of illumination modeling. Recently, owing to the rapid development of diffusion-based generative models, number of lighting control methods [10, 20, 40, 61] utilizing diffusion models have also been introduced. Relightful Harmonization [40] focuses on harmonizing sophisticated lighting effects for the foreground portrait conditioning on given background image. SwitchLight [23] suggests training physically co-designed framework for human portrait relighting. IC-Light [63] is state-of-the-art approach for image relighting. LumiSculpt [65] enables consistent lighting control in T2V generation models for the first time. However, in the domain of video lighting, the aforementioned approaches fail to simultaneously ensure precise lighting control and exceptional visual quality. This work incorporates pre-trained image lighting control model into the denoising process of T2V model through progressive guidance, leveraging the latters video priors to facilitate the smooth transfer of image lighting control knowledge, thereby enabling accurate and harmonized control of video lighting. 2.3. Video Editing with Diffusion Models. In recent years, diffusion-based video editing has undergone significant advancements. Some researches [29, 30, 32, 53, 56] adopt pretrained text-to-image (T2I) backbones for video editing. Another line of approaches [8, 19, 58, 59] leverages pre-trained optical flow models to enhance the temporal consistency of output video. Numerous studies [11, 21, 38] have concentrated on exploring zero-shot video editing approaches. COVE [52] leverages the inherent diffusion feature correspondence proposed by DIFT [49] to achieve consistent video editing. SDEdit [31] utilizes the intrinsic capability of diffusion models to refine details based on given layout, enabling efficient editing for both image and video. Despite the remarkable performance of existing video editing techniques in various settings, there remains lack of approaches specifically designed for controlling the lighting of videos. 3. Preliminary 3.1. Diffusion Model Given an image x0 that follows the real-world data distribution, we first encode x0 into latent space z0 = E(x0) using pretrained autoencoder {E(), D()}. The forward diffusion process is steps Markov chain [16], corresponding to the iterative introduction of Gaussian noise ϵ, which can be expressed as: zt = (cid:112)1 βtzt1 + (cid:112)βtϵ, where βt (0, 1) determines the amount of Gaussian noise introduced at time step t. Mathematically, the above cumulative noise adding has the following closed-form: (1) zt = αtz0 + 1 αtϵ, (2) where αt = (cid:81)t 1(1 βt). For numerical stability, v-prediction [42] approach is employed, where the diffusion model outputs predicted velocity to represent the denoising direction. Defined as: = αtϵ 1 αtz0 (3) During inference, the noise-free component ˆz0t can be recovered from the models output vt as follows: ˆz0t = αtzt 1 αtvt. (4) ˆz0t represents the denoising target based on vt. Figure 3. The pipeline of Light-A-Video. source video is first noised and processed through the VDM for denoising across Tm steps. At each step, the predicted noise-free component with details compensation serves as the Consistent Target zv 0t, inherently representing the VDMs denoising direction. Consistent Light Attention infuses zv 0t with unique lighting information, transforming it into the Relight Target zr 0t. The Progressive Light Fusion strategy then merges two targets to form the Fusion Target z0t, which provides refined direction for the current step. The bottom-right part illustrates the iterative evolution of zv 0t. 3.2. Light Transport Light transport theory [9, 63] demonstrates that arbitrary image appearance IL can be decomposed by the product of light transport matrix and environment illumination L, which can be expressed as: IL = TL, (5) where is single matrix for linear light transform [9] and denotes variable environment illumination. Given the linearity of T, the merging between environment illumination is equal to the fusion of image appearance IL, i.e., pipeline, which achieves stable lighting source generation and temporally consistent relighted video through Progressive Light Fusion (PLF) strategy. 4.1. Problem Formulation Given source video and lighting condition c, the objective of video relighting is to render the source video into the relighted video that aligns with the given condition c. Unlike image relighting solely concentrates on appearance, video relighting raises extra challenges in maintaining temporal consistency and motion preservation, necessitating high-quality visual coherence across frames. IL1+L2 = T(L1 + L2) = IL1 + IL2. (6) 4.2. Consistent Light Attention Such characteristic suggests the feasibility of lighting control by indirectly constraining image appearance, i.e., the consistent image light constraint in IC-Light [63]. 4. Light-A-Video In Section 4.1, we define the objectives of the video relighting task. In Section 4.2, we reveal that per-frame image relighting for video sequence suffers from lighting source inconsistency and accordingly propose Consistent Lighting Attention (CLA) module for enhanced lighting stability in per-frame image relighting results. In Section 4.3, we provide detailed description of the proposed Light-A-Video Given the achievement in image relighting model [63], straightforward approach for video relighting is to directly perform frame-by-frame image relighting under the same lighting condition. However, as illustrated in Fig. 2, this naive method fails to maintain appearance coherence across frames, resulting in frequent flickering of the generated light source and inconsistent temporal illumination. To improve inter-frame information integration and generate stable light source, we propose Consistent Light Attention (CLA) module. Specifically, for each selfattention layer in the IC-Light model, video feature map R(bf )(hw)c serves as the input, where is the batch size and is the number of video frames, and 4 denote the height and width of the feature map, with representing the number of tokens for attention computation. With linearly projections, is projected into query, key and value features Q, K, R(bf )(hw)c. The attention computation is defined as follows: Self-Attn(Q, K, ) = Softmax (cid:18) QK (cid:19) Note that the naive method treats the frame dimension as the batch size, performing self-attention frame by frame with the image relighting model, which results in each frame attending only to its own features. For our CLA module as shown in Fig. 3, dual-stream attention fusion strategy is applied. Given the input h, the original stream directly feeds the feature map into the attention module to compute frame-by-frame attention, resulting in the output 1. The average stream first reshapes into Rbf (hw)c, averages it along the temporal dimension, then expands it times to obtain h. Specifically, the average stream mitigates high-frequency temporal fluctuations, thereby facilitating the generation of stable background light source across frames. Meanwhile, the original stream retains the original high-frequency details, thereby compensating for the detail loss incurred by the averaging process. Then, is input into the self-attention module and the output is 2. The final output of the CLA module is weighted average between two streams, with the trade-off parameter γ, = (1 γ)h 1 + γh 2 (7) With the help of CLA, the result can capture global context across the entire video, and generated more stable relighting result as shown in Fig. 2. 4.3. Progressive Light Fusion CLA module improves cross-frame consistency but lacks pixel-level constraints, leading to inconsistencies in appearance details. To address this, we leverage motion priors in Video Diffusion Model (VDM), which are trained on largescale video datasets and use temporal attention module to ensure consistent motion and lighting changes. The novelty of our Light-A-Video lies in progressively injecting the relighting results as guidance into the denoising process. In the pipeline as shown in Fig 3, source video is first encoded into latent space, and then add Tm step noise to acquire the noisy latent zm. At each denoising step t, the noise-free component ˆz0t in Eq. 4 is predicted, which serves as the denoising target for the current step. Prior work demonstrated the potential of applying tailored manipulation in denoising targets for guided generation, with significant achievements observed in high-resolution image synthesis [24] and text-based image editing [41]. Driven by the motion priors in the VDM, the denoising process encourage ˆz0t to be temporally consistent. Thus, Figure 4. Visualization of the PLF Strategy. During the denoising process of the VDM, the PLF strategy progressively replaces the original Consistent Target , guiding the denoising direction from zv with the Fusion Target 0t to vt. we define this target as the video Consistent Target zv 0t with environment illumination Lv . However, discrepancies still exist between the predicted zv 0t and the original video. To address this issue, we incorporate the differences as details compensation into the Consistent Target at each step, thereby enhancing detail consistency between the relighted video and the original video. Then, zv 0t is sent into the CLA module to obtain the relighted latent, which serves as the Relight Target zr for the t-th denoising step. Aligning with the light transport theory in Section 3.2, pre-trained VAE {E(), D()} is used to decode the two targets into pixel level, yielding the image apt = D(zr pearances Iv 0t), respectively. Refer to Eq. 6, the fusing appearance If can be formulated as, 0t with the illumination Lr 0t) and Ir = D(zv If = T(Lv + Lr ) = Iv + Ir . (8) We observe that directly using encoded latent E(If ) as the new target at each step results in suboptimal performance. The reason is that the gap between the two targets is excessively large, surpassing the refinement capability of the VDM and causing visible temporal lighting jitter. To mitigate this gap, progressive lighting fusion strategy is proposed. Specifically, fusion weight λt is introduced, which decreases as denoising progresses, gradually reducing the influence of the relight target. The progressive light fusion appearance is defined as Ip , Ip = Iv + λt(Ir Iv ). (9) The encoded latent z0t = E(Ip ) is utilized as the Fusion Target for step t, replacing the original ˆz0t. Based on the fusion target, the less noisy latent zt1 can be computed with DDIM scheduler with v-prediction: at = (cid:114) 1 αt1 1 αt , bt = αt1 αtat zt1 = atzt + btz0t (10) (11) From Eq. 4, the fusion target z0t determines new de5 noising direction, denoted as vt, vt = αtzt z0t 1 αt , (12) which means our PLF strategy essentially refines vt iteratively and guides the denoising process towards the relighting direction shown in Fig 4. Notably, other schedulers (e.g., Euler Scheduler [22], Flow Matching [28], etc.) with predicted noise-free components for each step are also applicable. Finally, as the denoising progresses, we achieve smooth and consistent injection of illumination, ensuring coherent video relighting. 5. Experiments In this section, we provide comprehensive qualitative and quantitative evaluations to highlight the efficacy and generality of our method for zero-shot video illumination control. Furthermore, we conduct ablation studies to analyze the contributions of critical model components and provide additional results on text-conditioned video illumination modifying with background generation. 5.1. Experimental Details Baselines. In the absence of established video relighting methods, we adopt the state-of-the-art image relighting technique to perform frame-by-frame relighting on videos as baseline. To verify the temporal smoothing effect of illumination using VDM, we add 20% and 60% noise to the results obtained from IC-Light [63]. Subsequently, we apply SDEdit [31] on VDM for denoising, resulting in two sets of results named IC-Light + SDEdit-0.2 and IC-Light + SDEdit-0.6, respectively. Finally, AnyV2V [26] is utilized as another baseline to verify whether the temporal content migration method can be effectively applied to video relighting tasks. This baseline names IC-Light + AnyV2V, where the first frame of the IC-Light result and the source video are used as conditional inputs. Evaluation metrics. We report three widely adopted metrics for quantitative evaluation. to evaluate the temporal consistency of the generated video, the average CLIP [39] score across consecutive video frames is utilized. Second, RAFT [50] is used to estimate the optical flow for each baseline video, and assess the motion preservation of each method by calculating the optical flow similarity with the source video. Third, to evaluate the quality of image relighting for each method, video test dataset is collected, and the FID [43] score is calculated between each methods results And the frame-by-frame IC-Light results, serving as the relight quality evaluation metric. Datasets. We constructed video test dataset consisting of 73 videos. The majority of these videos were selected from the DAVIS [37] public dataset, which contains diverse First, collection of semantically rich videos with pronounced motion. Additionally, some videos were collected from Pixabay [36], featuring high-quality videos with significant motion. All quantitative metrics are evaluated on our collected dataset. For each video, two lighting prompts are applied, and three lighting directions are randomly chosen. Implementation details. If not specified, the default models employed for image relighting and VDM in the subsequent experiments are IC-Light [63] and AnimateDiff [13] motion-adapter-v3, respectively. In the IC-Light model, the lighting conditions for image relighting are derived from two components. First, text prompt that describes the characteristics of the light source (e.g., neon light, sunshine, etc.). Second, lighting map is utilized to represent the light intensity across the scene. This lighting map is then encoded by VAE and serves as the initial latent for the denoising process. During the inference stage, the source video is added with 50% noise. Then, the VDM employs denoising process with Tm = 25 steps to progressively fusion the relight target. For the parameters in our LightA-Video model, the γ = 0.5 in the CLA module is used to balance the original attention hidden state and the crossframe averaged hidden state. In our PLF strategy, the fusion weight λt is decreases as denoising progresses, so we set λ = 1 t/Tm. All experiments were conducted on single A100 GPU, with memory consumption of approximately 23GB for 16-frame relighted video generation. 5.2. Qualitative Results As shown in Fig. 5, the frame-by-frame IC-Light method ensures single-frame quality. However, the absence of consistency design and VDM temporal priors results in significant flickering of the light source and overall appearance. By introducing VDM priors, IC-Light + SDEdit0.2 maintains content consistent with the source video, but still exhibits noticeable relight appearance jitter. IC-Light + SDEdit-0.6 further enhances temporal smoothness, yet object identity shifts occur. AnyV2V can transfer the appearance of the first relight frame to subsequent frames, but this type of temporal content migration method inherently performs only pixel-level migration without perceiving the given light source, leading to unreasonable changes in object illumination. In contrast, Light-A-Video achieves high-quality video relighting, demonstrating strong temporal consistency and high fidelity to the light source. 5.3. Video Relighting with Background Generation As depicted in Fig. 6, Light-A-Video not only supports the direct relighting of the input source video but also accepts video foreground sequence and text prompt provided by users as input, generating corresponding video background and illumination that aligned with the prompt descriptions. Specifically, we first process the input fore6 Figure 5. Qualitative comparison of baseline methods. Given source video and guidance text prompt, we compare the performance of Light-A-Video with other video editing methods. VDM used: AnimateDiff (Left), CogVideoX (Right). ground sequence with IC-Light for frame-by-frame relighting, while completely noising the background to serve as the initialization video for VDM. From step to Tm, the background is gradually denoised, leveraging VDMs inpainting capability to generate the background. Then, from step Tm to 0, CLA and PLF are introduced to achieve temporally consistent relighting appearance of the video. The results in Fig. 6 demonstrate that our pipeline can achieve high-quality video relighting results with consistent background generation. 5.4. Quantitative Evaluation The quantitative comparison of our method with various baselines is presented in Tab. 1. As shown, we introduced an additional baseline SDEdit-0.6, which denoises the source video with 60% noise. SDEdit-0.6 results in high temporal Clip Score demonstrating relying solely on the relight prompt is insufficient for generating videos with the corresponding lighting characteristics using VDM. 7 When the frame-by-frame IC-Light results are used as the initialization video and then noised, IC-Light + SDEdit0.6 exhibits an increased FID score, indicating tradeoff with temporal stability. Similarly, IC-Light + SDEdit0.2, which incorporates 20% noise, maintains high consistency in lighting effects with frame-by-frame IC-Light, but it also suffers from unacceptable temporal flickering. For AnyV2V, temporal content migration method, the first frames appearance is consistent with the IC-Light results. Yet, its inability to perceive the light source, coupled with the models inherent quality degradation in subsequent frames, results in low motion preservation score. In contrast, our Light-A-Video method achieves low FID score while maintaining high level of temporal consistency, demonstrates the effectiveness in both relighted image quality and temporal stability. Evaluation Metric IC-Light [63] SDEdit-0.6 [31] IC-Light + SDEdit-0.2 IC-Light + SDEdit-0.6 IC-Light + AnyV2V [26] Light-A-Video (Ours) (a) Image Quality (b) Temporal Consistency FID Score() CLIP Score() Motion Preservation() / 64.03 13.79 62.61 32.73 29. 0.9040 0.9677 0.9199 0.9483 0.9436 0.9655 5.969 4.204 5.959 7.544 8.854 2.493 Table 1. Quantitative comparison of baseline methods. The best result is highlighted in bold, while the second-best result is underlined. 5.5. Ablation Study We conduct an ablation study to understand the importance of our CLA and PLF modules. As shown in Fig. 7, for the video relighting with background generation task, frame-by-frame IC-Light ensures high single-frame relighting quality but lacks temporal consistency control, leading to lighting source inconsistency and relighted appearance inconsistency. With the help of CLA, cross-frame information exchange stabilizes the generation of background lighting sources. Furthermore, by introducing VDM motion priors and using PLFs strategy to progressively fuse relight targets into the original denoising target, Light-AVideo achieves temporally smooth relighting of the video. The overall video quality is also significantly improved with the help of VDM priors. 5.6. Limitation and Future Work. Despite achieving impressive results, the performance of our training-free method is limited by the capabilities of both the image-relighting model and the VDM. While Light-A-Video excels at ensuring stable lighting and temporal consistency, the CLA module, which stabilizes the background lighting, is less effective for modeling dynamic lighting changes. To address this limitation, future work will focus on developing methods that can handle dynamic lighting conditions more effectively. 6. Conclusion introduces Light-A-Video, In summary, our paper training-free method that leverages the capabilities of stateof-the-art image relighting models to achieve temporally consistent video relighting. By integrating the Consistent Light Attention to stabilize lighting source generation and employ the Progressive Light Fusion strategy for smooth appearance transitions. Light-A-Video significantly enhances the temporal coherence of relighted videos while preserving the high-quality relighting of images. Experiments demonstrate the effective application of advanced image-relighting techniques in both video sequence relighting and foreground relighting with background generation. Figure 6. Text-conditioned video illumination modifying with background generation. Given video foreground sequence and text description of the target illumination, our method synthesizes suitable backgrounds and harmonious illumination. Figure 7. Ablation Study. We present the video relighted with background generation results by removing the CLA module or the PLF strategy."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron and Jitendra Malik. Shape, illumination, and reflectance from shading. IEEE transactions on pattern analysis and machine intelligence, 37(8):16701687, 2014. 3 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [4] Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, and JiBroadway: Boost your text-to-video genaqi Wang. arXiv preprint eration model arXiv:2410.06241, 2024. in training-free way. [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 3 [7] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real In Euimage animation with text-guided motion control. ropean Conference on Computer Vision, pages 475491. Springer, 2025. 3 [8] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flowguided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. 3 [9] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the In Proceedings of the reflectance field of human face. 27th annual conference on Computer graphics and interactive techniques, pages 145156, 2000. 4 [10] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, and Maneesh Agrawala. Flashtex: Fast relightable mesh texturing with In European Conference on Computer Vilightcontrolnet. sion, pages 90107. Springer, 2025. 2, 3 [11] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. [12] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, et al. I2v-adapter: general imageto-video adapter for video diffusion models. arXiv preprint arXiv:2312.16693, 2023. 3 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1, 2, 3, 6 [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision, pages 330348. Springer, 2025. 3 [15] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 2 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [18] Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Towards high fidelity face relighting with realistic shadows. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1471914728, 2021. 2, 3 [19] Zhihao Hu and Dong Xu. Videocontrolnet: motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. 3 [20] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. arXiv preprint arXiv:2406.07520, 2024. 2, 3 [21] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65076516, 2024. 3 [22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 6 [23] Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, and Sanghyun Woo. Switchlight: Co-design of physicsdriven architecture and pre-training framework for human In Proceedings of the IEEE/CVF Conportrait relighting. ference on Computer Vision and Pattern Recognition, pages 2509625106, 2024. 2, [24] Younghyun Kim, Geunmin Hwang, Junyu Zhang, and Eunbyung Park. Diffusehigh: Training-free progressive highresolution image synthesis through structure guidance. arXiv preprint arXiv:2406.18459, 2024. 5 9 [25] Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, and Yannick Hold-Geoffroy. Lightit: Illumination modeling and control for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93599369, 2024. 2 [26] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. 6, 8 [27] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. 3 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 6 [29] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. [30] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, HeungYeung Shum, Wei Liu, et al. Follow-your-click: Opendomain regional image animation via short prompts. arXiv preprint arXiv:2403.08268, 2024. 3 [31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 3, 6, 8 [32] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3 [33] Thomas Nestmeyer, Jean-Francois Lalonde, Iain Matthews, and Andreas Lehrmann. Learning physics-guided face reIn Proceedings of the lighting under directional light. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51245133, 2020. 2, 3 [34] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofa-video: Controllable image animation via generative motion field adaptions in In European Confrozen image-to-video diffusion model. ference on Computer Vision, pages 111128. Springer, 2025. 3 [35] Rohit Pandey, Sergio Orts-Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Ryan Fanello. Total relighting: learning to relight portraits for background replacement. ACM Trans. Graph., 40(4):431, 2021. 2, 3 [36] pixabay. pixabay. https://pixabay.com/videos/, 2025. 6 [37] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 6 [38] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. 3 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [40] Mengwei Ren, Wei Xiong, Jae Shin Yoon, Zhixin Shu, Jianming Zhang, HyunJoon Jung, Guido Gerig, and He Zhang. Relightful harmonization: Lighting-aware portrait background replacement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64526462, 2024. 2, 3 [41] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 5 [42] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 3 [43] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, 2020. Version 0.3.0. 6 [44] Soumyadip Sengupta, Angjoo Kanazawa, Carlos Castillo, and David Jacobs. Sfsnet: Learning shape, reflectance and illuminance of facesin the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 62966305, 2018. 3 [45] Soumyadip Sengupta, Brian Curless, Ira KemelmacherShlizerman, and Steven Seitz. light stage on every desk. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 24202429, 2021. 2, 3 [46] YiChang Shih, Sylvain Paris, Connelly Barnes, William Freeman, and Fredo Durand. Style transfer for headshot portraits. 2014. 3 [47] Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli, Sylvain Paris, and Dimitris Samaras. Portrait lighting transfer using mass transport approach. ACM Transactions on Graphics (TOG), 36(4):1, 2017. [48] Tiancheng Sun, Jonathan Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. Single image portrait relighting. ACM Transactions on Graphics (TOG), 38(4):112, 2019. 2, 3 [49] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. 3 [50] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 6 10 [63] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport, 2024. 1, 2, 3, 4, 6, 8 [64] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77477756, 2024. [65] Yuxin Zhang, Dandan Zheng, Biao Gong, Jingdong Chen, Ming Yang, Weiming Dong, and Changsheng Xu. Lumisculpt: consistency lighting control network for video generation. arXiv preprint arXiv:2410.22979, 2024. 2, 3 [66] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David Jacobs. Deep single-image portrait relighting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 71947202, 2019. 2, 3 [67] Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuixiang Shao, Wenzheng Chen, Lan Xu, and Jingyi Yu. Relightable neural human assets from multi-view gradient ilIn Proceedings of the IEEE/CVF Conference luminations. on Computer Vision and Pattern Recognition, pages 4315 4327, 2023. 2, 3 [51] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [52] Jiangshan Wang, Yue Ma, Jiayi Guo, Yicheng Xiao, Gao Huang, and Xiu Li. Cove: Unleashing the diffusion feature correspondence for consistent video editing. arXiv preprint arXiv:2406.08850, 2024. 3 [53] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. 3 [54] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 3 [55] Yifan Wang, Aleksander Holynski, Xiuming Zhang, and Xuaner Zhang. Sunstage: Portrait reconstruction and reIn Proceedings of lighting using the sun as light stage. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2079220802, 2023. 2, [56] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 3 [57] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2025. 3 [58] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-to-video In SIGGRAPH Asia 2023 Conference Papers, translation. pages 111, 2023. 3 [59] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Fresco: Spatial-temporal correspondence for zero-shot video In Proceedings of the IEEE/CVF Conference translation. on Computer Vision and Pattern Recognition, pages 8703 8712, 2024. 3 [60] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 3 [61] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH 2024 Conference Papers, pages 112, 2024. 2, [62] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "University of Science and Technology of China"
    ]
}