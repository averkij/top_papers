{
    "paper_title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam",
    "authors": [
        "Tianjin Huang",
        "Haotian Hu",
        "Zhenyu Zhang",
        "Gaojie Jin",
        "Xiang Li",
        "Li Shen",
        "Tianlong Chen",
        "Lu Liu",
        "Qingsong Wen",
        "Zhangyang Wang",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical $l_2$-norm statistics; and $(3)$ inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. Code is available at https://github.com/TianjinYellow/StableSPAM.git."
        },
        {
            "title": "Start",
            "content": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Tianjin Huang* 1 2 Haotian Hu* 3 Zhenyu Zhang* 4 Gaojie Jin 1 Xiang Li 5 Li Shen 6 Tianlong Chen 7 Lu Liu 1 Qingsong Wen 8 Zhangyang Wang 4 Shiwei Liu 2 9 5 2 0 2 4 2 ] . [ 1 5 5 0 7 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, recent optimizer featuring momentum reset and spikeaware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical l2-norm statistics; and (3) inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA1B trained with Adam by up to 2 perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. *Equal contribution 1Department of Computer Science, University of Exeter 2Department of Mathematics and Computer Science, Eindhoven University of Technology 3School of the Gifted Young, University of Science and Technology of China 4Department of Electrical and Computer Engineering, University of Texas at Austin 5Department of Computer Science, University of Reading 6School of Cyber Science and Technology, Sun Yat-sen University 7Department of Computer Science, The University of North Carolina at Chapel Hill 8Squirrel Ai Learning 9Mathematical Institute, University of Oxford. Correspondence to: Tianjin Huang <t.huang2@exeter.ac.uk>. Code is available at https://github.com/ TianjinYellow/StableSPAM.git. 1. Introduction Recently, several advanced optimizers have been proposed, claiming to either outperform the widely used Adam optimizer or achieve comparable performance at reduced costs in the context of Large Language Models (LLMs). Given the massive size of LLMs, reducing the memory footprint of Adam has become key objective in this line of research (Shazeer & Stern, 2018; Chen et al., 2024; Zhang et al., 2024a; Zhao et al., 2024a; Zhang et al., 2024b; Ma et al., 2024). Another area of focus is addressing the challenges of instability in LLM training. For instance, Huang et al. (2025) proposed SPAM which incorporates momentum reset and spike-aware gradient clip (SpikeClip) to mitigate the adverse effects of loss spikes. Zhao et al. (2024b) studied the stability of various optimizers to hyperparameters with BF16. These optimizers are predominantly evaluated using the standard BF16 precision, which is practical option for real-world LLM training (Touvron et al., 2023; Li et al., 2023). With the growing shift toward low-bit precisions such as FP8 and FP4 in LLMs due to their significant costsaving potential (Liu et al., 2024; Lee et al., 2024; Peng et al., 2023; Xi et al., 2023), it is crucial to investigate whether their effectiveness persists under lower-bit precisions. For the newly proposed optimizers to be economical, their training with low-bit precisions should be similarly robust to hyperparameter choice as trained using higher precision. This paper provides comprehensive evaluation of the effectiveness and robustness of learning rate choices across various recent optimizers, including Adam (Kingma, 2014), Adafactor (Shazeer & Stern, 2018), Adam-mini (Zhang et al., 2024a), and SPAM (Huang et al., 2025), when training with 4-bit weights and activations. Our study reveals several key observations: All evaluated optimizers exhibit increased sensitivity to learning rate choices during 4-bit training, often diverging quickly when larger learning rates are used as shown in Figure 2. SPAM consistently achieves the lowest evaluation loss 1 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Figure 1. Performance of 4-bit LLM training. Experiments are conducted with LLaMA-130M/350M/1B models on C4 Dataset. Adam-BF16 denotes that the model is trained with BF16 by Adam. Perplexity on validation set is reported. across various bit levels but requires careful learning rate tuning. Adafactor is surprisingly robust to learning rate choices, even outperforming Adam in this regard. both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. Our analysis of the training dynamics in Figure 4 reveals that 4-bit training often exhibits extremely unstable gradient norms, often accompanied by spikes, compared to BF16. This behavior can result in loss spikes and, in some cases, even training divergence with relatively larger learning rates. While SpikeClip introduced in SPAM mitigates the unstable gradient norms caused by 4-bit training to certain extent, it falls short of fully preventing training divergence, as shown in Figure 3. Despite its sensitivity to learning rate selection, SPAM consistently achieves the lowest evaluation loss across various bit levels, making it an ideal foundation for improvement. Building on this, we introduce Stable-SPAM to address the instability challenges associated with low-precision training of LLMs. Stable-SPAM retains the superior performance of SPAM1 while improving stability, offering significant advancement in low-precision optimization. Specifically, beyond the original momentum reset operation in SPAM, Stable-SPAM introduces two key techniques: Adaptive Spike-Aware Clipping (AdaClip), which enables adaptive clipping of spiked gradients, followed by Adaptive Gradient Norm (AdaGN), which normalizes the entire gradient matrix based on its historical l2 norm statistics. Our analysis demonstrates that these enhancements effectively stabilize the gradient norm of 4-bit training, achieving better performance than Adam and SPAM. Notably, our 4-bit LLaMA1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam. Furthermore, when 1Nevertheless, results in Table 3 show that our proposed techniques also improve the performance of other optimizers. 2 2. 4-bit Training Stability Investigation Recent studies (Zhao et al., 2024b; Wortsman et al., 2023b; Huang et al., 2025; Takase et al., 2023; Wortsman et al., 2023b) have investigated stability challenges in large language model (LLM) training, including issues such as learning rate instability, gradient spikes, and loss spikes. In this section, we extend the evaluation by analyzing the stability of various optimization algorithms under 4-bit LLM training setting. Following the experimental setup outlined in (Wortsman et al., 2023b; Zhao et al., 2024b), we evaluate the final performance using range of learning rates from 1e-4 to 3e-3. This evaluation includes two widely used optimizers, Adam (Kingma, 2014) and Adafactor (Shazeer & Stern, 2018), as well as two recently proposed methods, Adam-mini (Zhang et al., 2024a) and SPAM (Huang et al., 2025). Additionally, we monitor both the global gradient norm and training loss throughout the 4-bit LLM training process. The global gradient norm is defined as follows: (cid:113)(cid:80)N 2 where is the number of layers in model and gi denotes the gradient of i-th layer. The experiments are conducted on the LLaMA-130M/350M models using the C4 dataset and showed in Figure 2 and Figure 4. We observe: ① Lower-bit training exhibits reduced learning rate stability. As illustrated in Figure 2, the final evaluation loss for 4-bit training increases significantly with larger learning rates, whereas BF16 training exhibits more stable performance across different learning rates. This indicates that 4-bit training is more sensitive and less stable in terms of learning rate. i=0 gi2 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Figure 2. Final validation loss when training LLaMA-130M on C4, sweeping across learning rates (LR). The vertical dotted line indicates that the model cannot be trained further as increasing the learning rate, i.e. Training loss becomes NaN. Red dashed horizontal lines indicate the best performance achieved. θVi under the condition g2 Vi ment of gradients. Specifically, it follows the expression: gi = sign(gi) > θ where gi, Vi, θ are the gradient, second moment and pre-defined threshold (5000 used by default in their paper) respectively. We found that merely SpikeClip can mitigate the loss spike to some extent but can not prevent the training divergence completely. One possible explanation is that SpikeClip operates on an element-wise basis and may use threshold that is too high. If all gradient components increase simultaneously, SpikeClip may still allow large overall gradient norm, as it focuses solely on clipping individual outliers and does not effectively handle uniformly large gradients. This is supported by the observation in Figure 3 that the gradient norm remains high even after SpikeClip is applied. 3. Stable-SPAM To address the training instability in 4-bit LLM training, we propose Stable-SPAM, stabilized spike-aware Adam optimizer. Apart from the momentum reset inherited from the original SPAM, Stable-SPAM introduces two techniques: Adaptive Gradient Norm (AdaGN) and Adaptive Spike-Aware Clipping (AdaClip), which we will explain in detail. The pseudocode is provided in Appendix C. Adaptive Gradient Norm (AdaGN). As we can observe in Figures 4 and 3, spikes in training loss and instances of training divergence usually align with abrupt surges in the gradient norm, consistent with findings in (Takase et al., 2023; Huang et al., 2025). To address these training instabilities, we propose AdaGN, method that stabilizes gradients by adaptively scaling them based on their historical l2 norm statistics. To better track the dynamics of the gradient norm during training, we leverage the idea of Adam by maintaining moving averages of both the first and second moments of the gradient norm. Concretely, we compute and update the moving averages of the gradient norm (mnorm, vnorm), Figure 3. Effect of SpikeClip (Huang et al., 2025) on stabilizing training. Left: gradient norms before and after performing gradient spike clip. Right: training loss with and without gradient spike clip. Models are trained by Adam optimizer based on LLaMA130M and C4. ② Lower-bit training suffers more loss spikes and gradient norm spikes. Figure 4 illustrates this phenomenon by comparing the training loss and gradient norm curves of LLaMA-130M and LLaMA-350M trained under BF16 and FP4 (E1M2) precision, using various learning rates. We observe that BF16 training remains stable, but FP4 training exhibits significant loss spikes, which occur on both model sizes. Furthermore, these loss spikes are consistently accompanied by gradient norm explosions. ③ SPAM performs the best in 4-bit training but needs careful learning rate tuning. As shown in Figure 2, SPAM achieves the lowest eval loss among various optimizers in INT4 or FP4 with the optimal learning rate. However, its validation loss either diverges to NaN or sharply increases as the learning rate rises. Additionally, we monitored the training loss and gradient norm after applying the spike clipping technique (SpikeClip) proposed in SPAM. SpikeClip detects and mitigates gradient outliers by leveraging the second mo3 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Figure 4. Training loss and gradient norm of Adam using various learning rates with BF16 and FP4 precision. Experiments are conducted under the same training configuration with LLaMA-130M/350M. then use them to derive normalized gradient: gnorm = gt2, mnorm = γ1 mnorm + (cid:0)1 γ1 vnorm = γ2 vnorm + (cid:0)1 γ2 (cid:1) gnorm, (cid:1) g2 norm, ˆgt = gt gnorm mnorm vnorm + ϵ . (1) (2) (3) (4) where ˆgt is the normalized gradient, γ1 and γ2 are momentum coefficients and ϵ is small constant for numerical stability. By rescaling gt with ratio of its historical mean norm mnorm to the square root of its historical second moment vnorm, AdaGN mitigates abrupt gradient norm spikes. Note that as the gradient norm gnorm is essentially scalar for an entire layer, the additional parameter overhead introduced by AdaGN is negligible, i.e., two extra parameters per layer. Adaptive Spike-Aware Clipping (AdaClip). Different from the spike gradient clipping technique in (Huang et al., 2025), which sets fixed clipping threshold, we propose an adaptive clipping approach, i.e., AdaClip. The core idea is to dynamically adjust the clipping threshold by tracking the maximum gradient magnitude observed over time, rather than relying on pre-defined fixed value. Concretely, let gt be the gradient at time step t. We first compute gmax, the maximum absolute gradient value across all parameters. Then, we update the threshold Tthreshold with an exponential moving average that incorporates gmax. Finally, any entries of gt that exceed Tthreshold are rescaled to maintain stability. The procedure is formally expressed as follows: gmax = max (gt[i]), Tthreshold = γ3 Tthreshold + (1 γ3) gmax, Maskspikes = (gt > Tthreshold), gt[Maskspikes] = gt[Maskspikes] gmax Tthreshold, (5) (6) (7) (8) where γ3 [0, 1] controls the weight of the moving average. When γ3 is large, Tthreshold responds more slowly to new gradient maxima, leading to more stable updates. When γ3 is small, it adapts more quickly to sharp changes in gradient magnitude. Momentum Reset (MoRet). Following Huang et al. (2025), we adopt momentum reset (MoRet) to periodically reset the accumulated first and second moments in Adam. The effectiveness of MoRet lies in addressing the negative effects of gradient spikes, which can inflate the first and second moments of Adam. Since Adam uses exponential moving averages to track their historical information, these inflated values caused by spiked gradients can have prolonged detrimental effects (Huang et al., 2025) on moments. By resetting the momentum terms at fixed intervals (T ), MoRet mitigates the lasting influence of unusually large gradients, enabling more stable and consistent optimization. 4 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Table 1. Comparison of various optimizers of INT4 and FP4 training of LLaMA models on C4. Perplexity is reported. INT4 Training FP4 Training 130M 350M 1B 130M 350M 1B Adam Adam+GradClip Adafactor SPAM Stable-SPAM 26.4 26.30 25.11 25.03 24.33 24.14 21.64 20.45 20.19 17.76 21.59 19.74 20.65 19.98 17.42 28.9 28.27 26.89 26.78 26. 24.59 20.84 20.53 20.35 19.49 22.01 20.25 20.03 19.74 18.48 Adam (BF16) 24.53 21.38 19. 24.53 21.38 19.73 Training Tokens 2.2B 4. Experiments demonstrate To proposed Stable-SPAM, we conduct extensive experiments with various sizes of the LLaMA model on the C4 dataset. efficacy the the of Baselines. We adopt five popular optimizers as our baselines including Adam (Kingma, 2014), Adafactor (Shazeer & Stern, 2018), Lion (Chen et al., 2024), Adam-mini (Zhang et al., 2024a) and SPAM (Huang et al., 2025). Among these, Adam and Adafactor are well-established and widely used, while Adam-mini and SPAM have been introduced more recently. Besides, we also include gradient clipping (Goodfellow, 2016) (GradClip) in conjunction with Adam as an additional baseline. Experimental Setup. Following (Lialin et al., 2023; Zhao et al., 2024a), we train LLaMA-based architectures ranging from 60M to 1B parameters. Each architecture is configured with RMSNorm (Shazeer, 2020) and SwiGLU activations (Zhang & Sennrich, 2019). For every model size, we keep the same set of hyperparameters across methods and vary only the learning rate. Specifically, we sweep over learning rates from 1 104 to 1 103 , incrementing by 2 104 for each optimizer. Following the settings in (Takase et al., 2023; Huang et al., 2025), we set the threshold to 1 for the GradClip baseline. For Adafactor, we adopt the hyperparameters from the original paper (Shazeer & Stern, 2018), where ϵ1 = 1030, ϵ2 = 103, and = 1.0. The hyperparameters for SPAM are configured based on the settings in (Huang et al., 2025), with reset intervals set to 500, learning rate warmup steps to 150, and the GSS threshold to 5000. For Stable-SPAM, we set γ1 = 0.7, γ2 = 0.9 and θ = 0.999 for 4-bit LLM training and γ1 = 0.85, γ2 = 0.9999 and γ3 = 0.999 for BF16 training. Detailed descriptions of our task setups and hyperparameters are provided in the Appendix A. 4.1. Performence of 4-bit LLM Training To evaluate the performance of Stable-SPAM in 4-bit LLM training, we conduct experiments using both FP4 ( 5 Figure 5. StableSPAM under Extremely Low-Precision Training. Experiments are conducted with 350M models on C4 Dataset. BF16-Adam denotes that the model is trained with BF16 by Adam. The final loss on validation set is reported. E1M2: 1-bit exponent, 2-bit mantissa) and INT4 (4-bit integer) quantization-aware training strategies. The training curves of various LLaMA models on the C4 dataset are presented in Figure 1, and the final perplexity results are summarized in Table 1. We observe that ❶ 4-bit training leads to significant performance drop compared to BF16 training. As shown in Table 1, the perplexity gap between BF16 (Adam) and INT4/FP4 (Adam) exceeds 1.5 across all model sizes, highlighting the challenges of reduced precision. ❷ Figure 1 shows that Stable-SPAM consistently outperforms Adam by significant margin in 4-bit scenarios, even surpassing the performance of 16-bit Adam. Table 1 further demonstrates that Stable-SPAM outperforms other advanced optimizers, such as Adafactor and SPAM. Among the baselines, incorporating GradClip reduces perplexity, while Adafactor and SPAM both outperform the simple application of GradClip. ❸ Stable-SPAM is able to match Adams performance with half the tokens in 4-bit training. As illustrated in Figure 1, Stable-SPAM achieves the same perplexity as Adam in approximately half the training steps. ❹ Notably, Stable-SPAM performs particularly well with larger models, such as LLaMA-350M and LLaMA-1B, showcasing its strong potential for large-scale training. This is likely because large-scale, low-precision training is more susceptible to instability issues (Fishman et al., 2024), making stabilized training approaches like Stable-SPAM especially beneficial. 4.2. Performence of Extremely Low-Precision Training To evaluate the performance of Stable-SPAM under extremely low-precision training, we conducted experiments on LLaMA-350M using A2W2 (INT2), A3W3 (INT3), and A4W4 (INT4) configurations. The final validation loss is presented in Figure 5. The results indicate that Stable-SPAM consistently outperforms Adam across all low-precision settings and even matches the performance of BF16-Adam under INT3 training. Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Figure 6. Performance of BF16 training with various model sizes. Experiments are based on LLaMA models trained on C4 Dataset. 4.3. Performence of BF16 LLM Training To further evaluate the efficacy of Stable-SPAM, we conducted experiments on various LLaMA model sizes using standard BF16 training. The experiments are based on C4 dataset. The training curves and final perplexity values are presented in Figure 6 and Table 2, respectively. Table 2 highlights that Stable-SPAM consistently delivers superior performance across different model sizes, surpassing the second-best optimizer with significant improvements. Furthermore, Figure 6 illustrates that Stable-SPAM achieves the same performance as Adam in only half the training steps or even fewer for LLaMA-350M and LLaMA-1B, validating its ability to match Adams performance while requiring significantly fewer tokens under BF16 LLM training. The above results demonstrate that the promise of Stable-SPAM not only holds for low-precision LLM training but also holds for the standard BF16 training. Table 2. Comparison among various optimizers on BF16 training. Perplexity is reported."
        },
        {
            "title": "Optimizer",
            "content": "60M 130M 350M 1B Adam-mini Adam Adam + GradClip Adafactor SPAM Stable-SPAM 34.10 34.09 33.33 32.57 30.46 28.84 24.85 24.91 24.88 23.98 23.36 22.21 19.05 18.77 18.51 17.74 17.42 16.85 16.07 16.13 15.22 15.19 14.66 13."
        },
        {
            "title": "Training Tokens",
            "content": "1.1B 2.2B 6.4B 11.6B 4.4. Integration with Other Optimizers Although AdaGN and AdaClip are proposed specifically for Stable-SPAM, one may wonder, Can AdaGN and AdaClip also be compatible with other optimizers? To answer this question, we applied AdaGN and AdaClip to two recently published optimizers: Lion (Chen et al., 2024) and Adam-mini (Zhang et al., 2024a). We conducted comparative experiments using Lion and Adam-mini alone, as well as in combination with AdaGN and AdaClip, under 4-bit training setting. These experiments were performed on LLaMA-60M/130M models with the C4 dataset. The results in Table 3 show that AdaGN and AdaClip consistently enhance the performance of both Lion and Adam-mini under FP4 and INT4 training settings, across LLaMA-60M and LLaMA-130M model sizes. Notably, on LLaMA-130M with INT4 training, Lion achieves perplexity improvement of up to 5.88, and Adam-mini on LLaMA60M under FP4 training sees an improvement of 1.72. These improvements underscore the broad applicability and effectiveness of the proposed AdaGN and AdaClip methods. Table 3. Performence of AdaGN and AdaClip on Lion and Adam-mini optimizers. Experiments are based on LLaMA60M/130M with 4-Bit training. Optimizers INT4 Training FP4 Training 60M 130M 60M 130M 39.36 Lion Lion+AdaGN+AdaClip 38.49 Adam-mini 34.84 Adam-mini+AdaGN+AdaClip 34.61 35.28 29.40 29.79 29.65 39.89 36.75 36.37 34.65 34.20 31.63 32.95 32.39 Training Tokens 1.1B 4.5. Effect on Stabilizing Training To validate the effectiveness of our proposed AdaGN and AdaClip techniques in stabilizing the LLM training process, Firstly, we compared the training loss and gradient norm curves across three settings: using Adam alone, using Adam with AdaGN, and using Adam with both AdaGN and AdaClip. Our experiments employed LLaMA-130M with learning rate of 3e-3 under an FP4 training setting. As 6 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Figure 7. Effect of AdaGN and AdaClip on stabilizing FP4 LLM training. The left two figures use LLaMA-130M (LR = 3e-3), and the right two figures use LLaMA-60M. shown in Figure 7, training solely with Adam leads to divergence in the training loss and frequent spikes in the gradient norm. However, once AdaGN is introduced, the training loss converges, and the gradient norm is noticeably reduced. Adding AdaClip on top of AdaGN further decreases the gradient norm and yields smoother training loss curve. Secondly, we present the final performance across range of learning rates, from 5 104 to 5 103, evaluated on LLaMA-60M under both FP4 and INT4 training settings. The results in Figure 7 show that Stable-SPAM produces significantly flatter curve, highlighting its stability across varying learning rates. These results demonstrate the effectiveness of the proposed AdaGN and AdaClip techniques in achieving more stable and consistent training process. 4.6. Ablation Study To validate the effectiveness of the three components, MoRet, AdaGN, and AdaClip, in Stable-SPAM, we conduct comprehensive ablation study. Specifically, we take two approaches: (1) We iteratively incorporate MoRet, AdaGN, and AdaClip into the Adam optimizer to measure their individual and combined improvements under both FP4 and BF16 training settings. (2) We replace AdaClip with SpikeClip (Huang et al., 2025) and AdaGN with GradClip (Goodfellow, 2016) to further assess the unique contributions of our proposed components. The results, summarized in Table 4, reveal the following observations: ❶ MoRet consistently improves performance across both FP4 and BF16 settings. ❷ Under both FP4 training, AdaGN alone shows limited improvement. However, when combined with AdaClip, it substantially reduces final perplexity. ❸ Conversely, in the BF16 setting, AdaGN alone yields considerable performance gains, but adding AdaClip offers limited improvement. This discrepancy may stem from the higher frequency of extreme element-wise gradient spikes in this FP4 training experiments, which necessitates AdaClip to correct biased update directions effectively. Finally, replacing AdaClip with SpikeClip (Huang et al., 2025) and AdaGN with GradClip (Goodfellow, 2016) results in increased perplexity, further validating the efficacy of our proposed AdaGN and AdaClip. Table 4. Ablations on Stable-SPAM. Experiments are based on LLaMA-60M and C4. Optimizer FP4 BF16 Adam Adam + MoRet Adam + MoRet + AdaClip Adam + MoRet + AdaGN Adam + MoRet + AdaGN + AdaClip (Stable-SPAM) 35.47 32.4 31.97 32.26 31.40 Adam + MoRet+AdaGN+SpikeClip (Huang et al., 2025) 32.01 Adam + MoRet+ GradClip (Goodfellow, 2016)+AdaClip 31.95 Adam + MoRet+AdaGN+AdaClip (Stable-SPAM) 31.40 34.09 31.47 30.29 28.96 28. 28.90 29.87 28.84 Training Tokens 1.1B 4.7. Hyper-Parameter Analysis Stable-SPAM introduces four hyperparameters: γ1, γ2, γ3, and , which extend the functionality of Adam. Among these, γ1 and γ2 serve similar purpose to β1 and β2 in Adam, controlling the smoothness of updates to the first moment mnorm and the second moment vnorm. Larger values of γ1 and γ2 result in smoother updates, placing greater emphasis on historical gradient norm statistics when adapting the current gradient norm. Similarly, γ3 plays role in determining the threshold for identifying gradient spikes. larger γ3 leads to smoother and more conservative threshold, resulting in higher proportion of gradients being classified as spike gradients. To investigate the impact of these hyperparameters, we plot the final perplexity curve while varying γ1 from 0.5 to 0.9, γ2 from 0.8 to 0.999, γ3 from 0.9 to 0.999, and from 250 to 5000. The experiments are conducted using LLaMA-60M, trained on 1.1B C4 toStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Figure 8. Hyper-parameter Analysis. Experiments are conducted with FP4 training on LLaMA-60M and C4 with 1.1B tokens. kens under the FP4 training setting. The results in Figure 8 demonstrate that overly small or excessively large values of these hyperparameters can degrade performance. However, the intuitive interpretations of these hyperparameters make them straightforward to tune, and they typically require minimal adjustments. In this paper, we adopt the optimal values γ1 = 0.7, γ2 = 0.9, γ3 = 0.999, and = 1000, which work effectively for all 4-bit training scenarios. 5. Related Work (LLM) large language model Instability of Training Large Language Models. The training, instability of which are marked by loss spikes and catastrophic divergence(Chowdhery et al., 2023; Molybog et al., 2023), has driven extensive research into stabilization techniques. These methods generally fall into three main categories: (1) gradient preprocessing, (2) architectural modifications, and (3) initialization strategies. Gradient preprocessing typically involves scaling and clipping gradients at the start of the optimization process to stabilize the training. well-known example is gradient clipping (Goodfellow, 2016), which globally rescales the gradient norm to fixed value. Later, Adafactor (Shazeer & Stern, 2018) introduced capping the norm of the parameter updates instead of the raw gradients. More recently, SPAM (Huang et al., 2025) proposed detecting and clipping anomalous gradients based on historical gradient statistics. However, common drawback of these methods is that they require manually setting predefined threshold. Architecturally, Xiong et al. (2020) showed that Post-LayerNorm (Post-LN) amplifies gradients, causing instability with large learning rates, while Pre-LayerNorm (Pre-LN) preserves gradient norms for stable training. Embed LayerNorm (Embed LN) normalizes embeddings(Dettmers et al., 2021), though it may impact performance(Scao et al., 2022), while Embed Detach(Ding et al., 2021; Zeng et al., 2022) reduces loss spikes by truncating gradients. DeepNorm(Wang et al., 2024) scales residual connections to stabilize ultra-deep models, and αReparam(Zhai et al., 2023) prevents attention entropy collapse via spectral-normalized parameterization. Initialization strategies offer complementary stability benefits. Scaled Embed(Takase et al., 2023) stabilizes LayerNorm gradients, while Scaled Initialization(Nguyen & Salazar, 2019) reduces variance using (0, (cid:112)2/(5d)/ 2N ). Fixup(Zhang et al., 2019; Huang et al., 2020) eliminates LayerNorm entirely, inspiring normfree architectures. Though ongoing advancements refine these approaches, training stability remains key challenge in LLM development. Low-precision LLM Training. Low-precision training (Wang et al., 2018; Lin et al., 2022; Xi et al., 2024a;b; Wortsman et al., 2023a) has emerged as promising approach to improve both computational and memory efficiency during training. Among these methods, FP16 (Micikevicius et al., 2017) and BF16 (Kalamkar et al., 2019) are the most widely adopted precision formats. To push the efficiency further, 8-bit training has garnered increasing attention. For instance, LM-FP8 (Peng et al., 2023) enables training with FP8 precision While (Fishman et al., 2024) demonstrates that as training scales up (larger than 250B tokens), the issue of activation outliers becomes more pronounced, posing challenges to the representation range of low-bit data formats. To address this challenge, (Fishman et al., 2024) proposes smoothing strategy, while (Ashkboos et al., 2025) leverages Hadamard transformations to mitigate the impact of activation outliers. Furthermore, the choice of data format significantly influences training performance. The INT8 format is the most widely supported low-precision format, whereas FP8, available in NVIDIAs Hopper GPU architecture, provides specialized support. Additionally, the MX format (Rouhani et al., 2023) demonstrates superior representational capability, though it is rarely supported by current hardware. In this work, we investigate the training instability associated with low-precision training and propose enhancements through the design of optimizers. Our approach is compatible with existing techniques, providing complementary solution to improve the stability of low-precision training. 8 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam 6. Conclusion This paper presents comprehensive study on the training instability challenges of 4-bit quantization in large language models. We find that while low-precision training significantly reduces memory and computational costs, it also amplifies the sensitivity to learning rates, and increases the likelihood of gradient and loss spikes. To address these issues, we propose Stable-SPAM, an optimizer that combines three key techniques: AdaClip, AdaGN, and MoRet. Empirical results on LLaMA models of various sizes demonstrate that Stable-SPAM not only stabilizes 4-bit training but also achieves better performance compared to existing optimizers, sometimes even surpassing BF16 performance. We additionally show that these stabilization strategies are broadly applicable, benefiting other optimizers like Lion and Adam-mini."
        },
        {
            "title": "Acknowledgments",
            "content": "This work used the Dutch national e-infrastructure with the support of the SURF Cooperative using the funding of the projects EINF-12538 and EINF-10925."
        },
        {
            "title": "Impact Statement",
            "content": "This paper advances the field of large language model (LLM) training by proposing stable optimizer that enables more stable and efficient optimization at low-precision (4-bit) arithmetic. By reducing computational and memory overhead, our approach has the potential to lower energy consumption and lessen the environmental footprint of training large-scale models. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ashkboos, S., Nikdan, M., Tabesh, S., Castro, R. L., Hoefler, T., and Alistarh, D. Halo: Hadamard-assisted lossless optimization for efficient low-precision llm training and fine-tuning. arXiv preprint arXiv:2501.02625, 2025. Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y., et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36, 2024. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34: 1982219835, 2021. Fishman, M., Chmiel, B., Banner, R., and Soudry, D. Scaling fp8 training to trillion-token llms. arXiv preprint arXiv:2409.12517, 2024. Goodfellow, I. Deep learning, 2016. Huang, T., Zhu, Z., Jin, G., Liu, L., Wang, Z., and Liu, S. Spam: Spike-aware adam with momentum reset for stable llm training. arXiv preprint arXiv:2501.06842, 2025. Huang, X. S., Perez, F., Ba, J., and Volkovs, M. Improving transformer optimization through better initialization. In International Conference on Machine Learning, pp. 4475 4483. PMLR, 2020. Kalamkar, D., Mudigere, D., Mellempudi, N., Das, D., Banerjee, K., Avancha, S., Vooturi, D. T., Jammalamadaka, N., Huang, J., Yuen, H., et al. study of arXiv preprint bfloat16 for deep learning training. arXiv:1905.12322, 2019. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lee, J., Bae, J., Kim, B., Kwon, S. J., and Lee, D. To fp8 and back again: Quantifying the effects of reducing precision on llm training stability. arXiv preprint arXiv:2405.18710, 2024. Li, S., Liu, H., Bian, Z., Fang, J., Huang, H., Liu, Y., Wang, B., and You, Y. Colossal-ai: unified deep learning system for large-scale parallel training. In Proceedings of the 52nd International Conference on Parallel Processing, pp. 766775, 2023. Lialin, V., Muckatira, S., Shivagunde, N., and Rumshisky, A. Relora: High-rank training through low-rank updates. In The Twelfth International Conference on Learning Representations, 2023. Lin, J., Zhu, L., Chen, W.-M., Wang, W.-C., Gan, C., and Han, S. On-device training under 256kb memory. Advances in Neural Information Processing Systems, 35: 2294122954, 2022. 9 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Ma, C., Gong, W., Scetbon, M., and Meeds, E. Swan: Preprocessing sgd enables adam-level performance on llm training with significant memory reduction. arXiv preprint arXiv:2412.13148, 2024. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Molybog, I., Albert, P., Chen, M., DeVito, Z., Esiobu, D., Goyal, N., Koura, P. S., Narang, S., Poulton, A., Silva, R., et al. theory on adam instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. arXiv preprint arXiv:1910.05895, 2019. Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022. Peng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z., Xiong, Y., Yang, Z., Ni, B., Hu, J., et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. Rouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi, A., Deng, S., Choudhary, D., Cornea, M., Dellinger, E., Denolf, K., et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. Scao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M. S., Biderman, S., Elsahar, H., Muennighoff, N., Phang, J., et al. What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424, 2022. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 45964604. PMLR, 2018. Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., and Wei, F. Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Wang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakrishnan, K. Training deep neural networks with 8-bit floating point numbers. Advances in neural information processing systems, 31, 2018. Wortsman, M., Dettmers, T., Zettlemoyer, L., Morcos, A., Farhadi, A., and Schmidt, L. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36: 1027110298, 2023a. Wortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., et al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023b. Xi, H., Li, C., Chen, J., and Zhu, J. Training transformers with 4-bit integers. Advances in Neural Information Processing Systems, 36:4914649168, 2023. Xi, H., Cai, H., Zhu, L., Lu, Y., Keutzer, K., Chen, J., and Han, S. Coat: Compressing optimizer states and activation for memory-efficient fp8 training. arXiv preprint arXiv:2410.19313, 2024a. Xi, H., Chen, Y., Zhao, K., Teh, K. J., Chen, J., and Zhu, J. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024b. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 10524 10533. PMLR, 2020. Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In International Conference on Machine Learning, pp. 4077040803. PMLR, 2023. Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 10 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Zhang, H., Dauphin, Y. N., and Ma, T. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. Zhang, Y., Chen, C., Li, Z., Ding, T., Wu, C., Ye, Y., Luo, Z.-Q., and Sun, R. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024a. Zhang, Z., Jaiswal, A., Yin, L., Liu, S., Zhao, J., Tian, Y., and Wang, Z. Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients. arXiv preprint arXiv:2407.08296, 2024b. Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024a. Zhao, R., Morwani, D., Brandfonbrener, D., Vyas, N., and Kakade, S. Deconstructing what makes good optimizer for language models. arXiv preprint arXiv:2407.07972, 2024b. 11 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam A. Architecture and Hyperparameters We introduce details of the LLaMA architecture and hyperparameters used for 4-bit and BF16 pre-training, following Lialin et al. (2023); Zhao et al. (2024a). Table 5 shows the most hyperparameters of LLaMA models across model sizes. We use max sequence length of 256 for all models, with batch size of 512, with batch size of 131K tokens. For all experiments, we adopt learning rate warmup of 2000 training steps, and use cosine annealing for the learning rate schedule, decaying to 10% of the initial learning rate. Table 5. Configurations of LLaMA models used in this paper. Params Hidden Intermediate Heads Layers 60M 130M 350M 1 512 768 1024 2048 1376 2048 2736 5461 8 12 16 24 8 12 24 32 For all methods across each model size (from 60M to 1B), we tune the learning rates from 1e 4 to 1e 3 with an increasing step of 2 104 for pre-training tasks, and the best learning rate is selected based on the validation perplexity. The detailed hyperparameter of Stable-SPAM on 4-bit training and BF16 training are reported in Table 6 and Table 7. Table 6. Hyperparameters of Stable-SPAM for 4-bit pre-training experiments in this paper. Hyper-Parameters LLaMA-130M LLaMA-350M LLaMA-1B LR γ1 γ2 γ3 1e 3 1000 0.7 0.9 0.999 4e 4 1000 0.7 0.9 0. 2e 4 1000 0.7 0.9 0.999 Table 7. Hyperparameters of Stable-SPAM for BF6 pre-training experiments in this paper. Hyper-Parameters LLaMA-60M LLaMA-130M LLaMA-350M LLaMA-1B Standard Pretraining LR γ1 γ2 γ 1e 3 1000 0.85 0.99999 0.999 8e 4 1000 0.85 0.99999 0.999 4e 4 1000 0.85 0.99999 0.999 2e 4 1000 0.85 0.99999 0.999 B. Time Series Forescasting Task We conducted additional experiments on time-series prediction tasks. In these experiments, we intentionally introduced anomalous data with probability A=10% to simulate gradient anomalies. Experiments are conducted with 10 repeated runs on Weather time series data2 using PatchTST (Nie et al., 2022) model. The results are presented in Figure 9. The findings demonstrate that as the severity of anomalous data increases, Stable-SPAMs performance advantage over Adam becomes more pronounced. Besides, Stable-SPAM consistently surpasses SPAM across all settings. These results further highlight the effectiveness of the proposed Stable-SPAM. C. Pseudocode The pseudocode is presented in Alogrithm 1. 2https://www.bgc-jena.mpg.de/wetter/ 12 Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam Figure 9. Test Loss during Training Process on Weather Time-series Data. Anomalous data is generated by adding Gaussian noise to 10% of randomly selected input values. Specifically, the anomalies data are conducted with = X+Gaussin(0, SeverityMax(X)) where is the inputs. Algorithm 1 Stable-SPAM Input: layer weight matrix Rmn, learning rate α, decay rates β1 = 0.9, β2 = 0.999, initial parameters w0, γ1 = 0.7, γ2 = 0.9 for AdaGN and γ3 = 0.999 for AdaClip, momentum reset interval , small constant ϵ = 1 106, and total training steps . Output: Optimized parameters wT . while < do gt Rmn wϕt(wt) gmax Max(abs(gt)) Tthreshold Tthreshold θ + (1 θ) gmax Tthreshold (cid:98)Tthreshold 1 θt Maskspikes (cid:0)abs(gt) > (cid:98)Tthreshold if sum(cid:0)Maskspikes (cid:1) > 0 then (cid:1) gt[Maskspikes] gt[Maskspikes] gmax (cid:98)Tthreshold end gnorm gt2 mnorm γ1 mnorm + (1 γ1) gnorm vnorm γ2 vnorm + (1 γ2) g2 mnorm , (cid:98)vnorm (cid:98)mnorm 1 γt 1 adaptive norm (cid:98)mnorm norm vnorm 1 γt 2 gt gt gnorm (cid:98)vnorm + ϵ adaptive norm if (Mod(t, ) = 0) then zeros like(m) zeros like(v) end mt β1 mt1 + (1 β1) gt vt β2 vt1 + (1 β2) g2 (cid:99)mt mt 1 βt 1 vt (cid:98)vt 1 βt 2 wt wt1 α (cid:99)mt + 1 (cid:98)vt + ϵ end return wT . // Gradient of the objective at step t. // Bias correction for threshold // Bias-corrected norm estimates // bias correction // bias correction"
        }
    ],
    "affiliations": [
        "Department of Computer Science, The University of North Carolina at Chapel Hill",
        "Department of Computer Science, University of Exeter",
        "Department of Computer Science, University of Reading",
        "Department of Electrical and Computer Engineering, University of Texas at Austin",
        "Department of Mathematics and Computer Science, Eindhoven University of Technology",
        "Mathematical Institute, University of Oxford",
        "School of Cyber Science and Technology, Sun Yat-sen University",
        "School of the Gifted Young, University of Science and Technology of China",
        "Squirrel Ai Learning"
    ]
}