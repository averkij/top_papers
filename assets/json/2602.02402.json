{
    "paper_title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
    "authors": [
        "Mu Huang",
        "Hui Wang",
        "Kerui Ren",
        "Linning Xu",
        "Yunsong Zhou",
        "Mulin Yu",
        "Bo Dai",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding."
        },
        {
            "title": "Start",
            "content": "SoMA: Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation Mu Huang 1 2 Hui Wang 3 2 Kerui Ren 3 2 Linning Xu 4 2 Yunsong Zhou 2 Mulin Yu 2 Bo Dai 5 Jiangmiao Pang 2 6 2 0 2 2 ] . [ 1 2 0 4 2 0 . 2 0 6 2 : r Figure 1. SoMA is GS neural simulator that reconstructs and simulates deformable object dynamics from real-world robot manipulation. Learning from multi-view RGB observations, it performs action-conditioned simulation directly on Gaussian splats, enabling interactionconsistent, stable long-horizon resimulation with higher-fidelity rendering under both seen and unseen manipulations."
        },
        {
            "title": "Abstract",
            "content": "Simulating deformable objects under rich interactions remains fundamental challenge for realto-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable longhorizon manipulation and generalization beyond 1Fudan University, China 2Shanghai Artificial Intelligence Laboratory, China 3Shanghai Jiao Tong University, China 4The Chinese University of Hong Kong, China 5The University of Hong Kong, China. Correspondence to: Bo Dai <bdai@hku.hk>. Preprint. February 3, 2026. 1 observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding. Project Page: city-super.github.io/SoMA 1. Introduction Embodied learning is inherently data-driven, as it requires large-scale interaction data for robots to understand and act in the physical world. Yet, collecting real-world robot manipulation data is costly and risky (Zheng et al., 2025), as in tasks involving deformable objects like cloth folding and flexible object handling (Ganapathi et al., 2021; Mo et al., 2022). Thus, real-to-sim (R2S) simulation offers alternative by scalable reproducing real-world object behaviors in virtual environments, serving as foundation for data synthesis, augmentation, and policy learning. practical simulator must strike balance between physical fidelity and long-horizon interaction consistency. It should Submission and Formatting Instructions for ICML 2026 faithfully capture deformable object geometry and dynamics without introducing R2S bias, while remaining stable and coherent under sustained robotobject interaction. While geometry can often be recovered from visual observations, learning physically meaningful dynamics and interactions remains challenging. This challenge is exacerbated by partial occlusion and complex contact patterns. Existing approaches address these challenges along two largely separate directions. Physics-based simulators (Belytschko et al., 2014; Jiang et al., 2016) ensure consistent interaction over long horizons, but rely on predefined physical models and parameters that are difficult to infer from visual data. Differentiable simulators (Jiang et al., 2025a; Zhang et al., 2024b; Zhong et al., 2024) alleviate this limitation by optimizing small set of parameters, yet remain constrained by simplified physical assumptions. In contrast, neural dynamics modeling (Shao et al., 2024; Zhang et al., 2024a) and 4D reconstruction methods (Bahmani et al., 2024a;b; Ling et al., 2024; Ren et al., 2023) learn motion directly from data and recover dynamic geometry, but primarily focus on reproducing observed trajectories, offering limited support for robot-conditioned interaction and generalization beyond the training distribution. As result, neither direction alone suffices for R2S simulation in complex robot manipulation. We bridge the two worlds by rethinking the representation and learning of deformable object simulation. This paper proposes SoMA, real-to-sim neural Simulator of Manipulation for soft-body interaction, operating directly on learned Gaussian splat representations. SoMA models deformable objects, robot actions, and environmental effects in unified learned particle representation, enabling causal action-driven dynamics and stable long-horizon interaction without predefined physical rules. Explicit designs are proposed to achieve the functionality through the neural simulator. To ensure kinematically consistent interaction between the robot and deformable objects, it establishes robot-conditioned R2S mapping that anchors learned object dynamics to the robots joint-space actions, ensuring causal and kinematically consistent interaction. For physically consistent interaction, dynamics are modeled through force-driven updates defined directly on Gaussian splats, allowing local contact effects to propagate through the deformable object even when visual observations are partial or occluded. To support long-horizon interaction without drift or collapse, SoMA adopts multi-resolution training strategy that balances temporal coverage and computational efficiency, together with blended supervision scheme that combines occlusion-aware reconstruction signals with physics-inspired consistency constraints. Together, these components enable SoMA to serve as stable, actionconditioned neural simulator for R2S soft-body manipulation. We summarize our contributions as follows: (1) R2S neural simulation paradigm for soft-body manipulation, supporting long-horizon, interaction-consistent simulation. (2) Mechanisms that make such simulation feasible, including robot-conditioned alignment to anchor scenes to real kinematics, force-driven Gaussian-splat dynamics model for contact and occlusion, and multi-resolution training with blended supervision for stable long-horizon performance. (3) Extensive evaluation on public benchmarks and new real-world dataset, demonstrating state-of-the-art RGB and depth performance (20% improvement) and clear advantages for sustained interactive manipulation. 2. Related Works 3D reconstruction and scene representation. Camera pose estimation and 3D reconstruction form the geometric basis of vision-based simulation and real-to-sim pipelines. Classical multi-view geometry methods, such as SLAM and COLMAP (Cadena et al., 2017; Campos et al., 2021; Grisetti et al., 2011; Schonberger & Frahm, 2016), recover camera poses and scene structure from overlapping views. Recent feed-forward reconstruction approaches based on Gaussian Splatting, including VGGT, Pi3, and AnySplat (Wang et al., 2025b; Jiang et al., 2025b; Kerbl et al., 2023; Liu et al., 2025; Wang et al., 2025a), further enable reconstruction under sparse or unstructured observations. Our work builds on these methods to obtain camera poses and Gaussian splat representations, while focusing on dynamics modeling and robot-conditioned interaction rather than reconstruction. Physics-based simulators. Physics-based simulators, such as FEM, MPM, and SPH (Belytschko et al., 2014; Gingold & Monaghan, 1977; Jiang et al., 2016; Muller et al., 2007; Reddy, 1993; Sulsky et al., 1994), are widely used to model deformable objects under controlled settings and are supported by embodied simulation platforms such as Isaac Sim (NVIDIA, 2023). However, these methods rely on carefully specified physical parameters and simulator configurations, making accurate parameter identification from visual observations challenging in real-to-sim scenarios, especially under robot-driven interactions. Recent differentiable simulators (Macklin, 2022; Hu et al., 2019), including PhysDreamer and PhysTwin (Zhang et al., 2024b; Zhong et al., 2024; Jiang et al., 2025a), attempt inverse optimization of limited parameters but still depend on predefined physical models and simplified structures, limiting their ability to capture complex real-world motions and interactions. Neural-based dynamics modeling. Neural-based approaches learn object dynamics directly from data, reducing reliance on explicit physical parameter specification. Some 2 Submission and Formatting Instructions for ICML 2026 Figure 2. Framework of SoMA. SoMA takes RGB observations and robot joint-space actions collected from real-world manipulation as input (Left). It reconstructs deformable objects as hierarchical Gaussian splats, and propagates them through neural simulator with supervision from rendering and dynamics (Middle). Object motion is driven by force-based interactions, where environmental and robot-induced forces act on splats to produce deformation (Right). two-stage multi-resolution training strategy first captures global motion with large temporal gaps and then refines fine-grained dynamics under occlusion and contact using small gaps. 4D reconstruction methods (Bahmani et al., 2024a;b; Borycki et al., 2024; Feng et al., 2024; Ling et al., 2024; Ren et al., 2023; Shen et al., 2023; Singer et al., 2023; Yin et al., 2023) extend Gaussian Splatting to dynamic scenes, providing temporally consistent geometry but mainly reconstruct observed motions with limited interaction modeling. Neural simulators on GS representations, such as GausSim and GSDynamics (Shao et al., 2024; Zhang et al., 2024a), regress future states from past states, improving generalization over reconstruction-based methods but typically evaluated under simplified objects and interaction settings. In contrast, our method targets robot-manipulated, occlusion-heavy embodied scenarios by explicitly modeling robotobject interactions and performing end-to-end simulation directly from video observations. 3. Preliminaries Hierarchical graph simulator. We represents deformable object as collection of Gaussian splats (GS) (Shao et al., 2024; Kerbl et al., 2023), where each splat gi = {xi, Σi, mi, ai} encodes its spatial position xi R3, anisotropic covariance Σi R33, mass mi, and additional physical attributes ai Rattr dim. To improve efficiency and better capture the continuous dynamics of deformable objects, the GS are organized into hierarchical graph structure. Starting from the finest GS level, splats are recursively clustered to form higher-level nodes, each representing an aggregated local structure of the object. For cluster composed of Ni child nodes, its aggregated attributes are computed by combining the physical states of its child nodes: mcl = (cid:88) mi, xcl = (cid:80) mixi mcl , acl = 1 Ni (cid:88) ai. (1) 3 This bottom-up construction defines multi-level hierarchical graph that captures object structure at different spatial scales. Dynamics are then propagated through the hierarchy using graph neural networks in top-down manner. At each level, latent motion and deformation states are predicted for cluster nodes and subsequently transferred to their children through learned transformations. Following prior hierarchical GS simulators, the state of GS node at level 1 is obtained from its ancestor cluster ch as: ˆxh1 = ˆxh ch + (cid:89) Fj (Xk Xch ) , h1 ˆΣ = j=h (cid:33) Fi Σk (cid:32) (cid:89) i=h (cid:32) (cid:89) (cid:33) Fi , i=h (2) (3) where Fi denotes the learned transformation at level i. This hierarchical propagation enables coherent global motion while preserving local deformations at the GS level. 4. Method 4.1. Problem Defination We consider modeling deformable object dynamics under robot manipulation from multi-view RGB videos. At each time step t, we are given synchronized RGB images It,i captured by three cameras, together with the robot joint state Rt. An initial object state is reconstructed as set of Gaussian splats G0 = {gi}. The object state is then regressed over time using an action-conditioned simulator: Gt = ϕθ(Gt1, Gt2, Rt), (4) where ϕθ predicts the object dynamics conditioned on robot actions. Submission and Formatting Instructions for ICML 2026 At each time step, the predicted state Gt is rendered under known camera poses to obtain images ˆIt,i. The simulator is trained end-to-end by minimizing the reconstruction loss between rendered and observed images across time and views: min θ,a (cid:88) t,i L(cid:0)ˆIt,i, It,i (cid:1). (5) 4.2. SoMA Framework SoMA is unified neural simulator for soft-body robot manipulation, designed to model deformable object dynamics under direct robot joint-space control and environmental interaction. Unlike prior approaches that decouple object dynamics from control signals or rely on externally specified end-effector trajectories, SoMA jointly represents robots, deformable objects, and environments within shared simulation space. This unified formulation allows robot actions to directly drive object dynamics, enabling interaction-aware and numerically stable long-horizon simulation from visual observations. As shown in Figure 2, SoMA consists of three key components. First, scene-to-simulation mapping module lifts real-world observations into unified simulation space, producing consistent representations of the robot, deformable object, and environment (4.2.1). Second, hierarchical graph-based simulator models robotobjectenvironment interactions through structured interaction graphs, enabling localized contact reasoning with global physical consistency (4.2.2). Third, we adopt multi-resolution training strategy to optimize dynamics across temporal and spatial scales for efficient and stable long-horizon simulation, together with blended supervision scheme that combines occlusionaware image losses and physics-inspired consistency constraints (4.2.3, 4.2.4). Together, these components form an interaction-aware simulation pipeline for physically consistent soft-body manipulation. 4.2.1. SCENE INITIALIZATION VIA R2S MAPPING Embodied manipulation poses fundamental real-to-sim challenge: visual reconstructions, robot kinematics, and physical reference frames exist in heterogeneous coordinate systems and metric scales. Common initialization strategies that reconstruct objects alone or align object motion with tracked end-effector trajectories decouple geometry from control, overlooking robot kinematics and environmental constraints. As result, joint-space actions cannot be directly applied, and physically meaningful contact reasoning becomes ill-defined. Therefore, we construct unified simulation space consistent with robot kinematics, manipulated objects, and physical reference frames. Reconstruction. Given multi-view RGB images from calibrated cameras, camera poses are estimated using multiview geometry. Based on the recovered poses, the manipulated object are reconstructed using Gaussian Splatting (Kerbl et al., 2023), yielding GS representation G0 in the camera coordinate system. Robot Conditioning. We recover the global scale factor by enforcing metric consistency between reconstructed geometry and known reference dimensions observed across the robot, camera, and world coordinate systems. The rigid transformation parameters and are estimated from the relative poses of the camera expressed in the robot and camera coordinates. Given the robot joint configuration qt at time t, the end-effector pose in the robot base frame is obtained via forward kinematics:"
        },
        {
            "title": "Tee",
            "content": "rob(t) = FK(qt). (6) The corresponding pose in the simulation space is computed by similarity transformation: Tee sim(t) = (cid:21) (cid:20)s 0 1 Tee rob(t). (7) The gripper opening state ct, extracted from the joint configuration, together with Tee sim(t), defines the robot action used to drive object interactions in simulation. Physical reference frames. The physical reference direction confirmed by fitting the supporting table plane Π from the reconstructed point cloud. Let nΠ denote the plane normal and vc the camera viewing direction. The gravity direction is defined as: = sign(nΠ vc) nΠ, (8) which resolves the sign ambiguity of gravity and aligns the simulation with the real-world scene. 4.2.2. FORCE-DRIVEN GS DYNAMICS MODELING Existing neural GS dynamics models are primarily statebased or model interactions only weakly, which is sufficient for isolated or simply interacting deformable objects. In robot manipulation, however, object motion is largely governed by contact-induced forces from the robot and the environment. Such neural models often fail to generalize across diverse interaction patterns, as they entangle object motion with contact-specific effects. Therefore, we model environment interactions as explicit global forces and robot interactions as implicit forces computed through the interaction graph, both applied directly at the Gaussian splat (GS) level. These forces are hierarchically propagated through neural network, where each level predicts the next state conditioned on both the current state and interaction forces, enabling force-driven object dynamics rather than state-only regression. 4 Submission and Formatting Instructions for ICML 2026 Force-driven dynamics. For each GS node or cluster i, the linear velocity and rotation are predicted from its historical states and the aggregated interaction force: (vi, ωi) = ψθ (cid:0)gt1 , gt2 , fi (cid:1) , (9) where ψθ denotes hierarchical neural dynamics model and fi is the total interaction force acting on node i. Environment force. Environmental effects are modeled as external forces. Each GS node is subject to gravity, and nodes close to the supporting surface receive an additional support force: (cid:40) env = + si, di < τ, di τ, g, (10) where di denotes the signed distance from the Gaussian splat to the supporting plane. Environment forces are aggregated bottom-up across the hierarchical structure. Robot force. Robot interaction is modeled via an interaction graph constructed between robot control points and GS nodes. Given the robot action at time t, the robot-induced force on GS node is predicted as: Multi-resolution image. GS geometry is reconstructed from super-resolution images to preserve structural details, while dynamics training is conducted at original image resolution to reduce computational cost. 4.2.4. IMMIGRATED SUPERVISION Embodied manipulation inherently suffers from partial observability, as deformable objects are frequently occluded by the robot end-effector or by self-contact during interaction. This prevents the direct use of reliable 3D tracking as supervision and makes pure image-based losses insufficient for recovering occluded object states. To address this challenge, we adopt blended training strategy that combines occlusion-aware image supervision for visible regions with physics-inspired consistency constraints to regularize unobserved dynamics. Occlusion-aware image supervision. Naively supervising rendered images over the full image domain introduces spurious gradients from occluded regions and leads to unstable dynamics learning. We therefore apply image-based supervision selectively to visible object regions, allowing reliable visual evidence to guide GS-level state updates where observations are available. rob = Φθ (cid:0)gt , {rt j}jN (i), ct (cid:1) , (11) Given binary object mask Mt, the image reconstruction loss is defined as: where {rt j} are neighboring robot control points, ct denotes the gripper state, and Φθ is graph-based neural interaction module. The total interaction force is given by fi = env + rob . 4.2.3. MULTI-RESO. TRAINING FOR LONG HORIZON Learning GS-based dynamics for embodied manipulation is particularly challenging over long horizons. Small prediction errors at the splat level accumulate rapidly under repeated robot interactions, leading to drift and instability in both geometry and appearance. Moreover, GS-based simulation is conditioned on specific initial splat configuration G0, which prevents random temporal cropping or noise-based reinitialization commonly used in particlebased simulators. To address these challenges, we adopt multi-resolution training strategy along both temporal and image dimensions to enable efficient and stable long-horizon learning. L(t) img = λ (cid:13) (cid:13) 2 (cid:13)Mt (ˆIt It) (cid:13) (cid:13) (cid:13) 2 (cid:16) Mt ˆIt, Mt It + (1 λ) LD-SSIM (12) (cid:17) , where denotes element-wise masking and the loss is evaluated only within the masked region. Momentum consistency regularization. While image supervision constrains dynamics in visible regions, occluded splats lack direct visual feedback during training. To prevent physically inconsistent motion in these unobserved regions, we introduce momentum consistency regularization over the hierarchical GS representation. This constraint propagates physically plausible dynamics across hierarchy levels and mitigates long-horizon drift caused by accumulated prediction errors. Specifically, we enforce momentum conservation between adjacent hierarchy levels via: Multi-resolution temporal. We employ coarse-to-fine temporal training scheme. In the first stage, the model is trained with larger temporal stride dt to capture longrange dynamics. In the second stage, training is performed at the original resolution dt using randomly sampled subsequences of length k, enabling fine-grained dynamics learning while mitigating error accumulation. Lmom = L1 (cid:88) l=1 (cid:13) (cid:13) (cid:13) mcl ˆxcl (cid:13) (cid:13) (cid:13) (cid:88) mi ˆxi iCl1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 , (13) where Cl1 denotes the set of child nodes of cluster cl. This term provides self-supervised physical regularization without requiring ground-truth forces or contacts. 5 Submission and Formatting Instructions for ICML 2026 Figure 3. Qualitative resimulation and generalization under robot manipulation. Left: resimulation on training trajectories. Right: generalization to unseen robot actions and contact configurations. Across diverse soft-body objects, including near-linear (rope), nearplanar (cloth), and volumetric (doll) objects, SoMA produces stable, long-horizon simulations that closely match observed dynamics. PhysTwin shows deviations under complex or unseen interactions due to real-to-sim mismatch, while GausSim often remains static or unstable in challenging scenarios. 6 Submission and Formatting Instructions for ICML 5. Experiments 5.1. Experimental Settings Datasets. We collect real-world robot manipulation datasets on an ARX-Lift platform, covering four deformable objects: rope, doll, cloth, and T-shirt. RGB images are captured at resolution of 640 480 and 30 FPS, with robot joint states recorded synchronously. Most sequences contain 100150 frames. For each object, we collect 3040 sequences with diverse initial configurations and manipulation actions. The datasets are split into training and test sets with ratio of 7:3. (i) ReTasks. We evaluate models on two tasks: simulation, which measures reconstruction and simulation accuracy on training trajectories; (ii) Generalization, which evaluates performance on unseen manipulation sequences in the test set. In both settings, models are initialized with reconstructed Gaussian splats and perform open-loop simulation conditioned on per-frame robot actions. Baselines. As no existing method directly supports robotmanipulated scenes with complex interactions and occlusions, we compare against representative physics-based and neural approaches, including PhysTwin (Jiang et al., 2025a) and GausSim (Shao et al., 2024). All baselines are adapted to our setting for fair comparison (details in the supplementary material). Evaluation metrics. We evaluate simulation quality using observation-based metrics. For RGB observations, we report PSNR, SSIM, and LPIPS (Wang et al., 2004; Wang, 2004; Zhang et al., 2018) to measure pixel-level fidelity, structural similarity, and perceptual consistency, respectively. Since full 3D ground truth is unavailable in realworld manipulation scenes, we use depth as geometric proxy and evaluate depth accuracy using Absolute Relative Error (Abs Rel) and RMSE on valid regions. Abs Rel captures scale-invariant relative depth errors, while RMSE reflects absolute geometric deviations. All metrics are averaged over all frames within each video sequence and across evaluation scenarios. 5.2. Results ies to analyze the contribution of key components in our framework. Resimulation performance across deformable objects. Fig. 3 shows that our method produces resimulated trajectories that closely match ground-truth observations over long temporal horizons. The simulated object dynamics remain stable throughout the rollout, accurately preserving both global motion and local deformations. Quantitative results in Tab. 1 further confirm the robustness of our approach, where our method achieves the best performance across all metrics on the resimulation task. Compared to the differentiable simulator PhysTwin, which relies on explicit point tracking and degrades under occlusion-heavy robot manipulation, our method yields more accurate dynamics and more consistent visual reconstructions. In contrast, state-based neural simulators such as GausSim exhibit increasing deviation in later timesteps, failing to preserve long-term object dynamics, particularly under interaction. Overall, these results demonstrate that our method enables accurate and robust resimulation of deformable objects in complex robot manipulation scenarios. Generalization to unseen manipulations. We evaluate generalization under manipulation settings that differ from those observed during training, including novel action trajectories and contact configurations. As shown in Fig. 3, our method maintains accurate and stable dynamic evolution in such settings, closely matching the ground-truth observations. Quantitative results in Tab. 1 further confirm strong generalization performance. key advantage of our approach is its controllability: by conditioning dynamics on robot actions and interaction cues, the simulator can produce consistent object behavior beyond memorized trajectories. In contrast, state-based neural simulators exhibit large deviations when exposed to unseen actions, while differentiable physics-based methods often fail to preserve coherent object structure under complex interactions, resulting in inaccurate deformations. Overall, these results demonstrate the robustness of SoMA in generalizing real-to-sim simulation across unseen manipulation scenarios. We present experimental results to evaluate the real-to-sim simulation capability of our method under progressively more challenging settings. First, we assess both resimulation and generalization performance on cloth, rope, and doll datasets with diverse manipulation actions. Next, to examine the potential of our approach in embodied manipulation scenarios, we evaluate it on the more challenging T-shirt folding dataset, which involves complex interactions and large deformations. Finally, we provide ablation studComplex interaction: T-shirt folding. T-shirt folding involves long-horizon dynamics, large deformations, and frequent self-contacts, making it substantially more challenging than prior resimulation and generalization settings. Such complexity amplifies error accumulation and often leads to structural collapse or severe artifacts in existing simulators. Fig. 1 (more results shows in appendix) shows that our method can stably simulate the full folding process with 7 Submission and Formatting Instructions for ICML 2026 Table 1. Quantitative evaluation on resimulation and generalization under robot manipulation. We report performance comparisons with PhysTwin and GausSim across image-based and depth-based metrics. Our method achieves the best results across all metrics, demonstrating robust and accurate real-to-sim simulation. Task Method Resimulation Generalization Abs Rel RMSE PSNR SSIM LPIPS Abs Rel RMSE PSNR SSIM LPIPS PhysTwin (Jiang et al., 2025a) GausSim (Shao et al., 2024) SoMA (ours) 0.102 0.115 0.089 0.150 0.155 0.124 28.77 31.69 33.51 0.947 0.945 0.971 0.086 0.092 0. 0.128 0.143 0.112 0.168 0.186 0.137 26.54 31.29 32.89 0.941 0.942 0.968 0.092 0.127 0.062 Table 2. Quantitative results on the T-shirt folding task under robot manipulation. Method Abs Rel RMSE PSNR SSIM LPIPS PhysTwin SoMA 0.129 0. 0.208 0.172 22.85 27.57 0.842 0.896 0.198 0.128 coherent geometry and realistic dynamics. In contrast, PhysTwin exhibits pronounced artifacts and inconsistent deformations under the same setting, failing to preserve valid object structure throughout the rollout. Quantitatively, our method consistently outperforms baselines across all metrics, reflecting both improved stability and accuracy. These results demonstrate that SoMA can handle task-level soft-body manipulation with complex interactions, highlighting its potential as practical simulation tool for embodied manipulation scenarios. Ablation studies. We conduct ablation studies on the cloth domain using PSNR as the evaluation metric, as summarized in Tab. 3. We compare the full model with three variants: Jointly, which is trained on all object domains; Img-only, which removes the blended supervision loss (Sec. 4.4) and uses raw image supervision only; and w/o MRT, which disables the multi-resolution training strategy (Sec. 4.3). The full model achieves the best overall performance. Joint training slightly reduces resimulation accuracy but improves generalization performance, indicating its potential for learning more robust dynamics from diverse data and scaling to larger manipulation datasets. Removing multi-resolution training leads to consistent drop in performance, confirming its role in stabilizing long-horizon simulation. Notably, the Img-only variant suffers the largest degradation, especially in generalization, highlighting the importance of the blended supervision with masked image loss and physical constraints for learning transferable interaction dynamics. 6. Applications SoMA enables forward simulation of deformable objects under rich robotobject interactions, providing practical virTable 3. PSNR Ablation on cloth dataset (150-frame sequences). We evaluate SoMA with different components: blended supervision (Sec. 4.4), image-only supervision (Img-only), and without multi-resolution training (w/o MRT, Sec. 4.3). Jointly: training on all domains to test generalization. Full Model Jointly Img-Only w/o MRF Resim Genral 32.73 31.49 32.59 31. 30.29 29.17 31.97 30.29 tual environment for analyzing complex soft-body behaviors. Its stable long-horizon simulation and direct robot-action conditioning support prediction of object dynamics under different manipulation strategies. As real-to-sim backend with reduced reality gap, SoMA facilitates simulationdriven robot learning and improves policy transfer to realworld manipulation. For example, it can stably simulate long-horizon, self-contactheavy tasks such as T-shirt folding, enabling task-level analysis and policy development. 7. Conclusion We presented SoMA, neural real-to-sim simulator for softbody manipulation in robot interaction scenes. Operating directly on Gaussian splat representations, our approach learns deformable object dynamics end-to-end from RGB observations without relying on predefined physical models. By explicitly modeling robotobject interactions, the proposed simulator enables accurate and stable long-horizon simulation conditioned on robot actions. Experiments demonstrate strong generalization to unseen interactions and consistent improvements over prior methods across diverse objects and complex manipulation tasks such as cloth folding. Limitation and broader impact. The performance of SoMA depends on the quality of visual reconstruction and may degrade under severe occlusion or highly complex contact patterns beyond the training distribution. SoMA has the potential to reduce the cost of collecting large-scale robot manipulation data by enabling simulation-driven analysis and learning. We emphasize that simulation-based models should be applied with caution, as real-world deployment still requires verification to ensure safety and reliability. 8 Submission and Formatting Instructions for ICML"
        },
        {
            "title": "References",
            "content": "Bahmani, S., Liu, X., Yifan, W., Skorokhodov, I., Rong, V., Liu, Z., Liu, X., Park, J. J., Tulyakov, S., Wetzstein, G., et al. Tc4d: Trajectory-conditioned text-to-4d generation. In European Conference on Computer Vision, pp. 5372. Springer, 2024a. Bahmani, S., Skorokhodov, I., Siarohin, A., Menapace, W., Qian, G., Vasilkovsky, M., Lee, H.-Y., Wang, C., Zou, J., Tagliasacchi, A., et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024b. Belytschko, T., Liu, W. K., Moran, B., and Elkhodary, K. Nonlinear finite elements for continua and structures. John wiley & sons, 2014. Borycki, P., Smolak, W., Waczynska, J., Mazur, M., Tadeja, S., and Spurek, P. Gasp: Gaussian splatting for physicbased simulations. arXiv preprint arXiv:2409.05819, 2024. Cadena, C., Carlone, L., Carrillo, H., Latif, Y., Scaramuzza, D., Neira, J., Reid, I., and Leonard, J. J. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. IEEE Transactions on robotics, 32(6):13091332, 2017. Campos, C., Elvira, R., Rodrıguez, J. J. G., Montiel, J. M., and Tardos, J. D. Orb-slam3: An accurate open-source library for visual, visualinertial, and multimap slam. IEEE transactions on robotics, 37(6):18741890, 2021. Feng, Y., Feng, X., Shang, Y., Jiang, Y., Yu, C., Zong, Z., Shao, T., Wu, H., Zhou, K., Jiang, C., et al. Gaussian splashing: Dynamic fluid synthesis with gaussian splatting. CoRR, 2024. Ganapathi, A., Sundaresan, P., Thananjeyan, B., Balakrishna, A., Seita, D., Grannen, J., Hwang, M., Hoque, R., Gonzalez, J. E., Jamali, N., et al. Learning dense visual correspondences in simulation to smooth and fold real fabrics. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 1151511522. IEEE, 2021. Gingold, R. A. and Monaghan, J. J. Smoothed particle hydrodynamics: theory and application to non-spherical stars. Monthly notices of the royal astronomical society, 181(3):375389, 1977. Grisetti, G., Kummerle, R., Stachniss, C., and Burgard, IEEE Intelligent W. tutorial on graph-based slam. Transportation Systems Magazine, 2(4):3143, 2011. Hu, Y., Anderson, L., Li, T.-M., Sun, Q., Carr, N., RaganKelley, J., and Durand, F. Difftaichi: Differentiable programming for physical simulation. arXiv preprint arXiv:1910.00935, 2019. Huang, W., Chao, Y.-W., Mousavian, A., Liu, M.-Y., Fox, D., Mo, K., and Fei-Fei, L. Pointworld: Scaling 3d world models for in-the-wild robotic manipulation. arXiv preprint arXiv:2601.03782, 2026. Jiang, C., Schroeder, C., Teran, J., Stomakhin, A., and Selle, A. The material point method for simulating continuum materials. In Acm siggraph 2016 courses, pp. 152. 2016. Jiang, H., Hsu, H.-Y., Zhang, K., Yu, H.-N., Wang, S., and Li, Y. Phystwin: Physics-informed reconstruction and simulation of deformable objects from videos. arXiv preprint arXiv:2503.17973, 2025a. Jiang, L., Mao, Y., Xu, L., Lu, T., Ren, K., Jin, Y., Xu, X., Yu, M., Pang, J., Zhao, F., et al. Anysplat: Feed-forward 3d gaussian splatting from unconstrained views. ACM Transactions on Graphics (TOG), 44(6):116, 2025b. Kerbl, B., Kopanas, G., Leimkuhler, T., and Drettakis, G. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Li, Z., Yu, H.-X., Liu, W., Yang, Y., Herrmann, C., Wetzstein, G., and Wu, J. Wonderplay: Dynamic 3d scene arXiv generation from single image and actions. preprint arXiv:2505.18151, 2025. Ling, H., Kim, S. W., Torralba, A., Fidler, S., and Kreis, K. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 85768588, 2024. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Jiang, Q., Li, C., Yang, J., Su, H., et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pp. 3855. Springer, 2024. Liu, Y., Min, Z., Wang, Z., Wu, J., Wang, T., Yuan, Y., Luo, Y., and Guo, C. Worldmirror: Universal 3d world reconstruction with any-prior prompting. arXiv preprint arXiv:2510.10726, 2025. Macklin, M. Warp: high-performance python framework for gpu simulation and graphics. In NVIDIA GPU Technology Conference (GTC), volume 3, 2022. Mo, K., Xia, C., Wang, X., Deng, Y., Gao, X., and Liang, B. Foldsformer: Learning sequential multi-step cloth manipulation with space-time attention. IEEE Robotics and Automation Letters, 8(2):760767, 2022. Muller, M., Heidelberger, B., Hennix, M., and Ratcliff, J. Position based dynamics. Journal of Visual Communication and Image Representation, 18(2):109118, 2007. 9 Submission and Formatting Instructions for ICML 2026 NVIDIA. Nvidia isaac sim: robot simulation frameAvailable at robotics research, 2023. work for https://developer.nvidia.com/isaac-sim. Wang, Z. Image quality assessment: Form error visibility to structural similarity. IEEE Trans. Image Process., 13 (4):604606, 2004. Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Radle, R., Rolland, C., Gustafson, L., et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. Yin, Y., Xu, D., Wang, Z., Zhao, Y., and Wei, Y. 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023. Yu, H.-X., Duan, H., Herrmann, C., Freeman, W. T., and Wu, J. Wonderworld: Interactive 3d scene generation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 59165926, 2025. Zhang, M., Zhang, K., and Li, Y. Dynamic 3d gaussian tracking for graph-based neural dynamics modeling. arXiv preprint arXiv:2410.18912, 2024a. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Zhang, T., Yu, H.-X., Wu, R., Feng, B. Y., Zheng, C., Snavely, N., Wu, J., and Freeman, W. T. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pp. 388406. Springer, 2024b. Zheng, Y., Yao, L., Su, Y., Zhang, Y., Wang, Y., Zhao, S., Zhang, Y., and Chau, L.-P. survey of embodied learning for object-centric robotic manipulation. Machine Intelligence Research, pp. 139, 2025. Zhong, L., Yu, H.-X., Wu, J., and Li, Y. Reconstruction and simulation of elastic objects with spring-mass 3d gaussians. In European Conference on Computer Vision, pp. 407423. Springer, 2024. Reddy, J. N. An introduction to the finite element method. New York, 27(14), 1993. Ren, J., Pan, L., Tang, J., Zhang, C., Cao, A., Zeng, G., and Liu, Z. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Schonberger, J. L. and Frahm, J.-M. Structure-from-motion In Proceedings of the IEEE conference on revisited. computer vision and pattern recognition, pp. 41044113, 2016. Shao, Y., Huang, M., Loy, C. C., and Dai, B. Gaussim: Foreseeing reality by gaussian simulator for elastic objects. arXiv preprint arXiv:2412.17804, 2024. Shen, L., Li, X., Sun, H., Peng, J., Xian, K., Cao, Z., and Lin, G. Make-it-4d: Synthesizing consistent long-term dynamic scene video from single image. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 81678175, 2023. Singer, U., Sheynin, S., Polyak, A., Ashual, O., Makarov, I., Kokkinos, F., Goyal, N., Vedaldi, A., Parikh, D., Johnson, J., et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. Sulsky, D., Chen, Z., and Schreyer, H. L. particle method for history-dependent materials. Computer methods in applied mechanics and engineering, 118(1-2):179196, 1994. Wang, J., Chen, M., Karaev, N., Vedaldi, A., Rupprecht, C., and Novotny, D. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025a. Wang, Y., Zhou, J., Zhu, H., Chang, W., Zhou, Y., Li, Z., Chen, J., Pang, J., Shen, C., and He, T. Pi3: Permutationequivariant visual geometry learning. arXiv preprint arXiv:2507.13347, 2025b. 10 Submission and Formatting Instructions for ICML 2026 A. Discussion A.1. Distinguishing Different Technical Paradigms Recent advances in dynamic scene modeling and simulation have led to increasingly similar visual results across different methods, which may obscure their fundamental differences. Despite comparable visual quality, these approaches are motivated by distinct problem settings and adopt different design trade-offs. Passive Reconstruction vs. Interaction-Aware Simulation. 4D reconstruction methods (Bahmani et al., 2024a;b; Borycki et al., 2024) focus on recovering temporally consistent geometry and appearance from visual observations, without explicitly modeling physical states or interactions. Consequently, they are not suitable for controllable manipulation or physical reasoning. World Models vs. Physics-Oriented Simulation. World models (Huang et al., 2026; Li et al., 2025; Yu et al., 2025) are typically trained on large-scale video or interaction data and excel at predicting perceptually plausible future observations. However, they are optimized for appearance realism rather than physical or geometric consistency, which can lead to view inconsistency and physically implausible behaviors under external interventions. These characteristics limit their applicability to tasks requiring precise physical reasoning or controllable interaction. Rule-Based Simulation vs. Data-Driven Dynamics. Rule-based simulators (Jiang et al., 2025a; Zhang et al., 2024b) rely on predefined physical formulations and parameters, including simplified object properties for differentiability and simulator-specific parameters that require manual tuning. Such design choices constrain their expressiveness and adaptability in real-world settings. In contrast, data-driven neural simulators encode physical properties using latent embeddings and learn interaction dynamics directly from data, enabling better adaptability across diverse objects and scenes without explicit parameter reconfiguration. Position of SoMA. SoMA is designed for robot manipulation under real-to-sim settings. By explicitly representing physical states while learning interaction-aware dynamics from data, it occupies distinct point in the design space that balances physical consistency, controllability, and fidelity to real-world observations. Table 4. Comparison of different technical paradigms for dynamic scene modeling and simulation. 4D Recon. World Model Rule-based Sim. Neural Sim. SoMA Explicit Physical State Control-aware Dynamics Source of Consistency View Consistency Real-to-Sim Applicability Geometry Data Rules Data Data + Interaction A.2. Limitations and Future Directions While our experiments demonstrate that SoMA can generalize across multiple object categories and diverse manipulation actions, the current evaluation is still limited in scale. Specifically, our datasets include four object categories with range of interaction types, and generalization is evaluated on unseen actions within this scope. Although these results are encouraging, broader validation is required to assess scalability and robustness under more diverse object geometries, materials, and interaction patterns. natural future direction is to extend the training and evaluation to significantly larger and more diverse datasets, potentially covering hundreds of object instances and wider spectrum of manipulation behaviors. Such large-scale studies would help better characterize the limits of data-driven interaction-aware simulation and examine whether generalization continues to improve with increased data diversity. 11 Submission and Formatting Instructions for ICML 2026 B. Implementation Details B.1. Dataset Collection & Preprocessing We collect real-world manipulation data using the arx lift platform with three Intel RealSense D405 RGB cameras. The data collection setup is illustrated in Fig. 1. One camera is rigidly mounted on the robot end-effector and serves as key component for robot-conditioned real-to-sim mapping, while the other two cameras are placed around the tabletop to provide multi-view observations. All RGB images and robot joint states are synchronously recorded at 30 FPS. Image Processing. All captured images are segmented using GroundingDINO (Liu et al., 2024) and Grounded-SAM2 (Ravi et al., 2024) with text prompts. We generate two types of masks: object masks, which provide the primary visual observations for end-to-end supervision, and robot masks, which are used to identify occluded regions caused by the manipulator during interaction. Robot State Processing. Given the robot joint states, we compute the end-effector pose and gripper opening parameters in the robot base frame using the provided URDF model. These quantities are later mapped into unified simulation coordinate system via the robot-conditioned real-to-sim transformation. Camera Pose Estimation and Reconstruction. To establish the reconstruction coordinate system, we select frames with rich visual texture as references and apply segmentation and super-resolution preprocessing (Rombach et al., 2022). Camera poses and an initial point cloud are reconstructed using Pi3 (Wang et al., 2025b). The resulting point cloud is further optimized and converted into 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) representation for simulation and rendering. For the T-shirt folding scenes, we initialize all Gaussian splat colors to blue to ensure consistent appearance for splats corresponding to the back side of the cloth, which may be weakly observed or fully occluded. Transformation of Robot Conditioning. As defined in Eq. 6, the robot-to-simulation mapping is determined by scale factor and rigid transformation (R, t). The scale is computed by matching the physical size of reference objects across coordinate systems. The rigid transformation is resolved via the mounted camera, whose camera-to-world pose is known in both the robot frame and the reconstruction frame. Specifically, let Trob cam denote the camera-to-world transformation in the robot frame obtained from the URDF, and Trec cam the corresponding pose in the reconstruction frame estimated by Pi3. The robot-to-simulation transformation is then given by Trobsim = (cid:21) (cid:20) 0 1 = Trec cam (cid:0)Trob cam (cid:1) . (14) B.2. Model Implementation Implementation Efficiency. All experiments are conducted on NVIDIA H200 GPUs. We use four GPUs for training and single GPU for inference. The first-stage training takes approximately 24 hours, and the model runs at about 12 frames per second during inference. Hierarchical Clustering. We organize Gaussian splats into fixed three-level hierarchy (including the finest splat level) across all datasets. The hierarchy is constructed once using distance-based clustering and remains unchanged during training and inference. To satisfy the input requirements of the hierarchical network, we adopt fixed clustering scheme with approximately [n, n/2, 2] nodes from fine to coarse levels, where denotes the number of Gaussian splats. The number of control points is fixed to 30. Detailed clustering parameters for each dataset are provided in Tab. 5. Table 5. Quantitative results on the cloth folding task under robot manipulation. downsample rate num of splats num of clusters rope [0.02, 0.2] 13000 [8, 800] cloth [0.02, 0.2] 13000 [30, 2400] doll [0.02, 0.2] 13000 [22, 2200] T-shirt [0.02, 0.2] 13000 [60, 3000] Submission and Formatting Instructions for ICML 2026 Network Architecture. Our simulator backbone is hierarchical mesh graph network. Each hierarchy level employs mesh graph network with shared parameters, enabling consistent multi-scale message passing. All graph networks use an embedding dimension of 128 and consist of 16 encoder layers with SiLU activations and normalization applied at each layer. The propagated splat features are decoded by lightweight MLP head to predict per-splat state updates, including velocity, deformation, and rotation. Multi-Temporal Training. To improve training stability and long-horizon rollout performance, we adopt multi-resolution temporal training strategy. Instead of supervising dynamics at the original frame rate throughout training, we first train the simulator using temporally subsampled sequences with an interval of frames. Specifically, for the T-shirt folding dataset we use = 5, as this task involves larger deformations and more complex motions, requiring finer temporal resolution during coarse training. For all other datasets, we use = 10. This coarse temporal supervision encourages the model to capture global motion patterns and interaction effects over longer time spans. After the initial stage, the model is fine-tuned using full-resolution sequences with the original frame interval. An ablation study on the choice of is provided in Tab. 6, showing that the performance is relatively insensitive to the exact value of within reasonable range. Image Super-Resolution. To obtain higher-quality Gaussian splat reconstructions, we apply 4 image super-resolution preprocessing to selected frames before segmentation and reconstruction. The enhanced image resolution provides sharper object boundaries and more accurate masks, which in turn improves the quality of the reconstructed point clouds and the resulting 3D Gaussian splats. Table 6. Ablation Study on cloth lift. PSNR RMSE 5 31.87 0.120 10 32.73 0.126 15 32.57 0. 20 32.53 0.123 30 32.10 0.122 B.3. Baselines Implementation PhysTwin. For PhysTwin (Jiang et al., 2025a), the objects in our datasets are similar to those used in the original PhysTwin benchmark. We therefore follow the vanilla PhysTwin setting whenever applicable. To enable robot manipulation, we represent the robot end-effector as circular control point with fixed radius, which serves as the interaction primitive for grasping and contact. This modification only adapts the control abstraction and does not alter the underlying physical model of PhysTwin. GausSim. GausSim (Shao et al., 2024) does not support explicit control or action inputs and instead predicts future Gaussian splat states in purely state-based autoregressive manner. The model estimates the previous state GSt1 from the initial splats and regresses the full sequence accordingly. For the resimulation setting, we train GausSim on full sequences following its original protocol. For the generalization setting, we provide GS0 and GS1 as initial conditions and directly evaluate its rollout performance without additional control inputs. B.4. Evaluation Metrics Image Metrics. We evaluate rendered image quality using PSNR, SSIM, and LPIPS (Wang et al., 2004; Wang, 2004; Zhang et al., 2018). All image metrics are computed between the predicted images and the ground-truth RGB images within the object regions only. Specifically, object masks are applied to exclude background pixels and occluded regions, ensuring that the evaluation focuses on the visible object appearance. Depth Metrics. To assess geometric accuracy without full 3D ground truth, we report depth-based metrics including Absolute Relative Error (Abs Rel) and RMSE. Depth metrics are computed on valid object regions. For masked-out pixels corresponding to the background or occluded areas, we assign the depth value of the supporting tabletop to maintain consistent depth maps and avoid undefined values during metric computation. 13 Submission and Formatting Instructions for ICML Figure 4. More Reults (a) multi-view results; (b) T-shirt folding comparison results (c) results on phystwin datatests. 14 Submission and Formatting Instructions for ICML 2026 C. More Results C.1. Multi-view Results Multi-view qualitative results are shown in Fig. 4(a). Our method maintains consistent visual accuracy and physically plausible dynamics across both the main view and the side view, demonstrating robustness to viewpoint changes. C.2. T-shirt Folding Comparison Results Qualitative comparison results for the T-shirt folding task are shown in Fig. 4(b). Our method accurately simulates the folding process, while PhysTwin exhibits noticeable deviations from the ground-truth motion. C.3. Results on the Datasets of PhysTwin To evaluate SoMA under different experimental setting, we apply it to the PhysTwin datasets(Jiang et al., 2025a). PhysTwin provides 3D-tracked hand trajectories as control signals instead of explicit robot actions; we therefore treat these trajectories as robot end-effector motions and assume closed gripper during interaction. Qualitative results are shown in Fig. 4(c). Our method produces dynamics that are visually and physically consistent with the ground-truth sequences under this setting. Quantitative results are reported in Tab. 7, where SoMA consistently outperforms baseline methods across all evaluated metrics. These results demonstrate that our simulator generalizes beyond robot-action-driven settings and can effectively model interaction dynamics even when driven by externally provided motion trajectories. Table 7. Quantitative results on the Datasets of Phystwin. PhysTwin (avg) SoMA (avg) SoMA (rope) SoMA (cloth) SoMA (doll) PSNR SSIM LPIPS 28.214 0.945 0.034 32.640 0.981 0.026 34.261 0.988 0.018 32.739 0.981 0.032 30.920 0.973 0."
        }
    ],
    "affiliations": [
        "Fudan University, China",
        "Shanghai Artificial Intelligence Laboratory, China",
        "Shanghai Jiao Tong University, China",
        "The Chinese University of Hong Kong, China",
        "The University of Hong Kong, China"
    ]
}