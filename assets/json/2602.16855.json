{
    "paper_title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents",
    "authors": [
        "Haiyang Xu",
        "Xi Zhang",
        "Haowei Liu",
        "Junyang Wang",
        "Zhaozai Zhu",
        "Shengjie Zhou",
        "Xuhao Hu",
        "Feiyu Gao",
        "Junjie Cao",
        "Zihua Wang",
        "Zhiyuan Chen",
        "Jitong Liao",
        "Qi Zheng",
        "Jiahui Zeng",
        "Ze Xu",
        "Shuai Bai",
        "Junyang Lin",
        "Jingren Zhou",
        "Ming Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent."
        },
        {
            "title": "Start",
            "content": "February 20, 2026 Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents Haiyang Xu Xi Zhang Haowei Liu Zhou Qi Zheng Xuhao Hu Feiyu Gao Jiahui Zeng Ze Xu Junjie Cao Shuai Bai Junyang Wang Zihua Wang Junyang Lin Zhaoqing Zhu Zhiyuan Chen Jingren Zhou Shengjie Jitong Liao Ming Yan Tongyi Lab , Alibaba Group {shuofeng.xhy, ym11960}@alibaba-inc.com https://github.com/X-PLUG/MobileAgent"
        },
        {
            "title": "Abstract",
            "content": "The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use unified thought-synthesis pipeline to enhance the models reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent. 6 2 0 2 5 1 ] . [ 1 5 5 8 6 1 . 2 0 6 2 : r Figure 1: Performance overview on mainstream GUI task automation, grounding and knowledge benchmarks. Core Contributors Corresponding author and project leader 1 Figure 2: Overview of our Mobile-Agent-v3.5. We illustrate our multi-platform environment supporting and our highlight capability."
        },
        {
            "title": "Introduction",
            "content": "With the rapid development of Visionlanguage models (VLMs) (Bai et al., 2025a; Anthropic, 2025a; OpenAI, 2025c; DeepMind, 2025),multimodal agents (Wang et al., 2024b; Qin et al., 2025; Ye et al., 2025; Liu et al., 2024; Zhou et al., 2025; Wang et al., 2025a)have achieved substantive progress, especially Graphical user interface (GUI) agents. GUI Agents are mainly designed to perform automated operations across multiple devices, such as desktops, mobiles, browsers, and so on. Recently, native agent models (Ye et al., 2025; Qin et al., 2025; Seed, 2025d) based on end-to-end learning have demonstrated great potential, rather than only building agent frameworks on top of closed-source models (Wang et al., 2024a; 2025b; Agashe et al., 2025; Zhang et al., 2025). However,the development of robust and practically usable GUI agents still faces several challenges. (1) The efficiency of real-world data collection: Collecting large-scale trajectories is costly to hamper the scalability of GUI datasets, as it requires complex agentic workflows, manual annotation, and engineering-level handling of anomalous scenarios; (2) The adaptation to multiple platforms: The native agent model needs to perform automated tasks reliably across wide range of devices, including desktops, mobiles, browsers, and in-vehicle It should also support complex agentic real-time interactions, such as edgecloud collaboration and systems. coordination across multiple devices; (3) The comprehensive agentic capabilities: The General GUI Agent should be capable of completing tasks efficiently, not limited to GUI-only operations. It should also support tool/Model Context Protocol (MCP) invocation, short-term and long-term memory, multi-agent adaptation, and humanagent interaction. To address these challenges, we propose GUI-Owl1.5, our latest native GUI agent model for multi-platform GUI automation across desktops, mobiles, browsers, and more. Built on Qwen3-VL and powered by scalable data pipeline and multi-stage training paradigm, GUI-Owl1.5 comprises family of foundation GUI models covering full range of sizes, including instruct/thinking variants at 2B, 4B, 8B, 32B, and 235B-A22B. Smaller instruct models, which do not produce thoughts, enable faster inference and can be deployed on edge devices to support high-frequency, real-time interactions while addressing security and privacy concerns. Larger thinking models, with stronger capabilities in task planning and reflection, are better suited for complex tasks and can collaborate with edge-deployed instruct models in multi-agent setup to enable edgecloud collaboration and multi-platform coordination. The key technical points are highlighted next. 2 Hybird Data Flywheel: We develop the data pipeline for UI understanding and trajectory generation by synergistically integrating simulated environments with cloud-based platform environments, thereby enhancing both the efficiency and quality of data collection. For Grounding: comprehensive grounding data augmentation pipeline that encompasses both hard grounding data generationincluding challenging app GUI synthesis and multi-window high-resolution scenariosand scalable high-quality data extension through trajectory mining, tutorial knowledge extraction, and infeasible query generation. For trajectory, we build self-evolving trajectory synthesis workflow based on directed acyclic graph (DAG) (Ye et al., 2025). Meanwhile, we synthesize virtual environments via Vibe Coding to create high-frequency, complex atomic operations and apps featuring challenging cases such as pop-ups and CAPTCHA-style verifications. In addition, for some challenging apps and scenarios, we incorporate small amount of manual annotation to better align synthetic environments with real-world ones. Unified Enhancement of Agent Capabilities: Beyond basic GUI perception and action execution, practical GUI agent must possess range of higher-order skills. We introduce three complementary strategies to comprehensively enhance the native models agent capabilities. First, we inject GUI knowledge through large-scale QA data crawled from software documentation and forums, and train the model with world modeling supervision to anticipate interface state transitions before acting. Second, we design unified chain-of-thought (CoT) synthesis pipeline that augments all trajectory data with step-wise observation, reflection, memory management, and tool invocation reasoning, enabling superior long-horizon planning and in-context information retention. Third, we incorporate multi-agent collaboration data collected via the Mobile-Agent-v3.5 framework, allowing the model to function not only as standalone end-to-end agent but also as specialized roles (e.g., planner, executor, verifier) within structured multi-agent systems. Multi-platform Environment RL Scaling: To enable stable reinforcement learning training across multi-platform environments, we propose MRPO (Multi-platform Reinforcement Policy Optimization), large-scale RL framework that addresses four critical challenges in GUI agent training. First, we unify learning across mobile, desktop, and web environments under single device-conditioned policy. Second, we introduce an online rollout buffer that mitigates GRPO training instability when grouped rollouts collapse to identical outcomes by oversampling trajectories and strategically selecting diverse groups while maintaining on-policy guarantees. Third, we ensure consistency between environment-side inference and training-side optimization through token-ID transport, preventing tokenization mismatches. Finally, we adopt alternating multi-platform optimization to reduce gradient interference, training on single device types cyclically rather than mixing trajectories. This approach enables stable, unified policy learning while preserving cross-device generalization for long-horizon GUI control tasks. We evaluate GUI-Owl-1.5 on series of benchmarks spanning GUI task automation, grounding, tool invocation, memory and knowledge. Experimental results demonstrate that GUI-Owl-1.5 exhibits strong GUI understanding, grounding and execution capabilities, achieving state-of-the-art performance among open-source models across more than 20 GUI benchmarks. Specifically, it attains task success rates of 56.5%, 71.6% and 46.6% on OSWorldVerified, AndroidWorld and VisualWebArena respectively, which outperforms models such as UI-TARS-2, Claude4, and Gemini -2.5-Pro. On OSWorld-MCP, which evaluates the integration of GUI operations and tool invocation, it achieves task success rate of 47.6%. On the ScreenSpot-Pro grounding benchmark, it achieves state-of-the-art accuracy of 80.3% with crop-based refinement, and notably surpasses the large-scale Gemini-3-Pro even in its base configuration (72.9%) without crop tool. On MemGUI-Bench and GUI Knowledge Bench, our model also surpasses previous open-source models."
        },
        {
            "title": "2 Mobile-Agent-v3.5",
            "content": "GUI-Owl-1.5 is multimodal model for GUI operations, building on the previous GUI-Owl (Ye et al., 2025). Compared to its predecessor, it offers three main improvements: (1) broader action space; (2) improved context retention; (3)enhanced design in synthetic data generation, cross-platform adaptation, and agent capabilities. Building on Qwen3-VL and trained with extensive post-training datasets, GUI-Owl-1.5 maintains the core functions of the original modelperceiving, planning, decision-making, and locating elements in GUI scenarioswhile further optimizing them for different real-world cases. The model can autonomously interact with mobile, desktop, and browser interfaces across multiple turns, and can also work collaboratively in multi-agent systems. 2.1 Formulation We formulate the GUI agent task as multi-turn interactive decision-making problem, where the agent continuously perceives the environment, executes actions, and adapts its strategy based on real-time feedback. Input Space. At each interaction step t, the agent receives: Visual observation It RHW 3: screenshot capturing the current GUI state. User instruction Lt: natural language command expressing the users intent. 3 Figure 3: Illustration of the interaction flow of GUI-Owl-1.5. The system message defines the available action space, the user message contains the task instruction, compressed histories, and current observation, while the response message includes the agents reasoning, action summaries, and the final action output. Output Space. Given the input, the agent generates: Action conclusion Ct: natural language explanation summarizing the planned action. Tool call At: structured function call that executes the action. The nature of GUI agent tasks requires closed-loop interaction with the environment. Specifically, after executing At, the environment transitions to new state, providing updated visual feedback It+1 for the next turn. This iterative process continues until the task is completed or terminated. Notably, compared to the previous GUI-Owl, we significantly expand the action space to support external tool calls and API invocations in addition to primitive GUI operations (e.g., click, type, scroll). This extension enables the agent to orchestrate complex workflows across heterogeneous systems, such as querying databases through APIs, invoking specialized computational tools, and integrating with third-party services. Context Management. To handle long-horizon tasks while maintaining computational efficiency, we adopt sliding window mechanism with hierarchical context compression. The context at step is organized as: Recent context (full retention): The most recent complete dialogue turns, including all modalities: {(ItN , LtN , CtN , AtN ), . . . , (It1, Lt1, Ct1, At1)} Historical context (compressed summary): Earlier interactions beyond the -turn window are condensed into textual summary S1:tN 1, formed by concatenating action conclusions: S1:tN 1 = concat(C1, C2, . . . , CtN 1) This hierarchical design preserves fine-grained multi-modal information for immediate decision-making while maintaining high-level awareness of long-term task progression, effectively balancing context richness with memory efficiency. 2.2 Data Preparation To support training of GUI-Owl-1.5 across heterogeneous platforms and task families, we develop unified data preparation pipeline that targets both actionable interaction supervision and fine-grained visual grounding. Specifically, we curate (i) trajectory data that captures long-horizon decision-making and tool-augmented execution in realistic GUI environments, and (ii) grounding data that aligns natural-language intents with on-screen elements. 4 Figure 4: Overview of our high-quality grounding data construction pipeline. 2.2.1 Grounding Existing grounding datasets exhibit limited complexity and diversity, creating critical shortage of high-quality, challenging grounding data alongside scalable data augmentation solutions. As illustrated in Figure 7, we address these limitations through comprehensive data augmentation framework that enhances GUI grounding capabilities via two complementary strategies. For Hard Grounding Data Generation, which targets complex scenarios requiring specialized domain knowledge and high annotation costs, we develop two synthesis approaches: Challenging App GUI Grounding Data Synthesis: We leverage annotated UI elements and reference interfaces to generate diverse, high-quality professional application screenshots using MLLMs. This process incorporates iterative quality assessment and refinement mechanisms, where generated interfaces undergo validation checks and corrective regeneration to ensure data fidelity and domain accuracy. Multi-window High-resolution Grounding Data Synthesis: Utilizing existing single-window datasets combined with candidate organization pools (encompassing window count variations, layout configurations, and resolution options), we generate complex multi-window scenarios while ensuring target UI elements remain unoccluded through spatial constraint validation. For High-Quality Grounding Data Extension, which aims to achieve cost-effective and scalable data augmentation, we implement three synergistic enhancement pathways: Trajectory-based grounding extraction: We mine grounding annotations from existing PC and mobile simulation environment trajectories, employing critic models to filter and validate data quality, ensuring only high-fidelity grounding pairs are retained. Tutorial-based knowledge mining: Application tutorials are parsed to extract grounding-related questionanswer knowledge by analyzing embedded subtitles and identifying spatial-semantic relationships, ultimately generating comprehensive grounding-oriented QA pairs that capture real-world usage patterns. Infeasible query generation: To address the critical gap in handling infeasible grounding queries within existing datasets, we generate large-scale negative samples through strategic random pairing of queries and interface elements, followed by multi-model consensus filtering to identify and validate truly infeasible grounding instances. 2.2.2 Trajectory Data Collection We build hybrid trajectory corpus that scales to diverse applications and devices while maintaining high supervision fidelity. The pipeline consists of (i) DAG-based task synthesis to cover frequent workflows, (ii) automated rollouts on real devices with DAG-based validation, (iii) human demonstrations for tasks that remain unsolved by automation, (iv) trajectory production via virtual environments for basic actions (e.g., scroll, drag) and highfrequency challenging scenarios. 5 Figure 5: Overview of our trajectory collection pipeline. Task production via human-authored DAGs. For each application domain, annotators construct directed acyclic graph (DAG) = {vi}V where each node vi denotes an atomic subtask and each edge (vi, vj) denotes feasible transition under typical UI state evolution. Let and be the sets of valid start and terminal nodes separately, we synthesize task by sampling path from to : i=1, V, = (V, E), = (v1, . . . , vK), v1 S, vK T, (vk, vk+1) E, which represents realistic action sequence with multiple steps. Each node vk is associated with sub-instruction template d(vk) that optionally has slots for diverse entities. The final task instruction is composed by concatenating and rewriting the ordered sub-instructions: I(p) = Compose (cid:0)d(v1), d(v2), . . . , d(vK)(cid:1). By sampling diverse paths and instantiating templates, the DAG provides controllable coverage of high-frequency operation patterns in common apps, minimizing the impact of the LLM hallucination. Automated trajectory generation with checkpointing, truncation, and task repair. Given I(p), an agent interacts with real device environment to produce trajectory t=1, τ = {(ot, at)}T where ot is the observation (e.g., screenshot, UI structure, device metadata) and at is the executed action (touch/keyboard/tool call). To assess partial completion along the subtask path p, we define checkpoint predicate for each node vk: ϕk : {0, 1}, ϕk(ot) = 1 iff subtask vk is satisfied at ot. We compute whether subtask is achieved somewhere in the rollout by The longest completed prefix length is ck(τ ) = max t[1,T ] ϕk(ot). m(τ ) = max {m {0, . . . , K} : m, ck(τ ) = 1} . If m(τ ) = K, we accept τ as correct trajectory. Otherwise, we truncate the rollout to the last verified checkpoint of the completed prefix: and repair the original task by removing the completed subtasks: = max{t : ϕm(τ )(ot) = 1}, τ = {(ot, at)}t t=1, We then store (Irem, τ ) as partially-correct instance, which provides clean supervision for the successfully executed segment while avoiding noisy labels beyond the last correct subtask. prem = (vm(τ )+1, . . . , vK), Irem = I(prem). 6 Human annotation on real devices. For difficult tasks that remain unsolved after repeated automated attempts, we collect expert demonstrations via cloud annotation platform. Annotators directly operate the same real device environments and record gold trajectories τ human aligned with the task instruction, ensuring high-quality supervision for hard cases. Virtual environment-based trajectory production. Relying solely on agent exploration in real-world environments for trajectory generation presents two notable limitations: (i) Real-world applications and software often incorporate CAPTCHA verification, anti-bot mechanisms, and other protective measures that can interrupt or terminate the agents exploration process. (ii) Real-world environments cannot provide accurate feedback, which results in low efficiency of trajectory generation via agent exploration, and often yields trajectories that contain erroneous or redundant steps. To address these challenges, we develop suite of web-rendering-based virtual environments targeting fine-grained primitive actions (e.g., scroll, drag) and high-frequency difficult scenarios (e.g., document and spreadsheet editing, popular applications). These virtual environments serve two primary purposes: (i) providing precise sub-task-level feedback to guide agent exploration, and (ii) enabling automated and scalable trajectory generation through the integration of LLM-based instruction decomposition. Agent rollout + critic. Given sampled scenario ω and DAG path = (v1, . . . , vK), the agent produces simulated trajectory τ = {(ot, at)} . The simulator exposes subtask-completion predicates ϕk(st) {0, 1}, t=1 enabling an exact prefix-progress score: ck(τ ) = max ϕk(st), m(τ ) = max {m : m, ck(τ ) = 1} . We accept τ if m(τ ) = K; otherwise we truncate to the last verified checkpoint and keep clean partially-correct prefix for training: = max{t : ϕ m(τ )(st) = 1}, τ = τ1:t . Scalable Automated Trajectory Generation. Since our virtual environments are built upon web rendering, they inherently support the automated execution of atomic operations. For given virtual environment, such as virtual word document editor, we leverage an LLM in conjunction with the document content to decompose user instruction into sequence of atomic operations that the virtual environment can execute. These atomic operations are then fed into the virtual environment to produce corresponding precise operation trajectories. Also, for many concentrated scenarios, the canonical correct operation is known and can be standardized. We encode it as script/RPA policy ρ and directly execute: τ rpa = Rollout( E, ρ, ω, p), which yields high-quality successful trajectories at low cost. m(τ rpa) = K, 2.3 Agent Capability Enhancement Beyond grounding and GUI understanding, capable GUI agent must plan over long horizons, reason about action consequences, memorize key information, and invoke external tools. We introduce three complementary strategies to enhance these capabilities (Figure 6): (i) GUI Knowledge Injection, which enriches the models knowledge through QA data and world modeling; (ii) Unified CoT Synthesis, which augments trajectory data with step-wise reasoning, reflection, and memory; and (iii) Multi-Agent Collaboration, which enables the model to operate within structured multi-agent frameworks. 2.3.1 GUI Knowledge Injection QA & VQA. In addition to trajectory-format data, we further augment the agents GUI knowledge through data in other modalities. As illustrated in Figure 6, we crawl substantial volume of data from diverse sources on the Internet to construct knowledge base of GUI-related information, encompassing software feature configurations, operational instructions, website navigation, among others. The primary sources can be categorized into three types: (i) official documentation and tutorials of software applications (e.g., Microsoft Office, LibreOffice); (ii) software forums (e.g., WPS Academy) and Q&A platforms (e.g., Baidu Jingyan); and (iii) web navigation information extracted from existing open-source web datasets. After data cleaning, the crawled information is rewritten by LLMs into task-level QA or step-level VQA data, thereby enhancing the agents GUI knowledge. World Modeling. capable GUI agent should not only perceive the current screen state but also anticipate how the interface will change in response to its actions. To cultivate this predictive understanding, we construct world modeling data derived from trajectory recordings. Specifically, given screenshot and the action executed at that step, we prompt proprietary model (e.g., Claude-4.5) to produce fine-grained description of the subsequent screenshot, explicitly highlighting the state transitions. For example, newly appeared dialogs, changed text fields, shifted focus, or updated visual elements. These action-conditioned state-transition descriptions are then used as 7 Figure 6: Illustration of our agent capability enhancement pipeline. training supervision. Through this process, the model acquires an internalized understanding of GUI environment dynamics, enabling it to better reason about the consequences of candidate actions before execution, which in turn facilitates more informed decision-making in multi-step tasks. 2.3.2 Unified CoT Synthesis After obtaining trajectory data containing action sequences through various approaches (i.e., agent exploration, human annotation and virtual environments), we design chain-of-thought (CoT) synthesis pipeline to generate corresponding thoughts and conclusions for each step in the trajectory data, thereby enhancing the agents capabilities in screen observation, memory management, progress reflection, and tool invocation. As illustrated in figure 6, given the i-th step of trajectory, we first employ vision-language model (VLM) to describe the screen content and further extract query-relevant information from it. For queries that require memorizing key on-screen information, such as Check the weather in Paris and London for next Monday and record it in the memo, we extract information from the query-relevant content that may be needed in subsequent steps and incorporate it into the memory. Furthermore, we feed the action parameters executed at the i-th step, the screenshots captured before and after execution, and the user query into the VLM to determine whether the execution outcome of this step aligns with expectations. If the change in screen state is consistent with expectations, the progress of the current task is updated accordingly in the subsequent step; otherwise, corresponding reflections and error corrections are generated to inform the next action decision. The observation, memory, reflection and task progress information obtained above are then fed into an LLM to synthesize the thought and conclusion corresponding to the action at this step. Specifically, the thought simulates the agents reasoning process of integrating these pieces of information for action decision-making, while the conclusion provides concise action decision. Moreover, if the current trajectory involves tool invocation, the tool definitions from the tool set are also provided as input to the LLM, so that the synthesized thought incorporates reasoning about tool selection and invocation. The CoT synthesis pipeline designed above enables the model to: (i) reflect on the execution outcome of the previous action and analyze the overall task progress accordingly, thereby achieving superior long-horizon decision-making capability; and (ii) simultaneously record key on-screen information (e.g., prices, weather conditions) that may be required in subsequent steps during the operational process, thereby achieving enhanced memory capability. 2.3.3 Multi-agent Collaboration To enable the model to serve not only as an end-to-end agent but also as the components within multi-agent frameworks for multi-agent collaboration, we additionally employ the Mobile-Agent-v3.5 framework for agent exploration during the trajectory collection phase. Mobile-Agent-v3.5 agent framework largely follows MobileAgent-v3, and we only summarize the key components and interfaces here for completeness. The system instantiates small set of role-specialized modules and executes them in closed loop over device environment (mobile/desktop/web), with unified action abstraction and shared model backbone. 8 Problem setup. Given user instruction and the current device state St (e.g., screenshot, UI tree, device metadata), the goal is to produce an action at that drives the environment to St+1 ( St, at) until termination. Roles and state variables. We maintain four agent roles: Manager (planner), Worker (executor), Reflector (verifier), and Notetaker (memory). At step t, the system state is summarized by where SSt is the (ordered) subgoal list, Ft1 is the latest feedback, and Nt is persistent notes. Xt (I, St, SSt, Ft1, Nt), Manager: subgoal planning and update. The Manager decomposes the instruction into subgoals and dynamically updates them: where KRAG denotes optionally retrieved external knowledge. SS0 = fM (I, KRAG), SSt+1 = uM (SSt, Ft, St+1), Worker: action generation. Given the current context, the Worker selects subgoal and produces the next action (optionally as structured tuple with rationale and normalized action schema): at πW ( I, St, SSt, Ft1, Nt). Reflector: transition-level verification and feedback. After executing at on the device, the Reflector judges the transition and provides diagnostic feedback: (jt, ϕt) = fR(St, at, St+1), jt {SUCCESS, FAILURE}, and we set Ft (jt, ϕt). Notetaker: persistent memory update. Upon successful progress, the Notetaker extracts and stores salient transient information for future steps: Nt+1 = (cid:26)uC(Nt, St+1) Nt if jt = SUCCESS, otherwise. Execution loop. The framework iterates (SSt, at, Ft, Nt) updates until all subgoals are completed or termination condition is met (e.g., success, timeout, or safety stop). This design isolates planning, execution, verification, and memory, while remaining compatible with the multi-platform interfaces used throughout training and evaluation. 2.4 Training Paradigm GUI-Owl-1.5 is initialized from Qwen3-VL and trained through three-stage process. Compared with GUI-Owl, each stage is substantially expanded in data diversity and task coverage to support multi-platform automation, tool invocation, and complex agentic interactions. 2.4.1 Pre-training We construct large-scale pre-training corpus that extends beyond basic GUI understanding. In addition to the UI recognition and trajectory data used in GUI-Owl, we incorporate (i) QA and VQA knowledge data to strengthen general visual reasoning and knowledge comprehension, (ii) world-modeling data to train the model to predict how GUI states transition in response to actions, and (iii) tool invocation data to familiarize the model with tool-calling and MCP semantics from the earliest stage. 2.4.2 Supervised Fine-tuning We perform supervised fine-tuning (SFT) to align GUI-Owl-1.5 with diverse agentic tasks across multiple devices. The SFT data covers multi-device trajectory data with CoT annotations (Section 2.2.1), augmented grounding data (Section 2.2.2), structured tool invocation supervision for both conventional tool calls and MCP-based interactions, and dedicated browser interaction data capturing the unique characteristics of web-based GUIs. This stage transforms the pre-trained model into capable multi-device agent supporting GUI manipulation, tool invocation, and browser automation with explicit reasoning. Figure 7: Overview of our reinforcement learning pipeline. 2.4.3 Reinforcement Learning We perform large-scale reinforcement learning called MRPO (Multi-platform Reinforcement Policy Optimization) to further align GUI-Owl-1.5 with long-horizon, tool-augmented GUI control across heterogeneous devices. The key challenges are: (i) unifying learning across mobile/desktop/web environments under one policy, (ii) stabilizing GRPO training when grouped rollouts collapse to identical outcomes. We address these issues as follows, (iii) ensuring log-probability consistency between environment-side inference and training-side optimization, and (iv) mitigating cross-device optimization interference. Multi-device RL with unified policy. We optimize single policy πθ(a o) over trajectories collected from multiple device families = {mobile, desktop, web}. Each device defines its own environment Ed, action space Ad, and observation stream. We model device heterogeneity via device-conditioned policy: πθ(a o, d), Ad. Online rollout buffer for GRPO under outcome collapse. We use GRPO-style grouped rollouts. For task x, we sample group of trajectories {τi}n . In practice, it is common that all rollouts yield identical terminal outcome (e.g., all success or all failure), making the group uninformative and often discarded. replay buffer could increase diversity but introduces off-policy bias. We therefore propose an online rollout buffer that increases within-group diversity while remaining on-policy. i=1 For each x, we temporarily oversample kn rollouts on-policy: Gkn(x) = {τi}kn i=1, τi πθ( x), then uniformly subsample trajectories to form the training group Gn(x). Let Z(τ ) {0, 1} denote binary outcome (success/failure). The crucial property is that uniform subsampling preserves the marginal distribution of any statistic under on-policy sampling: 1 (cid:88) τ Gn(x) = Eτ πθ(x)[f (τ )], (τ ) because Gkn(x) is i.i.d. on-policy and Gn(x) is an exchangeable uniform subset. Thus oversample-then-subsample yields an approximately unbiased estimator. Let Z(τ ) {0, 1} be the terminal outcome (e.g., success). For task x, GRPO forms group Gn(x) = {τi}n with τi πθ( x). The update becomes uninformative when the group is collapsed: i=1 Collapse(Gn) Z(τ ) {0, n} (cid:17) . (cid:16) (cid:88) τ Gn To reduce collapsed groups without introducing off-policy replay, we use an online oversample-and-select buffer. First, sample an on-policy pool of size kn: Define the pool-diversity event Gkn(x) = {τi}kn i=1, τi πθ( x). (cid:16) 0 < (cid:88) Z(τ ) < kn (cid:17) , τ Gkn 10 with probability P(A) = 1 pkn (1 p)kn, Pτ πθ(x)[Z(τ ) = 1]. Then construct the training group (cid:98)Gn(x) by: (cid:98)Gn(x) = Subsamplen(Gkn(x)), Swap1 (cid:0) Subsamplen(Gkn(x)), Gkn(x)(cid:1), Collapse(Subsamplen(Gkn(x))) A, , Collapse(Subsamplen(Gkn(x))), A, where Subsamplen() is uniform random downsampling, and Swap1(S, ) replaces one random element in with random opposite-outcome element from pool (guaranteeing 0 < (cid:80) Z(τ ) < n). This keeps all candidates strictly on-policy (sampled from current πθ) while sharply increasing the probability of obtaining non-collapsed GRPO group. τ (cid:98)Gn Traininginference log-prob alignment via token-id transport. Our inference service is deployed on the environment side and returns trajectories as text (e.g., tool calls, typed strings, serialized actions). However, if the training-side tokenizer maps the returned text to token IDs differently from the inference-side tokenizer (due to non-unique segmentation), then the computed log-probabilities can be inconsistent: log πθ(y x) (cid:12) (cid:12) (cid:12)train-tokenize(y) = log πθ(y x) (cid:12) (cid:12) (cid:12)infer-tokenize(y) . This breaks KL regularization and policy-gradient estimators that assume the same sampled action representation. Our fix is to transport the original inference token IDs alongside the textual payload. Concretely, for each generated sequence y, the environment returns (y, tinfer) where tinfer = (t1, . . . , tL) are the exact token IDs used to sample y. The training process then computes: log πθ(y x) := (cid:88) i=1 log πθ(ti x, t<i) , thereby guaranteeing that the log-prob is evaluated on the same discrete event that was executed in the environment. Alternating multi-device optimization to reduce gradient interference. Mixing trajectories from different devices in single batch can induce strong gradient conflicts because Ad, UI conventions, and domain priors differ substantially. Let gd = Eτ πθ,Ed [θL(τ )] be the device-specific policy gradient. naive mixture update uses = (cid:80) λdgd, but when gd1 , gd2 < 0 frequently, optimization becomes tug-of-war. We adopt an alternating schedule across stages: θ(s+1) θ(s) η gds , ds D, where each stage trains on single device family (potentially with multiple environments within the family), and device families are visited cyclically or via curriculum. This isolates device-specific adaptation while keeping shared backbone, empirically improving stability and preserving cross-device generalization. Agent Model OSWorld-Verified AndroidWorld OSWorld-MCP Mobile-World WindowsAA General Models SeedVL-1.5 (Seed, 2025b) Claude-4-sonnet (Anthropic, 2025d) Claude-4-5-sonnet (Anthropic, 2025e) Gemini-2.5-pro (Deepmind, 2025) Kimi K2.5 (Kimi et al., 2025) Seed-1.8 (Seed, 2025a) OpenAI CUA o3 (Wang et al., 2025a) Qwen3-VL-8B-Instruct (Yang et al., 2025a) Qwen3-VL-8B-Think (Yang et al., 2025a) Qwen3-VL-32B-Instruct (Yang et al., 2025a) Qwen3-VL-32B-Think (Yang et al., 2025a) Qwen3-VL-235B-A22B-Instruct (Yang et al., 2025a) Qwen3-VL-235B-A22B-Think (Yang et al., 2025a) GUI Models (Single-Platform) OpenCUA-7B (Wang et al., 2025a) OpenCUA-32B (Wang et al., 2025a) OpenCUA-72B (Wang et al., 2025a) EvoCUA (Xue et al., 2026) MAI-UI-8b (Zhou et al., 2025) MAI-UI-32b (Zhou et al., 2025) MAI-UI-235b-A22b (Zhou et al., 2025) GUI Models (Multi-Platform) UI-TARS-72B-DPO (Seed, 2025e) UI-TARS-1.5-7B (Seed, 2025c) UI-TARS-1.5 (Seed, 2025c) UI-TARS-2 (Seed, 2025d) GELab-Zero-4B (Yan et al., 2025) GELab-Zero-8B (Yan et al., 2025) GUI-Owl-7b (Ye et al., 2025) GUI-Owl-32b (Ye et al., 2025) Ours (Multi-Platform) GUI-Owl-1.5-2B-Instruct GUI-Owl-1.5-4B-Instruct GUI-Owl-1.5-8B-Instruct GUI-Owl-1.5-8B-Thinking GUI-Owl-1.5-32B-Instruct GUI-Owl-1.5-32B-Thinking 34.1 43.9 62.9 - 63.3 61.9 31.3 33.9 33.9 32.6 41.0 31.6 38.1 28.2 34.8 45.0 56.7 - - - 27.1 27.4 - 53.1 31.9 40.2 34.9 - 43.5 48.2 52.3 52.9 56.5 56.0 62.1 - 56.0 69.7 - 70.7 - 47.6 50.0 57.3 63.7 63.7 62.0 - - - - 70.7 73.3 76. 46.6 - 64.2 73.3 63.9 67.7 66.4 - 67.9 69.8 69.0 71.6 69.8 69.8 38.4 43.3 - 27.2 - - - - - - - - 39.1 - - - - - - - - - - - - - - - 33.0 31.7 41.8 38.8 47.6 43. - - - 5.5 - 9.0 - 9.5 - - 24.9 37.3 41.7 20. 10.9 4.5 5.5 31.3 32.3 41.8 33.3 46.8 42.8 39.6 - - - - - - 28.8 24.1 30.9 42.9 28.9 32.1 - - - - - - - - 15.9 42.1 50.6 - - - - 25.78 29.44 31.66 35.07 44.76 44.13 Table 1: Comparison with state-of-the-art methods on online computer use and mobile use benchmarks. Agent Model WebArena VisualWebArena WebVoyager Online-Mind2Web Proprietary Models Browser-Use (Use, 2025) Claude-CUA-3.7 (Anthropic, 2025c) Operator (OpenAI, 2025a) Gemini-CUA (DeepMind, 2025) Navigator (Yutori, 2025) Magnitude + Claude-4-Sonnet (Magnitude, 2025) VisualWebArena + GPT-4o (Koh et al., 2024a) Tree Search + GPT-4o (Koh et al., 2024b) WALT + GPT-5 (Prabhu et al., 2025) SGV + Gemini-2.5-Flash (Andrade et al., 2025) DeepSky Agent + Claude-4-Sonnet (Tibrewal, 2025) OAgent + Gemini-3-Pro (CodeFuse, 2025) Open-Source Models WebStar-7B (He et al., 2026) WebStar-32B (He et al., 2026) DynaWeb-8B (Ding et al., 2026) ViGoRL-7B (Sarch et al., 2025) Llama-3-70B-Instruct + Tree Search (Koh et al., 2025) AgentSymbiotic-8B (Koh et al., 2025) Ours GUI-Owl-1.5-8B-Instruct GUI-Owl-1.5-8B-Thinking GUI-Owl-1.5-32B-Instruct GUI-Owl-1.5-32B-Thinking - - - - - - - 19.2 50.1 - 66.9 71.6 - - 31.0 - 10.1 43.2 45.7 46.7 - 48.4 - - - - - - 19.8 26.4 52.9 54.4 - - - - - 11.2 16.7 - 39.4 40.8 - 46.6 89.1 - 87.0 - - 93.9 - - - - - - 44.8 48.6 38.7 - - - 69.9 78.1 - 82.1 30.0 56.3 61.3 69.0 78.7 - - - - - - - 22.8 23.8 - - - - 41.7 48.6 - - Table 2: Comparison with state-of-the-art methods on online browser use benchmarks."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup In this section, we evaluate GUI-Owl-1.5 across wide range of benchmarks to thoroughly assess its performance as native GUI agent for multi-device automation. Built on Qwen3-VL, GUI-Owl-1.5 comprises family of models including instruct and thinking variants. In this report, we focus on 6 representative versions: GUI-Owl-1.52B-Instruct, GUI-Owl-1.5-4B-Instruct, GUI-Owl-1.5-8B-Instruct, GUI-Owl-1.5-8B-Think, GUI-Owl-1.5-32BInstruct, and GUI-Owl-1.5-32B-Think. We conduct extensive experiments to evaluate GUI-Owl-1.5 along four key dimensions consistent with GUI-Owl: grounding capability, comprehensive GUI understanding, end-to-end agent capability, and multi-agent capability. 3.2 Main Results 3.2.1 End2end and Multi-Agent capability on Online environment The benchmarks discussed above evaluate isolated, single-step actions, offering only partial view of an agents true capability. In practice, GUI automation requires chaining numerous decisions where earlier mistakes propagate and compound, and multiple valid execution paths may exist for the same task, yet offline benchmarks typically score against single reference trajectory. To overcome these limitations, we conduct end-to-end evaluations across live interactive environments spanning three domains in Fig. 1 and Fig. 9: Mobile Use (AndroidWorld (Rawles et al., 2024), MobileWorld (Kong et al., 2025), and MMGUI-Bench (Liu et al., 2026)), Computer Use (OSWorld (Xie et al., 2024), WindowsAgentArena (Bonatti et al., 2024), and OSWorld-MCP (Jia et al., 2025)), and Browser Use (WebArena (Zhou et al., 2023), VisualWebArena (Koh et al., 2024a), WebVoyager (He et al., 2024), and OnlineMind2Web (?)). Among these, MMGUI-Bench specifically evaluates the agents memory ability. MobileWorld and OSWorld-MCP further incorporate tool invocation, assessing the agents ability to coordinate GUI operations with external tool and MCP calls. In all environments, success is determined solely by whether the final goal state is achieved, regardless of the specific path taken. Computer and Mobile Use. As shown in Table 1, GUI-Owl-1.5 achieves state-of-the-art performance among multi-platform GUI models across both computer and mobile use benchmarks. On OSWorld-Verified, the most widely adopted computer use benchmark, GUI-Owl-1.5-8B-Thinking achieves 52.9, surpassing UI-TARS-2 (53.1) at comparable scale and outperforming all general-purpose models including Qwen3-VL-235B-A22B-Think (38.1). Even our 2B variant attains 43.5, exceeding models with over 10 more parameters such as UI-TARS-72B-DPO (27.1), showcasing strong parameter efficiency. Similarly, on WindowsAA, the 32B-Instruct model scores 44.76, outperforming all general-purpose models at comparable or larger scale. For mobile use, on AndroidWorld, our 8B-Thinking variant attains 71.6, on par with UI-TARS-2 (73.3). Beyond standard GUI interaction, MobileWorld and OSWorld-MCP further require coordinating GUI actions with external tool and MCP calls; on these two benchmarks, GUI-Owl-1.5-32B-Instruct scores 46.8 and 47.6 respectively, surpassing both single-platform specialists (e.g., MAI-UI-235B-A22B at 41.7) and leading proprietary models (e.g., Claude-4-Sonnet at 43.3 on OSWorld-MCP), demonstrating strong tool-use capability. Browser Use. As shown in Table 2, GUI-Owl-1.5-8B-Thinking achieves 46.7 on WebArena, 40.8 on VisualWebArena, 78.1 on WebVoyager, and 48.6 on Online-Mind2Web, surpassing all open-source models by wide margin and remaining competitive with proprietary systems. These results establish GUI-Owl-1.5 as one of the strongest open-source browser agents to date. Across almost all domains, the Thinking variants consistently outperform their Instruct counterparts, with pronounced gains on tasks requiring long-horizon planning (e.g., WebVoyager: 69.9 to 82.1, Online-Mind2Web: 41.7 to 48.6), validating the effectiveness of our thinking-mode training. 3.2.2 Grounding Capability The grounding capability evaluates models ability to locate the corresponding UI element given naturallanguage query. We use ScreenSpot Pro, OSWorld-G, OSWorld-G-Refine, ScreenSpot V2 and MMBench-GUI L2 as benchmarks. ScreenSpot V2 covers mobile, desktop, and web scenarios, while ScreenSpot-Pro primarily evaluates models localization ability at ultra-high resolutions. OSWorld-G/OSWorld-G-Refine contains finely annotated queries. MMBench-GUI L2 has the broadest coverage and more faithfully reflects models grounding performance in real-world settings. The performance comparisons are shown in Tables 3 to 7. In all grounding benchmarks, GUI-Owl-1.5-32B-Instruct achieves state-of-the-art performance among all Multiplatform GUI Models. Notably, on the Screenspot-Pro benchmark, which emphasizes high-resolution and challenging professional software grounding tasks, our GUI-Owl-1.5-32B-Instruct achieves an accuracy of 72.9, surpassing all existing GUI agents (including single-platform, multi-platform, and grounding-specialized models) as well as the large-scale Gemini-3-Pro. Moreover, when augmented with two-stage refinement strategy with crop toolfirst localizing coarse region, then cropping and zoomin for refined grounding, GUI-Owl-1.5-32B-Instruct attains substantially higher score of 80.3, outperforming all prior methods by significant margin. 13 Model Windows MacOS Linux iOS Android Web Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. Overall General Models GPT-4o (Hurst et al., 2024) Claude-3.7 (Anthropic, 2025c) Qwen-2.5-Max-VL (Bai et al., 2025b) InternVL3-72B (Zhu et al., 2025) 1.48 1.48 43.91 70.11 1.10 0.74 36.76 42.64 8.69 12.46 58.84 75. 4.34 7.51 56.07 52.31 1.05 1.05 53.93 59.16 1.02 0.00 30.10 41.33 5.10 13.69 77.39 93.63 3.33 10.61 59.09 80.61 2.53 1.40 79.49 92. 1.41 1.40 70.14 78.59 3.23 3.23 74.84 90.65 2.92 2.27 58.77 65.91 GUI Models (Single-Platform / Grounding Specialized) 38.97 UGround-V1-7B (Gou et al., 2024) 74.3 MAI-UI-8B (Zhou et al., 2025) 78.7 MAI-UI-32B (Zhou et al., 2025) 66.79 92.3 93.0 71.30 90.7 92. 48.55 86.4 87.6 56.54 81.2 86.9 31.12 67.3 77.6 92.68 97.1 97.1 70.91 90.0 92.4 93.54 97.5 98. 70.99 92.7 93.2 88.71 95.8 96.1 64.61 86.0 92.5 GUI Models (Multi-Platform) Aguvis-7B-720P (Xu et al., 2024) OS-Atlas-Base-7B (Wu et al., 2024) GUI-Owl-8B (Ye et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) UI-TARS-72B-DPO (Qin et al., 2025) GUI-Owl-32B (Ye et al., 2025) Ours GUI-Owl-1.5-2B-Instruct GUI-Owl-1.5-4B-Instruct GUI-Owl-1.5-8B-Instruct GUI-Owl-1.5-8B-Thinking GUI-Owl-1.5-32B-Instruct GUI-Owl-1.5-32B-Thinking 37.27 36.90 86.35 68.27 78.60 85. 82.28 87.82 89.66 84.50 91.51 89.67 21.69 18.75 61.76 38.97 51.84 65.07 47.79 69.11 65.44 63.60 68.75 66.18 48.12 44.35 81.74 68.99 80.29 84.93 83.18 88.40 88.11 85.22 92.46 88.12 33.27 21.68 64.45 44.51 62.72 67. 56.06 67.63 72.83 71.10 77.46 73.41 33.51 31.41 74.35 64.40 68.59 76.96 70.15 74.86 72.77 71.73 76.44 77.49 25.00 13.27 61.73 37.76 51.53 63.27 44.38 56.12 56.63 53.57 67.35 63.78 67.52 74.84 94.90 88.54 90.76 95. 88.53 97.13 95.85 92.99 97.13 94.59 65.15 48.79 83.03 69.39 81.21 85.45 69.39 83.33 83.93 81.82 90.61 89.09 60.96 69.60 95.78 90.45 92.98 96.07 81.69 96.05 95.21 89.30 96.06 92.96 50.99 46.76 83.66 69.29 80.00 87. 69.10 85.11 82.86 78.37 89.04 87.64 61.61 61.29 93.22 80.97 88.06 95.48 90.32 95.48 93.22 95.81 96.13 96.13 45.45 35.39 72.72 56.49 68.51 80.84 70.12 82.46 77.59 77.60 84.74 81.49 Table 3: Comparison with state-of-the-art methods on the MMBench-GUI-L2 dataset. Agent Model Development Creative CAD Scientific Office OS Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon General Models Claude 3.7 Sonnet (Anthropic, 2025c) Operator (OpenAI, 2025a) Gemini-3-Pro (DeepMind, 2025) Seed1.8 (Seed, 2025a) Qwen3-VL-8B-Instruct (Bai et al., 2025a) Qwen3-VL-8B-Thinking (Bai et al., 2025a) Qwen3-VL-32B-Instruct (Bai et al., 2025a) Qwen3-VL-32B-Thinking (Bai et al., 2025a) Qwen3-VL-235B-A22B-Instruct (Bai et al., 2025a) Qwen3-VL-235B-A22B-Thinking (Bai et al., 2025a) GUI Models (Single-Platform / Grounding Specialized) InfiGUI-R1-3B (Liu et al., 2025) JEDI-7B (Xie et al., 2025) GUI-G2-7B (Tang et al., 2025) OpenCUA-7B (Wang et al., 2025a) GTA1-New-7B (Yang et al., 2025b) MAI-UI-8B (Zhou et al., 2025) MAI-UI-8B + Zoom-in (Zhou et al., 2025) EvoCUA-8B (Xue et al., 2026) UI-TARS-72B (Qin et al., 2025) UI-Venus-72B (Gu et al., 2025) UGround-v1-72B (Gou et al., 2025) GTA1-32B (Yang et al., 2025b) GTA1-New-32B (Yang et al., 2025b) GTA1-72B (Yang et al., 2025b) MAI-UI-32B (Zhou et al., 2025) MAI-UI-32B + Zoom-in (Zhou et al., 2025) EvoCUA-32B (Xue et al., 2026) GUI Models (Multi-Platform) GUI-Owl-7B (Ye et al., 2025) GUI-Owl-32B (Ye et al., 2025) UI-TARS-1.5 (Seed, 2025c) Step-GUI-8B (Yan et al., 2025) - 50.0 - - - - - - - - 51.3 42.9 68.8 - - 83.8 78.6 - 63.0 84.4 55.8 82.5 - 79.9 86.4 84.4 - 76.6 84.4 - - - 19.3 - - - - - - - - 12.4 11.0 17.2 - - 52.4 58.6 - 17.3 33.1 4.8 28.3 - 33.1 40.7 57.9 - 31.0 39.3 - - - 51.5 - - - - - - - - 44.9 50.0 57.1 - - 76.3 78.8 - 57.1 73.2 54.0 69.2 - 73.2 82.8 87.9 - 59.6 65.2 - - - 23.1 - - - - - - - - 7.0 11.9 15.4 - - 33.6 46.9 - 15.4 30.8 10.5 14.7 - 20.3 37.8 46.2 - 27.3 18.2 - - - 16.8 - - - - - - - - 33.0 38.0 55.8 - - 72.6 80.7 - 18.8 66.5 16.8 43.7 - 56.9 70.1 79.2 - 64.5 62.4 - - - 14.1 - - - - - - - - 14.1 14.1 12.5 - - 35.9 43.8 - 12.5 29.7 4.7 23.4 - 28.1 45.3 53.1 - 21.9 28.1 - - - 58.3 - - - - - - - - 58.3 72.9 77.1 - - 79.9 86.1 - 64.6 84.7 70.8 79.9 - 81.9 91.7 91.7 - 79.1 82.6 - - - 24.5 - - - - - - - - 20.0 25.5 24.5 - - 37.3 49.1 - 20.9 42.7 22.7 31.8 - 38.2 46.4 54.5 - 37.3 39.1 - - - 60.5 - - - - - - - - 65.5 75.1 74.0 - - 88.7 88.1 - 63.3 83.1 61.0 80.8 - 85.3 90.4 88.1 - 77.4 81.4 - - - 28.3 - - - - - - - - 28.3 47.2 32.7 - - 60.4 81.1 - 26.4 60.4 18.9 43.4 - 49.1 71.7 79.2 - 39.6 39.6 - - - 34.6 - - - - - - - - 43.9 33.6 57.9 - - 76.6 76.6 - 42.1 75.7 40.2 70.1 - 73.8 78.5 80.4 - 59.8 70.1 - - - 30.3 - - - - - - - - 12.4 16.9 21.3 - - 49.4 51.7 - 15.7 36.0 7.9 32.6 - 37.1 34.8 47.2 - 33.7 36.0 - - 2.87 4.66 58.03 72. 65.68 88.8 91.3 45.66 41.42 80.49 64.32 74.25 82.97 72.17 83.24 82.52 80.08 86.84 84.47 Avg 27.7 36.6 72.7 73.1 54.6 46.6 57.9 57.1 62.0 61.8 35.7 39.5 47.5 50.0 55.5 65.8 70.9 45.4 38.1 61.9 34.5 53.6 63.6 58.4 67.9 73.5 49. 54.9 58.0 61.6 62.6 Ours GUI-Owl-1.5-2B-Instruct 57.8 70.4 + Zoom-In GUI-Owl-1.5-4B-Instruct 66.8 75.6 + Zoom-In GUI-Owl-1.5-8B-Instruct 71.1 77.8 + Zoom-In 57.6 GUI-Owl-1.5-8B-Thinking 72.5 + Zoom-In GUI-Owl-1.5-32B-Instruct 72.9 80.3 + Zoom-In GUI-Owl-1.5-32B-Thinking 57.0 72.4 + Zoom-In Table 4: Comparison with state-of-the-art methods on the ScreenSpot-Pro dataset. Values marked with were processed with the crop tool. 66.3 73.8 80.3 81.3 82.2 86.9 69.2 84.1 86.9 86.0 72.0 83.2 79.6 88.7 87.0 90.3 89.2 91.5 83.6 91.0 91.5 93.2 87.0 92.7 74.0 77.2 83.1 88.9 87.0 90.2 85.7 88.3 88.3 92.2 83.1 88.3 66.1 71.7 73.7 79.7 79.2 84.8 68.2 78.8 78.8 82.8 71.2 81. 53.2 75.1 59.8 83.7 76.1 86.8 56.9 84.3 80.2 89.3 46.7 83.8 26.5 51.5 45.3 56.2 43.7 62.5 28.1 48.4 48.4 65.6 20.3 42.2 78.4 84.0 87.5 90.2 87.5 89.5 75.7 86.8 90.3 91.7 81.3 87.5 39.0 49.0 43.6 53.6 47.2 55.4 30.9 45.5 54.5 57.3 34.5 46.4 45.2 77.3 60.3 77.3 56.6 71.6 35.8 64.2 56.6 75.5 47.2 66.0 50.5 53.9 50.5 62.9 49.4 53.9 38.2 52.8 44.9 57.3 31.5 55. 32.8 56.6 41.9 57.3 45.4 56.6 28.7 45.5 44.1 67.1 23.8 45.5 43.4 62.0 57.9 58.6 63.4 68.9 37.9 58.6 64.1 73.1 37.2 53.1 14 Agent Model General Models Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Refusal Avg Operator (OpenAI, 2025a) Seed1.5-VL (Seed, 2025b) Qwen3-VL-8B-Instruct (Bai et al., 2025a) Qwen3-VL-8B-Thinking (Bai et al., 2025a) Qwen3-VL-32B-Instruct (Bai et al., 2025a) Qwen3-VL-32B-Thinking (Bai et al., 2025a) 51.3 73.9 69.0 - - - GUI Models (Single-Platform / Grounding Specialized) UGround-7B (Gou et al., 2024) Aguvis-7B (Xu et al., 2024) JEDI-7B (Xie et al., 2025) GTA1-7B (Yang et al., 2025b) UI-Venus-7B (Gu et al., 2025) MAI-UI-8B (Zhou et al., 2025) OpenCUA-32B (Wang et al., 2025a) GTA1-32B (Yang et al., 2025b) MAI-UI-32B (Zhou et al., 2025) EvoCUA-32B (Xue et al., 2026) GUI Models (Multi-Platform) OS-Atlas-7B (Wu et al., 2024) UI-TARS-72B (Qin et al., 2025) UI-TARS-7B (Qin et al., 2025) UI-TARS-1.5-7B (Seed, 2025c) GUI-Owl-7B (Ye et al., 2025) GUI-Owl-32B (Ye et al., 2025) Ours GUI-Owl-1.5-2B-Instruct GUI-Owl-1.5-4B-Instruct GUI-Owl-1.5-8B-Instruct GUI-Owl-1.5-8B-Thinking GUI-Owl-1.5-32B-Instruct GUI-Owl-1.5-32B-Thinking 51.3 55.9 65.9 42.1 74.6 72.0 - 63.2 73.6 - 44.1 69.4 60.2 36.8 64.8 67.0 49.3 66.1 67.8 56.9 68.2 63.6 42.4 66.7 55.5 - - - 40.3 41.2 55.5 65.7 60.5 63.3 - 78.4 72.4 - 29.4 60.6 51.8 62.7 63.6 64.5 52.7 66.1 68.5 60.0 66.8 63. 46.6 69.6 59.7 - - - 43.5 43.9 57.7 62.7 61.5 66.0 - 73.3 73.9 - 35.2 62.9 54.9 62.2 61.3 67.2 48.7 64.8 68.5 60.7 65.5 67.0 31.5 47.0 47.7 - - - 24.8 28.2 46.9 56.1 45.5 51.0 - 65.2 57.7 - 16.8 45.6 35.6 50.8 41.0 45.6 52.4 64.8 65.5 53.8 62.1 55.2 0.0 18.5 - - - - 0.0 0.0 7.4 0.0 - - - 0.0 - - 7.4 0.0 0.0 0.0 - - 53.7 35.2 42.6 22.2 70.4 5. 40.6 62.9 54.8 56.7 65.1 64.0 36.4 38.7 54.1 55.1 58.8 60.1 59.6 65.2 67.6 63.9 27.7 57.1 47.5 52.8 55.9 58.0 52.8 63.7 65.8 55.0 66.8 57.6 Table 5: Comparison with state-of-the-art methods on the OSWorld-G dataset. Agent Model Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Refusal Avg General Models Operator (OpenAI, 2025a) Qwen3-VL-8B-Instruct (Bai et al., 2025a) Qwen3-VL-32B-Instruct (Bai et al., 2025a) - 73.9 77.4 GUI Models (Single-Platform / Grounding Specialized) JEDI-7B (Xie et al., 2025) GTA1-7B (Yang et al., 2025b) OpenCUA-32B (Wang et al., 2025a) GTA1-32B (Yang et al., 2025b) - 63.2 63.2 63.2 - 68.2 73.6 - 82.1 79.9 83.6 GUI Models (Multi-Platform) UI-TARS-1.5-7B (Seed, 2025b) Ours GUI-Owl-1.5-2B-Instruct GUI-Owl-1.5-4B-Instruct GUI-Owl-1.5-8B-Instruct GUI-Owl-1.5-8B-Thinking GUI-Owl-1.5-32B-Instruct GUI-Owl-1.5-32B-Thinking 52.6 75.4 65.1 73.1 73.1 64.7 68.5 68.9 60.3 71.8 69.4 61.1 68.5 69.7 - 73.1 76. - 74.2 84.9 84.4 72.4 63.3 72.9 71.0 64.9 71.0 72.6 - 54.4 57.7 - 70.5 62.1 70.5 - - - - 0.0 7.4 0.0 57.8 64.4 69.0 63.8 67.7 70.2 72.2 66.7 0.0 64. 65.9 74.1 74.8 65.3 70.1 72.1 53.7 29.6 42.5 20.4 68.5 5.6 62.6 68.4 69.3 59.8 69.7 64.7 Table 6: Performance comparison of state-of-the-art models on the OSWorld-G-Refine. 15 Agent Model Mobile Desktop Web Text Icon Text Icon Text Icon Overall General Models OmniParser-v2 (Yu et al., 2025) Operator (OpenAI, 2025a) Claude 3.7 Sonnet (Anthropic, 2025c) UI-TARS-1.5 (Qin et al., 2025) Seed-1.5-VL (Seed, 2025b) Qwen3-VL-8B-Instruct (Bai et al., 2025a) Qwen3-VL-8B-Thinking (Bai et al., 2025a) Qwen3-VL-32B-Instruct (Bai et al., 2025a) Qwen3-VL-32B-Thinking (Bai et al., 2025a) 95.5 47.3 - - - - - - - GUI Models (Single-Platform / Grounding Specialized) 96.6 JEDI-3B (Xie et al., 2025) 96.9 JEDI-7B (Xie et al., 2025) 99.0 GTA1-7B (Yang et al., 2025b) 98.6 GTA1-32B (Yang et al., 2025b) - EvoCUA-32B (Xue et al., 2026) 99.7 UI-Venus-72B (Gu et al., 2025) GUI Models (Multi-Platform) OS-Atlas-Base-4B (Wu et al., 2024) OS-Atlas-Base-7B (Wu et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) GUI-Owl-7B (Ye et al., 2025) GUI-Owl-32B (Ye et al., 2025) Ours GUI-Owl-1.5-2B-Instruct GUI-Owl-1.5-4B-Instruct GUI-Owl-1.5-8B-Instruct GUI-Owl-1.5-8B-Thinking GUI-Owl-1.5-32B-Instruct GUI-Owl-1.5-32B-Thinking 95.2 96.2 96.9 94.8 99.0 98.6 92.9 95.1 97.4 95.8 97.1 96.5 74.6 41.5 - - - - - - - 81.5 87.2 88.6 89.1 - 93.8 75.8 83.4 89.1 86.3 92.4 90.0 83.1 93.1 90.5 90.5 92.6 90.5 92.3 90.2 - - - - - - - 96.9 95.9 94.9 96.4 - 95.9 90.7 89.7 95.4 91.2 96.9 97. 94.8 95.9 96.4 97.4 97.9 96.9 60.9 80.3 - - - - - - - 78.6 87.9 89.3 86.4 - 90.0 63.6 69.3 85.0 87.9 85.0 87.8 86.4 86.4 90.7 90.0 89.3 86.4 88.0 92.8 - - - - - - - 88.5 94.4 92.3 95.7 - 96.2 90.6 94.0 93.6 91.5 93.6 94.4 90.1 92.9 94.2 95.0 95.5 93.8 59.6 84.3 - - - - - - - 83.7 84.2 86.7 88.7 - 92.6 77.3 79.8 85.2 87.7 85.2 86. 87.6 92.8 89.7 87.7 96.4 90.8 Table 7: Comparison with state-of-the-art methods on the ScreenSpot-V2 dataset. 80.7 70.5 87.6 94.2 95.2 94.4 93.5 95.8 95.7 88.6 91.7 92.4 93.2 90.4 95.3 85.1 87.1 91.6 90.3 92.8 93.2 89.7 93.2 93.7 93.2 95.3 93. Agent Model state widget layout effect type parameter goal plan Interface Perception Interaction Prediction Instruction Understanding Avg Proprietary Models O3 (OpenAI, 2025d) Gemini-2.5-Pro (Comanici et al., 2025) GPT-5-Chat (OpenAI, 2025c) Claude-Sonnet-4-5 (Anthropic, 2025b) Doubao-V-Pro (Seed, 2025b) Claude-Sonnet-4 (Anthropic, 2025a) Open-Source Models Qwen3-VL-8B-Instruct (Bai et al., 2025a) Qwen3-VL-8B-Thinking (Bai et al., 2025a) Qwen2.5VL-72B (Bai et al., 2025c) Qwen2.5VL-7B (Bai et al., 2025c) UITARS-1.5-7B (Seed, 2025c) GUI-OWL-7B (Ye et al., 2025) GLM-4.5 (Zeng et al., 2025) Ours GUI-Owl-1.5-2B-Instruct GUI-Owl-1.5-4B-Instruct GUI-Owl-1.5-8B-Instruct GUI-Owl-1.5-8B-Thinking GUI-Owl-1.5-32B-Instruct GUI-Owl-1.5-32B-Thinking 83.03 81.19 78.90 74.77 72.48 70.18 76.61 68.81 69.27 53.21 49.54 60.09 49.54 60.09 75.23 77.98 75.69 77.06 81.19 84.12 84.36 84.12 81.52 83.65 78. 89.81 76.30 77.49 67.77 59.48 64.93 48.10 77.73 88.15 88.86 90.05 92.65 90.76 88.39 87.10 88.39 82.58 81.29 78.06 83.87 83.23 80.00 60.00 59.35 63.23 53.55 72.26 82.58 84.52 87.74 85.81 85.81 74.83 71.03 71.55 49.83 67.24 41. 58.97 67.07 61.72 51.72 22.24 21.55 27.07 44.48 55.69 66.90 67.41 70.69 68.10 75.98 73.25 71.55 70.19 75.64 62.52 70.20 70.36 64.91 50.60 59.11 55.37 17.55 47.04 65.02 71.92 68.23 73.89 73.89 45.75 46.97 43.85 43.33 41.07 42. 51.58 40.73 38.99 39.34 34.32 36.05 35.53 41.95 54.22 61.61 53.43 64.12 57.65 69.45 67.72 68.98 70.30 33.07 65.20 67.40 64.09 62.20 16.22 38.74 21.26 28.98 62.99 69.45 73.54 67.72 73.39 72.91 95.47 92.56 91.26 91.56 94.17 94. 77.99 91.26 85.44 48.87 55.34 39.81 91.91 47.57 70.55 80.58 77.67 88.67 86.41 73.30 71.69 70.97 66.53 63.42 62.16 67.84 66.81 63.88 45.16 44.27 40.74 38.10 54.12 66.64 72.90 69.60 75.45 73.36 Table 8: Comparison with state-of-the-art methods on the GUI Knowledge Benchmark. 16 Agent Model Type Success Rate Proprietary / Workflow Models Workflow Agent-S2 w/ Gemini-2.5-Pro Workflow M3A w/ Gemini-2.5-Pro T3A w/ Gemini-2.5-Pro Workflow Mobile-Agent-E w/ Gemini-2.5-Pro Workflow AppAgent w/ Gemini-2.5-Pro Workflow Mobile-Agent-V2 w/ Gemini-2.5-Pro Workflow Workflow SeeAct w/ Gemini-2.5-Pro Native Agent Models Qwen3-VL-8B-Instruct GUI-Owl-7B UI-Venus-7B UI-TARS-1.5-7B CogAgent GUI-Owl-1.5-8B GUI-Owl-1.5-32B Model Model Model Model Model Model Model 41.7 39.6 31.2 12.5 8.3 8.3 6.2 18.8 14.6 14.6 8.3 0.0 22.9 27. Table 9: Evaluation results on MemGUI-Bench (Easy tasks). 3.2.3 Comprehensive GUI Understanding GUI Knowledge. The GUI Knowledge Benchmark (Shi et al., 2025) systematically evaluates whether GUI model possesses sufficient knowledge across three dimensions: Interface Perception (state information understanding, widget function understanding, and layout semantics understanding), Interaction Prediction (action effect, action type prediction, and action parameter prediction), and Instruction Understanding (goal interpretation and task planning). On this benchmark, GUI-Owl-1.5-32B-Instruct achieves an overall accuracy of 75.45, establishing the highest performance among all evaluated models, including proprietary ones such as o3 (73.30) (OpenAI, 2025b) and Gemini-2.5-Pro (71.69) (Deepmind, 2025). It attains particularly strong results on widget function understanding and action parameter prediction, substantially outperforming all other models in these categories. GUI Memory. We further evaluate on MemGUI-Bench (Liu et al., 2026)  (Table 9)  , which assesses an agents ability to recall and leverage interaction history over long horizons. Among native agent models, GUI-Owl1.5-32B achieves 27.1, substantially outperforming all prior baselines including Qwen3-VL-8B-Instruct (18.8) and UI-TARS-1.5-7B (8.3). Even our 8B variant (22.9) surpasses all existing native baselines, confirming that our training recipe effectively instills long-horizon memory capabilities without relying on external workflow orchestration. 3.3 Detailed Analyses Effect of Virtual-enviroment trajectory Production and Unified CoT Synthesis. We conduct ablation experiments to validate two key components: virtual environment-based trajectory production  (Table 11)  and unified CoT synthesis  (Table 10)  . As shown in Table 11, removing trajectory data produced by virtual environments leads to dramatic performance drops on both PC-Eval (75.4% to 42.0%) and Mobile-Eval (86.7% to 50.0%). Here, PC-Eval is an in-house benchmark focusing on atomic desktop operations such as drag and scroll, as well as office document and spreadsheet editing tasks; Mobile-Eval is an in-house benchmark covering popular Chinese mobile application scenarios including food delivery, ride-hailing, ticket booking, among others. The substantial degradation on both benchmarks confirms that our web-rendering-based virtual environments effectively bypass real-world exploration limitationssuch as CAPTCHA interruptions and the lack of accurate feedbackand provide scalable, highquality trajectories that are critical for mastering these challenging scenarios. As shown in Table 10, removing the unified CoT synthesis causes consistent drops on both OSWorld (52.9% to 47.4%) and AndroidWorld (71.6% to 65.0%), demonstrating that step-wise thought and conclusion augmentation provides essential reasoning supervision. By equipping each trajectory step with observation, memory, reflection, and progress tracking, CoT synthesis enables the model to plan over long horizons and retain key information across steps, which is particularly beneficial for multi-step online tasks across different platforms. The two components are complementary: virtual environments improve trajectory coverage and quality, while CoT synthesis enhances reasoning and decision-making supervision. Effect of Unstable-set Train and Interleaved Train in RL. We conduct ablation experiments to validate two critical training strategies for GUI-Owl-1.5s reinforcement learning optimization, demonstrating the effectiveness 17 Unified CoT Synthesis OSWorld AndroidWorld 47.4 52.9 65.0 71.6 Table 10: Ablation study on the unified CoT synthesis pipeline. The experiments are conducted with GUI-Owl1.5-8B-Thinking. Virtual Environments PC-Eval Mobile-Eval 42.0% 75.4% 50.0% 86.7% Table 11: Ablation study on the virtual environments. The experiments are conducted with GUI-Owl-1.5-8BThinking. PC-Eval is an in-house benchmark evaluating atomic operations such as drag and scroll, as well as office document and spreadsheet editing tasks. Mobile-Eval is an in-house benchmark evaluating popular Chinese mobile application scenarios, including food delivery, ride-hailing, ticket booking, among others. Figure 8: Ablation Study on Reinforcement Learning Training Strategies for GUI-Owl-1.5-8B-thinking: Task Selection and Multi-Platform Training Strategies. of targeted task selection and multi-platform training strategy, as shown in Fig. 8. Fig. 8(a) compares PC validation performance between full dataset training and unstable-task-only training (derived from multi-round rollouts). Unstable-task-focused training achieves faster convergence and higher final accuracy, demonstrating the efficacy of prioritizing challenging tasks for robust model optimization. In Fig. 8 (b), mix-platform training (simultaneous multi-platform data optimization) is contrasted with our interleaved training (switch from Mobile to PC at step 10). Mix-platform training exhibits performance oscillation due to cross-platform interference, whereas interleaved training enables focused optimization per platform while maintaining performance stability during transitions. This approach achieves synergistic multi-platform growth, validating the superiority of our presented interleaved RL training strategy. 3.4 Case Study We present three representative cases to illustrate the comprehensive capabilities of GUI-Owl 1.5 beyond basic GUI navigation. Mobile Use Case  (Fig. 9)  In this case, the user seeks to determine the total follower count of the ModelScope Community account across two social media platforms: Xiaohongshu and Douyin. The agent first launches the Xiaohongshu app, enters the account name in the search box, retrieves the follower information, and stores it in memory. Subsequently, the agent navigates to the Douyin app to obtain the corresponding follower count. By combining the retrieved information from memory with the current data, the agent calculates and reports the number of overall follower across both platforms. Computer Use Case  (Fig. 10)  . Figure 10 illustrates case of GUI-Owl-1.5 executing web search and notetaking task on the Windows platform. To fulfill the user query, the agent is required to accurately perform multiple 18 Figure 9: complete operation process on the Android platform, in which the user query requires the agent to search and summarize information on social media platforms. web searches and extract key information relevant to subsequent steps from the search results, which is stored as memory within the thought content (highlighted in green). Subsequently, the agent switches to different application, creates new spreadsheet in WPS Office, and fills in the corresponding content at the appropriate cells based on the memorized information. The thoughts generated by GUI-Owl-1.5 during the execution steps demonstrate its understanding of screen content, precise grounding, analysis of task progress, and memorization of key information, validating the effectiveness of our proposed unified CoT synthesis pipeline. In this case, the agent is tasked with completing partially implemented Python Tool Use Case  (Fig. 11)  . script on the desktop and saving its execution output. The agent seamlessly interleaves MCP tool calls with GUI operations: it first reads the source code via the filesystem_read_text_file tool, identifies and fixes the incomplete insertion sort implementation using filesystem_edit_file, then opens terminal through osworld_mcp_os.open_shell to execute the script via command-line input, and finally verifies the output by reading the generated log file. This case demonstrates GUI-Owl 1.5s ability to autonomously decide when to use tool invocation versus direct GUI manipulation within single trajectory."
        },
        {
            "title": "4 Conclusion",
            "content": "In this work, we presented GUI-Owl-1.5, the native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports range of devices (desktop, mobile, browser, and more). GUI-Owl-1.5 achieves state-of-the-art performance on 20+ GUI benchmarks, comprehensively covering GUI automation, grounding, tool calling, memory, and knowledge tasks. We innovatively improve the models robust generalization in real-world application scenarios through Hybrid Data Flywheel, unified enhancement of agent capabilities, and multi-device environment RL scaling. We hope that the open-source release of GUI-Owl-1.5 will advance the adoption of GUI agents for device automation across wide range of platforms. 19 Figure 10: complete operation process on the Windows platform, in which the user query requires the agent to memorize key on-screen information. 20 Figure 11: case of complete operation process on desktop platform, which combining extended tools and computer use actions."
        },
        {
            "title": "References",
            "content": "Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents, 2025. URL https://arxiv.org/abs/2504. 00906. 2 Moises Andrade, Joonhyuk Cha, Brandon Ho, Vriksha Srihari, Karmesh Yadav, and Zsolt Kira. Lets think in two steps: Mitigating agreement bias in mllms with self-grounded verification. arXiv preprint arXiv:2507.11662, 2025. 12 Anthropic. System card: Claude opus 4 & claude sonnet 4, 2025a. URL https://www.anthropic.com/ claude-4-system-card. Accessed: 2025-09-25. 2, 16 Anthropic. Claude sonnet 4.5. https://docs.claude.com/docs/about-claude/models/ whats-new-claude-4-5, 2025b. Accessed: 2025-11-22. 16 Anthropic. Claude 3.7 sonnet and claude code. Technical report, Anthropic, 2025c. URL https://www. anthropic.com/news/claude-3-7-sonnet. System Card. 12, 14, 16 Anthropic. Claude-4-sonnet. Technical report, Anthropic, 2025d. URL https://www.anthropic.com/ news/claude-4-sonnet. Anthropic. Claude-4-5-sonnet. Technical report, Anthropic, 2025e. URL https://www.anthropic.com/ news/claude-4-5-sonnet. 12 Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. 2, 14, 15, 16 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. 14 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025c. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. 13 CodeFuse. Oagent. https://github.com/codefuse-ai/Oagent, 2025. 12 Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 16 Deepmind. mind, gemini-model-thinking-updates-march-2025/. 12, 17 report, Deephttps://blog.google/technology/google-deepmind/ Our most ai model. intelligent Technical Gemini 2025. URL 2.5: Google DeepMind. Gemini 3 pro, 2025. URL https://deepmind.google/models/gemini/pro/. 2, 12, 14 Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, and Lei Yu. Dynaweb: Model-based reinforcement learning of web agents, 2026. URL https://arxiv. org/abs/2601.22149. 12 Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. 14, 15 Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=kxnoqaisCT. 14 Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, et al. Ui-venus technical report: Building high-performance ui agents with rft. arXiv preprint arXiv:2508.10833, 2025. 14, 15, 16 Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. 13 Yifei He, Pranit Chawla, Yaser Souri, Subhojit Som, and Xia Song. Webstar: Scalable data synthesis for computer use agents with step-level filtering, 2026. URL https://arxiv.org/abs/2512.10962. 12 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Hongrui Jia, Jitong Liao, Xi Zhang, Haiyang Xu, Tianbao Xie, Chaoya Jiang, Ming Yan, Si Liu, Wei Ye, and Fei Huang. Osworld-mcp: Benchmarking mcp tool invocation in computer-use agents. arXiv preprint arXiv:2510.24563, 2025. 13 Kimi, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. 12 Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 881905, 2024a. 12, 13 Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024b. 12 Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents, 2025. URL https://arxiv.org/abs/2407.01476. 12 Quyu Kong, Xu Zhang, Zhenyu Yang, Nolan Gao, Chen Liu, Panrong Tong, Chenglin Cai, Hanzhang Zhou, Jianan Zhang, Liangyu Chen, et al. Mobileworld: Benchmarking autonomous mobile agents in agent-user interactive and mcp-augmented environments. arXiv preprint arXiv:2512.19432, 2025. 13 Guangyi Liu, Pengxiang Zhao, Yaozhen Liang, Qinyi Luo, Shunye Tang, Yuxiang Chai, Weifeng Lin, Han Xiao, WenHao Wang, Siheng Chen, Zhengxi Lu, Gao Wu, Hao Wang, Liang Liu, and Yong Liu. Memgui-bench: Benchmarking memory of mobile gui agents in dynamic environments, 2026. URL https://arxiv.org/ abs/2602.06075. 13, 17 Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. 2 Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. 14 Magnitude. Magnitude. https://magnitude.run, 2025. OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world. 2025a. URL https://openai.com/index/computer-using-agent. 12, 14, 15, 16 OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025b. URL https://cdn.openai. com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card. pdf. System Card. 17 OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025c. 2, 16 OpenAI. Openai o3 and o4-mini system card, 2025d. o3-o4-mini-system-card/. Accessed: 2025-09-25. URL https://openai.com/index/ 23 Viraj Prabhu, Yutong Dai, Matthew Fernandez, Jing Gu, Krithika Ramakrishnan, Yanqi Luo, Silvio Savarese, Caiming Xiong, Junnan Li, Zeyuan Chen, et al. Walt: Web agents that learn tools. arXiv preprint arXiv:2510.01524, 2025. 12 Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 2, 14, 15, 16 Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. 13 Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning, 2025. URL https://arxiv.org/ abs/2505.23678. Bytedance Seed. Seed1.8 model card: Towards generalized real-world agency. arXiv preprint, December 2025a. Technical Report. 12, 14 ByteDance Seed. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. 12, 15, 16 ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025c. 12, 14, 15, 16 ByteDance Seed. Ui-tars-2. https://seed-tars.com/showcase/ui-tars-2, 2025d. 2, 12 ByteDance Seed. Ui-tars. https://seed-tars.com/1, 2025e. Chenrui Shi, Zedong Yu, Zhi Gao, Ruining Feng, Enqi Liu, Yuwei Wu, Yunde Jia, Liuyu Xiang, Zhaofeng He, and Qing Li. Gui knowledge bench: Revealing the knowledge gap behind vlm failures in gui tasks. arXiv preprint arXiv:2510.26098, 2025. 17 Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. 14 Anshul Tibrewal. Deepsky agent. building-a-practical-browser-agent, 2025. 12 https://deepskyai.substack.com/p/ Browser Use. Browser use. https://github.com/browser-use/browser-use, 2025. 12 Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:26862710, 2024a. 2 Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobileagent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024b. 2 Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025a. URL https: //arxiv.org/abs/2508.09123. 2, 12, 14, Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025b. 2 Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. 14, 15, 16 Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. 13 24 Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computeruse grounding via user interface decomposition and synthesis, 2025. URL https://arxiv.org/abs/ 2505.13227. 14, 15, 16 Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. 14, Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han, Haozhe Wang, Jianing Wang, Xiaocheng Zhang, Xin Yang, Dengchang Zhao, et al. Evocua: Evolving computer use agents via learning from scalable synthetic experience. arXiv preprint arXiv:2601.15876, 2026. 12, 14, 15, 16 Haolong Yan, Jia Wang, Xin Huang, Yeqing Shen, Ziyang Meng, Zhimin Fan, Kaijun Tan, Jin Gao, Lieyu Shi, Mi Yang, et al. Step-gui technical report. arXiv preprint arXiv:2512.15431, 2025. 12, 14 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. 12 Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025b. 14, 15, Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. 2, 3, 12, 14, 15, 16 Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. 16 Yutori. Navigator. https://yutori.com/blog/introducing-navigator, 2025. 12 Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. 16 Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 120, 2025. 2 Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, et al. Mai-ui technical report: Real-world centric foundation gui agents. arXiv preprint arXiv:2512.22047, 2025. 2, 12, 14, Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 13 Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}