{
    "paper_title": "Reinforcing General Reasoning without Verifiers",
    "authors": [
        "Xiangxin Zhou",
        "Zichen Liu",
        "Anya Sims",
        "Haonan Wang",
        "Tianyu Pang",
        "Chongxuan Li",
        "Liang Wang",
        "Min Lin",
        "Chao Du"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 9 4 1 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Reinforcing General Reasoning without Verifiers",
            "content": "Xiangxin Zhou23, Zichen Liu14, Anya Sims15, Haonan Wang14, Tianyu Pang1, Chongxuan Li6, Liang Wang23, Min Lin1, Chao Du1 1Sea AI Lab, Singapore; 2University of Chinese Academy of Sciences; 3Institute of Automation, Chinese Academy of Sciences; 4National University of Singapore; 5University of Oxford; 6Renmin University of China zhouxiangxin1998@gmail.com; {liuzc, tianyupang, linmin, duchao}@sea.com"
        },
        {
            "title": "Abstract",
            "content": "The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as model-based verifier; however, this introduces issues such as reliance on strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in unified model, and as variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree. Figure 1: The general reasoning capability is significantly improved when we apply VeriFree to fine-tune Qwen3 base models on general reasoning dataset. Notably, VeriFree can perform on par with or even surpass the instruct models and models RL-tuned from base with specialized LLM as verifier. Equal contribution. Work done during Anyas associate membership and Haonans internship at Sea AI Lab. Correspondence to Chao Du. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "DeepSeek-R1-Zero [10] recently demonstrated that training large language models (LLMs) using reinforcement learning (RL) with verifiable rewards can be extremely effective in improving reasoning capabilities. In this RL with verifiable rewards (RLVR) framework [17], the LLM generates reasoning trace (i.e., chain of thoughts, CoT) followed by final answer. rule-based program then extracts and evaluates the final answer, assigning reward of 1 to the response if the final answer is correct and 0 otherwise. The model is trained with RL using GRPO [37]a simplified variant of PPO [36]. The simplicity of this approach, coupled with impressive performance improvements in mathematical reasoning tasks, has sparked wave of follow-up works in this paradigm of RL with rule-based verifiable rewards [24, 26, 45], which we will refer to as the R1-Zero-style training in the following. However, these methods remain limited to domains such as mathematics and code, where rule-based verification is feasible. Reasoning is critical far beyond math and coding; however, the difficulty of answer verification in general reasoning tasks poses major obstacle to applying this training paradigm to broader domains. To address this limitation, we investigate how to extend R1-Zero-style training to tasks where rule-based answer verification is not possible. natural extension, as explored in recent general reasoning works [38, 27], is to introduce specialized LLM as verifier, similar to the reward model used in RL from human feedback (RLHF) [51, 30]. In these methods, the model-based verifier is queried to determine whether the generated answer is equivalent to the reference answer. Although this approach bypasses the need for rule-based evaluation, it introduces several potential drawbacks (as in standard RLHF): it depends on the availability of strong verifier LLM, it converts the R1-Zero-style paradigm into optimizing model-based reward, which makes it vulnerable to reward hacking [7], and it adds significant computational overhead by requiring an additional model to be held in memory and queried during training. In this work, we propose an alternative: verifier-free approach that preserves the benefits of the RL paradigm while eliminating the reliance on explicit verification, either performed by rules or by models. Our method proceeds as follows. Given question, we only generate the reasoning trace and concatenate it with the reference answer from the dataset. We then evaluate the likelihood of the reference answer conditioned on the question and the generated reasoning trace. This likelihood serves both as reward signal for policy gradients on the reasoning trace and as weighting term for supervised training of the reference answer. We term our method VeriFree since it does not rely on ruleor model-based verifiers, and give an illustration in Fig. 2. This approach has several appealing properties. First, when there is unique correct answer string, our method is equivalent in expectation to the objective in RLVR, but with lower variance, which can be viewed as form of reward shaping [28, 34]. Even when multiple valid answers exist, we show empirically that using just one as reference provides sufficient learning signal to elicit strong reasoning behavior. Additionally, this framework can be viewed through variational lens as neat way of optimizing over latent reasoning traces. To make this work in practice, we identify and address several subtle challenges, including effective variance reduction and precise handling of tokenization at the reasoning-answer patching point. We conduct comprehensive ablations to understand the impact of each design choice. We benchmark our method across diverse set of general reasoning tasks, and the results are striking: as shown in Fig. 1, VeriFree not only matches but often outperforms verifier-based alternatives, while being simpler, faster, less memory-intensive, and more robust."
        },
        {
            "title": "2 Methodology",
            "content": "2.1 Preliminaries: Verifier-Based Reinforcement Learning In RL applied to LLMs, the language model is treated as policy πθ that generates an output autoregressively in response to an input question x. The goal is typically to optimize πθ to maximize given reward function R(x, o): θ arg max θ Eoπθ(x) [R(x, o)] . (1) In R1-Zero-style RL, the reward is computed by first parsing the response into reasoning trace and final answer y. verifier then checks against the ground-truth reference answer and 2 Figure 2: VeriFree enables R1-Zero-style LLM training without requiring access to verifier. In the case of single correct answer format, VeriFree optimizes exactly the same objective as R1-Zero with lower variance gradient estimator. assigns binary reward based on correctness, namely RVerifier(y; y) = 1{yy}.1 Decomposing the model output as = (z, y), we can rewrite the objective in Eq. (1) as: JVerifier(θ; x, y) = Ezπθ(x)Eyπθ(x,z)[RVerifier(y; y)], which separates the sampling of the reasoning trace and the final answer. To maximize this objective, the model πθ is typically updated using the policy gradient estimator [39]: (2) θJVerifier(θ; x, y) = zπθ (x) (cid:104) yπθ (x,z) RVerifier(y; y)(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3)(cid:105) . (3) However, this approach requires evaluating answer correctness via RVerifier(y; y) = 1{yy}, which is often nontrivial. While in domains such as math and code, this evaluation is feasible via rules [10, 24] or test cases [8], accurate verification in general reasoning tasks is substantially more difficult. As result, recent advances in R1-Zero-style training have largely been restricted to verifiable domains, leaving reasoning tasks in general domains underexplored. In light of this, we present an alternative verifier-free approach which naturally extends this training paradigm to broader reasoning domains. 2.2 VeriFree Policy Optimization We begin with the standard objective in Eq. (2) and show that, in the case of single correct answer, we can derive an equivalent objective that does not require verifier. Moreover, we compare the gradient estimators of this new objective and the verifier-based objective (Eq. (3)), and demonstrate that our verifier-free gradient estimator has the additional benefit of lower variance. Starting from Eq. (2) and assuming unique correct answer such that RVerifier(y; y) = 1{y=y} (i.e., exact match rather than semantic equivalence 1{yy}), the VeriFree objective is derived as follows: JVerifier(θ; x, y) = Ezπθ(x) = Ezπθ(x) = Ezπθ(x) RVerifier (cid:105) (cid:122) (cid:125)(cid:124) (cid:123) 1{y=y} ] (cid:104) Eyπθ(x,z)[ (cid:104) (cid:88) πθ(yx, z)1{y=y} (cid:105) (cid:104) πθ(yx, z) (cid:123)(cid:122) (cid:125) (cid:124) RVeriFree (cid:105) JVeriFree(θ; x, y). (4) This can be interpreted as follows: if only one answer is correct and receives reward of 1 (while all others receive 0), then the expected reward given reasoning trace can be computed directly as 1We use to denote semantic equivalence, where multiple expressions can be judged correct. For example, if = 8/5, then 8/5, 1.6, and frac{8}{5} are all considered correct. 3 Verifier-based (R1-Zero) VeriFree (Ours) Model generates the reasoning trace and answer y. Model generates the reasoning trace z. Extract the answer y. Check answer using verifier. Reward RVerifier = 1 if correct, 0 otherwise. Train with gradient estimator θJVerifier (Eq. 3). Patch in the correct answer y. Evaluate probability πθ(yx, z). Reward RVeriFree = πθ(yx, z). Train with gradient estimator θJVeriFree (Eq. 5). Figure 3: pseudocode-like comparison of VeriFree (ours) and the standard R1-Zero approach. the probability assigned to y, effectively marginalizing out y. The corresponding gradient estimator is given by (see Appendix A.1 for full derivation): θJVeriFree(θ; x, y) = Ezπθ (x) (cid:20) RVeriFree(x, y, z)(cid:2) θ log πθ(zx) (cid:125) (cid:124) (cid:123)(cid:122) reasoning term + θ log πθ(yx, z) (cid:123)(cid:122) (cid:125) reference answer term (cid:124) (cid:21) . (cid:3) (5) Both the objective and its gradient estimator (Eq. (4) and (5)) are equivalent in expectation to their verifier-based counterparts (Eq. (2) and (3)). Intuitively, the reasoning term in Eq. (5) can be interpreted as policy gradient where the reward for reasoning trace is the probability that the policy will generate the correct answer given z, while reference answer term can be viewed as reward-weighted supervised learning term for given z. We will further elaborate on this interpretation in Sec. 2.3. In addition to bypassing the need for verifier, our VeriFree gradient estimator also benefits from reduced variance: Theorem 1. (Variance Reduction) Let ˆGVerifier(x, y, z) and ˆGVeriFree(x, y, z, y) denote the singlesample Monte Carlo estimators of θJVerifier and θJVeriFree given and y, respectively. Then we have Varzπθ(x) (cid:2) ˆGVeriFree(x, y, z)(cid:3) Varzπθ(x),yπθ(x,z) (cid:2) ˆGVerifier(x, y, z, y)(cid:3). (6) This reduction in variance arises from Rao-Blackwellization [2]. For intuition, the variance in the Monte Carlo estimate of θJVerifier stems from the randomness in sampling πθ(x), πθ(x, z), while for estimating θJVeriFree we analytically marginalizes out y, thereby removing this source of randomness. We provide full proof in Appendix A.2. Our gradient estimator θJVeriFree(θ; x, y) is fully compatible with other variance reduction techniques, including RLOO [1], GRPO [37] reward normalizations, and the PPO [36] clipping operation. As such, we sample multiple responses for each prompt and apply the RLOO baseline to the reasoning term in Eq. (5). We also adopt the corrected response-length normalization from Liu et al. [24]. The final on-policy gradient estimator is as follows: θJVeriFree(θ) = 1 (cid:88) i=1 [Ai θ log πθ(zix) + Ri θ log πθ(yx, zi)] , (7) where zi πθ(x), Ri = πθ(yx, zi), and Ai = πθ(yx, zi) 1 also provide the PPO-based off-policy variant in Appendix B. G1 (cid:80) j=i πθ(yx, zj). We 2.3 Comparison to Existing Approaches There have been two main prior works that, although derived from different perspective, arrive at related alternative gradient estimators: Tang et al. [40] (JLB), and Chen et al. [4] (LaTRO). θJVerifier = Ez,y (cid:2) 1{yy} reasoning term (cid:125)(cid:124) (cid:122) (cid:123) θ log πθ(zx) + 1{yy} answer term (cid:122) (cid:125)(cid:124) (cid:123) θ log πθ(yx, z) (cid:3) θJVeriFree = Ez θJJLB = Ez (cid:2) πθ(yx, z) θ log πθ(zx) + πθ(yx, z) (cid:2) log πθ(yx, z) θ log πθ(zx) + 1 θ log πθ(yx, z)(cid:3) reference answer term (cid:125)(cid:124) (cid:122) (cid:123) θ log πθ(yx, z) (cid:3) (R1-Zero) (Ours) (Tang et al. [40]) θJLaTRO = Ez (cid:2) (log πθ(yx, z) log πθ(zx) πref(zx) ) θ log πθ(zx) + 1 θ log πθ(yx, z)(cid:3) (Chen et al. [4]) Both JLB and LaTRO treat the reasoning trace as latent variable and extend the standard supervised learning objective (log-likelihood) to optimize lower bounds on log(Ezπθ(x) [πθ(yx, z)]) and log(Ezπref(x) [πθ(yx, z)]), respectively. The primary difference is that JLB samples from the learned policy πθ, while LaTRO uses fixed reference policy πref. Despite originating from different perspectives, these approaches arrive at alternative gradient estimators that can be used similarly to ours, as shown in the above comparisons. However, as reported in Tang et al. [40], these verifier-free, variational-inference-based methods consistently underperform the standard verifier-based R1-Zero approach. In contrast, our formulation matches or outperforms the verifier-based baseline. One possible explanation is that our method exactly recovers the original verifier-based objective under the single-correct-answer assumption, whereas JLB and LaTRO optimize subtly different objectives. For example, JLB effectively uses reward of = log πθ(yx, z), as highlighted in the gradient expressions above. Another distinction lies in the weighting of the reference answer term θ log πθ(yx, z). In our method, this term is weighted by the probability πθ(yx, z), which is the likelihood of the reference answer given the sampled reasoning trace. In contrast, both JLB and LaTRO use fixed weight of 1, thereby increasing the probability of regardless of the quality of the reasoning trace z. We hypothesize that this behavior could promote poor reasoning. For instance, suppose the model generates the reasoning trace ... minus 2 apples, finally resulting in total of 7 apples when the correct answer is 6. The JLB and LaTRO objectives would still push the model to output 6 from that flawed trace, reinforcing mismatch between reasoning and answer. Our method avoids this by down-weighting contributions from low-quality traces. 2.4 How to handle the tokenization at patching point? critical consideration when extracting reasoning traces from model responses (z, y) and subsequently replacing with stems from the fact that LLMs operate on token sequences, not raw text strings. While human-readable outputs (e.g., ...<answer> boxed{...} </answer> as in Template 1) suggest splitting reasoning traces at specific text patterns like <answer>, such textbased splitting strategy may cause tokenization inconsistencies. For example, the > character might be tokenized differently depending on its surrounding context in versus y. While one potential solution is to introduce special tokens to enforce consistent tokenization boundaries, these novel tokens could harm model performance due to their absence from the base models pretraining vocabulary. Instead, we resolve this by defining the end of at the token corresponding to <answer (i.e., without >), leveraging the fact that the pattern r> does not appear in standard tokenizer vocabularies. This ensures consistent token-space alignment between sampling and optimization in RL, avoiding instability due to off-policy mismatches. Notably, this approach is operationally equivalent to setting <answer (instead of <answer>) as the stop word during sampling, mechanism natively supported by modern inference engines like vLLM [16]. In this case, we can sample reasoning traces directly, instead of first generating the full response (z, y) and then extracting post hoc."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Setup Training. Following the zero setting widely adopted in recent work [10, 14, 24, 27], we directly finetune the base LLM, skipping the intermediate stage of supervised fine-tuning (SFT). We implement our RL training pipeline using Oat [22] by instantiating their base modules and incorporate our algorithmic changes. Our experiments are conducted using Qwen3 [41] base models across multiple scales, including 1.7B, 4B, and 8B parameters. We adopt the prompt template shown in Template 1. We do not employ KL regularization losses or KL penalties in rewards, as recent studies suggest that removing KL terms does not have significant impact [24, 14]. As result, our method does not require maintaining reference model in memory. Template 1 (for Ours). <im_start>usern{question}nPlease reason step by step, and put your final answer within <answer> boxed{} </answer>.<im_end>n<im_start>assistantn For the 1.7B and 4B models, we conduct fine-tuning for approximately 4,000 policy gradient steps; for the 8B models, we fine-tune for around 3,000 policy gradient steps. During each step, the 5 Table 1: Accuracy comparison on the MMLU-Pro benchmark. Method Len. Avg. CS Math Chem Eng Law Bio Health Phys Bus Phil Econ Other Psy Hist Qwen3-1.7B-Base Qwen3-1.7B (w/o thinking) Qwen3-1.7B (w/ thinking) Qwen3-1.7B-Base-Verifier Qwen3-1.7B-Base-VeriFree 618 33.3 34.6 38.9 893 46.1 49.5 64.4 3904 52.0 56.1 76.4 875 47.0 48.8 64.4 856 46.9 46.8 64.1 32.2 21.0 17.3 56.1 33.5 48.0 35.9 22.9 64.9 38.0 57.6 27.0 21.9 67.9 47.7 52.7 38.1 18.7 62.9 41.2 51.7 41.8 20.0 64.0 39.7 32.0 38.5 21.8 45.7 28.4 44.4 21.0 49.7 53.5 33.7 53.9 36.4 51.6 31.2 57.5 61.3 38.9 64.5 42.5 59.2 32.3 51.9 54.9 31.9 55.2 38.6 53.3 30.2 52.1 55.6 29.5 53.1 37.5 53.0 29. Qwen3-4B-Base Qwen3-4B (w/o thinking) Qwen3-4B (w/ thinking) Qwen3-4B-Base-Verifier Qwen3-4B-Base-VeriFree 825 47.2 42.9 67.1 838 60.0 65.9 79.1 3456 62.7 70.0 84.8 921 63.0 66.1 81.3 1241 63.5 64.4 82.2 55.5 40.0 22.5 56.9 43.6 65.8 45.7 29.0 76.6 57.0 66.6 38.6 28.7 81.3 60.4 69.7 52.8 29.1 79.8 62.8 70.1 55.6 30.7 81.7 59.2 55.4 54.9 27.5 52.7 34.3 48.6 34.7 65.1 66.7 48.9 69.2 52.1 64.3 44.6 67.4 69.2 53.7 75.1 57.8 67.9 49.6 67.6 71.2 48.5 73.1 52.8 68.5 45.4 71.0 71.0 47.1 71.7 53.4 66.8 47.5 Qwen2.5-7B 519 47.8 48.3 59.5 Qwen2.5-7B-SimpleRL-Zoo 705 51.2 51.2 52.0 Qwen2.5-Math-7B-Oat-Zero 556 40.5 47.6 47.7 481 55.3 56.6 70.4 Qwen2.5-7B-Instruct General-Reasoner-7B 867 58.7 63.4 73.7 613 59.8 61.2 75.0 Qwen3-8B-Base 1032 61.9 65.6 71.9 Qwen3-8B (w/o thinking) 3952 66.9 71.5 83.8 Qwen3-8B (w/ thinking) 594 65.9 63.9 81.8 Qwen3-8B-Base-Verifier 776 67.2 71.5 85.3 Qwen3-8B-Base-VeriFree 44.4 33.4 25.1 63.6 50.4 50.2 40.8 30.5 69.5 54.3 46.9 32.1 18.1 53.6 25.7 55.6 42.7 29.8 69.3 55.1 63.3 44.9 35.2 72.0 56.6 66.2 46.7 31.4 75.9 60.4 62.8 46.2 34.7 79.9 66.1 68.0 38.7 39.2 85.2 72.1 71.1 56.9 35.4 81.9 64.9 73.5 55.7 37.3 81.9 64. 48.0 55.9 34.7 60.6 46.0 58.2 38.3 52.5 57.3 41.9 62.8 52.6 60.8 42.3 49.4 52.9 29.5 54.7 32.8 43.0 22.8 57.9 63.5 41.5 63.4 53.6 62.4 43.6 61.5 66.7 43.1 68.1 52.8 62.8 47.8 62.1 65.9 48.7 69.0 54.3 63.9 47.2 63.7 69.3 55.9 72.9 58.9 67.9 52.5 69.8 73.3 57.5 79.2 66.3 71.8 57.7 71.6 74.1 53.9 74.2 58.4 68.4 54.3 73.1 74.1 54.9 74.8 59.6 67.7 54.6 policy model (i.e., the LLM) generates 8 responses for each question (i.e., group_size=8), with 16 questions processed per step. We use the sampling configurations temperature=1.0, top_p=1, and max_tokens=3000 for the rollout process. The responses are then parsed into reasoning traces and model-predicted answers. We replace the model-predicted answers with the reference answers from the training dataset. Subsequently, single forward pass is executed to compute the conditional probability of the reference answer, conditioned on all preceding tokens including the prompt and the reasoning trace. This procedure introduces only minimal additional computational cost, as the forward pass of the LLM does not require autoregressive decoding and does not require storing intermediate states for backpropagation. All collected samples from each step are used for one optimization step. The training is conducted on single node with 8H100 GPUs. Dataset. To support general reasoning, we begin with the dataset curated by Ma et al. [27], sourced from WebInstruct [49]. To improve data quality and reliability and reduce size, we retain samples with answers that consist of fewer than seven tokens, and use Qwen2.5-72B-Instruct [43] to filter out low-quality and noisy data. This process results in approximately 61,000 data samples spanning diverse domains, which we refer to as WebData. The category distribution is visualized in Fig. 7. Evaluation. In line with prior work [27], we employ multiple-choice questions for evaluation to facilitate verification. To assess general reasoning abilities, we utilize the following benchmarks: MMLU-Pro [42], challenging multi-task understanding benchmark designed to evaluate the capabilities of LLMs across various domains; SuperGPQA [6], large-scale benchmark consisting of graduate-level reasoning questions spanning 285 diverse disciplines; and GPQA [35], which focuses on graduate-level question-answering and is designed to resist shallow pattern-matching and memorization. While our primary focus is not on enhancing mathematical abilities, we also evaluate math reasoning using suite of standard math reasoning benchmarks. This suite includes MATH-500 [13], OlympiadBench [11], Minerva Math [18], GSM8K [5], AMC and AIME24 [19]. We utilize Math-Verify2 to check for answer equivalence. Except for AIME24, where we employ temperature=1.0 and repeat each question 32 times, all other benchmarks are evaluated using temperature=0.0. We use max_tokens=8192 for all evaluations. Baselines. Our primary baseline, denoted Verifier, is verifier-based approach using the verifier from Ma et al. [27]. The verifier is initialized from Qwen2.5-Math-1.5B [44] and fine-tuned on data generated by Gemini 2.0 Flash to assess equivalence between the reference and predicted answers, conditioned on the question. We apply Dr. GRPO [24] as the optimization algorithm for the baseline, ensuring that all other settings are consistent with our approach. Following Ma et al. [27], the reward definition incorporates additional factors beyond verifier correctness, including format compliance and the length of generated answers. If the format is incorrect (e.g., missing boxed{} in the model 2https://github.com/huggingface/Math-Verify 6 Table 2: Accuracy comparison on the SuperGPQA benchmark. Method Len. Avg. Eng. Med. Sci. Phil. M.S. Econ. Mgmt. Socio. L/A Hist. Agron. Law Edu. Qwen3-1.7B-Base Qwen3-1.7B (w/o thinking) Qwen3-1.7B (w/ thinking) Qwen3-1.7B-Base-Verifier Qwen3-1.7B-Base-VeriFree 997 17.4 17.7 18.6 16.0 27.4 27.3 20.5 1152 23.3 22.6 22.8 24.3 30.3 31.2 24.3 4799 23.6 21.8 25.3 23.6 33.1 33.7 29.6 1049 24.5 26.0 23.9 24.4 30.8 26.8 26.9 964 24.8 25.7 24.7 24.9 26.5 30.2 25. Qwen3-4B-Base Qwen3-4B (w/o thinking) Qwen3-4B (w/ thinking) Qwen3-4B-Base-Verifier Qwen3-4B-Base-VeriFree 902 24.7 25.7 23.6 26.0 23.6 25.4 28.8 1397 31.6 32.0 31.5 32.3 37.5 36.1 37.8 4568 31.7 30.7 33.2 32.1 41.2 31.7 41.7 1045 34.3 35.4 35.5 34.5 39.2 41.0 39.1 1451 35.1 36.3 34.5 36.9 35.7 37.1 39.1 Qwen2.5-7B 716 23.8 24.2 27.0 21.8 28.8 31.2 27.6 Qwen-2.5-7B-SimpleRL-Zoo 850 26.3 26.4 30.5 23.8 32.6 32.2 33.0 638 21.3 23.1 16.4 21.5 23.1 21.5 25.9 Qwen2.5-Math-7B-Oat-Zero 604 28.4 27.7 32.2 27.6 33.7 32.2 32.4 Qwen2.5-7B-Instruct General-Reasoner-7B 1047 30.8 31.5 32.2 29.9 35.2 41.5 38.4 825 31.0 31.3 34.0 30.6 36.0 37.1 34.7 Qwen3-8B-Base 1638 32.4 32.6 36.5 31.2 39.5 42.0 37.7 Qwen3-8B (w/o thinking) 4995 35.0 33.6 42.1 33.5 44.4 37.6 44.2 Qwen3-8B (w/ thinking) 713 37.1 38.2 39.5 37.2 39.5 39.5 43.0 Qwen3-8B-Base-Verifier 951 38.0 38.3 39.1 39.6 37.5 42.9 41.8 Qwen3-8B-Base-VeriFree 22.4 27.5 27.5 27.0 27.9 28.4 33.7 35.9 36.7 38.3 29.1 31.9 27.2 32.9 33.1 37.5 37.3 42.7 40.1 41. 23.1 15.5 11.1 23.8 19.0 18.1 32.2 19.0 18.0 26.6 18.9 16.6 28.0 20.4 15.9 19.6 16.4 16.8 33.6 24.3 20.6 32.9 24.5 22.4 37.1 26.6 22.3 31.5 24.7 22.0 22.4 20.8 20.2 28.7 24.1 21.4 25.2 17.7 15.9 32.9 24.5 22.1 35.0 25.5 22.7 35.0 24.2 20.0 38.5 25.0 22.6 42.7 27.9 24.9 38.5 28.9 24.8 44.8 28.6 23.3 18.1 20.8 25.0 22.3 22.9 20.6 28.3 30.9 33.8 33.0 24.5 27.2 21.4 29.7 28.9 28.5 33.2 37.9 34.2 33. 20.6 21.3 24.5 28.1 26.2 31.6 22.6 27.3 25.0 28.9 25.6 24.0 31.4 33.5 35.7 35.3 33.1 35.3 33.2 34.1 27.4 30.2 29.6 32.9 18.8 24.8 30.6 32.4 32.5 35.5 31.4 36.4 34.3 38.4 38.7 40.7 34.8 38.2 36.3 38.6 response), negative reward of -0.5 is applied. Moreover, length penalty of -0.05 min(10, abs(length_of_correct_answer - length_of_answer)) is added. We also report the results for the base and the instruct models of Qwen-3-1.7B/4B/8B [41] and Qwen2.5-7B [43], as well as the checkpoints released by Qwen2.5-7B-SimpleRL-Zoo [50], Qwen2.5Math-7B-Oat-Zero [24], and General-Reasoner-7B [27].3 Notably, Qwen3 integrates both thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) within unified framework. We report results of both modes on Qwen3 instruct models. 3.2 Main Results VeriFree improves general reasoning capabilities. We begin by evaluating the effectiveness of VeriFree in enhancing the general reasoning capabilities of LLMs using the MMLU-Pro and SuperGPQA benchmarks. Table 1 presents detailed comparison across model scales and domains in the MMLU-Pro benchmark. Starting from base models, applying RL with VeriFree yields substantial gains in average accuracyranging from 12% to 40%demonstrating that VeriFree effectively fine-tunes LLMs to improve general reasoning performance. Similar improvements are observed on the SuperGPQA benchmark, as shown in Table 2, where VeriFree consistently enhances the performance of base models by significant margin. Notably, VeriFree achieves performance comparable to, or even surpassing, that of the instruct model in thinking mode and the Verifier baseline (i.e., the RL-tuned model learned with an additional modelbased verifier)without relying on any explicit verification signals. In addition to accuracy gains, we also observe an increase in response length after tuned by VeriFree, suggesting that the model explores longer reasoning traces to arrive at more accurate answersa behavior reminiscent of DeepSeek-R1-Zero [10]. Results on the GPQA benchmark are provided in Appendix due to space constraints. VeriFree leads to better learning efficiency. We compare VeriFree with the baseline that learns from model-based verifier reward (i.e., Verifier). As shown in Fig. 4 (Left), VeriFree consistently outperforms the baseline, achieving higher accuracy with fewer training steps. We attribute this improved learning efficiency to reduced gradient variance, enabled by VeriFrees continuous reward signals and the RLOO objective. While both approaches optimize the same reward signal in expectation, VeriFree provides more stable and informative policy gradients, which accelerate convergence and leads to better final performance. 3Qwen2.5 and Qwen3 use different naming conventions in their official release. For base models: Qwen2.5 has no suffix (e.g., Qwen2.5-7B), whereas Qwen3 adds -Base (e.g., Qwen3-8B-Base). For instruct models: Qwen2.5 uses -Instruct (e.g., Qwen2.5-7B-Instruct), while Qwen3 omits the suffix (e.g., Qwen3-8B). We follow these conventions consistently in the paper. Figure 4: Left: MMLU-Pro accuracy of VeriFree and the baseline fine-tuned from Qwen3-8B base model along training steps. The curve is smoothed by moving average with an interval of 384. Right: The dynamics of MMLU-Pro evaluation accuracy and average model confidence πθ(yx, z) along training based on Qwen3-8B base model. Raw data points are depicted with more transparency, while smoothed data using Gaussian filter is shown with less transparency for emphasis. Darker colors represent larger training steps. Model confidence is good reasoning capability proxy. Our analysis based on Qwen3-8B base model reveals strong positive correlation (ρ = 0.82) between MMLU-Pro accuracy and the average model confidence πθ(yx, z) during training (Fig. 4, Right). This empirically demonstrates that the models self-estimated confidence in the correct answer, i.e., πθ(yx, z), serves as an effective metric for quantifying emergent reasoning capabilities in LLMs. VeriFree learns transferable reasoning skills. To evaluate the transferability of reasoning acquired through VeriFree, we train model on dataset with all math-related examples removed, and assess its performance on both general and math-specific benchmarks. As shown in Fig. 5, VeriFree not only improves reasoning performance on general tasks, as expected, but also demonstrates strong transfer to math benchmarksdespite the absence of math supervision during training. This highlights VeriFrees ability to induce general reasoning capabilities that extend across domains. 3.3 Ablation Study Figure 5: Reasoning transfer to math without math training. VeriFree enhances reasoning transfer: when trained only on non-math data, the model improves on general benchmarks and effectively transfers to math-specific tasks. To systematically evaluate method components and offer comprehensive understanding of VeriFree, we conduct ablation studies based on Qwen3-1.7B base models as follows. Effects of extraction strategy for reasoning trace z. Our method requires precise separation between reasoning path and answer to enable answer replacement. While human-readable splits using <answer> seem intuitive, we instead define to end at <answer (omitting >), ensuring consistent tokenization boundaries (see Sec. 2.4). We compare with variant using text-based splitting (denoted as VeriFree w/o token split) on Qwen3-1.7B via MMLU-Pro  (Fig. 6)  . Our tokenization-aware approach achieves superior convergence, while the variant suffers optimization instability due to effectively introducing off-policy data. Effects of RLOO. As observed in Fig. 6 (Left), removing RLOO leads to consistent drop in performance throughout training, with final accuracy more than 3% lower than that of the full method. This highlights the importance of RLOO in stabilizing learning and guiding the model toward better generalization. Without RLOO, the model converges prematurely and fails to reach the same peak accuracy. Effects of equivalence class. As mentioned in Sec. 2.1, verifier-based RL typically assesses answer correctness as rewards. Correct answers within specific class often form an equivalence class. Our method, however, utilizes model confidence by focusing on single reference answer for given question and the models reasoning trace. To explore the potential advantages of integrating an equivalence class into our approach, we conducted ablation studies as follows. We employed model 8 Figure 6: Left: MMLU-Pro Evaluation Accuracy of VeriFree against ablation variants (w/o RLOO, w/o tokenization-aware split strategy) along training steps. Note that all these models are based on Qwen3-1.7B base models. Right: Effects of introducing the equivalent class to VeriFree on model performance. fine-tuned on the MATH-12k dataset [12, 20] from Qwen3-8B base model through Dr. GRPO [24] with rule-based verification to sample answers on MATH-12k, subsequently verifying answer correctness using Math-Verify. This approach enabled us to create an extended dataset with set of equivalent correct answers for each question. We then fine-tuned Qwen3-1.7B base models using our method on both the original and the extended MATH-12k datasets incorporating equivalence classes. These models are evaluated on GSM8K, MATH-500, Minerva Math, and OlympiadBench to assess the impact of including equivalence classes. The results, shown in Fig. 6 (Right), indicate that considering equivalence classes in our method offers slight performance improvements, aligning with our expectations. This highlights minor limitation of our current formulation and motivates future work on algorithms that can better leverage answer equivalence."
        },
        {
            "title": "4 Related Works",
            "content": "DeepSeek-R1-Zero-like reinforcement learning. DeepSeek-R1-Zero [10], Tülu [17] and OpenAIs o1 [29] recently demonstrated that applying RL to learn directly from binary verification-based rewards can be extremely powerful in enhancing the reasoning capabilities of base LLMs. Since then several works have reproduced R1-Zero-like training on smaller scales [50, 31, 23, 14, 26, 25]. The aforementioned works all focus on math and coding, domains where verifiable rewards are readily available. By contrast, our work aims to extend the R1-Zero-like training paradigm to general domains where verifiable rewards are not available. Reasoning beyond verifiable domains. Previous work on reasoning without access to verifiable rewards has been based around employing an additional LLM to act as proxy verifier or reward model. NaturalReasoning [47] introduces large, multi-domain dataset and presents baselines trained using RFT [48] and DPO [33], leveraging second LLM as reward model, while Su et al. [38] and GeneralReasoner [27] similarly incorporate separate LLM to serve as verifier. X-Reasoner [21] also investigates general reasoning but circumvents the lack of rule-based verification by dropping the R1-Zerostyle training paradigm, instead training via SFT [9] on responses sampled from more capable models. Self-improving language models. Several works have explored training LLMs using signals based on the models own outputs. Yuan et al. [46] propose to prompt the model to judge and rank different responses and select the best and worst for preference learning. Chen et al. [3] leverage the DPO implicit rewards for more efficient and robust self-alignment via iterative DPO [33]. Zuo et al. [52] use majority voting to construct self-labeled rewards for RL to further improve well-trained models during test time, which can be understood as form of sharpening [15]. Another line of research [32, 4, 40] approaches LLM reasoning from the direction of variational optimization, treating the reasoning trace as latent variable. Despite starting from different viewpoint our method has interesting and close connections to this perspective which we discuss in detail in Sec. 2.3."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we rethink reinforcement learning with verifiable rewards (RLVR) for LLMs from novel perspective. By leveraging the gradient equivalence under the unique answer assumption, we derive new optimization objective that eliminates the need for explicit verification. Our proposed 9 method, VeriFree, is particularly well-suited for general reasoning tasks, where rule-based verifiers are infeasible and model-based verifiers are both expensive and vulnerable to reward hacking. Through extensive experiments and ablations, we demonstrate the effectiveness and robustness of VeriFree on wide range of general reasoning benchmarks. We hope our work offers fresh viewpoint for the LLM RL community and provides practical approach for building future general-purpose reasoners."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [2] George Casella and Christian P. Robert. Rao-blackwellisation of sampling schemes. Biometrika, 83(1):8194, 1996. [3] Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, and Min Lin. Bootstrapping language models with dpo implicit rewards. In International Conference on Learning Representations (ICLR), 2025. [4] Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, et al. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. arXiv preprint arXiv:2411.04282, 2024. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [6] Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. [7] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. [8] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024. [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL https://arxiv.org/abs/2503.24290. 10 [15] Audrey Huang, Adam Block, Dylan Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan Ash, and Akshay Krishnamurthy. Self-improvement in language models: The sharpening mechanism. arXiv preprint arXiv:2412.01951, 2024. [16] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [17] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [18] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. [19] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [20] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=v8L0pN6EOi. [21] Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, and Hoifung Poon. X-reasoner: Towards generalizable reasoning across modalities and domains, 2025. URL https://arxiv.org/ abs/2505.03981. [22] Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, and Min Lin. Oat: research-friendly framework for llm online alignment. https://github.com/sail-sg/oat, 2024. [23] Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion.site/ oat-zero, 2025. Notion Blog. [24] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [25] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. Notion Blog. [26] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [27] Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. Generalreasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025. [28] A. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, 1999. URL https://api.semanticscholar.org/CorpusID:5730166. [29] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. [30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [31] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [32] Du Phan, Matthew Douglas Hoffman, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif Saurous. Training chain-ofthought via latent-variable inference. Advances in Neural Information Processing Systems, 36: 7281972841, 2023. [33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [34] Jette Randlov and Preben Alstrøm. Learning to drive bicycle using reinforcement learning and shaping. pages 463471, 01 1998. [35] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [38] Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. [39] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. [40] Yunhao Tang, Sid Wang, and Rémi Munos. Learning to chain-of-thought with jensens evidence lower bound. arXiv preprint arXiv:2503.19618, 2025. [41] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/. [42] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [43] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [44] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [45] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 12 [46] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. International Conference on Machine Learning, 2024. doi: 10.48550/arXiv.2401.10020. [47] Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8 challenging questions. arXiv preprint arXiv:2502.13124, 2025. [48] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. [49] Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 37:9062990660, 2024. [50] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. [51] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. [52] Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "Table of Contents",
            "content": "A Theoretical Analysis A.1 Derivation of Gradient Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Proof of Lower Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Off-policy Gradient Estimators"
        },
        {
            "title": "C Dataset Details",
            "content": "D Extended Empirical Results Case Study Broader Impact"
        },
        {
            "title": "A Theoretical Analysis",
            "content": "A.1 Derivation of Gradient Estimators 14 14 15 16 16 17 21 Here we provide derivations for Eq. (5)the gradient estimator of VeriFree. We also include the corresponding derivation for the standard verifier-based gradient estimator for completeness. Proof. The gradient estimator for JVerifier is derived as follows: θJVerifier(θ; x, y) = θEzπθ(x) (cid:88) = θ (cid:104) Eyπθ(x,z) (cid:2)RVerifier(y; y)(cid:3)(cid:105) RVerifier(y; y)πθ(yx, z)πθ(zx) z,y = = (cid:88) z,y (cid:88) z,y RVerifier(y; y)(cid:2)πθ(yx, z)θπθ(zx) + πθ(zx)θπθ(yx, z)(cid:3) RVerifier(y; y)(cid:2)πθ(yx, z)πθ(zx)θ log πθ(zx) + πθ(zx)πθ(yx, z)θ log πθ(yx, z)(cid:3) = Ezπθ(x) (cid:20) Eyπθ(x,z) (cid:104) RVerifier(y; y)(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3)(cid:105)(cid:21) The gradient estimator for JVeriFree is derived as follows: θJVeriFree(θ; x, y) = θEzπθ(x)[RVeriFree(z; x, y)] = θEzπθ(x)[πθ(yx, z)] πθ(yx, z)πθ(zx) = θ (cid:88) = = (cid:88) (cid:88) (cid:2)πθ(yx, z)θπθ(zx) + πθ(zx)θπθ(yx, z)(cid:3) (cid:2)πθ(yx, z)πθ(zx)θ log πθ(zx) + πθ(zx)πθ(yx, z)θ log πθ(yx, z)(cid:3) = Ezπθ(x) = Ezπθ(x) (cid:104) (cid:104) πθ(yx, z)(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3)(cid:105) RVeriFree(z; x, y)(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3)(cid:105) A.2 Proof of Lower Variance Here we provide full proof of Theorem 1, the reduced variance property of VeriFree. We show that the policy gradient estimator derived from JVeriFree(θ; x, y) has variance less than or equal to that of the estimator derived from JVerifier(θ; x, y) for any given x, y. The same relationship will hold for the global objectives averaged over (x, y) D. Proof. The global objective functions are: (cid:104) Ezπθ(x) JVerifier(θ) = E(x,y)D JVeriFree(θ) = E(x,y)D (cid:104) Ezπθ(x) (cid:2)1{y = y}(cid:3)(cid:105)(cid:105) (cid:104) Eyπθ(x,z) (cid:2)πθ(yx, z)(cid:3)(cid:105) For given (x, y) D, the single-sample Monte Carlo gradient estimators are: ˆGVerifier(x, y, z, y) = 1{y = y}(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3) where πθ(x), πθ(x, z), and ˆGVeriFree(x, y, z) = πθ(yx, z)(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3) where πθ(x). The proof relies on the law of total variance and the relationship between ˆGVerifier and ˆGVeriFree. First, we show that ˆGVeriFree(x, y, z) is the conditional expectation of ˆGVerifier(x, y, z, y) given x, y, z. The expectation is taken over πθ(x, z): Eyπθ(x,z) = Eyπθ(x,z) (cid:2) ˆGVerifier(x, y, z, y)x, y, z(cid:3) 1{y = y}(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3)(cid:12) (cid:104) (cid:12)x, y, (cid:12) (cid:105) (cid:88) (cid:104) πθ(yx, z) 1{y = y}(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3)(cid:105) = = πθ(yx, z)(cid:2)θ log πθ(zx) + θ log πθ(yx, z)(cid:3) = ˆGVeriFree(x, y, z) (cid:2) ˆGVerifier(x, y, z, y)(cid:12) We denote Eyπθ(x,z) the following, since and are already given and fixed. Thus, we have (cid:2) ˆGVerifier(x, y, z, y)(cid:3) = ˆGVeriFree(x, y, z) (cid:12)x, y, z(cid:3) as Eyz Eyz (cid:2) ˆGVerifier(x, y, z, y)(cid:3) for brevity in (8) 15 The law of total variance states that for random variable and conditioning variables S, Var(W ) = ES[Var[W S]] + VarS[E[W S]]. Let = ˆGVerifier(x, y, z, y). Given and y, the sources of randomness for ˆGVerifier are and y. Let = be the conditioning variables. The randomness in ˆGVerifier given comes from πθ(x, z). Applying the law: (cid:2) ˆGVerifier(x, y, z, y)(cid:3)(cid:105) (cid:2) ˆGVerifier(x, y, z, y)(cid:3)(cid:105) The expectation Ez is over πθ(x). The conditional variance Varyz and expectation Eyz are over πθ(x, z) for fixed x, y, z. Substituting the result from Eq. (8) into the law of total variance: (cid:104) (cid:2) ˆGVerifier(x, y, z, y)(cid:3) = Ez (cid:104) +Varz Varyz Varz,y Eyz Varz,y (cid:104) (cid:2) ˆGVerifier(x, y, z, y)(cid:3) = Ez Varyz (cid:2) ˆGVerifier(x, y, z, y)(cid:3)(cid:105) + Varz (cid:2) ˆGVeriFree(x, y, z)(cid:3) The second term, Varz The first term, Ez always non-negative, Varyz (cid:104) Varyz (cid:2) ˆGVeriFree(x, y, z)(cid:3), is the definition of the variance of the estimator ˆGVeriFree. (cid:2) ˆGVerifier(x, y, z)(cid:3)(cid:105) , is an expectation of variance. Since variance is (cid:2) ˆGVerifier(x, y, z)(cid:3) 0. Therefore, its expectation is also non-negative: (cid:104) Ez Varyz (cid:2) ˆGVerifier(x, y, z)(cid:3)(cid:105) 0 Thus, we have: Varz,y (cid:2) ˆGVerifier(x, y, z, y)(cid:3) = (a non-negative term) + Varz (cid:2) ˆGVeriFree(x, y, z)(cid:3) This implies: Varz,y (cid:2) ˆGVerifier(x, y, z, y)(cid:3) Varz (cid:2) ˆGVeriFree(x, y, z)(cid:3) The variance of the policy gradient estimator ˆGVeriFree is less than or equal to that of ˆGVerifier. This is an instance of Rao-Blackwellization, where analytically integrating out source of randomness (the sampling of y) by using its conditional expectation reduces variance. Off-policy Gradient Estimators In the main paper we provide an expression for the gradient estimator when the data is fully on-policy. VeriFree is also fully compatible with PPO-style gradient clipping for the case when data is reused to improve sample efficiency. In this case the gradient estimator is: θJVeriFree(θ) = (cid:88) (cid:34) zi (cid:88) 1 Clip (cid:26) πθ (zi,t x, zi,<t) πθold (zi,t x, zi,<t) (cid:27) Ai + (cid:88) i=1 t=1 where πθold is the sampling policy, Ai = πθold(yx, zi) 1 G1 πθold(yx, zi), and Clip{} denotes the PPO clipping operation. t= Clip (cid:26) πθ (y πθold (y x, zi) x, zi) (cid:27) (cid:35) , Ri (cid:80) j=i πθold(yx, zj), Ri ="
        },
        {
            "title": "C Dataset Details",
            "content": "The category distribution in WebData (our training data) is visualized in Fig. 7."
        },
        {
            "title": "D Extended Empirical Results",
            "content": "In Tables 1 and 2, we provide detailed benchmark results for MMLU-Pro and SuperGPQA using domain name abbreviations. The full nomenclature is as follows: MMLU-Pro  (Table 1)  : CS (Computer Science), Math (Mathematics), Chem (Chemistry), Eng (Engineering), Law (Law), Bio (Biology), Health (Health), Phys (Physics), Bus (Business), Phil (Philosophy), Econ (Economics), Other (Other), Psy (Psychology), Hist (History). 16 Figure 7: Category distributions in WebData. breakdown of the grouped category (right) shows detailed distribution of various domains with fewer data samples. SuperGPQA  (Table 2)  : Eng. (Engineering), Med. (Medicine), Sci. (Science), Phil. (Philosophy), M.S. (Military Science), Econ. (Economics), Mgmt. (Management), Socio. (Sociology), L/A (Literature and Arts), Hist. (History), Agron. (Agronomy), Law (Law), Edu (Education). In Table 3, we present an accuracy comparison across six math evaluation benchmarks and GPQA. Models trained with VeriFree demonstrate consistent and significant improvements over the base models, further validating the effectiveness of our approach. Table 3: Accuracy comparison on math evaluation suite and GPQA. Method AIME24 AMC GSM8K MATH-500 Minerva Olympiad GPQA Acc. Len. Acc. Len. Acc. Len. Acc. Len. Acc. Len. Acc. Len. Acc. Len. Qwen3-1.7B-Base Qwen3-1.7B (w/o thinking) Qwen3-1.7B (w/ thinking) Qwen3-1.7B-Base-Verifier Qwen3-1.7B-Base-VeriFree 1.7 1287 40.0 1001 71.4 343 58.8 1094 19.5 1071 23.3 1737 17.7 1355 10.9 2552 40.0 2291 83.3 312 72.8 1021 27.9 776 39.1 1970 19.2 1885 20.9 7855 57.5 5973 89.0 2220 77.4 4525 36.8 5606 40.9 6497 17.7 7104 8.4 2317 42.5 1712 81.7 405 66.2 1057 30.5 1357 29.8 1885 36.4 1317 21.0 867 29.8 1484 30.3 1147 10.7 1783 37.5 1522 76.2 447 63.8 972 Qwen3-4B-Base Qwen3-4B (w/o thinking) Qwen3-4B (w/ thinking) Qwen3-4B-Base-Verifier Qwen3-4B-Base-VeriFree 29.4 1027 39.6 1202 24.7 1275 5.2 1312 50.0 1001 73.1 393 73.4 724 20.4 3146 67.5 2047 92.1 308 82.2 1143 41.2 822 49.5 2593 29.8 2127 33.0 7750 62.5 6123 92.9 2261 84.4 4370 41.5 5352 47.3 6432 31.8 6504 15.6 2407 57.5 1566 72.5 377 73.8 1023 24.6 1088 45.9 1576 44.4 1266 16.9 2706 65.0 1904 87.5 682 74.8 1269 25.4 1444 44.9 1899 42.4 1619 2. 26.8 819 30.2 963 24.2 595 869 32.5 922 84.6 260 63.2 582 Qwen2.5-7B 36.4 795 43.4 1083 23.7 990 Qwen-2.5-7B-SimpleRL-Zoo 15.5 1285 57.5 1097 92.1 331 78.6 690 33.1 655 43.0 860 24.7 722 Qwen2.5-Math-7B-Oat-Zero 28.3 1115 65.0 846 90.8 386 79.0 652 37.9 690 39.9 1123 31.8 643 11.2 993 52.5 986 91.7 318 78.2 649 Qwen2.5-7B-Instruct 23.5 968 39.3 1303 34.8 1199 13.1 1363 52.5 1083 81.7 408 74.6 858 General-Reasoner-7B 6.5 1213 65.0 917 91.7 304 77.0 671 Qwen3-8B-Base 38.2 884 41.3 1189 38.9 887 24.6 2897 62.5 1919 93.3 293 82.2 1087 36.4 751 49.5 2422 27.8 2204 Qwen3-8B (w/o thinking) 31.0 7789 62.5 6201 95.3 2203 83.0 4414 43.8 5410 45.2 6554 37.4 6681 Qwen3-8B (w/ thinking) 17.8 1954 57.5 1265 93.4 307 76.2 784 39.0 698 41.8 1201 43.9 733 Qwen3-8B-Base-Verifier 25.2 2461 67.5 1542 91.6 375 80.8 1024 30.5 902 50.2 1778 44.4 1080 Qwen3-8B-Base-VeriFree"
        },
        {
            "title": "E Case Study",
            "content": "In this section, we provide some examples of reasoning traces produced by the model trained by our method on training questions. And we also provide an example with both our models and base models responses on the test dataset. 17 Example question in training dataset (category: Chemistry) Prompt: <im_start>user Label or placard colored black and white with skull and crossbones falls under which class Please reason step by step, and put your final answer within <answer> boxed </answer>.<im_end> <im_start>assistant Our Models Response: label or placard colored black and white with skull and crossbones indicates the class of hazardous materials known as **Class 6**. Class 6 is specifically divided into two divisions: - **Division 6.1**: Toxic substances. These are materials that can cause injury or death if ingested or inhaled. - **Division 6.2**: Infectious substances. These are materials that contain pathogens and can cause disease in humans or animals. The skull and crossbones symbol is universally recognized as representing toxicity, which corresponds to **Division 6.1** of Class 6 hazardous materials. <answer> boxed{Class 6} </answer> Reference Answer: Class 6 18 Example question in training dataset (category: Physics) Prompt: <im_start>user For what type of object is the period-luminosity relation used for determining distances? Please reason step by step, and put your final answer within <answer> boxed{} </answer>.<im_end> <im_start>assistant Our Models Response: The period-luminosity relation is fundamental tool in astronomy used to determine distances, particularly for specific type of star known as Cepheid variables. Cepheid variables are pulsating stars that exhibit regular pattern of expansion and contraction. This pulsation causes them to vary in luminosity, and the key insight about these stars is that the period of their luminosity variation is directly related to their intrinsic luminosity (absolute magnitude). In other words, the longer the period of pulsation, the more intrinsically luminous the star is. This period-luminosity relation was first discovered by Henrietta Leavitt in 1908 by studying Cepheid variables in the Large Magellanic Cloud. Because Cepheids are so bright intrinsically, they can be used as \"standard candles\" to measure distances within our galaxy and to nearby galaxies. Heres how the period-luminosity relation is used to determine distances: 1. **Observe the Period**: First, astronomers measure the period of the stars brightness variation, which is the time it takes for the star to go from brightest to dimmest and back to brightest. 2. they **Determine Intrinsic Luminosity**: Using the period-luminosity relation, can then determine the stars intrinsic luminosity. This relation is usually given in form where graph or formula relates period to absolute magnitude (a measure of intrinsic brightness). 3. **Compare to Apparent Luminosity**: The apparent magnitude (how bright the star appears from Earth) can be measured directly. 4. **Calculate Distance**: Using the inverse square law for light, which relates the intrinsic luminosity, apparent brightness, and distance, astronomers can then calculate the distance to the Cepheid variable. This method is crucial for measuring distances in astronomy because it provides way to gauge how far away these stars are with high degree of accuracy. The distances determined using Cepheid variables are themselves used to calibrate other distance measurement techniques, such as those involving supernovae. Given these details, the type of object for which the period-luminosity relation is used for determining distances is Cepheid variables. <answer> boxed{Cepheid variables} </answer> Reference Answer: cepheid variable stars 19 Example question in MMLU-Pro benchmark (category: Psychology) Prompt (Here we ignore the prompt template and only show the question for brevity): During the second stage of Kohlbergs preconventional level of moral development, children obey rules because: Options are: A. they are taught that rules are unbreakable. B. they enjoy following the rules. C. they are rewarded for following rules. D. they feel they have personal duty to uphold rules and laws. E. doing so helps them satisfy their personal needs. F. they understand the concept of fairness and justice. G. doing so helps them avoid punishment. H. \"\"\"everyone else is doing it.\"\"\" I. they believe in the inherent goodness of rules. J. they fear societal disapproval. Our Models Response: In Kohlbergs theory of moral development, the preconventional level is the first stage of moral reasoning, and it is primarily focused on self-interest. This level is divided into two stages: 1. **Stage 1: Obedience and Punishment Orientation** In this stage, children obey rules to avoid punishment. Their reasoning is fear-based, and they view rules as set by authority figures. The focus is on the consequences of following or breaking rules, speci fically the avoidance of punishment. 2. **Stage 2: Individualism and Exchange** In this stage, which is the second stage of the preconventional level, children recognize that different people have different needs and interests. They obey rules only when it is in their own interest or when they can benefit from doing so. This stage is often described as the \"whats in it for me?\" stage. Children at this level understand the concept of exchange and may follow rules if it leads to reciprocal benefit. Now, lets evaluate the options: (A) they are taught that rules are unbreakable. - This does not align with either stage of the preconventional level. (B) they enjoy following the rules. - Enjoyment of rules is more characteristic of conventional levels of moral development. (C) they are rewarded for following rules. - While this could be somewhat related to Stage 2, the primary focus in Stage 2 is on personal benefit rather than external rewards. (D) they feel they have personal duty to uphold rules and laws. - This is more aligned with the conventional level, specifically the intermediate stage. (E) doing so helps them satisfy their personal needs. - This could be seen as related to Stage 2, but it is not the most precise description. (F) they understand the concept of fairness and justice. - Understanding fairness and justice is more associated with the conventional and post-conventional levels. (G) they avoid punishment. - This describes Stage 1 of the preconventional level, not Stage 2. Cont. (H) everyone else is doing it. - This is more related to conformity and is not key characteristic of Kohlbergs stages. (I) they believe in the inherent goodness of rules. - This is more characteristic of the post-conventional level. (J) they fear societal disapproval. - Fear of societal disapproval is more related to the conventional level. The best fit for Stage 2 of the preconventional level is (E) doing so helps them satisfy their personal needs, as it reflects the idea of mutual benefit and personal interest. However, (G) they avoid punishment describes Stage 1 more accurately. Given the typical interpretations of Kohlbergs stages, (E) is the more appropriate choice for Stage 2. But lets double-check: According to Kohlberg, in Stage 2, childrens moral reasoning is based on the notion of exchange and the satisfaction of self-interest. Therefore, (E) is the most accurate option. <answer> boxed{E} </answer> Base Models Response: During the second stage of Kohlbergs preconventional level of moral development, children obey rules because they understand the concept of fairness and justice. This stage is characterized by focus on the consequences of actions and the desire to avoid punishment or gain rewards. Children at this stage begin to recognize that rules are not absolute and can be negotiated based on the situation and the needs of the individual. They start to consider the perspectives of others and the fairness of the rules, which is significant step towards more advanced moral reasoning. Therefore, the correct answer is: boxed{F} Reference Answer: E"
        },
        {
            "title": "F Broader Impact",
            "content": "Improving the general reasoning capabilities of large language models (LLMs) has the potential to significantly expand their applicability across domains such as education, scientific discovery, legal analysis, and decision support. By removing reliance on explicit verifiersoften expensive or domain-specificour approach democratizes access to advanced reasoning systems, enabling broader use in resource-constrained settings. However, improved reasoning can amplify both beneficial and harmful behaviors, depending on the intent and oversight of the user. As such, responsible usage, transparent reporting of model limitations, and value alignment remain critical. We encourage future work that complements our method with safeguards for misuse prevention and performance guarantees under distributional shifts, particularly as general-purpose reasoning models become more widely integrated into real-world workflows."
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "National University of Singapore",
        "Renmin University of China",
        "Sea AI Lab, Singapore",
        "University of Chinese Academy of Sciences",
        "University of Oxford"
    ]
}