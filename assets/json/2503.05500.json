{
    "paper_title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
    "authors": [
        "Nicolas Boizard",
        "Hippolyte Gisserot-Boukhlef",
        "Duarte M. Alves",
        "André Martins",
        "Ayoub Hammal",
        "Caio Corro",
        "Céline Hudelot",
        "Emmanuel Malherbe",
        "Etienne Malaboeuf",
        "Fanny Jourdan",
        "Gabriel Hautreux",
        "João Alves",
        "Kevin El-Haddad",
        "Manuel Faysse",
        "Maxime Peyrard",
        "Nuno M. Guerreiro",
        "Patrick Fernandes",
        "Ricardo Rei",
        "Pierre Colombo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 0 0 5 5 0 . 3 0 5 2 : r EuroBERT: Scaling Multilingual Encoders for European Languages Nicolas Boizard 1,3 Hippolyte Gisserot-Boukhlef 2,3 Duarte M. Alves 4,5 Joao Alves 6 Kevin El-Haddad 1,17 Manuel Faysse 3,14 Maxime Andre Martins 4,5,6 Ayoub Hammal 7,8,9 Caio Corro 8,10,11 Celine Hudelot 3 Emmanuel Malherbe 2 Etienne Malaboeuf 12 Fanny Jourdan 13 Gabriel Hautreux 12 Peyrard 8,15 Nuno M. Guerreiro 3,4,5,6 Patrick Fernandes 4,5,18 Ricardo Rei 6 Pierre Colombo 3,16 1Diabolocom, 2Artefact, 3MICS, CentraleSupelec, Universite Paris-Saclay, 4Instituto Superior Tecnico & Universidade de Lisboa (Lisbon ELLIS Unit), 5Instituto de Telecomunicac oes, 6Unbabel, 7Universite Paris-Saclay, 8CNRS, 9LISN, 10INSA Rennes, 11IRISA, 12CINES, 13IRT Saint Exupery, 14Illuin Technology, 15Universite Grenoble Alpes, Grenoble INP, LIG, 16Equall, 17ISIA Lab, 18Carnegie Mellon University Equal contribution, Ordered alphabetically by the first name, Senior advisor General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework. Contact firstname.lastname@centralesupelec.com, duartemalves@tecnico.ulisboa.pt Website https://huggingface.co/EuroBERT Date March 10,"
        },
        {
            "title": "Introduction",
            "content": "Many important tasks in NLP, including information retrieval, classification, or regression, are built upon general-purpose vector representations. These representations are traditionally obtained from bidirectional encoder models, which aggregate information from the left and right contexts of each token (Devlin et al., 2019; Conneau et al., 2020; He et al., 2023). In contrast, recent advances in generative modeling have shifted the research communitys attention towards unidirectional architectures (Bai et al., 2023; Llama Team, 2024; OLMo et al., 2025). Notably, these efforts have identified several key performance drivers that span architectural advances, data improvements, and increased scale. Yet, despite no apparent barrier to transferring these insights to bidirectional architectures, little effort has been devoted towards this objective, forcing practitioners to depend on older models. In this paper, we introduce refreshed recipe for training general-purpose multilingual encoders, resulting in the EuroBERT family. Our models incorporate recent architectural advances from decoder models (2.1), and are trained on 5T-token multilingual dataset, covering European and widely spoken global languages, along with mathematics and code (2.2). We adopt masked language modeling objective, and employ two-phase EuroBERT: Scaling Multilingual Encoders for European Languages Figure 1: Pareto performance plots for key multilingual tasks, including retrieval (MIRACL), classification (XNLI), and regression (SeaHorse). The blue dotted line represents the Pareto frontier achieved by the EuroBERT model family. training pipeline, adjusting the data distribution in the second training phase to improve downstream performance (2.3). We extensively evaluate the EuroBERT models, comparing to several similarly sized alternatives across suite of tasks representative of real-world encoder applications (3). Our models match or exceed the performance of alternative models, such as XLM-RoBERTa (Conneau et al., 2020) and mGTE-MLM-base (Zhang et al., 2024), on multilingual retrieval, classification and regression tasks (Figure 1), and outperform them on tasks related to code and mathematics (Figure 2). We also examine the impact of our design choices through systematic ablations on several components of our annealing recipe (4). We explore the choice of masking ratio, showing that while higher masking ratios benefit retrieval tasks, lower ratios improve sentence classification. Additionally, we highlight that including data for code and mathematics can improve multilingual retrieval, but degrades classification accuracy. Contrary to expectations, we also find that, when using model-based filters for data selection, mixing data from lower and higher quality thresholds can improve both retrieval and classification. Accompanying this work, we release the EuroBERT family, comprising three models with 210m, 610m and 2.1B parameters. To facilitate future research, we also release intermediate training checkpoints, as well as our training framework."
        },
        {
            "title": "2 EuroBERT: A Refreshed Multilingual Encoder",
            "content": "The EuroBERT models are available in three sizes (210m, 610m, and 2.1B parameters) and closely follow the Llama 3 architecture (Llama Team, 2024) (2.1). They are trained on large multilingual corpus, which includes datasets of code and mathematics (2.2). The training pipeline has two stages, pre-training and annealing, and employs the masked language modeling (MLM) objective (2.3)."
        },
        {
            "title": "2.1 Architecture",
            "content": "The EuroBERT models are based on standard dense transformer (Vaswani et al., 2017), with several architectural changes. Similarly to Llama 2 (Touvron et al., 2023), we remove all biases. Additionally, we incorporate grouped query attention (Ainslie et al., 2023), swish gated linear units (Shazeer, 2020), root mean square layer normalization (Zhang & Sennrich, 2019), and rotary position embeddings (Su et al., 2024).1 1We provide more architecture details in Appendix A. 2 EuroBERT: Scaling Multilingual Encoders for European Languages Figure 2: Pareto performance plots for code-related (CodeSearchNet) and math-related (MathShepherd) tasks. The blue dotted line represents the Pareto frontier achieved by the EuroBERT model family."
        },
        {
            "title": "2.2 Dataset",
            "content": "To train EuroBERT, we construct multilingual 5T-token corpus 4.8T tokens for pretraining and 200B for annealing which includes 15 languages: English, French, German, Spanish, Chinese, Italian, Russian, Polish, Portuguese, Japanese, Vietnamese, Dutch, Arabic, Turkish, and Hindi.2 Following prior work on curriculum learning, we adjust the data distribution to emphasize higher-quality datasets during annealing. Pre-training mixture. We use FineWeb (Penedo et al., 2024) for English, and CulturaX (Nguyen et al., 2024) for multilingual data. We also incorporate parallel data, which can improve cross-lingual transfer (Conneau & Lample, 2019; Reid & Artetxe, 2022; 2023), by concatenating to-English and from-English translation pairs, separated by special <parallel_sep> token. Finally, we incorporate 38 programming languages from The Stack v2 and Proof-Pile-2, which we found to improve multilingual information retrieval (4). Annealing mixture. For annealing, we classified data not seen during pre-training into four quality levels using the EuroLLM classifier (Martins et al., 2024). We then selected data above the third threshold, representing mixture of medium and high quality data. Contrary to our expectations, we found that including data of lower quality improved performance (4). Additionally, we adjusted the data distribution based on multiple ablations (4). Specifically, we decreased the proportion of English while proportionally increasing the remaining languages. We also decreased the amount of code and math data while increasing parallel data."
        },
        {
            "title": "2.3 Training Recipe",
            "content": "Masked language modeling. We choose to pre-train EuroBERT models with 50% masking ratio, following the insights from Wettig et al. (2023), which find that masking 15% and 30% of tokens is sub-optimal, and that larger models benefit from higher masking ratios. For the subsequent annealing phase, however, we lower the masking ratio to 10% based on downstream evaluations (4), aligning with the findings from Yang et al. (2023) and Ankner et al. (2024). Hyperparameters. We employed the Warmup-Stable-Decay (WSD) scheduler (Shen et al., 2024), with linear warm-up phase of 2,000 steps, constant learning rate of 1 104 during pre-training, and cosine scheduler decaying to 0 during the annealing phase. During pre-training, we packed sentences to 2,048 tokens and used Rotary Position 2These languages were selected to balance European and widely spoken global languages, and ensure representation across diverse alphabets and language families. 3We provide further details on our pretraining and annealing datasets in Appendix C. EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "Benchmark",
            "content": "mDeBERTa mGTE XLM-RoBERTa 280m 305m 280m 560m 3.5B EuroBERT 210m 610m 2.1B MIRACL MLDR CC-News Wikipedia XNLI PAWS-X QAM AmazonReviews MassiveIntent"
        },
        {
            "title": "WMT\nSeaHorse\nSummEval",
            "content": "43.7 5 20.0 6 15.8 7 58.9 5 82.0 4 91.9 2 72.1 4 63.7 2 87.3 2 43.0 2 30.2 8 26."
        },
        {
            "title": "Retrieval",
            "content": "93.8 2 73.2 2 71.5 4 94.6 3 89.5 4 91.6 3 92.6 3 58.7 5 65.2 4 70.0 3 60.4 6 72.1 4 80.9 1 91.7 4 93.6 3 96.7 1 95.1 1 95.0 1 94.8 1 73.4 2 75.8 1 72.9 2 69.0 4 76.6 2 76.9 2 95.6 2 96.6 1 96."
        },
        {
            "title": "Classification",
            "content": "78.4 6 89.8 3 68.6 5 62.7 3 87.5 2 76.6 7 84.1 3 86.1 2 88.9 4 92.4 2 92.9 1 67.9 5 73.7 2 75.4 1 62.7 3 64.5 1 64.7 1 87.2 2 88.8 1 88.5 1 79.9 5 84.7 2 86.8 1 89.9 3 92.2 2 93.0 1 69.6 4 72.9 3 73.3 3 63.0 2 64.0 2 64.5 1 87.2 2 87.8 2 88."
        },
        {
            "title": "Regression",
            "content": "37.7 6 55.8 4 43.7 3 34.2 7 39.0 4 44.4 1 31.7 7 34.3 6 35.7 5 26.9 6 38.6 4 30.3 6 40.5 4 41.1 4 38.8 5 59.3 3 61.8 2 64.2 1 46.3 3 57.4 2 59.4 1 Table 1: Multilingual evaluation results. Scores represent NDCG@10 for retrieval, accuracy for classification, and Spearman rank correlation for regression tasks, averaged across European languages. Red-highlighted values indicate model rankings for each task, determined through pairwise statistical tests with 95% confidence level. Embedding (RoPE) value of 10,000. In the annealing phase, we increased the RoPE theta to 250,000 and randomly cropped our training documents to lengths between 12 and 8,192 tokens. We adopted this approach because, due to pre-processing constraints, our training data had already been segmented into fixed-length documents, making standard variablelength training infeasible. Therefore, we introduced random cropping of these fixed-length sequences as an approximation of variable-length training. Surprisingly, we found that this approach outperforms training only on fixed lengths (4), further highlighting the necessity for variable length documents during long context training (Gao et al., 2024). Infrastructure. We trained the EuroBERT family on Adastra, using 92 MI250X GPUs for EuroBERT-210M, 384 MI250X GPUs for EuroBERT-610M, and 96 MI300A GPUs for EuroBERT-2.1B, for total of 200k GPU hours. Our training framework incorporates FlashAttention (Dao, 2023), fused cross-entropy from LigerKernel (Hsu et al., 2024), torch.compile (Ansel et al., 2024), and hybrid sharding with Fully Sharded Data Parallel (Zhao et al., 2023)."
        },
        {
            "title": "3.1 Evaluation Setup",
            "content": "Datasets and tasks. We select suite of tasks to cover various real-world use cases for encoders. For multilingual tasks, we evaluate retrieval performance using MIRACL (Zhang et al., 2023), MLDR (Chen et al., 2024), WikipediaRetrieval4, and CC-News (de Gibert et al., 2024). We assess classification with XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), QAM (Liang et al., 2020), AmazonReviews (Keung et al., 2020) and MassiveIntent (Keung et al., 2020). Additionally, we measure sequence regression performance on the WMT (Bojar et al., 2017; 2018; Barrault et al., 2019; 2020; Akhbardeh et al., 2021; Kocmi et al., 2022) quality estimation task, as well as on summary evaluation using SeaHorse (Clark et al., 2023) and SummEval (Fabbri et al., 2021). For code-related tasks, we evaluate retrieval on CodeSearchNet (Husain et al., 2019) and DupStackMath (Hoogeveen et al., 2015), and 4https://huggingface.co/datasets/Samoed/WikipediaRetrievalMultilingual 4 EuroBERT: Scaling Multilingual Encoders for European Languages 0 1 @ N 70 60"
        },
        {
            "title": "MLDR",
            "content": "(1 K,5 K] (5 K,9 K] (9 K,17 K] (17 K,26 K] (26 K,151 K] r S 80 60 40"
        },
        {
            "title": "SeaHorse",
            "content": "(69,2 K] (2 K,2 K] (2 K,3 K] (3 K,4 K] (4 K,27 K]"
        },
        {
            "title": "Document Length",
            "content": "XLM-RoBERTa 280m EuroBERT 210m XLM-RoBERTa 560m EuroBERT 610m XLM-RoBERTa 3.5B EuroBERT 2.1B mGTE 305m Figure 3: Long context analysis. We examine how context length affects model performance in the XLM-RoBERTa and EuroBERT families across two long-context tasks (MLDR and SeaHorse). classification on CodeDefect (Zhou et al., 2019) and CodeComplexity (Jeon et al., 2023). Finally, in the mathematical domain, we test retrieval on the MathFormula (Drechsel et al., 2025) task, and process reward modeling on MathShepherd (Wang et al., 2024b). We believe that our chosen suite is representative of many practical applications of encoder models. Baselines. We compare EuroBERT with the multilingual encoders XLM-RoBERTa (Conneau et al., 2020; Goyal et al., 2021), mGTE-MLM-base (Zhang et al., 2024)6 and mDeBERTav3-base. Additionally, for code and mathematical tasks, we compare with the English-only ModernBERT (Warner et al., 2024) models. Fine-tuning. We follow standardized fine-tuning protocol to ensure fair model comparison. For sequence classification tasks, models are trained for 10,000 steps on the corresponding training split using the standard cross-entropy loss, batch size of 32, 10% warm-up ratio, and linear learning rate decay. For small datasets requiring multiple epochs, we apply early stopping with patience of one epoch based on validation performance. To account for model specificities, we fine-tune using 10 logarithmically spaced learning rates (1 105 to 1 104), selecting the one that achieves the highest validation metric.7 For sequence regression tasks, we use the same setting but replace the loss with the MSE. For long-context summarization datasets (SeaHorse and SummEval), fine-tuning is limited to 5,000 steps to reduce computational cost. For retrieval tasks, models are fine-tuned for 1,000 steps on MS-MARCO (Bajaj et al., 2016),8 using InfoNCE loss (Oord et al., 2018) with in-batch negatives and cosine similarity as the similarity metric. All other hyperparameters are aligned with those used in classification and regression tasks. 5Additional information on evaluation tasks is given in Appendix D. 6Since the EuroBERT models are general-purpose encoders, we evaluate them against the pretrained mGTE-MLM-base variant, which, similarly, was not optimized for retrieval tasks. 7Additional fine-tuning hyperparameters, such as AdamW parameters (β1, β2, ϵ, and weight decay), are set according to the values reported in the original source papers. For EuroBERT models, we maintain the same settings as those used during pre-training and annealing. 8Since many retrieval datasets lack dedicated training splits, we use MS-MARCO, an English-only dataset. This choice also allows us to assess cross-lingual generalization. 5 EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "Benchmark",
            "content": "ModernBERT mDeBERTa mGTE 150m 395m 280m XLM-RoBERTa 305m 280m 560m 3.5B EuroBERT 210m 610m 2.1B 53.9 5 65.8 3 CodeSearchNet DupStackMath 39.7 4 45.5 2 CodeComplexity 86.1 3 88.6 3 65.8 3 67.0 2 CodeDefect"
        },
        {
            "title": "MathFormula\nMathShepherd",
            "content": "89.6 5 91.9 2 77.7 4 83.6 2 2.8 10.2 7 73.9 5 64.7 3 85.2 7 75."
        },
        {
            "title": "Code",
            "content": "34.0 7 37.5 4 74.5 5 63."
        },
        {
            "title": "Math",
            "content": "23.0 8 40.8 6 54.1 5 58.9 4 69.9 2 72.6 1 29.3 6 36.9 5 42.9 3 41.7 3 46.0 2 48.3 1 74.1 5 83.6 4 84.3 4 91.9 2 94.2 1 95.2 1 61.9 4 54.3 5 65.8 3 69.5 1 69.0 1 67.7 2 83.4 8 77.2 4 89.1 6 91.5 3 92.6 1 91.0 4 83.1 8 81.4 71.9 6 67.6 7 82.5 3 84.0 2 87.3 1 86.8 1 Table 2: Evaluation results for code and math domains. Scores are reported as NDCG@10 for retrieval tasks (CodeSearchNet, DupStackMath, MathFormula) and accuracy for classification tasks (CodeComplexity, CodeDefect, MathShepherd). Evaluation metrics. We report accuracy for sequence classification, Spearman rank correlation for regression, and NDCG@10 for retrieval tasks. We also follow Freitag et al. (2023), and group systems into language-specific clusters based on statistically significant performance gaps at 95% confidence thresholds. We then compute system-level rankings using normalized Borda count (Colombo et al., 2022), defined as the average over the obtained per-language clusters."
        },
        {
            "title": "3.2 Results",
            "content": "Table 1 reports the aggregated results for all multilingual tasks, aggregating over the European languages seen during training.9 The EuroBERT family exhibits strong multilingual performance across domains and tasks. EuroBERT-2.1B, our largest model, achieves the highest performance among all systems, ranking first on 7 of 12 benchmarks. Importantly, it outperforms the largest system, XLM-RoBERTa-XL. Additionally, EuroBERT-610m is competitive with XLM-RoBERTa-XL, model 5 times its size, on most multilingual tasks, and surpasses it on code and mathematics. Similarly, the smaller EuroBERT-210m is competitive with XLM-RoBERTa-Large, which has twice the number of parameters, and globally outperforms all similarly sized systems. EuroBERT is effective at document ranking. Across domains, EuroBERT consistently ranks high for retrieval tasks. Notably, the 210m and 610m parameter models outperform all models of comparable sizes, and are competitive with the larger XLM-RoBERTa-XL. For sentence classification, EuroBERT models achieve results on par with similarly sized models. On sentence classification, no model significantly outperforms all others. During the development of EuroBERT, we found that several design decisions lead to trade-off between retrieval and classification capabilities (4). We highlight, however, that EuroBERT2.1B is still among the highest ranking systems, and that the smaller models in the family are competitive with models of comparable size. EuroBERT can function as an evaluation metric. In translation evaluation, while there is performance gap to the larger XLM-RoBERTa, the EuroBERT models remain competitive with the other alternatives. In the future, we would like to explore other training signals to further enhance cross-lingual capabilities of EuroBERT. In contrast, for summary evaluation, EuroBERT models consistently outperform competitors of any sizes, making them reliable choice for training metric on this type of task. 9More detailed results are provided in Appendix D. 6 EuroBERT: Scaling Multilingual Encoders for European Languages Figure 4: Impact of data subset ratios on model performance on XNLI and MIRACL. The first vertical axis of each subplot denotes the reference data mix named reference in Table C. EuroBERT maintains performance at longer context lengths. Figure 3 compares the long context performance of EuroBERT and XLM-RoBERTa. While both models achieve similar performance for shorter inputs, EuroBERT maintains performance at longer contexts, whereas XLM-RoBERTa suffers notable degradation. The EuroBERT family excels in tasks related to code and mathematics. Table 2 reports the results on tasks related to code and mathematics. On these tasks, all EuroBERT models consistently surpass other systems. Similarly to retrieval tasks, the EuroBERT-210m reflects most of the performance of the larger models in the family, and ranks above all baselines, highlighting its capabilities at smaller scale. Additionally, the larger EuroBERT-2.1B achieves the highest performance among all systems."
        },
        {
            "title": "4 Training Recipe Analysis",
            "content": "We measure the impact of various design decisions made during the development of EuroBERT with extensive ablations. Following Blakeney et al. (2024) and Llama Team (2024), we perform multiple annealing runs on 40B tokens, each varying different component of our recipe, and measure the performance on the XNLI and MIRACL validation sets, the former representing multilingual classification and the latter multilingual retrieval.10 Balancing the language distribution can enhance performance. The left-most plot in Figure 4 reports the retrieval and classification performance as the proportion of English is reduced and re-distributed between other languages. Remarkably, the retrieval performance consistently decreases, suggesting that increasing multilingual data may not lead to an increase in multilingual performance. Including math and code improves multilingual retrieval, but degrades multilingual classification. The second and third plots in Figure 4 show MIRACL performance dropping and XNLI accuracy rising as the proportions for math and code data decrease. This outcome underscores the specific trade-offs encountered during model development. In future work, we aim to investigate how to better balance task performance during pre-training. Increasing parallel data yields performance gains. The forth plot in Figure 4 presents the XNLI and MIRACL performance when increasing the amount of parallel data. Similar to Anil et al. (2023); Briakou et al. (2023); Alves et al. (2024), we find it increases performance on both benchmarks. Adding instruction fine-tuning data degrades model performance. The right-most plot in Figure 4 analyses the impact of adding instructions during annealing, which can improve performance for decoder language models. In contrast to decoders, it leads to worse performance when training an encoder model. 10We follow the evaluation procedure from 3, but instead test on the validation splits. 7 EuroBERT: Scaling Multilingual Encoders for European Languages Figure 5: Impact of hyperparameter choice on model performance on XNLI and MIRACL. The first vertical axis of each subplot denotes the reference data mix. Model-based quality filters can lead to worse results. Contrary to initial expectations, using the highest-quality data bucket quality during the annealing phase did not result in better performance on XNLI and MIRACL. Instead, as illustrated in the right plot of Figure 5, mixing the buckets with quality levels 3 and 4 leads to the best performance on XNLI, while the data bucket of quality 3 achieved the best results for MIRACL. reduced masking ratio during annealing enhances classification performance. Similar to previous research (Yang et al., 2023; Ankner et al., 2024), which advocates lowering the masking ratio in later training, we also find that reducing it to 10% during the annealing phase improves EuroBERTs performance on XNLI, though it leads to modest decline in MIRACL scores. Impact of variable sentence length on model performance. The first plot in Figure 5 examines the impact of variable sentence lengths during annealing. Compared to the fixed packed sentence lengths employed in pretraining, variable sentence lengths significantly boosts XNLI and moderately MIRACL performance. This improvement remains stable, without degradation when the maximum context length is extended to 8,192 tokens. Based on this analysis, we decided to create our final annealing dataset by selecting data above the third threshold. We reduced the proportion of English to 26% while proportionally increasing the share of the remaining languages. To balance retrieval and classification performance, we allocated 6% and 4% of the data mix to math and code, respectively. Additionally, we increased the proportion of parallel data to 6%, using remaining data not seen during pre-training, and removed instruction one. We finally lowered the masking ratio to 10% and performed annealing with random sentence lengths of up to 8,192 tokens."
        },
        {
            "title": "5 Related Work",
            "content": "Encoder models and learning objectives. Encoder-only models have consistently demonstrated strong performance in non-generative NLP tasks, such as classification (Acheampong et al., 2021; Ma et al., 2019) and retrieval (Karpukhin et al., 2020; Wang et al., 2024a), leveraging their ability to effectively represent sequences while maintaining relatively compact model sizes. Traditional encoder architectures, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), rely on the masked language modeling (MLM) objective, which, combined with bidirectional attention, enables them to learn rich contextual representations well-suited for sequence-level understanding. In contrast, DeBERTa (He et al., 2023) introduces replaced token detection (RTD) as an alternative pre-training objective, which improves efficiency and achieves strong results in classification tasks. In this paper, we chose to use the masked language modeling objective because initial evaluations of existing models showed more balanced results across all tasks. Multilingual encoders. Expanding on these monolingual architectures, multilingual encoder models such as mBERT (Devlin et al., 2019), XLM-RoBERTa (Conneau et al., 2020), and mDeBERTa (He et al., 2023) have extended pre-training benefits to diverse languages, 8 EuroBERT: Scaling Multilingual Encoders for European Languages enhancing cross-lingual understanding. However, training these models has also highlighted the so-called curse of multilinguality (Conneau et al., 2020; Chang et al., 2024), demonstrating the necessity of scaling in terms of training data, model size, and context length to maintain competitive performance across languages. Scaling encoder models. Initial efforts to scale encoder models focused on increasing model size and supported languages, as seen in larger variants like XLM-RoBERTa-XL and XLM-RoBERTa-XXL (Goyal et al., 2021), which demonstrated the benefits of scaling for multilingual performance. However, more recent advancements in encoder architectures have moved beyond mere size increases, incorporating sophisticated design improvements. Notably, concurrent work on modern pre-trained encoders, such as ModernBERT (Warner et al., 2024) and mGTE (Zhang et al., 2024), introduces innovations like grouped query attention (Ainslie et al., 2023), RoPE embeddings (Su et al., 2024), GLU activations (Shazeer, 2020), RMS normalization (Zhang & Sennrich, 2019), and extended context support. In line with these advancements and inspired by recent progress in decoder scaling (Brown et al., 2020; Yang et al., 2024; DeepSeek-AI et al., 2024), our work revisits the classical encoder pre-training paradigm. Specifically, we increase the MLM masking ratio, scale training across multiple languages with up to 5 trillion tokens, and integrate recent architectural improvements."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose recipe for training general-purpose multilingual encoders, creating the EuroBERT family. We incorporate recent architectural advances from decoder models, and train on multilingual dataset containing European and globally spoken languages, together with code and mathematics. Our models outperform existing alternatives on comprehensive suite of tasks covering multilingual capabilities, mathematics and code. We also extensively analyze the design decisions behind EuroBERTs dataset and training pipeline. Alongside this paper, we release all models in the EuroBERT family, including intermediate training checkpoints, and our training framework to facilitate future research."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely thank the ADASTRA supercomputer (CINES) for its technical support and high-performance computing (HPC) resources, provided through grants C1615122 and GDA2401. We also appreciate the support of the French government through the France 2030 program as part of the ArGiMi project. This work was also supported by the EUs Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for Responsible AI), and by FCT/MECI through national funds and when applicable co-funded EU funds under UID/50008: Instituto de Telecomunicac oes. Duarte was also partially supported by the DataIA Institute, whose contributions facilitated the completion of this work. 9 EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "References",
            "content": "Francisca Adoma Acheampong, Henry Nunoo-Mensah, and Wenyu Chen. Transformer models for text-based emotion detection: review of bert-based approaches. Artificial Intelligence Review, 54(8), 2021. URL https://dl.acm.org/doi/abs/10.1007/s10462-02109958-2. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.emnlp-main.298/. Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondˇrej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina Espa na-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, Online, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.wmt-1.1/. Duarte Miguel Alves, Jose Pombal, Nuno Guerreiro, Pedro Henrique Martins, Joao Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, et al. Tower: An open multilingual large language model for translation-related tasks. In First Conference on Language Modeling, 2024. URL https://arxiv.org/abs/2402.17733. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Dıaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. URL https://arxiv.org/abs/2305.10403. Zachary Ankner, Naomi Saphra, Davis Blalock, Jonathan Frankle, and Matthew Leavitt. Dynamic masking rate schedules for MLM pretraining. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-short.42/. 10 EuroBERT: Scaling Multilingual Encoders for European Languages Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph comIn Proceedings of the 29th ACM International Conference on Architectural Suppilation. port for Programming Languages and Operating Systems, Volume 2, 2024. URL https: //dl.acm.org/doi/abs/10.1145/3620665.3640366. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. URL https://arxiv.org/abs/1611.09268. Loıc Barrault, Ondˇrej Bojar, Marta R. Costa-juss`a, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias uller, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), Florence, Italy, August 2019. Association for Computational Linguistics. URL https://aclanthology.org/W19-5301/. Loıc Barrault, Magdalena Biesialska, Ondˇrej Bojar, Marta R. Costa-juss`a, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubeˇsic, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.wmt-1.1/. Cody Blakeney, Mansheej Paul, Brett Larsen, Sean Owen, and Jonathan Frankle. Does your data spark joy? performance gains from domain upsampling at the end of training. In First Conference on Language Modeling, 2024. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. URL https://aclanthology.org/W17-4717/. Ondˇrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. Findings of the 2018 conference on machine translation (WMT18). In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, Belgium, Brussels, October 2018. Association for Computational Linguistics. URL https://aclanthology.org/W18-6401/. Eleftheria Briakou, Colin Cherry, and George Foster. Searching for needles in haystack: On the role of incidental bilingualism in PaLMs translation capability. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long.524/. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya 11 EuroBERT: Scaling Multilingual Encoders for European Languages Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Tyler A. Chang, Catherine Arnett, Zhuowen Tu, and Ben Bergen. When is multilingualIn Proity curse? language modeling for 250 highand low-resource languages. ceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main.236/. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. In Findings of the Association for Computational Linguistics: ACL 2024, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.137/. Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and Ankur Parikh. SEAHORSE: multilingual, multifaceted dataset for summarization evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, December 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.emnlp-main.584. Pierre Colombo, Nathan Noiry, Ekhine Irurozki, and Stephan Clemencon. What In Adnew perspectives on nlp benchmarking. Information Processing Systems, volume 35. Curran Associates, URL https://proceedings.neurips.cc/paper files/paper/2022/file/ are the best systems? vances in Neural Inc., 2022. ac4920f4085b5662133dd751493946a6-Paper-Conference.pdf. Alexis Conneau and Guillaume Lample. language model pretrainIn Advances in Neural Information Processing Systems, volume 32. Curran Asing. sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper files/paper/2019/ file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf. Cross-lingual Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. URL https://aclanthology.org/D18-1269/. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, July 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.aclmain.747/. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2023. URL https:// arxiv.org/abs/2307.08691. Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Ba on, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramırez-Sanchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, and org Tiedemann. new massive mulIn Proceedings of the tilingual dataset for high-performance language technologies. 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.100/. 12 EuroBERT: Scaling Multilingual Encoders for European Languages DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism, 2024. URL https://arxiv.org/abs/2401.02954. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https://aclanthology.org/ N19-1423/. Jonathan Drechsel, Anja Reusch, and Steffen Herbold. MAMUT: novel framework for modifying mathematical formulas for the generation of specialized datasets for language model training, 2025. URL https://arxiv.org/abs/2502.20855. Alexander R. Fabbri, Wojciech Krysci nski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9, 2021. URL https://aclanthology.org/ 2021.tacl-1.24/. Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. Results of wmt23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.wmt-1.51. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024. URL https:// arxiv.org/abs/2410.02660. Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. Larger-scale transformers for multilingual masked language modeling. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), Online, August 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.repl4nlp-1.4/. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations, 2023. URL https://arxiv.org/abs/ 2111.09543. Doris Hoogeveen, Karin Verspoor,"
        },
        {
            "title": "A benchmark data\nIn Proceedings of",
            "content": "Cqaduprecomputing sympoURL https://dl.acm.org/doi/abs/10.1145/2838931.2838934?casa stack: search. sium, token=-tk7Uh-Jal4AAAAA:LP9O8GQO5yOQAW6m4nw81fVeZspyMSSae4QXz7vStNizdy6MNAEw393sY0kWvDZfDO7PwnKeHpX5A. the 20th Australasian document and Timothy Baldwin. question-answering community 2015. for set Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm training, 2024. URL https://arxiv.org/abs/2410.10989. 13 EuroBERT: Scaling Multilingual Encoders for European Languages Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019. URL https://arxiv.org/abs/1909.09436. Mingi Jeon, Seung-yeop Baik, Joonghyuk Hahn, Yo-Sub Han, and Sang-Ki Ko. Deep learning-based source code complexity prediction. 2023. URL https://openreview.net/ forum?id=9irBKvxsw9. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.emnlp-main.550/. Phillip Keung, Yichao Lu, Gy orgy Szarvas, and Noah A. Smith. The multilingual Amazon In Proceedings of the 2020 Conference on Empirical Methods in Natural reviews corpus. Language Processing (EMNLP), Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.emnlp-main.369/. Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novak, Martin Popel, and Maja Popovic. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.1/. Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. XGLUE: new benchmark dataset for cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.emnlp-main.484/. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach, 2019. URL https://arxiv.org/abs/1907.11692. AI @ Meta Llama Team. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. Domain adapIn Proceedings of tation with BERT-based domain classification and data selection. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), Hong Kong, China, November 2019. Association for Computational Linguistics. URL https://aclanthology.org/D19-6109/. Pedro Henrique Martins, Patrick Fernandes, Joao Alves, Nuno M. Guerreiro, Ricardo Rei, Duarte M. Alves, Jose Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, Pierre Colombo, Barry Haddow, Jose G. C. de Souza, Alexandra Birch, and Andre F. T. Martins. Eurollm: Multilingual language models for europe, 2024. URL https://arxiv.org/abs/2409.16235. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources 14 EuroBERT: Scaling Multilingual Encoders for European Languages and Evaluation (LREC-COLING 2024), Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.377/. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. URL https://arxiv.org/abs/ 1807.03748. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, MarThe In AdInformation Processing Systems, volume 37. Curran Associates, URL https://proceedings.neurips.cc/paper files/paper/2024/file/ garet Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. fineweb datasets: Decanting the web for the finest text data at scale. vances in Neural Inc., 2024. 370df50ccfdf8bde18f8f9c2d9151bda-Paper-Datasets and Benchmarks Track.pdf. Machel Reid and Mikel Artetxe. PARADISE: Exploiting parallel data for multilingual sequence-to-sequence pretraining. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seattle, United States, July 2022. Association for Computational Linguistics. URL https:// aclanthology.org/2022.naacl-main.58/. Machel Reid and Mikel Artetxe. On the role of parallel data in cross-lingual transfer learning. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/ 2023.findings-acl.372/. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/ 2002.05202. Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, and Rameswar Panda. Power scheduler: batch size and token number agnostic learning rate scheduler, 2024. URL https: //arxiv.org/abs/2408.13359. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568, ISSN 0925-2312. URL https://www.sciencedirect.com/science/article/pii/ 2024. S0925231223011864. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. 15 EuroBERT: Scaling Multilingual Encoders for European Languages Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017. URL https://proceedings.neurips.cc/paper files/ paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024a. URL https://arxiv.org/abs/2212.03533. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand, August 2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.510/. Benjamin Warner, Antoine Chaffin, Benjamin Clavie, Orion Weller, Oskar Hallstr om, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference, 2024. URL https://arxiv.org/abs/2412.13663. Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in masked language modeling? In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.217/. Adina Williams, Nikita Nangia, and Samuel Bowman. broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), New Orleans, Louisiana, June 2018. Association for Computational Linguistics. URL https://aclanthology.org/N18-1101/. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. Dongjie Yang, Zhuosheng Zhang, and Hai Zhao. Learning better masking for better language model pre-training. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long.400/. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, November 2019. Association for Computational Linguistics. URL https://aclanthology.org/D19-1382/. Biao Zhang and Rico Sennrich. vances in Neural Inc., 2019. 1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf. In AdInformation Processing Systems, volume 32. Curran Associates, URL https://proceedings.neurips.cc/paper files/paper/2019/file/ Root mean square layer normalization. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mGTE: Generalized long-context text representation and reranking models for multilingual text 16 EuroBERT: Scaling Multilingual Encoders for European Languages retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, Miami, Florida, US, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-industry.103/. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David AlfonsoHermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. MIRACL: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11, 2023. URL https://aclanthology.org/2023.tacl-1.63/. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proceedings of the VLDB Endowment, 16(12), 2023. URL https://dl.acm.org/doi/abs/10.14778/3611540.3611569. Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper files/paper/2019/ file/49265d2447bc3bbfe9e76306ce40a31f-Paper.pdf. 17 EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "A EuroBERT Model Architecture",
            "content": "EuroBERT consists of three variants: EuroBERT-210m with 210 million parameters, EuroBERT-610m with 610 million parameters and EuroBERT-2.1B with 2.1 billion parameters. These variants strike balance between traditional encoder model sizes and the benefits of parameter scaling. We report the architectural details of each model in Table 3."
        },
        {
            "title": "Model Size",
            "content": "210m 610m 2.1B 32 2,304 6,144 18 6 Layers Embedding Dimension FFN Dimension Attention Heads Key/Value Heads Layer Normalization RMSNorm ϵ Activation Function Vocabulary Size Positional Embeddings RoPE θ Tokenizer 12 768 3,072 12 12 26 1,152 4,096 18 6 RMSNorm 1 105 SwiGLU 128,000 RoPE 250,000 LLaMA 3 Table 3: Summary of architectural hyperparameters for EuroBERT models of different sizes."
        },
        {
            "title": "B Training Framework",
            "content": "We trained the EuroBERT family on Adastra, utilizing 92 MI250X GPUs for EuroBERT-210M over 15k hours, 384 MI250X GPUs for EuroBERT-610M over 92k hours, and 96 MI300A GPUs for EuroBERT-2.1B over 106k hours. The training recipe for the EuroBERT models consists of two main stages, applied uniformly to both model sizes: (1) the pre-training phase, (2) the annealing phase including context extension. Training Hyperparameters. We initialized the linear and embedding layers with values drawn from normal distribution with mean of 0 and standard deviation of 0.2. For stability, we increased the default epsilon value of AdamW (Loshchilov & Hutter, 2019) to 1 105 and set β1 = 0.9, β2 = 0.95, with weight decay of 0.1. The Warmup-Stable-Decay (WSD) scheduler (Shen et al., 2024) was employed, featuring warmup phase of 2, 000 steps and constant learning rate (LR) of 1 104 throughout training. To achieve similar effective batch size of 9 106 tokens between EuroBERT models, gradient accumulation was applied for the 2.1 billion parameter model and set to 5. Detailed hyperparameter choices are reported in Table 4. We find this training recipe highly stable, with no loss spikes or need for intervention to address model training divergence (Figure 6). Infrastructure, scaling, and efficiency. Trained large language models (LLMs) at scale is resource-intensive process that demands specialized hardware and an optimized codebase to effectively manage computational resources. We trained the EuroBERT family on the Adastra French supercomputer cluster, leveraging AMD GPUs: 192 MI250 GPUs for EuroBERT-210m, 384 MI250 GPUs for EuroBERT-610m, and 96 MI300A GPUs for EuroBERT2.1B. However, most open-source pre-training frameworks are designed for NVIDIA hardware, presenting significant compatibility challenges. To overcome this, we developed custom codebase tailored to training our models on AMD and NVIDIA GPUs11. 11https://github.com/Nicolas-BZRD/EuroBERT 18 EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "Parameter",
            "content": "210m 610m 2.1B"
        },
        {
            "title": "LR\nLR Scheduler\nContext Length",
            "content": "Optimizer Beta1 Beta2 Epsilon (eps) Weight Decay Clip Grad Norm Pre-training"
        },
        {
            "title": "Optimizer",
            "content": "1e-4 WSD 2,000 2,048 1e-4 to 0 Cosine 8,192 AdamW 0.9 0.95 1e-5 0.1 1."
        },
        {
            "title": "Training Setup",
            "content": "Per-GPU Batch Size Gradient Accumulation Steps GPUs Tokens/Step 24 1 192 9,437,184 12 1 384 9,437,184 10 5 96 9,830,400 Table 4: Training hyperparameters for EuroBERT models (210m, 610m, 2.1B). The optimizer and Tokens/Step remain consistent across both pre-training and annealing phases. Figure 6: Pre-training Loss for all EuroBERT models on logarithmic scale. Built on PyTorch, this code base includes several optimizations to increase training throughput. Specifically, we highlight FlashAttention (Dao, 2023), fused cross-entropy from LigerKernel (Hsu et al., 2024), torch.compile (Ansel et al., 2024), and hybrid sharding with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023), splitting model, gradients and optimizer states within the same node while replicating them across nodes. We achieved training throughput of 1.2M tokens/s on 96 MI300A. 19 EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "Subset",
            "content": "Tokens (M) Mix (%) Source"
        },
        {
            "title": "Subset",
            "content": "Tokens (M) Mix (%) English FineWeb French CulturaX German CulturaX Spanish CulturaX Chinese CulturaX Italian CulturaX Russian CulturaX Portuguese CulturaX Japanese CulturaX Polish CulturaX Turkish CulturaX Arabic CulturaX Vietnamese CulturaX Dutch CulturaX Hindi CulturaX es en Unbabel parallel fr en Unbabel parallel Unbabel parallel de en it en Unbabel parallel ru en Unbabel parallel nl en Unbabel parallel Unbabel parallel pl en ar en Unbabel parallel zh en Unbabel parallel cs en Unbabel parallel hu en Unbabel parallel vi en Unbabel parallel tr en Unbabel parallel ja en Unbabel parallel hi en Unbabel parallel Arxic Proof-pile-2 Open-Web-Math Proof-pile-2 Algebraic-stack Proof-pile-2 C++ The-Stack v2 SQL The-Stack v2 The-Stack v2 2, 002, 327 295, 113 291, 514 290, 489 238, 467 120, 128 116, 797 112, 321 112, 242 111, 659 53, 126 52, 413 50, 661 50, 646 25, 544 50, 613 44, 891 30, 541 18, 702 13, 808 12, 666 7, 280 6, 414 6, 206 5, 458 4, 599 3, 395 2, 975 2, 687 1, 136 121, 503 54, 168 35, 985 120, 085 75, 348 59,"
        },
        {
            "title": "6.09 The-Stack v2 PHP\n6.02 The-Stack v2 C#\n6.00 The-Stack v2 Python\n4.92 The-Stack v2\n2.48 The-Stack v2 Go\n2.41 The-Stack v2 TypeScript\n2.32 The-Stack v2 HTML\n2.32 The-Stack v2 Lua\n2.31 The-Stack v2 Ruby\n1.10 The-Stack v2 Vue\n1.08 The-Stack v2 R\nShell\n1.05 The-Stack v2\nSwift\n1.05 The-Stack v2\nreStructuredText\n0.53 The-Stack v2\n1.05 The-Stack v2\nJSON\n0.93 The-Stack v2 Rust\n0.63 The-Stack v2 YAML\n0.39 The-Stack v2 Dart\n0.29 The-Stack v2 RMarkdown\n0.26 The-Stack v2 HCL\n0.15 The-Stack v2 PowerShell\n0.13 The-Stack v2 VBA\n0.13 The-Stack v2 AsciiDoc\n0.11 The-Stack v2 Groovy\n0.09 The-Stack v2 CUDA\n0.07 The-Stack v2 Dockerfile\n0.06 The-Stack v2 Cython\n0.06 The-Stack v2 COBOL\n0.02 The-Stack v2 GraphQL\n2.51 The-Stack v2 HTTP\n1.12 The-Stack v2 ABAP\n0.74 The-Stack v2 RDoc\n2.48 The-Stack v2 Metal\n1.56 The-Stack v2 AppleScript\n1.23 Total",
            "content": "58, 440 25, 620 24, 842 21, 521 20, 950 14, 766 11, 307 7, 962 7, 733 5, 524 5, 411 5, 287 4, 793 3, 766 3, 761 3, 586 3, 152 2, 716 2, 678 2, 058 1, 423 1, 027 1, 027 970 540 406 281 103 96 83 82 71 16 8 7 4, 843, 357 1.21 0.53 0.51 0.44 0.43 0.30 0.23 0.16 0.16 0.11 0.11 0.11 0.10 0.08 0.08 0.07 0.07 0.06 0.06 0.04 0.03 0.02 0.02 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 100 Table 5: MLM pre-training data, with total of 4.8 trillion tokens (according to EuroBERTs tokenizer). We report the list of all dataset names and subsets used, including the number of tokens selected and their proportion in the final data mix."
        },
        {
            "title": "Reference",
            "content": "en 46.3 English 26% 26.0 English 17% 17.0 Math 4% Math 2% Code 8% Code 4% Code 2% Parallel 8% IFT 0% 46.3 46.3 46.3 46.3 46.3 46.3 46.3 fr 5.8 6.0 6.0 5.8 5.8 5.8 5.8 5.8 5.8 5. de 5.7 6.0 6.0 5.7 5.7 5.7 5.7 5.7 5. 5.7 nl 1.0 4.0 5.0 1.0 1.0 1.0 1.0 1. 1.0 1.0 hi 0.3 4.0 5.0 0.3 0. 0.3 0.3 0.3 0.3 0.3 it 1.5 4.0 5. 1.5 1.5 1.5 1.5 1.5 1.5 1.5 ja 0. 4.0 5.0 0.8 0.8 0.8 0.8 0.8 0.8 0.8 pl 1.0 4.0 5.0 1.0 1.0 1.0 1.0 1.0 1.0 1. pt 1.4 4.0 5.0 1.4 1.4 1.4 1.4 1.4 1. 1.4 ru 1.0 4.0 5.0 1.0 1.0 1.0 1.0 1. 1.0 1.0 es 5.7 6.0 6.0 5.7 5. 5.7 5.7 5.7 5.7 5.7 ar 0.4 4.0 5. 0.4 0.4 0.4 0.4 0.4 0.4 0.4 zh 4. 6.0 6.0 4.7 4.7 4.7 4.7 4.7 4.7 4.7 tr 1.0 4.0 5.0 1.0 1.0 1.0 1.0 1.0 1.0 1."
        },
        {
            "title": "IFT",
            "content": "8.7 4.0 4.0 8.7 8.7 6.0 4.0 2.0 8.7 8. 8.2 4.0 4.0 4.0 2.0 8.2 8.2 8.2 8.2 8. 5.2 5.0 5.0 5.2 5.2 5.2 5.2 5.2 8.0 5. 1.2 1.0 1.0 1.2 1.2 1.2 1.2 1.2 1.2 0. Table 6: Data mix employed in the ablation study measuring the importance of different data subsets in the EuroBERT annealing phase. We curate our multilingual corpus for MLM pre-training using variety of freely available and cleaned datasets  (Table 5)  . This data mix predominantly consists of web-based data, with the FineWeb dataset (Penedo et al., 2024) serving as the primary English corpus. We used CulturaX dataset (Nguyen et al., 2024) for multilingual text. We selected 14 languages (French, German, Spanish, Chinese, Italian, Russian, Polish, Portuguese, Japanese, Vietnamese, Dutch, Arabic, Turkish, and Hindi) to create corpus of European and most widely spoken languages, representing broad range of alphabets and cultures. In addition, we incorporated parallel data, which has been shown to improve cross-lingual transfer learning (Reid & Artetxe, 2022; 2023). Specifically, we concatenated in random order, 20 EuroBERT: Scaling Multilingual Encoders for European Languages pairs of English sentences and their translations into another language. These sentence pairs were separated by <parallel_sep> token and uniformly masked creating an asymmetric masking between sentences. This asymmetric masking encourages the model to use its bidirectional attention mechanism to decode masked tokens, leveraging both the original language and its translation. Finally, we enriched the dataset with 38 programming languages from The Stack v2 and science data with Proof-Pile-2 (upsampled 5). detailed summary of the dataset composition is available in Table 5. 21 EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "D Evaluation",
            "content": "D.1 Dataset Details This appendix offers additional details on the datasets used for evaluation. Table 7 presents the language coverage of all evaluation datasets, and below are additional specifications on the evaluated tasks."
        },
        {
            "title": "Task",
            "content": "en de es fr it nl pl pt ar hi ja ru tr vi zh"
        },
        {
            "title": "European Languages",
            "content": "Extra-European Languages"
        },
        {
            "title": "Sequence Classification",
            "content": "XNLI PAWS-X QAM AmazonReviews MassiveIntent CodeDefect CodeComplexity MathShepherd"
        },
        {
            "title": "WMT\nSeaHorse\nSummEval",
            "content": "MIRACL MLDR Wikipedia CC-News CodeSearchNet DupStackMath MathFormula"
        },
        {
            "title": "Sequence Regression",
            "content": ""
        },
        {
            "title": "Information Retrieval",
            "content": "Table 7: Language coverage across evaluation datasets. Classification datasets: XNLI (Conneau et al., 2018) Cross-lingual natural language inference task extending MNLI (Williams et al., 2018), consisting in classifying sentence pairs into entailment, contradiction, or neutral. In our experiments, we train multi-lingually instead of crosslingually to assess models capacity for multilingual fine-tuning. PAWS-X (Yang et al., 2019) Paraphrase identification task aimed at determining whether two sentences convey the same meaning. Fine-tuning is performed crosslingually, with training on the English subset and evaluation across all available languages. QAM (Liang et al., 2020) NLU task aimed at verifying whether question-passage pair forms valid question-answer pair. Fine-tuning is performed cross-lingually AmazonReviews (Keung et al., 2020) sentiment analysis task consisting in estimating the satisfaction level of multilingual Amazon product reviews on 1-to-5 scale. Fine-tuning is performed on all available languages. MassiveIntent (Keung et al., 2020) multilingual classification task consisting in assigning sentences to one of 60 topic categories. Fine-tuning is performed on all available languages. CodeDefect (Zhou et al., 2019) binary classification task aimed at identifying whether given code snippet contains defect. CodeComplexity (Jeon et al., 2023) Computational analysis task consisting in estimating the order of complexity of code-formulated computer science problem. 22 EuroBERT: Scaling Multilingual Encoders for European Languages MathShepherd (Wang et al., 2024b) Binary classification task aimed at determining whether step-by-step math rationale is correct given problem prompt. Since longer rationales are more error-prone, we filter the dataset to retain only 3-step rationales to prevent models from overfitting answer length. As the dataset lacks validation split, we allocate half of the test set for validation. Regression datasets: WMT (Bojar et al., 2017; 2018; Barrault et al., 2019; 2020; Akhbardeh et al., 2021; Kocmi et al., 2022) Regression task consisting in predicting translation quality given source sentence. As the original test set covers only three language pairs, we construct validation and test sets by sampling 5% of the training set for each, ensuring broader language coverage in evaluation. SeaHorse (Clark et al., 2023) Multilingual summarization evaluation task, where each text-summary pair is annotated across 6 binary evaluation dimensions. The final score is obtained by averaging these labels, yielding continuous value between 0 and 1. SummEval (Fabbri et al., 2021) Initially English-only summarization evaluation task, later extended to other languages, including German12, French13, Spanish14, and Turkish15. Each summary is assessed across 4 dimensions, averaged to produce continuous score. Retrieval datasets: MS-MARCO (Bajaj et al., 2016) English-only retrieval dataset used for fine-tuning, where each anchor-positive pair includes mined hard negative, forming triplet structure. MIRACL (Zhang et al., 2023) Multilingual retrieval dataset. We use the semisupervised version with labeled positive pairs provided by SentenceTransformers16 as the primary data source. Anchors serve as queries, and the corpus consists of all positive documents in the dataset. Since only single data split is available, we create validation and test sets by partitioning 50% of the original split for each, using queries as the split key to ensure no data leakage. MLDR (Chen et al., 2024) Long-context multilingual retrieval dataset. As with MIRACL, we use the triplet version provided by SentenceTransformers and apply the same validation-test split strategy. Wikipedia17 Multilingual information retrieval dataset. Since only single data split is available, we partition 50% of the queries into validation and test sets. CC-News (de Gibert et al., 2024) Highly multilingual retrieval dataset. As with MIRACL, we use the SentenceTransformers dataset version as the primary data source and apply the same test-validation split method. CodeSearchNet (Husain et al., 2019) Code retrieval dataset with comment-code query-positive pairs (SentenceTransformers version), processed similarly to the previous datasets. DupStackMath (Hoogeveen et al., 2015) Code retrieval dataset with queries, corpus, and relevant documents, processed the same way as the above datasets. MathFormula (Drechsel et al., 2025) Mathematical retrieval dataset consisting of pairs of equivalent formulas. The original dataset contains formula pairs labeled as true 12https://huggingface.co/datasets/sproos/summeval-de 13https://huggingface.co/datasets/sproos/summeval-fr 14https://huggingface.co/datasets/sproos/summeval-es 15https://huggingface.co/datasets/sproos/summeval-tr 16https://huggingface.co/collections/sentence-transformers/embedding-model-datasets6644d7a3673a511914aa7552 17https://huggingface.co/datasets/Samoed/WikipediaRetrievalMultilingual 23 EuroBERT: Scaling Multilingual Encoders for European Languages or false based on their equivalence, spanning 71 well-known mathematical formulas. To construct the retrieval dataset, we extract only equivalent formula pairs, retaining positive instances. Due to the datasets large size, we sample 100 positive pairs per formula type for both validation and test sets. The final dataset is processed following the same methodology as other pair-based datasets. D.2 Detailed Results This appendix presents detailed results for the multilingual evaluation tasks in our benchmark, including per-language scores as well as averages across European languages and all languages. Model en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World European Languages Extra-European Languages Average MIRACL XLM-RoBERTa-base XLM-RoBERTa-large XLM-RoBERTa-XL mDeBERTa-v3-base mGTE-MLM-base ModernBERT-base ModernBERT-large 82.5 83.8 88.0 88.6 88.6 90.3 89.7 91.6 92.1 91.9 90.4 93.0 29.6 35.3 45.3 39.6 91.4 94.6 88.3 91.4 91.9 84.6 93.5 89.0 94.5 91.5 46.2 34.4 95.2 91.6 77.9 81.1 85.1 36.0 85.3 85.7 91.1 92.6 33.7 91.5 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 94.1 95.4 93.6 95.1 94.2 95.0 95.8 90.0 96.3 91.8 95.3 93.0 83.2 88.3 87.1 90.8 92.4 93.4 85.7 90.9 90.7 92.4 91.5 94.1 MLDR 89.5 91.6 92.6 43.7 93.8 95.1 95.0 94.8 85.4 89.4 91.4 37.5 91.2 90.8 92.6 92.9 Model en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World European Languages Extra-European Languages Average XLM-RoBERTa-base XLM-RoBERTa-large XLM-RoBERTa-XL mDeBERTa-v3-base mGTE-MLM-base ModernBERT-base ModernBERT-large 58.0 66.9 72.7 15.4 79.5 56.7 61.1 66.1 24.3 68.7 43.6 53.8 59.4 51.5 58.9 63.4 60.8 63.5 68.9 13.2 18.1 18.8 63.5 60.8 60.9 53.4 60.3 61.9 67.1 67.5 71.0 18.2 19.0 71.4 78.1 44.4 51.5 56.5 12.3 55. 64.5 71.1 73.5 23.9 78.2 56.4 60.3 61.8 20.5 66.2 50.3 54.9 62.6 17.4 62.4 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 67.2 72.5 72.5 68.1 69.5 65. 78.2 80.3 77.6 80.0 79.8 77.6 68.9 77.9 73.9 79.0 69.2 75.0 52.1 55.5 53.0 51.3 60.9 58.1 60.8 61.6 61. 59.1 56.4 62.5 59.0 59.3 57.6 Wikipedia 58.7 65.2 70.0 20.0 73.2 73.4 75.8 72.9 54.6 60.8 65.9 18.3 67.8 65.4 68.6 66. Model en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World European Languages Extra-European Languages Average XLM-RoBERTa-base XLM-RoBERTa-large XLM-RoBERTa-XL mDeBERTa-v3-base mGTE-MLM-base ModernBERT-base ModernBERT-large 91.2 91.5 93.5 93.4 96.5 96.6 60.4 53.6 93.9 94.7 94.7 95.4 97.9 66.1 96.7 90.2 90.8 87.4 91.7 93.4 92.5 90.7 93.6 96.3 96.0 94.5 96.7 57.6 56.5 51.3 58.9 93.9 93.6 92.0 94.6 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 97.7 98.3 99.0 94.7 94.5 95.9 96.0 96.1 96.0 95.6 95.7 88.6 95.6 96.6 96.1 92.6 96.6 96.0 95.9 92.0 96.6 CC-News 91.0 93.1 96.3 57.6 94.1 94.4 95.9 95."
        },
        {
            "title": "Model",
            "content": "en de es fr it nl pl pt ar hi ja ru tr vi zh"
        },
        {
            "title": "European Languages",
            "content": "Extra-European Languages"
        },
        {
            "title": "Average",
            "content": "XLM-RoBERTa-base XLM-RoBERTa-large XLM-RoBERTa-XL mDeBERTa-v3-base mGTE-MLM-base ModernBERT-base ModernBERT-large 55.8 69.1 79.7 11.9 72.8 57.8 68.6 77.4 15.7 68.7 77.1 63.1 69.9 81.2 75.3 77.3 86.8 84.0 84.4 34.8 15.8 25.0 76.1 88.2 74.3 47.8 61.1 72.1 10.7 56.7 31.1 51.0 59.5 4.7 32. 75.7 82.3 87.4 33.6 85.1 57.2 70.8 79.6 13.1 68.4 55.1 69.6 79.5 12.4 65.1 57.5 70.1 79.1 12.4 70.1 66.8 75.9 83.8 20.4 76.5 76.2 83.3 88.8 23.6 83. 61.5 73.7 82.9 20.4 72.3 72.2 82.8 88.1 23.1 79.6 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 80.0 84.0 85.8 66.9 72.9 73.1 69.2 76.4 77. 69.7 75.5 76.9 65.9 73.8 73.8 73.5 79.6 79.0 57.8 70.9 70.3 69.1 79.9 79.7 76.7 84.0 84. 17.0 49.7 49.9 82.4 84.9 88.2 79.7 85.1 86.5 52.1 62.4 60.7 57.4 66.7 63.0 90.9 88.1 89. 60.4 72.1 80.9 15.8 71.5 69.0 76.6 76.9 61.6 72.8 80.9 18.5 71.3 67.2 75.6 75.9 Table 8: Detailed results on multilingual retrieval tasks (NDCG@10, in %). EuroBERT: Scaling Multilingual Encoders for European Languages Model en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World European Languages Extra-European Languages Average XNLI XLM-RoBERTa-base 80.0 XLM-RoBERTa-large 87.1 XLM-RoBERTa-XL 89.0 mDeBERTa-v3-base 84.9 mGTE-MLM-base 81.1 ModernBERT-base ModernBERT-large 75.1 71.5 82.9 80.4 84.5 81.5 81.0 77.6 77.2 73. 69.1 74.4 77.8 81.2 80.5 83.1 76.1 79.1 71.3 75.4 73.2 80.6 82.3 78.4 75.5 74.8 83.0 85.5 81.2 76.9 76.4 83.5 85.3 81.1 78.5 74.0 80.2 83.3 78.1 75.9 72.4 80.6 82.4 77.7 72. 76.6 84.1 86.1 82.0 78.4 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 83.5 87.8 89.6 77.8 82.9 85.5 79.4 84.6 86.4 78.9 74.3 83.6 79.5 85.8 82. 70.6 76.6 76.7 82.0 79.9 83.3 74.2 80.3 83.0 75.1 80.8 82.3 75.3 80.7 82.3 79.9 84.7 86.8 PAWS-X 74.1 81.7 83.7 79.5 75.8 76.6 81.9 84.1 Model en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World European Languages Extra-European Languages Average 93.8 XLM-RoBERTa-base 95.5 XLM-RoBERTa-large 95.8 XLM-RoBERTa-XL 95.7 mDeBERTa-v3-base 94.7 mGTE-MLM-base ModernBERT-base ModernBERT-large 88.0 88.9 91.8 92.4 92.3 92.9 91.3 91.9 88.8 89. 86.4 91.0 91.9 90.2 87.5 87.5 91.4 91.7 90.4 88.2 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 95.6 95.6 96.2 86.5 90.0 91.6 88.7 91.3 91. 88.9 89.9 92.0 92.2 92.5 93.0 QAM 88.9 92.4 92.9 91.9 89.8 89.9 92.2 93.0 European Languages Extra-European Languages Average Model en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World 69.0 XLM-RoBERTa-base 73.3 XLM-RoBERTa-large 75.3 XLM-RoBERTa-XL 71.2 mDeBERTa-v3-base 69.4 mGTE-MLM-base ModernBERT-base ModernBERT-large 67.4 67.3 67.9 74.5 73.3 73.7 76.8 74.1 75.4 73.5 71.5 72.1 68.1 68.4 68.6 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 71.6 73.6 74.7 69.0 68.2 69.6 73.3 71.7 72.9 72.8 72.5 73.3 67.9 73.7 75.4 72.1 68.6 69.6 72.9 73.3 AmazonReviews Model en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World European Languages Extra-European Languages Average XLM-RoBERTa-base 64.9 XLM-RoBERTa-large 66.9 XLM-RoBERTa-XL 67.1 mDeBERTa-v3-base 66.4 mGTE-MLM-base 65.0 ModernBERT-base ModernBERT-large 60.0 59.0 56.8 61.5 62.1 58.5 61.5 63.7 59.2 60.6 60.4 57.7 59.5 61.2 57. 65.0 67.0 67.8 66.1 65.3 60.6 62.4 62.4 61.6 60.9 62.7 64.5 64.7 63.7 62.7 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 65.9 66.7 66.5 65.4 66.4 67. 60.5 61.6 62.8 60.2 60.4 57.7 61.2 61.7 58.1 60.9 62.4 59.0 63.0 64.0 64."
        },
        {
            "title": "MassiveIntent",
            "content": "61.1 63.1 63.6 62.1 61.5 61.7 62.6 63."
        },
        {
            "title": "Model",
            "content": "en de es fr it nl pl pt ar hi ja ru tr vi zh"
        },
        {
            "title": "European Languages",
            "content": "Extra-European Languages"
        },
        {
            "title": "89.1\nXLM-RoBERTa-base\n90.3\nXLM-RoBERTa-large\n89.9\nXLM-RoBERTa-XL\n88.1\nmDeBERTa-v3-base\n89.0\nmGTE-MLM-base\nModernBERT-base\n— — — — — — — — — — — — — — — —\nModernBERT-large — — — — — — — — — — — — — — — —",
            "content": "86.0 87.1 86.9 85.9 86.3 86.8 88.8 87.9 87.0 86.3 86.2 88.1 88.0 86.0 86.6 86.1 87.7 87.6 86.4 86.3 87.0 88.0 88.2 86.9 87.4 87.5 88.5 88.7 87.6 87. 87.5 89.1 88.3 88.0 87.9 86.6 89.0 88.6 87.3 87.9 87.3 88.8 88.6 86.9 87.8 79.0 83.5 81.8 79.8 80.7 86.5 88.9 88.4 87.3 87.9 87.3 89.0 89.2 87.3 87. 85.6 87.8 87.8 85.9 86.4 86.7 88.9 88.4 86.5 87.4 87.2 88.8 88.5 87.3 87.5 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 89.0 89.2 88.9 86.0 86.6 87. 86.9 87.4 88.0 86.9 87.6 88.7 87.0 88.1 87.9 87.1 88.2 88.2 86.8 87.3 88.1 87.9 87.8 88. 81.2 82.7 83.2 86.9 87.3 87.6 87.4 88.3 89.0 87.2 88.2 88.1 85.8 86.8 87.1 85.0 86.1 85. 86.0 87.0 87.0 87.2 87.8 88.2 86.3 88.2 87.9 86.5 86.9 86.5 87.2 87.5 Table 9: Detailed results on multilingual sequence classification tasks (accuracy, in %). EuroBERT: Scaling Multilingual Encoders for European Languages"
        },
        {
            "title": "Model",
            "content": "en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World"
        },
        {
            "title": "European Languages",
            "content": "Extra-European Languages"
        },
        {
            "title": "SeaHorse",
            "content": "XLM-RoBERTa-base XLM-RoBERTa-large XLM-RoBERTa-XL mDeBERTa-v3-base mGTE-MLM-base ModernBERT-base ModernBERT-large 32.8 35.5 36.1 31.1 55.8 25.5 34.5 28.1 39.3 30.5 42.2 23.1 31.3 60.7 67.6 32.0 31.7 36.8 35.0 34.3 39.2 35.4 35.7 40.5 18.6 30.2 36.4 50.8 59.5 55.8 43.8 51.0 53.1 44.0 65.8 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 54.2 56.1 59.0 58.5 59.6 62.5 65.1 68.2 69.7 71.8 71.2 74.4 69.3 73.2 75.3 60.2 59.3 60.7 61.8 61.9 64."
        },
        {
            "title": "SummEval",
            "content": "34.2 38.0 39.6 30.7 60.0 62.6 65.2 67."
        },
        {
            "title": "Model",
            "content": "en de es fr it nl pl pt ar hi ja ru tr vi zh Euro World"
        },
        {
            "title": "European Languages",
            "content": "Extra-European Languages"
        },
        {
            "title": "Average",
            "content": "XLM-RoBERTa-base XLM-RoBERTa-large XLM-RoBERTa-XL mDeBERTa-v3-base mGTE-MLM-base ModernBERT-base ModernBERT-large 26.9 37.3 36.3 25.5 41.6 29.9 33.6 19.8 27.0 44.4 28.1 28.1 26.9 22.7 40.2 38.6 38.6 43.2 35.1 32.6 30.3 29.8 22.7 25.4 26.0 28.8 49.0 39.5 34.7 43.7 EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 57.5 57.4 65. 36.9 52.4 58.8 44.4 62.4 51.3 46.4 30.1 46.3 57.5 44.8 57.4 61.9 51.6 59.4 27.1 38.6 30.7 25.9 41.9 43.1 54.9 57.8 Table 10: Detailed results on multilingual summary evaluation tasks (Spearman rank correlation, in %). European Pairs Extra-European Pairs en-xx xx-en Other en-xx xx-en Average Euro World en-de en-pl de-en pl-en de-fr fr-de en-ja en-ru en-tr en-zh ja-en ru-en tr-en zh-en XLM-RoBERTa-base XLM-RoBERTa-large XLM-RoBERTa-XL mDeBERTa-v3-base mGTE-MLM-base ModernBERT-base ModernBERT-large EuroBERT-210m EuroBERT-610m EuroBERT-2.1B 45.3 50.7 55.1 53.5 48.6 52.9 52.9 49.1 53.6 66.0 66.9 63.1 55.2 58.4 61.1 57.8 26.7 30.7 35.9 33.1 30.4 33.2 32.4 29.8 16.3 15.8 18.7 20.1 18.5 17.5 18.2 19.3 31.4 41.1 46.7 45.5 37.5 40.6 42.6 38.3 32.0 29.8 43.3 42.6 35.9 40.3 39.2 38.5 47.0 52.1 56.3 53.0 48.3 51.1 51.3 47.8 56.5 61.2 64.9 61.9 57.2 57.9 59.4 56.9 61.5 66.2 64.8 67.9 59.7 57.3 62.3 56.5 42.5 47.2 52.2 48.5 45.5 48.3 48.6 45.0 10.4 11.0 13.1 10.5 10.6 14.3 12.2 10.7 21.8 24.8 27.0 25.2 23.4 26.7 26.6 23.5 40.7 45.4 47.7 47.9 41.5 44.3 44.1 41.3 25.3 29.0 30.6 29.5 27.3 30.8 29.7 27.5 34.2 39.0 44.4 43.0 37.7 40.5 41.1 38.8 36.5 40.8 44.5 43.0 38.5 41.0 41.5 38.7 Table 11: Detailed results on the WMT task (Spearman rank correlation, in %)."
        }
    ],
    "affiliations": [
        "Artefact",
        "CINES",
        "CNRS",
        "Carnegie Mellon University",
        "Diabolocom",
        "Equall",
        "INSA Rennes",
        "IRISA",
        "IRT Saint Exupery",
        "ISIA Lab",
        "Illuin Technology",
        "Instituto Superior Tecnico & Universidade de Lisboa (Lisbon ELLIS Unit)",
        "Instituto de Telecomunicacoes",
        "LISN",
        "MICS, CentraleSupelec, Universite Paris-Saclay",
        "Unbabel",
        "Universite Grenoble Alpes, Grenoble INP, LIG",
        "Universite Paris-Saclay"
    ]
}