{
    "paper_title": "Find Any Part in 3D",
    "authors": [
        "Ziqi Ma",
        "Yisong Yue",
        "Georgia Gkioxari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study open-world part segmentation in 3D: segmenting any part in any object based on any text query. Prior methods are limited in object categories and part vocabularies. Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zero-shot to any object. Our approach, called Find3D, trains a general-category point embedding model on large-scale 3D assets from the internet without any human annotation. It combines a data engine, powered by foundation models for annotating data, with a contrastive training method. We achieve strong performance and generalization across multiple datasets, with up to a 3x improvement in mIoU over the next best method. Our model is 6x to over 300x faster than existing baselines. To encourage research in general-category open-world 3D part segmentation, we also release a benchmark for general objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/"
        },
        {
            "title": "Start",
            "content": "Find Any Part in 3D"
        },
        {
            "title": "California Institute of Technology",
            "content": "4 2 0 2 0 2 ] . [ 1 0 5 5 3 1 . 1 1 4 2 : r Figure 1. FIND3D is an open-world part segmentation model that can segment any part of any object with any text query. Our method consists of Data Engine powered by 2D foundation models SAM & Gemini that automatically annotates 3D assets from the web. Using the labeled data, FIND3D trains transformer-based point cloud model with contrastive-based training recipe. Our method can segment diverse 3D objects with rich set of part queries, such as the easel, as well as in-the-wild 3D reconstructions obtained via imageto-3D methods, such as the push car captured with an iPhone and the capybara with hat generated by DALL-E, shown at the bottom right."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We study open-world part segmentation in 3D: segmenting any part in any object based on any text query. Prior methods are limited in object categories or part vocabularies. Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zeroshot to any object. Our approach, called FIND3D, trains general-category point embedding model on large-scale 3D assets from the internet without any human annotation. It combines data engine, powered by foundation models for annotating data, with contrastive training method. We achieve strong performance and generalization across multiple datasets, with up to 3 improvement in mIoU over the next best method. Our model is 6 to over 300 faster than existing baselines. To encourage research in generalcategory open-world 3D part segmentation, we also release benchmark for general objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/ We study open-world part segmentation in 3D. By openworld, we mean that: 1) The input can be any object. 2) the model should handle any text query, enabling flexibility in granularity (e.g., limbs versus arm) and description type (e.g., glove versus hand). Recent advancements in foundation models show that single models can achieve an impressive understanding of general concepts in language and 2D images. These results beg the question: how far are we from foundation model that understands the 3D world? To answer this question, we first ask: can we build model that can find any part of any object in 3D? Such capability is critical to robotic and VR/AR applications where grounding objects and their parts in 3D is necessary for precise manipulation, interaction, and spatial awareness. We study the point cloud representation, which is the most readily available representation in such applications. Segmentation is longstanding problem in computer vision. However, the problem has generally been formulated in closed world, and at the whole object level (e.g., car). Some efforts tackle perception below the object level (e.g., 1 wheel of car), mainly driven by ShapeNet-Part [29], closed-world benchmark of 16 object classes such as tables and chairs, with 6 or fewer predefined parts per object. Recent vision and language foundation models have shown impressive zero-shot generalization capabilities in 2D image understanding [1, 18]. These models are impressive as they operate in an open-world setting. Unfortunately, they do not apply to 3D representations. To build an effective open-world 3D model, 3D training data is needed for diverse set of object types, shapes, and parts. However, it is not easy to acquire such data. In this paper, we develop data engine for automatically labeling online 3D assets, and train an open-world 3D part segmentation model. The data engine is powered by 2D vision and language foundation models and annotates 3D objects with diverse part labels in an automated fashion. Using the data collected by the data engine, we train transformerbased 3D model that takes in point cloud and predicts queryable semantic features for every point. The features are in the latent embedding space of CLIP-like [18] model, so that they can be queried with any free-form text by calculating pointwise cosine similarities with the query embedding. We devise contrastive training objective to handle part hierarchy and ambiguity. Fig. 1 shows the overall schematic of our approach, which we call FIND3D. FIND3D, model of 46.2M total parameters trained on 27K labeled objects, can find any part of any object. We highlight the following contributions: We build zero-shot, part-level, open-world, directprediction model for 3D point cloud semantic segmentation that works on general object categories and part queries. We achieve 3 the mIoU and 6 to over 300 the inference speed compared to existing methods. We develop data engine that labels 3D object parts from large-scale internet data to train general-category model without the need for human annotation. Our data engine creatively combines existing vision and language foundation models. We release benchmark for evaluating open-world partlevel semantic segmentation for general object categories with diversity of objects and no pose constraints. 2. Related Work Closed-world 3D segmentation. 3D segmentation has been studied primarily in closed world and with coarse granularity that cannot go below whole objects. In specific settings such as indoor scenes or self-driving, state-of-theart models are starting to achieve better generalization by training on multiple datasets, such as Mask3D [20] and the PointTransformer series [26, 27, 32]. However, these models are still limited to their benchmark settings, and can only segment whole objects rather than parts. Part-level segmentation is less studied. Early efforts started with the ShapeNet-Part dataset [29] (16 object classes, 6 parts per object). PartNet-E, more recent part-level dataset combining PartNet-Mobility [28] and PartNet [14], introduces articulated objects but is still limited to only 45 categories. Due to the limited number of categories and shared orientations (e.g., chairs all facing right), state-of-the-art part-level models [11, 17] cannot generalize well. Our work tackles both the challenge of generalization and granularity our model is designed to be part-level, and can segment any object part in an open-world setting. 3D aggregation methods based on 2D renderings. With the progress of vision language models in 2D image understanding, some works directly assemble these models to obtain an aggregated 3D understanding without training 3D model. An exemplary aggregation method uses multiview renderings of 3D scenes or objects, obtains their features in 2D based on models like CLIP [18], SAM [8], or GLIP [9], and combines them in 3D based on projection geometry. On the whole object level, such methods include OpenMask3D [22]. On the part level, such methods include PointCLIP [31], PointCLIPV2 [35], PartSLIP [10] for point clouds, and PartSLIP++ [34] for meshes. Because these models take features only from 2D, they lack 3D geometry information and suffer from inconsistency across views. Furthermore, these methods are slow because they perform many inferences and the aggregation logic at test time. Our method works natively on 3D point clouds without an aggregation pipeline, and is significantly faster. Our method also uses 3D geometry information to achieve stronger performance and better robustness to pose changes. Test-time optimization. Test-time optimization methods combine features from 2D models with 3D representation, such as NeRF or Gaussian Splatting. At test time, these methods optimize the 3D representation with the 2Dsourced features attached. LERF [5], Distilled Feature Field [21], and Garfield [6] are based on radiance fields. Feature3DGS [33] is based on Gaussian splatting. These methods can provide good performance, but need to be optimized per scene (or per object), which can take several minutes. Besides being slow, the part-level capabilities have not been well-studied for such methods. Our method, being feed-forward 3D model designed for part segmentation, provides much faster inference with better performance. Distillation methods. Distillation methods train 3D models using 2D annotations. Generalization is key limitation in prior works distillation is usually performed per dataset, even per category. OpenScene [16], whole-object segmentation model for indoor scenes, is distilled per dataset. For part segmentation, PartDistill [23] is distilled per category. Such models cannot perform inference zero-shot on unseen object classes, which is critical in real-world use cases. Our Figure 2. The Data Engine. We render Objaverse assets into multiple views and pass each rendering to SAM with gridpoint prompts for segmentation. For each mask, we query Gemini for the corresponding part name, which gives us (mask, text) pairs. We embed the part name into the latent embedding space of vision and language foundation model such as SigLIP. We back-project mask pixels to obtain the points associated with each label embedding, yielding (points, text embedding) pairs as visualized on the right side of the figure. approach can be considered distillation method that tackles the challenge of zero-shot generalization. 3. Method We propose method, FIND3D, to locate any object part in 3D based on free-form language description, such as the wheel of car. As shown in Fig. 1 (panel b), we design model that takes in point cloud and outputs queryable semantic feature for every point. This semantic feature is in the latent embedding space of pre-trained CLIP-like [18] model, such as SigLIP [30]. For any text query, we embed the query using the same model and calculate its cosine similarity with each points feature. This yields pointwise similarity score that reflects the confidence of the part being located at that point. This score can be used to segment the object or localize specific parts. for and normals, any point pi Formally, given point cloud = {pi, ...pn} = with color (x, y, z, nx, ny, nz, r, g, b) R9, we want to find semantic feature fi Rd which belongs in the same latent embedding space as CLIP-like model, e.g., SigLIP. These semantic features attached to the point cloud allow us to query the point cloud in 3D space in an open-world fashion for any text s, we can get its SigLIP embedding (s) and compute its cosine similarity with fi, cos(T (s), fi). To obtain such model, FIND3D consists of data engine and contrastive training recipe. The data engine automatically annotates online 3D assets, leveraging 2D foundation models. These annotations are used to train Transformer-based [24] model for 3D point clouds. We use contrastive learning objective to overcome the challenge of label ambiguity (e.g. one point associated with snowball and body from different views). Our data engine and contrastive training recipe enable scalable training on diverse objects and part labels. unprojects them to 3D. As illustrated in Fig. 2, Our data engine leverages SAM [8] and Gemini [19] to annotate 3D assets from Objaverse [3]. Since Objaverse assets do not have fixed orientation, and Gemini provides higher-quality labels to objects seen in familiar orientations, we first prompt Gemini to select the best orientation based on 10 renderings (from different camera angles) of an object in each orientation. For the chosen orientation, we pass all renderings to SAM with grid point prompts. We discard masks that are too small (less than 350 pixels out of 500x500 image), too large (greater than 20% of all pixels), or with low confidence from SAM. We overlay each mask on the original image and ask Gemini to name the shaded part. Prompts are detailed in the appendix. Masks from the same camera view with the same label are merged. This process generates labeled (mask, text) pairs. We map each mask to set of points in the point cloud based on projection geometry. To make the point features queryable by language, we align point features to the language embedding space of pretrained model, such as SigLIP. We embed the label texts and use the text features as supervision. Thus, our label (mask, text) pairs become (points, text embedding) pairs. The data engine processes 36044 high-quality objects of Objaverse under LVIS categories selected by [12, 13]. After filtering out objects with insufficient labels, we obtain 30K annotated objects from 761 categories with 1.5 million part annotations. Each part is annotated differently from different views, denoting various aspects of part, such as location (e.g., bottom), material (e.g., snowball), and function (e.g., body). Labels also have different levels of granularity. For example, in Fig. 2, one granularity is individual snowballs, and another granularity is the whole snowman. The diversity of our labels helps the model handle the inherent ambiguity in segmentation. 3.1. Data Engine 3.2. Open-World 3D Part Model Obtaining large-scale 3D annotations for generic object categories with human-in-the-loop pipelines is onerous. To this end, we develop scalable data engine that leverages annotations from 2D foundation models and geometrically As shown in Fig. 3, FIND3D takes in point cloud and returns pointwise semantic feature that can be queried with any free-form text via cosine similarity. For any number of user-specified text queries, FIND3D first embeds them via 3 Figure 3. FIND3D: an open-world part segmentation model. FIND3D takes in point cloud, voxelizes and serializes the points via spacefilling curves into sequence. The sequence is passed through transformer architecture which performs block attention, shuffling, pooling and unpooling, returning pointwise feature that is in the embedding space of vision and language foundation model, denoted by . These features can be queried with any free-form text. FIND3D is trained with contrastive objective. For each (points, text embedding) label from the data engine, we use the averaged feature of these points as the predicted embedding, and pair it with the text embedding to form positive pair in the contrastive loss. the SigLIP text encoder. FIND3D then computes pairwise cosine similarity of each point and the text queries. To produce 3D segmentations, FIND3D assigns each point to the text query with the highest cosine similarity, and assigns no label if all queries yield negative similarity scores."
        },
        {
            "title": "3.2.1 Architecture",
            "content": "FIND3D adopts point transformer architecture that treats point clouds as sequences. We build upon the PointTransformer3 (PT3) model [26]. Our design is illustrated in Fig. 3. First, the point clouds are converted into sequences. To this end, the normalized point clouds are sampled with voxel size of 0.02, so that there is at most one point per voxel. The voxels can be serialized in different methods, including Z-Ordering, Trans-Z-Ordering, Hilbert, and Trans-Hilbert [4, 15, 26]. These different serialization schemes are used in different layers in our architecture, following PT3. The serialized point cloud forms sequence of length , where is the number of points. Each point carries its XYZ, RGB, and normal information. These values are embedded with linear layer into point embedding, and conditional positional encoding [2] is added to the embedding. The embedded sequence is then passed to the transformer, which comprises an encoder and decoder. Since the point sequence can be arbitrarily long and the quadratic complexity of attention can grow prohibitively expensive, we chunk the sequence into 1024-size blocks as in the PT3 architecture. Attention is only performed within each block. To achieve the large receptive field of transformers, shuffling is applied per layer. Each shuffling operation maps the current serialization scheme to the (different) serialization scheme used by the next layer. In the encoder, pooling is applied per layer. Although pooling is dependent on the serialization scheme, the goal is to pool points that are neighbors in the 3D space, since all our serialization schemes preserve locality (to the extent possible). As result, the sequence becomes shorter, and the receptive field becomes larger, even though block attention has fixed size. In the decoder, this process is reversed unpooling happens by back-tracing to pooled points in the encoding stage. Block attention happens similarly to the encoder, while shuffling is also back-traced. Eventually, we recover sequence of the same length as the input (i.e., number of points), but of different latent dimension. Since we want to align the point feature into the latent embedding space of SigLIP, we append lightweight 4-layer MLP to the last layer of the transformer. This returns 768-dimension feature per point. Our model contains 46.2 million parameters. During training, the contrastive loss is only applied to points that are kept during voxel sampling. At inference time, the model only predicts features for the points that are kept during voxel sampling. For the points that are dropped, we assign the feature of the nearest neighbor which is kept during voxel sampling."
        },
        {
            "title": "3.2.2 Training",
            "content": "The data engine provides diverse part labels. However, it is difficult to define direct pointwise loss because: 1) The same point can have multiple labels that denote various aspects of part such as location, material, and function. Some labels may also be incorrect. 2) Many points are unlabeled - as shown in Fig. 3 (right), each mask only labels points visible from one camera view. Due to these challenges, we resort to contrastive loss that does not rely on per-point labeling. As illustrated on the right side of Fig. 3, we define the pairing as follows: for each label, the ground truth is the SigLIP embedding of the text. The predicted value is taken as the average feature of all points belonging to the label. This pooling can also be regarded as way to denoise the labels while an individual point might be affected by conflicting or incorrect labels, it is unlikely that all points are subjected to the same 4 conflicting or incorrect labels. Our data engine provides (points, text embedding) labels, which we denote as (Ci, (labeli)) where Ci is subset of the point cloud that this label applies to, and (labeli) is the label embedding. We denote the pooled feature from the labeled points as (Ci), where is our model. We define the contrastive loss as follows: li = log exp(f (Ci) (labeli)) j=1 exp(f (Ci) (labelj)) (cid:80)B (1) where is the indices of all labels of all objects in batch. Because each label is unprojected from single view, they only include points visible from that view and cover part only partially. Although this illustration only shows one object, during training we include 64 objects in the batch, corresponding to 3000 positive pairs per batch. To achieve generalization, in addition to training on diverse data provided by the data engine, we also apply data augmentations, including full random rotation (implemented as sequential random rotation along all three axes), scaling, flipping, jittering, chromatic auto contrast, chromatic translation, and chromatic jitter. These augmentations help avoid over-reliance on object poses and color, and nudges the model to take up 3D geometric cues. We perform 90:10 train-validation split on the 27552 objects provided by the data engine, and train with the Adam optimizer [7] with cosine annealing learning rate schedule, starting at 0.0003 and ending at 0.00005 over 80 epochs. 4. General Open-World 3D Part Benchmark 3D part segmentation has largely been shaped by benchmarks like ShapeNet-Part [29] and PartNet-E [14, 28], which annotate CAD models from small number of categories, such as chairs and tables, with fixed set of parts. Additionally, these benchmarks position objects in canonical orientations (e.g., all chairs face right), biasing the trained methods to handle only canonically oriented objects. We introduce new benchmark featuring diverse range of objects, shapes, parts, and poses to advance 3D object part segmentation toward more general objects and parts, without assumptions about object orientation. Our hope for this benchmark is to advance real-world applicability of 3D models in more variable, in-the-wild scenarios. We source 3D objects from the large Objaverse dataset [3] with variety of object types and shapes. The benchmark is divided into two sets described below. Objaverse-General is dataset of 100 objects from 100 diverse object categories, such as gondola, slide, lamppost, easel, penguin. This set is labeled with total of 350 parts, annotated by humans to guarantee high quality annotations. These objects are in random orientations. Dataset Objaverse-General Objaverse-ShapeNetPart # Objects 100 32 # Categories 100 # Parts 350 90 Table 1. Statistics of the two benchmarks we propose for general, open-world 3D part segmentation. Objaverse-ShapeNetPart contains 32 objects from the same 16 object types as in ShapeNet-Part [29]. Unlike ShapeNet-Part, the objects come in random orientations. This benchmark is used to evaluate robustness with regard to distribution shift for models trained on ShapeNet-Part. Tab. 1 provides statistics of our proposed benchmarks. To comprehensively evaluate generalization of our model, we randomly select 50 out of the 100 Objaverse-General categories. We hold out all objects of these categories, as well as visually similar categories during training. We evaluate our model both on the 50 held-out categories (UnseenCategories) and the other 50 categories (Seen-Categories). All 100 instances in the benchmark are held out during training to ensure no data leakage during evaluation. 5. Experiments Our open-world 3D part segmentation experiments show: FIND3D obtains strong performance on generic object categories both for seen and unseen categories, achieving more than 3x mIoU of the second best method. FIND3D exhibits strong out-of-distribution generalization, whereas baseline methods perform poorly on datasets they are not trained on, as shown qualitatively in Fig. 4 and quantitatively in Tab. 2. FIND3D is robust to variations such as query text prompt rephrasing and object rotation, whereas baselines are sensitive to these changes. This is shown qualitatively in Fig. 5 and quantitatively in Fig. 6. FIND3D can segment single-image reconstructions from in-the-wild images, such as iPhone photos and DALLE2 generations, as shown in Fig. 1 (panel c). FIND3D is the most efficient method 6x to over 300x faster than open-vocabulary baselines, as shown in Tab. 2. 5.1. Experimental Settings Benchmarks. In addition to our proposed benchmark (Sec. 4), we also evaluate on two commonly used datasets for 3D part segmentation: ShapeNet-Part [29] and PartNetE [14]. ShapeNet-Part contains 16 object categories with canonical poses. PartNet-E combines PartNet [14] and PartNet-Mobility [28]. Its test set contains 45 household object classes. For both datasets, we evaluate on their test set both in the canonical pose and in randomly rotated (around all axes) pose, forming four additional benchmarks: ShapeNetPart-Canonical, ShapeNetPart-Rotated, PartNetE-Canonical, and PartNetE-Rotated. Figure 4. Qualitative results on the Objaverse-General benchmark. FIND3D demonstrates strong performance on diverse objects, both for seen and unseen categories. PointCLIPV2, trained on ShapeNet-Part, generalizes poorly to novel object categories. PartSLIP++, detection-based method, predicts sparsely because many parts are not successfully detected. OpenMask3D cannot go below the objectlevel granularity, and usually assigns the whole object to one part. mIoU (%) Objaverse-General ShapeNet-Part PartNet-E Canonical Rotated ObjaverseShapeNetPart Canonical Rotated FIND3D (ours) PointCLIPV2 PartSLIP++ OpenMask3D PointNext PartDistill - O - o Time 0.9s 5.4s 174.3s 296.5s Seen Categories 33.78 9.81 2.69 11.81 Unseen Categories Overall 29.99 10.04 1.63 9.41 26.21 10.27 0.57 7.01 28.39 16.91 1.43 8.94 29.64 16.88 0.93 6. 80.44 63.9 ** 32.18 42.15 15.14 1.54 15.87 28.70 16.86 11.28 5.12 12.54 17.62 10.32 3.87 11.93 39.9 ** Table 2. Performance comparison on Objaverse-General, ShapeNet-Part, and PartNet-E. FIND3D is trained on Objaverse, PointCLIPV2 and PointNext are trained on ShapeNet-Part, and PartSLIP++ is trained on PartNet-E (shaded). Besides PartDistill which is categoryspecific, all methods are evaluated zero-shot on other datasets. Among the open-world methods, FIND3D performs best on all datasets, achieving up to 3x higher mIoU and 6-300x faster inference time. - denotes that closed-world methods cannot be evaluated on datasets containing unseen categories. We report class-average mIoU evaluated with fixed prompt ({part} of {object}). Additional evaluations with varying query prompts can be found in the appendix. ** means metric not reproducible and directly taken from papers. Metric. We report class-average intersection-over-union (mIoU) as our metric, which is the mean IoU for all labeled parts per object, averaged across all object categories. Competing Methods. We compare to state-of-the-art openand closed-world methods for 3D segmentation. Open-world Baselines: We compare to three open-world methods. PointCLIPV2 [35] is an open-world 2D-to3D pipeline involving multiple invocations of CLIP [18] using top-k (k 1400) prompts selected on the test Partset. The pipeline is tuned on ShapeNet-Part. SLIP++ [34] is detection-based pipeline involving invocations of GLIP [9] and an algorithm for finding superpoints to improve segmentation. PartSLIP++ is trained on PartNetE. We evaluate its zero-shot checkpoint for fairness of comparison. OpenMask3D [22] is an open-vocabulary, 2D-to3D pipeline trained on holistic scenes. PointCLIPV2 and OpenMask3D are dense methods that assign label to every point. On benchmarks that contain unlabeled points, we provide the text query other as option for no label. Closed-world Baselines: We compare to two closed-world methods: PointNeXt [17] is state-of-the-art closed-world point cloud segmentation model trained on ShapeNet-Part. Due to its closed vocabulary, it cannot be evaluated on all datasets. PartDistill [23] is category-specific 2D-to-3D distillation method, which is open-world in the 2D stage but closed-world in the 3D stage. It cannot be evaluated on unseen object categories due to the category-specific nature of distillation. The code and data for this method are not fully released (only two categories are released). Since we cannot reproduce the approach, we show numbers claimed 6 in the paper. Because PartSLIP++ and OpenMask3D are slow (up to 5 minutes per object), they are infeasible to evaluate on the full test sets. For example, evaluating OpenMask3D on PartNet-E test set would take 628 hours. To facilitate complete and fair evaluations of all methods, we create smaller subsets of 160 objects (10 objects/category 16 categories) for ShapeNet-Part and 225 objects (5 objects/category 45 categories) for PartNet-E. For methods that are efficient to evaluate, we additionally report performance on the full test sets. We observe similar results and rankings of methods as on the subsets and full sets (when applicable). Complete results and the subset indices are provided in the appendix. 5.2. Experimental Results We present strong results both on the benchmarking datasets and for in-the-wild settings. Tab. 2 shows the mIoU of FIND3D and baselines across all benchmarks. Our method outperforms all baselines consistently. The advantage is strongest on generic object categories of the Objaverse benchmark, with 3 mIoU of the best baseline, PointCLIPV2. Our model performs significantly better not only on seen categories, but also on unseen categories, which demonstrates strong out-of-distribution generalization capabilities. On the existing datasets ShapeNet-Part and PartNet-E, our model outperforms models trained on these datasets without having seen these datasets at training time. We note that these numbers are evaluated on the zero-shot checkpoints of baselines, whereas some prior papers report higher numbers based on evaluation of categoryspecific checkpoints. For fairness of comparison, we pass the same text prompt {part} of {object} to all models in our evaluation. Some methods perform extensive top-k prompt search on the test set. In the appendix, we provide more comprehensive results with varying query prompts. Furthermore, our method only takes 0.9 seconds for inference, which is 6 faster than the fastest baseline, PointCLIPV2, and over 300 faster than OpenMask3D. Inference time is the average per-object inference time on the PartNet-E subset evaluated on an A100. Fig. 4 provides qualitative comparison on ObjaverseGeneral. Our method consistently outputs reasonable segmentations, while other methods struggle. PartSLIP++ is detection-based, and predicts no label if nothing is detected. It is trained on PartNet-E with sparse part annotations, and thus tends to predict sparsely. OpenMask3D struggles with the part-level granularity, and it usually only picks one part, or at most two parts, to represent the whole object. We do note that our method tends to generate dense predictions, and only rarely predicts no label. Figure 5. Robustness comparison of PointCLIPV2 and FIND3D. We evaluate both methods on ShapeNet-Part earphone with canonical and rotated orientations, and visually similar earphone from Objaverse-ShapeNetPart. Top-k prompt reproduces evaluation in the PointCLIPV2 paper. PointCLIPV2s performance drops up to 68% with variations in evaluation configurations, whereas our method stays consistent."
        },
        {
            "title": "5.2.1 Quantifying and Comparing Robustness",
            "content": "We evaluate the robustness of our method by varying the query text prompt, the object orientation, as well as the data the data source of similarly-looking objects. domain, i.e. Fig. 5 provides qualitative examples. Robustness to query prompt. PointCLIPV2s reported metric comes from an extensive top-k prompt search on the test set: for each object part, they try 700 LLM-generated prompts. They iterate 2 times over all parts on the test set to determine the best prompt. For fairness of comparison, we also perform this top-k search for our method. Additionally, we evaluate on two prompts that are more likely to be used in real application: the part of object prompt (such as leg of chair), or the part name prompt (leg). As shown in Fig. 6, with change of prompt from top-k to part of object, PointCLIPV2s performance drops from 48.472 to 17.417, i.e., 64% drop of performance, whereas our method only drops 31%, staying much more robust. Robustness to object orientation. To evaluate robustness to object orientation, we apply random rotation by sampling three angles from π to π and applying rotations along each of the X, Y, axis sequentially. As shown in Fig. 6, PointCLIPV2s performance drops 46% whereas our method does not drop but even increases 3%. Robustness to domain. We constructed ObjaverseShapeNetPart, benchmark with objects from the same categories as ShapeNet-Part, but sourced from Objaverse assets. With this domain shift, PointCLIPV2s performance drops from 48.472 to 21.177 by 56%, whereas our method stays robust with 20% increase. We provide more com7 Figure 6. Robustness evaluation of PointCLIPV2 and FIND3D on all ShapeNet-Part categories. We vary the query prompt, object orientation and data source (same object categories from ShapeNet-Part vs. Objaverse-ShapeNetPart). PointCLIPV2s performance drops up to 64%, while our method remains robust. prehensive evaluations with varying configurations on the other datasets in the appendix, which show similar trends."
        },
        {
            "title": "5.2.2 In-the-Wild Results",
            "content": "While varying evaluation settings on benchmark datasets allows for systematic study of robustness, the ultimate test of robustness is whether method can work in-the-wild. To this end, we demonstrate segmentation results on 3D reconstructions from iPhone photos or DALLE-2 generations. We also show the capability to handle flexible text queries in terms of granularity and description style. Segmenting 3D reconstructions of in-the-wild images. Point clouds are relatively easy to obtain in the wild for example, with single-image reconstruction techniques like MCC [25]. FIND3D can work on noisy point clouds reconstructed from iPhone photos or AI-generated images. In Fig. 1, FIND3D successfully segments push car reconstructed from an iPhone photo, and reconstructed point cloud from the DALLE2-generated image of capybara wearing hat. These results demonstrate that FIND3D has great potential in real applications it works with everyday objects that are very different from the synthetic training data, and works with 3D reconstructions that are noisy. Flexibility of text queries. FIND3D supports various query types that might occur in-the-wild. As shown in Fig. 7, FIND3D can locate hands via different query types either by the body part hand or by the clothing gloves. The teddy bear example demonstrates flexibility in query granularity one can query with limbs, combination of arms and legs, or with arms and legs separately. For ease of visualization, the scores are min-clipped at 0."
        },
        {
            "title": "5.2.3 Failure Modes",
            "content": "We observe some limitations of FIND3D: 1) Our model voxel-samples point clouds at the 0.02 resolution (after normalization). Fine-grained parts that are not geometrically prominent, such as bottons on surface, are difficult for point-cloud-only model like ours. 2) Because the model is trained to be rotational-equivariant, it tends to make symmetric predictions where all symmetric parts have the same Figure 7. Our method can support flexible text queries. For Mickey, one can either query by body part such as hand or by clothing such as gloves. For the teddy bear, one can either query the coarser-granularity concept limbs or the finergranularity arms and legs. Figure 8. failure example. The leftmost image is rendering of microwave. The second image shows the point cloud at FIND3Ds sampled granularity, which loses most features. label. Fig. 8 demonstrates an example from the PartNetE dataset. These limitations point to the complementary nature of the 2D and 3D modalities. While lacking in 3D geometry, the 2D modalities can better convey detailed appearance, which allows us to recognize entities such as buttons or inscriptions on surface. Combining the image and the point cloud modality is future direction. 6. Discussions and Conclusions FIND3D is zero-shot, open-world, direct-prediction model for 3D part segmentation on any object. It is enabled by scalable data engine and contrastive objective that allows for training on large-scale, diverse 3D assets from the internet. FIND3D is performant, robust, and efficient. It can work on in-the-wild 3D constructions from iPhone pho8 tos or AI-generated images. We propose some future directions to further improve openworld 3D segmentation. One approach is to combine 2D and 3D modalities to better capture parts without prominent color or geometry characteristics, which are more easily perceived in 2D. Another direction is to investigate the scaling laws of open-world 3D segmentation. FIND3D, at scale of 30K annotated objects and 46.2M parameters, already demonstrates the power of scaling up beyond existing part segmentation datasets. Although its current scale is constrained by budget, we are optimistic that further upscaling of training, and an investigation into the scaling laws, would be beneficial to the community. 7. Acknowledgments We would like to thank Ilona Demler, Raphi Kang, and Jiacheng Liu for feedback on the paper draft. We also thank Jiacheng Liu for help with the project demo. Ziqi Ma is supported by the Kortschak scholarship. This project is funded in part by NSF #1918655, William H. Hurt Scholars Program, Powell Foundation, Google, and Amazon."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional positional encodings for vision transformers. In International Conference on Learning Representations, 2021. 4 [3] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 3, 5 [4] David Hilbert. Uber die stetige abbildung einer linie auf ein flachenstuck. Dritter Band: Analysis Grundlagen der Mathematik Physik Verschiedenes: Nebst Einer Lebensgeschichte, pages 12, 1935. 4 [5] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded In Proceedings of the IEEE/CVF Internaradiance fields. tional Conference on Computer Vision, pages 1972919739, 2023. [6] Chung Min Kim, Mingxuan Wu, Justin Kerr, Ken Goldberg, Matthew Tancik, and Angjoo Kanazawa. Garfield: In Proceedings of Group anything with radiance fields. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2153021539, 2024. 2 Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 5 [8] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 2, 3 [9] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. 2, 6 In Proceedings of [10] Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained imageIn Proceedings of the IEEE/CVF conlanguage models. ference on computer vision and pattern recognition, pages 2173621746, 2023. 2 [11] Marios Loizou, Siddhant Garg, Dmitry Petrov, Melinos Averkiou, and Evangelos Kalogerakis. Cross-shape attention for part segmentation of 3d point clouds. In Computer Graphics Forum, page e14909. Wiley Online Library, 2023. 2 [12] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. [13] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d objaverse subset uids. https://github.com/xxlong0/ Wonder3D/blob/main/data_lists/lvis_uids_ filter_by_vertex.json, 2024. Accessed: 2024-1101. 3 [14] Kaichun Mo, Shilin Zhu, Angel Chang, Li Yi, Subarna Tripathi, Leonidas Guibas, and Hao Su. Partnet: largescale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 909918, 2019. 2, 5 [15] Guy Morton. computer oriented geodetic data base and new technique in file sequencing. 1966. 4 [16] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 815824, 2023. 2 [17] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: revisiting pointnet++ with improved training and scaling strategies. In Proceedings of the 36th International Conference on Neural Information Processing Systems, 2024. 2, 6 [7] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In 3rd International Conference on [18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 9 region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):112, 2016. 2, 5 [30] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 3 [31] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 85528562, 2022. 2 [32] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and In Proceedings of Vladlen Koltun. Point transformer. the IEEE/CVF international conference on computer vision, pages 1625916268, 2021. [33] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2167621685, 2024. 2 [34] Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. Partslip++: Enhancing low-shot 3d part segmentation via multi-view instance segmentation and maximum likelihood estimation. arXiv preprint arXiv:2312.03015, 2023. 2, 6 [35] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world In Proceedings of the IEEE/CVF International learning. Conference on Computer Vision, pages 26392650, 2023. 2, 6 Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3, 6 [19] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3 [20] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transIn 2023 former for 3d semantic instance segmentation. IEEE International Conference on Robotics and Automation (ICRA), pages 82168223. IEEE, 2023. 2 [21] William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, and Phillip Isola. Distilled feature fields enable few-shot language-guided manipulation. In 7th Annual Conference on Robot Learning, 2023. [22] Ayca Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenMask3D: Open-Vocabulary 3D Instance Segmentation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, 6 [23] Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, and Yen-Yu Lin. Partdistill: 3d shape part segmentation by vision-language model distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34703479, 2024. 2, 6 [24] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. 3 [25] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiview compresIn Proceedings of the sive coding for 3d reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90659075, 2023. 8 [26] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48404851, 2024. 2, 4 [27] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: grouped vector attention and partition-based pooling. In Proceedings of the 36th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2024. 2 [28] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: simulated part-based interactive environment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2, [29] Li Yi, Vladimir Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. scalable active framework for 10 Find Any Part in 3D"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Data Engine Fig. 9 shows the prompt we use to obtain object orientations from Gemini. For given orientation, we render the object in 10 different views, and pass the prompt along with 10 renderings to Gemini. We calculate the percentage of yes answers and choose the orientation with the highest yes percentage. Fig. 9 also provides some example objects with answers from Gemini. Figure 9. The prompt used to query Gemini for object orientation. The car and the Christmas tree are in common orientations (and thus will yield higher-quality annotations), whereas the camel and the parasol are not. Fig. 10 shows the prompt we use to obtain part names from Gemini, along with some examples. Figure 10. The prompt used to query Gemini for object part names. We show 2 example masks from different views for potted plant, pair of glasses, teapot, and ring. 1 B. Experiments B.1. Full Results In Table 2 of the main paper, in order to evaluate all methods on the exact same data, we had to report results on subsets of ShapeNet-Part and PartNet-E because methods like PartSLIP++ and OpenMask3D are slow and infeasible to evaluate on the full test sets (e.g., OpenMask3D would take 628 hours on PartNet-E). Here we provide full-set results for methods that are feasible for full-set evaluation in Tab. 5 and Tab. 6. The ranking of methods on the full sets and the subsets are the same. Additionally, in Table 2 of the main paper we evaluate all methods with the part of object prompt. We report results evaluated with various prompts on all datasets in Tab. 3, Tab. 4, Tab. 5, and Tab. 6. The subset indices for ShapeNet-Part can be found at code/Find3D/model/evaluation/benchmark/ benchmark_reproducibility/shapenetpart/ subset _ idxs . json in the supplementary code folder, and indices for PartNet-E can be found at code/Find3D/model/evaluation/benchmark/ benchmark_reproducibility/shapenetpart/ subsetidxs.json in the supplementary code folder. The random rotations used for evaluation are saved at the same folders. Below we detail results on each dataset. Objaverse-General. Tab. 3 shows full results on the Objaverse benchmark. FIND3D shows significant advantage in all configurations, up to 3x mIoU of the second best method. ShapeNet-Part. Tab. 4 and Tab. 5 show full results on the ShapeNetPart categories. Tab. 4 compares all methods with various prompts, orientations, and data sources (ShapeNet-Part vs. Objaverse-ShapeNetPart, benchmark of the same object classes as ShapeNet-Part but sourced from Objaverse that we constructed). We evaluate {part} of {object}, {part}, and top-k prompt. The top-k prompt reproduces PointCLIPV2s evaluation setup. For each object class, PointCLIPV2 runs an iterative search over 1400nparts prompts per object category on the test set to choose the best query text prompts. This hours-long testtime search is unrealistic to perform in real applications. Although our method is not designed with prompt searching in mind, we follow the same procedure for our method for fairness of evaluation. PointCLIPV2 is trained on this dataset, and other methods are evaluated zero-shot. Tab. 4 shows that our method performs the best in 8 out of 9 configurations. While Tab. 4 reports metrics on the subset of ShapeNet-Part so that all methods can be evaluated strictly on the same dataset, for methods that are fast enough to evaluate on the full test set (FIND3D and PointCLIPV2), Seen Categories Unseen Categories {part} of {object} 9.811 2.690 11.810 33.775 {part} 11.267 15.031 11.934 34.095 {part} of {object} 10.270 0.572 7.013 26.205 {part} 11.092 10.427 10.307 27.409 PointCLIPV2 PartSLIP++ OpenMask3D FIND3D (ours) Table 3. Detailed results on Objaverse. Two query prompts are evaluated: {part} of {object}, and {part}. Our method has significant advantage in all evaluation configurations. mIoU(%) Canonical Orientation top-k {part} of {object} {part} top-k PointCLIPV2 PartSLIP++ OpenMask3D Ours 48.666 43.613 16.912 1.432 8.938 28.386 20.215 6.460 10.373 24.085 26.111 43.781 Rotated {part} of {object} 16.878 0.937 6.748 29.637 Objaverse-ShapeNetPart {part} top-k 18.193 6.034 14.556 23.712 21.177 50. {part} of {object} 15.136 1.542 15.870 42.151 {part} 17.110 11.622 13.768 30.018 Table 4. Detailed results on ShapeNet-Part subset. PointCLIPV2 is trained on this dataset, whereas other methods are evaluated zero-shot. We evaluate different orientations, query prompts, and data domains (ShapeNet-Part vs. Objaverse-ShapeNetPart). We evaluate on 3 types of prompts: {part} of {object}, {part}, and top-k. Top-k prompt reproduces the PointCLIPV2 paper, which runs an iterative search over 1400 nparts prompts per object category to choose the best query text prompts. For fairness of comparison, we follow the same procedure to get top-k prompt metrics, although our method is not designed with prompt searching in mind, and it is not realistic to conduct this hours-long searching process at inference time. Our method, despite being zero-shot on this dataset, has the best performance in 8 out of 9 configurationsall configurations except for the canonical orientation with top-k prompt searching. mIoU(%) Canonical Orientation top-k {part} of {object} {part} top-k Rotated {part} of {object} PointCLIPV2 Ours 48.472 41.517 17.471 28.532 20.157 23.569 26.337 42. 17.034 29.966 {part} 18.021 23.794 Table 5. Detailed results on ShapeNet-Part full test set. PartSLIP2 and OpenMask3D are too slow and thus infeasible to evaluate on the full test set. The metrics are very close to the subset results in the previous table. Our method, despite being zero-shot on this dataset, has the best performance in 5 out of 6 configurationsall configurations except for the canonical orientation with top-k prompt searching. This searching process takes over an hour on an A100 and our method is not designed for test-time prompt searching. we also report the full-set evaluation results in Tab. 5. We note the full-set metrics are very close to the subset metrics. On the full set, we also see our method to perform better in 5 out of 6 settings. On both the full set and the subset, FIND3D, despite being zero-shot on this dataset, is the best-performing method in all configurations except for onethe canonical orientation with test-time top-k prompt searching. In this setting, PointCLIPV2, method trained on this dataset and designed with test-time prompt searching in mind, performs slightly better. We note that this searching takes over an hour on an A100, which is unrealistic to perform in real applications. Our method is not designed for test-time prompt searching. PartNet-E. Tab. 6 provides comprehensive evaluations on PartNet-E, both on the subset (for all methods) and on the full set (for methods that are fast enough to evaluate on the full set). PartSLIP++, trained on this dataset, achieves the highest performance with the {part} prompts, yet is very sensitive to prompt variation. We note that PartSLIP++ also releases category-specific checkpoints, but we 2 mIoU(%) Canonical Orientation Rotated Full Subset Full Subset {part} of {object} 11.619 17. {part} 9.647 16.211 {part} of {object} 11.275 5.123 12.538 16.861 {part} 9.700 32.705 11.242 16. {part} of {object} 10.943 17.703 {part} 10.261 16.819 {part} of {object} 10.317 3.866 11.933 17. {part} 10.216 23.033 11.673 17.164 PointCLIPV2 PartSLIP++ OpenMask3D Ours Table 6. Detailed results on PartNet-E test set. We evaluate different orientations and query prompts. PartSLIP++ is trained on this dataset, and other methods are evaluated zero-shot. Cells with - denote that the method is too slow to be evaluated on the full test set. We evaluate with 2 types of prompts: {part} of {object} and {part}. PartSLIP++ achieves the highest performance with the {part} prompts, yet the performance drops 84% when we vary the query prompt. This dataset is more challenging for our method due to the sparsity of labels and the presence of small parts that are not geometrically or colorfully prominent (e.g., buttons on surface with the same color). Nevertheless, our method is more robust to rotation and prompt variation, and clearly outperforms the other baselines not trained on this dataset. use the cross-category checkpoint for fairness of comparison. This dataset is more challenging for our method because 1) many objects contain small parts that are not geometrically or colorfully prominent, such as buttons on surface with the same color. 2) Some objects have very sparse labels, such as bottle with only less than 10% of points labeled as lid. Our method has tendency to predict densely (i.e. less likely to predict no label), and thus performs less well on this dataset. Nevertheless, we see our method to be more robust to rotation and prompt variation, and clearly outperforms the other baselines that are not trained on this dataset. Furthermore, PartSLIP++ is slow 2D-3D aggregation method, taking up to 3 minutes per object. Our method is over 30 faster."
        }
    ],
    "affiliations": ["California Institute of Technology"]
}