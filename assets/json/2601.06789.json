{
    "paper_title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "authors": [
        "Qihao Wang",
        "Ziming Cheng",
        "Shuo Zhang",
        "Fan Liu",
        "Rui Xu",
        "Heng Lian",
        "Kunyi Wang",
        "Xiaoming Yu",
        "Jianghao Yin",
        "Sen Hu",
        "Yue Hu",
        "Shaolei Zhang",
        "Yanbing Liu",
        "Ronghao Chen",
        "Huacan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure."
        },
        {
            "title": "Start",
            "content": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences Qihao Wang1*, Ziming Cheng2*, Shuo Zhang10*, Fan Liu3*, Rui Xu4, Heng Lian5,10, Kunyi Wang6,10, Xiaoming Yu7, Jianghao Yin3, Sen Hu8,10, Yue Hu1, Shaolei Zhang9, Yanbing Liu1, Ronghao Chen8,10, Huacan Wang1,10 2026-01-14 1UCAS, 2NUS, 3ECNU, 4FDU, 5XDU, 6UBC, 7HKUST(GZ), 8PKU, 9RUC, 10QuantaAlpha *These authors contributed equally to this work. Correspondence: zhangshaolei98@ruc.edu.cn, liuyanbing@iie.ac.cn, chenronghao@alumni.pku.edu.cn, wanghuacan17@mails.ucas.ac.cn https://github.com/QuantaAlpha/MemGovern"
        },
        {
            "title": "Abstract",
            "content": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from closed-world limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As plug-in approach, MemGovern provides solution for agent-friendly memory infrastructure. 6 2 0 2 3 1 ] . [ 2 9 8 7 6 0 . 1 0 6 2 : r Figure 1: Performance comparison of SWE-Agent(the base framework) and MemGovern across different LLM backbones on SWE bench Verified."
        },
        {
            "title": "Introduction",
            "content": "With the remarkable progress of large language models (LLMs) in code understanding and generation (Chen et al., 2021; Li et al., 2023; Rozière et al., 2023; Achiam et al., 2023), autonomous software engineering agents (a.k.a, Code Agents) are reshaping the paradigm of programming (Hong et al., 2024; Qian et al., 2023; Wu et al., 2023). Among such tasks, autonomously fixing bugs in GitHub repositories has emerged as key benchmark for evaluating code agent capabilities and became central research focus (Jimenez et al., 2024; Yang et al., 2024a; Liu et al., 2023). In real-world software engineering practice, developers rarely fix bugs from scratch (Ko et al., 2006). When confronted with complex issues, engineers typically search collaborative platforms such as GitHub to examine how similar problems were addressed in the past (Sadowski et al., 2015; Xia et al., 2017). These records on GitHub inherently encode expert debugging reasoning and repair patterns (Rahman and Roy, 2014; Tian et al., 2018). Ideally, an advanced code agent should be able to exploit such open-world experience like human developers, leveraging historical repair strategies to improve its reasoning depth and accuracy when handling difficult bugs (Shinn et al., 2023). Figure 2: Comparison of MemGovern with existing methods. MemGovern learns from human experience by governing raw data into agent-friendly memories. Although GitHub holds vast repository of human experience, converting it into agent-friendly knowledge remains challenging. First, raw issue and pull request discussions contain large amounts of many unstructured and fragmented information, such as social exchanges and procedural communication, which often obscures technical insights (Kalliamvakou et al., 2014; Bird et al., 2009). Second, variations in terminology, module organization, and coding styles across projects hinder the standardized transfer of repair knowledge (Allamanis et al., 2018). Overall, cross-repository human issue experience on GitHub is both noise-dense and highly heterogeneous, lacking an effective governance mechanism to convert it into retrievable and verifiable knowledge representations. This limitation constitutes primary bottleneck that confines most approaches to within-repository retrieval (Jimenez et al., 2024). To address these challenges, we propose MemGovern, governance framework that transforms human experience into agent-friendly experiential memory, thereby providing code agents with an experience infrastructure. MemGovern aims to transform disorganized GitHub repair records into structured memories that can be efficiently exploited by agents (Park et al., 2023; Packer et al., 2023). MemGovern introduces experience governance to automatically clean and standardize cross-repository experience, organizing it into standardized experience cards that capture key dimensions such as modification strategies and validation methods. Accordingly, MemGovern introduces agentic experience search, which enables agents to interact with the experiential memory through multiple rounds of searching and browsing, closely mirroring how human engineers explore prior cases (Singer et al., 2010). This design allows agents to identify the underlying repair logic in similar examples rather than relying solely on shallow semantic matching. Specifically, MemGovern is designed as plug-and-play module that can be seamlessly integrated into existing agent scaffolds with minimal modifications. In our implementation, we adopt the state-of-the-art SWE-Agent as the shared backbone. Our experimental results on the SWEbench demonstrate that MemGovern improves the ability of code agents to resolve real-world bugs by 4.65% on average, owing to 135k governed experience cards."
        },
        {
            "title": "2 Related Work",
            "content": "Memory Construction for Agents. Building effective memory systems requires ensuring data quality and standardization. Early approaches mined fix patterns from commits or patches (Pan et al., 2009; Kim et al., 2013) but captured only syntactic transformations. Recent agent memory systems have explored structured experience organization. ExpeRepair (Mu et al., 2025) maintains dual memory banks separating episodic demonstrations from semantic insights, while SWE-Exp (Chen et al., 2025) extracts multi-level experiences from agent trajectories. Code Agents. Recent advances in LLMs have driven autonomous code agents for automated software engineering. SWE-agent (Yang et al., 2024b) pioneered Agent-Computer Interfaces with specialized tools for LLM-based code navigation, while AutoCodeRover (Zhang et al., 2024) enhanced fault localization through syntax tree representations and spectrum-based techniques. Agentless (Xia et al., 2025) demonstrated that simpler pipeline approaches can match agent performance with lower costs. Trainingbased methods have also emerged: SWE-Fixer (Xie et al., 2025) compiled 110K instances for fine-tuning open-source repair models, BugPilot (Sonwane et al., 2025) generated synthetic bugs for efficient skill learning, and Co-PatcheR (Tang et al., 2025) explored modular patching with component-specific models. Existing approaches largely operate within single repositories or rely on self-generated experiences, overlooking the vast corpus of cross-repository human debugging expertise in GitHubs collaborative ecosystem. In contrast, MemGovern addresses these limitations through comprehensive governance that curates cross-repository human experiences from GitHub at scale. Our approach introduces multi-stage curation to filter noisy discussions, content purification to standardize heterogeneous issue reports, and duallayer protocol that separates retrieval signals from actionable repair logic, enabling effective learning from the collective debugging knowledge embedded in real-world collaborative development."
        },
        {
            "title": "3 MemGovern",
            "content": "In this paper, we propose MemGovern, unified experience governance framework that transforms raw human experiences into standardized, agent-friendly experiential memory , thereby allowing agents to learn from human experience in an effective and reliable manner. As illustrated in Figure 3, MemGovern constructs large collection of experience cards through experience governance and subsequently enables efficient utilization of human experience via agentic experience search. The details are introduced as follows."
        },
        {
            "title": "3.1 Experience Governance",
            "content": "The efficacy of experience-driven learning hinges on the quality of the underlying data. While GitHub offers vast repository of debugging expertise, this open-world knowledge is inherently unstructured and noisy. Historical records are often laden with social chatter, ambiguous process descriptions, and non-standardized terminology, creating \"semantic gap\" that hinders direct agentic retrieval. To bridge this gap, MemGovern involves Experience Governance, systematic pipeline that distills chaotic raw data into structured, high-fidelity experience cards. This process prioritizes information density and experience reliability through three stages: (1) Experience Selection, which filters low-signal noise at both the repository and instance levels; (2) Standardization, which reconstructs raw data through Unified repair-experience protocol; and (3) Quality Control, checklist-based mechanism to ensure memory 3 Figure 3: Architecture of MemGovern. MemGovern selects raw human experiences from GitHub and standardizes them into experience cards, enabling agents to utilize them through agentic experience search. fidelity."
        },
        {
            "title": "3.1.1 Hierarchical Experience Selection\nThe primary challenge in building a reliable experiential memory is the high quality variance, ranging\nfrom unmaintained hobby projects to knowledge-dense issues. Integrating such low-signal data risks\nmemory pollution. To address this, MemGovern uses a two-tiered selection strategy that ensures both\nsource authority and instance completeness.",
            "content": "Repository Selection. To ensure the agent learns from active and high-quality software engineering practices, MemGovern filters for repositories exhibiting sustained maintenance. MemGovern selects topM repositories based on score that balances popularity (Stars Sr) with maintenance intensity (Issues Ir and Pull Requests Pr): Score(r) = λs log(1 + Sr) + λi log(1 + Ir) + λp log(1 + Pr), (1) Instance Purification. where λs, λi, λp represent weights for stars, issues, and PRs respectively. This ensures the source data reflects robust, active development flows. Within rigorously filters (Issue, R, atch) triplets to retain only \"closed-loop\" repair records. valid instance must provide complete chain of evidence: explicit linkage between the issue and merged code, parsable diff, and diagnostic anchors (e.g., stack traces). Furthermore, to strip away social noise, MemGovern analyzes the comment threads and discard instances where the technical-content ratio falls below threshold τ = 0.2, preserving only threads rich in debugging logic. repositories, MemGovern selected"
        },
        {
            "title": "3.1.2 Experience Standardization\nRaw GitHub discussion threads are inherently verbose, often interleaving failure symptoms with imple-\nmentation details, social exchanges, and repository-specific context. Such entanglement obscures the\nessential signals required for effective retrieval. To render these records actionable, MemGovern defines\na unified repair experience protocol and standardizes selected instances accordingly.",
            "content": "The core philosophy of this protocol is to explicitly decouple retrieval semantics from reasoning logic. Concretely, MemGovern begins with content purification, where an LLM compresses the original comment stream by removing non-technical interactions (e.g., greetings, merge notifications) and redundant execution logs. The purified content is then reorganized into two functionally distinct layers within each experience card: the Index Layer and the Resolution Layer. Index Layer. This layer captures the information available at an agents initial observation stage and serves as the primary retrieval interface. It includes normalized Problem Summary and set of generalizable Diagnostic Signals (e.g., exception types, error signatures, component-level tags). Repository4 specific identifiers and incidental implementation details are intentionally removed to maximize semantic matchability across heterogeneous repositories. Resolution Layer. This layer encapsulates the transferable repair knowledge distilled from human debugging processes. It contains the Root Cause analysis, an abstract Fix Strategy, and concise Patch Digest. By isolating causal reasoning and procedural patterns from concrete code artifacts, this layer enables agents to reuse human repair logic beyond the original context. Formally, each standardized experience card Ei is represented as: Ei = Index = Ii, Resolution = Ri. (2) where Ii denotes the index layer with retrievable failure symptoms, and Ri denotes the resolution layer with reusable repair logic. This structural decoupling allows agents to retrieve experiences based on symptom-level similarity encoded in the Index Layer, while executing repairs using abstract reasoning strategies encoded in the Resolution Layer, thereby enabling effective cross-repository generalization."
        },
        {
            "title": "3.1.3 Checklist-Based Quality Control\nAutomated extraction pipelines can occasionally hallucinate details or miss key nuances. To guarantee\nthe reliability of the memory bank, we introduce a checklist-based quality control mechanism that acts\nas a final gatekeeper before ingestion.",
            "content": "We employ an LLM as structured evaluator to score each generated experience card against critical dimensions. Unlike simple filtering, this process incorporates Refine Loop: if cards aggregate score falls below threshold γ, the evaluator provides targeted feedback on the deficient dimensions. The extraction pipeline then regenerates only the problematic sections based on this feedback. This cycle repeats for maximum of three iterations, ensuring that only experiences passing this rigorous quality gate are indexed. Consequently, MemGovern operates on foundation of verifiable expert knowledge rather than noisy raw data."
        },
        {
            "title": "3.2 Experiential Memory Search",
            "content": "Constructing high-quality experiential memory is only half the challenge; the agent must also be able to retrieve and apply this experience effectively. Unlike standard Retrieval-Augmented Generation (RAG), which relies on single-shot context injection, real-world debugging is dynamic process of hypothesis formulation and validation. To support this process, MemGovern adopts an agentic experience search mechanism that mirrors how human engineers navigate technical documentation. Specifically, MemGovern introduces dual-primitive interface that enables agents to interact with well-governed experiential memory and complete complex coding tasks through progressively agentic search over stored experiences."
        },
        {
            "title": "3.2.1 Dual-Primitive Interface\nTo balance the need for broad semantic discovery with the cost of detailed context processing, MemGov-\nern exposes the experiential memory to agent through two primitives: Searching and Browsing.",
            "content": "Searching. The Searching tool serves as high-throughput filter that enables the agent to scan the memory index using query. Given task-specific query q, the tool traverses the index to identify relevant experience cards. Retrieval is based on the Index Layer of each card, ensuring that matches are driven by symptom-level similarity rather than implementation details. The relevance of an experience card Ei is computed via cosine similarity in the embedding space: sim(q, Ii) = ϕ(q) ϕ(Ii) ϕ(q) ϕ(Ii) (3) where ϕ() is embedding function and Ii represents index layer of card Ei. The tool returns ranked list of candidates {(E(k), sim(q, (k)))}K k=1, allowing the agent to assess relevance before proceeding to deeper inspection. Browsing. After promising candidates are identified, the Browsing tool grants access to the detailed resolution layer of selected card. It offers high-precision retrieval without overwhelming the agent with"
        },
        {
            "title": "LLM",
            "content": "Resolved Rate (%) (%) Avg. Tokens (M) Avg. Cost ($)"
        },
        {
            "title": "Other baselines",
            "content": "GPT-4o AutoCodeRover GPT-4o CodeAct GPT-4o SWESynInfer mini-SWE-agent DeepSeek-V3.2-Reasoner mini-SWE-agent Claude-4-Opus mini-SWE-agent GPT-5.2 SWE-Agent vs. MemGovern across LLMs SWE-Agent MemGovern SWE-Agent MemGovern SWE-Agent MemGovern SWE-Agent MemGovern SWE-Agent MemGovern SWE-Agent MemGovern SWE-Agent MemGovern SWE-Agent MemGovern Claude-4-Sonnet GPT5-Medium DeepSeek-V3.1T Qwen3-235B Kimi-K2-Instruct Qwen3-Coder-30B GPT-4o GPT-4o-Mini 28.8 30.0 31.8 60.0 67.6 69.0 66.6 69.8 65.0 67.4 62.8 65.8 47.2 55.4 43.8 51. 48.0 51.4 23.2 32.6 14.0 17.2 - - - - - - - +3.2 - +2. - +3.0 - +8.2 - +8.0 - +3.4 - +9.4 - +3. - - - - - - 2.29 2.35 0.78 0.91 0.89 0.95 1.17 1.42 0.57 1. 0.92 0.99 0.73 0.97 0.99 1.37 - - - - - - 6.94 7.27 1.14 1. 0.18 0.19 0.09 0.10 0.26 0.53 0.07 0.07 1.38 1.84 0.14 0. Table 1: Comparisons with prior agents. is computed against SWE-Agent under the same backbone. raw details. This design ensures that once relevant case is identified, the agent receives clear blueprint of what changed and why, enabling direct transfer of human expertise to the agent."
        },
        {
            "title": "3.2.2 Progressive Agentic Search",
            "content": "Building upon these primitives, we introduce progressive agentic search mechanism that adapts dynamically to the evolving problem-solving state. Unlike conventional approaches that follow rigid, predefined pipeline, the agent continuously reassesses its informational needs and autonomously choosing searching and browsing actions. Query Formulation and Retrieval. The process begins with problem representation. The agent analyzes the current issue to extract diagnostic keywords, such as failure symptoms, failing test names, stack traces, and relevant module identifiers, to construct an initial query Q. It then invokes the Searchings tool to retrieve semantically related candidates. Rather than indiscriminately consuming all results, the agent autonomously assesses the returned metadata and selects the most promising subset for deeper inspection. When the returned candidates are not sufficiently relevant, the agent can revise the query by incorporating additional observed symptoms and re-invoke Searching, enabling multi-round retrieval without prematurely committing to noisy details. Selective Browsing and Analogical Transfer. For the selected candidates, the agent employs the Browsing tool to access their resolution layers. The core challenge here is analogical transfer: the agent must map the historical solution to the current repositorys context. By synthesizing the browsed evidence, the agent induces transfer triplet: Root Cause Pattern Modification Logic Validation Strategy. For instance, if retrieved experience suggests \"adding boundary check for null inputs in the parser\", the agent maps this abstract strategy to the specific variable names and API versions of the current codebase. This results in concrete, actionable repair plan involving specific file locations and code edits. Totally, progressive agentic search enables the agent to autonomously and deeply acquire and exploit experiential memory."
        },
        {
            "title": "Resolved\nRate",
            "content": "DeepSeekV3.1-T Qwen3Coder GPT-4o Base Agent w/ RAG w/ Ag. RAG w/ Ag. Search Base Agent w/ RAG w/ Ag. RAG w/ Ag. Search Base Agent w/ RAG w/ Ag. RAG w/ Ag. Search 62.8 64.4 63.4 65.8 48.0 46.8 48.6 51.4 23.2 31.2 31.2 32.6 - +1.6 +0.6 +3.0 - -1.2 +0.6 +3. - +8.0 +8.0 +9.4 Table 2: Comparison of experience usage strategies. Exp denotes experiential memory. Ag. denotes Agentic. Figure 4: Results of MemGovern under various experiential memory size and MemGovern under various retrieval sizes."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We evaluate our method on SWE-bench Verified. For comprehensive and fair evaluation, we compare MemGovern against SWE-Agent and several strong baselines. Due to space limitations, we provide detailed introduction to the benchmark and baselines in Appendix A."
        },
        {
            "title": "4.2 Configuration",
            "content": "During the bug-fix experience governance process, we collected active, well-maintained open-source repositories from GitHub with more than 100 stars. After verification and data cleaning, we obtained approximately 150K IssuePRPatch triplets. These triplets were then processed by our experience governance pipeline using GPT-5.1 (medium reasoning), as described in Section 3.1. After de-duplication, the final experience card collection contains 135K items. Building on the SWE-Agent framework, we implemented two tools: an Experience Search tool and an Experience Browse tool. Both tools are integrated into SWE-Agents default toolset, together with the anthropic style code editor and patch submission module. The Experience Search tool takes search query and hyperparameter, top-k, which controls the number of experience-card previews returned (default by 10 to balance context length and information gain). The Experience Browse tool takes the unique index of an experience card and returns the full bug-fix experience."
        },
        {
            "title": "4.3 Main Results",
            "content": "Table 1 presents the main results on SWE-bench Verified, comparing MemGovern with the SWE-Agent baseline across seven LLMs. MemGovern consistently outperforms the baseline across all models, demonstrating robust and model-agnostic improvements. The gains are especially pronounced for models with weaker initial performance, while even the stronger models benefit noticeably, highlighting MemGoverns ability to enhance task-solving capabilities across diverse range of LLMs. Although MemGovern introduces some additional token overhead due to its memory mechanism, this cost is outweighed by the average performance improvement of around 4.65%, making it highly acceptable tradeoff."
        },
        {
            "title": "5.1 Ablation of Experience Governance",
            "content": "To study the effectiveness of experience governance in MemGovern, we conduct ablation studies on two fundamental characteristics of governed experience: size and quality. 7 Figure 5: Comparison of using raw unprocessed experience and MemGoverns experience. Effect of Experiential Memory Size. MemGovern adopts an experience-driven agentic framework with governed experiential memory. To verify whether MemGoverns performance gains stem from broader coverage of governed human experience, we evaluate the MemGovern under various experiential memory size. Specifically, as shown in Figure 4, we evaluate MemGovern under 10%-100% of the full experiential memory (135K experience cards). The results show that enlarging the experiential memory steadily improves task resolution performance across LLMs. This improvement can be attributed to higher probability of retrieving relevant and transferable experience cards when the memory coverage increases. Importantly, the gains are monotonic rather than abrupt, indicating that MemGovern does not rely on small subset of exceptional examples but instead benefits cumulatively from diverse, governed experiences. Overall, MemGovern effectively leverages large-scale experience aggregation, validating the necessity of constructing sizable experiential memory through systematic governance. Effect of Experiential Memory Quality. MemGovern further incorporates Experience Standardization and Quality Control to ensure that the experiential memory is not only large but also reliable and agent-friendly. To assess whether the observed gains truly arise from governed experience representations rather than raw historical data, we conduct an analysis experiment that ablates the quality of the experiential memory . Concretely, as reported in Figure 5 we compare MemGovern using the same experiential memory size but different memory contents: (i) raw PR+Patch records without standardization or quality filtering, and (ii) fully governed experience cards constructed by MemGovern through the governance pipeline (see Section 3.1). The results indicate that while raw records can occasionally provide partial benefits, their effectiveness is unstable and model-dependent, likely due to noise, verbosity, and weak alignment with agent retrieval semantics. In contrast, governed experience cards consistently yield stronger and more stable improvements across models. These findings show that MemGoverns improvements are driven by experience governance rather than sheer data exposure, and they empirically validate the effectiveness of standardization and quality control in constructing high-fidelity experiential memory ."
        },
        {
            "title": "5.2 Superiority of Agentic Search",
            "content": "MemGovern adopts an agentic experience search mechanism based on dual-primitive interface. To verify that MemGoverns performance gains stem from the proposed agentic search paradigm rather than from naive experience injection, we compare different experience utilization strategies. Specifically, as shown in Table 2, we evaluate three variants under the same experiential memory : (i) RAG, which performs single retrieval before the repair process and directly injects all retrieved experiences into the context; (ii) Agentic RAG, which allows retrieval to be triggered adaptively during iterative debugging but still follows retrieve-and-inject paradigm; and (iii) Agentic Search, which decouples candidate discovery from evidence usage by first retrieving broader candidate set and then selectively browsing and transferring only relevant experience cards. The results show that Agentic Search consistently outperforms both static and adaptive RAG variants across diverse backbone models, including DeepSeek-V3.1-Terminus, Qwen3-Coder-30B, and GPT-4o. This improvement is not merely result of accessing larger pool of experiences, but stems from the 8 Figure 6: Action composition of the agent across different experience settings. We report the fraction of steps spent on code editing versus other actions. Figure 7: Human evaluation of repository selection. Proportion of issues judged reusable as transferable experience across different ranking rules. agents strategic management of searching depth and breadth. By decoupling searching breadth (broad candidate discovery) from browsing depth (selective evidence grounding and logic extraction), the agent can explore wide range of candidate experiences without being overwhelmed by irrelevant information, while simultaneously diving deeply into the most promising ones leads to extract actionable insights. This allows reasoning over multiple experiences in structured and iterative manner, ensuring that only contextually relevant knowledge is integrated with the current debugging state. In contrast, both RAG variants tend to conflate breadth with depth, making them highly sensitive to noise: weakly related experiences are injected directly into the context, often disrupting the generation process. Overall, our proposed agentic experience search is highly effective, enabling principled exploration of broad and deep experience spaces essential for complex software debugging."
        },
        {
            "title": "5.3 Effect of Various Retrieval Sizes",
            "content": "Furthermore, to study how the number of retrieved candidates affects performance, we conduct ablation study on the retrieval Top-K parameter in the agentic search pipeline. Specifically, as reported in Figure 4, we show The performance of MemGovern under various Top-K retrieved candidates. The results reveal clear and consistent trend across different backbone models. When Top-K is small, increasing the retrieval size leads to steady performance improvements, indicating that larger candidate pool increases the likelihood of surfacing actionable and relevant experiences. However, beyond moderate value of K, the gains gradually diminish and eventually plateau.This behavior aligns well with the design of agentic experience search. At small K, retrieval coverage is the primary bottleneck, and expanding the candidate set helps the agent discover useful analogies. Once sufficient coverage is achieved, further enlarging the candidate pool yields limited benefits, as the agent already has access to representative experience patterns and additional candidates tend to be redundant. Importantly, performance does not degrade at larger K, which suggests that the selective browsing mechanism effectively filters out irrelevant experiences and prevents context overload. Therefore, MemGovern is robust to the choice of retrieval size and that moderate Top-K achieves favorable balance between effectiveness and efficiency. These findings further support the effectiveness of the decoupled retrieval-and-browsing design, showing that agentic search can safely leverage broader retrieval without sacrificing reliability."
        },
        {
            "title": "5.4 Effect of Repository Selection",
            "content": "To validate our repository selection strategy, we conduct human evaluation on 100 randomly sampled GitHub repositories with Sr 100 and similar Issue volumes. We then construct four Top-10 subsets ranked by Star only, Issue only, Pull Request only, and Our method (Section 3.1.1). Two experienced software engineers then assess whether issues in each subset can be distilled into transferable, reusable experiences. As shown in Figure 7, our method yields the highest proportion of issues deemed reusable as experience, suggesting that combining popularity and maintenance signals selects repositories with denser, higher-quality experience candidates and strengthens the memory. 9 Figure 8: Comparison of Agent behavior without experience (Top) vs. with MemGoverns experiential memory (Bottom). While the baseline applies defensive bypass that violates the API contract, MemGovern leverages historical logic to implement semantically correct fix."
        },
        {
            "title": "5.5 Can Agent Learn from Experience?",
            "content": "We further analyze agent behaviors by quantifying the proportion of each operation category (Figure 6). The results indicate that MemGovern improves both success rate and efficiency by reducing unguided exploration and encourages effective self-testing. Specifically, Info Gathering drops from 15.0% to 11.3% under MemGovern, indicating that experience cards offer targeted navigational guidance and reduce extensive repository exploration. This improvement aligns with more balanced editingexecution strategy: the baseline agent often edits prematurely before adequately reproducing or localizing faults (Code Editing: 40.9%; Code Execution: 19.6%), raising the risk of faulty fixes. With MemGovern, the agent shifts focus toward self-testing (Code Execution: 22.3%) and curtails unnecessary edits (Code Editing: 33.3%) by leveraging root-cause and verification cues from experience cards, resulting in more reliable repairs. Besides, although Raw Experience (unprocessed GitHub PR and patches) provides useful cues, it incurs substantially higher verification overhead (35.4% Code Execution), revealing that unrefined experience introduce noise and lead to low-quality fix. MemGovern mitigates this issue through knowledge governance, maintaining effective information while compressing irrelevant contexts."
        },
        {
            "title": "5.6 Case Study",
            "content": "To provide qualitative understanding of how MemGovern enhances autonomous software engineering, we examine how the presence of experience enables the agent to move beyond superficial fixes to robust solutions. Figure 8 illustrates case involving crash in Djangos order_by() function when Meta.ordering contains query expressions. Without Experience (Baseline). The baseline agent, lacking historical context, relies on Mechanical Diagnosis. It traces the indexing error in the stack trace and attempts to fix the crash by blindly enumerating input types. This results in Defensive Bypass patch: it applies superficial type check (if not isinstance(field, str)) to return the raw object. While this suppresses the immediate crash, it violates the expected return contract of the function, potentially causing silent failures downstream. With Experience (MemGovern). MemGovern retrieves relevant experience card and leverages its structured Resolution Layer to guide the repair. Specifically, the Root Cause field informs the agent that OrderBy expressions were accepted... without explicit validation, while the Fix Strategy provides clear directive to add explicit type checking rather than bypassing the object. Leveraging these governed insights, MemGovern implements semantic-aware fix that extracts the field name (field.expression.name), thereby preserving the API contract. This demonstrates how MemGoverns structured schemadecoupling diagnostic signals from actionable resolution logicprovides the precise reasoning necessary to distinguish between superficial symptom-fix and correct repair. We provide further analysis on the superiority of agentic search and the necessity of experience governance in Appendix B."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose MemGovern, framework for governing raw GitHub data into agent-friendly experiential memory, along with corresponding agentic experience search method that provides memory infrastructure for agents. MemGovern constructs 135k experience cards, and improves 4.65% resolution rates on the SWE-bench, demonstrating its superior performance."
        },
        {
            "title": "Limitations",
            "content": "MemGovern is an agent-friendly memory infrastructure that enables agents to better learn from human experiences, thereby achieving improved performance. limitation of MemGovern lies in the additional tokens generated when searching the memory during agent execution. Inevitably, the agent processes more tokens during searching in exchange for higher resolution rates. Overall, considering that MemGovern achieves an average 4.65% improvement in resolution rates compared to SWE-Agent, we believe the slightly higher token consumption is acceptable. Strategies for compressing memory length will be explored in future work."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Miltiadis Allamanis, Earl Barr, Premkumar Devanbu, and Charles Sutton. 2018. survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):137. Christian Bird, Adrian Bachmann, Eirini Aune, John Duffy, Premkumar Devanbu, Vladimir Filkov, and David Lo. 2009. Fair and balanced? bias in bug-fix datasets. In Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, pages 121130. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, HPea Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, and Qianxiang Wang. 2025. Swe-exp: Experience-driven software issue resolution. arXiv preprint arXiv:2507.23361. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zhiyong Wang, Steven Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, and 1 others. 2024. Metagpt: Meta programming for multi-agent collaborative framework. In International Conference on Learning Representations (ICLR). Carlos E. Jimenez, John Yang, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. SWE-bench: Can language models resolve real-world GitHub issues? In International Conference on Learning Representations (ICLR). Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel German, and Daniela Damian. 2014. The promises and perils of mining github. In Proceedings of the 11th Working Conference on Mining Software Repositories (MSR), pages 92101. Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic patch generation learned from human-written patches. In 2013 35th international conference on software engineering (ICSE), pages 802811. IEEE. Andrew Ko, Brad Myers, and Htet Htet Aung. 2006. Exploratory software development. In Proceedings of the 28th International Conference on Software Engineering (ICSE), pages 431440. Elizabeth Lawler. 2024. Appmap speedruns to the top of the swe bench leaderboard. https://appmap.io/blog/ 2024/06/20/appmap-navie-swe-bench-leader/. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, and 1 others. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161. Tianyang Liu, Can Xu, and Julian McAuley. 2023. Repobench: Benchmarking repository-level code autocompletion systems. arXiv preprint arXiv:2306.03091. Weijie Lv, Xuan Xia, and Sheng-Jun Huang. 2024. Codeact: Code adaptive compute-efficient tuning framework for code llms. arXiv preprint arXiv:2408.02193. Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. 2024. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622. Fangwen Mu, Junjie Wang, Lin Shi, Song Wang, Shoubin Li, and Qing Wang. 2025. Experepair: Dual-memory enhanced llm-based repository-level program repair. arXiv preprint arXiv:2506.10484. Charles Packer, Vivian Fang, Shishir Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez. 2023. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560. Kai Pan, Sunghun Kim, and James Whitehead Jr. 2009. Toward an understanding of bug fix patterns. Empirical Software Engineering, 14(3):286315. 12 Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 122. Chen Qian, Xin Cong, Wei Yang, Weize Liu, Yungeng Dang, Juyuan Li, Jiacheng Xu, Dahua Li, Zhiyuan Liu, and Maosong Sun. 2023. Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924. Md Masudur Rahman and Chanchal Roy. 2014. Insight into the pull request development model of github. In Proceedings of the 11th Working Conference on Mining Software Repositories (MSR). Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, and 1 others. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Caitlin Sadowski, Kathryn Stolee, and Sebastian Elbaum. 2015. How developers search for code: case study. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering (ESEC/FSE), pages 191201. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023). Leif Singer, Timothy Lethbridge, Norman Vinson, and Nicolas Anquetil. 2010. Software engineering at the speed of light: how developers search for code. In Proceedings of the 4th International Workshop on Cooperative and Human Aspects of Software Engineering, pages 8487. Atharv Sonwane, Isadora White, Hyunji Lee, Matheus Pereira, Lucas Caccia, Minseon Kim, Zhengyan Shi, Chinmay Singh, Alessandro Sordoni, Marc-Alexandre Côté, and 1 others. 2025. Bugpilot: Complex bug generation for efficient learning of swe skills. arXiv preprint arXiv:2510.19898. Yuheng Tang, Hongwei Li, Kaijie Zhu, Michael Yang, Yangruibo Ding, and Wenbo Guo. 2025. Copatcher: Collaborative software patching with component (s)-specific small reasoning models. arXiv preprint arXiv:2505.18955. Ke Tian, Bo Jiang, and WK Chan. 2018. Learning to represent programs with graphs. In International Conference on Learning Representations (ICLR). Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyuan Zhang, Shaokun Zhang, Jiane Liu, and 1 others. 2023. Autogen: Enabling next-gen llm applications with multi-agent conversations. arXiv preprint arXiv:2308.08155. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2025. Demystifying llm-based software engineering agents. Proceedings of the ACM on Software Engineering, 2(FSE):801824. Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed Hassan, and Shanping Li. 2017. Measuring program comprehension: large-scale field study with professionals. In Proceedings of the 39th International Conference on Software Engineering (ICSE), pages 117128. Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. 2025. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir SWE-agent: Agent-computer interfaces enable automated software engineering. CoRR, Press. 2024a. abs/2405.15793. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024b. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024c. Swe-agent: Agent-computer interfaces enable automated software engineering. Preprint, arXiv:2405.15793. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 15921604."
        },
        {
            "title": "A Experimental Setup",
            "content": "Benchmark. We evaluate our method on SWE-bench Verified, high-quality subset of SWE-bench containing 500 real-world GitHub issues, focused on functional bug fixing in controlled, self-contained setup. For each instance, the model is given only the natural-language issue description and the corresponding repository. Correctness is assessed by running developer-written unit tests against the generated patch, providing consistent and rigorous evaluation of automated bug-fixing performance. Baselines. For comprehensive and fair evaluation, we compare MemGovern against SWE-Agent and several strong baselines including SWE-Agent (Yang et al., 2024c), AppMap Navie (Lawler, 2024), AutoCodeRover (Zhang et al., 2024), CodeAct (Lv et al., 2024) and SWESynInfer (Ma et al., 2024). The comparison is conducted across multiple LLM backbones, covering both open and proprietary paradigms. Specifically, we evaluate four leading open-source models(DeepSeek-V3.1-Terminus, Qwen3-Coder-30B, Kimi-K2-Instruct, and Qwen3-235B) as well as five closed-source models(GPT-4o, GPT-4o mini, Claude Sonnet 4, GPT-5, and Gemini 3 Pro). Notably, MemGovern is implemented as plug-and-play module that can be integrated into existing agent scaffolds with minimal changes; unless otherwise stated, we adopt SWE-Agent as the shared backbone for subsequent experiments."
        },
        {
            "title": "B Additional Case Study Analysis",
            "content": "Following the case study in Section 5.6, we present two additional comparisons to highlight different aspects of MemGoverns performance: the superiority of agentic search over static retrieval, and the necessity of experience governance. B.1 Superiority of Agentic Experience Search Standard RAG often suffers from semantic drift, where retrieved documents share surface keywords but differ in technical substance. Figure 9 compares MemGoverns Agentic Search against Standard RAG baseline on Django issue where number_format renders small decimals in scientific notation. Standard RAG. As defined in our comparisons, the RAG baseline performs single retrieval before the repair process and directly injects all retrieved experiences into the context. In this case, the retrieval is driven by surface similarity with the query django numberformat decimal_pos, fetching irrelevant experiences related to frontend CSS alignment (e.g., text-align: right) or locale-based separators. Influenced by this noise, the agent attempts fix that aggressively mutates the number sign (abs(number)) early in the function, introducing Defensive Bypass that breaks sign handling for other numeric types. Agentic Search (MemGovern). In contrast, MemGovern employs Search-then-Browse workflow. The agent first retrieves candidates and then uses the Browsing tool to inspect the logic. It successfully identifies relevant case regarding scientific notation cutoff and rejects irrelevant frontend issues. By transferring the specific logic found in the experienceadding threshold check, MemGovern produces precise fix that resolves the issue without side effects. This highlights that the ability to iteratively browse and verify experience is crucial for filtering retrieval noise. B.2 Necessity of Experience Governance Finally, we analyze the importance of governing raw data into structured memory. Figure 10 depicts complex issue involving HttpResponse handling memoryview objects from PostgreSQL. Raw PR+Patch Records. When provided with raw PR+Patch records without standardization or quality filtering (Left), the agent is overwhelmed by noise. The retrieved raw data contains extraneous information about BaseHandler and make_bytes that is not central to the specific memoryview edge case. Confused by the verbose diffs and social commentary, the agent misclassifies the issue as generic iterable problem. It produces patch that intercepts memoryview but fails to handle bytearray correctly, causing the code to fall through to generic loop that crashes when iterating over bytes. Governed Experience Cards (MemGovern). MemGovern (Right) utilizes fully governed experience cards where noise has been stripped via Content Purification. The Resolution Layer clearly highlights the specific fix strategy: Handle memoryview and bytearray objects. Freed from the distraction of unrelated code changes in the original PR, the agent focuses on the core logic. It implements focused patch 14 Figure 9: Comparison of Standard RAG (Top) vs. MemGoverns Agentic Search (Bottom). RAG retrieves irrelevant frontend formatting experiences due to surface similarity. MemGoverns agentic workflow allows it to browse and locate the specific backend logic for handling scientific notation thresholds. that explicitly converts both memoryview and bytearray to raw bytes, ensuring correct serialization. This confirms that experience governance is essential to convert noisy open-source data into actionable, agent-friendly memory."
        },
        {
            "title": "C Experience Card Schema",
            "content": "A detailed explanation of MemGoverns Experience Card Schema is described in Fig 11. 15 Figure 10: Comparison of Raw Experience (Left) vs. MemGoverns Processed Experience (Right). Raw experience distracts the agent with unrelated code changes, leading to an incomplete patch. Governed experience isolates the core repair logic, enabling the agent to correctly handle both memoryview and bytearray cases."
        },
        {
            "title": "Fields",
            "content": "\"Problem Summary\" (String): Concise technical summary of the bug root pattern. Must be generalizable; do NOT include repository names, commit hashes, or overly specific variable names. \"Signals\" (List[String], 1018 items): High-signal matchable keywords/phrases extracted ONLY from the issue content. Each item is short term (typically 24 words) covering mechanism/type, symptom/failure mode, trigger/input/environment, and affected component; include common aliases when helpful (e.g., NPE / null pointer). Avoid vague stop words; no duplicates. \"Root Cause\" (String): Evidence-backed causal chain from trigger to failure and the violated assumption. If uncertain, provide 12 candidate root causes, each with supporting evidence and concrete observation/test to disambiguate. \"Fix Strategy\" (String): Design-level summary of how the bug was addressed (e.g., validation/guards, ordering/state transitions, parsing rules, synchronization, error handling/reporting). Mention constraints and compatibility notes (behavior/API, backward compatibility, performance impact), plus trade-offs/risks and mitigations. Do NOT restate the patch line-by-line. \"Patch Digest\" (String): Structured semantic digest grounded in the diff. Must include Changed Areas (generalized components/modules/files) and 38 Key Chunks describing what changed and why, linked back to the root cause. Do NOT paste raw diffs or large code blocks. \"Verification\" (String): Concrete, checkable verification plan and evidence: reproduce steps (before/after), tests added/updated (unit/integration/regression), and key edge cases/boundaries. If the patch has no tests, propose minimal steps with expected outcomes. Figure 11: MemGoverns Experience Card Schema."
        }
    ],
    "affiliations": [
        "ECNU",
        "FDU",
        "HKUST(GZ)",
        "NUS",
        "PKU",
        "QuantaAlpha",
        "RUC",
        "UBC",
        "UCAS",
        "XDU"
    ]
}