{
    "paper_title": "Controllable Human-centric Keyframe Interpolation with Generative Prior",
    "authors": [
        "Zujin Guo",
        "Size Wu",
        "Zhongang Cai",
        "Wei Li",
        "Chen Change Loy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over the synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human guidance signals into the diffusion process for Controllable Human-centric Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for interpolation, our PoseFuse3D, a 3D-informed control model, features a novel SMPL-X encoder that transforms 3D geometry and shape into the 2D latent conditioning space, alongside a fusion network that integrates these 3D cues with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset annotated with both 2D poses and 3D SMPL-X parameters. We show that PoseFuse3D-KI consistently outperforms state-of-the-art baselines on CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS. Comprehensive ablations demonstrate that our PoseFuse3D model improves interpolation fidelity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 9 1 1 3 0 . 6 0 5 2 : r Controllable Human-centric Keyframe Interpolation with Generative Prior Zujin Guo1 Size Wu1 Zhongang Cai2 Wei Li1 Chen Change Loy1 1S-Lab, Nanyang Technological University 2SenseTime Research https://gseancdat.github.io/projects/PoseFuse3D_KI"
        },
        {
            "title": "Abstract",
            "content": "Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over the synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), novel framework that integrates 3D human guidance signals into the diffusion process for Controllable Human-centric Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for interpolation, our PoseFuse3D, 3D-informed control model, features novel SMPL-X encoder that transforms 3D geometry and shape into the 2D latent conditioning space, alongside fusion network that integrates these 3D cues with 2D pose embeddings. For evaluation, we build CHKI-Video, new dataset annotated with both 2D poses and 3D SMPL-X parameters. We show that PoseFuse3D-KI consistently outperforms state-of-the-art baselines on CHKI-Video, achieving 9% improvement in PSNR and 38% reduction in LPIPS. Comprehensive ablations demonstrate that our PoseFuse3D model improves interpolation fidelity."
        },
        {
            "title": "Introduction",
            "content": "Frame interpolation aims to generate new frames between two consecutive video frames to improve temporal smoothness. Traditional interpolation methods [11, 43, 16] assume small, simple motion over short time spans. These methods are challenged when the input frames are widely separated, known as keyframe interpolation or generative inbetweening [52, 44, 39, 10], where the motion between them becomes complex and ambiguous. This challenge is magnified in human-centric videos, where articulated body movements encompass diverse poses and shapes. With human subjects being prevalent in todays video content, there is growing need for interpolation methods to handle large temporal gaps and intricate human motion while offering plausible results. Human-centric keyframe interpolation remains challenging for current methods. Recent approaches [10, 44, 39] leverage generative priors from image-to-video (I2V) models to bridge the temporal gap. These methods condition the interpolation solely on the input keyframes without intermediate guidance. Consequently, they often struggle to resolve motion ambiguities and accurately capture the complex articulated dynamics of human motion. For instance, when keyframes involve large occlusions or non-rigid joint movements, these methods often produce implausible or distorted interpolations (Figure 1(a)) due to insufficient intermediate guidance. FCVG [52] has explored interpolation keyframes with 2D skeletons as control signals for human subjects. However, 2D lines cannot convey full body shape and geometry, leading to unrealistic results (see Figure 1(b)). These methods lack fine-grained control over the interpolation process, limiting their ability to produce flexible, high-fidelity human-centric interpolations. Preprint. Under review. Figure 1: Keyframe Interpolation with Different Strategies. (a) Interpolation using I2V models without intermediate guidance often yields implausible or distorted frames, especially under large motion or occlusion. (b) Skeleton-guided interpolation offers structural cues but lacks geometric detail, resulting in unrealistic body shape and appearance. (c) Our PoseFuse3D-KI employs dense human-centric guidance, enabling temporally coherent and visually plausible interpolations. In this study, we investigate the integration of 3D human conditions into the human-centric keyframe interpolation pipeline. Drawing inspiration from recent advances in human animation [51, 50], we propose to integrate 2D human poses [45] with 3D SMPL-X models [28] as intermediate control signals. These signals provide precise guidance for complex articulated motions: 2D poses offer concise representations of human joint poses, while 3D models capture rich spatial geometry. However, effectively processing these control signals poses challenges. First, common practice renders 3D human models into 2D proxies (e.g., colored surface, normals, depth maps) before encoding, leading to substantial loss of spatial information in occluded regions. Therefore, we need to develop dedicated encoder that preserves occluded 3D details when converting models into control signals. Second, fusing signals with different information content and granularity is nontrivial. This necessitates designing appropriate neural architectures that can accurately extract 3D cues and harmonize them with 2D poses into unified, informative control input. To this end, we introduce PoseFuse3D Keyframe Interpolator, termed as PoseFuse3D-KI, novel pose-control framework for controllable human-centric keyframe interpolation (Figure 1(c)). Our framework is unique in its 3D-informed control model PoseFuse3D, which comprises three jointly trained modules. The first derives control features from visualized conditions; the second encodes and aggregates 3D body geometry into 2D image conditioning features; and the third combines outputs from the first two modules into unified control signal for interpolation. In contrast to Champ [51], which fuses 3D features from rendered visualizations, our encoder processes features directly in 3D and integrates projected features through feature aggregation. To evaluate the proposed PoseFuse3D-KI, we have also built high-quality video dataset for Controllable Human-centric Keyframe Interpolation (CHKI). Most existing interpolation datasets, such as SportsSlomo [5], target small temporal gaps, lack annotations for 2D poses or 3D human models, and offer limited human-centric motion diversity. Therefore, we introduce CHKI-Video, new dataset for systematic evaluation of CHKI algorithms. CHKI-Video comprises 2,614 high-quality video clips of over 180K frames sourced from SportsSlomo [5] and Pexels1 website that hosts high-quality stock videos. Each frame is carefully annotated with bounding boxes, segmentation masks, 2D human poses, and SMPL-X parameters, using state-of-the-art tools [3, 18, 23, 32, 40] supplemented by manual verification. From this collection, we derive benchmark specifically for the CHKI task. We hope this benchmark will help improve controlled keyframe interpolation techniques with its high-quality videos and diverse examples. Our contributions are threefold: (i) We present PoseFuse3D-KI, an effective interpolation framework for human-centric keyframe interpolation, characterized by novel pose-control model, PoseFuse3D. It effectively extracts control signals from 3D SMPL-X and fuses 2D signals, allowing precise and informative control. (ii) For evaluation, we construct CHKI-Video, benchmark dataset with comprehensive human-centric annotations, which are absent in existing interpolation benchmarks. 1https://www.pexels.com/videos/ 2 (iii) Through extensive experiments, we demonstrate that PoseFuse3D-KI delivers state-of-the-art performance on our CHKI-Video benchmark with an improvement of 1.85 dB in PSNR and reduction of 0.0796 in LPIPS."
        },
        {
            "title": "2 Related Work",
            "content": "Frame Interpolation. Traditional frame interpolation methods are primarily designed for temporally adjacent frames and rely on either direct synthesis using convolutional networks [7, 17], or motion representations such as dynamic kernels [26, 27, 30, 6, 8, 20] and optical flows [15, 21, 19, 35, 13, 1, 11, 16, 41, 46]. Recent advances [10, 44, 39] extend this task to more challenging keyframe interpolation scenarios by leveraging the generative priors of image-to-video diffusion models [2, 37, 33, 12]. These methods combine temporal forward and reverse denoising predictions in unified process to enable interpolation. However, such approaches struggle when faced with complex articulated motions or ambiguous transitions from human keyframes. To alleviate motion ambiguity in interpolation, FCVG [52] introduces 2D matched lines as control signals. However, this control signal lacks the 3D geometric context required for plausible human-centric interpolation, resulting in unrealistic body shape and appearance. Pose-Guided Human Animation. Recent advances in pose-guided human animation [50, 51, 14, 31, 47, 42, 38] harness the power of diffusion models and have achieved remarkable success in generating videos from single reference image. These methods offer flexible and precise control by incorporating enriched conditioning signals and increasingly sophisticated control mechanisms. 2D human poses, such as OpenPose [4] and DWPose [45], are widely used in existing works [51, 50, 31, 47], but they are limited in capturing fine-grained geometry and motion dynamics. To address this, recent works [51, 50] integrate 3D human parametric models [25, 28], which offer realistic body representation through blend shapes and skinning, resulting in more accurate and expressive human animations. Specifically, Champ [51] renders 3D human models into 2D proxies (e.g., normals, depth, and semantic maps) and combines them with 2D pose visualizations as control input. It operates directly on these visualizations and unifies them using simple summation operation. To incorporate control signals into diffusion models, many approaches [38, 42, 47, 50] adapt ControlNet [47] for their customized control networks. Some methods [51, 14] introduce task-specific pose guiders but often require retraining the majority of denoiser parameters. ControlNeXt [31] improves control efficiency by encoding conditions with lightweight convolutional network and injecting them via cross-normalization after the first denoising block, tuning only minimal subset of parameters. This efficient mechanism enables robust conditioning over large-scale pre-trained video generators [37, 2]. In this paper, we demonstrate the advantage of combining 2D poses (DWPose [45]) with 3D human models (SMPL-X [28]) through comprehensive experiments. We present novel pose control model that extracts unified, 3D-informed control features to provide precise guidance. Instead of relying on rendered normals and depth maps, the model directly extracts explicit 3D information from human models in 3D space, preserving richer control signals. It adopts ControlNeXt-inspired strategy to control video diffusion models for keyframe interpolation."
        },
        {
            "title": "3 Method",
            "content": "Given human-centric keyframe-pair I0, IN RHW 3 with timesteps {0, }, Controllable Human-centric Keyframe Interpolation (CHKI) is formulated as: { ˆIi}N 1 i=1 = G(I0, IN , {C}N where denotes an interpolation model guided by control signals {Ci}. In this work, we aim to address CHKI by proposing an effective controllable interpolation framework, PoseFuse3D-KI. This framework integrates 3D-aware human-centric signals into pre-trained Video Diffusion Model (VDM) through our 3D-informed control model PoseFuse3D, as illustrated in Figure 2(a). i=0) (1) 3.1 PoseFuse3D PoseFuse3D is 3D-informed control model that provides 3D human structure and geometry guidance for plausible human interpolation. This 3D-informed guidance is injected into the base diffusion model after the first denoising block via cross-normalization [31]. Internally, PoseFuse3D comprises 3 Figure 2: Model Architecture. Our PoseFuse3D-KI framework, as shown in (a), comprises video diffusion model (VDM) and novel control model, PoseFuse3D. The PoseFuse3D model extracts rich features from both 3D and 2D control signals and fuses them into unified representation to guide the VDM. The key component of PoseFuse3D is the SMPL-X encoder as illustrated in (b), which provides explicit 3D signal features. Specifically, the SMPL-X encoder first extracts 3D information from the SMPL-X model with 2D correspondences via projection. The 3D and 2D information is then encoded in parallel. With features of 2D correspondences, 3D information is aggregated onto the 2D image plane using attention mechanisms. The aggregated features are subsequently processed to produce the final feature S3D. three jointly trained components: visual encoding module that derives control features from 3D SMPL-X [28] renderings and 2D DWPose [45] visualizations; SMPL-X encoder that directly embeds 3D humans and aggregates them into image conditioning maps; and fusion module that integrates encoding streams into unified control tensor to guide interpolation in the base VDM. Visual Encoding. This visual encoding module extracts conditional features from visualized control images, maintaining natural pixel-level alignment with the controlled video latents. To enhance the control signals, we incorporate visualizations from both DWPose and SMPL-X. While SMPL-X renderings provide rich human surface details, their keypoint information is indirect, mixed with other vertices and mesh faces. Therefore, we add DWPose visualizations to emphasize the skeletal keypoint layout for robust pose understanding. This combination was also demonstrated to be effective in prior work [51]. Specifically, our visual encoding module employs two parallel convolutional encoders to capture comprehensive pose information. One encoder processes DWPose visualizations to capture compact pose information, while the other handles SMPL-X renderings that retain 3D cues such as occlusion boundaries and projected shapes. Notably, to enrich semantic detail, we use SMPL-X Colored Surface [50] during rendering, which assigns unique color to each vertex. The resulting feature maps, E2D and E3D, are then passed to the fusion module for unified conditioning. SMPL-X Encoder. Although 3D model renderings offer aligned image maps for conditioning in VDMs, the rendering operation discards parts of 3D information, particularly in occluded regions. This results in implausible interpolation for keyframes of large human motion. For example, in the first case of Figure 3, all variants relying solely on rendered controls fail to interpolate the occluded arms with correct spatial position. To enhance controllability with direct 3D information, we introduce SMPL-X encoder (Figure 2(b)) that processes the SMPL-X model in 3D space and transforms it into image conditioning feature S3D. Specifically, SMPL-X model [28] is parameterized with (θ, β, ψ), corresponding to pose, shape, and expression. We obtain structural information by forwarding these parameters into the SMPL-X model to generate vertex and joint coordinates 3D, 3D in 3D space and obtain their corresponding 2D coordinates 2D, 2D through projection: 3D, 3D = SMPLX(θ, β, ψ), 2D, 2D = Projection(P 3D, 3D), (2) (3) 4 where the joint coordinates 3D, 2D correspond to the pose parameter θ that indicates joint rotations. Notably, the projection step establishes correspondence between the 3D space and the 2D image plane, making it possible to retain 3D spatial structure while producing image conditioning maps. Next, the raw 3D coordinates are processed using an MLP to produce point-wise vertex features and joint features. These joint features are refined through temporal residual block that fuses them with pose information into expressive joint-level representations . To aggregate these 3D features into 2D image control maps for joints and vertices, we employ separate attention mechanisms: OJ = JointAttn(Q, , ), OP = VertexAttn(Q, , ), (4) (5) where RBHW is the flattened d-dimension feature of standard 2D meshgrid extracted by convolutional encoder. Finally, the outputs OJ and OP are concatenated and passed through downsampling block to produce the final SMPL-X control representation S3D, which serves as an informative and compact image embedding of the underlying 3D human structure. Condition Fusion. The condition fusion module combines control features from both 2D and 3D signals into unified representation to guide keyframe interpolation. For robust feature representation, we introduce coarse-to-fine fusion strategy that progressively integrates rich geometric information from the 3D features into the compact 2D pose features. Specifically, we adopt two attentionbased fusion blocks to perform this integration, where each block contains three attention layers for progressive refinement. The first layer is self-attention module that processes the 3D features by operating on the sum of rendering encoding E3D and SMPL-X features S3D. The second layer performs cross-attention, aligning the 3D features with the 2D encoding E2D through spatially localized interaction scheme. Notably, we adapt the shifted window-partition strategy [24, 46] to restrict attention computation to adjacent regions, enhancing local alignment. The third layer applies temporal self-attention to capture temporal dynamic correlations in the fused representation. We use the second fusion block output as the final control signal, which is injected into the base interpolation engine to provide fine-grained, structure-aware guidance during synthesis. 3.2 VDMs as Base Interpolation Engine To supply generative priors for human-centric keyframe interpolation, we adapt pre-trained Video Diffusion Models (VDMs) under the latent diffusion framework [33]. VDMs perform the diffusion process in VAE-encoded latent space, conditioning video synthesis on input frames. Our main experiments investigate Wan2.1 [37] as the base model for keyframe interpolation. Wan2.1 consists of scaled-up DiT-based denoiser [29] and causal 3D-VAE that performs spatiotemporal compression. It employs the flow matching strategy [22, 9] for the diffusion process. It formulates the forward diffusion process as linear interpolation between the clean video latent and the noise ϵ, which adds the noise by: zt = (1 t)z + tϵ. In the backward process, the denoiser fθ iteratively refines zt conditioned on the first frame I0. The training objective is formulated as: = Ez,t,I0,ϵN (0,I)[fθ(zt, I0, t) y2 2], (6) where the target objective is dzt dt = ϵ z. Since the latent space is unevenly compressed in time, noise fusion strategies [10, 52] that combine temporally forward and reverse denoising paths are ineffective. Instead, we adapt Wan2.1 for keyframe interpolation in one unified denoising process. Specifically, we condition Wan2.1 on both endpoint frames I0, IN , and apply LoRA tuning to its input patch embeddings. Additional details are provided in the supplementary material."
        },
        {
            "title": "4 The CHKI-Video Dataset",
            "content": "Existing interpolation datasets [43, 7, 5, 34, 36], which are designed for interpolating temporally adjacent frames, are not suitable for the CHKI task. To address this gap, we built the CHKI-Video dataset in three consecutive stages. More details can be found in the supplementary material. Stage 1: Dataset Collection. To create dataset with challenging and diverse human motion, we curate video clips from SportSlomo [5] and Pexels2. While SportsSlomo provides challenging 2https://www.pexels.com/videos/ 5 human-centric videos, its exclusive focus on sports limits the diversity of activities. To enhance diversity, we compile list of keywords, spanning everyday activities to high-intensity actions. We use these keywords to retrieve videos from Pexels, complementing the sports videos. The curated videos are then downsampled to eliminate frame redundancy unnecessary for keyframe interpolation. Stage 2: Pre-annotation Processing. We first perform general filtering for the low-quality videos according to brightness changes and assessed scores [40]. Then, we use Grounding-DINO [23] and SAM2 [32] to detect, segment, and track human instances in each video. We discard any video of more than three people or is shorter than 20 consecutive frames to guarantee sufficient temporal span for keyframe interpolation. After automated processing, we manually review and filter detections in complex sports scenarios. Stage 3: Human-centric Annotation. Building on the accurate human detections obtained in Stage 2, we annotate each clip for precise human-centric information. First, we employ Sapiens [18] to extract 2D human keypoints and perform whole-body detection to filter out clips with incomplete human figures. This ensures our dataset remains strictly human-centric. Finally, we apply SMPLer-X [3], leveraging its high re-projection accuracy to fit detailed SMPL-X models for human images and produce reliable 3D body parameters for each frame. As result, CHKI-Video comprises 2,614 video clips of over 180K frames, carefully annotated with human bounding boxes, masks, 2D keypoints and 3D parametric models. To prepare the train and test split, we follow the original division for SportsSlomo [5] videos and distribute the Pexels videos according to their keyword frequencies to maintain balanced coverage of all motion categories."
        },
        {
            "title": "5 Experiments",
            "content": "We present quantitative and qualitative results in Sec. 5.1 to validate the effectiveness of our 3D control strategy in PoseFuse3D. We compare our interpolation performance against state-of-the-art methods in Sec. 5.2. Finally, Sec. 5.3 provides detailed ablation study to justify our model design. Implementation Details. We fine-tune PoseFuse3D-KI on the CHKI-Video training split for 70k iterations. Specifically, we fine-tune our 3D-informed control model PoseFuse3D, and employ LoRA on the input patch embeddings, as well as the value and output projections of the VDMs attention modules. During training, we randomly sample 25 consecutive frames from video clips and process them to resolution of 512 320. To maximize human-centric content, we crop each clip around its largest annotated human bounding box. Given the ratio from the target input size, we first crop the maximum scale of the image with the largest box as the center. The cropped image is then resized to match the target resolution. More implementation details are provided in the supplementary material. Evaluation Protocols. We evaluate methods on CHKI-Videos test set using the ground-truth annotations to drive controllable interpolation. With motion ambiguity limited by the ground truth controls, we adopt the standard interpolation metrics of PSNR and LPIPS computed on the whole images for evaluation. To quantify performance specifically on human regions, we further leverage the annotated human boxes and masks, yielding PSNRbbox, LPIPSbbox, PSNRmask, and LPIPSmask metrics. Notably, we apply binary dilation to expand and smooth the masks before computing the metrics, preventing potential artifacts along mask boundaries. 5.1 3D Control Strategy in PoseFuse3D Setup. We evaluate the effectiveness of the 3D-informed control design in PoseFuse3D via comparisons among 3D control strategies, including VE, VE+DN and VE+SE. Specifically, VE refers to the visual encoding on the SMPL-X colored surface rendering, removing the SMPL-X encoder from the full design of our control model. VE+DN extends VE by incorporating depth and normal renderings, using two additional encoders identical to the one used in VE. VE+SE represents our proposed strategy in PoseFuse3D that directly encodes 3D information via the SMPL-X encoder and integrates it with VE. Experiments for these strategies use Wan2.1-I2V [37] as the interpolation backbone. For efficiency, they are trained for 40K iterations. To assess the necessity of 3D information, we compare against FCVG [52], which is conditioned solely on 2D signals. For fair comparison, we create variant of VE by replacing the backbone with 6 Table 1: Comparisons of 3D Control Strategies. This table presents quantitative results of different 3D control strategies. The PSNR and LPIPS metrics are calculated for the whole image, as well as for the human-centric parts via annotated boxes or masks. Best results are highlighted with boldface. Method Backbone 3D Control Strategy Evaluation Metrics FCVG PoseFuse3D SVD SVD N.A. VE-SVD PSNR PSNRbbox PSNRmask LPIPS LPIPSbbox LPIPSmask 20.42 20.96 0.0606 0. 0.2100 0.1975 0.0899 0.0835 13.28 13.79 12.14 12.70 PoseFuse3D Wan2.1-I2V PoseFuse3D Wan2.1-I2V PoseFuse3D Wan2.1-I2V VE VE+DN VE+SE (Ours) 21.91 22.07 22.14 14.36 14.35 14.53 13.10 13.03 13.24 0.1400 0.1363 0.1330 0.0682 0.0667 0.0653 0.0484 0.0473 0. Figure 3: Qualitative Results of Different 3D Control Strategies. We use red circles to highlight regions where the 3D controls and our strategy significantly improve the interpolation quality. SVD [2]. We adapt SVD for interpolation with the temporal forward and reverse denoising path fusion strategy used in FCVG. This variant is referred to as VE-SVD for ease of analysis. Results. We provide quantitative comparisons of 3D control strategies in Table 1.In comparisons between FCVG and VE-SVD, we find that adding 3D control improves the interpolation performance. VE-SVD outperforms FCVG across metrics with more than 0.50 dB increase on all the PSNRs, indicating the improvements on both whole-image and human-centric levels. Moreover, our study highlights the importance of explicit 3D information. VE+DN and VE+SE, which incorporate depth and normal maps or direct SMPL-X information, outperform the simpler strategy VE. VE+DN and VE+SE show clear improvements in perceptual quality as reflected by the LPIPS metrics. Notably, our VE+SE strategy, which directly encodes information in 3D space, delivers the best performance, achieving the lowest LPIPSbbox of 0.0653 and the highest PSNR of 22.14 dB. Visualizations. Besides our quantitative analysis, we perform qualitative comparison of the 3D control strategies, as shown in Figure 3. We find that incorporating 3D controls better preserves human shape during interpolation. Take the Tennis case in Figure 3 for example, methods with 3D control strategies interpolate the players body close to the ground truth, whereas FCVG exhibits noticeable distortion. Moreover, our VE+SE, which directly encodes 3D information from SMPL-X, proves effective in handling occluded human motion. In both the Skateboarding (1st row) and Jumping (2nd row) cases in Figure 3, we can observe that our VE+SE strategy produces plausible results for the occluded arms, demonstrating its advantage in complex scenarios. 5.2 Benchmark Results Setup. We compare PoseFuse3D-KI against several advanced interpolation methods on our CHKIVideo dataset. The main comparison is with FCVG [52], which also enables intermediate control during interpolation. For broader coverage, we also include the keyframe interpolation method GI [39] and the traditional video frame interpolation method GIMM-VFI [11]. We also include Wan2.1-KI, an adaptation of Wan2.1 [37] for keyframe interpolation, following the strategy in Sec. 3.2. Since not 7 Table 2: Comparisons with State-of-the-art Interpolation Methods. Methods Metrics PSNR PSNRbbox PSNRmask LPIPS LPIPSbbox LPIPSmask HA GIMM-VFI [11] GI [39] Wan2.1-KI (Ours) FCVG [52] PoseFuse3D-KI (Ours) 20.29 15.81 19.53 20.42 22.27 11. 8.27 11.19 13.28 14.72 10.16 0.1954 0.1187 0. 0.9146 7.26 9.85 12.14 13.47 0.3364 0.2081 0.2100 0.1304 0.1672 0. 0.0899 0.0636 0.1146 0.0868 0.0606 0.0450 0.8954 0.9180 0.9187 0.9189 Table 3: Ablation Study on Visual Encoding and Fusion Module. Model Variants Evaluation Metrics Visual Encoding Fusion Module PSNR PSNRbbox PSNRmask LPIPS LPIPSbbox LPIPSmask Conv-Enc (2D) Dual Conv-Enc Dual Conv-Enc Dual Conv-Enc Dual Conv-Enc N.A. Sum Non-TSA Non-WP Full 0.1990 0.1953 0.1956 0.1938 0.1927 0.1096 0.1059 0.1053 0.1038 0. 0.0836 0.0806 0.0790 0.0775 0.0773 11.33 11.59 11.35 11.60 11.71 12.17 12.43 12.19 12.42 12.53 20.29 20.50 20.30 20.46 20.55 all baselines support ground truth controls, we additionally compute the Human Anatomy (HA) [49] score to assess the quality of human synthesis and support fair comparison. Results. We report quantitative results of PoseFuse3D-KI on the CHKI-Video benchmark in Table 2. Our method delivers state-of-the-art performance on human-centric keyframe interpolation. On whole-image metrics, it boosts PSNR by 1.85 dB and lowers LPIPS by 0.0796 in comparison with the state-of-the-art method FCVG [52]. Crucially, it also outperforms on human-centric metrics compared with other methods, achieving PSNRbbox of 14.72 dB, an LPIPSmask of 0.045, and an HA score of 0.9189. This indicates that our method produces plausible, high-fidelity human interpolations that closely follow the ground-truth dynamics, demonstrating the effectiveness of our method. Visualizations. For qualitative evaluation, we qualitatively compare PoseFuse3D-KI with other advanced methods, as illustrated in Figure 4. Our approach delivers more accurate human interpolations, faithfully following real-world motions and preserving body shape. For example, in the 2nd Fencing case and 4th Stunt Bike case, only PoseFuse3D-KI correctly interpolates leg and arm movements while maintaining consistent shapes. Moreover, our method naturally handles the occluded human motion well with 3D-informed control. We observe that our method interpolates correct spatial positions for the occluded legs (1st case) and arms (3nd case), achieving significant improvements over FCVG. Furthermore, although the control-free keyframe interpolation methods GI and Wan2.1-KI occasionally generate undistorted humans, they often generate implausible motion that violates real-world dynamics, as observed in 1st, 2nd, and 4th cases. 5.3 Ablation Study In this section, we ablate the visual encoding and fusion modules of PoseFuse3D in Table 3. For efficiency, we use SVD as the backbone and process video clips into 9 consecutive frames of 256256. We use 3 temporally downsampled videos from the CHKI-Video test set for evaluation. 3D Visual Encoding. Our visual encoding module includes two convolutional encoders for 2D and 3D control maps, respectively. We denote the variant including this entire module as Dual Conv-Enc, and the one using only the 2D encoder as Conv-Enc (2D). Removing the 3D visual encoding leads to performance drop of 0.26 dB on both PSNRbbox and PSNRmask, highlighting the importance of 3D visual encoding. Fusion Module. In PoseFuse3D, condition features are fused through carefully designed fusion module. To validate its effectiveness, we replace it with simple summation operation [51], denoted as Sum in Table 3. This change leads to significant performance drop, particularly in perceptual quality, with an increase of 0.0033 in LPIPSmask. These results demonstrate the fusion modules contribution to providing informative control for high-quality interpolation. Window-Partition Strategy. PoseFuse3D employs cross-attention layer with shifted windowpartition strategy to fuse features across neighboring windows. To validate this design, we remove the 8 Figure 4: Qualitative Comparisons with State-of-The-Art Methods. window partitioning, denoted as Non-WP. This results in notable drops of 0.11 dB in both PSNRbbox and PSNRmask, indicating that the window-partition strategy enhances controlled interpolation. Temporal Attention in Fusion Module. To justify the efficacy of the temporal self-attention (TSA) layer in the fusion module, we conduct experiments excluding the TSA layer (Non-TSA). This removal causes increases of 0.0022 and 0.0017 in LPIPSbbox and LPIPSmask, demonstrating the crucial role of the temporal self-attention layers in the fusion module."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose PoseFuse3D-KI, controllable human-centric keyframe interpolation framework powered by our novel 3D-informed control model, PoseFuse3D. PoseFuse3D embeds rich spatial geometry from 3D human signals together with 2D poses into unified control feature, enabling the generation of more plausible and realistic intermediate frames. For evaluation, we construct CHKI-Video dataset with comprehensive human-centric annotations. Extensive experiments on the benchmark demonstrate that PoseFuse3D-KI outperforms previous interpolation methods with 9% improvement in PSNR and 38% reduction in LPIPS."
        },
        {
            "title": "References",
            "content": "[1] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In CVPR, 2019. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable Video Diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, et al. SMPLer-X: Scaling up expressive human pose and shape estimation. In NeurIPS, 2023. [4] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In CVPR, 2017. [5] Jiaben Chen and Huaizu Jiang. Sportsslomo: new benchmark and baselines for human-centric video frame interpolation. In CVPR, 2024. [6] Xianhang Cheng and Zhenzhong Chen. Multiple video frame interpolation via enhanced deformable separable convolution. IEEE TPAMI, 2021. [7] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and Kyoung Mu Lee. Channel attention is all you need for video frame interpolation. In AAAI, 2020. [8] Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov. CDFI: Compression-driven network design for frame interpolation. In CVPR, 2021. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [10] Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael Black, and Xuaner Zhang. Explorative inbetweening of time and space. In ECCV, 2024. [11] Zujin Guo, Wei Li, and Chen Change Loy. Generalizable implicit motion modeling for video frame interpolation. In NeurIPS, 2024. [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [13] Xinyu Hou, Liming Jiang, Rui Shao, and Chen Change Loy. Video infilling with rich motion prior. In BMVC, 2023. [14] Li Hu. Animate Anyone: Consistent and controllable image-to-video synthesis for character animation. In CVPR, 2024. [15] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In ECCV, 2022. [16] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and Jan Kautz. Super SloMo: High quality estimation of multiple intermediate frames for video interpolation. In CVPR, 2018. [17] Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, and Du Tran. FLAVR: Flow-agnostic video representations for fast frame interpolation. In WACV, 2023. [18] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision models. In ECCV. Springer, 2024. [19] Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang. IFRNet: Intermediate feature refine network for efficient frame interpolation. In CVPR, 2022. 10 [20] Hyeongmin Lee, Taeoh Kim, Tae-Young Chung, Daehyun Pak, Yuseok Ban, and Sangyoun Lee. AdaCoF: Adaptive collaboration of flows for video frame interpolation. In CVPR, 2020. [21] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. AMT: All-pairs multi-field transforms for efficient frame interpolation. In CVPR, 2023. [22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2022. [23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding DINO: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. [24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. [25] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. SMPL: skinned multi-person linear model. ACM TOG, 2015. [26] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution. In CVPR, 2017. [27] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In ICCV, 2017. [28] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [30] Tomer Peleg, Pablo Szekely, Doron Sabo, and Omry Sendik. Im-net for high resolution video frame interpolation. In CVPR, 2019. [31] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. ControlNeXt: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [32] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. SAM 2: Segment anything in images and videos. In ICLR, 2025. [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [34] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. XVFI: extreme video frame interpolation. In ICCV, 2021. [35] Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris Metaxas, Chen Change Loy, and Ziwei Liu. Deep animation video interpolation in the wild. In CVPR, 2021. [36] Alexandros Stergiou. Lavib: large-scale video interpolation benchmark. In NeurIPS, 2024. [37] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [38] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. DisCo: Disentangled control for realistic human dance generation. In CVPR, 2024. [39] Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steven Seitz. Generative Inbetweening: Adapting image-to-video models for keyframe interpolation. In ICLR, 2025. 11 [40] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. [41] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video interpolation. In NeurIPS, 2019. [42] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In CVPR, 2024. [43] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William Freeman. Video enhancement with task-oriented flow. IJCV, 2019. [44] Serin Yang, Taesung Kwon, and Jong Chul Ye. ViBiDSampler: Enhancing video interpolation using bidirectional diffusion sampler. In ICLR, 2025. [45] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In ICCV, 2023. [46] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In CVPR, 2023. [47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [48] Kaifeng Zhao, Gen Li, and Siyu Tang. DartControl: diffusion-based autoregressive motion model for real-time text-driven motion control. In ICLR, 2024. [49] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, et al. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. [50] Jingkai Zhou, Benzhi Wang, Weihua Chen, Jingqi Bai, Dongyang Li, Aixi Zhang, Hao Xu, Mingyang Yang, and Fan Wang. RealisDance: Equip controllable character animation with realistic hands. arXiv preprint arXiv:2409.06202, 2024. [51] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In ECCV, 2024. [52] Tianyi Zhu, Dongwei Ren, Qilong Wang, Xiaohe Wu, and Wangmeng Zuo. Generative inbetweening through frame-wise conditions-driven video generation. In CVPR, 2025."
        },
        {
            "title": "7 Appendix",
            "content": "In this section, as referenced in the main text, we first provide detailed descriptions of our method and dataset in Sec. 7.1, Sec. 7.2, and Sec. 7.3. We then present additional experiments on our approach, including in-the-wild interpolation results(Sec.7.4), extended benchmarking on the FCVG [52] test set (Sec.7.5), and ablation studies on the control signals (Sec.7.6). as well as the SMPL encoder (Sec.7.7). Additional qualitative results on CHKI-Video are provided in Sec.7.8. Finally, we discuss the limitations and broader impact of our method in Sec.7.9 and Sec. 7.10. 7.1 Wan2.1 for Keyframe Interpolation We adapt the Wan2.1 [37] Image-to-Video model (14B, 480P) as the keyframe interpolator Wan2.1-KI along single temporal forward diffusion path. Specifically, we insert zero-padding between the input keyframes to construct full-length video sequence. This sequence is then encoded into latents using the VAE encoder of Wan2.1. The resulting latent representation is concatenated with noisy latent and latent mask, and passed to the denoising network for prediction. In parallel, the input keyframes are encoded using the image CLIP encoder to produce condition tokens, which guide the denoising process through cross-attention mechanisms. To accommodate changes in both the latent inputs and attention layer inputs, we perform parameter-efficient LoRA fine-tuning on the input embedding layer and on the value and output projection matrices of the attention layers. 7.2 CHKI-Video: Detailed Construction Stages. Stage 1: Dataset Collection. We begin by collecting video clips from SportSlomo [5], which are temporally downsampled to 60 fps due to the large motions typically present in sports scenarios, making them more challenging for keyframe interpolation. To enhance dataset diversity, we additionally crawl high-quality stock videos from the Pexels website. We compile list of keywords representing fundamental human movements such as Walking, Kicking, Throwing, Catching, and Climbing, to cover broad range of human activities. For each keyword, we collect 100 unique videos with resolutions above 720p and durations under 30 seconds. These keywords are grouped into three motion categories: arm motion, leg motion, and general motion, ensuring balanced labels for subsequent train-test splitting. To match the motion characteristics of the SportSlomo videos, we downsample the collected stock videos based on their optical flow scores, ensuring the flow score distributions are aligned. Stage 2: Pre-annotation Processing. To ensure the quality of the collected videos, we use DOVER [40] to obtain both technical and overall quality scores, and compute brightness change scores between adjacent frames. Videos falling below the bottom 5th percentile in any of these metrics are filtered out. Given the importance of accurate human detection for downstream keypoint and SMPL-X annotation, we design robust detection pipeline. We combine Grounding-DINO [23] with SAM2 [32] to achieve reliable human detection. For challenging sports scenes, we prioritize videos with prominent foreground humans and relatively static or blurred backgrounds, striking balance between annotation complexity and scenario diversity. Additionally, we exclude videos containing more than three people or fewer than 20 frames to maintain clean motion patterns and ensure sufficient temporal coverage. All sports videos are manually reviewed to verify compliance with these criteria and to confirm the accuracy of the human detections. Stage 3: Human-centric Annotation. We perform frame-wise human-centric annotations for all video clips based on the detections in the previous stage. First, we use Sapiens [18] to estimate whole-body keypoints. To ensure the dataset remains strictly human-centric, we perform whole-body detection based on these keypoints. Specifically, we extract the keypoints into DWPose to better define the human figure. We merge all head keypoints into single point, as significant motion rarely occurs in that region. whole-body detection is considered valid if it contains fewer than three invalid keypoints, using keypoint score threshold of 0.3. We further filter video clips to retain only those with more than 20 consecutive valid frames. Finally, we apply SMPLer-X [3], which provides high re-projection accuracy, to fit detailed SMPL-X models to each frame and generate reliable 3D body parameters. 13 Figure 5: Qualitative Results of In-the-wild Control and Keyframe Interpolation. 7.3 Implementation Details We fine-tune the entire PoseFuse3D-KI framework in an end-to-end manner using the AdamW optimizer with learning rate of 8 105. The fine-tuning is applied to our 3D-informed control model, PoseFuse3D, with additional LoRA adaptation on the input patch embeddings, as well as the value and output projections of the VDMs attention modules. Both the LoRA rank and LoRA alpha are set to 32. For implementation, we leverage Fully Sharded Data Parallel (FSDP) across 4 GPUs. 7.4 In-the-wild Interpolation Our PoseFuse3D-KI framework can be readily applied to interpolate in-the-wild human-centric keyframes. In this subsection, we present simple pipeline that uses linear interpolation for human body joints. Given human-centric keyframe pair I0, IN , we first employ 3D human model estimator, such as SMPLer-X [3], to fit SMPL-X models [28] for each keyframe input. Leveraging the strong human body priors from SMPL-X, we linearly interpolate the SMPL-X parameters to generate intermediate 3D human models, which serve as control signals for interpolation. We then extract 2D DWPose keypoints from the 2D projections of the interpolated SMPL-X models. With these steps, all necessary guidance inputs for PoseFuse3D-KI are prepared and can be directly used for keyframe interpolation. Figure 5 shows the results of the interpolated SMPL-X models and the corresponding video frames. While this pipeline offers straightforward and efficient approach for generating intermediate poses, it is limited in its ability to handle complex human motion. In particular, it struggles to capture realistic dynamics such as acceleration or multi-step actions. To address these limitations, we could explore text-to-motion models [48] as part of the future work, which generate intermediate SMPL-X models from textual descriptions, offering more flexible and semantically rich alternative to linear interpolation. 7.5 Evaluation on Additional Benchmark Setup. We conduct an additional evaluation on the test set from FCVG [52]. We first perform human detection and extract human-centric video clips from the test set. The extracted videos are then annotated using the same processing pipeline detailed in Sec. 7.2. This results in FCVG-Test-HC, curated human-centric subset of 54 clips suitable for CHKI benchmarking. The FCVG-TestTable 4: Benchmark Results on FCVG-Test-HC. Methods Metrics PSNR PSNRbbox PSNRmask LPIPS LPIPSbbox LPIPSmask HA GIMM-VFI [11] GI [39] Wan2.1-KI (Ours) FCVG [52] PoseFuse3D-KI (Ours) 23.61 17.21 21.50 22.49 24.84 16.48 10.43 14. 16.26 18.20 15.51 9.59 13.40 15.92 17.49 0.1324 0. 0.0587 0.9459 0.2701 0.1553 0.1738 0.0915 0.1422 0.0915 0.0734 0. 0.1045 0.0704 0.0493 0.0340 0.9438 0.9312 0.9241 0.9245 Figure 6: Qualitative Comparisons on FCVG-Test-HC. HC benchmark is relatively easier than CHKI-Video, primarily consisting of human-centric clips with limited motion rather than more challenging scenarios such as sports and dancing. Other benchmarking settings follow those described in the main paper. Results. We present quantitative comparisons on the FCVG-Test-HC benchmark in Table 4. Our PoseFuse3D-KI outperforms other methods for human-centric keyframe interpolation. Compared with the previous state-of-the-art method FCVG [52], our method achieves 10% improvement in PSNRmask and 31% reduction in LPIPSmask. We observe that all methods achieve higher PSNR and lower LPIPS scores on the FCVG-Test-HC benchmark compared to the CHKI-Video dataset, indicating that FCVG-Test-HC is an easier benchmark for interpolation. This aligns with our earlier observation during dataset construction, where FCVG-Test-HC primarily consists of human-centric clips with limited motion. Interestingly, methods with fewer learned priors tend to achieve higher HA scores in this setting. For instance, the traditional interpolation method GIMM-VFI [11] records the highest Human Anatomy (HA) score. This is likely because such methods rely more heavily on the input keyframes. While this reliance leads to motion artifacts under large movement, it better preserves human textures from inputs when the motion between keyframes is small. 15 Table 5: Ablation on the Visual Encoding. Evaluation Metrics PSNR PSNRbbox PSNRmask LPIPS LPIPSbbox LPIPSmask 19.63 21.71 22.14 0.2097 0.1438 0. 0.1232 0.0738 0.0653 0.0889 0.0531 0.0464 9.92 12.51 13.24 11.25 13.88 14.53 Table 6: Ablation on SMPL-X Encoder. Evaluation Metrics PSNR PSNRbbox PSNRmask LPIPS LPIPSbbox LPIPSmask 22.07 22.15 22.14 0.1348 0.1374 0.1330 0.0659 0.0667 0.0653 0.0466 0.0470 0.0464 13.18 13.22 13.24 14.47 14.50 14. Model Variant Non-Vis Non-2D Full Model Variant Non-JA Non-VA Full Visualizations. We qualitatively compare PoseFuse3D-KI with other advanced methods on the FCVG-Test-HC benchmark, as shown in Figure 6. Consistent with our findings in the Benchmark Results section of the main paper, our method achieves robust human-centric interpolation, accurately follows real-world dynamics, and effectively preserves human body shape. For instance, our method generates plausible interpolations of complex body movements while maintaining the correct leg structure and posture in the last Breaking Dance case. 7.6 Ablation Study on Visual Encoding The core of our framework is the control module, PoseFuse3D. As detailed in the Method section of the main paper, it includes encoding visualizations from both SMPL-X and DWPose [45]. To evaluate the importance of encoding these visualizations and explore whether 2D visual cues can be omitted, we conduct an ablation study on the visual encoding component of PoseFuse3D. Necessity of Encoding Visualizations. In PoseFuse3D, we encode visualizations of control signals. Since they preserve natural pixel-level alignment with the video latent, thereby providing direct control signals on the pixel plane. To assess its necessity, we ablate all visual encoding components in PoseFuse3D and rely solely on the SMPL-X encoded information as the control representation. We refer to this variant as Non-Vis. In Table 5, this modification results in significant performance degradation, with 4.32 dB drop in PSNRmask and 0.0425 increase in LPIPSmask, underscoring the critical role of encoding visualizations in achieving high-fidelity results. Importance of Encoding 2D Visualization. The visual encoding module of PoseFuse3D integrates 2D DWPose visualizations with rendered SMPL-X images. The 2D DWPose visualizations emphasize skeletal keypoint layouts, contributing to robust pose understanding. To assess its importance, we exclude the encoding of 2D visualizations, denoting this variant as Non-2D. This leads to drop of 0.65 dB in PSNRbbox and 13% increase in LPIPSbbox, demonstrating the significance of encoding 2D visualizations in the visual encoding module. 7.7 Ablation on SMPL-X Encoder We conduct an additional ablation study on the SMPL-X encoder to justify our design. Joint Aggregation. The SMPL-X encoder extracts joint motion and position features from 3D space and projects them onto the 2D image plane via an attention mechanism, providing spatial human body motion cues. To assess the impact of this design, we remove the joint aggregation module, denoted as Non-JA. As shown in Table 6, this leads to 0.06 dB drop in both PSNRbbox and PSNRmask, emphasizing the importance of 3D joint aggregation for accurate body control representation. Vertex Aggregation. We also apply separate attention mechanism to aggregate vertex information into the 2D image plane. To examine its necessity, we remove the vertex attention module. We denote this variant as Non-VA. As reported in Table 6, this leads to noticeable degradation in performance across all LPIPS scores, including 0.0040 rise in LPIPS and 0.0014 increase in LPIPSbbox. These 16 Figure 7: Additional Qualitative Results on CHKI-Video. results demonstrate the significance of incorporating 3D vertex information for effective control representation from SMPL-X. 7.8 Additional Qualitative Comparisons We present additional qualitative comparisons with other interpolation methods in Figure 7. Our PoseFuse3D-KI framework consistently produces more plausible interpolations, closely capturing real-world dynamics observed in the ground truth. 7.9 Limitations There are several known limitations to our method. First of all, PoseFuse3D-KI relies on accurate SMPL-X estimations to generate reliable 3D control signals. Therefore, it inherits the limitations of the 3D human model estimators, where inaccurate predictions can degrade the quality of interpolated results. Additionally, our method, while offering strong control via 3D and 2D fusion, still depends on the base diffusion models generative priors. As result, output quality is influenced by the models learned behavior and inherits its high GPU memory demands. Finally, our method does not explicitly model human-object interactions, which may lead to artifacts or misaligned object motion in scenarios involving close interaction with external objects. 7.10 Broader Impacts Our proposed method, PoseFuse3D-KI, enables accurate and controllable human-centric keyframe interpolation, with applications in areas such as human animation and video generation. By integrating explicit 3D information from human models and 2D pose cues, our framework supports 3D-informed and semantically meaningful guidance for interpolating realistic human motion across frames. This technique not only enriches creative workflows but also opens new opportunities for research in human motion understanding and video synthesis. While powerful, our method shares common limitations of generative models and may pose risks if misused to produce manipulated or deceptive human videos, highlighting the importance of responsible use and ethical safeguards."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research"
    ]
}