{
    "paper_title": "Unified Personalized Reward Model for Vision Generation",
    "authors": [
        "Yibin Wang",
        "Yuhang Zang",
        "Feng Han",
        "Jiazi Bu",
        "Yujie Zhou",
        "Cheng Jin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority."
        },
        {
            "title": "Start",
            "content": "Yibin Wang1,2, Yuhang Zang4, Feng Han1,2, Yujie Zhou3,4, Jiazi Bu3,4, Cheng Jin1,2, Jiaqi Wang2 1Fudan University 3Shanghai Jiaotong University 2Shanghai Innovation Institute 4Shanghai AI Lab"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt BradleyTerry-style preference modeling or leverage generative vision-language models (VLMs) as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow one-size-fits-all paradigm that assumes monolithic preference distribution or relies on fixed evaluation rubrics. As result, they are insensitive to contentspecific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap supervised fine-tuning (SFT), equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the Group Relative Policy Optimization (GRPO) framework for image and video synthesis. Extensive results demonstrate its superiority: it consistently delivers more robust, context-aware reward signals than existing baselines and achieves substantial improvements in both image and video synthesis. Website: https://codegoat24.github.io/UnifiedReward/flex 6 2 0 2 2 ] . [ 1 0 8 3 2 0 . 2 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "With the rapid progress of multimodal reward models (RMs) [1, 2, 3, 4, 5], their potential in aligning vision generation models with human preferences has attracted growing attention. By converting subjective and high-level human judgments into learnable reward signals, RMs enable effective preference-driven post-training for vision generators. [6, 7, 8, 9, 10, 11]. Early reward modeling for visual generation typically Corresponding authors. uses fixed, discriminative scorers [12, 13, 14, 15] to assign scalar rewards. Later works shift to BradleyTerry [16] pairwise preference modeling to learn rewards from relative comparisons [4, 17]. More recently, VLM-asa-judge [1, 2, 3, 18] has emerged, leveraging strong generative VLMs [19, 20] to provide context-dependent evaluations for reinforcement learning. Despite their effectiveness, current RMs often follow one-size-fits-all assessment paradigm: (1) Fixed discriminative scorers (e.g., CLIP [12], PickScore [15]) and BradleyTerry preference models (e.g., VideoAlign [4], HPSv3 [17]) typically learn single global reward function that assigns scalar score to each input (or induces preferences via score differences), implicitly assuming monolithic preference distribution shared across diverse prompts and visual contents. (2) VLM-as-a-judge approaches (e.g., UnifiedReward-Think [2]) leverage generative VLMs to produce richer textual judgments, yet they often follow static evaluation rubrics with fixed checklist of criteria. As result, their reward feedback remains insensitive to content-specific visual cues, which can misguide optimization and cause systematic misalignment with human preferences. In this work, we posit that reliable reward assessment should be flexibly adapted to the prompt intent and visual content. For example, prompts with implicit narrative intent should emphasize storytelling consistency, subject relationships, and emotional tone (see Fig. 1), whereas motion-intensive videos with frequent physical contact demand explicit evaluation of action dynamics and physical plausibility (see Fig. 2). This behavior closely mirrors how humans assess visual generations: evaluators first interpret the prompt intent and the depicted content, then evaluate along small set of common, task-specific high-level dimensions. Within each dimension, they selectively attend to the aspects most relevant to the given instance. When the scene exhibits additional characteristics, evaluators naturally introduce new high-level dimensions to capture these salient factors. In other words, human evaluation is inherently content-adaptive, context-aware reasoning process, where both the evaluation criteria and their relative importance are dynamically adjusted to match the semantic intent and visual evidence. As inspired, this work proposes UnifiedReward-Flex, unified personalized reward model for vision generation that couples reward modeling with context-adaptive reasoning to dynamically tailor evaluation criteria, which performs assessment in hierarchical manner. Specifically, given prompt and the generated visual content, it first interprets the semantic intent and extracts salient visual evidence, then composes hierarchical evaluation plan by instantiating fine-grained sub-criteria under few predefined coarse-grained dimensions. When the context demands additional considerations, it augments the hierarchy with new high-level dimensions and their corresponding sub-criteria. This dynamic criterion composition yields adaptive and informative reward feedback that aligns with the human evaluation process and remains robust across diverse prompts and visual content. Our training follows two-stage pipeline: (1) We first distill structured, high-quality reasoning traces from advanced closed-source VLMs [21] to bootstrap supervised fine-tuning (SFT), endowing the model with content-aware criterion composition. (2) We then apply direct preference optimization (DPO) using preference pairs with human-grounded labels: given two sampled responses for the same input, if one reaches the correct conclusion while the other does not, the preference naturally favors the correct one; if both are correct, we further assign the preference based on the quality of their adaptive reasoning trajectories, prioritizing more flexible and context-grounded evaluation hierarchies. Empirically, this reasoning-aware preference alignment improves the reward models discriminative power, even among samples that are all correct. Extensive experiments show that UnifiedReward-Flex consistently outperforms strong reward model baselines on both image and video reward tasks, providing more robust and context-aware reward signals. To further validate its practical utility, we use it as the reward signal for pairwise preference reward-based GRPO [6] in both image and video synthesis, yielding substantial quantitative and qualitative improvements in downstream generation quality. Contributions. (1) We identify key limitation of existing multimodal reward models, i.e., their onesize-fits-all evaluation paradigm, and propose UnifiedReward-Flex, unified personalized reward model for vision generation that dynamically composes evaluation hierarchies via content-aware reasoning. (2) Our UnifiedReward-Flex consistently outperforms strong reward model baselines on both image and video reward tasks, delivering more robust and context-aware reward signals. (3) We further validate its practical utility by integrating it into pairwise preference reward-based GRPO for image and video synthesis, achieving substantial improvements in downstream generation quality both quantitatively and qualitatively. 2 Figure 1 Qualitative Result of UnifiedReward-Flex on Image Generation Personalized Reward Reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Reward Models (RMs) are crucial in aligning vision generation models with human preferences. Early reward modeling typically relies on fixed, discriminative scorers that assign scalar score to each generated sample [12, 13, 14, 15]. These scorers are lightweight and easy to deploy, but they often behave as task-agnostic heuristics: the reward function is largely fixed across prompts and contents, making it difficult to reflect diverse evaluation focuses. To better capture the relative nature of human judgment, later works shift from absolute scoring to pairwise preference modeling under the Bradley-Terry pairwise preference modeling [16]. Instead of regressing single target score, the RM is trained to assign higher reward to preferred samples within comparison pair, which often yields more stable and better calibrated training signals for preference optimization [4, 5, 17]. Nevertheless, BradleyTerry based RMs still typically learn single global scalar reward function shared across heterogeneous prompts and visual contents, implicitly assuming monolithic preference distribution. More recently, VLM-as-a-judge has emerged as flexible alternative that leverages strong generative VLMs to directly assess and compare candidates [1, 2, 3, 18, 22]. Compared to discriminative scorers, these judge models can provide richer evaluative feedback, potentially incorporating multi-aspect reasoning and content-dependent judgments. However, existing VLM-judge approaches commonly follow static evaluation rubrics or fixed checklists of criteria, which limits their ability to dynamically tailor evaluation priorities to the semantic intent and visual evidence of each input. To this end, this work proposes UnifiedReward-Flex, unified personalized reward model for vision generation that couples reward modeling with content-aware reasoning. Unlike prior RMs that apply fixed global scoring function or static rubrics, UnifiedReward-Flex dynamically constructs context-adaptive hierarchical 3 Figure 2 Qualitative Result of UnifiedReward-Flex on Video Generation Personalized Reward Reasoning. assessment by self-instantiating fine-grained criteria based on given prompt intent and visual evidence. Reinforcement Learning for Vision Generation has evolved rapidly in recent years. Early attempts either fine-tuned models with scalar reward supervision [23, 24] or adopted reward-weighted regression to exploit reward feedback in more stable manner [3, 4, 25]. Subsequent work drew inspiration from Proximal Policy Optimization (PPO) [26] and incorporated policy-gradient updates into diffusion-based generators, showing promising improvements in sample quality [27, 28, 29]. Despite their effectiveness, these methods are often computationally demanding and sensitive to hyperparameter tuning. To improve training efficiency, growing line of research explores preference optimization with supervised objectives, such as direct preference optimization (DPO), which directly leverages human preference data and avoids expensive on-policy rollouts [30, 31, 32, 33]. More recently, Group Relative Policy Optimization (GPRO) [34] has emerged as promising objective for preference optimization in complex reasoning tasks. [9, 10] extend GRPO to flow matching models by reformulating the ODE sampling process as an equivalent SDE, enabling diverse sampling while performing preference-driven optimization. [7, 11, 35] further improve efficiency with sliding-window mechanism that localizes SDE sampling and GRPO updates within window while retaining ODE sampling elsewhere. However, these approaches typically optimize pointwise scalar rewards and may suffer from reward hacking [8, 9]. Pref-GRPO [6] reveals that reward hacking is largely driven by illusory advantages induced by unreliable reward signals, and stabilizes learning by replacing pointwise rewards with pairwise preference-based feedback. In this work, we integrate our UnifiedReward-Flex into the Pref-GRPO framework for both image and video synthesis to validate its effectiveness."
        },
        {
            "title": "3.1 Overview",
            "content": "Multimodal reward models (RMs) serve as learned proxies for human visual preferences. However, existing RMs often follow one-size-fits-all paradigm: they either assume monolithic preference distribution captured by single global scoring function, or rely on fixed evaluation rubrics that apply the same criteria uniformly across inputs. As result, their reward feedback is often insensitive to content-specific visual cues and prompt intent, leading to systematic misalignment with inherently subjective and context-dependent human preferences. To this end, we propose UnifiedReward-Flex, unified personalized reward model for vision generation that couples reward modeling with context-adaptive reasoning to dynamically tailor evaluation criteria. In this section, we first present our unified personalized reward modeling framework (Sec. 3.2), including the design of our context-adaptive reasoning process (Sec. 3.2.1) and two-stage training pipeline: (i) reasoning distillation for SFT (Sec. 3.2.2) and (ii) reasoning-aware preference alignment via DPO (Sec. 3.2.3). We then describe how to apply UnifiedReward-Flex to reinforcement learning for vision generation (Sec. 3.3), starting with the necessary preliminaries (Sec. 3.3.1), and followed by our employed personalized multi-dimensional preference rewards for GRPO (Sec. 3.3.2)."
        },
        {
            "title": "3.2.1 Context-adaptive Reasoning Process Design",
            "content": "Our goal is to mimic how humans evaluate generated visual content. In practice, for given task, human evaluators typically assess outputs along small set of common high-level dimensions (e.g., semantic alignment and perceptual quality), while instantiating more fine-grained factors depending on the prompt intent and the visual evidence under each dimensions. When additional considerations become salient (e.g., Narrative & Interaction in Fig. 1), they naturally extend the evaluation dimensions to better capture what matters for the current instance. Guided by this principle, we design context-adaptive hierarchical reasoning process that starts from three predefined common dimensions as stable anchors (e.g., semantic alignment, visual quality, and aesthetics for image generation). Under each anchor, the model instantiates prompt-specific sub-dimensions for assessment, and it can further introduce new high-level dimensions (with corresponding sub-dimensions) when required by the context. For each high-level dimension, the model aggregates the evidence across its instantiated sub-dimensions to produce an overall comparative analysis and dimension-level winner; it then combines the outcomes from all high-level dimensions to determine final overall winner. Specifically, as shown in Fig. 2, UnifiedReward-Flex derives prompt-relevant sub-dimensions under the given anchors to jointly assess semantic adherence, visual quality under motion, and cinematographic coherence of generated videos. Since this case emphasizes rich motion and physical interactions, the model further introduces an additional high-level dimension (i.e., Action Dynamics & Physics) to explicitly evaluate temporal coherence and physical plausibility, which is then incorporated into the final overall decision. By tailoring the evaluation hierarchy to the generation context, our model provides richer reward supervision for preference optimization, improving both intent satisfaction and content-critical quality factors."
        },
        {
            "title": "3.2.2 Stage I: Reasoning Distillation for SFT",
            "content": "To bootstrap the model with context-adaptive assessment behaviors, we first distill structured reasoning traces from the powerful closed-source VLM [21] and use them for supervised fine-tuning (SFT). Distillation data. Let ğ’Ÿ = {ğ‘¥ğ‘–}ğ‘ ğ‘–=1 be set of preference-evaluation instances, where each input ğ‘¥ğ‘– = (ğ‘ğ‘– , ğ‘£(0) ğ‘– , ğ‘£(1) ğ‘– ) (1) contains text prompt ğ‘ğ‘– and pair of candidate visual generations (cid:0)ğ‘£(0) ğ‘– teacher model ğ’¯ outputs structured evaluation trace , ğ‘£(1) ğ‘– (cid:1). Given ğ‘¥ğ‘–, the closed-source ğ‘– = ğ’¯ (ğ‘¥ğ‘–) = (cid:0)â„‹ğ‘– , â„›ğ‘– , ğ’²ğ‘– (cid:1) , ğ‘¦ğ’¯ (2) 5 where â„‹ğ‘– = {(ğ‘‘ğ‘˜ , ğ’®ğ‘–,ğ‘˜)}ğ¾ğ‘– denotes the instantiated high-level dimensions ğ‘‘ğ‘˜ together with their promptğ‘˜=1 specific sub-dimensions ğ’®ğ‘–,ğ‘˜, â„›ğ‘– is the corresponding evidence-grounded reasoning trace, and ğ’²ğ‘– = (cid:0){ğ‘¤ğ‘–,ğ‘˜}ğ¾ğ‘– , ğ‘¤ğ‘– (cid:1) are winner labels. Here ğ‘¤ğ‘–,ğ‘˜ {0, 1} denotes the winner under dimension ğ‘‘ğ‘˜ and ğ‘¤ğ‘– {0, 1} ğ‘˜=1 denotes the overall winner. SFT objective. We fine-tune the base model [2] with parameters ğœƒ to imitate the teacher outputs via conditional language modeling: â„’SFT(ğœƒ) = ğ‘ (cid:213) ğ‘–= log ğ‘ğœƒ (cid:16) ğ‘¦ğ’¯ ğ‘– (cid:17) ğ‘¥ğ‘– = ğ‘ (cid:213) ğ‘¦ğ’¯ ğ‘– (cid:213) ğ‘–=1 ğ‘¡=1 (cid:16) log ğ‘ğœƒ ğ‘–,ğ‘¡ ğ‘¥ğ‘– , ğ‘¦ğ’¯ ğ‘¦ğ’¯ ğ‘–,<ğ‘¡ (cid:17) . (3) This stage initializes the model to generate structured, context-adaptive evaluations for paired visual comparisons, providing strong foundation for subsequent preference alignment."
        },
        {
            "title": "3.2.3 Stage II: Reasoning-Aware Preference Alignment via DPO",
            "content": "Building on the SFT-initialized context-adaptive evaluations, we further optimize it for preference discrimination under human-grounded supervision. Specifically, we apply Direct Preference Optimization (DPO) to jointly align the final decision and the quality of the adaptive reasoning trajectory. Preference pair construction. For each input ğ‘¥ğ‘– = (ğ‘ğ‘– , ğ‘£(0) ğ‘¤ ğ‘– preferred. Starting from the SFT model, we sample two candidate structured evaluations {0, 1} provided by the dataset, where ğ‘¤ ğ‘– = 0 indicates ğ‘£(0) is preferred and ğ‘¤ , ğ‘£(1) ğ‘– ğ‘– ğ‘– ), we assume an annotated preference label ğ‘– = 1 indicates ğ‘£(1) ğ‘– ğ‘¦(ğ‘) ğ‘– , ğ‘¦(ğ‘) ğ‘– ğœ‹ğœƒ( ğ‘¥ğ‘–), each containing an overall predicted winner Ë†ğ‘¤(ğ‘¦) {0, 1} and context-adaptive reasoning trace. We define the correctness indicator (cid:16) ğ‘ (cid:17) ğ‘¦(ğ‘—) ğ‘– (cid:16) (cid:104) Ë†ğ‘¤ (cid:17) ğ‘¦(ğ‘—) ğ‘– = ğ•€ (cid:105) = ğ‘¤ ğ‘– , ğ‘— {ğ‘, ğ‘}. (5) If exactly one sample is correct, we set the preference to favor the correct one. If both samples are correct, we further rank them by the quality of their adaptive reasoning trajectories, preferring evaluations with more context-grounded and flexible hierarchies. Concretely, when both samples are correct, we obtain trajectory-level preference by querying closed-source judge ğ’¯judge to perform pairwise comparison and output the preferred reasoning trace, followed by human verification on the judged pairs. We denote the judged preference as â„“ traj ğ‘– = ğ’¯judge(ğ‘¥ğ‘– , ğ‘¦(ğ‘) ğ‘– , ğ‘¦(ğ‘) ğ‘– ) {ğ‘, ğ‘}, (6) indicates which trace is preferred in terms of reasoning quality. We then construct the DPO pair where â„“ traj ğ‘– (ğ‘¦+ , ğ‘¦ ) as ğ‘– ğ‘– ), (ğ‘¦+ ğ‘– ) = ğ‘– , ğ‘¦ }, and we discard the pair if ğ‘(ğ‘¦(ğ‘) (ğ‘¦(ğ‘) ğ‘– (ğ‘¦(ğ‘) ğ‘– (â„“ traj ğ‘– ğ‘– , ğ‘¦(ğ‘) ğ‘– , ğ‘¦(ğ‘) ğ‘– ) ), ( â„“ traj ğ‘– ğ‘– , ğ‘¦ (ğ‘¦ ), ) ğ‘(ğ‘¦(ğ‘) ğ‘– ğ‘(ğ‘¦(ğ‘) ğ‘– ğ‘(ğ‘¦(ğ‘) ğ‘– ğ‘– ) > ğ‘(ğ‘¦(ğ‘) ) > ğ‘(ğ‘¦(ğ‘) ğ‘– ) = ğ‘(ğ‘¦(ğ‘) ğ‘– ) ) ) = 1, (7) where â„“ traj the final preference decision but also the quality of the underlying context-adaptive reasoning trajectory. ) = 0. This construction aligns not only {ğ‘, ğ‘} {â„“ traj ) = ğ‘(ğ‘¦(ğ‘) ğ‘– ğ‘– ğ‘– ğ‘– DPO objective. Given the resulting preference dataset ğ’« = {(ğ‘¥ğ‘– , ğ‘¦+ ğ‘– DPO loss using frozen reference policy ğœ‹ref (the SFT model): , ğ‘¦ ğ‘– )}, we optimize ğœ‹ğœƒ with the standard â„’DPO(ğœƒ) = ğ”¼(ğ‘¥,ğ‘¦+,ğ‘¦)ğ’« (cid:104) (cid:16) log ğœ ğ›½ğ‘‘ğ‘ğ‘œ (cid:0) log ğœ‹ğœƒ(ğ‘¦+ ğ‘¥) log ğœ‹ğœƒ(ğ‘¦ ğ‘¥) log ğœ‹ref(ğ‘¦+ ğ‘¥) + log ğœ‹ref(ğ‘¦ ğ‘¥)(cid:1) (cid:17)(cid:105) , (8) where ğ›½ controls the strength of preference optimization. By directly increasing the likelihood of preferred structured evaluations, this stage sharpens the reward models discriminative ability while reinforcing its 6 is (4) context-adaptive reasoning behavior. Empirically, such reasoning-aware preference alignment improves the reward models discriminative power, even among samples that are all correct."
        },
        {
            "title": "3.3.1 Preliminaries",
            "content": "Flow Matching GRPO. We briefly review GRPO [34] in the context of flow-matching models [9, 10]. Given prompt ğ‘, flow-based generator produces an image/video ğ‘¥0 by iteratively refining noisy sample from ğ‘¡ = ğ‘‡ to ğ‘¡ = 0. For each sample, we consider terminal reward supervision, where the reward is provided at the end of the trajectory as ğ‘…(ğ‘¥0, ğ‘). Group-relative advantage. Given prompt ğ‘, GRPO samples group of ğº generations {ğ‘¥ğ‘– 0, ğ‘)}ğº them with reward model to obtain {ğ‘…(ğ‘¥ğ‘– ğ‘–=1 prompt-wise standardized advantage: and evaluates . To stabilize learning under noisy rewards, GRPO constructs 0}ğº ğ‘–=1 Ë†ğ´ğ‘– = ğ‘…(ğ‘¥ğ‘– 0, ğ‘) ğœ‡ğ‘ ğœğ‘ , ğœ‡ğ‘ = mean(cid:0){ğ‘…(ğ‘¥ ğ‘— 0, ğ‘)}ğº ğ‘—=1 (cid:1) , ğœğ‘ = std(cid:0){ğ‘…(ğ‘¥ ğ‘— 0, ğ‘)}ğº ğ‘—=1 (cid:1) . (9) This relative advantage encourages the policy to increase the likelihood of higher-quality generations within the same prompt group, rather than chasing absolute reward values. GRPO objective. Let ğœ‹ğœƒ denote the sampling policy induced by the generator, and ğœ‹ğœƒold policy used to collect trajectories. For the ğ‘–-th trajectory at step ğ‘¡, the importance ratio is be the behavior ğ‘Ÿ ğ‘– ğ‘¡ (ğœƒ) = ğœ‹ğœƒ(ğ‘¥ğ‘– ğ‘¡1 ğœ‹ğœƒold(ğ‘¥ğ‘– ğ‘¡1 ğ‘¥ğ‘– ğ‘¡ , ğ‘) ğ‘¥ğ‘– ğ‘¡ , ğ‘) . (10) GRPO then maximizes clipped surrogate objective with KL regularization to reference policy ğœ‹ref: ğ’¥GRPO(ğœƒ) = ğ”¼ğ‘ (cid:34) 1 ğº ğº (cid:213) ğ‘–= 1 ğ‘‡ ğ‘‡1 (cid:213) ğ‘¡=0 (cid:16) min ğ‘¡ (ğœƒ) Ë†ğ´ğ‘– , clip(cid:0)ğ‘Ÿ ğ‘– ğ‘Ÿ ğ‘– ğ‘¡ (ğœƒ), 1 ğœ‚, 1 + ğœ‚(cid:1) Ë†ğ´ğ‘– (cid:17) ğ›½ğ‘˜ğ‘™ ğ·KL(ğœ‹ğœƒ ğœ‹ref) , (11) (cid:35) where ğœ‚ is the clipping threshold and ğ›½ğ‘˜ğ‘™ controls the strength of regularization. This formulation provides principled and stable way to post-train vision generators with learned reward signals. Pairwise Preference Reward-based GRPO (Pref-GRPO) [6] replaces absolute reward scores with relative preference judgments, which better align with the common practice of evaluating visual generations via pairwise comparisons. Instead of assigning an independent scalar reward to each sample, Pref-GRPO derives terminal rewards from comparative outcomes among group of generated samples. Given prompt ğ‘, group of ğº images (or videos) {ğ‘¥ğ‘– is sampled from the current policy ğœ‹ğœƒ. For 0 ğ‘¥ ğ‘— each unordered pair (ğ‘¥ğ‘– . 0 Based on all pairwise comparisons within the group, Pref-GRPO defines relative preference reward for each sample as its normalized win rate: 0), preference model determines which sample is preferred, denoted by ğ‘¥ğ‘– 0, ğ‘¥ ğ‘— 0}ğº ğ‘–=1 ğ‘…(ğ‘¥ğ‘– 0, ğ‘) = 1 ğº 1 (cid:213) ğ‘—ğ‘– ğŸ™(cid:0)ğ‘¥ğ‘– 0 ğ‘¥ ğ‘— 0 (cid:1) , (12) which reflects how often sample ğ‘– is preferred over other candidates under the same prompt. These preference-derived rewards are then directly plugged into the standard GRPO framework. In particular, the group-relative advantage Ë†ğ´ğ‘– is computed using Eq. (9), and the policy is optimized with the same clipped surrogate objective in Eq. (11)."
        },
        {
            "title": "3.3.2 Personalized Multi-Dimensional Preference Rewards",
            "content": "In this work, we integrate our flexible, context-adaptive evaluations from UnifiedReward-Flex into PrefGRPO to provide personalized, multi-dimensional rewards. Specifically, given prompt ğ‘, the policy ğœ‹ğœƒ 7 Table 1 Image and Video Generation Assessment Comparison."
        },
        {
            "title": "Image Generation",
            "content": "GenAI-Bench MMRB"
        },
        {
            "title": "Video Generation",
            "content": "GenAI-Bench MJBench HPSv2 PickScore HPSv3 UnifiedReward UnifiedReward-Think Ours w/o DPO Ours w/o DPO (Both correct) Ours 68.8 70.0 70.9 71.5 72.3 71.5 72.0 73.4 55.0 57.6 58.5 60.0 66.0 67.5 68.4 69.2 LiFT VideoScore VideoReward UnifiedReward UnifiedReward-Think Ours w/o DPO Ours w/o DPO (Both correct) Ours 60.1 70.6 73.1 76.8 80.3 79.4 80.6 82. 51.0 62.8 63.4 68.8 70.9 69.1 70.3 72.0 samples group of ğº candidates {ğ‘¥ğ‘– pairwise preference judgments along ğ· predefined anchor dimensions: , and for each unordered pair (ğ‘¥ğ‘– 0}ğº ğ‘–=1 0, ğ‘¥ ğ‘— 0), the reward model produces For each candidate, we compute the dimension-wise win rates and their average over the anchor 0 ğ‘‘ ğ‘¥ ğ‘— ğ‘¥ğ‘– 0, ğ‘‘ = 1, . . . , ğ·. dimensions: ğ‘…dim(ğ‘¥ğ‘– 0, ğ‘) = 1 ğ· ğ· (cid:213) ğ‘‘=1 ğ‘…ğ‘‘(ğ‘¥ğ‘– 0, ğ‘), ğ‘…ğ‘‘(ğ‘¥ğ‘– 0, ğ‘) = 1 ğº 1 (cid:213) ğ‘—ğ‘– ğŸ™(cid:0)ğ‘¥ğ‘– 0 ğ‘‘ ğ‘¥ ğ‘— (cid:1) . To account for dynamic, personalized high-level dimensions that may not appear consistently in each pairwise comparison, we also compute the overall win rate: ğ‘…overall(ğ‘¥ğ‘– 0, ğ‘) = 1 ğº (cid:213) ğ‘—ğ‘– ğŸ™(cid:0)ğ‘¥ğ‘– 0 ğ‘¥ ğ‘— 0 (cid:1) . The group-relative advantages are then computed separately for the averaged dimension-wise win rate and the overall win rate: Ë†ğ´ğ‘– dim = ğ‘…dim(ğ‘¥ğ‘– 0, ğ‘) ğœ‡dim ğœdim , Ë†ğ´ğ‘– overall = ğ‘…overall(ğ‘¥ğ‘– 0, ğ‘) ğœ‡overall ğœoverall , where ğœ‡ and ğœ are the mean and standard deviation within the prompt group. Finally, the combined advantage used in GRPO is Ë†ğ´ğ‘– = ğ›¼ Ë†ğ´ğ‘– dim + (1 ğ›¼) Ë†ğ´ğ‘– overall, where ğ›¼ controls the relative contributions of the averaged dimension-wise advantage and the overall advantage, respectively. This procedure ensures that both the fine-grained anchor preferences and the holistic, context-adaptive evaluation contribute to the policy update, while remaining compatible with the training objective in Eq. (11)."
        },
        {
            "title": "4 Experiment",
            "content": "4."
        },
        {
            "title": "4.1.1 UnifiedReward-Flex",
            "content": "Datasets. For image generation, we sample 50K image preference pairs from HPDv3 [17]. For video generation, we combine two human preference datasets: Text2Video-Human Preferences (15K), provided by Rapidata, and VideoFeedback2 [18], from which we preprocess and construct an additional 35K video preference pairs. In the SFT stage, we distill reward reasoning traces for both images and videos from GPT-5.2 [21], using 45K image pairs and 45K video pairs to construct our UnifiedReward-Flex-SFT-90K dataset. Training samples are 8 Table 2 In-domain Semantic Consistency Comparison on UniGenBench. UniGenBench-EvalModel-qwen3vl-32b-v1 is used as the VLM for evaluation. Model Overall Style World Know. Attribute Action Relation. Compound Grammar Logic.Reason. Layout Text FLUX.1-dev w/ HPSv2 w/ HPSv3 w/ PickScore w/ UnifiedReward w/ UnifiedReward-Think w/ UnifiedReward-Flex 59.39 57.77 57.98 58.63 60.87 68.89 73.95 85.10 77.90 79.40 79.70 83.50 88.00 90.30 85.92 87.03 90.03 87.03 87.97 91.77 89.87 65.28 65.92 66.24 64.42 66.13 77.99 79.38 61.41 57.41 57.89 61.12 63.88 69.20 73. 64.97 65.86 63.58 67.64 68.65 75.13 78.55 43.56 44.46 39.82 47.42 46.52 61.47 69.46 60.16 55.75 58.82 58.02 58.69 61.63 63.10 24.77 29.09 24.09 27.50 24.32 41.36 46.59 70.52 64.93 67.16 67.54 71.08 77.24 79.66 32.18 29.31 32.76 25.86 37.93 45.11 59. Table 3 Out-of-Domain Semantic Consistency and Image Quality Evaluations. The best results are in bold, and the second best are underlined. Model FLUX.1-dev w/ HPSv2 w/ HPSv3 w/ PickScore w/ UnifiedReward w/ UnifiedReward-Think w/ UnifiedReward-Flex Semantic Consistency Image Quality UniGenBench T2I-CompBench GenEval CLIP PickScore UnifiedReward Aesthetic 59.39 57.77 57.98 58.63 60.87 68.89 73.95 48.57 44.84 46.46 45.92 49.13 50.10 51.37 62.18 58.43 61.14 58.76 66.25 68.20 69.62 34.40 33.35 34.12 33.61 34.43 35.85 36.25 22.70 23.12 23.26 23.78 23.31 23.38 23.42 3.07 3.10 3.14 3.12 3.19 3.27 3. 6.13 6.23 6.37 6.42 6.44 6.53 6.56 randomly drawn from the collected datasets, while the remaining data are reserved for the subsequent DPO stage. In the DPO stage, we use non-greedy decoding with temperature 0.7 and sample two reasoning traces per prompt. If both samples yield the correct preference decision, we further use GPT-5.2 to perform pairwise comparison of their reasoning trajectories and select the higher-quality trace as the preferred response for constructing DPO training pairs. Reward model. We adopt UnifiedReward-Think-qwen3vl-8B [2] as the base reward model, which supports long-chain reasoning for both visual perception and generation. Building on its strong prior knowledge, we further steer it toward flexible, context-adaptive reward reasoning. Training details. Our training is conducted with batch size of 2 and 2 gradient accumulation steps, using learning rate of 2.5 106 and warm-up ratio of 0.1. All experiments are performed on 32 NVIDIA H200 GPUs. For the DPO stage, we set ğ›½ğ‘‘ğ‘ğ‘œ to 0.1. Evaluation. We evaluate image reward models on GenAI-Bench-Image [36] and Multimodal RewardBench 2 (MMRB2) [37], and video reward models on GenAI-Bench-Video [36] and MJ-Bench-Video [38]. For pairwise preference-based reward models, we randomly permute the order of the two candidates at evaluation time."
        },
        {
            "title": "4.1.2 Reinforcement Learning for Vision Generation",
            "content": "Text-to-Image Generation. Training. We perform GRPO on FLUX.1-dev [39] using training prompts from UniGenBench++ [40]. Training is conducted on 32 NVIDIA H200 GPUs with 15 sampling steps and 9 rollouts per prompt from the same initial noise, using 3 gradient accumulation steps and learning rate of 3 106. We set ğ›½KL = 0. Evaluation. For inference, we use 30 sampling steps and classifier-free guidance scale of 3.5, following the official configuration. We evaluate in-domain performance on UniGenBench++. For out-of-domain evaluation, we measure semantic consistency on GenEval [41] and T2I-CompBench [42], and assess image quality using UnifiedReward [1], Pickscore [15], and the aesthetic predictor [14]. Text-to-Video Generation. Training. We perform GRPO on Wan2.1-T2V-14B [43] using the training prompts provided by [10]. Training is conducted on 32 NVIDIA H200 GPUs with LoRA rank 64 and alpha 128, using 20 sampling steps and 6 rollouts per prompt from the same initial noise. We train with videos at 240 416 resolution with 33 frames, 2 gradient accumulation steps, and learning rate of 3 105. We set ğ›½KL = 0.004. Evaluation. For inference, we use 30 sampling steps and classifier-free guidance scale of 5, generating videos at 480 832 resolution with 33 frames, and evaluating the performance on VBench [44]. 9 Figure 3 Qualitative Comparison on Text-to-Image GRPO."
        },
        {
            "title": "4.2.1 Reward Model Comparison",
            "content": "We compare our UnifiedReward-Flex against representative baselines on both image and video preferenceevaluation benchmarks. Fixed scorers (e.g., HPSv2 [13], PickScore [15]) and BradleyTerry preference models (e.g., HPSv3 [17], VideoReward [4]) provide single global signal that is insensitive to prompt-specific requirements. Besides, UnifiedReward-Think [2] is strong VLM-as-a-judge baseline that performs multimodal assessment with long-chain reasoning, but still follows fixed checklist of evaluation criteria. In contrast, UnifiedReward-Flex dynamically instantiates fine-grained criteria conditioned on the prompt and visual evidence, yielding more reliable pairwise decisions. Quantitatively, as shown in Tab. 1, UnifiedRewardFlex achieves the best performance across all benchmarks; notably, it improves over UnifiedReward-Think by +3.2 points on MMRB2 and +2.2 points on GenAI-Bench-Video, highlighting the benefit of context-adaptive criterion composition beyond long-chain reasoning alone. Qualitative results are provided in Figs. 1 and 2. For example, Fig. 1 highlights our hierarchical, context-adaptive evaluation in story-implied prompt (a child healing wounded kirin). Starting from common anchor dimensions (semantic alignment, visual quality, and aesthetics), our model instantiates prompt-specific sub-criteria to check core requirements such as subject completeness and style coherence. Crucially, because the prompt implicitly describes narrative moment rather than static portrait, the model further augments the evaluation hierarchy with an additional high-level dimension, i.e., Narrative & Interaction, to judge whether the generation conveys coherent healing scene with meaningful entity relations. This adaptive criterion composition prevents the evaluation from being dominated by surface-level rendering quality alone, and yields preference judgments that better reflect prompt-critical intent, providing more informative reward supervision for downstream optimization. 10 Table 4 Quantitative results on VBench. The first seven metrics correspond to the Quality type, while the remaining correspond to the Semantic type. Models Subject Consistency Background Consistency Aesthetic Quality Imaging Quality Temporal Flickering Motion Smoothness Dynamic Degree Human Action Wan2.1-T2V-14B w/ VideoReward w/ UnifiedReward-Think w/ UnifiedReward-Flex Models Wan2.1-T2V-14B w/ VideoReward w/ UnifiedReward-Think w/ UnifiedReward-Flex 96.6 96.7 96. 96.9 Color 87.7 87.8 86.1 89.6 97.6 97.9 97.7 97. 62.4 62.9 63.9 65.1 64.9 66.5 65.2 66.9 99.2 99.3 99.4 99. 98.5 98.5 98.4 99.0 58.6 41.6 58.3 70.8 79.4 78.2 78.4 79. Spatial Relationship Scene Temporal Style Overall Consistency Object Class Multiple Objects Appearance Style 72.6 77.0 77.3 80.8 28.8 28.2 27.2 30.5 23.6 23.7 23. 24.2 25.1 25.3 25.4 25.6 79.1 82.1 78.4 83.2 61.8 70.2 63. 70.6 22.2 21.0 22.3 22.4 Figure 4 Qualitative Comparison on Text-to-Video GRPO."
        },
        {
            "title": "4.2.2 Text-to-Image GRPO",
            "content": "Quantitatively, on UniGenBench (Tab. 2), UnifiedReward-Flex improves the overall semantic consistency (+14.56) over the base model, and also surpasses the strong VLM-as-a-judge baseline UnifiedReward-Think (+5.06). The improvements are broad-based across challenging dimensions that require compositional and intent-aware evaluation, such as Compound and Logical Reasoning. It also generalizes well to out-of-domain benchmarks (Tab. 3): achieves the best semantic consistency on T2I-CompBench [42] and GenEval [41], while maintaining or improving image quality metrics (e.g., UnifiedReward [1]) compared to other reward baselines. These results indicate that optimizing with our personalized reward does not merely overfit to in-domain prompts; instead, it promotes more robust alignment with diverse prompt intents and visual evidence. Qualitatively, Fig. 3 further shows that our model better enforces prompt-critical constraints. In the Newton example (row 2), the prompt couples multiple requirements (a square falling apple and circular shadow), which demands reasoning over attributes and geometry. Among the compared methods, only the generator optimized with our reward consistently satisfies this coupled constraint. 11 Table 5 Hyperparameter Analysis of ğ›¼. The best results are in bold, and the second best are underlined. Model FLUX.1-dev ğ›¼ = 0 (w/o ğ‘…dim) ğ›¼ = 0.3 ğ›¼ = 0.5 ğ›¼ = 1 (w/o ğ‘…overall) ğ›¼ = 0.7 (Ours) Text-to-Image Generation Model Text-to-Video Generation UniGenBench T2I-CompBench UnifiedReward Total Semantic Quality 59.39 71.13 72.50 73.10 73.44 73.95 48.57 50.32 50.42 50.90 51.59 51.37 3.07 3.25 3.23 3.29 3.26 3.31 Wan2.1-T2V-14B ğ›¼ = 0 (w/o ğ‘…dim) ğ›¼ = 0.3 ğ›¼ = 0.5 ğ›¼ = 1 (w/o ğ‘…overall) ğ›¼ = 0.7 (Ours) 80.81 82.46 82.56 82.82 82.89 83.08 69.66 71.79 72.11 72.34 72.42 72.94 83.60 85.13 85.17 85.44 85.51 85.62 Figure 5 Qualitative Results of Text-to-Video Generation during Training Progress."
        },
        {
            "title": "4.2.3 Text-to-Video GRPO",
            "content": "Quantitatively, Tab. 4 summarizes VBench results for Wan2.1-T2V-14B GRPO with different reward signals. UnifiedReward-Flex yields clear improvements on both dynamic quality and compositional semantics. For example, on the quality side, it notably boosts the Dynamic Degree score (from 58.6 to 70.8), indicating that our reward provides more informative supervision for motion-intensive generations beyond static appearance. On the semantic side, it substantially improves Spatial Relationship (72.6 to 80.8) and Color consistency (87.7 to 89.6), suggesting better prompt grounding in relational and attribute-level constraints. Compared with VideoReward and UnifiedReward-Think, while both baselines also achieve competitive results, their improvements are less pronounced; in particular, they incur drops on Dynamic Degree, indicating limited ability to encourage richer motion. This contrast further supports that our context-personalized evaluation provides more effective reward supervision for GRPO-based video post-training. Qualitatively, Fig. 4 shows that using UnifiedReward-Flex as the reward for GRPO yields more coherent videos under motionand interaction-centric prompts. In the upper-left example (Two AI models fighting in Mortal Kombat), the base Wan2.1-T2V-14B output is largely static, and GRPO with VideoReward or UnifiedReward-Think often further dampens the motion dynamics, resulting in reduced action amplitude and weaker interaction intensity across frames. In contrast, UnifiedReward-Flex-guided GRPO preserves stronger, continuous motion and clearer contact-driven progression, better matching the prompts intent. 12 Figure 6 Qualitative Results of Text-to-Video Generation during Training Progress. Table 6 Comparison of Training Efficiency (Seconds per Step). PickScore HPSv UnifiedReward VideoReward UnifiedReward-Think UnifiedReward-Flex FLUX.1-dev Wan2.1-T2V-14B 102s 103s 109s 285s 124s 328s 143s 336s"
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "Effect of DPO Alignment. As shown in Tab. 1, applying DPO yields consistent improvements, validating the role of preference alignment in sharpening preference discrimination. Importantly, the gains persist even in the Both correct setting, where both sampled traces predict the correct final preference. In this harder regime, DPO provides supervision on how the decision is reached: explicitly preferring higher-quality, more context-adaptive reasoning trajectories further improves discriminative accuracy by separating subtle quality differences that are invisible to decision-level correctness alone. Together, these results motivate coupling personalized criterion composition with reasoning-aware preference alignment to build robust reward models for both image and video generation. Hyperparameter Analysis of ğ›¼. We analyze the impact of ğ›¼ in Tab. 6, which controls the balance between the averaged dimension-wise win rate ( ğ‘…dim) and the overall win rate (ğ‘…overall) in our reward model. When ğ›¼ = 0, the model relies solely on the overall win rate, potentially overlooking finer, context-specific details, resulting in suboptimal performance. As ğ›¼ increases, the model shifts focus towards dimension-wise rewards, capturing more context-adaptive nuances. However, when ğ›¼ = 1, the model becomes overly focused on dimension-wise rewards, limiting its ability to assess broader, global preferences. Our results show that ğ›¼ = 0.7 strikes an optimal balance, combining context-aware reasoning with global quality assessment, leading to the best performance across both image and video generation tasks. This further demonstrates the effectiveness of our personalized multi-dimensional rewards in enhancing the models preference alignment."
        },
        {
            "title": "4.4 Discussion",
            "content": "Training Efficiency. Tab. 6 compares the per-step training time across different reward models. Fixed scorers and Bradley-Terrystyle models (e.g., PickScore, HPSv3) are the most efficient, as they directly output scalar rewards without explicit reasoning. However, this efficiency comes at the cost of coarse and less comprehensive reward signals with limited expressiveness. Reasoning-based VLMs introduce additional overhead. Specifically, UnifiedReward-Think is noticeably slower, since it performs long-chain reasoning under fixed evaluation rubric to produce judgments. Our UnifiedReward-Flex further increases the computation cost, as it goes beyond fixed criteria and conducts context-personalized reasoning: it dynamically 13 Figure 7 Monitoring Training Progress via CLIP Score. instantiates fine-grained sub-criteria and, when necessary, introduces new high-level dimensions conditioned on the prompt intent and visual evidence, rather than applying static checklist. Despite this extra compute, UnifiedReward-Flex delivers substantially stronger reward accuracy (Tab. 1) and consistently larger gains in GRPO for both image and video generation (Tabs. 3 and 4). These results indicate that the added training cost is reasonable and practical trade-off for improved alignment quality and generation performance. Training Progress Visualization. Quantitatively. Fig. 7 visualizes the training dynamics when using CLIP score as proxy metric to monitor semantic alignment during GRPO with UnifiedReward-Flex. Across both text-to-image (FLUX.1-dev) and text-to-video (Wan2.1-T2V-14B) settings, the CLIP score exhibits clear and consistent upward trend over training steps, indicating steadily improved prompt-image/video semantic consistency. The similarly stable increase in both domains suggests that our personalized, context-adaptive reward provides reliable optimization signals, reinforcing semantic understanding for both static image synthesis and temporal video generation. Qualitatively. Figs. 5 and 6 show consistent visual improvement trend during GRPO when using UnifiedReward-Flex as the reward. For image generation  (Fig. 5)  , later checkpoints better enforce prompt-critical constraints and compositional intent (e.g., the chest engraving becomes clearer and more faithful), indicating stronger semantic grounding beyond surface aesthetics. For video generation  (Fig. 6)  , the origami-dancer example evolves from limited, less coordinated motion to smoother, more coherent action progression with improved temporal consistency across frames. Overall, these training-time visualizations corroborate that UnifiedReward-Flex provides reliable, context-adaptive reward guidance that steadily strengthens both semantic adherence and temporal coherence during training."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces UnifiedReward-Flex, unified personalized reward model that circumvents the limitations of traditional one-size-fits-all evaluation in visual generation. By coupling dynamic hierarchical assessment with context-adaptive reasoning, our approach transcends rigid rubrics to capture the nuanced and subjective nature of human preferences. Leveraging two-stage training pipeline, i.e., structured reasoning distillation and reasoning-aware Direct Preference Optimization (DPO), we demonstrate that UnifiedReward-Flex yields significantly more precise and context-sensitive reward signals. Empirical validation within the Group Relative Policy Optimization (GRPO) framework reveals that our method consistently outperforms existing baselines, achieving significant improvements in both visual fidelity and semantic alignment for image and video synthesis."
        },
        {
            "title": "References",
            "content": "[1] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. [2] Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Unified multimodal chain-of-thought reward model through reinforcement fine-tuning. arXiv preprint arXiv:2505.03318, 2025. [3] Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024. 14 [4] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. [5] Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2.5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. [6] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for stable text-to-image reinforcement learning. arXiv preprint arXiv:2508.20751, 2025. [7] Yujie Zhou, Pengyang Ling, Jiazi Bu, Yibin Wang, Yuhang Zang, Jiaqi Wang, Li Niu, and Guangtao Zhai. G2rpo: Granular grpo for precise reward in flow models. arXiv preprint arXiv:2510.01982, 3, 2025. [8] Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025. [9] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [10] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. [11] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. [12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [13] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [14] Chrisoph Schuhmann. Laion aesthetics. https://github.com/LAION-AI/aesthetic-predictor, 2022. [15] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NeurIPS, 36:3665236663, 2023. [16] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [17] Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. In ICCV, pages 1508615095, 2025. [18] Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, et al. Videoscore2: Think before you score in generative video evaluation. arXiv preprint arXiv:2509.22799, 2025. [19] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, ShÄ³ie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [21] OpenAI. Update to GPT-5 system card: GPT-5.2. OpenAI System Card, 2025. URL https://cdn.openai.com/ pdf/3a4153c8-c748-4b71-8e31-aecbde944f8d/oai_5_2_system-card.pdf. [22] Qunzhong Wang, Jie Liu, Jiajun Liang, Yilei Jiang, Yuanxing Zhang, Jinyuan Chen, Yaozhi Zheng, Xintao Wang, Pengfei Wan, Xiangyu Yue, et al. Vr-thinker: Boosting video reward models through thinking-with-image reasoning. arXiv preprint arXiv:2510.10518, 2025. [23] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. [24] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. [25] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [26] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [27] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [28] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, 2023. [29] Zichen Miao, Jiang Wang, Ze Wang, Zhengyuan Yang, LÄ³uan Wang, Qiang Qiu, and Zicheng Liu. Training diffusion models towards diverse image generation with reinforcement learning. In CVPR, pages 1084410853, 2024. [30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36:5372853741, 2023. [31] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, pages 82288238, 2024. [32] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In CVPR, pages 89418951, 2024. [33] Runtao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omnipreference alignment for video diffusion generation. In CVPR, pages 80098019, 2025. [34] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [35] Haoyou Deng, Keyu Yan, Chaojie Mao, Xiang Wang, Yu Liu, Changxin Gao, and Nong Sang. Densegrpo: From sparse to dense reward for flow matching model alignment. arXiv preprint arXiv:2601.20218, 2026. [36] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024. [37] Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, and Marjan Ghazvininejad. Multimodal rewardbench 2: Evaluating omni reward models for interleaved text and image. arXiv preprint arXiv:2512.16899, 2025. [38] Haibo Tong, Zhaoyang Wang, Zhaorun Chen, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, Mingyu Ding, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Mj-video: Fine-grained benchmarking and rewarding video preferences in video generation, 2025. URL https://arxiv.org/abs/2502.01719. [39] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [40] Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, et al. Unigenbench++: unified semantic evaluation benchmark for text-to-image generation. arXiv preprint arXiv:2510.18701, 2025. [41] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. NeurIPS, 36:5213252152, 2023. [42] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. NeurIPS, 36:7872378747, 2023. [43] Team Wan et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [44] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024."
        },
        {
            "title": "Appendix",
            "content": "Figure 8 More Qualitative Result of UnifiedReward-Flex on Image Generation Personalized Reward Reasoning. 17 Figure 9 More Qualitative Result of UnifiedReward-Flex on Video Generation Personalized Reward Reasoning."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Lab",
        "Shanghai Innovation Institute",
        "Shanghai Jiaotong University"
    ]
}