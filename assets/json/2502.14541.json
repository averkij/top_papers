{
    "paper_title": "LLM-based User Profile Management for Recommender System",
    "authors": [
        "Seunghwan Bang",
        "Hwanjun Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations."
        },
        {
            "title": "Start",
            "content": "LLM-based User Profile Management for Recommender System Seunghwan Bang1 Hwanjun Song2, 1Ulsan National Institute of Science and Technology 2Korea Advanced Institute of Science and Technology shbang1422@unist.ac.kr songhwanjun@kaist.ac.kr 5 2 0 2 0 2 ] . [ 1 1 4 5 4 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: Review Extractor for identifying user preferences and key product features, Profile Updater for refining and updating user profiles, and Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of Large Language Models (LLMs) (Touvron et al., 2023; Dubey et al., 2024; Achiam et al., 2023; Team et al., 2024) has significantly impacted various domains, such as text summarization (Lewis et al., 2020a) and search (Karpukhin et al., 2020). Recent studies leverage LLMs in recommender systems for their human-like reasoning and external knowledge integration through in-context learning (Brown et al., 2020) and retrieval-augmented generation (Lewis et al., 2020b). As such, LLMs exhibit the potential indicates the corresponding author. to be used as zero-shot recommendation models without conventional training, which traditionally relies on explicit user-item interactions and training data (He et al., 2017; Kang and McAuley, 2018; He et al., 2020). Despite the advanced capability of LLMs, most recent works (Hou et al., 2024; Wei et al., 2024; Ren et al., 2024; He et al., 2023; Zhai et al., 2023) rely solely on users past purchase history (i.e., list of purchased items). This leaves significant room for further improvement by incorporating additional user-generated textual information, such as user reviews and product descriptions, which have yet to be fully leveraged. In other words, they still fail to fully leverage various text data due to their inability to retain and process the increasing contextual information as users continue to make purchases, leading to longer recommendation sessions. This issue is primarily attributed to the omission of the context, either due to the information loss within the LLMs memory (Liu et al., 2024) or the memory capacity by the token limit (Li et al., 2024; Ding et al., 2024). Thus, extracting key features from users diverse textual sources is essential, as demonstrated in MemoryBank (Zhong et al., 2024), framework that enhances LLMs with long-term memory by summarizing key information from conversations and updating user profiles. Building on this foundation, we take the first step in extending LLMs long-term memory beyond conversations in MemoryBank, adapting it to the evolving dynamics of recommendation systems. We propose PURE, novel LLM-based Profile Update for REcommender that constructs user profile by integrating users purchase history and reviews, which naturally expand as the recommendation sessions progress. Designed specifically for recommendation in Fig. 1, PURE systematically extracts user preferences, dislikes, and key features from reviews and integrates them into structured user profiles. Specifically, PURE consists of three Figure 1: Overall system of PURE. PURE incorporates reviews, ratings, and item interactions, whereas LLM Recommender handles only item interactions. By using the \"Review Extractor\" to identify key information and the \"Profile Updater\" to refine the user profile, PURE addresses scalability issue (i.e., growth of input token size). main components: \"Review Extractor\", which analyzes user reviews to identify and extract user preferences, dislikes, and preferred product features, referred to as \"key features\", offering comprehensive view of user interests and purchase-driving attributes; \"Profile Updater\", which refines newly extracted representations by eliminating redundancies and resolves conflicts with the existing user profile, ensuring compact and coherent user profile; and \"Recommender\", which utilizes the most up-to-date user profile for recommendation task. Our main contributions are as follows: (1) We propose PURE, novel framework that systematically extracts, summarizes, and stores key information from user reviews, optimizing LLM memory management for the recommendation. (2) We validate the effectiveness of PURE by introducing more realistic sequential recommendation setting, where reviews are incrementally added over time, allowing the model to update user profiles and predict the next purchase continuously. This setup more accurately reflects real-world recommendation scenarios compared to prior works, which assume all past purchases are provided at once, ignoring the evolving nature of user preferences. (3) We empirically show that PURE surpasses existing LLM-based recommendation methods on Amazon data, demonstrating its effectiveness in leveraging lengthy purchase history and user reviews."
        },
        {
            "title": "2 Related Works",
            "content": "els predict single target item, failing to capture evolving user behavior. Tallrec (Bao et al., 2023) framed the task as binary classification to predict whether an item should be recommended. LLM-based Recommendation. Tallrec (Bao et al., 2023) proposed the parameter efficient finetuning (PEFT) method in recommendation system, and A-LLMRec (Kim et al., 2024) proposed to finetune the embedding model for LLM to leverage the collaborative knowledge. In contrast, LLM elicited responses and extracted multiple representations from the conversation without extra training (Wang and Lim, 2023). Moreover, the authors (Dai et al., 2023) showed the potential of ChatGPT for reranking the candidates. InstructRec (Zhang et al., 2023) designed the instruction to recognize the users intention and preference from context."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "u, , iku u, , rku In our recommender system, we consider the user dataset as follow: Du = {Ru, Iu}, where Ru = {r1 } represents the historical reviews, Iu = {i1 } denotes the corresponding purchased items, and ku is the total number of purchased items from user u. Leveraging the users dataset Du, we aim to predict the next purchased item iku+1 from candidate set Cku+1 , which conu tains the ground-truth item. Recommendation Setup. Conventional sequential recommendation methods (Wang et al., 2019; Kang and McAuley, 2018; Sun et al., 2019; Hidasi and Karatzoglou, 2018; Kim et al., 2024) followed one-shot prediction setup, where user history is split: the last item as the test set, the second-to-last as validation, and the rest for training. These modOne-shot Sequential Recommendation. It predicts single next item based on static history of user interactions up to timestep ku 1. Given the dataset Du, the model observes Dku1 = {Rku1 , Iku1 from the candidate set Cku . This focuses on one-time prediction without considering future timesteps. } and predicts the last item iku Algorithm 1: PURE Input: Review extractor E(), User profile updater U(), Recommender R(), Dataset Du = {Ru, Iu} for user u, User profile Pt u, next purchase candidates Ct+1 , timestep # Extract representations from reviews u, u, dt lt = E(rt u) lt ˆlt = lt1 List of items user likes dt ˆdt = dt1 ˆf = t1 # Update user profile after redundancy removal u, ˆdt = U( ˆlt lt u, u, dt u, u, dt = {lt Pt u} # Recommend next purchase item pred = R(Pt u, It Output: pred List of users key features List of items user dislikes u, Ct+1 u, ˆf u) ) Continuous Sequential Recommendation. This setup predicts the next item at every timestep (4 ku 1), making it multi-step prediction task. At each timestep t, the model observes the updated interaction history Dt u} and predicts the next item it+1 from the candidate set Ct+1 . This multi-step prediction process effectively captures temporal dependencies and allows continuous updates of user preferences, making it more aligned with real-world scenarios. = {Rt u, It 3.2 PURE: Profile Update for REcommender In this section, we introduce PURE, novel framework that manages the user profile Pu from user reviews Ru and predict the next item with user profile. Algorithm 1 can be divided into three steps (See Appendix for prompt template). STEP 1: Extract User Representation. We begin by providing the LLM with raw inputs, including user reviews Ru and product names Iu. The LLM extracts lt u(items the user dislikes), and u(key user features) from the incoming review as user representation. u(items the user likes), dt u, dt , dt1 STEP 2: Update User Profile. After the extraction in STEP 1, the extracted repu, resentation < lt u> concatenates with previous user profile Pt1 , t1 = {lt1 }. However, this faces scalability issue as the number of reviews increases. Thus, leveraging the previous profile, we use an LLM to remove redundant and conflicting content from the extracted representation, yielding more compact and up-to-date user profile Pt after concatenation. STEP 3: Recommend Next Purhcase Item. Recommender reranks the given candidate item list to predict the users next purchase by leveraging the updated profile Pt and purchased items Iu."
        },
        {
            "title": "4 Experiment",
            "content": "Datasets. For thorough evaluation, we utilize two datasets from the Amazon collection (Ni et al., 2019): Video Games and Movies & TV. To ensure comprehensive analysis, we intentionally select datasets with diverse statistical properties, particularly in terms of the number of items (See Appendix for details). Baselines. (Hou et al., 2024) is the recommendation method that utilizes pre-trained LLMs without additional training or fine-tuning, making it suitable baseline. It describes three approaches for LLM-based recommendation: Sequential, Recency, and in-context learning (ICL). We compare our method with all three approaches and demonstrate the superiority of PURE when these techniques were applied to our framework, further highlighting its effectiveness. (See Appendix C.1 for details.) Evaluation Setting. To assess the performance of PURE, we adopt continuous sequential recommendation task. Note that NDCG scores are first aggregated per user across multiple recommendation sessions and then across all users, reflecting the continuous nature of our setup. Implementation Details. The prediction process is framed as classification task where the model selects one item from candidate set Cu. Each candidate set consists of 19 randomly selected noninteracted items and ground truth item. We adopt Llama-3.2-3B-Instruct (Touvron et al., 2023) as the backbone model for all the experiments."
        },
        {
            "title": "4.1 Experimental Results",
            "content": "Impact of Review Extractor. Tab. 1 compares PURE with (1) three baselines solely based on purchased items; (2) modified baselines, marked with , that additionally utilize users raw reviews. The results reveal that baselines that simply combine item interactions with raw reviews show inconsistent performance improvements. In contrast, PURE, which leverages the review extractor and profile updater, significantly outperforms all baselines. This demonstrates that processing reviews at three levels, i.e., like, dislike, and key features, is essential for enhancing performance. Component-wise Study. Tab. 2 shows the ablation study of PURE, where we analyze the impact Games Movies Data Method t s v + t Sequential Recency ICL Sequential Recency ICL PURE (Sequential) PURE (Recency) PURE (ICL) N@1 10.75 15.34 14. 11.14 12.19 15.11 15.06 18.18 16.62 N@5 18.25 24.31 26.57 19.95 23.64 26.34 25.71 28.90 29. N@10 N@20 23.13 28.82 30.51 24.97 28.37 31.25 31.08 33.91 35.60 28.97 34.24 35. 32.00 35.35 37.39 38.28 40.69 42.00 N@1 9.99 12.17 12.03 8.05 8.54 12.24 12.59 13.85 15. N@5 15.92 17.75 19.56 13.11 15.78 22.10 21.33 21.99 26.32 N@10 N@ 20.17 22.18 23.36 17.72 21.31 27.31 25.96 26.53 32.03 26.94 28.19 29.91 25.57 29.21 34.52 32.21 33.37 38. Table 1: Comparison PURE with Baselines. We evaluate performance under two data settings: using only item interactions and using item interactions augmented with reviews. indicates customized baselines where review data is naively incorporated into the original prompt templates designed for item interactions only (see Appendix C.2). Data Components Games Movies Method items reviews Rec. Ext. Upd. N@1 N@ N@10 N@20 N@1 N@5 N@10 N@20 l n e n R 10.75 11.14 16.09 15.06 15.34 12.19 20.85 18.18 14.28 15.11 19.60 16.62 18.25 19.95 26.94 25.71 24.31 23.64 31.36 28. 26.57 26.34 32.96 29.81 23.13 24.97 32.35 31.08 28.82 28.37 36.51 33.91 30.51 31.25 38.21 35.60 28.97 32.00 40.08 38.28 34.24 35.35 43.19 40. 35.72 37.39 44.97 42.00 245.52 29165.17 486.49 415.01 253.31 29235.16 602.13 485.85 268.40 29388.72 803.60 592.48 9.99 8.05 13.05 12.59 12.17 8.54 16.00 13. 12.03 12.24 16.05 15.80 15.92 13.11 21.38 21.33 17.75 15.78 24.81 21.99 19.56 22.10 27.25 26.32 20.17 17.72 26.11 25.96 22.18 21.31 29.66 26. 23.36 27.31 33.11 32.03 26.94 25.57 32.62 32.21 28.19 29.21 36.98 33.37 29.91 34.52 40.15 38.93 243.89 60429.80 459.69 384.87 249.64 60509.43 565.13 458. 261.58 60800.61 867.36 634.02 Table 2: Component-wise study of PURE. Each configuration varies which data sources (items, reviews) and which PURE components are used (Rec. = Recommendation, Ext. = Extractor, Upd. = Updater), as indicated by . We report N@k scores (k {1, 5, 10, 20}) and average of input token size (T) for Recommender. (Upd.) to maintain compact user profiles, reducing input token size by 1520% with only slight 13% performance drop. This trade-off underscores the importance of using Profile Updater for long-term recommendations. Trade-off Analysis. We categorize users into three groups based on the total cumulative review token count per user, as the criterion: 0500 (short), 5001000 (middle), and 10002000 (long) tokens. Fig. 2 presents the trade-off between recommendation performance and input token length of the three models including PURE. PURE achieves the best trade-off, showing the steepest NDCG increase compared to other methods as input token size grows. Therefore, this demonstrates that PURE accurately distills key information from long reviews, while achieving high efficiency by minimizing input token growth without information loss, even for long-group users."
        },
        {
            "title": "5 Conclusion",
            "content": "We present PURE, novel framework for LLMbased recommendation that builds and maintains evolving user profiles by systematically extracting and summarizing user representations from reviews. By introducing continuous sequential (a) Video Games (b) Movies and TV Figure 2: Trade-off between NDCG and token size. of reviews (using or not using) and the effect of components (enabling or disabling the review extractor and profile updater). The use of reviews bring high performance gains only when accompanied by Review Extractor (Ext.). This is due to the sharp increase in input tokens (see the column of the 2nd and 3rd rows of each method) as the user continues purchases. Notably, the best recommendation performance is achieved when Profile Updater (Upd.) is disabled (see the 3rd and 4th rows for each method). That is well-formed context by Review Extractor can bring higher gains when simply concatenated. However, it may face challenge, as the number of purchases grows, leading to significant computational overhead. Thus, we use the Profile Updater recommendation task, we demonstrated how updating user profiles improves recommendation quality while addressing token limitation challenges. the Ministry of Science and ICT (MSIT, Korea) & Gwangju Metropolitan City (No. BA00001698)."
        },
        {
            "title": "References",
            "content": "A notable limitation of our approach is the tendency of the LLM to exhibit hallucination by occasionally recommending items beyond the predefined candidate set, even when explicitly instructed to select from it. This phenomenon underscores the inherent difficulty in imposing strict constraints within LLM-based recommendation models while maintaining flexibility and accuracy. Also, our study was constrained by the inability to utilize datasets containing larger number of user reviews, which may have provided richer context."
        },
        {
            "title": "7 Potential Risks",
            "content": "A potential risk associated with our approach is the possibility that user reviews may contain personal information, making data management and privacy protection critical concerns. Ensuring secure handling and anonymization of such data is essential to prevent breaches of user privacy."
        },
        {
            "title": "8 Ethical Statement",
            "content": "This study used Amazon datasets which is publicly available. The dataset does not contain any personal identifiable information (PII), ensuring user privacy and ethical compliance. To ensure fair and accurate evaluation, we customized the baseline models by incorporating both item interactions and user reviews, enabling more balanced comparison with our proposed approach. Lastly, our research aims to advance the development of recommendation systems while avoiding potential negative impacts such as bias or misuse."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was mainly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea goverment (MSIT) (No. RS-2024-00445087, Enhancing AI Model Reliability Through DomainSpecific Automated Value Alignment Assessment) and by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00334343). Additionally, this work utilized GPU infrastructure supported by Artificial Intelligence industrial convergence cluster development project funded by Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. ArXiv:2303.08774. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 10071014. Tom Brown, Benjamin Mann, Nick Ryder, et al. 2020. Language models are few-shot learners. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 18771901. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering chatgpts capabilities in recommender systems. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 11261132. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Longrope: Extending and Mao Yang. 2024. llm context window beyond 2 million tokens. ArXiv:2402.13753. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. ArXiv:2407.21783. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of international ACM SIGIR conference on research and development in Information Retrieval, pages 639648. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of International Conference on World Wide Web (WWW), pages 173182. Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large language models as zero-shot conversational recommenders. In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM), pages 720730. Balázs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for sessionbased recommendations. In Proceedings of the 27th ACM international conference on information and knowledge management, pages 843852. models for recommendation. In Proceedings of the ACM on Web Conference, pages 34643475. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers In Proceedings of Eufor recommender systems. ropean Conference on Information Retrieval, pages 364381. Springer. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 14411450. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. ArXiv:2408.00118. Hugo Touvron, Thibaut Lavril, et al. 2023. Llama: Open and efficient foundation language models. ArXiv:2302.13971. Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. ArXiv:2304.03153. Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Sheng, and Mehmet Orgun. 2019. Sequential recommender systems: Challenges, progress and prospects. In Proceedings of International Joint Conference on Artificial Intelligence Organization (IJCAI), pages 63326338. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models with In Prograph augmentation for recommendation. ceedings of the ACM International Conference on Web Search and Data Mining, pages 806815. Jianyang Zhai, Xiawu Zheng, Chang-Dong Wang, Hui Li, and Yonghong Tian. 2023. Knowledge prompttuning for sequential recommendation. In Proceedings of ACM International Conference on Multimedia, pages 64516461. Junjie Zhang, Ruobing Xie, Yupeng Hou, Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: large language model empowered recommendation approach. ACM Transactions on Information Systems. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI), volume 38, pages 1972419731. Wang-Cheng Kang and Julian McAuley. 2018. Selfattentive sequential recommendation. In Proceedings of International Conference on Data Mining (ICDM), pages 197206. IEEE. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 67696781. Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park. 2024. Large language models meet collaborative filtering: An efficient all-round llm-based recommender system. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 13951406. Mark Lewis, Yinhan Liu, et al. 2020a. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 94599474. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. 2024. survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 1(1):9. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLPIJCNLP), pages 188197. Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language -Supplementary MaterialLLM-based User Profile Management for Recommender System"
        },
        {
            "title": "A Prompt Template",
            "content": "A.1 Extractor The extractor aims to extract the user representations from reviews. Here is the prompt template."
        },
        {
            "title": "Prompt template for Extractor E",
            "content": "reviews chronologically for each user. Here are the specific descriptions for each dataset. Video Games. We select about 15K users and 37K items. Following existing studies (Kang and McAuley, 2018), we removed users and items with fewer than 10 interactions. purchased f o g d s and t i i r l c r : Analyze user k / l s / key t s by i . e n t r { u _ i } Movies and TV. We select about 98K users and 126K items, removing users and items with fewer than 10 interactions as in the Video Games dataset. A.2 Profile Updater The purpose of the profile updater is to remove the redundant information in the user profile. As such, the prompt template is designed as below:"
        },
        {
            "title": "Prompt template for User Profile Updater U",
            "content": "You are e i : Update s r p i r i . Note t c i r i should be preserved . t by removing redundant { t } A.3 Recommender Due to utilizing both item interactions and user profile, prompt can be constituted of various components. Below one is the prompt template of the recommender."
        },
        {
            "title": "Prompt template for Recommender R",
            "content": "{ e } { l s } { _ t s } i e aspects : Negative aspects : Key Features : Based on these u , from 1 20 by l i h being purchased . rank { d t _ t } e o f"
        },
        {
            "title": "B Dataset",
            "content": "Amazon Review Dataset (Ni et al., 2019) contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 July 2014. Specifically, this dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs). Among them, we selected two domain datasets (Video Games and Movies & TV), and we utilized ASIN, product name, rating, and review for each data and sort the"
        },
        {
            "title": "C Baselines",
            "content": "C.1 User-Item interactions In our experimental setup, the LLM is tasked with predicting the item that user is likely to purchase at time step t. We utilize user-item interactions up to time step (t-1) in chronological order and constructed candidate list consisting of one groundtruth item and 19 non-interacted items as input. Here, time step refers to the period starting from the users 4th purchase up to their final purchase k. Sequential. We provide the LLM with instructions, supplying only the user-item interactions and the candidate list. The LLM was then tasked with ranking the items in the candidate list based on the likelihood of being purchased at time step t. In the sequential prompt Recency-Focused. above, we add an instruction to emphasize the most recently purchased item, specifically the item bought at time step (t-1). The additional prompt is as follows: \"Note that my most recently purchased item is {recent item}.\" In-Context Learning. Unlike the previous sequential and recency-focused prompts, this approach utilize user-item interactions only up to time step (t-2) and recently purchased item which is bought at time step (t-1) as input. The additional prompt is as follows: \"Ive purchased the following products: {user-item interactions}, then you should recommend {recent item} to me and now that Ive bought {recent item}.\" C.2 User-Item interactions & User Reviews In this setup, we extend user-item interactions to include both interactions and user reviews. Based on Appendix C.1, the present the results when both user-item interactions and user reviews are used as input."
        },
        {
            "title": "D Qualitative Results",
            "content": "To validate the effectiveness of each component of PURE, we summarized the qualitative results in Tab. 3, which illustrates the entire input/output process for both the baselines and PURE in the sequential recommendation task. We can observe that the Review Extractor first removes irrelevant or uninformative content for the given reviews, while the Profile Updater reduces redundancy and overlapping information in the user profile. As such, we can conclude that PURE reduces the input token size of the recommender system while retaining essential information, making it more memory-efficient and potentially improving overall performance. Method Type Contents Recommender Input Baselines Ive purchased the following products in chronological order: {user-item interactions & reviews} Then if ask you to recommend new product to me according to the given purchasing history, you should recommend {recent item} and now that Ive just purchased {recent item}. There are 20 candidate products that can consider to purchase next: {20 candidate items} Please rank these 20 products by measuring the possibilities that would like to purchase next most, according to the given purchasing records. Please think step by step. Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the given candidate product. You cannot generate products that are not in the given candidate list. No other description is needed. Recommender Output [20 ordered items] Review Extractor Input purchased the following products in chronological order: {user-item interactions & reviews} Then if ask you to recommend new product to me according to the given purchasing history, you should recommend {recent item} and now Ive just purchased {recent item}. And left review: {recent item review} Your task is to analyze users purchasing behavior and extract users likes, dislikes and key features from the input review. Response only likes/dislikes/key features in descriptive form. Please prioritize the most recent item {recent item} when analyzing likes/dislikes/key features. Split likes, dislikes, and key features and response in same format. Likes: {[*Long gameplay experience(50-60 hours), *Responsive controls , *Fantastic storyline , *Challenging puzzles, *Emotional resonance (e.g.remorse), *Ability to gain new posers by killing enemies, *Humor and fun in games , *References to the simpsons franchise , *Variety of playable characters (Marge, Lisa, Apu, Bart, and Homer) , *Ability to drive or walk depending on preference, *Great voice acting from the cast members , *Presence of key locations from the Simpsons universe (Kwik-E-Mart, Power Plant, Church, etc.) , Review Extractor Output *Cool vehicle designs and stats, *Fantastic game overall ]} Dislikes: {[*No pause time when selecting weapon, making the player vulnerable, *Inventory management can be inconvenient, requiring the player to switch to the inventory screen to user gadgets , *Boring story , *Not funny , *Awful weapons , *Unresponsive controls, *Terrible graphics, *Worse gameplay]} Key Features: {[ *No in-game loading , *Fighting mechanics, *Soul-hunger gameplay mechanic, *Ability to cover up face to hide disfigured jaw , *New camera system (Devil May Cry position) , *Redone fighting mechanics, *Playable as both Raziel and Kain, *Puzzles with challenging but fun diffculty level ]} Profile Updater Input You are given list: {list of likes/dislikes/key features} You have to update this list by removing redundant or overlapping information. Note that crucial information should be preserved. Please response only list. No other description is needed. PURE (Ours) Likes: {[*Long Gameplay experience (50-60 hours), *Challenging puzzles, *Emotional resonance (e.g.remorse), *Ability to gain new powers by killing enemies, *Variety of playable characters , *Ability to drive or walk depending on preference, *Presence of key locations from the Simpsons universe , Profile Updater Output *Great voice acting , *Cool vehicle designs and stats]} Dislikes: {[*No pause time when selecting weapon, making the player vulnerable, *Inventory management can be inconvenient , *Unresponsive controls, *Terrible graphics, *Worse gameplay]} Key Features: {[*Fighting mechanics, *Soul-hunger gameplay mechanic, *Ability to cover up face , *New camera system , *Redone fighting mechanics, *Playable as both Raziel and Kain, *Puzzles ]}"
        },
        {
            "title": "Recommender\nInput",
            "content": "This is positive aspects from purchase history: {[*Long Gameplay experience (50-60 hours), *Challenging puzzles, *Emotional resonance (e.g.remorse), *Ability to gain new powers by killing enemies, *Variety of playable characters, *Ability to drive or walk depending on preference, *Presence of key locations from the Simpsons universe, *Great voice acting, *Cool vehicle designs and stats]} This is negative aspects from purchase history: {[*No pause time when selecting weapon, making the player vulnerable, *Inventory management can be inconvenient, *Unresponsive controls, *Terrible graphics, *Worse gameplay]} This is key features of products: {[*Fighting mechanics, *Soul-hunger gameplay mechanic, *Ability to cover up face, *New camera system, *Redone fighting mechanics, *Playable as both Raziel and Kain, *Puzzles]} Based on these inputs, your task is to rank 20 candidate products by evaluating their likelihood of being purchased. Now there are 20 candidate products that consider to purchase next. Note that there is no specific order for these candidate items. Please rank the {20 candidate items} from 1 to 20. Your task is to rank these products based on the likelihood of purchase. You cannot generate products that are not in the given candidate list. No other description is needed."
        },
        {
            "title": "Recommender\nOutput",
            "content": "{[20 ordered items]} Table 3: Qualitative Results: Baselines vs PURE. Note that green-highlighted boxes indicate portions removed due to redundancy or overlapping information, while yellow-highlighted boxes represent summarized content where unnecessary modifiers or examples were omitted for conciseness."
        }
    ],
    "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Ulsan National Institute of Science and Technology"
    ]
}