{
    "paper_title": "MultiRef: Controllable Image Generation with Multiple Visual References",
    "authors": [
        "Ruoxi Chen",
        "Dongping Chen",
        "Siyuan Wu",
        "Sinan Wang",
        "Shiyun Lang",
        "Petr Sushko",
        "Gaoyang Jiang",
        "Yao Wan",
        "Ranjay Krishna"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/."
        },
        {
            "title": "Start",
            "content": "MultiRef: Controllable Image Generation with Multiple Visual References Dongping Chen University of Washington Seattle, USA Ruoxi Chen Zhejiang Wanli University Ningbo, China Siyuan Wu Huazhong University of Science and Technology Wuhan, China 5 2 0 2 9 ] . [ 1 5 0 9 6 0 . 8 0 5 2 : r Sinan Wang Huazhong University of Science and Technology Wuhan, China Shiyun Lang Huazhong University of Science and Technology Wuhan, China Peter Sushko Allen Institute for AI Seattle, USA Gaoyang Jiang Huazhong University of Science and Technology Wuhan, China Yao Wan Huazhong University of Science and Technology Wuhan, China Ranjay Krishna* University of Washington Allen Institute for AI Seattle, USA"
        },
        {
            "title": "Keywords",
            "content": "Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs - either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/. CCS Concepts Computing methodologies Artificial intelligence; General and reference Evaluation. This work is licensed under Creative Commons Attribution 4.0 International License. MM 25, Dublin, Ireland 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2035-2/2025/10 https://doi.org/10.1145/3746027.3758292 Controllable image generation, multi-images-to-image, unified models, Benchmark, Dataset ACM Reference Format: Ruoxi Chen, Dongping Chen, Siyuan Wu, Sinan Wang, Shiyun Lang, Peter Sushko, Gaoyang Jiang, Yao Wan, and Ranjay Krishna*. 2025. MultiRef: Controllable Image Generation with Multiple Visual References. In Proceedings of the 33rd ACM International Conference on Multimedia (MM 25), October 2731, 2025, Dublin, Ireland. ACM, New York, NY, USA, 22 pages. https://doi.org/10.1145/3746027."
        },
        {
            "title": "1 Introduction",
            "content": "Digital artists and visual designers often create new scene by blending elements from multiple source images: color palette from Monet painting, the architectural form of the Eiffel Tower from photograph, and the texture from hand-drawn sketch. Artists draw inspiration from multiple visual references, mixing diverse elements. This multi-reference creative process allows far more controllable image creation than relying on single source of inspiration (Figure 1). However, current tools for this artistic process remain too primitive to be directly useful. Despite this artistic need, todays image generators predominantly rely on single-source conditioningeither text prompt (i.e., text-to-image [12, 45]) or one reference image (i.e., image editing [31, 46], image translation [20, 54]) at time. In essence, asking modern image generative model to paint scene in the style of Van Gogh with the composition of photograph requires specific prompt engineering [18, 27] or sequential editing [26, 53]. Moreover, visual references may have inconsistent viewpoints, styles, or semantics, and merging them can produce contradictions (e.g., blending daytime landscape with night-time style reference). Existing approaches like ControlNet [62] excel at following one conditioning signal (i.e., edge map and depth), but they are not inherently designed to handle multiple different conditions at once. Additionally, naively adding more control inputs usually confuses the model, leading to jumbled or degraded outputs [63]. Corresponding author MM 25, October 2731, 2025, Dublin, Ireland Chen et al. We propose new protocols to evaluate the generations using our benchmark. We leverage rule-based (e.g., MSE for depth) and modelbased (e.g., ClipScore [21] for aesthetic) assessments for conditions that require precise evaluation (e.g., depth, mask and bbox) and fine-tuned MLLM-as-a-Judge [5] for semantic-level assessments (e.g., caption, sketch and semantic map) in both reference-following and overall quality with human-annotated scores. We evaluate three interleaved image-text generation models (e.g., OmniGen [56], ACE [19], Show-o [57]) and 6 agentic frameworks (e.g., ChatDiT [26], LLM [2, 16] + Diffusion [12, 45]). Experimental results reveal that even the most advanced general-purpose image generators today struggle with multi-reference conditioning. State-of-the-art diffusion and autoregressive models that claim to support arbitrary conditioning (e.g., recent unified models) often falter when actually confronted with multiple visual inputs. For instance, model might capture the style of one reference image well but completely ignore the content from another subject reference. Quantitatively, we observe substantial performance gaps: the best existing model OmniGen achieves only 0.496 of the desired alignment score on multi-reference tasks, compared to its near-perfect performance on single-reference inputs. These results expose clear weakness in current systems despite their advertised flexibility, they are not truly equipped for multi-reference generation. By highlighting these shortcomings, our study provides valuable insights and direction for future research."
        },
        {
            "title": "2 MultiRef-bench",
            "content": "To facilitate evaluation of multi-reference image generation models, we introduce MultiRef-bench, the first benchmark combining real-world examples and synthetic data through dual-pipeline methodology. The first pipeline gathers authentic tasks from publicly available sources, while the second leverages computer vision techniques to generate diverse conditional features. MultiRefbench comprises 1,990 examples: 1,000 real-world tasks from Reddits r/PhotoshopRequest community, selected for its diverse editing tasks and active engagement, and 990 synthetic examples from MultiRefs 38,076 samples generated using RefBlend our framework producing diverse guidance signals including depth maps, bounding boxes, and art styles for comprehensive conditional generation scenarios. Statistics are shown in Figure 3."
        },
        {
            "title": "2.1 Real-World Queries Collection",
            "content": "To develop robust benchmark for conditional image generation, we incorporate real-world tasks from Reddits r/PhotoshopRequest community, following RealEdit [51]. This platform provides authentic user-submitted editing requests requiring multiple input images. We collected 2,300 queries, selecting tasks that combine multiple images. Each datapoint includes input images, original instructions, and output images. Manual evaluation ensures data quality by verifying image necessity, instruction coherence, and output accuracy. When multiple outputs exist, annotators select the best based on clarity and instruction fidelity. To handle noisy instructions and specify image references, we employ GPT-4o [40] to generate structured prompts with image Figure 1: Image generation conditioned on multiple visual references provides more controllable and creative digital art generation than single image or textual reference. Given these limitations in handling multiple references, there is growing need to benchmark current multi-reference generation models. From our investigation, most popular benchmarks in generative modeling focus on text-to-image alignment or singleimage editing. For example, IDEA-Bench [33] targets professional design scenarios but still typically deals with one reference at time or sequential editing. Similarly, ACE [19] evaluates alignment with instructions but does not stress-test combining several images. No established benchmark yet examines models on truly multireference tasks for their integrating complexity, making it hard to quantify current research progress. In this paper, we introduce MultiRef-bench, benchmark that rigorously evaluates multi-reference generation models with 1,000 real-world samples and 990 synthetic samples that are programmatically generated. Specifically, we compile challenging user requests from Reddit [51], where both references and ground truth images are real, to evaluate the image generation from multiple visual references. Our benchmark covers spectrum of tasks, ranging from relatively straightforward scenariossuch as applying two independent referencesto complex scenarios requiring simultaneous spatial and semantic alignment across multiple sources. To address the scarcity of multi-reference image generation datasets, we develop novel synthetic data engine, termed RefBlend, that efficiently creates diverse training samples. RefBlend first extracts various visual references (e.g., depth maps, Canny edge, object masks) from original images using state-of-the-art extraction models. These references are then organized into compatibility graph structure, where nodes represent individual references and edges indicate which references can be combined without contradictions, enabling diverse and high-quality multi-reference to image samples at scale. This engine can generate synthetic samples by flexibly combining diverse reference modalitiese.g., semantic map, human pose, and caption, each describing different aspects of the intended outputwhile treating the original image as the corresponding target. By controlling the data generation process, we automatically obtain rich ground-truth pairings of inputs and outputs. Finally, MultiRef-bench contains 990 synthetic samples covering 10 reference types and 33 reference combinations, far surpassing any existing collection in both scale and complexity. MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland Figure 2: An overview of RefBlend. It consists of reference generation (in yellow) and instruction prompt generation (in blue). First, various references (canny, depth, etc) are extracted from an original image. Then, basic instruction prompt is formed from selected compatible references. Finally, the enhanced prompt is integrated with references to construct sample. Table 1: Reference Compatibility Matrix. Both rows and columns represent reference names. Yellow: Local Spatial Constraints. Green: Semantic Content Specification. Purple: Global Structural Guidance. Pink: Semantic Content Specification. Bounding box Mask Pose Bounding box Mask Pose Caption Subject Semantic map Depth Canny Sketch Art style - - - Caption - Subject - Semantic map Depth Canny Sketch Art style - - - - - : possible, i.e., the combination of row and column is feasible but does not depend on each other. invalid and cannot coexist. : dependency, i.e., when the row is present, the corresponding column condition must also be met. : impossible, i.e., the combination of row and column is Figure 3: Left, Middle: Distribution analysis of textual content length and image count for synthetic and real-world parts. Right: Reference frequency in synthetic data. reference tokens (e.g., <image1>). All generated instructions undergo manual review for clarity and consistency. Missing image references are manually corrected by annotators. We categorized datapoints using OmniEdits taxonomy [55]: Element Replacement, Element Addition, Style and Appearance Modifications, Spatial/Environment Modifications, and Attribute Transfer. Categorization was performed using GPT-4o. We balance the benchmark following the real-world distribution in the crawled raw user requests. After rigorous quality control, 45% of the collected data (1,000 examples) met our criteria. Each example comprises 2-6 input images, one structured instruction, and one golden output image. See Supplementary Material for more details. MM 25, October 2731, 2025, Dublin, Ireland Chen et al. Table 2: Evaluation dimension and metrics of MultiRef-bench for synthetic multi-ref generation. Rule: Golden standard for evaluation criteria. Model: We leverage fine-tuned MLLM-as-a-judge for human-aligned semantic visual references evaluation. Evaluation Dimension General Quality Reference Fidelity Evaluation Aspect Image Quality Visual Attractiveness Bounding Box Semantic Map Mask Depth Map Canny Edge Sketch Caption Pose Subject Art Style Evaluation Criteria Visual Fidelity Aesthetic Appeal Spatial Accuracy Segmentation Accuracy Mask Alignmen Depth Accuracy Edge Preservation Structural Fidelity Text-Image Alignment Pose Accuracy Subject Consistency Style Consistency Quantitative Metrics FID CLIP Aesthetic Scores IoU IoU IoU MSE MSE MSE CLIP Text-Image Score mAP CLIP Image Score CLIP Image Score Instruction Following Instruction Adherence Instruction-Output Alignment - For pose, single reference image may contain multiple instances (e.g., multiple poses merged in one reference image). Rule Model -"
        },
        {
            "title": "2.2 RefBlend: The Synthetic Data Engine",
            "content": "To construct an extensive benchmark, we develop dataset generation engine RefBlend that employs four-step process to produce 38,076 samples across 34 reference combinations. The process includes: (1) generating reference conditioning (bounding boxes, depth maps, etc.), (2) programmatically producing condition combinations based on compatibility rules, (3) aligning multiple references through text prompts, and (4) deploying filtering to eliminate low-quality results. Step 1: Generate Reference Conditions. Given an original image, RefBlend leverages recent advanced models (e.g. Grounded SAM2 [44], SAM2 [43], Depth Anything2 [59]), to synthesize diverse set of conditioning inputs. These include canny edges, semantic maps, sketches, depth maps, bounding boxes, masks, poses, art styles and subjects, along with textual captions generated by GPT4o-mini [39]. These reference guidance types have proved themselves in controllable image generation in prior work [23, 42, 62, 63]. Our original images are sampled in wide range from DreamBooth [46], CustomConcept101 [29], Subjects200K [52], WikiArt [47], Human-Art [28], StyleBooth [20] and VITON-HD [10], which attach references about pose, subject, and art style within the dataset and for the diversity of metadata. Step 2: Combining References. References have compatibility constraints and dependencies. Some references are mutually exclusive, while others have specific dependencies that must be considered. We establish Reference Compatibility Rules that define valid combinations among different conditions, avoiding conflicts and redundancy. Based on it, we can obtain the reference compatibility matrix for better visualization  (Table 1)  . To ensure diversity and complexity within the dataset, we generate possible combinations of 2, 3, or 4 references per instruction while strictly adhering to compatibility rules. Visual Reference Compatibility Rules. These rules define valid combinations among image references (style, depth, edges, segmentation), specifying compatible pairings, mutual exclusions, and dependencies. We establish three fundamental rules for references: Mutual Exclusivity of Global Reference: Reference containing global information cannot be combined with each other, as this would result in information overlap. Global-Local Information Incompatibility: References with local information cannot be combined with those containing global information to avoid redundancy. Reference Dependencies: ➀ Universal Combinability: Style and caption can be combined with any other references; ➁ Semantic Context Requirement: Mask and bounding box references require semantic context through either subject or caption. These spatial localization references need to be attached to specific objects or concepts. Step 3: Generating Instructions. Using the valid reference combinations generated in Step 2, we create two types of prompts: structured and enhanced. Structured prompts are generated using template-based approach that maps each reference type to standardized phrase. For example, depth reference might use the placeholder <depth_image> with associated phrases such as guided by the depth of <depth_image>. Caption references are appended with simple introductory phrases like following the caption:. This method ensures that prompts are clear, consistent, and easy to parse. To broaden the scope and realism of our dataset, we transform structured prompts into more diverse and natural instructions using GPT-4o [40]. By applying different personas from Persona Hub [15], we vary the language, tone, and style of the prompts while maintaining the reference structure and intended content. This process not only enriches the prompts with creative and contextually relevant variations but also challenges models with wide range of linguistic expressions and scenarios. The enhanced prompts, when combined with the generated references, result in robust and versatile dataset suitable for comprehensive model evaluation. Step 4: Filtering. After generating visual references, we apply rule-based filter using metrics such as confidence score threshold of 0.8 for the IoU (Intersection over Union) of semantic maps. For more semantic-level visual referencessuch as subject, style, sketch, and cannythat do not provide confidence scores, we finetuned Qwen-2.5-VL-7B-Instruct as scoring-based filter [5]. This model evaluates both the alignment between original images and generated references and their overall quality. Trained on 16,590 human-annotated scoring samples, the model achieves 0.914 MAE and 0.642 Pearson correlation on 1,750-sample test set, demonstrating performance comparable to strong proprietary models such MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland Table 3: Evaluating MLLM-as-a-Judge using Pearson similarity with cross-validated human-annotated ground truth. Human-Human shows the alignment between humans."
        },
        {
            "title": "3 Experiments and Analysis\n3.1 Experiment Setups",
            "content": "Model Gemini-2.0-Flash GPT-4o-mini GPT-4o Human-Human IQ 0.385 0.466 0.432 0.589 Realistic IF SF IQ Synthetic IF 0.422 0.530 0.624 0.665 0.354 0.514 0.613 0.571 0.369 0.438 0.406 0.629 0.627 0.632 0.668 0. SF 0.588 0.616 0.659 0.694 Figure 4: Real-world image generation conditioned on multiple image references. Most image generative models struggle with accurately following instructions and maintaining fidelity to source images. as GPT-4o-mini. See Supplementary Materials for additional details."
        },
        {
            "title": "2.3 Evaluation",
            "content": "Our approach combines rule-based and model-based metrics to provide comprehensive assessment of reference following capabilities across diverse conditions. The evaluation dimension and metrics of MultiRef-bench are shown in Table 2. All evaluation metrics are finally normalized to [0, 1] range for consistency. Reference Fidelity. It measures how accurately generated images preserve and incorporate the specific attributes, features, and characteristics from reference inputs. For the 10 reference types included in our benchmark, we employ specialized evaluation criteria and metrics tailored to each reference, then derive the overall fidelity score by averaging across all references involved. Notably, for aspects where rule-based metrics may not fully capture nuanced performanceparticularly style consistency and subject fidelity we supplement our evaluation with MLLM-as-a-Judge assessments by our finetuned model for complementary qualitative insights. Image Quality. It assesses the visual quality and aesthetic appeal of generated images, independent of reference fidelity. To evaluate this dimension comprehensively, we employ two complementary metrics: FID [22] and CLIP aesthetic scores [48], to evaluate the image quality and creative aspects of the generated content. Overall Assessment. We follow Chen et al. [9] leveraging MLLMas-a-Judge [39] to evaluate overall Image Quality (IQ), Instruction Following (IF), and Source fidelity (SF) in holistic manner. We leverage GPT-4o-mini as our primary model for its superior alignment with human judgment shown in Table 3. We conduct evaluations on three open-source unified image generation models: OmniGen [56], ACE [19], Show-o [57] 1. For ACE and Show-o, we implement multi-turn dialogues to enable image generation with multiple references, incorporating one reference image per conversational turn. Additionally, we evaluate six compositional settings that specifically leverage Gemini-2.0-Flash [16] and Claude-3.7-Sonnet [2] as preceptors,2 SD3 serves as the primary generator for dataset synthesis, with SD2.1 employed in ablation studies. See Supplementary Material for detailed configurations."
        },
        {
            "title": "3.2 Empirical Results and Analysis\nCompositional framework exceeds in image quality, while\nfailing to maintain consistency on real-world cases. As shown\nin Figure 4, LLM+SD combinations achieve the highest image qual-\nity scores, with Claude + SD3.5 reaching 0.774, occasionally sur-\npassing ground truth. However, all compositional frameworks con-\nsistently underperform in instruction following and source fidelity.\nWhile ground truth achieves 0.767 and 0.706 for IF and SF respec-\ntively, Claude + SD3.5 only reaches 0.589 and 0.462, indicating\nthat a separated perceptor-generator architecture fundamentally\ncompromises complex visual instruction execution.\nUnified models struggled with generation quality and han-\ndling real-world images. Although unified models theoretically\nend-to-end advantage that contributes to maintaining consistency,\nthey underperform in fidelity preservation. OmniGen’s perfor-\nmance in various metrics even approaches some compositional\nframeworks that generate images with state-of-the-art diffusion\nmodels, demonstrating its effectiveness in balancing quality with\ninstruction adherence. However, all models still fall short when\ncompared with the golden answer (created with professional soft-\nware), highlighting significant room for improvement in real-world\nimage generation scenarios.\nControllable image generation from multiple references is\nchallenging. Even advanced models like ACE, despite strong per-\nformance in specific areas (Bbox: 0.219, Pose: 0.090), show substan-\ntial gaps in reference fidelity compared to Ground Truth. While\nunified end-to-end architectures offer greater potential than com-\npositional frameworks, both struggle with complex reference com-\nbinations or image generation without captions, highlighting the\nneed for improved generalization in multi-image generation.\nModels show strong and varied preferences for reference\nformats. Figure 6 presents results from an ablation study investi-\ngating how different input formats for Bounding Box (BBox), Depth,\nand Mask conditions affect the generation performance of three\nmodels. There is no universally superior format that works best\nacross all tested models. Instead, each model often exhibits a dis-\ntinct preference. ACE and ChatDiT show more robust performance\non the depth and mask format. For Depth MSE, ACE performs sig-\nnificantly better with “ori depth,\" whereas OmniGen and ChatDiT\nshow slightly better or comparable performance with “color depth\".",
            "content": "1Due to computation limitation, we do not employ Emu2-Gen [49]. 2Given that GPT-4o participated in most of our experiments, we select alternative models for these compositional settings to avoid bias. MM 25, October 2731, 2025, Dublin, Ireland Chen et al. Table 4: Comparison of model performance on the synthetic part. Although models perform well in overall assessment, they fail at generating images with multiple references. Model Overall Assessment IF IQ SF Image Quality FID Aesthetic AVG BBox Semantic Map Mask Depth Map Canny Edge Sketch Caption Pose* Subject Art Style Reference Fidelity Show-o OmniGen ACE 0.764 0.616 0.730 0.532 0.740 0. 0.462 0.438 0.528 0.110 0.111 0.108 0.811 0.713 0.812 0.726 0.876 0.817 0.574 ChatDiT Claude + SD 2.1 0.572 Claude + SD 3 0.658 Claude + SD 3.5 0.913 0.853 0.691 0.547 Gemini + SD 2.1 0.791 0.708 Gemini + SD 3 0.639 0.856 0.804 0.676 Gemini + SD 3.5 0.893 0.839 0.100 0.114 0.102 0.111 0.113 0.103 0.111 0.607 0.593 0. 0.559 0.612 0.635 0.647 0.615 0.635 0.646 Ground Truth 0.842 0.803 0.668 0.108 0. Unified Model 0.469 0.464 0.553 0.051 0.179 0.219 0.263 0.197 0.382 0.332 0.320 0.439 Compositional Framework 0.128 0.512 0.488 0.174 0.500 0.134 0.513 0.124 0.161 0.477 0.141 0.507 0.510 0.132 0.709 0.410 0.176 0.132 0.145 0.147 0.133 0.135 0.130 0.772 0.393 0.292 0.360 0.358 0.255 0.368 0.371 0.893 0.104 0.087 0.044 0.088 0.203 0.203 0.082 0.202 0.083 0.077 0. 0.061 0.092 0.079 0.065 0.080 0.087 0.082 0.092 0.121 0.096 0.000 0.203 0.221 0.112 0.207 0.230 0.215 0.213 0.239 0.216 0.216 0.000 0.569 0.382 0.521 0.008 0.014 0. 0.532 0.623 0.720 0.543 0.547 0.576 0.573 0.550 0.581 0.579 0.584 0.018 0.005 0.009 0.009 0.003 0.008 0.008 0.149 0.855 0.817 0.859 0.858 0.791 0.840 0.845 0.869 0.301 0.329 0.397 0.369 0.424 0.420 0.434 0.406 0.414 0. 0.417 Table 5: Ablation study on image order and caption removal using the subset of MultiRef-bench. Switch order: switch the image input order. w/o caption: delete the caption in input instructions. Model Setting Image Quality FID Aesthetic BBox Semantic Map Mask Depth Canny Sketch Caption Pose Subject Art Style Reference Fidelity OmniGen ACE ChatDiT Original 0.114 Switch order 0.114 w/o caption 0.120 Original 0.114 Switch order 0.112 w/o caption 0.114 Original 0.107 Switch order 0.105 w/o caption 0. 0.588 0.579 0.534 0.597 0.598 0.567 0.560 0.574 0.550 0.267 0.382 0.180 0.326 0.303 0.231 0.147 0.125 0. 0.272 0.315 0.308 0.296 0.243 0.470 0.160 0.150 0.132 0.273 0.290 0.272 0.311 0.386 0.481 0.261 0.284 0. 0.062 0.068 0.081 0.037 0.077 0.019 0.098 0.092 0.113 0.098 0.105 0.137 0.089 0.105 0.111 0.065 0.063 0. 0.216 0.219 0.191 0.120 0.222 0.069 0.227 0.220 0.196 0.014 0.005 0.089 0.036 0.022 0.022 0.193 0.195 0.202 0.191 0.191 0.197 0.194 0.194 0.202 0.735 0.737 0.669 0.715 0.802 0.657 0.818 0.830 0. 0.565 0.556 0.581 0.552 0.600 0.567 0.541 0.556 0.553 Figure 5: Images generated under multiple references. More are provided in the Supplementary Materials. GT: Ground Truth. substantial and often model-specific impacts on adherence to particular conditions. For example, ACEs depth error and sketch error both increased dramatically when the order was switched. These observations suggest that the sequence of processing conditions is more critical for controlling specific visual attributes than for overall image realism as measured by FID. The presence of captions also improves depth fidelity and aesthetic quality across all models. For instance, ACEs depth error significantly increases when captions are removed. However, for semantic map fidelity, OmniGen and ACE perform better without captions. Similarly, sketch fidelity improves for all three models when captions are absent, with ACE showing notable reduction in sketch error. Figure 6: Ablation study on the impact of different reference formats. Ori/white bbox: bounding boxes drawn in black- /white backgrounds. Ori/color depth: depth maps in grey/- turbo style. Ori/color mask: masks in white/light colors. Input order primarily influences specific conditional fidelities rather than global image quality. As shown in Table 5, switching the input order resulted in only minor FID improvements or no change for the models. However, this operation had more MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland"
        },
        {
            "title": "4 Conclusion",
            "content": "Our work presents the first investigation of image generation conditioned on multiple visual references. Through developing sophisticated synthetic data engine, we have constructed MultiRef, large-scale dataset for multi-reference image generation, from which we carefully curated high-quality benchmark suite alongside real-world application to MultiRef-bench. Our evaluation reveals that existing models face challenges when handling multireference generation tasks. These findings provide insights for developing models that can better handle multi-reference creative processes. MM 25, October 2731, 2025, Dublin, Ireland Chen et al. References [1] Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim, Daesik Kim, Seung-Hun Nam, and Kibeom Hong. 2024. Dreamstyler: Paint by style inversion with textto-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 674681. [2] Anthropic. 2024. Claude 3.5: Sonnet. https://www.anthropic.com/news/claude3-5-sonnet. Accessed: 2024-09-04. [3] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 843852. [4] Caroline Chan, Frédo Durand, and Phillip Isola. 2022. Learning to generate line drawings that convey geometry and semantics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 79157925. [5] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024. Mllm-as-ajudge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning. [6] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William Cohen. 2024. Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural Information Processing Systems 36 (2024). [7] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. 2024. Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 65936602. [8] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. 2024. UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics. arXiv preprint arXiv:2412.07774 (2024). [9] Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. 2024. MJBench: Is Your Multimodal Reward Model Really Good Judge for Text-to-Image Generation? arXiv preprint arXiv:2407.04842 (2024). [10] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. 2021. VITONHD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). [11] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. 2023. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems 36 (2023), 1622216239. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Fortyfirst International Conference on Machine Learning. [13] Flux. 2024. Black Forest Labs. https://blackforestlabs.ai/. [14] Ziqi Gao, Weikai Huang, Jieyu Zhang, Aniruddha Kembhavi, and Ranjay Krishna. 2024. Generate Any Scene: Evaluating and Improving Text-to-Vision Generation with Scene Graph Programming. arXiv preprint arXiv:2412.08221 (2024). [15] Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094 (2024). [16] GeminiTeam. 2023. Gemini: Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL] [17] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. 2023. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems 36 (2023), 5213252152. [18] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. 2025. Can We Generate Images with CoT? Lets Verify and Reinforce Image Generation Step by Step. arXiv preprint arXiv:2501.13926 (2025). [19] Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. 2024. ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer. arXiv preprint arXiv:2410.00086 (2024). [20] Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, and Jingfeng Zhang. 2024. StyleBooth: Image Style Editing with Multimodal Instruction. arXiv preprint arXiv:2404.12154 (2024). [21] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718 (2021). [22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems 30 (2017). [23] Hexiang Hu, Kelvin CK Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, et al. 2024. InstructImagen: Image generation with multi-modal instruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 47544763. [24] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. 2023. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2040620417. [25] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023. T2icompbench: comprehensive benchmark for open-world compositional text-toimage generation. Advances in Neural Information Processing Systems 36 (2023), 7872378747. [26] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Chen Liang, Tong Shen, Han Zhang, Huanzhang Dou, Yu Liu, and Jingren Zhou. 2024. ChatDiT: Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers. arXiv preprint arXiv:2412.12571 (2024). [27] Chengyou Jia, Changliang Xia, Zhuohang Dang, Weijia Wu, Hangwei Qian, and Minnan Luo. 2024. ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting. arXiv preprint arXiv:2411.17176 (2024). [28] Xuan Ju, Ailing Zeng, Jianan Wang, Qiang Xu, and Lei Zhang. 2023. Humanart: versatile human-centric dataset bridging natural and artificial scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 618629. [29] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19311941. [30] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. 2025. ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback. In European Conference on Computer Vision. Springer, 129147. [31] Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. 2023. Dreamedit: Subject-driven image editing. arXiv preprint arXiv:2306.12624 (2023). [32] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2251122521. [33] Chen Liang, Lianghua Huang, Jingwu Fang, Huanzhang Dou, Wei Wang, ZhiFan Wu, Yupeng Shi, Junge Zhang, Xin Zhao, and Yu Liu. 2024. IDEA-Bench: How Far are Generative Models from Professional Designing? arXiv preprint arXiv:2412.11767 (2024). [34] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. 2024. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision. Springer, 366384. [35] Xiaoyu Liu, Yuxiang Wei, Ming Liu, Xianhui Lin, Peiran Ren, Xuansong Xie, and Wangmeng Zuo. 2024. Smartcontrol: Enhancing controlnet for handling rough visual conditions. In European Conference on Computer Vision. Springer, 117. [36] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. 2024. Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 74657475. [37] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. 2024. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 42964304. [38] Nithin Gopalakrishnan Nair, Jeya Maria Jose Valanarasu, and Vishal Patel. 2024. Maxfusion: Plug&play multi-modal generation in text-to-image diffusion models. In European Conference on Computer Vision. Springer, 93110. [39] OpenAI. 2024. GPT-4O Mini: Advancing Cost-Efficient Intelligence. https:// openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/. Accessed: 2024-09-04. [40] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ Accessed: 2024-06-06. [41] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. 2023. Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023). [42] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. 2024. UniControl: Unified Diffusion Model for Controllable Visual Generation In the Wild. Advances in Neural Information Processing Systems 36 (2024). [43] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. 2024. SAM 2: Segment Anything in Images and Videos. arXiv preprint arXiv:2408.00714 (2024). https://arxiv.org/abs/2408.00714 [44] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159 (2024). [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2250022510. [47] Babak Saleh and Ahmed Elgammal. 2015. Large-scale classification of fineart paintings: Learning the right metric on the right feature. arXiv preprint arXiv:1505.00855 (2015). [48] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems 35 (2022), 2527825294. [49] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. 2024. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 88718879. [50] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2024. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1439814409. [51] Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, and Ranjay Krishna. 2025. REALEDIT: Reddit Edits As Large-scale Empirical Dataset for Image Transformations. arXiv:2502.03629 [cs.CV] https://arxiv.org/abs/2502.03629 [52] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. 2024. OminiControl: Minimal and Universal Control for Diffusion Transformer. arXiv preprint arXiv:2411.15098 (2024). [53] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. 2024. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems 37 (2024), 128374128395. [54] Zhizhong Wang, Lei Zhao, and Wei Xing. 2023. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 76777689. [55] Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. 2024. OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision. arXiv:2411.07199 [cs.CV] https://arxiv.org/abs/2411.07199 [56] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. 2024. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340 (2024). [57] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. 2024. Show-o: One Single Transformer to Unify Multimodal Understanding and Generation. arXiv preprint arXiv:2408.12528 (2024). [58] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. 2023. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 7754 7765. [59] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. 2024. Depth Anything V2. arXiv:2406.09414 (2024). [60] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. 2023. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1424614255. [61] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. 2024. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems 36 (2024). [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. [63] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. 2024. Uni-controlnet: All-in-one control to text-toimage diffusion models. Advances in Neural Information Processing Systems 36 (2024). [64] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. 2023. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2249022499."
        },
        {
            "title": "5 Acknowledgments",
            "content": "We would like to thank Wanting Liang, Jieyu Zhang, Weikai Huang, and Zixian Ma for their insightful feedback and support. MM 25, October 2731, 2025, Dublin, Ireland"
        },
        {
            "title": "A Related Work",
            "content": "Controllable Image Generation. The emergence of controllable image generation has revolutionized artificial intelligence by enabling users to create images that precisely match their specified criteria, including composition [32, 60, 64], style [1, 54], and content elements [6, 7]. ControlNet [62] advanced this field by introducing spatially localized input conditions to pre-trained text-to-image diffusion models through efficient fine-tuning methods. Subsequent research [11, 30, 36, 37] has further enhanced image controllability by implementing additional customization layers and adaptive mechanisms, enabling more sophisticated and precise image generation processes. Building upon these advancements, some work has studied universal guidance for image generation with diffusion models [3, 35, 38, 41, 42, 58, 63]. While early approaches often required complex, condition-specific adapters, new generation of unified models has expanded possibilities by incorporating diverse input modalities to facilitate multi-modal controllable generation. These recent unified architectures support multiple visual features as conditions. Emu2-Gen [50] uses an autoregressive model to predict the next tokens and uses separated diffusion model to generate images. Instruct-Imagen [23] unifies image generation tasks together using multi-modal instructions. ACE [19] introduces the condition unit designed specifically for multi-modal tasks. OmniGen [56] uses an LLM as initialization and jointly models text and images within single model to achieve unified representations across different modalities. UniReal [8] treats image-level tasks as discontinuous video generation, enabling wide range of image generation and editing capabilities. In parallel developments, ChatDit [26] employs multi-agent system for general-purpose, and interactive visual generation. Dataset for Controllable Generation. Recent controllable image generation models have succeeded largely due to extensive training datasets like MultiGen-20M [42], which spans nine tasks across five categories with condition-specific instructions, while X2I dataset [56] incorporates flexible multi-modal instructions - yet these approaches still predominantly address single or dual conditions rather than complex, multi-reference combinations. Previous work has established benchmarks for evaluating image generation, primarily focused on text-to-image quality and alignment [14, 17, 24, 25, 34] or image editing tasks [31, 49, 61]. Existing benchmarks like IDEA-Bench [33] and ACE benchmark [19] are limited in scope, with the former including images-to-image tasks but focusing primarily on editing operations like font transfer, while the latter only evaluates alignment with textual instructionsboth failing to address complex scenarios involving multiple image references and their combinations."
        },
        {
            "title": "B Details of Collecting MultiRef",
            "content": "We provide further details on the collection of MultiRef. Our dataset contains 38,076 samples in total, where 990 samples are split into test set for evaluation. Using RefBlend, we generate more than 100k raw data, and finally gain 38k dataset of highquality after filtering. See Table 6 for detailed statistics and Figure 7 for reference distribution in MultiRef. Table 6: Distribution of Combinations by Count and Percentage in MultiRef. Chen et al."
        },
        {
            "title": "Count",
            "content": "caption+depth+subject caption+mask+subject caption+depth bbox+caption+subject canny+caption+subject caption+sketch+subject canny+caption caption+sketch caption+semantic map+subject caption+semantic map bbox+caption caption+mask caption+pose+subject depth+subject caption+depth+style canny+caption+style caption+sketch+style bbox+subject bbox+caption+style mask+subject caption+mask+style sketch+subject canny+subject caption+semantic map+style semantic map+subject caption+subject pose+subject caption+pose canny+style depth+style sketch+style semantic map+style Total 2,580 2,487 2,470 2,461 2,455 2,422 2,413 2,404 2,125 2,009 1,961 1,946 1,918 1,003 700 652 644 578 544 542 535 508 490 480 463 257 248 215 131 131 124 93 (%) 6.78 6.53 6.49 6.46 6.45 6.36 6.34 6.31 5.58 5.28 5.15 5.11 5.04 2.63 1.84 1.71 1.69 1.52 1.43 1.42 1.41 1.33 1.29 1.26 1.22 0.67 0.65 0.56 0.34 0.34 0.33 0.24 38,076 100.00% Figure 7: Reference Distribution in MultiRef. MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland B.1 Reference Generation We choose the guidance of image generation from prior work [23, 42, 62, 63], combining commonly used references, standardizing their names and adapting them to work with flexible input and output formats. Our final set of references includes edge maps (Canny), semantic maps, sketches, depth maps, bounding boxes, masks, poses, art styles and subjects, along with textual captions. Bounding box. bounding box is small possible rectangular box that can completely enclose an object in an image, typically defined by the (x,y) coordinates of its top-left and bottom-right corners. We utilize phrase grounding in Grounded SAM2 [44] to identify and localize the main objects in given image. The bounding box is visualized by drawing it on black background of the same dimensions as the input image. Mask. mask is binary image representation where the object of interest is separated from the background. It precisely outlines the shape and contour of the target object, creating silhouette that exactly matches the objects boundaries rather than using rectangular bounding box. We use Grounded SAM2 to generate masks, with one object corresponding to one mask. The mask is typically visualized as binary image, where the background is represented by black pixels (value 0), and the object mask is represented by white pixels (value 1). Pose. pose refers to the spatial arrangement of key body parts (such as head, shoulders, elbows, wrists, hips, knees, and ankles) in human figure, typically represented as skeleton structure with joints and connections. The pose reference is visualized on black background, with colored joints and connections highlighting the bodys key positions and movements. Semantic map. semantic map, is visual representation where each object class or semantic category is assigned unique color or label, showing the location and boundaries of different semantic concepts in an image. We use AutomaticMaskGenerator in SAM2 [43] to generate the semantic map. Depth map. depth map is grayscale image where each pixels intensity represents the distance between the camera and the corresponding point in the scene. Typically, lighter/brighter pixels indicate points closer to the camera while darker pixels represent points that are farther away, creating visual representation of the scenes 3D structure in 2D format. We use Depth Anything V2 [59] to generate the depth map with default parameters. Canny edge. Canny edge map is binary image that shows the boundaries and edges detected in an original image using the Canny edge detection algorithm. It identifies edges by looking for areas of rapid intensity change in the image, producing clean, thin outline where white pixels represent detected edges against black background. We use the Canny operation in OpenCV with thresholds in [100, 200]. Sketch. sketch of an image is simplified, line-based representation that captures the original images essential contours and structural elements using only black lines on white background. It focuses on preserving the key visual information while removing details like color, texture, and shading, similar to hand-drawn outline. We use the line drawing method by Chan et al. [4] to generate the sketch reference, with contour_style and resize_and_crop process. Art style. An art style of an image refers to the distinctive visual aesthetic, technique, or artistic treatment applied to transform the original image into specific artistic rendering - such as watercolor, oil painting, cartoon and impressionist. Subject. subject reference image provides the main content or subject matter that needs to be transformed or recreated. It serves as the primary visual input that specifies what object or subject should be generated in the new image while maintaining its key characteristics and identity. Caption. caption of an image is concise textual description that explains what is shown in the image, often describing the key subjects, actions, or notable elements present in the visual content. We use GPT-4o-mini [39] to describe the input image with prompts as follows. Generate image caption System prompt: You are helpful assistant that can analyze images and provide detailed descriptions. Here is the image: [INSERT_IMAGES] For subject-related images: Describe this image in detail using no more than 20 words. Focus on the main subject in the image. Do not include any other unrelated information. For other images: Describe this image in detail using no more than 20 words. Do not include any other unrelated information. B.2 Details of Metadata Original images used for reference generation are from seven datasets, as follows. DreamBooth [46]. It is collection of images used for finetuning text-to-image diffusion models for subject-driven generation. It includes 30 subjects from 15 different classes. Images of the subjects are usually captured in different conditions, environments, and under different angles. While DreamBooth offers subject references, it does not include art style or pose references. Subjects200K [52]. It is large-scale dataset containing 200,000 paired images. Each image pair maintains subject consistency while presenting variations in the scene context. The dataset does not include art style or pose references. We leverage subject references provided by the dataset itself. CustomConcept101 [29]. It is dataset consisting of 101 concepts with 3-15 images in each concept. The categories include toys, plushies, wearables, scenes, transport vehicles, furniture, home decor items, luggage, human faces, musical instruments, rare flowers, food items, pet animals. While it offers subject references, it does not include art style or pose references. Human-Art [28]. It is versatile human-centric dataset to bridge the gap between natural and artificial scenes. It includes twenty high-quality human scenes, including natural and artificial humans in both 2D representation and 3D representations. It includes 50,000 images in 20 scenarios, with annotations of human bounding box and human keypoints. From this dataset, we utilize MM 25, October 2731, 2025, Dublin, Ireland Chen et al. two subsets: 2D_virtual_human and real_human, containing 22,000 and 10,000 images, respectively. Specifically, 2D_virtual_human provides art style and pose references while real_human provides pose references. Additionally, we leverage the art style and pose annotations provided within the dataset. WikiArt [47]. WikiArt contains art paintings from 195 different artists. The dataset has 42,129 images for training and 10,628 images for testing. It does not include the subject reference or pose reference. We use images that share the same style as the art style references. StyleBooth [20]. It is high-quality style editing dataset accepting 67 prompt formats and 217 diverse content prompts, ending up with 67 different styles and 217 images per style. We use images that share the same style as the art style references. VITON-HD [10]. It is high-resolution virtual try-on dataset consisting of 11,647 person images and 11,647 corresponding clothing images. Each sample contains person image, target clothing image, and pose representation. We use images as subject and pose references. B.3 Prompt Template For each reference, we generate 10 structured basic instructions, as shown below. Basic instructions for Art Style Inspired by the essence of style_image, this reflects its distinctive flair Crafted in the characteristic tone of style_image Modeled with the unique influence of style_image Echoing the artistic spirit of style_image Infused with the signature style of style_image Reflecting the aesthetic nuances of style_image reinterpretation influenced by style_image Harmonizing with the thematic essence of style_image Inspired by and shaped in the vein of style_image Capturing the creative vision embodied by style_image Basic instructions for Sketch Following the sketch of sketch_image, this mirrors its essence. Designed in alignment with the sketch of sketch_image. Echoing the framework drawn by sketch_image. Guided by the outline of sketch_image, it retains its authenticity. Reflecting the initial strokes of sketch_image. Infused with the skeletal form of sketch_image. Shaped under the influence of sketch_images sketch. Structured around the design of sketch_image. Capturing the structural integrity of sketch_image. Crafted to reflect the framework of sketch_image. Basic instructions for Depth Following the depth of depth_image, this delves into its essence. Inspired by the dimensionality of depth_image, it captures its core. Reflecting the profound layers of depth_image. Echoing the spatial depth of depth_image, it retains its integrity. Infused with the visual perspective of depth_image. Guided by the textured depth of depth_image. Structured to align with the depths captured by depth_image. Modeled after the layered depth of depth_image. Harmonizing with the multi-dimensional feel of depth_image. Crafted to embrace the depth portrayed by depth_image. Basic instructions for Canny Following the edge of canny_image, this captures its sharpness. Inspired by the contours of canny_image, it traces its form. Reflecting the defined edges of canny_image. Echoing the precision lines of canny_image, it retains its clarity. Infused with the sharp boundaries of canny_image. Guided by the linear features of canny_image. Structured to follow the contours highlighted by canny_image. Modeled after the crisp edges of canny_image. Harmonizing with the boundary lines of canny_image. Crafted to reflect the edge details of canny_image. Basic instructions for Semantic Map Following the semantic map in semantic_image, this aligns with its meaning. Inspired by the structure of semantic_image, it conveys its intent. Reflecting the mapped semantics of semantic_image. Echoing the visual language of semantic_image, it captures its essence. Infused with semantic_image. the meaningful contours of Guided by the symbolic layout of semantic_image. Structured the semantics depicted around in semantic_image. Modeled to align with the conceptual map of semantic_image. Harmonizing with semantic_image. the thematic essence Crafted to reflect the semantic details semantic_image. of of MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland Basic instructions for Bounding Box Basic instructions for Subject Following the bounding box in bbox_image, this outlines its structure. Inspired by the box constraints of bbox_image, it defines its scope. Reflecting the encapsulated regions of bbox_image. Echoing the boundary lines of bbox_image, it retains its precision. Infused with the spatial framework of bbox_image. Guided by the rectangular limits of bbox_image. Structured to follow the defined areas in bbox_image. Modeled of parameters bounding after the featuring subject_1. showcasing subject_1. focusing on subject_1. while emphasizing subject_1 with focus on subject_1. centered on subject_1. highlighting subject_1. to better display subject_1. while emphasizing subject_1. to reveal finer details of subject_1. bbox_image. Harmonizing with the enclosed regions of bbox_image. Crafted to reflect bbox_image. the boundary specifications of We use the prompt Diversity enhancement to write enhanced instructions, shown as below. Diversity enhancement Basic instructions for Single Mask Following the mask in mask_image, this captures its shape. Inspired by the masked outline of mask_image, it defines its form. Reflecting the contours covered by mask_image. Echoing the masked regions of mask_image, it retains its detail. Infused with the coverage specified by mask_image. Guided by the spatial coverage of mask_image. Structured to align with the masked features in mask_image. Modeled after the outlined mask of mask_image. Harmonizing with the masked boundaries of mask_image. Crafted to reflect the regions defined by the mask in mask_image. Basic instructions for Pose Following the pose in pose_1, this mirrors its stance. Inspired by the posture captured in pose_1, it reflects its form. Reflecting the alignment depicted in pose_1. Echoing the position shown in pose_1, it retains its essence. Infused with the dynamic structure of pose_1. Guided by the articulated motion of pose_1. Structured around the pose outlined in pose_1. Modeled to replicate the position in pose_1. Harmonizing with the posture embodied in pose_1. Crafted to reflect the expressive pose of pose_1. You will adopt the persona of selected_persona. You will be given text and your task is to rewrite and polish it in more diverse and creative manner that reflects the personas style. Do not include any direct references to the persona itself. You may alter sentence structure, wording, and tone. Do not modify text enclosed in angle brackets . If there is caption: section in the text, do not change anything following caption: Here is the text: basic_instruction Please provide the revised text directly without any additional commentary. Table 7 illustrates side-by-side comparison of the basic instructions and their enhanced versions used in our prompt design pipeline. Each basic instruction provides concise description of the target visual generation goal, typically referencing specific condition input (e.g., <bbox_image> or <mask_image>). The enhanced prompts further enrich this guidance by incorporating more descriptive verbs, clarifying intentions, and explicitly reinforcing alignment with auxiliary conditions or captions. Overall, the enhanced prompts are not only longer but also more expressive and instructive. B.4 Data Filtering To evaluate the complex outputs of free-form image generation, we assess both image quality and reference alignment using scoringbased MLLM as Filter [5], which has gained widespread adoption in the field [33]. For each reference, the multimodal large language model examines both the original and generated images, evaluating alignment between them and assessing the quality of the generated reference (if applicable). The evaluation produces numerical scores on 5point scale (1-5), following specific scoring rubrics detailed below. References with score under 3 will be filtered in the checking process. Pose, subject, and art style references are manually verified, as they are provided by the dataset and contain minimal annotation errors. MM 25, October 2731, 2025, Dublin, Ireland Chen et al. # Basic Instruction Enhanced Prompt Table 7: Basic Instructions and their enhanced prompts. 1 Generate an image featuring <subject_1>. Reflecting the encapsulated regions of <bbox_image>. Create an image showcasing <subject_1>, capturing the essence of the defined areas within <bbox_image>. 2 Generate an image following the edge of <canny_image>, capturing its sharpness. Harmonizing with the thematic essence of <style_image> following the caption: The image features woman with neatly styled hair bun, dressed in simple, elegant garment. Create an image that traces the contours of <canny_image>, highlighting its precision. It should resonate with the thematic core of <style_image> while adhering to the caption: The image features woman with neatly styled hair bun, dressed in simple, elegant garment. 3 Generate an image crafted to reflect the regions defined by the mask in <mask_image>. Following the caption: woman in pink dress stands confidently at vibrant outdoor market, surrounded by colorful produce and stalls. Generate an image designed to capture the essence of the areas delineated by the mask in <mask_image>, following the caption: woman in pink dress stands confidently at vibrant outdoor market, surrounded by colorful produce and stalls. Figure 8: Distribution of human preference scores (1-5) for image-text alignment and image quality across various conditioning types in the MLLM-as-a-Judge fine-tuning dataset. The top row shows alignment scores for conditions like BBox, Canny, Caption, Depth, Pose, and Semantic Map. The bottom row shows quality scores for conditions such as Single Mask, Sketch, Canny, Semantic Map, and Depth. Percentages indicate the proportion of samples receiving each score within that specific conditioning type. For more semantic-level visual references, such as subject, style, sketch, canny, and edge that do not provide confidence scores, we establish fine-tuned MLLM-as-a-Judge [5] that verifies both the alignment between original images and generated references and their quality. Specifically, we collect subset with 6,400 original images and their corresponding references, constructing them into (image, reference) pairs and subsequently collect cross-validated human scoring from 1-5 for alignment and quality score individually. Finally, we split it into train/test sets, each with 16,590 and 1,750 samples, and fine-tune Qwen-2.5-7B-VL. Figure 8 illustrates the distribution of these scores across different conditioning types and assessment criteria (alignment and quality). Table 8 presents comprehensive performance comparison of various models, evaluating their ability to align with human judgments across diverse condition types. The metrics used are Pearson correlation (higher indicates better alignment) and Mean Absolute Error (MAE, lower is better). This analysis focuses on two key aspects: the effectiveness of fine-tuning MLLM-as-a-Judge (by comparing fine-tuned \"-FT\" versions of Qwen models to their zero-shot base versions) and benchmarking against VQA scores [34] from GPT-4o-mini. In summary, the results in Table 8 robustly validate the finetuning strategy for the MLLM-as-a-Judge, with Qwen2-7B-VL-FT achieving the best overall alignment with human preferences. While this fine-tuned judge performs well across many conditions, particularly in comparison to general VQA model like GPT-4o-mini, challenges persist in accurately judging highly nuanced spatial conditions such as pose and bounding box accuracy. The fine-tuned model with human-annotated scores as ground truth across Pearson similarity and MAE in Table 8 reveals close alignment with human annotators, validating it as good judge for filtering. MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland Table 8: Performance comparison across different models and condition types. ZS: zero-shot, FT: finetuned. Model Overall Single Mask Pearson MAE Pearson MAE Pearson MAE Pearson MAE Pearson MAE Pearson MAE Pearson MAE Pearson MAE Pearson MAE Pearson MAE Pearson MAE Semantic Map Caption Subject Sketch Canny Depth BBox Style Pose Qwen2-2B-VL-ZS Qwen2-2B-VL-FT Qwen2-7B-VL-ZS Qwen2-7B-VL-FT (Ours) GPT-4o-mini 0.122 0.602 0.364 0.642 0.424 1.550 0.998 1.397 0.914 1. 0.028 0.102 0.104 0.257 0.272 1.596 1.513 1.603 1.397 1.789 0.231 0.787 0.694 0.838 0. 1.475 0.633 0.892 0.557 0.779 0.044 0.695 0.148 0.635 0.367 1.631 0.818 1.757 0.894 1. 0.864 0.874 0.869 0.865 0.782 0.491 0.462 0.515 0.485 0.580 0.051 0.308 0.495 0.605 0. 1.987 1.068 1.757 0.797 1.662 0.173 0.473 0.293 0.629 0.457 2.671 0.975 1.171 0.728 0. 0.239 0.617 0.270 0.634 0.490 1.446 1.025 1.515 0.946 1.290 0.209 0.497 0.618 0.605 0. 1.338 1.461 1.117 1.162 1.188 -0.006 0.093 0.118 0.404 0.046 2.200 1.177 2.588 1.129 2. 0.196 0.270 0.378 0.402 0.197 1.253 1.228 1.309 1.124 1.167 Eval rubrics for canny Definitions: Canny Edge Map is visual representation that highlights the edges and contours of objects in an image, where white lines represent detected edges and black represents non-edge regions. Alignment: Not Aligned (Score 1) - Major object contours are unrecognizable or wrongly placed compared to the target image. Minimally Aligned (Score 2) - Few contours match the target image, with significant placement issues. Partially Aligned (Score 3) - Some major contours match while others are missing or misplaced. Mostly Aligned (Score 4) - Most main contours are recognizable and properly placed with minor misalignments. Well Aligned (Score 5) - Main object contours are recognizable and properly placed throughout the image. Quality: Poor Quality (Score 1) - Excessive noise or breaks prevent object recognition entirely. Below Average Quality (Score 2) - Significant noise or breaks make most objects difficult to recognize. Average Quality (Score 3) - Key objects are recognizable despite moderate noise or breaks in contours. Good Quality (Score 4) - Main edges form clear object contours with minimal noise or breaks. High Quality (Score 5) - Main edges form recognizable object contours with the appropriate level of detail. Eval rubrics for caption Definitions: Caption is textual description that describes the content, context, objects, actions, or scene depicted in an image. Alignment: Not Aligned (Score 1) - The caption describes elements that arent present in the image, or fails to describe the main elements that are clearly visible. Minimally Aligned (Score 2) - The caption has minimal connection to the image content, with only one or two elements correctly identified. Partially Aligned (Score 3) -Some parts of the caption correctly describe the image while other described elements are missing or different, or the caption captures the general scene but misses key elements. Mostly Aligned (Score 4) - The caption describes most main elements and the overall scene with minor inaccuracies or omissions. Well Aligned (Score 5) - The caption accurately describes the main elements and scene in the image. Eval rubrics for sketch Definitions: sketch is simplified, hand-drawn representation of an image, typically in black and white or grayscale, focusing on the main outlines and shapes of objects. Alignment: Not Aligned (Score 1) - The basic object or scene structure is not captured at all. Minimally Aligned (Score 2) - Vague resemblance to the original image with major structural inaccuracies. Partially Aligned (Score 3) -The main concept is recognizable but with significant structural deviations. Mostly Aligned (Score 4) - Basic shapes and composition generally match with minor proportional variations. Well Aligned (Score 5) - The basic shapes and composition match accurately to the original image. Quality: Poor Quality (Score 1) - Excessive noise or unclear lines make it difficult to interpret the intended subject. Below Average Quality (Score 2) - Substantial noise or rough elements that significantly detract from the subject. Average Quality (Score 3) -The sketch shows the subject but includes noticeable noise, scattered marks, or rough elements while maintaining recognizable forms. Good Quality (Score 4) - Clear lines with minimal noise that effectively represent the subject. High Quality (Score 5) - Clean, clear lines that effectively convey the subject with minimal noise or distraction. MM 25, October 2731, 2025, Dublin, Ireland Chen et al. Eval rubrics for semantic map Definitions: semantic map is visual representation where an image is divided into distinct regions to represent different objects, areas, or elements of the scene, using any colors or styles to distinguish between regions. Alignment: Not Aligned (Score 1) - The basic scene structure or main objects are unrecognizable. Minimally Aligned (Score 2) - Only few elements are recognizable, with significant missing or misplaced components. Partially Aligned (Score 3) - Some key elements are recognizable but others are missing or unclear. Mostly Aligned (Score 4) - Most elements capture recognizable objects and scene layout with minor inaccuracies. Well Aligned (Score 5) -The map captures recognizable objects and scene layout appropriately (simplified shapes are acceptable, textures and fine details not required). Quality: Poor Quality (Score 1) - Semantic regions are too sparse or scattered to identify main objects; regions are too minimal to understand scene content. Below Average Quality (Score 2) - Main elements are barely distinguishable with significant noise, artifacts, or fragmented segments that impair understanding. Average Quality (Score 3) - Main elements are clearly visible but with noticeable noise/artifacts or scattered segments, while still maintaining recognizable object shapes. Good Quality (Score 4) - Key objects/regions are welldefined with limited noise or artifacts; segmentation is generally clean with only minor issues. High Quality (Score 5) - Main objects/regions are clearly visible and distinguishable, with clean segmentation of major elements; minimal artifacts or noise around edges. Eval rubrics for mask Definitions: Mask Image is binary image where white regions indicate areas of interest or target regions for object placement/- generation, while black regions represent background or non-target areas. Alignment: Not Aligned (Score 1) - Main parts of the main object are not covered by the mask, or the mask position doesnt correspond to the object location. Minimally Aligned (Score 2) - The mask covers only small portion of the main object or is significantly misplaced. Partially Aligned (Score 3) - The mask covers most but not all of the main object, or if positioning is noticeably off. Mostly Aligned (Score 4) - The mask covers the main object with minor positioning issues or slight shape inaccuracies. Well Aligned (Score 5) - The mask captures the general outline and position of the main object accurately. B.5 Human Annotation The annotation process was conducted by three independent evaluators: two authors of this paper and one volunteer. Recognizing that annotator diversity is essential for minimizing bias and maximizing dataset reliability, we selected annotators with varying demographic characteristics (gender, age, and educational background) while ensuring all possessed domain expertise in image generation evaluation. To establish annotation consistency and objectivity, all evaluators underwent comprehensive training sessions before beginning the task. These sessions included detailed tutorials on objective image assessment techniques, familiarization with reference rubrics, and instruction on the specific criteria used in our Score Evaluation framework. The annotation platform is shown in Figure 9. Figure 9: Human Annotation Platform Details of MultiRef-bench MultiRef-bench consists of 1,990 examples. The 1,000 examples from the real-world part represent real-world tasks sampled from the Reddit community r/PhotoshopRequest. The synthetic 990 examples are test set split from MultiRef generated using RefBlend. We show some examples in Figure 10. C.1 Real-world We collect 2,300 user queries from r/PhotoshopRequest community on Reddit, explicitly selecting tasks that require combining multiple input images to fulfill the requested edits. For each query, MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland Figure 10: Samples in MultiRef-bench. The first row present real-world examples while others are from the synthetic part. we gather all associated input images, the original text-based user instructions, and corresponding output images. To ensure data integrity and quality, each datapoint undergoes manual evaluation according to rigorous criteria. These criteria include verifying the necessity and appropriateness of each input image, assessing the logical coherence and relevance of instructions, and confirming accurate adherence to the instructions in the output image. In cases where multiple output images are provided for single query, annotators select only one based on clarity, fidelity to the instruction, and overall quality. Taxonomy creation. We adopted the taxonomy structure introduced in OmniEdit [55] to categorize the types of edits represented in our benchmark. We utilized GPT-4o with the following prompt to generate the taxonomy for our dataset. The distribution of edit types in the real-world part is shown in Table 9. To produce the meta-style prompts from noisy user instructions, we used the prompt with GPT-4o. We supplied all input images, the corresponding output image, as well as the original user instructions. Prompt of generating taxonomy for real-world queries You are tasked with classifying image editing instructions into one of the following 5 categories: Table 9: Distribution of examples across different categories in real-world samples. Category Element Replacement Element Addition Spatial/Environment Modifications Attribute Transfer Style and Appearance Modifications Total Num. 529 246 111 73 41 1,000 1. Element Replacement - Face swaps - Object substitutions - Background replacements - Text replacements - Component swaps (wheels, screens, etc.) 2. Element Addition - Adding people to scenes - Adding objects to environments - Adding details or elements to objects - Adding text or graphics - Adding visual effects 3. Style and Appearance Modifications - Color adjustments - Lighting modifications - Artistic style transfers - Texture changes - Visual quality enhancements 4. Spatial/Environment Manipulations - Repositioning elements - Combining multiple images into layouts - Changing scale or proportion - Adjusting orientation or alignment - Creating composite images MM 25, October 2731, 2025, Dublin, Ireland Chen et al. Table 10: Evaluating MLLM-as-a-Judge in scoring with cross-validated human-annotated ground truth. GPT-4o and 4o-mini aligns closely with human scores in overall assessment. Human-Human shows the alignment between human annotators. Model Pearson Image Quality Spearman MSE MAE Instruction Following Pearson Spearman MSE MAE Pearson Source Fidelity Spearman MSE MAE Realistic Gemini-2.0-Flash GPT-4o-mini GPT-4o Human-Human Gemini-2.0-Flash GPT-4o-mini GPT-4o Human-Human 0.385 0.466 0.432 0.589 0.369 0.438 0.406 0.629 0.403 0.466 0.420 0.573 0.347 0.410 0.374 0.648 2.220 1.676 2.486 1. 1.118 0.986 1.223 0.936 0.422 0.530 0.624 0.665 0.447 0.569 0.616 0.590 2.750 1.493 1.405 1.152 1.216 0.858 0.764 0.720 Synthetic 2.078 1.680 2.350 1.823 1.052 1.013 1.083 0.930 0.627 0.632 0.668 0.721 0.592 0.552 0.608 0.735 1.662 1.503 1.537 1.820 0.855 0.870 0.843 0. 0.354 0.514 0.613 0.571 0.588 0.616 0.659 0.694 0.356 0.518 0.513 0.441 0.574 0.615 0.626 0.708 3.747 1.193 1.216 1.473 1.409 0.733 0.736 0. 2.057 2.173 1.573 1.840 0.960 1.140 0.860 0.840 5. Attribute Transfers - Transferring expressions between faces - Applying visual characteristics across images - Maintaining specific features while changing others - Matching visual properties (lighting, color) - Transferring specific details while preserving context Given the following image editing instruction, classify it into exactly one of these 5 categories. Respond with JSON object with single key \"category\" and the value being the category number (1-5). GPT-4o prompt for rewriting instructions You are an expert at image editing. Your job is to write prompt that would help machine learning models to edit images. Im showing you: 1. First, the INPUT IMAGE(S) that the user wants to edit. 2. Then, the users ORIGINAL INSTRUCTION (which might be noisy or unclear). 3. Finally, the OUTPUT IMAGE after editing. Based on comparing these, please: 1. Infer what specific edit was performed 2. Write clear, precise prompt that would help an AI model achieve this exact edit Your prompt should follow this format: \"Edit image <image1> by [specific editing instruction using clear terminology]\" Here are some examples of good output prompts: - \"Edit image <image1> by taking the person from <image2>, person from <image3> and adding them to <image1>.\" - \"Edit image <image2> by transferring the background from <image1> and replacing the person with the person from <image3>\" - \"Edit image <image1> by faceswapping the person from <image2> into <image1>\" Now, analyze the following: ORIGINAL INSTRUCTION: {{description}} Please provide well-structured, clear editing prompt that precisely describes the transformation shown in the images. C.2 Synthetic Following established compatibility rules, we identified 33 different reference combinations. To ensure statistical robustness while maintaining manageable evaluation scope, we selected 30 samples for each combination, resulting in comprehensive set of 990 evaluation instances in the synthetic portion of MultiRef-bench. Some examples are shown in row 2-3 in Figure 10. C.3 Evaluation For overall assessment, we leverage MLLM-as-a-Judge using GPT4o-mini. We validate the correlation of MLLM-as-a-Judge and human with selected test set of 300 samples for either Realistic and Synthetic dataset. Our experiment in Table 10 reveals that GPT-4omini surpasses other models in aligning with humans. Therefore, we use GPT-4o-mini for overall assessment. Details of Experiment D.1 Model Settings In this section, we will introduce the hyper-parameters of image generative models to facilitate experiment reproducibility and transparency. All our experiments were conducted on server equipped with two A800 and two 4090 GPUs. Open-source Unified Models. We employed four open-source unified models. All hyper-parameters are detailed as follows: OmniGen [56]. We set height=1024, width=1024, guidance_scale=2.5, img_guidance_scale=1.6, seed=0 as default settings. MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland ChatDit [26]. We use the images-to-image API call provided in the GitHub. ACE [19]. We use the ACE-0.6B-512px as ACE-chat model for multi-reference image generation in multi-turn. We set sampler=ddim, sample_steps=20, guidance_scale=4.5, guide_rescale=0.5. Show-o [57]. We use multi-turn dialogue for multi-reference image generation. We set guidance_scale=1.75, generation timesteps=18, temperature=0.7, resolution: 256 256. As reported in GitHub, Emu2-Gen [49] needs at least 75GB of memory. Due to the limitation of computation, it is not employed in our experiments. Other Models. We utilize three proprietary models, GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro-latest as multimodal preceptors and Flux-dev, SD3, SD2.1 as image generators, with detailed settings as follows: Gemini-1.5-pro-latest [16]. Temperature=1, top_p= 0.95. Claude-3.5-Sonnet [2]. Temperature=0.9. GPT-4o [40]. Temperature=1, top_p=1. Flux1-dev [13]. guidance scale=3.5, num inference steps=50. Stable Diffusion 3 [12]. guidance scale=7.0, num inference steps=28. Stable Diffusion 2.1 [45]. guidance scale=7.5, num inference steps=25. Additional Experiment Results E.1 Full Results We present full results of model generation in Table 11. E.2 More Visualizations General Results. We show images generated by models under combination of two references and three references in Figure 11 and Figure 12, respectively. GT\" means ground truth image. Figure 11 illustrates model performance when handling dual conditioning inputs. The results reveal significant variations in how different models interpret and integrate these multiple reference signals. Notably, most models perform poorly in maintaining the spatial accuracy of bounding boxes. For instance, ACE and ChatDiT inadequately follow sketch references, while compositional frameworks struggle with adherence to depth references. As an example, the Subject + Depth combination for flower generation, which tests the handling of both object appearance and spatial depth information, shows ACE maintaining depth relationships more effectively than other models. Encouragingly, all models demonstrate good performance in preserving object identity and art styles. Figure 12 extends this evaluation to three-reference scenarios, introducing more complex conditioning challenges. In the Canny + Style combination for Venetian canal scenes, for example, OmniGen and ACE exhibit better preservation of architectural details, whereas other models struggle with structural consistency. Furthermore, ACE demonstrates superior performance in spatial scene composition compared to other models, as evidenced by the Semantic map + Subject + Caption combination. Ablation study on reference formats. To evaluate the effectiveness of different conditioning formats, we conduct comprehensive ablation studies on depth map, mask, and bounding box inputs. Figure 13 reveals distinct model behaviors under depth and mask conditioning. Depth-based conditioning (left panel) shows that models vary considerably in their ability to respect spatial depth relationships, with ACE can generate plausible scenes while OmniGen and ChatDiT struggle with geometric consistency. Mask-based conditioning (right panel) demonstrates each models capacity for precise object placement and boundary adherence. ChatDiT shows more robust performance on different colors of mask references than others. Furthermore, Figure 15a demonstrates that while all models can incorporate bounding box constraints, significant variations exist in spatial accuracy and semantic fidelity. ACE and ChatDiT exhibit different strengths in scene composition and detail preservation, but both perform poorly on spatial accuracy. Ablation study on input image orders. Figure 15b presents grid of images demonstrating an ablation study on the effect of input order for different reference images. The results indicate that while all models can process combined visual inputs, the sequence in which these conditions are provided can noticeably influence the final output characteristics. For example, across OmniGen, ACE, and ChatDiT, variations in adherence to depth cues versus stylistic elements are observable when comparing Depth+Style\" to Style+Depth\" generations for the garlic scene. Similarly, the interplay between semantic maps and subject guidance, as shown in the bottom row of examples, yields distinct visual differences based on their ordering for all three models. This suggests that the sequential integration of features is not always commutative; the chosen order can be significant factor in achieving the desired emphasis and balance between multiple visual constraints, affecting aspects like textural detail, spatial definition, and overall compositional fidelity. Ablation study on captions. Figure 14 highlights the significant impact of captions on the fidelity and semantic accuracy of generated images. When provided with descriptive caption, models like OmniGen, ACE, and ChatDiT generally succeed in producing images that align well with the specified content and scene description, as demonstrated with the modern gray leather sectional sofa\" and the vibrant green potted plant stands beside cozy armchair\" examples. However, in the absence of captions (w/o caption\"), the generated outputs often exhibit marked degradation in quality and relevance across all tested models. These uncaptioned results can range from abstract or distorted representations of the intended subject, as seen with OmniGen and ACE in the sofa examples, to the generation of entirely unrelated subject matter, notably where ChatDiT produced an image of person wearing mask instead of sofa when no caption was provided for that scene. This underscores the crucial role of captions in guiding the models towards specific, coherent, and contextually appropriate image synthesis. MM 25, October 2731, 2025, Dublin, Ireland Chen et al. Figure 11: Image generation conditioned on combination of two references. Figure 12: Image generation conditioned on combination of three references. MultiRef: Controllable Image Generation with Multiple Visual References MM 25, October 2731, 2025, Dublin, Ireland Table 11: Real-world image generation conditioned on multiple image references. Although todays image generative models produce high-quality outputs, most struggle with accurately following instructions and maintaining fidelity to source images. IQ - Image Quality, IF - Instruction Following, SF - Source Fidelity. Model Element Add. IF SF IQ Spatial Mani. IF SF IQ Element Rep. IF SF IQ Attribute Tran. SF IF IQ Style Modi. IF SF IQ Overall IF SF IQ Show-o OmniGen ACE 0.511 0.553 0. 0.290 0.498 0.207 0.253 0.429 0.205 0.525 0.553 0.260 0.300 0.461 0.207 0.258 0.422 0.205 0.508 0.484 0. 0.268 0.450 0.207 0.240 0.379 0.203 0.548 0.567 0.234 0.301 0.479 0.200 0.260 0.408 0.200 0.473 0.620 0. 0.307 0.590 0.205 0.259 0.468 0.200 0.513 0.555 0.254 0.293 0.496 0.205 0.254 0.421 0.203 Compositional Framework Unified Model 0.345 0.629 ChatDiT 0.329 0.611 Gemini+SD2.1 0.330 0.620 Claude+SD2.1 0.478 0.764 Gemini+SD3 Claude+SD3 0.454 0.744 Gemini+SD3.5 0.786 0.615 0.500 0.469 0.767 Claude+SD3.5 0.390 0.372 0.402 0.590 0.578 0.563 0.352 0.324 0.339 0.453 0.456 0.643 0.620 0.625 0.729 0.751 0.756 0.777 0.598 0. 0.466 0.434 0.411 0.397 0.391 0.404 0.419 0.371 0.416 0.556 0.540 0.589 0.586 0.556 0.497 0.591 0.473 0.759 0.558 0.459 0.789 0.564 0.506 0.395 0.332 0.345 0.452 0.441 0.441 0.789 0.625 0.466 0.360 0.339 0.322 0.452 0.408 0.643 0.574 0.555 0.725 0.675 0.682 0.605 0.674 0.715 0.745 0. 0.700 0.424 0.522 0.688 0.385 0.495 0.660 0.390 0.507 0.717 0.485 0.785 0.640 0.795 0.629 0.478 0.780 0.460 0.610 0.790 0.654 0.498 0.375 0.445 0.657 0.342 0.412 0.614 0.345 0.423 0.638 0.464 0.583 0.744 0.742 0.447 0.569 0.774 0.588 0.467 0.765 0.589 0.462 Ground Truth 0.711 0. 0.712 0.751 0.780 0.748 0.651 0. 0.624 0.772 0.722 0.692 0.780 0. 0.756 0.733 0.767 0.706 Figure 13: Ablation study on depth map and mask conditioning formats. Left: Depth-conditioned generation results comparing model performance. Right: Mask-conditioned generation result. All models receive the same conditioning inputs but demonstrate varying adherence to spatial and semantic constraints. MM 25, October 2731, 2025, Dublin, Ireland Chen et al. Figure 14: Ablation study on the impact of captions. This figure illustrates the differences in image generation when text caption is provided (\"w/ caption\") versus when it is omitted (\"w/o caption\"). (a) Ablation study on bounding box conditioning formats. We compare different models using bounding boxes in black and white backgrounds for conditional image generation. (b) Ablation study on the impact of input reference order. Figure 15: Ablation studies: (a) bounding box conditioning formats; (b) impact of input reference order."
        }
    ],
    "affiliations": [
        "Allen Institute for AI Seattle, USA",
        "Huazhong University of Science and Technology Wuhan, China",
        "University of Washington Seattle, USA",
        "Zhejiang Wanli University Ningbo, China"
    ]
}