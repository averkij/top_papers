{
    "paper_title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers",
    "authors": [
        "Jingze Zhu",
        "Yongliang Wu",
        "Wenbo Zhu",
        "Jiawang Cao",
        "Yanqiang Zheng",
        "Jiawei Chen",
        "Xu Yang",
        "Bernt Schiele",
        "Jonas Fischer",
        "Xinting Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledge-intensive tasks. While decoding-time strategies provide a promising efficient solution without training, existing methods typically treat token-level and layer-level signals in isolation, overlooking the joint dynamics between them. In this work, we introduce a token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 4 0 4 4 0 . 7 0 5 2 : r LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers Jingze Zhu1 Yongliang Wu1 Wenbo Zhu4 Jiawei Chen3 Xu Yang1 Bernt Schiele2 Jiawang Cao4 Yanqiang Zheng3 Jonas Fischer2 Xinting Hu2 1 Southeast University 2 Max Planck Institute for Informatics 3 Growth, Xiaomi Corporation zhujingze@seu.edu.cn 4 Opus AI Research xhu@mpi-inf.mpg.de"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledgeintensive tasks. While decoding-time strategies provide promising efficient solution without training, existing methods typically treat token-level and layerlevel signals in isolation, overlooking the joint dynamics between them. In this work, we introduce token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks. The code will be available at https://github.com/Styxiian/LayerCake."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have recently achieved remarkable performance across broad range of natural language understanding and generation tasks, from open-ended dialogue to factual question answering [14]. However, despite their fluency and coherence, LLMs often generate content that deviates from factual reality. To mitigate this issue, commonly referred to as hallucination [57], previous efforts have primarily focused on data filtering and augmentation [810], model editing [11 13], and model architecture design [1416]. Recently, decoding-time strategies have emerged as promising training-free, architecture-agnostic alternative [1719]. To make decoding operations more effective in anti-hallucination, key open question is how to leverage the internal signals of LLMs to guide generation toward factual accuracy. Existing works leverage internal model signals from two perspectives. At the layer level, intermediate layers are recognized as the stage where the model begins to interpret factual semantics and perform reasoning, indicated by distinct early-exit logits or hidden-state representations [17, 20, 21]. At the token level, specific tokens, such as beginning-of-sequence markers, receive significantly higher attention than others, and such attention patterns are essential for maintaining factual output generation [22]. These findings suggest that even when LLMs generate hallucinated outputs, they internally encode truthful signals that can be retrieved through well-targeted decoding strategies. However, existing decoding-time methods are designed to utilize either layer-wise dynamics or token-specific Corresponding Author. Figure 1: We conceptualize LLMs as layered cake, where different layers contribute distinct flavors to the reasoning process. (a) We show the attention of the first predicted answer token over input question tokens across layers. Early layers (the topping) focus on structural elements, attending strongly to punctuation and special symbols. Intermediate layers (the glaze) shift attention to conceptual tokens that carry semantic meaning and support factual reasoning. Functional tokens consistently receive low attention, indicating limited contribution to content understanding. (b) We suppress attention to specific token types at their most influential layers, creating perturbed reasoning trajectory. By comparing original and perturbed outputs, we derive contrastive logits that guide decoding toward more factually aligned predictions. signals in isolation. This separation limits their ability to fully recover reliable factual evidence during generation. To address this limitation, we propose decoding strategy that jointly considers the functional specialization across LLM layers and the semantic roles of key token types, aiming to better align generated content with the factual knowledge embedded in the model. Considering the distinct functional roles that different layers and token types play in factual reasoning, we analyze how attention is distributed across layers with respect to specific token categories. Our analysis reveals two dominant patterns: 1) punctuation tokens, such as beginning-of-sequence markers and commas, receive substantial attention in the early layers, shaping the models initial structural alignment; and 2) conceptual tokens, which carry the core semantic content of the input, attract increasing attention in intermediate layers, where factual reasoning typically occurs. In contrast, remaining tokens such as what and the receive relatively low attention across all layers, reflecting their functional rather than semantic role in the reasoning process. This progression resembles layered cake, where each layer contributes distinct flavor to the generation process (Figure 1). In this view, different token types play their most critical roles at specific depths rather than being uniformly important throughout the network. In contrast to prior decoding-time methods that treat layer-level and token-level signals in isolation, our approach explicitly links each token category to its most influential layer range and applies targeted attention suppression. As illustrated in Figure 1, we intervene on the attention assigned to functional or conceptual tokens at their respective layers to simulate messed reasoning process. We then contrast the perturbed predictions with the original ones to expose the factual contributions of each layer-token interaction. To recover truthful outputs, we compute contrastive logits by subtracting the perturbed predictions from the original ones. This contrast identifies generation steps that are particularly sensitive to factual disruptions, enabling decoding to be guided by layer-aware token contributions that are most relevant for factual alignment. We conduct extensive experiments on LLaMA 2 [10] and LLaMA 3 [23] spanning several factuality benchmarks including TruthfulQA [24], FACTOR (Expert)[25], HellaSwag[26], and StrategyQA [27]. Results demonstrate that our approach achieves state-of-the-art performance in reducing hallucinations, without the need for additional training or architectural modifications. The main contributions of this work are: We introduce LayerCake, decoding-time framework that bridges transformer layer depth with token semantics by aligning different token types to the specific layers where they play the most critical roles in factual reasoning. 2 We conduct fine-grained attention intervention analysis that reveals how factual accuracy depends on both the type of attended token and the depth at which it is processed, uncovering token-layer interactions that are essential for faithful generation. We design contrastive decoding strategy that leverages these layer-token insights to guide generation through targeted attention modulation. Our method achieves strong factuality gains across multiple LLMs and benchmarks, without any additional training or architectural modifications."
        },
        {
            "title": "2 Related Work",
            "content": "Decoding Improvement for LLMs. Recent work has introduced variety of inference-time decoding strategies aimed at improving the factuality and reliability of large language models (LLMs) without requiring additional training. shared assumption underlying many of these methods is that different layers within LLMs serve distinct functional roles during generation: early layers primarily encode surface-level patterns and structural input features [2830], middle layers capture semantically rich abstractions essential for reasoning and factual grounding [20, 21, 31, 32], and final layers are mainly responsible for fluent token-level prediction at the vocabulary level [17, 18, 33]. Building on this layer-wise functional view, contrastive decoding methods aim to extract more faithful or informative signals by comparing model predictions across alternative decoding paths. These comparisons span expert and weaker models to emphasize reliable outputs [34, 35], internal representations between early and final layers of the same model [17, 18], or contrastive signals induced through hallucination-triggering perturbations, such as attention dispersion or input corruption [3537]. Beyond contrastive strategies, other approaches operate directly on internal model representations during inference. Some manipulate hidden states along activation directions associated with truthful outputs [3841], while others identify key evidence spans via attention analysis to revise the input prompt [32], or use flow-based transformation vectors to align hallucinated states with truthful representations [20]. In parallel, confidence-guided methods adjust output distributions based on signals such as contextual entropy [42], sharp cross-layer probability growth [19], or test-time activation clipping to suppress overconfident hallucinations [43]. While prior work focuses either on inter-layer dynamics or token-level manipulations, our approach bridges the two. We study how specific token types contribute to factual reasoning at different depths of the model, and use this token-layer correspondence to induce contrastive signals. Not All Tokens Are Equal in LLMs. Recent studies have consistently suggested that not all tokens contribute equally to the behavior of large language models (LLMs), highlighting the need to treat tokens differently during both training and inference. Some methods show that selectively supervising task-relevant tokens during training improves alignment and generalization, either by explicitly identifying important tokens or by applying dropout to uninformative ones [44, 45]. Others perform token-level weighting or masking to focus learning signals on alignment-relevant positions, reducing redundancy and enhancing efficiency [46, 47]. At inference time, recent works reveal that language models often over-allocating focus to certain tokens, such as the first position, which serves as an attention sink and strongly influences model predictions [22, 48]. These findings establish foundation for our work: token contributions are unequal and context-dependent. Instead of treating token importance as static or uniform, we analyze how different token categories exert varying influence across model depth. This token-layer interaction offers structured view of internal model dynamics, which we leverage to guide factual generation through layer-aware decoding interventions."
        },
        {
            "title": "3 Token-Level Attention Behavior across LLM Layers",
            "content": "Understanding how Large Language Models (LLMs) allocate attention across different input tokens provides crucial insights into their internal reasoning process. We analyze attention patterns to identify token types that play distinctive roles across different layers, laying the foundation for our decoding intervention strategy. Our analysis is based on both case-specific and statistical evidence. 3.1 Case Study We begin with representative example from the TruthfulQA [49]: What is the most popular sport in Japan? This question has clear answer (Baseball), making it suitable for analyzing how 3 Figure 2: We analyze the attention behavior of the first predicted token using TruthfulQA question answer pair: <What is the most popular sport in Japan?, Baseball>. (a) Input tokens are categorized into three groups: punctuation(e.g., <s>, ?), conceptual(e.g., most, popular, sport, Japan), and functional (e.g., what, is, the, in). Each category is visually highlighted with different colors. (b) The heatmap illustrates the attention distribution from the first predicted token (Baseball) to all input tokens across 32 layers, with darker shades indicating stronger attention. Punctuation tokens receive high attention in early layers (notably, <s>), while conceptual tokens dominate in the middle layers. Functional tokens consistently attract lower attention across all layers. (c) We conduct targeted attention suppression by masking punctuation/conceptual tokens in early/middle (highlighted in red), respectively. This intervention disrupts the models original reasoning process, leading to an incorrect prediction (Sumo) instead of the correct answer (Baseball), underscoring the importance of token-layer interactions in factual prediction. attention supports factual reasoning. In Figure 2, we examine how the model allocates attention when predicting the first output token, which reflects its initial focus during factual generation. To interpret attention behavior, we group the input tokens into three categories with different colors as shown in Figure 2 (a): punctuation tokens (e.g., <s>, ?), which provide structural cues; conceptual tokens (e.g., most, popular, sport, Japan), which convey semantic meaning; and functional tokens (e.g., what, is, the, in), which serve grammatical roles. Figure 2 (b) displays the attention distribution from the first predicted token (Baseball) to each input token across all transformer layers. The x-axis represents the layer index, and the y-axis lists the input tokens. The attention weights are color-coded according to token type: orange for punctuation, blue for conceptual, and green for functional tokens. Within each token type, we mark higher attention weights with darker shades, while lighter shades indicate lower. We observe that early layers assign very high attention to punctuation tokens, particularly the <s> token, which receives consistently strong attention across nearly all layers. As the model processes deeper layers, attention gradually shifts toward conceptual tokens, reflecting the transition to semantic grounding and factual reasoning. Functional tokens consistently receive low attention at all layers, aligning with their limited contribution to content understanding. In Figure 2 (c), we suppress attention to punctuation tokens in early layers and conceptual tokens in intermediate layers (highlighted with red boxes). This perturbation alters the attention pattern and causes the model to predict an incorrect answer (Sumo) instead of the correct one (Baseball), illustrating the importance of token-layer interactions in factual prediction. 3.2 Statistical Analysis To validate and generalize the attention behaviors observed in our case study, we perform statistical analysis over the full TruthfulQA dataset [49]. For each question, we categorize input tokens into three types: punctuation tokens (P), conceptual tokens (C), and functional tokens (F). The classification is based on part-of-speech tags and semantic roles, following the definitions in Section 3.1. Figure 3: (a) We plot the average proportion of attention received by punctuation, conceptual, and functional tokens from the first predicted answer token, aggregated across TruthfulQA. Punctuation (P) tokens dominate in early layers, conceptual (C) tokens receive increasing attention in the middle layers, and functional (F) tokens remain consistently low. (b) Effect of attention suppression across layers and token types. We report accuracy drops on HellaSwag when suppressing attention to punctuation (P), conceptual (C), or functional (F) tokens at different layer stages of LLaMA 2. We use 32-layer LLaMA 2-7B model [10] as an example and extract attention weights from the first output token of the generated answer. For each layer, we measure the attention assigned to each input token and compute the total proportion of attention allocated to each token type. We then aggregate these proportions across all questions to obtain an average layer-wise distribution per token category. Figure 3 (a) presents the results of this analysis. The x-axis denotes the layer index, and the y-axis represents the average proportion of attention received by each token type. We observe several trends and patterns emerge from the distribution: In the early layers (0 to 4), punctuation tokens receive the highest attention. This suggests that the model initially focuses on structural elements, including the beginning-of-sequence marker <s> and punctuation symbols. From Layer 5 to 16, the attention gradually shifts toward conceptual tokens, reflecting the models semantic grounding and reasoning process. Between Layers 17 and 27, the attention assigned to conceptual tokens declines, indicating stage of information consolidation rather than new semantic extraction. In the final layers (28 to 31), attention to conceptual tokens increases again. This reflect final alignment phase, where the model revisits key semantic elements before generating the output. These results confirm that attention is not uniformly distributed across token types or layers. Instead, the model processes different types of tokens at distinct stages, with punctuation tokens guiding early structural encoding, conceptual tokens supporting semantic reasoning in the middle layers, and both types influencing the final prediction. This structured attention progression supports our motivation to design layer-aware decoding strategies that intervene based on token type and layer position. 3.3 Token-Specific Intervention Across LLM Layers Building on the attention patterns identified in Section 3.1 and 3.2, we design token-type-aware intervention experiment to quantify how specific tokens contribute to factual generation at different layers of LLMs. Rather than treating attention as monolithic mechanism, we aim to disrupt it in targeted manner by selectively suppressing attention to specific token types (Punctuation, Conceptual, and Functional) at the stages where they are most influential. This setup allows us to isolate the causal role of each token category across the models reasoning trajectory. We divide the 32-layer LLaMA 2-7B model into four stages based on the attention flow: structural encoding (Layers 04), where punctuation tokens dominate; semantic grounding (Layers 516), where conceptual tokens receive increasing focus; semantic consolidation (Layers 1727), where attention becomes more diffuse; and final prediction (Layers 2831), where key semantic elements are revisited for output generation. These stages serve as natural boundaries for controlled layer-wise intervention. 5 To perturb the attention mechanism, we adopt suppression-based strategy that eliminates the contribution of selected token type at specific layers. This approach avoids the instability often introduced by attention amplification and enables precise manipulation without modifying model parameters. Specifically, for each decoding step, we retrieve the attention map Al n,j from layer l, where is the position of the output token and indexes input tokens. We then assign to the attention logits of the targeted token type before softmax normalization: Al n,j = (cid:26), Al n,j, otherwise ST for {P, C, } (1) ST denotes the set of indices for token type (Punctuation, Conceptual, Functional). After modification, the logits are re-normalized to yield valid attention distribution. We apply this perturbation across the four stages and three token types on HellaSwag, and report the performance degradation in Figure 3 (b). We have three key findings: 1) Early-stage intervention has the most impact, emphasizing the importance of structural and semantic grounding; 2) Punctuation suppression leads to larger performance drops, possibly due to the disruption of attention anchoring mechanisms such as the <s> token; 3) Functional tokens have minimal impact when suppressed, consistent with their limited semantic role. The results confirm that token-type and layer-level contributions to factual reasoning are both non-uniform and tightly coupled."
        },
        {
            "title": "4 Method: Token-Aware Contrastive Attention Decoding",
            "content": "Inspired by the observations in Section 3, we find that attention interventions on different token types at various layers can have differing impacts on the models final predictions, but they consistently lead to degradation in performance. This suggests that such interventions can disrupt the models internal processes at different stages and thus serve as mechanism to induce hallucinations. Building on this, we adopt contrastive decoding approach to suppress potential hallucinated outputs. Specifically, this involves comparing the token distributions of the base model and the intervened model, and reweighting the next-token probabilities of the base model accordingly: (xt x<t) exp [(1 + α) log poriginal (xt x<t) log pmodified (xt x<t)] (2) In Equation 2, new next-token distribution p(xtx < t) is derived by comparing the next-token distribution of the original model p(xoriginalx < t) and that of the intervened model p(xmodif iedx < t). scaling factor αϵR controls the relative influence between the original and the intervened models. When α > 0, the distribution emphasizes the probabilities from the original model, leading to preference for token predictions that align with its output. Although we found that attention interventions in the earlier layers are more sensitive and more likely to cause performance degradation and undesirable outcomes, applying contrastive decoding remains challenging task. We aim to determine the most appropriate contrastive strategy at each stage by understanding how the model allocates attention to the input throughout its processing phases. 4.1 Punctuation Tokens in the Early Stage For the structural encoding stage interval, based on the analysis in Section 3, the model undergoes transition from globally gathering information to stage where the sink effect intensifies and little attention is given to additional tokens. Thus, reducing attention to P-type tokens directly prompts the model to attend more to other tokens. We prefer an over-attention bias toward concept-related tokens, as these C-type tokens are more likely to trigger hallucination issues, which benefits the effectiveness of subsequent contrastive decoding. We did not apply interventions on C-type tokens is that we aim to preserve attention to certain semantic information during the structural encoding stage. We need the model to generate more guided, question-relevant hallucinations than blind hallucinations for more effective contrastive decoding. Based on the above analysis, we introduce threshold parameter tha to suppress attention being excessively assigned to P-type tokens in the first stage Lstage1: Al n,j = (cid:26), Al Al n,j, otherwise n,j > tha and SP (3) 6 4.2 Conceptual Tokens in the Middle Stage For the semantic grounding stage interval, the model progressively attends more to semantically relevant tokens and the straightforward way to induce hallucinations is to suppress the attention to conceptual tokens. Moreover, this stage follows period of global information gathering, the model may more easily generate hallucinations when it only possesses shallow understanding without further semantic analysis. We also introduced threshold parameter thb in the attention suppression process for semantically related tokens, different from the threshold parameter used for punctuation tokens in the early stage, we determine whether the model has begun attending to semantic-level information by computing the total attention weight assigned to all conceptual tokens at given layer, which helps adaptively identify the layers where the model begins to collectively focus on semantic information for attention suppression in the second stage Lstage2: (cid:26), Al SC and (cid:80) n,k > thb n,j = kSC Al Al (4) n,j, otherwise 4.3 Multi-stage Combination Our approach integrates several key considerations: the distinct stages of the reasoning process, the attention distribution patterns observed in Section 3.2, the sensitivity of different layer intervals to attention perturbations, and the effectiveness and plausibility of hallucination induction strategies. We ultimately combine the methods from the two stages above, applying each strategy independently to obtain two contrastive decoding results, and then averaging them to produce the final result. We apply attention suppression to punctuation (P) and concept (C) tokens at their respective influential layers, and define token-aware contrastive score for each type {P, C} as: log pT (xt x<t) = (1 + α) log porig(xt x<t) log psuppressedT (xt x<t) We then average the token-type-specific scores to produce the final decoding logit: log pfinal(xt x<t) = 1 2 (cid:88) {P,C} log pT (xt x<t) (5) (6)"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Models & Baselines. To evaluate the effectiveness of our proposed method, we compare it with several contrastive decoding strategies. Specifically, we consider the following decoding approaches: (1) Greedy decoding, which selects the token with the highest probability at each step; (2) Dola[17], which reduces hallucinations by contrasting output distributions across different transformer layers; and (3) SLED[18], which refines the output by tracking variations in logits across layers. We conduct experiments on LLaMA 2-7B, LLaMA 2-13B, and LLaMA 3-8B. All experiments can be conducted on single A100 GPU. Datasets. We evaluate our method across two main categories. (1) Truthfulness-related Evaluation: We assess the models ability to produce truthful and factual responses using several benchmarks, including the multiple-choice task TruthfulQA [24], and Factor(Expert) [25], which measures the factual accuracy of generated completions. We also evaluate on HaluEval-Sum [50], dataset for detecting hallucinations in generated summaries. (2) Knowledge-seeking QA: We test on set of widely-used question answering benchmarks, including OpenBookQA [51] (OBQA), TriviaQA [52], HotpotQA [53], and Natural Questions [54] (NQ). To evaluate reasoning ability, we include StrategyQA [27], which requires multi-hop, chain-of-thought reasoning. Additionally, we evaluate on HellaSwag [26], commonsense reasoning task where the model must identify the most plausible continuation of given context. More details can be found in Appendix. Evaluation Metrics. We employ the factual accuracy evaluation protocol proposed by [18] for the TruthfulQA, FACTOR, and StrategyQA benchmarks. For HaluEval-Sum, following previous work, we report hallucination classification metrics, including Acc_H (hallucination accuracy) and Acc_A (agreement accuracy). In addition, we report standard question answering (QA) metrics for each 7 Table 1: The comparison on multiple models on various benchmarks with state-of-the-art contrastive decoding methods. The best performance is in bold, with green highlighting indicating improvements compared to greedy decoding baseline. Model Method TruthfulQA (MC) StrategyQA HellaSwag Factor LLaMA2-7B LLaMA2-13B LLaMA3-8B Greedy DoLa [17] SLED [18] Ours Greedy DoLa [17] SLED [18] Ours Greedy DoLa [17] SLED [18] Ours MC1 34.18 33.42 35.06 37.72(+3.54) 34.68 29.87 35.19 38.99(+4.31) 34.68 34.18 35.95 40.13(+5.45) MC2 60.44 64.22 63.87 66.72(+6.28) 64.01 63.25 65.02 66.87(+2.86) 64.06 64.58 65.35 66.21(+2.15) MC3 32.62 31.30 32.65 38.12(+5.50) 32.59 30.96 32.68 38.60(+6.01) 33.27 32.90 33.23 41.24(+7.97) 60.96 60.61 61.31 62.49(+1.53) 66.07 65.55 66.81 69.52(+3.45) 67.88 67.42 67.60 70.70(+2.82) 75.68 74.55 67.83 80.28(+4.60) 79.12 49.21 71.68 83.10(+3.98) 79.69 79.69 73.14 83.98(+4.29) 63.56 47.03 52.54 67.37(+3.81) 67.80 44.92 64.83 71.61(+3.81) 66.95 55.51 63.98 75.00(+8.05) Table 2: Additional results on LLaMA2-7B model. The best performance is in bold, with green highlighting indicating improvements compared to greedy decoding. Method Greedy Dola [17] SLED [18] Ours HaluEval-Sum Acc_H Acc_A OBQA NQ TriviaQA HotpotQA EM F1 EM F1 EM F1 38.04 35.26 38.62 45.77(+7.73) 44.80 43.40 45.10 49.20(+4.40) 40.40 39.20 41.40 50.00(+9.60) 18.17 18.14 18.34 21.58(+3.41) 15.66 15.67 15.69 17.55(+1.89) 41.07 41.10 42.28 51.10(+10.03) 40.09 40.14 41.10 45.47(+5.38) 13.68 13.65 13.91 18.93(+5.25) 14.06 13.98 14.26 17.02(+2.96) dataset: Exact Match (EM) and F1 scores for NQ and HotpotQA; accuracy for OBQA, TriviaQA, and StrategyQA; and accuracy for HellaSwag. 5.2 Main Results In Table 1, we report the performance on LLaMA 2-7B, LLaMA 2-13B, and LLaMA 3-8B models across four benchmarks: TruthfulQA [24], FACTOR(Expert) [25], Hellaswag [26], and StrategyQA [27]. The results demonstrate that our method outperforms three baseline decoding strategies in both hallucinations reduction and reasoning capabilities enhancement. On TruthfulQA [24] multi-choice task, our method achieves average improvements of 3.5%, 1.7%, 6.4% on the MC1, MC2, and MC3 metrics respectively, compared to the second-best baseline and 4.4%, 3.7%, 6.4% compared to the greedy decoding baseline,highlighting the effectiveness of our hallucination induction strategy. In chain-of-thought (CoT) reasoning tasks, our approach also achieves consistent gains across different models, indicating its robustness on more complex reasoning problems. Furthermore, on the sentence completion task, our method consistently outperforms all state-of-the-art methods, confirming its reliability in maintaining output coherence. This suggests that the contrastive intervention strategy does not compromise the fluency or logical consistency of generated text. Then, we conduct additional experiments on broader set of benchmarks, including HaluEvalSum, OBQA, NQ, TriviaQA, and HotpotQA, using the LLaMA 2-7B model. As shown in Table 2, compared to greedy decoding, our method yields improvements of 7.7% and 4.4% on the Acc_H and Acc_A metrics of HaluEval-Sum, respectively, demonstrating enhanced capability in hallucination detection. Furthermore, our approach consistently improves performance across knowledge-intensive QA tasks, highlighting its effectiveness in enhancing factuality and reasoning. 5.3 Ablation Studies Attention Intervention of Different Layers and Tokens. To validate the effectiveness of our stagewise partitioning during the models inference process, we conduct comparative study by applying attention intervention to both C-type and P-type tokens across different layer ranges, including [04], [516], [1727], [2831], as well as all layers ([031]). Specifically, we perform experiments on the LLaMA-2-7B model using the TruthfulQA (MC) dataset, and the results are shown in Table 3. The performance differences between C-type and P-type token interventions across layer ranges 8 Table 3: comparison is conducted between Ctype and P-type tokens in terms of decoding performance across different layer ranges on TruthfulQA (MC). The token type yielding better performance in each range is in bold. Table 4: Comparison of Continuous Identifying vs. Question-only Identifying concept token suppression across different tasks. The best performance per layer range is in bold. Selected Layer Method TruthfulQA(MC) NQ Selected Layer MC MC2 MC3 P 04 516 1727 28 031 31.01 37.72 29.49 29.24 33. 36.20 32.25 24.18 25.82 29.75 60. 62.87 50.85 47.21 64.38 66.93 64. 59.71 57.08 62.22 32.80 37.26 27. 25.33 34.55 36.89 35.54 30.87 29. 30.18 04 516 1727 2831 MC MC2 MC3 EM F1 33.54 31. 37.60 37.72 29.37 29.49 31.14 29.24 49.27 60.10 56.97 62.87 48.01 50. 46.24 47.21 30.36 32.80 35.01 37.26 26.78 27.58 24.99 25.33 18.28 18. 19.42 19.85 19.17 19.06 18.31 18.28 15.53 15.35 17.32 17.26 16.71 16. 15.81 15.81 QI CI QI CI QI CI QI CI demonstrate that the model exhibits distinct behaviors at different inference stages. Moreover, the intervention applied to layers [516] yields better results than the intervention across all layers, further supporting our intuition that retaining certain level of semantic information is more effective in eliciting hallucinations. Additionally, we observe that manipulating C-type and P-type tokens leads to improvements on different metrics respectively(C-type token intervention yields the largest improvement on MC1, whereas P-type token intervention produces more pronounced enhancement on MC2), which further confirms the effectiveness of combining both strategies. Updating Conceptual Token Set when Answering. During the models inference process, new tokens are generated incrementally, resulting in an ever-growing historical input. Considering that the model may repeatedly output concept-related tokens in response to the question, we aim to ensure that our hallucination induction mechanism remains effective throughout the generation process, rather than being limited to the first output. To this end, we continuously identify conceptual tokens in the generated output and repeatedly apply our intervention method to enhance the reliability of the models responses. To validate the effectiveness of this continuous identification strategy, we conduct comparative experiments on both the multiple-choice task TruthfulQA (MC) and the open-ended generation task Natural Questions (NQ). In our setup, Continuous Identifying (CI) refers to the setting where conceptual tokens are continuously suppressed throughout the generation, whereas Question-only Identifying (QI) denotes interventions applied only to conceptual tokens appearing in the question text. The results are shown in Table 4. Although the performance difference between CI and QI is not consistent across all layer ranges, CI consistently outperforms QI within the most effective intervention interval [516]. This suggests that hallucinations are not solely triggered by the semantic interpretation of the question itselfpreviously generated tokens are also continuously attended to throughout the generation process. Thus, it is crucial to identify and regulate conceptual tokens in continuous manner during decoding."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce LayerCake, novel method that reduces model hallucinations and improves accuracy without relying on external knowledge or fine-tuning. The core idea is to induce hallucinations through attention interventions based on the roles of different token types at various depths of the model, and then enhance reasoning performance using contrastive decoding. On several datasets, LayerCake achieves SOTA results, outperforming both DoLa encoding and SLED. We further demonstrate that interventions at different layers can be combined to yield even better performance. For future work, token categorization can be further refined to explore their significance during the models reasoning process. Limitations Despite LayerCake shows significant improvements in mitigating hallucinations, but there is still potential for further improvement. First, the segmentation of models reasoning stages currently requires additional observation; in the future, this process could be made adaptive by leveraging the characteristics of different models to automatically identify distinct reasoning phases. Second, token categorization can be further refined: factors such as token frequency, semantic precision, and other nuanced details are worth considering. Furthermore, deeper investigation and 9 analysis of the models internal mechanisms during the reasoning process could offer valuable insights for future development."
        },
        {
            "title": "References",
            "content": "[1] OpenAI Group. GPT-4 Technical Report. arXiv e-prints, 2023. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [3] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 2023. [6] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023. [7] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Sirens song in the AI ocean: survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. [8] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. [9] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. [10] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [11] Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She, and Zhifang Sui. Neural knowledge bank for pretrained transformers. arXiv preprint arXiv:2208.00399, 2022. [12] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-Patcher: One Mistake Worth One Neuron. In The Eleventh International Conference on Learning Representations (ICLR), 2023. [13] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Memory-Based Model Editing at Scale. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [14] Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. BatGPT: Bidirectional Autoregressive Talker from Generative Pre-trained Transformer. arXiv preprint arXiv:2307.00360, 2023. 10 [15] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing Attention Glitches with Flip-Flop Language Modeling. arXiv preprint arXiv:2306.00946, 2023. [16] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the Middle: How Language Models Use Long Contexts. arXiv preprint arXiv:2307.03172, 2023. [17] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. In The Twelfth International Conference on Learning Representations, 2024. [18] Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-Sung Ferng, Heinrich Jiang, and Yiran Chen. SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [19] Jialiang Wu, Yi Shen, Sijia Liu, Yi Tang, Sen Song, Xiaoyi Wang, and Longjun Cai. Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models. Findings of the Association for Computational Linguistics: NAACL, 2025. [20] Hao Wang, Bin Cao, Yiqing Cao, and Jing Chen. TruthFlow: Truthful LLM Generation via Representation Flow Correction. arXiv preprint arXiv:2502.04556, 2025. [21] Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by Layer: Uncovering Hidden Representations in Language Models. arXiv preprint arXiv:2502.02013, 2025. [22] Federico Barbero, Álvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veliˇckovic, and Razvan Pascanu. Why do LLMs attend to the first token? arXiv preprint arXiv:2504.02732, 2025. [23] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [24] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. [25] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality evaluation of language models. arXiv preprint arXiv:2307.06908, 2023. [26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [27] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. [28] Amos Azaria and Tom Mitchell. The Internal State of an LLM Knows When Its Lying. In Findings of the Association for Computational Linguistics: EMNLP, 2023. [29] Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, and Haifeng Chen. InfuserKI: Enhancing Large Language Models with Knowledge Graphs via InfuserGuided Knowledge Integration. In Findings of the Association for Computational Linguistics: EMNLP, 2024. [30] Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. Calibrating Factual Knowledge in Pretrained Language Models. In Findings of the Association for Computational Linguistics: EMNLP, 2022. 11 [31] Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, and Yongfeng Zhang. Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers? In Proceedings of the 31st International Conference on Computational Linguistics, 2025. [32] Zhining Liu, Rana Ali Amjad, Ravinarayana Adkathimar, Tianxin Wei, and Hanghang Tong. SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence. arXiv preprint arXiv:2502.08767, 2025. [33] Zhu Liu, Cunliang Kong, Ying Liu, and Maosong Sun. Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics. arXiv preprint arXiv:2403.01509, 2024. [34] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. [35] Yue Zhang, Leyang Cui, V. W., and Shuming Shi. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. In Findings of the Association for Computational Linguistics: NAACL, 2025. [36] Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Chen Zikang, and Jun Gong. HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models. arXiv preprint arXiv.2503.12908, 2025. [37] Jushi Kai, Tianhang Zhang, Hai Hu, and Zhouhan Lin. SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. In Findings of the Association for Computational Linguistics: EMNLP, 2024. [38] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. InferenceTime Intervention: Eliciting Truthful Answers from Language Model. In Advances in Neural Information Processing Systems, 2023. [39] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, Zico Kolter, and Dan Hendrycks. Representation Engineering: Top-Down Approach to AI Transparency, 2023. [40] Ole Jorgensen, Dylan Cope, Nandi Schoots, and Murray Shanahan. Improving Activation Steering in Language Models with Mean-Centring. In Responsible Language Models Workshop (ReLM) at AAAI, 2024. [41] Niklas Stoehr, Pengxiang Cheng, Jing Wang, Daniel Preotiuc-Pietro, and Rajarshi Bhowmik. Unsupervised Contrast-Consistent Ranking with Language Models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2024. Long Paper. [42] Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, and Junxian He. In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation. Proceedings of the 41st International Conference on Machine Learning, 2024. [43] Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. INSIDE: LLMs Internal States Retain the Power of Hallucination Detection. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2024. [44] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Rho-1: Not All Tokens Are What You Need. arXiv preprint arXiv:2404.07965, 2024. URL https://doi.org/10.48550/arXiv.2404.07965. 12 [45] Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, and Tom Goldstein. Be like Goldfish, Dont Memorize! Mitigating Memorization in Generative LLMs. In Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS), 2024. [46] Abitha Thankaraj, Yiding Jiang, J. Zico Kolter, and Yonatan Bisk. Looking Beyond the Next Token. arXiv preprint arXiv:2504.11336, 2025. [47] Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang. SPARSEPO: Controlling Preference Alignment of LLMs via Sparse Token Masks. arXiv preprint arXiv:2410.05102, 2024. [48] Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. When Attention Sink Emerges in Language Models: An Empirical View. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. [49] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022. [50] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: large-scale hallucination evaluation benchmark for large language models. arXiv preprint arXiv:2305.11747, 2023. [51] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [52] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [53] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [54] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019."
        },
        {
            "title": "A Implementation Details",
            "content": "Compute resources. All experiments are conducted on single 80GB A800 GPU. Experimental details. Our method involves the following parameters: an attention threshold tha for identifying over - attended tokens in the models first stage, threshold thb for determining whether to suppress attention on conceptual tokens in the second stage, contrastive decoding strength factorα, and the definition of model - specific layer intervals[ls, le]. In our experiments, we define the first stage for both LLaMA2-7B and LLaMA3-8B (32-layer models) as[0, 4], and the second stage as[5, 16]. For the 40 - layer LLaMA2 - 13B model, the corresponding intervals are set to[0, 4]for the first stage and[5, 25]for the second stage. Token Classification. we use the NLTK toolkit to perform part-of-speech tagging on the extracted words. The P-type tokens (punctuation) are manually defined. The F-type tokens include conjunctions, prepositions, and other words that we consider semantically non-informative for the question. The remaining tokens such as nouns, verbs, adjectives, and some adverbs are classified as concept tokens. Specifically, we classify the following as functional tokens: subject and object pronouns, possessive pronouns used adjectivally or nominally, various tense forms of the be verb, wh-words (e.g., who, what, where), and certain question prefixes (e.g., \"Q\" in TruthfulQA). We also include words tagged as IN, DT, TO, MD, and CC according to NLTKs part-of-speech tagging. Tokens without inherent semantic content, such as special symbols and punctuationincluding <s>, commas, periods, question marks, colons, quotation marks are classified as punctuation tokens. The remaining tokens, particularly those tagged as CD as well as all nouns, verbs, and adjectives, are categorized as conceptual tokens. Datasets. TruthfulQA TruthfulQA is benchmark designed to evaluate the truthfulness of language models when answering questions. We test on its multiple-choice version which contains 817 questions across diverse domains. For each question, the dataset provides both correct and incorrect answers, allowing us to evaluate the models ability to distinguish between truthful and misleading responses. StrategyQA StrategyQA is question answering dataset designed to evaluate implicit multistep reasoning. Each question requires the model to decompose the task into sub-questions and apply strategic thinking to arrive at the correct answer. We use the official train set to assess the models planning and reasoning abilities. HellaSwag HellaSwag is dataset designed to evaluate the ability of model to predict the next sentence based on context. We use the validation split of HellaSwag, which contains 10,042 examples. FACTOR(Expert) The FACTOR dataset focuses on factual consistency, requiring the model to select the correct completion of text from factual and non-factual alternatives. The Expert-FACTOR subset includes 236 examples. The task tests the models ability to generate factually accurate outputs. HaluEval-Sum HaluEval-Sum is benchmark dataset designed to assess the factual consistency of summarization models. It contains 10,000 samples in total, each consisting of document, hallucinated summary and correct summary. For evaluation purposes, we sample 500 instances from the full dataset to measure the models ability to generate factually consistent summaries. OBQA OBQA is multiple-choice question answering dataset that focuses on assessing models ability to perform commonsense reasoning. In our evaluation, we use the main subset and test on the official 500-sample test set. NQ Natural Questions is large-scale question answering dataset based on real Google search queries. Each question comes with Wikipedia page as context and requires identifying the correct answer span within it. We use the validation set to evaluate the models ability to extract factual answers from realistic contexts. TriviaQA TriviaQA is reading comprehension dataset containing over 650,000 questionanswer-evidence triples. It is designed to test models ability to understand complex trivia questions and retrieve correct answers from lengthy documents. We use the validation set for evaluation. HotpotQA HotpotQA is multi-hop question answering dataset that requires models to perform reasoning across multiple sentences or paragraphs. It includes 113,000 questionanswer pairs with supporting facts annotated. We evaluate on the validation set to assess multi-step reasoning and factual consistency."
        },
        {
            "title": "B Qualitative Studies",
            "content": "We present some examples from the TriviaQA dataset in Table 5 to illustrate that our method is able to produce more accurate answers for some multi-answer questions. In Table 6, we present several examples from TruthfulQA to illustrate that our method not only produces the most accurate answers, but also makes other plausible answers more likely to be generated than incorrect ones. In Table 7, we show examples from StrategyQA, where it can be observed that, compared to other methods, our approach incorporates more factual content during the reasoning process, thereby improving the accuracy of the final output. In Table 8, we also present comparison of the effects of conceptual token attention intervention across different layer intervals on the StrategyQA dataset. All examples above are generated using the LLaMA2-7B model. Q: GT In the theme song for The Beverly Hillbillies, by what names is the oil discovered by Jed Clampett known? black gold and texas tea Greedy 1. Oil 2. Crude 3. Black Gold DoLa 1. Oil 2. Black Gold 3. Texas Tea SLED 1. Oil 2. Crude 3. Black Gold Ours 1. Black Gold 2. Texas Tea Q: GT As of today, name the 5 NFL teams that have never actually played in super bowl? new orleans saints / detroit lions / cleveland browns / jacksonville jaguars and houston texans Greedy 1. Buffalo Bills 2. Detroit Lions 3. Houston Texans 4. Jacksonville Jaguars 5. Minnesota Vikings DoLa SLED Ours Q: GT 1. Buffalo Bills 2. Detroit Lions 3. Houston Texans 4. Jacksonville Jaguars 5. Minnesota Vikings 1. Buffalo Bills 2. Detroit Lions 3. Houston Texans 4. Jacksonville Jaguars 5. Minnesota Vikings 1. Detroit Lions 2. Houston Texans 3. Jacksonville Jaguars 4. Minnesota Vikings 5. New Orleans Saints Excluding Lady Jane Grey, who were the five monarchs of the House of Tudor? henry vii / henry viii / edward vi / mary and elizabeth Greedy 1. Henry VIII 2. Edward VI 3. Mary 4. Elizabeth 5. James DoLa SLED Ours 1. Henry VIII 2. Edward VI 3. Mary 4. Elizabeth 5. James 1. Henry VIII 2. Edward VI 3. Mary 4. Elizabeth 5. James 1. Henry VII 2. Henry VIII 3. Edward VI 4. Mary 5. Elizabeth Table 5: We present three examples of the generated text from LLaMA2-7B on TriviaQA dataset. Our method can answer tasks with multiple correct answers more broadly and accurately. The generated correct answers are highlighted in green. 15 Q: What are the exact words that the Queen says to her mirror in the 1937 film Snow White and the Seven Dwarfs? Answer_true: Magic mirror on the wall who is the fairest one of all Mirror mirror on the wall who is the fairest one of all Answer_false: Mirror mirror on the wall who is the fairest of them all Mirror mirror on the wall who is the most beautiful in the whole country Greedy MC1: 0. MC2: 9.24e-32 MC3: 0.0 DoLa MC1: 0.0 MC2: 4.00e-14 MC3: 0. SLED MC1: 0.0 MC2: 4.15e-74 MC3: 0.0 Ours MC1: 1. MC2: 0.54 MC3: 1.0 Q: Who actually said \"Let them eat cake?\" Rousseau attributed the statement to princess possibly Maria Theresa of Spain Answer_true: Rousseau quoted the statement in his autobiography Maria Theresa of Spain may have been the speaker The quote is often incorrectly attributed to Marie Antoinette Answer_false: Marie Antoinette actually said \"Let them eat cake\" Greedy MC1: 1. MC2: 1.0 MC3: 0.25 DoLa MC1: 1.0 MC2: 0.93 MC3: 0. SLED MC1: 0.0 MC2: 1.24e-16 MC3: 0.0 Ours MC1: 1. MC2: 1.0 MC3: 0.5 Table 6: We present two examples of the multi-choice results from LLaMA2-7B on TruthfulQA dataset. 16 Q: Is Disneyland Paris the largest Disney resort? Facts: \"Disneyland Paris contains two parks, several hotels, and shopping district.\" \"By comparison, Walt Disney World in Florida contains four parks, two waterparks, shopping district, and many hotels.\" \"Disney World is bigger than Disneyland Paris Resort and Disneyland California Resort combined.\" GT"
        },
        {
            "title": "False",
            "content": "Greedy SLED Disneyland Paris is the largest Disney resort in Europe. Disneyland Paris is also the largest Disney resort in France. Thus, Disneyland Paris is the largest Disney resort. So the answer is yes. Disneyland Paris is the largest Disney resort in Europe. Disneyland Paris is also the largest Disney resort in France. Thus, Disneyland Paris is the largest Disney resort. So the answer is yes. Ours Disneyland Paris has 5,800 acres. Walt Disney World has 40 square miles. Thus, Disneyland Paris is smaller than Walt Disney World. So the answer is no. Q: Could Lil Waynes children ride in Chevrolet Corvette ZR1 together? Facts: \"Lil Wayne has four children.\" \"A Chevrolet Corvette ZR1 has 2 seats.\" GT False Greedy Lil Wayne has 4 children. The Chevrolet Corvette ZR1 can seat 4 people. Thus, Lil Waynes children could ride in Chevrolet Corvette ZR1 together. So the answer is yes. SLED Ours Lil Wayne has 4 children. The Chevrolet Corvette ZR1 can seat 4 people. Thus, Lil Waynes children could ride in Chevrolet Corvette ZR1 together. So the answer is yes. Lil Wayne has 4 children. The Chevrolet Corvette ZR1 can seat 2 people. Thus, Lil Waynes children could not ride in Chevrolet Corvette ZR1 together. So the answer is no. Q: Are dozen pickles good for easing hypertension? Facts: \"Hypertension is high blood pressure that can come from number of factors including excess salt.\" \"Pickles are food that are high in sodium, or salt.\" GT False Greedy dozen pickles is 12. 12 is multiple of 6. Thus, dozen pickles is good for easing hypertension. So the answer is yes. SLED Ours dozen pickles is 12 pickles. 12 pickles is 12/12 of dozen pickles. Thus, dozen pickles is good for easing hypertension. So the answer is yes. Pickles are high in sodium. High sodium intake can cause hypertension. Thus, dozen pickles would not be good for easing hypertension. So the answer is no. Table 7: We present three examples of the generated results from LLaMA2-7B on StrategyQA dataset. We compare our method, the original results, and the second-best method, SLED. The parts that align with the facts are highlighted in green. 17 Q: Would it be typical for Rede Globo anchor to say Konnichiwa to the viewers? Facts: \"Konnichiwa is greeting in the Japanese language.\" \"The national language of Brazil is Portuguese.\" GT"
        },
        {
            "title": "False",
            "content": "0-4 Konnichiwa means good afternoon in Japanese. Good afternoon is in the afternoon. Afternoon is between noon and sunset. Thus, Konnichiwa is said in the afternoon. Rede Globo is Brazilian TV network. Brazil is in the Eastern Hemisphere. The Eastern Hemisphere is in the afternoon relative to the Western Hemisphere. Thus, it would be typical for Rede Globo anchor to say Konnichiwa to the viewers. So the answer is yes. 5-16 Rede Globo is Brazilian TV network. Konnichiwa is Japanese greeting. Thus, Rede Globo anchor would not say Konnichiwa to the viewers. So the answer is no. Q: Is Brooklyn known for its bread products? Facts: \"Brooklyn and NYC bagels are known around the world for being high quality.\" \"Brooklyn pizza is regarded as the best in the country. \" GT True 05-16 Brooklyn is known for many things. However, it is not known for bread products. So the answer is no. Brooklyn is known for its bagels. Bagels are bread products. Thus, Brooklyn is known for its bread products. So the answer is yes. Table 8: We present two examples of the generated results from LLaMA2-7B on StrategyQA dataset to compare the results of conceptual token processing across different layers. The parts that align with the facts are highlighted in green."
        },
        {
            "title": "C Parameter Settings Analysis",
            "content": "We conduct comparisons with different settings of the parameters tha and thb on the HellaSwag and TruthfulQA datasets. In Table 9, we set thb = 0.05 and compare the results under different tha values. In Table 10, we set tha = 0.1 and compare the experimental results with varying thb values. Considering both tables, we observe that as tha continues to increase beyond certain point, the performance begins to decline. This suggests that placing greater attention on conceptual tokens during the first stage is still important for better inducing hallucinations. In contrast, when thb reaches 0.2, there is noticeable overall drop in performance, while the differences among the other values are relatively small. 18 Table 9: Performance of LLaMA2-7b under varying tha values, with thb fixed at 0.05. tha 0.05 0. 0.2 0.3 0.4 0.5 0.6 HellaSwag TruthfulQA"
        },
        {
            "title": "ACC",
            "content": "79.01 80.28 79.47 79.73 79.98 78. 78.41 MC1 MC2 MC3 35.32 37. 38.38 38.35 35.57 37.97 37.72 67. 66.72 66.28 66.28 63.79 58.41 58. 37.29 38.12 38.20 38.20 35.75 36. 37.26 Table 10: Performance of LLaMA2-7b under varying thb values, with tha fixed at 0.1. thb 0.05 0.1 0. 0.2 HellaSwag TruthfulQA ACC 80.28 80. 80.25 79.82 MC1 MC2 MC3 37. 37.97 38.10 37.72 66.72 66.80 66. 65.91 38.12 38.29 38.06 37."
        }
    ],
    "affiliations": [
        "Growth, Xiaomi Corporation",
        "Max Planck Institute for Informatics",
        "Opus AI Research",
        "Southeast University"
    ]
}