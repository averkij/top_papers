{
    "paper_title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization",
    "authors": [
        "Dongchao Yang",
        "Yuanyuan Wang",
        "Dading Chong",
        "Songxiang Liu",
        "Xixin Wu",
        "Helen Meng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, a discrete audio codec that factorizes audio into (i) reasoning tokens, which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens, which encode semantic-rich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce a unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction. Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across a wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on in-domain evaluations and demonstrates strong few-shot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at \\href{https://dongchaoyang.top/UniAudio2Demo/}{https://dongchaoyang.top/UniAudio2Demo/}."
        },
        {
            "title": "Start",
            "content": "UniAudio 2.0: Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Dongchao Yang 1 Yuanyuan Wang 1 Dading Chong 2 Songxiang Liu 2 Xixin Wu 1 Helen Meng"
        },
        {
            "title": "Abstract",
            "content": "We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, discrete audio codec that factorizes audio into (i) reasoning tokens, which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens, which encode semanticrich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction. Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on indomain evaluations and demonstrates strong fewshot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at https://dongchaoyang.top/UniAudio2Demo/. 6 2 0 2 5 ] . [ 2 3 8 6 4 0 . 2 0 6 2 : r 1. Introduction Large language models (LLMs) (OpenAI, 2023; Dubey et al., 2024) have demonstrated remarkable success by unifying diverse language tasks under single autoregressive framework. Inspired by this paradigm, recent research has 1The Chinese University of Hong Kong, China 2Independent Helen Meng <hmCorrespondence to: Researcher. meng@se.cuhk.edu.hk>. Preprint. February 6, 2026. 1 applied similar modeling principles to the audio domain, such as LM-based audio generation tasks (Borsos et al., 2023; Wang et al., 2023; Kharitonov et al., 2023), LMbased audio understanding tasks (Chu et al., 2024; Tang et al., 2024), cross-modal interaction (Defossez et al., 2024; Ding et al., 2025). Despite rapid progress, however, current audio language models still fall short of the generalization, scalability, and task versatility exhibited by their text counterparts. We argue that this limitation primarily stems from three fundamental challenges: the design of audio representations, the architecture of unified autoregressive models and the construction of large-scale multi-task training data. On the representation side, existing approaches largely fall into two categories. Continuous representations, such as self-supervised representations (SSL features) (Hsu et al., 2021; Radford et al., 2023), are effective for perception and understanding tasks but are difficult to integrate into autoregressive audio generation due to the difficulty of modeling high-dimensional features. In contrast, discrete audio codecs (Zeghidour et al., 2021; Defossez et al., 2022; Kumar et al., 2023; Yang et al., 2023b; 2024a) enable efficient generation and scalable modeling, yet their tokens mainly encode low-level acoustic details and lack text-aligned, highlevel abstractions for understanding. In this study, we focus on discrete tokenizers due to their scalability and compatibility with unified text-audio modeling objectives. To address their limited abstraction capability, we introduce ReasoningCodec, novel audio codec that explicitly factorizes audio representations into reasoning tokens and reconstruction tokens. Reasoning tokens encode text-aligned, high-level analysis and planning representations that support audio understanding and hierarchical generation, while reconstruction tokens preserve semantic content and fine-grained acoustics for high-fidelity waveform reconstruction. On the architectural side, most existing audio language models adopt naive unified autoregressive transformer (Zeng et al., 2024; Defossez et al., 2024; Ding et al., 2025) inherited from text LLMs, in which all layers indiscriminately process both text and audio tokens. Although such design is simple and convenient, we argue that it is suboptimal for audio foundation models even with improved tokenization, because: (1) discrete audio tokens remain lossy, and propagating them uniformly across all layers can limit perceptual Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Figure 1. The overview of the proposed UniAudio 2.0. abstraction and reasoning for audio understanding; and (2) directly aligning text and audio tokens throughout all transformer layers is highly challenging and can lead to rapid forgetting of pre-trained textual knowledge. To address these challenges, we propose unified autoregressive architecture with functional layer specialization. Rather than treating all transformer layers uniformly, we conceptually partition the model into three stages: the lower layers act as audio understanding experts that focus on perceptual abstraction and reasoning over audio; the intermediate layers serve as cross-modal experts to align and integrate text and audio, initialized from pre-trained LLM (e.g., LLaMA3.2 3B) to preserve rich textual knowledge; and the upper layers act as audio generation experts that specialize in modeling fine-grained acoustics. This design maintains specialized inductive biases for understanding and generation while operating within unified autoregressive framework. On the data side, we curate large-scale open-sourced audio corpora spanning speech, sound, and music, and unify them into diverse set of audio-centric tasks covering both understanding and generation. Furthermore, inspired by sequential training in LVMs (Bai et al., 2023), we introduce the concept of auditory sentences: long-context sequences composed of multiple segments that are linked by semantic or acoustic relations, where each segment can be an audio span, text span (e.g., caption), or their paired form. Auditory sentences essentially serve as unified task constructor. By organizing multiple related segments into single longcontext sequence, an auditory sentence naturally induces variety of task forms, including within-segment modeling (e.g., ASR/captioning), cross-segment dependency tracking (e.g., style/event consistency), and multi-step conditional generation (e.g., continuation conditioned on earlier segments). This enables scalable multi-task pre-training without manually designing separate task-specific pipelines, while encouraging the model to reason over compositional structure and long-range dependencies. Building on these design choices and the proposed multitask data construction strategy, we train unified audio understanding and generation model on 100B text tokens and 60B audio tokens, which we name UniAudio 2.0. Extensive experiments show that UniAudio 2.0 achieves competitive performance on seen tasks. Moreover, UniAudio 2.0 demonstrates strong few-shot and zero-shot generalization on wide range of unseen tasks, highlighting its potential as foundation model for audio language processing. Our main contributions include: ReasoningCodec: We propose discrete audio tokenizer that factorizes audio into reasoning tokens and reconstruction tokens, enabling text-aligned high-level abstraction while preserving high-fidelity waveform reconstruction. Functional layer specialization: We introduce unified autoregressive architecture that specializes lower, middle, and upper transformer layers into audio understanding, cross-modal alignment (initialized from pretrained LLM), and audio generation experts, improving both cross-modal alignment and acoustic modeling. Large-scale training and evaluation: We curate diverse set of audio-related tasks and introduce auditory sentences for scalable multi-task pre-training. We then train UniAudio 2.0 on 100B text tokens and 60B audio tokens across text, speech, sound, and music, achieving competitive performance on seen tasks and strong few-shot/zero-shot generalization on wide range of unseen tasks. 2 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Figure 2. An overview of the framework of the proposed ReasoningCodec. Semantic Decoder denotes the semantic feature decoder, which consists of several convolutional layers. 2. Related Works 2.1. Audio Language Models Recent years have witnessed rapid progress in audio language models that aim to bridge audio and text through multimodal learning (Xu et al., 2025; Zhang et al., 2025; Borsos et al., 2023; Ding et al., 2025). Existing approaches largely fall into two distinct paradigms, depending on how audio representations are integrated with language models. The first paradigm focuses on audio understanding by coupling continuous audio representations with pre-trained language models (Chu et al., 2024; Tang et al., 2024; Xu et al., 2025). In this line of work, audio signals are encoded into continuous features using pre-trained audio encoders (Radford et al., 2023; Chen et al., 2022; Hsu et al., 2021), which are then aligned with textual representations to support perception and reasoning tasks. The second paradigm formulates audio modeling as discrete sequence prediction problem, drawing inspiration from autoregressive text language models (Borsos et al., 2023; Wang et al., 2023; Kharitonov et al., 2023; Yang et al., 2023c). In this setting, raw audio is first converted into sequence of discrete tokens using an audio tokenizer or codec (Zeghidour et al., 2021; Defossez et al., 2022; Kumar et al., 2023; Zhang et al., 2023; Yang et al., 2023b), and an autoregressive transformer is trained to model and generate audio token sequences. This paradigm has been successfully applied to text-to-speech (Wang et al., 2023; Kharitonov et al., 2023), music generation (Copet et al., 2023; Agostinelli et al., 2023), speech-to-speech dialogue (Defossez et al., 2024; Zeng et al., 2024; Ding et al., 2025; Nguyen et al., 2025), and unified multi-task audio generation models (Yang et al., 2023c; Liu et al., 2025a; Wang et al., 2024; Vyas et al., 2023). Recently, more work has focused on building unified speech understanding and generation models under the LM paradigm, such as OpusLM (Tian et al., 2025a), DualSpeechLM (Wang et al., 2025c), and Ming-UniAudio (Yan et al., 2025). In this work, we focus on building unified audio understanding and generation models that can understand and generate text, speech, sound, and music. 2.2. Audio Tokenizer Audio tokenization is key design choice in audio language models, as it determines the intermediate representation on which both understanding and generation models are built. Existing approaches can be broadly grouped into two families: continuous representations and discrete tokens, which offer fundamentally different trade-offs. Continuous representations large body of work performs audio understanding by coupling continuous audio features with language models. In this paradigm, audio is encoded into continuous embeddings using HuBERT (Hsu et al., 2021), the Whisper encoder (Radford et al., 2023), WavLM (Chen et al., 2022), and so on. Continuous audio features preserve rich perceptual information and typically provide strong performance for understanding-oriented tasks. However, directly generating high-dimensional continuous sequences is less amenable to scalable autoregressive modeling: the large embedding dimension makes sequence prediction and sampling substantially more difficult compared to token-based generation, and often require additional modeling assumptions or specialized decoders 1. Discrete tokenizers To enable scalable autoregressive generation, many audio language models instead discretize audio into token sequences via clustering or vector quantization (Zeghidour et al., 2021; Defossez et al., 2022; Kumar et al., 2023). Discrete representations are naturally com1We note that many works focus on continuous autoregressive audio generation with diffusion decoder (Rouard et al., 2025), where the modeling target is typically low-dimensional VAEbased feature. 3 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization patible with language-model-style sequence prediction, but they vary significantly in the type of information retained. first subclass constructs semantic discrete tokens by quantizing intermediate representations of SSL encoders (e.g., via K-means or VQ) (Zeng et al., 2024; Du et al., 2024; Liu et al., 2024). Prior work (Borsos et al., 2023) suggests that semantic tokens are generally easier for language models to predict in generation tasks. While semantic discrete tokens capture partial high-level information, they often remain less effective than continuous semantic representations on language model based audio understanding tasks, due to the quantization-induced information bottleneck. second subclass comprises acoustic discrete tokens produced by neural audio codecs trained primarily for waveform reconstruction (Zeghidour et al., 2021; Defossez et al., 2022; Yang et al., 2023b; Kumar et al., 2023; Siuzdak, 2023; Ai et al., 2024; Yang et al., 2024c; Parker et al., 2024; Wu et al., 2024; Li et al., 2024). However, most acoustic tokenizers emphasize low-level fidelity and provide limited semantic abstraction, which can restrict their usefulness as intermediate representations for understanding. Furthermore, directly modeling the acoustic token with LM is also hard than modeling semantic token (Borsos et al., 2023). Towards unified representations The above discussion highlights persistent tension: continuous representations are strong for understanding but inconvenient for scalable generation, whereas discrete tokens enable generation but often impose an information bottleneck for understanding. Motivated by these limitations, we propose ReasoningCodec, an audio codec that explicitly factorizes audio into reasoning tokens and reconstruction tokens. This design aims to preserve language-aligned, reasoning-relevant information for understanding while maintaining sufficient acoustic structure for faithful reconstruction and high-quality autoregressive generation, thereby serving as unified intermediate representation for both capabilities. 3. ReasoningCodec core challenge for unified audio models is to design an intermediate audio representation that is simultaneously LM-friendly for autoregressive modeling and informationpreserving for LM-based understanding. Prior work (Borsos et al., 2023; Yang et al., 2025a) shows that semantic discrete tokens are generally easier to model than purely acoustic tokens in autoregressive generation. However, directly using discrete semantic tokens for understanding remains non-trivial: vector quantization introduces information loss, which often degrades performance on comprehension tasks (see Table 3). To bridge this gap, we propose ReasoningCodec, factorized audio tokenizer that decomposes an audio waveform into two complementary token streams: (i) Reasoning tokens, which encode text-aligned, high-level perceptual analyses and planning representations, optimized to match the inductive biases of text LLMs for efficient understanding and generation. Here, reasoning denotes grounded perceptual inference over acoustic cues, rather than explicit multi-step chain-of-thought reasoning in text- (ii) Multi-level reconbased LLMs (Guo et al., 2025). struction tokens that capture semantic content and finegrained acoustics for high-fidelity waveform reconstruction and LM-based autoregressive generation. Formally, given an audio x, ReasoningCodec produces = Tr(x) and = Ts(x r), where denotes reasoning tokens and denotes multi-level reconstruction tokens. Tr and Ts denote the reasoning branch and reconstruction branch, respectively. Note that the waveform is reconstructed only from ˆx = D(s). Figure 2 shows the framework of ReasoningCodec. In the following, we present the details of the reasoning branch, the reconstruction branch, and the training procedure of ReasoningCodec. 3.1. Reasoning Branch Encoders and architecture The reasoning branch uses multiple frozen pre-trained audio encoders to cover diverse domains: Whisper encoder (Boson AI, 2025) and music SSL encoder (Zhu et al., 2025). Since the music encoder outputs 25 Hz features, we downsample Whisper features to the same temporal rate before fusion. The query-based transformer encoder follows (Yang et al., 2025a) with 4 transformer layers. Finally, pre-trained text LLM (LLaMA-3.23B (Dubey et al., 2024)) is used as the decoder head and updated with LoRA. Query-based compression and quantization. We obtain reasoning tokens via query-based quantization (Yang et al., 2025a), which compresses audio into an extremely low frame-rate token sequence (5 Hz). Let RT be the continuous audio features extracted from pre-trained encoders. We use learnable queries RM and apply lightweight transformer to summarize into query states: = Enc(Q, h) RM d, (1) where = /K is controlled by an interleaving factor (we set = 5). We then quantize using residual vector quantization (RVQ) with 8 codebooks. Training objectives Reasoning tokens are optimized to support fine-grained audio perception and text-aligned analytical abstractions. Specifically, we train the reasoning branch in two stages. First, we supervise the model on multiple audio understanding tasks (e.g., ASR and audio captioning) using supervised fine-tuning (SFT); detailed task configurations are provided in Appendix A.2. Second, we further apply GRPO-style reinforcement learning (Shao et al., 2024) to encourage detailed and grounded analytical descriptions of audio properties. Let denote sampled output and R(Y ) the corresponding reward computed by task-specific verifiers. In our implementation, we use Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization LLaMA 3.1-Instruct 8B as the verifier to score the generated analyses based on their consistency with the final answers. Additional details are provided in Appendix A.2. 3.2. Reconstruction Branch Motivation Reasoning tokens are aligned with text-level information and cannot be used to recover waveforms. We thus introduce reconstruction branch that produces reconstruction tokens while retaining structural semantics across speech, sound, and music. Multi-expert semantic features Following prior work on LM-friendly audio tokenizers (Yang et al., 2025a), we use multiple frozen experts to extract semantically rich features that cover speech, music, and general sounds: (i) WavLM for phone-level speech semantics; (ii) music SSL encoder for music-structure semantics; (iii) Whisper encoder for environmental sound semantics and residual acoustic cues. Let these features be hph, hmu, henv. Group-wise quantization We quantize these features using three group-wise VQ modules. Specifically, we allocate one VQ layer for phone semantics, one for music structure, and Lenv = 6 layers for environmental sound semantics and remaining acoustic information: (cid:16) = VQ1(hph), VQ2(hmu), RVQ3:8(henv) (cid:17) . (2) This yields multi-level reconstruction token sequence s. Conditioning with reasoning tokens via FiLM To reduce redundancy between and and to inject reasoning-level context into the reconstruction process, following (Yang et al., 2025b), we condition semantic features on reasoning tokens using feature-wise linear modulation (FiLM) (Perez et al., 2018). Given semantic feature embeddings Se and quantized reasoning feature ˆR, we compute FiLM(Se; ˆR) = γ( ˆR) Se + β( ˆR), (3) where γ() and β() are small networks (implemented as lightweight MLPs). Decoder and training strategy We use flow-based scalar latent diffusion decoder (Yang et al., 2024c) as the decoder (details in Appendix A.2.3). Specifically, we expect the decoder to predict the latent representations of SQ-Codec (Yang et al., 2024c) based on the reconstruction tokens. The reconstruction branch is trained with flow-based objective and semantic feature reconstruction loss. Lrec = Lflow + λsem Lsem, (cid:104)(cid:13) (cid:13)vθ(zt, t, s) ϵ(cid:13) 2 (cid:13) 2 Lflow = Et, ϵ (cid:105) , (4) (5) where Lsem denotes semantic feature matching terms adopted in prior work (Ye et al., 2025). During training, we freeze the pre-trained experts and the reasoning branch, and update the VQ modules and the flow-based decoder. 4. UniAudio 2.0 Section 3 introduces ReasoningCodec, which factorizes an audio waveform into (i) reasoning tokens that capture language-aligned abstractions for understanding and (ii) multi-level reconstruction tokens that preserve reconstruction-friendly information. Building on this tokenizer, we develop unified multi-task audio foundation model, termed UniAudio 2.0. In the following, we describe the tokenization scheme, the unified vocabulary, the multi-stream input representation, the proposed functionally specialized autoregressive architecture, and multi-stage training strategy. 4.1. Tokenization and Vocabulary UniAudio 2.0 supports two modalities: audio and text. For audio, we apply ReasoningCodec to obtain two token sequences: reasoning tokens and reconstruction tokens s. In our implementation, both and are represented with = 8 codebooks (i.e., 8 parallel token streams per time step). For text, we adopt the tokenizer of the underlying pre-trained LLM and represent text as single token stream. Joint vocabulary We build UniAudio 2.0 with unified vocabulary that includes text tokens, reasoning tokens, and reconstruction tokens, together with special control symbols (e.g., PAD, BOS, EOS, and modality markers). Let the vocabulary sizes be Nt (text), Nr (reasoning), and Ns (reconstruction). We initialize the text embedding from the pre-trained LLM, while audio-related embeddings are randomly initialized. 4.2. Multi-Stream Representation Packing multi-modal sequences. To enable single autoregressive transformer to process both modalities, we represent each time step as multi-stream token vector. Let = 8 denote the number of audio codebooks and let the last stream index be reserved for text. We form = 9 stream representation, where the first streams are audio and the last stream is text. Concretely, we construct an input token tensor ZBT where is the batch size, is the packed sequence length, and = 9 is the number of streams. For text position, we place text token in the last stream and set all audio streams to PAD. For an audio position, we place audio tokens in the first streams and set the text stream to PAD. This design allows single transformer to consume heterogeneous sequences without changing the backbone architecture. Stream-wise embeddings and fusion We assign separate embedding table to each stream and fuse them by masked summation. Let xt,i denote the token at time step and stream i. We define binary mask mt,i {0, 1} indicating whether xt,i is valid (non-PAD) token. The fused token representation is computed as ht = (cid:80)S i=1 mt,i Ei(xt,i) 5 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization where Ei() is the embedding lookup for the i-th stream. In practice, only one modality is active at each time step. 4.3. Unified Autoregressive Architecture Backbone initialization UniAudio 2.0 is initialized from pre-trained text LLM (LLaMA 3.2 3B (Dubey et al., 2024)) to inherit strong text knowledge. To incorporate audio perception and audio generation capabilities while retaining unified autoregressive interface, we introduce functionally specialized architecture. Layer specialization. Let the whole transformer backbone consist of three consecutive blocks: (u) = Fu(h), (c) = Fcm(H (u)), (g) = Fg(H (c)) (6) where Fu denotes audio understanding experts (lower layers), Fcm denotes cross-modal experts (middle layers), and Fg denotes audio generation experts (upper layers). The cross-modal experts are initialized from the pre-trained LLM to preserve textual knowledge, while the audio-specific experts are randomly initialized. Audio-only computation in specialized experts key design is that both audio understanding experts and audio generation experts operate exclusively on audio streams, leaving text tokens unchanged. Let Maud {0, 1}BT be binary mask indicating whether position corresponds to audio (i.e., the text stream is PAD). For transformer block output (), we implement audio-only updates as = + Maud (cid:0)f (H) H(cid:1) (7) which updates hidden states only at audio positions and keeps text positions intact. This mechanism preserves the pre-trained text processing pathway while enabling dedicated capacity for audio perception and synthesis. Autoregressive modeling. UniAudio 2.0 is trained under unified autoregressive framework over the packed multistream sequence. While text and audio tokens are modeled within single transformer backbone, their prediction heads are different. The text tokens are predicted at the token level following standard language modeling practice. The text autoregressive loss is defined as Ltext = (cid:88) tT log pθ(xt,text X<t). (8) where denotes the multi-stream sequence. denotes the set of text token positions. Audio tokens are modeled at the frame level. Each audio frame corresponds to parallel reconstruction or reasoning tokens. Rather than predicting these tokens directly from the backbone, we follow the local autoregressive decoding strategy introduced in (Yang et al., 2023c; Defossez et al., 2024), and employ lightweight audio decoder conditioned on the hidden states from the audio generation experts, (g) = {h(g) t=1. For an audio frame }T at time step t, the local decoder autoregressively predicts the audio tokens. The audio autoregressive loss is then given by Laudio = (cid:88) (cid:88) tA k= log pθ(xt,k xt,<k, h(g) ), (9) with denoting the set of audio frame positions. Overall training objective The final autoregressive objective for UniAudio 2.0 combines text and audio losses: LAR = λtextLtext + λaudio Laudio, (10) where λtext and λaudio balances the contributions of the two modalities. Details of the multi-stage training procedure are provided in Section 4.4. 4.4. Data and Training Strategy UniAudio 2.0 is trained with diverse collection of text and audio data under multi-task, multi-stage training paradigm. Our data construction and training strategy are designed to (i) preserve the strong textual capability inherited from pretrained LLMs, (ii) progressively inject audio understanding and generation abilities, and (iii) improve generalization to unseen tasks. Multi-task Data Construction We organize the training corpus into several complementary data types, each corresponding to class of tasks supported by UniAudio 2.0. The training data include (1) text-only data, (2) audio-only data, (3) speech-transcription paired data, (4) speech-captiontranscription paired data, (5) audio/music-caption paired data, (6) lyric-song paired data, and (7) auditory sentences constructed using our proposed task-construction strategy. We provide details of the data and tasks in Appendix B.2. Multi-stage Training Strategy We adopt four-stage training strategy that progressively integrates audio understanding and generation capabilities into the unified autoregressive model, including: (1) Stage 1: Audio understanding warm-up, (2) Stage 2: Audio generation warm-up; (3) Stage 3: Audio-text pre-training; (4) Stage 4: Audio-text mid-training. Figure 5 presents an overview of the fourstage training. Stage 1: Audio understanding warm-up In the first stage, we focus on initializing the audio understanding experts. We train the model using subset of audio understanding tasks, while freezing all other components. To encourage the understanding experts to encode rich semantic information, we introduce an auxiliary semantic distillation objective. Following the training of ReasoningCodec 3.2, lightweight decoder is attached to reconstruct semantic features extracted from frozen WavLM and music SSL. The overall objective includes reconstruction loss and text lm loss. After the training, we will discard the decoder. Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 1. Unified codec reconstruction results on speech, sound, and music. PESQ is reported in two variants: wideband (WB) and narrowband (NB). The AudioBox score includes CE, CU, PC, and PQ. Since some tokenizers (e.g., DAC and X-Codec) operate at higher frame rate, we report comparisons under the same token-rate setting by adjusting the number of RVQ layers for the baselines. Model Speech Sound Music PESQ STOI UT-MOS VISQOL SIM VISQOL AudioBox Score VISQOL AudioBox Score DAC-Codec EnCodec MimiCodec Higg-Audio X-Codec 2.10/2.29 0.81 2.00/2.24 0.81 2.09/2.82 0.82 2.20/2.90 0.78 2.08/2.72 0.83 ALMTokenizer 2.00/2.30 0.81 2.36/2.93 0."
        },
        {
            "title": "Ours",
            "content": "3.13 2.58 3.65 3.90 3.75 3.76 3.91 3.67 3.64 3.82 3.84 3.90 3.78 3.94 0.91 0.92 0.96 0.96 0.94 0.92 0.97 3.02 2.99 3.20 3.01 2.99 3.10 3.34/4.25/3.78/5.44 3.61/4.62/3.82/5.49 4.01/4.97/3.65/5.87 3.97/4.82/3.98/5.87 4.02/4.65/3.24/5.66 4.12/5.06/3.58/5.96 4.06 4.04 4.01 3.82 3.96 4. 6.98/6.97/6.25/7.09 6.60/6.57/6.27/6.71 6.95/7.48/5.01/7.65 7.43/7.19/6.21/7.24 6.44/6.68/6.12/6.94 7.51/7.68/6.12/7.87 Table 2. LLM-based perplexity (PPL) across codebooks. We present the results from first four VQ layers. Following the common practice, lower PPL denotes better performance. Note that all of tokenizers have the same codebook size (1024)."
        },
        {
            "title": "Model",
            "content": "XCodec HiggCodec DAC Codec VQ1 VQ2 16.31 9.22 29.49 9.12 66.06 13.74 Reason-only Reconstruction-only Reason + Reconstruction 3.68 8.69 6.92 7.73 13.99 11."
        },
        {
            "title": "Speech",
            "content": "VQ3 21.73 51.32 116.54 15.88 19.75 19.42 VQ4 26.36 65.92 163.89 18.99 75.02 55.36 Avg 18.40 38.46 90.06 11.57 29.36 23."
        },
        {
            "title": "Music",
            "content": "VQ1 VQ2 VQ3 VQ4 42.3 11.77 88.1 12.83 133.9 25.67 23.60 21.34 68.09 34.47 57.82 99.66 3.81 9.19 7.02 6.97 22.94 15.55 7.26 15.94 15. 8.20 59.3 42.5 Avg 28.5 45.02 81.34 6.56 26.33 20.1 Table 3. Downstream understanding evaluation on LLM-based ASR (ASR), emotion classification (ER), audio classification (AC), and music classification (MC). Model Whisper DAC XCodec Higg-Codec ALMTokenizer Reason-only Reconstruction-only Reason+Reconstruction ASR 8.5 93.2 37.6 31.2 26.9 10.1 16.3 9. ER 59.2 5.2 29.4 30 32.4 50.2 42.1 56.4 AC MC 74 54.3 28 19 46 44.7 38 49.4 45 50.1 80 33 65 48.7 63.3 70 Stage 2: Audio generation warm-up We train the audio generation expert and the local audio decoder. We train the model on subset of audio generation tasks while keeping the understanding and cross-modal experts fixed. Stage 3: Audio-text pre-training We jointly update all model parameters using mixture of audio understanding tasks, audio generation tasks, text-only data, and audio-only data. This stage aligns the two modalities under the autoregressive objective. In this stage, the largest context is 1024. Stage 4: Audio-text mid-training In the final stage, we aim to extend the effective context length and enhance generalization to unseen tasks. We continue training on subset of the pre-training data in stage 3, augmented with our constructed auditory sentence data. This stage encourages the model to reason over longer and more complex audio-text sequences and improves robustness across diverse task settings. In this stage, the largest context is 2048. We provide more details about each training stage in the Appendix B.3. 4.5. Connection to the UniAudio Series UniAudio 2.0 continues the UniAudio research line (Yang et al., 2023c; 2024a), which aims to build unified foundation model for diverse audio understanding and generation tasks. Compared with previous versions, UniAudio 2.0 introduces substantial advances in representation learning, model architecture, and training paradigm. Goal. Similar to UniAudio and UniAudio 1.5, UniAudio 2.0 aims to develop general-purpose audio foundation model that supports speech, sound, and music understanding and generation within unified framework. While earlier systems primarily focused on multi-task learning and in-context adaptation, UniAudio 2.0 further emphasizes representation-level alignment between audio and language, enabling more scalable and transferable modeling. Representation. UniAudio and UniAudio 1.5 mainly relied on acoustic codecs or LLM-driven tokenization schemes. In contrast, UniAudio 2.0 introduces ReasoningCodec, factorized audio tokenizer that explicitly separates 7 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization text-aligned reasoning tokens from reconstruction tokens. This design provides stronger semantic abstraction for understanding while preserving fine-grained acoustic information for high-fidelity generation. Architecture. Previous versions adopted decoder-only transformer backbones without considering the cross-modal fusion. UniAudio 2.0 employs unified autoregressive backbone with specialized understanding, generation, and cross-modal experts, allowing more stable and scalable joint optimization across heterogeneous tasks. Training paradigm. Earlier systems were mainly trained using supervised multi-task learning and limited-scale pretraining. UniAudio 2.0 adopts multi-stage training pipeline and large-scale audio-text pre-training. This paradigm substantially improves few-shot and zero-shot generalization. Capabilities. Benefiting from these advances, UniAudio 2.0 extends previous systems from primarily in-domain performance to stronger cross-task and cross-domain generalization. It demonstrates improved robustness on complex understanding tasks, more controllable generation, and enhanced adaptability to unseen scenarios. Overall, UniAudio 2.0 preserves the unification philosophy of the UniAudio series while advancing its core representation and learning paradigm to new level of scalability and generalization. 5. Experiments 5.1. Dataset Data preparation for the ReasoningCodec ReasoningCodec is trained on approximately 10,000 hours of data. In the speech domain, we utilize subset of Multilingual LibriSpeech (MLS) (Pratap et al., 2020), with 5,000 hours randomly selected. In the sound domain, we utilize subset of AudioSet, with 3,000 hours randomly selected; in the music domain, we employ subset of the Million Song Dataset (Bertin-Mahieux et al., 2011), also with 2,000 hours randomly selected. Following (Yang et al., 2025a), we evaluate the codecs speech reconstruction performance using subset of the VCTK dataset (Veaux et al., 2017), and assess both audio and music reconstruction performance using the AudioCaps (Kim et al., 2019) validation set and the MusicCaps dataset (Agostinelli et al., 2023), respectively. Data preparation for audio language models As we discussed in Section 4, the training data of UniAudio 2.0 includes multiple source. We list the data sources and detailed statistics in Appendix B.2. 8 5.2. Evaluation Metrics Audio Tokenizer Evaluation We evaluate the performance of audio tokenizers in (1) audio reconstruction, (2) LLMbased audio understanding, and (3) LLM-based audio generation. Audio Reconstruction For speech reconstruction, we use DNS-MOS, UT-MOS, PESQ, STOI (Short-time Objective Intelligibility), and VISQOL. For sound and music data evaluation, we follow (Boson AI, 2025), using VISQOL (audio version) and AudioBox aesthetics score. Furthermore, following (Kumar et al., 2023), the MUSHRA subjective test is conducted for speech, sound, and music. Refer to Appendix A.3.4 for more details. LLM-based audio understanding tasks To validate whether the discrete audio tokenizer is suitable as an intermediate representation for LLM-based audio understanding tasks, we follow the setting of Qwen-Audio (Chu et al., 2024; Tang et al., 2024) to conduct multi-task training, including ASR (Panayotov et al., 2015), emotion recognition (esd, 2022), audio classification (Mesaros et al., 2017), and music classification (Tzanetakis et al., 2001). When we train the model on these downstream tasks, only the adapter and LoRA (Hu et al., 2021) are updated. LLM-based audio generation tasks To validate whether the proposed audio tokenizer is suitable for AR modeling, we follow (Yang et al., 2025a; Wang et al., 2025b), using PPL and token prediction accuracy as the metric. Audio understanding and generation tasks evaluation In this study, we focus on multiple audio-related understanding and generation tasks. To better demonstrate the capacity of UniAudio 2.0, we design three levels of evaluation: (1) seentask performance; (2) few-shot evaluation; and (3) zero-shot evaluation. We list the details of these tasks in Appendix B.4. Seen tasks These tasks are seen during the pre-training stage, including TTS, Instruct TTS, ASR, audio generation/- caption, music generation/caption, song generation, and lyric recognition. For these tasks, we follow the commonly used benchmark and evaluation metrics. Few-shot tasks We also design several few-shot tasks, including speech denoising, voice conversion, emotion recognition, and audio classification. For each task, we consider one-shot and two-shot settings. Note that MiMo-Audio (Zhang et al., 2025) also introduces few-shot setting, but our context length is 2048, which is not suitable for using the same setup as MiMo-Audio. Zero-shot tasks Lastly, we expect UniAudio 2.0 to exhibit zero-shot generalization to unseen tasks. To this end, we evaluate it on several tasks, including text question answering, speech-to-speech conversation, dysarthric speech recognition, speech-to-sound generation, and audio-prompted Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 4. The performance comparison on Seen tasks between previous SOTA models and UniAudio 2.0. For each task, we choose the commonly used benchmark and metrics. Note that we list the most representative and related works with us, the more comprehensive comparison can be found in Appendix B.6. Bold for the best result. Task & Datasets TTS ZH / EN / LS-clean InstructTTS WER/ Style-Acc / UTMOSv2 ASR LS-clean / LS-other / Seed-EN / Seed-ZH Audio Caption & Generation CIDER / KL / FD Music Caption & Generation GPT-score / KL / FAD Song Generation WER / CE/CU/PQ Lyric Recognition WER Model MiMo-Audio-7B-Instruct Qwen2.5-Omni 7B UniAudio 2.0 (Ours) MiMo-Audio-7B-Instruct CapSpeech-AR UniAudio 2.0 (Ours) MiMo-Audio-7B-Instruct Qwen2.5-Omni-7B UniAudio 2.0 (Ours) Qwen2.5-Omni Stable Audio Open UniAudio 2.0 Qwen2.5-Omni-7B MusicGen UniAudio 2. SongGen UniAudio 2.0 Qwen2.5-Omni-7B UniAudio 2.0 Performance 1.93 / 5.37 / 4.74 1.21 / 3.10 / 4.28 2.30 / 3.63 / 3.46 7.8 / 40.5 / 3.17 9.1 / 52.2 / 3.18 7.3 / 42.3 / 3.38 3.5 / 35.4 / 29.8 / 7.0 3.9 / 5.5 / 1.3 / 2.9 2.7 / 6.3 / 2.6 / 2. 0.39 / - / - - / 2.14 / 78.2 0.69 / 3.26 / 50.7 5.33 / - / - - / 1.31 / 5.0 5.14 / 1.8 / 3.44 40.58 / 6.77 / 6.86 / 7.19 36.5 / 6.87 / 7.41 / 7.62 56.99 28.57 instruction-following TTS. For these tasks, we do not explicitly include any corresponding task-specific training data; therefore, we treat them as zero-shot evaluations. 5.3. The Performance of Audio Tokenizer Reconstruction Performance Table 1 presents reconstruction performance on speech, sound, and music evaluation sets. We compare against previous SOTA universal audio codecs, including DAC (Kumar et al., 2023), Encodec (Defossez et al., 2022), Higg-AudioCodec (Boson AI, 2025), X-Codec (Ye et al., 2025), and ALMTokenizer (Yang et al., 2025a). We observe that the proposed ReasoningCodec achieves strong reconstruction quality across different audio modalities. In Appendix A.3.4, we conduct subjective evaluations of different audio tokenizers. As Table 14 shows, at the same token rate, ReasoningCodec achieves better reconstruction performance. Token Modeling Performance We explore whether ReasoningCodec is suitable for LLM-based audio generation and understanding tasks. For the generation experiments, we use the same LLM backbone, training data, and evaluation data, but choose different audio tokenizers. As Table 2 shows, we observe that (1) semantically enhanced audio tokenizers (e.g., XCodec) perform better than purely acoustic tokenizers (e.g., DAC); (2) our proposed ReasoningCodec achieves strong performance in PPL and token prediction accuracy; (3) reasoning tokens are easier for an LM to model, and combining reasoning tokens with reconstruction tokens significantly improves the prediction accuracy of reconstruction tokens. These results further demonstrate the effectiveness of ReasoningCodec. Table 3 shows the results of audio understanding tasks. We can see that ReasoningCodec achieves the best understanding performance among discrete audio tokenizers. Furthermore, its performance is close to that of continuous audio tokenizer (Whisper) on multiple audio understanding tasks. Figure 3 shows the training loss for each tokenizer; the training loss of reasoning tokens decreases very quickly: these phenomenon further validate the effectiveness of ReasoningCodec. In Section 5.4, we show that UniAudio 2.0 built on ReasoningCodec achieves comparable performance with other audio understanding competitors. In Appendix A.3.5, we present more detailed analysis of why introducing reasoning tokens is effective. 5.4. The Performance on Seen Tasks In this section, we evaluate UniAudio 2.0 on seen pretraining tasks spanning speech, general audio, and music/song. We follow standard benchmarks and protocols and compare against both unified audio/speech foundation models (e.g., MiMo-Audio and Qwen2.5-Omni) and strong task-specific systems (e.g., MusicGen), depending on task availability. As summarized in Table 4, we can see that (1) UniAudio 2.0 obtains good performance in both speech generation and recognition tasks (e.g. ASR, TTS, Instructed TTS). Moreover, UniAudio 2.0 supports multiple languages, 9 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 5. Few-shot results across tasks. MOS denotes the DNS-MOS score. ACC denotes the accuracy. Task Metrics Model 1-shot 2-shot SE PESQ/STOI/WER/MOS MiMo-Audio Ours 1.20 / 0.21 / 65.29 / 3.46 1.55 / 0.64 / 14.13 / 3.76 1.16 / 0.21 / 29.29 / 3.48 1.52 / 0.66 / 14.82 / 3.77 VC WER/SIM/MOS"
        },
        {
            "title": "Emotion",
            "content": "EN / ZH ACC (%)"
        },
        {
            "title": "Sound",
            "content": "ACC (%) MiMo-Audio Ours UniAudio 1.5 MiMo-Audio Ours UniAudio 1.5 MiMo-Audio Ours 20.95 / 0.90 / 3.80 18.61 / 0.89 / 3.74 14.05 / 0.93 / 3.78 19.01 / 0.90 / 3. 45.0 / 46.6 42.5 / 45.0 67.0 / 59.8 48.0 45.3 59.8 52.0 / 51.0 53.6 / 52.4 70.2 / 62.8 55.2 76.0 62.8 Table 6. Zero-shot results across tasks. Metrics: MMLU reports Acc (%); S2S denotes speech-to-speech/text instructionfollowing, we report S2S/S2T GPT-score; DSR reports WER (%); A-I-TTS reports SIM / Style-Acc (%) / WER (%) / UTMOSv2; SpeechSound reports WER (%) / CLAP-score / UTMOSv2. A-I-TTS denotes audio and caption guided speech generation. Speech-S denotes generate speech and sound, we use WER/CLAPscore/UTMOSv2 as the metrics. Task Model Text S2S DSR LLaMA 3.2 1B LLaMA 3.2 3B Ours LLAMA-Omni SpeechGPT Ours Qwen2.5-Omni Ours Score 34.14 47.63 44.10 3.47 / 3.99 2.19 / 2.98 2.16 / 3.66 80.6 19.4 A-I-TTS Speech-S Ours Ours 0.89 / 32.62 / 11.57 / 2.87 6.15 / 0.11 / 2.96 including English, Mandarin Chinese, and Cantonese (see Appendix B.6); (2) For general audio, UniAudio 2.0 also achieves performance that is competitive with previous SOTA task-specific models (e.g. Stable Audio); (3) For the music modality, it can effectively complete music/song understanding and generation tasks. Overall, UniAudio 2.0 consistently performs strongly across multiple benchmarks. 5.5. The Performance on Few-shot Tasks In this part, we investigate the generalization ability of UniAudio 2.0 to unseen tasks. Following the setting of GPT-3 (Mann et al., 2020), we conduct series of few-shot tasks 10 covering both generation and understanding. For the fewshot generation tasks, we follow (Yang et al., 2024a; Zhang et al., 2025) and conduct few-shot speech denoising and voice conversion. For the few-shot understanding tasks, we conduct few-shot sound classification and emotion classification. Table 5 shows the experimental results: UniAudio 2.0 achieves better generalization on most benchmarks, especially in the 1-shot setting. 5.6. The Performance on Zero-shot Tasks In this part, we further conduct more challenging zero-shot tasks to evaluate the models generalization ability to unseen tasks. Specifically, we first evaluate the text capability on the MMLU benchmark (Hendrycks et al., 2020) in zero-shot setting. Following (Fang et al., 2024), we use the InstructS2S-Eval benchmark to evaluate speech conversation ability in zero-shot setting. Lastly, we design three tasks: dysarthric speech recognition (DSR) (Kim et al., 2008), speech-sound generation (Speech-S), and audiopromptand caption-guided speech generation (A-I-TTS). Table 6 shows the results; we have the following observations: (1) UniAudio 2.0 has strong text understanding ability, and introducing audio modalities does not significantly degrade text performance; (2) it performs well on many unseen tasks, e.g., speech-to-text conversation and dysarthric speech recognition; (3) it shows some generalization ability to newly defined tasks. For example, for speech-sound generation, we ask the model to generate speech and sound events based on the text content and sound tags. Furthermore, we observe that the model can follow instructions to use an audio prompts timbre and captions style to synthesize the desired speech. However, these tasks are not solved perfectly, and we summarize the limitations in Appendix C. Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 7. Ablation studies on multi-stage training, layer specialization, and model size. Due to space limitations, we present only representative tasks in this table. More results are provided in the Appendix Table 22. Setting MMLU ASR TTS Audio Gen Music Gen Song Gen w/o Stage 4 38.77 w/o Experts 37. 1B 30.2 LS-clean: 3.76 LS-other: 7.38 SEED-ZH: 4.3 SEED-EN: 3.87 LS-clean: 4.17 LS-other: 8.0 SEED-ZH: 5.1 SEED-EN: 4.22 LS-clean: 3.8 LS-other: 7.4 SEED-ZH: 3.5 SEED-EN: 3.3 SEED-EN: 6.21 SEED-ZH: 1. KL: 3.08 FD: 47.6 KL: 1.87 FAD: 3.66 WER: 37.4 CE/PQ: 6.8/7.6 SEED-EN: 5.31 SEED-ZH: 2.76 KL: 3.98 FD: 60.8 KL: 4.12 FAD: 6. WER: 42.5 CE/PQ: 6.68 / 7.55 SEED-EN: 4.2 SEED-ZH: 5.3 KL: 3.78 FD: 82.4 KL: 4.3 FAD: 6.9 WER: 40.3 CE/PQ: 6.42/7.04 5.7. Ablation Study The influence of reasoning tokens We first explore the influence of reasoning tokens. As Table 2 and 3 show, when we do not use reasoning tokens in both understanding and generation settings, the performance is worse than combining reasoning tokens with reconstruction tokens. In Appendix A.3, we present more ablation studies about ReasoningCodec, including the influence of GRPO training loss A.3.1, the influence of multi-expert semantic encoders A.3.2, the effectiveness of FiLM A.3.3, the comparision with previous semantic tokenizer A.3.6, and the influence of classifier-free guidance (CFG) for the reconstruction performance A.3.7. The influence of multi-stage training We compare performance across different training stages. As Table 7 shows, removing Stage 4 (the mid-training stage) degrades performance on several tasks, especially those related to text capability. We speculate that Stage 4 provides additional text exposure and longer-context training that help preserve and strengthen text understanding. Furthermore, without this mid-training stage, the model performs poorly on fewshot and zero-shot evaluations. These results suggest that increasing data and task diversity (by introducing auditory sentences) is key to improving generalization to unseen tasks. The influence of layer specialization We conduct experiments to explore the effectiveness of the proposed functional layer specialization. Specifically, we drop the audio understanding and generation experts and directly train the cross-modal expert with Stage 3. As Table 7 shows, we find that the models performance significantly decreases on multiple benchmarks. This phenomenon further validates the effectiveness of our proposed unified AR structure. 11 The influence of model size We explore the influence of the cross-modal experts model size. Due to resource limitations, we use LLaMA 3.2 1B and LLaMA 3.2 3B as the cross-modal expert, respectively. For the 1B variant, we use the same training strategy as the 3B version. Table 7 shows the results: compared with the 3B model, the 1B models performance significantly decreases in multiple bencharks. In Appendix B.6.5, we also show that its generalization ability on few-shot and zero-shot tasks is much weaker. We believe model size is key factor that determines the limit of model capability, especially for multi-task foundation models. Similarly, OpusLM (Tian et al., 2025b) also found that model size has strong influence on unified understanding and generation models. 6. Conclusion In this study, we investigate how to build unified audio foundation model that supports both understanding and generation. We propose ReasoningCodec, which factorizes audio into reasoning tokens and reconstruction tokens, and train UniAudio 2.0 with unified autoregressive architecture and multi-stage, multi-task training strategy. Experiments show strong performance on seen speech, sound, and music tasks, as well as encouraging few-shot and zero-shot generalization to unseen tasks. Extensive ablation studies suggest that scaling data/task diversity and model size is key to improving generalization on unseen tasks. In the future, we plan to scale both the model and the training data to further improve generalization. We discuss limitations in Appendix C. Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization 7. Impact Statement This work studies unified audio foundation model for understanding and generation. While such models enable beneficial applications (e.g., creative assistance and humancomputer interaction), they also introduce potential risks. (1) misuse and harm: Audio generation and voice conversion capabilities can be misused for impersonation, fraud, harassment, or the creation of unauthorized content. To mitigate these risks, we encourage responsible deployment practices, including clear user consent for voice cloning, identity verification in high-stakes settings, and downstream safeguards such as content provenance and detection where applicable. (2) Copyright and content ownership: Music and audio generation can reproduce or closely imitate styles present in training data, raising copyright and attribution concerns. We advise that generated content should not be used to infringe on copyrighted works, and that deployments should incorporate policy constraints and usage guidelines aligned with local regulations."
        },
        {
            "title": "References",
            "content": "Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., et al. Audiolm: language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. Boson AI. Higgs Audio V2: Redefining Expressiveness in Audio Generation. https://github.com/ boson-ai/higgs-audio, 2025. GitHub repository. Release blog available at https://www.boson.ai/ blog/higgs-audio-v2. Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J. N., Lee, S., and Narayanan, S. S. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42(4): 335359, 2008. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. Wavlm: Largescale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518, 2022. Emotional voice conversion: Theory, databases and esd. Speech Communication, 137:118, 2022. ISSN 01676393. Chu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng, Y., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., and Vijayanarasimhan, S. Youtube8m: large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016. Agostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M., Caillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., et al. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325, 2023. Ai, Y., Jiang, X.-H., Lu, Y.-X., Du, H.-P., and Ling, Z.- H. Apcodec: neural audio codec with parallel amplitude and phase spectrum encoding and decoding. arXiv preprint arXiv:2402.10533, 2024. Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A., Darrell, T., Malik, J., and Efros, A. A. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023. Bertin-Mahieux, T., Ellis, D. P., Whitman, B., and Lamere, P. The million song dataset. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011), 2011. Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. arXiv preprint arXiv:2306.05284, 2023. Defferrard, M., Benzi, K., Vandergheynst, P., and Bresson, X. FMA: dataset for music analysis. In 18th International Society for Music Information Retrieval Conference (ISMIR), 2017. URL https://arxiv.org/ abs/1612.01840. Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. Defossez, A., Mazare, L., Orsini, M., Royer, A., Perez, P., Jegou, H., Grave, E., and Zeghidour, N. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Ding, D., Ju, Z., Leng, Y., Liu, S., Liu, T., Shang, Z., Shen, K., Song, W., Tan, X., Tang, H., et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. Doh, S., Choi, K., Lee, J., and Nam, J. Lp-musiccaps: Llm-based pseudo music captioning. arXiv preprint arXiv:2307.16372, 2023. Bogdanov, D., Won, M., Tovstogan, P., Porter, A., and Serra, X. The mtg-jamendo dataset for automatic music tagging. ICML, 2019. Du, Z., Chen, Q., Zhang, S., Hu, K., Lu, H., Yang, Y., Hu, H., Zheng, S., Gu, Y., Ma, Z., et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer 12 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. Du, Z., Gao, C., Wang, Y., Yu, F., Zhao, T., Wang, H., Lv, X., Wang, H., Ni, C., Shi, X., et al. Cosyvoice 3: Towards in-the-wild speech generation via scaling-up and post-training. arXiv preprint arXiv:2505.17589, 2025. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Evans, Z., Parker, J. D., Carr, C., Zukowski, Z., Taylor, J., and Pons, J. Stable audio open. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025. Fang, Q., Guo, S., Zhou, Y., Ma, Z., Zhang, S., and Feng, Y. Llama-omni: Seamless speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 776780. IEEE, 2017. Goel, A., Ghosh, S., Kim, J., Kumar, S., Kong, Z., Lee, S.-g., Yang, C.-H. H., Duraiswami, R., Manocha, D., Valle, R., et al. Audio flamingo 3: Advancing audio intelligence with fully open large audio language models. arXiv preprint arXiv:2507.08128, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, H., Shang, Z., Wang, C., Li, X., Gu, Y., Hua, H., Liu, L., Yang, C., Li, J., Shi, P., Wang, Y., Chen, K., Zhang, P., and Wu, Z. Emilia: large-scale, extensive, multilingual, and diverse dataset for speech generation. In arXiv:2501.15907, 2025. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Selfsupervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:34513460, 2021. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Huang, R., Hu, R., Wang, Y., Wang, Z., Cheng, X., Jiang, Z., Ye, Z., Yang, D., Liu, L., Gao, P., et al. Instructspeech: Following speech editing instructions via large language models. In Forty-first International Conference on Machine Learning, 2024. Kahn, J., Rivi`ere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., et al. Libri-light: benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 76697673. IEEE, 2020. Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and Zeghidour, N. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540, 2023. Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119132, 2019. Kim, H., Hasegawa-Johnson, M., Perlman, A., Gunderson, J. R., Huang, T. S., Watkin, K. L., Frame, S., et al. Dysarthric speech database for universal access research. In Interspeech, volume 2008, pp. 17411744, 2008. Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Kumar, K. High-fidelity audio compression with improved RVQGAN. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=qjnl1QUnFA. Lee, K., Park, K., and Kim, D. Dailytalk: Spoken dialogue In ICASSP dataset for conversational text-to-speech. 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Li, G., Liu, J., Dinkel, H., Niu, Y., Zhang, J., and Luan, J. Reinforcement learning outperforms supervised finetuning: case study on audio question answering. arXiv preprint arXiv:2503.11197, 2025a. Li, H., Xue, L., Guo, H., Zhu, X., Lv, Y., Xie, L., Chen, Y., Yin, H., and Li, Z. Single-codec: Single-codebook speech codec towards high-performance speech generation. arXiv preprint arXiv:2406.07422, 2024. 13 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Li, L., Guo, Z., Chen, H., Dai, Y., Zhang, Z., Xue, H., Zuo, T., Wang, C., Wang, S., Li, J., Xu, X., Bu, H., Zhang, B., Yuan, R., Zhou, Z., Xue, W., and Xie, L. Wenetspeech-yue: large-scale cantonese speech corpus with multi-dimensional annotation, 2025b. URL https: //arxiv.org/abs/2509.03959. Nguyen, T. A., Muller, B., Yu, B., Costa-Jussa, M. R., Elbayad, M., Popuri, S., Ropers, C., Duquenne, P.-A., Algayres, R., Mavlyutov, R., et al. Spirit-lm: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052, 2025. Li, X., Takamichi, S., Saeki, T., Chen, W., Shiota, S., and Watanabe, S. Yodas: Youtube-oriented dataset for audio and speech. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 18. IEEE, 2023. Liu, C., Yan, H., Xue, S., Liang, X., Liu, Y., Xue, Z., Song, G., and Zhou, B. Unitok-audio: unified audio generation framework via generative modeling on discrete codec tokens. arXiv preprint arXiv:2510.26372, 2025a. Liu, H., Xu, X., Yuan, Y., Wu, M., Wang, W., and Plumbley, M. D. Semanticodec: An ultra low bitrate semantic audio codec for general sound. arXiv preprint arXiv:2405.00233, 2024. Liu, Z., Ding, S., Zhang, Z., Dong, X., Zhang, P., Zang, Y., Cao, Y., Lin, D., and Wang, J. Songgen: single stage auto-regressive transformer for text-to-song generation, 2025b. URL https://arxiv.org/abs/ 2502.13128. Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 1(3):3, 2020. Olmo, T., Ettinger, A., Bertsch, A., Kuehl, B., Graham, D., Heineman, D., Groeneveld, D., Brahman, F., Timbers, F., Ivison, H., et al. Olmo 3. arXiv preprint arXiv:2512.13961, 2025. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2204.06125, 2023. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. Parker, J. D., Smirnov, A., Pons, J., Carr, C., Zukowski, Z., Evans, Z., and Liu, X. Scaling transformers for low-bitrate high-quality speech coding. arXiv preprint arXiv:2411.19842, 2024. Perez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. Mls: large-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020. McFee, B., Bertin-Mahieux, T., Ellis, D. P., and Lanckriet, G. R. The million song dataset challenge. In Proceedings of the 21st International Conference on World Wide Web, pp. 909916, 2012. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Mei, X., Meng, C., Liu, H., Kong, Q., Ko, T., Zhao, C., Plumbley, M. D., Zou, Y., and Wang, W. Wavcaps: chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. arXiv preprint arXiv:2303.17395, 2023. Mesaros, A., Heittola, T., Diment, A., Elizalde, B., Shah, A., Vincent, E., Raj, B., and Virtanen, T. Dcase 2017 challenge setup: Tasks, datasets and baseline system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events, 2017. Nguyen, T. A., Hsu, W.-N., dAvirro, A., Shi, B., Gat, I., Fazel-Zarani, M., Remez, T., Copet, J., Synnaeve, G., Hassid, M., et al. Expresso: benchmark and analysis of discrete expressive speech resynthesis. arXiv preprint arXiv:2308.05725, 2023. Rouard, S., Massa, F., and Defossez, A. Hybrid transformers for music source separation. In ICASSP 23, 2023. Rouard, S., Orsini, M., Roebel, A., Zeghidour, N., and Defossez, A. Continuous audio language models. arXiv preprint arXiv:2509.06926, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Siuzdak, H. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis. arXiv preprint arXiv:2306.00814, 2023. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., MA, Z., and Zhang, C. Salmonn: Towards generic 14 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=14rn7HpKVk. Tian, J., Chen, W., Peng, Y., Shi, J., Arora, S., Bharadwaj, S., Maekaku, T., Shinohara, Y., Goto, K., Yue, X., Yang, H., and Watanabe, S. OpusLM: Family of Open Unified Speech Language Models. In Interspeech 2025, pp. 3259 3263, 2025a. doi: 10.21437/Interspeech.2025-1184. Tian, J., Chen, W., Peng, Y., Shi, J., Arora, S., Bharadwaj, S., Maekaku, T., Shinohara, Y., Goto, K., Yue, X., et al. Opuslm: family of open unified speech language models. arXiv preprint arXiv:2506.17611, 2025b. Tjandra, A., Wu, Y.-C., Guo, B., Hoffman, J., Ellis, B., Vyas, A., Shi, B., Chen, S., Le, M., Zacharov, N., et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. Tzanetakis, G., Essl, G., Automatic musical genre classification of audio signals, URL http://ismir2001.ismir.net/ 2001. pdf/tzanetakis.pdf. and Cook, P. Veaux, C., Yamagishi, J., MacDonald, K., et al. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 6:15, 2017. Vyas, A., Shi, B., Le, M., Tjandra, A., Wu, Y.-C., Guo, B., Zhang, J., Zhang, X., Adkins, R., Ngan, W., et al. Audiobox: Unified audio generation with natural language prompts. arXiv preprint arXiv:2312.15821, 2023. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Wang, H., Hai, J., Chong, D., Thakkar, K., Feng, T., Yang, D., Lee, J., Velazquez, L. M., Villalba, J., Qin, Z., Narayanan, S., Elhiali, M., and Dehak, N. Capspeech: Enabling downstream applications in style-captioned textto-speech, 2025a. URL https://arxiv.org/abs/ 2506.02863. Wang, Y., Yang, D., Shao, Y., Chen, H., Zhao, J., Wu, Z., Meng, H., and Wu, X. Dualspeechlm: Towards unified speech understanding and generation via dual speech token modeling with large language models. arXiv preprint arXiv:2508.08961, 2025c. Wang, Y., Yang, D., Shao, Y., Chen, H., Zhao, J., Wu, Z., Meng, H., and Wu, X. Dualspeechlm: Towards unified speech understanding and generation via dual speech token modeling with large language models. arXiv preprint arXiv:2508.08961, 2025d. Wichern, G., Antognini, J., Flynn, M., Zhu, L. R., McQuinn, E., Crow, D., Manilow, E., and Roux, J. L. Wham!: Extending speech separation to noisy environments. arXiv preprint arXiv:1907.01160, 2019. Wu, H., Kanda, N., Eskimez, S. E., and Li, J. Ts3-codec: Transformer-based simple streaming single codec. arXiv preprint arXiv:2411.18803, 2024. Xu, J., Guo, Z., He, J., Hu, H., He, T., Bai, S., Chen, K., Wang, J., Fan, Y., Dang, K., et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. Yan, C., Jin, C., Huang, D., Yu, H., Peng, H., Zhan, H., Gao, J., Peng, J., Chen, J., Zhou, J., Ren, K., Yang, M., Yang, M., Xu, Q., Zhao, Q., Xiong, R., Lin, S., Wang, X., Yuan, Y., Wu, Y., Lyu, Y., He, Z., Qiu, Z., Fang, Z., and Huang, Z. Ming-uniaudio: Speech llm for joint understanding, generation and editing with unified representation, 2025. URL https://arxiv.org/abs/2511.05516. Yang, D., Liu, S., Huang, R., Lei, G., Weng, C., Meng, H., and Yu, D. Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt. arXiv preprint arXiv:2301.13662, 2023a. Yang, D., Liu, S., Huang, R., Tian, J., Weng, C., and Zou, Y. Hifi-codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint arXiv:2305.02765, 2023b. Yang, D., Tian, J., Tan, X., Huang, R., Liu, S., Chang, X., Shi, J., Zhao, S., Bian, J., Wu, X., et al. Uniaudio: An audio foundation model toward universal audio generation. arXiv preprint arXiv:2310.00704, 2023c. Wang, L., Chen, H., Wu, S., Wu, Z., Zhou, H., Zhang, C., Wang, T., and Zhang, H. Audiocodecbench: comprehensive benchmark for audio codec evaluation. arXiv preprint arXiv:2509.02349, 2025b. Yang, D., Guo, H., Wang, Y., Huang, R., Li, X., Tan, X., Wu, X., and Meng, H. Uniaudio 1.5: Large language model-driven audio codec is few-shot audio task learner. arXiv preprint arXiv:2406.10056, 2024a. Wang, X., Thakker, M., Chen, Z., Kanda, N., Eskimez, S. E., Chen, S., Tang, M., Liu, S., Li, J., and Yoshioka, T. Speechx: Neural codec language model as versatile speech transformer. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:33553364, 2024. Yang, D., Huang, R., Wang, Y., Guo, H., Chong, D., Liu, S., Wu, X., and Meng, H. Simplespeech 2: Towards simple and efficient text-to-speech with flow-based scalar latent transformer diffusion models. arXiv preprint arXiv:2408.13893, 2024b. 15 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Yang, D., Wang, D., Guo, H., Chen, X., Wu, X., and Meng, H. Simplespeech: Towards simple and efficient text-tospeech with scalar latent transformer diffusion models. arXiv preprint arXiv:2406.02328, 2024c. Yang, D., Liu, S., Guo, H., Zhao, J., Wang, Y., Wang, H., Ju, Z., Liu, X., Chen, X., Tan, X., et al. Almtokenizer: lowbitrate and semantic-rich audio codec tokenizer for audio language modeling. arXiv preprint arXiv:2504.10344, 2025a. Yang, Y., Li, Y., Sung, G., Shih, S.-F., Dooley, C., Centazzo, A., and Rajeswaran, R. Diffsoundstream: Efficient speech tokenization via diffusion decoding. arXiv preprint arXiv:2506.22362, 2025b. Ye, Z., Sun, P., Lei, J., Lin, H., Tan, X., Dai, Z., Kong, Q., Chen, J., Pan, J., Liu, Q., et al. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2569725705, 2025. Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. Zeng, A., Du, Z., Liu, M., Wang, K., Jiang, S., Zhao, L., Dong, Y., and Tang, J. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Zhang, B., Lv, H., Guo, P., Shao, Q., Yang, C., Xie, L., Xu, X., Bu, H., Chen, X., Zeng, C., et al. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 61826186. IEEE, 2022. Zhang, D., Wang, G., Xue, J., Fang, K., Zhao, L., Ma, R., Ren, S., Liu, S., Guo, T., Zhuang, W., et al. Mimo-audio: Audio language models are few-shot learners. arXiv preprint arXiv:2512.23808, 2025. Zhang, X., Zhang, D., Li, S., Zhou, Y., and Qiu, X. Speechtokenizer: Unified speech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023. Zhu, H., Zhou, Y., Chen, H., Yu, J., Ma, Z., Gu, R., Luo, Y., Tan, W., and Chen, X. Muq: Self-supervised music representation learning with mel residual vector quantization. arXiv preprint arXiv:2501.01108, 2025. 16 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization"
        },
        {
            "title": "Input shape",
            "content": "Reasoning Branch Encoder modules Token Interleaving and Retrieval Dimension of transformer encoder The number of transformer encoder Codebook size VQ layers Reconstruction Branch Encoder modules Codebook size VQ layers Semantic decoder layers Flow-based Scalar Diffusion Decoder The number of transformer decoder Dimension of transformer decoder Latent space dimension"
        },
        {
            "title": "ReasoningCodec",
            "content": "(B, 1, N) Whisper & Music Encoder w=5 768 4 1024 8 Whisper & WavLM & Music Encoder 1024 8 4 convolutional layers 24 768 136 Table 8. ReasoningCodec model backbone configurations A. ReasoningCodec In this section, we provide additional details about the proposed ReasoningCodec. Figure 2 provides an overview of the ReasoningCodec framework. A.1. Model Structure of ReasoningCodec Table 8 gives the details of ReasoningCodec configuration. A.2. The training details of ReasoningCodec The training of ReasoningCodec includes three stages: (1) supervised fine-tuning (SFT) on large-scale audio understanding tasks for the reasoning branch; (2) GRPO-style reinforcement learning to encourage detailed and grounded analytical descriptions and improve the perceptual reasoning capability of the reasoning branch; and (3) freezing the reasoning branch and training the reconstruction branch with flow-based decoder. Table 10 presents the training data and configurations for each stage. We build the mixture of audio reasoning data 2 based on Qwen3-Omni (Xu et al., 2025) and Gemini 2.5 Pro. Boxes A.4A.4 show examples. Below, we describe each stage in detail. A.2.1. STAGE 1: SFT FOR REASONING BRANCH In this stage, we follow SALMONN (Tang et al., 2024) and use multiple audio understanding tasks as training objectives. During training, we only update the VQ modules and LoRA parameters to preserve pretrained linguistic knowledge. For different tasks, we prepare multiple prompts and randomly sample one prompt per instance. A.2.2. STAGE 2: GRPO FOR REASONING BRANCH In this stage, we follow common GRPO practice (Shao et al., 2024; Li et al., 2025a) and design an accuracy-based reward to train the model. Specifically, we classify audio understanding tasks into two categories: (1) rule-based verifiable tasks, such as ASR and audio classification, where we use WER and label accuracy as rewards; and (2) tasks that are difficult to verify automatically, such as fine-grained audio analysis and interpretation. For the latter, we introduce an LLM-based judge to 2We use the term reasoning to denote fine-grained, perceptual analyses and inference cues grounded in audio (e.g., events, rhythm, timbre, and scene context), rather than symbolic multi-step logical deduction. Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 9. Details of the hyper-parameters used for GRPO training. Setting Batch Size per Device Gradient Accumulation Steps Learning Rate Temperature Maximum Response Length Number of rollouts Kullback-Leibler Coefficient Value 1 2 1 106 1.0 2048 8 0. score each rollout. We use LLaMA 3.1-Instruct 8B as the judge and ask it to evaluate the gap between the rollout and the ground-truth answer. Table 9 presents the hyper-parameters used for GRPO training. Given an input query q, the model first samples distinct outputs {o1, o2, . . . , oG}. Each output is evaluated by our reward model R, which assigns scalar reward ri to each sample. Based on these rewards, the relative advantage for each output can be computed as shown in Equation 11. The full GRPO objective is summarized in the Equation 12. ˆAi,t = r(q, oi) mean{r(q, o1), . . . , r(q, oG)} std{r(q, o1), . . . , r(q, oG)} LGRPO,i,t = min (cid:34) ˆAi,t, πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) (cid:18) πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) clip (cid:19) , 1 ϵ, 1 + ϵ (cid:35) ˆAi,t (11) (12) where ϵ denotes the PPO clipping range. A.2.3. STAGE 3: RECONSTRUCTION WAVEFORM BY FLOW-BASED DIFFUSION DECODER In this stage, we freeze the reasoning branch and train the reconstruction branch to reconstruct the waveform. Specifically, we first use multi-expert semantic encoder to extract semantically rich features for speech, music, and sound. Then, we use three parallel VQ groups to quantize these features into discrete tokens, with 1:1:6 allocation of VQ layers across the three groups. We use six VQ layers for the last group because we expect it to encode more acoustic details for better reconstruction. Together, the three groups produce 8 discrete tokens per frame, which we refer to as reconstruction tokens. Finally, we follow SimpleSpeech (Yang et al., 2024b) and build flow-based scalar diffusion decoder to recover the waveform from these reconstruction tokens. We do not directly condition waveform reconstruction on reasoning tokens for two reasons: (1) the reconstruction tokens already contain the necessary information provided by reasoning tokens, and (2) conditioning would increase the sequence length for the diffusion transformer. In early experiments, upsampling reasoning tokens and combining them with reconstruction tokens brought only marginal improvements. Instead, we use FiLM (Perez et al., 2018) to inject reasoning-token information into the multi-expert semantic encoder. Empirically, we observe no significant degradation in reconstruction quality. For SQ-Codec, we follow the training recipe of SimpleSpeech (Yang et al., 2024c) and set the latent dimension to 136. We train SQ-Codec on the same dataset used in Stage 3 of Table 10. A.2.4. IMPLEMENTATION DETAILS We train ReasoningCodec in multiple stages. For each stage, we train the model on 8 NVIDIA A100 GPUs. The learning rate is set to 1e 4 with cosine annealing schedule. 18 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 10. Training stages and configurations of ReasoningCodec. The first two stages train the reasoning branch, and the last stage trains the reconstruction branch and decoder while freezing the reasoning branch. Setting Stage 1 Stage 2 Training data WenetSpeech (Zhang et al., 2022), MLS (Pratap et al., 2020), AudioSet (Gemmeke et al., 2017), IEMOCAP (Busso et al., 2008), AudioCaps (Kim et al., 2019), WavCaps (Mei et al., 2023), LibriSpeech (Panayotov et al., 2015), TUT Acoustic scenes 2017 (Mesaros et al., 2017) LP-MusicCaps (Doh et al., 2023), mixture of audio reasoning data LibriSpeech (Panayotov et al., 2015), AudioCaps (Kim et al., 2019), small-set of LP-MusicCaps (Doh et al., 2023), IEMOCAP (Busso et al., 2008), AudioCaps (Kim et al., 2019), WavCaps (Mei et al., 2023), LibriSpeech (Panayotov et al., 2015), TUT Acoustic scenes 2017 (Mesaros et al., 2017), mixture of audio reasoning data Other configuration learning rate = 2e-4 LoRA rank = 64 learning rate = 1e-6 Stage 3 Million Song (McFee et al., 2012), MLS (Pratap et al., 2020), AudioSet (Gemmeke et al., 2017) learning rate = 2eTable 11. Ablation study for the effectiveness of FiLM. Similarly, we also use the PPL across codebooks as the metric. Model Speech Music Reason + Reconstruction W/O FiLM VQ1 VQ2 VQ3 VQ4 55.36 11.37 19.42 6.92 60.12 23.45 12.92 7. Avg VQ1 VQ2 VQ3 VQ4 Avg 20.1 23.77 24.9 25.9 15.23 19.67 15.55 17.23 42.5 54.6 7.02 7.98 A.3. Experiments In this part, we include additional more experiments for ReasoningCodec, focusing on the effectiveness of GRPO, why choose multi-expert semantic features, the effectiveness of FiLM, the subjective evaluation for audio tokenizer, why the reasoning tokens are important, the comparison between ReasoningCodec and other semantic tokenizers, and the influence of Classifier-free guidance for reconstruction performance. A.3.1. THE EFFECTIVENESS OF GRPO FOR THE REASONING BRANCH TRAINING To better understand the influence of GRPO training, we use the same audio understanding test set to evaluate the reasoning branch on ASR and audio classification tasks. As Table 13 shows, GRPO further improves performance on audio understanding tasks and also improves the quality of detailed audio reasoning analysis. Overall, GRPO brings significant improvement. A.3.2. THE EFFECTIVENESS OF MULTI-EXPERT SEMANTIC FEATURES Different audio modalities may emphasize different types of semantic information. We therefore use multiple expert encoders to extract modality-specific semantic features and apply separate VQ heads to quantize them. To demonstrate the effectiveness of this strategy, we build two baselines: (1) concatenating all semantic features and applying query-based quantization; and (2) concatenating all semantic features and applying standard RVQ. Table 16 shows the results. From the reconstruction perspective, query-based quantization performs better, while group-wise VQ yields better token modeling performance, especially for music. We attribute this to using separate VQ layers for different semantic features. Although query-based quantization can further improve reconstruction, it requires multiple transformer encoders. Considering the additional inference cost, we do not apply query-based quantization in the reconstruction branch. Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Figure 3. The training loss of different audio tokenizer for understanding tasks. Table 12. The influence of FiLM module for the Reconstruction performance. PESQ includes two versions: WB and NB. AudioBox Score includes CE, CU, PC, and PQ. Model Speech Sound Music PESQ STOI UT-MOS VISQOL SIM VISQOL AudioBox Score VISQOL AudioBox Score ReasonCodec 2.36/2.93 0.85 W/O FiLM 2.39/2.92 0.85 3.91 3.90 3.94 3.92 0.97 0. 3.10 3.14 4.12/5.06/3.58/5.96 4.21/5.12/3.49/5.92 4.03 4.01 7.51/7.68/6.12/7.87 7.48/7.62/6.14/7.78 A.3.3. THE EFFECTIVENESS OF FILM To explore whether using FiLM (Perez et al., 2018) to connect both reasoning branch and reconstruction branch is useful, we design an ablation study, as shown in Table 2. We can see that using FiLM to build the connection between reasoning tokens and reconstruction tokens is effective. Furthermore, as Table 12 shows, introducing the FiLM module does not influence the reconstruction performance. A.3.4. THE SUBJECTIVE EVALUATION FOR AUDIO TOKENIZER Table 14 shows the subjective evaluation. We can see that our proposed ReasoningCodec obtains best performance on both speech, sound, and music reconstruction. A.3.5. WHY REASONING TOKENS ARE IMPORTANT FOR BOTH UNDERSTANDING AND GENERATION TASKS? As the results in Table 2 and Table 3 show, introducing Reasoning tokens consistently improves understanding performance and reduces the modeling difficulty (e.g., lower PPL) of reconstruction tokens. We attribute the gains to two complementary factors. (1) language-aligned bottleneck that filters task-irrelevant acoustic details. Reasoning tokens are explicitly designed to align with the latent space of the text LLM, thus encouraging the representation to retain languageand reasoning-relevant content (e.g., lexical/semantic cues, high-level events) while discarding irrelevant factors (e.g., recording condition, acoustic noisy) that are less useful for understanding. This results in more learnable target for the LLM and faster convergence. As shown in Figure 3, the training loss on reasoning tokens decreases rapidly, suggesting that the LLM can efficiently capture the structure of these tokens under the autoregressive objective. (2) shared intermediate representation that benefits both understanding and generation. From the understanding perspective, reasoning tokens serve as an intermediate feature layer that is directly optimized for semantic abstraction, providing compact and discriminative signal for downstream tasks (ASR, classification, and reasoning-style evaluations). Compared with purely acoustic tokens or reconstruction tokens, this abstraction has better alignment with the text LLM. 20 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 13. Ablation results of SFT and GRPO on understanding tasks. Audio CLS denotes the audio classification tasks. For Audio Reasoning, we report GPT-score in terms of relevance and fluency. Setting ASR Audio CLS Audio Reasoning (GPT-score) Relevance Fluency SFT SFT+GRPO 10.68 7.64 36.4 40. 5.2 5.7 7.8 8.4 Table 14. The subjective reconstruction results using MUSHRA (comparative scoring of samples) of codec models on speech, sound and music. Bold for the best result. FPS denotes that the frame number in one second. TPS denotes that the token number in one second. Models FPS/TPS Cookbook size Speech () Sound () Music () MimiCodec (8 RVQ) (Defossez et al., 2024) XCodec (Ye et al., 2025) Higgs-Audio (Boson AI, 2025) Encodec (Defossez et al., 2022) DAC (Kumar et al., 2023) ReasoningCodec 12.5/100 50/100 25/100 75/150 50/100 12.5/100 2048 1024 1024 1024 1024 86.7 2.1 78.5 4.5 84.4 2.6 69.3 2.4 71.3 1.9 - 72.6 2.1 79.2 1.8 68.5 2.0 70.0 1.9 - 69.8 1.9 81.0 1.6 62.6 2.2 63.0 1.8 90.5 2.8 80.8 2. 86.6 2.3 From the generation perspective, reasoning tokens also act as planning role 3: they summarize high-level intent and semantic content, which simplifies the subsequent prediction of fine-grained reconstruction tokens. Concretely, conditioning the generation process on reasoning tokens reduces long-range uncertainty and stabilizes autoregressive decoding, leading to lower reconstruction-token perplexity  (Table 2)  and improved controllability (e.g., better adherence to captions/instructions). Overall, reasoning tokens bridge text-aligned semantics and audio realizations, enabling single autoregressive model to scale to diverse understanding and generation tasks with improved efficiency and generalization. Information-theoretic view. From an information-theoretic perspective, reasoning tokens act as compact intermediate variable that reduces the uncertainty of reconstruction-token generation. Let denote the conditioning input (e.g., caption/instruction) and S1:T the reconstruction token sequence. For an autoregressive model, the optimal negative log-likelihood (NLL) decomposes as Introducing reasoning tokens yields sem = (cid:88) t=1 H(St X, S<t) . sem = (cid:88) t=1 H(St X, R, S<t) , and conditioning can only reduce entropy: The reduction equals the conditional mutual information: H(St X, R, S<t) H(St X, S<t) . H(St X, S<t) H(St X, R, S<t) = I(St; X, S<t) , which directly explains the lower reconstruction-token perplexity. Moreover, marginalizing over gives mixture decomposition, p(S1:T X) = p(R X) p(S1:T X, R) , (cid:88) where selects high-level plan/reasoning stage, mitigating long-range multimodality and stabilizing autoregressive decoding. 3In the context of LLMs, this can be viewed as latent reasoning process: the model first produces high-level plan/thought, which then helps predict the final output. 21 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Figure 4. The influence of CFG for reconstruction performance. Table 15. Ablation study of different semantic tokenizer. We report the ASR and emotion recognition tasks. Model CosyVoice3 Ours (reasoning token) ASR 32.3 10.1 ER 34.5 50. A.3.6. COMPARED TO PREVIOUS SEMANTIC TOKENIZERS To obtain the reasoning tokens, we introduce multi-task audio understanding objective. Similar ideas of injecting supervision into tokenizer/encoder have also been explored in CosyVoice 3 (Du et al., 2025) and USTokenizer (Wang et al., 2025d). Nevertheless, our target and design are fundamentally different from prior works in several key aspects. (1) Alignment to text LLM. CosyVoice 3 inserts an FSQ layer into an intermediate encoder layer and updates the entire encoder during training. As result, its learned semantic tokens are not explicitly constrained to align with the latent space of text LLM. In contrast, we freeze the audio encoder and learn only lightweight linear projection that maps the reasoning tokens into the latent space of the text LLM, enabling direct and stable cross-modal alignment. (2) Modality coverage. Prior work mainly targets the speech domain, whereas our model is designed for unified understanding across speech, general sounds, and music. (3) Temporal granularity. Previous approaches typically adopt finer-grained discrete representations (e.g., 25 Hz). Our reasoning tokens operate at 5 Hz, yielding much more compact representation that is better suited for high-level latent reasoning/planning rather than dense acoustic content modeling. (4) Richer understanding targets. Beyond task-level supervision, we incorporate more detailed understanding signals, encouraging the model to capture fine-grained audio attributes and to form an explicit reasoning process over audio details. Furthermore, we apply the same training protocol to an alternative tokenizer (CosyVoice 3 tokenizer) on the same set of understanding tasks. As shown in Table 15, its performance is consistently worse than that of our reasoning tokens, validating the advantage of our design. A.3.7. THE INFLUENCE OF CLASSIFIER-FREE GUIDANCE AND DECODING STEPS In this part, we conduct experiments to explore the influence of classifier-free guidance (CFG) and the diffusion steps for audio reconstruction. Figure 4 shows the relationship between speech reconstruction and CFG parameter. Based on this results, we default to use CFG = 1.5 for all of experiments. We also find the minimum diffusion step is 10 steps for speech and music. But for the sound data, we recommend to use 25 steps. A.4. Audio Tokenizer Baselines To make fair comparison, we classify audio tokenizers into two types: (1) speech-based tokenizers trained on speech datasets, and (2) audio-based tokenizers trained on speech, sound, and music datasets. In this study, we mainly compare against audio-based tokenizers trained on speech, sound, and music datasets. Below, we list our chosen baselines: 22 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 16. Comparison of different quantization strategies. Model PESQ (WB) PESQ (NB) STOI UT-MOS ViSQOL SIM PPLspeech PPLmusic RVQ Query-based Group-wise VQ 2.12 2.54 2.36 2.76 3.07 2.93 0.83 0.85 0. 3.84 3.89 3.91 3.78 3.88 3.94 0.95 0.97 0.97 32.8 26.3 23.77 29.2 24.4 20.1 (1) Encodec (Defossez et al., 2022), SOTA audio codec model trained on large-scale speech, sound, and music datasets. We use the official open-sourced 24 kHz version. The frame rate is 75 Hz. (2) DAC-Codec (Kumar et al., 2023), which offers very high reconstruction performance. It is trained on large-scale speech, sound, and music datasets. The official open-sourced 24 kHz version is used. The sampling rate is 16 kHz, the frame-rate is 50hz. (3) MimiCodec (Defossez et al., 2024), SOTA low-bitrate speech codec model trained on large-scale speech dataset. The sampling rate is 24 kHz, the frame-rate is 12.5hz. (4) X-Codec (Ye et al., 2025), semantic-rich audio codec model trained on large-scale speech, sound, and music datasets. The sampling rate is 16k Hz, the frame-rate is 50hz. (5) Higgs-Audio-tokenizer4, SOTA audio codec model trained on large-scale speech, sound, and music dataset. The sampling rate is 24k Hz, the frame-rate is 25hz. To make fair comparison, for Encodec, DAC-Codec, X-Codec, and Higgs-Audio-tokenizer, we control the bitrate by dropping some RVQ layers during inference. Speech Reasoning Case The audio begins with clear, albeit slightly muffled, sound. The speakers gender is discernible as male. The language is English. There is only one speaker. The recording environment seems to be indoors, possibly quiet room, but with some ambient background noise, suggesting its not professional studio. The speaker exhibits moderate speech rate. There are no significant pauses indicating hesitation or thought, suggesting natural flow. The pitch variation is relatively consistent, without dramatic shifts, implying neutral to slightly conversational prosody. The pronunciation is generally fluent and clear, with no noticeable impediments. The audio contains the spoken phrase 00:03. This phrase is time indicator. The emotional tendency is neutral, and the tone is informative or declarative. Sound Reasoning Case The audio consists of distinct, rhythmic pattern of impacts. These impacts are predominantly in the mid-frequency range and suggest mechanical or physical action occurring outdoors or in large space. There is clear periodic repetition. The primary sound event is repeating impact, sounding like hammer or similar percussive tool. The first instance occurs around 00:00:01 and repeats approximately every second, with sharp, distinct onset. The duration of each impact is very short, less than half second. The spectral content is broadband but with strong emphasis on mid to high frequencies due to the impact. Theres slight sense of reverberation, suggesting the sound is not in completely enclosed space but perhaps outdoors or in large hall. The sound appears to be localized to central or slightly directional source. The repetitive striking sound strongly suggests construction work, hammering, or perhaps pile driving. The consistent rhythm and sharp impacts indicate deliberate and ongoing activity. The emotional connotation is neutral to slightly industrious, not alarming. The source interaction is minimal, with the sound of the impact itself being the dominant element, potentially masking very quiet background noises. The action appears to be single, repeating task. 4https://github.com/boson-ai/higgs-audio 23 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 17. UniAudio 2.0 architecture configuration. Module"
        },
        {
            "title": "Backbone",
            "content": "Configuration Init from LLaMA-3.2-3B Audio understanding experts #layers = 3; dmodel = 3072; #heads = 24; = [2048] #layers = 28; dmodel = 3072; #heads = 24; = [2048] #layers = 2; dmodel = 3072; #heads = 24; = [2048] #layers = 4; dmodel = 2048; #heads = 32; = [8] Cross-modal experts Audio generation experts Audio Local Decoder"
        },
        {
            "title": "Text tokenizer\nAudio tokenizer\nAudio sample rate\nReasoning token rate\nReconstruction token rate",
            "content": "L = 37 LLaMA3.2 tokenizer ReasoningCodec [24 kHz] [5 Hz] [12.5 Hz] Music Reasoning Case The audio has moderate to high fidelity, with clear instrumentation and vocals. The genre appears to be rock or hard rock, characterized by distorted electric guitars, prominent drum beat, and aggressive vocals. The main instruments are electric guitar, bass guitar, and drums, with male vocals. The tempo is moderately fast, around 120 BPM, with consistent and driving beat. The key appears to be minor, and the chord progression is likely based on standard rock progressions, possibly with power chords. The meter is 4/4 time, and the rhythmic complexity is moderate, with strong backbeat. The melody is delivered by the vocals and guitar, with relatively narrow range in the vocals and more varied melodic lines in the guitar riffs. The timbre is dominated by distorted guitars, solid bass tone, and punchy drums. Dynamics are generally loud, with some variation in intensity. The mood is energetic, aggressive, and defiant. The lyrics, though partially obscured by the music, seem to convey sense of rebellion or challenging authority (smash the machine, aint no stopping). The expressive techniques include distorted guitar tones, heavy drumming with prominent cymbal work, and powerful, somewhat raw vocals. potential hook can be identified in the main guitar riff and vocal chorus, which is memorable and driving. The structure appears to follow typical verse-chorus rock song format, likely with an intro, verses, choruses, and potentially bridge or guitar solo (though not fully audible in this short clip). B. UniAudio 2.0 B.1. Model Configuration We list the detailed configuration of UniAudio 2.0 in Table 17. B.2. Training Data Details We organize the training corpus into several data types, each corresponding to class of tasks supported by UniAudio 2.0. The training data includes the following data types: Text-only data To maintain and stabilize the language modeling capability of UniAudio 2.0, we incorporate large-scale text-only data during pre-training. Specifically, we use approximately 100B tokens from the high-quality annealed corpus of OLMo 3 (Olmo et al., 2025). This data is used to maintain the text capability of the original LLM when we introduce audio-modality data. 24 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Audio-only data To expose the model to diverse acoustic patterns beyond speech, we use large-scale unlabeled audio data from LAION-Audio-300M.5 This dataset covers speech, environmental sounds, and music. Speech-transcription data Paired speech and transcription data are used to construct automatic speech recognition (ASR) and text-to-speech (TTS) tasks. We collect such data from multiple sources, including Emilia (He et al., 2025), YODAS-English (Li et al., 2023), WenetSpeech (Zhang et al., 2022), and WenetSpeech-Yue (Li et al., 2025b). Speech-caption-transcription data To support speech captioning and instruction-following TTS tasks, we use datasets that provide speech, transcription, and descriptive captions. In particular, we adopt CapSpeech-MLS dataset (Wang et al., 2025a), following prior work on instruction-based speech generation (Yang et al., 2023a). Audio-caption data Audio-caption pairs enable text-to-audio and text-to-music generation. We aggregate such data from multiple sources, including WavCaps (Mei et al., 2023), AudioSet (Gemmeke et al., 2017), the Million Song Dataset (MSD) (McFee et al., 2012), and YouTube-8M (Abu-El-Haija et al., 2016). Lyric-song data To model lyric recognition and lyric-to-song generation, we construct lyricsong pairs by following the preprocessing pipeline of SongGen (Liu et al., 2025b) on the MSD dataset, Free Music Archive (Defferrard et al., 2017) and MTG-Jamendo Dataset (Bogdanov et al., 2019). Auditory sentences To further improve compositional generalization and long-context reasoning over audio, we introduce the concept of auditory sentences, inspired by (Bai et al., 2023). An auditory sentence is long-context training sequence composed of multiple, related segments (audio and/or text), designed to encourage the model to reason over compositional structures and cross-segment dependencies. We construct such sequences using several strategies: (1) segmenting long-form audio such as speech conversations, environmental recordings, or songs: we split each recording into 28 segments (ensuring the token sequence length does not exceed 2048). We choose these long-form audio samples from LAION-Audio-300M, LibriLight (Kahn et al., 2020), WavCaps, MSD, DailyTalk (Lee et al., 2023), and Expresso (Nguyen et al., 2023). (2) interleaving speech and text segments: to build this data, we use the MLS (Pratap et al., 2020) and LibriSpeech (Panayotov et al., 2015) datasets. We randomly choose several speech-transcription pairs from the same speaker to construct such sentences. We fix the order of speech and text in each sentence to keep the input format consistent. (3) interleaving audio/music and captions: similarly, we use the pairs from AudioSet, WavCaps, AudioCaps, and MSD to build auditory sentences with alternating audio and caption segments. (4) Building the mixture-clean triples: We randomly choose two audio samples (a and b) and build mixture audio c. We then form an auditory sentence such as {a, b, c} or {c, a, b} or {c, b, a} to encourage mixture reasoning. We can also concatenate multiple such triples, e.g., {a1, b1, c1, a2, b2, c2, . . ..} For both and b, they can be speech, sound, or music samples from pre-defined dataset A. The dataset consists of speech data from MLS, LibriSpeech, and WenetSpeech, sound data from AudioSet and WavCaps, and music data from MSD. Furthermore, we use demucs (Rouard et al., 2023) to separate music into vocals and accompaniment to improve data diversity. (5) Building semantic-consistent but acoustically varied pairs: we follow InstructSpeech (Huang et al., 2024) and use TTS model to construct multiple speech samples that have the same text content but different pitch, volume, speed, and emotion. This encourages the model to distinguish fine-grained acoustic attributes from semantic content. B.3. The details of multiple stage training As Figure 5 shows, the training process of UniAudio 2.0 includes four stages. We list the configuration of the four training stages in Table 18. For all four stages, we use 64 NVIDIA H100 GPUs to train the model. Below, we describe each stage in detail. Stage 1: Audio understanding warm-up In the first stage, we focus on initializing the audio understanding experts. We train the model using subset of audio understanding tasks, while freezing all other components. To encourage the understanding experts to encode rich semantic information, we introduce an auxiliary semantic distillation objective. 5https://huggingface.co/datasets/laion/LAION-Audio-300M 25 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Figure 5. Pre-training stages. For simplicity, the audio generation expert layers includes both generation expert and local decoder layers. Following ReasoningCodec (Section 3.2), lightweight decoder is attached to reconstruct continuous semantic features extracted from frozen WavLM and music SSL encoders. The overall objective includes reconstruction loss and an LM text loss. After training, we discard the decoder. Formally, the training objective in this stage is defined as Lstage1 = LLM + λrecLrec, (13) where LLM denotes the language modeling loss on text outputs, and Lrec is the reconstruction loss for distilling continuous semantic features. Specifically, the reconstruction loss is given by Lrec = D(h) zSSL2 2 , (14) Stage 2: Audio generation warm-up. In this stage, we update the audio generation expert and the local audio decoder. We train the model on subset of audio generation tasks while keeping the understanding and cross-modal experts fixed. The training objective is defined as Lstage2 = LAR, (15) where LAR denotes the weighted autoregressive prediction loss over multi-stream audio tokens. Weighted autoregressive loss over multi-stream tokens. Our ReasoningCodec produces L=8 token streams per audio frame. Let s(ℓ) denote the token from stream ℓ {1, . . . , L} at time step {1, . . . , }. We define the stream-weighted autoregressive loss as LAR = (cid:88) (cid:88) t=1 ℓ=1 wℓ log pθ (cid:16) s(ℓ) x, s(1:L) <t (cid:17) , (16) where denotes the conditioning input (e.g., text prompt), s(1:L) implementation, we set <t denotes all token streams before time step t. In our = (cid:104) 2 8 , 2 8 , 8 , 1 8 , 1 8 , 1 8 , 1 8 , 1 (cid:105) . (17) Stage 3: Audio-text pre-training. We jointly update all model parameters on mixture of audio understanding tasks, audio generation tasks, text-only data, and audio-only data. This stage aligns the two modalities under unified autoregressive 26 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 18. Four-stage training recipe of UniAudio 2.0. Trainable indicates the modules updated at each stage, and Ctx denotes the maximum context length used during training. LR denotes the learning rate. The text-only tokens are used only in Stages 3 and 4. The number of audio tokens is computed as 17.5 Ds, where Ds is the audio duration in seconds. Note that an audio clip may be reused across multiple tasks when applicable. Stage Goal Task Audio Data Steps LR Ctx Trainable 1 2 4 Understanding warm-up Generation warm-up Audiotext pre-training ASR, Audio Caption Music Caption Lyric recognization TTS, text-to-audio, song/music generation 3B 3B 50k 2e-"
        },
        {
            "title": "1024 Understanding experts",
            "content": "50k 2e-4 1024 Generation experts + audio local decoder understanding and generation tasks, text-only, audio-only 50B 500k 2e-"
        },
        {
            "title": "1024 All parameters",
            "content": "Mid-training subset of stage 3 mixture, new auditory sentence data 20B 300k 1e-"
        },
        {
            "title": "2048 All parameters",
            "content": "objective. We use maximum context length of 1024 tokens in this stage. The overall training loss is LAR = λtextLtext + λaudio Laudio, (18) where λtext = 1.6 and λaudio = 1. Here, Laudio is instantiated as the weighted token-level loss in Eq. (16). This setting is based on early experiments, which helps preserve the LLMs text capability and improves audio understanding performance. Stage 4: Audio-text mid-training In the final stage, we aim to extend the effective context length and enhance generalization to unseen tasks. We continue training on subset of the Stage 3 pre-training data, augmented with our constructed auditory sentence data. This stage encourages the model to model longer and more complex audio-text sequences and improves robustness across diverse task settings. We use maximum context length of 2048 tokens in this stage. The training objective remains identical to that in Stage 3 B.4. The details of evaluation data and evaluation metrics UniAudio 2.0 is multi-task audio foundation model, which supports multiple audio-related tasks. For each task, we follow the commonly used evaluation benchmark and metrics. In the following, we present the details for each task. B.4.1. SEEN TASKS ASR For the ASR task, we follow previous works (Xu et al., 2025; Zhang et al., 2025; Li et al., 2025b), and choose LibriSpeech-test-clean, LibriSpeech-test-other, SEED-TTS-Eval-EN, SEED-TTS-Eval-ZH, and WSYue-ASR-eval (Li et al., 2025b) as the evaluation benchmark. For English datasets, we use WER as the metric. For Chinese and Cantonese, we use CER as the metric. TTS For the TTS task, we use LibriSpeech-test-clean, SEED-TTS-Eval-EN, SEED-TTS-Eval-ZH, WSYue-TTS-eval (Li et al., 2025b) as the benchmark. For evaluation, we use Whisper-large-v3 to evaluate the English speech performance, Paraformer-zh is used for Chinese speech, and SenseVoice-s-Yue is used for Cantonese speech. Furthermore, we also use DNS-MOS to evaluate the speech quality. Instruct TTS For the Instruct TTS task, we follow the setting of CapSpeech (Wang et al., 2025a) and use WER, style accuracy, and UTMOSv2 as evaluation metrics. Although our model is not trained on Chinese Instruct TTS, we show that it performs well on Chinese instruction TTS by following InstructTTS (Yang et al., 2023a) and using the same evaluation dataset. Audio Caption For the audio caption task, we follow Audio Flamingo 3(Goel et al., 2025) use CIDER as the evaluation metric. Furthermore, we also use GPT-score to evaluate the models prediction. The prompt as Box B.5 shows. 27 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Music Caption Similarly, for the music caption task, we use MusicCaps test set. The same evaluation metrics are used as the audio caption task. Audio Generation For the text-to-audio task, we follow the setting of Stable Audio Open (Evans et al., 2025), using FD, KL, and CLAP score as the metric. Music Generation For the text-to-music task, we follow MusicGen(Copet et al., 2023), using FAD, KL, and CLAP-score as the metrics. Song Generation For the song generation task, we follow SongGen (Liu et al., 2025b), using their benchmark and evaluation metrics: WER and AudioBox Score (Tjandra et al., 2025). Lyric Recognition For the lyric recognition task, we use the benchmark from SongGen. B.4.2. FEW-SHOT TASKS few-shot speech denoising task We build the few-shot speech denoising evaluation set based on LibriTTS-test-clean and WHAM noise (Wichern et al., 2019) to build the mixture-clean pairs. For the evaluation, we follow Mimo-Audio(Zhang et al., 2025), using PESQ, STOI, WER, DNS-MOS as the metrics. Few-shot voice conversion We build the few-shot voice conversion evaluation set based on VCTK (Veaux et al., 2017) dataset. For the evaluation, we follow Mimo-Audio, using WER, Speaker Similarity (SIM), and DNS-MOS as the metrics. Few-shot emotion classification We build the few-shot emotion classification evaluation set based ESD (esd, 2022) dataset. We use both English and Chinese splits to build the evaluation set. Few-shot sound event classification We build the few-shot sound event classification evaluation set based on TUT acoustic scenes 2017 (Mesaros et al., 2017). B.5. Zero-shot tasks In this study, we define zero-shot task as one that is never seen during training, and we do not provide any demonstrations at inference time. The model is asked to predict the output based only on the task input. text understanding We follow the standard zero-shot evaluation setting for text LLMs and use the MMLU dataset (Hendrycks et al., 2020) as the benchmark. We note that some previous work (Tian et al., 2025b) uses few-shot setting to evaluate text understanding ability; in this study, we directly evaluate its zero-shot ability. speech-to-speech/text question answer During our training stage, we do not add the speech conversation data. Instead, we build lot of auditory sentence without explicit instruction. Thus, we view the speech-to-speech/text question answer as the zero-shot task. We follow LLAMA-Omni (Fang et al., 2024), and use the InstructS2S-Eval as the benchmark. Following LLAMA-Omni, GPT-score is used as the metric. We use the same prompt to evaluate the performance as LLAMA-Omni. Dysarthric Speech Recognition Dysarthric speech recognition is very similar to the ASR task, but it asks the model to recognize dysarthric speech. Each utterance only includes one word. We also include it as zero-shot setting, since the model was not trained on such data. Audio prompt and caption guided TTS We define new task: using audio prompt to provide the timbre and the caption to guide the speaking style. We use speaker similarity, style accuracy, WER, UTMOSv2 as the metrics. speech-sound generation We define new task: asking the model to generate speech and corresponding sound event. The input includes sound event tag and speech content. We use WER, CLAP score, and UTMOSv2 as the metrics. 28 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 19. ASR performance on LibriSpeech and Seed-TTS benchmarks. Note that we do not find useful ASR instruction for StepAudio-chat, thus we directly use the official reported results on Librispeech benchmark. Model LibriSpeech-clean Libri-other Seed-TTS ZH Seed-TTS EN Mimo-Audio-Instruct-7B Qwen2.5-Omni-7B Step-Audio-chat-3B Whisper-large-v3 UniAudio 2.0 (Ours) 3.50 3.92 3.11 1.81 2.71 35.43 5.52 8.44 3.55 6.33 29.81 1.3 6.8 2. 7.01 2.89 1.47 2.14 Table 20. ASR results on WenetSpeech-Yue Benchmark. Model Long sentence Short sentence Whisper-large-v3 SenseVoice-Yue Qwen2.5-Omni-7B UniAudio 2.0 (Ours) 36.8 15.7 23.5 12.1 31.5 6.0 31.0 7.7 Prompt: Audio Caption Evaluation You are an expert evaluator for audio captioning. Given: - PRED (model caption) - GT (human reference caption) Evaluate PRED against GT and output ONLY valid JSON object. Scoring (integers 0-10): - relevance: whether PRED describes the same sound event(s) as GT - fluency: whether PRED covers key details without grammaticality Also provide: - overall: integer 0-10 (holistic score) - brief_reason: <= 60 words, concise explanation focusing on mismatches B.6. The details of experimental results Due to the page limit, we only choose some representative works in Table 4. In this part, we will compare with more baseline models on different tasks. B.6.1. ASR PERFORMANCE COMPARISON Table 19 shows the ASR performance comparison on Chinese and English benchmarks. Table 20 shows the Cantonese ASR performance. We compare with SenseVoice-Yue (Li et al., 2025b), Whisper-large-v3 (Radford et al., 2023), and Qwen2.5-Omni. B.6.2. TTS PERFORMANCE COMPARISON Table 21 shows the TTS performance on Seed-TTS-eval (EN/ZH), LibriSpeech-clean (EN), and WenetSpeech-Yue-TTS (Cantonese) benchmarks. 29 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 21. TTS results on Seed-TTS (EN/ZH), LibriSpeech-clean, and Cantonese TTS. We report the WER and DNS-MOS score (WER / DNS-MOS). Model Seed-TTS-EN Seed-TTS-ZH LibriSpeech-clean WenetSpeech-Yue-TTS MiMo-Audio-Instruct-7B Qwen2.5-Omni-7B CosyVoice-Yue Ours 4.74 / 3.27 3.10 / 3.62 - 3.63 / 3.80 1.93 / 3.34 1.21 / 3.65 - 2.30 / 3.82 5.30 / 3.35 4.28 / 3.72 - 3.46 / 3. - 12.2 / 3.36 13.9 / 3.34 12.5 / 3.41 Table 22. Ablation studies for multi-stage training, We include additional tasks that are not reported in Table 7. For Few-shot VC and Few-shot Sound, we report results under the 1-shot setting. We use NA to indicate that the model cannot perform the task without the stage 4 training. Setting AudioCaps MusicCaps Lyric Recognition InstructTTS Few-shot VC Few-shot Sound w/o Stage 4 w/o Experts 1B Ours (3B) 0. 0.22 0.30 0.69 4.56 4.33 4. 5.14 29.7 34.3 32.8 8.8/35.2/3.41 9.7/22.9/3. NA NA 7.9/31.1/3.16 21.3/0.82/3.66 28.57 7.3/42.3/3. 18.61/0.89/3.74 NA NA 48.7 59.8 B.6.3. AUDIO/MUSIC CAPTION PERFORMANCE COMPARISON Table 23 shows the performance comparison on audio and music caption benchmarks. B.6.4. AUDIO GENERATION PERFORMANCE COMPARISON Table 24 shows the text-to-audio generation performance on audio caption tasks. B.6.5. ZERO-SHOT EXPERIMENTS FOR 1B MODEL Table 25 shows the performance comparison between our 3B and 1B models. We find that the 1B models zero-shot performance is far behind the 3B version, which further highlights the importance of model size for model generalization. In Table 22, we report more task performance comparison about the ablation study. C. Limitation In this study, we focus on building multi-task audio foundation model that supports diverse audio understanding and generation tasks. It can also generalize to many unseen tasks in few-shot or zero-shot settings. However, several limitations remain. (1) To improve reconstruction quality for sound and music, we adopt flow-based decoder to recover waveforms from semantic tokens. The multi-step decoding procedure in flow matching increases inference latency for generation. In the future, it is necessary to explore few-step decoding (e.g., two steps) to better balance quality and generation speed. (2) Although UniAudio 2.0 demonstrates the ability to handle unseen tasks, there is still room for improvement. In addition, it has not yet been shown to solve arbitrary audio-related tasks. We acknowledge that the set of supported unseen tasks is closely related to the training data. For example, the model currently cannot handle speech diarization, likely because we do not include diarizationor duration-related supervision during training. (3) Due to limited GPU resources, we have not fully explored scaling behaviors (i.e., scaling laws) of UniAudio 2.0. We only conduct experiments on 1Band 3B-parameter variants. In the future, scaling to 7B and larger models is promising direction. (4) Due to the relatively limited amount of sound and music data compared to speech data, UniAudio 2.0 currently performs better on speech-related tasks. In future work, expanding and improving sound and music datasets is expected to further Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 23. Captioning results on audio and music. Audio Caption Music Caption Model CIDEr Relevance Fluency Relevance Fluency SALMON 13B Music Flamingo Audio Flamingo Audio Flamingo 2 Audio Flamingo 3 Qwen2.5-Omni UniAudio 2.0 (Ours) 0.355 0.51 0.58 0.79 0.39 0.69 5.41 5.25 5.71 6.34 5.61 5.51 8.06 8.01 8.13 8.24 8.21 8.31 3.24 4.74 5.77 5.33 5.14 7.92 8.19 8.24 7.90 8. Table 24. Audio generation results. We report these baselines results from Stable Audio Open paper. Model AudioLDM2-large Stable Audio Open AudioGen Ours KL 1.57 2.14 1.42 3.26 FD CLAP-score 170.31 78.24 186.53 50.69 0.41 0.29 0.45 0.17 enhance performance in these domains. (5) This work primarily focuses on pre-training design choices, such as the audio tokenizer and the unified LLM architecture. As result, we do not extensively investigate post-training strategies (e.g., multi-task SFT and reinforcement learning). We plan to incorporate more post-training techniques to further improve UniAudio 2.0. (6) We acknowledge that the set of compared models is not exhaustive. This is partly because many related models are not publicly available, and partly because our framework supports broad spectrum of tasks, which makes comprehensive comparisons challenging. We respect and appreciate all prior work in this area, even if some are not explicitly discussed due to space limitations. We also do not claim that UniAudio 2.0 universally outperforms all existing approaches; instead, different model architectures, different special task design (e.g. special models for TTS, ASR, diffusion-based unified models) also offer complementary strengths. 31 Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization Table 25. Zero-shot results comparison on different tasks. Metrics: MMLU reports Acc (%); InstructS2S-Eval reports S2S/S2T Acc; DSR reports WER (%); A-I-TTS reports SIM / Style-Acc (%) / WER (%) / UTMOSv2; SpeechSound reports WER (%) / CLAP-score / UTMOSv2. S2S denotes speech-to-speech instruction-following and S2T denotes speech-to-text instruction-following. A-I-TTS denotes audio+caption guided speech generation. Task Text S2S DSR Model LLAMA 3.2 1B LLAMA 3.2 3B UniAudio 2.0 (Ours 3B) Ablation (Ours 1B) LLAMA-Omni SpeechGPT Ours Ablation (Ours 1B) Qwen2.5-Omni UniAudio 2.0 (Ours 3B) Ablation (Ours 1B) Score 34.14 47.63 44.1 30.2 3.47 / 3.99 2.19 / 2.98 2.16 / 3.66 1.12 / 1.41 80.6 19.4 61.1 A-I-TTS UniAudio 2.0 (Ours 3B) Ablation (Ours 1B) 0.89 / 32.62 / 11.57 / 2.87 0.62 / 5.2 / 14.8 / 2. Speech-S UniAudio 2.0 (Ours 3B) Ablation (Ours 1B) 6.15 / 0.11 / 2.96 6.79 / 0.04 / 2."
        }
    ],
    "affiliations": [
        "Independent",
        "The Chinese University of Hong Kong, China"
    ]
}