{
    "paper_title": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding",
    "authors": [
        "Pengcheng Li",
        "Xulong Zhang",
        "Jing Xiao",
        "Jianzong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio watermarking, has not been adequately studied. In this paper, we design a dual-embedding watermarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods."
        },
        {
            "title": "Start",
            "content": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding Pengcheng Li1,2, Xulong Zhang1, Jing Xiao1, Jianzong Wang1(cid:66) 1Ping An Technology (Shenzhen) Co., Ltd. 2University of Science and Technology of China lipengcheng@ustc.edu, zhangxulong@ieee.org, xiaojing661@pingan.com.cn, jzwang@188.com 4 2 0 2 9 2 ]"
        },
        {
            "title": "M\nM",
            "content": ". [ 1 7 2 6 9 1 . 9 0 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio watermarking, has not been adequately studied. In this paper, we design dual-embedding watermarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods. The code is available at https://github.com/PecholaL/IDEAW."
        },
        {
            "title": "Introduction",
            "content": "Digital watermarking (Singh et al., 2023) embeds messages indicating the ownership or authenticity into multimedia like images, video and audio, imperceptibly. This technique is widely used for ownership statements and anti-counterfeit. Imperceptibility and robustness are the two most challenging requirements for digital watermarking, which means, it is expected to be hard to feel the presence of the embedded watermark with human perception, and the watermark can be preserved and Equal contribution. (cid:66) Corresponding author. Figure 1: (a) Pipeline of robust neural audio watermarking. (b) Embedding strategy of existing methods. (c) Dual-embedding strategy of IDEAW. extracted accurately even after the watermarked media has been subjected to unintentional damage or malicious removal attacks. Audio watermarking has been around for decades. Traditional techniques (Singha and Ullah, 2022; Zhang et al., 2023) embeds the watermark into either the time-domain or transform-domain of audio signals via algorithms designed based on expert knowledge (Prabha and Sam, 2022). The robustness of traditional watermarking methods generally stems from subjective design, which results in limitations. The advancement of deep learning brings new solutions to steganography (Hussain et al., 2020; Chanchal et al., 2020) and digital watermarking techniques (Amrit and Singh, 2022; Singh and Singh, 2023). End-to-end neural watermarking model completes the embedding and extraction process in each training iteration and constrains the imperceptibility and the integrity of the extracted watermarks through the designed training objectives. The attack layer which simulates common damages on the watermarked media is introduced into the Embedder-Extractor (i.e. encoder-decoder) structure to guarantee the robustness. Neural audio watermarking is currently in its early stages. As human auditory perception is sometimes more sensitive than visual perception (Lee et al., 2019). It can also easily distinguish noise, making subtle alterations caused by watermarking to be perceived. As for the robustness of watermarking. The Embedding-AttackingExtracting pipeline of neural audio watermarking is shown in Fig. 1(a), where the attack layer simulates various removal attacks on watermarked audio during training. Redundancy is required for embedding digital watermarks to enhance applicability. The same watermark is repeatedly embedded at various locations within an audio segment. However, this strategy raises the issue of locating. The embedding location of the watermark is unknown during extraction in practical scenarios. Additionally, trimming and splicing cause changes in the watermarking location. Compared to the traditional method, the extraction of neural audio watermarking relies on the forward process of neural networks which induces non-negligible time cost that increases with the complexity of the network. Existing methods typically use an exhaustive approach, extracting synchronization code and watermark message together step by step, as shown in Fig. 1(b). Localization efficiency is an issue that neural audio watermarking must face. The symmetry of the embedding and extraction processes of watermarking provides the invertible neural network with ample opportunities, but the introduction of the attack layer disrupts the original symmetry (Liu et al., 2019). In other words, due to the presence of the attack layer, the output of the encoder (i.e. watermarked audio) and the input of the decoder (i.e. attack-performed watermarked audio) are inconsistent, while the encoder and decoder are opposing and share parameters, this mismatch limits the training effect. In this paper, we propose model called Invertible Dual-Embedding Audio Watermarking, IDEAW, which uses dual-stage invertible neural network with dual-embedding strategy to embed watermark message and synchronization code (referred to as locating code in this paper) separately, as illustrated in Fig. 1(c). During extraction, we first extract the less computational cost locating code. Upon successful matching, the extraction of the message which has more computational cost is conducted. This also makes it possible to enlarge the capacity of watermarking flexibly. To alleviate the asymmetric impact caused by the attack layer, we apply balance block to enhance the training stability while preserving the characteristics of the invertible neural network. Our contributions in this paper can be summarized as follows: Considering the characteristics of neural audio watermarking, we design dual-embedding strategy to embed the watermark message and locating code separately, to accelerate the locating process. We introduce the balance block to alleviate the asymmetry caused by the attack layer between the invertible network and retain the symmetry of the invertible neural network. The proposed watermarking model is able to embed more bits of watermark while ensuring imperceptibility and robustness."
        },
        {
            "title": "2.1 Neural Audio Watermarking",
            "content": "The neural audio watermarking model is typically composed of two neural networks for watermark embedding and extraction in the Short Time Fourier Transform (STFT) or Discrete Wavelet Transform (DWT) domain. Pavlovic et al. (Pavlovic et al., 2020) design an Encoder-Decoder architecture for speech watermarking, where the encoder and the decoder form an adversarial relationship and are trained together. Hereafter, they introduce an attack layer to their previous work to form an Encoder-Attack Layer-Decoder structure, enhancing the robustness of the DNN-based speech watermarking (Pavlovic et al., 2022). WavMark (Chen et al., 2023) considers the extraction as the inverse process of embedding the watermark, and leverages an invertible neural network to perform embedding and extraction for audio watermarking. DeAR (Liu et al., 2023) focuses on the threat of rerecording attack to audio watermarking, modelling re-recording as several differentiable processes. In addition to imperceptibility and robustness, capacity and locating effectiveness are also important criteria for evaluating audio watermarking methods. Compared to traditional watermarking methods, most neural audio watermarking methods suffer Figure 2: Architecture of IDEAW and the training objectives. low capacity and do not consider the high consumption resulting from the locating process. 2.2 Invertible Neural Network NICE (Dinh et al., 2015) firstly introduces the conception of an Invertible Neural Network (INN), normalizing flow-based framework, learning transformation that converts data that follows the original distribution to predefined distribution. Dinh et al. improve the performance of INN through convolutional layers in Real NVP (Dinh et al., 2017). Ardizzone et al. introduce the conditional INN (cINN) (Ardizzone et al., 2019) to establish control over the generation. Behrmann et al. (Behrmann et al., 2019) utilize ResNet as the Euler discretization of ordinary differential equations and prove that the invertible ResNet can be constructed by changing the normalization mechanism. INN has been widely applied in generation (Dinh et al., 2015, 2017; Kingma and Dhariwal, 2018; van der Ouderaa and Worrall, 2019), image super-resolution (Lugmayr et al., 2020), image compression (Wang et al., 2020), image-to-image translation (van der Ouderaa and Worrall, 2019), digital steganography (Lu et al., 2021; Mou et al., 2023), etc."
        },
        {
            "title": "3.1 Overall Architecture of IDEAW",
            "content": "Fig. 2 showcases the architecture of our proposed audio watermarking model, IDEAW, which follows an Embedder-Attack Layer-Extractor structure, where the embedder and extractor are carefully designed as dual-stage structures for embedding messages and locating codes at vertical latIn the embedding process, an itude separately. Lmbit binary watermark message {0, 1}Lm is embedded into fixed-length audio chunk in the STFT domain via the first stage INN, then the second stage INN embeds binary locating code {0, 1}Lc into the audio containing from the former step. The final watermarked audio is reconstructed through Inverse Short-Time Fourier Transform (ISTFT). discriminator distinguishes between the host audio and the watermarked audio to guarantee the imperceptibility of watermarking. In the extraction process, the two-stage extractor first extracts from the watermarked audio and then m, in the reverse order of embedding. An attack layer is introduced to enhance the robustness to various removal attacks. The extractor must accurately extract and from the attack-performed watermarked audio. The balance block aims to alleviate the asymmetry introduced by the attack layer, preserving the symmetry of INN."
        },
        {
            "title": "3.2 Dual-stage INN for Dual-Embedding",
            "content": "To vertically separate the locating code and message, dual-stage INN is designed, with each stage designated as INN#1 for the embedding and extraction of watermark message, and INN#2 for that of locating code. Each INN consists of several invertible blocks that take transformed audio and the watermark (i.e. the watermark message or locating code) as inputs, producing two outputs. We refer to these input-output pairs as two data streams: the audio stream and the watermark stream. The audio stream outputs watermarked audio during the embedding process, while the watermark stream provides watermark message during extraction. Fig. 3 showcases the architecture of the invertible block, in which the block processes the data as code is extracted from the final watermarked audio via INN#2R and compared with the original locating code. The integrity loss is as follows. Linteg = ˆm m2 + INN#1R(INN#1(x, m)a, xaux1)wm m2 + INN#2R(xwmd, xaux2)wm c2 (4) In practical scenarios, watermarks are embedded into several segments of audio as shown in Fig. 1 (c). However, watermarked audio may be trimmed or spliced (known as the de-synchronization attacks (Mushgil et al., 2018)), making it challenging to determine the watermark location. The extractor extracts and matches the locating code quickly as INN#2 is lighter and costs less computation than INN#1, extracting the watermark message only if the locating code is matched. In addition, similar training manner to the shift module in WavMark (Chen et al., 2023) is deployed in our proposed model, which helps the extractor to gain the ability to extract watermark from proximity location. 3."
        },
        {
            "title": "Imperceptibility Guaranty",
            "content": "As one of the most important indicators for evaluating digital watermarking, imperceptibility ensures that the embedding of the watermark cannot be distinguished from human auditory perception. In IDEAW, we take two measures to ensure and improve the imperceptibility of watermarking. The perceptual loss intuitively requires that the difference between the watermarked audio and the original host audio is as narrow as possible, that is, reducing the impact of watermarking on the host audio. The perceptual loss is shown in Eq. 5. Lpercept = xwmd x2 (5) Whats more, we leverage discriminator to distinguish the watermarked audio from the origin audio. Essentially, the discriminator is binary classifier which classifies host and watermarked audio labeled with 0 and 1 (denoted as label y) respectively. The discriminator aims to identify the watermarked audio while the embedder tries to hide the watermark so that the embedder and the discriminator form an adversarial relationship and mutually force each other during training (Goodfellow et al., 2020). The discriminate loss and identify loss is as follows: Ldiscr = log(D(x)) (1 y) log(1 D(x)) (6) Figure 3: Structure and forward/backward processes of the invertible block. illustrated in Eq. 1: xi+1 = xi exp(α(ψ(si))) + ϕ(si) si+1 = si exp(α(ρ(xi+1))) + η(xi+1) (1) where denotes the host data including the original audio fed to INN#1 and the audio with message embedded fed to INN#2, denotes the secret data including the message fed to INN#1 and the locating code fed to INN#2. α() is sigmoid function, while ψ(), ϕ(), ρ() and η() are subnets which are constructed from dense blocks. In the first embedding stage, INN#1 embeds the watermark message into the host audio, where the audio and bit sequences are transformed into the time-frequency domain via STFT at first, respectively. Then INN#2 embeds the locating code into the audio stream output of INN#1. The dualembedding can be described by Eq. 2. Subscripts and wm of INN() denote the output of the audio stream and watermark stream of INN, respectively. xwmd = INN#2(INN#1(x, m)a, c)a (2) The extraction process of IDEAW is exactly the opposite of the embedding process. INN#2 firstly extract the locating code from the watermarked audio, then the output of the audio stream from INN#2 is sent to INN#1 to extract watermark message. Eq. 3 describes the dual extraction to obtain the embedded message. ˆm = INN#1R(INN#2R(xwmd, xaux2)a, xaux1)wm (3) where xaux represents the randomly sampled signal which is fed to the message stream of INN. The subscript represents the reverse process of the reversible network as the extraction process employs the same network and parameters as the embedding process, only with the order reversed. During training, the message is extracted by INN#1R after each stage of embedding, then compared with the original message m. The locating Lident = log(1 D(xwmd)) (7) 3.4 The Robustness and Symmetry of INN The robustness of watermarking method is critical to the attackers ability to effectively remove the embedded watermarks. Generally, attackers try to remove the watermark from the watermarked audio through several removal techniques including passing the audio through filters only to maintain the information that can be perceived by humans, or adding noises to the watermarked audio to interfere with accurate extraction. Effective watermark removal requires that the watermark cannot be extracted correctly from the watermark-removed audio and that the watermark-removed audio should be usable, i.e. the listening quality should not significantly decrease, otherwise the removal of the watermark is meaningless. In order to endow the neural audio watermarking with robustness against various watermark removal attacks, an attack layer is incorporated into the watermarking model and trained alongside the embedding and extraction networks. The attack layer subjects the watermarked audio (i.e. the output from the embedder) to variety of attacks. Subsequently, the extractor attempts to extract the locating code and watermark message from the audio that has undergone attack as accurately as possible. Considering the effectiveness of the watermark removal, the predefined attacks should ensure the quality of attack-performed watermarked audio does not degrade too much. These attacks include Gaussian additive noise, lower-pass filter, MP3 compression, quantization, resampling, random dropout, amplitude modification and time stretch. However, the introduction of the attack layer disrupts the symmetry of the entire embeddingextraction process, impacting the training of INN. We define the integral dual-embedding process and extraction process as () and 1() respectively. During the embedding process, IDEAW performs (x, m, c) = w, where x, m, c, are host audio, watermark message, locating code and watermarked audio as mentioned above. And follows the distribution of watermarked audio PW (w) (Ma et al., 2022). While in the extraction process, thanks to the parameter-sharing between the embedder and the extractor, and can be easily sampled with m, = 1(w) according to PW (w). But the including of the attack layer causes changes in PW (w) leading to PW (w), while the extraction process is still based on the unchanged PW (w), which affects the performance of watermarking. The parameter-sharing strategy of INN limits the extractors learning ability to adapt to the attacked audio, resulting in distorted watermarking performance. To simultaneously maintain the parametersharing of INN and the symmetry of INNs training, balance block is employed to mitigate the asymmetry caused by the attack layer and stabilize the models symmetric structure. The balance block consists of group of dense blocks (Huang et al., 2017) which process the input to equal-size output, providing extra trainable parameters for the extractor. This manner learns to transform the attack-performed watermarked audio distribution PW (w) to the revised distribution ˆW ( ˆw) that close to the expected distribution PW (w) under the guidance of the mentioned integrity loss, to counteract the effects of the offset introduced by the attack layer without bothering the embedder."
        },
        {
            "title": "3.5 Training Strategy",
            "content": "Simultaneously ensuring accuracy, imperceptibility and robustness is sticky. Therefore, the training of IDEAW is divided into two stages: The first stage only considers the imperceptibility and the watermark integrity of extraction, aiming to build dual-stage INN that can embed the watermark imperceptibly and extract the watermark accurately. In the second stage, the requirement for the robustness of watermarking is introduced. The attack layer and balance block are incorporated into the model, and the entire model is trained collectively. The same total loss function, as introduced in Eq. 8, is applied for both training stages, where λ1, λ2 and λ3 are weights of each component, but note that the second stage contains more trainable parameters. The discriminator is trained along with the watermarking model in each iteration with the loss function in Eq. 6. The pseudo script of the training process as well as the acquisition of the total loss is shown in Section A.1 of the appendix. Ltotal = λ1Linteg + λ2Lpercept + λ3Lident (8)"
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Settings 4.1.1 Dataset and Implementation IDEAW is trained on VCTK corpus (Yamagishi et al., 2016) and FMA corpus (Defferrard et al., 2017). VCTK comprises over 100 hours of multispeaker speech data, while FMA contains large amount of music audio. These two types of data are prevalent in scenarios where audio watermarking is commonly applied. All the audio is resampled to 16,000 Hz and split into 1-second segments during training. STFT and ISTFT operations with parameters of {n_f = 1000, hop_length = 250, win_length = 1000} perform the interdomain transformation. Two Adam optimizers (Kingma and Ba, 2015) with {β1 = 0.9, β2 = 0.99, ϵ = 108, lr = 105} (with StepLR scheduler) are leveraged for the introduced two-stage training. Super-parameters λ1, λ2 and λ3 in total loss (Eq. 8) are set to 1, 0.1 and 0.1, respectively. Each stage contains 100,000 iterations. The attacks are sample-wise, each audio segment in the batch undergoes different types of attacks during training. The configuration of the attack layer is shown in Section A.2 of appendix. We set the length of the locating code to 10 bits, so there is about 1/210 probability of potential conflicts with extracted non-locating code bit sequences. In each batch, the locating code and watermark message are randomly generated to ensure that the trained model can handle any arrangement of 0-1 sequences. We select two existing neural audio watermarking works, WavMark (Chen et al., 2023) and DeAR (Liu et al., 2023), as baselines."
        },
        {
            "title": "Method",
            "content": "SN ACC Capacity (dB) (%) (bps) DeAR IDEAW10+10 WavMark IDEAW22+10 IDEAW46+10 26.18 40.43 38.55 37.72 35. 99.61 99.64 99.35 99.52 99.44 8.8 20 32 32 56 Table 1: Comparison of the basic metrics with baseline methods. The ACC of IDEAW is calculated from the locating code and message. 4.1.2 Metrics Signal-to-noise ratio (SNR) measures the impact of the watermarking. It is calculated as 10 times the logarithm of the ratio of host audio power to watermark noise power. Accuracy (ACC) measures the differences between the extracted message and the ground-truth message. ACC is typically used to measure the robustness of watermarking methods. Capacity is the number of watermark bits that can be embedded per second of audio while ensuring imperceptibility and ACC of the watermarking. 4.2 Results 4.2.1 Overall Comparison The comparison of basic metrics of various watermarking methods is shown in Table 1. As each baseline watermarking model is designed for different capacities, and IDEAW owns the maximum one, we also train the other two IDEAW models which have similar or larger capacities as other methods to alleviate the impact of capacity on comparison. As the locating code and message are separated in our proposed model, we take the total length of the locating code and message as the capacity of IDEAW and the length of the locating code is fixed to 10 bits. We find that the watermarking model with lower capacity obtains better imperceptibility and higher accuracy. IDEAW gains considerable SNR and ACC when designed with large payload. Fig. 4 illustrates the impact of the watermarking on low-energy speech waveform and highenergy music waveform. The same watermark is iteratively embedded into the audio. From Fig. 4(a), we can see that the original audio in the foreground and the watermarked audio in the background almost overlap. Fig. 5 shows comparison of the linear-frequency power spectrograms of the original and watermarked audio, illustrating the impact of the watermarking in the time-frequency domain. More watermarked audio samples, waveform samples and practical application examples are available at our demo page https:// largeaudiomodel.com/IDEAW."
        },
        {
            "title": "4.2.2 Robustness Comparison\nWe measure the watermark extraction accuracy of\neach model under different attacks to evaluate their\nrobustness. Eight common attacks including Gaus-\nsian additive noise (GN), lower-pass filter (LF),\nMP3 compression (CP), quantization (QZ), ran-\ndom dropout (RD), resampling (RS), amplitude",
            "content": "Attack Method DeAR IDEAW10+10 WavMark IDEAW22+10 IDEAW46+10 GN 35dB 99.61 99.47 97.84 99.33 98.72 LF 5kHz 99.04 98.62 98.54 98.53 98.12 CP 64kbps QZ 29 99.55 99.41 98.81 99.15 99.00 99.63 98.86 96.60 98.73 98.61 RD 0.1% 99.61 99.60 98.68 99.23 98.84 RS 200% 99.62 99.11 98.42 99.02 98.87 AM 90% 99.61 99.49 99.29 99.48 99.42 TS 90% 99.31 98.95 95.35 98.82 98.66 Table 2: Comparison of the robustness with baseline methods. The robustness is evaluated according to ACC(%) under different watermark removal attacks. Figure 4: Waveforms of (a) host audio (foreground) and watermarked audio (background), (b) host audio and tenfold-magnified residual caused by watermarking, (c) local details (100 points) of the (a) and (b). The left audio is low-energy speech audio while the right is highenergy music audio. modification (AM) and time stretch (TS) are taken into consideration. As mentioned above, watermark removal attacks need to consider the degree of damage to audio, and the strength settings of each attack continue the previous research settings (Chen et al., 2023). The robustness evaluation results are shown in Table 2. IDEAW shows comparable robustness to the baseline model while with larger capacity."
        },
        {
            "title": "4.2.3 Locating Test",
            "content": "To compare the time cost of each locating method, we embed the watermark (10-bit locating code or synchronization code and 46-bit message) at different locations and use different methods for loFigure 5: Linear-frequency power spectrograms of lowenergy speech audio (left) and high-energy music audio (right). (a) the host audio, (b) the watermarked audio. cating. The tested methods include (1) the same as WavMark (Chen et al., 2023), synchronization code and message are concatenated and embedded into the audio segment together via single-stage INN model. The model is trained with the shift strategy. The synchronization code and message are extracted at the same time iteratively during locating. The step size is 10% of the chunk size, the same as the baseline method. (2) the proposed method, only extracts the locating code during the locating process with step size of 10% of the chunk size. Method (1) builds single stage INN watermarking model that has the same layer quan-"
        },
        {
            "title": "Method",
            "content": "SN ACC (dB) (%) M1 M2 IDEAW46+10 32.30 35.27 35. 99.40 98.70 99.44 Table 3: Basic metrics comparison for ablation study. Attack Method M1 M2 IDEAW46+10 GN 35dB 98.25 98.14 98.72 LF 5kHz 98.18 97.79 98.12 CP 64kbps 98.79 98.55 99.00 QZ 98.73 96.07 98.61 RD 0.1% 98.90 98.24 98.84 RS 200% 98.46 98.39 98.87 AM 90% 99.28 97.62 99.42 TS 90% 98.58 96.57 98.66 Table 4: Comparison of the robustness in ablation study. The ACC(%) of the extracted message and locating code under attack via each model. built. (1) M1 (w/o discriminator) removes the discriminator from the proposed method. (2) M2 (w/o balance block) removes the balance block from the proposed method during the robustness training. These models as well as the proposed model are trained on the same datasets and in the same manner. We measure the basic metrics and the robustness of each model."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose neural audio watermarking model, IDEAW, which embeds the locating code and watermark message separately via designed dual-stage INN. In response to the challenge of neural audio watermarking localization, the dual-embedding strategy avoids the huge computation of extracting synchronization code and message at the same time, accelerating the locating process. On the other hand, we make an effort to mitigate the asymmetry factor introduced by the attack layer. The balance block is leveraged to provide the extractor with subtle training spatial while maintaining the advantage of the invertible network. Experimental results show that IDEAW achieves satisfactory performance from comprehensive perspective of imperceptibility, capacity, robustness and locating efficiency. Figure 6: Comparison of locating time consumption for different methods at various watermark embedding locations (the location is indicated by the seconds from the start of audio to the watermarking location). tity as the proposed dual-stage model. Note that in order to eliminate the impact of errors during locating as we cannot guarantee the ACC of the model with carrying capacity of 56 bits in Method (1), we only perform extraction without further verification, until reaching the watermark location, as the former experiments show that each model can reach the ideal accuracy with designed capacity. The measurement of locating time for each method is conducted on the same device and takes an average of 100 processes for each watermarking location. The comparison of the time consumption of each method on the same device is shown in Fig. 6. The results show that the proposed method reduces time overhead by approximately 40% 50%. Especially when the watermark is far from the head, the advantage of our dual-embedding strategy is more obvious."
        },
        {
            "title": "4.3.1 Setting Up",
            "content": "To validate the positive effects of the proposed balance block and the discriminator on IDEAWs performance, we conduct an ablation study. The following models related to the proposed methods are"
        },
        {
            "title": "Acknowledgement",
            "content": "This work focuses on the high overhead problem of localization of neural audio watermarks by innovatively separating the locating code from the watermark message and embedding/extracting them separately. However, we find that the dual-embedding watermarking method has the following limitations so far: The lengths of the locating code and messages of the trained model cannot be flexibly adjusted because the design of the dual-stage invertible neural network fixes the lengths of both, we cannot designate the bits starting at arbitrary lengths as synchronization codes like previous methods. We find that the watermark embedded in lowenergy audio is less imperceptible than in high-energy audio via the proposed model. The reason might be that, apart from selecting two types of datasets for training, the design of the model as well as its training strategy does not consider the carriers energy."
        },
        {
            "title": "Future Works",
            "content": "With the rapid development of social media and the increasing awareness of copyright, digital watermarking technology has promising prospects for widespread application. Neural audio watermarking remains valuable area of research due to its robustness, independence from expert intervention, and scalability, compared to traditional methods. However, neural audio watermarking has not yet fully surpassed traditional digital watermarking methods, particularly in terms of capacity. Future works on neural audio watermarking may consider, but is not limited to, the following aspects: Supported by the Key Research and Development Program of Guangdong Province (grant No. 2021B0101400003) and the corresponding author is Jianzong Wang (jzwang@188.com)."
        },
        {
            "title": "References",
            "content": "Preetam Amrit and Amit Kumar Singh. 2022. Survey on watermarking methods in the artificial intelligence domain and beyond. Computer Communications, 188:5265. Lynton Ardizzone, Carsten Lüth, Jakob Kruse, Carsten Rother, and Ullrich Köthe. 2019. Guided image generation with conditional invertible neural networks. arXiv preprint arXiv:1907.02392. Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jörn-Henrik Jacobsen. 2019. Invertible residual networks. In International conference on machine learning, pages 573582. PMLR. Chanchal, Malathi, and Gireesh Kumar. 2020. comprehensive survey on neural network based imIn the 4th International age data hiding scheme. Conference on IoT in Social, Mobile, Analytics and Cloud, pages 12451249. IEEE. Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, and Furu Wei. 2023. Wavmark: Watermarking for audio generation. CoRR, abs/2308.12770. Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. 2017. FMA: dataset for music analysis. In The 18th International Society for Music Information Retrieval Conference, ISMIR, pages 316 323. Laurent Dinh, David Krueger, and Yoshua Bengio. 2015. NICE: non-linear independent components estimation. In The 3rd International Conference on Learning Representations, ICLR, Workshop Track Proceedings. Maximizing capacity through novel neural network architectures or embedding strategies. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2017. Density estimation using real NVP. In The Fifth International Conference on Learning Representations, ICLR, Conference Track Proceedings. Exploring adaptive approaches to narrow the performance gap between high-energy and low-energy audio. Considering implementing watermark embedding directly within the audio generation model to obtain generated audio with watermarks, rather than using post-processing watermarking methods. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Communications of the ACM, 63(11):139144. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. 2017. Densely connected convolutional networks. In Conference on Computer Vision and Pattern Recognition, pages 47004708. Israr Hussain, Jishen Zeng, Xinhong Qin, and Shunquan Tan. 2020. survey on deep convolutional neural networks for image steganography and steganalysis. KSII Transactions on Internet and Information Systems, 14(3):12281248. Diederik P. Kingma and Jimmy Ba. 2015. Adam: method for stochastic optimization. In The 3rd International Conference on Learning Representations, ICLR, Conference Track Proceedings. Durk Kingma and Prafulla Dhariwal. 2018. Glow: Generative flow with invertible 1x1 convolutions. Advances in Neural Information Processing Systems, 31. Hyungeol Lee, Eunsil Lee, Jiye Jung, and Junsuk Kim. 2019. Surface stickiness perception by auditory, tactile, and visual cues. Frontiers in Psychology, 10:2135. Chang Liu, Jie Zhang, Han Fang, Zehua Ma, Weiming Zhang, and Nenghai Yu. 2023. Dear: deeplearning-based audio re-recording resilient watermarking. In The 37th AAAI Conference on Artificial Intelligence, pages 1320113209. Kosta Pavlovic, Slavko Kovaˇcevic, and Igor Ðurovic. 2020. Speech watermarking using deep neural netIn The 28th Telecommunications Forum, works. pages 14. Kosta Pavlovic, Slavko Kovaˇcevic, Igor Ðurovic, and Adam Wojciechowski. 2022. Robust speech watermarking by jointly trained embedder and detector using DNN. Digital Signal Processing, 122:103381. K. Prabha and I. Shatheesh Sam. 2022. survey of digital image watermarking techniques in spatial, transform, and hybrid domains. International Journal of Software Innovation, 10(1):121. Himanshu Kumar Singh and Amit Kumar Singh. 2023. Comprehensive review of watermarking techniques in deep-learning environments. Journal of Electronic Imaging, 32(3):031804031804. Roop Singh, Mukesh Saraswat, Alaknanda Ashok, Himanshu Mittal, Ashish Tripathi, Avinash Chandra Pandey, and Raju Pal. 2023. From classical to soft computing based watermarking techniques: comprehensive review. Future Generation Computer Systems, 141:738754. Yang Liu, Mengxi Guo, Jian Zhang, Yuesheng Zhu, and Xiaodong Xie. 2019. novel two-stage separable deep learning framework for practical blind watermarking. In The 27th ACM International Conference on Multimedia, pages 15091517. Amita Singha and Muhammad Ahsan Ullah. 2022. Development of an audio watermarking with decentralization of the watermarks. Journal of King Saud University-Computer and Information Sciences, 34(6):30553061. Shao-Ping Lu, Rong Wang, Tao Zhong, and Paul Rosin. 2021. Large-capacity image steganography based on invertible neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1081610825. Tycho F. A. van der Ouderaa and Daniel E. Worrall. 2019. Reversible gans for memory-efficient imageto-image translation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4720 4728. Yaolong Wang, Mingqing Xiao, Chang Liu, Shuxin Zheng, and Tie-Yan Liu. 2020. Modeling lost information in lossy image compression. CoRR, abs/2006.11999. Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. 2016. Cstr vctk corpus: English multispeaker corpus for cstr voice cloning toolkit. Guofu Zhang, Lulu Zheng, Zhaopin Su, Yifei Zeng, and Guoquan Wang. 2023. M-sequences and sliding window based audio watermarking robust against large-scale cropping attacks. IEEE Transactions on Information Forensics and Security, 18:11821195. Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and Radu Timofte. 2020. Srflow: Learning the superresolution space with normalizing flow. In Computer Vision - ECCV - The 16th European Conference, volume 12350, pages 715732. Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li, Huizhu Jia, and Xiaodong Xie. 2022. Towards blind watermarking: Combining invertible and noninvertible mechanisms. In The 30th ACM International Conference on Multimedia, pages 15321542. Chong Mou, Youmin Xu, Jiechong Song, Chen Zhao, Bernard Ghanem, and Jian Zhang. 2023. Largecapacity and flexible video steganography via invertible neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2260622615. Baydaa Mohammad Mushgil, Wan Azizun Wan Adnan, Syed Abdul-Rahman Al-Hadad, and Sharifah Mumtazah Syed Ahmad. 2018. An efficient selective method for audio watermarking against desynchronization attacks. Journal of Electrical Engineering and Technology, 13(1):476484."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Training Pipeline of IDEAW Alg. 1 shows the training pipeline and the acquisition of each loss function. The length regulators draw mappings between the bit sequence space (i.e. locating code and message) and space whose elements are of equal length to the audio waveform segment. Note that x, m, in the pseudo script refers exclusively to the host audio waveform, message bit sequence and locating code, while the prime denotes the data in the STFT domain. Algorithm 1 Acquisition of total loss Ltotal in the training stage. Input: host audio segment x, watermark message m, locating code Module: message embedder Emb1, locating code embedder Emb2, message extractor Ext1, locating code extractor Ext2, length regulator LR1, LR2, LR3, LR4, attack layer Att, balance block Operation: short-time Fourier transform ST (), inverse short-time Fourier transform IST () Parameter: robustness training flag Robust, loss weights λ1, λ2, λ3 Output: total loss Ltotal 1: Regular the length, transform to STFT domain: ST (LR1(m)) ST (LR2(c)) ST (x) 2: Embed message: w1 Emb1(x, m) 3: First extraction: ˆm1 LR3(IST (Ext1(x w1))) 4: Embed locating code: Emb2(x w1, c) 5: Obtain watermarked audio waveform: xw IST (x w) 6: Train D: Obtain LD from {x, xw} Perform the backward propagation of 7: if Robust = rue then xw B(Att(xw)) 8: 9: end if 10: Extract locating code: mid, ˆc Ext2(x w) ˆc LR4(IST (ˆc)) 11: Extract message: ˆm LR3(IST (Ext1(x mid))) 12: Obtain losses: Obtain integrity loss Linteg from {m, ˆm1, ˆm, c, ˆc} Obtain perceptual loss Lpercpt from {x, Obtain identify loss Lident from {x, xw} Ltotal λ1Linteg + λ2Lpercept + λ3Lident w} 13: return Ltotal A.2 Configuration of the Attacks The configuration follows previous works. The descriptions and settings are shown in Table 5. ID Attack Description and Configuration LF lower-pass filter CP MP3 compression GN Gaussian additive noise Add Gaussian noise to the watermarked audio and maintain the signalto-noise ratio at approximately 35dB. Pass the audio through lower-pass filter of 5kHz, the range of the filter is set according to the human auditory range Compress the waveform to 64kbps MP3 format and then convert back to wav format. This conversion process results in information loss. Quantize the sample points of watermarked audio waveform to 29 levels. Randomly value 0.1% of the sample points in the watermarked audio to zero. Resample the audio to new sampling rate (200% of the original sample rate) then resample back to the original sample rate. RD random dropout quantization resampling QZ RS AM amplitude modification Multiply 90% to the overall amplitude of the audio by modification TS time stretch factor. Compress the audio in the time domain to 90% of the original length, then stretch it to maintain the original length. Table 5: Descriptions and settings of the attacks."
        }
    ],
    "affiliations": [
        "Ping An Technology (Shenzhen) Co., Ltd.",
        "University of Science and Technology of China"
    ]
}